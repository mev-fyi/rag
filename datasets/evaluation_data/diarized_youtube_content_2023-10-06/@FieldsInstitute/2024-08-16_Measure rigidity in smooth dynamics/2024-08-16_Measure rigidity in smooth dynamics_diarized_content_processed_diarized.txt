00:00:00.440 - 00:00:37.230, Speaker A: Thank you guys for coming, and welcome to the Dynamics morning, I guess, of the conference. It's my great pleasure to introduce our first speaker, Alex Eskin. Alex got a PhD at 93 from Princeton, extremely decorated mathematician. I couldn't, like, fit all this in the notes I have. So, his work starts from number theory to homogeneous dynamics to taekwondo theory. I want to say, since that's my field. He has won numerous awards, including a clay prize, breakthrough prize, and ICM talks at 98 and 2010.
00:00:37.230 - 00:00:39.410, Speaker A: Please welcome Alex Eski.
00:00:43.910 - 00:01:19.260, Speaker B: Thanks for the embarrassing introduction. So, today I'm going to talk about. This is a joint work with Aaron Brown, Simeon Philippe, and Federico Rodriguez Holtz. Okay, sure. I'm sorry. So anyway, so everything which I'm going to say, which is somewhat new, is joined with these people. A lot of the stuff will be reviewed.
00:01:19.260 - 00:01:50.670, Speaker B: So let me start with some sort of review. So, the first topic I want to talk about is, really. Well, first, what is the problem? I mean, what is ergodic theory? So you start with a map from some space to itself. Let's say x is some compact, nice, compact space. And, well, what I really want to do is I want to look at the orbits of a point. So take Xenore X, and then I want to look, I want to understand the orbit of x. So I look at.
00:01:50.670 - 00:02:35.480, Speaker B: And the typical question. So, actually, this is dynamical systems. And if you want the ergodic series, then you also assume that t preserves a measure mu, and always mu of the whole space is equal to one. And so if you pick a point, you want to understand the orbit of this point in some sort of statistical way. And typically what you want is a sequence distribution theorem. So you want, you know, so I want average with respect to. I guess.
00:02:35.480 - 00:03:00.210, Speaker B: So you want this to converge to something. And. And you. Well, the most natural thing for this to do is to converge with respect to the invariant, to integral of f with respect to the measure. Yeah, sorry. Thank you. Thank you.
00:03:00.210 - 00:03:57.006, Speaker B: And so this is a. And so this is actually a serum, which is a classical Burkhoffergolic serum in 1930. So this is true for almost every x, provided mu is ergodic. And what does ergodic mean? It means that any invariant set as measure zero or one. So this is a classical theorem in which, like, huge swaths of mathematics have been built. And it's very wonderful. It's amazing.
00:03:57.006 - 00:04:33.540, Speaker B: In general, it does, like, every serum has a weakness, and the weakness of the serum is this almost every. So you don't really know what happens. For if somebody comes to you and says, I don't care about almost every point I have, this one point I care about. Tell me what happens at that point. Then you have to very politely tell them to go away so you can try to improve this. I mean, for like lots of applications, you really do want to know what happens in individual points. And there is a, there is a situation which was.
00:04:33.540 - 00:05:09.670, Speaker B: So now we are jump, going to jump from 1930s to the 1990s. So this is a serum due to Ratner. Well, actually, maybe I'll state the setup first. But, and also a lot of the stuff is actually also done by Margolis. And so here is the setup. So g is a d group and gamma is lattice in g. So here I should think of g as being like slnr.
00:05:09.670 - 00:06:15.242, Speaker B: So think of gamma as being slnzd. And u is one parameter unipotent subgroup of G. So unipotent just means all eigenvalues are always equal to one. And so an example would be something of this form. So this is, these matrices are a subgroup of sl three, r, which is uniform. This is actually the example which was most famous because it was that of the proof of Doppenheim projection. And so what is the theorem? So then there's kind of three segments which are basically all trying to say.
00:06:15.242 - 00:06:47.140, Speaker B: Yeah, so the point here is that in this system, you can understand the orbit of every single point. So you, I guess, maybe first, what is the system u acts on g mod gamma by that multiplication. So this is a dynamical system, which is nice. It has, it's very nice. I mean, particularly, it's homogeneous Gmod gamma. Every point looks like every other point. This is called homogeneous dynamics.
00:06:47.140 - 00:07:41.454, Speaker B: And the fact that it's unipotent is crucial for the proof. If you drop this assumption, as stated, the theorem is false. But so what? So the point is, you can understand the orbit of every single point. So maybe I'll write down what are the three statements? One is, well, I guess for all x, the closure is. So if I look at the closure of any orbit is a closed orbit of a subgroup l, which is sitting between u and g. So in particular, it's a manifold. So it's a very special kind of manifold.
00:07:41.454 - 00:08:52.070, Speaker B: So the orbit loaders are manifolds. If you drop the assumption that this is unipotent, then orbit closures are typically some sort of complicated chanter sets or whoops, this is very, very complicated. But this is quite a remarkable statement. And again, now, if it wasn't true for all, if you just wanted almost all x just followed immediately from work of zero. But the fact that you understand every x is what the point here, the second statement is that all ergodic invariant measures are essentially supported in a push forwards of the hard measure on some sort of, again, intermediate subgroup. So, so here the closed orbit of subgroup l supports an l invariant measure. And this is saying that all the ergodic invariant measures are this one.
00:08:52.070 - 00:09:31.324, Speaker B: And the search statement, and I'm not going to write at all, but basically any orbit is uniformly distributed in its closure. So of course. So closure depends on the point. But if once you know the closure, then you understand the orbit spreads out even. So, this is, this is proven this morning, 30 years ago. It's amazing. I mean, there's an awful lot of applications of this, like the number theory to other things.
00:09:31.324 - 00:10:10.720, Speaker B: The geometry, people are still finding new ways to use it, which kind of, I think, kind of amazing. So this kind of theorems are kind of the crown jewels from my point of view. So we have time to figure out if you can, and also in what extent can you generalize this. So there is one direction which I'm not going to go into at all, but it's, well, first you can just say, well, what if I drop the unipointed assumption? For example, if I start looking at some diagonalism like e to the minus, like this kind of group, then you can be easy to see that everything goes wrong. Orbits are fractal and so on. This is typical behavior and dynamics in some sense, which you expect. But we don't have this.
00:10:10.720 - 00:10:40.680, Speaker B: So okay, then fine, you don't do this. But then turns out that if you actually work not with one element, but with several different elements, which commute again in this homogeneous setting, then maybe you can say something. And this is, there is a, there is actually fields, metal going for progress on this. There are mineral shelves on this. But this is a well known, very well known problem. But, and then somehow I think it's much more difficult than anything I'm going to say about right now. So I'm going to stay away from it.
00:10:40.680 - 00:11:27.632, Speaker B: Now you can also try to, I mean, the main problem with the proof is that the proof basically, what do you do? You have your unipolar matrix and you conjugate, so you have this unipolar matrix, this is like this, this unipolar matrix, and then you conjugate it by some. So this is u and this is u inverse. And maybe depending on the parameter t, and you kind of look at the result and you look at the matrix centers of the result, and you really want them to be polynomial in t. This is absolutely necessary for the proof. And if they're not just have polynomial upper bounds, it's not good enough, or if it's just anything other than polynomial there, it doesn't work. So the proof is very, very special to the situation. And so it's very hard to chart.
00:11:27.632 - 00:12:09.080, Speaker B: I mean, really, people didn't really know how to generalize this kind of theorems for if, even if there are kind of theorems in other settings. And now there is another theorem, which is actually quite big. There's a Benoit Khan theorem, which is going to be, it kind of has to do with sort of random dynamics, which is going to be something which I'm. But again, in the homogeneous setting, I'll come back to this later. And then also, at some point, Mirzakhani and I actually found some analog of this theorem working on modular space of Riemann surfaces. I'm not really going to talk about this at all. This is technodynamics.
00:12:09.080 - 00:12:39.672, Speaker B: The proof is very, very different from this one. And it is that proof, which is basically what I'm going to talk is what leads to what's happening today. So today I'm going to try to work in a completely different setting. And by the way, if you. If you told me ten years ago that you should try to prove some analog of Ratner's theorem in this setting, I would have just laughed out loud. I mean, it makes absolutely no sense. It's way too general, because, see, all of those theorems kind of seem to require an awful lot of structure.
00:12:39.672 - 00:13:47.164, Speaker B: So you have to be a lot of stuff going on in order to improve something like this, because this is, I mean, very far from expected behavior, but so what, so what is the serum that I'm going to talk about? So I'm going to take m is going to be some sort of compact manifold, and I'm going to pick some. Mu is maybe finally supported measure on the diffeomorphisms. So I'm going to synth in a different. So really, what this is kind of fancy graph. So basically, the example should think of is I pick two dphomorphisms, f one and f two. If I pick two different, and then mu is going to be one half times delta at f one plus one half times delta of f two. So what I'm really going to do is I'm going to do the following.
00:13:47.164 - 00:14:26.790, Speaker B: I'm just going to apply. I'm going to take a point x, and I'm going to apply f one and f two, which is probability one two. And I'm going to get some sort of a random dynamical system. But I can also think about just is the group, I can also think about the orbit. So I can think about just, you know, what is the closure? So I look at the group generated by f one and f two, and then I act on x. And then I look at, for example, ask what, what can I say, say about these kind of things? And again, the answer should be probably, shouldn't it? Probably nothing, because I haven't assumed anything. But it turns out that it somehow seems like sometimes you can almost get something from nothing.
00:14:26.790 - 00:15:09.160, Speaker B: So before I start trying to talk about any sort of theorems, I want to. There is kind of an example in which. Yeah, actually, okay, maybe before I get to the example, let me talk about, I want to introduce one other thing. So I wanted to introduce what's called a stationary measure. So if I want, I mean, if I want to approach this or this question using ergodic theory as opposed to just topological dynamics, I want to have a measure. So the problem is there might not be an invariant measure for both generic non amino group. There is no reason to believe they have an invariant measure.
00:15:09.160 - 00:16:25.908, Speaker B: And so you define a measure and let's say nu on m, it's called stationary. Okay, sorry. If. So what I want is if I want to think about integral. Well, this is gonna, this look gonna look ridiculous. Yeah, sorry. I wanna just, and this is just, this is really just, I should think this is really just push, pushing the forward.
00:16:25.908 - 00:16:54.540, Speaker B: So I push a measure new by f one. It might be different. Then I push the measure by f two, might again be different. But if I take the average, then, then you should get new back. This is what's called a stationary measure. It's not invariant, but it's invariant on average. And stationary measures are somehow the right object in this game.
00:16:54.540 - 00:17:24.039, Speaker B: They're certainly the right object. If you want to talk about orbit closures, because turns out the closure of any orbit supports a stationary measure. It's very easy to prove it does not. Certainly there is no reason to believe there are invariant measure, but. Okay, so I just defined a stationary measure. So now let me look at the example. This is an example, which I think is very familiar to most mathematicians, is just a look at.
00:17:24.039 - 00:18:27.790, Speaker B: Maybe I just pick f one and f two in sl two r two matrices a one and a two and act on the projective space by Mobius transformations. Mobilization. So familiar picture you have the circle infinity f one, they're both kind of, I mean, I'm assuming both of them are hyperbolic. So they're kind of. So f one has a contracting fixed point and an expanding fixed point. So essentially the only, if you look at measures just invariant by a one, it's going to be only here or here, because everything else goes to one of those points. And, but that's not going to be invariant by a two.
00:18:27.790 - 00:19:35.090, Speaker B: So there is no invariant measurement and there are always stationary measures. So there's lots of stationary measures which are not invariant. I'll just. Maybe. So this is another bullet point. Now, orbit closures are complicated. So it's actually any limit set of a foxen group is going to be in all the closure.
00:19:35.090 - 00:20:41.710, Speaker B: I mean you can, you can easily imagine just like to make this double occupy point to get some sort of complicated fractal in general. And the last item is something which, I mean, use a term which is very familiar to all dynamic, but I think I'm going to have to define it. It says all Rapunov exponents are negative. So, so what's the lap enough exponent? Well, if I, if I have this kind of random dynamical system, so I'm going to think about a future which is going to be. So each omega I is either f one or f two. And then I'll write maybe capital f sub n. This is going to be omega zero.
00:20:41.710 - 00:21:30.344, Speaker B: I guess I'll go another order. I just multiply them together. And now if I look at lambda. So I'm going to define operative exponents in the minimal wave, even though it's not the right definition, but just want to say as little about this as possible. These are the eigenvalues of the derivative of. I'm thinking of just of these as being diffeomorphism. So this is a different, has a derivative.
00:21:30.344 - 00:22:04.960, Speaker B: So this is a matrix as eigenvalues. And then the, and I'm also going to, I guess, order them in decreasing order. And the lambda I, this is. I slapped enough exponent is the limit is n, goes to infinity of one over n times log. It's this. So basically the asymptotic growth rate of the end exponent of this product. Now this is nothing the right definition, but it will do for my purposes.
00:22:04.960 - 00:22:35.670, Speaker B: So maybe a few observations. Lambda one bigger than zero means there are some exponential growth. Since this is what the public thinks of as a butterfly effect, make little change in initial conditions, things kind of change. This is precise definition of what that is. And all the upper exponents are negative just means that everything kind of contracts, which is exactly the picture here on the projector space. Everything kind of seems to contract. So this is, this is bad from my point of view.
00:22:35.670 - 00:24:30.010, Speaker B: And now let me maybe write down, write this one theorem, which is, I guess this is the theorem that's joined with these people. Uh, so suppose, um. So, um, maybe I'll write like this, uh, either. Sorry. Suppose, okay, actually let me define, actually let me make a definition before, I'm sorry, I'm going to define my, make another definition. A negative sub bundle is an invariant sub bundle of the tangent space. So invariant means invariant by both f one and f two, on which the sum of the weapon of exponents is negative.
00:24:30.010 - 00:26:01.308, Speaker B: So the sum just means kind of the volume of that some bottle contacts. And so now the theorem is that suppose there exists no negative sub bundles, then any stationary measure is invariant. So you cannot have, so that you cannot have none invariant stationary measures. Unless somehow, to me, putting in a negative sub bundle is kind of like putting secretly putting in some copy of projective space inside your, inside your system. If you don't do that, then you really don't have this kind of behavior. Also in this case, and I think I won't be able to say exactly what. Maybe I'll just write down the, I'll just write so conditionals.
00:26:01.308 - 00:26:46.070, Speaker B: I'll write the statement without really explanation because I have time to explain it. Of any invariant measure are homogeneous. So I'll just try to, like this. This is some sort of progress towards measure classification problem. It gives you a lot of control over the measure. So there's a lot of information which is buried in here, but I don't have time to unpack it because it's a very short talk. Now this is, I guess this is actually a 250 page proof, so I'm not going to be able to say much about the proof either, but I want to highlight some feature of it at some point.
00:26:46.070 - 00:28:11.690, Speaker B: So kind of the moral of this is that either you have this negative sub bundles or you're somehow in a Ratner world. You expect to have some sort of measure rigidity like in this homogeneous space situation. So if you push the system far enough, you can work on it. I mean, it's the series not complete, but at least the expectation is that the, you really do get like Ratner like phenomena in a very, very general setting, as long as you don't have this condition. I'll also try to figure out how do you ever check that there is no very negative someone who's also put some condition which implies this and why it's a reasonable condition if you, if you prove the complete, I mean, thus there are situations, I mean, if you can show everything, you know, I mean, we don't quite have a complete theorem here, because we don't quite have complete measure classification. In this generality, you have some sort of, if you do get a complete measure classification and you do more work, then you get it orbitzerically distributed. But that's what you expect.
00:28:11.690 - 00:29:44.390, Speaker B: Yeah, and there are some situation, I guess I won't talk about this, but there are some situations where we can actually expect to be able to carry out this program completely. For example, actions on character varieties. So in there you should get that. Okay, so maybe what, how do you rule out negative sub bundles? So there is another condition, which is what you call uniform expansion. So, yeah, so, so here is a, here is a, this is so, suppose. So, suppose, suppose, so what does, this is the condition so exists n and c, such that if I look at integral, I always feel silly writing this of stuff and Rolex in my manifold, all tangent vectors. If I look at, sorry, I have to give myself a bit more room.
00:29:44.390 - 00:31:02.540, Speaker B: So this is supposed to be bigger than circumental. So what is this really? Maybe it's an, I'll just write it again, because it's just, it's like the average. So it's really one over two to the n. This is average over words in my, in f one, f, two of len, ten. And then here let's call the word f. And then I can have this thing, log, I look at df and c is bigger than zero. So this is saying that on average, if I apply like a random word to a vector, it gets longer.
00:31:02.540 - 00:31:38.626, Speaker B: And so if I average over all the words I really get, on average, I get things longer. And this is supposed to be uniformly over the whole manifold and over all the vectors. So this is like a universal condition. So now why is this reasonable? First, why does it, why should they get longer and not shorter? And this will also relate to why it's, things go very wrong for. I'll make another comment. So suppose you just have a single map. So if you have a single map, then you expect the action on the tangent space to be given by some sort of hyperbolic matrix.
00:31:38.626 - 00:32:39.820, Speaker B: So it has a positive, so it has an expanding, it has like an expanding direction, and now the contracting direction, right. So this is what you expect for a typical word for any given f, right. And so of course here things don't quite work the way you want, because if you are, if you pick a vector here, then okay, it's expanded, but the vector here gets contracted, which is no good, right? So it looks like there's complete symmetry, but of course it's not symmetric because if you take like a vector at 45 degree angle, it also gets expanded. So it's not really, in order to be contracted, you really have to be, because it's expanding, component expanding. And so, so in order to be contracted, you have to be very, very close, essentially on the contracting direction, right. So it's possible to, for a single map, it's possible to have this. If you have like an abelian commuting system of maps, then of course all of them compute, they'll all have the same picture.
00:32:39.820 - 00:33:16.014, Speaker B: And again, things will not work because anything in the contracting direction will be contracted. But now imagine you have a non obedient group, actually, so you have lots of different words, and each word will have its own picture, right. It will have its own expanding and its own contacting direction, right. Somehow. So the contracting directions don't coincide. So if someone is contracted by the first word that's expanded by the second word, and you expect the average to be positive. So it's very, very difficult kind of, to imagine this not working.
00:33:16.014 - 00:33:50.466, Speaker B: You kind of have to have a conspiracy against you. So somehow, to me, this is a, this condition is actually, could be quite difficult to prove, but you can actually, it's actually a fine condition. So this is a, so I have words of fixed length. It's a finer condition, so you can check it on the computer. Actually, I had a student actually tried to do that at some point. It's surprisingly difficult, but it's possible. So somehow when I give any sort of, kind of random system like this, you can ask if it has it.
00:33:50.466 - 00:34:47.943, Speaker B: And I would say, yeah, it sure, probably has it, unless you can give me a reason why it shouldn't have. And there could be reasons why it shouldn't, but it's really expected to be true. And so, and then it's easy exercise. Uniform expansion implies no negative sub bundles. Unfortunately, this thing only implies no of dimension one. No one dimensional bundle cannot be contracted. But in order to rule out uniform, you need to generalize this definition.
00:34:47.943 - 00:35:45.510, Speaker B: So I can, instead of working on the tangent space, I have to work on something grassmannian. But there is none other condition which works like this. And then this will. So you can talk about uniform expansion and dimension d, and the uniform expansion and dimension djdehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehe implies non negative sub one will dimension. So this is kind of tells you that this, in some sense, this behavior looks like it's very common. And this does give you a Ratner like serum. If you actually assume uniform expansion in all dimensions, then you can prove that any invariant measure is absolute continuous vector volume, which basically is volume, or it's finely supported.
00:35:45.510 - 00:36:24.548, Speaker B: So you kind of have a kind of almost a complete classification, and then you can also do other things. So maybe one almost out of time. I just want to make one more comment about how the proof goes. So the proof, well, it's, it's horribly long. It's, it's fix. I'm not going to give you the outline of it, but there is just one mystery, which was, for example, Merriam and I were working on this in technodynamic sense, and somehow all of the theorems are in and are about proving that some measure is the bag measure. You have to show something is invariant.
00:36:24.548 - 00:36:59.860, Speaker B: Invariant means the condition measure along the orbit is the bag. So how do you prove a measure is the bag measure? Well, we all learn in undergraduate, or maybe, maybe depending on is that measure is the bag if it's invariant by arbitrary small translations. Right? Now, if you, if you're a ergodic theorist, you know, there another way to prove measure is the bag is by showing it has maximal entropy. This is works for some things, but not for everything. And in most arguments, you actually have to combine both methods. You have to use both. But how do you.
00:36:59.860 - 00:37:38.380, Speaker B: But in this setting, we have a manifold, arbitrary manifold. So what. What does invariant by translations actually mean? This makes absolutely no sense, right? So this is why in techno space, it does. There is, there is an affine structure on the, on the modular space, and invariant by translation makes sense. And when you're working on the techno space version, the Rapture de Shura has no generalization anywhere. But it turns out part of some much bigger story. And the story is the reason that it's part of this bigger story is a theory of what's called non stationary, non stationary normal forms.
00:37:38.380 - 00:38:18.710, Speaker B: I'm going to be out of space here, and maybe the people that involved, there are many people involved in developing the theory. A token spots here. It was actually in its final form. It's by Kalinian and Sadow's there. And what does it tell you? It tells you the following. So this is actually amazing, the general thing, so I'll probably try to finish it here, is that, suppose you have a defirmorphism. This is, maybe you can apply to the standard setting, but it's really something about a single map.
00:38:18.710 - 00:38:49.544, Speaker B: So if you have a defeomorphism, by the way, most of our theorems are really about a single d femorphism. So the grand nag is kind of built on top of this. So if you have, so it has a foliation by, let's say, unstable manifold or stable manifold. Stable manifold just means things which come together as you go forward. Unstable manifold things come together as you go into the past. I'll talk about unstables for some reason. So the unstable manifolds are kind of, you know, squiggly curves.
00:38:49.544 - 00:39:39.450, Speaker B: The poliation is nothing. You know, these are smooth manifolds, if f is smooth, but the foliation is not smooth as a whole. But it's still, they have no kind of special structure. They're just kind of smooth submanifolds of your original manifold. But it turns out there is sort of a change of variable, which is called the normal form, which converts this, converts these to, well, what I'm doing, these are homogeneous spaces for a certain nil potent group. For example, GSSR, which is a neopotent d group associated to f. And it depends what the group is, depends on the weapon of experiments.
00:39:39.450 - 00:40:22.970, Speaker B: You, and so you kind of have a picture here. So you have, on this side you have the dynamics. So I'll try to. So here I have, I have my squishy, my squiggly manifold, stable manifold. I have FD, which translates into another one. And here on the other side, I have these kind of homogeneous spaces. So I have this map, which is a normal form map, and I can do it here and here.
00:40:22.970 - 00:41:03.686, Speaker B: And these are homogeneous spaces for this new potent d group. And here you act by some sort, by solvable d group. This is something in this map is an element of what's called the sub resonant maps, which is really basically the automorphism group. So this, it's another, it's a solid body group. So you have kind of conjugated your dynamics to something which looks like, kind of like homogeneous. And here you can talk about being invariant, like this is, this is a homogeneous space for an group. You can ask about being invariant by some sort of one parameter, subgroups of this group and so on.
00:41:03.686 - 00:41:35.194, Speaker B: So all the structure is here. Now, there are lots of problems, because the issue with this change of variable is that it's smooth along the unstable manifolds, but it's only measurable transversely. So it's not like a global picture, but it's still good enough after a lot of work to carry out this kind of function. And so anyway, that's all I wanted to say. I can write down the formula for you. I think I'm kind of out of time, but I'll write down the. It depends only on some, on the light, on the, on the, on the dap and abexponse.
00:41:35.194 - 00:42:05.240, Speaker B: It gives you collection of numbers that can directly group associate to that. It's a group of sub resonant maps. I don't know anything other than that. Yeah, it's quite an amazing exist and it hasn't been used that much. I mean it has been used in some other. I mean the theory is only measurable, which is maybe why people don't, you know, people that are done, but it actually is very useful.
