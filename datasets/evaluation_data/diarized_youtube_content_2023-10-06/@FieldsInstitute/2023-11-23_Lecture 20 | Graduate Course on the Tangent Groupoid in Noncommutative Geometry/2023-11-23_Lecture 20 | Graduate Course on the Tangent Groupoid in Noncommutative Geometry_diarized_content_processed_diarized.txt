00:00:00.720 - 00:00:56.470, Speaker A: All right, so here's just something worth keeping in mind. Suppose you do the whole business of building a deformation space not for the inclusion of k into g, but the inclusion of k into g zero. I mean, it's a sublea group of g zero and. Yep. So let me recall that this group, g zero, is a semi direct product group of a certain compact group acting on a vector group like that. And so what I'm about to say applies to any k acting on any vector group like that.
00:00:56.622 - 00:00:58.494, Speaker B: So remember that in case k is.
00:00:58.574 - 00:01:50.898, Speaker A: Like un or something, k is un, or maybe some subgroup of un, depending on what the, what the group is. Yeah. For example, if it's sl two r, it would be so two from your point of view, thought of as a subgroup of u two. All right, very good. So what? Well, this is a very simple situation in which all of the directions normal, so to speak, to the sub manifold. Remember, we're building the deformation to the normal cone of k inside of g. And what you're supposed to g zero, and what you're supposed to do to build this deformation to the normal cone is somehow increasingly stretch the big manifold in the normal directions without doing any stretching at all in the submanifold directions, in this case, the k directions.
00:01:50.898 - 00:02:44.614, Speaker A: Well, if you stretch a vector space, you just get a vector space. It's probably not very surprising that this whole thing just looks like g zero times r. So the family is actually constant family, no new groups arise. You don't have to talk about g zero zero, because the normal bundle of a submanifold as it sits inside, well, this is some normal bundle, some vector bundle. The normal bundle is the vector bundle you started with, so no further simplications arise. But the isomorphism is not completely trivial. It's not the identity map.
00:02:44.614 - 00:04:05.314, Speaker A: If you take a vector. So we know that away from zero, the various groups in this family are copies of whatever the big group is, which in this case is g zero. We have a whole bunch of copies of g zero already in here and a whole bunch of copies of g zero already in here. But the way these guys get identified under this styomorphism is not by the identity map. Instead, some rescaling is involved like that. So this map that I've just written down, which is defined when t is not zero on a big open subset of g to a big open subset of the right hand side, it extends to a diffumorphism. That's what I'm saying.
00:04:05.314 - 00:05:32.964, Speaker A: Okay. And it's not completely trivial because there's some rescaling involved this rescaling, which is to say, automorphism g zero. There's one for each t which isn't zero. In this particular case, it's a group automorphism. And so it gives or leads to a rescaling with the same number of quotation marks of the joule, because if you have a representation of the jewel, you can just compose it with this automobile of g, and then you get another representation of g zero. All the g's that I said should have been g zeros. Okay, but at least in some cases, we discussed last time why it is that g zero is the same thing going back to g now as g g zero hat, excuse me, is the same thing as g hat.
00:05:32.964 - 00:06:46.534, Speaker A: It's what I was previously calling and what I should continue to call the reduced dual, the spectrum of the reduced group c star algebra, those irreducible representations, unitary representations of g, which are somehow related to the regular representation on L 2g weakly contained in, as the seastar people say, this isomorphism is a little tricky and complicated. Remember, we broke up the reduced dual into, I don't know, strata pieces, which were locally closed subsets, and each of those separately was made in bijection with a piece over here. It happens that those individual pieces get rescaled into themselves by these rescalings. That is just a feature of this particular bijection. But let's not worry about that. We can certainly say rather that this thing in some similar sense gets rescaled. I just mean there's some family of automorphisms of this set.
00:06:46.534 - 00:09:18.074, Speaker A: And now something rather interesting happens, which is in joint work with me, but it's in Angel Roman's thesis. The interesting thing that happens is that the automorphisms of the reduced sea star algebra, rescaling, if you like, of this sea star algebra, let's just call them, I don't know, alpha t implement these rescalings that we're talking about. That's interesting. And using these rescalings, you can sort of trivial, you can go backwards now. You can sort of trivialize, well, not the bundle of groups gt or the family of groups gt, because those groups are not all the same, but the family of sea star algebras. And only sort of trivialize in the following way. You can build an inclusion of the sea star algebra of g zero.
00:09:18.074 - 00:10:41.356, Speaker A: It's not the case that g zero is isomorphic to g. It's also not the case that the sea star algebra of g zero is isomorphic to the sea star algebra of g, because these dual spaces that we've been discussing are, although they're in bijection to one another, they're not homeomorphic. Nevertheless, there's a way of embedding the c star algebra of g zero into the c star algebra of g, and it's characterized in a very nice way. If you want to apply alpha to a function f zero, which is a function on g zero, what you should do is look at the functions ft. So what's going on here is that f is a function on the entire deformation space. And there's a little lie I'm telling, having to do with how measures, but I'm just going to ignore it. You could take a function on this space, perhaps it extends the function f zero.
00:10:41.356 - 00:11:17.254, Speaker A: Of course, then it'll have a whole bunch of other slices, so to speak. Ft, which is a function on gt, which is a subgroup of g, a subspace of g. And we can do something, which I'll describe in just a moment. Yeah, yeah. So now, thank you very much. The top part was just for inspiration. This is the deformation space, ok? As it sits inside of g.
00:11:17.254 - 00:11:44.884, Speaker A: So, thank you. I switched. This board was a preamble. This board is a transition. And now, well, and then a theorem, which I'm sort of continuing over here. And what you're supposed to do is take ft and rescale by alpha t like that. And then just take the limit as t goes to zero.
00:11:44.884 - 00:12:16.614, Speaker A: And this will work either for alpha t or alpha one over t. And so let's define alpha t appropriately so it works. I was sufficiently vague over there, but I cannot be wrong. Okay, that's rather nice. And what that says, this formula says that the continuous field we have a proper word for. That was my favorite eraser. Here it is.
00:12:16.614 - 00:12:42.296, Speaker A: There is this thing too. Was this here all along? Ah, so this is what you're supposed to do with that, I guess. It's got like a ball joint. Well, that's no good. Hello. Okay, but this one is even bigger. I think after a certain point, there are diminishing returns.
00:12:42.296 - 00:12:57.294, Speaker A: If the eraser is too big, it loses that nimbleness. You know, it's like driving a Bentley instead of a mini. You can get yourself into trouble driving a Bentley. Might have made it. Yeah. Might have gone all the way over. Right over niagara Falls.
00:12:57.294 - 00:13:56.188, Speaker A: Anyway, so the. The continuous field, that's what I was trying to do, was erase the word family associated to this big g, which is the deformation space for k in g, not k in g zero. That's a very simple structure. It's what you might call a mapping cone field. Here's a. In fact, the mapping cold field associated to this embedding. What do I mean by this last thing, a mapping cold field.
00:13:56.188 - 00:14:36.354, Speaker A: Well, suppose I have a sister algebra a zero and an inclusion of a zero into another seastor algebra a. Then I can build a continuous field in the following way. It's going to be a continuous field over the real line. And all of the fibers at are just going to be a except when t is equal to zero. And then of course it'll be a zero. And what are going to be the continuous sections? Well, they're going to be arbitrary continuous sections from the non zero reals. Arbitrary continuous functions from the non zero reals into the, into the big algebra a.
00:14:36.354 - 00:15:21.318, Speaker A: With the property that as t goes to zero together with evaluates in a zero. It has to be a section which is defined everywhere. And then the matching condition for the value of the section at a zero at zero with the value everywhere else, is that the family inside of a should limit to the given point of a zero. Remember, a zero is embedded inside of a. If you have a sea star algebra, I could have said it in a better way. I'll try and say it again. If you have a sea star algebra and you have a subalgebra, a zero, of that sea star algebra a, you can define the continuous sections to be all of those continuous functions from the real line into a which at zero, lie inside of a zero.
00:15:21.318 - 00:15:56.554, Speaker A: That would have done the job better. And we'll call that thing the mapping cone. And that's all this guy is. So this particular continuous field of sea star algebras becomes trivial from the point of view of this inclusion. Once you have this conclusion, the whole continuous field, the whole deformation to the normal cone, the notion of a pseudo differential operator, I don't know what it all somehow ought to become trivial. The field certainly becomes not exactly trivial in the sense of very easy, not a constant field. Field becomes easy to describe.
00:15:56.554 - 00:16:16.362, Speaker A: Hopefully everything else becomes easy to describe as well. Okay, this was just a little. We were wasting time waiting for echo to arrive. But here, here he is. Great. Welcome. Okay, I think maybe it's readable.
00:16:16.362 - 00:17:26.734, Speaker A: Maybe it's actually readable. I tried to make an effort to write enough down that you could follow. The takeaway. Is that this business of the Mackie bijection that we were describing, discussing last time, if you package it into the invention of this family of rescaling automorphisms, alpha t, these guys here, which have the effect of making a rescaling of the dual, which corresponds to the natural rescaling of g zero. The takeaway is that this final family of objects, these automorphisms of C star of g, somehow tell you everything about the deformation to the normal cone. Everything is packaged into that one parameter family. It's another way of understanding what is the one parameter family, what is the continuous field, which to a certain extent is the same thing as understanding what is the deformation space as a family of groups.
00:17:26.734 - 00:18:04.382, Speaker A: It's a little bit mysterious to think of it that way. It's less geometric, isn't it? One parameter family of automorphisms of a sister algebra. But it's interesting to think that maybe that you could make something of that. Start with this one parameter family. Search for it before you search for the Mackie bijection, for instance. Well, it exists in the norm topology. Alpha T of Ft is an element of C star of Gt, but all of the C star of Gt's are the same.
00:18:04.382 - 00:18:48.762, Speaker A: That's the little lie I was telling you, because they're not quite the same, unless you do this sensible thing for groups, which is not, in my opinion, the sensible thing for manipulates, which is to use densities, and then they would really all be the same. There's a little issue of densities versus measures, which causes you to pause a moment before you identify one C star of GT with another C star of GT. Modulo that. All I'm saying here in this limit is take the norm limit. I'm asserting the norm limit exists and it defines alpha. That's how you actually build alpha. And you get this mysterious inclusion of two group sister algebras for which the groups don't include at all, one into another.
00:18:48.762 - 00:18:56.866, Speaker A: They're quite different. The only bit of g zero that fits inside of G is k. Is.
00:18:56.890 - 00:19:06.046, Speaker B: There a way that I can understand it of, like definition? So you have this field, and like, is there a set? Like, is there like a notion of automorphisms of the field?
00:19:06.110 - 00:20:02.240, Speaker A: And somehow, yeah, you can make extended, yeah, you can make various sort of definitions. But I find it very hard to understand what's going on in Angel's theorem and this construction from a purely geometric point of view. And part of the problem is that we're talking about C star of G, not the function space of GT. So the relevant spaces that we're gluing together into some sort of family of spaces are the duals of these groups, not the groups themselves. You would like to, I don't think this is right, but you could sort of imagine taking, I don't know, the dual of G. That's a closed subset of the dual of G zero, or, and therefore a closed subset of the dual of G. Somehow you could imagine doing some deformation to the normal cone construction with those spaces which are not quite real spaces, they're some sort of non commutative spaces.
00:20:02.240 - 00:20:34.924, Speaker A: That's kind of what's going on. But I don't know how to make proper sense of anything I just said. In other words, I offer this to you as a little bit of a mystery. This is some non commutative construction that is not easy to understand in purely geometric terms. At least it's not easy for me to understand it purely geometric terms. All right, good. So, gosh, we spent way more time on that than I wanted.
00:20:34.924 - 00:21:13.354, Speaker A: Yeah, I suppose I'm telling you all of this, I mean, apart from killing time so that I could get here, I'm telling you all of this because now we're going to do something else with the family of groups. Gt if you like, we're going to do something else with this deformation space, big g, and it's all going to be a little strange what we're doing. And maybe these automobiles will make it less strange, or maybe no more strange than the automorphisms themselves. Maybe the real mystery is this family of automorphisms. I guess that's what I'm saying. Yeah, go ahead.
00:21:15.284 - 00:21:18.180, Speaker B: Do we already know the MACD correspondence or something?
00:21:18.332 - 00:21:22.804, Speaker A: I'm assuming that the theorem which I sketched last time is a true theorem.
00:21:22.844 - 00:21:25.572, Speaker B: Yeah, which was the theorem.
00:21:25.708 - 00:21:28.476, Speaker A: The theorem says that if you took.
00:21:28.540 - 00:21:31.424, Speaker B: The continuous field of things, that Merida.
00:21:31.764 - 00:22:08.804, Speaker A: Yeah, so the field of sea star algebras that we're talking about up here to my left, this field is assembled in a complicated way at the moment, only understandable using representation theory. It's assembled in a complicated way from constant fields. That's what I'm talking about. That leads to a simpler sort of statement, which is what I started with up here, right here, this set. Theoretic bijections. Yeah.
00:22:08.844 - 00:22:09.508, Speaker B: In principle.
00:22:09.596 - 00:22:39.524, Speaker A: In principle, yeah. Everything that we discussed, everything of real substance that we discussed last time and everything we're going to of real substance that we're going to discuss today, that's not quite true. Many of the things of real substance that we'll discuss today require in the background some effort in the world of representation theory, which I have not expended. But I mean in front I have in real life, I've paid my dues, but I did not expend that effort in front of you.
00:22:43.704 - 00:22:44.600, Speaker B: Bijection.
00:22:44.712 - 00:23:21.984, Speaker A: It's a bijection. It's this, this thing is what is called the Mackie bi junction. It's, well, at least it's a bijection. It's sort of arguable what it has to do with George Mackey, but it's certainly a bijection. Mackey is certainly the inspiration for the construction of this bijection. I guess that's a definitely true statement. But this is not the bijection that Mackie had in mind when he wrote an inspiring paper a long time ago.
00:23:21.984 - 00:23:26.232, Speaker A: That help?
00:23:26.408 - 00:23:27.164, Speaker B: Yeah.
00:23:29.304 - 00:24:09.168, Speaker A: Okay, good. I want to tell you something which to a certain extent applies to all groups, but it's particularly decisive in the world of real rank r1 reductive groups to say real life quant groups. And for these groups it's possible to be a bit more explicit about what the representation theory is. You're waiting for some terrible thing to happen. I don't know. It looks like you're fascinated, like someone is fascinated in a car crash or something. No.
00:24:09.168 - 00:24:53.164, Speaker A: Yeah, yeah. All right, let's see what happens. I'm going to tell you what a real rank one group is. So it's one of our real reductive groups. So we'll start off in this way. It's closed under the transpose operation, which means that the entire lialgebra of G breaks up into a kick, possibly like that. And now some more letters.
00:24:53.164 - 00:25:30.814, Speaker A: Representation theory is famous for using all parts of the polar bear, just like up in the north. They, you know, they use the claws for something and for something else, we use all they, the representation theorists use all letters. Every letter is used for something. In fact, many letters are used for many things, which many letters are used again and again and again, which is a nuisance anyway. This is a maximal abelian subspace. P is not a lie algebra. P is a bunch of matrices.
00:25:30.814 - 00:26:45.254, Speaker A: But you can ask for a vector subspace of p in which all of the matrices, remember they're all self adjoint matrices, a bunch of matrices, all mutually commute. And when you found one, you call it a. And this thing, it's not such a bad object really. This thing is known to be unique up to k conjugacy, more or less in the same way that the maximal torus of a compact lead group is known to be unique. Okay, so you fix one, and the real rank of g is just the dimension of a. So that's what we're talking about. This, this vector space is one dimensional and if you take sl two r or sl two c, or some fancier groups, like so n one, these groups that the physicists study, or su n one, I don't know if anyone studies those for any particular physical reason, or SPN one, the symplectic version.
00:26:45.254 - 00:28:01.658, Speaker A: These are all examples of real rank one group. So there's a bunch of them and there's one, one exceptional group, also real rank one. And I want to particularly focus on those because there's a very simple statement which tells you what is the representation theory of these next board. The group can be broken up into pieces. This works in any real rank, and it involves what's called the Iwasawa decomposition. And it says that the entire group G as a manifold is the direct product of k times a. What is big a? Well, it's the Lie algebra associated to little a.
00:28:01.658 - 00:29:10.220, Speaker A: It's the exponential of little a. So that's a certain closed subgroup, and then times another subgroup n. So this works in general, but in real rank one, it's possible, pretty easy to say what n is. It's just the elements of the group. Very nice sort of dynamical way of thinking about this. It's just the elements of the group which gets shrunk when you conjugate by things in a, what am I calling it? G. So if you have an element of a, any element which is not zero, then this recipe defines a subgroup.
00:29:10.220 - 00:29:47.364, Speaker A: It's easy to see it's a certain closed subgroup of G. And that closed subgroup always has the property that you can fit it into this Kn picture. That's some closed connected subgroup, nil potent subgroup. And G is always Kam. We're almost there, almost completely dealt with the polar bear. Just a few grotty bits from the inside to discuss. But yeah, the word is abelian.
00:29:47.364 - 00:30:16.960, Speaker A: So it's a collection of matrices inside of p, and they all commute with each other in the usual sense of matrices. And yeah, it's maximal. With respect to being collection of commuting matrices. That automatically makes it a vector subspace. And that's what we're talking about. Okay, so you can do a little experiment for sl to r. How many different matrices are there in p? Well, there are two, it's a two dimensional space.
00:30:16.960 - 00:30:42.224, Speaker A: There's two genuinely different matrices. Basis will have two elements in it. Maybe that's a better way of saying it. And they don't commute with each other, no matter which basis you choose. So the maximal dimension is going to be one of a commutative subspace. Yes, this thing defined in this way is a group. That's an exercise based on this definition.
00:30:42.224 - 00:31:09.368, Speaker A: A is a group because it's the exponential of this commuting family of matrices. A stands for abelian and. Yeah, if you take the product of two commuting exponentials, that is to say x one times x two, where x one and x two commute, then that's x x one plus x two. The usual law applies when the matrices commute. So a is a group. It's a one dimensional subgroup. Yeah.
00:31:09.368 - 00:31:10.640, Speaker A: Not very interesting.
00:31:10.792 - 00:31:11.928, Speaker B: Maybe it helps to say what it.
00:31:11.936 - 00:31:36.496, Speaker A: Is for as a two r. Yeah, good point. So, first of all, in general, let me just put it on the board. This means the exponential of a. So, as Eckhart says, let's just do an example. So k is so two. According to our definition, k you can.
00:31:36.496 - 00:31:57.744, Speaker A: A is not unique. You have to choose a. The choice doesn't really matter much, but you have to choose one. And a good one to choose is just all of the diagonal matrices like this. These are the exponentials of all of the diagonal traceless matrices.
00:32:00.924 - 00:32:03.464, Speaker B: In general, is it always just like a copy of the circle?
00:32:05.244 - 00:32:15.384, Speaker A: This one k, it's not a copy of the circle, it's a copy of the real line. Right, that's what you meant. Yeah, I was thinking the one dimensional.
00:32:15.424 - 00:32:16.444, Speaker B: Sub animal.
00:32:19.424 - 00:32:46.664, Speaker A: Which we're not. But if we were, then yeah, it's always a copy of a product of copies of r as a lie group. So rn is a lie group, and n. Well, there are two choices for n because there are two different ways. You could choose a non zero element inside of a. You could choose a basis element or minus the basis element. And clearly that's going to affect what happens here.
00:32:46.664 - 00:33:21.628, Speaker A: But you can choose one of the basis elements, you can choose the x correctly so that the n subgroup looks like this. And this is basically how it always looks. And if you chose, made the other choice, the n subgroup would be the lower triangular matrices, not the upper triangular matrices. There's always a finite number of choices, some combinatorial finite number of choices for what n is going to look like. Yeah, for rank one, just two choices.
00:33:21.756 - 00:33:29.784, Speaker B: So, yeah, in real rank one, you can actually say what n is, but in general, the theorem states that there exists an n we don't know much about.
00:33:30.724 - 00:34:38.411, Speaker A: So what you have to do in general is the following thing. You need to choose x generically. If you choose x non generically, you'll get an interesting subgroup, which is also called n in representation theory, but it's not the n which appears in this famous Iwasawa decomposition, it's related to some other so called parabolic subgroup. So this is correct for a generic choice of x, roughly speaking, because we're dealing with some self adjoint matrices which commute with one another, we have some commuting family of matrices. They can all be simultaneously diagonalized. And roughly speaking, generic means all of the eigenvalues are different, roughly speaking. So if you choose a generic x which, for which there are no identities among eigenvalues other than those imposed by the ambient group g, then so there are as many independent eigenvalues as couldve could possibly be, then that x will do the job here, sl three, there should be, you're not allowed to choose the three eigenvalues independently.
00:34:38.411 - 00:35:22.154, Speaker A: They have to add up to zero, but you can certainly make them all different. And if they are all different, then you're in business here. Okay, that's what we're talking about. Yeah, there's a little bit of polar bear still to eat. This is going way slower. Centralizer of. Okay, so this is the subgroup of k consisting of all elements in k, which commute with all elements in a.
00:35:22.154 - 00:36:48.584, Speaker A: So everything in m commutes with everything in a. Now you can build this thing ma n while we're at it, continuing the sl two r example for the same choices that you see up there. M is just the, in this case, it's a tiny group. It's a finite group with just two elements, and this p subgroup is the group of all upper triangular matrices inside of S l, two r like that. And we're done with all of these crazy letters. No more crazy letters. This group ma, it's the group of diagonal matrices in this example, of course, it's a subgroup of this group p, but it's also a quotient of this group p, because the n part is a normal subgroup of p can easily satisfy yourself using the definition of n, which I wrote above, that if you have something in Ma, it normalizes n.
00:36:48.584 - 00:37:29.504, Speaker A: The condition defining n is invariant under conjugation by anything in m and also anything in a. Not surprising because of commutativity. Okay, so this guy here, Ma is a quotient. It's a group because m and a commute. So we've seen this. We haven't seen this yet. Each representation of MA, unitary representation of Ma, is therefore a representation of p, just by composing with the quotient map.
00:37:29.504 - 00:38:44.250, Speaker A: Okay, and we can induce, this is a heavy dose of representation theory. That's the bad news? The good news is it's just about done. And then there's a really interesting theorem. What are we going to induce? Well, we're going to take an irreducible representation of M and an irreducible representation of A. A is an abelian group, so all of its irreducible representations are one dimensional, and they all come from exponentiating an element in the dual vector space of a fracture, a. So sigma here is an irreducible representation of m, and nu is an element of a star, the dual vector space. And Lander is new.
00:38:44.250 - 00:39:18.654, Speaker A: Yeah, or I knew. I guess David Vogen would put an either. So let's follow him. And induction is a certain species of induction. Induction is the flip side of restrictions somehow adjoint to restriction. That's how you're supposed to think of induction. And so it's a way of constructing representations, in this case, unitary representations, because there's a version of induction called unitary induction, and that's what I'm really talking about.
00:39:18.654 - 00:40:06.270, Speaker A: And that's it. And so we have here unitary representations of g, and what happens is they're all irreducible. Pretty much. Certainly they're irreducible if nu is not zero. Ah, that's a statement which is true. In real rank one, there are always unitary representations who happen to be in real rank. They're always almost all irreducible.
00:40:06.270 - 00:41:25.052, Speaker A: And in the world of real rank one, almost all just means that this new should be non zero. And when they're irreducible, or when you're studying an irreducible summand, they're actually in the reduced. You're probably exhausted from, you know, this tiring journey through remote parts of the roman Alphabet. But the good news is we've now emerged from the woods. And so here's a beautiful theorem of Harish Chandra and Langlands. Why might you possibly be interested in such an involved and crazy construction of representations as the one I just sketched to you? You might think there's a million things a person could do to build representations, irreducible representations of a group g. Well, that turns out not to be the case you now see before you.
00:41:25.052 - 00:43:04.164, Speaker A: If you combine what you see with what we said yesterday, you now see before you all of the irreducible representations in the tempo dual of a group G. So there's a general version of this result in any real rank, but I'll state it in real rank one, because I don't have to stress myself too much, or yourselves too much. So, in the world of real rank one, for these groups that were in that list, which is somewhere this one down here, if you have an irreducible unitary representation of g, which is in the reduced dual. Well, there are two possibilities. One is that it's one of these discrete series representations that we discussed last time. And the other is that it's an irreducible constituent. It's an irreducible sub representation of one of these that we just constructed.
00:43:04.164 - 00:43:40.304, Speaker A: Maybe we could just continue, maybe just here somewhere. Yeah. Most of these representations are irreducible, especially in real rank one. The only issue is when nu is equal to zero. When nu is zero, there are only two possibilities. One is that the representation is still irreducible. That can happen.
00:43:40.304 - 00:44:45.838, Speaker A: The other possibility is that it breaks up into two irreducible bits, and a constituent means either one of those two irreducible bits, or the entire, in the other cases, entire irreducible, PI, sigma, I, nu. Oh, this is not part of the langless classification. This is a relatively easy result. That's although it uses the same techniques that you see in the Langland's classification, if you happen to know what that is, namely, asymptotics of matrix coefficients. This is built using that same technology of matrix coefficients, analyzing the asymptotics of matrix coefficients. And speaking of asymptotics, the way in which this n is defined according to some asymptotic formula is very relevant to the proof. It's really the definition of n, which makes this theorem over here true.
00:44:45.838 - 00:45:12.482, Speaker A: Now, it doesn't seem so bad. It seems like you're led to this definition by the wish to create a theorem like this. That's not what happened historically, but it could have happened that way. If Langlands had been in charge from the beginning, it probably would have happened that way. But he wasn't. Okay, Dan, just. Just complain.
00:45:12.482 - 00:45:15.854, Speaker A: I know you're just trying to understand.
00:45:17.354 - 00:45:26.654, Speaker B: I mean, I was just going to make a comment about how you kind of see, like, you have a nu not equal to zero, then you also have this x not equal to zero above. So you can kind of see.
00:45:29.114 - 00:45:46.250, Speaker A: Maybe there really is. There definitely is. I mean, those are not totally unrelated issues. Yeah. Those are not totally unrelated clauses or whatever. On the other hand, it's not. It's a bit of a journey to prove this.
00:45:46.250 - 00:46:17.564, Speaker A: It's not a. It's not a steep and difficult journey, but it's still a bit of a journey to prove this. But it's kind of beautiful. Now, you know what the representations of these groups are, at least these real rank one groups, and it's not hard to tell you the statement for a general group, but I won't. There's a variation on this. We're missing some parts of the story, and it has to do with the fact that this definition event is. You have to be a little bit.
00:46:17.564 - 00:46:25.824, Speaker A: It can lead to many different ends, as I was just saying. And somehow all of those ends are part of the story, whereas here I'm only talking about one end.
00:46:27.804 - 00:46:47.904, Speaker B: In this case, basically, what we've done is we say, and we've reduced it to understanding screen series and representations of.
00:46:50.924 - 00:47:19.184, Speaker A: It's mostly. Yeah, the complicated thing is representations of M. That's some compact group, and it's not, as you can see in this example, it's not necessarily connected. So it can actually be a chore to work out what M is and what its representation theory is. As for the news, they just belong to vector spaces. That's not a terribly difficult part of the story. So it's really the sigmas there and also the sigmas, which are discrete series representations of G.
00:47:22.644 - 00:47:26.984, Speaker B: We have a non compactor and we're essentially representation of.
00:47:28.124 - 00:47:50.524, Speaker A: Correct, correct. This is definitely progress, because M is, first of all a smaller group, K is already a smaller group, and M is a subgroup of K. So it's not a very big group, although it's not always as small as it turned out to be for sl two. And so if you like, apply this.
00:47:50.564 - 00:47:52.184, Speaker B: 2G equals sl two.
00:47:53.764 - 00:48:13.416, Speaker A: Yeah, you know everything. M is a two element group. How many irreducible representations are there of a two element group? Well, ask your algebra professor. There are two and they're both one dimensional. Then you can send the obvious generator minus the identity to minus one or one. That's it. So there are two types of so called principle series.
00:48:13.416 - 00:48:30.188, Speaker A: These guys here, one where Sigma is the trivial representation and one where Sigma isn't. Those are those two lines that I drew on Tuesday of representations. And then there are the discrete series. Let's not forget about them. Those are a bit more mysterious.
00:48:30.356 - 00:48:31.944, Speaker B: Do you understand discrete series?
00:48:32.684 - 00:48:54.134, Speaker A: Yeah, we do. That's a long story. But yes, we do. We understand them in a way which is strongly reminiscent of the vile character formula. That's the good news. The bad news is that when Harris Chandra proved this, it turned out to be so difficult that he had a mental breakdown. It almost killed him.
00:48:54.134 - 00:49:10.254, Speaker A: So it's. It's doable, but it's not easy. Even for Harris Chandra, it's not easy. It almost killed Harish Chandra. Yep. Driving is bentley completely left the road. Damn.
00:49:10.254 - 00:49:52.342, Speaker A: All right, good. Let's keep these fancy. I'm going to have to make a decision about what to tell you, because we're ambling along it much less than 100 miles an hour. That's right. That's just in the final ten minutes of the lecture, when I'll get panicky. That. Yeah.
00:49:52.342 - 00:51:00.502, Speaker A: Right. Now back to Mackie for a moment. There is this motion group. This is the normal, geometrically, the normal bundle of k inside of the group g. And it's a semi direct product, like I've just drawn. And so k is a quotient of g zero, because the p part in the semi direct product is the normal subgroup, and k is acting on it, conjugating p into itself. So each time you have an irreducible representation of k, you get an irreducible representation of g, just by composing by this quotient map, which sends g zero to k.
00:51:00.502 - 00:52:00.284, Speaker A: So we could say in that way, g zero includes a copy of k hat. Sorry, g zero hat includes a copy of k hat. But Apgustides, who's the guy who did the major part of the work here, proved that macubijection. So there is this bijection, which is more or less natural. It's a reasonable bijection between g zero hat and g hat reduced the unitary dual of this easy group, supposedly easy group g zero. In fact, we wrote down what is the unitary dual of this, and the tempered or reduced unitary dual of G, which is a little more complicated. But it's coming a bit into focus here, thanks to David Morgan.
00:52:00.284 - 00:53:29.854, Speaker A: So k hat sits inside somehow, the reduced stool may be embedded, is embedded by Mackie and Afko studios and friends into jihad reduced stool. And it turns out that, thanks to the way in which the machibajection is defined, these are what Alexander calls the temporic representations, the ones corresponding to points of k hat of G. This is a contraction of tempered, irreducible, real, infinitesimal character representations. And now you know why Alexander calls them temporary. A bit of a mouthful. I won't tell you what all of these words mean, because it's not terribly important, but one can translate all of these words into the picture that Langlands is trying to paint here. And I'll just tell you what the story is in the Langlands language.
00:53:29.854 - 00:54:25.774, Speaker A: So every discrete series representation satisfies all of these words, I should say, by the way, that they don't always exist. We drew a picture of the tempered jewel of sl two c last time, and there weren't any isolated points. So these guys don't have to exist. And then the constituents of all of the bases, bases of the. So the representations you build this way. Oh, I was going to give an example. Let me just come back to that in a moment.
00:54:25.774 - 00:55:06.958, Speaker A: The basis of the so called principle series. So these are the other representations that Langland speaks of. And by a base representation, I mean a representation where the continuous parameter is zero. When the continuous parameter is zero, like I said, the representation need not be irreducible. And so, in that situation, all of the individual constituents are considered to be temporaric. Are temporaric according to the official definition of David Wogan. So this is what these distinguished representations turn out to be.
00:55:06.958 - 00:56:42.082, Speaker A: And Mackey puts them in bijection with the representations of k. Let me just say, seems a little lonely just leaving this example here without saying something. If g is sl two r, then the space that you get, the homogeneous space you get by dividing out by p, this subgroup that you see somewhere here, this group of upper triangular matrices, that's a three dimensional group divided by a two dimensional subgroup, because, remember the determinant condition. So it's a one dimensional manifold. So what can it be? It's just rp one with its usual action of sl two. And the principal series can be just realized as sections of g equivariant bundles over. Just to make them seem a little bit more concrete, you take a suitable equivariant g equivariant vector bundle over rp one.
00:56:42.082 - 00:57:06.014, Speaker A: Well, what does suitable mean? A g equivariant bundle over a homogeneous space is the same thing as a p equivariant bundle over a point, because. Yeah, because. And the representations that we're talking about here, of course, are exactly these ones. So they should be the unitary on this fiber.
00:57:06.394 - 00:57:11.338, Speaker B: So, excuse me, sir. So we are defining a group structure.
00:57:11.466 - 00:58:00.180, Speaker A: I mean, on the manifold we are defined. No, I'm saying that rp one is not a group, but it's a homogeneous space, it's not a group, p is not a normal subgroup. I'm trying to make this construction of induced representations just seem a little more geometric. I didn't tell you what the induction process was, but now I am going to tell you. It means you take a representation of p, the subgroup, you build out of that subgroup, a vector bundle, a g equivariant vector bundle over g mod p, and then you take the sections, and then there's a little detail about half densities, just to please the french. And that's what the induced representations are. So I just tell you that just to make it seem a little more concrete.
00:58:00.180 - 00:59:12.890, Speaker A: Thank you. Okay. Okay. That was just me noticing that I forgot to tell you about the example. All right, very good. Okay, so we're learning a little representation theory, and now let me tell you a beautiful theorem of David Wogan, which you can see is somehow baked in to the Mackie bijection. First of all, if you have a representation of G, then you can break it up, break up the Hilbert space H PI, not necessarily as a representation of G.
00:59:12.890 - 01:00:44.408, Speaker A: It might be an irreducible representation, but you can always break it up in a canonical way. When you restrict to, what's a good letter, maybe tau, restrict to k. This is the isotypical decomposition of h PI as a representation of k. And what you do is, what is h, sub PI, upper tau? It's just the union of all of the images of possible ways that you can embed the representation tau into h PI. That union turns out to be a little Hilbert space, and that's what we're talking about here. And that course is an orthogonal direct sum, an L two Hilbert space direct sum, if you want to get a Hilbert space, okay? And the tau for which this isotypical space, it could be for a given tau, that the space is zero, but the tau for which h PI, tau is not zero. These guys are the K types, and that's typically an infinite set of Taos most of the time.
01:00:44.408 - 01:01:44.340, Speaker A: For example, for all of these principal series representations we were just describing, it's a big infinite set of taus. And. Yeah, and now a line which I alluded to last time, and I'm just going to tell you the official definition, just so we have it on the board. We talked about highest weights a little bit last time. Those were the weights of the representation tau, which were extreme in those diagrams that we were drawing. And you choose one of the. In the drawing diagrams, we were drawing six points according to some combinatorial recipe.
01:01:44.340 - 01:02:41.570, Speaker A: The same combinatorial recipe chooses another point for you. In the example from last time, a plain, which is standard fare in representation theory, something called rho. And David says that the way of ordering the representations is not what I explained last time. According to inclusion of weight diagrams, it's this small variation on that, where you take the weight and you slightly adjust it this way, and you just take the norm. This is a point in some plane with an inner product, not necessarily a plane, some euclidean space with an inner product, and you take it, take the norm. It's a funny definition for lots of reasons. And what you should think about when you see this, is that we're just talking about the same ordering we discussed last time, the intrinsic ordering on representations by inclusion of weight diagrams.
01:02:41.570 - 01:03:27.870, Speaker A: That's not what this is, but it's pretty darn close. And so that's what we'll, that's what you can think of, some natural ordering. Some representations are bigger than others. Some groups like so two, the representations are labeled by integers. Well, I'm just asking, when is one integer bigger than another? When is the absolute value of one integer bigger than another? That's all I'm talking about. All right, good. So here's now David's theorem back to these temporary representations, which in the case of real rank one, are written down here.
01:03:27.870 - 01:04:11.238, Speaker A: But this theorem is true in any rank. It's just easier to think about. In this case, each one has a unique minimal k type. It's minimal because you're just taking this norm here. And it's entirely possible that two different taus could have the same norm according to this formula, in which case how to judge between them while you can't. But in these temporary representations, there's a unique k type, which minimizes this quantity. Quantity, it's kind.
01:04:11.238 - 01:05:05.134, Speaker A: If you do this for sl two, what you find is that this uniqueness does indeed hold for these, as it has to, because it's the theorem of David's. But if you nudge away, if you move the continuous parameter away from zero just a little bit, then it fails. It's kind of interesting. It all fits together, but only just and in the other direction. Each k type is the minimal k type. Yes. We could just say at minimal k type, and in fact, it has multiplicity one in a unique, in a unique temperate representation.
01:05:05.134 - 01:06:12.708, Speaker A: Yeah, yeah. There's a bijection between k types and temperate representations, and the bijection has the property that the temporary representation, including associated to a representation tau of k, includes tau as a minimal k type, and it's the only minimal k type, and it has multiplicity one. Yeah. Is minimal k type in fact a minimal k type of multiplayer one in some, and indeed a unique temporary representation. Ah, it's the. Yeah. The number of it could be that h PI tau is just isomorphic to a single copy of the representation tau.
01:06:12.708 - 01:06:52.164, Speaker A: That's multiplicity one. It could be that h PI tau is isomorphic to a direct sum of two copies of tau, and then it's multiplicity two and so on. I mean, yeah, all of these multiplicities are finite. By this theorem of Harris Chandra that we mentioned last time. Yep, it's saying that. So there is a bijection, and in fact the bijection is the Mackie bijection that's not written anywhere here. But it follows from the way in which the minimalist, the macubijection is assembled using minimal k types.
01:06:52.164 - 01:08:22.714, Speaker A: All right, and so the question I want to ask, question really that David asks is, was the following, is K theory of Siesta algebras useful here in understanding or illustrating this theorem in one way or another? Ah, Vogen. There's a certain set of representations that David has identified as being important, namely these temporary representations, namely these ones here. It's a certain countable set of representations. On the other hand, there is a certain countable set of irreducible representations of k. And now you can imagine building a gigantic matrix. And in the entries of this matrix, they're indexed by a temperature representation and an irreducible representation of K. And what you put in the IJ entry is the multiplicity with which tau I appears in temperate representation J.
01:08:22.714 - 01:09:09.906, Speaker A: So it's a matrix whose entries are non negative integers. And if you order the representations of both g, the temporary representations of g, and the representations of k, I use ordering a little bit loosely here. If you imagine ordering them in your mind's eye, sorry, Jacob, then you'll have a big matrix. And just by definition of minimal k type, the way in which we're ordering the temporary representations by minimal k type, this matrix will be upper triangular. The multiplicity. One statement says that this is an upper triangular matrix with ones down the diagonal. That's what David is saying.
01:09:09.906 - 01:10:30.946, Speaker A: Okay? But if you have an upper triangular matrix with ones down the diagonal, then it's an invertible matrix. All right, so some invertible, some, some matrix, some upper triangular matrix is invertible. And, well, that's sort of thing you could conceivably prove using k theory, because you would get a triangular matrix, or at least a matrix with integer entries by looking at some isomorphism or some morphism between k theory groups. If those groups were free abelian groups, which they are in this world that we're talking about here, all of these k theory of sea star algebra groups, as the k theory of group sister algebras, they're all actually free abelian groups. So there's a bunch of interesting matrices floating around, and you might ask, is David Vogen's multiplicity matrix one of them? And that's the question which we're going to try to sort of answer if you're optimistic and sort of sea star centric like me, you might ask, could you actually use sea Star algebras to prove David's theorem? And that's, of course, a much harder question. And the answer is sort of in some special cases, namely these real rank one cases that we're talking about up there. That's where we're going.
01:10:30.946 - 01:11:28.526, Speaker A: David Vogen has proved a theorem here. It's not possible to tell just from the statement, but this is really in the thick of things when it comes to understanding not just the temper duel, but the full admissible dual of gee. These temperate representations play an absolutely fundamental role in the gigantic computer, the deep thought like computer, which answers the question, you know, the great question of life. I have a representation here, is it unitary or not? And so the computer says, I don't know, I have to think about it. And then it, then it goes away, and it does calculations with these temperic representations, and it does this that, and a few years later it gives you the answer, right? That's how it works. There is a real such program called Atlas, which does exactly that. There's a command called is unitary.
01:11:28.526 - 01:11:49.434, Speaker A: It should be is unitary, question mark. But it's just, I think is unitary. And it will tell you, you have to tell it what the representations are. There's an art to that. But if you can tell it the name of a certain representation, it will tell you actually in an instant, unless it's really big group, whether it's unitary or not. And it's all about these temperate representations. That's what's going on behind the scenes.
01:11:49.434 - 01:12:29.658, Speaker A: Some people think that what the program does is it just phones up David Bogan and asks, David, but that's not the party line. The party line is an actual computer program which answers the question. And it's organized around these, around many things, but a big part of the story are these temporary representations. All right, very good. So, and the reason I'm telling you all of this stuff, rabbiting on and on and on about this is that the answer to this question involves pseudo differential operators, scalable operators. So it's, you know, we sort of come full circle. There's something poetic to the whole story.
01:12:29.658 - 01:13:51.862, Speaker A: And that's what it's all about. There's poetry, isn't it? And that's what we're aiming for in these lectures. Okay, so we've made an adventure into representation theory, and we're pretty much done. We're going to now go back into the world of geometry, geometry analysis, the world with very fewer groups in its group actions, if you like. Not representations, not completely. At the end, we'll have to go back and visit Langlands again to go back to David's question, question over there. So we'll answer, or attempt to answer by looking at scalable operators again.
01:13:51.862 - 01:14:33.362, Speaker A: Remember those, if you were undergraduates, you wouldn't, because you just remember what you need for the next midterm. And this was way back, wasn't it? Here you go, we're back in business. So we're going to be working on the symmetric space. And after years of getting confused, I've decided that you should take the collection of these cosets, the right cosets, so that you should make a homogeneous space on which GX on the right. That just seems to be better, of course, all equivalent, but that just seems to be better. So that's a certain manifold. It's a symmetric space.
01:14:33.362 - 01:15:13.946, Speaker A: It's a very nice manifold. It's just a copy as a manifold, it's just a copy of RN. And so we know what these things are. And the only, and when I say scalable operators, remember these operators had built into them a certain continuity assumption and a certain proper support assumption. So all of that still stands. We're dealing with properly supported operators, just like we were before. And now the new thing is that these operators should be G.
01:15:13.946 - 01:15:16.774, Speaker A: Aqua variant. Yeah.
01:15:17.314 - 01:15:29.734, Speaker B: So maybe this is like a good premature to ask, why would I go from that to that? What's the logical lead from the bottom right?
01:15:31.194 - 01:16:30.322, Speaker A: The way this worked was I was giving some lectures. We have an online community where we talk about sea star algebra is in front of representation theorists and vice versa. And so I was giving some lectures and about the concept prop isomorphism, exactly what we were talking about last time. And David Vogen said, that's not what I want. When I described all of the wonderful properties that sea Star of G has and the conceptor of isomorphism, he said, what I want is some kind of convolutionality of functions on giving, which has the property that the k theory of that convolution algebra of functions naturally contains a copy. In fact, naturally is the representation ring of K. I said last time that when you calculate what is the k theory of c star of g, you get something which is very close to the presentation ring of k, but it's not the representation ring of k.
01:16:30.322 - 01:16:56.338, Speaker A: For example, I didn't really emphasize this, but that there's a dimensionality in this whole story. And this symmetric space could be odd dimensional. And if it is odd dimensional. Then the k theory, the one I've been talking about in these lectures, is actually just zero. There's also a group called k one, and that's the interesting one in that case. And David wouldn't like that at all. He wants all of the interesting k theory always to be in degree zero and it always to be r of k.
01:16:56.338 - 01:17:26.668, Speaker A: And so then I had a back and forth with David about over email in which I suggested a couple of things and he didn't like them. And then I said, well, if you don't like those, there's nothing else abandoned. Hope it can't be done. And then I thought, well, maybe, hang about, maybe it can be done. So that's the creation story. And it was a bit hit and miss. In other words, we were looking, I was looking for a certain algebra because David Vogen demanded it and I thought it didn't exist.
01:17:26.668 - 01:18:18.594, Speaker A: And lo and behold, it does exist. Okay, okay. In fact, what's written here is not quite good enough. To really feel the deal, we need to study something a little, a little fancier, not much fancier. This is a homogeneous space, and we encountered a homogeneous space just a moment ago up in the example up there. And over the homogeneous space there are lots of equivariant vector bundles, and they all come from representations of g. Excuse me, kick.
01:18:18.594 - 01:19:12.324, Speaker A: And what I want to consider are more fancy operators which look like this. They're hardly from an analytic point of view, they're not fancier at all. So these ease just mean the vector bundles that you get by taking two unitary representations of g and forming this induced vector bundle construction. So these are unitary finite dimensional representations. We certainly want to play with all of the finite dimensional representations of k all at once. That's obvious from the question that we're trying to answer. And so we shall.
01:19:12.324 - 01:19:55.504, Speaker A: So. And there's no harm in thinking of operators which map sections of one vector bundle to sections of another vector bundle. That's like saying there's no harm in thinking of a matrix of scalable or pseudo differential operators, which is just a rectangular matrix. If you have any ring, you can form a rectangular matrix whose elements are from that ring, and then by matrix multiplication you can build the sort of algebra of such matrices over elements in the ring. That's what we're going to do here. But of course you don't get an associative algebra. What you get is a category.
01:19:55.504 - 01:21:07.124, Speaker A: This is not a big deal at all, so much so that I'm going to put it in parentheses. So we can look at a category whose objects, not a group word. So the objects are finite dimensional, for reasons that are going to be relevant later, unitary representations of K and then morphisms as above are scalable operators. So it's not, it's not really a single algebra anymore. It's a whole category, but no big deal. That's what we're going to study. It's a category, but people know how to take the k theory of a category just as easily as they know how to take the k theory of a sea star algebra.
01:21:07.124 - 01:21:56.154, Speaker A: And that's what we're going to do. We're going to take the category that is being discussed here. To avoid menacing issues in algebra, we're going to complete these various morphism spaces to make what's called a sea star category. That's because if you have a sea star category, figuring out what is the k theory of that C star category is particularly simple, whereas forming the algebraic K theory of any old category, that's a lot messier. So we're going to take this category, we're going to turn it into a so called C star category. That just means we're going to take some norm completions of these spaces of morphisms. In order to do that, we're going to be working with order zero operators because those are the ones that extend to bounded Hilbert space operators.
01:21:56.154 - 01:22:47.454, Speaker A: And then we'll have a sea star category, and then we can take the k theory of that C star category, and we shall see that the k theory of that C star category, okay, it's not quite an algebra, but it's pretty close, is exactly the representation ring of K. And that assertion that I just made I know to be true because I know the concasper of isomorphism to be true. This is an equivalent formulation of the concasperov isomorphism. So it's a non trivial statement, but it's true. And so there's an isomorphism from r of K into this c star category. We'll also build amorphism from the k theory of this sea star category. Two, the free abelian group on David Vogen's these guys, where are they? Temporary representations which are here.
01:22:47.454 - 01:23:37.436, Speaker A: And if we a natural morphism by evaluating pseudo differential operators on representations, that's some kind of Fourier transform some very natural morphism. And then when you compose the morphism from r of k to the k theory with the morphism I just hinted at, from k theory to the free abelian group on temper representations, you'll get a map from r of k to the freelian group on temperic representations. Each of those has a natural basis. And so that what you actually have is a gigantic matrix of integers. And that will be a gigantic matrix of integers which is upper triangular and the ones down the diagonal. It's exactly the matrix that I was talking about before. So just under the surface here, or just above the surface, I don't know where it is.
01:23:37.436 - 01:24:11.004, Speaker A: Just a little bit beyond this definition, is a matrix. And that matrix, the invertibility of that matrix is basically equivalent to David's theorem. It's a big part of David's theorem, and it's closely related to the Kon Kasparov isomorphism. And so we're kind of in the right territory to, to provide to David a reasonable answer to this question. That's what we'll discuss. Not today, there's only three minutes left. That's what we'll discuss on Monday.
01:24:11.004 - 01:24:43.744, Speaker A: And I wanted to say one more thing. Oh yeah. Getting back to real rank one groups, that's still on the board there. We'll check David's isomorphism using a calculation in K theory of sea star algebras in real rank one. I don't think this is a terribly profound contribution to representation theory, because in this particular case, real rank one is a profound simplification. But it's a start. So we'll see you get a little insight into David's theorem just by doing a calculation in K theory in real rank one.
01:24:43.744 - 01:25:05.764, Speaker A: Yeah, so it's an encouraging story. I'm not claiming we're about to put David Vogen out of business. That doesn't appear to be the case, but it's an interesting connection that's emerging. Very good. We shall continue this whole adventure after Thanksgiving.
