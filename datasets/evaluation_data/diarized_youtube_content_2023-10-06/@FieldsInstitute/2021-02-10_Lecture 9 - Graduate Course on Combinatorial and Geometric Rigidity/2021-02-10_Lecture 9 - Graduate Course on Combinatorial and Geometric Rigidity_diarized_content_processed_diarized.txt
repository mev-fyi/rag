00:00:00.240 - 00:00:41.930, Speaker A: What is meant by the title? So we're going to be dealing with complete bipartite frameworks. I'm just going to call them bipartite frameworks all the time, because we're not bothering with anything that's not complete. Just got here. The vertex that I'm going to denote by m and n. So m is going to be the vertices of p, n is going to be the vertices of q. You notice I split the realization into two parts, just to make things a lot easier. The m brackets and n bracket just means the first m or the first n natural numbers.
00:00:41.930 - 00:01:28.864, Speaker A: So one to m and one to n, just for some notation. So what is the question we're asking? Basically, what we're saying is if I give you a complete bipartite graph, what choices of p and q will make it infinitesimally rigid, and what choices of p and q will make it infinitesimally flexible? So this is what Tony's talked about previously. I'm not asking what makes it rigid. So when I mean rigid, I mean like continuously deforms, because that's a very different question. It's very hard. We've only managed to do it for a few graphs, and I've got a reference at the end if you're interested in that sort of thing. Most of the stuff in this lecture will be from Bulkhorn Roth's paper.
00:01:28.864 - 00:01:44.884, Speaker A: So they wrote a paper on this. I'm going to be following through their ideas. I'm going to simplify some of it. Hopefully some of their ideas are going to skim over. So if you want to come back and reread some of this stuff, it's in their paper. I've got the reference just here. Okay.
00:01:44.884 - 00:02:41.168, Speaker A: Equilibrium stresses, I can't remember if Tony's mentioned these or not, but equilibrium stress we're going to define, because we've got the special case of bipartite frameworks, we can define it as m by n matrix. So we just entries omega, I, omega I, j. Okay. And what it's going to do is it's going to satisfy this equation here, right? So equilibrium stresses are kind of like the dual concept of infinitesimal flexes, because infinitesimal flexes is you're looking at the kernel of your rigidity matrix. Equilibrium stresses is you're looking at the co kernel, or you can think of it as like the right kernel or something like this. You're looking at that instead. So you're looking at the linear dependencies of the row of your rigidity matrix.
00:02:41.168 - 00:03:36.904, Speaker A: But for us, we can just think of it as being anything that satisfies these two equations. And we're going to denote the space of them by big omega PQ. Okay, we don't really need to talk about the graph, because the graph is always going to be km. Kmn, right. So we're going to say a bipartite framework is spanning if the affine span of all the points, all the vertices, if that affinely spans the whole space. If we've not got this, you can show that any framework that's not too small, so on at least D plus one vertices, and there's a complete graph which we don't have because we're dealing with bipartite graphs, you can always show that they're infinitesimally flexible if they're not spanning. So we never really have to deal with anything that's not spanning.
00:03:36.904 - 00:03:38.052, Speaker A: Okay.
00:03:38.188 - 00:03:42.364, Speaker B: Hey, Sean. Oh, yeah, can you go back a slide?
00:03:42.524 - 00:03:45.852, Speaker A: Yeah, sure. Sorry, what?
00:03:45.908 - 00:03:51.144, Speaker B: Can you help me understand this? Because I thought I knew what an equilibrium stress is, but how is it a matrix?
00:03:51.724 - 00:04:20.608, Speaker A: So this is because I'm dealing with a very special case. So it's not usually a matrix. You are correct. But because I'm dealing with bipartite graphs, I can write it as a matrix. I mean, just think about it. An equilibrium stress, you would usually think, is like a vector. Well, because I've got m vertices on one side and n vertices on the other, and my edges are going to be exactly the pairs mn, you know, all those, the cartesian products of them, then I can write it as a matrix.
00:04:20.608 - 00:04:27.824, Speaker A: And the reason why will hopefully become apparent a bit later why I'm doing this. This is a special case of it, though. You are correct. You wouldn't usually write it like that.
00:04:27.944 - 00:04:44.048, Speaker B: Okay, can I make sure I understand? So there are m times n entries of this matrix, just like there are m times n entries in the vector, which would be in the left kernel of the rigidity matrix.
00:04:44.176 - 00:04:44.884, Speaker A: Yes.
00:04:45.264 - 00:04:51.830, Speaker B: Okay. So you're rearranging a very long vector into a matrix. Okay, thank you.
00:04:51.982 - 00:05:12.234, Speaker A: Yep. Okay. What you're going to see later on is that if I write the p's and the Q's and matrices, I can then just multiply this matrix on one side by one and on the other side by the other, and that will be the equilibrium stress condition for certain stressors. This is why we're doing it. But I'll explain that later.
00:05:13.114 - 00:05:27.738, Speaker C: Sean, I have a real quick dumb question. So is this basically about the whole spanning stuff, saying that if our framework can live in a smaller dimension than what we're actually caring about, then it's flexible.
00:05:27.906 - 00:05:38.564, Speaker A: Yeah, it's exactly that. I've just words that slightly different, but it's exactly that result. And I'm not going to go into proving why it's true that we don't need to bother with them, but we don't need to bother with them.
00:05:38.714 - 00:05:56.856, Speaker D: So one more question, Sean. So when you say it's infinitesimally flexible, like, is this like, is the framework actually flex or this like a technicality? Coming from the fact that since it's in like a smaller dimensional subspace, you just happen to have, like, space, it.
00:05:56.880 - 00:06:22.260, Speaker A: Will be an infinitesimal flex. You can't guarantee it's a flex when it's living in a smaller space. I guess, like, I mean, if. Okay, so think about it. And if you take a universally rigid framework, what you're saying is, okay, it's rigid in all higher dimensions, but it won't be when you embed it as it is, but it won't be infinitesimally rigid in any of the higher dimensions because it's flat. Okay, I see, I see. Unless it's like complete or something.
00:06:22.452 - 00:06:30.180, Speaker D: But in general, is that sort of like the generic picture to have in mind, like a universally rigid framework that does have an infinitesimal flex, but it's.
00:06:30.212 - 00:06:47.108, Speaker A: Like, I wouldn't think about universal rigidity with these bipartite crafts at all. Okay. Yeah. Just think that if it's flat, then I can do an infinitesimal flex. Stick to infinitesimal flexes, don't think about rigidity, because we're not dealing with that right now. Okay. Okay, gotcha.
00:06:47.108 - 00:07:30.624, Speaker A: Okay, thanks. Right. Okay, so if we take a spanning bipartite framework, so it's not flat, then we can show that it's going to be infinitesimally rigid if and only if this equation here is true. So why would this be true? Just to give you some intuition. This bit here will be the number of edges. This bit, if you times it by minus one, then I guess if you think about it, this will be d times the number of vertices minus the number of degrees of freedom. Okay, so this thing here kind of tells you about the freedom of your framework in some ways.
00:07:30.624 - 00:07:58.290, Speaker A: Like, and just by using some linear algebra, you can kind of convince yourself that this is going to be true. Because instead of talking about the image, instead of using rank nullity, I'm using like rank naughty with co kernels and kernels. But it's this sort of linear algebra argument. You shuffle things around and prove it. And I've left it as an exercise if anyone wants to do it. But that's the general reason. And we're going to be using this because this is much easier to use.
00:07:58.290 - 00:08:00.698, Speaker A: So could you, could you just one.
00:08:00.706 - 00:08:04.586, Speaker D: More time state the definition of omega PQ or just go back to the slide?
00:08:04.730 - 00:08:13.282, Speaker A: That's just the set of all these equilibrium stresses when considering. We're considering as matrices, but, yeah, it's the set of them. Great, thanks.
00:08:13.458 - 00:08:16.654, Speaker B: Okay, which is a vector space.
00:08:17.114 - 00:08:38.794, Speaker A: Yeah, yeah. Because if you add two stresses, you still have a stress. Just look at the equation, you can convince yourself. Yep. Okay. Right. What was I going to say? Ah, okay.
00:08:38.794 - 00:09:09.374, Speaker A: So, naively we would want to say that this is going to be. So a bipartite graph is going to be generically rigid if it satisfies Maxwell's condition. So we know that if it is infinitesimally rigid, if it's generically rigid, sorry, it has to satisfy Maxwell's condition. So that's this counting condition here for the bipartite graph anyway. And we would want to say it's an if and only if, but that's not true, unfortunately. We should actually prove it later. But it's not true.
00:09:09.374 - 00:09:42.764, Speaker A: And K 67, which we'll deal with later, or actually we won't deal with that one later, but this one doesn't work. So this one satisfies the four dimensional Maxwell count, but it's not generically rigid in 4d. Okay, that's one example. There's many other examples. Okay, but we'll deal with this later. This is what we're proving. Okay, so what do we want to do instead? We're going to go a bit weird and we're going to look at the dependencies on our placements of vertices.
00:09:42.764 - 00:10:18.072, Speaker A: I've got a message. Oh, yeah. Lewis has solved the puzzle. So we're going to look at the dependencies on the placements of points. Okay, so what I was saying before we got our placement p of some of the vertices of our bipartite graph. So one half and Q's the placement of the other half. We're going to just say that big p is the d by m matrix where the columns are the PI's.
00:10:18.072 - 00:11:02.848, Speaker A: Okay, so we put all the p's together and we put them like columns in this matrix and we do the same for q. So they're going to be the columns of this matrix. Okay, we're now going to be looking at the affine dependencies of these sets of points. So P defines a set of points. We can talk about the affine dependencies. So these are, these lambdas such that this holds and this holds. Okay, now this just basically says that we can put some scalars on each one of our points, PI, sum them all together and get zero.
00:11:02.848 - 00:12:00.964, Speaker A: And we can also say that the scalars themselves sum to zero. Okay, I won't go into it too much, but that is, that is what an affine dependency is and we've rewritten it so it looks like matrices. Okay, what I'm doing here is I'm considering this to be a column vector, if you're wondering, and, yeah, and we can do the same for DQ, okay, what we're going to be doing then after that is we're going to be talking about tensor products. Okay? Now these aren't going to be anywhere near as general as tensor products usually are. When I'm saying tensor product, you can just imagine it as taking a column vector and multiplying it by the transpose of another column vector. That will get you a big matrix. In our case, we're going to be getting n by n matrices, okay? So you can just think of it like this.
00:12:00.964 - 00:12:29.644, Speaker A: It's very simple, tensor products, I'm not going to go too much into tensor products. And if we take the linear space X and a linear space Y, we can tensor them together. And what that is is we tensor every single pair of things. So everything in x tensors with everything in Y, but then we take the linear span of it. So importantly, not everything in here is going to be equal to x. Some x tends to some y. It could be some linear combination of the two.
00:12:29.644 - 00:13:07.264, Speaker A: So this is important. It's not just pairs. It's got to be a bit more complicated than that. Otherwise we just have like the cartesian product. So now we're going to prove this theorem. So why are we looking at these dependencies? Well, firstly, they're very geometric, so that's going to help things. And secondly, we can relate it to this space w of equilibrium stresses with row and column sums of zero, okay? Because remember, we're thinking of our equilibrium stress as a matrix, n by n matrix.
00:13:07.264 - 00:13:41.516, Speaker A: So these are the stresses where when we think of it as a matrix, we take a row, we add them all together, we get zero, we take a column, we add them all together, we get zero, okay? And we're going to say that, well, actually they're just the same as taking dependencies on P, dependencies on Q, tensoring together, okay? This is a very bipartite thing to do. You're not going to do this in a general framework, but we're going to show this, it seems very counterintuitive result. Oh, sorry, Alex, have you got a question?
00:13:41.700 - 00:14:11.316, Speaker B: Yeah. So the space of all the equilibrium stresses is a vector space of some dimension. And now when you say that row and column sums are zero, you're making some more linear equations on it, requiring. So now we're going to get a subspace of that. Okay. And then the vectors in this subspace w, they can be interpreted as stresses coming from an affine dependence. Can you help like explain this?
00:14:11.420 - 00:14:24.184, Speaker A: Well, I'm going to do it on the next slide. So I'm going to convince you that these two sets are the same because they look very differently and you think about them very differently. So it's very nice that they are the same thing. That's the point of the result.
00:14:25.244 - 00:15:13.344, Speaker C: So, Sean, I have a, just to make sure I'm understanding this correctly. So DP and DQ are sets of these vectors that represent affine dependencies on P and Q. And then we're taking those vectors in RM and we're taking their tensor product, which just gives us an m by n matrix. And then, so now what we're saying is we're taking DP tensor product, or however you say it, with DQ, and that'll give us a set of m by n matrices. And we're saying that those are the equilibrium stresses with some of these added sums or zero.
00:15:14.484 - 00:15:51.204, Speaker A: That's exactly what I'm saying. It doesn't seem like it's true, but it is. So to prove this, we're going to notice some observations first. Oh, there we go. So we've got some observations to make first. So first is if I take omega and it's an equilibrium stress, that's going to be true if and only if these sums are true. Now, all I've done is I've took the sums here and I've took the PA part and shoved it over here, took the QP part and put it over here.
00:15:51.204 - 00:16:20.108, Speaker A: So for any a and any b, we're going to have that qb times all this is equal to this sum. So we can take the sum out because it's just going to be qb is going to be inside here. So it's going to be the same for every one, so we can pull it out. And similarly over here. Okay, now we're interested in the space w. Now, in this space w, I said that all the column sums and row sums are equal to zero. So this thing is going to be equal to zero.
00:16:20.108 - 00:16:43.068, Speaker A: This thing is going to be equal to zero. So what we're going to have is that we're going to have, this thing over here is equal to zero. This thing over here is equal to zero that can be rewritten as a matrix. And this is why we've done it as matrices. So this is going to be true if and only if. So omega is going to be in w if and only if p. And we think of the points as a big matrix.
00:16:43.068 - 00:17:37.914, Speaker A: The d by n matrix times w or product with w is equal to a zero vector. And similarly, if q product with omega transposed is going to be equal to zero and we have to transpose it because it's an m by n matrix. So we need to make an n by m matrix to make it work. Basically, I'm not going to go into why, but you can kind of convince yourself of this. I probably should have put easy exercise. Now, I'm just going to state this. The latter two conditions are equivalent to if I take all the vectors of size n and multiply it by omega, this lies inside here, inside the dependencies of p, and similarly over here, this lies inside the set of dependencies over here.
00:17:37.914 - 00:18:42.552, Speaker A: So this is equivalent to, sorry, this, this statement and this statement are equivalent. And this statement and this statement are equivalent. This is kind of just some, by kind of looking at the definitions, looking at what we're dealing with and just noticing that this has to be true. Okay, now this condition isn't particularly very nice because we've got the transpose of w, but we've got this nice little result here. So if you have any n by n matrix m and any set a, if we have the image of a mate of the transpose of a matrix inside a, then the set of all vectors that are perpendicular to everything in a, which is actually a linear space. Something else to prove that will lie inside the kernel of the matrix, because this is what we talk about here is the image of omega. So we want to kind of do something over here with the kernel of omega we'll be using later.
00:18:42.688 - 00:19:10.714, Speaker C: Yeah, so, okay, so we're saying here that, for example, p omega equals zero if and only if omega times, like some vector in r to the n is a dependence for dp. Yeah. So I guess what I'm wondering is why do we have to multiply by r to the n? Like, wouldn't it, wouldn't it just be omega? Is it a dependence or is that.
00:19:11.894 - 00:19:41.490, Speaker A: No, we still need to so we can basically, it's to do with. So what we got is we've got a matrix, we're taking a column vector and we're multiplying the two together. And what you end up with is kind of like a sum of the columns which will still satisfy. If you sum the columns together, that will still work. It's not just the individual columns, it's basically the idea. Okay, I don't want to go into it too much, because a lot to cover, but that's the idea.
00:19:41.642 - 00:19:42.454, Speaker C: Okay.
00:19:43.794 - 00:20:10.394, Speaker A: Okay. So using these properties, we're going to show it's true. So we've got something in the chat. Yeah, yeah, exactly what Alex is saying. It's the column space. Okay, so first thing we'll do is prove that this lies inside w. So every single one of these tensor products and dependencies is inside w.
00:20:10.394 - 00:20:39.072, Speaker A: We just pick one. So we've got some lambda, some mu. I'm just going to bother with the lambda tensor with mu. I did say that the span of all those things. But basically, if we show it's true for every single pair like this, then as I said here, then they span the whole space. So we'll be done basically, because w is a linear space. And if you show that a basis of a linear space lies inside another linear space, then the whole space lies inside the space, basically.
00:20:39.072 - 00:21:07.384, Speaker A: Idea. Okay, what we do is just simple calculation. So we multiply p by this. That's equal to this. Well, this is saying p times lambda, which if you remember correctly, this is literally a condition of it being an affine dependence. Okay, so this is going to be equal to zero. So we've got zero times, this is equal to zero.
00:21:07.384 - 00:21:27.776, Speaker A: And similarly, we have something over here. And this is just, yeah, we're using this observation here. So omega is in w if and only if this is true. Okay. So we proved that this is true, therefore this is inside, that should say w. I do apologize. Okay.
00:21:27.776 - 00:22:14.454, Speaker A: And then, like I said, okay, the next thing is just the dimension argument. So we take some, right, well, okay, yeah, this still works. So we take some equilibrium stress. Then we're going to have that the equilibrium stress lies inside w if and only if the image lies inside the dependencies of p. So this is just this observation here. And the, what's it called? The set of all vectors that are orthogonal to dq lies inside the kernel. And this is just this result plus this result here.
00:22:14.454 - 00:23:05.124, Speaker A: So this is true if and only if these two things are true, this linear space. So it's all matrices where this linear space is inside the kernel and this, the image lies inside some other linear space. You can work out what they are quite easily and you can actually show that it's going to have dimension. It's going to have dimension. What it goes from rn to rm. Well, the image has to be inside this space with this dimension, so that bounds it over here, but the image has to be inside the n minus, the dimension of this. Well, n minus the dimension of this is just equal to the dimension of DQ.
00:23:05.124 - 00:23:34.464, Speaker A: And all such matrices because you've got going from one space to another, then you just multiply them together and that gives you the dimension of all the matrices that go between. We know w lies inside this space. Quite easy to see why from what I've just said. And this has dimension like so. So we've said that the dimension of this is less than or equal to this, but w also contains this. So you have it from the top and the bottom. Bam, it's the same thing and we're done.
00:23:35.084 - 00:23:55.736, Speaker D: Okay, Sean, I have a question real quick about just one of the things on the previous slide. So the bullet point, the latter two conditions, are you saying both of those two conditions together are equivalent to those two conditions on the next line? Or is it like, you know, the p condition is equivalent to the p and the q is to the q.
00:23:55.920 - 00:24:02.764, Speaker A: So the p with the p and the q with the q. I think so.
00:24:03.144 - 00:24:06.440, Speaker D: I am confused about that because maybe.
00:24:06.472 - 00:24:07.992, Speaker A: It needs to be both at the same time then.
00:24:08.048 - 00:24:14.924, Speaker D: Okay, because, yeah, because the p condition in the top seems strictly weaker than the p condition in the bottom.
00:24:14.964 - 00:24:18.064, Speaker A: Right, because, let me think.
00:24:18.404 - 00:24:24.864, Speaker D: Because the p condition of the bottom says pw zero and also.
00:24:26.364 - 00:24:35.904, Speaker A: Ah, yeah, okay, I get what you mean. Yeah, I think. Okay, so I think it's both of them at the same time then. Okay, I wrote this a week ago, so I convinced myself it was true anyway.
00:24:36.884 - 00:24:39.854, Speaker D: Okay, just great, making sure I'm not going crazy. Thanks.
00:24:40.434 - 00:24:41.694, Speaker E: Can I say that again?
00:24:43.834 - 00:24:44.814, Speaker A: I need to.
00:24:45.274 - 00:25:07.884, Speaker E: Okay, so on top you have just the equilibrium condition rewritten. If you further impose that the right hand sides are by fiat zero, because the sums of numbers are. That's your w space. Yeah, right. But there are other things in equilibrium that don't have this property.
00:25:08.984 - 00:25:13.964, Speaker A: Yes, right. Yes.
00:25:15.544 - 00:25:20.288, Speaker D: But then if you're simultaneously an equilibrium of both, you do have to have that property, I guess.
00:25:20.336 - 00:25:21.324, Speaker A: Is that the.
00:25:21.624 - 00:25:24.404, Speaker E: No, it's that w is a subspace of all the stresses.
00:25:26.104 - 00:25:28.792, Speaker D: Oh wait.
00:25:28.848 - 00:25:36.052, Speaker A: Yeah, yeah. I'm not saying it's equal. Thinking about it, I'm just saying it's. Oh, oh, okay.
00:25:36.228 - 00:25:54.624, Speaker E: So somehow the point is that like in the language of stress matrices, if the stress matrix has zero diagonals, then the stress coefficients are actually affine dependencies on the neighbors of every vertex.
00:25:55.364 - 00:25:59.344, Speaker A: It's worth mentioning that the stress matrices Lewis is talking about is not the same.
00:25:59.644 - 00:26:04.104, Speaker E: Yeah. So strangely, like my stress matrices, these are not. Sean? Stress matrices. Sorry.
00:26:04.404 - 00:26:27.020, Speaker A: Yeah, so these are, I'm calling them equilibrium stresses, not stress matrices, to make this clear, because stress matrices are their own thing, which will, Tony, will get to at some point in the lecture series, but don't confuse yourself with those stress matrices. These are completely different. But they, you know, you can use ideas from stress matrices, obviously. Yeah. Okay.
00:26:27.052 - 00:26:36.504, Speaker C: Yeah. Sean, I have another quick dumb question. So when you say image of m, or image of m transposed there, what do you mean by that?
00:26:37.404 - 00:26:53.020, Speaker A: So it's just literally this. So if we think of m as the matrix over here, we're just timing it by everything on the right, basically. So you think of a matrix as a linear map where it's the matrix times something. It's just to show that you're not doing it the other way around.
00:26:53.132 - 00:26:57.706, Speaker C: Right. Yeah, I just didn't see like what we were multiplying m by down there, so. Okay, thanks.
00:26:57.730 - 00:27:29.716, Speaker A: Yeah. Okay. Right. This is a corollary that just comes immediately from it. So what I'm going to do is I'm going to take a linear map. This goes from our equilibrium stresses to this space here, r plus n. And what it's going to do is it's going to basically show you what the stress is summed at each vertex on p and every vertex at q.
00:27:29.716 - 00:28:07.000, Speaker A: Okay. These are kind of what Lewis talks about when he talks about the diagonals of your, of your stress matrix, but the language is possibly a bit confusing. Okay, so we got this thing here. You can just take it as a linear map that looks like this. You don't have to think too much about it, but what we've just proved from the previous result is the kernel of this has to be equal to this because the kernel is just when each one of these is equal to zero. Well, when each one of these is equal to zero, we've got a equilibrium stress with all the row sums and all the column sums equal to zero. So it has to be exactly the same thing.
00:28:07.000 - 00:28:37.868, Speaker A: And that's what we've just proved. Okay, well, this, this gives us some hope because we've managed to say that the kernel of this map is equal to this thing. Here, which is very geometric. Okay, well, we know that the domain over here is the equilibrium stresses. So if we know what the image does, we've characterized the equilibrium stresses, and we use the equilibrium stresses to then say, is infinitesimally rigid or flexible. So we're basically done at that point. Okay, that's kind of the idea.
00:28:37.868 - 00:28:44.904, Speaker A: What we're going to see is this isn't going to work as perfectly as we hope. But that is the general argument.
00:28:45.254 - 00:28:50.902, Speaker F: Sean, the conclusion here, kernel of t is TFq, is.
00:28:51.038 - 00:29:00.534, Speaker A: Oh, God. Sorry. This should be FPQ. Okay, cool. Very sorry about that. Didn't even notice. So this should be kernel of FPQ.
00:29:00.534 - 00:29:26.364, Speaker A: Do apologize. Okay, so this is the argument we're going to go for. Okay. We're now going to define yet another space, because this space is going to be our image. And as you're going to see, it's not very nice to look at. It's very unwieldy. So we're going to be wanting to get rid of this space as quickly as possible.
00:29:26.364 - 00:29:53.920, Speaker A: So what we're taking is this set d, two of p and q. This will give me the set of all linear, the linear space of all vectors of this form. So you got a lambda in here and a mu in here. Okay. And it's got to have these three properties. Now, why it's got the little square, just because you're dealing with tensor products and you're tensor in something with itself. So it's kind of like a square.
00:29:53.920 - 00:31:01.614, Speaker A: So that's why it's got the square there. If you're wondering, I actually added an extra condition, which Bolker and Moff get to later, but I'm just going to add it straight in now, because we're skipping this result in some ways. Okay, so else in the chat? Okay, so what we're going to have is we're going to have that if you take all the tensor products and do these sums, you're going to get zero. So what this means is that lambda mu is going to be an affine dependence on the set of all points, PI, tensor with PI, and QJ tends with QJ. If you think of them as a set of vectors, then lambda mu is going to be a affine. Dependence on that set of points is one way of thinking about it. Okay, so this is a much, we're dealing with a much bigger space, but we're still kind of dealing with affine things.
00:31:01.614 - 00:31:41.394, Speaker A: Okay. The next thing is just saying that the row sums, you know, like we said before, we want it to be an affine dependence, so we want it to always sum to zero. So these two conditions, like I said here, so this is a subset of the affine dependencies of this set. This condition here is going to become apparent why we need it later. So this is just saying that we think of our points p and q. Okay, we take a point PI. We want to know, is it in the affine span of the points in Q? If it is, we include it, we allow it to be zero, non zero.
00:31:41.394 - 00:32:21.236, Speaker A: We can be whatever it wants. But if, say, the affine points of p are like flat like this. Sorry, the points of p are flat like this, Q. Sorry, the points of Q are flat like this, and they, the affine span is some plane and this isn't lying in it, then we have to give it zero. Okay, and we do the same for the p points. So we're going to say that vice versa is going to be true. Okay, so if I take a point qi, and I take the affine span of the points PI, if qi isn't in the affine span of this, then it has to have a zero for its corresponding point in Mu.
00:32:21.236 - 00:33:03.514, Speaker A: Okay, so this is quite important condition. Basically, if we don't assume this, this is not true. That is why we do it. And there's some good geometric ways of thinking about it, I guess, but I don't fully understand them sometimes. But kind of one way to think about it is, no, I won't go into it because I'll say something wrong. But this is an important condition. If we define this set as we just have, then for any bipartite framework, we're going to have that the image of the map we just defined before is going to be equal to this.
00:33:03.514 - 00:33:41.990, Speaker A: Okay. And then we can then just use the rank nullity theorem to say that the dimension of this is equal to the dimension of this, which is the kernel of the should say FPQ. Very sorry. So the kernel of FPQ plus the image of FPQ. Okay, so now we got this formula. We know that the stresses are equal to this, times this plus this. But as you've seen, and you probably think about it, but this set here is not very well, nicely defined at all.
00:33:41.990 - 00:34:07.974, Speaker A: It's very obtuse, and it's not very, very helpful at the moment. So unless we can kind of characterize it in a nice geometric way that we can use, it's effectively useless. We haven't really done anything. We just moved, we kicked the can down the road right but we can. So it's not all bad, but any questions about this before I move on, because this is quite a bit of an info dump.
00:34:11.594 - 00:34:16.946, Speaker B: These are d by d matrices in I or one.
00:34:17.130 - 00:34:22.534, Speaker A: Which one? Sorry, these things, no, these are just vectors.
00:34:23.154 - 00:34:27.054, Speaker B: No, I mean low and condition little I.
00:34:28.714 - 00:34:33.359, Speaker A: Which one? Sorry, which condition?
00:34:33.481 - 00:34:34.019, Speaker F: The first condition.
00:34:34.051 - 00:34:36.891, Speaker B: The first condition, little I p I.
00:34:36.987 - 00:34:40.387, Speaker F: Tensor PI is d by d matrix.
00:34:40.515 - 00:34:42.343, Speaker A: Yeah, that'll be a d by d matrix.
00:34:44.923 - 00:34:56.983, Speaker B: Okay, so the lambdas and mus are an affine dependence of the d dimensional vectors and these d by d dimensional or d by d matrices as well.
00:34:57.323 - 00:35:15.964, Speaker A: The just say affine dependence on these things. But if we say this, then we get this as well. We just get this for free, which I forgot to mention, we get that it's an affine dependence on the set PQ at the same time. Oh, sorry. What are you gonna say?
00:35:16.084 - 00:35:18.464, Speaker B: They are equivalent, I think.
00:35:18.844 - 00:35:28.192, Speaker A: No, they're not. So you need this condition to imply this condition, but this condition does not imply this condition. As far as I know it doesn't.
00:35:28.248 - 00:35:39.724, Speaker E: Um, so one way to think about this, if you like homogeneous coordinates, is that if you are tensoring p hat with itself and q hat with itself, then you would also get the, um, affine part.
00:35:41.544 - 00:36:02.106, Speaker A: Yeah. If you want to, if you want to do the exercise I've suggested, if you look at the matrices, you're going to get d and d by d entries, sort of thing. And you can use these entries to prove this basically is the idea. It's not too hard to show.
00:36:02.250 - 00:36:05.170, Speaker B: Okay, I'll do the exercise. I just took a screenshot.
00:36:05.322 - 00:36:09.374, Speaker A: Good, good. This is a nice exercise. It's not too bad.
00:36:09.754 - 00:36:18.174, Speaker C: I have a quick question. Any idea how long it took them to concoct this d squared pq part?
00:36:19.994 - 00:36:54.558, Speaker A: Well, no, but I've never asked them. Um, it's not the most, in some ways, it's not the most logical way to think about it, but it's also the most logical way to think about it. Once you've thought about it a bit. Basically, the more you look at these bipartite frameworks, the more you realize that it's just the, you already know what all the edges are. All the edges are there, so you don't really have to think about edges doing anything. So it's kind of, and the edges go between the two sets. So it's kind of like how this set affects this set, and it can only really affect it by these affine dependencies.
00:36:54.558 - 00:37:23.856, Speaker A: It's a bit more complicated than that as we're showing, but that's kind of why. So at some point they realize that these are the things that they should be looking at. They probably take a complete guess. They got this map and then realized that if they know what the image is, then they can do everything. And then they computed that this was the image. That's how I think they probably did it. But, you know, I can't, you know, I don't know what their thought process is, but that's why I'd assume so.
00:37:23.920 - 00:37:25.684, Speaker E: There is if. Is Walter here?
00:37:27.264 - 00:37:29.144, Speaker A: I don't think so.
00:37:29.304 - 00:37:53.228, Speaker E: Okay. So I don't know how this works in sequence, but either right before or right after, Walter has a paper where he argues about the importance of conics to infinitesimal motions of these frameworks. Okay. And you can think. So.
00:37:53.356 - 00:37:54.664, Speaker A: So that came out before.
00:37:57.244 - 00:38:05.104, Speaker E: Right. And there's a deep. There's a connection between these dependencies and tensor powers and conics.
00:38:06.084 - 00:38:10.572, Speaker A: So they probably built on his work and fully characterized it through that.
00:38:10.628 - 00:38:14.744, Speaker E: I have no idea. But, yeah. So I don't know who was learning from who here.
00:38:15.284 - 00:38:16.124, Speaker A: Okay.
00:38:16.284 - 00:38:25.744, Speaker E: Right. But one line of reasoning is that you could say, well, stresses appear from failures of general position. And so you could say, well, let's go up one degree.
00:38:27.484 - 00:38:28.984, Speaker A: Yeah. Okay. Yeah.
00:38:30.444 - 00:38:32.104, Speaker E: And then you could just be bold.
00:38:32.604 - 00:38:55.404, Speaker A: Yeah. Okay. Because the idea of this is we, if you have the complete graph, it's very easy to say what the stresses are. And when they fail, like you said, in general position. And we've just gone one up where it's almost complete, but it's bipartite instead. So, yeah, I guess that's the next logical step would be to have quadratics, which I'll get to, actually, in the next slide. Right.
00:38:55.404 - 00:38:59.264, Speaker A: So, yeah, we want to characterize what I should say.
00:38:59.304 - 00:39:04.960, Speaker E: Because, you know, this is. There's many experts here and stuff. I have no idea if this is actually like a true sequence at all. Right.
00:39:04.992 - 00:39:05.644, Speaker A: Okay.
00:39:06.784 - 00:39:09.044, Speaker E: And if Walter was here, he could probably tell us.
00:39:09.434 - 00:39:35.734, Speaker A: I'll ask him later, actually, because I'm interested to see what he said. I've got a few other questions about this. Right. Okay. So like I mentioned, we were talking about quadratics. Now they call it quadric polynomials instead of quadratic polynomials. So if anyone here can tell me what the difference between a quadric polynomial and a quadratic polynomial.
00:39:35.734 - 00:39:50.994, Speaker A: Anyone here? I'll be very welcome. I'll be very glad to hear. Because when I did a wiki definition, they're exactly the same thing. But put it in the chat, if you know the difference?
00:39:51.374 - 00:39:53.982, Speaker F: I always thought that quadrant polynomials referred.
00:39:53.998 - 00:39:58.110, Speaker A: To degree four, but no, that'd be a quartic.
00:39:58.302 - 00:40:01.784, Speaker F: Quartic. Ah, yes. Yes, you're right, quartz.
00:40:02.354 - 00:40:35.692, Speaker A: So I've got something in the chat. Yeah. So the definition of a quadrant polynomial they give is what I would just say is a quadratic polynomial. If you go on Wikipedia, what they've got as a quadric polynomial is a quadratic polynomial and vice versa. You tend to talk about quadric surfaces instead of quadratic surfaces and stuff like this, but there's no really much. I can't really tell any difference between the two. Usually the quadric is the geometric object in our age.
00:40:35.692 - 00:41:08.492, Speaker A: Yeah, so I've seen that. But I've also seen quadric polynomials usually then defining quadric hypersurfaces, but it doesn't seem to be set, which seems really odd to me because I've seen all the notation, everything in between I've just described as. Well, I'm getting off on a tangent here, but. Yeah, yeah, as William says, having one name would be too easy. Anyway, you can think of them with quadratics as you want. Doesn't make any difference. So it's just one of these things here.
00:41:08.492 - 00:41:44.772, Speaker A: So this is with h variables. Okay. If we take the set to be quad, we can say that it's a linear space of dimension h times h three over two plus one. And that's just because it's defined exactly by these triples. So these triples are going to have you got h plus one, choose two choices here, you've got h choices here, and you've obviously got one choice here. And that's where the linear dimension comes from. Okay, so we're going to take these quadrant polynomials.
00:41:44.772 - 00:42:16.784, Speaker A: We want to see where they vanish basically on our points, but we don't actually really care about some of our points. The only points we really care about are the ones which any PIs that are in the affine span of the qs and any q eyes that are in the affine span of the p's, basically. And this is boiling down to. Oh, sorry. This is boiling down to this condition here. Okay. It's linked, but yeah, we'll see why.
00:42:16.784 - 00:42:56.624, Speaker A: So we're going to define RpQ to be the set of PIs and qis which satisfy this. Okay. And we're going to, basically, we want to kind of get an affine transformation to kind of normalize things in some ways. So this is going to be a transformation which this should be the affine span of RPQ so it's an affine transformation that moves your points around, but doesn't flatten any points into a line or anything like this. And we're just going to do that. So it just kind of makes everything nice. It maps it to a space of dimension R.
00:42:56.624 - 00:43:18.072, Speaker A: H. H. Here will be our dimension of the affine span of RPQ. So all we've done is we've taken our space and just made it our affine span, which could be some horrible hyperplane in some higher dimension, and we just made it a linear space so we can deal with it nicely. And then we're going to define this space here.
00:43:18.248 - 00:43:24.764, Speaker F: Can you clarify a little bit more what RPQ is? Where does it. It's living inside of RD.
00:43:25.264 - 00:43:26.832, Speaker A: It's living inside RD.
00:43:26.968 - 00:43:48.184, Speaker F: So it consists of. I'm not sure how to think of this. It consists of those vectors PI that are in the affine span of Q. Together with those vectors QJ, they're in the affine span of P. And it's the span of such.
00:43:48.684 - 00:43:50.704, Speaker A: No, it's just the set of those points.
00:43:51.164 - 00:43:54.092, Speaker F: It's the set of points that have this property.
00:43:54.188 - 00:44:16.080, Speaker A: Okay, I thought you made a typo here. So that's what, that was my union of two sets. Yeah. Okay. Yeah. And just because I unfortunately forgot to put it, the affine span of this, the dimension of the affine span of this is going to be h. Okay.
00:44:16.080 - 00:44:21.924, Speaker A: And that's why this map is, this map t is just kind of normalizing things. It's making it a linear space.
00:44:23.464 - 00:44:38.624, Speaker C: Is this just saying that basically p sub I doesn't, like, live in some higher dimension than q sub j or the other way around for any framework, p sub I, q sub j in this set?
00:44:41.284 - 00:44:42.212, Speaker A: Sorry, say that again.
00:44:42.268 - 00:44:43.024, Speaker E: I didn't.
00:44:43.324 - 00:44:58.564, Speaker C: So this, I guess this kind of says that if you have a framework p sub I, q sub j, then p sub I isn't living in some higher affine space then q, sub j, because then one wouldn't be in the affine span of the other.
00:44:59.824 - 00:45:00.604, Speaker A: Yes.
00:45:01.104 - 00:45:01.936, Speaker C: Okay.
00:45:02.080 - 00:45:21.412, Speaker A: And they're the points that we care about when we want to do these affine dependencies. We want it to be that we want the points where we can actually do an affine dependence of the points in queue to get the point in P. These are the points we care about, and that's why we have this set of points.
00:45:21.588 - 00:45:27.492, Speaker E: Okay, can I ask a question about this? What is so, first of all, what is r?
00:45:27.668 - 00:45:42.524, Speaker A: So r is a typo. It was supposed to be because this is the notation more that Bolker and Roth used. And I forgot to change it when I went through the slides and changed some stuff. This is supposed to be the affine span of RPQ. So this is a typo which I'll fix before I.
00:45:42.564 - 00:45:49.464, Speaker E: So r is the affine span of RPQ. Okay. And you just want any non singular.
00:45:50.004 - 00:45:50.784, Speaker A: Yeah.
00:45:51.444 - 00:45:52.744, Speaker E: Find map.
00:45:56.284 - 00:45:59.596, Speaker A: It's just the kind of a technicality that fixes our.
00:45:59.700 - 00:46:02.372, Speaker E: Okay. Yeah. Does it need to fix it?
00:46:02.428 - 00:46:16.694, Speaker A: So, okay, we're going to show that the affine transformation is actually irrelevant what we do. As long as it's not flatting any points of RPQ into a line or something like this, like you say, it's non singular, then we're fine, it doesn't matter.
00:46:20.394 - 00:46:23.814, Speaker C: So we're taking a. Oh, sorry. Gone.
00:46:26.234 - 00:46:49.694, Speaker E: So all this is going to be ultimately, is if you look at the span of like PI, tensor PI in this RPQ, then you look at the orthogonal to that. That's what this Q. So that's what this Q will be.
00:46:50.914 - 00:46:52.722, Speaker A: Yeah, I think.
00:46:52.858 - 00:46:53.974, Speaker E: Okay, good.
00:46:55.074 - 00:47:39.314, Speaker A: Okay, so we're now looking at this space, which is the set of quadrant polynomials. We've got it with h here. If we take a point in RPQ and we transform it with t and then apply q, we get zero, because the quadratic polynomials, we want to define them over a linear space of dimension H. And this is why we've transposed our, transformed our points into R H. That's why we've done it. Okay. Things are obviously simpler if h is equal to d, which will happen if this is true for all the points and everything's affinely spanning and stuff like this.
00:47:39.314 - 00:48:00.426, Speaker A: But this is just for these kind of smaller sub dimensional special cases. That's why we're doing it very carefully like this. Okay. Now I had a few little results. Yes. H is the dimension of that. It would be the affine dimension to be.
00:48:00.426 - 00:48:14.418, Speaker A: Yeah. Okay. You put the. If I expand, that's fine. Okay. I had a few results here. We're probably going to have to skip over them for time reasons, but what you basically do is you prove a lot of little results like this.
00:48:14.418 - 00:49:09.894, Speaker A: Okay. Which I won't go into, involves building some matrix, showing that something is equal to something else and then getting this equation here. And this is basically how the two things are linked. We've now shown that the dimension of this is equal to the dimension of what I was talking about, plus this, minus this, minus this. So it's a bit of a bit of a long equation, but that's what these previous lambdas, you put them all together and you show this, and then with this equation we can sub it in and we get this very long equation here. Now what I've done here is basically with the previous result, this is true for any choice of t. So what we end up basically noting is that the transformation that we chose, as long as it's a nice suitable one that's non singular, doesn't matter what we picked.
00:49:09.894 - 00:49:26.194, Speaker A: So what we then tend to do is we define q R. This is old notation, qr to be the dimension of qr to just be the dimension of any of these qtRs, rpQs. They should be.
00:49:26.694 - 00:49:30.554, Speaker F: Okay, one slide for a second.
00:49:31.574 - 00:49:40.114, Speaker A: Okay, but I'm not going to be proving this stuff because we're running out of time. These slides will be online after the lecture as well.
00:49:43.594 - 00:49:48.334, Speaker B: Can you remind what q, sub t of RPq is?
00:49:48.754 - 00:50:09.194, Speaker A: Yep. So this is this space here. So it's any of the quadric polynomials that disappear on these points that vanish on these points after you apply the transformation to them. These are the ones we're interested in.
00:50:12.774 - 00:50:25.354, Speaker B: And this transformation t can be anything that doesn't collapse R. Yeah, anything, yeah. So just project onto the affine span of R. Yeah. Okay.
00:50:25.854 - 00:50:48.578, Speaker A: Yeah. So I had to skip a few results, which I'm to apologize about. Right. So we have this theorem. Now, this is Bolker and Roth's theorem. So any bipartite framework, like. So we have that the dimension of this is equal to the dimension of the dependencies on p.
00:50:48.578 - 00:51:52.032, Speaker A: The affine dependencies times the dimension of the affine dependencies on Q times by the quadratics that disappear on the points RPQ. Plus the size of this set, which pops up earlier, minus this thing here where H is going to be the dimension of the affine span of this. Okay, it's a bit of a long winded equation, but we're going to be using it as a tool to use. What you're going to notice is that if I chose, say if I choose p and Q, so that they're both affinely spanning, so they both span RD. When you take the affine span, this will just be equal to, if I get this correct. Okay, so if you take it so they're in general position. So you don't have like collinear points, you don't have four points on a plane and so on and so forth.
00:51:52.032 - 00:52:40.424, Speaker A: You don't have any extra affine dependencies that you don't need, this is going to be equal to M minus D minus one, and this will be equal to n minus d minus one. Okay, this will then just become all the points. This will just be the quadric polynomials that disappear on all the points and h will become d. Now I'm just going to skim over this, but you do that, you plug all that in. So this is what I've said here. If this is in general position with the number of points are at least d plus one, then this equation just simplifies down so much to this. Okay, you can maybe try and convince yourself, but, yeah, like I said, you just plug in the things, you get this equation.
00:52:40.424 - 00:53:48.080, Speaker A: Okay, look familiar? Well, if I cover this part, our condition for infinitesimal rigidity is literally, this is equal to these three things. Okay, so this is equal to this if and only if our bipartite framework is infinitesimally rigid. So we've just got this extra thing plugged on the end. Well, this is perfect, because now we know that a bipartite, bipartite framework is going to be infinitesimally rigid if and only if the dimension of the quadrant polynomials that disappear on its, well, points that are in affine dependence with the other ones. Well, because we're dealing with in general position and everything, this is going to be all the vertices, this is going to be everything. So everybody vertex. So we're then just saying that this is going to be infinitesimally rigid if and only if the dimension of the quadric polynomials disappear.
00:53:48.080 - 00:54:43.252, Speaker A: Sorry, the, there are no quadric polynomials that disappear on all the points. Okay, so this is kind of a bit of a weird result, but it's very nice result. So you can now just think about it with a bipartite framework, as I long as everything's in general position and you have enough vertices, then you're going to be infinitesimally rigid if and only if you don't, your points aren't vanished, aren't zeros of some quadric polynomial. And then we just, we are going to use this result for this result here. So bipartite graph is going to be generically rigid if and only if this is true. So you need both conditions. So this one just by itself will give you the Maxwell count.
00:54:43.252 - 00:55:29.092, Speaker A: It will give you the correct Maxwell count, but we also need this condition here for generic rigidity. Basically the idea boils down to if one of these is less than or equal to duty, then we can just remove that vertex and get to a smaller graph so we know that this has to be true. That's quite easy to check for this thing. This boils down to quite a nice result, which is that if you have d plus two, choose two points minus one. And we're assuming they're in some sort of general position, they will define a unique quadric hypersurface. Okay. So in 2d, this will be four points.
00:55:29.092 - 00:55:49.144, Speaker A: Define a conic, for instance. Okay. With four points to find a conic. And I have a choice of where my fifth point goes because I'm dealing with, like I'm doing with generic rigidity. I can always just pick this so it doesn't lie on whatever conic these four have described.
00:55:49.744 - 00:55:51.444, Speaker E: Well, then numbers are wrong.
00:55:52.184 - 00:55:53.124, Speaker F: Five points.
00:55:54.024 - 00:55:55.504, Speaker E: Five points. Make a comic.
00:55:55.624 - 00:56:14.844, Speaker A: Oh, yeah, you're right. So five points. And another point. So it's four points for a circle, isn't it? So five points. Do apologize. Yeah. So, yeah, that basically, it's as simple as that.
00:56:14.844 - 00:56:40.094, Speaker A: And that gives us the generic rigidity result we want, the if and only if. Okay. It also shows that this is going to be best possible. We can't get anything smaller. If there's anything smaller, I could then have a quadrant polynomial that vanishes on all those points. So it's not possible. Tripartite.
00:56:40.094 - 00:57:17.944, Speaker A: I was going to actually ask Walter about this stuff, but we'll get to that. Okay. Because we're strapped for time. I'm just going to skim over these last two things. But another use of this, because I've just said a generic reducing result. Another use for this is to specify exactly when some bipartite graphs are infinitesimally rigid. So if we take the k three phi in the plane, that's going to be infinitesimally rigid if and only if the points of p and q don't lie on a conic.
00:57:17.944 - 00:57:45.254, Speaker A: Okay. Which is quite a famous result. This is the one that you probably have heard of every so often. I'm going to skim the proof. But basically, if you notice the case, when this is equal to both of the dependencies are equal to zero, that means that the points that each of the three points are not, are not collinear. So the p points aren't collinear. The cue points aren't collinear.
00:57:45.254 - 00:58:06.550, Speaker A: If you have this case, then you already know that this is equal to six. This is equal to two. This boils down to this being equal to this. So that means exactly what we just said previously. This framework is only infinitesimally rigid if and only if the points don't lie on a conic. And that's it. It's as simple as that.
00:58:06.550 - 00:58:34.824, Speaker A: We just use the equation, we get this result out. There's a few of the cases to check. I'm going to skip this because of time, but basically you just check the other cases. So this case and this case and this case and so on and so forth. But it's the same idea. What to take away from this, is that what I did for case one is not unique to k 33. There's a large class of graphs you can use it for.
00:58:34.824 - 00:59:43.154, Speaker A: So if I was to take a bipartite graph, which was KD plus one, and the KD plus 1d plus one, choose two. If I take that graph and I take some bipartite framework of it, so long as the point p and q are finally spanning, then it's going to be infinitesimally rigid if and only if the points don't lie on a quadricep hypersurface. Now this is noticeably weaker because basically the while case one will work, case two, case three and case four won't hold or one of them won't hold. I can't remember, but some of them won't hold. Okay, but we can sort of extend this result to higher dimensions. And because my time is up, I'm going to have to skim the counterexample. But there is a counterexample for D equals three, which shows that you can build a bipartite framework of k 46, which is infinitesimally flexible.
00:59:43.154 - 00:59:58.974, Speaker A: But the points don't lie in a quadric surface. And this is because you force the points p to lie in some subsurface, a subspace, and the same for some points at Q. But I don't really have, unfortunately, time to discuss.
01:00:00.314 - 01:00:18.440, Speaker E: I mean, Q is big. This is really straightforward. Right, so Q is big, so it has affine dependencies. P is small, so it shouldn't have any. But if you stick them in a plane, it gets one.
01:00:18.592 - 01:00:18.944, Speaker A: Yeah.
01:00:18.984 - 01:00:21.124, Speaker E: And then you get your zero diagonal stress.
01:00:21.424 - 01:00:22.208, Speaker A: Yeah.
01:00:22.376 - 01:00:26.244, Speaker E: But Q can be whatever, it's big enough to make it not lie on a quadric surface.
01:00:26.624 - 01:01:04.584, Speaker A: Basically, yeah. So what we do is we basically, we so quickly go over it, then we build it so that this set RPQ is all the P's, but it's also got Q one and Q four. And how I've picked it is that this set doesn't have any conics going through it in two reals. And then that means there's no quadric polynomial of three variables that can pass through it, because then when you flatten it down into two reals, it would need to be a conic. So that's it. Basically, when you slice it at third plane, actually not flatten it. It's not projection.
01:01:04.584 - 01:01:41.424, Speaker A: That's the general gist, and you can do it, but this result will still hold for as long as you assume some level of affine spanning. So that's fine. It's not perfect, but it's good. And I think that is all I want to talk about. I've just left two references here, so this one is obviously the very important reference, because the whole lecture is based on this. And I've also got this reference just for what I mentioned at the start. So I mentioned about how we were dealing with infinitesimal rigidity.
01:01:41.424 - 01:02:28.364, Speaker A: Rigidity is a lot more difficult, a lot, lot more difficult. And this paper here basically proves that. So while we characterize where K 33 is infinitesimally rigid or infinitesimally flexible, this here will prove when it's rigid and when it's flexible, but it only works for K 33. You have to do these things framework by framework cases. You can say some general things about it, but you can't say exactly what's going on by using infinitesimal flexibility, because some things can be infinitesimally flexible, but not flexible. But it's a nice reference to have a look at. But yeah, that's why I put it up here.
01:02:28.364 - 01:02:50.112, Speaker A: And I think that is all I want to talk about. So I'll very quickly take questions, I guess, because we're already running over, which I do apologize for. Thanks, Sean. I think there was a couple of questions from Alex and William in the chat, but feel free to ask questions.
01:02:50.208 - 01:03:09.400, Speaker B: Oh, sure. I was just wondering about if there's any graphs where infinitesimal rigidity is characterized by cubic or quartic or higher. So, because it kind of reminds me of higher order rigidity, but higher order infinitesimal rigidity might be more tractable.
01:03:09.592 - 01:03:57.450, Speaker A: So I was wondering about this myself, actually. So my, my thought for something to first look at will be the, as I think Will said, was it? Or someone said, the tripartite graphs will be my first port of call if I was to do this, just as the complete tripartite graphs would be the first portal call, just from a completely naive understanding that with a complete graph, you just need to look at the linear kind of the affine subspaces that cause problems. With complete bipartite graphs, you have to look at the kind of like the quadratic things that can go wrong. Tripartite, maybe cubic, I don't know. But that will be my kind of line of thinking.
01:03:57.642 - 01:04:16.574, Speaker D: I mean, I think the connection goes even deeper than that, right? Because you're these tensor products you're taking of, you know, things in like, two different sets. There are two different classes. Like, you can just do like three classes, and then now, you know, having your linear condition on van, like I.
01:04:17.094 - 01:04:22.678, Speaker A: But you can see why it gets hard, because everything, like a lot of.
01:04:22.686 - 01:04:55.346, Speaker D: The basic stuff, seems to just generalize in a very straightforward way, right? Like your linear dependency, or like your linear, you know, you have like, a linear space spanned by you these, like, rank one tensors in the two dimensional case, you know, corresponding to some set of quadratics. I mean, that generalizes in a very straightforward way where now, instead of, you know, rank one matrices spanning some space, you have, like, rank one tensor spanning some space. I mean, things will probably get harder, but like.
01:04:55.450 - 01:05:06.666, Speaker B: But rank one tensors are not so much harder. It's where you get to higher rank tensors, where things get bad. So I think you're right that this is maybe even promising, but I don't.
01:05:06.690 - 01:05:10.534, Speaker E: Think you should be thinking about this as tensors, by the way.
01:05:11.674 - 01:05:18.138, Speaker B: Well, a cubic polynomial is equivalent to a symmetric order three.
01:05:18.306 - 01:05:22.054, Speaker E: You should really be thinking about these as like, sort of Vernese images.
01:05:23.994 - 01:05:25.490, Speaker B: Wait, can you say that one more time?
01:05:25.602 - 01:05:51.824, Speaker E: You should really be thinking of this as like, as some kind of, like Vernese embedding, right? So you're taking your space where the points live, and you're looking at kind of the images under a vernis map, and then that turns the polynomials of the right degree into linear forms on those things.
01:05:54.524 - 01:06:00.812, Speaker D: You're looking at your span of points on a Veronese, which I guess is.
01:06:00.908 - 01:06:02.076, Speaker A: I mean, yeah, I guess it's just.
01:06:02.100 - 01:06:05.816, Speaker D: A matter of, like, how you like thinking about it. But that just, I mean, that just is a tensor, right?
01:06:05.880 - 01:06:06.176, Speaker A: Like.
01:06:06.240 - 01:06:19.044, Speaker E: Yeah, I mean, but this is. I guess that, like, you should. Yeah, I would not think about it, per se, as like, a tensor problem. It's really. So. So essentially what they're showing is, like, how much general position is required.
01:06:19.624 - 01:06:20.364, Speaker A: Yeah.
01:06:20.824 - 01:06:29.004, Speaker E: To avoid. Right. So, so, like, what Sean is saying, and I like this example. So, complete graphs are infinitesimally rigid as long as they're not completely flat.
01:06:29.844 - 01:06:31.100, Speaker A: Yeah. Right.
01:06:31.132 - 01:06:57.524, Speaker E: This is easy to check, right? As soon as you have a simplex, then every other point is like some kind of Henneberg one over the simplex, and it's infinitesimally. Rigid. Right. No matter how bad. So for complete graphs, like just affinely spanning is good enough. Right? So for these bipartites, somehow, like some kind of quadratically general position is good enough.
01:06:58.944 - 01:07:00.184, Speaker A: Yeah, exactly.
01:07:00.224 - 01:07:32.004, Speaker E: Right. So to answer like, everybody's question about what is going on with this funny RPQ, it's just a way of defining how many conics you expect to vanish on this point set, roughly speaking. So it's representing a defect, like when do you have more than you wanted kind of thing. Right. And so if there's like very affinely degenerate things, then that number can be different than like the sort of obvious count by dimension.
01:07:33.704 - 01:07:34.080, Speaker A: Yeah.
01:07:34.112 - 01:08:02.344, Speaker E: And so it is their number, I guess. Right? And so then you could ask like, well, what degree level of general position is required for some given framework to achieve its kind of typical rigidity matrix rank. I got no idea.
01:08:02.724 - 01:08:20.234, Speaker F: Yeah, it's not clear that why it should just be a full space of cubics or whatever degree polynomial you're looking at detects the non rigidity.
01:08:22.014 - 01:08:24.078, Speaker E: Well, I mean, I guess, I don't know.
01:08:24.126 - 01:08:24.310, Speaker A: Right?
01:08:24.342 - 01:09:17.313, Speaker E: I mean, it's definitely not. So instead, sort of go with the reverse. Right? So suppose the graph is typically infinitesimally rigid, right? Just to make our life simple, there's some set of stressable frameworks, right? Which is generated, you know, which has components that are generated in some degree, I guess. And so is it then. So now you guys tell me, is it then true that like for some sufficiently high degree, if you're in general position with respect to that degree, you're going to be okay? So my question is, is that even true?
01:09:23.533 - 01:10:00.756, Speaker A: I don't know. I would have to look at it. A lot of this for me is just a lot of, it's a lot of equations and stuff to kind of get my head around. But I think going back to the original point, I think, yeah, you could, these methods could be extended to tripartites, full partites, I don't know what they call properly quad partites, I guess, and so on and so forth. But the methods get messier and messier every time and you maybe can't think of it as nice matrices anymore. Yeah, yeah.
01:10:00.780 - 01:10:16.748, Speaker D: And I guess you could also wind up in a situation where like, you know, there might be like, it might not be as clean as, like they can't lie on a cubic surface. It might be like they can't lie on a cubic surface with a defining equation. Who's like wearing rank?
01:10:16.796 - 01:10:17.980, Speaker F: Is it singular?
01:10:18.132 - 01:10:24.596, Speaker A: Singular cubic, something like that, yes, it might not be as nice as that.
01:10:24.740 - 01:10:43.988, Speaker D: But I think it's, like, a very straightforward sort of, like, thing to try here. Just by, like, you know, tripartite graphs, and then instead of your tensor space of two things, you have a tensor space of three things, and then just like. Like, at some point, something might break down, and seeing where that is would, like. Would lead you to the conjecture to make. Right, I think.
01:10:44.036 - 01:10:55.212, Speaker A: I hope. I mean, I don't know. Yeah, there's a lot of. There's a lot of shortcuts you can take. If you just want to get a generic rigidity result. There's a lot short to go cuts you can take, which will simplify things.
01:10:55.348 - 01:10:57.460, Speaker D: I mean, that would be cool. And, I mean, I don't know.
01:10:57.492 - 01:10:57.740, Speaker A: I mean.
01:10:57.772 - 01:11:08.796, Speaker D: Okay, I'm sort of at the stage where everything seems much simpler than it probably is, but it seems like you could. Yeah, it seems like there's a very clear sort of, like, path here.
01:11:08.940 - 01:11:32.244, Speaker A: Is it actually known any rigidity results for tripartite? And further, anyone knows that? So I've not heard of any off the top of my head. No. I don't know either. So I was just going to say that. I mean, I agree with Daniel that this would be a really nice thing to investigate, but my guess is Boelker and Roth themselves had the idea of, we're going to tripartite. And so something goes hard, quickly, I bet.
01:11:33.264 - 01:11:57.854, Speaker D: I will say, since Boca and Roth wrote this paper, that there have been a lot of people studying, wearing ranks of polynomials, which, you know, which is just like, the minimum. Basically, if you think of a polynomial as a symmetric tensor, it's the minimum number of rank one symmetric tensors that you need to express it as a sum. I don't know that people were thinking about this in Bocker and Roth's day. So, like, maybe.
01:11:58.594 - 01:12:00.082, Speaker A: Yeah. I mean, yeah, I don't know.
01:12:00.098 - 01:12:00.370, Speaker D: Who knows?
01:12:00.402 - 01:12:00.610, Speaker A: Maybe.
01:12:00.642 - 01:12:03.254, Speaker D: Maybe some of this machinery that's come out since then could.
01:12:04.074 - 01:12:06.002, Speaker A: Yeah, yeah.
01:12:06.098 - 01:12:08.074, Speaker D: If this is even an interesting question.
01:12:08.154 - 01:12:29.444, Speaker A: But, I mean, a lot of their paper is just crunching algebra. It's not more complicated than that. So a lot of the proofs I skipped over are exactly the same thing. You're just checking stuff. Stuff is equal to other stuff, dimension wise. And you just do that for the whole paper. Basically.
01:12:29.444 - 01:12:59.272, Speaker A: The original idea is very clever, but once you know where you're going, it's then easy to replicate in some way. Well, not easy, but it's replicatable. You know, you could theoretically give this to an undergrad who knows linear algebra, and they could theoretically understand it. It's a lot to go through, but, yeah, theoretically possible. Yeah. I mean. Yeah.
01:12:59.272 - 01:13:10.338, Speaker A: So this is the thing, when you're saying about tripartite, then you end up with, like, three tenses, and then you can't think of it as matrices, really. Not very nicely, anyway. So they become nightmarish.
01:13:10.466 - 01:13:11.214, Speaker D: Yeah.
01:13:12.834 - 01:13:16.250, Speaker F: Think of them as three tensors. I mean, major two tensors.
01:13:16.282 - 01:13:16.554, Speaker B: Right. So.
