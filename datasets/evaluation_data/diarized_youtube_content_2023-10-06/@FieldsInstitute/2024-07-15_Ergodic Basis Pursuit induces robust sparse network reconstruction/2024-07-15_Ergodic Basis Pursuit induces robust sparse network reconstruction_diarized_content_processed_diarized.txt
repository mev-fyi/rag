00:00:00.280 - 00:00:26.740, Speaker A: Thank you very much, the organizers, particularly Bhumi Dien. I've been trying to present or to participate in this conference for a long time. Actually in the first year, 2019, I was in my first year of PhD, so I didn't have anything to present. So I only was there absorbing in Imperial. And then Covid came. I didn't have anything then 2022, I believe, was the last session. My visa didn't get through.
00:00:26.740 - 00:00:53.550, Speaker A: So when we've been there and said, okay, we'll be in Toronto again. And then I got a position with Eric in Clarkson. So I'm now in Clarkson, which is Potsdam, which is basically five to 6 hours away from here. And then I said, even walking, I'm going then. So probably I was the first one to register to the conference. So it's a huge pleasure to be here. So my talk will be slightly different from what Matthew presented.
00:00:53.550 - 00:01:27.864, Speaker A: I'm interested in reconstruction of network dynamics, so particularly networks that can be represented by a triple. So will be slightly different from what we heard yesterday, which reasonably almost will be slightly different. So I would denote by small f. When these nodes in this graph are isolated to each other, but they are coupled via some coupling function. Hdd. Okay, on top of a graph structure, g. So the dynamics itself will look like this.
00:01:27.864 - 00:02:03.596, Speaker A: So if alpha, which will be decoupling strength is zero, they will interact over by iterations of the map f, but they are coupled on top of this graph, which encoded by these adjacency matrix, which will tell you who is connected to whom. Okay, so you can pick your favorite network dynamics. And I will be interested in knowing the dynamics of this. Okay, so a prior audience don't have any assumptions on this triple. The point is this. F, g and h can be unknown. So we just have time series.
00:02:03.596 - 00:02:36.292, Speaker A: So a priority. This is unknown. You receive timing series, which I will be noted by x, t and the length of the time series. So the number of sample, the number of samples will be this small n. You receive this and the objective will be to recall f, g and h, for the obvious reasons. If I'm able to obtain gfg and h, I will have the network dynamics, then I can control and can predict fine. So this is a hard problem initially.
00:02:36.292 - 00:03:25.614, Speaker A: So in particular, I will be interested, by the way I pose the problem. I'm interested to obtain the trajectory of every single node in my network. So this will be a particularly my main assumption over the entire presentation. I should observe every single node in my network. This problem has been studied. I could come with some heuristic saying, oh, if I measure long time enough, I will be able to reconstruct it to do something. However, I'm mostly interested for large dynamical systems, large networks, so experimentally you may not have access to acquire the precision time, long time measurements necessary to use any kind of low dimensional techniques that will be available.
00:03:25.614 - 00:04:06.150, Speaker A: So the challenge is precisely the following. I have a graph g, so I will simplify even the problem. I will forget about f, I will forget about h. I just, I'm interested to obtain g, which is large, but I have limited amount of time series, so I don't have enough measurements to use canonical low dimensional techniques. So that's the problem. The strategy that I will use, that's why was in my title, in my title of the presentation is sparsity. So my strategy will be to restrict the class of natural dynamics that I can make this problem well posed instead of imposed.
00:04:06.150 - 00:04:45.178, Speaker A: Okay, so that's where it comes this sparsity into play. The first assumption I will be in the graph. So most of the time sparsity on graphs is defined for sequences of graphs. Here I will give more loosely a kind of definition. So for me, network will be sparse if the number of edges is much less than the number of the nodes in your system squared. So a typical node in your network will not be connected to every other nodes, only a few neighbors. The other samples are now about f and h.
00:04:45.178 - 00:05:30.824, Speaker A: They should be in the span of a given non basis functions. So just to write it down, the iterations of a given node will be given by linear combination of these basis functions that I know someone should give me this in order to solve the problem. And it boils down that this partial of the graph is encoded in DC. So these c's will be sparse, meaning many of the entries of DC's will be zero because the network is sparse. So just to illustrate, my favorite example is the following. Imagine I have my network dynamics, and then for f I will take the logistic map, okay, polynomial, polynomial map. And the coupling is also polynomial.
00:05:30.824 - 00:06:18.498, Speaker A: They should be spanned by the same kind of basis, so will be x squared and on top of that I have in this graph. So for instance, the node two which highlighted the dynamics will be this, this one. So if I choose the right base function will be polynomial. So I should include all terms which are necessary, one, the linear terms and the quadratic terms. I can rewrite the dynamics of the node two as follows. I could think of these base functions written as such, multiplied by some coefficients which I can stack the timing series that I have in the following sense. The next time iterate, times this matrix, which evaluates the polynomials along the trajectories, times the coefficients.
00:06:18.498 - 00:07:02.326, Speaker A: So there, the coefficients are many zeros, which is one of the things that I mentioned. I can repeat the same kind of exercise to every single node in my network. So I identify that this coefficient indeed encodes what is necessary to understand about node dynamics or the network dynamics. Particularly just to call your attention, if I take node one which is coupled to two and three, I can see that the non zero terms is precisely one which is coupled with two and three and so on and so forth. So, having DC, I encode the network structure. So this boils down to what kind of reconstruction problem I will be recasting then. So here's the general form.
00:07:02.326 - 00:07:55.242, Speaker A: I need to take the node dynamics node time series in the left. In the next time iterate times some big matrix which evaluates the base functions along with the time series time c. So, I have a linear system now which the left hand side I have the matrix I have, and the only unknown is DC. Okay, this is a linear equation that I want to solve. If I had the situation where I have many times samples available and a small number of coefficients to fit, I could use least square methods. And in particular, I even have conditions on the library on this matrix that I for now want, I will call a library matrix to have a unique solution. So if you have a full column rank, I will have a unique solution, then I will be happy.
00:07:55.242 - 00:08:32.762, Speaker A: But my situation is not this one. I'm interested in the opposite case where I have limited, limited number of time measurements. So this n is much smaller than m. This m is the number of base functions which scales with the number of nodes in your system. So you have a situation different. Actually, if you try to use least square to a problem like this, you don't have a unique solution. So we can show that it highly depend on the number of nodes that you probe.
00:08:32.762 - 00:09:12.870, Speaker A: So you don't have a consistent kind of approach to do it. However, no, not however. So the point is you have this small time measurement, a huge fat matrix, which is undetermined. So the interest is to use the sparsity of the solution that you're searching to solve. This has been in the literature for a long time. In the late nineties, early two thousands, the solution came to choose another kind of minimization problem, which is this one, which is called basis pursuit. You will minimize l, one subject to a given constraint.
00:09:12.870 - 00:09:57.908, Speaker A: Okay, so just to give an intuition. If someone hasn't seen this, I'm searching for a sparse solution which should lie in one of the x's. That's my c here should lie in one of the x's, because the solution has many zeros and you have a given constraints, which is given by this. So you have many possible vectors that satisfy these constraints, and you are minimizing l one, which is this l one ball, that the intersection should give you the right solution. So geometrically or intuitively, you could see that why this problem gives this part solution. So, since it's been a while in the community, there are several packages that solve this problem. So I did the same.
00:09:57.908 - 00:10:43.782, Speaker A: I would take this package and then I run to solve this problem. In particular, I would take for each node in my network, solve this problem, obtain ic, and see if this matches with the original network. Okay, so in the particular example that I do, I will take my favorite map, logistic map in a large network dynamic, so n equals 40 and they are coupled by x times y. I will take polynomial bases up to degree three, and that's what we obtain. So I will consider a ring, quite simple network, okay, ring of 40 nodes, and that's what I measure here. In the horizontal axis is the number of measurements n, so the length of the time series. And in the vertical axis I have the false positive proportion.
00:10:43.782 - 00:11:37.610, Speaker A: So I'm not showing the false negative because everything is zero. So you don't have false positive, false negatives, but false positives you can see you have one, one, one until it boils down to 40. So in particular, in this region where it should satisfy the conditions of the problem, which is a fat matrix, as I mentioned, undertamed the problem should give you right solution. But since you don't have enough times measurement, you still have false positive equals to one. So you need to wait or have large and larger time series until you satisfy to have false positives equal to zero. Then it comes the question about what is the critical length of a time series such that you have an exact reconstruction, you can obtain precisely what is the original network. This.
00:11:37.610 - 00:12:23.762, Speaker A: Okay, now it's better. This is precisely what I'm plotting, the right hand side here. So I denoted by n zero, the minimal length of a time series, such that the false positive and false negatives are zero. So this is in the vertical axis. In the horizontal axis, I take the large, how large or how big is my network? And as you can see, this grows linearly. So in boils down that although the problem was telling me, oh, you will be able to solve this problem. For in the limit of small data set, small data set or small time measurements, we are needing something that scales with n.
00:12:23.762 - 00:13:12.030, Speaker A: So for large network you will require many data points. So the natural question, at least in that time for us, was the following. We want to determine the minimal length of time series for an exact reconstruction. It boils down that I cannot prove for basis pursuit originally, but we can change a little bit the problem. And that comes our result that we called ergodic basis purchutes. This is a joint work with my collaborator Ciago Pereira, which is now in so Paulo in Brazil, and Sebastian van String, which is in Imperial College, that we called a ergodic based pursuit that outperforms basis pursued the original one require much less data. And I can give you some theoretical bound for this n zero, which was something that we got quite interested.
00:13:12.030 - 00:14:02.550, Speaker A: So let me just describe how it works in practice so people can oh, this is nice. It's valid to prove this. So the idea is the original. The key idea is the following, the polynomial basis that the person tells me that the network dynamic should be represented for not necessarily is a good basis. So the strategy is instead of using l the original, so the polynomial ones, I will need to change this basis to have a better way to solve the problem. And this basis constructed as follows. I will have my data out of this data that can be noisy with some level of noisy, I will estimate some probability distribution.
00:14:02.550 - 00:14:59.196, Speaker A: Out of this probability distribution, I will orthonormalize the original polynomials to make orthonormal Bayes functions. And with the orthonormal Bayes functions, I will replace the original basis pursuit by this new one, where this new, this lump, this phi subscript new will be this library matrix evaluated along the orthonormal based functions. And that's how it works in practice. So in the purple is precisely line that I showed you before, but in green you can see that drastically reduce the number of data measurements necessary to reconstruct. And in particular, if I repeat the plot of n zero with respect to n, you can see that the growth is much lower than the linear case. In part, even more. I can give just a hint.
00:14:59.196 - 00:15:56.888, Speaker A: We can give some expression how this n zero behaves. N behaves with the degrees of the number of connections. This node has squared times logarithm of the network size, and r is the maximum degree of the polynomial we required. Okay, so my intention for the next ten minutes or so is to explain why this happens and give some idea how to prove it. So I believe here's a good time to take questions. So I use the second option. Yes, I use the second option because one thing that I need to balance, I'm coming here and say I have small data samples and I want to estimate some probability distribution.
00:15:56.888 - 00:16:30.070, Speaker A: So I use particular district, I imagine that every, all nodes are basically the same dynamics and I, to improve the accuracy and roughly it works. If the alpha decoupling is small, it works well. And, but I don't have a really good proof that for this. Anything else? Yes, but it's grunchy meat. Just a heads up. It's grunchy meat. But I will.
00:16:30.070 - 00:17:13.312, Speaker A: Yes, why anything else? Okay, so I want to explain then how to work to do it. So instead of taking each node individually, let's take the problem as more general, more general enough. I will assume that I have, I want to solve this linear equation where left hand side and the matrix I have and the only unknown is circumental. DC will call the node dynamics. So, and I know that it's sparse, so I'll give a level of sparsity, I will call it. This is s, sorry, s sparse. So at most s number of entries are nonzero, the rest are zero.
00:17:13.312 - 00:17:53.392, Speaker A: Okay, that's it. So that's not my result. This is a Terence tau and condescend kind of result, which I'm stating here informally. And they have a nice criteria on this library. Now, the library matrix, such that if you take two s columns of this library matrix and you can say that they are nearly orthonormal to each other, we can prove that this basis pursuit has unique s par solution, which is precisely what I needed. Okay, this is called restricted isometric property, or for short rib condition. So just to be a little bit more precise, newly autonomous will mean this following quantity.
00:17:53.392 - 00:18:33.998, Speaker A: You have two s, you want to know two s columns. So you have this delta two s, you want to take and construct your matrix submatrix encoded by this s. So inner products, you're taking all inner products you subtract the identity. And this should be at most square root of two minus one. So you have aim to do it. You have a where to look at. As I mentioned, the original base functions not necessarily satisfied this condition show the idea is to change the basis such that I can satisfy the conditions that this theorem give.
00:18:33.998 - 00:19:00.824, Speaker A: So I need to massage the problem. How I massage the problem is using the statistical properties of the original natural dynamics. So up to now I just had one assumption. Now comes other two assumptions, such that I can fit my problem to this result. Okay, so that's the strategy, since I'm talking about nearly orthonormal. So I'm calculating inner product. Let's see how this looks like.
00:19:00.824 - 00:20:03.932, Speaker A: So, this is the original basis, the library matrix. If I divide by one square root of n, one over square root of n, the inner product, or any distinct two columns, looks precise, like this. So it's one over n over this, along the trajectory, which I can encode as the sum n, looking a lot as a Birkov average along that given a given trajectory. Right. So, and here comes the idea. Instead of then calculating the inner product on your client space, I will go to l two over a given measure mu. So, for instance, if the network dynamics was ergodic, that comes where come the idea to call ergodic basis pursuit, I could use Birchov theorem and ensure that, oh, this inner product should be like an average.
00:20:03.932 - 00:21:04.634, Speaker A: Okay, so if these base functions are orthonormal with respect to the measure, I will be happy. So how I autonomize, how I get this new is grant, she meets over this probability distribution. The tricky part is that ergodicity doesn't give any notion of speed of convergence. So you need to have additional property to ensure this. So, for this particular case, we require explanation kind of dynamics, because then we can have exponential decay of correlation and a new result, and another result, which is quite useful, which I will give informal statement goes as follows. They have a Bernstein type of inequality that help us to obtain this m zero, this minimal length of time series. So, what is telling you is under reasonable assumption of the observable that you have in hand, which always satisfies.
00:21:04.634 - 00:22:26.502, Speaker A: We are dealing with polynomials, so we are fine. You have a bunch of initial conditions that might violate some, violate the birch of average or the inner product that you have, but this decay exponentially fast. So, with this theta, we can use to estimate this minimal length of time series. So, when I say that my statements holding high probability is because there are some typical, there are some orbits or there are some initial conditions that I will need to throw out. So when I say high probabilities, because if you give me data, I will say, oh, typically my results will hold, okay, this is the first one. The second one, which we had somehow a problem to prove, is the fact that for a general distribution, for a general probability measure, if anyone has worked with Granschi meat, granschemit is recursive. So you start with this polynomial, nice polynomial basis, and you start doing grant she meets in this general probability distribution, you start creating dependencies among the basis functions, and you lose track about some network structure.
00:22:26.502 - 00:23:12.846, Speaker A: And more importantly, you lose the fact that you had a sparse representation, which is quite crucial for me to use, because I'm trying to search and solve a problem, searching for a sparse solution. If you lose sparsity, you cannot do anything. So the trick and that we. Oops, sorry. The trick that we use is that actually for product measures, grassy meat satisfies what we needed. So the kind of proposition that we have is imagine that you have a product measure, rho one, rho two times rho n, rho one up to rho n. They need to satisfy slightly nice conditions to work.
00:23:12.846 - 00:24:19.910, Speaker A: But the result is, if you have this measure, the grass image on this measure satisfies represents a network. And more importantly, if you take a s parts representation on the previous basis, in this new basis, you have omega R as sparsity. So this omega R is the maximal degree of the polynomial that expands. I don't want to explain, give more details about it, but I'm happy to discuss, which is I still need to play some role to have some additional assumption, because this new does have, has nothing to do with the original dynamics. So I should have a way to connect these new with the original dynamics in order to use the exponential decay of correlation that I stated before. So, just to wrap things up, so I need assumption one, sparsity assumption two, exponential decay of correlations, and three, which I'm not showing. That's why it's three star.
00:24:19.910 - 00:25:44.930, Speaker A: The strategy is we will construct a set of orthonormal bases along this product measure nu. So for sufficient large network size in large initial conditions, I can give an estimate of how this n zero behaves, behave with some constant that it's okay. Omega R is this representation, these new Bayes functions that depends on r, the maximum degree of polynomial, the maximum degree of polynomial that we expanded squared. That's why I claim that it's related to the degree of the node times logarithm of the network size times r. And if n zero satisfies this, I can guarantee that this library matrix satisfies the condition that I was searching for, the square root two minus one. And since I could satisfy this condition, I can boil, I can put in the conveyor's a theorem and say that the associated minimization problem, now replacing the original Bayes function by this new one, has unique sparse solution. Okay, is there any comment or any questions? Okay, so just to summarize and finish, we can determine this n zero for exact reconstruction.
00:25:44.930 - 00:26:05.850, Speaker A: In particular, I show the expression, but there are some things to be discussed. L should be given. So someone should tell me, oh, that's the representation. That's the good basis for the time series that you're receiving, which is quite limiting. Okay. Estimating probability distribution. As Fein Lu mentioned, I should take care of this.
00:26:05.850 - 00:26:40.348, Speaker A: So I use some tricks in the practice and there are some additional costs to autonomize the base functions. Okay. Yes. So indeed, I said exact and also robust. So it's robust because we could also do it and apply the theory for noise measurement. And actually we could apply for experimental data. So actually we had some couple friends that have coupled lasers and we try to use this technique to descopel lasers.
00:26:40.348 - 00:26:56.140, Speaker A: So we needed to also deal with the noise measurements with this. We had this public, this prep print is an archive since last year and we are under revision in a journal, so thank you very much.
