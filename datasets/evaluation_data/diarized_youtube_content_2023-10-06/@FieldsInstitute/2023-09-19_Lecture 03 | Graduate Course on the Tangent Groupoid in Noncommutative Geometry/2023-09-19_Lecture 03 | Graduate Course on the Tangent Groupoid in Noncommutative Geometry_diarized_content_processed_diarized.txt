00:00:00.320 - 00:00:52.744, Speaker A: Okay, shall we begin? Yes. So I have a, if I was a little dishonest, I mean, I am dishonest, but if I was a little more dishonest than I really am, then I would call it, I don't know, adjustment. But it's actually a correction that I need to make because there was a misconception in my head. And so I will make that correction first, and then I'll write down a formula with all sorts of indices, and then we'll use the formula all day. And then if we're lucky, that'll be it. With all of the messy formulas and indices, we won't need them on Thursday. All right, here's what I should have said.
00:00:52.744 - 00:02:29.504, Speaker A: Said, let me just think about the, let me do it over here about smooth families, which I did talk about, but now I want to talk about smoothing operators. So suppose I have, we were looking at this sort of thing before, the collection of operators parameterized by the points in some open subset of RN or manifold, if you like. But now I want to assume that these follows. Each of them is a smoothing operator. So it's given by a smooth integral kernel, which I could write this way. We defined last time what it means for these kus collectively to be a smooth family of operators. It means that they somehow glue together to give an operator defined on the functions on rn times u into the functions on rn times u, the smooth functions in both cases.
00:02:29.504 - 00:03:27.164, Speaker A: But the question might occur to you, as it did to me last night, whether or not the, if these operators are smoothing operators, whether or not the condition of smoothness implies that this function, k sub u of x y is smooth in x and y and u and. Yeah, of course it does. I said to myself, well, it doesn't. So on top of assuming, when I say a smooth family of smoothing operators, on top of assuming the condition that we discussed last time, which is that the operators glued together, as I just said, into a single operator from test functions on rn times u to test functions on rn times u. Apart from assuming that, we shall also assume that this guy here is a c infinity function of all three variables all at once, u, x and y.
00:03:30.614 - 00:03:36.714, Speaker B: Continuity, whatever. Point wise.
00:03:37.174 - 00:04:09.294, Speaker A: Yeah, I mean, there's some kind of smoothness condition built in because not just any collection of k sub use will glue together to give a map from test functions on u times rn to test functions on u times rn. But this is a stronger condition. So rewind the tape to Thursday, insert this and then forward again. And now we're okay. Okay, good. That's the correction. So when I say, as I'm now going to say, with a nice long, necessary.
00:04:09.294 - 00:04:33.694, Speaker A: Yes, it's necessary to assume this. There's a homework exercise on the current lecture, three notes which I discovered, to my chagrin last night. I thought I spent, I spent an hour trying to show that this thing was smooth in you. And then I said, well, damn it, I'm just going to find a counterexample. It can't be any harder than this. And sure enough. So there you go.
00:04:33.694 - 00:05:32.862, Speaker A: All right, good. Last time we were discussing. Maybe I'll write it here. Smoothing families. So here's the so far we're imagining being given a smooth family of operators, one for each t which is not zero. We're imagining building a whole bunch of other operators by rescaling these operators. We're imagining assuming that this family of operators extends to a smooth family of operators, just in the ordinary sense we're talking about last time.
00:05:32.862 - 00:06:17.474, Speaker A: But we were also insisting on this scaling relation, which I'm not going to write down. I'm able to. Well, this is supposed to be lambda to the minus. Well, there's some integer floating in this story, which is the order. So this is supposed to be a scaling family water mix for us. M will be an integer that will be enough. And what you're supposed to get back is just a omega of t, omega of lambda t like this.
00:06:17.474 - 00:07:28.744, Speaker A: But then there's an error term which I called k, and it depends on omega and it depends on lambda and it depends on t like that. And this is the thing which these are all smoothing operators, and these are the things which have to be smooth in this sense up here. Okay, omega lambda t. I'm just going to hold. Imagine lambda being fixed for just a moment. Is one of these guys here a smooth family of smoothing operators. So the place where this correction, this clarification, becomes essential is in this definition of scaling family, which is to say that these are a scaling family if there exists operators.
00:07:28.744 - 00:08:14.674, Speaker A: Well, there is a smooth family like this, which extends to t is equal to zero, and satisfies this law here, where k is a smoothing family, like I've written down. What about lambda? What happens when you move lambda? Is it smooth in lambda? And in what sense is it smooth in lambda, if it's smooth in lambda at all? Well, the answer is, it's a little theorem that this is also smooth in lambda. If you vary lambda, then the integral kernels of these fellows, the little k, sub omega, sub lambda, sub t's, they vary smoothly, not just with omega and not just with t, but also with lambda. Lambda not zero, lambda positive. That's a little theorem, but we don't actually need that theorem. So it's just a little theorem. All right.
00:08:16.834 - 00:08:18.094, Speaker B: Variable in that.
00:08:19.474 - 00:08:58.682, Speaker A: Yeah, it's not, it's a little bit more than that. There's some messing around. As far as I'm aware. This is a little theorem of Bob and Eric, and they mess around a little bit to get this. But throughout today's lecture, Lambda will be one half weird convolution trick. It's some, I mean, there's this co cycle relation which we're going to be talking about. It's a bit like saying that if you have a group homomorphism and if it's the least bit reasonable, then it's automatically morphs in between lead groups.
00:08:58.682 - 00:09:32.892, Speaker A: It's automatically a c infinity group morphism. It's a little, in that spirit. Yeah, you just, yeah, you get started and then you bootstrap up to. All right, good. Now, so that's just a correction. Now let's do something with this relation here. I'm going to rewrite this relation through a couple of manipulations, and then when I've reached the desired formula, which is some horrible thing, I'll write it over there and we'll try and keep it.
00:09:32.892 - 00:10:47.190, Speaker A: Okay, so this guy here is the, what I've been calling in the notes, the scaling relation. And of course you can rewrite it in the following way as a formula for a omega t in terms of everything else, just by moving the r's over like this. Maybe I should leave space here to put in the lambda minus m. So far, so good. Ah, let's not forget the, now that we just labored over these things, let's not forget them. That would be no good. They also have to get conjugated k omega lambda t r omega lambda like that.
00:10:47.190 - 00:11:43.968, Speaker A: So that's a formula for a omega t. And there's a formula like this, no matter what omega is and no matter what t is. So in particular, there's a formula like this a of I'm just going to change t to lambda times t. And here I'll have to put lambda times lambda times t like that. And here I have to not forget to change lambda to lambda t here as well. And this now is a formula for a omega lambda t, which is exactly what appears in here. So you can substitute the second identity into the first identity.
00:11:43.968 - 00:13:01.804, Speaker A: Do I have the courage to do this in front of you? Let's see what happens? So what am I going to get? I'm going to get lambda to the minus two m, first of all, or r omega lambda to the minus two times a omega lambda squared t. R to the two. Yes. And then there's going to be this error term, but there's also going to be this error term conjugated by r omega Lambda. Oh, and it should also get multiplied by lambda to the minus m, which I missed out. And then one final error term like that. And now you can.
00:13:01.804 - 00:13:26.944, Speaker A: I can just do this. P times echoed. You look very upset. Yeah, well, so am I, if you do it p times. This, I claim, is what emerges. So now let's write this down and let's keep this thing for a while. It's a formula for a omega t.
00:13:26.944 - 00:14:21.810, Speaker A: So in the next stage, we would substitute in a formula for a omega lambda squared t, and we get some new thing. We'd get three terms with k's in them, and still there'd only be one term with a in it. So there would be a formula for a omega t with three terms involving k's and one term involving a, and so on and so forth. And here's what you end up with in the end, after you do it p times. And we'll see how this goes. A omega lambda to the p, t to the p omega lambda like that. Of course, the p th power of r omega lambda is just r omega lambda to the p.
00:14:21.810 - 00:14:50.686, Speaker A: I could have written it that way as well. Okay. And then there'll be a whole bunch of terms, p of them, which involve k's. And here's what they look like. Lambda to the minus j minus one times m. M is the order of the family that you see up here. And then r to the minus j, omega lambda.
00:14:50.686 - 00:15:46.024, Speaker A: K of omega lambda. Lambda to the j minus one, t r to the plus j omega lambda. Oh, I'm exhausted. Can we. Can we go home now? How do you feel about that? So, I don't know. Let's call this the iterated. So, the goal of today's lecture is to exploit this thing several times in order to get a few interesting consequences.
00:15:46.024 - 00:16:12.154, Speaker A: Any questions? Yeah. Yeah. The goal is to let P go to infinity and see what happens. And it's rather remarkable what happens. We have several interesting conclusions. I have several interesting conclusions in store. Yeah.
00:16:12.154 - 00:16:25.474, Speaker A: Well, this is an algebraic formula, so it just is what it is. But, yeah, if you want to exploit this, you're going to need. I'm going to need this stronger assumption on k. Yeah.
00:16:26.494 - 00:16:31.634, Speaker B: Are those other terms where j is based on even smoother things.
00:16:38.404 - 00:17:08.664, Speaker A: Yeah, we're going to see that in certain circumstances the series sort of converges. You know, you can let P get bigger and bigger and bigger. And for example, in certain circumstances, the leading term, the a term on the right hand side will go away and then you'll just have, it's kind of remarkable. A formula for a only involving K, an infinite series of K terms, seems kind of strange. After all, the K thing is what you get, it's just like a little bit of error tacked on at the end.
00:17:08.744 - 00:17:09.952, Speaker B: But it remembers the age.
00:17:10.008 - 00:18:22.934, Speaker A: It remembers everything. Yeah, it's very interesting. It's very interesting how it works, in my opinion. All right, very good. Which one of these things do I want to do first? Let's talk about pseudo local operators again. And this has to do with the following condition that if you have two functions, two smooth functions which have disjoint supports, then phi, a PSi is supposed to be a smoothing operator. To check this condition, you can require that find.
00:18:22.934 - 00:18:29.414, Speaker A: So I always have compact supports, and then it implies the stronger thing without compact supports.
00:18:40.514 - 00:18:41.874, Speaker B: Bigger and bigger and bigger.
00:18:42.034 - 00:19:23.164, Speaker A: Yeah. You mean to go from compact sports to arbitrary supports? Yeah, I mean, the fact of the matter is that a function, or rather an operator, is a smoothing operator if and only if all of the smooth, compactly supported functions times operator times smooth, compactly supported functions are all smoothing operators too. So you can check whether or not an operator is smoothing locally. That's the reason it's true. Okay. And it won't surprise you, I don't think that we're going to now prove that all of these scales, scalable operators, are pseudo local.
00:19:27.224 - 00:19:38.164, Speaker B: Excuse me, sir, when you are telling me compact, when you are telling supports, it's only compact supports, or supports can be also not compact.
00:19:38.624 - 00:20:42.854, Speaker A: If this condition, if you can see this board, if this condition holds for all fire and all sigh. Which are compactly supported, and it also holds for all Phi and Psi, whether or not they're compactly supported. Okay, that's an exercise. Okay. And now the theorem is that every scalable operator is pseudo local. Yeah, that's, yeah, of course, if you have an actual infinite sum of smoothing operators, then you have to worry that an infinite sum of smoothing operators is not necessarily a smoothing operator. But we're going to prove this theorem by getting away with this iterated scaling relation in its current finitary form.
00:20:42.854 - 00:20:51.794, Speaker A: We're not going to yet let p go to infinity. Oh, I'm sorry.
00:21:01.454 - 00:21:06.626, Speaker B: And then you say, okay, well, now, I put in a partition here. It could be some people.
00:21:06.770 - 00:21:41.456, Speaker A: Yes, you need to pay a modest amount of attention to that. But after all, if you have a partition of unity, you have a whole bunch of fires, an infinite number of fires. It still makes perfect sense to add them all up in the world of partitions of unity because it's sort of. Yeah, exactly. Yeah. Yeah. So we're not yet going to let P, that integer over there, go to up at the top.
00:21:41.456 - 00:22:21.954, Speaker A: We're not yet going to let it go to infinity. That won't be necessary for this argument. This relation will be necessary, but only in its finite form. Okay, here's a yellow, here's a little lemma. I've managed to break every single piece of chalk, and I hate using short pieces of chalk. Okay, which may clarify a little bit. What's going on.
00:22:21.954 - 00:23:10.874, Speaker A: Suppose I have a compact subset of rn, and here's what I'm going to do. I'm suppose, going to suppose that I've been given a scale ABL operator. That is an operator which appears in one of these families as the member of the family at t equals one. That's what a scale Abel operator is. It means associated to it. There is at least one scale ing family. Okay, so I'm going to assume we have a scale Abel operator and we've attached to it at this scale ing family, and then we attach these operators here.
00:23:10.874 - 00:23:57.554, Speaker A: I'm keeping repeating these things because remember, my strategy is to drive you crazy by talking about all of these a, omega, t, so that you demand that the tangent group would be defined eventually. So I'm supposing we have these guys and then we have these k's. So all of the stuff on this right hand board is going to be used in the discussion in particular right now, the a omega t's are going to be used. So there is some constant. Is k a good letter? No. What's another good letter for a constant? C k a, b, c, m. We don't have any manifolds yet, so we can call this guy m.
00:23:57.554 - 00:25:05.944, Speaker A: Here's the condition that if omega is now just allowed to vary over this compact set, c and f is going to be a smooth, compactly supported function. But I'm going to suppose that it lives far away from c. So the distance between c and the support of f is bigger than m. And now I'm going to apply a omega t. Or let's say that put in t's here, just let's say in the unit interval, just a compact number of t's and a compact number of omegas. And I want to look at this thing here just for fun. I'm going to apply a omega t to f, but then evaluate at omega, and what I get is zero.
00:25:05.944 - 00:25:57.384, Speaker A: And the reason that this is true is that we're looking at just a compact number of omegas and a compact number of t's. Part of the definition of a smooth family is that functions rather operators in a smooth family, they should be properly supported. I'm carrying that along all the time. But over a compact part of the parameter space, they should not just be compactly, excuse me, properly supported, but also uniformly properly supported. That means for any compact set or any compact supported function phi, there exists another compactly supported function psi, such that phi a is equal to phi a psi. That's part of the proper support condition. That's half of the proper support condition, but it's what we need for this argument.
00:25:57.384 - 00:26:54.540, Speaker A: So because I'm only dealing with a compact collection of these a omega t's, and because I'm only evaluating those operators apply to f on a compact number of omegas here, also, it's irrelevant that this letter is the same as this one. I could put on omega one here and omega two here, as long as they both are living inside of a compact set. I could insert between the a and the f some compactly supported function psi without changing anything. Namely, I just have to choose a compactly supported function psi which is equal to one not just on C, but on some big neighborhood around C. And then by proper support, aF will equal a psi f. Well, now we're in business, because if f is supported m units of distance away from c, then f times psi will just be zero. And so that's it.
00:26:54.540 - 00:27:34.906, Speaker A: It's not a matter of a times f being zero, f times psi is going to be zero. Psi times f is going to be zero. Yes, well, because it wouldn't be true, I suppose, to say that this property would hold when the distance is bigger than zero. It's the same thing as saying that a is not just a pseudo local operator, it's actually a local operator. It doesn't change supports, doesn't increase supports. Those aren't the sort of operators we're interested in. Local operators, by famous theorem, are actually differential operators.
00:27:34.906 - 00:27:39.134, Speaker A: Well, we are interested in those, but we want more operators to play with.
00:27:43.514 - 00:27:44.654, Speaker B: I just don't understand.
00:27:48.354 - 00:27:48.802, Speaker A: Yep.
00:27:48.858 - 00:27:54.834, Speaker B: Yeah, but if it's bigger than zero, then in particular I can just find some of between. And so I just get the m.
00:27:58.854 - 00:28:09.674, Speaker A: Maybe the quantifiers a is given for every a, and for every compact set c, there is some number m such that yadda, yadda.
00:28:10.534 - 00:28:12.526, Speaker B: Oh, I see, I see, I see, I see. Okay.
00:28:12.550 - 00:28:13.114, Speaker A: Yeah.
00:28:15.094 - 00:28:16.774, Speaker B: The choice of m. Okay.
00:28:16.814 - 00:29:16.214, Speaker A: Yeah. So I think I said we're going to assume all of this package is given, but I didn't write it in the lemma. So right here in the lemma, after the word lemma and before the word let, there should be assume, you should assume as being given a and at, and therefore a omega t. All of those things are given. Once those are given and c is given, then you have to choose the mis. All right, who's my favorite eraser? I'm not going to say anything more about this, this proof, even though maybe I should, but it, it's an exercise for them. Okay, now it's good.
00:29:16.214 - 00:30:24.594, Speaker A: All right, good. So here's what we have to prove. And a is given to us and the ats are given to us. Yet all of these things are already given in the background. And now I want to show that phi, a psi, is a smoothing operator. Okay. And in order to show that this is a smoothing operator, I just have to show that in my magic formula at the top, the first term, if you multiply on the left by phi and on the right by psi, the first term is going to be zero.
00:30:24.594 - 00:31:37.634, Speaker A: If the first term is zero, then the formula says that a is smoothing operator. Well, what it says is that a omega t is a smoothing operator. But according to the formula, a omega one is always just a, because this rescaling, when the rescaling factor is one, it's just doing nothing at all. So first of all, it suffices to show that r omega, not r, but a for a phi and a psi, like you see here, is smoothing. Since a omega one is just another way of writing a. Okay. And for this it suffices to show, well, Phi, and I'm just looking at that horrible thing at the top.
00:31:37.634 - 00:32:38.774, Speaker A: R to the minus p omega lambda. A omega. Gosh, lambda to the pie or what? Well, for any compact set, c, yeah, I don't even have to do that because I put in the PI's already. I just have to say for sufficiently large pieces, because the rest of this formula is certainly a smoothing operator. I mean, it doesn't even matter what it is, doesn't matter about the lambdas and any of the other details. Just, it's just a smoothing operator. I guess we should keep that package for a while.
00:32:38.774 - 00:34:21.734, Speaker A: What else do I want to say maybe nothing at this point. Yes. So suppose I have some function f, just any old test function. Let's call g psi of f, because after all, we want to multiply by psi. So what we want to do is apply to psi times f, or in other words, g, this rescaling operator, and see what kind of a function we get. Well, that's what I propose we do first and see what happens. So what does this rescaling operator do to supports? It stretches, supports out by a factor of lambda to the p.
00:34:21.734 - 00:35:50.574, Speaker A: And one way of saying that is that this is omega plus lambda to the minus p times the support of g minus omega. So you take the support, you translate it initially by lambda, then you stretch it out, and then you translate by lambda back. That's because we're doing this stretching around the point lambda, not lambda equals zero. Okay, so, great. So if I were to pick an omega in the support of phi, and let's just call by epsilon the distance between the support of phi and the support of Psi, it's just some positive number. Then the distance between omega and support of r to the peak omega lambda g is big. It's at least lambda to the minus p times epsilon.
00:35:50.574 - 00:36:44.324, Speaker A: So when you do this whole thing and you apply a to this quantity, this quantity here times a function f, what you're doing is you're applying this particular operator to a function whose support is far away from the support of phi. And because of that, what you're going to get is zero. One more thing. If I evaluate this whole expression at omega, of course this r to the minus p, omega doesn't change where omega is. It's a fixed point for the rescaling. So this whole thing evaluated at Omega is just phi of omega times this quantity. So what I'm trying to do is evaluate this thing at Omega.
00:36:44.324 - 00:37:20.428, Speaker A: I'm trying to take this operator, I'm going to apply it to a function f, and then I'm going to evaluate it, omega. And what I'm going to conclude is definitely, I'm going to get zero, as long as p is big enough. No. Yeah, it's not an issue because we stuck in. Yeah, yeah, it's this. Indeed. Somewhere there was a.
00:37:20.428 - 00:37:22.756, Speaker A: Here it is. Good. I was looking for this guy.
00:37:22.860 - 00:37:23.564, Speaker B: That one.
00:37:23.684 - 00:38:27.300, Speaker A: Yeah. We don't know what the sport of f is, but we're multiplying it by psi. And now for sure, the support of g sits inside a given fixed compact set, which is the supportive psi. So the conclusion from all of this is the let me write it down somewhere. Let's just do it here. If you have an omega which is in the support of phi, if it's not in the support of phi, this is going to be zero anyway, because the last thing you do in phi omega psi is multiply by phi of omega. What you get here for this is an interesting sum, namely this thing up here.
00:38:27.300 - 00:39:20.044, Speaker A: I don't know if I want to write it all down again. Maybe I should. So there's some lambdas which are lambda minus j minus one times m. Later on it'll be important what that exponent is, but it's certainly not important here. And then r to the minus j omega lambda k omega lambda t lambda lambda to the j minus one t, and then r to the j omega lambda. This whole thing applied to f evaluated it omega. As long as omega lies, varies.
00:39:20.044 - 00:40:02.408, Speaker A: In a compact set, there is some sufficiently large p, so that this is always true. And so a is not a, but phi, a psi, is a smoothing operator. It's a finite sum, finite sum of smoothing operators. So this is true for especially large p. To be precise, for every omega there's a p, or indeed, for every compact set of omegas, there's a p. So here's a formula for almost a formula for a. Okay, you could say it like this somewhere.
00:40:02.408 - 00:40:24.730, Speaker A: Let's go back over here. Yes. Oh, yes indeed. I guess it's not written down anywhere. Let's see. So this is always true. Yeah.
00:40:24.730 - 00:41:37.760, Speaker A: So now in this whole argument, we're just working with a single lambda, but as you say, it should be some number less than one. All right, good. Now if you have any pseudo local operator, should have said this before. It has a smooth, what you might call integral kernel away from the diagonal. It's just a k of xy, a c infinity function, k of xy. Let's call the operator, I don't know t. We can call it a, but this a doesn't have to be a smoothing operator.
00:41:37.760 - 00:42:55.820, Speaker A: It just has to have this pseudo, it doesn't have to be a scalable operator, it just has to be pseudo local. So this is a c infinity function defined on rn times rn minus the diagonal. And it has the property that if you have a couple of functions with disjoint supports equals empty, then of course we know that this product is a smoothing operator. I can write it down, catch up to myself. This guy is a smoothing operator. And now you can just say, what is the smoothing kernel? It's just phi of x k of x y sine of yeah, if you have a smoothing operator, excuse me, a pseudo local operator. It's not quite a smoothing operator, but it's almost a smoothing operator away from the diagonal.
00:42:55.820 - 00:43:54.714, Speaker A: It is a smoothing operator and it can be represented by a smooth function which is not defined yet on the diagonal, but it's defined away from the diagonal and it's characterized by this property. So the way the operator acts on functions which are supported over here, after you evaluate them over here, is through this integral kernel. And what we're saying is that in our case, for these smoothing, for these scaling operators, this guy here take the sum now from one to infinity, the integral kernel, the smooth kernel of this guy, is exactly that smoothing kernel away from the diagonal. If you take the sum from one to infinity, then, and you evaluate the integral kernels which appear at a point x and y, where x is not equal to y. It's not actually an infinite series. It's actually just a finite series and it defines a c infinity function. And that's what we're talking about.
00:43:54.714 - 00:44:40.634, Speaker A: So this guy here, is it in the case of a scaling operator, troubles me, echo that you're so unhappy. Yes. What I should have done is put psi f or the function g. Thank you. Yeah. And for that matter, on the right hand side, I should have five omega. Thank you.
00:44:40.634 - 00:45:19.846, Speaker A: Yeah, I made the formula shorter. That's a plus. All right. So, I mean, if this wasn't true, we would have insisted that Gail operators be pseudo local, because that's the sort of theory we're trying to build. But as it happens, you get it for free. In other words, the proof of this is not terribly important, because if the proof didn't work, we'd put in the conclusion as a hypothesis anyway. However, though, I guess the method of proof manipulating this iterated scaling relation, that's important.
00:45:19.846 - 00:46:56.728, Speaker A: As we'll now show with some other calculations, there are three of these, so we'll see how it goes. We've used 50% of the time and done a third of the work, so that's not so good. All right. Oops, excuse me. Now, if you have a scalable operator of negative order, then you can sort of improve the whole story here, because in the scaling relation that you see above, the first term gets smaller and smaller and smaller as p goes to infinity. So it sort of goes away in the limit. And now there's no relationship between omega and the support of f.
00:46:56.728 - 00:47:51.430, Speaker A: It could be, so to speak, on the diagonal. That wouldn't matter. What we're going to do is examine the first term that you see up there, which is a big mess of things. Well, first of all, there's a lambda to the minus p m and then there's this thing here, r omega lambda to the minus p and then a omega lambda p. And Wes just evaluated at one and then r to the piece omega lambda. So I'm going to take this whole operator, I'm going to evaluate it on a function f and then evaluate the resulting function at a point omega. And then I'm going to take the limit as p goes to infinity and there's not much room left.
00:47:51.430 - 00:48:49.564, Speaker A: Just about the only thing that'll fit there is zero, and that's what the limit is. So in other words, obviously we now have a formula for a, just in terms of smoothing operators in the case where the operator has a negative order. So obviously cannot work for positive order because, for example, or non negative order, the identity operator has. Yeah, it's not going to work for the identity. Yeah. Yes, very much. I keep forgetting to say that I think I said at the very beginning today lambda will be one half, but indeed, yeah, it's looking pretty good, right? The lambdas, so to speak, the powers of lambda are going to zero.
00:48:49.564 - 00:49:44.822, Speaker A: It's not over yet because there's that other thing. But it's looking good for exactly the reason that you mentioned. Yeah, the order is m and m is negative here and p is 12345 and so on. So yeah, it looks good. It's not completely over yet, but it's, it's looking good. So let's just look at this whole thing. So what does this mean? It means you take the function f and then you apply rp to it, and then you apply a to it.
00:49:44.822 - 00:50:40.170, Speaker A: And the last thing you do is you apply this thing. And what this does is it's just a rescaling. So what you're supposed to do is take this function here and evaluate not at omega, but at the point which is omega minus lambda to the minus p times omega minus omega. But that's just omega. Omega is a fixed point of the corresponding transformation of rn. You can forget about this term here, this operator, as long as you're evaluating at this fixed point omega, this is just a omega lambda p r to the p, omega lambda f omega. As Jamie said, the, the exponents, the powers of lambda are in our favor and we just want to show that the remaining crazy operator term is not so bad.
00:50:40.170 - 00:51:02.962, Speaker A: We've already chopped off a third of it. That sounds like good progress. And now we just have to worry about this thing. So that's point number one. Guess, maybe point number two can be fit in here in this theorem. I hope I said that. Yeah, omega is given to us.
00:51:02.962 - 00:51:59.974, Speaker A: Omega is just some fixed single point. So omega is given to us, just like in the lemma we saw a little while ago, we can find the compactly supported, as we called it, psi. And now what I'm looking at are these functions, okay? And I want to imagine evaluating those on any function, I don't know, g like this. So I take the operators a, omega, lambda to the p, and I evaluate those. This whole thing is supposed to be a subscript. It doesn't look like it does. Put it down where it belongs, like that.
00:51:59.974 - 00:52:52.112, Speaker A: So that's a bunch of numbers, and it's the same as this bunch of numbers, same operator. I'm just going to stick in upside like this for every g, because omega is fixed. And these operators, omega, a sub omega, lambda, p. As lambda goes, excuse me, as p goes to infinity, omega to the p goes to zero. So, excuse me, the lambda to the p's, they form a pre compact set. All of these operators are uniformly properly supported. SP is one, two, three, and so on.
00:52:52.112 - 00:53:47.630, Speaker A: These are uniformly properly supported. So if I'm only interested in evaluating at omega, what contributes to that value of this function at omega is just a certain uniformly compact part of g, meaning I can stick in a psi here the bits. So imagine that psi, again, is some function which is equal to one. This is exactly what you should be imagining. Here's omega. So psi is some function which is equal to one very, not just close to omega, but very far away from omega, but it's still compactly supported. What the function g is over here has no bearing on what a of g is at omega, because a cannot move supports more than a certain compact amount by uniform compact support ad Nas.
00:53:47.630 - 00:54:59.274, Speaker A: Okay, and now there's something interesting that happens somewhere. Let's build a bunch of functions by taking rf, not f, but this rescaling of f, and applying psi to it like this. What should we call, I don't know, hn hp to be the following thing. Psi times the rescaling of f. It's a sequence of functions. It's the same. But I was writing it like that.
00:54:59.274 - 00:55:24.494, Speaker A: Yeah, yeah. Rescaling stretching around omega by a factor of lambda p times is the same thing as stretching around omega by a single factor of lambda to the p. So the fact that that particular p fell down from the exponent and the top row to the exponent in the bottom row doesn't, doesn't matter.
00:55:25.114 - 00:55:26.294, Speaker B: I'm testing.
00:55:28.504 - 00:56:14.898, Speaker A: I mean, who knows where in my handwriting whether p should have where I intended the p to be? I don't know. Okay, what are these functions? Well, it's. This is point wise products. This is psi of x times this rescaling of f. So this is f of omega plus lambda to the p x minus omega, like that. And what happens as p goes to infinity? Well, as p goes to infinity, this function is getting stretched out more and more and more. It's looking more and more like a constant function.
00:56:14.898 - 00:57:10.776, Speaker A: And then the limit as p goes to infinity, it is the constant function, just constant, infinitely supported function. But I'm multiplying it by this fixed, compactly supported function. So these functions are now converging in the topology of cc infinity of rn. So H is a real easy function. H of x is just psi of x times f of omega. And this is convergence in the good topology, the topology for us of cc infinity or m. And these operators are continuous.
00:57:10.776 - 00:57:54.832, Speaker A: This is a hypothesis we've not yet used, but finally, we'll dust it off and use it. So what am I interested in? Well, I guess it doesn't really matter what I'm interested in just yet. Let me just write down the following thing. Here's a collection of operators. If I apply them to these fellows here, and then I evaluated zero. This limit exists. Because these are continuous operators.
00:57:54.832 - 00:58:25.464, Speaker A: They send convergent sequences to convergence sequences. Okay. The operators also depend on p. So you have to use an epsilon over three argument. But it's still the case that the limit exists. So, in particular, if you look at these numbers here, what are we looking at? Well, hp is psi times f, and so we're looking at a omega lambda p times psi times f. But that's the same thing as a omega lambda to the p times f, according to this.
00:58:25.464 - 00:59:14.654, Speaker A: And somewhere I forgot to put in the r to the p in what I just said, this is a collection of numbers and it's a bounded sequence. Finally, it's exhausting. It's a bounded sequence. But as Jamie pointed out, we're multiplying that bounded sequence by this decreasing, geometrically decreasing sequence of powers of lambda. So the product sequence is converging to zero. That's what the lemma says. So we're done.
00:59:14.654 - 00:59:54.074, Speaker A: It actually worked. Yeah. Yes, yes. This is a collection of operators which is smooth in lambda to the p, if you like, all the way to p equals infinity. So the a omega, comma, lambda to the p's converge in the appropriate sense to a sub omega zero. That's crucial. The fact that there is an operator at zero is crucial for this argument.
00:59:54.074 - 01:01:24.584, Speaker A: Yes, everything works uniformly over compact sets of omega space. All right, so that's progress. It means we have a formula now for a purely in terms of k when the order is less than zero. If the order is less than zero, this series, or this sequence of partial sums from one to p converges as p goes to infinity, obviously. And therefore we have a formula for a negative order operator purely in terms of kick. Okay, and let's try and capitalize on that somewhere. Maybe here, I guess we could start over here you have an operator which is a scalable operator of negative order.
01:01:24.584 - 01:02:25.928, Speaker A: Then it's in particular a scalable operator, so it's pseudo local, and so it has one of these integral kernels away from the diagonal. Smooth integral kernel away from the diagonal. At the moment, as of this very instant in time, what this integral kernel could look like as you approach the diagonal is it could blow up, like, for example, one over x minus y. That's an actual example of a smooth kernel away from the diagonal for a pseudo differential operator, scalable operator on the line of order zero, not of order one, and it does indeed blow up. But if it's order zero, that doesn't actually happen. It has order less than. Oh, hang on, I didn't.
01:02:25.928 - 01:03:09.334, Speaker A: Yeah, yeah. Um, yeah. And then I have to remind me to correct, if you take the operator, the integral operator in quotation marks, whose integral kernel is one over x minus y. Well, what happens when x is equal to y? Let's not worry about that. That integral kernel, which is certainly defined away from the diagonal, is a smooth function away from the diagonal, and it is the smooth kernel away from the diagonal of a scalable operator on the line of order zero. Strictly speaking, all of our operators are properly supported. So I should somehow cut off the function one over x minus y to make it properly supported in x and y.
01:03:09.334 - 01:03:46.584, Speaker A: Well, so what we did, we checked this, we looked at this particular example, and we said that this particular example is indeed scalable operator. We constructed a scaling family. Yeah, so, and maybe you're thinking of more advanced questions. This actually is an l two bounded operator. We'll get to that later on. Lots of good things you can say about that operator. Now, as for the correction, so so far I didn't write down anything wrong, but I was certainly heading in the wrong direction.
01:03:46.584 - 01:05:12.196, Speaker A: This is what I wanted to say. If it has sufficiently negative order order less than the dimension of the rn on which we're working this n here, then the integral kernel away from the diagonal extends across the diagonal to a continuous function at least, and let's just call that extension k of xy. So this is a continuous, properly supported function of x and y. And if this is the extension, then there's a formula for what the operator is. Namely, it's just an integral operator, not a smoothing operator, because f, or rather k, is not smooth. This thing certainly makes sense anyway, because it's continuous at least, and this is a formula for a. So these guys, the operators of sufficiently negative order, are not particularly mysterious, you might say.
01:05:12.196 - 01:06:13.714, Speaker A: They're all just usual integral operators with continuous kernels. And it's probably evident what the strategy should be. The iterated scaling relation together with the lemma, which applies for any negative order operator in particular order less than minus n. The two things together give you a formula for what a is, namely, it's this infinite series. So we just have to take the infinite series and evaluate it and show that indeed it corresponds to an operator with a continuous integral kernel, which is what we'll do. I guess we'll keep that there for a moment. A is a scalable operator on cc infinity of rn over order less than minus nn.
01:06:13.714 - 01:06:21.834, Speaker A: All of this stuff up here applies to a.
01:06:24.494 - 01:06:27.914, Speaker B: Less than, excuse me, strictly.
01:06:28.214 - 01:07:12.374, Speaker A: Strictly less than n. Yeah. One example that we looked at was the operator, scalable operator given by the logarithm function. We looked at the function whose integral kernel was log of the absolute value of x minus y, and we argued that that was a scalable operator of order minus one on the line. So if you have equality, then all bets are off. This is not true. All right, so let, hey, omega.
01:07:12.374 - 01:08:43.933, Speaker A: Well, first of all, let's just fix like we did above, do it where I'm supposed to do it, right at the top of the argument. So now we want to prove this thing. So let's fix the lambda like one half, and let this thing here be the integral kernel of the operator big k with the same indices. This guy is a smoothing operator. So there's nothing wrong with this function. This is a c infinity function of both variables, including on the diagonal. But what we need to do is take this operator k like you see in the formula up here, and conjugated by this rescaling operator j times.
01:08:43.933 - 01:10:11.980, Speaker A: Now I got to figure out what this thing is. Well, we had a discussion about this general thing about how integral kernels transform effectively under diffumorphisms. In this case, rescalings of rn, it's not very hard at all. It's, well, you take the old guy and you just apply this rescaling operation not to functions, but to the underlying space. So what I want to put here is omega plus lambda to the minus j, x minus omega, omega plus lambda to the minus j, y minus omega. And we're not quite done because you also have to put in the rescaling effect of given by the measure, and that's lambda to the minus j. Well, that doesn't look so good, because now n is positive and so is j, and this is one over lambda.
01:10:11.980 - 01:10:43.912, Speaker A: And I'm imagining that lambda is a number less than one for other purposes. To make the rest of the argument work, this lemma, for example. So this guy's blowing up. Well, that's just the way it is. On the other hand, we know that for a given x and y, if x is not equal to y, then the distance between these numbers is getting bigger and bigger and bigger. And so eventually this quantity here is zero. So that's acting in our favor as well, not as well.
01:10:43.912 - 01:11:34.924, Speaker A: That's acting in our favor. Okay, so far so good. But we still have these lambdas to play around with. In fact, we could do the following thing. The reason that they both end up being the same is that in order to obtain the formula for the integral kernel, you have to do a change of variables. And the change of variables. Yeah, you do the math.
01:11:34.924 - 01:12:00.866, Speaker A: I did it. Okay, so let's, rather than write this whole thing out again, let's remember this exponent that appears up here, minus j minus one times m. That's the good. The sign there is good because m is negative. And so this is a positive power of lambda and lambda is less than one. So these numbers are getting small quickly. Okay.
01:12:00.866 - 01:12:42.130, Speaker A: And all I have to do here is now put in this guy. So what are we looking at here? Well, there's lambda to the j times. That's going to cause me endless confusion. Lambda to the minus j and then m plus. Nice. Good. And then there's an extra minus plus m.
01:12:42.130 - 01:13:18.288, Speaker A: That doesn't really matter, but there we go. Now it's looking okay, because m is supposed to be less than minus n. So m plus m. M plus n is some negative number, and there's a minus sign here. So these guys are actually decreasing geometrically quickly. So we're basically there and that's. Now you see, the reason why we wanted the order to be less than minus n is to counteract this.
01:13:18.288 - 01:14:02.304, Speaker A: Radon, nicodem, derivative that appears here. Well, and also to, because we have that annoying counter example of the logarithm function. We had to do something about that. We had to rule it out. Okay. But what we're actually interested in doing is taking this integral kernel and evaluating at the point where x is equal to omega. So we're interested in this integral kernel, just to make it seem a little bit easier.
01:14:02.304 - 01:14:57.114, Speaker A: So what we're going to get here is this nice geometrically decreasing sequence of numbers, and then k, omega, lambda. I wrote t here. This is true for any t. There's a little typo, not really a typo, but t is supposed to be lambda to the j minus one, like you see up there times t. But let's set t is equal to one. And now we just get omega, because this whole thing evaluated at omega is just omega, and then omega plus lambda to the minus j. Y minus omega.
01:14:57.114 - 01:16:11.764, Speaker A: M is not going anywhere. Yeah, you're right. Yeah. These guys here are uniformly bounded over compact sets over maybe y and c impact. And as j gets bigger and bigger and bigger, these numbers lambda to the j converge to zero. And these smoothing kernels extend smoothly to t equal to zero. So there's some uniform boundedness here.
01:16:11.764 - 01:16:42.362, Speaker A: So the integral kernels omega in the story, too, I couldn't use the same compact set. So these are all continuous. In fact, they're smooth functions. They're uniformly bounded, at least when you restrict omega and y to a compact set. And now you multiply them by this geometrically decreasing sequence. So now you can add these things up, and it's a c infinity. It's not a c infinity.
01:16:42.362 - 01:17:27.334, Speaker A: Well, it is. Well, we don't know that yet. It's a continuous function. We have convergence. And so there, it's not sort of obvious, not so obvious from this formula here. But when you write down what the integral kernels are, you see that the integral kernels add up to a single integral kernel, which is a continuous function of the two variables. And it doesn't matter here whether x is equal to y or x is not equal to to y.
01:17:27.334 - 01:18:20.494, Speaker A: These formulas don't make complete. Remember, what we're adding up here is we're neglecting this funny thing here, which we're allowed to neglect by this theorem, and we're getting a formula for a. How do you feel about that? How does the Internet feel about that? Yeah. If you look at the integral kernels, k omega lambda t, and you let omega range over a compact set, and you let the t's range over a compact set, for example, the closed interval from zero to one. Then that collection of integral kernels is actually smooth in omega and t. And so in particular, it's uniformly bounded.
01:18:20.884 - 01:18:26.584, Speaker B: Guess I was worried that the entry, the second entry.
01:18:30.284 - 01:19:09.084, Speaker A: Well, it can leave some compact set, but then it's just zero. Right? Because there's another compact set which is telling you, which is related to proper support. I'm going to declare this result proved. We looked at this crazy formula that a is given by this infinite sum. We examined. Well, we examined the finite partial sums, and we examined the integral kernels related to those finite partial sums, and we saw that we obtained convergence. And so we're in business.
01:19:15.124 - 01:19:18.828, Speaker B: Excuse me, we have, we have fixed today, lambda is equal to half, right?
01:19:18.996 - 01:19:20.180, Speaker A: Yes, right.
01:19:20.212 - 01:19:21.864, Speaker B: So mod of lambda less than one.
01:19:22.484 - 01:19:27.420, Speaker A: Lambda should be less than one. Yeah, half is irrelevant, of course, but lambda should be less than one for this argument. Right?
01:19:27.452 - 01:19:27.884, Speaker B: Right.
01:19:28.004 - 01:19:39.434, Speaker A: Thank you. Okay, let me mention just one more thing. Yeah, but go ahead. First.
01:19:45.414 - 01:19:48.870, Speaker B: Are the kernels, like, special in some way?
01:19:49.062 - 01:21:06.064, Speaker A: Yes, the kernels where the order is exactly minus the dimension of the manifolds are special. And I'm tempted to go on a digression to explain why, but I'll resist and instead describe a minor elaboration of this theorem, which is which we'll need. Rather than try and remind you all about all of this stuff, next time, let's just write down what happens when you push this argument just a little bit further. Now we can maybe get rid of this. So here's a theorem which is a variation on the theorem we just proved. If a has order minus big nice for every n, if you like, it has infinite negative order. It's a scalable operator.
01:21:06.064 - 01:22:15.854, Speaker A: Then a is a smoothing operator. We already know that smoothing operators are indeed scalable of negative order for any minus n. So this exactly characterizes the operators of infinite negative order. And the trick to proving this is to consider the following thing. Imagine composing a on the left and on the right with a partial derivative operator like this. So this guy also has order minus n. Well, the order is minus n plus alpha plus plus beta, but minus n can be anything.
01:22:15.854 - 01:23:37.980, Speaker A: So this also has order minus n for every n. And the integral kernel of this thing away from the diagonal is just, well, in the x direction you should differentiate alpha times, and in the y direction you should differentiate beta times. And there's some integration by parts which is involved. So there's also a minus one. And then kick if a is the integral kernel for little, a is the integral kernel for big a away from the diagonal, which makes sense, it's a c infinity function there. Then the integral kernel away from the diagonal of this composition is just given by some partial derivatives. So if you take this c infinity function and you compose it, then what you find according to the previous arguments is that it is that, that these derivatives extend to a continuous function across the diagonal, no matter how many derivatives you take.
01:23:37.980 - 01:24:01.920, Speaker A: But that means the function a of x y is actually a smooth function. So that didn't take long, fortunately, and we are almost done with the magic scaling formula, I don't have time to rehearse one more argument. Yeah.
01:24:02.032 - 01:24:04.404, Speaker B: Should there be something like dependence on alpha n theta?
01:24:07.224 - 01:24:09.400, Speaker A: Where would you like that dependence to be?
01:24:09.592 - 01:24:10.760, Speaker B: Maybe order.
01:24:10.912 - 01:24:45.882, Speaker A: Yeah, so the, if a has order of twelve, then the order of this product is twelve plus alpha plus beta. But because a has order minus n for every n, the composition will also have order minus n for every n. So. Yeah, so I skipped a step. Yeah, that's exactly what happens. Let me show you one thing about order minus n. We got just a few minutes, not enough time to do one more.
01:24:45.882 - 01:25:27.204, Speaker A: There's one more of these arguments which I may just, in fact, I already did write down. I may not actually do it live in front of you because, damn. But let me show you something nice instead, somewhere. Suppose you're exactly on the cusp here, order minus n. Suppose you have.
01:25:31.784 - 01:25:32.120, Speaker B: A.
01:25:32.152 - 01:26:36.250, Speaker A: Smoothing operator of that order, and now you consider the usual thing, which would be this thing. In the case of order minus n, this is what you see in the scaling relation. And the scaling relation, which I just erased inconveniently tells you that this thing is a smoothing operator. So this thing is a smoothing operator. Therefore you can evaluate it on the diagonal and you get a number. Or if you're a more fancy sort of french oriented person, you get a one density. Okay, and what does it depend on? Well, it depends on, let's just take lambda t equal to one.
01:26:36.250 - 01:26:51.986, Speaker A: Let's get rid of the t dependence for a moment. Oh no. What I really want to do is evaluate zero. It's much more interesting there. This is what I want to do. Zero is just a particular case of a t. So I'm allowed to do this.
01:26:51.986 - 01:27:54.082, Speaker A: And now I get a number, like I said, and it doesn't depend on t anymore because I said t equal to zero and it just depends on lambda. Okay, well, I prefer to consider this quantity here. So now let me call this thing delta. It depends on omega and it also depends on lambda. Like that, and here's a beautiful observation due to crucian babyankan, which is that this quantity for lambda times mu is equal to this quantity for lambda plus this quantity for mu. Just algebra. Just mess around.
01:27:54.082 - 01:28:46.864, Speaker A: Once you know it's true, it's easy to check that it's correct. So what that means is the delta of omega lambda is just the logarithm of lambda times some absolute, well, not constant, but some number depending on delta, like that. So this attaches to an order minus n operator, and any point in rn, a number delta of omega, it's a c infinity function of omega. And if you do the math carefully, it's not really a number, it's a one density. So it's meant to be integrated. Maybe we'll just do it over here, go in this direction. Over the manifold or over rn.
01:28:46.864 - 01:29:32.236, Speaker A: You're not allowed to do this integration unless the operator was compactly supported in some sense. But let's assume that that's true. Now you get something which just depends on a, because delta depended on a stuck in a in here. And this is a trace. It's not the trace of an operator, it's another trace of an operator. This is the order minus n operators, the non commutative residue given by this rather beautiful formula. In particular, this consequence, or it's related to this very elegant fact here.
01:29:32.236 - 01:29:46.614, Speaker A: So order minus n is very, very interesting and special. Okay, we'll come back to this. We'll do this properly some point later in the class. We are done for today. Been a wonderful audience. Thank you very much.
01:29:49.834 - 01:29:53.946, Speaker B: It's got a tracial property like the trace of ab is trace of ba.
01:29:54.090 - 01:30:24.494, Speaker A: Yeah, the tracial. Yeah. If you have two scalable operators, the sum of whose orders is minus n, then the trace of ab is the trace of ba. So it's. Yeah. If you multiply two order minus n operators, you get an order minus two n operator and then this trace is actually zero. But if you multiply two operators, you know, one of order one and one of order minus n minus one, then the trace property holds.
01:30:24.494 - 01:30:42.734, Speaker A: Nice. Yeah, you good? Any questions from the Internet? Which case? See you later.
