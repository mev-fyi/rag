00:00:01.720 - 00:00:01.904, Speaker A: Great.
00:00:01.904 - 00:00:03.118, Speaker B: Thanks. Thanks, Zara.
00:00:03.246 - 00:00:03.582, Speaker A: Great.
00:00:03.638 - 00:00:41.842, Speaker B: So welcome, everyone. Good evening, good afternoon, good morning, wherever you happen to be, and welcome to our quantitative finance seminar series here at the Fields Institute. Before getting started and introducing our speaker, I just like to say thank you to our sponsors through the center for Financial Industries Scotiabank for sponsoring the seminar series, and the Fields Institute generally through the years. And without further ado, I'd like to introduce our speaker for today. So that's Samuel Cohen, Professor Cohen. He is a mathematician at the Mathematical Institute at Oxford University. He's also positioned at the Alan Turing Institute in London.
00:00:41.842 - 00:01:22.752, Speaker B: He obtained his PhD in 2011 at the University of Adelaide under supervision of Robert Elliott. And his interests these days are in the interaction between statistical learning, decision making and modeling, with particular applications, of course, to finance and economics. And today, Sam is going to tell us all about an exciting new line of work on arbitrage free neural SDE markets. So, Sam, the floor is yours. Please share your screen. And actually, sorry, one small admittance, you put in your questions into the chat, and if appropriate, I'll interrupt Sam in between. Otherwise, we'll wait to the end and have a formal q and a followed by an informal one.
00:01:22.752 - 00:01:24.524, Speaker B: So take it away.
00:01:25.944 - 00:01:29.604, Speaker A: Excellent. So I'm hoping that's all shared properly now.
00:01:30.304 - 00:01:32.284, Speaker B: Yes, we can see it quite well.
00:01:32.664 - 00:02:23.372, Speaker A: Excellent. Well, thank you for the invitation. It's great to remotely be in Toronto for this seminar. So, as Sam has said, I'm here to speak about some work I've been doing on arbitrage free neural SDE market models. So this is all joint work with my colleague Christoph Risinger and our student Victor Wang, and has been supported by the CME group. So what's the idea here? Well, a very basic problem that we have to do a lot in finance is model the joint dynamics of vanilla options. This is important as an input to many other things, whether we're trying to price a liquid derivatives, whether we're trying to manage the risk of option trade books.
00:02:23.372 - 00:03:52.274, Speaker A: Many other things that we would like to do depend on us understanding the behavior of basic liquid vanilla options. So what I want to do in this talk is talk about a way of trying to build a non parametric model for the european options book. In particular, I'm just going to focus on european call options, the simplest little things that we can do, but we're going to try and do this using non parametric models. So, using machine learning to try and understand what's going on, but we're going to do this bringing in our understanding of financial constraints in particular, no arbitrage. What we want to understand is how does knowing that a market should be free of arbitrage that we're looking at european call options, how can we put those things together with a machine learning model for the dynamics of those options prices? What we're going to do has really four things. We're going to look at building a state space for factor approximation, which should be free from static or model independent arbitrage. We'll then look at how we can do the inference problem where using discrete time series, for example, daily prices of the stock and different options, how we can perform inference to figure out our model.
00:03:52.274 - 00:04:51.270, Speaker A: We're going to use neural networks as approximators for the drift and the diffusion of an SDE, which we'll assume describes the options book. And we're going to do this in a way which is practically implementable. So you can calibrate this, you can run it, you can use it using very reasonable computational complexity. So machine learning and finance, this has been around for a long time, particularly the idea of using neural networks or deep pricing and hedging. So there's over 150 papers since the 1990s in this area. And of course, the last few years has seen a massive explosion of interest in how we can use machine learning in finance. So here's an excerpt from a table in a paper by Johannes Rouf and Wegen Wang, which was published last year.
00:04:51.270 - 00:06:09.974, Speaker A: So, of course, it's already massively out of date. But they document 150 odd papers with what are they trying to do? What are the features they use? What do they try and output? Is it hedging, is it pricing? And how do they build models? There's many, many things that one could talk about. So I'm not going to attempt to summarize everything, but one way of looking at the use of if we focus on neural networks as a machine learning tool, we can say, how can we use neural networks for just looking at option prices? And there's really three broad applications that people have studied. The first is using neural networks principally as a computational tool. People look at if I have a parametric model, I would normally have to do pricing that involves solving a PDE or doing a Monte Carlo simulation. That's expensive. So can I train a neural network to speed that process up so that I can, for example, do calibration far more efficiently? So, an example of recent work in this direction is by Bayer, Blanke, Hovarth and others, where they looked at deep calibration of rough stochastic volume models.
00:06:09.974 - 00:07:22.664, Speaker A: Another approach which is closer to what we'll do is using neural sdes. So here you write down an SDE model, but you parameterize the drift and the diffusion in that model using neural networks. So, a couple of examples of work in this direction by Christa Cuchiro, Wahid Khusrahi and Joseph Taishman, where they try and build a model in the Q measure, in the Martingale measure, where you can simulate out the underlying price, and then they calibrate this model so that it prices all the derivatives correctly. And another paper similar idea is by Patrick Gijatovic and others in Edinburgh, and I should say Edinburgh and Imperial. Here we're going to do something a little different, instead of looking at Martingale models. So, working under the Q measure, we're going to look at a market model where I'm going to build a model which treats each option as an independent asset. I'm then going to build a model which can simulate forward in time all those options and the underlying at the same time.
00:07:22.664 - 00:08:02.596, Speaker A: And this seems like a fairly natural thing to do. Machine learning is very good at dealing with problems in high dimension, but the problem is, when you think about options, they are not a free set of processes. They are restricted. No, arbitrage tells us there are restrictions on the prices of options. We know that at expiry, options have to converge to a deterministic function of the price, et cetera. And so we need to see how we can include those economic ideas into our neural SDE market model. So, modeling and underlying.
00:08:02.596 - 00:08:36.832, Speaker A: So, I'll think of a stock. Most of our examples actually are in foreign exchange. But for now, if I say stock, just treat it as a generic underlying asset. Modeling an underlying stock and the options on that stock is an interesting problem, because, as I've said, they're closely related. We know that options, when the stock is high call options, have high prices, etcetera. We're going to focus on vanilla european call options. So those are, I know pretty well everyone here will be familiar with this.
00:08:36.832 - 00:09:27.302, Speaker A: Contracts which have a payoff, which is the maximum of zero and St K, some fixed future time for a fixed strike price. And the prices of these options encode market perceptions of the volatility of the underlying, of market expectations of what's going to happen in the future. So, using models of this form, one can then apply these in various ways for hedging, risk quantification, etcetera. So, as I've said a few times, the prices of options are heavily constrained and interrelated. So no arbitrage tells us bounce on option prices. We teach this the first course you do in financial modeling. You learn this idea that, for example, the call option should never be worth more than the stock.
00:09:27.302 - 00:10:14.154, Speaker A: If we ignore discounting the right to buy a stock for a fixed price in the future, that's never worth more than just having the stock today. Similarly, if I have two options with different strike prices, the one with a higher strike price must have a lower value. And similarly, convexity type conditions also hold. In general, the absence of model free arbitrage for a set of static prices is characterized by a collection of linear constraints. These constraints encode positivity monotonicity in both time and in the strike price, and convexity in the strike price. So many people have worked on this. Davis and Hobson have some really nice work in this area.
00:10:14.154 - 00:11:11.662, Speaker A: In an earlier paper we looked, we extended some work by cuso, or simplified it to try and reduce the number of constraints that we have to work with to make it computationally feasible. So the thing to remember is that no static arbitrage tells us that prices satisfy some linear constraints. Let's think a little bit about markets, and then we'll come back to this problem of how we build a model. So, in real markets, not all strikes and all expiries are traded at any time. So here, for example, this is on CME listed Eurodollar European calls in 2018. This is a fairly typical picture. On the left, we show the expiry and the strike, and I've put a dot for every combination which is actually being traded on that day.
00:11:11.662 - 00:11:46.824, Speaker A: And you can see you get a pattern that for very long expiries, there are few expiries that get traded. On the other hand, close to maturity, close to expiry zero, there's a lot more expiries that get traded further away from maturity. Long expiry, you get a wide variety of strikes. Close to maturity, you get relatively few strikes that are actually traded in the market. So writing this in strike and expiry isn't the most convenient. We can represent it more conveniently if we write it in moneyness or log moneyness and expiry. And there, you see, you get a nice symmetric shape.
00:11:46.824 - 00:12:32.592, Speaker A: You get something focused on at the money options, at small expiries. And then as you move deeper, further into the book, further in terms of expiry time, you get a wider range coming out. If we look at the black Scholes deltas, these actually come together in a relatively consistent way. One thing you'll note, though, is they don't form a simple rectangular grid, although it's pretty close. This is unsurprising in a way, because this is a foreign exchange market and the convention is often prices are quoted in terms of delta in this market. So just the main thing to get from here is that not every strike and expiry is equal. They're not all equally traded in the markets.
00:12:32.592 - 00:13:15.866, Speaker A: And if I want to look at real data, then I shouldn't be necessarily assuming that I have a nice grid of strikes and expiries that I could work with at any time. Now, this has some strange consequences. For example, here, what I've shown on the left, these are the eurodollar options, which expired in 2020, in March. And what I've plotted is from one year out where they were. And you can see these are the same contracts with a fixed strike price. So here, for example, in orange is something which has a strike price of 1.1. It starts off here and it actually moves around randomly.
00:13:15.866 - 00:14:14.874, Speaker A: And that random movement is because the stock, or in this case the currency, is changing. Now, this orange contract, turns out that this is always reasonably close to the money and so is traded all the way through to expiry. On the other hand, this blue contract, which is a slightly lower strike price, it gets traded most of the time, but there's actually a period close to expiry where it's not traded at all. And similarly, if I look at options with a high strike, they get not traded to begin with. They start being traded and then they simply disappear from the market and then never traded again. And so this is the type of thing that you see when you look at market data. What this means is that it's not necessarily a good idea to try and directly model the option price in terms of the absolute maturity time, capital t and the strike price k.
00:14:14.874 - 00:15:11.776, Speaker A: Instead, what we're going to do is we're going to build a model for a normalized call price surface expressed in time to maturity and moneyness. So I'm not going to try and build a model for a particular contract as it moves towards expiry. I'm going to say, what can we say about the price of, for example, a three month at the money option? That's the type of option I'm going to say, how does that change through time? So as I move through time, the expiry date of a three month option moves through time as well. But I can see the price fluctuating and try and build a model for that. This also has the advantage of generally speaking, it's closer to being a stationary process. And so all the time series analysis gets a bit simpler. So what we're going to do is we're going to start by fixing a collection of different maturities and moneyness points.
00:15:11.776 - 00:16:00.584, Speaker A: They don't need to be on a regular grid. We're just going to fix a range of those different points as a lattice, and we're going to try and build a model for those option prices as we move through time. So I'm going to try. I'm building a model for these collection of moneynesses and times to maturity, and trying to see, as we move through time, how do those prices fluctuate. Now, it's always hard to design a talk in this area, because you want to talk about the data or where you're building things, but then you also want to talk about the theory. So what I'm going to do is I'm going to illustrate everything using synthetic data which will come from a Heston SLV model, the details of which will come later. But it means that I can show you some pictures.
00:16:00.584 - 00:16:47.764, Speaker A: For now, they come from a Heston SLV model. So our aim is to construct a model for option prices based on this data, and we want to make sure it doesn't introduce arbitrage due to calibration error. We're then going to apply this methodology to some real data at the end. So how are we going to do it? Well, for all, machine learning is fantastic at dealing with very high dimensional problems in practice, it struggles, particularly when you don't have an enormous amount of training data. Now, of course, if I'm working with simulated data, I could simulate as much as I like and train with it. But let's try and keep ourselves closer to the problems we'll face in the real world. And so I'm going to build a model which only uses a relatively modest amount of training data.
00:16:47.764 - 00:17:31.307, Speaker A: Because of this, I'm going to build a factor model. So, I'm going to say that the normalized call price should be written in terms of a constant factor, g naught, and then a sum of random factors, gi. Now, these factors, g naught and gi, I'll obtain from looking at historical data to try and see in which ways does the surface actually move. Once I have these factors, I'll build a stochastic model for their coefficients, psi. Now, in practice, what happens is that you end up with a g naught term which looks like this. So it's the standard option shape or the shape of an options book and then there are some factors, g one. So here's our g one factor.
00:17:31.307 - 00:18:02.530, Speaker A: I'll explain how we get this in a minute. And here's the g two factor we get from this mark, which looks a little bit stranger. A thing to note is that the g one and g two, these stochastic factors here, they are always zero at maturity. This has the advantage of. It means that we can guarantee that at maturity, all options have the correct price. That's hard coded into the model. I'm going to build a two factor model.
00:18:02.530 - 00:18:56.684, Speaker A: This is a very simple thing to do, simple two factor model. And the reason for doing it is, if nothing else, it means I can draw some pictures and we can actually see what's going on in this neural network that's trying to understand this market. So what I've drawn here is, for these two factors, g one and g two, I've drawn, over our simulated historical path, the psi one and psi two coefficients you get from them. So here, these points, they lie in this sort of ribbon shape. There's a reason why it's got such a hard boundary, which I'll come to in a minute. But they lie in this broad shape, psi one and Psi two. Now, remember I told you earlier that no arbitrage or no static arbitrage, is equivalent to prices satisfying a collection of linear inequalities.
00:18:56.684 - 00:19:51.608, Speaker A: Now, because we have a linear factor decomposition, this means that we can convert the linear inequalities on prices onto a set of linear inequalities in the size. So here in red, we've drawn those linear inequalities. And what this means is that, provided our process always lives in the green region, which is described by that set of linear inequalities, any values of psi one and psi two inside the green region correspond to a set of prices which do not admit static arbitrage. And so we're able to hard code that lack of static arbitrage into the dynamics we're going to learn. For Psi, we want to build an SDe. So let's write down what it should be. Generically, we have a fairly open model for S.
00:19:51.608 - 00:20:45.894, Speaker A: We'll come back to this later. Essentially, we have a drift which depends on s and psi, and we have a volatility. Again, depending on s and psi, we can have a dividend rate or a interest rate if we wish, foreign interest rate. Then, for my Psi factors, I'm just modeling them as a simple SDE with a mu and a sigma term. And the idea is that alpha, gamma, mu and Sigma are all going to be parameterized by some neural nets, and then I'm going to use historical data to learn what those neural nets should be. Now, there's a few things that we could ask. Why is it sensible that I've written here what is effectively a Markov model? So why does it make sense? Well, here, while I'm going to try and fit this model under the historical measure.
00:20:45.894 - 00:21:27.626, Speaker A: So using real historical data. So that's under p measure. If I was thinking in Q measure, the Breeden Litzenberger theorem tells me that under the q measure, the joint process of s and all options on s must be a Markov process. Now, here I have a model for s, and through the size and the linear representation, the options on s, provided I've got enough size. It's not unreasonable for me to say that this is a Markov process in the Q measure. Now, of course not in the Q measure, I'm in the historical measure. So it's an assumption, but let's run with it.
00:21:27.626 - 00:22:15.620, Speaker A: Of course, one of the beauties of these things is if you had other information sources that you wanted to put into the model, that would be a very simple thing to do. If you had another process that you wanted to build in, you could just add it in as another term appearing in these different networks that you're going to try and learn. Now, I've also written here for s. I've written a very flexible model where I've got a drift in real data. Learning the drift of a soc is fool's errand, as anyone who's tried it will find out. So we fix alpha to be constant, and we don't allow dependence on s in any of these, which is effectively a scale invariance property of the model. So again, it's a restriction.
00:22:15.620 - 00:23:05.994, Speaker A: It can be relaxed, but it turns out it gives us much better results. Okay, so we are going to learn the model under the physical measure. I've spoken a bit about static arbitrage. Now, static arbitrage tells me that there is no portfolio that you can build right now, seeing the market prices, which will guarantee a reward in the future, positive reward with no possibility of loss. Now, that's not quite what we usually think about when we think of no arbitrage in mathematical finance. We're used to thinking of no dynamic arbitrage. So no dynamic arbitrage, fundamentally speaking, tells us that there's no dynamic trading policy which would result in an arbitrage.
00:23:05.994 - 00:24:19.032, Speaker A: Well, there's been many a lot of ink spilt over what does no arbitrage mean mathematically, but fundamentally up to technical qualifications? No. Arbitrage is equivalent to the existence of an equivalent martingale measure, in particular, equivalent to the existence of a market price of risk. So, let's write down what it would mean to have a market price of risk process. Well, for us, you can write down, because we've got our linear model over the Psi factors. This corresponds to there being a solution to this equation here, where g is the basis of my different risk factor terms. So if this equation admits a solution psi, that Psi is the market price of risk, and therefore I will not have any dynamic arbitrage up to assuming I need a solution, I need some integrability, et cetera, et cetera. Now, the z process here, that corresponds to taking the option prices, differentiating them using the Heath Jarrah Morton type conditions, and then calculating.
00:24:19.032 - 00:25:03.584, Speaker A: And that's what you get here. Now, the first thing you notice, though, is that provided Psi is invertible, and that's going to be an assumption we make for statistical reasons. We want to have the existence of a likelihood. So we're going to have Psi being an invertible quantity, an invertible matrix, sorry, sigma, not psi. Sigma, too many greek letters. So, if sigma is invertible, then the existence of a solution psi corresponds to saying that z should live in the corresponding linear space generated by g transpose. So, g transpose generates some linear space columns, though, and then z.
00:25:03.584 - 00:25:52.886, Speaker A: If z lives in the right linear space for g, then I have no dynamic arbitrage. I have the existence of a process, psi. This gives me a few different ideas on how I can build factors. So I'm going to say c are my collection of prices or normalized prices. I've divided by the discount factor, by the forward price, particularly, and Psi is the matrix of Psi processes. In this case, we can represent the observed data as this linear equation. So, prices, see, normalized prices is the constant factor, plus my risky factors, plus some error.
00:25:52.886 - 00:26:35.358, Speaker A: And there's three real ways that you could think about then choosing what is a good family of factors, g, you could try and go for statistical accuracy. Let's try and make this error on reconstruction as small as possible. We could do that either directly, just using least squares. We could do some reweighting. For example, it's natural to weight these by the black Scholes Vega of the options that you focus your attention on, locations where the implied volatility. Well, when prices are small, you want to make sure you match things more accurately so you don't get crazy implied volatilities. We could also aim to build factors which have minimal dynamic arbitrage.
00:26:35.358 - 00:27:36.074, Speaker A: So that means that I look for the z process on the previous slide and I find G's to represent the z process very well. And I also could ask for no static arbitrage. If I ignore the upsilon error term, I might want the reconstructed prices using just the first two terms on the right hand side to lie within the no static arbitrage bands. And what we do in practice is we use a combination of these three objectives to choose factors. So the two factors I showed you earlier, the first one is chosen to try and do minimal dynamic arbitrage. And the second to minimize the number of static arbitrages in reconstructed prices. All right, so if we go back to the basic picture here, what I've got now is I know my G's, my factors that give me a representation of prices.
00:27:36.074 - 00:28:12.730, Speaker A: With those G's, I have a process of risk factors, Psi moving around. And then I'm going to try and learn the dynamics for Psi. Now, the key thing about the size, as I mentioned, is that they should always satisfy this set of linear inequalities here depicted by the red lines. So I want to parameterize these by a neural network. So let's just think about the PSi for a moment. This is so neural networks, I'm sure pretty much everyone in the room will have seen a neural network before. It's the standard thing.
00:28:12.730 - 00:28:56.028, Speaker A: You get lots of things coming in. They get combined linearly, some nonlinearity. It all gets done many times on top of itself, and out comes some magic, and you can represent many functions with them. So what we're thinking is our inputs are s and the different size, our outputs are the drift and the volatility. Let's just focus on the PSi process for now. Now, the question is, is this neural network going to give me a drift and a volatility such that the process I've calibrated will live inside that set of linear inequalities? And that's the question that I want to answer next. So the answer in general is no.
00:28:56.028 - 00:30:03.146, Speaker A: If I just write down some SDE, it doesn't necessarily satisfy a given set of linear inequalities. And even if you give it a lot of training data, it won't necessarily reliably. You can't guarantee that you'll satisfy a set of linear inequalities. So what we had to do in order to calibrate this is figure out a way to force the neural net to give me an output where the SDE will satisfy this set of inequalities. Now, there's a classic work by Friedman and Pensky which tells you, well, if you have some region in space, particularly in this case, it's very simple. If you have linear sides to your region, so you just have a polytope, then the process will stay in the interior of this region. If for every boundary I take an inward normal VK, if the drift when I hit the boundary is always pointing inwards, the volatility when I hit the boundary is only allowed to diffuse along the boundary, never across the boundary.
00:30:03.146 - 00:30:42.204, Speaker A: And if those two conditions are satisfied, then for a Lipschitz SDE, your process will stay on the inside of the region. So that's the condition. It's really the most general condition around for these sorts of things. It's reasonable to assume that our neural network will produce lipschitz outputs. So what we need to do is guarantee this condition is satisfied. So this is not so easy to do directly, but there's an easy trick which, if we go back to the previous slide, we have our neural network, it goes along and does its magic. It produces an output mu hat.
00:30:42.204 - 00:31:33.964, Speaker A: I'm then going to pass this through a transform layer, represented by this square box, before getting my actual output mu. This transform layer is going to be built to ensure that this drift condition and volatility condition are satisfied. So whenever your process gets close to the boundary of this polytope, then the drift will always be pointing inwards, and the diffusion will only be along the boundary. The way we do that, if you're interested, we had to think about it for a while, because you need it to be done in a smooth way. You can still do backpropagation with everything. We needed to do it in a way that we could still actually code up easily in the tensorflow environment. I know we should be using Pytorch or other tools, but we started back using Tensorflow, and so that's what our code's written in.
00:31:33.964 - 00:32:20.476, Speaker A: So what we do is we actually define a region of space near the boundaries. And when you approach the boundary, we start adding drift towards a point which we choose on, if you like, the opposite side of the polytope. So we have a point living far away, the other side of the polytope, and we're going to start adding drift towards that point as you approach this boundary, and you can do that in a reasonably smooth way. This is the advantage, that when you approach a corner, you start adding drift towards both of the opposite points. And so you're always pointing in a convex way towards the middle of the region for the diffusion. It's the obvious thing to do. We just as you.
00:32:20.476 - 00:32:52.016, Speaker A: As the diffusion, as the process approaches the boundary, we just squash the diffusion down to zero in a smooth way by defining a function p, which squashes that normal component to zero. Oh, that's jumped a long way. Sorry about this. For all I said that I've learned how to use zoom. Always ends badly. Okay, so, once we have an STe, we can do various things. We can simulate with it.
00:32:52.016 - 00:33:25.726, Speaker A: We can also calibrate it. So if we just take a simple Euler Mariama approximation, then we can write down the approximate likelihood for our process. So we do that. So, here's the log likelihood written on the right for the Euler Maruyama approximation. You can add terms to encourage sparsity, the usual regularization terms one might have when using a neural net. For us, we found good performance when we just dropped half the values in our neural net. The smallest half of the coefficients get set to zero.
00:33:25.726 - 00:33:53.922, Speaker A: This makes everything work better. It's the black art of fitting neural nets. You just go and do it. And we also parameterized the drift in a simple way. The volatility we parameterized as a lower triangular matrix with exponentials on the diagonal to make sure that this never became singular, because if it becomes singular, then the likelihood explodes, and the computer says that it doesn't know how to work. So, there we are. So we.
00:33:53.922 - 00:34:22.174, Speaker A: Then you've got a likelihood. You run stochastic gradient descent, you try train this thing. There you are. It takes a little while, but not too long in practice. So where's all this data come from? So, we actually calibrated a Hestan SLV model. I think we calibrated it to some brazilian us dollar options because they were appropriately weird and interesting. So we thought that's where we'll see something fun happen.
00:34:22.174 - 00:34:43.676, Speaker A: We did this using the standard Quantlib library. So it's a Heston SLV model. Here are the dynamics, usual dynamics. Here are all the parameters you get out. The interesting thing, of course, is the leverage function, because you've got a local volatility process. Now, this is where you begin thinking everything's going to be easy. It's just the Heston SLV model.
00:34:43.676 - 00:35:34.320, Speaker A: But as soon as you have calibrated leverage function, you then realize that when you want to do forward simulations from this model, if the stock happens to drift outside the range in which your leverage function is defined, you have to extrapolate somehow. And it is not well documented what Quantlib does when you ask it to extrapolate. And it just seems to do very, very strange things. So in the end, what we chose for our experiment was we chose a path of s, which never leaves the range, 85 to 115. That hard boundary is why you saw that hard ribbon in the earlier plots. So that's an artifact of the fact that we chose a particular simulation. It also means that our simulation is not under a risk neutral measure, because you do get a definite bias.
00:35:34.320 - 00:36:12.444, Speaker A: If S starts to approach on those boundaries, it has a strong bias to return towards 100. So what comes out? Well, we can do in sample comparisons, we can fit a drift and a diffusion process. So this is the drift we fit. And what you can see is it still captures this ribbon. This is for a collection of points. And if you look carefully, you can see little arrows I've drawn, and they're all pushing in towards the center of this ribbon type manifold. As soon as you go up far to the right, they're pushing down further, and they're always pushing you along this space.
00:36:12.444 - 00:36:55.960, Speaker A: You'll notice that they are also, when we get close to these boundaries, they're never pushing outwards, they're pushing along the boundary, or they're pushing inwards, as we would hope to happen. Similarly, we can find the diffusion shown here with ellipses. You get a variety of things, obviously, mainly diffusing along that curved shape. But in particular, when we get close to these boundaries, the diffusion is forced to only ever diffuse along the boundary, never cross it. So this is just what you get from trying to calibrate this model. We can also then try and understand things a bit better. We can ask, well, the volatility of the stock, is that well calibrated? Actually, yes, it is.
00:36:55.960 - 00:37:44.152, Speaker A: So we plot here, this is the true simulation volatility process on the vertical. This is what our neural net says, and it's a reasonably good fit. Interestingly, this first factor, I suppose it's unsurprising. It corresponds very closely to a linear transformation of the new process, the volatility process, in the Hestan SLV. And once we realize this, this gives us something we can really compare with. If we say that Psi one and Nu are the same process, then we've got a base truth that we can compare our dynamics for psi one with. Okay, so using that as an approximate base truth, what do we see? Well, you can see that as a function of psi one.
00:37:44.152 - 00:38:23.994, Speaker A: We fitted the drift of psi one and you can see we get these black dots, which are lying fairly well along the red line, which is what you would expect if Psi one were actually new. The process in the Hess and SLV model volatility. Similarly for the diffusion, it's not quite right. There's a slight shift that's due to the linear relationship not being perfect. But you can see we are capturing that square root type behavior with some noise, admittedly, but that's what you get when you try and calibrate things non parametrically. You get additional noise. But we can see we are basically capturing the shape of the simulation model.
00:38:23.994 - 00:38:53.238, Speaker A: If we look at the market price of risk and the residual errors. The residual errors are fairly evenly scattered over our range of training data. The market price of risk is a bit interesting. It's quite low down here. When cy one is slightly below zero. That's where most of our data lies. We get quite big market prices of risk on the boundaries of this ribbon shape.
00:38:53.238 - 00:39:31.488, Speaker A: And also up here, that's where you're going to get the biggest effect of our s process being limited between 85 and 115. That's our broad expectation of why this is happening. While we're getting those big market prices of risk in our model, we can also look at a training history and out of sample performance. So you can see people like seeing loss history plots and see it looks like it's converged. Training loss is not doing strange things. There you are. I never find that I learn too much from seeing these plots, but people seem to like them.
00:39:31.488 - 00:40:02.612, Speaker A: We can also simulate from our model. So if we look here in blue, we have the training data. In orange we have simulated data. So our model does seem to have learned the broad shape. And if I look on the right here, I have a simulated path of s from the learnt neural network model, and a simulated path using the training hest and SLV model, and similarly for psi one and Psi two. And they qualitatively look okay. You can see that you.
00:40:02.612 - 00:40:31.964, Speaker A: We're getting the right sort of shape. Now, you can't do a path by path comparison here, because the Hestan SLV model uses different brownian motions to the neural network model. So you can't do a single path comparison to check what's going on. We can also look at the joint behavior. So this is the joint distribution of psi one and Cy two. And you get this sort of strange sausage shape. The ribbon that we saw is replicated.
00:40:31.964 - 00:41:17.064, Speaker A: You can also look at their individual distributions. We do seem to be fitting things reasonably well in the size space, and we can look at what the implied volatility looks like. So here I've got four different maturities. I've got moneyness, and I've got the implied volatility on the vertical axis. And then for a random selection of days, I've plotted what the implied volatility actually looks like. And what you can see is that this model has learned that the long dated implied volatilities, they really undergo just a parallel shift up and down. But if I look at short dated options, then there's a definite skew coming in and you get an asymmetric change in the implied volatility over time.
00:41:17.064 - 00:41:51.794, Speaker A: Okay, so let's see how this performs as a model for other portfolios. So we started with a set of vanilla options. So let's consider the VIX calculation. So if you look at how VIX is calculated, there are many definitions in the literature, but when you go to the white paper from CbOe, it's actually just a portfolio of options. So we use their methodology. We create this portfolio of options and we calculate what's going on. So here we use a VIX like index.
00:41:51.794 - 00:42:32.954, Speaker A: On my market, that's in the top plot, and on the bottom plot we have the log return of s. And when you see this, you go, well, that looks very nice. When the VIX is high, you clearly get more volatile changes in s. So we seem to be capturing the real behavior of the market. Sadly, when you plot the joint distribution, you don't get good performance. So if you look at the log return of s, you get a nice fit. On the other hand, the volatility doesn't fit particularly well, and it seems that you've ended up with very much a downward bias in the variance of the VIX portfolio.
00:42:32.954 - 00:43:22.708, Speaker A: Now, in a way that's not completely unexpected, I'm using a two factor model here to represent the market. Whenever you use a two factor model, you're compressing all the information just down to two factors, and that's not going to be as volatile as the real market. So this suggests that my two factor model isn't good enough to replicate everything. On the other hand, the relevant information does seem to be that. So if instead of simulating out all the option prices and calculating VIX from them, I use my training data, calculate VIX, regress that on the risk factors. Psi one, Cy two, simulate Psi one, Psi two going forward, and then compute the VIX from the regression. I actually get quite a decent fit for the volatility index and simulations.
00:43:22.708 - 00:43:53.752, Speaker A: So this suggests the problem is, I simply don't have enough noise in my model. My model's missing some of that extra volatility, but no real information is being lost. So what can we do? We could try adding further factors. Doesn't work. The model doesn't train particularly well when you start adding a lot of factors to it. Instead, what seemed to work quite well is saying, well, we've got our two factor model built from neural networks. That's the high powered part.
00:43:53.752 - 00:44:21.134, Speaker A: But we think interesting things are happening. Then I'm going to add things which are meant to be just noise. They're going to be built up of a polynomial in Psi one, Cy two. In fact, a cubicle polynomial and Psi one, Psi two, plus an Ornstein Uhlenbeck process. And I'm only going to calibrate the OU process parameters and that polynomial. I'm going to do no further training. Um, this.
00:44:21.134 - 00:44:51.494, Speaker A: Okay. It means that you might sometimes lie outside the no arbitrage region. If you just do this with a direct ou, you can project back in. It doesn't have much impact in practice because these factors are very, very small. So what we do is we build secondary factors just by running PCA on the residuals to build a collection of extra factors. Here we use eleven, because once you've got eleven factors, you reconstruct things without static arbitrage in your training data. So that seemed as good a reason as any.
00:44:51.494 - 00:45:29.904, Speaker A: And when we use eleven factors, what do you know? We get a good fit for the volatility index as well as for the returns on s. Okay, in the last couple of minutes, let's see what happens with some real data. So, the data I'm going to look at is from european options markets. So I'll use a combination of the euro stocks 50 and the DAX. These are both reasonably well. They're large cap european options. In particular, the european european options, and therefore, our no arbitrage theory all applies.
00:45:29.904 - 00:45:55.084, Speaker A: They're also very, very similar, the DAX and Eurostox 50. They're very, very similar in terms of the. At the money calls. If you look at the paths, why they look so very different through time. That's because the DAX has reinvestment of dividends, whereas the euro stocks does not. Okay, so we build here. We've tried building a two primary factor, three secondary factor models.
00:45:55.084 - 00:46:25.362, Speaker A: So, two neural network factors, three built just by Einstein. Ullenbeck processes. So here's our g zero, our very basic shape of the options book. Here's g one and g two. These are our neural network type factors, g. We assume everything oscillates around here fundamentally as noise. When we use these six factors, we get price reconstruction, which is within 1.33%
00:46:25.362 - 00:47:03.526, Speaker A: vega weighted prices. So that's fairly decent reconstruction of the book. And you get in 10,000 training points. We have five where the static arbitrage conditions are not satisfied when we reconstruct using these six factors. So what does our data look like? We get the same heuristic, the same in principle shape that we've seen before in terms of psi one, psi two. The set of no arbitrage conditions is slightly different just because of the different factors we get. But there you are, we train it, we get a learnt drift and diffusion.
00:47:03.526 - 00:48:00.640, Speaker A: Again, we still get this sort of curved shape and the drift is pushing you back in towards it. The diffusion, you get bigger diffusion when you're out on the right, lower diffusion on the left, and it also becomes parallel with those no arbitrage conditions. Points on the right correspond to the 2008 financial crisis, when implied volatilities went through the roof. So we have real data in blue, simulated data in orange, and you can see that there's, at least by eye, very little to distinguish between them. Here's the curve. Well, the plots of the density drawn a different way, and you can see the densities of our risk factors, psi one, psi two, psi three, four, five. We're getting quite decent out of sample fits from our simulated data using this model, and we can also look at what happens to the implied volume.
00:48:00.640 - 00:48:36.316, Speaker A: So, on the top, I've shown you two points for the DAX, two points for the Eurostox 50, the implied volatility surface observed on different days. And you can see there's a big range of shapes shown. I've used a mixture of DAX and eurostox. I could have used just one or the other. It wouldn't have made any real difference. So sometimes, for example, during the financial crisis in 2008, short dated options had very, very high implied volatilities. Usually, on the other hand, the short dated options here in 2017 were quite low implied volatility.
00:48:36.316 - 00:49:17.472, Speaker A: So you had a monotonicity in the iv surface. 2002, short dated were higher than long dated, et cetera. And you can see these different shapes then what I've done on the horizontal, on the lower line, is I've picked out the points with the closest corresponding size in my simulated data. And you can see that I'm replicating at least the qualitative shapes of the options book that we saw in the real data in the simulation. It's giving us that full range of different scenarios that we would hope to replicate. We can also see the volatility index. This is for the real data from the Eurostox 50.
00:49:17.472 - 00:49:55.436, Speaker A: This is simulated data. You can see, again, you get this localization where when the volatility index is high, you're getting higher, more volatile returns. And finally, we can see what our VIX index looks like. And the fit is not perfect, but it's qualitatively there. And for something that is learned just from historical data, we thought this is at least an interesting first step in trying to use these sorts of modeling for understanding options. Options books. What are we hoping to do in the future? Risk simulation for options books.
00:49:55.436 - 00:50:28.684, Speaker A: How do you calculate value at risk using these things? Can you get a sensible answer? Can we do hedging? And can we extend to other options types? Everything I've said here is about european calls. European calls are fantastic because they have a really, really well understood no arbitrage theory. You can write down linear inequalities. Sadly, even american options do not have this. And so we can't directly apply our method to the american options case, but we're hoping to think about how that might be done. Thank you.
00:50:29.624 - 00:50:44.824, Speaker B: Wonderful. Thank you, Sam. We do have some time for questions. I do see that there's one currently posted in the chat. Don't be shy, please. Please either raise your hand and we can. You can join in.
00:50:44.824 - 00:50:59.144, Speaker B: So that is Elnaz. I can't actually read as a sad Zedai here. I'm sorry, I butchered your name. Elnaz, are you here?
00:50:59.264 - 00:51:01.832, Speaker A: Oh, hi. Can you hear me?
00:51:01.928 - 00:51:02.928, Speaker B: Yes, we can.
00:51:03.096 - 00:51:32.224, Speaker A: Yeah. Sorry, it's a hard last name. It's Asad Zadeh. So I think I might know the answer now. I asked this question whether you used fixed lattice through time, but I think you used to fix lattice. We fix our lattice to be representative. But of course, you would normally then have an additional layer of interpolation on top of your lattice.
00:51:32.224 - 00:51:54.544, Speaker A: So we fix a lattice of points that we think is representative. That's going to give us a good understanding of the market. And then you would normally interpolate between them in practice. But yes, having a fixed lattice means that we can then have fixed risk factors going through time, and it makes our life a lot simpler. Thank you.
00:51:55.244 - 00:52:01.264, Speaker B: Great, thanks. Thanks for the question. JP, would you like to ask your question live? Yeah.
00:52:02.964 - 00:52:04.292, Speaker A: It'S a very simple question.
00:52:04.388 - 00:52:09.664, Speaker B: How do you choose a number of factors? Can it be something that you learn.
00:52:09.744 - 00:52:52.602, Speaker A: That you put in your learning scheme? So we haven't done it as part of, if you like, the online neural network learning. So we've kept the two problems separate. What we looked at was these three principles of trying to keep statistical accuracy. So reconstructing prices well, historically, not creating dynamic arbitrage, and not having static arbitrage in our simulated data, the latter two worked really well. They gave us good performance. So you create something which doesn't create dynamic arbitrage. And also, this is just a PCA is all you need to do to create it.
00:52:52.602 - 00:53:48.594, Speaker A: And then no static arbitrage, that's a non smooth optimization problem which you need to solve, but you can do it numerically. And we found that using a dynamic arbitrage factor, then a static arbitrage factor gave good performance on the real data. We just use statistical accuracy vega weighted, because so far, whenever we've tried to use the no dynamic arbitrage constraint, that involves some form of interpolation of prices, and it's just too noisy. It seems to give us a consistently good understanding of the space in terms of how many factors. It depends, really on how much training data you have. So here we've worked with 10,000 points to give us an order of magnitude. If you thought you had more training data, then you could use more factors.
00:53:48.594 - 00:54:22.394, Speaker A: On the other hand, it seems that markets are pretty well represented by two factors, plus some noise. That seems to be a pretty good representation, and that's unsurprising. We've been using Hestan SLV models for a while, and they work very, very well. So does sabre, and those are one noise factor type model. So it's not surprising that only having a couple of things is enough to capture at least the broad brushstrokes of what's going on in the market. Okay, thanks.
00:54:24.134 - 00:54:37.514, Speaker B: Any other. Other questions from the audience? You can maybe just unmute yourself and turn on your video if you wish to ask. Tom, it looks like you wanted to ask a question. Yeah.
00:54:39.534 - 00:54:40.918, Speaker C: The Tom or the other Tom?
00:54:40.966 - 00:54:43.876, Speaker B: Yeah, yeah, the current tom. Tom. Hi.
00:54:43.930 - 00:55:08.776, Speaker C: Yeah, now, just a question about the lattice you chose. I guess the real observed options are not on the lattice, so they're moving all around the lattice. So what kind of interpolation or sort of do you do to kind of. You're using that grid to track a bunch of moving points.
00:55:08.960 - 00:55:31.418, Speaker A: So for the simulated data, of course, we can just simulate on the lattice directly. That's the natural thing to do. For the real data, I would have to check. I believe we're interpolating the iv surface and using that to reconstruct. But I would have to go and check the code. Victor's been doing that and I need to check with him exactly what's going on.
00:55:31.546 - 00:55:42.244, Speaker C: So is the idea the number of lattice points should reflect the sort of number of actual traded options or heuristically, yes.
00:55:42.704 - 00:56:22.764, Speaker A: You want to have enough that you think you've captured the different changes in curvature and shape of the options book. And so if you have too few, then you're going to run yourself into trouble. You can have quite a lot because you then build factors from this. So you build the PSi factors or G factors, and then the Psi factors using these points, you can have quite a lot. So I think we have order 120 here. It also depends on the type of market you're looking at, whether it's OTC, whether it's exchange traded. So exactly what the quoting conventions on that market are.
00:56:22.764 - 00:56:39.410, Speaker A: So it may be there are markets where it seems things are traded very much in a money ness and time to expiry way, rather than anyone ever really saying, I want an option that expires on the 31 March. Okay. Yeah.
00:56:39.442 - 00:56:46.738, Speaker C: And then the g, I guess the G one g functions. Is that strictly a PCA principle?
00:56:46.786 - 00:57:18.398, Speaker A: Component analysis. So it's a linear expansion, you can build it from PCA. That's one of the principles we use. In fact, there we would use weighted PCA so that you weight the importance of a point by the Vega, because otherwise you end up with not fitting the iv very well. When you're very deeply in the money, particularly so. Sorry, out of the money. Out of the money options don't fit iv very well.
00:57:18.398 - 00:57:49.894, Speaker A: But if you vaguely, that works well. Otherwise we have these other things which try and capture the no arbitrage, no dynamic arbitrage condition, or that try and say the most important thing is that reconstructed prices shouldn't display static arbitrage. And so we use a combination of these different factors and in practice you try them and see for your market what seems to be needed to reflect what's going on. But certainly, yes, the basic idea is PCA. Thanks.
00:57:51.214 - 00:58:25.344, Speaker B: Okay, any other questions? Maybe I'll ask a quick one. So what was the question that I was going to ask? I was going to ask also about these G's, how you chose them. But maybe can you say something about, you know, you have these corners in this, in the, for the linear constraints, how do you deal with projecting at these corners? And if you were in high, if you had three factors and you have the problem with planes intersecting and so on. Is there any issue there or.
00:58:25.724 - 00:58:31.224, Speaker A: So the beautiful thing about the Friedman and Pinsky result is that it applies separately to each boundary.
00:58:32.444 - 00:58:33.140, Speaker B: Okay.
00:58:33.212 - 00:59:01.154, Speaker A: Interesting. So it's defined as the. They work with nonlinear boundaries, and then you have to be a little bit more careful to get the curvature right. But for linear boundaries, it's for each boundary. I need this to hold. And so what we do is, this is why we ended up using this idea. Instead of adding just a normal vector, we add a vector to pointing towards an opposite point, because we know our space is convex.
00:59:01.154 - 00:59:13.146, Speaker A: That's always going to be inward. Pointing is our idea, and it means that if you're at a very acute angle, you're not adding something which points you out of the space in a different direction.
00:59:13.290 - 00:59:14.346, Speaker B: Yeah, yeah, yeah, yeah.
00:59:14.410 - 00:59:20.626, Speaker A: I see. So that's for the drift. For diffusion, it just means you've got to compress both directions. Sure, sure.
00:59:20.690 - 00:59:21.466, Speaker B: Yeah, yeah.
00:59:21.610 - 00:59:22.346, Speaker A: That's all we.
00:59:22.410 - 00:59:29.258, Speaker B: Yeah. For the diffusion part, it's. It's quote unquote easier. Right. It's the drift that was the concern. Yeah, yeah. Okay.
00:59:29.346 - 00:59:38.134, Speaker A: And. Okay, so this is why we. Why we had to think about the convexity of the space being important in terms of choosing how to fix the drift function.
00:59:38.494 - 00:59:39.918, Speaker B: Okay. Makes sense.
00:59:40.086 - 00:59:40.414, Speaker A: Great.
00:59:40.454 - 00:59:56.854, Speaker B: Well, why don't we wrap up the formal part of the talk and say thank you very much for. To Sam. For, you know, first of all, giving the talk here and coming to fields virtually, and also for doing it quite so late. I know it's pretty late there in. In Oxford. Thank you very much. We very much appreciate it.
00:59:56.854 - 01:00:05.254, Speaker B: And I'll stop the formal part of the recording. So, virtual claps and let's see.
