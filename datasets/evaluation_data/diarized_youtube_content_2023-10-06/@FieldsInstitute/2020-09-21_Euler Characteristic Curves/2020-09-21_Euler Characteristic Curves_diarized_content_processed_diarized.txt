00:00:01.320 - 00:01:03.134, Speaker A: Hello, I'm David Iguornari, and I'm going to talk about algorithms for computing Euler characteristic curves. This is a joint work with Pavel Docklo, and before starting, I would like to thank the organizers for giving me the opportunity to share our work. I really appreciate it. So the goal of our project is to develop fishing techniques to compute the Euler characteristic curve of simplicia complex, and you want to use it as a shape descriptor of our data. So let me briefly introduce the main mathematical structure that we're going to need, especially an abstract simplicial complex, which is a finite collection of sets, such that every subset of a set is in the collection itself. We call the set simplices, and their dimension is their cardinality minus one. We can also define a filtration over a simplicial complex k as an ordering of its simplices, such that all the prefaces in the ordering are sub complexes of k.
00:01:03.134 - 00:02:36.114, Speaker A: This means that given two simplices, sigma and tau, if sigma is a face of tau, Sigma will appear before tau in the ordering. We can use a monotonic realvalue function to give such an ordering, and moreover, the monotonicity implies that all the sublevel sets of the function are sub complexes of k. We can obtain a geometric realization of an abstract simplicial complex starting from a point cloud by using a vitoris strip construction. Given a parameter epsilon, we create the vitorius strip complex as the collection of all the subset of points having diameter less or equal to epsilon, where we indicate the diameter of a subset as the maximum distances between any pair points. Moreover, we can use the diameter of its simplex as its filtration. We are interested in computing the Euler characteristic curve of a simplicial complex k, defined as the alternating sum of the cardinality of the sets of its and simplices. From the Euler point career formula, we know that the Euler characteristics of a simplex is also the alternating sum of its petty numbers, and since homology group do not depend on the triangulation, the Euler characteristic is indeed an invariant of the space and do not depend on the specific simplicial complex we used triangulated.
00:02:36.114 - 00:03:47.956, Speaker A: So over here you can see two different triangulation of the annulus which has Euler characteristic equal to zero because both is zero and its first petty number are equals to one and all the other ones are zero. And so you can see that the Euler characteristic is the same for both simplicial complexes. So we can define the Euler characteristic curve for a filtered simplicial complex as a function that assigns an Euler number at each filtration level. So in other words, in a function that average filtration levels returns Euler characteristic of the corresponding sub complex. So each simplex contribution to the Euler characteristics will be plus or minus one, depending on its dimension, and it will appear at its filtration level. So the idea is that if we are able to obtain a list of all the simplices ordered by their filtration, we can reconstruct the Euler characteristic curve by simply summing up all the contributions. But unfortunately, listing all the simplices may require exponential time.
00:03:47.956 - 00:04:37.384, Speaker A: So this approach is not very feasible. But what we want to do is to exploit the fact that the Euler characteristics is addictive. And so this means that we can compute the contribution locally and only at the end merge them together. So what I will present you now is an online algorithm that allows us to compute all the contribution to the other characteristics without having to explicitly build up the whole Vitoria strip complex. So, let me start with an example. So we consider a collection of points, and we suppose that they are ordered, so in this case ordered from one to g, from a to g. And the order is very important, because what we want to do at each iteration is considering only one point and the points that came after it in the order.
00:04:37.384 - 00:05:42.026, Speaker A: So let's start from a and over here we just need to focus on over its epsilon neighbors. So in this case only b and d. And so we construct a local victorious rib complex with a, b and d, and we add into the list of the local contribution the simplices that are in the star of a. So in this sample are the right ones. So this local contribution will be four. We will have a plus one at zero filtration level given by the single point a, then two minus one s a different filtration level at the level of the edges ab and ad, and then plus one the filtration level of the triangle a, b and d. We can go on, we move to b, we find its epsilon neighbors and we create another local vital complex.
00:05:42.026 - 00:06:40.224, Speaker A: Note that in this case we are no longer considering a, because a is before being de ordering. So we build up a local complex made only by b, c, d and f. And so over here we take the star with respect to b, and we have another list of local contributions. So we can repeat this procedure for every point at the end. When we reach the last point, only the contribution from the point itself will remain and by doing so we are guaranteed that we are counted each simplest contribution only once, exactly once. So we have this list, we can simply order it by the filtration value and we can reconstruct the order characteristic curve up to a filtration value epsilon, as you can see over here. So this is the algorithm that I just explained to you with an example.
00:06:40.224 - 00:07:51.416, Speaker A: So we take as input an order point cloud and a positive value epsilon and we output a Dorder list of pairs with the first number is plus or minus one and then the corresponding filtration. I can show you some example and over here I compared the algorithm that I just described against a brute force clicks counting over the full one skeleton graph of the full victorious rib complex. And as you can see our error metal is quite faster. So this is for example 20 random points in the unit circle and then 100 random points in the Unix cube. And as you can see the number of simplices, well it grows very fast and we can do more. We can implement a special search data structure to speed up our computations. The reason for this is that at each iteration of our algorithm we consider a point and we built a local VR epsilon complex around it.
00:07:51.416 - 00:09:00.934, Speaker A: So when doing that we do not need to check all the other points, but we are actually only interested in those that are distance less than epsilon. So how we do that, we start by building an overlapping cover of our input cloud. We first pick a point and we construct a ball of radius epsilon centered on it and we flog all the other points that are inside it. Then we choose a point that is not covered and create a second ball and we keep on repeating the process till all the points are covered by at least one ball. So this is the greedy algorithm that I just described that takes as input a point cloud and a positive value epsilon and returns a cover vector. So these data structures has two main advantages. The first one is that when constructing a vitarious rip complex for a point covered by a certain ball, we will need only to consider the points covered by the balls that are distant less than three epsilon from our current ball.
00:09:00.934 - 00:10:15.064, Speaker A: As explained in the picture, the limit case for a point in a ball that can be an epsilon neighbor of a point in our second ball is that the two centers are at distance less than three epsilon. The second advantage is that we can divide our dataset in batches, one for each ball, and we can then run multiple instances of our algorithms, one for each ball. So we run some experiments using a 20 CPU course of the Sunbird computer cluster from Swansea University. We computed the Euler characteristic curves contributions for a vitoris rip complex constructed from 50,000 points sampled from the fourth sphere in five reals. So in the first case we built simplices up to a maximum dimension of five, while in the second one we put no constraints. So we report the number of simplices that we constructed as k the number of balls in our data structure, and as you can see they are around 2000. And then in tb min we report the average running time of our algorithm for each bow.
00:10:15.064 - 00:11:29.414, Speaker A: Moreover, we compare also the total running time for a sequential version of the algorithm using the described data structure as in TBTOT, against the same algorithm that does not use this ball based data structure. And as you can see, there is a great improvement also in the running time because of this improvement in the spatial search. So in the last minutes I want to show you some classification experiment where we use the Euler characteristic curves as a feature. So the first one is a toy example where we try to classify shapes. So we created an artificial dataset with 3000 point clouds, each made of 100 points in 3d, that was uniformly sampled from one of three dimensions, a sphere, a torus and a box. And we also added various level of gaussian noise. So for each of them we computed the Euler characteristic curve by constructing a vitorious ribs complex up to a filtration value of 0.5.
00:11:29.414 - 00:12:36.614, Speaker A: And then we vectorize them by sampling 100 times at constant step size between zero and 0.5. And we use these vectors as input for a support vector machine classifier. So this is on the left are the curves for the noiseless datasets, and on the right you can see the mean accuracies we were able to obtain. And it's quite good in our opinion, up to levels of noise that are comparable to the maximum filtration value. So in the second experiment we tackled the graph classifications. So following the approach by oftener and collaborators, we can define a filtration for an undirect graph based on the vertex degree. So we set the filtration for each vertex as its degree, so the number of connection it has, and we move it up to higher dimensional simplices by taking the maximum over the vertices.
00:12:36.614 - 00:13:51.734, Speaker A: So also in this case, of course, we obtain a family of nested complexes and we can compute the Euler characteristic curves. So we consider datasets made by large collection of labeled graphs, in particular social graphs obtained by interaction over Reddit. And as before, we vectorize the ACC for each graph and we use them as input for a feedforward neural network with two hidden layers. And so as here you can see the results we obtained for three different versions of these datasets. So the radit is a binary, the first one is a binary datasets with two classes, the second one has five classes and the third one has eleven classes. So we are quite happy with the results that we managed to achieve because we have similar performances to Hofner and collaborators that used a deep learning approach with the whole persistent diagram and we managed to outperform graph kernel based methods. Okay, this was just the beginning and there is a lot left to do.
00:13:51.734 - 00:14:36.624, Speaker A: So first of all, we want to extend our algorithm to work with general CW complexes, and not only with victory risk complexes obtained from point clouds in RN. Then we plan to implement bootstrap analysis to define confidence interval for the Euler characteristic curves, and we want to keep testing it on real world high dimensional datasets. In particular, we want to keep investigating the application to network analysis for which we want to explore morphine function than just the one defined on the vertex degree. So this is the end for now. Thank you very much for your attention and please drop me an email if you have any questions.
