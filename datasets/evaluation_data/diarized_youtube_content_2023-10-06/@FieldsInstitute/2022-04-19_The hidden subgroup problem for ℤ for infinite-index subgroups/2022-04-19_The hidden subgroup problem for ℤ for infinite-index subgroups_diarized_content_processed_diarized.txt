00:00:00.560 - 00:00:36.470, Speaker A: I am very happy to introduce Greg Cooperberg, who will be talking to us today about the hidden subgroup problem for z to the k for infinite index subgroups. Thanks, Greg. Okay, so I didn't. Yeah, thanks for. Thanks for inviting me to this workshop. It was just said that the, that the workshop has a quantum side and number theory side, and we're hoping for an intersection. I'm definitely more on the quantum algorithm side, but I gather that the sort of thing that I'm talking about is, is of interest in algorithmic number theory.
00:00:36.470 - 00:01:09.684, Speaker A: And, in fact, part of my. Part of the point of this talk is to, is to make an appeal to see, to see what the interaction can be. Okay, so sorry. The eprint is just in preparation. It's kind of a long e print with a bunch of results. Well, it'll come. So I want to start by discussing the hidden subgroup problem, and this is going to be a little bit unnecessary, but I'm going to try my underlining facility.
00:01:09.684 - 00:01:29.124, Speaker A: That's not the right color. I'm going to try with that color. Well, that's not. I don't know about that color either. Oh, here we go. Okay, well, fine, whatever. Anyway, so just in general, there's a general class of problems for which you can ask whether there's a quantum algorithm.
00:01:29.124 - 00:01:53.422, Speaker A: You have a discrete group, g. In fact, not just any group, but I mean, whoa, I don't want that font size. So I'm having all the completely different technical problems here. Okay. Okay. So, in other words, I don't want just a discrete group, but I want an algorithmic group. Oh, good grief.
00:01:53.422 - 00:02:13.306, Speaker A: Okay. I really. Okay. I really having. Okay, whatever. I'm gonna, I'm gonna deal with goofy font. Anyway.
00:02:13.306 - 00:02:56.796, Speaker A: I want an algorithmic group, okay? And so, in other words, there are algorithm. There's a system of names for the group elements. You can tell just by the names whether the group elements are equal. And there are algorithms for the group law and the group inverse and all that jazz. In the case of interest, that's not going to be much of an issue anyway. You have a function f from g to an unstructured set, which is periodic relative to which can itself be computed in polynomial time. So the hidden subgroup problem has functional input, and it's h periodic with respect to an unknown subgroup H.
00:02:56.796 - 00:04:06.650, Speaker A: And this is also key here. It's otherwise injective, okay. That you need both properties, h periodic and otherwise injective. So then the hidden subgroup problem is the computational problem of finding h given f as functional input. As I said, so, more explicitly, to say that f hides h means that f of x equals f of y if and only if x and y differ by an element of the hidden subgroup. Okay? And actually, for this audience, I'm not sure that I needed to spell it out, but I'm going to mainly discuss additive subgroups, even though in this slide, I wrote it in multiplicative form, you have commutative and non commutative cases of this problem. So, since you have functional input, you can't discuss polynomial time in the usual way.
00:04:06.650 - 00:05:13.580, Speaker A: Polynomial time, the bit complexity of the input. The input is a function. Instead, you rate the hidden, the performance of an algorithm by the bit complexity of the output. Okay? Now, one of the spectacular cases, which is going to be the top this talk is related to, is the case when g is z to the k. So that's additive. And the hidden subgroup H has some finite index, okay? So in other words, it's, it's not just periodic, but periodic with maximum rank. And then the theorem of shor, refined and extended a bit by kataya, is that you can calculate the hidden subgroup in quantum polynomial time.
00:05:13.580 - 00:05:46.212, Speaker A: So in other words, in the complexity class BQP, not just in quantum polynomial time. So when I say calculate h, do I mean find a generating set? Yeah, I mean find a generating set. Okay? In fact, you can even find a canonical, canonical free generators. You can find a lattice basis, and you, and there are ways to canonicalize it as well. So, for instance, with Smith, normal form. And anyway, it's not just for each. Yeah.
00:05:46.212 - 00:06:25.024, Speaker A: So I guess I can write that in. That is find the basis for age. Okay. Even a canonical basis, if you want. And it's not just polynomial time in each fixed dimension for in the bit complexity of the description, but it's actually uniformly polynomial time in the dimension as well. Maybe that's partly the, maybe that's Katayev's contribution, actually. So anyway, it has this corollary in this generalized form from what Shor originally says.
00:06:25.024 - 00:07:16.526, Speaker A: It doesn't matter where the heighting function comes from. It's a very powerful algorithm that way. But the corollary is that if you, in particular, you can set the target of the heighting function to be another finite group. So if a is an algorithmic finite group, then you have the classification of finite abelian groups. They're all products of cyclic groups. And that the isomorphism to the canonical form, which is a discrete logarithm map, can be constructed and evaluated in quantum polynomial time. In other words, you can find not just the order but the divisors, the isomorphism type of a, and then you can get a map from a in the difficult direction from a to its isomorphism type and then evaluate that in quantum polynomial time.
00:07:16.526 - 00:08:19.708, Speaker A: So basically you have learned everything about a a, okay, you've killed all mystery no matter where a comes from. Elliptic curve, prime residues, mod n, anything like that. Okay, so this corollary in particular has many applications to algorithmic number theory and public key cryptography. Now, most of the algorithms for the hidden subgroup problem, other than short Kataev, I won't say all, okay, assume that the ambient group g is finite, and here are various positive results where you do get quantum polynomial time algorithms. Not all of them have applications. Some of them have applications to algorithmic number theory and some of them don't. In fact, some of them don't even have any known challenging applications at all, and some of them do.
00:08:19.708 - 00:09:26.764, Speaker A: But in any case, there's been various work, and this is where most of the progress has been. And in the finite case, the HSP is known to always have finite, sorry, polynomial quantum query complexity, but it still looks hard in other cases such as the symmetric group. But actually that's all a separate topic. The result that I want to talk about came from meditating on the case when g is infinite and discrete. Okay, okay. And really mainly my project was about negative results at first, okay? And just as this slide is kind of an aside, but I got the following negative results which are also of interest. If our ambient group is the rationals under addition, just with the discrete topology, with the standard notation for rational numbers, then the hidden silver problem is NP hard.
00:09:26.764 - 00:10:22.824, Speaker A: And actually I should just mention, my slides aren't great about this. Until the end, I want to compare with a case that I didn't define where the ambient group can be a lie group or r to the k. That was settled and is important for algorithmic number theory and cryptography. The first case was done by Halgren and the second case was eisentrigger, Holgren, Katayev and Song. Okay, that's actually important for this whole story, but I'll just get back to that at the end. For now, let's just worry about discrete groups. Okay, discrete infinite groups, actually, by contrast to the finite case where there was greater attention despite that shortcut, is for an infinite group case.
00:10:22.824 - 00:11:41.794, Speaker A: So if you have a non abelian free group where the elements are encoded as words, then the hidden subgroup problem is NP hard even for normal subgroups. That was another thing that I found. And finally, if g is z to the k, which is the case of short Katayev, and even if h has finite index, which is also the case of short kataev. But if you make this change, if you change the encoding of elements in z to the k from binary to unary, in other words, what's really meant there is that the query cost is pseudo polynomial rather than polynomial, then the hidden subgroup problem is as hard as the unique short lattice vector problem. So that's what I thought my contribution was going to be, and I was ready to put in a background section that short Kotayev completely solves z to the k with standard binary encoding vectors. I thought the good news was already there in literature. Well, it mostly is a shortcut is surely the, I mean that they looked at the case that has known applications.
00:11:41.794 - 00:12:30.210, Speaker A: But then when I look more carefully, actually it's not always spelled out completely. It only works when the hidden subgroup has finite index. Okay. And certainly in my counterexample constructions I didn't for, particularly for the first two, I did not just restrict attention to the finite index hypothesis. Okay, so, well, I wasn't sure what to do about that, but happily I figured out how to extend short kataev to the infinite index case. So this is the new positive result, which is what I mainly want to talk about. G is z to the k with standard binary encoding.
00:12:30.210 - 00:13:45.124, Speaker A: Well, the specific base doesn't matter, but just not unary encoding of the vectors. And h is a lattice with any index. I really should have put rank here, or rank then h can be found in quantum polynomial time uniformly in the dimension and the bit complexity. The answer, and to address Victor's good question again, it can be a generating set or even lattice basis, or even a canonical lattice basis. So the new algorithm begins the same way as shorkataev, and in some respects the same way as all of the quantum algorithms for the hidden subgroup problem. But it requires new ideas at a stage of the algorithm that is kind of almost an aft, just like after the climax for short kataya itself. What is meant by binary encoding? Well, just the usual way that you have to.
00:13:45.124 - 00:15:14.224, Speaker A: So binary or digit encoding versus unary, the specific base doesn't matter, but I can write a vector like that versus eleven one one. Just okay. The binary unary encoding is much more verbose, so that you have access to a limit, a much more limited amount piece of the ambient group z to the k. When I say uniformly in k, doesn't mean that the polynomial time algorithm depends on the full group g. Well, look, the algorithm is given the dimension of the lattice that you're working in, you're given k, you're given a hiding function, and then the question is polynomial time, how polynomial time separately for each dimension k, or polynomial time for all dimensions together. This distinction is important for lattice algorithms in general. If you take a problem like the short lattice vector problem, that is a polynomial time problem in each dimension separately.
00:15:14.224 - 00:16:13.954, Speaker A: On the other hand, short vector is recognized as a hard problem. How? Well, it becomes intractable as the dimension increases. So short vector is a good example of something of a question that classically, people have understood this for a long time, but now there's a recognition that it could be true quantum Lee at all, quantum Lee as well. It has an efficient algorithm in each fixed dimension, but it does not have an efficient algorithm. It does not have a known efficient algorithm uniformly in the dimension. Okay, now, and by the way, actually, my algorithm, even though it does achieve uniformity in the dimension, it is of interest even in dimension two. There's even something new to do when k equals two just in the discrete lattice plane.
00:16:13.954 - 00:17:10.614, Speaker A: In the integer plane, it's already doing something interesting. Now, unlike Short Katayev, I don't know of any challenging instances of hiding functions. You can make a lame hiding function by putting the hidden subgroup directly into the algorithm to compute f. The reductions actually have some of that, but have that kind of thing. But if you were really to make any of these quantum algorithms useful, you should have hiding functions, such as, in the case of Shor's algorithm, you have modular exponentiation, or the group law and elliptic curve and so on, where you don't know the hiding function, you don't know the hidden periodicity ahead of time. That's the computational challenge. So that's called a challenging, I'm calling that a challenging instance.
00:17:10.614 - 00:18:18.996, Speaker A: And certainly, I don't even care whether it's actually something useful for number theory yet, or cryptography. But it's not going to be interesting for number theory or cryptography unless it's at least a challenging instance of some kind. And you might use number theory to try to come up with a challenging instance for any of these hidden subgroup problems. But I'm just going to cheerfully conjecture that challenging instances exist, maybe even number theoretic ones. After all, that's short Katayev has many applications, such applications. Okay, so now here is an outline of how the algorithm works, and you can take it also to some extent, as a review of shortcut itself. Okay, so suppose that our hiding function hides a sub lattice h of some rank luck, less than or equal to k, the less than case is more than more important.
00:18:18.996 - 00:18:54.644, Speaker A: Maybe I could have put strictly less than. Okay, now I'm going to choose two enormous parameters, exponential scale parameters q and s. Actually, in practice, they're redundant. S is not needed for the. It doesn't. I don't think it helps the algorithm itself, but it does help the proof that the algorithm works. Okay, so using these two parameters, I'll follow a version of the standard quantum opening for this hidden subgroup problem, or for many hidden subgroup problems, it's sort of similar.
00:18:54.644 - 00:19:55.004, Speaker A: Okay, so step one is to prepare an approximate gaussian state on a large cube of size q over two in the lattice space, z to the k. Okay. And to underline that s is needed for the proof more than for the algorithm itself. It could have been a flat pure state without s. Okay, probably I just needed. The gaussian thing is useful for, for the rigorous estimates. So then apply the hiding function f to this gaussian state in unitary form, unitary form of the hiding function.
00:19:55.004 - 00:21:36.784, Speaker A: So that whereas the input is a superposition, quantum superposition over lattice vectors, when you apply the hiding function, you get a superposition of inputs and values of the heighting functions together, and you have these gaussian coefficients, which, as is said, are probably, I'll conjecture that the gaussian shape is optimal, but it certainly is useful. Now, there's some funny things going on with all hidden sub quantums, hidden subgroup algorithms. What you do with this evaluation of fucking. First of all, the algorithm would be wrecked if you left behind scratch work, or if you erase scratch work in the usual way of throwing it in the, resetting the qubits that have the trash work, exporting the environment, that kind of thing. If you did anything like that, then the algorithm would, the, the quantum part of the algorithm would be rect. So the trick that goes back to Shor's paper is to use uncomputation to erase scratch work in the, in the same sort of way that, that when you solve a Rubik's cube, you often use conjugates and commutators, particularly the commutators undo part of what a primitive in the Rubik's for solving the Rubik's cube does to get rid of the side effects of part of the rubik's cube move. So the commutator has a controlled effect.
00:21:36.784 - 00:22:14.170, Speaker A: So the uncomputation is a lot like a group commutator, and it'll get rid of any scratch work you use to compute the hiding function. But now the other funny thing that's going on, and this is not specific to my work. This is the way that all these algorithms work, is that after computing the output, you throw that away, you don't compute it, you export it to the environment. In effect, it doesn't even matter whether you export it to the environment. You never use the output qubits again. You never use that register again in the algorithm. So you could not throw away and say you did, you can measure it.
00:22:14.170 - 00:23:00.602, Speaker A: You can not measure it and say you did. You can leave it to the custodial services to measure those qubits that have the value of the hiding function. In any case, you don't use it because actually you don't understand it. Even though you do take a query in quantum superposition, you have not probed the hiding function enough to be able to decipher, in the full generality, the target set x or its values. So that gets left alone. Now, it's convenient, it's not actually part of the algorithm, but it is convenient for the analysis to go ahead and measure the value of the hiding function. And what you get is a partially measured input instead.
00:23:00.602 - 00:24:27.914, Speaker A: And that's what the algorithm, the rest of the algorithm works with. Okay, it's the same quantum wave function, if you like, but restricted to a coset of the hidden subgroup h, because that's the inverse image of a given value of the heighting function. Okay, again, these, both of these steps are, it could, you could just drop the, except maybe sometimes for the gaussian shape, you can just drop them in to standard shortcut, for instance, step three, also standard enough, since the, it can't be a complete gaussian in all of z to the k. Because quantum computers are finite, you would need infinitely many qubits to encode a superposition with unbounded integer vectors. So we're on a finite cube of size, big q, it can be exponential. You only need log q qubits to encode a vector x or times the dimension. But still, you are on this finite cube, and I'm mapping the finite cube here to actually a discrete torus z mod q to the k.
00:24:27.914 - 00:25:15.514, Speaker A: That's a gauche step. I mean, embedding z mod q into z is what I call a gauche embedding. That also goes back to shore. It's not a group homomorphism, but it's still what you do. But anyway, having moved this valuable, partially measured quantum input to z mod q to the k, you apply the quantum Fourier operator. That's another piece of, that's another quantum algorithm thing, which is not due to me, but is by now standard to this partially measured input. And you measure a Fourier mode, why not in the dual finite abelian group z mod q to the k.
00:25:15.514 - 00:26:28.190, Speaker A: Now, I want to interpret this as a Fourier measurement in the dual of z to the k, not z mod q to the k. And at least as interpretation, all you have to do is divide by the modulus q, the modulus of the qft quantum Fourier transform q. So just by rescaling this Fourier mode that you measured, why not you get a Fourier mode similar to a Fourier mode. Anyway, y one in the continuous torus r mod z to the kitchen. Okay, so this vector y one is approximately a randomly chosen element of the dual group h sharp. What is h sharp, explicitly? Well, it's the pontrial dual of z of the lattice mod, the hidden subgroup, which is then a subgroup of the torus r mod z to the k. And so explicitly h consists of those vectors y that are orthogonal to everything in this hidden subgroup, h.
00:26:28.190 - 00:27:03.184, Speaker A: Well, orthogonal how? Okay. Well, a vector x in h is an integer vector. A vector y here is a torus vector. But you can multiply an integer by an element of the circle to get something. To get an element of the circle, if you lift y to a real vector, the condition is that x dot y should be an integer. Okay. Or I could have said equals zero in r mod z in the circle r mod z.
00:27:03.184 - 00:27:27.424, Speaker A: That would have been equivalent. So h sharp is perpendicular that way. And that's why I'm calling it h sharp rather than, for instance, h perp. I could have called it that. It's a type of perpendicularity. In any case, it also generalizes the concept of a dual lattice. It's related to that concept, I shouldn't say generalizes.
00:27:27.424 - 00:28:05.786, Speaker A: Now, the sample y one has noise for two reasons. It has noise due to gaussian blur, because the input had gaussian shape, and it has noise due to discretization, because basically, this finite discrete Fourier transform was used as an approximation to the full fledged quantum transform from little l two of z to the k to big l two of this torus. It'll have noise. From both of those cases. The noise is exponentially small. That's good. But the feature scale of h can also be exponentially sharp.
00:28:05.786 - 00:28:47.886, Speaker A: That's bad. Potentially a challenge. Okay, so I have a picture of what happens, except not at the exponential scale, where things are actually going. So I drew moderately big subgroups, h sharp, but you should just imagine that they're much, much more complicated than what's indicated. Okay, so on the left, here's what happens when h has full rank. So, I'll go back to the definition. If h has full rank, then this quotient that you're taking, the pontrial and dual of, is a finite group.
00:28:47.886 - 00:29:48.074, Speaker A: So, h sharp, if h has full rank, then h is a finite group whose cardinality is the index of hook. And I've drawn a picture of that. I think this was 26 dots. So h is some subgroup of z squared with index 26 in this picture, which then means that h sharp is a group with 26 elements. So there is my approximate sample of some element. Now, on the right, when h has rank, I gave h rank one and the co rank of h k minus the rank l of h is the dimension of h sharp. So h then is a striped pattern when h has deficient rank, whose connected subgroup is some very complicated torus.
00:29:48.074 - 00:30:21.044, Speaker A: The slope of this torus is something that you don't get to know. If you did know that slope, I mean, you look at this picture, you think, well, there'll be a reason to try to, like, rotate it around. I'll get to more than just a second. But in any case, to emphasize this, now, you don't know the spacing, you don't know the space, the slopes of these stripes either. And I'm drawing a two dimensional case, which is already of interest. But you should imagine a hundred dimensional picture version of this picture where maybe a sharp is 50 dimensional. Okay.
00:30:21.044 - 00:31:37.804, Speaker A: All right. Now I want to find h sharp, find a complete description of h from these random samples, which are also noisy. Okay, so did Kataev, he more or less spelled it out explicitly. If H has full rank and h sharp is finite, then back here, what you do is you just make sure to make big q in particular, big enough that there's not very much noise so that Y one is as close as it needs to be to this lattice point. There may be to this element of H. There may be exponentially many of them, but you can swamp that problem with a much bigger exponential, with a bigger exponential for the scale of the algorithm q and q and q and s, because you only need a logarithmic number of qubits for this to work. So if there's little enough noise, then you use the continued fraction algorithm to find rational approximations to each coordinate of y one.
00:31:37.804 - 00:32:39.694, Speaker A: And then you know that you have elements of h one, h sharp, rather, and probably a logarithmic number of samples. Generate H sharp with high probability, a logarithmic in the, in this exponential cardinality, which is therefore polynomial in the bit complexity of the answer. How many samples do you need? Well, in the traditional textbook expositions of Shor's algorithm, this is actually the one dimensional case. H is a sublattice of z itself. H sharp is a finite cyclic subgroup of the circle. And two samples will give you the period, little hook, the integer period little h, probably with good probability. Even if you just have one sample, you'll probably get a large divisor of little h.
00:32:39.694 - 00:33:16.944, Speaker A: It's good enough in higher dimensions, you need polynomially many samples log of this exponential thing. Anyway, it works. It's been recognized for quite a while. Now. There's this dilemma. If h has deficient rank. And so I guess what I want to say, I was alluding to this before this continued if you know enough number theory, you look at this continued fraction algorithm step and you say, oh yeah, yeah, you use the continued fraction algorithm to approximate a noisy rational, to remove the noise from a noisy rational number.
00:33:16.944 - 00:34:20.746, Speaker A: Peter Shore's paper was brilliant for pulling in common wisdom from so many different directions. But in algorithmic number theory, the continued fraction algorithm is not that big of a deal. And it may read as an astronaut that one thing may read as an afterthought in the short kite of paper. But if h has deficient rank, well, okay, then the dimension of H sharp is the difference in ranks, k minus l. And now going back to the picture, any one coordinate of y one is uniformly random, maybe plus some noise and some discretization. But if it didn't have any noise at all, it would just really be flat random. So that the continued fraction algorithm is junk for removing the noise, you think, oh, okay, well, let's just do a change of coordinates.
00:34:20.746 - 00:35:17.528, Speaker A: Or at least I thought, maybe I can do a change of coordinates. I can rotate these stripes to then use the continued fraction algorithm. But if you knew, but for instance, it's interesting if h is its own rational closure, so that then h is equal to h one sharp. It may happen that H is even just connected, so that if you knew the slopes of these lines, you would just know the hidden subgroup immediately. And it actually turns out to be circular to ask for these slopes. To try to figure out how to remove the noise from y one, you should imagine a Google stripes at some complicated slope with 100 digits. And no, you don't get to know that from one sample or even from a polynomial of number of samples, not unless you do some real work.
00:35:17.528 - 00:35:56.394, Speaker A: You just cannot use the continued fraction algorithm. So rational approximation of the coordinates does not work. There's no obvious change of coordinates ahead of time. However, the LLL algorithm does work. And part of the reason that I arrived at this is actually that it's commonly said that the LL algorithm is a higher dimensional version of the continued fraction algorithm. That's not, that's a bit simplistic, but you can interpret it kind of that way. Okay, now we're in the middle of the steps of the algorithm.
00:35:56.394 - 00:37:00.434, Speaker A: This if you could choose uniformly at random an element of h itself with no noise, it would almost surely densely generate the connected subgroup. And with good probability, you wouldn't even need, you may need exponential size multiples of y zero to get a finite version of dense generation. But with good probability, you wouldn't need anything worse than an exponential scale of multiples of y zero to get a good epsilon net for h one sharp in particular to get multiples near the origin. So there's some noise to worry about. But still, if there's little enough noise, there will be multiples near the origin. That could be useful. But to find which multiples are near the origin, then we need a lattice algorithm.
00:37:00.434 - 00:37:46.090, Speaker A: So it's kind of ironic that I can get quantum polynomial time with just a single sample for part of what the algorithm does. It's not necessarily the best thing to do, but it's a way to have a proof that it works. So just using one sample, one noisy sample, y one, make a lattice in the next dimension. What is the lattice basis? Just the standard basis in the first k dimensions. This part is from the torus. Okay. And now if we stayed in k dimensions, we would get a non lattice abelian subgroup that would be dense usually.
00:37:46.090 - 00:38:52.026, Speaker A: Well, it would be indiscreet anyway in r to the k with the extra vector y one, or at least if it were y naught, then it would be indiscreet. So I lift it to a controlled lattice. It's a very squished lattice that is meant to resemble this dense orbit of y one in the next dimension. By adding a one over t, where t is another large number, t should be, on the one hand much smaller than the gaussian blur, than the gaussian parameter s, which is the main source of noise in my proof. On the other hand, much larger than capital r, where one over r, which in my paper I call little r, is a lower bound for the feature scale of h. What do I mean by the feature scale? Well, the idea is that these stripes are only so close together. They can be exponentially close together, but you can get a lower bound on how close.
00:38:52.026 - 00:39:26.834, Speaker A: So that's our estimate for the feature scale r. And you do want the amount that you squished lattice in the new dimension to be bigger than that. Okay, remember, it was meant as a stand in for a dense orbit in a torus. So then we do have a lattice that's our lattice, l. And we can calculate an ll basis of, well, not super short vectors, but pretty short vectors. There's another exponential factor in how short they can be. Exponential in the dimension, but still good enough, pretty short vectors.
00:39:26.834 - 00:40:35.184, Speaker A: And so there's our lattice basis, and the first k minus l of plus one of them span approximately, span the tangent space at the origin to hide plus the new direction r. Okay, that's what the short vectors are going to do. At least if they're short enough, they have to cluster near the origin plus the displacement in the new dimension that I threw in to get a lattice in the first place. Now still there's the noise which, the noise in Y one, which was carried through these steps. That noise is a problem. But at least now after this lattice technique, we have a way to remove it. So using these first k, k minus, this is, this should be k minus l plus one vectors.
00:40:35.184 - 00:41:30.054, Speaker A: This is a rectangular matrix b. Okay, this is k plus one by k minus l plus one. So I want, it's a tall rectangular matrix, and in order to be able to denoise it, I put it in column reduced echelon form by inverting a square submatrix. Now, I don't want to choose just any square submatrix because some of them might be singular, or at least would have been singular without noise. If noise is the only reason that a is nonsingular, that's bad. The issue at this moment is a generalization of what happens in just two dimensions. In just two dimensions, if we did have a vector close to the origin, then we should take the ratio of the two coordinates to get the slope.
00:41:30.054 - 00:42:37.494, Speaker A: But to get a stable ratio, you should take the small coordinate divided by the large, the smaller coordinate divided by the larger one, or else you may get a numerical instability if the slope is too close to horizontal or vertical. So this question of just ordering the coordinates so that the slope is numerically stable for this column reduced echelon form is in fact a non trivial problem if the dimension is high and the co dimension is also high. But there's a greedy algorithm to do that using the Cauchy binet formula. This total, it's a generalization of the pythagorean theorem. Actually, this total determinant is the sum of the squares of these minors. And you can just eliminate rows one by one, using a greedy algorithm to get a sub matrix that has a large enough determinant to know that the determinant doesn't just come from noise. It doesn't even mainly come from noise.
00:42:37.494 - 00:43:32.594, Speaker A: You can invert that one. So this is our cref form, a inverse b using an appropriately chosen submatrix. And the entries, well, there's the identity where you inverted, but the other entries are approximate rational numbers, and the noiseless, irrational numbers that they approximate is the valuable information. Then you can denoise those entries with a continued fraction algorithm. So what does that give us? We were looking for short vectors in the torus in this lattice. And those short vectors, they come from multiples of the noisy of the sample. And in denoised form, they are a rational basis for the tangent space at zero to h plus the new dimension that we needed to get a lattice.
00:43:32.594 - 00:44:19.536, Speaker A: So we can call that, by the way that h sharp is defined, that's perpendicular to a vector space that I want to call Hr in r plus one. And what is Hr? Well, it's just the original hidden subgroup h tensor with the real numbers. In other words, we have a rational basis that points in the direction of the hidden subgroup h. Even though it doesn't give us specific elements of h, it's very valuable information. Actually, a rational basis for this perpendicular for this orthogonal space gives us a rational basis for hr two. So we're actually almost done. So we used the a quantum part of the algorithm.
00:44:19.536 - 00:45:10.492, Speaker A: Using the quantum part of the algorithm, we got this noisy sample that is near h sharp, where h is the hidden subgroup. So h sharp is a type of dual. We then used y one to define a lattice in the next dimension, and we use the ll algorithm and denoise the result to find a rational basis for not h itself, but tensor with the real numbers. And actually, since it's a rational basis, I could have made it h tensor q. It doesn't. You know, I said I've used reals, but I, this being a number theory workshop, I didn't really need the real coefficients for some of this. I mean, for the Fourier theory is better to state this in terms of real coefficients, because they're locally compact.
00:45:10.492 - 00:46:18.674, Speaker A: But for the actual data that you get from step six or so, I could have called it, I could have said Hq h tensor q. It would have come to the same thing, since it is a rational basis. So, to finish the algorithm, we can use Smith normal form to convert a rational basis for Hr to an integral basis for its intersection with z to the k, and this intersection, h one. What is that? Well, you can recognize that as the rational closure of the hidden subgroup h, those integer vectors whose multiples lie in h. That's what h one is. So h has finite index in h one, and using a single sample, we have found h one. So we're not really done, but we have reduced to a previous case, using some change of basis to change h one to z to the l.
00:46:18.674 - 00:47:07.834, Speaker A: We now have a hidden subgroup in that that does have finite index. And so we can use the standard shortcut algorithm to find h from h one. Okay, so that's how the algorithm works. But now let me go to open problems, particularly the second one, which I think is the best one for this workshop. So, one open problem is. Well, I had only one sample to get the theorem, not because that's the best thing to do. Shortcut itself works better with it can require some logarithmic number of samples, or polynomial in the side in the bit complexity, number of samples.
00:47:07.834 - 00:47:50.850, Speaker A: And probably using more samples for the stage that I described is better. Actually, I more or less know it's better. I don't know how much better, but I know it's better. So, the open problem is to understand the best number of samples. It's a trade off with more samples. It looks like the quantum computer is doing more work, but it's actually doing less work, because the advantage of more samples is that you don't have to take as big of a multiple of the sample that you had in the lattice stage of the algorithm. So this is to make q smaller, and making the number of qubits you use smaller is much more important than the number of samples you take.
00:47:50.850 - 00:49:08.932, Speaker A: The whole algorithm will go faster, although how much faster it'll go if you decrease q depends on the computational complexity of the heighting function itself. Well, there's this problem with this research question, although my student is, is working on this on, on how to understand multiple samples, which is that I don't even have a hiding function to use. That's interesting. So this is, this is the open problem that I want to put forward. Is there a, a challenging, or even better interesting, useful number theoretic choice for a heighting function with the property that the target set is now infinite and the periodicity has lower rank. And challenging means that you didn't build the hiding function backwards from direct knowledge of its periodicity, not even indirectly. So that it's not just that it's just sitting there in the code for f, but that there's not a fast classical algorithm to compute h, not from the values of f, but from the algorithm to compute f.
00:49:08.932 - 00:49:54.504, Speaker A: That's not interesting. So I don't know of a challenging hiding function for this lower rank problem. I said I'm optimistic that there is one. And the third thing that I want to list is an open problem. I'm not completely sure how, it just seems within reach, but I should call it an open problem because there's extra work to do. But I just bet you I mentioned I should have given it more prominence that there's the Eisentrager Holgren Kataev song algorithm when h is a lattice. In their case a maximum rank lattice in not in z to the k, but in r to the k.
00:49:54.504 - 00:50:49.342, Speaker A: And they have a different type of hiding function, f Hilbert, you know, f maps to a Hilbert space and is Lipschitz. So they need in order to have a well founded hidden subgroup problem. When the ambient space is a lie group like r to the k, they have different rules for the hiding function. Its values are quantum states, and it has to be Lipschitz. Nonetheless, they found an application. In fact, there are several quite important applications in algorithmic number theory for what they did when h has maximum rank. So I bet you that there is a mutual generalization of what I did and what they did when h has lower rank in r and is in r to the k rather than z to the k.
00:50:49.342 - 00:50:56.934, Speaker A: But that still should be developed. Okay, that's the end of my talk, and I will stop there and we can do questions.
