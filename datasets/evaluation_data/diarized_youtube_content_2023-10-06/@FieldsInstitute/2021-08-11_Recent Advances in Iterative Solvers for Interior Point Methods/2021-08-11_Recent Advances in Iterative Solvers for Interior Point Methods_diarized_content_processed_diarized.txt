00:00:02.040 - 00:00:49.830, Speaker A: Okay, so welcome to this series, the joint seminar series between with the fields and University of Waterloo. And it's my great pleasure to introduce today's the first of today's two speakers. This is Jasek Gonzio, and he's from the University of Edinburgh. And Jasek has a long history of important results in the area of. Well, I guess what he's going to be talking about today in using interior point methods for linear programming related problems and especially for solving very huge, large scale problems. So, Jessic, I'll give the floor to you. Please.
00:00:49.830 - 00:00:50.594, Speaker A: Go ahead.
00:00:50.974 - 00:01:08.454, Speaker B: Great. Thanks, Henry. Let me just share the screen. Can you see my talk now?
00:01:13.514 - 00:01:16.130, Speaker A: Yes, I see it perfectly. Yes. Great.
00:01:16.202 - 00:01:21.334, Speaker B: Great. It does not need to be increased, decreased, or something. Fine.
00:01:22.314 - 00:01:26.802, Speaker A: It's fine for me. Okay. For other people. Yep.
00:01:26.858 - 00:01:27.654, Speaker B: That's good.
00:01:31.474 - 00:01:34.934, Speaker A: And recording is on? I think so, yes.
00:01:37.114 - 00:02:16.954, Speaker B: So thanks. Thanks, Henry. Thanks, everybody else who was kind enough to come and spend this 1 hour. It's always a pleasure to talk about something that we are working on. There is this general belief that what we work on is interesting. So I'd like to share a little bit of my enthusiasm for interior point methods with the audience of today's seminar. So I'll guide you through some recent advances in iterative solvers which are used in interior point methods.
00:02:16.954 - 00:02:49.314, Speaker B: Here's the outline of my talk. I will very briefly go through interior point methods. They were born in 84. I'll give a few key ideas, but then we'll move to today's challenges related to interior point methods. And in particular, I would go. I would like to go beyond the obvious things, the textbook information on interior point methods. So I'd like then to focus on those four items.
00:02:49.314 - 00:03:49.568, Speaker B: Right. 1984. If you read George Orwell, then you would have rather bad connotations with 1984. But actually, I have the most positive ones, and think like Frank Sinatra, that when I was 24, it was a very good year. This was because Karl Marcar come up with his paper, and it shook the area of optimization, because it was the first polynomial time, well, not the first after ellipsoid method, but the very competitive new polynomial time algorithm for Lennar programming. And when you think of it, it was by far a non obvious thing to do, partly because it was using some kind of shocking mathematical concept. You take a linear optimization problem, and you add a nonlinear function to the objective.
00:03:49.568 - 00:04:49.414, Speaker B: So it goes against the common sense, against the practice of mathematics, the centuries of practice in mathematics, we do something like non linearizing the linear problem. Now, I'd like to very briefly say how this is done, although maybe the vast majority of you know it to perfection, but it will also set up certain elements for discussion later on. So let's go, let's do it. For linear programming problems, this is an Lp, the primal form and the dual form well joined through the Lagrangian. And here we have the optimality conditions for such problem. Optimality conditions require primal feasibility, dual feasibility and complementarity, which either may be written component wise. Component wise, either xj or sj has to be zero.
00:04:49.414 - 00:05:50.714, Speaker B: Or we can write them in the matrix notation, placing vectors x and and s on a diagonal of a matrix. So capital x is a diagonal matrix with vector elements on the diagonal. If we do it that way, with the vector e equal to all ones, we can write this xj sj equal to zero for row j just in one matrix equation. That's a very basic element of theory for linear programming. But what comes with interior point methods is this beauty, which is the logarithmic barrier function, to be precise, and minus logarithm, which can replace, which can mimic the inequality. Because if we add such a function to the objective, we will promote x's which are bounded away from zero. So another way we will promote satisfying this constraint.
00:05:50.714 - 00:06:53.804, Speaker B: And it doesn't take long to realize that if there are more inequalities like this, we can embrace them all. By doing this summation of logarithms, and then minimizing the function, the minus sum of logarithms, which is equivalent to minimizing e to such a power. But e to the logarithm just produces the value of variable. So if we minimize such an object, it is equivalent to maximizing the product of variables. And this already clarifies the intuition of interior point method that maximizing the product of variables means we do not want any of the variables to go to zero prematurely. We will try to keep all xjs far away from zero. Now, how does it work in Lp? Well, you take the Lp problem and you replace the inequality with the logarithmic barrier function with a parameter, the greek letter me here.
00:06:53.804 - 00:08:01.362, Speaker B: And then you formulate the Lagrangian so standard duality and write down optimality conditions for it. The optimality conditions now involve this term me x inverse e, which of course comes from logarithm. The derivative of logarithm is one over x. So this x inverse is the diagonal matrix, where on diagonal we have elements one over xj for the vector x. And it suffices to introduce a new variable s, defined like that which is equivalent to saying capital x times capital s times the vector of ones is uniformly equal to me times the vector of ones. To get these new optimality conditions where a perturbation appears on the right hand side of complementarity condition, and we can see it here on page eight, the essential difference. These are the optimality conditions for the barrier problem.
00:08:01.362 - 00:09:09.922, Speaker B: And here we have optimality conditions for an LP. The only difference is here in this right hand side, which is a perturbed in case of interior point, in case of the use of logarithmic barrier, and does not ask for xj times sj to be equal to zero like in optimality conditions, but allows it to be a small number. When I say small, well, we can start from arbitrarily large value of mean, but we will gradually squeeze it to zero and that way we will force that this condition eventually in the limit will get very close to that one. So optimality in interior point methods is approached in a very different way, is approached by convergence to, but by driving complementarity products to zero. This parameter me has a lot of interesting property. Well, not parameter. The presence of this logarithmic barrier, the presence of the parameter me helps us to interpret, for example, things like duality gap.
00:09:09.922 - 00:10:13.674, Speaker B: It's just proportional to the barrier term me. There is also a uniquely defined point, a point solving these optimality conditions for any value of parameter me and all the solutions, if you go with me, start at me equal to infinity and go with me to zero. And this will define so called primal dual central trajectory, the set of all these points. It's in case of linear programming. It's a continuous curve that drives, that takes us from very far away from optimality to optimality. Interior point methods follow this path and try to maintain points which will be primal, feasible. Feasible algorithms try to do it, keep primal and dual feasibility, and also attempt to stay close to satisfying this perturbed complementarity condition.
00:10:13.674 - 00:12:10.654, Speaker B: When I say try to be close, it can be captured, for example by the euclidean norm, asking for euclidean norm of the error in this equation to be bounded by a term which is proportional to the barrier term mean. And this, as long as we really maintain such features, that is, all the iterates stay in such a small neighborhood of the central path, and we try to do small reductions of the barrier term from iteration k to iteration k plus one, small reductions, meaning by a parameter sigma which is very close to one, as you can see, one minus a tiny fraction divided by square root of n. Then we will be able to demonstrate that already after a square root of iterations, a systematic decrease will give us a noticeable reduction in the duality gap. And after the number of iterations proportional to the square root of n, we will obtain arbitrarily large reduction, meaning we can get the duality gap to arbitrarily small value. In other words, we will achieve convergence, and without being very precise, this means that interior point methods enjoy this iteration complexity of o of square root of n, meaning that after the number of iterations proportional to the square root of n, an optimal solution will be found. IPMs have beautiful extensions. Two slightly more slightly later developed ones are the second order cone programming problems and semi definite programming problems.
00:12:10.654 - 00:13:09.534, Speaker B: And of course, there were earlier developed extensions from linear programming to quadratic programming and non linear programming. But they are always based on this clever use of logarithmic function, which is very well suited to handle inequalities, to mimic, to replace inequalities. So, for example, if you have a quadratic cone which is defined here, we want to satisfy this inequality t squared greater than or equal the norm of x. Then instead of satisfying this inequality, we may penalize for approaching dangerously close to zero with this inequality. And this would mean we penalize for the logarithm of a difference of these two sides. As you can see here, very similar thing happens for SDP. SDP is much more complicated, as certainly Henry would have told you already.
00:13:09.534 - 00:14:01.534, Speaker B: So in SDP and semi definite programming, we have an object which is a positive semi definite matrix. But in this case we can also define a logarithmic barrier function. This time we will take the logarithm of the determinant of the matrix, and you can see the analogy. In case of the determinant of the matrix, the determinant is equal to the product of eigenvalues. But what is the logarithm of the product of eigenvalues? It's the sum of logarithms of eigenvalues. And here you see, it's exactly the same object as in case of LP. Before I finish this brief introduction to interior point methods, I would like to comment on the computational perspective of it.
00:14:01.534 - 00:14:50.660, Speaker B: Well, at the heart of interior point methods, there is always Newton method, a method well suited to solve non linear equations. So whenever we formulate first order optimality conditions, like in linear programming, this was the system of optimality conditions. We will apply Newton method to find a point which approximates the solution of it. And applying Newton method means computing the jacobian of this function. Here on the left hand side times Newton direction is equal to the current violation of this nonlinear system. In case of linear programming, the first two equations were linear only. The third one was bilinear.
00:14:50.660 - 00:16:11.214, Speaker B: So it's not heavily nonlinear, it's just mildly non linear system. And the differentiation would give us something very nice and easy. In more elaborate optimization problems, these systems may get noticeably more complicated than here. But what joins them all, at least in case of linear, quadratic and nonlinear programming, is that they always will have such a structure, which is a very well studied system of linar equations, which is at the origin of almost any computational linear algebra, I would say PDE community, partial differential equations community knows it under the name subtle point systems. We tend to refer to them as reduced KKT systems in optimization community. So, to summarize this brief introduction, interior point methods delivered a unified view of optimization, starting from l programming via quadratic to non linear programming, and also the extensions to second order cone programs and semidefinite programs. They have enjoyably predictable behavior.
00:16:11.214 - 00:16:53.772, Speaker B: They're always solved problems in something like 20, occasionally 30 iterations, and they have unequal efficiency. Maybe for small problems, they are certainly competitive. But the larger the problems become, the more beyond competition ipms become. It's possible to push things far. It's possible to solve problems of dimension ten power nine. This was done already 16 years ago, but probably you could go much, much further than that with today's computers. Now, I'd like to address some of the possible improvements of this basic methodology.
00:16:53.772 - 00:18:05.890, Speaker B: So obviously, interior point methods heavily rely on Newton method on Newton direction for the barrier problem. And for many years, these l systems were solved exactly. However, it's possible also to use an inexact Newton method. After all, why should we solve the systems exactly, knowing that optimality conditions for the barrier problem are anyway not our ultimate goal, they are just a tool to make step to get closer to the solution of the optimization problem. And indeed, the use of inexact Newton method, which is very well studied in the general context of nonlinear equations, has also been brought quickly as an option inside. In the context of interior point methods, this was done by Stefania Bellavia in 98. In this context, the main tool is to replace the exact Newton direction with the inexact one.
00:18:05.890 - 00:19:12.754, Speaker B: And this inexact one would admit certain error in this equation. And the general theory says that as long as this error is only a fraction of the right hand side, inexact Newton method is almost as good as the exact one. Well, based on this positive perspective on it, we'd like to be daring and use them in interior point method, that is, replace the exact one exact Newton method with the inexact one. This raises theoretical questions as well as practical ones. So in terms of theoretical questions, I did the analysis of what will happen if you use small neighborhood, the so called n two neighborhood induced by norm, two by the euclidean norm. And it turns out that you can be very daring. You can actually accept up to 30% error in the direction, and this will not affect the worst case complexity.
00:19:12.754 - 00:20:37.164, Speaker B: In more practical symmetric neighborhood, which is a much wider neighborhood and therefore much more attractive for practical usage, one has to be a little bit more cautious, but still up to 5% error in the direction. So up to 5% of inexactness does not have any impact on the worst case complexity, which is certainly a very encouraging theoretical result, which allows us to go for inexact Newton method in this context. I think at this point it's worth mentioning that interior point methods have an amazing ability of handling inequalities. Inequalities are difficult. In optimization equations, are easy equations. You know what you have to do with an equation, you have to satisfy it. However, an inequality is really a full, a full interval, and you never know whether inequality will be active, that is satisfied as an equation on the boundary of inequality, or will it be satisfied as a strict inequality, in which case you could dispose it.
00:20:37.164 - 00:21:33.760, Speaker B: Whether it is present or not, it doesn't really matter for the optimization problem, and an overarching feature of interior point methods is an incredible ability of finding these inequalities, which are important, finding the essential subspace of the problem where the optimal solution is really hidden. By the way, may I say that if anybody has questions in the middle of my talk, that I absolutely do not mind, you can ask them. We've just finished part one of it, because now I'd like to take you through a journey of things which are beyond the obvious interior point method tricks. And I have four items to cover, four little contributions to mention. So, first of all, I'd like to.
00:21:33.792 - 00:21:47.244, Speaker A: Talk about, I have a question. Yes. When you say inexact, do you mean inexact for the entire Newton equation, the entire big system, the optimality conditions.
00:21:50.854 - 00:22:30.534, Speaker B: My analysis, the one that I referred to, applied to feasible method. So in case of feasible method, I assumed that the primal and dual feasibility was assumed, so there was no inexactness there. In such case, inexactness only applied to complementarity condition. But it's just a technicality. One can develop an inexact method, which allows inexactness everywhere. I just picked up an easier model, the feasible method. Have I answered your question?
00:22:32.794 - 00:22:33.930, Speaker A: Yes, thank you.
00:22:34.042 - 00:23:49.794, Speaker B: Okay. Right, so coming back to this, beyond the obvious, I'd like to draw your attention to the fact that we can relax significantly the rigid structure of interior point. And then the question will be how much of this methodology really has to be maintained and which elements of it could be relaxed? Here in the second item, I'd like to show you that a lot can be relaxed. We can use an inexact to the square interior point method, where inexactness will be almost everywhere, will be an overwhelming feature of it. And still interior point methods for some easier problems converge beautifully fast and, and are very attractive. I also would like to mention a joint work with Kimonas von Tulakis, who by the way, now is professor at Waterloo University on primal dual Newton conjugate gradient method. And the very last thing developed only this year, the variety of sparse approximations with interior point methods.
00:23:49.794 - 00:25:17.264, Speaker B: So let's talk about this relaxed interior point method, and here I'll step into a difficult case of problem that is semi definite programming problem. They still remain a challenge, and this is because linear algebra in SDP is significantly more complicated than in the other classes of problems. To cut long story short, linear algebra of SDP, the problem of dimension n, requires solving linear systems of dimension n squared by n squared, which obviously is a tough task. So our aim here was to redesign interior point methods for SDP and of course replace exact Newton within exact one, do everything in the matrix free regime, and also make sure that we use as little memory as possible, ideally the properly limited memory in work and properly limited memory regime. In this context, of course Linux systems have to be solved by Krilov methods and the appropriate choice of the preconditioner is also very important. So let's have a look at SDP. This is the SDP in the primal form and in the dual form.
00:25:17.264 - 00:26:23.320, Speaker B: And let us make this assumption that the matrix s is sparse. It may be sparse because it is a union of various sparse matrices AI. Many relaxations of combinatorial optimization problems, for example, contribute here matrices AI, which have one or two or four nonzero entries. So even if there are quite a few of them, their aggregation, the merging of the sparsity pattern still maintains as pretty sparse and the sparsity should be exploited. Optimality conditions for general SDP can be written in this way, although the more typical way of writing them would be that this last equation would be written as capital x times capital s is equal to me times identity. Here I want to underline that here capital x is a general positive semi definite matrix. So is capital s.
00:26:23.320 - 00:27:30.656, Speaker B: So these are not diagonal matrices like in Lp. These are general positive semi definite matrices. But I would like to draw your attention to this equation x minus me s inverse, because it illustrates certain important feature of sparse matrices. Although the matrix s may be very sparse, its inverse is frequently a completely dense matrix, which means that it would be very unwise to have to store explicitly matrix x. And this is one of the arguments that working with the dual barrier problem, which does not make x appear explicitly here, might be an attractive option. If we want to use this dual path following IPM, we will do it in the following way. We will use the same philosophy as interior point.
00:27:30.656 - 00:28:44.232, Speaker B: However, we will make much more brutal reductions of the barrier term, just the new barrier will be the previous barrier term times sigma. And rather than strictly following the path, we will make jumps from one point close to the path to another point close to the path, accepting the fact of having to do more Newton iterations. So Newton iterations will be done on this nonlinear system, and they will continue until the scaled Newton step in variable s. You can see pre scaled and post scaled by the current value of dual variable s is small, small understood in terms of Frobenius norm. So the moment the Frobenius norm on the Newton step on s becomes small, we know we have approached sufficiently well the point on the central path. This approach is attractive. It can solve certain semi definite programming problems without necessarily explicitly using matrix x.
00:28:44.232 - 00:30:29.500, Speaker B: So it's very well suited for problems where s is very sparse, as these two papers demonstrate in more detail. My second example deals with this very inexact interior point method. So let me remind you that a standard inexact Newton method admits certain residual, certain error capital r here, and requires that this error should be the norm of this error should be bounded by a fraction times the right hand side term, where the fraction should be, well, any number between zero and one. This is enough to study convergence of the general inexact Newton method. But even such a condition, when I looked at it very closely, turns out to be disappointingly conservative. And let me just show you the following thing. If you deal with linear optimization in which Newton system has this form, so the jacobian of first order conditions times Newton direction is equal to the current invisibilities, current violation of optimality conditions, then should we be lucky and do full Newton step with alpha equal to one, we will immediately absorb these linear terms, we will reach primal visibility and dual visibility, and we will also nicely reduce the complementarity.
00:30:29.500 - 00:31:27.764, Speaker B: But in practice, such steps happen very rarely. It would be a dream of any optimizer to have step sizes equal to one, which almost means finding the optimal solution in just one shot. If we are prepared to do more iterations, why should we waste time on computing an exact solution of the system? We should admit we should accept considerable errors in this system of equations. Let me share with you an intriguing observation. This is what happens at a particular interior point iteration. The pictures that are displayed for you are the following. The first one shows the evolution of error in the conjugate gradient method.
00:31:27.764 - 00:32:11.782, Speaker B: It non monotonically decreases. You see, the tendency is to decrease. CG is not monotonically reducing the residual. So occasionally there are little jumps. But after 20 iterations we reach the accuracy ten power minus three. After 60 iterations we reach the accuracy ten power minus six. However, this is just a linear algebra perspective on solving the linar equation system, the Newton system, while at the same time we looked what happened to the infeasibility with the same evolution of iterations from zero to 60, what happened to complementarity and what happened to primal and dual step sizes.
00:32:11.782 - 00:33:05.514, Speaker B: And as you can see, after about 15 iterations, invisibility has been reduced and it will not be reduced anymore. Whether we get more accurate direction or not. The same happens with complementarity. Complementarity has been reduced and after about 20 iterations we will not get any more improvement in it. We will only waste 2040 more conjugate gradient iterations, the same with step sizes. They stabilize after about 15 or 16 iterations. So what's the point? Why should we do four times more or three times more iterations getting here while we actually could have stopped at this point and enjoy already a very constructive Newton direction and making significant progress in interior point method.
00:33:05.514 - 00:34:18.124, Speaker B: Well, this was the computational observation, but actually we can convert it into a piece of rigorous theory. We can accept the direction produced by inner solver. By inner solver, I mean the linear algebra solver, as soon as those conditions are satisfied, meaning the Newton step in x compared with the variable is not too large, Newton step in s compared with the variable is not too large. And we already achieved some reduction of primal infeasibility and dual infeasibility. We have implemented this technique either with both with conjugate gradient and with meanness, depending on whether we solve normal equations or augmented system. And we've observed that it really reduces the number of linear iterations by 70% to 90% of iterations. And this usually translates into something like 60% to 80% reduction of the overall cpu time for interior point method.
00:34:18.124 - 00:35:23.724, Speaker B: The negative side of it is that we lose something in terms of complexity. Complexity drops by an order, and from o of n to o of n squared, but, well, that's the worst case complexity. It's, it's weaker, it's poorer, but from computational point of view, the method works better, it's faster. There are details in the, in the report on this graph. I'm just showing you what would happen if we used a standard method. Sorry, a standard stopping criterion for conjugate gradient, which would ask always to find a very precise solution to the L system with a six digit accurate solution. Well, this would mean that at some initial iterations of interior point, we would have to do as much as many as 400, 500 conjugate gradient iterations.
00:35:23.724 - 00:37:22.098, Speaker B: While we know very well that we are so far away from optimality, then going for accurate solutions in the Newton system is just waste of energy, waste of time, you would say, immediately, well, we do not have to go to ten power minus six. But then the question is, so what should be this precision? And there is no good answer to this. That's why we wanted to design completely new stopping criteria, which would give us this uniform behavior, this yellow line, this is interior point conjugate gradient, the new way of the new stopping criterion for the CG method, which always keeps the number of iterations to its minimum, well, about 50, almost through the whole solution process. This was used in particular in a very interesting problem, which requires a separation of materials in x ray tomography. The interest in this problem came from physicists working in Helsinki who wanted to use this interesting looking regularizer, which you reveal immediately gives us an indefinite matrix, and therefore non convex term added to the function. So this is the least squares problem, non negative least squares problem, in which there is a tehon of regularization on x. And on top of it, there is this extra inner product regularization, which, considering the fact that x is non negative, means that the scalar product between variables corresponding to material one and material two should be as small as possible.
00:37:22.098 - 00:38:43.514, Speaker B: This is the material separation as image processing people explained to me. We applied this new method, this interior point conjugate gradient, and we could really see significant reductions, you see here, 25,000 cg iterations with a standard stopping criterion, 2600 overall iterations in this more intelligent conjugate gradient, more intelligent stopping criteria, and of course, the clear gain in the solution time, 600 seconds, as opposed to 5000 seconds. So eight times faster for cpu time, about ten times fewer iterations. Right. And I'll go to a different application. This one originated from big data optimization and the whole buzz about machine learning, doing classifications and computational statistics and lots of signal and image processing applications. All those problems, but not all, but many of those problems can be represented as a large, possibly dense quadratic programming problem.
00:38:43.514 - 00:39:32.062, Speaker B: But in these problems there is a regularizer often added. And I'd like to illustrate the difference between regularizer, which involves norm one, and regularizer, which involves norm two. So that's a case of binary classification. You would like to find a separation between one cluster of points and another cluster of points. And if you use norm one, then you will promote as few components. You would promote the solution which involves s few components of vector x as possible. If you use norm two, you try to spread the error uniformly between all the components.
00:39:32.062 - 00:40:25.438, Speaker B: So in this case, just one component, component, a one, is enough to separate the data points. In this case, both a one and a two will be used to do the separation. And you probably think, what on earth is he talking about? What is the difference? Of course, the difference in dimension two is irrelevant. But if we talk about the dimension of 2 billion, then we would like to have 1000 or a few thousand non zero entries, rather than full 2 billion of nonzero entries promoted by this second, by this euclidean law. So there is a genuine interest in using l one regularization in computational statistics and machine learning, in image processing. That's the problem. We have to minimize the function.
00:40:25.438 - 00:41:02.734, Speaker B: It could be just the least squares, but we want to promote sparsity in x. And when optimizer looks at such a problem, well, the striking thing is that, well, it should be easy. There are no constraints here. So problems without constraints are usually chapter one of any book on optimization. But there is a little bit of a difficulty. This is the norm one, which is non differentiable. So you immediately say bye bye to gradient, to hessian, or any other higher order information.
00:41:02.734 - 00:42:06.504, Speaker B: So we'd like to overcome this restriction imposed by norm one. This can be done, for example, by splitting the variable x into positive and negative parts, and that way replacing the absolute value of x of the component of x by the difference, well, the absolute value by the sum of those two terms. Another option is to use some kind of smoothing which would replace the cask with a smooth function. I'll show you this Huber regularization or pseudo Huber regularization. This is a zoomed picture in which the blue line indicates the cask, the absolute value of x, while the green dashed line shows the graph of the pseudo Huber function. You see, it agrees pretty much with the blue line, far away from zero, close to zero. It just smoothly replaces the cask with something which looks pretty much like a quadratic function.
00:42:06.504 - 00:43:18.694, Speaker B: Well, this leads me to a generalization of interior point method, which is which looks at interior point as a continuation, as a homotopy approach. What happens in interior point methods in interior point method is that we replace a problem with a sequence of problems with gradually reduced barrier term meaning. And we could generalize this procedure to say that, well, the same could be used with, for example, pseudo Huber regression. That is, replace absolute value of the variable with this pseudo Huber function and gradually reduce this parameter me from a large value to a smaller value to get the, to improve the quality of the approximation. Well, whenever you see such a thing, there are two questions. Would the theory allow it? Would it work in practice? But you probably know that a lecturer, a speaker will not put questions on one slide if there was no answer on the following slide. So yeah, that's what we can do.
00:43:18.694 - 00:44:10.516, Speaker B: Let's take a specific example of compressed sensing and let's apply this continuation technique. In compressed sensing we try to find the minimum of least squares, and on top of it the vector x should be such that in certain space controlled by this matrix w, it will give us a sparse solution. This means in certain dictionary the signal x should be sparse, in certain space it should be sparse. Well, this is norm one, which is a troublemaker. So we will replace it with pseudo cuber function. And now the question appears, well, that's the true solution we would like to find x tau. If we replace, if we use the pseudo cuber function, we will find a different solution.
00:44:10.516 - 00:46:05.564, Speaker B: The question is how big an error will we make between this solution and that one? And there, there is a positive, there is a theoretical argument to say not such a big one because we can generate a sequence short sequence of quickly decreasing values of this parameter in the pseudo Huber function. And the solutions obtained by these approximate problems which use pseudo Huber function are not far away from the true problem x from the true solution x tau. Again, this has been studied both from theoretical point of view and from the practical point of view, leading to specific preconditioner that can be used in this context. By the way, when we work on it with Kimona's font lackeys, we also realized that these problems that l one regularized list squares appear nowadays in so many applications, and also there are so many algorithms dedicated, specialized for solving such problems, that it would be good to have some kind of systematic way of comparing these algorithms. So Kimonos developed this generator for problems where the matrix a can be fully controlled by the user. It is determined by singular value decomposition, and we can control the matrices u and v because they are a sequence of products of givens rotations. So that way we can easily control the condition number of a by choosing element sigma, and the sparsity of matrix a by choosing how many givens rotations will be here hidden in matrices u and v.
00:46:05.564 - 00:46:52.804, Speaker B: This is Matlab development, so anybody interested can use that code. But then we performed excessive computational tests, which overall took four months of cpu running, and we compared several methods for optimization. Vista parallel coordinate descent method. Also this projected scaled subgradient and our primal dual Newton conjugate method. Newton conjugate gradient. So primal dual Newton conjugate gradient. So what we could observe is that those three first order methods work really amazingly well, as long as the condition number of the matrix is small.
00:46:52.804 - 00:47:44.334, Speaker B: When condition number starts to grow, they start to struggle. When condition number grows further, they usually just stole completely and cannot converge. Second order method, on the other hand, with rather simple diagonal preconditioner, could go much further, could deal with problems with noticeably larger condition number. If you ask me what to do for more conditioned problems, the answer would be of course. Well, we need something more elaborate than just a diagonal preconditioner. We would need a preconditioner that understands better the spectral properties of the matrix a. And we also run tests on really very large problems using high performance computer.
00:47:44.334 - 00:48:20.224, Speaker B: This is an archer. The computer which in March 2015, so when the computations were done, was ranked 25 on top 500 list. But probably now it would surprise me if it was still in top 100. Probably is somewhere beyond top 100. It's a powerful machine. It has a lot of terraflops per second and a lot of memory. This allowed us to solve problems which start from 1 billion and go to 1 trillion of variables.
00:48:20.224 - 00:48:53.732, Speaker B: Every time we increase the number of problems by a factor of four, we increase the number of processors by a factor of four. No surprise the memory increases by a factor of four. But look at the time. The time stays the same, which reveals almost perfect scalability of the method. Indeed, when you think of conjugate gradient, conjugate gradient is doing what is doing matrix vector product. Matrix vector product can be parallelized to perfection. You have more processors, you do it faster.
00:48:53.828 - 00:48:54.476, Speaker A: That's it.
00:48:54.580 - 00:50:27.170, Speaker B: That's why there is practically perfect scalability of the approach of the primal dual Newton conjugate gradient method on such problems. Finally, I'd like to talk about other sparse approximations, and I just have one last example here. There are lots of problems in which you have several sparsity promoting terms added sparsity on vector x and maybe sparsity in some other space in a transformed vector x, linearly transformed vector x. We dealt with several applications of this type, sparse portfolio selection, classification models with functional magnetic resonance, total variation based Poisson image restoration, or linear classification, the typical machine learning trick. So that's a new work finished only in February this year. I'll show you just one example, this functional magnetic resonance imaging, because here we perform the comparison of interior point method with Vista and ADMM, two very fashionable first order method. And we report here three features, three characteristics, classification accuracy, corrected pairwise overlap, and the solution density.
00:50:27.170 - 00:51:01.242, Speaker B: Don't forget, we look for sparse solutions. So we'd like the density to be as small as possible. But with those two parameters, we would like them to be as close to 100% accuracy as possible. So we want ACC large, we want corrected overlap large, we want small density. And look what happens when interior point is used. It can, for certain parameters, tau really develop perfect sparsity. Vista can do it for some parameters, not for the others.
00:51:01.242 - 00:51:59.084, Speaker B: ADMM well, actually can never produce sparse solutions. It would really have to run for years probably to do it. But as far as the other parameters are concerned, pheestat struggles with this. Corrected overlap cannot really match what is delivered by interior point. So it's just good to realize that although interior point methods require serious investment, require significant computational effort per iteration, then they eventually deliver very high quality solutions. And you can see this very well on this picture, which shows the evolution of all those three accuracy, corrected overlap, and density. How it changes.
00:51:59.084 - 00:52:45.604, Speaker B: So interior point spends about ten minutes without doing much, but after about ten minutes, shoots quickly to almost perfect sparsity to the minimum. Look what Vista is doing. Vista starts almost immediately after two minutes, has already a good idea of sparsity, but will never ever reach this level of sparsity. It would take not 60 minutes, believe me, it would take much longer for it to get to the same sparsity of solutions. So I think many people just use post order methods because they want to stop them earlier. And in some applications that's good enough, but not in all applications. This was my last technical slide.
00:52:45.604 - 00:53:44.042, Speaker B: It takes me to conclusions well, undoubtedly, interior point methods have delivered a revolution to optimization, although they are second order methods. They are very competitive. The second order information does not have to be exploited with full accuracy. Sometimes very little of second order information is enough through the notions of inexact Newton method, which makes also matrix reimplementations possible. The tricks important to remember from interior point is that the method is very good in detecting where the optimal solution is hidden. They are good in detecting the essential linear subspace in which the solution resides. And they can be specialized by exploiting Lena, Roger, Brad.
00:53:44.042 - 00:54:14.854, Speaker B: So they can work for those well studied classes of problems like LPQP, second order programming and SDP. But they also can be generalized to l one regularization and the other problems which originate from sparse approximation applications. This really brings me to the last slide. Thank you very much for your attention and I hope you will all stay safe, safe and healthy. Thank you.
00:54:16.074 - 00:54:56.804, Speaker A: Okay, thank you very much, Jose, for very interesting talk and lots of information about the theory behind it and of course the implementation and sort of the, I guess it's the latest information on what the latest software can do. I would like to ask people for questions, and I think if you don't mind, we can go longer and start this next talk a little bit later and treat this sort of like a bit of a workshop. So hopefully, please ask questions.
00:55:00.384 - 00:55:01.644, Speaker C: Hi, Yasak.
00:55:01.944 - 00:55:03.444, Speaker B: Yes, hi.
00:55:04.054 - 00:55:10.354, Speaker A: When you showed those strong scaling results were those with dense matrices, you were multiplying.
00:55:12.454 - 00:55:52.266, Speaker B: Some of them have dense blocks. For example, this functional magnetic resonance imaging is a very large matrix, but there is a noticeable, say, kernel information, which actually is dense. If I remember correctly, you would have something like maybe 5000 dense rows in a problem that has maybe 20,000 constraints. Okay. All right, I have an idea I.
00:55:52.290 - 00:56:44.334, Speaker A: Want to discuss with you, but we do it offline. Yeah. Okay. Anybody else? Well, I don't want to dominate, but I'll ask a question if you don't mind. So when you're working on the inexact problem, this sort of goes to my question that I asked already, and you said that, okay, you can change things and not assume primal dual feasibility, but are you working on the normal equations or something like the normal equations when you're applying the inexact method, or are you treating the theory on the original optimality conditions?
00:56:45.274 - 00:57:30.042, Speaker B: The theory was really dealing with the optimality conditions. So in the theory dealt with inexactness and allowed inexactness in a specific equation. If you would like to have inexactness in the other equations, inexactness in the dual is easy because there is this identity matrix the dual for Lp is a transpose, y plus s is equal to, to c, so you can absorb a lot of error inside s. Okay, almost any, any error. And y. It's not always that flexible, not in every optimization problem, but, but in the implement.
00:57:30.138 - 00:57:37.154, Speaker A: In the implementation, are you doing the inexact method on the normal equation or on the.
00:57:37.314 - 00:58:14.288, Speaker B: Not necessarily. Depending on application. We always look at what is best for a given application. It can be on normal equations, it can be on augmented system. Then instead of CG, we would use meanres. And still in minres. There are also ways of incorporating an exactness stopping minres earlier, but one could also anticipate working with the full three by three blocks, although I haven't implemented it that way, we don't have any implementation.
00:58:14.288 - 00:58:40.064, Speaker B: It's one of the things that I actually asked Filippo Zanetti, my PhD student, to look closer into, because the system potentially may have even more flexibility. But to answer shortly your question, we work either with normal equations or with augmented systems, either with CG for normal equations or mean rest for augmented system.
00:58:40.564 - 00:58:44.984, Speaker A: Thank you. Anybody else?
00:58:48.244 - 00:59:09.624, Speaker B: I would like to ask one question. What's your take on like continuous versus discrete variables? And say again, you want to have discrete variables? Did I? Yes.
00:59:09.704 - 00:59:12.564, Speaker C: And so I feel, I think yes.
00:59:15.064 - 01:00:39.854, Speaker B: Well, interior point methods are of course methods for continuous optimization. So the only hope that I would see in this context is that if you have a quality relaxation of your combinatorial optimization problem, and you probably know that, for example, SDP relaxations for Maxcat have a nice guarantee of not being too far away from optimality. There are some attractive SDP relaxations of quadratic assignment problems, and I believe of many, many other then yes, you can solve the relaxation, but how to jump from a solution of a relaxation or a solution of an approximation to integer solution is beyond me. I wish I could say it's doable, but I think there's a lot of art required to do it in an intelligent way. I certainly don't know how to do it. You mentioned somewhere about subgradient methods. Maybe so if you use subgradient methods, if you use, for example, cutting plane methods, then yes, oh absolutely.
01:00:39.854 - 01:01:13.992, Speaker B: Then I can talk for another hour. So just be careful with asking such questions. But yes, interior point methods have a lot to offer, because if you use cutting plane methods, then rather than adding one cut at a time and struggle with the simplex method, it's much better to solve the restricted master problems with interior points, because then you are staying in the interior of the polytope and you are significantly less affected by specific cuts that are added to it.
01:01:14.048 - 01:01:14.544, Speaker C: So that's.
01:01:14.584 - 01:01:34.964, Speaker B: That's a big piece of theory, well studied and very attractive computationally. It's something that. I didn't mention it here, but I developed a very powerful methodology like this with my other two PhD students, Pablo Gonzalez and Pedro Munari. A sequence of papers. You can. You can find them on my website.
01:01:37.664 - 01:01:42.276, Speaker A: Thank you. Any other questions?
01:01:42.460 - 01:02:08.784, Speaker C: Okay, could you please. Okay, I have a question. Could you please give some comments on the factorization based precondition? Precondition. For example, for CT, I mean, the Au decomposition and uv auto. How to balance this?
01:02:12.364 - 01:02:16.184, Speaker B: I'm sorry, would you mind repeating question?
01:02:16.524 - 01:02:43.912, Speaker C: Sure, sure. I mean, for preconditioner, can you get gradient? You mentioned that the factorization best method should be a good choice. For example, is lu decomposition or cholesterol decomposition. Normally, it's very basic. I mean, how to. How to balance the computer effort for the precondition.
01:02:43.968 - 01:03:17.578, Speaker B: I already know. Right. So I think you're asking me about preconditioners. They. Preconditioners have generally formulated objective as capturing the spectral properties of the matrix. And preconditioners usually depend on the specific application and the specific features of the matrix. So, one general approach is some kind of incomplete cholesky factorization for normal equations.
01:03:17.578 - 01:03:33.834, Speaker B: Could be a preconditioner. But I have to say, this is a rather. Generally speaking, this is not a good preconditioner. It's a preconditioner. Yes, I agree. If you don't know anything better, you could use it. But it's a very black box approach.
01:03:33.834 - 01:03:47.074, Speaker B: And if you know something more about the problem, then you might be able to develop much more sophisticated preconditioners and cut down the number of CG iterations significantly.
01:03:48.534 - 01:03:49.994, Speaker C: Okay, thank you very much.
01:03:51.344 - 01:03:52.120, Speaker A: Thank you.
01:03:52.232 - 01:03:54.880, Speaker D: I have a quick question, Henry, if possible.
01:03:55.072 - 01:03:55.992, Speaker A: Yes, mahala.
01:03:56.088 - 01:04:26.376, Speaker D: Okay, so, yes, thank you so much for the interesting presentation. When you compare the fista, the AGMM and the interior point methods, it was clear in this was your comment that Fista or ATMM would get you to like, would perform a bit faster at the beginning, but then they will never get you to the required space. Have you tried like, mixing box starting with the fist tower of ATMM to get you to closer to a nicer solution than apply interior point method? Something like that?
01:04:26.520 - 01:05:12.744, Speaker B: No, we haven't tried. One of the difficulties with such an approach is that interior point method, I do not want to say cannot warm start, because I always say just the opposite. Yes, they can. But they can warm start from points which do take interior point mentality very seriously. That is, they can warm start from points which do have certain centrality properties. The starting points, like the ones for example, originating from ADNM or from Vista would not have such features, which means that they will be advanced. You are absolutely right, they would be closer to optimality, but it's very difficult to translate them as good initial points for IPM.
01:05:13.964 - 01:05:17.504, Speaker D: I see. Okay, thank you.
01:05:18.444 - 01:06:03.294, Speaker A: Okay, thank you. Anybody else? Let me have one question. Hopefully it's quick and short when you your your comment about step one step equal one length equal one. So you said that this hardly ever happens. I think it hardly ever happens because of the choice of the sigma in these predictor corrector methods is just too aggressive. Have you given any thought to, or, I don't know, has anybody? Because it's so nice to have step length equal one. Because then you know forever after you do have exact feasibility.
01:06:03.294 - 01:06:05.284, Speaker A: Is it worthwhile?
01:06:08.424 - 01:07:02.172, Speaker B: I would have mixed feelings about it. The reason for that is that sometimes not having perfect primal visibility and perfect primal dual visibility actually leaves us with more room to maneuver. So I can imagine situations in which it would be very welcome to obtain perfect primal feasibility and perfect dual feasibility early enough and then only continue the struggle to reduce complementarity. But I can also imagine situations which are just the opposite, that if we force ourselves to be primal and dual feasible, we will significantly restrict the area to maneuver and we will have to do quite a few interior point iterations. So, tricky question. No, no easy one way or another answer, Henry.
01:07:02.348 - 01:07:14.804, Speaker A: Thank you. Anybody else? Okay, and in that case, thank you again for very nice talk and all the nice questions and answers.
