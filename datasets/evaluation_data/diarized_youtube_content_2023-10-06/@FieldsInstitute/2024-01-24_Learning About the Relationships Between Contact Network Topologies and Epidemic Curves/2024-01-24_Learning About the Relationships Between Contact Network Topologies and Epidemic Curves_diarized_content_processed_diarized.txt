00:00:00.160 - 00:00:16.914, Speaker A: Is doctor Michael Wolfson from the University of Ottawa from the School of Epidemiology and Public Health. And he's going to talk to us today about contact networks and epidemic curves, and he's a member of the MFPH research group. All right.
00:00:19.374 - 00:00:57.062, Speaker B: Thank you, Ben, and hello, everybody. Thank you for joining us. I'm a relative novice at infectious disease modeling, so I thought it better for me to talk about what I've been learning and raise some questions, in particular about the relationship between contact networks. There's a lot of discussion about how people come in contact with one another in order to pass infectious diseases and what the shape of the epidemic curve might be. Now I'm hitting page down, and that doesn't seem to be moving me anywhere on this thing. Let me try and give the focus. There we go.
00:00:57.062 - 00:02:23.154, Speaker B: So the most common approach in infectious disease modeling is ordinary differential equation compartment models, and the key data input is daily new cases. There are other fundamentally important inputs, like how long the infectious period is likely to last, probabilities of transmission given contact, and the probability of contacts. But in virtually all of these models, there's a very strong assumption about random mixing, that the chance of me coming into contact with you is equal to me coming into contact with somebody anywhere else in the geography which is being modeled, there is, of course, some disaggregation by age groups, particularly to distinguish school age children from adults. But my concern here has been that this assumption is almost certainly wrong, very wrong. And so one of the questions is how much bias is introduced in the modeling with the assumption of random mixing? And when we have more realistic assumptions about contact patterns, what kinds of data collection would be most useful for epidemic preparedness? Let me pause for a second. You know, I propose to talk for about half of the time and leave lots of time for discussion, because I'm going to end with asking you guys some questions. Another point is that I spent a lot of my working career at Statistics Canada, so I have to admit to being kind of a data geek.
00:02:23.154 - 00:03:26.094, Speaker B: An alternative theoretical approach to differential equations is using networks. It's not a new idea. It's been around for decades. And in a network theory or the mathematical models that use networks as the fundamental object of analysis, contact patterns are explicitly represented. This can be done graphically, where you have nodes, which in this case represent individuals, and edges represent not necessarily actual transmission, but contacts where transmission could possibly occur. And one of the key ways of characterizing these kind of mathematical networks is in terms of their degree distribution, namely, how many edges does each node have and you can do a probability, or, you know, the frequency distribution. And the key variable here is a degree distribution variable function f of k.
00:03:26.094 - 00:04:34.074, Speaker B: And what I shown on the picture here, taken from a paper by Mark Newman ages ago, is three examples of different kinds of networks, a regular lattice, something where everybody's connected to everybody else. This middle one is what's implicitly being assumed in the ode, random mixing, and then a random graph. The question is, you know, how realistic is this fully connected thing? Well, unfortunately, there are no good data for the canadian population or american population or some country population, but there's an awful lot of suggestive evidence from more specialized subpopulations. So here's a an example of a network of co authors of papers on network theory from a paper by Newman and Park, and they've gone further. You know, we'll come back to this to cluster these different sets of connections, but it's clearly not a random network. So the image I just showed was context defined by co authorship. For infectious disease.
00:04:34.074 - 00:05:33.886, Speaker B: The relevant thing is contacts being defined in terms of possibilities to transmit disease. There is a considerable, very substantial literature on abstract disease networks that have explored the relationships between their topologies and disease spread. If we take a simple theoretical abstract network, an epidemic curve, given the network, can easily be computed. In the simplest case, where each node is either susceptible or infected, we have a binary value. We can use a discrete time step. It could be 6 hours, 4 hours a day. And at each time step, you look at everybody, every edge, and if you see an edge where one person is infected and the other node is susceptible, you say, well, what? You know, we'll just apply a probability of transmission to those nodes, consult our random number generator many times, and update the nodes, and then move on to the next step.
00:05:33.886 - 00:07:25.544, Speaker B: And after we've done that for a while, we can count up the number of nodes that are a one rather than a zero, that are infected at each time step, and the result and plot that time series, and that would be the epidemic curve. These kinds of network models are easily extended, and if we assume that we're going to use computer simulation, which is one of my favorite things to do, we don't need to worry about the analytic tractability, although a lot of the mathematical literature in this area does use applied mathematical analysis in order to solve for key characteristics of, among other things, the epidemic curve. By the way, an awful lot of this analysis has been done by physicists who have something called percolation theory that they've used in other contexts. So we can add to the simple loop that I have on the bottom of this slide that I'm showing now. Say, well, instead of just having a zero or one for each node, we can have something like how long the person has been infected and have a parameter about how long infection and infectiousness should last. Similarly for immunity post recovery, whether it's an SIS or an SI R, and then s kind of a model. One of the papers that I've looked at says, well, what if we allow people to break edges, so if they see their neighbors are infected, they'll on their own self isolate, so that there would be a disappearing edge, at least temporarily.
00:07:25.544 - 00:08:10.054, Speaker B: One can also model vaccination by changing the probability of transmission. And in general, these kinds of network models, even if they can't be solved mathematically, they can be solved generally using agent based models. In simulation, they're upward compatible from conventional compartment models. In other words, they're generalization. Networks can be visualized graphically, but practically they can be represented by square matrices. If you have n nodes, then it's an n by n matrix. And if you don't worry about whether the connection is just one way, but you allow connections to go both ways, then it's really only a triangular matrix.
00:08:10.054 - 00:08:58.533, Speaker B: And as I said, these can be written as agent based models with potentially fully interacting populations. So why develop the network models? Well, real world contact patterns are not. I already showed you one example, and here are four more examples. Two in the first picture, and these are the degree distributions from a paper by Newman et al. Mark Newman is a big name in this area from the University of Michigan at Ann Arbor. And these I've used DD for degree distribution. In random mixing, these graphs would be zero everywhere except for a spike where everybody at level n has n connections.
00:08:58.533 - 00:10:08.244, Speaker B: There's been considerable discussion, you know, I'd say in fits and starts, not so much in the mathematical literature, but early on in the pandemic and the popular media about super spreaders, it seems to me that that discussion was a little bit limited because they seemed to talk about super spreading events. You know, if everybody was at a choir together singing, or at some big meeting of, I forget what the corporation was in Boston. But we learned, I think, in SARS 120 years ago that there could be an infected person in BC and another infected person in Toronto. The pandemic took off, or the epidemic in Toronto, but not BC. Well, depending on how many contacts that one individual had. And there are also, I think the phrase is not politically correct in parts of the US, but if we talk about structural factors. There are structural situations like workplaces, meatpacking, or larger households where there's a greater likelihood of contact.
00:10:08.244 - 00:11:23.910, Speaker B: So it's been known for ages, and I've cited Anderson and May and May and Anderson from like 30 years ago, that if you want to reduce transmission, you should target the most highly connected nodes or individuals. The idea that COVID in fact has a fat tail degree distribution, hence the likelihood of super spreading either events, individuals, or structural situations, has also been. There's evidence induced in this paper by Hwang and Collins, though as I already noted, more general data on full population degree distributions are lacking. An important class of degree distributions is power law, where f of k is k to the alpha. My apologies for not doing the nice mathematical representation, but I was just using PowerPoint here. The propagation of diseases on power law networks is analytically tractable, and Mark Newman, among others, have shown the mathematics for this. But I have to say, part of my motivation for this whole talk and getting into this field was having met Mark Newman more than a decade ago.
00:11:23.910 - 00:12:17.592, Speaker B: And I remember him saying, r naught does not exist in an important class of power law networks. And if you're curious about having a more formal demonstration of this, you can look at the couple papers that I've noted here. But almost everywhere that one was seeing discussions, even on the popular media. When the Ontario table was talking, Staney Brown, or whatever, you'd hear mention of Arnott as if Arnaut was. There was absolutely no controversy or question at all that it was a meaningful concept. But here we have the mathematics to show that as soon as you relax this very restrictive assumption of random mixing, and indeed assume a heavy tail distribution, in particular, power law distribution, it makes no sense, it has no meaning. So there are.
00:12:17.592 - 00:13:06.054, Speaker B: The interesting thing is, in addition, there's always the degree distribution in these abstract networks. And to start, the kind of epidemic curve that one generates from a network model will depend on the average degree. Whether it's the average number of connections is five or seven or eleven or 20 does make a difference. But the interesting thing, it was my intuition that this was the case. And we started out with a postdoc colleague, Sun Ju, writing some code. But then, as I dug into the literature, I found that we didn't really need to write any code and do any modeling ourselves. An awful lot of this stuff has already been well established in the published literature.
00:13:06.054 - 00:14:04.660, Speaker B: And in particular, one of the interesting things is you can construct two random networks, random graphs that have identical degree distributions, but differ in other important characteristics. More generically, we'll refer to it as its topology. So one of them is this idea of clustering. And first order clustering is really simple. It's a kind of a transitivity, so that if a is linked or connected, or has an edge with node b and a is linked or has a connection, or has an edge with node c, then the question is, how often is b connected to c? And this concept has been formalized and explored here by green and kiss more than a decade ago. Another thing that's been around for a long time, I think Barabbasi and company, 20 or 25 years ago, talked about what they call a small world phenomenon. There was the sociologist who, you know, did this six degrees of connection.
00:14:04.660 - 00:14:51.024, Speaker B: You know, how many steps does it take for any one person to be able to connect themselves to any other person on the planet or in the United States? So that has to do with the average path length, or there are different metrics for ways of measuring. You know, you'd need a recursive algorithm. And when n gets really large, it might be computationally quite expensive to measure it. But anyways, this seems to well more than seems to. It does have an effect on the shape of the epidemic curve. If you have two networks that are identical in terms of their degree distribution, they have the same heavy tail, positively skewed distribution, things will still play out differently. But interestingly, there's a paper I came across by Haw et al.
00:14:51.024 - 00:16:15.936, Speaker B: From a few years ago, where he finds that fourth order clustering is more important than 1st, second and third degree clustering. So the proportion of the time that you find that any set of four nodes that could be connected by three edges are actually connected by those edges, matters a lot to the topology. Interestingly further, that while the ode formulation always results in epidemic curves that start out with an exponential growth phase, these analyses have shown that they could be sub or super exponential. So the shapes of the curves are not necessarily what you would think, or are necessarily what you would expect from only an ode kind of approach. So what are some of the implications of network theory for, for emerging infectious disease modeling? Standard compartment models, as I've already said, embody seriously unrealistic assumptions, especially random mixing. There is a theoretical alternative, which I've just briefly reviewed, and if you want, you can dig out the references that random mixing is a very simple, singular special case. There's only one fully connected random graph.
00:16:15.936 - 00:17:15.708, Speaker B: Otherwise you can use random graphs and computer simulation. And when you vary the topologies, these other characteristics, beyond the degree distribution itself matter. But let me turn to the second half of my thoughts here, and the data geek part of me is going to emerge, which is the empirical implications. It seems to me that an obvious one is you need better data if you really want to take on board and relax the assumption of random mixing. And I'm going to go through four examples here and come out with the idea of a digital twin for Canada. But before doing so, I also recently came across this paper by Stevek and Berrill in science a couple of years ago. And, you know, they've referred to some of the same, essentially the parallel literature to what I've just referred to.
00:17:15.708 - 00:18:22.952, Speaker B: And what they say is, in the context of an epidemic, although each contact carries a risk of acquiring an infection, real world social networks are complex, often exhibiting extreme heterogeneity in the number of contacts. And these have large scale effects on the spread of infection. And many, they go on to make, you know, a series of strong points with respect to the socioeconomic and equity implications of the failure to take account of the heterogeneous nature of the contacts. In many countries, those working in low paid and public facing jobs at the highest risk of being infected, and particularly in Canada, those involving long term care, residences, and personal support workers leveraging network heterogeneity and infectious disease models may better demonstrate these differential risks. People living in multigenerational households, serving in high exposure occupations, and residing in densely populated communities could be prioritized. I think they're being a little gentle and could, might be, should. Let me go back then.
00:18:22.952 - 00:19:18.294, Speaker B: There's a large computer model called episyms that was originally developed by Los Alamos national laboratories. And they specifically gathered data on hundreds of millions of individuals across the United States. And they broke people down by geography, by transportation, and all sorts of characteristics. So they assembled all this data, and when they looked at the number of contacts implied by just using these mostly census type data, but I think they also had some transportation data. This is the shape of their degree distribution. It's not anywhere near a random one. It's a heavy tailed, not quite power law distribution, although this is just for southern California, so highly skewed.
00:19:18.294 - 00:20:21.986, Speaker B: And they stimulated for Los Angeles county here the attack rates of infectious disease by census tract. And this was still from an animated image. I got into this personally back around 2009 when Babak Purbelal got in touch with me when I was at stack can and said, michael, can you help us? We've built this model for Vancouver, and we'd like to extend it across the whole country. It was for h one, n one. And they, you know, according to Babak, right, we use contact network epidemiology to compare intervention strategies based on explicit mathematical models of heterogeneity differs from fully mixed compartment models. I've just bolded some of the key phrases so you don't have to read all the words. And contacts can take place within households, schools, workplaces, hospitals and other public venues.
00:20:21.986 - 00:21:03.584, Speaker B: And they had already built this contact network with 2000 households. And here's some pictures or images of the structures that they have. So they have kids in classrooms and schools. On the left, hospitals, shopping centers. For the stuff that I pulled together from the 2006 census. For Babak at the time, he was also keen to distinguish hospital workers from all others, obviously because of their higher likelihood of exposure. And this is where the context within which I met Mark Newman and heard him comment that Arnot simply does not exist.
00:21:03.584 - 00:22:17.474, Speaker B: Here's a couple of other graphs. The left is the household size distribution and on the right is school size distributions. They do not, you know, they're not about the degree distribution directly, but what they do clearly suggest is that contact rates or contact probabilities are not random. They're going to be skewed, positively skewed. I came across more recently a bunch of software called Covasim, and they're building on the same kinds of ideas we already saw in episims and in Babak's stuff for Vancouver, where they say, we want to keep track of community contacts that are broken down amongst household contacts, school contacts, workplace contacts and everything else. And so they specifically got the software to model these kinds of patterns. You can produce various kinds of data, hopefully, and then it will generate a contact set of contact patterns that are more complicated, more richly structured and textured.
00:22:17.474 - 00:23:20.908, Speaker B: And it includes, for example, on the upper left, the kind of age breakdown that has been used in a number of extensions of more conventional ode compartment models, where there's variation in the contact probabilities. Compartments are disaggregated by age group, but you see also that they're making explicit school networks and workplace networks. But lastly, let me also mention a huge digital twin and agent based model that's been developed in the United States. The key guy there is Madhav Maratha. And I've indicated YouTube where you can. It's along YouTube, but at about the 27 minutes mark, you can see his discussion of the digital twin and the agent based model. These folks put together this model with over 300 million nodes.
00:23:20.908 - 00:25:21.118, Speaker B: So basically representing the US population 10 billion time varying edges, 3243 counties, all the counties in the United States. And according to Murat, during March November 2020, they ran about 10,000 simulation simulations per week. They were only able to do this because Madhav Murat is responsible in Virginia for their supercomputer cluster, and he has a colleague, Sean Brown, at the University of Pittsburgh Computer center. And they were able to couple these two high performance computing centers in order to do this huge number of runs that included not only they were simulating various kinds of non pharmaceutical interventions, lockdowns and physical distancing, subsequently, they were simulating different kinds of rollouts of vaccination. And judging by some of the co authors, I'm wondering whether this is a descendant of epistems originally developed at Los Alamos Labs, because common co authorship there here's an image of the kinds of digital twin, and my goodness, it looks a bit like episyms, and it looks a bit like Babak, and it looks a bit like Covasim in terms of the idea of having these different kinds of structures, households, workplaces, etcetera. So what are the implications of both the theory, which was the first few minutes of this talk, and the examples that I've just shown you there, is pervasive. The real world is pervasive heterogeneity, and it includes by age, household competency, composition, neighborhood workplaces, school and other main activities, and socioeconomic status.
00:25:21.118 - 00:26:32.588, Speaker B: We also should be cognizant of importations. So ideally, we would really want highly disaggregated data in order to observe disease spread, to have the foundations for projecting disease spread, for modeling, simulating possible interventions, and ideally, to do it all with an equity lens. Odes, as far as I'm concerned, fail miserably on all of this. It's not like I don't have a prior view here, but in order to assemble these kinds of data, and I have a fair bit of experience at Statistics Canada, it's not good enough just to say, well, for emerging infectious disease preparedness, we really ought to spend $100 million a year creating a digital twin. By the way, I don't know where I put it. We'll come to it, but the EpI sims, and then most recently Marat's thing, has all kinds of Defense Department spending behind it or underneath it. So, but the idea is that digital twins, ideally and in practice, should be useful in all sorts of contexts above and beyond infectious disease modeling.
00:26:32.588 - 00:27:17.154, Speaker B: So the idea is that it would be good if we could collect once and use many so finding a wider range of data needs would be critical for longer term sustainability for such a richly detailed evergreen data set. So the idea is, I think Canada should be constructing a digital twin database. So in the next few slides, let me briefly sketch what this would involve. It would be highly multivariate, individual level microdata file. As we saw with the latest us one, that's 300 million individuals. Canada, 40 million. Not a big deal.
00:27:17.154 - 00:28:29.466, Speaker B: From a distance, it would look like an augmented long form population census microdata file. So 40 million records, preserving the correlation patterns of covariates, household size, occupation, geographic location, have fine grained geographic detail, more finely grained, for example, than Peel county by Toronto, where the lockdown was all of Peel county, which was, you know, the proverbial hitting of a fly with a sledgehammer. And it would be hierarchical in structure. We would have individuals nested in households and turn nested in neighborhoods, workplaces, schools, and other important venues, as we've seen in those pictures from the previous examples. So how might we construct a digital twin database? The starting point would be the geodemography of the canadian population, individuals within households and small geographic areas. And then we would add factors of general interest, occupation, industry, education, ethnicity. Why is that important? Well, it's important for understanding aspects of disease transmission, but it's also of much more general interest.
00:28:29.466 - 00:29:26.364, Speaker B: There is a firm in Canada, and veronics analytics, and its bread and butter is creating precisely this kind of digital twin, except they do it on a small area basis, not on a micro databases. And then one would also want to impute added factors that would be key for infectious disease spread, in particular, time spent in various settings. It's crucial that this resulting digital twin database be non confidential to support basic disease surveillance, to enable broad use and sharing as an input to epidemic modeling, and to support many other uses between epidemics. The good news is epidemics are rare. The bad news is that they're rare. So that one or two years, I think we're seeing it already, interest in funding, the kind of infrastructure and the capacities needed to understand an epidemic wane. Their budgets get cut.
00:29:26.364 - 00:30:29.810, Speaker B: So we want to be able to make sure that this investment is such that people would want to keep it going between epidemics. The main approach, in order to ensure it's accessible, is to use only already published data as input, so that by construction, the resulting digital twin database is non confidential. There is a growing literature on using artificial intelligence, in particular artificial neural networks, as a way of constructing synthetic data. You know, I spent a little while at a conference a month ago on this. But I quoted Stadler here saying, synthetic data does not provide a better trade off between privacy and utility than than traditional anonymization techniques. So from an infectious disease perspective, there are further data needs. So clearly one wants to have a count of newly diagnosed infections and their genotypes.
00:30:29.810 - 00:31:25.604, Speaker B: You want to be able to keep track of vaccinations, hospital admissions, and be able to infer things like transmissibility. How long does an agent asymptomatic phase last, et cetera. Those are not included in this sketch of a digital twin database. Those would be corollary kinds of data collection. Another challenge is that for the digital twin database that was very briefly sketched a moment ago, the key input data, the population census, is always at least a few years old. The good news there, though, is Statistics Canada has a really powerful micro simulation model called demosim that runs off the long form census and can be used for now casting for updating the census. Right now, the most recent is 2021, but a year ago, the most recent was 2016.
00:31:25.604 - 00:32:27.764, Speaker B: Another crucial area is the lack of real time data to update physical locations and mobility patterns. We have the potential to do this with cell phone data. There was a big foo for with tullus and the public health agency a year ago with a bunch of erroneous false malarkey claim by one opposition political party. So it will be absolutely essential to develop a good protocol for protecting privacy and obtaining a social license, as it were. The public consensus that it won't be everybody. There's always going to be folks who are unwilling to believe the possibility here. But my view is there's no doubt that one can do a reasonable job of preserving privacy, yet at the same time developing a capacity for public good and for pandemic disease modeling for real time updating.
00:32:27.764 - 00:33:59.626, Speaker B: So let me conclude there. A little over a half hour here, and I began to hear you guys comments and discussion. So how many unnecessary deaths and what were the economic costs in Canada from a failure to have a digital twin database informing the epidemic disease modeling here? We had overly broad lockdowns. I think we had no attention being paid for the longest time to multiple job holding by personal workers and nursing homes. There's a question about whether the school closings were in some cases excessive, with undue or insufficient weight being given to the mental health and family aspects of children staying at home compared to the risks of disease spread. If we persisted in, as we did in using compartment models with random mixing, how biased would our model results have been the sub or super exponential growth phase. The examples that I alluded to at the beginning, the first half, the relationship between the original initial growth curve and the growth path, and the final peak size of the epidemic varied across these more interesting kinds of network topologies.
00:33:59.626 - 00:34:54.624, Speaker B: It wasn't the same as what you would get from an ode. And finally, compared to other important data gaps, what priority should be given to developing a digital twin database, given that it would be multi use? You know, there were clearly major gaps in terms of, for example, data linkage in a number of provinces for testing, vaccination, hospitalization. There was a shortage and very slow take up of development of prevalence surveys and serious problems within various provinces with genotyping. Canada was to an unfortunate extent a free rider, in particular on studies from the United Kingdom. So that's my last slide and I'm keen to hear comments, questions and discussion. So, Ben, should I stop sharing at this point so we can see everybody?
00:34:56.684 - 00:35:01.024, Speaker A: It all depends on what you want to do, whether or not people are going to ask you to switch to other.
00:35:01.404 - 00:35:07.124, Speaker B: Well, I'll keep sharing them for the moment. So, yeah, thank you.
00:35:07.244 - 00:35:20.684, Speaker A: Anybody who has any questions, feel free to raise your hand or type it into the chat. Richard.
00:35:21.904 - 00:36:25.404, Speaker C: Oh, hey, thank you very much, Doctor Walton, for the very exciting presentation. I'm also kind of like conduct my research about the random network model, especially from Newman's percolation method. And I kind of like wondering if you think is it like a worthy trade off between two use random networks. We just need a degree distribution or we really have this data twin which we can somehow get exact structure of our network that really worth it because the random networks were using mean failed ideas to have an average among all possible ability, which seems to be more easy, like the data is more easy or like the model is easier to construct it with. But like to do exact network structure will need a quite large data set and a lot of like modeling and analysis based on it.
00:36:26.864 - 00:38:05.886, Speaker B: I think they're complementary, but I wouldn't stop at the random networks or the mathematical modeling, you know, as I hope I've indicated, the mathematics that people have done is very useful and highly informative of intuition and saying, gee, this thing really does matter. And this simplifying assumption of random mixing is not only inaccurate, but it leads potentially to wrong conclusions. But I guess I've spent too much time over the years in applied policy analysis. And once you've got the intuition and you've sorted out at least some feelings for where you should be going, what kinds of information you should be collecting, then why stop there? Why not collect the information? The big challenge is, and Canada in a way, is so close, like the half a billion dollars that's spent every five years on the population census and on all kinds of in between, in terms of updating the geography that's there. So the marginal cost of creating something like the digital twins that Babak wanted to build for Vancouver and then Canada, and what the US has done and what Covasim is able to do if it's fed the right data, we're talking there. We're not talking hundreds of thousands of dollars or a million dollars, which is what the granting councils typically are able to disgorge. We're talking several million dollars a year.
00:38:05.886 - 00:39:32.712, Speaker B: But given the billions of dollars of costs from screwing up, and my guess is hundreds if not thousands of lives lost unnecessarily from not thinking this through as well as one could, that's a small price to pay. And finally, as I've sort of emphasized, creating the right kind of data, hopefully will serve many purposes. You know, my favorite example, you know, in other contexts, in the health area, I worry about what some folks, what I call indicatoritis, and if the only thing we were interested in was life expectancy, that would be a very expensive indicator because we'd have to do a population census and we to get denominators of the mortality rates, and we would have to do complete death registration. So call that 100 million a year, half a billion spread over five years, plus the vital statistics program. But of course, we do the population census and vital registration for a myriad other reasons. And it seems to me that the marginal cost of 510 $20 million to have an evergreen digital twin that's updated with demostim and takes advantage of cell phone data once the privacy protocols are sorted out, would have tremendous value in all sorts of other venues. So to come back to your question, Richard, sure, do your random graphs and all that.
00:39:32.712 - 00:39:42.964, Speaker B: There may be other things that will be interesting to inform our intuition, but let's use those intuitions to inform a much smarter collection of data.
00:39:44.034 - 00:39:46.454, Speaker C: Yeah, thank you very much. Thank you for the answer.
00:39:50.594 - 00:39:53.614, Speaker A: All right, Justin, did you want to mention your comment?
00:40:04.474 - 00:40:05.814, Speaker D: Ben, can you hear me?
00:40:06.474 - 00:40:12.750, Speaker B: Yeah, I can hear you now there.
00:40:12.782 - 00:40:13.874, Speaker D: Ben, can you hear me?
00:40:14.614 - 00:40:22.318, Speaker A: I can. Maybe he can't hear us.
00:40:22.406 - 00:40:45.894, Speaker D: Oh, sorry, that. Sorry, that's on me. Yeah, no, I just. Based on your description, Michael, it seems like there's already basically some of the smartest people in the world with tens of billions of dollars and a decade ahead of us, constructing digital twins of the society. But they're trying to sell us crap on Amazon versus trying to model infectious, infectious diseases.
00:40:48.314 - 00:40:54.154, Speaker B: Can you expand on that when you're talking about the tens of billions and crap on Amazon? What are you thinking of?
00:40:54.274 - 00:41:44.044, Speaker D: Well, no, so I'm thinking about the fact these online data brokerages, and a good example would be Facebook, have been mining our data for, since it started developing, presumably massive databases on everything from your movement, who's in your social network, what your education is, male, female, age, how much you make, etcetera, is all part of their data gathering process right now, the people that are working on it, I mean, essentially like they're hiring phds, you know, hundreds of thousands of phds, just to get it. The only issue is that their objective is get you to click on more Amazon ads versus they're not doing it to presumably model infectious diseases again, you'd buy more crap.
00:41:46.424 - 00:42:58.224, Speaker B: Yes, I agree. And I used to talk about a privacy chill up until about five or more years ago, because of all of the people, for example, in vital statistics or provincial ministries of health, who are blocking the flow of data, for example, across provincial boundaries, in order to facilitate bona fide public good research. But since the growth of Amazon, Facebook, Twitter, Google, it's become a much more complicated environment to think about privacy and digital twins, because we've got these huge, what are some people call them, Vlop, very large online providers who are running roughshod over privacy permission and all that kind of stuff. So it's harder, I think, for the, for a government or for Statistics Canada, in Canada's case, to push ahead when you've got this, a terrible situation going on with the vlops. So, yeah, it's unfortunate. Thank you.
00:43:00.084 - 00:43:01.484, Speaker A: All right, Edamar.
00:43:01.644 - 00:43:39.644, Speaker E: Yeah. The fully connected graph that you showed at the very beginning was super symmetric. And maybe you have touched on it, but I'm not sure. Is it possible to systematically break that symmetry according to some characteristics of contact tracing and other situations that will be then modeled more specifically?
00:43:44.484 - 00:44:45.944, Speaker B: I'm just going up here. Yeah. This graph in the middle, the fully connected graph, but I'm not sure what's your intuition, if I can turn it around, as to how breaking symmetry is going to be interesting in terms of drawing out the implications of a particular topology for a network represented graphically. I've already alluded to a number of key characteristics, the first one being the degree distribution, the second one being the number of the extent of clustering, and there's different orders or versions of clustering and the business of small worldness. Is there something else that you have in mind about a topological property of one of these random graphs that might be salient?
00:44:46.324 - 00:45:16.014, Speaker E: For example, if I just pick up one of the nodes and I say, let this one now represent low income or low facility population, then it's not the same as the rest of them, which have a better facility or better life, if you want.
00:45:18.154 - 00:46:19.920, Speaker B: Oh, I think, I think being able to differentiate the nodes according to, you know, the example you've given is socioeconomic status is fundamentally important, but I guess that's an extension of the network graph. But once you start doing that, these are no longer nodes that are either zero or one. Each node would have a suite of attributes. My view is you move into the agent based modeling type stuff. You can have discrete or continuous time, but whatever, you update the nodes according to, in the first instance, the disease dynamics. But you could also update them in terms of information on spatial mobility and in terms of lockdowns, losing their income or temporarily being, losing their job or temporarily staying home for a longer period of time because the school is closed. Actually, you wouldn't see stay at home.
00:46:19.920 - 00:46:39.680, Speaker B: You'd hang out at the shopping mall and pass the disease there. So I don't know, am I getting at your concern about making these nodes more richly structured or textured or characterized and simply, yes, no, infected or not infected? Yeah.
00:46:39.712 - 00:47:04.464, Speaker E: Well, in principle, some nodes may be different than others, but maybe I can try another angle here. The random mixing that was key in your talk, if I understood it, can it be replaced in a simpler model of approaching things by random walk?
00:47:07.364 - 00:48:18.264, Speaker B: Short answer is, I don't think so. And, you know, let me jump on the word simpler. You know, I think it's Einstein who's attributed with the phrase keep it simple, but not too simple. And there's a guy named Hal Ashby, who was a famous fellow in information theory, and he talked about, I think it was requisite variety picked up by Stafford beer. I like to use a modified phrase, requisite complexity. You think about the problem you're trying to tackle, and following Einstein, you want to have a model or an approach that's as simple as possible, but you need a requisite level, you need a minimum level of complexity in order to be able to take account, to absorb or to reflect what you've learned is essential. And if you learn that, part of what's essential is that some nodes are more highly connected than others, or some nodes hang out at places where they're more likely to pass infections around, then the canon, or the criterion of requisite complexity says, well, my model really ought to include that.
00:48:20.004 - 00:48:30.384, Speaker E: Okay, but can you break down the random mixing to weakly partial mixing?
00:48:31.004 - 00:48:44.134, Speaker B: Strongly mixing, sure, you can do that, but once you do that, I'm not sure I bother sticking inside an ode framework. I moved to an agent based model.
00:48:45.754 - 00:48:55.054, Speaker E: Okay, that was just curiosity, not really criticism. Your models and descriptions are very impressive. Thank you.
00:49:01.814 - 00:49:12.994, Speaker A: All right, I had a quick question for you, Doctor Wolfson. The digital twin versus the agent based model, is that just a larger version of an agent based model? Simulation?
00:49:13.654 - 00:50:27.744, Speaker B: No, digital twin is the data foundation for the agent based model. So the idea is you'd start with a, well, in some simulation modeling, it's called the starting population. Say, okay, I'm going to start with a population of whatever, 40 million synthetic individuals, and I'm going to have some laws of motion, as it were, so that I can update, well, each one of those individuals, 40 million has a vector of attributes, a set of characteristics, age, where they live, are they infected, occupation, so on. And then the agent based model starts with that population database, ideally as of January 2024, and then it updates. It could be on a daily or weekly basis, depending on what kind of data you have, and says, okay, it got seeded with an infection here. And how is that infection going to spread? So the agent based model takes the digital twin as input and updates it, in effect, over time projects.
00:50:28.044 - 00:50:47.684, Speaker A: I gotcha. And the second question I had for you, when you're talking about the cost to set up a digital twin, are you thinking that the main cost would be computers, computing power and building the ability to do it? Or you're talking about the cost to create the data in the first place?
00:50:49.344 - 00:51:41.644, Speaker B: Sort of neither of the above. What it is, is the census already produces very detailed crosstabs by census tract, for example, and it creates public use files. So there's a technique called simulated annealing. So basically you need a data scientist and a bit of computer time, not a huge amount of computer time. You can probably do this on a capable laptop. You don't need a supercomputer to say, okay, I want to sprinkle individuals or create synthetic individuals whose age and household structure match the tables from the census. So the crucial input here is not computer time per se, or software data scientist time to create the digital twin.
00:51:41.644 - 00:51:47.904, Speaker B: And there are a variety of techniques. Synthetic matching and simulated annealing are two of the key ones.
00:51:49.444 - 00:51:58.904, Speaker A: All right, thank you. Anybody else with any more questions? Mortazza, you had a comment here.
00:52:07.144 - 00:53:14.904, Speaker F: Thank you, Michael, for your presentation and raising the critical point regarding the contacts. I work on this contact matrix and I applied the age group contact matrix in my research and simulated for the results and I got that the results is different from those OD models that just assume same mogenius contact on the. Yeah. The question that I wanted to have your comment is that regarding the policies during the pandemic, how can we simulate for the policies and their effect on the context? Because it's important for me that during the pandemic, the policies effectively affect the distribution and also the number of contacts. So how can we do that regarding the importance of the contact model? Thank you.
00:53:15.564 - 00:54:16.926, Speaker B: Yeah, that's a very important and challenging question because it's one thing to construct a digital twin, but it's going to represent the recent past. It's not going to represent a particular how people's movements and daily activities unfold over time. So it seems to me that the most important new ingredient that we have now, which we didn't have five or ten years ago, was potentially data from cell phones. Not everybody has one, but most people have one. And even if you're not turning on your location stuff, the towers can tell where the cell phones are. So that the idea of getting six hourly or four hourly, or ideally not just daily updates on where people are moving around will be critical. And that could be used with some sophisticated data science kind of analysis to update.
00:54:16.926 - 00:55:15.104, Speaker B: You say, well, people are unlikely to move house, although some of that did happen as people moved to the country during the lockdown, but that was a slower, you know, multi week or multi month process. But in terms of lockdowns or school closures and things like that, that would be revealed by the cell phone data. Of course, a bunch of simplifying assumptions are still going to be required to understand or to make guesstimates, but I think they would be far superior to having no data at all. I don't know if that helps. And do you have any other, you know, the neat thing is cell phones do provide potentially almost real time data. The labor force survey and the Canadian Community Health Survey potentially provide monthly data, but I think that's too slow for purposes of rapidly spreading infectious disease.
00:55:17.804 - 00:56:23.924, Speaker F: Thank you. But you know, that just to synchronize this, the mobile data to census, I mean that to disaggregate it into the age groups, it needs more to manipulate the data, to work on the data because as my knowledge, first of all, all mobile are not under the name of those people that are using and the other problem is that maybe they are not using them always during the day, their activities, and we cannot figure out their specific location and the contacts with others, you know, but the problem is that if we have the data exactly for all people and assume that everyone is using and we can trace the location, then yeah, that will be true. But otherwise we have to make too many assumptions to have at least a reliable model or database to apply in the modeling.
00:56:24.744 - 00:57:19.326, Speaker B: Yeah, I think approximation and guesstimating are going to be inevitable. So, one thing. This is where an important discussion needs to be undertaken about what are the boundaries of preserving privacy. But most cell phones spend night in a household, and if you have some geographic data, it can't be too fine grained or it would be potentially an invasion of privacy. You would be able to say something about what is the age structure of the households where these cell phones are resting at 02:00 in the morning. And somehow extrapolating from those data to get some idea about, you know, what portion of these cell phone pings are school age children versus adults versus seniors. Some inevitably guesstimating will be required.
00:57:19.326 - 00:57:39.694, Speaker B: I don't think we should expect or aim to do significantly better than that because of the privacy constraints. Constraints. But just doing that would be a major advance over basically the black hole of data that we have now. I don't know. What do you think that's true, but.
00:57:40.314 - 00:58:05.984, Speaker F: The problem is the level of approximate. Yeah, you talk about the costs and benefits. Any policy that restrict the context as cost and economy, you know, and on the other side, if you wanted to rely on such a model that is a little biased, we should compare the cost of these two different approaches and then decide them.
00:58:08.164 - 00:58:13.356, Speaker B: Okay. Yeah. Thank you. I don't disagree. You know.
00:58:13.420 - 00:58:15.984, Speaker F: No, no, no. We are talking to each other.
00:58:17.504 - 00:58:37.444, Speaker B: No, I think, you know, I've just, you know, in my kind of way, I like to, you know, I've sketched an idea, but, you know, I don't know, there's probably dozens of masters and PhD theses that are waiting to be done in order to figure out the next couple levels of detail about how best to make this thing happen and to work.
00:58:39.504 - 00:58:41.804, Speaker F: Sure. Thank you, Rishi.
00:58:43.164 - 00:58:53.464, Speaker A: All right, well, I think that's it for questions. So thank you very much for the great presentation and very thoughtful discussion over where we go from here.
00:58:54.204 - 00:58:55.604, Speaker B: Okay, well, thank you, guys.
