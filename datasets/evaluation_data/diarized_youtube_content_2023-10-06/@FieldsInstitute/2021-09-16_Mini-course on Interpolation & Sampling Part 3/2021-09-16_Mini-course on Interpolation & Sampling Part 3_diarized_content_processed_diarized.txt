00:00:02.520 - 00:00:30.984, Speaker A: Hello everybody. Welcome to the third lecture of Andreas. If you have missed the first two or you have listened, but you want to listen again, they are available on YouTube and the link to YouTube is on the Fields website. Please go there and watch again if you wish, or watch if you haven't have been absent. So Andreas, please with the third lecture.
00:00:31.764 - 00:01:14.878, Speaker B: Okay, thanks a lot for the introduction and for giving me the opportunity to talk about all this stuff. So, the third lecture will be, as I announced earlier, on, a specific technique to solve interpolation problems. And it will be the debar method. Well, debar methods, they can be used in different contexts. And I would like to show you a specific situation, connectionless interpolation. And the setting I've chosen is the setting of Fox spaces. I could also have chosen Bergman spaces.
00:01:14.878 - 00:02:22.554, Speaker B: There's a kind of meta theorem which tells you that if you are able to prove something in the fox space, then you should be able to do it in the Berman space and vice versa. Maybe that's not always true, but often it's true, and the techniques are quite close. And then you have to replace the underlying metric, which is euclidean in the fog space, and so the hyperbolic in the Bergmann space. So to make things easier, we do it in the fog space. So what is a fox space? Well, we saw definitions yesterday in the last two talks. So, fog space, you can define it in a quite general setting. So if you pick phi, a sub harmonic function on C, then you can introduce this weight here, so e to the minus phi z, and then you want this integral to converge against this weighted the back measure.
00:02:22.554 - 00:03:21.568, Speaker B: Of course, you could also consider other p's. But again, I will stay in the inversion situation, though probably for debar techniques, this is not very relevant. So again, also, as I said, the m here is llama, the back measure. And so this problem was considered in the classical fox space by Seib and Waldstein, who characterized interpolation and sampling sequences for the space when phi z is z to the square. So then you have a gaussian measure here. Then there were several generalizations to more german phi. I mentioned Ubasky and SaIp, and also in particular Bernsen.
00:03:21.568 - 00:04:41.200, Speaker B: And particularly I mentioned this paper, because I want to give you the proof which was given in this paper by Bernsen, Ortega, Cerda, in the easy situation when phi z is z squared, because then the computations are quite clear, transparent, but in fact they generalize to the more general situation. There's also a paper by Nicolas Marco, Shalima Seneda and Kimov de Garceada in 2003, where they consider a quite general situation. And while this is a very nice paper, it contains many, many tools that you can use in other contexts. There is a preprint by Sony and Konate on sampling constants using tools which have been developed there. Okay. More recently. Well, okay, this preprint I was just talking about is very recent, but I would like also to mention this recent activity in so called small fog spaces.
00:04:41.200 - 00:05:22.284, Speaker B: Small fog spaces means phi grows slowly. If feed grows slowly, it means e to the minus phi decreases slowly. And so the functions f cannot be too big. And of course, this weight phi should not grow too slowly, because otherwise this, by Uville's theorem, the space will be just constant, it will be empty zero. So. But if fire grows sufficiently slowly, then we call these spaces small fog spaces. And there were different developments.
00:05:22.284 - 00:06:37.264, Speaker B: Here's a paper by Belov, Mangesky and Saip from 2011. They made some developments related to discrete Hilbert transforms, allowing to discuss the and something problem in this connection. It should also be mentioned that for these small fog spaces, in fact, you can identify these spaces with the branch spaces for specific small phi fire of the logarithm to some power, Boyce and Rybasky, they exhibited reference respaces. So it was mentioned yesterday, I think in Marcin's talk, that in fox spaces there are no respaces, because there's some contradiction between this interpolation and sampling. I will discuss this in a minute. But if the weight phi is small enough, then respaces appear. And Sol, Borice and Klawaksky, they were able to identify respaces in this setting.
00:06:37.264 - 00:07:35.574, Speaker B: So yesterday, Marcin Bonick asked the question, does respaces exist in paddy Wiener spaces for arbitrary, measurable, rounded, measurable sets. So in these fox spaces with small, this is possible. And we were working on this problem with Baranov, Limon and Keren Kelly. And starting from this result by Borchev and Lyubarsky, we were able to give a perturbation result. So, if you perturbate this reference re spaces from the paper by Luvasky and Vojev, then if you perturbate in the right manner, then you still get re spaces. And this is kind of optimal. It's in the spirit of this Ingham Kadetz theorem that I mentioned in the very first lecture.
00:07:35.574 - 00:08:31.806, Speaker B: Okay, this is some history for these spaces. Here is more. I recall Saipes paper from 92, which is a very important paper, as also mentioned yesterday by Charlie Groszenik, who characterized interpolating and sampling sequences in the Bergmann space using densities. Densities, well, they might appear somehow difficult to compute, but normally you should be able to compute them because you are counting points and you relate the number of points to area and so on. So this should be possible. These are geometric characterizations. And, well, yesterday the question was asked about interpolation and hardy spaces.
00:08:31.806 - 00:09:15.714, Speaker B: And Hayden mentioned, I was mentioning, well, for checkable conditions, I think the situation is difficult. Okay. I mentioned also that there, the Carlson condition I discussed yesterday, and I mentioned from the very first talk on. So this infamous and lambda n, this is an extremal function characterization. I also mentioned that this is equivalent to have uniform minimality. And in fact, it turns out that a similar characterization exists also in the dormant space. And this is due to Schuster and Saipe.
00:09:15.714 - 00:10:16.944, Speaker B: And I commented on this yesterday. At the end, in fact, this extremely function characterization is again connected to its uniform minimality. But this is an uncheckable condition because it's very difficult to get ahead on these extreme functions. I also mentioned that there are many, many results in the dear clay space. And strange thing is that there are some unpublished papers, but, okay, so we have this result by Alma MacArthur, who give this result which says certain spaces separation and cards measure condition imply interpolation. Okay, this was just to complete some history on this interpolation and something problems. Okay, so let's get back to a concrete situation.
00:10:16.944 - 00:11:06.010, Speaker B: And I think the aim of these courses is also to introduce to known techniques, maybe PhD students or want to know a little bit about interpolation or sampling. So, as I said, I will consider this problem in the fox space. And now I take very specific situation, and I apologize that my normalization is not exactly the one which was chosen yesterday in Charlie's talk. He put a pie here. But I take one here. Okay, so this is the prox space of entire functions. The space is, of course, not reduced to zero.
00:11:06.010 - 00:11:44.628, Speaker B: You have polynomials in the space because the weight is big enough. This space is reproducing kernel Hilbert space. And I have a reproducing kernel. I mentioned this already in the first talk. And once you have the reproducing kernel, you can of course, compute its norm. And in the fox space, there's a natural density condition which appears, and it's defined this way. So you can define the quantities n plus and n minus.
00:11:44.628 - 00:12:27.474, Speaker B: It's just counting the number of points that you have in a disk, the euclidean disk d of radius r. Okay. And then while you walk with your disk around the complex plane, and each time you will count the number of points that you need, and you will take the biggest number, or soup of the number of points that you can meet. This is n plus. And similarly, we'll call n minus the minimal number of points that you can meet when you walk with your disk of radius r's through the plane. Of course, n minus could be zero. Okay, this is a possibility.
00:12:27.474 - 00:13:12.024, Speaker B: And then we can define densities. So of course, if you increase the value of the disk, then you need more points. This should be somehow comparable to r squared, because if you double the radius, then you quadruple the area. And so that's exactly what we do. We count the number of points and we divide by the area of this disc. Okay? Of course, if I give you an arbitrary sequence, it can be complicated to compute these numbers. But if your sequence are more regular, then you can get some, some results.
00:13:12.024 - 00:14:06.224, Speaker B: Okay, and d plus will be the limit soup of these mean number of points, and d minus will be the limit of these mean number of points. The fact of choosing here, this is not very important. Landau show. I think it follows from Landau works that you can replace this by other compact sets. Okay, so I have given the definition of interpolating sequences. So, interpolating sequences are sequence for which the restriction operator is the restriction operator. So I consider the trace of the function in this space restricted to lambda, and it should be in the weighted two space.
00:14:06.224 - 00:15:20.824, Speaker B: Here it's again one over the norm of the kernel square and for sampling sequences, while there are different ways of expressing it, while it's essentially having this double inequality, which was mentioned also yesterday, talks in the afternoon. Another way of saying it this is that the restriction operator is isomorphic onto its range. Okay, we have also a definition of uniformly discrete or uniformly separated, which means that the now euclidean distance of two points is bounded from below by some constant. Okay, yesterday in the hardy space, the metric we had in mind was the Soyuz metric. And today it will just be euclidean metric. Okay, we have two theorems, so, which go back to work by Seipp and Waldstein. The first one is lambda is a sampling sequence.
00:15:20.824 - 00:16:15.874, Speaker B: If lambda is finite. Tuning of separated sets, and at least one of these separated sets, sets which compose lambda, satisfies this lower density condition. Lower density means in the sense that if r is big enough, then all the disks, this radius, will meet the least number of points. So I was saying n would be zero. But for sampling, this case cannot happen. You have to have in a disk, if its radius is big enough, then for this radius, if we run through the plane, it has to meet least number of points. And this is reasonable, because if you want to sample, it means that YOu can recover the function from its samples, from its discrete values.
00:16:15.874 - 00:17:22.270, Speaker B: And so you have to have many points. And so, if you don't have enough points in these discs, then well, the function, you cannot reconstruct it. Similarly, for interpolation, the characterization is the sequence should be separated, and the argument to be separated is a little bit like the arguments that I have given before in the Hardy space. Again, you interpolate one, well, one weighted one in one point and zeros in the other points. And then you can use separation, okay? You cannot get in a very small time from the value one to the value zero. That's the idea. If you want to do this, your function has to be many big energy and it has to have a normal just exploding, okay? And moreover, your density, your upper density should be bounded now from above, of course, to interpolate.
00:17:22.270 - 00:18:44.208, Speaker B: If you have too many points, then you have too many oscillations and you have no hope to interpolate. So your upper density has to be bounded from above. And interesting consequence, which was already mentioned yesterday in the talks, is from these two conditions, you can conclude that in the usual classical fox phase, there are no simultaneous interpolating and sampling sequences, which I called also complete interpolating sequences, so previously, and this was my first slide from the first day. Lagrange interpolation is, you construct a function vanishing in all the interpolation nodes, if you have finitely many and it's one in one point. And from this you can construct interpolating functions easily for polynomials. If you go to the infinity case, then the situation gets more difficult. But still you can use this idea, constructing a function vanishing in all the points except one, and this in fact constructing b orthogonal sequence to normalize reproducing terms.
00:18:44.208 - 00:19:28.624, Speaker B: Okay, this was an idea which was used several times. And this idea you can use also in, for example in the page Wiener space, and certainly the branch spaces of entire functions, or in the fox space. So here we will try to use another road, and we want to use the bar methods. And in particular, I will use the method which is presented in the Bernsen of Tega Serda paper. But I will just apply it to this very simple case where the rate is e to the minus z squared. So the subharmonic function phi is mod z squared. I mentioned that there is a debar proof by Amar for interpolation in h infinity.
00:19:28.624 - 00:20:23.292, Speaker B: I don't remember exactly which year it was okay. So in fact, we want to try to show the sufficient condition, the sufficient part of the cyphile stain result using this method. Okay, so here we go. The main idea is to construct first a smooth interpolating function. And this is easy, but then once you have constructed the smooth interpolating function, you have to do two things. You have to correct this function in order to make it analytic. And so in order to make it analytic, so you have to ensure that the correction you will add to make it analytic stays in the right space.
00:20:23.292 - 00:21:00.632, Speaker B: And also you shouldn't destroy the interpolation. Okay, this is the main idea of what I will explain now. So our assumption is the sequence is separated. Let's say delta is separation constant, strictly positive. I should have made this precise here. And the upper density is controlled by one over PI. This means again, so d is a limit soup over r.
00:21:00.632 - 00:21:37.482, Speaker B: And if the limit soup over r is strictly less than one over PI, then I know that for an r big enough, this number of points, well, I could have said n plus. Maybe I should have put n plus here. Okay. Okay, there's a plus missing here. N is then also satisfying this condition. But okay, I could have said it's true for every z, so I don't need to put the n plus. Okay, excuse me, I'm confused.
00:21:37.482 - 00:22:32.494, Speaker B: So this is for every, okay, so first step is computing the smooth interpolating function. And for this I will start exploit exploring the separation condition. For this, I will introduce this function, chi, which is a smooth radial cutoff function. So it's one in the neighborhood of zero and it's zero outside delta two. Okay. And so now I take all my points of the sequence and I have drawn these gray discs, disks of radius delta over four. And the bigger disks are of radius delta two.
00:22:32.494 - 00:23:02.452, Speaker B: So chi is one in the gray region. It's zero outside. Okay, so yeah, chi is one inside this gray disk. And outside here, the function is zero. Okay. Everywhere. Okay.
00:23:02.452 - 00:23:56.878, Speaker B: Well, in fact, if I take a specific chi, maybe a little bit to fasten my explanations. So in fact, if I take the function f zero, so I take, now kite, I will translate it and the chi z minus lambda n. So this will be one inside. So it's this function, okay, which is one inside the gray disk and zero outside. Okay, so, well, kai has been fixed once and for all. And it smooth. And then the derivative, I mean, the derivatives of this function are uniformly bounded.
00:23:56.878 - 00:24:43.806, Speaker B: Okay, there's no problem here. And, okay, the gradient of this function is supported on this annulus because inside the gray disk doesn't vary. Okay? So the derivative is zero and outside also. So the support of delta chi is only on the amuli and naive approach would now be to set f zero to be the function some vn chi z minus lambda n. So this chi minus z of z minus lambda n. Okay, it's a little bit like this, like ancient polynomial. It's one in the point lambda n.
00:24:43.806 - 00:25:14.404, Speaker B: It's zero in all the other points. But, of course, chi is not very analytic. All right, so let's move on. I recall that my function f zero. I should remove all the stuff here. So, phi was a smooth deprecating function supported on d zero and delta two. It was one in the smaller disk inside.
00:25:14.404 - 00:25:47.532, Speaker B: And I've set f zero equal to this function. And clearly, f zero in lambda k is vk. So I've used here the separation condition. Okay, so the sum, in fact is. Okay, it's written here. So by separation, the sum is reduced to only one terminal for each z. And if I compute in lambda k, so I have only the term in lambda k, so I get vk.
00:25:47.532 - 00:26:33.632, Speaker B: This is no problem. So I have interpolation. But of course, my first requirement would be that f zero should be integral with respect to this weight. So that's my requirement for f. And now, this is not true for f zero. Okay? If I take f zero, then, well, f zero takes this value vn, which is more or less like e, to the minus lambda n squared. Okay, it's not exactly e to the minus lambda n squared, but, well, I could be in a very specific situation.
00:26:33.632 - 00:27:20.988, Speaker B: Vn could be the sequence which is exactly equal to e to the minus lambda n squared, and zero everywhere. And it has norm one in l two space. So my interpolating function should have a controlled norm. But now, if I integrate against this way, well, this term is unfortunately not bounded in this disk. Okay, I will try to draw some picture here. So maybe if your disk is like this, v lambda n is like this. Well, if you are, let's say, inside the disk here, okay, then the e minus.
00:27:20.988 - 00:27:47.736, Speaker B: Well, it's e to the lambda n squared, the value of vn. But inside the disk, the e minus z squared is much smaller. Okay, well, it's. Well, it's not. Well, it's bigger than this expression. Okay, we should do the computation as we know that e to the x squared over e to the minus e x squared over e to the y squared is not bounded if x minus y is bounded by one. For example.
00:27:47.736 - 00:28:25.204, Speaker B: Okay, you get e to the some linear terminal. So this explodes. So this is not exactly the right way to construct the smooth interpolating function. So we have to work a little bit more in the first step. And this is a little, which I will do. Now, the idea is that you write z squared s lambda n squared. So if you can do this, okay, in a sense, the z squared which is here will compensate.
00:28:25.204 - 00:29:02.108, Speaker B: But let's look about the remaining part. In fact, this z squared, you can write it actually as lambda n squared plus a harmonic function and a bounded function, uniformly bounded function. And in the specific situation, when the sub harmonic function is z squared, everything is explicit. That's why this is quite easy to do. And okay, this is a direct computation. You can choose h n to be this function, and we all agree that it's harmonic. Okay, it's a real part of the holomorphic function.
00:29:02.108 - 00:29:46.090, Speaker B: And what's remaining is this rn, which is c minus lambda n squared. And this has the nice behavior that it's bounded in the disk. It's bounded by delta over two to the square. All right, so how do we apply this? Well, since hm is harmonic, it's the real part of the holomorphic function. Well, the holomorphic function is explicit here. Again, this is a very specific situation where I apply a general result from this Bernsen or figure Siada paper, where they do this for five for subharmonic functions. So we can run this argument more generally.
00:29:46.090 - 00:30:26.194, Speaker B: But then, of course, this holomorphic function is not as explicit as here. Okay, so the idea now is to replace vn and f zero. We had vn here, and instead of picking vn, I will choose gn, which is vn e to the hno two. So hn is essentially this function. And if you compute this function in lambda n, well, it's zero. Okay, so this exponential will be zero. Okay, so gn is vn and lambda.
00:30:26.194 - 00:31:13.566, Speaker B: Nice. So I can put the gn here. And again, this interpolates as it should. So the introduction of this additional factor e to the hn over two, will this save our situation? And, yes, it will. Now, if I'm in a neighborhood of the disk, so again, since chi vanishes outside these disks, it's enough to consider what happens inside the disk. So here I compute the function I have to integrate, and now I replace, as I should, so f. Well, inside the disk, it's just gn.
00:31:13.566 - 00:31:45.642, Speaker B: So I have to compute the modulus of gn. And the modulus of gn is the modulus of gnome e to the real part of hn. Okay, maybe I have lost over two here. No, no, it's okay, because I have the square here. Okay, and then this expression, I can rewrite it using this, and I get v n squared e to the minus lambda n squared. And then here's some bounded expression. So all this is bounded by vn squared e to the minus lambda n squared.
00:31:45.642 - 00:32:15.956, Speaker B: And now by construction, this is summable because my trace is in this sequence space. Okay, so this is not the principal part of the debau method. It's just there's some complication because the weight is exponential with z squared. And so we have to adapt a little bit. This function, other situations, this is not required. Okay, so let's move on. F is this function.
00:32:15.956 - 00:32:56.866, Speaker B: It interpolates as it should. And so if I integrate this function, so I get this sum of all these disks, and by the computation we just obtained, we get this chain of inequalities. And at the end of the day, okay, this is just the area of this disk. Okay, maybe this should have been z. Okay, this is discovery centered at z, no, at lambda n. Excuse me, I'm confused. Okay.
00:32:56.866 - 00:33:35.954, Speaker B: And, well, this sum is of course, bounded by hypothesis. Okay, so buff reasoning, as I already mentioned, applies to more general settings. And that's the paper of Bo, Berson and Kim. The function f is holomorphic. In the small discs, it interpolates and it satisfies the widened condition. And now we have to modify f to get it, to make it holomorphic. But keeping the interpolation condition and the integrability condition.
00:33:35.954 - 00:34:26.390, Speaker B: Okay. And that's the next step I would like to explain. And for this, we will use the bar method. Okay, again, f is this function gn is holomorphic. In a small disk, gn takes the right values, is smooth interpolating, and I recall that, well, in fact, it's correctly. It's not this function, which is supported in this, it's this function, okay, which is supported in this annual risk. Okay, the function chi of z minus lambda n.
00:34:26.390 - 00:34:56.743, Speaker B: This gradient is supported in the annulus. Okay, so we set, we compute the d bar of the function f of the interpolating function. Okay. And the d bar, well, it's supported well, gn is polymorphic. Okay. And so it doesn't feel d bar. And so the d bar is only felt by this function chi.
00:34:56.743 - 00:35:38.476, Speaker B: Okay. And then I have to look if this w is in the right space, if it also satisfies the integrability condition, and the argument is essentially the same that I gave before for the function f itself. Okay, so the d bar of this function is in the right space. This is nice. Okay, so w is the debar of this function. And now. So what we will do is, we will do solve the debar problem.
00:35:38.476 - 00:36:10.242, Speaker B: So, we will take the given function w, and we are looking for a function u such that d bar u is equal to w. So we solve a partial differential equation. So. Well, it's essentially something related to Cauchy Riemann. Well, at the end, my resulting function will satisfy the Kosher Riemann equations. Okay, I give. Of course, if you want to solve this problem, then of course, you would like to know in which space you solve it.
00:36:10.242 - 00:36:57.052, Speaker B: And so, we have some liberty here. And Herman's theorem, which says that such a solution exists, says, claims that, well, you can do this in a quiet channel setting for a subharmonic function file. So if your right hand side is in the space, then you will get a solution more or less in the same space. So we have, unfortunately, here some additional weight appearing that we have to take into account. But suppose this is possible. Then, if I set f to be big f u, then I compute the bar of this is the bar of this. Little f is d bar of big f minus d bar u.
00:36:57.052 - 00:37:37.234, Speaker B: D bar of big f is w, and w minus d by u is zero, because that's what we solved here. So this means f is holomorphic. So we came from a smooth interpolating function to go to the. To, um, holomorphic function. The next step is we have to choose, we cannot choose phi to be just z squared, because phi z squared. Well, this is no problem, but, well, it will destroy the interpolation condition. Okay? So we have to choose phi in a convenient way.
00:37:37.234 - 00:38:39.634, Speaker B: So, first condition is phi, z has to be bound by z squared, because then this expression is bounded by this expression. And so my function f will be in the space. Okay, if we have this inequality, and then we get this inequality, and this means the u that we will correct add as the correction to term. Well, it's also, it satisfies the right interoperability condition, and so the VDF will satisfy the right integrability condition, and so f will be in the Fox space because it's holomorphic. The other condition, which is a little bit more intricate, is that, in fact, phi, when I am close to a point, an interpolation point. So where I interpolate, then, in fact, my phi should be in such a way that this gets very big. In fact, this should have a pole.
00:38:39.634 - 00:39:23.556, Speaker B: If this has a pole, then necessary w, which is holomorphic. No, it's not a holomorphic function, but it's continuous. In fact, then this function has to be zero. It's not possible to integrate this pole against the holomorphic function. So the idea is to do this, okay? And ideally, the laplacian of phi should be bounded from below by a strict constant, because then I don't need to care about this. Okay? So I have to look for function phi satisfying these conditions. Okay, how do we do that? Now, I have to consider the density, of course.
00:39:23.556 - 00:40:21.384, Speaker B: And I recall that the density told us that the number of points in the disk with radius r is controlled by q r squared, and q is strictly less than one. Now, in fact, my phi, I will add this weight v, and v is more or less the Taylor expansion, Taylor development, the Taylor series for the order one for the logarithm. Okay, so if I take the logarithm, I subtract, essentially the first order data expansion. Okay? And so I've drawn the graph of the function log x plus one minus x. So this is negative. Okay, so this means phi z is less than z squared. And so we get this condition for free.
00:40:21.384 - 00:40:58.474, Speaker B: Okay, and what did I just say? Okay, so we also observe that this function log x plus one minus x, well, it goes smoothly to zero at one. This is important observation because this means that the Laplacian of v will not have any weight here. Okay. At some point, we have to compute the subhominicity of phi. And so we have to compute the Laplacian of phi. And we know the Laplacian of z squared. This is one, essentially.
00:40:58.474 - 00:41:20.014, Speaker B: And then we have to compute the Laplacian of V. And so we don't get a contribution from this point. Okay, that's what is written here. That's. I already mentioned it. So, five z satisfies condition two. And if z is close to lambda n, let's say it's inside some disk.
00:41:20.014 - 00:42:06.546, Speaker B: Then if I look at this expansion here, then I will get only. Well, I get a principal term. Okay, I'm just getting aware that I did something wrong here. Excuse me. But so, essentially, we get the principal term, and all the other terms, they sum up. Well, they are neglectable or they sum up to this constant. So, and this first term also, it's like a constant.
00:42:06.546 - 00:42:48.284, Speaker B: Okay? This comes from this z squared. So the function phi essentially behaves like this logarithm to the squared plus kn. And kn is a constant which depends on n, but not on z if I'm in this neighborhood. And so we have the third condition. So we generate a pole, e, to the minus phi. We generate a pole in lambda n. All right, so as I said earlier, now we have to ensure the estimates on V.
00:42:48.284 - 00:43:35.350, Speaker B: So in order to that our new weight phi, which is phi z. Okay, it's written here, phi Z. So in order to see that it's subharmonic, I have to compute the Laplacian and see if it's positive. And since in Hermann's theorem, the Laplacian of phi appears in the denominator, it's even better if it's uniformly bounded. And now this is a consequence of the density condition, and everything is written here. So now we compute the Laplacian of phi, which, for convenience, I define in this way. Of course, Laplacian should be maybe four times this expression.
00:43:35.350 - 00:44:02.676, Speaker B: But this is just a normalization issue, which doesn't play a role here. Okay? And then if I compute the Laplacian of phi, so the Laplacian of this is just one. This is x squared plus y squared. Okay, this, I compute the dd bar of this z bar. Mod z squared is z times z bar. So dd bar of this is one times one, which is one. Okay, this comes from z squared.
00:44:02.676 - 00:44:30.608, Speaker B: And then I have to compute the d bar of this expression. The d bar of this expression will give you some positive constant and Dirac mass in lambda k, lambda. It's lambda n, in fact. Okay, this is lambda, nice. Lambda n. Okay. And this is a positive measure, so you don't need to care about it.
00:44:30.608 - 00:44:59.842, Speaker B: And it's, the minus one comes from this expression. Okay, and you have, of course, the coefficient one over r squared. And so this is bound from below by. I still have the one here. And minus the sum over one s divided by r squared. But the sum of the ones, well, I do it in this disk, and I know this is bounded by NZR. And NZR is bounded by q r squared.
00:44:59.842 - 00:45:41.924, Speaker B: So that this expression is bounded by q. And so everything is bounded from below by one minus q, which is a strictly bigger constant than zero. Okay? So outside the union of the DN prime, we have, of course, that this is equal to one, because you only have z squared. And so this Laplacian is uniformly bounded from below. And now we can use the result by Hermander. The w, it was integrable against the weight, and this doesn't harm the estimates. So I get hu, which is in the right space.
00:45:41.924 - 00:46:18.136, Speaker B: Okay. And this is it. Okay, so with the debau method, you are able to find an interpolating function. And the idea is you start from a smooth interpolating function, and then you have to correct the smooth interpolating function. And for this you have to solve a debar problem. So it's a partial differential equation. And to do that well, we have to introduce a new subharmonic weight, which behaves more or less like the initial one, but close to the points.
00:46:18.136 - 00:46:58.624, Speaker B: It has to have a singularity, and it plays some role that this is only close to the lambda case. And if you are in the annulus, then you don't feel anymore this singularity. And so everything works correctly. Okay, so this is just a small observation. So I haven't really spoken about sampling. I just gave you the results by Saipan Walstein in this situation. Yesterday, Martin mentioned the Feistinger contracture, which is now theorem.
00:46:58.624 - 00:48:00.414, Speaker B: So I recall this sampling condition. And the right part of this implies that we have some cards embedding, and in fact, it's like the Bessel property. And so necessarily we can deduce that the reproducing currents have to be finite union of three sequences. So in a sense, one could expect that the left hand side is more or less for free, once you can establish more or less that you have some completeness property. So maybe not so direct. Okay. But at least we can say that none of these sequences that you get from the Feistinger conjecture can be a re spaces, because such things do not exist in the fox space due to this result, which I cite here.
00:48:00.414 - 00:48:52.988, Speaker B: Okay, so we are on page 14 of this presentation, and there are still 15 pages left, which I of course, will not explain in 1 minute. But let me just mention that. So for the moment, we have considered at least for interpolation problems, and interpolation problems, they get complete if you are in the right space or if you stick to the right space. But these interpolation problems, you can in fact consider much more general problems. And I just want to give you some feeling of this more general situation. And what I say here, that the Hardy space situation is more or less the mother of general interpolation. Of course, I mean interpolating derivatives.
00:48:52.988 - 00:49:37.068, Speaker B: This idea is older. And for example, also if you make spline interpolation, then you interpolate derivatives. But in the Hardy space, you have a setting where this is quite natural and generalizes in a more or less easy way. So the idea is that you start from an inner function and then you write it as a product of other inner functions. And then you can look for this interpolation problem. So, okay, I defined the trace this way, which seems to be reasonable. And we had already always summed a little l two of the values.
00:49:37.068 - 00:50:46.402, Speaker B: So here we do it this norm, and we want to get f minus fn, which is in theta nh two. So maybe in this generality, it's not so clear what I mean by this. But if you take, for example, the most, the easiest inner function, which is a simple Blaschke factor, which vanishes in lambda n, then this condition says exactly that f and f n have the same values in lambda n. And so, in fact, this yields classical interpolation. This notion was introduced in the lenient grad school in the seventies by these people. And I want just mention this more general interpolation problem. So now, if you take, instead of a simple BlaschKe factor, you take powers of these, then the interpolation means that f and fn, they have the same Taylor Taylor expansion until a certain order, the order being given by the kn k minus one, in fact.
00:50:46.402 - 00:51:37.264, Speaker B: And so you get Hermit type interpolation. And sometimes this is also called multiple interpolation. And you can consider more general situations, which I will not discuss. But what is very nice, that there is a very nice characterization of the interpolating sequences in this sense. And so I don't want to go into details of this, and I just want to lie. I just want to mention, and this is maybe the last thing that these multiple interpolations have been considered in many other spaces. Like for the Hardy space, of course, but also the Fox space, the Bergman space.
00:51:37.264 - 00:52:18.466, Speaker B: There was also some questions about unbounded multiplicities. And so quite recently, we were able to give some partial answers to the question of unbounded multiplicities in the Fox space. There are other developments. You can also consider random interpolation, and for which there have been recent and less recent results. But of course, the time is not unextendable. And I should better stop here. I see Javad appearing, looking at me with a very serious look.
00:52:18.466 - 00:52:22.854, Speaker B: So I stop here. And thank you very much for your attention.
00:52:24.274 - 00:52:41.808, Speaker A: Thank you, Andreas. Indeed, thank you, Andreas. A few more minutes for questions, if any, or comments you may write.
00:52:41.856 - 00:52:59.004, Speaker B: There's a question with the answer. Okay, we have to subtract the d bar to correct to get the Riemann equation. So if the function is given f, then you have to.
00:53:00.504 - 00:53:08.364, Speaker A: Andreas, am I correct? Assuming that a PDF file will be available with more details later on by you.
00:53:09.184 - 00:53:18.164, Speaker B: Maybe these files. I can make them available. No problem. You should explain how we.
00:53:19.824 - 00:53:21.144, Speaker A: We will discuss. Yes.
00:53:21.224 - 00:53:21.704, Speaker B: Okay.
00:53:21.784 - 00:53:42.924, Speaker A: So that that will be available for all. For. I mean, detailed studying these topics, if they are interested in. So let's thank Andreas again. Thank you very much for. I mean, these wonderful talks. We have six more minutes and I see Damir is with us too, so.
00:53:42.924 - 00:53:48.904, Speaker A: Hello. And Lyubarsky is there too. Jura.
