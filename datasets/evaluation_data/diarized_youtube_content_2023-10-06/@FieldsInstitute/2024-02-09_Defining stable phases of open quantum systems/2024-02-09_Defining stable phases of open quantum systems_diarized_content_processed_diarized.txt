00:00:00.400 - 00:00:48.254, Speaker A: All right. Good morning, ladies and gentlemen. Today we are extremely honored to have Professor Sarong Gopala Christian to join us in celebrating the lunar university by giving us a talk about his recent research on open systems. So, Ron obtained his bachelor's degree from Armhurst University, Alpert College, before he moved on to pursuing his PhD at UIMC. Now, as an effective member at Princeton University, his research broadly intersects with metabolic physics, quantum information, quantum dynamics, and even computer science. So, on this warmest day of this chronicle winter, let's give Ciron a warm welcome.
00:00:53.154 - 00:01:32.334, Speaker B: Thanks. That's, I guess, very interesting day to be here. Am I going to get some mooncakes at some point? Okay. Right. So I want to talk about some stuff that I've been thinking about with Kurt and T bore, who have kind of different attitudes towards that. I guess you can see their faces, but, yeah, it's sort of. It's sort of beginning of some project of trying to make sense of what the phases of open quantum systems are.
00:01:32.334 - 00:02:05.074, Speaker B: When I talk about phases, when I talk about steady state phases, I'm going to talk about something fairly specific. So I'm going to think about a system coupled to a large environment, and I'm mostly going to treat the environment as markovian for the things that I talk about today. But, of course, that's an additional, um, assumption that you can imagine relaxing. So, systems out of equilibrium, because, for example, it's coupled to two baths at different temperatures. Um, or. And. Or because it's being driven by, um, a classical field.
00:02:05.074 - 00:02:45.900, Speaker B: Um, and, um. And, you know, if you keep the system finite and you take the bath to infinity and you take time to infinity, eventually the system's going to come to some steady state, maybe a limit cycle, but like some kind of. Of steady distribution. And so what I want to talk about is the sense in which these steady states can fall into distinct phases the way equilibrium states fall into distinct phases. So that's going to involve a set of limits, which I'm just going to specify right now. It's just like some choice. It's not necessarily the only choice, but it's a reasonable one.
00:02:45.900 - 00:03:27.908, Speaker B: The bath, of course, you want to be functioning as a bat, so you want it to be much bigger than the system. Take the bath to infinity first. Then you take the system size to infinity, and you let the time, the system is evolving go to infinity as well. And I'm going to assume that that's done in a way that the time is blowing up as some large polynomial of system size. As you take both to infinity. But that's a bit negotiable. We know that if you take the thermodynamic limit on an equilibrium system, it goes into some equilibrium state.
00:03:27.908 - 00:04:35.602, Speaker B: These equilibrium states form phases, so properties remain the same inside a phase, and they change abruptly when you hit a phase transition. That's the boundary of the phase. Right? And so why am I thinking about this? Are there non trivial phases? We know there are, because effectively people who constructed a bunch of examples in the seventies and eighties of steady states that provably form the provably robust perturbations. So there are regions of parameter space where the steady state of a non equilibrium system, um, retain some property. Like for example, it might have long range order, um, and, um. And, and then, um, when you change, um, your parameters past a certain value, the, the phase goes away, the long range order goes away, and you end up in sort of a randomly fluctuating super stuff. Um, one thing that's interesting about these non equilibrium phases is that they can have properties, um, that equilibrium phases in the same dimension wouldn't.
00:04:35.602 - 00:05:37.228, Speaker B: So, for example, there's this dynamical system called Toombs rule and its properties. You sort of see what it consists of is a bunch of domain walls being eaten away, but in a sort of patterned top to bottom way. So, as you might all remember, the ising model in two dimensions does order at finite temperature, but it only forms an ordered phase, um, when the external magnetic field is zero, um in, in this, in, in Toom's rule, um, you have ordered states, um, that are stable in the presence. You have bistability that's, that's robust, um against, um, a, um against external fields. So it's like it's ordered in the presence of a finite field. And so, so it's clear that not only are there phases with a classification of these phases is different than it would be in equilibrium in the same dimensionality. So, so that's some motivation for thinking about these.
00:05:37.228 - 00:06:52.348, Speaker B: Now, what am I going to talk about today? So the two basic questions you could ask about phases of non equilibrium systems, one of them is a really hard question, which is constructing and proving the existence of certain stable phases. So it would be very cool if I could do that, but that's hard, and I'm not gonna address that question today. I'm gonna use results that mathematicians came up with establishing these things exist. What I'm gonna talk about today is slightly easier question of just trying to understand what the landscape of parameter space in these open systems is like. What does it mean? To be in a phase, what properties are common across the phase, what changes that are phase translation. And as you'll see when you start thinking about this question, which might seem not terribly contentful at first, you realize there are a bunch of puzzling phenomena that occur out of equilibrium that you don't really see, that don't really have equilibrium analogs, I should say, as I'm speaking. Just interrupt me if you have questions.
00:06:52.348 - 00:07:21.848, Speaker B: Don't wait for the end. Um, right, so, so that's, that's going to be the rest of this talk. So, um, I'm gonna talk about. I'm gonna give you a quick review of how the structure of phases, um, at zero temperature works. Um, and then I'm gonna, um. I'm gonna talk about, um, what changes, um, when and what stays the same when you go to open systems. Right.
00:07:21.848 - 00:07:42.824, Speaker B: So a quick review of gapped phases of zero temperature systems. So you've all probably seen phase diagrams. I guess yellow and green look the same on this screen, but that's okay. So this is supposed to be green. It's a region where there's no gap. Right. We're talking about zero temperature stuff.
00:07:42.824 - 00:08:02.410, Speaker B: So we're talking about the ground state of a Hamiltonian. And so if you like, we're talking about the properties of a Hamiltonian, um, as well as its ground state. Um, yeah, the. Yeah, the Hamiltonian implies its ground state, but not necessarily the way around. Um. Right. So, um.
00:08:02.410 - 00:08:39.106, Speaker B: So, so a gapped, um, region. A gapped phase is, um, is. Is sort of a situation where you have, um, some number of. Of ground states, and they're separated from the rest of the spectrum by a. By an energy gap that remains or the one, um, in the thermodynamic, um, limit. Right. And so, um, phase gap phases have, um, certain properties, um, that are standard, um, so, um, severe low entanglement, um, expectation values evolve smoothly across a gapped phase.
00:08:39.106 - 00:09:37.784, Speaker B: Correlation functions evolve smoothly. Um, and then there's this interesting, um, property that I'll revisit, which is called the principle that local perturbations perturbed locally. So what that means is I take my hamiltonian and I get to its ground state somehow. And then what I do, starting from ground state, is I put in local perturbation. That's a perturbation that acts in a finite region of my space, and I ask how the expectation values far away from that finite region change in the new grand state in response to perturbation. And the idea, again, this is totally intuitive. If you've thought about this stuff is that as you go away from the perturbing region, the effect of the perturbation decays exponentially with some characteristic correlation length that's related to the gap.
00:09:37.784 - 00:10:42.272, Speaker B: Okay, so this is sort of a sort of dictionary of. This is, if you like, a list of properties of gapped phase. And they. And so you might say, well, what do they have to do with each other? Where does, where does the, you know, what holds this bag of properties together? And so the logic, I think, is, like, nicely formalized by Hastings and wen in a couple of papers around 2010, is something like this. So, inside a gap phase, there's, you construct a path between two points in a gapped phase, and the path remains entirely inside the phase, right? Um, so that's, that's what it means to have a gap path connecting two points. Um, and, um. And so if you move along a gapped path, um, then by the adiabatic theorem, um, if you do it slow enough, um, you're going to stay, in some sense, I'm not going to be super, um, super rigorous about.
00:10:42.272 - 00:11:37.756, Speaker B: You remain close to the instantaneous ground state by the adiabatic theorem, and you don't have to do this arbitrarily slow. You can just go, if you like, much slower than the inverse of the gap. So this says that you can go from here to here in finite time. And the key point is that this thing now means, by the Lee Robinson theorem, that if you evolve the system for a finite time, um, then there's a light cone, so nothing changes outside the light cone. And so, in particular, say, the asymptotics of correlators, doesn't change, um, after a finite time evolution. And so this implies that correlation functions must remain the same, um, asymptotically, everywhere inside a gapped phase. And so, so the thing that holds all these properties together is this idea that, um, is, is the idea that, um, any two points in the same.
00:11:37.756 - 00:12:51.376, Speaker B: Any two. Yeah, any two points in the same gap phase connected by a finite time evolution to whatever approximation you care about, or equivalently, if you try to decompose the evolution by a finite depth local unitary circuit. And so this is what it means for two points to be equivalent in this Hastings and wen perspective. And so, this characterization makes this list of properties that I gave you in the previous slide quite intuitive, because, effectively, for example, if you want to think about long range order, you want to consider the correlation function of two points that are very far away. And so, what I've drawn here is a unitary circuit that, for example, took my, um, initial ground state to my final ground state by assumption. I assumed there was such, you know, by the exit, by the fact that in the gap phase there is such a circuit. Um, I can write down the circuit and then I can say, okay, what does it do? Well, um, the key point is that, um, the gates that are acting directly on the ground state don't do anything.
00:12:51.376 - 00:13:10.304, Speaker B: They just leave it. Um, yeah, sorry. They say that. They say the other end. Um, so, um, so, so I want to compute this correlation function. And so this unit tree, for example, is a u dagger that crosses out against this u. So all of these, everything outside the light cone just cancels out.
00:13:10.304 - 00:13:53.982, Speaker B: And so if you want to compute this guy, it disconnects into two pieces that are two sort of finitely fattened local operators that are being evaluated now in the. Yeah, so a single site operator in, at point a fattens to a finite support operator in point b if they're both in the same phase. Yeah. This is a spatial correlation function. But the point is, the light cone is connecting two points in a phase. Right? So I say that. Let me go back to my previous slide, if I can figure out how to do that.
00:13:53.982 - 00:14:55.014, Speaker B: So I start out here. I want to argue, the thing I'm trying to argue is that the long range properties, correlation functions, this point are the same as long range properties at that point. Okay? And so the time evolution is the adiabatic time evolution that goes from here to there. Does that make sense? So I'm saying that if I want to evaluate a single site operator, a single site, a correlation function between two sites in this grand state, then that's like evaluating the correlation function of fattened operator at this other point. And so this is, and so because those two, because, because of the light cone, you know, those guys just finitely fattened, I can move them arbitrarily far away. And so I don't change the asymptotics. The correlation function is that argument clearly? Yeah, keep asking if something's unclear.
00:14:55.014 - 00:15:44.652, Speaker B: Okay, so another one of these properties of a phase is this principle that local perturbations perturbed locally. So I already said what this was. So you perturb it locally at some point. And so because the key point is that locally you sort of adiabatically turn on this perturbation if the gap remains open at all times throughout turning on the perturbation. So you turn on the perturbation. And that just corresponds to implementing a finite time evolution on the state. Now, if I apply a finite time evolution on the state, these black gates are the native Hamiltonian, that's h.
00:15:44.652 - 00:16:12.672, Speaker B: And the red gates are the trotterized version of the perturbation that I'm putting in locally. So the black gates hitting the ground state, just Hamiltonian acting the ground state. So don't do anything. So I can just delete them. And so what I have after this finite time evolution is this triangular set of non trivial. Yeah. The vertical axis is time.
00:16:12.672 - 00:16:30.832, Speaker B: Yeah. So let me be very clear about this. So this is my representation of the unperturbed ground state. And so I'm thinking about it as a matrix product state in 1d, but that's not important. It's just. So this guy. So this tensor is the unperturbed ground state.
00:16:30.832 - 00:17:15.426, Speaker B: And then I'm turning on the perturbation via this time evolution process, where I adiabatically ramp it up from zero to some final value. Yeah. So if you like, when I draw something that looks like. So my way of representing a quantum state of a system is something like that. So if I have five qubits, for example, a five qubit state is a block with five legs sticking out of it, which correspond to the states of each of the five qubits. So, yeah, thanks. So this guy, I wrote it out in this way with red lines horizontally.
00:17:15.426 - 00:17:45.774, Speaker B: That's not important. The idea is, this entire thing, this entire tensor, it represents the ground state of a many spin system. And now what I'm doing is I'm time evolving the ground state. I'm time evolving it by a circuit because they've made a charter decomposition. And the circuit almost everywhere doesn't do much because it's the Hamiltonian acting on its own ground state. So it just annihilates the, you know, so it just goes through, it just returns the ground state. Yeah.
00:17:45.774 - 00:18:42.294, Speaker B: Okay, so that's, that's, that's what's, that's what's going on here. And so I'm emphasizing this particular feature of a standard ground states where it's going to change when I go out of equilibrium. Um, okay. Um, so, so that's, that's, that's so that's the kind of structure that you have, um, in, um, in, in Hamiltonians and their ground states. So the space of Hamiltonian splits into a bunch of these gap regions separated by phase transitions and maybe some gapless phases as well. And so we want to see, okay, what do we, what do we do? How much of this can be moved can be applied to open quantum systems. So to do that, I'm going to give you now a brief review of what open quantum systems are and what specific assumptions I'm going to make when I talk about open systems.
00:18:42.294 - 00:19:32.954, Speaker B: I'm going to think about systems in which your entire universe is undergoing some quantum evolution. You care about the state of some part of it. That's your system. Your system interacts with the environment, which is assumed to be prepared in some reference state, which could be either a ground state or a thermal state. It's not important. And then what you do afterwards is you trace out the environment and that gives you an evolution on the system alone, which it's given by a super operator, because what it does, it takes any density matrix to a density matrix, right? So, but it doesn't, it doesn't necessarily take a pure state to pure state. It does convert peer states into mixtures.
00:19:32.954 - 00:20:36.026, Speaker B: Right? And so this object is called a, it's a super operator. And we, I'm going to call it a quantum channel, which is sort of the finite time version of it. I'm also going to use that term interchangeably with Lindladian, which is what happens, which is basically a quantum channel. That's like close to the identity from the perspective of this talk. Okay, so when you think about evaluating things in quantum channels, you'll notice that what you do is you start out the initial state, then you apply the super operator, the state, and then in the end you're going to measure some observable, right? And so if you, so one way of thinking about the super operator is it sort of, it takes the input state, it takes a density matrix as a vector and converts it in density matrix to contract that density matrix and operator. Notice that observables and density matrices don't. They both look like square matrices, but they play, and they're both omission.
00:20:36.026 - 00:21:01.512, Speaker B: They play different roles in this evolution. This guy's the top of the circuit. This guy's the bottom of the circuit. And think about this as this evolution operator is not hermitian. So when it acts on the right, it does something pretty different from when it acts on the left. Its properties, uh, its left acting and right acting properties are fundamentally different. So, yeah, so, so this, so these guys live in the dual space, in the space of, in the space of row vectors.
00:21:01.512 - 00:21:59.700, Speaker B: These guys live in a space of, of column vectors, in this, in this sort of d squared dimensional space, right? And so a very important property of, of quantum channels is this thing called the unital properties. So remember that the way I got this evolution was by taking a system, applying unit trees between the system and environment that was prepared in a reference state. And so if I now instead of measuring an observable I just measure the identity operator, which is sort of a silly thing to do. But the identity operator is a kind of, it's a hermitian operator. You can think about it as observable. And so now you look at this circuit, you see that there's nothing stopping you from multiplying the m by the m dagger and canceling them out. And then you can multiply the U by the U dagger and cancel them out.
00:21:59.700 - 00:22:38.908, Speaker B: And then what you're left with is that this entire revolution just becomes a trivial evolution. So if you like, the identity operator considered as an observable is a steady state observable. And the intuition here is just that. If you don't perturb the system, it doesn't matter. When you don't perturb the system, it's the same regardless when you do it. Um, but notice that um, this, this property, which is going to be very important for us is not true. Um, when you act to the right.
00:22:38.908 - 00:23:04.184, Speaker B: So if you take, if you take a density matrix, um, that's the identity. And you apply, and you apply this, and you apply this entire um. Map to it, um, it does not remain the identity. In fact a simple example of a case where it doesn't is um, is, is um, spontaneous decay. You take a maximally mixed state in a qubit. Um, you let it evolve. Um, it just goes to the, the ground state.
00:23:04.184 - 00:23:35.656, Speaker B: So um. So, so you don't expect these things to be um. You don't expect the channel to have the unit of property acting from bottom to top, but you do expect it to happen from top to bottom. And so the unital property is very important because um. Okay, and this is just uh, so this is just notation for just combining all the stuff. So what you know, because when you have a channel you have all these. So this is a very inefficient way of writing the channel because I have an explicit representation of the environment and then I have all this brass side stuff and all this cat side stuff.
00:23:35.656 - 00:24:18.410, Speaker B: So in order to do diagrams, in order to circuit diagrams involving channels, it's helpful to sort of fold, to fold this entire thing on itself. So the U dagger is now sitting, has been squished behind the uh. And so the whole thing is just like one matrix acting on the density matrix. So now my fat input leg is a density matrix and my fat output leg is a density matrix. This box is super operator. And the unital property is just saying that if I contract the super operator with the identity interpreted as a state, then it's like not having this. So that's right.
00:24:18.410 - 00:25:12.424, Speaker B: So this is a better way of saying it. So the unital property of channels is the identity that if I take the identity and I apply a channel to it to the left, then it's like not doing anything. Okay. And so this seems like a rather technical point, but, but the significance of it is, okay, so I don't really need to give you this example of a channel. Yeah, maybe I should very quickly talk about this. So the idea behind this, this is like a spontaneous decay channel. So you start out with some density matrix and then what you do is you toss out the density matrix by swapping it into the environment and you replace it with the state zero.
00:25:12.424 - 00:26:17.368, Speaker B: Okay? And so this has, this has a representation in fat line notation that you just bring in whatever, you kill it and then you replace it with a reference state. So that's just the way this notation works. Okay. Why am I telling you all this? Well, the key point is that this unital property gives me something that functions the Lee Robinson theorem, um, for um, quantum channels. So that basically says that for example, if I want to evaluate a correlation function between a and b and my time evolution is given by this open system evolution, um, then I can, I can write it out like this and I can, I can, I can delete, I can delete all of these, um, all of these operations of the channel because they're all acting on, they're all acting on the identity to the left six. So I can remove them. So the fact that this should remind you of the property that I specified for the property that I used to prove various things about the unitary case.
00:26:17.368 - 00:27:10.974, Speaker B: But the key point is that it also applies. Light cones also exist in quantum channels. This shouldn't be surprising because if you like, a quantum channel just consists of a unitary evolution involving an auxiliary system and the auxiliary system just doesn't carry any correlations. You're not going to beat the light cone by bringing extra stuff in that's not moving over long distances. But okay, this is how it works technically. I'm going to talk about the phases of these circuits that are made up of, of quantum channels. And so when I restrict to these guys, I leave out one very important class of examples of non equilibrium phases.
00:27:10.974 - 00:28:02.254, Speaker B: And those are phases that are stabilized by active quantum error correction. Why is that? Because when I perform error correction, even though the error correction operations are local. Um, I measure all my error syndromes and my correction operation is based on global information about the measurement data. So in some sense, even though the, the quantum stuff that I do is local, it's based on it. There is implicitly non local classical communication that my computer performs in order to read out the syndromes and figure out what error correction operation to perform. And so the things I'm going to talk about don't apply to active error correction. In fact, there's no Lee Robinson theorem for this case.
00:28:02.254 - 00:28:38.564, Speaker B: And so, the question of how to think about phases of actively error corrected systems is a very wide open question. It's not even clear what it means to have phases of such systems. I'm going to specialize to cases where this doesn't happen. I make, let's say that I, any, any measurement data that I, that I collect is used locally. So I don't use measurement out. So when I define the challenge I'm going to talk about, I don't use measurement outcomes at site one to decide what to do at site 25. If I did, then I'd break this entire structure.
00:28:38.564 - 00:29:13.014, Speaker B: Okay, so that's, that's what I'm going to. So that's, that's, I guess, what I'm going to talk about. So I have these channels and I'm going to make circuits consisting of repeated application of some channel. And I can think about that instead as repeated application of a lemdlab master equation. It's sort of from the perspective that I'm taking. The distinction is not important, though. It's going to be important for some specific details that I'm not going to talk about.
00:29:13.014 - 00:30:25.034, Speaker B: Right? So these guys, so I take quantum channel, and as I showed you, it's some matrix acting up. It's some super operator acting a density matrix. And the super operator is going to take every density matrix eventually to whatever the leading eigenvector of this thing is. And this leading eigenvector for the channel has eigenvalue one, and it corresponds to steady state, which means if you take the steady state, you apply the channel to it, it just returns a steady state. Okay? And so a special case of these channels are classical Markov chains, which are going to give us some of the simplest examples of non trivial phases. So you can make any, you convert any channel into a Markov chain by just a completely phasing operation. Right? So I am talking about some kind of extreme eigenvector problem in a context where there is Lee Robinson bound.
00:30:25.034 - 00:30:57.164, Speaker B: It looks a lot like the problem of thinking about ground states of Hamiltonians in these respects. So it seems suggestive. It seems that you should have some notion, you should have some set of corresponding notions. But a few things don't seem to work. For example, Hamiltonians or unitary operations are always invertible, right? They're inverse, the unitary operation. And a disparate process is not invertible. It's invert is not physical.
00:30:57.164 - 00:32:02.424, Speaker B: And so can I even think about two points being in the same phase as an equivalent relation? And so that's one thing that would puzzle you. Another thing that might bother you is the gap of a dissipative process can be interpreted as a decay rate. So if I take, for example, if I take a density matrix and I look at its time evolution, um, it's something like, um, I, um, I write it in terms of, um, in terms of its eigen, um, vectors. As, um, as you know, it's, it's lambda. So it's, this is the eigenvalue. I'm just expanding time evolution, the eigenvasor and these guys, um, these eigen values are less than one. So if I apply, if I apply, um, the channel a bunch of times, I'm going to get exponential decay.
00:32:02.424 - 00:33:12.884, Speaker B: Of all the things that are not sustaining, you might say, well, okay, here's a system with a gap. And so if I evolve for a time much longer than the gap, I'm going to turn everything into the steady state. So I should not have slow relaxation. So the idea of having a non trivial phase seems almost incompatible with the idea of having a gapped quantum channel or a gap Memladian, right? Because in order to have, if I can always start with a product with a trivial density matrix, and this argument seems to suggest that if I take this trivial density matrix and I evolve it to some time much longer, the gap, it's just going to turn into steady state. But in that saying, the steady state was connected to the trivial density matrix by a finite time evolution, which would, in my way of thinking, say they're in the same phase, you might say, well, this doesn't, this doesn't make any sense at all. Yeah, yeah, that's right. So it's just repeated time steps of the math.
00:33:12.884 - 00:34:13.054, Speaker B: Um, so let me, let me go back over here. So, um, this, so I've done, I've done one and a half iterations. Like this thing is my, is a single iteration, right? And so when I apply the map, say, ten times, that means I'm applying this block ten times in succession to, to whatever my initial density matrix there is. But when you chart rise, the velocity is set to one, at least in this, in this traumatization, the velocity is set to one. But yeah, so you want to think about the velocity as being one. In the circuit construction, you insert it to be five as well. But then that means I've got to draw a lot more gates and it's kind of painful for me, but that's all.
00:34:13.054 - 00:34:55.835, Speaker B: It's not. It's just some number. Right. Okay, so, okay, how do you make all this stuff fit together? And so in this next bit, I'm going to try and walk you through an example of how non trivial phases of open systems, um, can arise. Um, and the example I'm going to talk about is not very quantum, in fact, completely classical. Um, and it is, um, a biased, um, random walk. So it's a biased random walk with open boundary conditions.
00:34:55.835 - 00:35:38.294, Speaker B: So, um, so if my, if my left topic. So I've got a, I've got a master equation, a classical master equation where my particle pops to the left of some rate and to the right some other rate. And if the rates are different and my boundaries and my boundary conditions are open, then I always end up sticking to either the left wall or the right wall at late times. Right. Because I'm sort of moving. Because if I started out in the middle, I'd eventually sort of move all the way to one end. Okay, and so a fact about this problem about this Markov chain or this Markov process is that it's gapped.
00:35:38.294 - 00:36:25.340, Speaker B: And so why is that bothersome? Well, you might say, okay, look, I have a system of size. L I start the part, let's say that my left hopping winds over my right hopping. Start a particle out here, it's going to take a time of order l to move all the way over to the boundary where it's going to get stuck. Right? So relaxation time is kind of obviously going to scale with system size. But if I just write down this matrix and diagnize it, I find that its spectrum has a gap. Okay, so what gives? It turns out if you just write down the extreme limit where there's no left hopping at all, it's rather simple to see what's going on. So this is a Markov chain.
00:36:25.340 - 00:37:11.450, Speaker B: You'll notice that all the columns add up to one. And what the Markov chain does, it pushes everything to one end. And so if you look at this matrix, you realize immediately the thing that makes it work out to have both a long relaxation time and a gap is that the matrix is not actually diagnizable at all. In fact, it's a matrix with a giant Jordan block. And so this guy is a five by five matrix, but only has two, um, it only has two eigenvectors. Um, and one of the eigenvectors is a steady state, which corresponds to particle, um, all the way at one end. And the other eigenvector is, this is this guy which is going to, um, relax after one time step to the, to the steady state.
00:37:11.450 - 00:38:03.960, Speaker B: Nothing else is an eigenvector. So this is a, this is a funny feature of these matrices, you might say. Okay, this is, this is a fine tuned, um, property of this limit of the problem. But what I can do is I can, um, I can go away from that limit and I remain, you know, qualitatively with the same dynamics. And, um, and so what happens basically, is that even when the matrix is diagnizable, its eigenvectors almost, almost exactly parallel to each other. And so, um, what does this signify physically? It means that if I try to write a initial state, like a particle at the wrong end, it's like you have all your eigenvectors that are pointing this way, and you're trying to write some state that's almost perpendicular to all the eigenvectors. It's not exactly perpendicular because there is a complete set of eigenvectors.
00:38:03.960 - 00:39:14.874, Speaker B: So there is a way to write the initial state in terms of the eigen basis. But when you try to do that in the large system limit, you find that you need these gigantic coefficients which actually blow up exponentially with system size. And so this makes the existence of a gap compatible with the Lee Robinson theorem, because what happens is that if I write the initial state, if I write initial state like this out in terms of the eigenbasis, what I'm going to need is I'm going to need these coefficients. They're exponentially big and system size. So if I look at the time evolution, it's either the something times l times, either minus something times t. And so the point at which all these coefficients for non trivial eigenvectors become small is precisely when t is void or l. So even though there is a gap, the gap is compatible, because the strongly, the strongly non diagnizable, strongly non hermitian character of the matrix, it's compatible with the existence of long relaxation patterns.
00:39:14.874 - 00:40:06.004, Speaker B: The way that people think about these matrices is in terms of an object called the pseudo spectrum. So remember, that the eigenvalues are the poles, that you think about them as poles are resolved into a matrix. And so what you do is, instead of saying, instead of requiring an exact polygon whole, you say that you want to study regions in which the resolvent is big instead of being strictly infinite. So you take the resolvent to be large and finite. That defines some region. If you take epsilon to be not too big, then you have a big blob. And as you make epsilon finer and finer for a finite size matrix, then the pseudo spectrum condenses into a set of little circles around each eigenvalue value.
00:40:06.004 - 00:40:56.056, Speaker B: And the key point is that there's now a pair of limits that don't commute with each other. There's a limit where you take, where you take epsilon, the pseudo spectrum to zero, and there's the thermodynamic limit. And if you keep system size finite and take epsilon to zero, of course you're going to end up with a complete set of eigenvalues. And the pseudo spectrum is going to condense into a bunch of points. But the key point is that if you take the opposite order of limits, you never form a discrete set of eigenvalues. Instead, the pseudo spectrum occupies the entire block from it occupies the entire spectral bandwidth. What happens is that there's a strict sense in which these matrices become non diagnizable in the thermodynamic limit.
00:40:56.056 - 00:41:41.548, Speaker B: And that's precisely a sense in which the pseudo spectrum doesn't, doesn't become a spectrum if you take the thermodynamic limit first. Okay, so, so you might think Jordan block structure is a curiosity. It's a fine tuned thing. But in fact, Jordan block structure is an emergent feature of a bunch of non trivial phases of open systems. Yeah, yeah, yeah. So let me. So, yeah, so what do I mean by a non trivial phase? I mean, in the language of my initial definitions, some place, some steady state.
00:41:41.548 - 00:42:58.008, Speaker B: Now that it's impossible to get to from a trivial steady state, which I'm going to take to be the identity density matrix in a finite time, I'm not actually, I'm going about it the other way around. So if you think about the Hastings land concept of ground state phases, they're not defined, if you like, by the fundamental definition is not in terms of some concrete order parameter. The fundamental definition is a non trivial ground state is one that cannot be reached from a trivial ground state by a finite depth circuit. And so I'm defining open system phases in the same, the same basic way somewhere that I cannot go from a trivial point in a finite time. And so long range order automatically implies that you cannot go in finite time because. Because you have to build up the long range correlations. But of course, there are many types of, like strange topological order where you're in a non trivial phase, but you don't necessarily have an order parameter.
00:42:58.008 - 00:43:31.554, Speaker B: That's conventional. You define order. I'm not saying anything yet about quantum, but. Yeah, you define what it means for an open system to be in some sense, in a non trivial phase, and then ordered phases are going to be a subset of non trivial phases. Yeah, exactly. In the equilibrium case, I think what ends up happening is that you define quantum order in this way. That's expansive enough that eventually you encompass everything.
00:43:31.554 - 00:44:01.714, Speaker B: But in the out of equilibrium setting, the notion of a non trivial phase comes first. And then you have to say, well, you know, what is the, what is the parameter? What is the thing that is the characterizing feature of this non trivial phase? And that's going to be. There's no classification of that yet, but I'm sort of right now just defining what it means to even have a classification. Yeah, yeah, yeah, exactly. Under condition that you have. Yeah, you have a steady state. You have a steady state.
00:44:01.714 - 00:44:28.712, Speaker B: We're talking about Lambladian in terms of its steady state. We're going to specialize the case where there is a gap in the spectrum and, and we're going to ask about the phases of those guys. Okay, so that's, that's, that's, that's the. That's the structure here. Yeah. Um. You're, um.
00:44:28.712 - 00:45:01.694, Speaker B: Everything is. Is limited by locality, and everything is limited by having to be time independent because I'm talking about. Yeah. So each block just repeats over and over and over again because that I'm talking about phases of a Lindlar general channel. Right. So that means that whatever the time evolution operator is for one finite amount of time, that just repeats. It excludes non locality, it excludes time dependence beyond, like.
00:45:01.694 - 00:45:26.502, Speaker B: Okay, time dependence, which I can fold into my definition in a natural way. Yeah. That's not allowed. Yeah. Okay, so one interesting consequence of this, um. Of this nature of this non trivial, um. You know.
00:45:26.502 - 00:45:35.038, Speaker B: Okay, so I don't really. I don't really argue for this being a phase. Right. It's just like a. This is just a situation where you get, um. Where you get a, um. Right.
00:45:35.038 - 00:46:06.086, Speaker B: You get a non trivial phase. You're a non trivial state. Why am I calling it non trivial? Because it's qualitatively. Totally different from, um, the uniform distribution, um, over, um, all states. And so in some sense, in that sense at least, it's non trivial. But I haven't told you about stability. And in fact, if you think about completely general perturbations, it's easy to see that this guy is not stable.
00:46:06.086 - 00:46:55.106, Speaker B: Because if I include some arbitrarily recopping between the left end and the right end, then that's going to break this entire structure. Because now you're going to. Now you'll find that the new steady state is actually just essentially uniform. So if I allow completely general perturbations, then it is not going to be the case that these states are stable. But if I forbid this because of locality, then adding perturbations, like for example, a bit of right hopping or whatever, is not going to change the nature of the state. And so it is under that restriction in effect. Okay, okay, so this is a simple example of the kind of thing that I'm going to talk about.
00:46:55.106 - 00:48:01.014, Speaker B: But of course, the single particle example that I just chose because it was easy to work through. One thing that you realize when you start thinking about these problems is in fact there's a very large family of problems that look a lot like this simple particle on a line example, but they now act on many body state spaces. And in fact, all deterministic cellular automata have this property that their eigenvalues are either, they're either magnitude one or magnitude zero. And so if a cellular automaton has a unique steady state, then it sort of automatically has to have this property. And it takes a long time to get there, then automatically has to have this property of having large Jordan blocks in a configuration space. And so this is a very general property of open systems. That's true in most cases that we looked at.
00:48:01.014 - 00:48:49.084, Speaker B: And so I gave you this example of the instability to adding hopping that goes the wrong way. And so in that example it was clear what I had to do to forbid it. But the conjecture you might have, generalizing this to any other, to general configuration spaces, is basically that even though there are these Jordan blocks and they give rise for long times, local perturbations don't move you non locally in the Jordan blocks. They only sort of move you around. That if you have a big Jordan block, it's like a staircase. And so you're up here in the steady state. And if you move all the way down there, then that destabilizes you.
00:48:49.084 - 00:49:25.696, Speaker B: But local perturbation is not allowed to do that. They only move you around locally in this, on this decade. That would be the conjecture for the structure of these objects. Okay, so I think I have successfully managed to eat up almost all of my time. So that's good. I won't go much longer. So that's, in fact, most of, um, the part of the talk that's designed to be at all comprehensible.
00:49:25.696 - 00:50:34.450, Speaker B: So it's probably a good time to, um, to sort of zip through, um, what remains. Um, so, yeah, so, so the, the basic principle, um, that you draw from, the lesson you draw from this simple example is that, um, you know, you need, you need a gap. And, and the gap, by dimensional analysis is some timescale, um, and so we've already said the gap doesn't control the relaxation of arbitrary initial states. And so what does it actually do? And so the main insight into all this stuff is that even though general initial states don't relax on a timescale that's set by the gap, local perturbations to the steady state relax on a timescale that's set by the gap. And so when you get there, then you're inside a phase. Different points inside the same phase are connected by local perturbations, and so they relax fast to each other. So you can use the channel to move efficiently between points in the same phase.
00:50:34.450 - 00:51:31.764, Speaker B: But when you try to cross a phase boundary, then the channel becomes very inefficient at moving you into steady state. So that's the sort of summary of where we ended up. And so I'll just briefly present this rather formal definition of how this works. So the idea is that you say that you have a channel over here, and then, so you say that any steady state inside this ball around this initial channel can be reached from it by a quick evolution. Not, you know, so this is just relation among steady states. It's not relation among general states. And so you have this uniform across these, across the ball, and then you just sort of tile the entire phase with these little balls in them, and you sort of move from one to the other.
00:51:31.764 - 00:52:39.820, Speaker B: You move from any point to the other by going down a sequence of, of these overlapping logs. Okay, and so, long story short. Yeah, so one more thing I should say about this, which is, you might say, well, what does it mean to say that there's an equivalent relation of phases in open systems? Of course, if I take my arbitrarily complicated state and I just depolarize everything, then I can do that in a finite amount of time with a finite depth channel. Right. So what does it mean to think about being in a phase equivalence relation at all? It seems like it's a hierarchical relation. It turns out that if you define phases in this local way, then actually being in a phase is an equivalence relation. And basically the point is that if you imagine taking a highly ordered state and then um, and then um, letting it, um, and then adding, let's say a depolarizing channel, you have to do it continuously.
00:52:39.820 - 00:53:29.194, Speaker B: You can't just put it in all at once. And when there's a very weak depolarizing channel and that corresponds to very long time depolarized. And so there is a diverging time scale, um, even if you, even if you go in the direction that you might think is not an equivalent. Okay, so um, let me just zip through the stuff. So um, so, so you can, long story shorty, improve all the same things as you would prove in the equilibrium case once you use this definition that I presented very briefly in the previous slide. And so the one thing that we aren't able to get a really good theoretical handle on is the spectral gap. So unfortunately we don't really have good techniques to talk about the gaps of general many body matrices.
00:53:29.194 - 00:54:06.244, Speaker B: And for that. So the conjecture that we have is that the spectral gap is the time it takes for two points in the same phase to relax to each other, two nearby points, the same face to relax to each other, and to check that we have to resort humorics. And so, know the, the point of the slide is that it checks out and it seems to work. The two things track each other extremely well throughout the phase. Um. And um. Okay, so um, I had a comment about quantum order, but it's too much to talk about now.
00:54:06.244 - 00:54:28.438, Speaker B: Um. And um. And so yeah, so this is, this criterion we came up with. Um. Is it actually ever satisfied? Um, we checked explicitly, it was satisfied for some non trivial cellular automata which have bistabilities. So in some sense they really are non trivial because they have order or conventional them. Right.
00:54:28.438 - 00:55:34.784, Speaker B: And so let me stop because I've run over. So the basic point of this talk was to sort of set up the question of what it would, what one would be doing if one were to classify phases of open systems and to present some, some notion of like what such a classification would look like with a couple of concrete examples. But yeah, I mean, a lot remains to be done, including a proper understanding of the space of quantum orders in open systems. And a very important thing that remains to be figured out is how active error correction works inside this framework. Like, how do you define phases of systems in a way that allows, say, the toric code below threshold to be a phase? And we don't really have a lot to say there because techniques we've been using seem to fall apart because no leap ROBINSON all right. Thanks for your attention.
