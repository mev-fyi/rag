00:00:00.240 - 00:01:13.944, Speaker A: I'm going to present a paper by Professor Droswatsky, Professor Pataky and Professor Wolkowicz here. The title is coordinate shadows of semidefinite and euclidean distance matrices. So this is a wonderful paper explaining some phenomena that we observed in the literature about official reduction algorithm for solving this positive semi depth completion problem and euclidean distance matrices. So it's theoretical, but it gives a good, very well explanation of what is happening in the literature. So we're going to start with the first, we're going to start with a linear image of closed convex cones. So for given close convex cones, we look at its linear image and we investigate if the linear image remains closed. So assume that we are given a linear mapping a and c is a closed convex cone.
00:01:13.944 - 00:01:54.864, Speaker A: Essential question in convex analysis is that is the linear image ac close. So you take any elements from the closed convex con c and compute a applied to that element. And altogether this is itself a closed convex cone. But even c is closed, the linear image ac may not be closed. So this is a very interesting question in convex analysis. And this question is related to several different other problems. For example, the preservation of lower semi continuity.
00:01:54.864 - 00:03:08.524, Speaker A: If you have a lower semi continuous function, f, for example, and you may consider the composition of a and f, and you ask yourself if a and f remains lower semicontinuous. And this question can be related to the problem here by looking at epigraph, actually a whole chapter in the classical textbook by Rockefeller. Convex analysis is devoted to this problem here. There are also other applications, uniform duality in conic linear systems, in this paper too, by Dufin, and the existence of solutions to extreme problems. So I'm not an expert in those problems, but just give you an idea how this problem are interesting, why this problem become interesting in the literature. There is a more recent paper by Pataky talking about the relation between the nice cons and the linear image. So let me just give two concrete examples of linear images of closed convex cone.
00:03:08.524 - 00:03:53.514, Speaker A: So let's define linear mapping from the set of two by two positive semi definite matrices to two reals. By mapping the diagonal elements x and z over PSD matrix big x to two reals, they arranged in this way. So it should be clear that the image set is precisely r two. Sorry, it's r two plus here. I'm sorry, there's, there's plus missing here, because you can take any positive non negative elements for x and z and simply set y equals zero. And this is positive semi definite. Now the set, this set is clearly closed in this case the linear image is closed.
00:03:53.514 - 00:05:04.644, Speaker A: Just to see that image is closed you simply take a series sequence in the image set and this sequence has a limit and we have to show that the limit remains in this set and this is clearly the case here. So the image set is closed. As a counter example, the image set of this linear mapping here is not closed. So instead of mapping the diagonal elements x and z, we instead mapped. We instead map the first row x and the off diagonal entry y. Now the image of the PSD cone under this linear mapping is union of orange zero and positive in the first entry and the real numbers in the statement. So in the picture it looks as follows, it's everything here but except this line, but it still contains this zero here.
00:05:04.644 - 00:06:33.680, Speaker A: Now this set is clearly not closed. To show that this is not close, it's simply taking another sequence here which converge to this dash line here and and upon here is not image is not in the image set of this mapping here. So hope this gives you a rough idea about the linear image of closed convex cones. And next we are going to talk about the linear image of closed convex cones and its connection with the problem called the positive semi definite completion problem. And I'm pretty sure you are all familiar with this problem, but let me just go through this quickly once more to get familiar with the setting here. So in the PSD completion problem we are given an undirected graph g with n vertices and additionally we have a vector a whose length which is a column vector whose length is the number of edges in the graph g. And we're looking for a positive semi definite matrix x so that the ij's entry in this Psd matrix x equals the ij's entry in the column vector a.
00:06:33.680 - 00:07:40.396, Speaker A: So the column vector are indexed by the edges in the graph g. So for example, the question is can we find values for the free entries in these question marks so that the matrix x remains becomes a steer positive semi definite. And in this case the graph g looks as follows. We have the vertex one corresponds to the first row and column. We have a self loop because the diagonal entry has a value on it which is seven, one and two is connected because there is an entry corresponds to the one two entry in x. And if we can find such a completion then we call the vector a PSD completable. Now how is this problem related to the linear image of closed convex cone? Because the set of PSD completable vectors can be represented as follows.
00:07:40.396 - 00:09:19.984, Speaker A: We define a projection p which maps the any PSD matrix into a column vector in re by writing down the corresponding entries xiji in the corresponding position. And clearly for any vector in this projected set is PSD completable because that's how we constructed the set. Any PSD matrix here this projection has to be PSD completable. So in this paper they have answered the following interesting question, namely, when is the projected set is closed? And actually they give a very, give a characterization for this problem with a very nice combinatorial they have a combinatorial characterization of this problem, namely the projected set is closed if and only if l and l complement are disconnected, where l is a set of vertices in the given graph, which has a self loop. So a self loop is an edge connecting cell. So if we look at our example here, a self loop would be the vertex is one has a self loop connecting to the self, the vertex two has a self loop connecting the self three. Now the complement is three and four, the remaining two vertices here.
00:09:19.984 - 00:10:51.884, Speaker A: Now if you have followed the theorem, then you know that the projected set is actually not closed, because here L and L complements are actually not disconnected, they are connected because this, because of this edge here. So this is a very nice result. It's proof is very elegant, it's very simple with elementary arguments. So I think it might be helpful to go through it quickly, as you may find it helpful for proving other results. So to prove this statement that we first go from the direction, if L is disconnected from L complement, then the image of the PSD cone is closed. So as we mentioned already, to show that this set image set is closed, we simply take a sequence in this set which has a limit a, and all we need to do to do is that we have to show a is also in the set, namely a is also PSD completable. Since all the AI is PSD completable, there exists an auxiliary sequence, Big xi, which is positive, semi definite, whose projection is little AI.
00:10:51.884 - 00:11:50.452, Speaker A: Actually, we look at a special case here. If l is all the vertices, so namely all the diagonal entries are specified here. But that means the diagonal elements of all the matrices xi in this sequence actually converging to some constant. That's the diagonal entries here. So that means the diagonal elements of xi are actually bounded. And it's a elementary exercise to do to show that if a positive semidefinite matrix has a bonded diagonal elements, then the whole matrix is bounded, because off diagonal elements cannot be too large or too small. So if you have bounded sequence, then there exists a convergent subsequence and we just assume that xi itself is converging to some x positive semidefinite.
00:11:50.452 - 00:13:10.844, Speaker A: Now by continuity of the projection, we have the projection of this limit x is precisely the little vector a at the beginning, and this shows that a the vector a is PSD completable, which means it is in this set again, and this shows that the linear image is also closed. The general case is actually an application of this special case. In the general case, not all the diagonal elements are specified. So for example, in this case only the first two elements are specified. First two diagonal elements are specified, so arrow is one and two only. We can simply apply our previous arguments for the special case to the first two by two leading blocks here, and conclude that there exists a PSD completion for the leading block here, and for the lower right block there exists, there always exists a PSD completion. And to see that we simply take any completion not necessarily positive semi definite.
00:13:10.844 - 00:14:20.916, Speaker A: And then since the diagonal entries are not specified, or we can simply add a big identity matrix to this lower right block and this gives you a PSD completion. So this block is always PSD completable. Now by our assumption, l and l complement are disconnected. So we have the freedom to choose whatever values we wish for this off block of diagonal block. So in particular we choose it to be zero here, and such a completion is positive semi definite, which shows that the set is closed in the general case. Now to show the converse direction, actually we show that if l is not disconnected from L complements, then the set is actually not closed. We again start with a special case where we only have two vertices.
00:14:20.916 - 00:15:20.134, Speaker A: So we're looking at the two by two matrices, which is pretty simple. For example, we construct a sequence of PSD completable matrices. So the matrix has k inverse in the diagonal. In the first diagonal entry one is off diagonal. And it remains to ask, can you find some values here so that this matrix is positive semi definite? Well, the answer is yes, you can just take a big value for lava and that gives you a PSD completion which is denoted by x k. Now with this in mind, we can just construct a sequence ak, which is the projection of xk. Now since xk is positive semidefinite, Ak is clearly in the set of in the image set of the PSD positive semidefinite cone.
00:15:20.134 - 00:16:45.904, Speaker A: And what is the limit of Ak? And such a sequence ak converges to zero and one, because k inverse goes to zero and one remains one. But the limit a is a partial matrix zero one one here, and this matrix is not PSD completable, because if you have a zero in the diagonal and for to be positive semi definite, the corresponding row and the column has to be zero as well. And that means we have constructed a sequence ak in the, in the set, but it has a limit which is not in the set. And this shows that the set is actually not closed. Now, the general case is even is actually a special, actually uses a special case by simply looking at the two by two submatrix generally associated to any vertex in L and j in the complement of L. Because if there is a two by two submatrix that is not, that cannot be completed, then the whole matrix is not PSG completed. So, all right, so that's the end of the proof.
00:16:45.904 - 00:18:09.474, Speaker A: So are there any questions before I move to the next topic? Right? Okay, if not, then I will proceed. So next I will move to another interesting result from the paper, which is about the the boundary structures of the linear image of a closed convex cone. And this result has some interesting connection with visual reduction and exposed phases. So I'll give a short introduction to facial reduction first. So what is visual reduction actually? Problem starts with strict feasibility, a constraint qualification for conic feasible sets. If you are linear mapping a and see a closed convex cone, we can define the following set, which is intersection of affine space and the cone and this feasible set. And many interesting problems can be modeled using this feasible set, including the PSD completion problem.
00:18:09.474 - 00:19:19.140, Speaker A: And quite often people find it very difficult to solve problems, optimization problems or feasibility problems for general conical feasible set. So one way to make it easier is that we have to impose the so called strict feasibility condition on this feasible set, which requires f to contain a feasible solution x f and x has to stay in the interior of this closed convex conceit. Now, without this condition, then many better things can happen. For example, the KKT condition is no longer necessary for optimality. For solvers that relies on solving the kikuti system like interior point method, it might suffer some difficulties. Strong duality may not hold. So typically you look at, especially for optimization problems where you would like to have a reliable lower bound.
00:19:19.140 - 00:20:08.254, Speaker A: For example, a standard strategy is to look at the dual problem and compute the dual objective function. That's always but if strong duality does not go, then your dual objective function does not reflect the true value, true optimum value. Small perturbations may render the problem infeasible because you don't have a point in the interior. And for all those reasons, many solvers might run into numerical errors and actually, that's it. Actually, this is a very serious problem. There are examples of semi definite programming whose problem size is tiny. For example, x is just five by five PSD matrices.
00:20:08.254 - 00:21:11.364, Speaker A: But it is so ill conditions that even commercial solvers returns garbage for you. Now, fortunately, there is a way to save to handle this problem, which is called fish reduction, and it is a regularization technique that can be used for any abstract convex problems without strict feasibility. So it covers definitely a lot of different problems. We will especially talk about the linear program and semi definite program here. The idea of feature reduction is actually very simple. Let's use linear programs as an example. If we have a linear program whose feasible set is depicted as follow, you can see that because the variable z is fixed to be zero, the feasible set is actually not four dimensional, and therefore it's not strictly feasible.
00:21:11.364 - 00:22:11.254, Speaker A: And official reduction simply removes these redundant variables that and gives you a problem which is four dimensional. So, so in general, the variable. So of course, if you see a variable like this z equals zero, then you should clearly remove it. But feature reduction actually handles something more general, because in general you may not have a, a zero variable that is stated explicitly in the problem formulation. It might be hidden, it might be implied by a set of linear equalities and inequalities, and it's a highly non trivial task to find this. And this is actually the key of visual reduction. So let me elaborate this idea further using semi definite programs, which is more relevant for the PSD completion problem.
00:22:11.254 - 00:23:18.254, Speaker A: So if we have defined a spectral hedron in this way, let's look at what a facial reduction does. So let's start with the special case. Now assume that all the feasible matrices x is actually block diagonal with a zero diagonal block. Then the problem becomes simpler, because we know that x is positive semi definite if and only if this nonzero diagonal block r is positive semidefinite. And that means we can replace this PSD constraint by this smaller PSD constraint and just remove all those extra variables. And this use a smaller problem, which is what feature reduction does in this special case. So this case is pretty is essentially the same as the previous linear program example, where a variable is explicitly told to be zero, so you can remove it like directly.
00:23:18.254 - 00:24:37.004, Speaker A: Now the general case is more complicated. Such a zero diagonal block may be hidden from the user. If strictly feasible feasibility fails, then there always exists an orthogonal matrix p, so that if you pick any feasible matrix x and you compute this orthogonal transformation, then you will end up with the same diagonal, same block diagonal structure with a zero diagonal block. So essentially this means that if strict feasibility fails, the orthogonal transformation is essentially a rotation of the feasible set f here. So after rotation then you can see that all the feasible matrices has a block diagonal structure, and therefore you can proceed with the same argument in a special case here and end up with a smaller problem. And the whole story about the feature reduction is how do we find such a, this orthogonal transformation p here? And this problem is highly non trivial. So let's see how we can find this formal transformation.
00:24:37.004 - 00:25:27.386, Speaker A: Intuitively, if strict feasibility fails, set is satisfied, then we know that there exists a matrix x satisfying this equality constraint, and x is positive definite. So interior of PSD coin is positive definite matrices. So you have x star laying in the intersection. And if strict feasibility fails, then there exists no matrix in the intersection. Now this set is an open set, so there exists no matrix intersection. But then we know that those two sets are convex sets. And therefore we can call the hyperplane separation theorem which states that there exists a separating hyperplane separating these two sets.
00:25:27.386 - 00:26:26.004, Speaker A: And such a separating hyperplane can be used to construct this orthogonal transformation p that we are looking for easily. So in some sense they are actually equivalent. And this separating hyperplane w in the context of fissure reduction is called an exposing vector. And what is good about this, what is magic in the feature reduction is that such an exposing vector can always be found by solving an auxiliary system stated here. And then you find this separate hyperplane by setting w equals a adjunct b. So here is a description of the Fisher reduction algorithm. If we are given a feasible set f.
00:26:26.004 - 00:27:39.206, Speaker A: Now exactly one of the following statements holds. Either f is strictly feasible, then we are happy, we don't have to do anything, or f is not strictly feasible. And in that case we can always find a vector v so that this auxiliary resistance holds. And as we just discussed, by solving this auxiliary system, we obtain exposing vector w, which was the separating hyperplane in the last slide. And this separating hyper plane gives you a rotation which you can discard the extra zero redundant block to obtain a smaller problem. Now once we get a smaller problem, if it is strictly feasible, then we are happy, and if not, we can actually repeat this procedure to this smaller problem until we obtain a strictly feasible formulation. Right? This so why do we call this exposing vector and why the whole algorithm is called fish reduction? That can be explained in the that is what we are going to do next.
00:27:39.206 - 00:28:39.454, Speaker A: The connection between facial reduction and exposed phases so exposed phases was also discussed in the previous talk. Let me just go read slightly here. Again, if we have a convex set of c, a face is essentially a generalization of vertices. So if we pick any elements x in the face, so that the element x is laid in the line segment between two elements y and z in the set, then we must have that y and z is also in the face. So for example, a vertex is a phase. So why is a face? Because by the definition of vertex it cannot be written as a convex combination of any two different points in c. So this definition holds trivially.
00:28:39.454 - 00:29:42.444, Speaker A: Each edge is a face of dimension one. Because if you pick any two points in the convex set c, the line segment connecting those two points, if the line segment of connecting those two points has a non empty intersection with this edge here, then you can see easily that those two points has to, those two points have to lay inside this edge. So this edge is also a face. So the face, the amplitude and the convex set c itself are clearly a face of c. And in that case it's not proper. So otherwise we call f the face f proper. So the face is definitely something about the boundary of the convex set.
00:29:42.444 - 00:30:27.104, Speaker A: So the next question is what is an exposed face? For example, if you have a vertex, you can draw a hyperplane which intersects the convex set, gives you this vertex here. And in that case if you can find such a supporting hyperplane, then this vertex is called exposed. For example, the edge here is also exposed because you can find such a hyperplane. But what is interesting is that not all the faces are exposed. An example is given here. This is a union of a unit disk and a square here. Now the point a is a phase, is a phase.
00:30:27.104 - 00:31:25.978, Speaker A: However, it is not exposed because the only supporting hyperplane intersecting a is this vertical line here. And this vertical line actually contains the whole line segments connecting a and b. So it does not give you a, it gives you something extra, and therefore the phase a is not exposed. So that's exposed face of convex sets. Let's look at a special case of exposed phase for convex cones. So the definition is essentially specialized to convex cones. The convex cone is itself a convex set.
00:31:25.978 - 00:32:57.264, Speaker A: And in that case the supporting hyperplane boils down into a linear subspace which is characterized by a vector v by the orthogonal complement of a given vector v in the dual cone, say star, and in that case we say v exposes f. For example, in this linear program example, we know that the feasible set does not satisfy slater's condition and we can construct an exposing vector b which is zero zero, one, which is clearly in the dual cone of r. Three plus. And that vector exposes the minimal phase of f, the minimum phase f of the non negative oscones containing the feasible origin. Indeed, if you check the definition, the phase f is everything of the feasible set intersecting what is orthogonal to be. But what is orthogonal to v is everything that has zero in the last entry. Because if you multiply v by a non negative vector, which is zero, then the last entry has to be zero here, and that eliminates the third dimension here.
00:32:57.264 - 00:34:35.813, Speaker A: So, okay, so the connection between Fisher reduction and exposed phases. We have stated a theorem of fish reduction algorithm here again, and this auxiliary system solved by Fish reduction algorithm essentially says that this vector w exposes a face f of the cone c containing the feasible sets. And that explains the reason why we call the algorithm visual reduction, because we are actually looking for phase of the underlying convex cone and the vector w is called an exposing. And that's the reason why we call the matrix or vector w as an exposing vector here. And also that's the, the main idea of facial reduction, where you try to find a face, a smaller face of c and a proper face of f of c containing the feasible sets. And that's reason why we can reduce the dimension of the problem. So right now, are there any questions so far before we move on to the, the second main result? Okay, so if not, let's proceed.
00:34:35.813 - 00:36:00.308, Speaker A: So the second main result in this paper state as follows. If we could find a vector v so that the second, the auxiliary system, the fish reduction algorithm, is satisfied, which means from the previous slides that the star v exposes a proper face of c containing the feasible region f. And that is correct if and only if the vector v exposes a proper phase of image set ac containing the right hand side b. So this result establish a connection between the boundary structure of the feasible set f and the boundary structure of the image set ac. So this is a very interesting result because the first characterization was used in the feature reduction algorithms. But it's sort of abstract and hard to understand on its own. And having this alternative characterization in terms of the linear image of c gives you a different way to view the same thing.
00:36:00.308 - 00:37:01.174, Speaker A: And sometimes it can be helpful to understand the problem better using this alternative characterization. For example, let's look at the following concrete example. We define the linear mapping a from the three by three positive semidefinite matrices to two reals, and the right hand side bits are vector one and zero. Sorry. The linear image is defined by mapping the the first diagonal entry in x and the last diagonal entry in x. Now clearly, just like the previous example, the image is all the non negative authors in two reals. So that's the image, and the right hand side is a vector b, which is 10, which is here, and.
00:37:01.174 - 00:38:29.224, Speaker A: And the previous theorem says that if b, which is zero, one exposes a face of image set containing b, which is clearly the case here. If you draw the vector v which is here, and the orthogonal complement of v, which is this red line here, and this red line is clearly, this red line clearly contains the face of the non negative oceans containing the feasible set b, which can be visualized here. And by the theorem we know that a star v, which is the three by three matrix here, will expose a face of the PSD cone containing the original feasible set. And clearly it's not possible to visualize what is happening in the original problem. However, if you look at the image of the PSD set, then we can visualize it in this way. Of course, both characterization do not use a very efficient way to compute the actual exposing vector. It's merely theoretical, gives you a different way to see how it works.
00:38:29.224 - 00:40:13.254, Speaker A: But it does have another useful aspect which is related to the so called singularity degree here. So let's go back to the facial reduction algorithm. Again we see that if f is strictly feasible, then with heavy, and if not, we can solve this auxiliary system to find a face containing the feasible region. But again, if this does not make f tier the smaller problem f tier straight feasible, we can repeat this algorithm over and over again. And it is natural to ask how many steps do we need before you end up with a strictly feasible formulation? Eventually, and such a question is already imposed in the literature. And John soon was the first one who realized that the number of steps which denoted by the singularity degree is a very important parameter, and this parameter was studied recently by many experts in this area, and Stone was the first one who realized that the singularity degree provides an error bond for solving this error bond, for computing the distance from, for computing an upper bound of the arrow of a given solution to the to this feasible set f. And this error bond was very useful.
00:40:13.254 - 00:41:52.194, Speaker A: And later on many new papers have come out relating the difficulty of solving the problems related to a feasible set f and singularity degree. And also, two weeks ago there was also a talk talking about a singularity degree and its relation with the solution size of the of the student size of the feasible solutions in the set in the feasible set f. So here is the second main result here. So it's actually identical to the one presented earlier, but here it is slightly stronger where we put a minimal here, so minimal here it means that this exposing vector gives you the smallest phase of, say containing f. So that means after reducing the problem size using this exposing vector, the dimension of the problem becomes the smallest possible and the problem is strictly feasible. And in that case the singularity degree is exact one, because you have found the strictly feasible formulation in one step. And this alternative characterization sometimes gives you information about a singularity degree where it is not easy to see using the standard characterization.
00:41:52.194 - 00:42:52.644, Speaker A: For example, let's look at her for an example, which is which gives for a given linear mapping a from the three by three PSD cone to three reals here. So again we have, it's similar. We have the first diagonal entry, one x eleven, last diagonal entry, x three three. And additionally we have the last entry which is the x 22 plus x two. And we look at the right hand side piece 100. Now you can figure out the linear image of a p's. The linear image of PSD cone under a is everything non negative in three reals union x and y z, where the first entry and the second entry are non negative, and additionally x times y greater or equal than z squared.
00:42:52.644 - 00:44:13.814, Speaker A: So the feasible set looks as follows. So you have a non negative or something here, and this is handled by the second part. Now the vector b is here in the red dot here, and it is again clear from the picture that if you have to find a supporting hyperplane containing this vector b here, it will necessarily include the whole surface here. So that means smaller space containing b is not exposed and therefore the singularity degrades at least two, because if it is exposed than from the theorem in the previous slide. If we could find an exposing vector for the minimal phase, then that means we find the minimal. That means for the original problem we also find the minimum phase, which would imply the singularity degrees, exact one. And such an observation may not always be clear from the original set, but for the image set you can still show it in this case.
00:44:13.814 - 00:45:23.498, Speaker A: Yeah, I guess I'm running out of time. Let's look at the last part. An interesting application in the PSD completion problem. So in the PSD completion problem we have an undirected graph g, as mentioned earlier. And this problem can be modeled as a PSD feasibility problem where you are looking for PSD matrices where certain entries are fixed to be a. Now an interesting observation is that every click, each click in the graph g actually use an exposing vector for f, so you can reduce the problem size of this, you can reduce the dimension of this set f here using those clicks or the exposing vectors. For example, if we have a p's completion problem here, this two by two mage submatrix is a click on the lane graph used to reduce the problem size of f.
00:45:23.498 - 00:47:14.744, Speaker A: And in fact, if you look at this click here, second click here, and third click here, it eventually reduce the problem size from four dimensional to two dimensional. The last results from their paper, or the last result that I presented from their paper is that if the subgraph g associated to the vertices with self loop, that's the matrix with diagonal entries specified is actually chordal. So those are graphs with no triangles in it, then the exposing vectors obtained in this way from the cliques are actually forming the minimum phase containing the feasible sets. So there's an interesting result, and this also answered an empirical observation in the literature where applying this technique, we have reduced the problem size substantially. As you can imagine that when the problem size is reduced sufficiently to a sufficiently small problem, and then those linear equalities imposed on those variables actually means you only have to solve a linear system instead of solving a semi definite program. And such an empirical observation is partially answered by this theorem here. There was another paper by Nathan and Professor Wolkowicz which used this technique to solve huge PSD completion problems using the visual reduction in this way.
00:47:14.744 - 00:47:20.544, Speaker A: So. Right, I think I finished. Thank you for your attention.
00:47:27.324 - 00:47:28.704, Speaker B: Okay, thanks, Hal.
00:47:29.484 - 00:47:30.384, Speaker A: Thank you.
00:47:43.904 - 00:47:48.564, Speaker B: Any questions? Discussion? We have some time.
00:47:50.024 - 00:48:23.664, Speaker C: Yeah, I have a question. Yeah, I'll go back to the picture of the, showing the similarity. Yeah, this picture. So, yeah, this picture. So is this a singularity degree exactly two. I mean, you can see this, b is not exposed, but then if you reduce it to this plane, then you apply it, you find another single exposing vector, then you will be able to expose b, right?
00:48:24.084 - 00:48:32.304, Speaker A: Yeah, exactly. Your observation is absolutely correct. So the singularity degree is exactly two.
00:48:33.384 - 00:48:42.280, Speaker C: Yeah. Then I think so the single egg degree of the linear image is exactly the single leg degree of the original problem.
00:48:42.352 - 00:48:42.656, Speaker A: Right.
00:48:42.720 - 00:48:44.004, Speaker C: It's not just two.
00:48:44.784 - 00:49:05.734, Speaker A: Let me see. So the singularity degree of the image set is the same as the singularity degree of the original set. Yeah, I think you are, you are right. If we look at this, actually this theorem.
00:49:07.474 - 00:49:52.014, Speaker C: Because the linear image may not be exposed. So the definition of this thing like this should be a little bit different. There's no auxiliary problem. But you may think about if this face is, the minimum face is not exposed, then you find the exposing vector and expose the face containing this minimal phase and then you treat this face as a cone. And if the minimal phase is still not exposed in this bigger phase, then you do it again until you it, the minimum phase becomes visually supposed in the bigger phase. So that's the definition of the synthetic degree of the linear image. So there's.
00:49:54.564 - 00:50:08.868, Speaker A: Right, right. Yeah, that's a, that's a good question actually. Because in the standard setting where we usually work with closed convex conceit where all the faces are exposed but the linear image may not be exposed.
00:50:08.996 - 00:50:17.344, Speaker C: If everything is, everything in the image is posed, then the one, there's nothing to. Nothing interesting. Right.
00:50:18.464 - 00:50:21.964, Speaker A: Let me see. So you mean if the linear image.
00:50:22.344 - 00:50:36.964, Speaker C: Yeah, if this is a picture exposed, then everything is, can be exposed. So there's, so there's a. Therefore it's not true. So it must be not visually exposed.
00:50:37.784 - 00:50:38.264, Speaker A: Right.
00:50:38.344 - 00:50:38.964, Speaker C: Right.
00:50:39.624 - 00:51:06.174, Speaker A: You are right. If this is exposed then there always exists a v so that this holds and which gives you an exposing vector for the minimum phase for the, for the original problem. Yeah. So for example, if this face, the smallest face containing this point is this face, this, this horizontal line here, right?
00:51:06.334 - 00:51:07.078, Speaker C: Yeah.
00:51:07.246 - 00:51:33.854, Speaker A: And this face is not its first. But if it is exposed then we find the singularity degree of oriental problem. One. Exact. Yeah, you are right. Yeah. It's, the thing is the structure of the linear image is also very complicated.
00:51:33.854 - 00:51:39.574, Speaker A: So in general it's, it's pretty non trivial to check if.
00:51:41.714 - 00:51:50.494, Speaker C: This is my conjecture. I don't, but I don't know. But this was, if this is paper only the single digit degree, why is discussed.
00:51:52.714 - 00:52:08.314, Speaker B: So there's a problem or maybe a connected result about the image being polyhedral. Because if the image is polyhedral then the singularity degrees at most one.
00:52:08.934 - 00:52:12.394, Speaker A: Yeah, yeah.
00:52:13.214 - 00:52:28.714, Speaker B: But then. So, yeah, so is that, does that come into play at all? Yeah, I think with this discussion.
00:52:29.174 - 00:53:30.674, Speaker A: Yeah. So yeah, this is related to what Fe ask. So Fei was claiming that if the smallest face of the image set containing b is exposed, then the singularity degree of the original problem is one. Because if it was exposed, if the smallest phase of Ac containing b is exposed, then we have a vector v exposing the minimum phase. And by theorem a star v is going to be a minimal phase of the original problem containing the original physical set. So the singularity degree is one. So there, so in general there are two scenarios.
00:53:30.674 - 00:53:56.614, Speaker A: One is what we mentioned. If the image set is a polyhedron and in that case the face, all the faces are exposed and therefore singularity degree is always one at most one. Now their second case is that not all the phases of image sets containing b are exposed.
00:53:58.914 - 00:54:02.974, Speaker C: And then, I mean, the minimum phase is not exposed.
00:54:03.674 - 00:54:32.954, Speaker A: Yeah, yeah, yeah. That's, that's what it has to be. The. If the. In this case, yeah, the minimum phase is not exposed, then the singularity degree is not. But if I. But I think if b is here, then this face is exposed, then singularity degree would be one out would be zero, one or zero.
00:54:32.954 - 00:55:44.114, Speaker A: So actually, it depends on both the linear image and b. It depends on where b is. So that's what you said. It depends on the minimum phase containing b. It's not about a particular face. It's what is relevant is the face containing the minimum phase containing b. So I think, as we discussed earlier, like this algorithm, like in general, this algorithm would just solve a random instance completely by the problem collapses into solving a linear system.
00:55:44.114 - 00:56:29.942, Speaker A: So I think it's not, still not clear in the literature about when does this happen. So here, in case the graph is cold, we have the minimum phase here. But it can be an interesting question to ask if, when does this algorithm actually find the PSD completion by solving a linear system? So when does the clips can be used to decompose a problem? To collapse the problem into solving a linear system? The problem could be related to the graph rigidity, or should. It should certainly be related to the graph structure rigidity, et cetera.
00:56:30.118 - 00:56:43.398, Speaker C: Yeah. You can carry out this theorem to the EDM completion problem. Instead of a PSD comb, you have a same theorem for EDM code, right?
00:56:43.486 - 00:56:44.154, Speaker A: Yeah.
00:56:44.534 - 00:57:10.618, Speaker C: Then you can see the stress matrix of the chordal graph. It's like. It's exactly n minus one, minus r. No, no, it's exactly like. It's exactly maximum. So the stress matrix is like. So the EDM completion problem is.
00:57:10.618 - 00:57:34.264, Speaker C: So you have an EDM, and then there's a projective. And then the max rank. Suppose the max rank of the EDM. The maximum dimension of the EDM is r. Then your stress matrix will be n minus r minus one. For choreographs. Well, no, graph is.
00:57:34.264 - 00:57:39.484, Speaker C: You can always complete it to maximum degree, right? Yeah. Forget about that.
00:57:44.164 - 00:57:46.772, Speaker A: What do you mean? Chordal graph is always what?
00:57:46.948 - 00:57:50.524, Speaker B: You can always complete it. But it's nothing to do with the degree.
00:57:50.684 - 00:57:51.424, Speaker A: Yeah.
00:57:53.484 - 00:58:07.084, Speaker C: Okay. It may not be complete to full rank, right? Can chondograph always be completed to full rank to maximum dimension? Like for Edmund completion problem?
00:58:07.744 - 00:58:38.424, Speaker B: Well, depends on. So, like for positive definite completion, you have to start with that. All the principal minors are positive definite. So if you're starting with an. Depends what embedding dimension you start with if you have if you're.
00:58:41.244 - 00:58:41.692, Speaker A: If you.
00:58:41.708 - 00:59:11.534, Speaker B: Don'T have the right embedding dimension, then you can't complete to a high embedding dimension because you're stuck before you start. Same with the positive definite completion. If you have a principal minor that's not full rank, then you cannot complete the matrix to full rank. So if you have a.
00:59:13.634 - 00:59:14.082, Speaker A: If you're.
00:59:14.098 - 00:59:29.024, Speaker B: Trying to do a PST completion and you have a sub matrix principal sub matrix, which is not full rank, then you're stuck. You can never complete the positive definitely.
