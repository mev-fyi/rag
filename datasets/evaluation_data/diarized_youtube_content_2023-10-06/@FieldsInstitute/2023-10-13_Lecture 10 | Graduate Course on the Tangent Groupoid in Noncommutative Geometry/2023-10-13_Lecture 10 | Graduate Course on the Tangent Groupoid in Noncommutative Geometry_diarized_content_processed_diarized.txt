00:00:00.240 - 00:01:17.202, Speaker A: Today it's. Well, today we'll look at the tangent groupoid from another point of view on the grounds that if it's an important thing, you should understand it from its front and its back and underneath and on top and so on. I'm not claiming that any particular perspective is better than any other. This is just another perspective on the same thing that we already constructed last time, roughly speaking, using local coordinates. So today we will be systematic and global and try to avoid mentioning coordinates at all. That will be partially accomplished by suppressing some details where coordinates are really unavoidable. Okay, so let's suppose that one is given a smooth manifold, v, like a unit sphere or rn or I don't know what.
00:01:17.202 - 00:01:56.804, Speaker A: It doesn't have to be compact. It could be some, something like RN and it, let's agree that all decent manifolds are, and this is an example, are Hausdorff and Matreisville. I mean, because life is too short to consider. Are there? I don't think so. When was the last time you really. Well, non hostile manifolds do occur from time to time. Non matrizable manifolds, do they ever occur in the minds of anyone except the criminally insane? I'm not sure.
00:01:56.804 - 00:02:39.076, Speaker A: Yeah, yeah. You're removing ridiculous things like this very long line. When you say matrizable, if it's second countable, then it's automatically metrisable for you. Yeah, that's even stronger. There are times you might want to consider a non second countable manifold. For example, if you decomposed a manifold into the leaves of a foliation, you might possibly want to say that the disjoint union of all of the leaves is a manifold. You might want to do that, in which case you'd have to dispense with second countable, maybe, but not matrizable.
00:02:39.076 - 00:03:11.474, Speaker A: I mean, now we're venturing into the, what Paul Baum would say is territory of mental illness, you know. Yeah, so let's not do that. Anyway, visa manifold. Before I get into further trouble, let me just stop there. So this will be today the algebra over r of real value. No particular reason for doing this. Smooth functions.
00:03:11.474 - 00:04:11.594, Speaker A: You could use complex functions if you want, but these are real manifolds and it somehow seems logical to use real functions on real manifolds. Okay. And I'll probably just call this a throughout most of the lecture, although once in a while I'll borrow a. For something else. And in this particular discussion, it's going to be just an algebra. I mean, it does carry a beautiful topology, but as you'll see it's not really important to carry to worry about that. So I'm just thinking of this as a, as an algebra.
00:04:11.594 - 00:05:27.084, Speaker A: It wouldn't make any difference if you did think of it as a fresher algebra, but you don't have to, okay? And it's a fact that if you look at the algebra homomorphisms, so let's just throw away the zero homomorphism that, depending on your upbringing, that may or may not be an algebra homomorphism. But today it isn't. From a to r is a point evaluation. I think it's pretty clear what that means. There's a point, you pick a point, little v in the manifold big v, and you send a function f to f of v. That's an algebra homomorphism to the realm numbers. And that's it, that's all of them.
00:05:27.084 - 00:06:30.112, Speaker A: And the proof of that is that if there was another one, I mean, we're all adults, our combined age is in the hundreds of years, and if there was another one, one of us would have seen it by now, and we didn't. Right? So there you go. Maybe I'll make an exercise out of it that's not completely obvious. I mean, if you look at inside of c, infinity of v, the cc infinity of v, the smooth, compactly supported functions, that's an ideal inside of here. And so there's a quotient algebra in our algebra, which is all smooth functions, modular, all smooth, compactly supported functions. And it's a non zero algebra, a loss of non compactly supported functions. And you might say to yourself, well, inside of that thing, I'm just going to pick a maximal ideal, which Zorn's lemma tells me exists.
00:06:30.112 - 00:06:52.604, Speaker A: And then I'll take the quotient. It's a field. So I'm automatically going to get a morphism from c infinity of v to c infinity of v, modulo c, c infinity of V. And now to this field, done. It looks like we have another character, but that field is not the field of real numbers, and so that's not allowed. I'm talking about actual r. Character here means more.
00:06:52.604 - 00:07:43.814, Speaker A: I haven't used the word character yet, but I'll be looking at only the field of real numbers, not these crazy, whatever they are, fields that you get in the way that I just described. Okay, so the character spectrum of this thing a, is just v, that is. So each element of v gives an element of the character spectrum. And that's it. And we have some, it's sort of obvious. It's a theorem, I guess that different points of v give different characters. So this is just a, this is some bijection of sense.
00:07:43.814 - 00:09:02.644, Speaker A: And these symbols together with this lemma explain what I mean by the character spectrum. But however, I'm not sure. In addition, it doesn't matter what kind of a real algebra a is, this thing here carries a topology which is a decent topology. What is this topology? We'll say that a net of characters Phi alpha converges to Phi if and only if Phi Alpha applied to any element of this algebra, converges to Phi Alpha. So I'm defining the the topology on this spectrum for NEa or all at once. Yeah, you're right. Thank you.
00:09:02.644 - 00:10:53.454, Speaker A: My favorite eraser. Go. So the pfizer, obviously characters, and another fact is, in the particular case that we're interested in the character, we'll give it a name. Star, the map star, which is a bijection which is given by the previous lemma, is a homoeomorphism. But of course, V is not just a topological space, it's a manifold. And you might ask, how can you understand the manifold structure from a? And it's hard not to make this question sort of stupid, because after all, to give the manifold structure is basically to talk about the space, you know, the collection of smooth functions on the manifold. But let's try to express this in some reasonable way, which is not totally vacuous, because we're going to consider cases where it isn't totally vacuous in a moment.
00:10:53.454 - 00:11:51.378, Speaker A: So what does it mean? I wouldn't say obvious, but, but it's homework. Great. Maybe we have to, now that I could have said that, maybe we have to prove it very slightly surprisingly, ignore the topology on a, but nevertheless, you get the topology on B. Well, the topology on V, of course, is coded into the definition of smooth functional. All of these smooth functions are continuous. That's why it works. People who are siesta algebra, people are not surprised by this in the least, because they know that the actual topology on a sister algebra is sort of secondary.
00:11:51.378 - 00:12:37.274, Speaker A: You don't need to mention it ever. And you still get all of the theorems. Yeah, actually I was mentioning this, someone contested it's more likely to be this thing than this thing. But. So this is not. You asked, is this also obvious? I would say this is not obvious in the case where I don't think I claimed it was obvious, I just said it's a lemon. This is not completely obvious when, excuse me, V is not compact.
00:12:37.274 - 00:13:13.390, Speaker A: So this will definitely be on the homework set of homeworks. But this thing above, behind me is, I think, easier than that, lemon and I haven't. We'll see how it goes. But if we luxuriate an extra time at the end of the lecture, let's just prove this thing, which has become awkward and contentious. I mean, I could say at this point, as Eckhard was saying before the lecture, ah, students these days, you know, they just. They know less and less. So.
00:13:13.390 - 00:13:35.534, Speaker A: But we can, we can come back to this. Well, yeah, that's professors these days. I mean, there's no doubt that each individual professor, as time goes on, knows less and less and less. That's called getting old. Yeah. So don't. Shouldn't trust your elders.
00:13:35.534 - 00:14:00.130, Speaker A: I mean, you should know that. Young, rebellious person like you. You just look. Okay. Okay. What? So we'll come back to this. What about the c infinity manifold structure? Well, yeah, now I remember what I was about to say.
00:14:00.130 - 00:14:48.704, Speaker A: What is. What is a smooth manifold? It's is topological space, Hausdorff, materializable topological space like this one v. And it's given a sheaf of functions it's provided with. To be a smooth manifold means to be provided with a certain sheaf of functions called the sheaf of smooth functions. And all of these functions, and the sheaf should have the following property that there should be, around any point, there should be an open set and a homeomorphism of that open set to an open set in rn, which carries this sheaf of functions precisely to the c infinity functions on that open set. That's what a smooth manifold is. And.
00:14:48.704 - 00:16:11.014, Speaker A: Yeah, so we need to say, what is the sheaf of smooth functions? And, of course, the sheaf of smooth functions. It's sort of tautological here, because it's practically just given to us in the form of c infinity of m v. Excuse me. So, but if you have some other open set u inside of v, then we're looking at functions from, um, say we're talking about sheaf of smooth, real valued functions with the property that locally f is of the form, well, v, which we can think of as a character to g of v, or v of g, if you like, for signing g, which is in a. Maybe I should call that a. And that's it. I'm just saying that the functions which are smooth on an open set u are precisely the functions which are locally equal to some function in c infinity v.
00:16:11.014 - 00:16:55.640, Speaker A: Any function which is a c infinity function on u, a function which is a c infinity function on you may or may not extend to a c infinity function on v, it might blow up near the edges of you and then it wouldn't extend. But locally at least it is a function in a, a function which does extend to a c infinity function on v. No big deal. Little messing around with bump functions will give you this result. And it's, the more you think about this, the less interesting and, you know, more stupid it seems. So let's quickly move on. Okay, so we can recover everything in this particular case from the algebra a.
00:16:55.640 - 00:18:09.346, Speaker A: Its character spectrum is v, and the topology is the standard topology, the what it's called to standard topology on the, on the character spectrum and the smooth functions of those which are just locally functions in a. Let's consider a slightly more complicated examples. Example. So although a is mostly in this lecture going to be the smooth functions on v, just to understand a little better what this sheath construction, let's consider a different a, which is just polynomial functions in some number of variables. So these are real polynomial functions in n variables. This is the algebra over r. So we can ask for the non zero algebra homomorphisms to r.
00:18:09.346 - 00:19:02.514, Speaker A: And of course, once you say what they are on x one up to xn, you know everything. And not only that, but you can assign any values to x one up to xn, and that will give you a character. And so the character spectrum, in an obvious way both ways very easily, is just rn. Well, to begin with, this is just a bijection, but now both sides carry their own individual natural topologies. On the left there is this topology that we just discussed here, and on the right it's just rn. And this thing is a homeomorphism. But maybe you'd like to capture the c infinity structure, the smooth manifold structure of Rn, just from, from a.
00:19:02.514 - 00:20:47.386, Speaker A: And now what's written above is not going to be right, is it? Because you only ever get polynomial functions that way. And so, but it's no big deal to figure out what to do. The sheaf of smooth functions on a, excuse me, on rn can be gotten, that's the word from a pretty easily. So now u stands for an open subset of Rn. And this is just what does it mean to be a smooth function on such an open subset? It just means for every little point point, little u in big u, the function f can be expressed in a little neighborhood of that little point in the following way. So locally, just trying to make it look like what's written up there. Well, now it's a little bit more complicated how to do this.
00:20:47.386 - 00:21:43.748, Speaker A: This, one of the things you can do with smooth functions is you can compose them, because the chain rule, anything smooth composed with smooth is smooth. So that's what's being built in here. So the a's, there's a whole bunch of a's here. Doesn't matter how many they are, and they belong to a and h. Here is a c infinity function on our big n. So elements of a give enough functions to generate all smooth functions where generation means use the chain rule, use composition like this, and then you get all possible c infinity functions. Of course you can.
00:21:43.748 - 00:22:12.954, Speaker A: You don't need an arbitrary, maybe I should say n. Here is some number, and then these are functions. And this is, you don't need an arbitrary n to do this. You just need big n to be little n. But, but who's counting? Anything of this form is going to be smooth. And every function of this form is smooth. And, you know, for those detail checkers, you could get away with setting big n equal to little n.
00:22:12.954 - 00:22:44.974, Speaker A: All right, so you can make the manifold c infinity of, sorry, excuse me. You can make the manifold rn from the algebra polynomial functions. And we'll use this technique in just a moment to make another manifold which is not rn out of polynomial functions. Did you. No, I was just thinking about having. Yeah, yeah. That, yeah, yeah.
00:22:44.974 - 00:23:23.306, Speaker A: You could say, well, this is kind of rubbish because you sort of presupposing what the smooth functions on RNR. Okay, it's fair enough, but there is some value in writing what I just wrote. Some value. All right. Yes. So now let's go back. This case that we were discussing before, temperate.
00:23:23.306 - 00:24:26.962, Speaker A: Well, most of the rest of the lecture, but we'll learn from this bottom board. Suppose I have inside of m like that. Neither of these words is completely essential to the construction that I'm about to write down. But we're only interested in this case where m is a closed subset and an embedded submanifold, the nicest possible type of submanifold of v. In other words, what if m was not closed? Could I do what I'm about to do? Yeah. What if m was not even embedded? It was an immersed submanifold. Could I do what I'm about to do? Yeah.
00:24:26.962 - 00:25:12.454, Speaker A: And, but there, you know, there would be some asterisks and would it be useful? Well, yeah, once in a while it would be useful, but this is good enough for us. So the very first thing I'm going to write down would not be the correct thing to write down if m was not embedded. But let j be just the functions on v. It's the infinity functions on v. Maybe I'll just say f in a. Make it clear where this is. With the property that f restricted to m, the submanifold is zero, the vanishing ideal of the submanifold.
00:25:12.454 - 00:26:07.964, Speaker A: So that's a two sided ideal. What's commutative? It's an ideal inside of a. No big deal there. If you take a function on a, you can restrict it to m, and then you'll get a smooth function on m. And of course, when you do that restriction, the kernel of the restriction, by definition, is going to be j. So there's an obvious map by restriction from a mod j, this quotient algebra into m. And the fact of the matter is that it's nice.
00:26:07.964 - 00:26:40.732, Speaker A: Morse. So in the reverse direction, if you have a smooth function on m, you can extend it so that it becomes a smooth function on v. Sorry, searching my mind for the correct letter. Okay, how do you extend it? Well, you have to use little bump functions and local coordinates, and I don't know what, but. Okay. Gonna argue about that. Okay, good.
00:26:40.732 - 00:27:13.598, Speaker A: Thank you. Yeah, yeah, this is, well, it's. I mean, in terms of app. Well, trying to, you know, if you actually were to write everything down here and up here, it's sort of a race. I think this might actually take more paper, depending. Well, of course, it depends on what you assume doing. Yeah, I mean, okay, if you're going to pick the shit out of it, then no, fine.
00:27:13.598 - 00:28:19.398, Speaker A: But. All right. And what we're interested in is the following thing. Speaking of tubular neighborhoods, the famous normal bundle of M as it sits inside of v, of this inclusion, if you like. So this is a vector bundle, and here's what it is. You take the tangent bundle to v, long letter, which is a vector bundle, bundle of vector spaces over v, and you just restrict it to m. And then you divide by the tangent bundle to m.
00:28:19.398 - 00:28:41.774, Speaker A: Each tangent vector to M can be viewed as a tangent vector to v. It just happens to be pointing in the direction of m. And so you perform this quotient. That's what we're talking about. Okay. Yeah, just checking. Okay.
00:28:41.774 - 00:30:25.580, Speaker A: Right. And what should I like to say about this? Oh, yeah. What's. If I have an element of j, smooth function on the big manifold, which vanishes on the little manifold, it gives, determines a function, smooth function on n by the following recipe. Maybe I'll just add here that is linear on each fiber of. Nice. So if I have a function, I don't know, a in j, how do I get a function on this normal bundle here? Well, let's just take a tangent vector for vlog modulo tangent space event.
00:30:25.580 - 00:30:56.960, Speaker A: So that's one point in one of the fibers of this vector bundle n. And I'll just take x m and I'll evaluate it on a. That's what you're allowed to do. Tangent vectors are like we discussed last time, derivatives. You can apply them to smooth functions and you get numbers. And this all makes sense because a vanishes on M. So if you attempt to differentiate it in the directions which are M directions, of course you'll just get zero.
00:30:56.960 - 00:31:41.200, Speaker A: So this doesn't actually depend on the choice of representative xm of this element, of the corresponding element in the quotient vector space. So there you go, there's a certain map, and we're out of space. Maybe we'll just pack it in here. Anyway, if you do this construction not for an element of j, but for an element of j squared, then you're going to get zero. If you have two functions, f one and f two, a one and a two, and they're both in j, they both vanish on m. And you ask, what is the derivative of the product? Well, there's less. Leibniz's rule.
00:31:41.200 - 00:33:19.796, Speaker A: And Leibniz's rule tells you that the derivative xm of a one, a two is xm of a one times a evaluated at m plus, and then you do it the other way around. But a evaluated at m is zero because m is a point of m. So this thing vanishes on j squared, and that's it. Quotient here is precisely the c infinity functions on this n, the total space of this bundle that are linear on each fiber. So great linear, linear means grown up linear, just ax, not ax plus b, not affine. If you want to get c infinity functions on m that are, say, constant on each fiber, then you need a mod j. If you do this for a mod j, then you'll, a mod j can be identified just by pulling back with the c infinity functions on n which are constant on each fiber.
00:33:19.796 - 00:34:09.494, Speaker A: So we have the functions which are constant on each fiber. We have the functions which are linear on each fiber. So together we have the functions which are affine ax plus b on each fiber. And now we can talk about functions which are quadratic on each fiber and so on. And that's what we'll do over here. The other ones we get by some symmetric algebra construction from this guy here. Yeah.
00:34:09.494 - 00:35:42.594, Speaker A: Yes. Yeah. That's maybe a worthwhile point to make. We'll do it over here. So the arbitrary length all of the AI's and bi's come from j, and because it's about to show, let's say, similarly or any power. Jt, come here. And while we're at it, at the business of answering Paul's questions, it's going to be convenient also to study other, just notationally speaking, other powers of j, negative powers of j, like the 0th power and the minus one power, just, just for the sake of notation.
00:35:42.594 - 00:36:35.484, Speaker A: And we'll just mean a if you keep raising j to higher and higher powers, then j to the k gets smaller and smaller and smaller. J squared sits inside of J, obviously, because J is an ideal and so on. So if you raise the negative powers, it has to get bigger and bigger and bigger. But how big can it get? I mean, everything's happening inside of a. So that stands to reason that this is a reason that this is the right definition. Yep. Okay, I forgot to check this formula.
00:36:35.484 - 00:37:21.296, Speaker A: Let's just work it out on the fly. Let's look at this space. So this thing, before we go on, let's just figure out what kind of a thing this is. This is a direct sum of vector spaces. But this is actually an algebra, because if you have something in j to the k and you multiply it against something in j to the l, then you'll get something in j to the k plus l. And what you get will not depend on small perturbations of order. K plus one here and l plus one.
00:37:21.296 - 00:37:51.304, Speaker A: If you take the product which is in j to the k plus l and study it modulo j to the k plus l plus one. So in other words, you can multiply j to the k mod j to the k plus one times j to the l modulo j to the l plus one. And you'll get a well defined product with values in j to the k plus l modulo j to the k plus lie plus one. You can play that back slowly on the zoom video and check how many mistakes I made.
00:37:51.724 - 00:37:53.460, Speaker B: I had a question, sir.
00:37:53.652 - 00:37:55.236, Speaker A: Okay. Yes, please.
00:37:55.420 - 00:38:07.844, Speaker B: So when you are considering spec of a, so are you using, so are you trying to say, with the shape of smooth functions, is it, is it similar to a scheme or something like that?
00:38:07.884 - 00:38:41.070, Speaker A: And then, I mean, it's clearly inspired by that sort of language. But the scheme language was inspired by Gelfand and sea star algebras and so on. So, and we're not talking here about ideals, we're talking here about, well, first of all, we're not talking about prime ideals. We're talking about maximal ideals. And secondly, we're only talking about maximal ideals where the quotient is the field of real numbers. So this is not the same as what the scheme people would do.
00:38:41.262 - 00:38:50.190, Speaker B: Okay, so is there some, is there some topology on the ideal? I mean, like the only topology that.
00:38:50.222 - 00:39:11.624, Speaker A: We'Ve spoken of is the topology on the set of characters, okay? Which we're calling the character spectrum. And that was the topology. It's now disappeared. The topology of, if you like, point wise convergence, weakest topologies. So that all of the evaluation maps which send phi to phi of a, all of those evaluation maps are continuous.
00:39:12.284 - 00:39:16.144, Speaker B: Okay, okay, okay, okay. Okay, thank you.
00:39:18.684 - 00:40:38.484, Speaker A: Yeah. And so this thing, so I've just discussed that this thing here is in algebra, and by this multiplication that we talked about, it's more than an algebra, it's a graded algebra. We'll come to that in a moment. But what is it? It's just the same thing as, how did I write this over here? C infinity functions on. Nice. That our polynomial functions of uniformly bounded degree on each fiber. I mean, if the manifold m is not compact, then you could imagine a c infinity function on the normal bundle, which in the neighborhood of this point, restricted to a linear function on each fiber, and over here, restricted to a quadratic function on each fiber, and over here, a cubic function on each fiber, and so on.
00:40:38.484 - 00:41:13.460, Speaker A: And dida dee da de da. So that would not be allowed here, the sum number for each function. In this collection, I've just defined some number like seven, so that when you restrict to the, to any fiber, you never get a polynomial of degree more than seven. Yes. Yeah, yeah. It's like the differential operators. I mean, you could have a differential operator in rn of unbounded degree, which was sort of locally a finite degree.
00:41:13.460 - 00:41:55.844, Speaker A: And now you have to decide, is that something you want to play with on not, no, we don't want to play with such. Yes, this is an isomorphism of graded r algebras. Okay, fine. And what is it? That's the formula that I forgot to write down. So let's just tough it out. If we can. Maybe we'll go over here, because we'll undoubtedly make a mistake.
00:41:55.844 - 00:43:06.598, Speaker A: Well, suppose, let me put a little square bracket around a like this. Suppose this, maybe I'll put a k here. What am I supposed to do with an element of this quotient vector space? So little a means an element in the kth power of j modulo elements in the k plus first part, just. Oh, notation. You weren't asking for some. Yeah. So we have this function modulo other functions.
00:43:06.598 - 00:43:48.650, Speaker A: And now we're supposed to get a function on cotangent vectors. So we had a little discussion about cotangent vectors before. Here's an example. Now I have to get a number out of this, and the right thing to do is something like this. Well, there's going to be lots of derivatives. So let's, I'm pretty sure the right thing to do is to divide by k factorial, because it always is. And let's just do the following thing.
00:43:48.650 - 00:45:09.712, Speaker A: We'll just apply x to x to x of a, which I do it like that. So they're supposed to be kx's here, if I got the parentheses approximately right, looks like it's sort of. Okay, and what's x? Oh, maybe now we want to evaluate this thing at the point m. So maybe in the last one we'll do it there. Go back to. Because a is a function which vanishes so much. First of all, it vanishes in the m direction, just plain.
00:45:09.712 - 00:45:40.680, Speaker A: Absolutely. But actually it's a product of k functions which vanish in the m direction. So it vanishes real good on m, and it also vanishes to a high order k in the directions orthogonal to m. Because of that, this thing here doesn't actually depend on this choice of extension. And this is an algebra map. If all goes according to plan, it's like, yeah, and here we go. This is what we're talking about.
00:45:40.680 - 00:47:12.900, Speaker A: This is the isomorphism I'm interested in. So the character spectrum. So now we have a new algebra, this gigantic fellow, ah, what should we call it? We have an a, we have a b, and in a short time we'll also have an r. So b means, well, either one of these, they're isomorphic. Let's say it's specifically this one here. ThiS FormULA that I just described is a bijection is a map from n into the character spectrum of b. You have an element of n.
00:47:12.900 - 00:47:44.198, Speaker A: N is the normal bundle. So an element of n is one of these normal vectors. It's tangent vector in the v direction, modular tangent vectors in the m direction, and you can get a function out of that normal vector, a function on b, by this crazy formula up here. And that function is actually an algebra homomorphism is what I'm asserting here. And that's so it's a character. When I say it's a function, it's the r valued function. So it's an algebra homomorphism to r, it's a character.
00:47:44.198 - 00:48:53.494, Speaker A: And what I'm saying is that those are the only ones. This is a combination of the results that we had before. This space n is a smooth manifold in one direction, in the m direction, and it's just a vector space in the other direction, in the fiber direction, and in the m direction. We're talking here about, in this algebra, we're talking about smooth functions on m, and in the fiber direction. We're talking about polynomial functions on mix. And so this fact here is a combination of the lemmas that we gave before, stated before, and I'm just shoving this one down your throat, too. Okay, so we have vector bundles, sort of like we had the tangent bundle on Tuesday, vector bundles creeping into the picture in an odd way.
00:48:53.494 - 00:50:10.626, Speaker A: Yeah, we'll see what we want to do with those. The main construction is going to happen almost immediately. But let's just pause for a moment to observe, as the crowd already did observe, that this b breaks up into a direct sum of spaces indexed by the natural numbers, french natural numbers, including zero. And if you multiply something in the, like we were just talking about the jth space by something in the kth space, you get something in the j plus k th space. That's what it means to have a grading on this algebra beat. The grading is what's given right here. And to have a grading is basically the same thing as saying you have an action of, well, r cross, or, depending on which field you're talking about, r cross or the circle or c cross.
00:50:10.626 - 00:50:54.014, Speaker A: First of all, it carries a derivation. If you have something in this algebra, that's something in degree k for all of the k's added together finite sums. Of course, we're just talking about the direct sum. If you have something in this algebra, you can define a derivation just by multiplying kth element by k. Sorry for so many k's. They're all supposed to be there. Unusually, it's not a typo.
00:50:54.014 - 00:52:46.214, Speaker A: And if you have an algebra which carries a derivation, then it sure is tempting to exponentiate that derivation to get an action of the group of positive real numbers, or the real numbers, if you like, on the group on the algebra, and that in this particular case works out just fine. I said I was going to start something new over here, but ran out of space. So we get out of this in action of the positive real numbers, which is what we were talking about on Tuesday. I have such a sum. Well, I don't even have to do. Let me change, making this unnecessarily complicated. I don't particularly happy with this notation, but let's just go with it for now.
00:52:46.214 - 00:53:50.840, Speaker A: So here's an element of the algebra. And what kind of action is this by automobiles? This doesn't look like an algebra automobile. Let's just go with this notation. All you do is you just stick in lambda to the k a k like that. I just can't stand looking at that. It's called this alpha lambda, nothing complicated. Well, the fact of the matter is that once you have an action and an automorphism of an algebra, of course you get a corresponding homeomorphism, self homeomorphism of the character spectrum of the algebra.
00:53:50.840 - 00:54:58.072, Speaker A: So this gives us now an action of the group of positive real numbers on the character spectrum and what that action is. So we're talking about the algebra b, which was my completely uninformative notation. I'm always criticizing my students for crappy notation. Okay, here we go. B is this thing on this board here, this graded algebra is. Well, here's an element, as we've been discussing, of the normal bundle, single normal vector, and it just gets multiplied by lambda. So this rescaling action, which we caught a glimpse of on Tuesday, comes whenever you have a grading.
00:54:58.072 - 00:56:13.660, Speaker A: Gradings just give rise to actions of, well, in this real context, actions of the post of real numbers. Now finally, for the main, the main construction, one more algebra over the real numbers. We'll call it r. And it looks a little crazy, but we'll try and immediately make a, make contact with this b over here. It's going to be an algebra of Laurent polynomials. So these sums are going to be finite sums. I'm going to write down the power of t times a coefficient function and all but finitely many of the coefficient functions are supposed to be zero.
00:56:13.660 - 00:57:05.304, Speaker A: Anyway. Let's call them a to the ak, t to the k, like that. So almost all, that is to say, all but finitely many of the aks are supposed to be zero. No, no, no complex analysis, no worrying about infinite sums. And the other condition, which I left too much room for, is that Ak should be in the not the case, but the way I've written it, the minus, I like it better this way, the minus kth power of j. So these, you can think of these in two ways. You can think of these as actual functions from, let's say the positive real numbers or the non zero real numbers into.
00:57:05.304 - 00:57:28.046, Speaker A: Yeah, into the algebra a. I did say where these guys come from. Or you can think of these as just formal Laurent polynomials. There is such a thing as the Laurent polynomials in t with values in a and that's a certain algebra. And this is a subalgebra of that. That's what we're talking about. Okay.
00:57:28.046 - 00:58:26.444, Speaker A: Oh, is it a subalgebra? Well, yeah, because j to the minus k times j to the minus l is j to the minus k plus l. What about those minus signs? Well, I told you about the minus signs before. J to the minus k means just j to the absolute value of k if k is negative and j to the minus k is positive. Well, j to the minus k if k is negative means j to the minus k. J to the minus k of k is positive just means a. That's what we said before. So if you look at the polynomial part of this, you look at Laurent's series, which just happened to be polynomials have no negative powers, then that's just polynomial functions of a variable t with values in a.
00:58:26.444 - 00:59:22.046, Speaker A: So as far as the positive powers of t are concerned, there's no constraint on what the coefficients Ak are. They can be anything in a. But when you start to look at the negative powers of t, then you get into constraint lengths. So, for example, the coefficient of t to the minus one has to be in j, and the coefficient of t to the minus two has to be in j squared, and so on and so forth. That's how it's working. Okay, so what do we, first of all, this is a commutative algebra, where, of course, we're going to study its character spectrum, because I spent the whole day prattling on about character spectrum. When we've built the character spectrum of this thing, it's going to be equipped under, if you like, restriction of characters, is going to be equipped with a map to the character spectrum of polynomial functions on a with values in t.
00:59:22.046 - 00:59:41.634, Speaker A: But the character spectrum of that is just the character spectrum of a times r times the character spectrum of polynomials in t. So whatever this space is, which is the character spectrum of r, it's going to come with a map to v times r. I just had a question. It's going to be. Yes.
00:59:42.374 - 00:59:51.434, Speaker B: So e of t can also be, I mean, this Lora polynomials can also be graded right in the same way as we were doing before.
00:59:51.974 - 00:59:54.434, Speaker A: So can you repeat, please? I didn't quite hear.
00:59:54.974 - 01:00:00.070, Speaker B: So the Loran polynomial Laura polynomials can also be graded, as we were doing.
01:00:00.182 - 01:00:03.394, Speaker A: Yes, this is indeed, this is a graded algebra, as I'll mention.
01:00:05.224 - 01:00:06.124, Speaker B: Rightly.
01:00:08.424 - 01:01:27.264, Speaker A: In fact, let's do it right now. So now it's a, this r is a direct sum of spaces, namely the monomial spaces, ak times just a fixed t power t to the k. And those monomial spaces are now indexed by integers, not by non negative integers, by natural numbers like they were before. But otherwise we're still in business. This is a graded algebra, so it carries a derivation by exactly the same formula, and it carries an action of the positive reals by exactly the same formula. And it's going to be interesting to ask what is the character spectrum and how do the positive reals act on this thing? Okay, so we'll figure that out in a moment. Now, concerning this map, we're mostly going to be interested in just the subsidiary fact that the real polynomials sit inside of r.
01:01:27.264 - 01:01:48.832, Speaker A: So that means the character spectrum after we've built it, is going to be equipped with a map down to r. It is indeed also equipped with a map down to v times R. But that's not such a good map. The map to r is really the important thing in this particular construction. Great. So now let's play around. Let's go back to this fellow.
01:01:48.832 - 01:02:49.004, Speaker A: Maybe we'll keep this guy for a moment. Let's start to calculate what are the characters of the, of the, of the ring? Excuse me, the algebra, commutative algebra. Oh, did I say it? I don't think I did. I should say two things. We're just copying in a not particularly imaginative way a construction from commutative algebra, which was a construction of ries. And it's a very interesting construction. Whenever you have a ring and a bunch of ideals, j one, j two, j three, and so on, in that ring, which are nested, and they have the property that jk times jl fits inside of j to the sub k plus l.
01:02:49.004 - 01:03:47.692, Speaker A: Whenever you're in that situation, you can obviously build a counterpart of this thing. And it's rather complicated what this ring is and what its properties are in that level of generality. In our particular case, all of the ideals j, sub k are just the powers of a single ideal j, and that makes things a lot easier. But you can knock yourself out by studying this much more general situation where you have a commutative ring and a decreasing sequence of ideals, and then lots of interesting questions emerge. If you have a smooth manifold v, and inside of v you have a sub closed subset m, which is not necessarily a sub manifold event. Then you could look at the functions on v, which vanish on m, and you could look at the functions on v, which vanish on m to second order. That means the functions and all of their derivatives vanish on m and so on and so forth.
01:03:47.692 - 01:04:19.494, Speaker A: And now it's not so obvious what these ideals are in relation to one another, except the obvious fact that j, sub k times j sub l sits inside of j k l. So now it's a little bit interesting what you'd get in such a case. But when we're considering just this very regular case where m is a sub manifold. Good. Oh yeah, right. So back over here. So what we want to do is study characters on this algebra R.
01:04:19.494 - 01:05:07.454, Speaker A: And inside of the algebra R are polynomials, and in particular inside of r is the monomial t, just t. And if you have a character phi on r, of course it must assign a value to t. And you may say to yourself that value, because we're dealing with Laurent polynomials, that value has to be a non zero number. But it ain't. So, because we're not talking about all polynomials here, we're just talking about some Laurent polynomials. And in fact, you are allowed to assign the value zero to the monomial t, even though there's a one over t in this formula, because the one over t gets multiplied by something in j. And so all bets are off.
01:05:07.454 - 01:05:49.384, Speaker A: And so it's interesting to ask the following question. What is r divided by t times R? This principal ideal inside of R? T is not t is a unit in the ring of Laurent polynomials. So this is not a very interesting question there. But t is not a unit. It's not an invertible element of this ring R, because one over t has to be multiplied by an element of j. And the constant function one is not in the ideal j. So this thing has the potential to be non trivial, and we'll see that indeed it is non trivial.
01:05:49.384 - 01:06:37.710, Speaker A: And if you have a character of R which happens to vanish on t, then of course it's a character of this quotient ring and vice versa. So this quotient algebra and vice versa. So it's sort of interesting to know what is this quotient algebra? The characters of it will be some of the characters of R, namely those characters of R which have the property that they vanish on t, which is just one part of the characters of R. But let's just start with that. Well, okay, let's just think about it. I mean, let's just take the definition that's written here and multiply it by t. And then this is dangerous.
01:06:37.710 - 01:07:22.422, Speaker A: Let's try to shift the indices and not make a mistake. So we'll get sums like this. It's really like a sub k minus one times t to the k in terms of the notation over there. So the condition on a k is that it's in j, one minus k. Oh, yeah, yeah. Okay. The chances of me getting through this without, you know, making some catastrophic error like that are very small.
01:07:22.422 - 01:08:18.118, Speaker A: But maybe so far it's still okay. So, yes, good. It's going to work out. This is something that's a little bit hard to think about, but let's imagine for a moment, let's examine what happens when k is positive like k is three. If k is three, then a k has to be in j to the one minus three, which is j to the minus two, which is just a. So the coefficients here, as far as the k equals three term are, there's no constraint on them whatsoever. And this is what you're dividing out by in order to get the quotient.
01:08:18.118 - 01:09:11.903, Speaker A: So when you take the quotient here, there's going to be no, so to speak, contribution from the k equals three term of these series. If you have an element of R which is totally concentrated in degree three, then it's also in t times R. Let me say that again. If you have something of degree three for this grading in R, it's automatically in t times r. If you have something in two degree two, it's the same thing. In degree one, it's the same thing, degree zero, not the same thing, because this condition means that to be in t times r and in degree zero, the coefficient has to be in j. So as far as the k equals zero degree zero term is concerned, to be in t times r, you have to be in j.
01:09:11.903 - 01:09:43.483, Speaker A: The coefficient has to be in j. But to be in R, you just have to be in a. So the, in the quotient, the coefficient has to be in a mod j. If you go to k equals minus one, then what you'll end up with is j modulo j squared. What you'll end up with is the set of all sums. If you like, you could write them like this. Now, only involving negative powers of t and some coefficients here, which I'll call Ak.
01:09:43.483 - 01:10:12.402, Speaker A: And the condition is that ak is in j to the k. What do these square brackets mean? These, this means like we had it before somewhere. Yeah. It means an element of j to the k modulo, an element of j to the k plus one, modulo j to the k plus one. Yeah. Oh, just. Yes.
01:10:12.402 - 01:11:05.146, Speaker A: I mean, how about that? Yeah, sorry. Well, the t to the minus k's are neither here nor there. The multiplication on this is exactly the same as the multiplication in b. If you look at the obvious map to balance, which is, forget about the t's, because in the b we didn't have any t notation. That's an isomorphism of algebraic, so r modulo tr is b. The characters of r which vanish on t are exactly the characters of B, which is to say they're exactly normal vectors. Okay, that worked out.
01:11:05.146 - 01:12:28.516, Speaker A: Okay, so if you have a real character of, let's say an algebra homomorphism from R to the real numbers, it must assign to t some number mu, and if that number isn't zero, then it's non zero. And it now becomes interesting to study this quotient. If mu is not zero, I can take Laurent's series like this and just evaluated at mu. Oops, evaluated at mu. It's a finite Laurent polynomial we're talking about. I misspoke when I said Laurent series. And so this thing that I wrote down, the sum of mu to the k ak is just some element of a just like that.
01:12:28.516 - 01:13:31.086, Speaker A: Okay? And clearly, if you're, if you're in t minus mu times r, this thing is going to be zero. So this is factors through the quotient like that. Let's call this map, I don't know, evaluation at mu like that, I guess. Well, maybe I'm not even going to give it a name. Don't really need give it a name. Take it back. Suppose you have an element of r and it goes to zero.
01:13:31.086 - 01:14:06.226, Speaker A: Here. I'd like to say it comes from t minus mu. It is in t minus mu times r. Then this map induced from the quotient into a would be an isomorphism. Well, then, of course, here's our element. It's certainly equal to the following thing, which is subtracted off zero. And now I have t to the k minus mu.
01:14:06.226 - 01:15:11.644, Speaker A: At least if k is positive, you can factor out t minus mu by algebra. This is t minus mu times the summation t to the k minus mu to the k over t minus mu. Okay. And now you have to just think about the other cases. First of all, if k is positive, what I mean by this is the usual long division. This is supposed to be something with the property that when you multiply it by t minus mu, something which is some kind of polynomial with a property that it is supposed to be a polynomial, with the property that when you multiply by t minus mu, you get p to the k minus mu to the k. And this thing is of course just a polynomial in the t of degree k minus one.
01:15:11.644 - 01:15:46.764, Speaker A: What happens if k is equal to zero? Well, t to the zero minus mu to the zero. It's like one minus one. That's just zero here. We're just talking about. This is just notation. Well, zero. And what if k is negative? Well, if k is negative, I ought to think of t to the k minus mu to the k as a polynomial in one over t instead of t.
01:15:46.764 - 01:17:11.792, Speaker A: And if you think about it, what's going to emerge from this is that this thing, there is such a thing as this which exists, and it's a polynomial in one over t of degree. Okay, imagine taking t to the k minus, sorry, minus k. Imagine taking t to the k minus mu to the k and factoring out a t to the k on one side and a mu to the k on the other side. Maybe I'll just do that, something like that, over t. And now you do the math, and you're supposed to get polynomial degree minus k. And what it means is that this thing is actually in r. You have to do, you have to check, you know, is it going to be an r? Is it not going to be an r? It depends on what the degrees of this thing are.
01:17:11.792 - 01:18:27.054, Speaker A: But in this situation, everything is fine, really is in r. So if back to our little diagram, we have an evaluation map, which is pretty easy to understand, and the kernel of this evaluation map is precisely t minus mu times r. Nothing very complicated. So the quotient is just a. Okay, slower than I expected, but that's always the case. So the character spectrum now breaks into two pieces. One piece is the set of all characters which evaluate to zero on t, and that's a copy of the normal bundle.
01:18:27.054 - 01:19:20.328, Speaker A: And then there are all of the pieces of the character spectrum where the character evaluates to mu for some non zero number, mu on t. If the character evaluates to a non zero number on t, then the character is, in effect, a character of a, which is to say, an element of v. And that happens each time you have a non zero node. The non zero number tells you what value you assigned to t, and the element of v tells you what to do with the coefficients, how to go from a coefficient to a number by evaluating at a point. All right, so now it looks familiar. It looks like the sort of thing we were studying before. It's a vector bundle.
01:19:20.328 - 01:20:37.774, Speaker A: T is equal to zero, so to speak, and away from zero, it's just some manifold. And the theorem, I guess we're going to doll it up. Let's just call it the lemma, like we did all along. And the other thing I should have done, which I'll do now is say that this is something that was worked out by my former student Ahmad Reza, whose name is very long like that. So this thing is a new, I mean, for this class, a new thing. But we know what it is as a set. What are going at r? It's actually a smooth manifold.
01:20:37.774 - 01:21:59.342, Speaker A: So first of all, I'm saying it's a certain topological space which is locally homeomorphic to rn. That's the first thing. And now I'm going to tell you what is the sheaf of smooth functions. And now it's going to look like what we had with polynomials, what we had in this world of normal bundles and so on. So it's functions that are locally of the form f equals some, which we call them f g h one up to hn like that. Need a bit more space somewhere. Let's put this somewhere else.
01:21:59.342 - 01:23:48.878, Speaker A: Just over here. Take two. Which way did I do it? G, exactly as before. Now I just have to tell you what the H's are, which probably should still be within the curly braces. Well, so these are supposed to be elements of r. So if you have an element of r, it becomes a function on the character spectrum by evaluation. And the relevant functions are, well, the coordinate function t coordinates on v.
01:23:48.878 - 01:24:48.314, Speaker A: And then there's one more thing you can do, which is what we were talking about just at the very end of the last lecture, namely, you can look at this thing here, t to the minus one times y one y, where y they'll call it vanishes on m. Maybe I should call it f g h k. You know, I'm just going to change this. Let's, instead of talking about coordinate functions, let's just take an a here, which is in c infinity of b. And here I'll take a b. Same jack. So in particular, you can find coordinates for the manifold just from these functions here.
01:24:48.314 - 01:26:06.454, Speaker A: And finally, two remarks. First of all, definition, the character spectrum here of rich is the. Well, this isn't great notation in the world of c infinity manifolds, but this is called the defamation to the normal cone, or maybe just deformation space for the inclusion of m into v. It's this space here with a certain topology about which we can say a little more. I don't know, maybe Dan. Oh, I need to make one announcement. Maybe dan will say some more on Tuesday.
01:26:06.454 - 01:27:28.260, Speaker A: It carries a scaling action of the positive reals, which comes from the fact that, as was observed, r is actually a grape by the Internet. R is graded algebra. And what it is in terms of these coefficients is lambda times a normal vector is just lambda times normal vector. What else is in this space? Well, what remains in this space are pairs consisting of elements in the manifold v and a non zero number t. And now this is just v lambda inverse t. T is not zero, just like we saw in the tangent groupoid situation. Why is it lambda inverse? Well, it's right somewhere.
01:27:28.260 - 01:28:08.424, Speaker A: We made a strategic decision. Oh, here it is. To get their signs right, we changed a plus k to a minus k right here. And that's where the lambda inverse comes from. Okay. And the last thing that needs to be said somewhere, maybe right here. Whenever you study the geometry of sub manifolds in a manifold, there's a fundamental case, and that's always the case of a manifold embedded into its square.
01:28:08.424 - 01:29:06.704, Speaker A: I guess we should give this thing a name. I don't know. The big n, the normal space in v associated to m, something like that. I think that's the notation we used. This is our friend, the tangent group board. Mostly it's like you see up there, mostly as M times M times R cross, just like the tangent group load. Exceptionally, it's the normal bundle of the diagonal embedding of M into M times N.
01:29:06.704 - 01:30:21.040, Speaker A: And now you have to make a choice. There's a little strategic choice here. But the normal bundle in this situation, n has to be identified with t of M via the first factor projection or inclusion, depending on which way you want to go. Once you do that, the tangent, the deformation space becomes t of m disjoint union, m times n times R cross. And now it carries a certain smooth structure, the one we've just been discussing. And you might ask whether that smooth structure agrees with the one that we discussed in the last lecture. Answer, yes, it does.
01:30:21.040 - 01:30:47.866, Speaker A: If you make this identification, you don't make this identification. You're off by the minus map on tangent vectors. That sends x to minus x. So you have to do that tiny little thing. So here's another algebraical way of understanding the tangent groupoid out of time. But it's possible to, there's extra structure in the world of the tangent groupoid. There are the source and the range map.
01:30:47.866 - 01:31:48.682, Speaker A: In the tangent group, there's a composition. Or it's possible to recover all of those from the fact that this tangent, this construction we were just describing, this one here. Maybe I'll say one more thing in this tiny box to constrain myself not to go on too long. This thing is a functor. From the category of C infinity manifolds with sub manifolds to the category of manifolds equipped with the submersion to the real numbers call those manifolds over R. And the functoriality of this construction in a way that I'll add to the notes allows you to reconstruct the source map on the range map and the composition law just using functoriality. All right, so now, important announcement.
01:31:48.682 - 01:32:19.866, Speaker A: I won't be here on Tuesday, but we shall continue nothing, neither snow nor other things. We'll stop the class from going on. I mean, until I bugger off on holiday. But for now, we shall continue. And Dan will give a lecture on Tuesday related to this defamation space and a wonderful theorem of echoed on tubular neighborhoods. So. And, friends, excuse me, on tubular neighborhoods.
01:32:19.866 - 01:32:31.084, Speaker A: So there will be a class on Tuesday just without me. I might tune in. Actually, now that I think about it, I have to check and see if I'm able to do that. Great. Thanks very much.
