00:00:00.200 - 00:00:09.638, Speaker A: Pleasure to introduce Andreas Hartmann for his second lecture of his mini course on depilation and sentinel.
00:00:09.766 - 00:01:01.004, Speaker B: Thank you. Okay, thank you for the introduction and thank you for coming back to this talk. I don't know what's happening now. Okay, so, as I explained yesterday in the second part of the lecture, I will talk about a very classical situation. So, interpolation in the hard space h two, and I will would like to give the proof of the interpolation result. I think I will prove everything except Carlson's embedding theorem. And I feel that this is a quite representative situation for, for interpolation in a large class of spaces.
00:01:01.004 - 00:01:56.434, Speaker B: Okay, here are some reminders, maybe for from yesterday. When studying interpolation, we need to consider by which class of function or which other functions we want to interpolate with. And then we also have to ask the question, what are the values we would like to interpolate? And so there were two situations where the answers were quite natural. It was when we look at polymorphic functions, which are uniformly bounded. So this is the choice of the class of functions you want to use to interpolate. And then the traces, just the bounded sequences. There's still people arriving here.
00:01:56.434 - 00:03:10.534, Speaker B: But it's good because I'm just repeating things from yesterday and for p two in particular, when we have a reproducing kernel space. And again, that's the main situation, which is interesting for interpolation, because when you interpolate, you need to give a sense to values and points, and the value and points make sense. If point evaluation is continuous. It's a way of considering the problem. I recall the definition for interpolating sequences. So once we have fixed reasonable sequence space little l, we will say that sequence number of points in the domain omega in which the space x is defined, the trace of the space is equal to this sequence space. Another way of expressing this is to say that the restriction operator, so, which associates to each function f, the sequence of values f lambda n is into little l and onto little l.
00:03:10.534 - 00:04:12.064, Speaker B: And so yesterday we saw one half of the proof of this theorem. So if you have a reproducing Colonel Hilbert space, then sequence is interpolating if and only if. The associate sequence of normalized reproducing kernels is so called tree sequence. And I remind that, okay, there's a definition for resequences, but I remind that tree sequences are, for example, isomorphic images of orthonormal bases or sequences. It doesn't really play a role. And also they are equivalent to unconditional bases, which means that you can summon whatever order you want. Okay, so let me go back to the Hardy space.
00:04:12.064 - 00:05:03.556, Speaker B: So, yesterday we discussed h infinity, so the space of uniformly bounded analytic functions. But today I would like to focus more on the Hilbert situation. So, Navy space h two. There are different ways of defining the space. Either you can consider it from Taylor series, okay, since these are holomorphic in the disk, we can represent it in the Taylor series, which has convergence radius, at least one. And in particular, if an is l two summable, then we define the Holly space h two. It's known that spaces and functions in the high space emit non torrential boundary values everywhere.
00:05:03.556 - 00:05:46.570, Speaker B: And so you can give a sense to these functions on t the circle, which is the boundary of the unit disk. And then you can consider free series. And it turns out that h two is exactly the space of those functions defined on the circle, for which the negative Fourier coefficients are vanishing. And then there's, of course, also this definition based on dilating radii. So if this supremum is bounded, then the function is in the hard space. But I won't really use that. The Hardy space.
00:05:46.570 - 00:06:21.604, Speaker B: We have two useful tools. The first is factorization. So there was a course, I think, on Hardy spaces. So this is rather a reminder for those who have followed the course on hardy spaces. Maybe there are still people who are not acquainted very much with that. So these are some reminders, which, of course, will not replace, of course, on Audi spaces. So, factorization means that whenever you take a function in the hardy space, be it hp h two h infinity.
00:06:21.604 - 00:07:17.134, Speaker B: In fact, this result is even true in a very large class, which is called the Nevanina class. I want to talk about this. Then you can write the function f as a product of a so called inner function, which means I is uniformly bounded by one in the unit disk, and it takes boundary values of modulus one almost everywhere. As I told earlier, function the hardest phase. They admit non tangential boundary values almost everywhere. And so f will be the zero free part, but in a more general setting, so it's zero free inside the disk and also somehow on the circle. And so this means, for example, you have no function which can go very fast to zero close to the boundary.
00:07:17.134 - 00:08:01.574, Speaker B: Okay, what's interesting, what will be interesting for us later, is that so I is in inner function has boundary values of modulus one almost everywhere. And then this implies immediately that little f and big f has the same hp norm, which you can see from the formula, which is given here. Okay, this we will need in a second. And there's also the reproducing kernel. I've already talked about this yesterday. This is just the sega kernel. And so this is more or less the Cauchy formula, which explains why f integrated against k, lambda, the value of lambda.
00:08:01.574 - 00:08:38.120, Speaker B: Okay, this should be a lambda here. Excuse me, the z here is of course, a lambda. And also, as I said yesterday, I can compute its norm, which is given by this formula. Inner functions, they can be categorized in two parts. There are blaschke products which take. Which take the zeros inside the disk, and singular inner functions which take care of what can happen on the circle. Okay, a blushker product.
00:08:38.120 - 00:09:18.254, Speaker B: So it's a function which vanishes in points inside the disk. These points, I call them lambda. And in order that this works, they have to satisfy this blush condition. So the distance to the boundary have to be summable. And then this elementary mobius vector, which you re normalize to make them positive at zero, well, they are multipliable at infinity. So you get convergence of the corresponding blush product. A singular nf function is more or less constructed like an outer function.
00:09:18.254 - 00:10:04.424, Speaker B: Instead of putting here the logarithm of the modulus of f dt, you take a singular measure. Okay, I take the logarithm of the modulus f here. In a sense, I can write f as an exponential of this poisson extension. But if I look also at the arguments, then I would get the haglot scarlet, which I see appear in here. Okay. And it's possible to check that a single line function also has boundary values of modulus one almost everywhere on the circuit. Okay, these are just some little reminders, reminders on harder spaces.
00:10:04.424 - 00:11:06.834, Speaker B: Okay, so let us look at necessary condition for interpolation. And here I will repeat an argument from yesterday and say repeating things. I think someone has his mic open. So if I take a banner space, the result is quite challenging. And this is again a consequence of the open mapping theorem, as I explained yesterday. So if my restriction operator is into and onto this little l, which is a van of space, then by the open mapping theorem, I know that I can solve the interpolation problem in a stable way, which means that I can control the norm of the interpolating function by the norm of the trace. Okay, this is quite reasonable.
00:11:06.834 - 00:11:54.684, Speaker B: And now I will do the argument from yesterday. Now, in h two, I could do it in a more general case, but for the moment, I prefer sticking to this situation. So for h two, we agreed that the right trace is the little l two. With the weight given by the lucrative kernel and the replacing kernel squared one over is one minus lambda. Script. So, as yesterday I picked the sequence in this little l, which is a weight of l two space which takes zero in all the points different from lambda n. And in lambda n, I take the value one over square root of one minus lambda n squared.
00:11:54.684 - 00:12:45.844, Speaker B: Okay, all the points are zero except in lambda n, where it's exactly k lambda. Okay, the norm of k lambda. And if I compute the norm in this little l sequence space, then of course I get that this norm has to be one because it takes just one value and which is one over the weight. Okay? So then of course I can interpolate this sequence and by proposition I can interpolate in a stable way. And so the argument is exactly the same as yesterday, except that I have to introduce the weight factor fn will now be equal fn of lambda k v kn. And this is exactly the delta nk. So this sequence and f is bounded by constant.
00:12:45.844 - 00:13:34.148, Speaker B: Now by factorization, the function fn contains a factor bn. So this is a Blaschke product which vanishes in all the points of the sequence lambda except lambda n and some function fn. And now this is not necessarily outer. So fn could split into an inner function times an outer function. And I know that the inner function, since it vanishes on lambda without lambda n. Well, this contains a factor vn and the rest of the singular function of the inner function. Excuse me, I will just put it into the capital fn, which doesn't change the norm of capital fn.
00:13:34.148 - 00:14:24.924, Speaker B: It's still of the same norm as well fn, because they're only in a function simple. And this, now I can run the argument as yesterday, I get the weight. The weight is the value of fn fn. I can write it as this product now, f lambda n. I can for example, write this f lambda n as a scalar product of f, okay, this as yesterday, I can write this as f against k lambda n, okay, the red, okay. And then by Cauchysvat I can bound this by the norm of f times the norm of kn. And the norm of kn is one over square root, one minus lambda n.
00:14:24.924 - 00:15:42.656, Speaker B: And at the end of the day, I can cancel out this factor. The norm of f is controlled by c, okay? And so dividing through this c, I get again this, I get again this CArlson condition, which tells us that the plasch products which vanishes on lambda without lambda n has to be bounded uniformly bounded from below by constant. So the aim of what follows will be to analyze this condition further. Okay, so, well, first, I don't know if this deserves to be mentioned as a theorem, but, okay, it's a result. So the proof is essentially the same as for h infinity. The proof works the same way in Hp. So yesterday, Javad, he said, what about the other cases? P? And he's right, asking this question.
00:15:42.656 - 00:16:29.434, Speaker B: So if you have a reproducing turnspace, often you have p versions, and then you can copy some of the techniques from the heaven version case to the more general p case, p in general, between strictly between one and infinity, because there can be some things happening for P. Okay, it's called the Carlson condition, which we will discuss in a moment. And I would like to again to mention these two results. This Carlson condition implies that you can solve a weak interpolation problem in h infinity. If you have the Carlson condition, then you can solve, then you can interpolate sequences which are zero everywhere except one point where one. So you can solve zero one problem. And here is the explicit solution.
00:16:29.434 - 00:17:27.864, Speaker B: And in h two, you can also solve zero one, quote unquote problem. The difference is now that we have to introduce the weight, its nature, because functions in h two, then they can grow, and that growth is controlled by the norm of the reducing kernel. And so you can solve also this zero one problem where one is replaced by one over the norm of the regression kernel, one is replaced by the norm of the regression kernel. Excuse me. And here we can also give explicitly the solution which is given by this function. Okay, bn here will guarantee that the function vanishes in all the points except lambda n, and in the point lambda n. Well, the first factor will be one, and this will produce exactly also what was mentioned yesterday.
00:17:27.864 - 00:18:23.084, Speaker B: We had this b orthogonal family. And so you can check that phi n, this function is in fact b orthogonal to the sequence of normalized ribbon kernels. Since the b orthogonal exists, so automatically the sequence of reproducing terms, this normalized sequence, has to be minimal. And it can be checked that it is uniformly minimal. But because the norm of phi n, well, this is essentially given by one over v and lambda n, which is controlled by the Carlson condition. Okay, so the Carlson condition guarantees that your sequence is uniformly minimal. Okay, so I said that, okay, we have seen that the Carlson condition is necessary.
00:18:23.084 - 00:19:18.174, Speaker B: We have also seen that we can solve a weak interpolation problem. And as I just explained, this implies that under the Carlson condition, the which is, I remind that this is the sequence of normalized reproducing kernels. So this is a uniformly minimal sequence. All right. And we'll see a little bit later on, in a sense. Now, one question is, does uniform minimality already imply re spaces, miss, or resequence of t? And I will come back to this question later. Okay? So if you want to show that it's a respasis or equivalency, that it's interpolating, then we have to solve two problems.
00:19:18.174 - 00:20:23.944, Speaker B: So the first one is, we have to show that the restriction operator is well defined. It goes into the sequence space. And it, if I define this measure here, which is just weighted combination of the direct measures associated with the points, then this is exactly the Carlson embedding. So we have to show, does the CARLSON condition imply this embedding result? And the second condition is, does the CARLSON condition imply the interpolation properties? So, do we have the reverse embedding? Is the restriction operator also onto. Okay, so let us discuss some geometric, geometric characterization of interpolation. And there's an easy consequence, which is separation. And here I have a picture that I have, I should have put already yesterday on my slides.
00:20:23.944 - 00:21:17.106, Speaker B: I draw them a little bit in a bad fashion, but okay, it's maybe a little nicer. So let's suppose that lambda satisfies the Carlson condition. NOW, I WANT TO COMPUTE THE DISTANCE BETWEEN lamBdA N aND lamBdA K. AND AS WE HAVE SEEN YESTERDAY, THIS METRIC Is DEFINED AS THE MODULUS OF THE MOBIUS TRANSFORM IN lambdA N WITH ReSPECT TO lambdA N in lambdA K. BUT THIS Is ALSO AN ELEMENTARY FACTOR OF THE BLASCHKE PRODUCT. And now, if I multiply much more blaschke elementary factors, then I will make this smaller because they are all of modulus less than one. Okay? So I make the product smaller.
00:21:17.106 - 00:22:00.560, Speaker B: If I multiply with other factors, which are smaller than one. And at the end of the day, I will exactly obtain the plaschko product vanishing on lambda without lambda k. And I compute this in lambda K. And according to my Carlasson condition, this is bounded from below by a constant, which is bounded away from zero. This means my sequence is separated in the pseudo hyperbolic metric. So hall defines pseudohyperbolic metric. And in particular, this means, if I take pseudo hyperbolic discs, which are defined like euclidean disks, but I replace the euclidean metric by the pseudohyperbolic metric, they have to be disjoint.
00:22:00.560 - 00:22:28.398, Speaker B: Pseudo hyperbolic metric is a metric. It satisfies, in particular, the triangular inequality. And so, at the end of the day, we see that, in fact, these neighborhoods will separate. And as I said, mentioned yesterday. And it's on this. Nice picture. If I fix the radius of this solid hyperbolic metric neighborhood, then this is like a euclidean disk.
00:22:28.398 - 00:22:53.174, Speaker B: It's not centered in lambda K. It's a little bit deep, placed with respect to the center. But still, it's a euclidean disk, and its radius is comparable to the distance to the boundary. So all these discs have the same pseudohyperbolic radius. This has to be clear. So they get smaller and smaller when you get to the boundary. Okay? So Carlson condition implies separation.
00:22:53.174 - 00:23:54.034, Speaker B: And now let me speak about the Carlson embedding condition. So let me do this computation here. So, by the Carlson condition, we have just seen that the lambda is separated in the soto hyperbolic distance. Now, I will pass through logarithms, and so these b lambda k, lambda n, they are bounded from below by delta. Okay? Now, if I call this x, then I remind you, this elementary estimate from first year or second year courses depends on where you are. So, ln one over x, of course, they can write it like this. And when x is strictly bounded away from zero, so between delta and one, so I get universal constants which depend on delta.
00:23:54.034 - 00:24:39.132, Speaker B: So that lambda logarithm of one over x is bounded by constant times one minus x from above. And by from below, it's also bounded by another constant times one minus x. Okay? So this is just another way of writing one over the Lascaux product, okay? And one of the Blaschke product is bounded by one over delta. This is the Carlson condition. And now I put allen in front, and I push the allen through the product, which turns it into the sum. And so I have to estimate allen of one over b lambda k lambda n. And then maybe I would like to replace the minus one by minus two.
00:24:39.132 - 00:25:22.764, Speaker B: And I take one half in front. Okay? And then this whole thing, I will call it x, or this is one over x, in fact. And according to the estimate which I have above, I can bound ln of one over x by one minus x. The constant times one minus x. So this inequality with the twiddle means there's a constant. And one half just comes because I have put a square here. All right? So this is not very difficult.
00:25:22.764 - 00:26:29.744, Speaker B: Then we have a well known identity, which is direct forward computation, that one minus this blasphem squared is exactly one minus lambda squared, one minus mu squared over this denominator. And so at the end of the day, so if I plug in this expression here, I've shown that this sum is uniformly bounded in n. Okay? And the soup of this, I will call it n. Sometimes this is called the invariant BlasCHKA condition, because it tells us that the sum of these terms is uniformly bounded. And the usual blaschka condition is that instead of the b lambda k lambda n, you just take b of just lambda n. Okay? And if this is uniformly bounded independent on n, so this is called the invariant blasphemy condition. Another condition is this implies, quite immediately, that, if I recall, this measure mu lambda.
00:26:29.744 - 00:27:10.374, Speaker B: So which is this combination of the direct measures. Okay. Then I can compute the norm of the reproducing kernel with respect to this measure in the l two space of this measure. And by definition, this is exactly this sum. And, well, we recognize this sum. So this is bounded by n. So what we have just shown, and those who were awake until the very end of the conference yesterday, saw this riparian carnal test or thesis or hypothesis appearing also yesterday.
00:27:10.374 - 00:27:49.776, Speaker B: So, in fact, we have just shown that the carluccin condition, this condition, circumflex, implies so that the sum is uniformly bounded. Another way of stating this is that you have a reverse income test in this l two space. So, in a sense, we have a partial embedding. So we want that h two embeds into l two mu completely. But for the moment, we know only that this is true for the normalized reproduction currents, therefore, the rubrics and currents. Okay, so let me go further. Okay.
00:27:49.776 - 00:28:28.364, Speaker B: I have to erase. I will introduce this definition. A positive Borrell measure is a Carlson measure. If h embeds into this l two mu. And so this is characterized by a quite elementary condition. And this is a result by Carlson, which I will cite and which I will not really prove. This is the only theorem today which maybe I will not prove.
00:28:28.364 - 00:28:57.404, Speaker B: And the characterization is given by these Carlson boxes. So if you have an arc on the circle. So then you take kind of square. It's not really a square. It's a part of an annulus which has width, which is exactly the length of the interval. Okay. And then, okay, you will call this a Carlson window and the Carlson measure condition.
00:28:57.404 - 00:30:06.374, Speaker B: So it's Carlson measure condition is this geometric condition which tells you that you measure mu should not put value. You should be able to control the mass that you put onto the Carlson box Carlson window, and it's controlled by the length of the interval. And the famous result by Carlson tells us that under this condition, okay, we have the embedding condition, which means that the sequence, the measure mu, is indeed a Carlasson measure. Okay, Carlson measures in h two are characterized by this geometric condition here. Okay? And this is the one which we use. Just a little remark. This theorem, you can prove it, for example, with, you can show that the redressing kernel test, in a sense, implies more or less this using kind of sure test.
00:30:06.374 - 00:31:04.310, Speaker B: Okay? So I recall that the calisthen condition implied this invariant flush recondition just gives also the embedding for rippers and kernels. And now I pick an arc in the circle. And I want to prove that with this condition, I get the geometric CArlson measure condition. So I have to prove that my measure mu, which is measure mu lambda. Okay, it's, I've maybe defined it here. Okay, the measure mu lambda, which I defined here. I want to prove that with this, Carlos, in the condition I can show that mu lambda is the Carlson measure.
00:31:04.310 - 00:31:40.338, Speaker B: So I have to show, I have to prove the Carlson measure condition. So pick I an arbitrary arc and let s I be the associated Carlson box. And then I will consider the upper half of this Carson box, which I will call tr. It's the top half of this box I can show. And I will explain in a minute why I can make this assumption. It's in fact enough to check it for those boxes which contain in the upper half a point of the sequence. So I have a point somewhere here.
00:31:40.338 - 00:32:38.254, Speaker B: Okay, this is my lambda n. Okay, if my point lambda n is here, then of course, the distance to the boundary of this point is compared to the length, because it's at most the lengths of I, and it's at least half of the lengths. Okay? So the first condition is clear. The next condition, this argument condition is also clear, because lambda n and lambda k, they are in the box. And so if I take lambda n bar and I look at the arguments, the difference of the arguments is at most I. So the second inequality here is clear also, maybe even without the twiddle here, the last condition, one minus lambda n bar lambda k. Well, I can do it in two steps.
00:32:38.254 - 00:33:15.788, Speaker B: The worst case is maybe when lambda k is one the modulus, then, well, it's just one minus lambda n. And the other extreme situation is when lambda k is on this boundary, but then it's lambda k is one minus I more or less. And if I multiply all the things here that I obtain, I can see that it's bounded from below also by this constant. So this equivalence is easy to prove. Okay, then I can show. Okay, and maybe I should try to do a drawing here. Okay.
00:33:15.788 - 00:33:46.764, Speaker B: And if I take a .1 here, now, I look, I look with my microscope. So I zoom inside so my circle is almost flat. And if I wanted to compute the distance from here to a point, let's call it z. Okay, I can compute it also. Computing first, this distance and this distance. It's essentially Pythagoras theorem.
00:33:46.764 - 00:34:27.043, Speaker B: But since I take only two directions, all norms are comparable. So the one norm is also comparable. And this tells me that if this is z, okay, then one minus z, this distance is compared to one minus this distance, which is exactly the argument. And plus this distance, which is one minus the modulus of z. And all pieces are now comparable, more or less to one minus lambda. So I get this condition. Okay, this is quite easy computation as a conclusion.
00:34:27.043 - 00:35:07.696, Speaker B: Now, I take. I want to compute the mu lambda. So mu lambda is my. I don't write it here because I won't explain it all the details here, but I have to compute mu I of the Carlson box, si. And this is exactly the sum of all lambda k and si, one minus lambda k. Now, one minus lambda n is compared to one minus lambda n bar lambda k, so I can divide. And this is compared to constant.
00:35:07.696 - 00:35:23.122, Speaker B: And I raise it to a square. It's still comparable constant. And in fact, this all, this is all comparable to a constant. Okay. This is a constant. Okay. So I didn't change much the sum, the first sum.
00:35:23.122 - 00:35:51.836, Speaker B: I just introduced a multiplicative constant. But now I recognize that the sum is bounded by m, and this is just one minus lambda n. And one minus lambda n is comparable to the lengths of I. So I'm done. And I have proved that the CArlson condition. So this condition on the plasket products implies the calcium measure condition. And in case this is not, if you don't have anything in the upper half.
00:35:51.836 - 00:36:51.218, Speaker B: Okay, well, then I get down in dyadic way and I do the argument as soon as I meet someone, okay, maybe this one and this one, and on, and so on. And I run the argument on all these pieces, okay? And at the end of the day, each one is controlled by the length of this, the length of this, the lengths of this, and all these lengths add up to something, which is, at most, the lengths of the whole thing. It's a kind of generation argument, but it's very easy. Okay, so we have shown that the Carlton condition on the Blaschke product implies separation and the Carlson measure condition. All right, let's move on because we want to see more. Okay, so we have shown that the condition c on the blaschke products implies the Carl's measure condition. Okay? Here, the measure was mu lambda.
00:36:51.218 - 00:37:27.004, Speaker B: And by Carlson's theorem, this implies that mu lambda is a Carlson measure. So I have this embedding. Okay? So this gives an answer to the first question. Okay, this is just a remark. So, mular, it's in general, if I don't have the separation condition, if I define if I have a lascar sequence, and I associate such a measure, then this defines the carbon measure. If and only if lambda is a finite union of these interpolating sequences. You can also extend it to measure supported on the circle.
00:37:27.004 - 00:38:07.090, Speaker B: I will not go into details here. And there's also notion of reverse Carlson measures, which is useful for sampling sequences, but sampling problems. But I don't want to insist on this now. All right? So, for the moment, what we have proved is that the Carlson condition implies this embedding, okay? And this uses Carlson's theorem. So we have the Carlson measure property. And what we want to prove now is interpolation properties. So if I take a sequence from the sequence space, then I am in this.
00:38:07.090 - 00:38:34.534, Speaker B: I'm a trace of h two. Okay? So, as I just said, preceding discussion gives a positive answer to the first question. So, let's turn to the second question. So now I take a sequence bk, which is in the sequence space. So it has to be summable with respect to this. So Marcin wants to indicate something here. Marcin.
00:38:34.534 - 00:39:51.942, Speaker B: Oh, you have to maybe deactivate your annotation tool, except if you want to maybe make some remark. Okay, so we have already met this function phi n, which is the b orthogonal sequence to the normalized reproducing kernels. And we have seen that it solves the weak interpolation problem. Okay? So we have this condition, and now we define an interpolation operator, at least for finite sequences. Okay? When vn is finite, which means it's zero except on finitely many points, then I will introduce this interpolation operator. So, I have to introduce this additional factor here, because phi n, if I evaluate it only against the reproducing kernel, then, well, I get this factor, which comes to the other side. I have to compensate it, okay? But, well, if I write it out, then you get exactly vn times this function.
00:39:51.942 - 00:40:48.540, Speaker B: And this is, again, like we have seen in the Lagrange interpolation formula, okay? This is something which satisfies delta and k. If z is lambda lambda k. Okay? Z is lambda k. Okay, so this is, again the same idea. And this implies, quite immediately, that if v is finite, then I have the interpolation property. And of course, it's in the h two space, because phi n, this function is, of course, in h two, and a finite combination still in h two. All right? But now we want to look if, if it converges, and this is quite nice, because we use, again, the embedding property.
00:40:48.540 - 00:41:57.040, Speaker B: And how do we do this? We have to compute the norm of we. So let's compute the norm of v, and we will do it by duality. This is maybe an idea, which goes back to Shapiro, who, while I haven't been very rigorous for the history, so Carlson proved the interpolation result in h infinity. And it was in the beginning of the sixties, and maybe two years later, and maybe two years later, Shapiro and Scheel, they proved the result in hp, and they used an argument by duality, and it's not exactly the same what I'm doing now, but I think the philosophy behind is maybe the same. Okay, so let's compute it by a duality. And so I integrate, take the scalar product of this interpolation operator against h, and h has to be bound in normal. Okay? So then, of course, I can pull out the sum.
00:41:57.040 - 00:42:28.648, Speaker B: I can pull out the values vn, I can pull out one minus lambda n squared. I can pull out one over b, and lambda n. I can pull all this out of the scalar product. And then I can use triangular in quality. And I know that one over b and lambda n is controlled by one over delta. This is the Carlson condition we had. Okay, and what is remaining after this application of the triangular inequality? So this factor is here, vn is here, the one over vn, lambda n is bounded by this.
00:42:28.648 - 00:43:14.010, Speaker B: And all what remains in the scalar product is this expression. Now, I use kosischwa, and this produces, luckily, the norm of Vienna in the weighted space, and it produces this expression here, which we have now to analyze. Okay, so let's analyze this expression. And in order to analyze this, so we have to compute this bn column, then h. So I have reversed the order. It's not very important, because we take moduli of this. I bring over the bn.
00:43:14.010 - 00:43:41.674, Speaker B: It gets a bar sign here. Bn is a Blaschke product, which contains all blaschke factor except b lambda n. I multiply by b lambda n. So this will give me b, the entire ab. And on the other side, I also get b lambda n. So recall that p lambda n is a Blaschkov factor. It's of modulus one on the circle, so it doesn't affect, it's an isometry on the Hardy space.
00:43:41.674 - 00:44:16.914, Speaker B: Okay, so I've done this operation. I'm cheating here a little bit, because I haven't taken to account some unimodular constant modulus of lambda over lambda or something like that. Okay? It's not a problem here. B lambda is modulus one on the circle. So if I take the conjugation, it's like dividing by it. And if I divide by b lambda n, recall that b lambda is lambda minus z over one minus lambda z. So it's essentially lambda minus z times the reproducing kernel.
00:44:16.914 - 00:44:56.156, Speaker B: So it absorbs this reproducing kernel, and all what is remaining is this kernel. So it's Cauchy kernel. Okay? And then I multiply all this by z on both sides. So on the left side, I get z. Again, z is of modulus one on the circle, so it's an isometry, and it's no problem. And instead of multiplying z on the right, I multiply the denominator by z bar, and then I get, oh, again, the ripple is in kernel. And at the end of the day, what I see is that this function is integrated against the reproducing kernel or minus the reproducing kernel.
00:44:56.156 - 00:45:39.752, Speaker B: And this is exactly a projection of this function onto a point. Okay, now we are in business because we can run our argument again. I recall we were computing the norm of the interpolation operator. Then by Kosischwarz, we have this estimate. Now, according to what we just computed upstairs, this expression, I can replace it by this expression. And what is nice now is that here, this function doesn't depend on n. Okay? It's just an h two function, because bzh bar, this is an l two of the circle.
00:45:39.752 - 00:46:10.724, Speaker B: And then I project into h two. And so this gets is an h two function. P plus is the risk projection. Okay? So I evaluate this function in lambda n. And so that at the end of the day, I get the supreme. Okay, I can pull out the supreme here of the sum. Okay, because the sum doesn't depend on the supreme, I can pull in the sum.
00:46:10.724 - 00:46:55.034, Speaker B: And so at the end of the day, by the Carlson embedding, I know that this function, well, I can sum it to the square against this weight, because I have the Carlson embedding. And so I am done, because this is controlled by the norm of h, which is one, and this is it. Okay, so this shows that the interpolation operator. Okay, I was starting from a finite sequence, and for all finite sequences, I get this estimate. But then, of course, since finite sequences are dense in this weighted l two. I can do it for arbitrary sequences. And this operator is founded.
00:46:55.034 - 00:47:34.326, Speaker B: All right, so the previous case is quite instructive. So we have seen that phi n. Okay, we have seen now that the Carlson condition implies interpolation by the theorem from yesterday. This implies that the normalized ruptures and cars are resequence. And so also the phi n will be re sequenced. In fact, they are basis in the corresponding model space. And now it's clear that this operator is bounded.
00:47:34.326 - 00:48:50.554, Speaker B: And it's exactly the operator. It's exactly the interpolation operator. But now, from an abstract point of view, if I knew from another way that fine is a respasis, I could have stated immediately that the interpolation operator should be that, okay, so if lambda is interpolating, we have seen, this implies the Carlson condition. The Carlson condition implies separation in the Carlson measure condition. And we have seen in this setting that the Carlson measure condition or the Carlson embedding was a central piece to prove also the convergence of the interpolation operator. So these objects are not disconnected. And more generally, we can ask, if you have separation and CArlsOn embedding, does this imply that you have interpolation in the reproducing Kahn Hilbert space? And I want just to indicate this result by Alaman, Hart, McCarthy and Trishta, which is quite recent, in which they gave a positive answer to this question, provided that the space satisfies the complete peak property.
00:48:50.554 - 00:49:28.598, Speaker B: There were partial results earlier by, for example, Bohr, et cetera. This is a summary result, maybe just to recall all what we have done until now. So, the interpolating sequences in h two, they are characterized by this analytic condition. It's on functions. We have a geometry condition, which is separation. And Karl's measure condition, you have this condition, separation, and this invariant Blaschke condition. It's equivalent to separation.
00:49:28.598 - 00:50:00.544, Speaker B: And this reproducing kernel hypothetus, it's equivalent to this sequence to be resequence. And of course, then that the b orthogonal sequence is resequence. It's equivalent to this. And this, I haven't shown it. Really? This, I haven't shown it. And you can show also that it works for arbitrary p. And the interpolation operator is quite closely related to that.
00:50:00.544 - 00:50:33.954, Speaker B: So there's the last slide. I don't know exactly what I'm saying. It's just some remarks. Okay, so you have more or less all the elements to prove the interpolation in h two. And so the connection with separation and cast measures. And there's a general statement for this by Alleman and his co author. Okay, maybe I want to recall this because I was mentioning this before.
00:50:33.954 - 00:51:09.664, Speaker B: In the beginning, I was saying this Carlson condition implies weak interpolation. Weak interpolation tells you that we have uniform minimality of the reprocene kernels. And now we have seen that in this space it implies also interpolation. So reads condition. And a natural question could be, does uniform minimality of a normalized sequence of reproducing kernels. And reproducing kernel space always implies unconditionality. Or what? It can even ask this question in a more general setting.
00:51:09.664 - 00:51:47.714, Speaker B: And there are several positive answers. Of course. In the Hardy space, we have just shown the result for the backman space, Schusten Saib, they were exploring Hedden Malen's extremal function, which is the right translation of this blaschke function. So in this space, the answer is yes also. And for foch spaces, Schuster and Sahib also gave a result in 2000. And so for p less or equal to two, it's still okay. But if you take p bigger than two uniform one, minimality does not longer imply interpolation.
00:51:47.714 - 00:52:07.374, Speaker B: Okay, that's all for today. Tomorrow I will speak about Fox basis and I will explain how to use debau methods to use an interpolation problem. Okay, thank you very much.
00:52:08.114 - 00:52:25.454, Speaker A: Thank you for the wonderful talk. Any questions? Comments, suggestions? So there is a question in the chat, is there a characterization of interpolating sequence for hard space on the poly disk or the unit ball? Great question.
00:52:28.324 - 00:52:50.784, Speaker B: I don't think so. At least there's no usable definition. Maybe there's some abstract. There are many partial results. It's a difficult problem. And of course, between the unit ball and the poly disc, there are quite some differences of geometrical differences. Of course you can give some sufficient conditions from what has been done here.
00:52:50.784 - 00:53:04.824, Speaker B: There are many experts in several variables who have worked on this, and there's some literature on this. I think there is no definite answer to this question.
00:53:07.124 - 00:53:13.224, Speaker A: More questions? Comments? If not, let's thank Andreas again.
00:53:14.244 - 00:53:14.644, Speaker B: Thank you.
