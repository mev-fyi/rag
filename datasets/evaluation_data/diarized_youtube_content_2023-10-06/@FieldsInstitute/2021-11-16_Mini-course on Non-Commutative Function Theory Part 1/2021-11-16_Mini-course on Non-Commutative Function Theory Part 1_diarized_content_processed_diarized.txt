00:00:01.640 - 00:00:23.486, Speaker A: Let's start. Good morning, good afternoon. Good evening, everybody. Welcome to week 14 of our focus program. And we start with mini course lectures by Mike jury from University of Florida on non commutative function theory. Mike, please.
00:00:23.670 - 00:00:58.204, Speaker B: All right, well, thanks, Javan. I'd like to thank the organizers for the invitation. Unfortunately, it can't actually be in Toronto to do this. But I guess, as we were discussing, I think given that it's November, maybe November in Florida is preferable to November in Toronto, although maybe that's a personality test. Okay, so non cumulative function theory. And so this is sort of, in some sense, a big topic and still relatively new. It's rapidly developing.
00:00:58.204 - 00:02:04.104, Speaker B: And this mini course is part of, well, this focus program on function spaces, which we've been enjoying for the past several months. And so I'll get into it in more detail in a minute. But there are many different approaches one could take to this, and many different reasons why one would be led to studying these sort of non commutative functions, whatever they are. And so my presentation, because it's part of this focus program, is going to be extremely prejudiced, and it's going to be prejudiced towards those parts of the non commutative function theory that will sort of be most recognizable to people who are familiar already with some aspects of function spaces. Like, for example, the theory of the Hardy space in the disk that Javad started the whole program with. So, in particular, there can be lots of other parts of this theory that I will only mention in passing and not talk about. And again, it's going to be a presentation that is meant to be a point of entry into this theory for people who are familiar with function spaces.
00:02:04.104 - 00:03:05.102, Speaker B: And so, that said, I'm going to assume no background about any of this non commutative stuff. So I will start from scratch. And as I said, there are lots of big, important parts of this subject that I will make, I will not discuss at all, again, because with three lectures, I have to bear and choose. And as I said, I want to make this as useful as possible to the audience that we have. Okay, so let me. Okay, so the way I'm going to do, I do have text slides here, but every once in a while I'm going to have a blank page to let me write and ramble a little bit. So where does this come from? Well, where it comes from, well, it might come from several different things, but a good way to start thinking about it is that from the point of view of those of us who work in function spaces and care about interactions between spaces of functions and operators, like the shift operator and the hurdy space, the holomorphic functional calculus plays a central role.
00:03:05.102 - 00:04:14.254, Speaker B: So if I have, let's say, just say, a matrix a or an operator, trivially, because matrices form an algebra, or operators form an algebra, I have a polynomial functional calculus. I can send any polynomial, I can evaluate it on this matrix of this operator, and I get another matrix or another operator. And this is all trivial enough, but the sort of non trivial and non obvious thing at the very beginning is that you can go beyond polynomials. And so we know that we have this holomorphic functional calculus, the Ries Dunford functional calculus, and it says that you can go beyond polynomials, you can evaluate holomorphic functions on operators, as long as the domain of the holomorphic function is sufficiently large. Sufficiently large means the domain contains the spectrum of the operator. From this point of view, really the entire point of the spectrum. The job of the spectrum is to support a functional calculus.
00:04:14.254 - 00:05:12.070, Speaker B: If I go beyond, sorry, so if I go beyond polynomials, that's polynomial. But if I have a holomorphic function, f, in a neighborhood of the spectrum of a, we get a functional calculus here. So we can define f of a, but we can define it in a sort of coherent way, so that this is actually because this collection of holomorphic functions is an algebra. And this map, this functional calculus, is an algebra homomorphism. And even in undergraduate mathematics, we know that it's important to be able to go beyond the polynomial functional calculus to the holomorphic functional calculus. So even, for example, in undergraduate odes, if I have a linear system like that, we know how to solve it. And the solution is given by this holomorphic functional calculus.
00:05:12.070 - 00:05:53.262, Speaker B: I need to be able to take the exponential of a matrix. Of course, the exponential is holomorphic. It's an entire function, but not a polynomial. So again, I don't think, for this audience, I need to persuade you that the holomorphic functional calculus is important. But what it means is, and especially for people interested in function spaces, is that when you think about holomorphic functions, you almost automatically don't think of them simply as functions defined on some domain in the plane. You are inexorably led to think of them in some sense as enlarging the domain vertically. In the sense, you think of their domain as including matrices with spectrum in the domain, or operators with spectrum in the domain.
00:05:53.262 - 00:06:35.744, Speaker B: And for many kinds of problems that arise in that theory. In this theory of function, spaces and adjacent subjects. The functional calculus point of view is central. And holomorphic functions are things that exist to be evaluated on matrices or operators. And so this is the functional calculus for a single matrix or operator, which is, of course, very classical and well known for systems of, let's say, commuting operators. It's already less clear what to do. And this was done by Taylor in the sixties and seventies, a commutative functional calculus.
00:06:35.744 - 00:07:46.352, Speaker B: And the big thing that he invented there was what's called the Taylor spectrum. And the point is that for a system of commuting operators, it's non trivial to define the right notion of spectrum, joint spectrum, for several commuting operators. And how do you know when you've defined the right notion of spectrum? Well, the right notion of the spectrum, if you believe what I said before, the job of the spectrum is to support a functional calculus. And so the right notion of spectrum is the one that gives you sort of the correct functional calculus, one that will be valid for all holomorphic functions in a neighborhood of whatever it is spectrum you've defined. And so the Taylor spectrum for a system of commuting operators is the right one, although that's, again, highly non trivial to define, but it works. So, Taylor, having been successful in the commutative realm, set himself the question of what about systems of operators that don't commute with each other? I mean, generically, if you just write down two random matrices, they don't commute. And so he asked himself, is there a coherent notion of functional calculus for operators or matrices that don't commute? And.
00:07:46.352 - 00:08:26.892, Speaker B: Okay, I guess I should go to my next slide now. So, certainly, so, I've written here a polynomial in two variables, but I've written this polynomial in a way that I want to think of the variables as not commuting. And so I have x squared, y and xyx there, which if they were commutative, those would be the same thing, and I'd combine them. Likewise, they have xy and yx, and if those were commuting, I would collapse them to zero. But I've written them separately precisely because I want to think of these as non commuting indeterminants. So n C for non commuting. And so certainly if I have a pair of non commuting matrices or something, I can evaluate a polynomial in non committing arguments on them.
00:08:26.892 - 00:09:25.168, Speaker B: So there's, again, sort of obviously, trivially, a functional calculus for polynomials and non commuting arguments. And so then Taylor's question was, well, if you think of a holomorphic function as sort of being one step beyond the polynomial. They share many properties of polynomials, and they're sort of locally approximable by polynomials and so forth. So his question was, is there some natural class of objects that goes beyond non commutative polynomials? And in particular, the way, you know, you had the right class of objects is if you could somehow coherently evaluate them on systems of non commuting matrices or maybe operators. So those are what are going to be the non commutative functions of the title. And so I'll build up to the definition, but to move on, so p will be a polynomial, in this case in two non commutative indeterminates. I could obviously have more variables for these lectures.
00:09:25.168 - 00:10:22.488, Speaker B: Since we're interested in analysis, I'll assume these will have either real or complex coefficients, and really just always complex coefficients for me, although the real variable theory is important too. So if we have a polynomial with say, complex coefficients and non committing indeterminants, we can evaluate. So again, if I just have two variables here, we can evaluate these on pairs of square matrices. I could evaluate them on scalars, of course, although I'd lose something, right, if I evaluate these on scalars. Of course, this x y minus yx disappears. If I want to sort of capture for, to see the full polynomial, I'll need to evaluate it on things that aren't just scalars, so I can evaluate it on, well, scalars, which I want to think of as one by one matrices, or two by two or three by three, or any size. And so in some sense, if I want to think of this polynomial in non cubittic indeterminates as a function, it's really, if I want to be careful, it's not one function, it's really a whole family of functions.
00:10:22.488 - 00:11:01.508, Speaker B: I have one function for every size of matrices, and so we'll call that a a graded function. So for each n I have, this polynomial defines a map from mn, cross mn into mn. So this one polynomial defines a sort of hierarchy of functions, one at each level. I could, of course define an operators too, and you can think of operators as being something like the infinite level, but I won't worry about that for now. Okay, so that's an nc polynomial. And to go on, let me fix some notation very quickly. So Mn will be the n by n matrices with complex coefficients.
00:11:01.508 - 00:11:41.264, Speaker B: I'm going to work entirely over the complex field now, mnd, these will be detuples of complex matrices. And the idea is these are going to be sort of points at which we will evaluate non commutative polynomials. So that's what MnD is. And then, as I said, I want to consider all of these levels sort of simultaneously. And so I sort of assemble all of these, take a disjoint union over n. They get this graded set graded by n. And this is sometimes called the matrix universe.
00:11:41.264 - 00:12:27.974, Speaker B: I'll note here, I mean implicitly when I'm saying mand, I'm working over CD. You can do this very abstractly where you replace d dimensional complex space with some abstract vector space. But again, I want to keep things elementary and won't worry about that. So we have MnD, we have MD, we have this matrix universe. And as I just said, so an NC polynomial in now d variables will define what I'll call a graded function. So at every level I can evaluate square matrices at the appropriate size and return a square matrix of the same size. So it respects, so it's a graded function, it respects the sizes.
00:12:27.974 - 00:13:01.524, Speaker B: Okay, so polynomials are, Nc polynomials are then graded functions on this matrix universe. But what Taylor observed is that, and it's easy enough to observe. And I'll show you, you have one of these things at each level. But what happens at the different levels are tied together. And this is really what's important. And what we'll, let us move on. Uh, let's look at, uh, I want to look at, uh, two operations that, that sort of are respected by this, uh, polynomial functional calculus on, on matrices.
00:13:01.524 - 00:13:30.700, Speaker B: So, uh, if I take a point x at some level n, so this will be my notation. So a single x now is always going to stand for a d tuple at whichever level. So if I fix a point x and another point y at possibly a different level. So I don't require these to be the same. I have some n there and some m there, I can form their direct sum. So this is just the entry. So I take x one, direct sum y one and xd x two, direct sum y two, etcetera.
00:13:30.700 - 00:14:11.624, Speaker B: And this gives me a new point in the matrix universe at level n plus m. And so I fill in with zeros just to make the matrix matrices square. And if you think about how polynomial algebra works, it's immediate that if I now evaluate my polynomial at this direct sum, well, it's the same thing as I get if I evaluated the polynomial just at x and just at y and then took the direct sum of the results. And so we say p respects direct sums or commutes with this direct sum operation. This is one thing that these nc polynomials do when acting in the matrix universe. They respect direct sums. So what happens at different levels is connected at least a little bit.
00:14:11.624 - 00:14:44.088, Speaker B: There's another sort of obvious algebraic thing that polynomials do. So again, let me fix a point x in the matrix universe. And if I take, so my point x is at level n. If I take an invertible n by n matrix, then I can conjugate this point by this s, by this similarity. So this is, again, I write this down partly to fix the notation. So I have s inverse xs. Remember, x is really this d tuple.
00:14:44.088 - 00:15:59.844, Speaker B: So this is just a shorthand for conjugate each of the entries x one through xd by this same fixed similarity s. All of these are size n by n. So I move one point at level n, I conjugate it over to another point at level n. And again, just by how matrix algebra works, it's clear that polynomials respect similarities in this sense that if I take a similarity and conjugate x and then apply a polynomial, it's the same thing as if I apply the polynomial and then the similarity, simply because, for example, if I look at, say, a product x one x two, well, if I conjugate both of those, of course the ss inverse is canceled and I'm left with s inverse x one x two s. And so simply because whenever I multiply two of these sort of monomials, the SS inverse cancels out, we have respect similarities. So our polynomial functional calculus in the matrix universe respects these direct sums and similarities. And I'll give the full definition in a minute.
00:15:59.844 - 00:17:11.074, Speaker B: But the point is, Taylor's big insight and sort of the non obvious thing is that it turns out that these two properties of respecting direct sums and similarities actually allow you to go a very, very long way. And these are going to figure into the definition of NC function, which I'll give in a minute, but I want to just pause and say, okay, I alluded at the beginning of this kind of construction arises in many different contexts, and so I put a few of them here. So in general, the objects we're going to look at, the way we're going to go beyond polynomials is to look at graded functions on this matrix universe, or maybe subsets of it that will respect direct sums and similarities and see what we can do with them. And it turns out that this kind of object arises in many places in modern analysis. So voikowescu sort of bumped into this in looking at free probability, which is sort of natural, because if you know anything about free probability, which I don't. But you have these systems of sort of free random random variables, where a random variable now is a d tuple of matrices or operators, which in general don't commute. And so it's natural to look at this sort of functional calculus there.
00:17:11.074 - 00:18:21.304, Speaker B: It arose in a very different direction coming from engineering and the work of Bill Helton and his many collaborators. This is more like the real variable theory, where, and you might restrict yourself to symmetric matrices, but dealing with matricial notions of convexity and lmis, of linear matrix inequalities and things like that. And it turns out this NC machinery is also somehow the right language to pose those problems. For topics that are sort of closest to the purpose of this focus program, we have operator theory and dilation theory and function spaces. So the big name dimension here is Jodo Popescu, who I think is here with us today. And much of the theory that I'm going to describe in the subsequent lectures, the second and third lecture, originates in Pepescu's work in the nineties and two thousands, where he extended many notions of sort of familiar function theory and function spaces to this NC realm. Some of that, and I'll mention again what are called row contractions and row isometries in the operator theory that's attended there.
00:18:21.304 - 00:19:03.724, Speaker B: Davidson and Pitts also sort of independently discovered some of those things at the same time, and then later worked by Matt Kennedy and others and many, many other people. And the thing about listing names is you can never list all of them. So there are many, many people that are leaving out here, but I'll mention especially ball and Binakoff and their collaborators. I'll talk about some of that work later. In this lecture is also Eckler, McCarthy, many other names that I'll mention as I'll go on. So anyway, the theory is now sort of developed in these many different directions and looks somewhat different depending on what direction you come to it from. But there are a certain basic set of notions that are common to all of these approaches.
00:19:03.724 - 00:19:31.506, Speaker B: And so this first lecture, I'm going to focus simply on what is an NC function and what are its basic properties. So everything I've said so far has been sort of algebraic. I've only talked about polynomials. And so if you're going to go from algebra to analysis, you need. Well, okay, so we need something like sets. And so I've talked only about the matrix universe, which would sort of be like entire functions. But in general, if you think about holomorphic functions, you don't always have entire ones.
00:19:31.506 - 00:20:14.704, Speaker B: You they're defined only on some domain. So I need to say what NC sets are, which are going to be the sets that support these NC functions. They don't need to tell you what NC functions actually are. And then even then, when you have sets and functions, you still, to get analysis, you want to go beyond this and you need something like topology, and you want to talk about something like continuity or really even that might just be topology. To get analysis, you really want something like differentiability. So we'll get there, but I need to build up some of that machinery. So again, if you're thinking of an NC function, whatever it is, and I'll tell you very soon, as generalizing a polynomial, you need to think about what kind of sets it's going to be defined on.
00:20:14.704 - 00:20:45.296, Speaker B: And so here is the definition. So now what you call an NC set will vary somewhat depending on where you start. And so really to get going, the only thing that you really need for an NC set will be this item two. Okay, so what is an NC set going to be? So it's going to be a subset. So it's going to be a graded set. So first off, I'm going to have a subset of the Matrix universe, but my subset is going to be graded. So I'm going to have a sort of a piece of my set at every level.
00:20:45.296 - 00:21:13.588, Speaker B: Well, maybe not every level. I'll comment that in a moment. It's going to be a graded set. So at each level I'll have some omega n, but it will turn out that it's allowed to be empty. So for example, I'll show you later in the lecture an example of a naturally arising NC set that's actually empty at level one. So I don't require all the omega ends to be non empty, but it should be a graded set. And if it's going to play well with polynomials or generalized polynomials at a bare minimum, I need this item too.
00:21:13.588 - 00:22:01.286, Speaker B: It needs to respect direct sums, and in some sources I mean, this will be the definition of NC set, simply a graded set that respects direct sums. Since I want to get to analysis quickly, I'll throw into the definition something that you don't have to throw in at the beginning. But I'm going to insist rather naively that my set be open in the usual topology at every level. And I'll say more about that requirement later, but you don't really need that for the definition. But to get anything interesting, you need some kind of openness. And then this other item three, I'll mention, simply because it frequently occurs that your sets are sort of unitarily invariant. And the prime example, the main examples I'm going to show you that we're going to work with in these lectures will be unitary invariant.
00:22:01.286 - 00:22:26.134, Speaker B: But again, you don't really need it as part of the definition, but it arises naturally. So the main thing it needs to do is be graded in respect direct sums. And I'm going to talk about open ones. So those will be NC sets, because again, you think of a domain of a holomorphic function. It should be an open set in some sense. So that seems reasonable. Okay, so some quick examples of NC sets before I actually get to the functions.
00:22:26.134 - 00:23:05.702, Speaker B: So one that's going to figure prominently in these lectures is what I'm going to call the row ball. So at each level end I'm going to take my two of matrices and I'm going to insist that this norm of this sum xj, xj is less than one. So really what I'm saying is I'm going to take my square matrices and align them in a row. And so I get a long, a sort of wide matrix of height n. And, sorry, this is my n squareds here, not squared, it's n by dn and high. And I div them along this wide matrix. I insist that that matrix have norm strictly less than one.
00:23:05.702 - 00:23:34.134, Speaker B: So it's called the row ball because this is called the row norm. And what you can see is that if I look, so again, I can do this at each level n. So at level one, what am I looking at? These things are scalars. And then I'm just looking at the ordinary open unit ball in cd, usually euclidean norm. Okay, so this is what I'm going to call the row ball, very similarly to that. So that, that's an NC set. Again, this, this is clearly open at every level because this is an open condition.
00:23:34.134 - 00:23:56.110, Speaker B: And you can check quickly from the definition that it respects direct sums. It's also unitarily invariant. But that's most important. Like I said, similarly to the row ball, I could do the column ball, I could take my x's and arrange them in a column. And again, these squared shouldn't be there. This is DN by nice. That's a column ball, the column ball.
00:23:56.110 - 00:24:41.014, Speaker B: Again, if I look at it just at level one, at level one, this is again just the ordinary euclidean ball. So these are two NC sets that look exactly the same at level one. But you can write down a quick example to convince yourself that already at level two, these are different. I'll let you work it out if you get bored. But the point is I can have different NC sets that look the same at level one or look the same at level one, two and different three and so forth. And this will figure into something I want to say later on. And besides this, you can even invent more domains that look like the euclidean ball at level one that aren't either of these.
00:24:41.014 - 00:25:03.844, Speaker B: And if you're familiar with the language of operator spaces, these would be different operator spaces, operator space structures over your podium space. But I won't need that formalism. Maybe one more distance. We're here. Since the unit ball is a familiar domain, the poly disc is also a familiar domain. And I could talk about the NC poly disk. I don't know why I have squares here DN by DN.
00:25:03.844 - 00:25:36.384, Speaker B: This is just, I asked each matrix individually to be contractive. And again at level one, that's just the ordinary unit polydisc. Okay, so those are some examples of naturally occurring NC sets. Maybe if I wanted to be careful, I would say an NC set is just graded and respects direct sums. And NC domain would be also open at every level. But that's fine. Okay, so these examples are all special cases in a more general class of domain, which you would call a polyhedral domain, they're just defined by some matrix polynomial being contractive.
00:25:36.384 - 00:26:15.910, Speaker B: So if I make my matrix polynomial to be that row, I get the row ball or the column ball or the NC poly disk there in two variables. And you can imagine many more elaborate. So the point is I just take a matrix, put some NC polynomials in each entry and define my domain to be those things where that matrix is contracted. That's called a polyhedral domain. And these kinds of domains also I'll revisit later on in the lectures. But these are three simple examples of them. Okay, so now I want to actually get to defining an NC function and talking about them.
00:26:15.910 - 00:26:41.356, Speaker B: So my omega is going to be an NC set, graded respects direct sums, and I'll go ahead and assume it's open at every level. So here's an NC function. Now finally the definition. So an NC function on omega is a graded function. So it's a function from omega to n. But in particular, like I said, for polynomials, it's going to respect sizes. So in each level omega, n, it's going to map into mn.
00:26:41.356 - 00:27:02.794, Speaker B: That's what graded means. And it's going to do what polynomials do. It's going to respect direct sums and respect similarities. So it's basically, I don't know, polynomial would be entire. Its domain would be the entire matrix universe. So f, I don't assume it's defined everywhere. It might be defined just in the roll ball or the polydisc or wherever, but this is what an NC function is going to.
00:27:02.794 - 00:27:45.742, Speaker B: And then Taylor's big insight was that this is a good definition. And I'm going to, in the rest of this lecture, I'm going to show you a couple of theorems and prove a couple of things that will maybe convince you this is a good definition. But still, at this point, these two conditions of respecting direct sums and respecting similarities are still purely algebraic. It's not obvious that this leads to any analysis yet, and that's kind of the big surprise. And that's what I'm going to show you. Let's at least look at some examples of NC functions. So, as I've already said, we have NC polynomials, and these are entire, the only thing I want to mention here.
00:27:45.742 - 00:28:21.376, Speaker B: So here's an example of an NC polynomial and two variables I can allow constant terms. And the way I handle a constant term, this five I that I've written here, the point is when you, of course, when you evaluate at some level, you just put in the n by n, the identity matrix of the appropriate size there. That's how you evaluate them. Scaler there. Another interesting class of functions. Okay, so going beyond polynomials, well, if you like, one way to say the next thing beyond polynomials will be rational functions where you throw in inverses. Once I take inverses, of course, I have to restrict the domain, right? Because if I'm looking at the thing may not be invertible.
00:28:21.376 - 00:28:57.904, Speaker B: So here I've written a thing that you should reasonably want to call an NC rational function, because all I've done is taken a polynomial, this commutator, and slapped an inverse on it. And you can say, well, okay, once I take an inverse, now, this won't be defined everywhere. I don't expect it to be entire. It'll be defined only where that inverse exists or makes sense, which is just, well, whatever. This determinant is non zero. And you can check that this omega that I've written here will be an NC set. This will respect direct sums and will be open at every level.
00:28:57.904 - 00:29:36.116, Speaker B: The thing I want to point out here, and I mentioned this a second ago, is that this is an NC set that's empty at level one. There are no scalars in the domain of this rational functions. For scalars, I get zero. There are naturally occurring domains that are going to be empty, say, at level one, or, you know, if it's not, as soon as it's not empty at one level. Of course it's not empty at infinitely many levels because I can at least take direct sums. But the point is, I don't insist it's not empty at every level. Again, thinking with ordinary analytic functions, besides rational functions, you have things expressible as power series.
00:29:36.116 - 00:30:27.424, Speaker B: And I've written down a simple example of an NC power series here, which you can believe would converge to an NC rational function, at least on some domains, say, where x one and x, sorry, x two and x three are contractive, maybe on some larger domain. In the second and third lectures tomorrow and Thursday, I'm going to talk a lot about NC power series, because you have to think about where they're going to converge. And if we're thinking about NC function spaces, things that would generalize the hardy space, it's natural. Look at power series. So I'll say much, much more about power series, not today, but tomorrow. It's also worth thinking about some things that aren't examples. And here is a hint of sort of the strength of this direct sum and similarities business.
00:30:27.424 - 00:31:10.174, Speaker B: Look at the map that sends x to x star just in one variable. Okay? So at every matrix level, I just send x to its adjoint. It's clear that this map respects direct sums. Sorry, this map will respect direct sums, but taking adjoints does not respect. Taking adjoints will not respect similarities because you're going to kick out a star, and then that doesn't work. On the other hand, I can look at this map that sends x to the trace of x times the identity. I throw in the identity of the correct size to keep it graded so it's graded and the trace is a similarity invariant.
00:31:10.174 - 00:32:11.624, Speaker B: So it obviously respects similarities, but it will not respect direct sums, which is easy to check. So those are examples of sort of natural things that you might want to do on matrices, but that don't give you an NC function the way. Okay, so, but as I said, everything that I've said up to this point and even this definition of NC function is still algebraic. Direct sums and similarities. And so at some point we need to get to analysis, and now is that point. So the amazing thing is that these purely algebraic operations respecting direct sums and similarities, as soon as you combine them with a little bit of topology, insisting that the domain be open at each level, you all of a sudden are led to analysis. So I'm going to state that I'm going to show you the proofs of these theorems, because they're not so hard to prove, and they give you a good taste of sort of how some of this theory begins to work.
00:32:11.624 - 00:32:59.314, Speaker B: So here is a proposition. So take an NC function on some NC set. Again, I assume it's open at every level. If it's continuous on each of those domains omega n, at each level n, then it's actually holomorphic. And so what do I mean by holomorphic? Well, if you like, you can think of omega n is just a domain in some very large dimensional complex space, a complex base of dimension dn squared, just as a function of all of the matrix entries that appear there. And my claim is that if I view my function as a function of just all of those matrix entries all at once, it's actually a holomorphic function of all of those entries, those dn squared variables. So this is hopefully surprising, because continuous shouldn't apply holomorphic.
00:32:59.314 - 00:33:45.310, Speaker B: I mean, that's kind of crazy. Let me explain why this is true, and, well, only do this and I'll do another theorem. So the reason that it's not quite so crazy, we have to start to think about it, is, as I mentioned, the beginning, our framework is the functional calculus. If you think about the functional calculus for one variable, if you allow ordinary polynomials to act on matrices, you can discover derivatives in a sort of algebraic way. So I've written this sort of thing, this triangular matrix here, which is sort of morally a Jourdan block. If I think about how polynomials act on Jordan blocks, well, if I take this thing and square it, I get x squared. Then I get this xh plus hx, which I'm being sort of Nc hygienic here.
00:33:45.310 - 00:34:38.142, Speaker B: So if I literally do the matrix multiplication, I got xh plus hx. But of course, if these are scalar entries, then I can commute them and I get a two xh there. So if I square it, I get this two xh in the upper right corner, and you can see where that's going. If I cube this thing, I get three x squared h in the corner, or in the middle term, you see the sort of Nc hygienic version. And so this immediately generalizes to say that if I take any polynomial in this sort of Jordan block, I get this derivative of the polynomial appearing in the upper right corner. So I can in fact discover derivatives in this algebraic way. And the point is that I can generalize this sort of algebra or systematize it is maybe a better word if I understand how direct sums and similarities work.
00:34:38.142 - 00:35:06.634, Speaker B: So let me prove now a fundamental lemma, which is sort of goes back to Taylor and is really essential to the whole theory and will allow me to prove this sort of continuous implies holomorphic theorem. It's a sort of intertwining result. So I have an nc function on some omega. I'm going to assume it's open at every level. You don't quite need that. Okay, so I'll take an x at some level n and a y at some other level n. We know that f respects direct sum.
00:35:06.634 - 00:35:39.038, Speaker B: So if I let f act on just the direct sum of x and y, I know that what that would be. But the point is, you can learn something about how f acts on these sort of upper triangular objects. So if I take another w, a rectangular matrix w of the correct size. If this expression makes sense here, I've said norm of w is small. And the point is, if norm of w is small enough, then my openness, this matrix that I'm looking at here, will be in the domain. Actually, I don't really need norm of w to be small. I just need to take any w so that this object is in the domain.
00:35:39.038 - 00:36:17.024, Speaker B: That's all that matters. And then this is sort of this identity then, is going one step beyond respecting direct sums. It also respects these sort of, these upper triangular intertwiner things. So if I have f acting on x and y, and then this x w minus w y expression there, the point is f sort of passes through the w. And just I get fx w minus wfx. If you believe this, and I'll prove it in a second. But what that tells you is that if I have a w that intertwines x x and y, so x w equals w y, that means that upper corner would be zero.
00:36:17.024 - 00:36:56.194, Speaker B: And then respecting direct sums would force that intertwined expression to be zero. And so what you get is that if w intertwines x and y, then w will also intertwine fx with fy. And in fact, you could use this intertwining property to give an alternative definition of Nc function. Instead of saying that I respect I'm graded and respect direct sums and similarities, you could say I'm graded and respect intertwinings. And Taylor proved that those definitions are equivalent. In fact, the proof of the lemma will prove one of those directions. So how do you prove this limit? Let me show you the proof, because it's quick.
00:36:56.194 - 00:38:20.684, Speaker B: The idea is to first use the fact that f respects direct sums, and then use the fact that f respects similarities. And for your similarity you make a clever choice of similarity. So for my similarity I'm going to take identities of the correct sizes on the diagonal and w in the corner. And then while that matrix is the form identity plus null potent, so s inverse is simply identity minus w zero one. And so if I let s look at s inverse acting on this direct sum, well you can do the calculation, but what you get is exactly this, x x w minus w y zero y. And then that means, well, if f of that by respecting direct sums, f of s inverse that thing, by respecting direct sums and similarities, I'm going to run out of room. But the point is I get exactly what I want.
00:38:20.684 - 00:38:55.444, Speaker B: I get xfx, I ran out of room there. But that's okay. But the point is you simply respect direct sums and you respect similarities with that careful choice of similarity and it works. And so that lets you go from saying direct sums plus similarities gives you intertwiners, conversely intertwine. You can go backwards and prove that if you respect intertwiners and are graded, you can get similarities in direct sums. So I'll leave that. Okay, so now let me prove that continuous implies differentiable and all we'll need is that level.
00:38:55.444 - 00:39:37.606, Speaker B: So again I'm going to fix a point x at some level. I'll fix an h at the same level with norm h small. So again small, so that when I perturb things by h I stay within my open set, because again I'm assuming my set has some openness and I'll fix a scalar t. And the trick is that if I look at this expression on the left, I can look at x plus th there, an x and an h in the corner. I can write that h in this clever way as x plus th times one over t minus one over t times x. But I want to think of that one over t as one over t times the identity. And that's a w.
00:39:37.606 - 00:40:41.474, Speaker B: And so that means that this expression here has the form a zero b and then aw minus wb, which means the lemma applies to it. So I can evaluate f on this expression using the lemma, and that's what I've written in the last line there. So f on that thing equals the right hand side. And so now by magic, what you see in the upper right corner is exactly this difference quotient for f. So now what have I assumed about f? All I assumed was that f was continuous and so if I look in the left hand side, I only have, sorry, I only have one appearance of t in the left hand side that's within this x plus th. And since f is continuous, assumed continuous at x and the domain is open, et cetera, when I take the limit as t goes to zero, the limit on the left hand side certainly exists just by the continuity of f. But that means that the limit in the right hand side also exists.
00:40:41.474 - 00:41:11.634, Speaker B: But in the right hand side, in the upper right corner, I have a difference quotient. And so that limit, that difference quotient has a limit, and so f has a directional derivative in the direction of h. And then from there you quickly get that it's quickly derived, that it's holomorphic in the complex sense. But this is the trick for getting continuous to imply differentiable. And I think I was going to say one more thing about it, actually. Sorry, whoops. To go up.
00:41:11.634 - 00:42:14.182, Speaker B: So this fact that sort of the similarities and direct sums allow you to smuggle these things into the upper right corner, and you use these sort of Jordan block like objects, allows you to detect differentiability in this algebraic way. But you can use this same lemma to push this argument even further, to say you can even weaken continuity. So now let me prove a similar kind of proposition that says in order to be continuous, I just need to be locally bounded. So again, if I have, I'm open at every level and I'm locally bounded, then I'm continuous. So if I combine that with the first proposition that says locally bounded implies continuous, and then continuous implies differentiable, as soon as I have an NC function that's simply locally bounded, which is an extremely weak thing for a function to be. It's already differentiable and already holomorphic. And so that's the real strength of this direct sums and similarities business.
00:42:14.182 - 00:42:46.364, Speaker B: So the proof of this is actually going to be quite similar. The point is you just have to look for the object that you're trying to control and smuggle it up into the upper right corner using the lemma. So it'll be the same thing. For simplicity, let me just assume, assume it's globally bounded. So I'm just gonna assume f is globally bounded by M on the whole domain. I'll prove globally bounded implies continuous. But you can, if you inspect the proof, you'll quickly see that you didn't need it globally, you just needed it locally.
00:42:46.364 - 00:43:13.044, Speaker B: But if I assume it's globally bonded by m, I let epsilon be given. So I choose an x and Y in my domain at this. Sorry, I should have said that they're going to be at the same level here. X and Y are the same level. X is going to be close to y. So let me prove continuity at X. Okay, so I choose delta small enough so that this upper triangular thing is an omega.
00:43:13.044 - 00:44:00.104, Speaker B: And I can guarantee that using the openness, right? I mean, the point is y will be close to x. And if X minus Y is small enough, this upper right corner will be very small. And so this thing will be close to, this thing will be close to x, direct sum x. And so using openness, I can make those perturbations small enough to keep this in omega, because x, omega is closed under direct sum. So x direct sum x is in there. But again, that upper right corner is, I am in the situation of the lemma, where again, my w is just m over epsilon times the identity. So really, by the exact same argument, do I apply f to this, and I get the same thing just with fx and fy replacing x and y.
00:44:00.104 - 00:44:30.644, Speaker B: And so now f. So now this expression that I've written here is just f applied to the first thing in omega. But I've assumed f was bounded. So this two by two thing with the f's in it is now bounded. But if it's bounded, that means in particular, its upper right corner is bounded. But if the upper right corner is bounded by m, well, if m over epsilon times this thing is bounded by m, then the thing itself was bounded by epsilon. If you just unwind the constants.
00:44:30.644 - 00:45:13.960, Speaker B: So what I've got then is if x minus y is less than delta is, if y is delta close to x, then fy is delta close to fx, and that's continuity. And you see, I did globally bound it. But you don't need that. I mean, the point is, you just need locally bounded near x, direct sum x, because that's where this thing is going to live. Okay? So this very, very simple trick of smuggling these intertwining expressions up into the right corner is actually extremely powerful. And so that's that. So, in fact, locally bounded already implies continuous, and continuous already implies differentiable.
00:45:13.960 - 00:45:51.554, Speaker B: So in fact, if I want to say, if I take, say if I, so if I talk later about an NC function that's bounded in some domain, well, that's already holomorphic everywhere. It's already a bounded holomorphic function. So that's, that's sort of some of the magic. So with that done, then what I can then do is formally define the derivative of an NC function, because the algebra that we've done says that if I evaluate f. So if I unpack the proof of the first proposition that continuous implies differentiable. What the lemma actually showed you was that f, acting on one of these Jordan block style things, looks like fx. Fx.
00:45:51.554 - 00:46:50.644, Speaker B: And you got something up here in the corner, which was the limit of these difference quotients. So you define that thing to be this nc derivative, and the proof shows you that that expression is actually linear in h, like a derivative should be. And so, if you actually calculate this for a polynomial, you can see how it goes. If I look at this polynomial pxy, which I've written here, I think of that x squared y. If I write that as x xy, then if I have two variables x and y, and my h is really again in two variables h and k, the way I evaluate this derivative, this d f x at h, is I just one by one, substitute each occurrence of x with one occurrence of h, and I substitute each occurrence with y with one occurrence of k. So if I apply, if I replace the first x in this expression by an h, I get that term. If I replace the second expression by h, I get that term, I replace the y by a keg, and so on.
00:46:50.644 - 00:47:52.906, Speaker B: You can compute these nc derivatives of polynomials in this mechanical way, and that's what they are. You can also get higher derivatives, which I'm not going to do explicitly. But the point is, I can look at higher derivatives by looking at, if I go up to the three by three case and I evaluate this, then up in the last corner, I'm going to get some sort of second derivative, which is then going to be bilinear in each and k and so on, you can define these higher derivatives and it all hangs together. And then if you look in the book of Vinikoff and Kalyushny Verbibesky, they give a thorough treatment of, you can call these Val Taylor Taylor series. So one Taylor is JL Taylor, doing this in 1970s. The other Taylor is Brooke Taylor of the classical Taylor series. And so these NC functions, because they are automatically differentiable in a certain sense, in fact mathematically infinite, infinitely differentiable by going up to higher sizes like this.
00:47:52.906 - 00:48:23.356, Speaker B: They have these kind of Taylor expansions. If you do these expansions, if the domain contains, say, the origin and do expansions about the origin, these are easy to understand. Expansions about matrix points are more complicated. And I'm going to sort of ignore that for now, but that's what you can do. So, as a sanity check. Let's go back and do this in one variable. This entire matrix construction, with direct sums and similarities and so forth, didn't need d variables to work, right.
00:48:23.356 - 00:49:11.878, Speaker B: I could have done all this in a domain in just one complex variable. And if you work out what happens there, which I won't do. But if I take, say, the open unit disk, I can sort of form a matricial domain or a matrix disk by putting that, putting levels over it. Like I said before, a one by one level is the scalar disk, and then I have a two by two matrices and three by three and so forth, which I could take to be, I'll call it the matrix disk. Just at each level, I take the n by n matrix, excuse me, n by n matrices that are contracted, I can then define. Well, maybe it's weird to call it Nc in this case, because it's only one variable. But if I look at functions that do that and work out the Taylor, Taylor, Taylor series and so forth, you find that you would just end on land on the usual Taylor series.
00:49:11.878 - 00:49:50.704, Speaker B: And this NC functional calculus, the way I defined it, really just is the classical functional calculus. And everything sort of hangs together as expected. So this gives you strong evidence that you're working with the right objects. And this functional calculus point of view is fruitful. So since I'm nearly at the end of my time, I think I'll maybe postpone a little bit of this till at the beginning of the next lecture. But I've talked about continuity and differentiability and strictly and locally bounded and so forth, and I did this naively. I just said, well, I'm going to assume my domain is open at each level.
00:49:50.704 - 00:50:42.444, Speaker B: But to go further and to get deeper theorems from calculus and so forth, you do need to be careful about topologies. And the only sort of objectionable thing about to say, well, the domain is open at each level, is that if I give my domain a topology, then by just saying it's open, if it's open at each level, the basic open sets there are not NC sets. They don't respect direct sums, because I can just have a set that just lives at a single level, doesn't play with direct sums. So if you wanted to be sort of rigorous about or sort of determined to do everything in this sort of NC realm, then you might want to look at topologies where the open sets for the topology are themselves NC sets that respect direct sums. And it turns out there's more than one way to do that. And so, since I'm out of time. Maybe I'll postpone this discussion until the second lecture.
00:50:42.444 - 00:51:26.388, Speaker B: But the find topology is what I've been talking about. So not ncsets, just open at every level, but there are at least two different topologies that you could build out of NC sets. One will be called the free topology, and the other is called the fat topology. And I'll recommend the book of Aguirre McCarthy Young here. And then I'll stop and I'll say more about this at the beginning of the second lecture, and then talk about power series. But they have this recent book which is called operator analysis, which in some sense is some classical objects viewed from this sort of functional calculus point of view. But they have some chapters at the end about the NC stuff, and in particular, these topologies are discussed there.
00:51:26.388 - 00:51:55.374, Speaker B: So I'll say a little bit about this in the next lecture. But it turns out, if you want theorems from calculus, like the implicit function theorem or the inverse function theorem and so forth, you can get them, but you have to be very, very careful about the topology. And in some sense, the phrase they use is, there's no goldilocks topology. To get a good theorem, to get one theorem to work well, you need one topology. To get another theorem, you need a different topology. But no, one topology is sort of the correct one. So I'll stop here.
00:51:55.374 - 00:52:17.044, Speaker B: Next lecture, I'll pick up a brief comment about topology and some theorems from calculus. And the next lecture, what I'll do is look at specifically the case of NC power series and start to look at function space things. I'll talk about convergence of power series, reproducing kernels, maybe some rational functions, and then in the third lecture, get to multipliers and things like that. So, thank you, and I'll stop.
00:52:18.304 - 00:52:27.364, Speaker A: Thank you indeed, Mike. Let's thank the speaker first. Is there any question or comment for Mike?
00:52:29.144 - 00:53:19.030, Speaker B: Yes, Mike, yes, thank you for a very informative lecture. In several complex variables, there's the theorem of Hart dogs that the separate holomorphy implies holomorphy, and the proof reduces to showing local boundedness. Now, for NC, in the NC setting, you. You've shown that local bounded implies implies holomorphic. Is there something analogous to the separate holomorphy theorem? Off the top of my head, I don't know. I guess. I mean, so the point is, you're just going to say, well, if you're holomorphic in each variable separately, then you're holomorphic.
00:53:19.030 - 00:53:47.504, Speaker B: And so I guess in some sense, it seems like it would be automatic. Because the point is, if I think about one level at a time, if I'm separately holomorphic, then I do get by the by the classical Hart hogs theorem, I do get local boundedness. But then by local boundedness I get. So it seems like you could get it just by invoking the classical Hart hogs theorem. I don't know that there's a separate. I don't know if there's a separate way to. Some other way to go about that using the NC machinery.
00:53:47.504 - 00:53:55.924, Speaker B: I haven't thought about it. Yes, I agree. It seems that might work. Yes. Thank you, Mike.
00:54:00.564 - 00:54:08.604, Speaker A: Further comments or questions? If not, let's thank Mike again.
