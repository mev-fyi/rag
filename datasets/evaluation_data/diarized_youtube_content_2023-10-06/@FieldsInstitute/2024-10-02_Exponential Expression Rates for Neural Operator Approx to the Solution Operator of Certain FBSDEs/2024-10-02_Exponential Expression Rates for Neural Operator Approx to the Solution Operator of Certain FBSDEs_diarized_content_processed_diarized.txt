00:00:00.240 - 00:00:14.285, Speaker A: For the morning. And that is Anastasias Kratsus and he is assistant professor at McMaster University in the department of Math and stats. And he'll be telling us about exponential expression rates for. You can read the rest of the time.
00:00:16.225 - 00:00:40.671, Speaker B: Thanks. Actually, thank you very much for the introduction and for the invitation. I'm super excited to be here and I think the size of the title reflects my excitement about this topic. So this is actually one of a partner talk. The other talks given by Shui tomorrow he'll be doing dealing with an open loop analog to this I'll be looking for closed loop thing. I'm going to be looking for good rates. He's going to be looking for generality.
00:00:40.671 - 00:01:16.965, Speaker B: So if you want the whole picture, you'll get it over the next few days. Yeah, so this is joint work with a good friend of mine, Takashi Furiya. So he's inverse problems guy in Japan and we always meet at weird times because of that. Okay, so long story short, before I start talking with the problem, I'm just going to make some general comments, even though I think we all know already that FBSDs are important. So let's just start by saying some general remarks. So of course FBSDs are super interesting to us because they arise naturally in a lot of places like control problems, games. And because of this, they come up all the time in finance and economics.
00:01:16.965 - 00:02:02.295, Speaker B: So the problem with FBSDE is not the problem with them, but with dealing with them is one of two things. They're almost never available in closed form. And then numerical schemes for solving FBSDs which might be more realistic for a given problem, are challenging to say the least. Right? So they're very, they want to solve, they don't work in high dimensions, blah, blah, blah. Okay, so that said, what is the state of the art for numerical schemes? And then I'll criticize it. So state of the art. One branch of state of the art is one of these deep learning powered schemes or class of works which called like deep BSD solvers.
00:02:02.295 - 00:02:41.099, Speaker B: There's a bunch of work by Janssen, Patrick and a bunch of other people. But the idea is that you take a classical BST solver, you replace some of the basis regression steps with the deep neural networks and things work slightly better. But the problem with that is that every time you want to perturb your, your fbsd, you have to rerun, you have to rerun these guys. Okay? And so at a high level, if I change the terminal condition or change the dynamics of my process, I Have to rerun this. Right. So if you have a lot of models or you have some kind of uncertainty, you understand how the problem changes for each model in your class. This could be a huge runtime.
00:02:41.099 - 00:03:02.795, Speaker B: Right? So big problems, very slow because they rerun them many times. Potentially infeasible. So instead of saying this last line, I snuck in this slide in the morning. So just to give an idea, okay, So X is going to be my forward process. I'm not going to talk about what it is right now. T is some future time horizon. And imagine I have this infinite family of BSDs or FESTs on itself.
00:03:02.795 - 00:03:21.659, Speaker B: We're just going to look at the backwards process. So these are indexed by my terminal condition, which is. Yeah. Which is basically indexed by some C1 ball. Okay. That's the big set. So of course it's obviously infeasible to solve all of these simultaneously, which is actually what I want to do.
00:03:21.659 - 00:03:39.717, Speaker B: Yes, yes, yes, yes. All these, yeah, all the notation that I just like gloss over is a TBD and like this. Yeah. Alpha space and T6X fixed. And Y and G are like. Yeah, yes, that's the actual plus. Yeah, yeah, yeah.
00:03:39.717 - 00:03:50.477, Speaker B: That's, that's, that's an extra plus just to keep you on the toes. Thank you. Cool. Okay, so that's it. But we have an infinite set of these. Okay. I like to solve them simultaneously.
00:03:50.477 - 00:04:07.485, Speaker B: This is the point of the talk. Of course. This is infeasible by rerunning this many times. So what if I just make do and say I want to solve a really good finite subset. But what I mean by this is say you pick some delta. Barzilla Scoli. We know this guy is compact in a supremum norm.
00:04:07.485 - 00:05:00.193, Speaker B: Yeah. And so I just take a finite collection of these and so in that case you can just rerun your DPSD solver once reach FPSD that corresponds to one of these GNS in my delta net of my C1 ball. Okay. Now to make it to really drive the last point home on the last slide, why is this completely wildly infeasible? Well, the question is how many, how big is N? In other words, how many of these BSTs do I have to solve FPS? Well, a lower bound is double exponential in delta in the dimension. So like and the reciprocal besides the net. So in other words, even if your solver takes a second to solve, you'll be here till like the world loses heat, know? So it's impossible. So, so this is not to say that the problem is impossible, but the approach is impossible.
00:05:00.193 - 00:05:13.337, Speaker B: So yeah, no way. So this is the goal of my talk. So I'm going to do a funny structure for the talk. I'll start with the goal. I'll start with a high level theorem and I'll break it down for the rest of the time. So the objective is. Yeah.
00:05:13.337 - 00:05:36.387, Speaker B: To solve FBSDs with random, random terminal time families of them. And by inputs are going to be the following. The inputs are going to be a terminal condition and dynamics of the backwards process. That's what I get. My model gets that as input. My model states pre chained my output to my model is going to be automatically gives me the solution to the corresponding fpsd. So by this I'm not going to rerun some solver.
00:05:36.387 - 00:05:55.801, Speaker B: The model just takes this function like takes the input data and the dynamics and automatically gives me an answer. Right. And that's the key point. I don't want any retraining. So there should be an automatic solution for each input. Okay, so let's now kind of give a high level idea of the setting. I'm going to gloss over some technical details because that's not fun.
00:05:55.801 - 00:06:14.057, Speaker B: So my forward processes are going to be simple, but actually these are kind of already hard. They're going to be in high dimensions. So it's D at least three. You can make it whatever you want, just at least three. For some technical reason it's just going to be some Brownian motion with some kind of correlation. Gamma is positive, definite matrix. Okay, that's my forward process.
00:06:14.057 - 00:06:39.475, Speaker B: There's one forward process for all my problems. Okay. These are going to evolve inside some domain d in rd. And this domain is bounded with c1 boundary. Okay? And tau is going to be the random time where this whole thing, this whole show stops. And it's the first time where my forward processes x from this domain. Okay.
00:06:39.475 - 00:06:55.195, Speaker B: And for each. So now I'm going to consider the following family of FBSDs which looks like this. So to answer that questions, this explains the plus. Yeah. So I'm going to be given. So a couple system. Here are my forward dynamics.
00:06:55.195 - 00:07:28.665, Speaker B: Alpha is given and what isn't given. And like this creates indexing. My family is two things. A perturbation to the terminal condition which is the volume form it just like some boundary. So some function evaluate my forward process at the time which exits the domain and some additive perturbation of the dynamics of the backwards process given but just additively by some function of forward process. Okay. So in other words, for every G and for every F0 I have some FBSD, same board process, same terminal time, same domain.
00:07:28.665 - 00:07:56.375, Speaker B: Yeah, when I have infinitely many backwards processes and I want to solve them all one shot. Okay, so how do we do this is the setup okay for everybody. I'm so used to teaching that. Okay, so slightly formally, not too formally. The object I'm interested in is approximating the solution operator to this problem. So what that looks like is. Oops, how'd it go? This is the pointer.
00:07:56.375 - 00:08:12.579, Speaker B: Oh cool. It's as stable as me. Okay, so basically. Oh wait, I have to point it there. So six seconds. So what it takes is again boundary data. So g this terminal condition, some source data, so some perturbation to my dynamics.
00:08:12.579 - 00:08:30.625, Speaker B: These will live in some sublev spaces. Gymnastic appointment by hand. And it gives you the solution to the fbsd which is again a pair of Y and the Z. So Martin Gilpart and the backwards process. Oh thanks. I wish I was taller. But cool, thanks.
00:08:30.625 - 00:08:59.339, Speaker B: Okay, again, so the solution operator, all it does is just. It's a fictitious, it's a theoretical object, right? So it gives you a terminal condition perturbation to the dynamics and you get the solution to the FBSC out. And I like to actually represent this thing and how. Okay, here's the semi cool part. Using some infinite dimensional deep learning models. So it's good that we had some infinite dimensional talk so far. But so far I'll say that the fact that we can do this is perhaps not surprising.
00:08:59.339 - 00:09:25.185, Speaker B: Right. So no surprise here. I mean there's a massive amount of literature in the last few years on deep learning in different dimensions and there's actually two schools who are really interested in this. There's actually our community and there's the PDE's community. Okay, so there's a bunch of results out there. There's also Yosef, Krista, me, a bunch of people over the years. And so there are universal approximation theorems that can say that you can approximate such an operator.
00:09:25.185 - 00:10:05.221, Speaker B: Okay, so of course, as usual, this talk doesn't end here. So what's the issue? The issue is that if you want to tackle this problem by universality and sometimes that might be appropriate, it will be appropriate in Xue's case. In this case it's not necessarily appropriate. The reason is because a universal approximation theorem. Well, what is it really saying? It says that if you have some function you're interested in, say the solution operator here, and you know that belongs to some broader class of functions and there is a neural network or deep learning model, whatever that can approximate this thing. But the complexity of the network corresponds to the worst possible function in that class. It's possible that the functions in this class, imagine this is like some Lipschitz operator or something.
00:10:05.221 - 00:10:41.885, Speaker B: Imagine the functions in that class can be highly irregular, much worse structured than this guy. So in this case, the problem that I pose on the board, and this is why it's fundamentally different from the talk you'll see later, is rather well structured. And so even though I can, even though there exists a deep learning model that can approximate this thing, it's the number the parametric estimates are going to be astronomical because they're corresponding to the worst possible Lipschitz function between these spaces. And that's just crazy, you know. So the key point here is I want to have good rates. And so now the true heart of the problem starts. And the point is I'm not going to go via universal approximation.
00:10:41.885 - 00:11:01.605, Speaker B: I'm going to have to approximate the sky from scratch by really understanding the problem itself. Okay, so I'm gonna do the talk backwards. This is the first time I tried this. I think it makes a lot of sense though. Let's suppose this. The informally the theorem the soon to be on archive paper has the formal thing. Okay, so I'm gonna look for a solution operator.
00:11:01.605 - 00:11:26.529, Speaker B: For me, hats are things that are approximators. So gamma hats approximate solution operator gamma star. Takashi chose a star. I don't know why. So what that does again it takes boundary data, so G thermal condition, some source data, so F0 the perturbations of the dynamics. And it gives me some other. Yeah, so it gives me a candidate for the solution of my fpsd.
00:11:26.529 - 00:12:03.833, Speaker B: So Y and a Z. Ok? And the idea is that for every approximation error and for every. Yeah, for broad set, broad set of G and F and for every time horizon T, I can get this uniform approximation error uniformly over all the F and G and F. Okay, I flipped the order. So what that means, especially explicitly means that once I have this gamma hats, then all I need to do is plug in F0 and G and I'll automatically have a good solution to my fpsd. So this is the high level thing so far I could have gotten this from universal approximation. But here's the key point.
00:12:03.833 - 00:12:39.079, Speaker B: The main point, which I actually really do think it's really cool, is that the parameter, the parametric complexity of this object is surprisingly low. Right? So for everyone who's seen the deep learning model, there's essentially two or three parameters. If you're in dimensions of Three, one is the depth, so the depth of my model. So if I want an epsilon good approximation error, it's logarithmic and a reciprocal approximation error, so that converges exponentially fast. Wow. The width is actually constant in epsilon, which is extremely impressive too. I'm literally impressed by the time I see it.
00:12:39.079 - 00:13:08.135, Speaker B: I can't believe that's possible, but it is. And then the rank, which is sort of the latent dimension of this thing, is linear in the reciprocal approximation. So long story short, the number of parameters is just over quadratic in the reciprocal approximation. That's really low. Okay, now to understand how low that is, I'm going to juxtapose that with the recent result of a friend of mine, Samuel Lanteiler. He's at Caltech and soon he'd be somewhere nice, like permanently. Yeah, I won't say until it's official.
00:13:08.135 - 00:13:32.155, Speaker B: Here is a lower bound in the general setting. So we'll say what's possible in the universal approximation theorem and you'll see how much of an improvement this is over the worst case scenario. So F and Y are nice infinite dimensional binance bases. For example, take separable Hilbert spaces like in Dennis talk. And F is a literate nonlinear operator. Between these things. Yeah.
00:13:32.155 - 00:14:02.691, Speaker B: Then how much parameters you need to approximate this to Epsilon goodness, epsilon goodness. I mean for this statement to hold, if you want via universality, again, that's exponential reciprocal approximation error. Okay, now to understand how bad that is and how much better this is, let's actually take the next closest thing. So there's a result by a magnificent mathematician, Dmitry Urozky, a computer scientist from 2017. Let's understand what this looks like. Not an infinite dimensional case, in the finite dimensional case. And then let's juxtapose everything.
00:14:02.691 - 00:14:34.827, Speaker B: So if you have a function that slips between RD and R and you want to approximate it on some compact full dimension, then if you want epsilon good approximation, so uniform, then you need one over. So you need a polynomial number of parameters whose degree is equal to the dimension of the space. Yes. So this is the so called cursive dimensionality. Okay, so that's what breaks down all your algorithms in infinite dimensions. It's not, it's not even polynomial, it's exponential. And actually I'm claiming to you that I can do it quadratically and many of my parameters even converge constantly or exponentially.
00:14:34.827 - 00:14:57.827, Speaker B: That's really, really a massive improvement. So how is that possible? Right, so here's the rest of the talk. So phase two, this is the theorem. Why should you believe me? Let's explain how this strategy works. So this is a new proof technique that's inspired by some results by Schwab, but really took it to the next level. And we'll have some sequence of papers coming out soon. So here's the idea.
00:14:57.827 - 00:15:39.975, Speaker B: So idea is it starts from not thinking about the BST but rather going to the pd, right? So there's a results by in like right before 2000, right before Y2K that says that for every such bfst, right, like in the previous form. This is why I chose this setup. You can write Y and Z as some function and its gradient. Evaluate the forward process where that math triple function solves this elliptic problem. So what I want to be doing is not approximating the solution operator to the fbsd, but rather just approximate the solution operator's elliptic problem. And what that means that you give me boundary data, this, I keep calling it that source data. So I call it calling this additive perturbation terminal time.
00:15:39.975 - 00:16:01.825, Speaker B: And the solution operatively elliptic problem will give you back this U or approximation of it. And I just plug in X and voila. And its gradient. So this is how a gamma is going to work. And I'll go into this, into more detail to understand why this is happening. So gamma plus Picachi chose this notation. I let him.
00:16:01.825 - 00:16:39.135, Speaker B: So is this a solution offered to the pde? So that takes the boundary data and the source data and gives me the solution operator U to the elliptic pd, which when I plug in X into U and its gradient, I get the solution operator to FPSD, I get the solution to FPSD for that particular G and that particular F0. OK? And yeah, this is what it does. So here's the whole pipeline. So you give me an F and a G. I give you again, hats are approximation things. So I solve the solution operator to this elliptic problem approximately and I just. I get some new hat and some approximation to gradients and I just plug in X and that gives me an approximate solution to my psd.
00:16:39.135 - 00:17:12.335, Speaker B: Right? Okay, so how are these rates possible? Right? Exactly. How is it? So in other words, I've transferred the question to ask myself. How is it possible to efficiently approximate the same rates the solution operator this elliptic problem, right? So to understand that, we need to start actually going into the elliptic problem. Okay? So one way to solve it. So let's just fix a G and fix it at 0. Understand that 1 at 1 case at a time, you can obtain this, the solution for reasonable GNF0 by iterating the following operator onto itself. So you can get as a fixed point.
00:17:12.335 - 00:17:32.104, Speaker B: Right. And what is this operator? This is some nonlinear non local operator. It looks like this, there's some components. Let me use a Sebastian stick. Okay, so this guy solves the following elliptic problem. Okay. And okay, that's not really that crazy.
00:17:32.104 - 00:18:00.007, Speaker B: But here's the really beautiful part. So this guy is the Green function associated to my elliptical, right? Or yeah, without the answer. Okay, so the idea is going to be I want using very few layers of my neural operator. So my infinite dimensional neural network encode this as best as I can. So imagine that's one layer. And then the depth of my network is going to. Exactly what it's going to do is going to.
00:18:00.007 - 00:18:24.723, Speaker B: It's going to. Yeah, it's going to iterate this operator onto itself. So in other words, my neural network model is going to be literally approximately implementing a fixed point iteration and that's what the exponential rate is going to come from. So the main challenge is not really getting the wg that's easy to do. It's going to be the Green's function. Okay, so how do I encode this into my network? Right. So in other words, I forgot to say this.
00:18:24.723 - 00:18:58.657, Speaker B: My objective is to come with a neural network that is built around the structure of my elliptic problem so that those rates are possible. Okay, so let's understand the Green's function to the elliptical. So the Green's function has in this case has the following beautiful decomposition. So this dates back to the late 70s, effectively 80s. And just from this year we got the last piece of the puzzle is very convenient. So the objective is going to be notice that the Green's function does not depend on g or an F0. So I need to be able to encode this thing directly into the layers of my neural network.
00:18:58.657 - 00:19:22.071, Speaker B: So then when I make this iteration, yeah, I can converge a fixed point. So I understand the Green's function. So there's two parts, the smooth parts. And because this does not depend on the input Data, G and F0, I can approximate this very easily using some classical neural network theory. So smooth function is easy to approximate. That c gamma is not a problem. The main problem is that the Green function is singular.
00:19:22.071 - 00:19:45.711, Speaker B: It has a big spike. Now it's not very surprising to imagine that a function with a massive spike has such low regularity around the spike that it is impossible to approximate. Right. But here is the magic, actually it has a very, very simple closed one, right? This is just the following thing. So I put some constants and blah blah, blah. But it's effectively the roots of gamma, which makes sense. The positive matrix times some power of the norm.
00:19:45.711 - 00:20:18.635, Speaker B: So that's something you can code in like half a line, you know, to like zoom out in your screen. Okay, so the idea is now the following thing is I can hard code the singular part as part of one convolution against one single part as one of the layers of my neural operator. This thing I can absorb in the classical neural operator into layers. And I'm actually going to, by doing that, I'll get my rate. So the idea is basically this. I come up with a layer of my neural operator. So neural operator infinite dimensional neural network, which sounds cooler, right? Gamma hat that approximates one of this.
00:20:18.635 - 00:20:48.855, Speaker B: The T that when iterated gives me a solution to pd. And then, yeah, and then depth is just iteration. So I just create depth by literally putting the same operator, the same layer into itself. So invented many times. Then I approximately have implemented a fixed point iteration and that they should give me a solution to my PD and efficiently, because as we all know, fixed point iterations converge very fast. So now in the last few moments I'll just write down. So the talk is completely structured backwards on purpose.
00:20:48.855 - 00:21:14.361, Speaker B: I'm going to now define to you the neural operator, because now you see why it works. So let's see what it's built, how it's built. So I abridged notation. This thing makes sense in more general setting. So imagine you have a bunch of functions from some solve spaces are going in and some bunch of functions of some solver space are going out. So yeah, so what that does is you take the V0, V0. It's just supposed to be like a G and F0.
00:21:14.361 - 00:21:31.021, Speaker B: Okay. And in this case, okay, the notation is terrible on the first line. Yeah. Oh my God. Okay, so that, that should be like L1 times L1. Yeah. So then you just get some other function out that you evaluate at x and its gradient and that will be the solution approximately.
00:21:31.021 - 00:22:00.575, Speaker B: So what does it look like? So what you do is. Well, you get some, the function comes in or the pair of function comes in and you do the following operations iteratively, right? So first you do some component wise multiplication via some matrix, just like a classical neural network in bind dimensions. You do some specific convolution which I explain on the next slide and you do some other kind of convolution which I do on the next slide. Long story short, this is like the smooth part of the Green's function. It's like the singular part. Okay. And then you add some additive functions, like an infinite dimensional bias.
00:22:00.575 - 00:22:20.175, Speaker B: That's just a classical neural network. This doesn't matter, whatever. After you do that, you apply component bias non linearity just like in the finite dimensional case. And specifically we're going to be choosing the square of the relu function for highly technical reason. This sigma is in the black dot. Needs component wise composition. Okay, you do this again and again and again.
00:22:20.175 - 00:22:52.861, Speaker B: This is the depth, the number of times you do this is the layers. And at the last layer you did it one last time without adding the component wise composition. So it's like a really direct analog of the fine dimensional neural network, except there's these convolution operations at each layer. Now what are these convolution operations? They're just very straightforward. With the R, the singular part is exactly this, since we have. Oh yeah, I have my height booster. So since you have access to the closed form expression for the singular part of the Green's function, I literally just convolve this to some matrix multiplication at the end and the smooth part, I approximately implement this.
00:22:52.861 - 00:23:16.551, Speaker B: And how to do that? It's kind of obvious, maybe not just it. I think it's straightforward. It's just the following thing. You do some kind of expansion via some debaucheries wavelets in my sub level space which are optimal. Okay, and that's it. So when you construct it. So that's the model that gave the efficient approximation rates, which were impossible via a universal approximation theorem.
00:23:16.551 - 00:23:39.723, Speaker B: And summary. This construction is, I think, pretty cool. It avoids terrible rates by directly encoding the PV structure into the neural network. And the depth of my neural network has a very concrete meaning. Everyone's always saying, what are neural networks doing? I'll tell you. In this case, the depth is encoding iterates of fixed point contraction operator T. I took away the gamma to make it more so.
00:23:39.723 - 00:24:09.195, Speaker B: In conclusion, what if I talk to you guys in the this talk. Right. So with this technology you can now simultaneously solve infinitely many FBs in one shot, which is of course completely infeasible even by really fast algorithm before. Okay, you need roughly reciprocal quadratic number of parameters in the approximation error to do it. That's pretty impressive. That's not even possible typically in fine dimensions. Unless you're in two dimensions or one maybe, I guess.
00:24:09.195 - 00:24:27.883, Speaker B: Okay. And unlike classical deep BSD or FPSD solvers, we don't have to reoptimize any time. You optimize once and you can keep plugging in and play. And unlike neural operators. So these are interdimensional neural networks. They have a cool name. We don't need wildly infeasible number of parameters.
00:24:27.883 - 00:24:34.575, Speaker B: We have a very grounded number of parameters, at least in order. I can't promise the constants. Okay. And that's it.
00:24:41.735 - 00:24:42.895, Speaker A: Open the floor to some questions.
