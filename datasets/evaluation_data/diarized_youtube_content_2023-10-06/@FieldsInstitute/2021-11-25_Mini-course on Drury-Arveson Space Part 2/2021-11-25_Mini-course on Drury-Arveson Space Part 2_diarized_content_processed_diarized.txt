00:00:00.120 - 00:00:16.634, Speaker A: Good afternoon and good evening, everybody. It's time to begin the second day of the workshop. And it's my great pleasure to introduce our first speaker, Michael Kaertz from Sutherland University, who will continue his mini course on Drury harvest in space, please.
00:00:18.374 - 00:00:57.604, Speaker B: Well, thank you very much. Thank you for coming back. So the first slide of today is the last slide of yesterday, just to remind you of the different descriptions of the Druyavesen space that we worked out. So we can think of it as a space of power series, where the power series coefficients are supposed to belong to some weighted l two space. With these weights, you can think of it as a reproducing kernel, herbal space with a very natural reproducing kernel. We can think of it as what's called a base of Sobel f space. So you demand that a certain number of radial derivatives is in l two with respect to an appropriate weight.
00:00:57.604 - 00:01:36.884, Speaker B: And we also have this NC point of view, which was, you take functions in this NC hardy space and you restrict them to level one. Okay? So what I want to talk about today are multipliers and how they relate to operator theory. So we've seen multipliers in other mini courses and in other talks. And so here's the definition. So the multiplier algebra of the duriasen space just consists of all functions on the unit ball that leave the Deirden space invariant under multiplication. So the derivation space itself is not an algebra, just like h two is not an algebra. But this thing is an algebra.
00:01:36.884 - 00:02:11.076, Speaker B: And it's an easy exercise, using the ghost graph theorem, to show that every multiplier gives you a bounded multiplication operator. And this defines the multiplier norm. So you just take the operator norm of the multiplication operator. So with this norm, it's a unital commutative Banach algebra. In fact, it's aaa, weak operator topology, closed non self adjoint operator algebra. If you identify a multiplier with its multiplication operator. So I think a good case can be made that these multipliers are actually what's sort of really important about this space.
00:02:11.076 - 00:02:49.972, Speaker B: And maybe they're actually the reason why people care so much. And I'm going to try to make this precise today. So the most important example of a multiplier are the coordinate functions. So these functions z one through zd, they turn out to be multipliers and they have multiply norm one. There are multiple ways to see it. You can see it, for instance, in this power series description, because if you multiply by zi, then you just shift the coefficients and the inequalities work out so that you have multiplier norm one. In fact, if you do this computation a little more carefully, then you can get something better.
00:02:49.972 - 00:03:46.360, Speaker B: Namely you can work out that the sum of Mzi Mzi is a recognizable operator, namely its identity minus the orthogonal projection onto the constant functions. So this is supposed to remind you of the Hardy space, because in the Hardy space multiplication by z is just the unilateral forward shift. So the Android is the backward shift, and then forward shift times backward shift gives you identity minus projection onto the constant functions. So in particular, this operator here on the right is less than or equal to the identity. And this tells you that if you stick these operators into a row and you think of them as acting from D copies of the derialis in space into one copy just by row column multiplication, then this is a contraction. So we've seen this in Mike Joy's lectures. This is what's called a roll contraction.
00:03:46.360 - 00:04:33.704, Speaker B: So these multiplication operators by the coordinate functions form a commuting tuple of role contractions. And we're going to see today that it's not just an example, but it's in some sense universal commuting role contraction. And this is why this space, or one of the reasons why this space turns out to be so important. Okay, so what are, so what is this multiplier algebra then can we, can we describe this in somehow concretely? Well, the first thing you can observe is that if D is at least two, well, you have an inclusion into h infinity, but the inclusion is tricked. And so let's have a quick look at why this is the case. So the inclusion is nothing special. It works in great generality.
00:04:33.704 - 00:04:58.560, Speaker B: And the reason is quite simple. We've seen it before, namely, if you have a multiplier, then, well, the space contains the constant function one. So the multiplier has to be in the Hilbert space. And everything in the Dryadisin space is holomorphic. So you're certainly holomorphic. And then there is the standard identity that the kernel functions. Here they are eigenvectors for the joint of multiplication of the multiplication operator.
00:04:58.560 - 00:05:26.056, Speaker B: And the eigenvalues are just the values of the, uh, multiplier complex conjugate. So they're all in the spectrum of the adjoin. And therefore the, the supernorm of the multiplier is at most the operator norm of the multiplication operator. So you have a contractive inclusion from the multiplier algebra into h infinity. And, and this has not, doesn't really have anything to do with the duality in space. It works in great generality. But the interesting point here is that the inclusion is strict, so unlike in d one.
00:05:26.056 - 00:06:22.290, Speaker B: In the case d equals one, when we have equality, of course. And this is another instance where different points of view suggest different results. Because if you think of the dualisin space essentially as the hardy space just in several variables, then this is perhaps surprising. But if you have this definition with the radial derivative in mind, then this is maybe not such a big surprise, because as soon as the derivative comes up, you don't really expect the multipliers to be h infinity. I mean, this is just like we saw in Tom Ransford's mini coils on the Dirichlet space. And indeed, Jing Bo Xia and Shelley Fang point out in their survey that if you look at this function theory description, then it's kind of clear that non constant inner functions can never be in the duriavasan space because the derivative of those has bad integrability properties. So this sort of blows it out of the water, but you can actually do it in elementary computation.
00:06:22.290 - 00:07:21.684, Speaker B: So let me show you the elementary computation that doesn't rely on enough functions on the ballot. What you can do is you can look at this simple polynomial two z one z two to the n, and by the inequality of arithmetic and geometric means, this has supernorm equal to one. It's attained at one over root two, one over root two. But you can estimate the multiply norm in the dualison space from below, because the multiply norm is always at least the norm in the Hilbert space by applying to the constant function one. And then you can work this out using the explicit formula for the norm of a monomial, so that the two to the n gives you four to the n. And this thing is this alpha factorial divided by mod alpha factorial, because alpha is the multi index n comma n in this case. And so then you use Sterling's formula, you get a whole bunch of cancellation, but in the end, essentially a root n factor survives, which tends to infinity.
00:07:21.684 - 00:08:00.144, Speaker B: So this tells you that you don't have a bounded inclusion from h infinity into the multiplier algebra. And then there's various ways of finishing it up. You can just say, well, by the closed graph theorem, then you don't have any inclusion at all, because if you did, then it would have to be bounded. Or you can also do an explicit construction. Once you know this, you can write down fairly explicitly a power series in z one z two to the n, which belongs to the ball algebra, but is not contained in the trialisan space. In particular, it's not a multiplier. So either way you can, this computation finishes it up.
00:08:00.144 - 00:08:46.104, Speaker B: Okay. So then you can ask, well, so what is the description of this multiply algebra? So it has to be a stronger condition than h infinity. And the basic point is, well, if you take derivatives, then you need to use the product rule, right? So let's do a little computation in the case d equals two. So we know being an h infinity is a necessary condition. So suppose you have an h infinity function and you want to figure out if it's a multiplier or not. And we're going to use this function theory description, which says that a function is in the triavesin space if and only if its radial derivative belongs to the Bergmann space. And so when you realize, when you use this, then you want to check if phi times f is again in the Bergmann space.
00:08:46.104 - 00:09:34.614, Speaker B: Sorry, in the duriasen space, you want to check if the radial derivative of phi times f belongs to the Bergman space. And so you have to use the product rule. And if you use the product rule, then you see that this is r phi times f plus phi times rf. And because phi was assumed to be an h infinity, the second sum, and is always fine, right, because rf is in the Bergman space, you multiply with an h infinity function and you're still in the Bergman space. So this is in l two a. And so what you need to control is you need to control the first summand. So what you need is that r phi times f is in the Bergman space, which is just the same as saying you need to know that r phi is a multiplier.
00:09:34.614 - 00:10:19.222, Speaker B: Now, from the Drury Alveson space into the Bergmann space. So it should take the Druyawesen space into l two a. Right? Because for every function f in the Duravison space, r phi times f should land in the Bergen space. And you can reformulate this in function theoretic terms. So what this means is that there exists a constant c such that, well, if you look at phi times rf and you take the l two norm, so mod f squared, rv mod squared, devolume should be less than or equal to the constant times the norm of the function in the Dreyalveson space. And so this is the condition you get in addition to being an h infinity. And this is what's known as a Carlisle measure condition.
00:10:19.222 - 00:10:59.718, Speaker B: So you need to know that this inequality holds, which means that r phi mod squared. The volume needs to be a Carlisle measure for the dualis in space. This is similar to what we saw in the Dirichlet space in Thomas talks. It turns out you can do this for higher d as well. The computations do become a bit more complicated because now you have to take higher order derivatives and then you get a long sum. But it turns out it's enough to control the top and the bottom sum and this long sum. And then this leads to this theorem of Ortega and Fabrega, who characterized multipliers of the dreyal space in the following way.
00:10:59.718 - 00:11:29.884, Speaker B: So, phi is a multiplier if and only if phi is an h infinity. This we know. And you need a Carlisle measure condition. And so the Carlson measure condition should remind you of the function theory description of the Druyaveson space. So you have to pick some m so that two m d is bigger than minus one. And then you need to know that this quantity here. So the radial derivative times the weight, the volume, is a coalescent measure for the Droe Allison space, which precisely.
00:11:29.884 - 00:12:21.634, Speaker B: So, by definition, this is, this is what a coliseum measure for the droias in space is. So there is an extra condition you need to check, and it's not automatic just from knowing that phi is an h infinity. Now, these coliseum measures for the dualisin space were actually characterized in geometric terms by Arkatsi, Rockberg and Sawyer for the characterization. It's not particularly simple, so I'm not going to write it down here, but in principle, it does give a function theoretic description of multipliers. Let me also mention that even if you don't know what the Carlson measures look like, precisely, having something like this Ortega Fabrega theorem is actually very useful. And let's look at another toy example, namely, let's look again at d two. And suppose we have a multiplier which is bounded below.
00:12:21.634 - 00:12:56.774, Speaker B: And the question is, is one over the multiplier also a multiplier? So that's a pretty reasonable question. And this far pega theorem says that what you need to check are two things. You need to check that one over phi is an h infinity, and you need to check the color's measure condition. Well, one over phi being an h infinity. Sorry, I'm struggling with my pen. All right, so one over phi in h infinity. This is, of course, clear, because phi is bounded below.
00:12:56.774 - 00:13:10.774, Speaker B: And then you need to take the Carlisle measure condition. So you need to look at. So d is two here. So you can take m equals one above, and then you look at the radial derivative of one over phi.
00:13:12.594 - 00:13:12.882, Speaker A: And.
00:13:12.898 - 00:13:43.394, Speaker B: Then you need to use the quotient rule. And so if you take absolute values, this is rv divided by phi squared, an absolute value. So you can bound this above, because phi is bounded below by one over epsilon squared, mod, or phi. So this tells you that if feed generates a coliseum measure, then so does one over feed. Because clearly, if you're dominated by a coliseum measure, then your coliseum measure as well. So the coliseum measure condition is also fine. It holds as well.
00:13:43.394 - 00:14:18.464, Speaker B: So this shows that at least in two variables, if you have a multiplier that's bounded below, then the reciprocal is a multiplier again and again. This can be generalized, although the computations do become significantly more complicated. But it can be done. And so then what you get is that if you have, in any finite dimension d, if you have a multiplier that's found below, then the reciprocal is a multiplier as well. So you might recognize this as a very special case of the corona theorem due to Costillo, soy and Wic. I'm going to say more about this tomorrow. At least that's my plan.
00:14:18.464 - 00:15:37.254, Speaker B: But just for this one function fee, there are easier arguments along the lines of what I just did in dimension two due to Fang and Shah and to Reichstag and Sanders. So, one point I want to make is here. Even if you're only interested really in an operator theory, and you don't care at all about function theoretic descriptions, this kind of result is something that you might care about, because it tells you, for instance, that the spectrum of a multiplication operator is exactly the closure of the image of the multiplier. And this is something that operator theorists care about. All right, so, so why are these multipliers so important? Well, to explain this, let's switch gears and let's talk about operator theory. So, the motivation for this comes from classical operator theory in one variable, namely von ArmaNDen equality, which says that if you have a contraction operator on a HILbERt space, so it has norm at most one, and if you plug it into a polynomial, then the norm of p of t is at most the supernormal of the polynomial on the unit disk. And as many of you, of course, know, this is a fundamental result in operator theory that really connects function theory on the disk to operator theory on Hilbert space.
00:15:37.254 - 00:16:17.010, Speaker B: And so there are many proofs of this phenomenon inequality. Now, but the textbook proof usually goes through dilation theory. And let me explain this as well. So the textbook proof uses this dilation theorem due to Nagasaki, which goes like this. So again, we have the same setup. We assume that we have a contraction operator on Hilbert space, and then it says that you can find a larger Hilbert space K, and a unitary operator on this larger Hilbert space so that you can recover your operator T from U in the following strong way. Whenever you plug it into a pOlynomial, you get p of t back by taking p of U.
00:16:17.010 - 00:17:09.274, Speaker B: So this operator on the big space and compressing it down to the small space. So you restrict to h, and then you put the orthogonal projection in front, so you can think of this as t in some way a piece of this unitary operator U. And this of course implies von Erman's inequality, because this reduces von Armand's inequality to checking it for unitary operators u. And there it's easy. It falls from spectral theory of continuous functional calculus, for instance. Now it turns out there is a second version of Nag's dilation theorem, which I want to mention, because that's the one that actually generalizes nicely, and it looks slightly different. And so the setup is the same, but the difference is that now we only get an isometry v, so we don't get a unitary but an isometry.
00:17:09.274 - 00:17:43.460, Speaker B: But the advantage we have is we have a tighter relationship between the, the dilation and the original operator because we get an invariant statement. So we get that the original herbal space age is invariant under the adjoin of the isometry. And then you can recover t star as the restriction of v star to h. And once you have invariance, this also implies it for polynomials. So you get a tighter relationship to the original operator. But the price you pay is you only get an isometry now in one variable. It's fairly easy to go back and forth between these two versions.
00:17:43.460 - 00:18:34.874, Speaker B: So, for instance, if you want to go from the second one to the first one. So if you want to go this way, then what you can do is you can take your isometry and you can apply the wall decomposition. So you write it as a direct sum of a bunch of unilateral shifts and a unitary, and then you extend the unilateral shifts to bilateral shifts, and then you get a unitary. So this, in this setting of the second version, we say that t co extends to an isometry. So a coextension is an extension of the adjunct. By the way, there is sort of a block matrix way of thinking of this, which I personally find very useful, and it looks like this. So in the setting of the first version of the theorem, you can write u as a block three by three operator lower triangle, and you can recover t on the diagonal here.
00:18:34.874 - 00:19:17.554, Speaker B: And in the second version, you can write your isometry v as a two by two matrix, and you can recover t as the one one entry. First version, you have it as a two two entry, and the second version is a one one entry. So in the second version, this says that h is co invariant. And in the first version, it's only what's known as semi invariant. All right, so how about multivariable operator theory then? So how does this look in several variables? Suppose we have a couple of committing operators. So this means that ti tj is tj ti for all in j. So this is different now from the situation that Mike Joy talked about last week.
00:19:17.554 - 00:20:04.470, Speaker B: And then the question is, can you do something like Noggs dilation theorem and phonomenon quality? Now, you have to impose some kind of contractivity assumption in several variables. There's more than one reasonable contractivity assumption, just like there's more than one reasonable generalization of unit disk. So one way to do it is to say, well, you think of the poly disk, and this corresponds to saying that your operators should all have norm less than or equal to one. So this connects to function theory on a poly disk. And the one sentence summary is that this works well for d equals two. There is a dilation theorem due to Ando, which generalizes Nags dilation theorem, but it's more difficult for d at least three. Now, this is not the setting I'm going to talk about.
00:20:04.470 - 00:20:47.804, Speaker B: I'm going to talk about the ball. And if you think about the ball, then a reasonable notion of contractivity is that of being a row contraction. So this is what we just saw. You take the operator, stick them into a row, and you want this to be a contraction. And the reason why it connects to the ball is very simple, right? If you take a couple of scalars and then it's a row contraction, if an only, it belongs to the closed unit ball. You could also talk about column contractions if you like, but then you have to take adjoints and all your statements. And just like in the non commutative world, the row is perhaps slightly preferable, because these multiplication operators by the Zi's on the dualisin space are row contraction, not a column contraction.
00:20:47.804 - 00:21:36.424, Speaker B: But you can translate things there by taking adjoints. Okay, so there is a perfect generalization of the second version of nagstylation theorem, which involves the durialis in space, and it works like this. So we need one additional piece of terminology. Namely, a spherical unitary is a tuple of commuting normal operators, so that the sum of Ui Ui star is equal to the identity. So, well, you can write this down the other side, because they're normal, it doesn't matter. And so these are quite well understood because you have the spectral theorem. So at least in the separable case, they all essentially look like multiplication by the coordinate functions on some l two space, where the measure is supported on the unit sphere.
00:21:36.424 - 00:22:41.294, Speaker B: And then this dilation theorem, which the first version was due to Drury, and then it was, and there's a version of Mller and Basilescu, and so the ultimate version is due to arvisem, says the following. So suppose you have a commuting row contraction on Hilbert space. Then t co extends to a very special operator Tuple, namely to one of the form s direct sum u, where u is spherically unitary. So this is something we think we understand well, and s is a direct sum of copies of multiplication by z on this durialisin space. So you should think of this direct sum as being component wise, right? So it's s one direct sum u, one all the way up to sd direct sum ud. And so this says that up to this spherical unitary summand, this multiplication arbiter by z governs the whole behavior of these commuting or contractions. And it's in this sense that the multiplication by z on this realison space is universal.
00:22:41.294 - 00:23:53.120, Speaker B: Now, one thing you can do with this is you can prove a Vonniemann type inequality, which was done first by Drury, and namely, if you have a commuting role contraction and you plug it into a polynomial, then the norm is at most the multiplier norm of the polynomial on the Drury Aveson space. And so this is similar to the one variable when I'm on inequality, because in the one variable phenomenon, you had the supernorm on the right hand side, which is the multiply norm on the hardy space. But now we get a bigger norm than the supernorm. We just saw that the multiply norm on the joyalisian space isn't generally bigger than the supernorm. So I think when you see these two results, it's sort of, at least to me, it's very convincing that this dualis in space is an important space and that it's worth studying. And let me also point out, I mean, I hinted at it, but let me also mention again this, this dilation theorem. If d equals one, it really reduces to the classical narc dilation theorem, because in this case the Arpidus, you get a direct sum of unilateral shifts and unitary, which by the whole decomposition is just a general form of maximum.
00:23:53.120 - 00:24:34.924, Speaker B: So this is a perfect analog of the of the one variable theorem. Okay, so let me say a bit more about this dilation theorem and how the derialis in space comes up. So there is a stronger statement in what's known as the pure case. So let me explain this. Suppose again we have a commuting wall contraction t. And then you can look at a certain map theta, which goes from b of h to b of h, and it sends an operator a to some tia ti. So this takes positive operators to positive operators, and it's actually a completely positive map.
00:24:34.924 - 00:25:17.134, Speaker B: The fact that t is a commuting or is a row contraction translates to saying that theta of I is less than or equal to I. So if you apply theta successively, then you get a decreasing sequence of positive operators on Hilbert space. So they converge in the strong operator topology. And we say that t is pure if this strong operator topology limit is equal to zero. So just think of this in one variable. For instance, in one variable this says that t to the n t converges to zero in strong operator topology, which is the same as saying that t to the n converges to zero in strong operative apology. So this is what you know as a pure operator in one variable.
00:25:17.134 - 00:26:25.714, Speaker B: Now for pure row contractions, the dilation theorem looks even nicer, because then it says that every pure committing row contraction coextends to a direct sum of copies of this multiplication tuple Mz on the Drury Alison space. So the spherical unitary sum and is absent in this case because of purity. So in this sense, people also say that Mz is a universal commuting pure row contraction, because every other commuting period contraction coextends to a direct sum of this one particular operator. And this is actually enough to prove Drury's von Armand inequality. Because the key observation you need to make is that if t is a row contraction and you multiply it by a number r which is less than one, then it becomes pure. This is easy to see, because if, if you multiply by r, then this theta I theta of I is less than or equal to r squared. So when you iterate it, it actually converges to zero in norm.
00:26:25.714 - 00:27:24.494, Speaker B: So if you have a row contraction, you do this trick, you multiply by r, and then it coextends to direct sum of copies of mz and so p of r t is at most the norm of p of m z and p of m z is nothing else than the multiplication operator by p. So this is by definition the multiply norm on the dualis in space. And then you let r go to one in the end, and because you have a polynomial, there's no issue with convergence here. So this special case of the dilation theorem involving pure committing work contractions is enough to get the phenomenon inequality. All right, so let me actually show you at least the main ingredients of this proof in the, in the pure case, because it's actually not all that difficult. So what you do is something that you can also do in one variable. You look at the defect operator.
00:27:24.494 - 00:28:00.868, Speaker B: So you look at this identity minus some ti ti. This is a positive operator because you have a row contraction, and so it has a positive square root, which you can also rewrite as this, just by this definition of, of this map theta. So theta of I is some ti ti star. And then the whole trick is to write down a very particular operator. It goes from h into duriavas in space tensor h. Or you can make it smaller, but this is one way of doing it. And it has this form.
00:28:00.868 - 00:28:41.804, Speaker B: So you apply it to a vector h. And you look at this sum, there are these weights that we've seen before. And then the first tensor component, you look at z to the alpha, and the second tensor component, you write this, you take the defect operator times t star to the alpha applied to h. Okay, so this might look strange, but this operator is to do, is supposed to do one thing and one thing only. Namely, it's supposed to intertwine multiplication by Zi and Ti. And let's look at this if it's at least plausible. Well, what happens if you multiply by, by Ti on the right here? Well, then you increase your multi index.
00:28:41.804 - 00:29:29.044, Speaker B: Here it becomes alpha plus ei. And similarly, if you multiply by MZI on the left, then you decrease the multi index of Z to the alpha. It becomes something like alpha minus ei. And then there are maybe some weights that you have to take care of. But if you ignore the scalars, then this matches up, because then if you do an index shift, these terms you get by right multiplying with Ti and left multiplying by NZI, they match up and the weights just work out so that this is actually not just up to scalars, but it's really on the notes. So we have this operator, and the fact that t is a pure rho contraction turns out to translate to the fact that this operator V is an isometry. So this is a computation which I wrote down.
00:29:29.044 - 00:30:27.432, Speaker B: I mean, it's not going to come up again, but I wanted to show it to you because it's actually not really difficult. And it also shows, again, why these weights pop up. So what you can do is you look at the norm of VH squared, which, by definition, if you look at the definition of VH and bring delta T star to the alpha on the other side, in the inner product, it looks like this. Then you do the usual trick. You write the sum over all multi indices, your first sum over all multi indices of length n, and then you sum over all n at the end. And the thing to remember here is that delta squared, this defect operator, is identity minus theta of I. So then, when you think about this for a minute, you see that if you sum up all multi indices of length n here, what you get is a difference of two terms, and the first one is theta to the n of identity, and the second one is theta to the n plus one of the identity.
00:30:27.432 - 00:30:53.994, Speaker B: This is essentially like in a multinomial theorem. And it's, again, why these weights are the appropriate ones. And then you get a telescoping sum and you can write it like this. And then the tail tends to zero, which is exactly what purity is about. So this is why you want purity. So V is an isometry, and we have this important relation here. So what this relation says is that the range of v is invariant under NZI tensor identity.
00:30:53.994 - 00:31:27.350, Speaker B: And also on the range Mzi star tensor, the identity is unitarily equivalent to Ti, because the unitary is now just v. But you think of it as an operator onto the range. And so this is it, right? So we have co extended. So we already have extended Ti star to MZI star tensor identity. So this coextends T to NZ tensor identity. So, modulo these computations, which I didn't do in detail, but they're not particularly difficult. This actually proves the dilation theorem in the pure case.
00:31:27.350 - 00:32:13.102, Speaker B: So it's maybe not as hard to do as you might think. Okay, now what about the non pure case? Well, let me just very roughly hint at it, because one way to do it involves an object which is of independent interest. And this is the Turplitz algebra. So, the Turplitz algebra is the c star algebra generated by these multiplication operators. So, inside of bounded operators on the duray Alison space. So it's called the Turplitz algebra, because in one variable, this is the classical Turblitz algebra, which is generated by all Turplitz operators on the hardy space with continuous symbol. And it's known that in one variable you have this nice short exact sequence.
00:32:13.102 - 00:33:00.974, Speaker B: And Arvisson proved that this is also true on the Drury Alveson space. So you have a short exact sequence of sister algebras from the compacts going into the triplets algebra going into continuous functions on the sphere, where the maps are the natural one. So the first one is inclusion of the compacts, and the second one is a continuous function, is the one that sends multiplication by zi to zi. So this shortly exact sequence contains actually a lot of information. For instance, it says that, well, the tuplets operator contains the compacts. It says that multiplication by zi is essentially normal, because it's commutative module of the compact. This tuplets Easter algebra, and it tells you that the essential spectrum of this multiplication tuple is the unit sphere.
00:33:00.974 - 00:34:03.934, Speaker B: And so this has a lot of uses, but one of which is that you can use it to prove the dilation theorem in the non pure case. And the idea behind this is, well, you want, so you start with your operator tuple t on B of H, and you want to apply the dilation theorem in the pure case. So you apply the dilation theorem to rt, and then you want to take some kind of limit as r goes to one. But you can't really take a limit off the dilations, because the dilations live on this big Hilbert space and they don't really hang together very nicely. So what you can do instead is you can use abstract ideas from dilation theory to reformulate this. And the crucial point is that you get a map, let's say, rho r on the tablets algebra into b of h now, so into the small Hilbert space, which sends, let's say, mzi to r times ti. And this map is what's known as a completely positive map.
00:34:03.934 - 00:34:57.470, Speaker B: So, so it's completely positive. I'm not specifying it completely here, but there is a completely positive map that satisfies this, and you can arrange it to be unital as well. And so here you can take a limit, then you can take a limit as r goes to one, because this map only goes into the small Hilbert space. And then you can dilate that map using what's called the steinspring dilation theorem. And then you need to know, what do these representations of the turbulence algebra look like? And this is the key point that every unital star homomorphism from the triplet CStr algebra into b of k splits as a direct sum of two representations, one of which comes from the compact operators and one of which comes from the continuous functions on the sphere. And so the representations of the compact operators, they are very simple. They are all unitarily equivalent to direct sum of the identity representation.
00:34:57.470 - 00:35:55.644, Speaker B: So this PI one of mz is a direct sum of copies of multiplication by z, and this PI two of mz comes from continuous function of the sphere. So this gives you spherical, unitary. So the point I want to make here is that this sister algebraic statement about the turplate sister algebra is one way to explain where the two different pieces in the dilation come from, right. One of them comes from the first bit in the short exact sequence, and the second one comes from the second bit. Now this is also a good place to mention the essential normality conjecture of Arvisson. So yesterday we had a talk by Yi Wong, where she explained a lot of this much better than I'm going to, I'm just going to briefly skim the surface here. So the basic fact that motivates this is that this multiplication tuple, as I mentioned, is essentially normal.
00:35:55.644 - 00:36:38.748, Speaker B: So the commutators of Mz with the adjoint are compact. So this is encoded in a short exact sequence, but you can prove it also using elementary means. And then suppose you have a homogeneous ideal in the polynomial ring. And to avoid trivialities, let's assume it has infinite co dimension. So this means by the Neul Stellen sats that in the vanishing locus there's a point which is not the origin. And then what you can do is you can take your multiplication tuple Mz and you can compress it to the orthogonal complement of the homogeneous ideal I. And because I is an ideal, it's invariant under multiplication by z.
00:36:38.748 - 00:37:20.444, Speaker B: So this, this tuple Si is still a commuting tuple, it's still a commuting tuple of role contractions. And what Arvisson conjectured is that it's also essentially normal. This is not obvious, because in general, restrictions of essentially normal Arpidus to invariant subspaces are not essentially normal. So it's something that needs proof. And as I explained yesterday, he had a number of motivations for this, but one of which has to do with the short exact sequence that I just mentioned. Namely, if this conjecture is true, then you get a similar short exact sequence for the triplets algebra generated by si. So this Ti is the triplets algebra generated by, by si.
00:37:20.444 - 00:38:07.524, Speaker B: So saying that this is essentially normal says that the quotient by the compact is commutative, and you can work out what the maxima deal space is. It's the vanishing locus of the ideal intersect with the unit sphere. And so you get this short exact sequence, and I mean, very roughly speaking, Arvisson's vision was to connect operator theory of this tuple si, and of the turplates algebra generated by Si to the algebraic geometry of the vanishing locus of. I also, as I explained yesterday, there's actually a stronger conjecture. So it's not just about compactness, but the commutator should belong to some schotten class. And then there's a refinement about which schatten class it should be due to Douglas. So sometimes people also call this the August and Douglas conjecture.
00:38:07.524 - 00:38:52.796, Speaker B: So there are lots, so this is still open, this conjecture, but there are lots of partial results. And I just picked out one, which is due to English and Eschmeyer and independently due to Douglas Tang. And, and it says that if you have a radical ideal, so this is an ideal by the neolster and Sadz, which is completely determined by the vanishing locus. And if you assume that the vanishing locus is smooth away from the origin, then this Arvisson conjecture is true, and Si is essentially normal. And actually this stronger Arvisson divers connector is true. So as I said, this is just skimming the surface. If you want to know more about this.
00:38:52.796 - 00:39:35.848, Speaker B: Well, hopefully you were at east hoc yesterday. But if you want, and you want to know more about this, then check out her talk on YouTube. And so she also explained a lot of other partial results that are known, but in general, it's still open. All right, let me also mention functional cacos as another application of this dilation ideas. So you can rephrase the von Neumann inequality for the dualison space in terms of functional calculus. And the way this works is like this. So let's denote by ad the norm closure of the polynomials in the multiplier algebra.
00:39:35.848 - 00:40:26.326, Speaker B: So you're supposed to think of something like the disk algebra. If d is one, then this is really the disk algebra. And so you can rephrase this phenomenon inequality as saying that if you have a commuting row contraction on a Hilbert space, then it admits an ad functional calculus, which means that you have a unital contractive homomorphism on ad that extends the polynomial functional calculus, because these tuple, these operators commute, you can plug them into ordinary polynomials, of course, but then you want to, want to do more. And actually, this is a completely contractive homomorphism. So it's contractive at every matrix level. And this is really, this falls immediately from the Vonnemann inequality, because the fundamental inequality says that it's contractive on polynomials, and then it just extends by continuity. Now, if you're familiar with ARPU theory in one variable, then you know that this is not the end of the story.
00:40:26.326 - 00:41:17.054, Speaker B: Far from it. For many operators, you can extend this functional calculus in one variable to an h infinity functional calculus, which turned out to be very important, for instance, when you try to prove that certain operators have invariant subspaces. And so there is a theorem of cluatro and Davidson which does exactly this, namely for completely non unitary committing role contractions. So completely non unitary just means that you don't have a spherical unitary summit, which is really a harmless assumption, because a general commuting role contraction splits as a direct sum of a completely non unitary bit and a spherical unitary bit. And so the spherical unitary bit, you understand using the spectral theorem. And so what's left is the completely non unitary bit. So this is the one that you want to study.
00:41:17.054 - 00:42:02.884, Speaker B: They showed if you have such a completely nonunitary committing role contraction, then it admits a multiplier functional calculus. So you can extend this map that you have on ad by the von Oman inequality to a weak star continuous homomorphism on the whole multiplier algebra. So the multiply algebra b of H turn out to be dual spaces. And so you have continuity with respect to both weakstar topologies here. So if D is one, then this is the denarc voyage h infinity functional calculus that you might have heard of. There's also different proof of this Galatro Davidson theorem, which I did with, with Kelly Bickle and John McCarthy. And so, if you want to learn more about this, you can either look at the paper of Ken and Raphael, or at our paper.
00:42:02.884 - 00:43:17.474, Speaker B: Okay, so I could talk a lot more about the operator theory, but since this is a conference about function spaces, I thought I'd spend a little bit more time on the function space aspects of it. And so this last part today is mostly to warm you up for tomorrow when I'm going to talk about these complete pic spaces and what the role that realis in space plays there. So, let's start at the beginning of the story, which is over 100 years ago, when Pik proved an interpolation theorem in the unit disk. And independently, this was done by Nemana a couple of years later. And so the interpolation problem they looked at is you have n points in the disk. So these are your interpolation nodes, and you have n targets, lambda one up to lambda n, and then the question is, can you find an h infinity function that solves your interpolation problem and has supernorm at most one and pick proof that you can do this if and only if this hermitian n by n matrix, which is written here, is a positive semi definite matrix. So we saw this theorem yesterday in Jada's talk, for instance.
00:43:17.474 - 00:44:29.584, Speaker B: So, I mean, the usual comments I make at this point is first of all, you don't have to assume that the targets belong to the closed disk because this is encoded in positivity of the pig matrix. Because if the pick matrix is positive semi definite, then the diagonal entries are non negative, which exactly encodes that the lambda Isf marches at most one. And the second comment is that this is so it's a non trivial interpolation problem because you want your function to be holomorphic and bounded at the same time. If you drop the boundedness assumption, then you can just do lagrange interpolation with polynomials, and if you just want it to be bounded and real smooth, then it's also easy to do. But having holomorphic and bounded as a requirement is what makes it not trivial. Okay, now of course this theorem predates Hilbert's basis, but there is a reproducing Colonel Hilbert space interpretation of this, which Tondell would be very influential and I think in this focus program. This is fairly easy to motivate because we should think of h infinity as the multipliers of h two, and we should think of the supernorm as the multiply norm on h two.
00:44:29.584 - 00:45:39.194, Speaker B: And then the connection to an abstract reproducing Colonel Hilbert's base theory is the following basic fact. Namely, if you have any reproducing Colonel Hilbert space, then you can characterize the multipliers in terms of positivity of certain matrices that look like the pig matrix. So the statement is that if you have a function, then it has multiplier norm at most one if and only if for every finite set f in your domain. These n by n matrices that kind of look like the Pik matrix are positive semi definite matrices. So if you take as an example, if you take the reproducing kernel here to be the zego kernel, one over one minus z w bar, and you just look at your interpolation nodes, then this precisely becomes the pig matrix. So this proves one direction in the pig theorem. It proves necessity, right? Because if you have an interpolation, if you have a solution to the interpolation problem, then there's this function phi, and then this basic multiplier criterion says that all the pig matrices of this function have to be positive.
00:45:39.194 - 00:46:09.950, Speaker B: And so in particular, the pig matrix at your given interpolation nodes has to be positive. Saracen observed that you can also turn this around and you can use comitant lifting, for instance, to prove the pig theorem. In fact, Saracen did this before the general Compton lifting theorem was around. So he proved the first important special case. Okay, so the usual joke is that good theorems never die. They only become definitions. And so this is exactly what happens here.
00:46:09.950 - 00:46:50.900, Speaker B: You can turn this into a definition. So if you have a Hilbert, a reproducing kernel Hilbert space with reproducing kernel, then we say it's a pick space. If picks theorem holds true. So if whenever you have endpoints in your domain and end targets where the pig matrix is positive, semi definite, where the pig matrix now involves this reproducing kernel, then the multi interpolation problem has a solution. So you can find an interpolating multiplier that has multiply norm at most one. And it turns out that things become cleaner if you don't just assume this for scalar targets, but for matrix targets. And this is what a complete pick space is.
00:46:50.900 - 00:47:37.842, Speaker B: So a complete pick space is one where. Well, this is true for matrix targets. And sometimes it's also convenient to say that k is a complete kernel. So you can either talk about the Hilbert space or about the kernel. It's the same thing. So you might say, well, why should we take this particular theorem and turn it into a definition? It's maybe not the first theorem from complex analysis that comes to your mind, but I think in this case, the proof of the pudding is really in the eating. And so there are interesting spaces in this class, in particular, spaces that people cared about before anyone talked about complete pig spaces, such as the Dirsley space that we learned about in Tom Ransford's lecture series.
00:47:37.842 - 00:48:38.856, Speaker B: And this point of view gives a new handle on these objects, which has, I think, been quite useful. Okay, so let me finish up for today by reformulating this pick condition, and then we're going to pick it up there tomorrow. So there's a reformulation of this pick condition, which I find fairly useful, and it involves restricting your reproducing kernel Hilbert space to a subset. So the idea is, if you have a reproducing kernel Hilbert space on a big set x, then for every subset, you can just restrict your functions to the subset and give it the quotient norm. So you say that the norm of g on the restriction is the inf over all norms of functions on the big set that restrict to your given function g. And then this is a reproducing Colonel Hibbert's based on y, whose reproducing kernel is the restriction of the kernel on the big space to y. And the point is that it's very easy.
00:48:38.856 - 00:49:23.284, Speaker B: It's essentially taught logical from the definition that if you have a multiplier on the whole thing and you restrict it to a subset, then you also get a multiplier on the subset and the norm can only go down. So this restriction map is a complete contraction. And the pick property is exactly saying that this restriction map is a quotient map. So h is a pick space if for every finite set, this restriction map is a quotient map, and it's a complete pick space if it's a complete quotient map. So quotient map means. Well, usually quotient map means the open ball gets mapped into the open ball. But in this case it doesn't matter if you say open ball or closed ball because you have weeks are compactness.
00:49:23.284 - 00:50:14.682, Speaker B: So it's the same thing. So you can just think of the closed ball, which is a bit easier to think about. So why is this the same thing? Well, it just follows from chasing definitions, because if you have a finite subset of your domain, and if you have so a function on this finite subset is nothing else than you specify the values for the endpoints in your set and saying that your function has multiplier norm at most one on the subset by this basic multiplier criterion ahead. On the other side, it's just the same as saying that the pig matrix is positive. So this condition says that if you have pick interpolation data on f, then it lifts to a multiply on the whole thing. So because of this, you can think of this pick property as some sort of localization property. It says that local means you define on finite sets.
00:50:14.682 - 00:50:35.984, Speaker B: So if you have something that looks like a multiplier and it's defined on finite sets, then you can lift it to a bona fide multiplier on the whole thing. Okay, so I'm going to pick up here tomorrow and talk about, well, what these pick spaces are good for and also how they relate to the durialisin space. So that's it for today.
00:50:37.644 - 00:50:56.188, Speaker A: So Michael, questions, comments, suggestions? If there are no questions, let's thank Michael again. The next talk will be in nine minutes.
00:50:56.316 - 00:50:56.644, Speaker B: Thank you.
