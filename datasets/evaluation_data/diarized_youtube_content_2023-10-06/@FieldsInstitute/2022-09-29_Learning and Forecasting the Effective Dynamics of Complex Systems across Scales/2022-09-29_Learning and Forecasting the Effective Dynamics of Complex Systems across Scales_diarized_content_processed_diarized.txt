00:00:00.120 - 00:01:03.274, Speaker A: Yes, thank you for your introduction. So today I'm going to start with a small introduction and motivation about the subject that we're going to be talking about. And we know that there is an abundance of deterministic and stochastic multi scale systems where they involve, where the associated scales in the dynamics are spanning multiple spatial and table scales from the climate to ocean currents, turbulence, and of course even in proteins. This is just some examples, but there are many more and that they span different spatial resolutions, basically that we need to resolve. And the dynamics are expensive to simulate or challenging to forecast due to chaoticity or due to the difficulty capturing all the associated effects that span this time, these scales. The question that we try to answer is if we can design fast multiscale methods that reproduce the system dynamics. Examples that are we try to see from an abstracting lens.
00:01:03.274 - 00:01:37.514, Speaker A: But basically there are many examples that fit to this framework. For example, Les and runs in turbulence, surrogate models like DMD, course grading in molecular dynamics, etcetera. So the main topic of the talk will be how to develop data driven frameworks for learning and forecasting effective dynamics across case. So we will start with regarding neural networks and a very, very small intro of how we use them in practice. So we assume that we have data from an observable of a dynamical system. We cannot observe the full state space of the dynamical system. We only observe some parts of it which may be noisy.
00:01:37.514 - 00:02:03.914, Speaker A: We do not know the underlying dynamics, but we want to model the system and have a surrogate model, a reduced order model or surrogate model for its dynamics. So we discretize the model. This is an assumption. Of course you can generalize even to continuous to the continuous case. But we assume that there is a model and there is a forecasting rule. The forecasting rule is that the next observation comes from the previous observation, plus sometimes derivative and the dynamics. And this is an assumption.
00:02:03.914 - 00:03:06.042, Speaker A: This is a forward Deutero scheme, but you can generalize. And the main assumption inspired by the Tokins theorem is that we can encode the history of what happened before in this dynamical system, incorporating time delay bendings of this observable in some large scale vector H. And an RNN fits to this lens that we use and we can unfold the RNN in time. The RNN is responsible for updating the history through its hidden state and making predictions. So this is the main framework, the main assumptions that we are going to be using in this talk. Of course, the first RNN that was introduced was LMR in 1990 that includes a hidden to hidden mapping and a hidden to output mapping, where you predict in regression either the next time step or some form of derivative, some form of dynamics, depending on the discretization on the integration scheme you use. But the main important limitation is that you have here a nonlinear activation function, which is the tanib hyperbolic.
00:03:06.042 - 00:03:37.920, Speaker A: But inside there are only linear relationships with respect to the input and the previous hidden state. And this is of course a limitation. And this is an architecture that has weights that need to be learned. If you just randomly initialize the weights, you will not reproduce the dynamics. You need to learn the weights such that when you let the system running autoregressively, you reproduce the dynamics of the system. So training these networks, fitting the weights to the data is difficult, which is a very famous problem. Vanity gradients, basically discovered by Schmidt, Huber and hochreiter.
00:03:37.920 - 00:04:23.534, Speaker A: So much of the success of RNN's is owed to the long shelter memory architecture. I will not go into much detail, but that's what we will be using and referring to throughout the talk. Now, we have an example, which is for example a Kuromoto Sebasinski system. I will not go into the details because we already saw yesterday in the talks of Professor Oth, examples from the system. Basically it's a fourth order PDE. We discretize the PDE and we solve it using stifod method. And by controlling this bifurcation parameter lambda tilde, by increasing the lambda tilde, we see a much higher chaoticity, a larger Liapunov exponents, etcetera, etcetera.
00:04:23.534 - 00:05:23.096, Speaker A: So what do we do? Actually, we construct an observable, an observable to mimic any realistic scenario. We don't know the full state of the system, we only have access to partial information about the system state. So we simulate, for example, the system, and we assume that this is the real system that looks like that. We do singular value decomposition or PCA, or any dimensionality reduction scheme, and we only keep, we throw away modes with low energy and we end up in this example here, just for the reference of this application, we only keep 20 modes, which is our observable. And this is how the observable evolutionary dynamics look like. Now, predicting this observable is not only difficult because of chaoticity, because the system is already chaotic, but also because we do not have access on the full state information. And just because we throw out modes with low energy, that does not mean that these modes do not play a very big fat factor in the dynamics of the system.
00:05:23.096 - 00:05:50.474, Speaker A: Low energy does not mean, it just has to do with the variance of the data, not the dynamics. So this is, we're going to use this dynamic data. And now how are we going to forecast on unseen data? So we start with a data and a model we train, which is like identifying the hyper parameters. I will not go into much details also here. And we take the optimal set of hyper parameters. Now we're going to use this parameters to predict the dynamics. On unseen data.
00:05:50.474 - 00:06:32.812, Speaker A: We have an unknown state space evolution, a learn out trajectory. We do singular value decomposition, and we have the true SVD mode dynamics. We assume that we know some certain history, because we attempt to reconstruct the full state space information by the history, by the time delay embedding basically, and we feed it through the LSTM. So we feed the sort of history. And now after we fit the history, we do, and we have trained our network, we take the train network and we autoregressively predict the next state. So at the beginning we have accurate predictions, which we can fairly well reproduce the dynamics. But after some time, of course, predictions then diverge.
00:06:32.812 - 00:07:06.490, Speaker A: Why does this happen? We see here an example and we see here, this is not so chaotic. This is very chaotic. For the fully turbulent cases, it's very chaotic. We see that the predictions immediately diverge from the attractor and they are completely unrealistic. For the other case, for the first row, we see that up to some extent, we can predict the system fairly well. So what happens? So the problem is that iterative prediction error accumulates, leading to unphysical predictions, divergence from the attractor, completely unphysical, very large numbers, et cetera, et cetera. The dynamics are underrepresenting the training data and specific into the attractor boundaries.
00:07:06.490 - 00:07:41.756, Speaker A: It's very difficult to assemble the attractor at its boundaries. Also, we have under resolved high dimensional dynamics, and maybe, probably models are not generalized. We have some distribution shift, for example, between training a test data. So how can we capture the long term prediction, the long term behavior? So the idea here is to use a minstrel Hastik model, which is a model that is based on an Ornstein Nullenberg process. We fit these parameters, it has two parameters, I will not go into details. We fit these parameters in data, the decorrelation time and the data set of deviation. And it relies on global attractive statistics.
00:07:41.756 - 00:08:15.762, Speaker A: Basically, we use the LSTM. The LSTM is used for local regions where we try to capture the local dynamics. And we use the mistochastic model for the large scale behavior. So in the long term, we have a guarantee by design, by construction that the min stochastic model will converge to the statistics of the system. So, and it's also very efficient and effective in highly cautious systems. And what we proposed is this hybrid LSTM mSM. We used LSTM whenever we have data and whenever we detect, we want to predict on a new data point that is not closed or training data.
00:08:15.762 - 00:09:09.294, Speaker A: We use the Min stochastic model. And indeed, if you plot the error here is the root mean square error on the first mode, and we see here we plot the sad deviation of the attractor and of course our error should converge on the long term. In this deviation, we see that if you use first of all, we also compare with gaussian processes, we see that LSTM variants are better have lower error than gaussian process. There's a lot of details on how we perform the study, but basically LSTM is a lot more data efficient. You can process a lot more data and the plane LSTM will diverge. Whereas when we couple the LSTM with MSM in this blue dotted line, we see that on the long term we will convert to the invariant mesh, to the error of the attractor, to the salivation of the attractor. And of course here we plot, it's averaged over 1000 different initial conditions.
00:09:09.294 - 00:10:25.324, Speaker A: So a mitigation that we proposed is a hybrid LSTM approach. But another very important problem is the problem of vanity gradients, which means that gains during bug propagation vanish to zero or explode. And there are many sophisticated architectures proposed recently to tackle this problem, from the long short term memory and gate the recurrent units to reserve computers, where you had many, many talks about this issue, also to unitary RNN's, where they try to lift, to use complex unitary matrices. So to do like machine learning on the complex space, to tackle the problem, which is basically use a recurrent complex unitary matrix, that you have a guarantee that its eigenvalues are on the unit cycle. So a very short illustrative, without going into too much detail, a very short illustrative idea why these gating architectures are effective in mitigating the problem. Now, if you unfold the LSTM in time, you have an uninterrupted gradient flow back in time, just a very short detail on how they try to actually cope the problem. You only learn what you need to learn to update the information, but always you guarantee that you have an interrupted gradient flow and that's how you ensure that gradients do not finish to zero or explode.
00:10:25.324 - 00:11:19.064, Speaker A: So we took this architecture and we do a benchmark on another system, a Lawrence 96 system, with either observing the 40 modes, the full state information, or 35 out of 40, throwing out five modes, the five modes with the lowest energy. As a prediction metric, we use the valid prediction time, which is just how much time we can predict, normalized, accurately normalized by the largest level of exponent. I don't have much time to explain, but the main purpose is the higher the better. And what we see here with blue is reservoir computers, GRU LSTMs and unit lnns. And when we observe the full state information of the system. So when our observable is four dimensional, we observe the full state of the system at every time. Step reservoir computers here, it's a violent plot, but the main point is that reservoir computers show much better performance in case of complete full state observed information.
00:11:19.064 - 00:12:26.862, Speaker A: And also that they also, if you plot the performance on the training dataset versus the performance on the test data, you have an idea about generalization error. Ideally, the best models that generalize very well should be on the y equals x on the diagonal line. And we see that they are fairly generalizing fairly well. However, on the reduced order case, where it's actually more relevant in practice, when you don't observe the full state information and you have a very complicated observable, constructed by singular value decompositional modes, which might not be. No, smooth, which might not be smooth, then we see that gated architecture you see here have better performance, but also by this, a very thin line. You see that it's very difficult to find hyper parameters that actually achieve this high performance, because it's a much, much more difficult task when you have reduced order information to capture the dynamics. And what we also find out when we plot the generalization plot, is that reservoir computers are expressive, which means the performance in the training data is fairly big.
00:12:26.862 - 00:13:12.714, Speaker A: But it's a problem of overfitting. You see that even though reservoir commuters on the tiny data have similar performances to GRUs and LSTMs, they are not overfitting as good as GRus. So it's probably we are hinting more towards identifying better mechanisms against overfitting the server computer and not just the regularization. So, gated architectures, in our study, we found out that they are more robust against overfitting. And this is a study with thousands of models per architecture. So. And yeah, as I told you, like regularization procedures utilized in bug propagation through time, and gate architectures, which is we utilize zone out, dropout, etcetera, are more effective according to our study.
00:13:12.714 - 00:14:06.172, Speaker A: So with that, I'm going to proceed to the second part of the talk, which is about the framework of learning effective dynamics. And this is a framework based on the equation of frame framework by Janis, by professor Kiev Kides. And the idea, briefly, is that in complex multiscale systems, we assume here, for simplicity, that we have two scales, the micro scale and the macro scale. For example, the microscale can be particles and macro scale can be a continuum mechanics or continuum dynamics. So the micro scale simulations are accurate, but very expensive to evaluate, or you might not have access to equations, whereas the macro scale simulations are approximate, but they are inexpensive to evaluate. With that may be some coarse grained model that you have, or some access to, some observable or some access to some characteristic function that you care, for example, the drug in a simulation. So there are many, many works, previous works by Professor Kevorkides and his group about this.
00:14:06.172 - 00:14:48.762, Speaker A: So the idea is that you propagate for short times the micro scale. You use a restricting or an average operator to get, which can be data driven operator, diffusion maps, etcetera. Or it can be even analytic, like identifying a characteristic metric that you want, and with that, you initialize the macro scale. You use a propagator for long times, which again can be analytic, an OD or a data driven propagator. Then you return to the micro scale using a lifting operator. Then you propagate for short times again, and you go back and forth, and with that you try to accelerate the dynamics. The idea is that whenever you know the dynamics of the system, you use a macro propagator.
00:14:48.762 - 00:15:37.036, Speaker A: Whenever you are trying to extrapolate and discover something new, you use the microscope. So the generalization of more complex problems is hindered in this framework by the macrodynamics propagator, because it's not clear how to identify a macrodynamics propagator. And a very complicated procedure is how to go from the macrodynamics, the less information to more information, to the microdynamics. So luckily, recently we have many operators from machine learning that can identify these operators automatically in a data driven manner. We have convolutional auto encoders, optional convolutional, depending on the application. We have mixed density networks that can cope with stochastic dynamics, and we have recovered neural networks. And actually we can use recurrent neural networks as macro dynamics propagators.
00:15:37.036 - 00:16:40.110, Speaker A: We can use convolutional encoders and mix with these networks for restricting, averaging and lifting operators. And the idea we tried out is to replace here the operators used in the equation free framework, with operators from machine learning architectures. And that's in the end our framework. So we applied that to Kuromoto Sivasinski again described before. And we know from previous works that the dynamics of the system lie on an eight dimensional manifold, even though you have a 64 dimensional observable, we know that the effective dynamics, we know there is a manifold of eight dimensions that describe the dynamics, can be mapped to this manifold. So indeed, if you perform an analysis on the reconstruction error based on the latin dimension, it could also be a prediction error. But for now, for simplicity, we here present reconstruction the same plot, you can get it for prediction.
00:16:40.110 - 00:17:23.060, Speaker A: Also we see that the autoencoder and PCA and the convolutional autoencoder, they all saturate after eight dimensions. We see here that here is the construction error with respect to the latin dimension. If after eight we have a saturation. And we also see that autonomous are better than PCA. For this specific example, the question is if we can identify this observable, this eight dimensional manifold, and led can indeed identify and reconstruct the dynamics in this eight dimensional manifold. You see here on the middle, the dynamics of the latent state and the reconstruction. Of course we don't expect to predict well, because now we are reducing the dimensionality and then going back and of course cautiousity will prevail.
00:17:23.060 - 00:18:03.536, Speaker A: But we want to capture their climate, the large scale statistics. And here we see that we are indeed reproduced the statistics and the long term climate. And of course, what we can do now is switch between the equations and the system. Of course, this is just a proof of concept. We have more results which better represent this switching mechanism. But actually, if you just use the macro propagator of led, you can achieve up to two orders of magnitude speedup. But by switching between the equations and the macropropagator, you can achieve less speedup, but you also can achieve less error.
00:18:03.536 - 00:18:50.974, Speaker A: So you have an interplay between error and speedup. So we also tried this approach in the flow behind the cylinder. The micro solver is a finite difference solver, and we have here the velocity and the pressure and the vorticity as a state. We try led with only four dimensions, and we see that led can capture the long term evolution of the fields, whereas also is two orders of magnitude faster than the solver. And of course, by switching between the solver and the microscope and the macropropagator and LSTM, basically we can reduce the approximation error at the cost of reduced speedup. And we also recover the drug with lower error. And we apply the same method now to a more complicated case, a flow behind a cylinder at rayon number 1000.
00:18:50.974 - 00:19:26.846, Speaker A: And we can indeed take the similar results. And what we find out is that most error is concentrated around the cylinder. And we are trying to alleviate this problem by doing adaptive mesh refinement, machine learning, which is the steps that follow. We also benchmark against the different macro propagators. Of course, this is a general framework, but you can change the propagators. Instead of an LSTM, you can use reservoir commuters or even cindy. And we also did a benchmarking among different propagators, where we find out that LSTM is the most, is the most, offers good performance without having to tune a lot of hyperparameters.
00:19:26.846 - 00:20:13.934, Speaker A: And it's more versatile and more robust, as we're going to see later. So I'm going to very briefly talk about another application where we can apply the same framework with slight changes. Instead of using long sort of memory networks, we augment them with mixer density networks. And we can also do stochastics. So, because here we, we can train everything with bug propagation through time or other more variational, for example, variational algorithms for training the neural networks, we can also do stochastic systems. Here we try to apply the same method in the dynamics of alanine depthida, which is a small molecule. And what we see here is that we use molecular dynamic solver to run for some time.
00:20:13.934 - 00:21:25.268, Speaker A: We take the data and we try to see if led learning on this data can produce dynamics again. This is another paper with Professor Julia Zavadlav and Professor Praprotin and Professor Kumutzakos, where we actually saw that you can use this same framework, with minor modifications on the architectures to actually sample the large scale characteristics, the course characteristics of a small molecule. And there are many, many more problems that we need to solve to make it more efficient. But this was a proof of concept that it can even work in stochastic systems. So for future research, we are trying to investigate alternative architectures. For example, wavelet filters and do wavelet auto encoders, multitask scales required neural networks. In terms of the framework, we would like to be able to do adaptive online learning with uncertainty quantification, add parametric dependencies of physical parameters, incorporating system properties into the architecture, for example, energy conservation, et cetera, et cetera, and do adaptive resolution, machine learning.
00:21:25.268 - 00:22:32.124, Speaker A: And this will enable us to do more applications scaled to very high dimensional problems like 3d fluid flows employ this framework for reinforcement learning and also apply adalaid with adaptive led on real world experiments. Of course there are many challenges here. The idea is to use adelaide on experimental settings but there are many many challenges until we are able to cope with this problem and also climate data in the form of the digital wind paradigm and we are now developing a framework the preprint will be uploaded in the next weeks which is called adelet which is adaptive learning of effective dynamics which is adaptive scalable parametric and it contains asset codification. It gives you a way to know when to transition between micro and macroscope. A very small example again the system we saw before the flow behind the cylinder and here we plot the green lines is the fragment. So we have the simulation and the digital twin running in parallel and the surrogate model running in parallel and here we have the fraction of the time that the surrogate model is used. In the beginning we start the simulation.
00:22:32.124 - 00:23:30.940, Speaker A: The surrogate is not used because it does not know what to do. It tells you that I don't know what to do use the simulation. So here the simulation is used and here we plot also the error and after 2000 time steps we start. This error is of course the error of the surrogate which is not used. After 2000 time steps the surrogate is confident that it should start to get used here in the middle is the macro propagator and here the Reynolds number is 500 and now there is an although the surrogate here it's used suddenly the Reynolds number changes to 1000 with this change in the Reynolds number we want to mimic some distribution shift in the data and again so the surrogate automatically detects that it should not be used anymore. I don't know what I'm doing, I don't know the dynamics and then again after some time it is relearning and once it's confident again it's getting used again. So in this context we are doing adaptive machine learning.
00:23:30.940 - 00:24:05.124, Speaker A: We detect when to use when we should be using the surrogate and we switch to the surrogate only when we are confident in this way. That's pretty cool because you can prescribe an error threshold on your time steps. For example I want to do an error less than 1% and by this of course you don't achieve orders of manual speedup. The speedup is reduced but at least you are sure that your surrogate is only getting used when it knows what to do. It has training data around it. Of course there are many many more challenges to solve here where we discuss about them in the paper. For example, one of the most important is catastrophic forgetting.
00:24:05.124 - 00:25:06.206, Speaker A: Once we are relearning the dynamics from the different Reynolds number, we should make sure that we don't forget the previous one, and indeed we have ways to cope with that and we have many more interesting results towards this direction. Adaptive led that we try to alleviate the problem of distribution shift, which is very relevant in practice when dynamics are changing over time. So this we I am also working on a spin off from our lab which we are trying to cope with similar problems, real time adaptive continuous learning and automatic detection of distribution shifts. So we have many more interesting problems coming from this sector from the associated sector. So, a short overview slide we discussed about hybridizing lstms with min stochastic models. We discussed about different RNN architectures and the problems they solve. We introduced led based on the equation free framework of Professor Khe kiddies.
00:25:06.206 - 00:25:25.184, Speaker A: We discussed briefly discussed an application of led to proteins and we also applied lstms on clojure modeling. And in the future we are working towards Adalaide which is adaptive real time and continuous learning. And with that I would like to thank you for your attention.
