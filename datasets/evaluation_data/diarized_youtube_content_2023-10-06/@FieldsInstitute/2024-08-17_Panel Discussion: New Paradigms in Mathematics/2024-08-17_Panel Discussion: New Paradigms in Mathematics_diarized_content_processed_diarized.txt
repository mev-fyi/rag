00:00:00.720 - 00:01:01.870, Speaker A: Okay, so welcome, everybody, to the new paradigms session this afternoon. And while we're going to be considering various new paradigms in mathematics, of course, a main one that we all sort of approach here is AI and mathematics. And so the two talks that we've just heard, these are, of course, two cases of pure mathematicians, pure and applied, who've engaged extensively with questions about learning. And so we're going to kind of continue this topic. We'll try to avoid diving into too much technicality, especially too much on the machine learning side. And I won't sort of take up a lot of time here because it's a very short session and we have lots of things to cover. So a first question, and each panelist, as they speak, will just briefly explain their connection here.
00:01:01.870 - 00:01:58.280, Speaker A: And sort of a first question I'd like to pose to the panel. It's just, if you want to comment on the interaction, and this is maybe especially for Misha Belkin to start with, interaction between machine learning and mathematics. And so here I'm kind of following up on yesterday's session in theoretical computer science, and it's a part of theoretical computer science, which deals with learning theory and just sort of area of machine learning that's mathematical machine learning. So I guess I'm kind of interested in comments about this, the role of interactions between mathematics and machine learning, especially as we look at how AI systems might be used in mathematics. So, Misha, do you have any comments on this?
00:02:00.340 - 00:02:47.560, Speaker B: Sure. And I guess I'm a little bit of an. Thank you, Maya. And I should say, very nice to be here. It's too bad I couldn't come to Toronto in person. I actually went to U of T for my undergrad, and my parents still live in Toronto. Yeah, I guess I'm a little bit of an outlier in the sense that I kind of view myself not so much as a mathematician, as a physicist, in some sort of physics like kind of effort, trying to understand the real phenomena that we observe in sort of empirically, the phenomenon that we can learn the things given examples effectively, the phenomenon of learning using mathematical methods.
00:02:47.560 - 00:03:24.420, Speaker B: So it's very interesting for me to kind of see that both ways. And perhaps very unexpectedly recently, it became so powerful going the other way around, not just from using mathematical methods to understand something about learning in this new phenomenon of learning, but also that this pretty much immediately become useful for mathematics itself in this pretty non traditional, this machine learning techniques, at least non traditional in the context of mathematics. Just some thoughts, maybe, to start the discussion.
00:03:26.160 - 00:03:40.260, Speaker A: Thanks. Michael. Do either of you want to comment on sort of how you see in the development of AI tools that are useful in mathematics, how you see the kind of interaction with machine learning and with machine learners?
00:03:43.530 - 00:04:25.410, Speaker C: Is my mic working? I think it's not, is it? It's on. Okay. Yeah. So, my name is Heather McBest. I am a pure math edition by training. And the reason I guess I'm here is that in the last three or four years, I've also started to work a lot on computer proof assistants, both on building libraries for them, but also on building tooling for them to make them more useful to mathematicians. So this is not AI in its own right, but of course, it intersects with it in the sense that it's the language of mathematics that is most accessible to machine learning, as David already mentioned in his talk earlier.
00:04:25.410 - 00:05:06.370, Speaker C: Yeah, I think it's a natural problem, and I would say it's not only natural that as mathematicians, we'd like to see whether these tools can be helpful to us. But I think we should also acknowledge that, of course, as mathematicians, we think math is great. But I think even without the bias of being a mathematician, I think it's clear that it's a great problem for machine learning people. I think we shouldn't be surprised that people who work on machine learning should identify the question of trying to solve mathematics, whatever that means, as a real holy grail for their fields. So whether or not it becomes useful for us as mathematicians, I'm sure that it will continue to inspire people who work on machine learning.
00:05:09.110 - 00:06:04.562, Speaker D: I'm Robbie McGill. I'm an algebraic geometer, and I guess I'm here as maybe mostly an outsider, as a lay pure mathematician with some. With some minimal experience with lean, and I've attended a lean workshop, and I've been interested in indirectly watching what's going on. So also. But I think, speaking from what's happened so far, I feel as though nothing at this stage, things are very clearly very experimental, and there's been no phase transition yet. And at this point, we're just there being some fantastic examples that are more proofs of concept, and what's needed are more key proofs of concept, where important things happen. But there are things where one should expect ten years from now, we look back and see as the first signs of what's about to happen, rather than that the things have actually happened.
00:06:04.562 - 00:07:00.804, Speaker D: And in particular, the use of machine learning, actually proving theorems is something which maybe we'll talk about later, but I feel, but is well, in its way, but is really often talked about in terms that are. It's better to discuss the many stages it takes to get there, rather than to talk about the things which might be in existence 20 years from now. Because the more interesting. Oh, I'm sorry, I'll try to speak more loudly, I guess. Let me just conclude by saying that it's that. Let me just conclude by saying that the things we should focus on are the next steps and seeding the ground for the advances we expect to come. And it's clear at this point, more and more, what ground needs seeding next in the next few years.
00:07:00.804 - 00:07:03.680, Speaker D: And these seem like the most interesting questions right now.
00:07:04.750 - 00:07:46.908, Speaker A: Thanks, Ravi. Yeah, so that would be nice to kind of. We can explore that a little bit in conversation here. Like, so, I mean, you mentioned formalization, and so there's, in principle, the possibility of getting machines to prove theorems, just going with a natural language version of the theorem. But then, of course, if you want something that's verifiable, then there's a huge advantage to have a language like lean underneath the hood. Of course, you could have a machine that's translating a natural language version of a statement to lean and then back again. But let's just, like, for a while, dwell on this question of formalization.
00:07:46.908 - 00:08:27.060, Speaker A: And so maybe let's talk a little bit about why we formalize and sort of in this domain of formalization, what's transformative and what is nothing. And so it's kind of an opportunity to situate formalization. And we can also do this for machine learning and this to situate it somehow in reality, like not as a dystopia or a utopia, but kind of where we're at. What are some things that we can try for and so on? Does anybody want to comment on sort of why formalize? Colin has some more kind of targeted questions?
00:08:28.140 - 00:09:12.542, Speaker E: Well, one thing that I was thinking of in that line is 100 years ago, or a little bit more. As set theory and logic are being invented, most mathematicians do not learn them in any detail, but the practice of mathematics really did become more unified and more rigorous because those ideals were being developed in some quarters. And I'm wondering what we might expect to see in terms of even mathematicians who aren't doing machine proofs, who maybe aren't using machine proof assistance, but adapting to a climate where proofs are more often written in machine readable terms. Should we expect to see mathematicians change their own work because of the presence of machines, even apart from using them? Would you expect the machines to change how?
00:09:12.566 - 00:09:13.130, Speaker B: You.
00:09:18.110 - 00:10:26.400, Speaker C: Sure? Does it work any better? Yeah. Okay, so maybe I'll even just say a little bit about what formalization is in case there are people in the audience who haven't experimented themselves. So what's being talked about here is somehow the idea of taking mathematics back to the axioms of set theory or whatever, and really expressing the logical argument that gets to whatever events you care about in algebraic geometry as a 10 million line logical argument, beginning to end axioms to your conclusion, and then sending it off to a computer program that checks it, checks it for correctness of all of the logical deductions along the way. And as you can imagine, I mean, these have existed for a long time. People experimented with them ever since the dawn of the computing age. But, you know, it's impractical for human beings to construct those millions of lines of logical reasoning by hand. And so the systems like lean, that was mentioned, and like clock, that was mentioned in David Mumford's talk, these systems now sort of auto construct most of those lines.
00:10:26.400 - 00:11:24.836, Speaker C: So they contain high level computer programs that themselves construct long chains of logical reasoning that are then checked. So what is it for? I mean, so it's as you, as you say, I mean, there's a precision that was being achieved when people were trying to do this by hand without computers 100 years ago. There's a standard of correctness that I think is also very appealing. Um, I think there's also somehow a notion of reusability. Um, there's somehow the idea that we spend a lot of time doing proofs that are, you know, small variations on, on existing proofs in some way. And having mathematics expressed in a way that can be modified automatically might, might actually save us some time in the long run. Having more flexible expressions of proofs allows for transformations of proofs and adaptations of proofs to new context in a way that frees us from having to do all that rechecking of slight variance of existing proofs by hand.
00:11:24.836 - 00:11:35.480, Speaker C: So I think for me, this is the big thing that's offered. It's a chance to outsource, to take some of the thinking out of our minds and hand it off to the computer.
00:11:38.260 - 00:12:32.404, Speaker D: Great. I guess the only thing I would add to that is there's something that initially sounds dystopian, maybe to a mathematician far from it, which is that, will this change what I do? And the answer I used to think was, well, for most people, it won't change what you do, but I realized subconsciously, of course, it will change what you do in a way that won't actually bother you. And something which makes me changed how I thought about things was the realization that the way in which we humans understand mathematics, we mathematician humans understand mathematical proof always work this way. I don't worry about the foundations of set theory, but I'm glad someone has, so someone else has, and I can use that, and I can just not worry about it. So I could give you a hierarchy of state statements of versions of truth. It isn't just, it's this is a mathematical truth, and this is not. And the question is, where would you put.
00:12:32.404 - 00:12:59.084, Speaker D: This is an open question to ask yourself. Where would you put a computer formalized proof in this? And this is not exactly, it's not an ordered list, it's a continuum, but it kind of shows the nature of proof and how, and the nature of knowledge, which is there's a statement. This statement sounds like. It sounds right, sounds like it should be true. This statement is mathematically precise. This statement is falsifiable. This statement has the proof that the statement has been claimed by someone.
00:12:59.084 - 00:13:23.814, Speaker D: There is a proof of the statement in the literature. At this point, we're very far from something that you should necessarily believe. Perhaps there's a proof in the refereed literature. There is a proof of literature that is trusted. There is one that is believed to be understood by more than a few experts. There's a proof that looks like one I could read, at least in theory, which for me is an important line where I'm not going to read everything, but I can at least believe it. But that's certainly far from the proof.
00:13:23.814 - 00:13:57.640, Speaker D: There's one that I've actually read and followed. There's one that I understood line by line. And then there's the proof that I understand in my soul, where I really understand in my gut exactly what's going on. And so, really, the fact that, you know, when mathematics changes, when there's new versions of understandings of rigor, it changes how mathematicians do business and how we think about things, and that's just natural. And I was a little bit surprised at how much being at a lean workshop changed my. The way I thought about these, these questions, and not in a bad way or a dystopian.
00:14:00.580 - 00:14:03.440, Speaker A: Thanks, Misha. Did you want to add anything?
00:14:05.460 - 00:15:03.740, Speaker B: I guess I sort of come from maybe a different direction in the thing, because rather than, you know, like using computers to do math, I would like to, you know, like, do math to understand AI in that sense, I feel I have become actually less rigorous recently because I feel that the value of an exact proof is maybe less than the value of some sort of plausible mathematical reasoning. Again, I'm saying this for physics like kind of argument. Okay. Maybe you can neglect this term. Right. Because it's a low order term or something like that. So for me, actually, I feel like it pushed me toward, like, in some sense less rigor, like proving life theorems and making more plausible arguments and checking that they somehow, like, fit with what we observe.
00:15:03.740 - 00:15:15.660, Speaker B: That's probably a very different experience from many, you know, people in the. In. From what? Like, from the directions. Many people in the room, probably the way they do work.
00:15:20.490 - 00:15:22.498, Speaker E: Yeah. And looking at it from.
00:15:22.594 - 00:15:27.830, Speaker A: We have a question. Yes, please do.
00:15:43.140 - 00:15:59.840, Speaker D: And then have a sense of that in their own mind, and then to know that there is a fruit that they haven't read in the literature, to me, that's. That's. That's all you need to be or not most of what you need. It gives you a better feeling for the whole subject, of the whole.
00:16:02.580 - 00:16:20.750, Speaker A: Yeah, thanks for bringing that up. And this mention of proof that's in the literature also touches on the community aspect, that other people have checked it, other people have talked about it. Colin, you had a question along those lines. Did you want to. Maybe now is a good time for that.
00:16:22.450 - 00:17:15.630, Speaker E: Well, yeah, in terms of checking proofs, we were. Something really exciting is happening right now, as the ten people who have proved the geometric Langlands are collaborating online, creating drafts of better and better proof, improving the proof, and improving the exposition. And they're doing that right in front of people. And because no matter how it was mentioned, Wiles working in considerable isolation, proving Fermat. But Wiles did not trust that proof himself until it had been refereed, until it had been checked. Do we envisage machines checking other machine proofs there, is that a little bit like buying two copies of the newspaper to see if they agree. What about that social aspect of will machines eliminate? That social aspect.
00:17:19.450 - 00:17:58.570, Speaker D: May develop. On Jim Arthur's point and answer this question, I do feel as though the discussion maybe in some of the earlier sessions, has been about proof and a machine, verifiable proof, which is an important part. But proof and understanding go hand in hand, and the human understanding is essential to the math, to the enterprise. We need to understand. There are proofs, such as the classification of finite simple groups, where in some sense, I'm pleased we have a proof and we do not have any gut level understanding. Maybe no one alive has a gut level understanding, and that's somewhat unsatisfactory. Whereas Tanya Mashimura, we have already a gut level.
00:17:58.570 - 00:18:45.480, Speaker D: We also want this understanding, and this is an essential part. And proof, at the entire point of proof assistance is not to supplant understanding, but to assist understanding. And so the goal is not just to have something that can beat humans at chess, but to help us play, but to help us understand the game of go better and maybe out. And that's a good. So I feel like that social aspect cannot be, and that's something which currently people talk about. This is something that AI will, that will, this will be designed, that it'll be able to explain mathematics to us. And frankly, a harder question to explain it to a 10th grader who's struggling with the subject.
00:18:45.480 - 00:18:57.980, Speaker D: This is a very hard, this is a flying car prediction. This is something which is a very hard thing to do and really much farther down the road than the more interesting questions that are possible in the next.
00:19:01.290 - 00:20:13.216, Speaker C: Yeah, I really like your hierarchy of standards of proof. I would like to say that proof is not the only thing that mathematicians do. And even if what you primarily care about is proofs, you need to make sure that the proofs address the right questions. A real problem that you can imagine in a world in which more of the task of trust is turned over to computer verified proofs, is to find out whether the proofs really prove the thing that people think they do. If you phrase a question very carefully in a logical language like lean, and, you know, and someone comes up with a many thousand line long proof that you don't read in detail, but the computer gives a check mark to, can you be even sure that the statement that the person has written down is the statement you meant? What if, for example, the statement was, there exists a function such that blah, blah, blah, blah, blah, and the person who phrased this statement forgot to exclude the case when the function is zero, of course you meant to say there exists a nonzero function such that blah, blah, blah, blah, blah. But the phrasing of the statement is subtly wrong, and so the claimed proof is totally worthless. This is not a hypothetical example.
00:20:13.216 - 00:21:05.210, Speaker C: If you look back at the proof of the Imavi problem that turned out to be wrong and that took ten years or 20 years to be fixed. Back in the sixties and seventies and eighties, the gap in that first proof of the imam problem was precisely the construction of a function that was going to have all the properties, except that it turns out that it was also zero and therefore failed the one basic property. Yeah. So I think somehow the task of humans in framing the problems is a real one, and people who've started to experiment with formalization know that the proofs are not the part that take you the longest time to construct. It's framing the theory correctly. So I think having even total verification of all statements we make doesn't insulate us from the intellectual care of framing our statements correctly.
00:21:10.870 - 00:21:36.000, Speaker E: Yeah, that was another thing that I thought about. I mean, when you, sometimes you'll prove a theorem just because you can, you've got some concepts in front of you. You think, oh, I think they imply this. You checked and. But more often you're trying to prove something that you think is important because of your overall vision of what's going on. And currently, that vision isn't put into a lean proof. They don't want to try to put that vision in.
00:21:36.000 - 00:21:56.350, Speaker E: Should we envisage a time when people, those vision people write essays trying to explain their vision? It's a great genre, but it's not constitutive of math research. Will it ever be constitutive of machines? Will we try to put the one challenge I thought of, will Langland's famous letter to Andre Weil, ever be written in lean?
00:22:04.850 - 00:22:06.390, Speaker A: If you have a comment, Michel.
00:22:06.770 - 00:23:23.570, Speaker B: Yeah, I think that's a really excellent question. I feel what has been quite unexpected about the power of these machines is that the things that we thought were somehow associated to humans writing poetry or writing essays and so on, they're remarkably good at it. And maybe this kind of, in fact, it turns out that having them do some sort of formal, at least with the models like LLMs, having them do formal computation is actually harder than for them do some sort of informal, which is, you know, often wrong. But, you know, informal things often are wrong in any case. So it may be not that far fetched to think that they will be able to catch this kind of informal goal, or like, what is it that we're trying to do? Rather than give me like a thousand, whatever million line long proof and lien I and have this very formal, perhaps not verifiable by humans formal proof. I'm just saying it's maybe not as far fetched as it sort of seems initially.
00:23:24.630 - 00:24:03.630, Speaker A: Yeah, I would just add this also sort of touches on what Mumford was mentioning about having different subsystems within an AI system. And already so, for example, the recent successes with IMO problems that use reinforcement learning in a big way. So not just LLMs. And yeah, there's going to be lots of room for combining different types of machine learning tools, and maybe some of them indeed are good at these poetic aspects and others are good at more hard logic.
00:24:05.730 - 00:24:46.246, Speaker D: So let me give another list of. Of things that can't be done to try to make the problem simpler and simpler, to see where we are and where. So I do think it would be fantastically interesting to invent a new theory, to have a new, and to talk about it. But to me, that's, again, a flying car sort of question, because here are things we can't. Here are things that currently can't be done, and I'm going to strip things down to where the current front line is, and then I feel like interesting things are the next stage after the front line and the one after that. So, inventing a new theory, a whole new theory, in proposing, proposing, you know, doing what Langlands did. That's the.
00:24:46.246 - 00:25:07.478, Speaker D: That's way. That's. So I'll start with that. Let's say, making an insightful new conjecture not there yet, making an insightful new definition, which is something that a mathematician is. That's a hard thing for a mathematician to do. I think David Mumford mentioned this, that making definitions is cheap. Making the right insightful new definition is challenging.
00:25:07.478 - 00:25:39.292, Speaker D: Next, inventing a new insightful theorem in a well bounded region of human knowledge that is not, in my opinion, happened so far. We're on its own without human help. We've done that. Explaining a theorem that already exists, I don't think a serious theorem. I don't think that's happened yet. Asking the insightful right question of the frontier of research. We're not there yet, but now we're close to where we are now, inventing a less interesting theorem in a well bounded region.
00:25:39.292 - 00:26:27.350, Speaker D: And I don't want to undergo, because that is a super important benchmark, and that's the sort of thing which is now happening. So now we're at the. So this is a sign of really exciting things that are actually happening, and then verifying a proof. I don't want to absolutely undersell that, because really mind boggling days transition. Things that happened recently, and talking about inventing a theory is getting us distracted from the liquid tensor experiment, or the prospect of geometric lengths being verifiable, which is, which is something which I think would be unthinkable five years ago. Is that a safe thing to say? Or maybe, I don't know, except for that, really, I would not have predicted. And then making hand waving arguments is something that already chat GPT can do, and just like an undergraduate.
00:26:27.350 - 00:27:09.672, Speaker D: And so that's the cheapest thing. But I think the really interesting things are begin to have computers be able to inventory interesting theorems in a well bounded region. And, for example, the geometry problem. I'm much less impressed, I'm much more impressed by the liquid tensor experiment and by geometric linguins than I am by a geometry problem, mainly because it's in a sandbox. It's a small sandbox, which was frankly guided with a lot of human hand holding to get there. So the really amazing stuff is happening, but not everything which is amazing. Some unsexy stuff is incredibly amazing, and some things which seem like they're being advertised is amazing, I don't yet understand.
00:27:09.672 - 00:27:13.560, Speaker D: I'm not yet convinced that they're amazing, but you know, way better than me, so I will.
00:27:13.640 - 00:28:13.600, Speaker C: Well, I would maybe just add one comment on your nice description of the frontier, which is that as far as I can tell, that the frontier almost is, has two connected components at the moment. There's the people who are getting early successes and having AI to have ideas, and there are people that are having early successes in getting AI's to proof theorems. And, you know, in both cases, you know, it's very, very early work. But still, I think that this two work has these two phases of work. These two kinds of work are almost disjointed at the moment. And you can, I mean, I think it's even funny to ask yourself, you know, if you could choose only one of these, which one would you rather have? Would you rather have the AI that comes up with the ideas and you do the grunt work of proving it, or would you rather have an aih that proves things that you frame yourself? Personally, I'd much rather have the latter one, but I think it'll be interesting to see how these two thread two strands of research develop currently separately, but eventually intertwining.
00:28:16.780 - 00:28:18.520, Speaker A: Misha, did you want to add anything?
00:28:20.620 - 00:28:26.200, Speaker B: No, that's a great discussion, great summary.
00:28:31.470 - 00:28:33.010, Speaker A: Was there a question in the audience.
00:28:39.790 - 00:28:40.806, Speaker D: If I heard you?
00:28:40.838 - 00:28:49.450, Speaker B: Well, you excluded conjectures. How about finding counter examples?
00:28:50.030 - 00:28:53.980, Speaker D: Ah, that's good. Okay, I guess I can't remember if I write an excellent.
00:28:54.070 - 00:28:55.272, Speaker C: Put that under ideas.
00:28:55.376 - 00:29:35.142, Speaker D: Yes, I think I will go with that. I guess what she said, and I guess certain kinds of counter examples already, it seems, are well suited. I mean, this idea that the mathematicians have been using computers to find counterexamples for decades, but we're getting better and better at it, and now machine learning is being used to find interesting, remarkable counterexamples. And that's actually pretty. Actually, that's the kind of thing conjectures seem harder I haven't heard a really good, insightful conjecture to come out. I don't know, maybe I've missed. But the counterexamples have been kind of remarkable that I've, that I.
00:29:35.142 - 00:29:36.650, Speaker D: That I've heard of.
00:29:37.670 - 00:30:00.660, Speaker C: Yeah, I. So I agree. I've been really impressed by the, the work that I've seen in the last year or two from people who are using, you know, machine learning to. To find interesting objects, to find noodles in haystacks. I. There have been several different people who have done this kind of thing, and I find this a really compelling use case. I like it a lot.
00:30:00.660 - 00:30:25.380, Speaker C: I guess I should say that. So there has, there has been some work on people who are using AI to frame conjectures. So the DeepMind work, for example, done with Jordy Williamson or on knot theory, I mean, so you can look up this work yourself. But certainly that, I think, is the closest I've seen to people really addressing head on the question of trying to frame conjectures.
00:30:28.320 - 00:31:18.060, Speaker E: Yeah, I think when I hear people talking about the liquid tensor experience, the brunery number experiment, the brunery number, it's not a framework where they're very interested in conjecture. You've got the sorry command and lean. You can say, let's consider this without committing to it, but it has to be something you can state precisely. And proof verification, per se, is a mindset where you're not exploring, you're trying to verify a proof. But that learning environment, that's a different one that I haven't thought as much about, and that does. Another question that I was thinking about here is the importance of genuine mistakes. When Lamay really thought he'd proved the Fermat theorem by a faulty argument with the fault that he could not conceive, even though the math had been developed earlier by Kumar Lemay, could never have conceived it.
00:31:18.060 - 00:31:40.340, Speaker E: But even if proof verification doesn't support that kind of proceeding and error, machine learning does promote proceeding without certainty, without any. Yeah, yeah. Do we see a unifying machine proof verification with, with learning? With machine learning, do you see a prospect for unifying those?
00:31:42.820 - 00:32:32.480, Speaker C: I mean, there certainly has been work in this, in this direction. It's a very popular domain of research for people who work on machine learning. Maybe I should just mention the topical news for those that don't follow this kind of thing too closely. It's that, as Maya alluded to, DeepMind announced last month that they had developed a system which was capable of solving sort of hard high school level like olympiad style, number theory and algebra problems in lean, you know, given statements that were formalized by human beings and then sent off to their sort of proprietary reinforcement learning based system. So this is just the culmination in a line of work that goes back at least five years. So certainly it's a very popular problem to work on, but I would say it's not solved yet.
00:32:36.010 - 00:32:39.562, Speaker E: Misha, did you have. Yeah, yeah.
00:32:39.586 - 00:33:12.410, Speaker B: I thought I would maybe add one more usage of machine learning, which is a little different in conducting numerical experiments. I mean, numerical experiments, of course. You know, people have been doing them for a long time, these computers, but now with machine learning, it's somehow the friction is so much less because you can just ask one of the systems to give you code. Right. And like, okay, I would like to test this, like, generate some, let's say, random matrix or something. Look at the eigenvalues. I don't know.
00:33:12.410 - 00:33:25.650, Speaker B: It's extremely easy to conduct pretty broad range of numerical experiments using machine learning problems. I think there is. It's become a very useful tool for many people. I feel.
00:33:28.070 - 00:34:08.492, Speaker D: Actually, that's a great point I would not have made if you had not said that is the actual uses of machine learning that I. That I've actually seen by real algebraic geometers, who have no clue what they're doing in this area, has been exactly. I think the word also he said that I did not think of until he said it was reducing the friction. Currently, lean is high friction for new users, and over time, there's going to be later versions, 100% for sure, will be way lower friction, because that's how nature, that's how things evolve, but already things. The reduction of friction is that people can use this to write code. They don't. Sorry, they don't write code.
00:34:08.492 - 00:34:20.980, Speaker D: It's a great thing, is you can just. This is actually being used and worth doing. You want to. You need to check something out. It used to be you just write the code yourself. Now you can simply. And it really works.
00:34:20.980 - 00:34:23.440, Speaker D: It's really. Yeah, it's worth experimenting.
00:34:29.350 - 00:34:58.580, Speaker A: So, one thing we could expand a little bit on not only machine learning, but just in the framework of new paradigms, we've touched a little bit on facilitating human collaboration. So not even, like humans working with machines, but facilitating a collaboration between humans. And. I don't know, I think you had a question kind of related to this. Did you? You commented on this earlier, but we could delve into that.
00:35:00.440 - 00:35:28.630, Speaker E: Yeah, I guess what these comments are making me think, and what I kind of think we want to see increased interaction with humans and machines. You don't want to see machines just covering the material, and then we take the output. And I had not thought about the machine learning aspect as much. You talked about going to that lean workshop. Did you want to say more about that? Could you say, what did you get?
00:35:30.330 - 00:35:57.750, Speaker D: I mean, I guess what I will say will be the uninformed person speaking, not the informed person speaking in this panel. But maybe that is useful because most of the mathematical world is like me. I guess what I got out of it was a much better sense. I feel like. I would not say everyone should go and do this. You should do this only if you want to. But if you are curious, it is actually quite interesting to see.
00:35:57.750 - 00:36:58.176, Speaker D: There are some, I should say quite impressive, algebraic geometers who are quite deep in the lean or coding. And there are also some rather impressive of young, usually graduate student level people programming as well, and in some cases, some undergraduates who are also on the frontier. And it was quite impressive seeing the rate at which things are being done, but also the nature of what the frontier is like, the fact that there are two connected components of the frontier, and within algebraic geometry, the border of what's getting formalized. I'm going to get a little tiny bit into the weeds, but the reasons for what's happening and wherever is just because of the accident of who's doing it. And on the border with topology, on the infinity category side, they're the computational number theorists, they're the graduate students just learning the subject. And that's where it just happens to be, where people are, where things are getting formalized. And I guess it's not systematic, but that's the way it's got to be, because that's where the people have energy and that sort of thing.
00:36:58.176 - 00:37:31.520, Speaker D: You cannot order people to systematically go through mathematics. But I was somewhat shocked at the fact that you cannot just have march through the basics of a subject and formalize, formalize, formalize. I thought that's the way we would get to the frontier, and instead we have a lot of fancy stuff formalized and a lot of massive holes in the invasive material formalized. And that's natural. That's just a feature of how things get done. But I'm absolutely speaking as an outsider, but I feel like you should speak to this.
00:37:31.590 - 00:37:36.348, Speaker C: Yeah, yeah, I'm sorry, I've forgotten. What was the original question? What was the original question?
00:37:36.524 - 00:37:42.000, Speaker E: Well, about the future of human interaction, about the.
00:37:42.860 - 00:37:44.120, Speaker C: Yeah, yeah.
00:37:50.180 - 00:37:57.400, Speaker E: Well, but also. Also about integrating machine learning with proof verification, which is that some is going on.
00:38:05.190 - 00:38:41.658, Speaker C: Yeah, I'm sorry, maybe I don't have anything in particular to add except that, I mean, somehow there's. I think I agree with the point you were making, that somehow there's no point in teaching machines to do mathematics if we don't bring ourselves along with them. Like, you know, whatever a machine is generating, it's not any good to us if we don't understand it. What's more, it takes a lot of time to understand anyone's proof. You know, if someone gives you a proof and says, you know, if someone, if someone writes a paper for you and, and says, read this paper, you'll love it. It's the best paper you've ever read. You'll find it more interesting than any paper you've ever read before.
00:38:41.658 - 00:39:02.690, Speaker C: You have to really trust the person who gave you that paper before you, you sit down and you spend 10 hours or 20 hours reading that paper. And in the same way, I mean, it'll take a very high standard before we trust both the correctness and also the interestingness of machines enough that we are willing to invest the time to learn the mathematics that they claim to be able to teach us.
00:39:05.470 - 00:39:08.050, Speaker A: Were there some more questions from the audience?
00:39:13.590 - 00:40:00.276, Speaker F: So there's something which all of you mentioned, but I want to make some comments on this. I remember when one time Hendriklenstein was mentioning that some of these things are coming already. Gauss, when Gauss was calculating, he was a fantastic calculator, but he still used some help of other human calculators at the time, of course. And so he told the reinstall, he told this man was not sitting on his desk like this, but he was just still helping him. But now I could imagine that before it was long, short thinking that everybody would have personal computer a long time ago. Is it personal computer in calculating? Thinking. So in.
00:40:00.276 - 00:40:59.430, Speaker F: But now we are thinking that perhaps there will be some kind of machine learning, personal mathematician tool when you could ask the, ask this computer, is this. My idea is good. What should I do? Not just like Misha was already suggesting something like that, that it would be that not just what is this conjecture reasonably, would I possibly contact some player? Should I look? And also, Avi was mentioning this, a number of these things, finding the literature. So there could be very meaningful collaboration between computers on the like I collaborate and also with human and together, so this collaboration could be extended to machines. Thank you. Yeah. Now asking, what do you think about?
00:41:02.410 - 00:41:53.000, Speaker C: Maybe I'll just say that I think that before mathematicians really started using this kind of thing, it would have to be very good when. When you're concentrating very hard, you don't want to break your concentration. And if you have a tool which you can ask the kinds of questions you were describing to, and it gives the correct result 80% of the time, I think that might still be not a good enough tool for me to be willing to break my concentration to consult that tool. I think mathematics is almost unique among human pursuits in that somehow the standards are so high. And I think that tools, AI, tools that are useful to human beings need to be really, really good, like before. They're worth sort of breaking our own concentration and our own thought patterns to temporarily enter into other people's patterns, where other people now perhaps includes AI as well.
00:41:55.580 - 00:42:54.042, Speaker D: Actually, I want to say something which is, which is going to at first sound in contradiction, but is absolutely not in contradiction, which is, which is. And the key thing is, again, I guess, the phrase frictionless, not making concentration. When I was in grad school, I had a friend who's a biologist, and I laughed at her because, and biologists in general, because they were using Google to actually find information. And that just was the most ridiculous thing. And the good thing is, so secretly we are using AI because we are already in our research, at least most of us, we're using, getting advice because at least most of us, I suspect at least under a certain age, have gone to Google and wondered something and thinking something of this must be true and must be known to someone. Instead of going down your hall and asking your colleague because you're at home, because of COVID you just google it and you find something, which you then have to, you don't trust it necessarily. It's a preprint, it's on someone's webpage.
00:42:54.042 - 00:43:14.830, Speaker D: But you got some advice, just like the colleague down the hall. And maybe I trust Google more than some colleagues and less than some other colleagues, but we're already sort of doing that, but only in a way that does not, that does not break in the same way that I would not want to break my concentration to go to the library, but we might quickly consult to find some things we are already doing that.
00:43:17.930 - 00:43:54.812, Speaker B: Maybe, if I can add, I completely agree with Rava's point. It, I would maybe just expand on it a little bit, is that we may like, you know, suppose we need a result of some kind. We need to know something, and maybe we don't know that or don't even know where to look for it, right? It's. And this is much better than Google. Google. Okay, we can look for some keywords. Now with one of the systems with JPT, you can actually have a discussion and it can suggest something may be wrong, but it's often right.
00:43:54.812 - 00:44:21.820, Speaker B: And you can ask rather specific questions, which it's almost like having a person who is like maybe like an undergraduate student who is maybe not like familiar with the detail, not really an expert on the topic, but knows a lot of stuff, like very extraordinarily broad scope of knowledge that's useful. Right. Even if we don't necessarily trust everything those systems give us.
00:44:28.410 - 00:45:03.190, Speaker A: So this also brings up the question of, you're mentioning not breaking concentration, but if we think of like individual humans, but also communities of humans as learning systems. So just by interacting with tools of a certain kind, it will somewhat change the way humans. Yeah. Do the things they do, change their skill set, potentially. So do you have any thoughts? Does anybody want to comment on either concerns or non concerns about these prospects?
00:45:04.370 - 00:46:16.786, Speaker C: Yeah, so I'll make an analogy that I've made, possibly even to you before, which is that I think that for formalization in particular, which is not exactly the same as AI, there's a kind of change that happens in the way people think. And I would liken it to the way that people say that humans started thinking after they learned to read. So people say that when communities of humans learn to read it, they lose their sort of long sort of narrative memory, they lose the ability to memorize stories, to memorize poems on a large scale, because it's no longer necessary to keep that in people's heads. There's a sort of external record. I will say that now that I've been formalizing with lean for four years, I have de emphasized the part of my brain that keeps entire arguments in my head. One of the things offered by the user interface of a formalization system is a nice little layout on the side of everything you currently know and what you're currently working towards. And in very complicated arguments, this is actually much more convenient than having to keep that whole system, that whole proof in your working memory as you move through it.
00:46:16.786 - 00:46:44.340, Speaker C: I can imagine that, you know, when we start to have students like the undergraduates I teach who have seen these systems right through their mathematical education from the beginning, that will see that their thinking patterns are transformed in the same way, and that, I mean, while losing the somehow superpower of keeping a whole proof in their heads, they might gain the superpower of being able to deal with more complicated proof states that have more moving pieces that can be kept in someone's head.
00:46:52.920 - 00:46:54.660, Speaker A: Did you want to add anything, Misha?
00:46:58.920 - 00:47:01.100, Speaker B: No, thanks, Maya.
00:47:06.970 - 00:47:29.270, Speaker D: It made me think of a dinner party I was at where I was sitting across from a couple of professional musicians. And I was always really. I've always been impressed because I thought, it's amazing how they can hold in their mind a huge, long piece of music. All those notes. They know them. They somehow memorize them all. And so I told them I was very impressed.
00:47:29.270 - 00:48:08.880, Speaker D: And then I realized, explained to me, that, no, absolutely not. They do not memorize every single piece because every single note, because they know. They know it in exactly, or at least metaphorically, exactly this way, where it's a very long, complicated piece. But they've practiced. They know it, they understand it, and they start going, and, of course, this comes next, and then this happens next, and this, then this. And so the way in which the information is stored, they're able to, in effect, store far, far more understanding in their heads. And it made me feel like when I understand something in a long, complicated paper, I don't memorize the paper.
00:48:08.880 - 00:48:17.432, Speaker D: I know everything in it. And being able to offload more things out of my brain, out of my working memory, presumably can only help me.
00:48:17.456 - 00:48:54.280, Speaker B: So this actually resonates, actually, if I can make a point about that. I think there was recently a study, maybe not so recently, and they asked a chess expert and amateurs to memorize position, like, people who don't play chess, memorized chess positions. And very strong chess players were extremely good at memorizing positions, which sort of made sense from a chess point of view, but they were no better than non chess players at memorizing, just, like, random sets of pieces. So it's very much like it has to make some sense. There is some pattern there.
00:48:57.830 - 00:49:11.890, Speaker A: There's some questions from the audience. So I'm not sure. Should we try to finish soonish? Okay, so feel free to, if anybody wants to ask something.
00:49:18.110 - 00:50:06.354, Speaker G: Thank you. So, when I have a. A student working with me, I do computability theory, and sometimes I give them an assignment question, and they hand back to me some pseudo code that is, like, absolutely impossible to read. And I try and explain to them that, you know, proof is to convince another human being beyond the shadow of the doubt that something is true. And so this is something that we've touched upon in this conversation is that sort of interaction of how to get to the, you know, our language to the machine, and then get the trust back so that we understand it. And so now my question to you experts is that, to me, there are sometimes when we're doing our mathematics. There are a number of instances where you, you know, you can figure it out for n equals one and n equals two, and.
00:50:06.354 - 00:50:41.932, Speaker G: And then it starts to get complicated. And there are certain examples in my research area where someone's actually computed that, you know, there's a good reason why it was okay for n equals two and much harder for n equals three, and insanely hard for n equals four, and said, now, because of this other big number, it's just like for each different as n was, the thing was exploding. It's like the actual proof. It wasn't checking the cases. It was like, we have to figure out how to handle them. And so what I've thought was happening with this modern computing things is that we're teaching them. They learn and whatever.
00:50:41.932 - 00:50:57.040, Speaker G: Is there a possibility that we crunch out some of the cases and then it learns from those cases? But then would it be able to give it back to us in a way that we would believe? What are your thoughts? Or.
00:51:14.790 - 00:51:50.630, Speaker E: Well, I will mention that the lean people are very excited about someone who's working on machine translation of lean into comprehensible English. This is actually a project besides the documentation. To me, what's persuasive about a lean verification now is the documentation. The fact that the computer said yes is not really very important. But when it said yes in a documented situation, and I believe the ask means yes, you brought up that question. But they're working on mechanizing the english language summary.
00:51:51.330 - 00:51:56.706, Speaker C: Yes, I can speak to that. I think you're talking of work of Patrick Meso and Kyle, Kyle Miller, is that right?
00:51:56.738 - 00:51:57.802, Speaker E: Yes, yes, yes.
00:51:57.826 - 00:52:33.092, Speaker C: So one thing I would say is that this is not a system that uses machine learning at all. It's a very clever collection of hand encodings of, you know, what this lemma would have as its natural language name in English, and what this operation would be the corresponding algebraic operation in English, like a very complicated, subtle dictionary with parts of speech and the grammar and everything that allows for a translation. It's very clever work, but it almost comes from a point of view of linguistics or sort of of grammatical analysis or structural analysis rather than machine learning point of view.
00:52:33.196 - 00:52:38.324, Speaker E: And it sounds like it's. Right now it's aimed at a particular vocabulary. It's sort of.
00:52:38.412 - 00:53:09.886, Speaker C: So it's designed to be general purpose, but it hasn't been released yet, I should say, so that people have seen prototypes. It's designed to be general purpose, but of course it requires a hand encoding of the dictionary associated to a particular topic area, but it's really impressive work, maybe I would say, returning to the question that you were starting to ask, and I think also a point was touched on earlier, I think there's a real question of how to write mathematics when you have an associated formal verification.
00:53:09.958 - 00:53:12.566, Speaker D: And this is even without.
00:53:12.678 - 00:53:36.108, Speaker C: Even without. Even without. It's not like writing mathematics is easy. But I'll just say that there are starting to be examples of people who have written articles, you know, within their back pocket, a formal proof that they've generated of the same statement. There's one that I'll mention. It's by Sebastien Gozal and Vladimir Schur. It's a proof of a lemma about gram of hyperbolic metric spaces.
00:53:36.108 - 00:54:26.800, Speaker C: And they say, I think in the introduction to this paper, that writing a paper in which you have a formalized proof already of the result presents certain challenges of exposition or questions of exposition. And the way they say they're going to address it is that they will they choose to be more free in the exposition than you would be. They don't need to, you know, address every potential objection of a referee or a skeptical reader. They can take a slightly looser approach. In particular, I think they're much looser about constants and getting the best constant in their exposition than they have been in the formal proof, because they know that anyone who cares about the details has someone else, somewhere else to turn to. And so I think this is actually a really nice example of an exposition that is improved by knowing that it doesn't have to dot every I and cross every t.
00:54:28.940 - 00:54:41.788, Speaker D: Actually, I find that a super interesting, motivating example. And, but, and that's precisely a human's writing that was written by humans. Not, not. That was not a. Not a. Not lean code. That was so, that was.
00:54:41.788 - 00:54:53.710, Speaker D: So they produced the lean code, and then they wrote. They wrote the explanation, which feels very. If the future is like that, that's a future I could be very happy to look at.
00:54:55.170 - 00:55:06.230, Speaker E: Misha, this reminds me of what you said early on about how machine proof might affect you. Does it seem. Does it seem like a similar point to you, that being less rigorous because the machine alternative is there?
00:55:08.530 - 00:56:36.910, Speaker B: Yeah, actually, in fact, I was thinking, you know, when this is purely anecdotal, I don't know if there is, like, any study of this or not, but certainly what you notice, like, when machine produces code, say, python code or some, you know, code, it actually very often gives, like, very nice. It's commented very, quite frequently, commented quite nicely, and, in fact, better than like 99% of the actual, like programmers or coders would comment this. So I think it's maybe not too much to expect that I should add. There are also many, it's kind of anecdotal cases, but I think they're quite widespread. Like you can try to get the machine to put a piece of code in the machine and you just ask okay, what does this piece of code do? Feel very hard problem for a person know this code does and it actually like very frequently produces nice explanation for what it does. So I think given that it works so well for this type of formal languages, like, you know, programming languages, maybe it can do the same for lean for formal proofs. I mean this is obviously a little bit of more than a little bit of speculative, but, but it seems plausible at least.
00:56:39.050 - 00:56:40.682, Speaker D: Maybe we can wrap up soon.
00:56:40.706 - 00:56:55.590, Speaker E: But I was going to say this really has changed my view of the relation of machine learning with proof verification. I had not seen that integration before and that's very interesting. So thanks for that.
00:56:56.570 - 00:57:31.210, Speaker A: I was going to suggest one last thing to ponder, especially in the context of this. So what are thoughts on in the future when a fields medal is awarded for research that made significant use of AI or. Yeah, so if the award is going to the human, do we need to have some restrictions on the kind of AI.
00:57:34.440 - 00:58:08.830, Speaker C: That'S an even more. Well, when physicists, when physicists win Nobel prizes for experimental physics, the award doesn't go to the machine that they used. And what's more, it's not acknowledged that some people have more access to machines than others. That's just a fact of life. And only the people who have the machines can win the Nobel prizes. Because unless the people who have the machines can do the research that leads to the Nobel prizes, my guess is that that will be the paradigm that will also be followed if and when this comes to mathematics.
00:58:11.970 - 00:58:39.044, Speaker D: Another analogy might be the best chess players in the world. We don't. We could just say the best. We do still have the best human chess player in the world. And the fact that the machines are playing a different game, it really is not the same. It's not the same thing. So if sure, perhaps at some point an AI purely produces our feels metal worth result.
00:58:39.044 - 00:59:00.110, Speaker D: But to me that's speculatively enough in the future and there's so many steps in between now and then that I feel like a good, like I feel absolutely unconcerned that, that we'll have to worry about this soon. But I could imagine what the rules would be when the time comes. But we have more serious things to worry about when that before that happens.
00:59:01.130 - 00:59:17.390, Speaker B: But by that talking, right. We have to really worry. Right. Because the best chess engines are so much better than, like, we cannot understand their games at all. If the same is true, that's kind of scary for Martin. If the same is true.
00:59:20.330 - 00:59:27.540, Speaker A: Does anybody want to add anything or last question from the audience. Well, thank you, everybody, for participating.
