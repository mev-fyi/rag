00:00:01.200 - 00:00:57.494, Speaker A: Okay, well, let me say something about the main subject of the course. It's not clear whether it's k theory or CSP algebra, because it's not a course just on k theory. You can have a course on k theory. There are many different kinds of k theory. There's k theory as it's applied in algebraic topology, which is closely related to this Easter algebra one. Then there's what's called algebraic k theory, where you don't just have, well, in any setting whatsoever, k zero is a purely algebraic construction for Banach algebra. Everyone know what a Banach algebra is.
00:00:57.494 - 00:01:58.964, Speaker A: It's an algebra over the complex, real or complex number with a norm, certain properties, and it's complete. If. Oh, so if you have a Banach algebra, then there's what's called, could be called Banach algebra, case theory called topological character. But that's, I think that's confusing because it refers to the applications of k theory to topological spaces, which is basically commutative. Manic algebra, continuous functions on compact hostar space. All right, so the, as I said, k zero is a purely algebraic concept, even for Banic algebra. You just look at the algebra, or even just ring structures.
00:01:58.964 - 00:02:35.160, Speaker A: But if it's looking at k one, then this is for a Banach algebra. Then this is the topology of the Banach algebra. That's why I like to call it Banach algebra. K theory, that's theoretical. Or it could be c straw. Give off sea strauss. Okay, but so there's k zero and k one and k two, k three, and so on.
00:02:35.160 - 00:03:14.164, Speaker A: Urban algebras. But this, now I'm going to make a statement. I don't really mean, this is not really very interesting, because if it's a Banach altruist over the complex numbers, then you have k zero and you have k one. And that's it. K two for the Banach algebra is exactly the same as k zero. This was discovered by Raoul Bauer, who was a Canadian. Well, he was educated in Canada.
00:03:14.164 - 00:04:07.746, Speaker A: After 1968, he arrived here, Czechoslovakia. Czechoslovakia. And then when I was a postdoc, I met him because I was at the Institute for Advanced Study as a postdoc, and he was there as a visitor from Harvard. I remember mentioning to him that I was interested in applying to a vacancy a job position that had been advertised at Harvard. And he institute old hall. And he was encouraging. He said, you better move quickly, but now over.
00:04:07.746 - 00:05:01.988, Speaker A: And if it's a manic algebra with real numbers, then almost the same thing happens. But you might say, again with the tongue in cheek, it's by the way, I don't think I can talk with my tongue in my cheek. But anyway, if at the real Banach algebra, then you have k zero and k one, and then k two, which is different from k zero, for real manic algebra in general, then k three and k keeps on going like this for a while. How many people have. Have heard where it stops? Okay, Rishi, but when does it stop going? Yeah, so you get k zero. So up to k eight, which is the same as k zero. So whether it stopped the step before or there, it's hard to tell.
00:05:01.988 - 00:05:44.104, Speaker A: Okay, that's a matter of. That's a matter of judgment call. But then it just repeats. So again, that's bot very distant. Notice that eight is a multiple of two. Okay? Again, you might think that's not a very deep comment, but, but also notice that the complex Banach algebra is an example of a real Banach algebra, okay? So because the complex numbers are an algebra over the reals or contain the real number. And so if the.
00:05:44.104 - 00:07:23.734, Speaker A: If the groups, if these k groups are a complex banner calculator, alternate k zero, k one, k k zero, k one, and so on, refer to k zero, k one, k two, k three, and so on, then if you look at the real k groups, there has to be a. Has to mesh, right? Yeah, because otherwise, eventually you'd have them all the same. They all have to be the same. By the way, I noticed once that the certain insects, I think cicadas, and in particular, their underground stage, lasts a certain number of years, sometimes quite a few years, even more than ten, but it's always a prime number. Okay, so there's an exercise in case area in biology. Okay, why is that? And of course, even when they're underground, they have predators, but they especially have to worry about when they come up above ground. They don't want to have someone that knows when they're coming up and is ready for it, hatched around the same time.
00:07:23.734 - 00:08:18.364, Speaker A: Some motive. Well, okay, but then there's this algebraic case theory, which is very important, turns out in two straight algebra theory. So there's an algebraic k one, and it's related to the Banach algebra, k one. If you have a Banach algebra, it's. They're both groups, right? The algebraic group is a group, abelian group. And the Banic algebra, k one group, is the billion group. Well, the, the, um, the algebraic, um, a one group maps onto the, um, mapped onto the natural map onto the binocular k one.
00:08:18.364 - 00:09:38.198, Speaker A: Okay, well, um, but maybe I should just say a few things about Banach algebras and these two algebras. I don't know how much independently of k theory. I'm not sure how much people are familiar, to what degree people are familiar with hispanic algebras and future algebra. But by the way, the simplest example is just scalars, the complex numbers or the real numbers. Generally speaking, in this course, we'll be looking at the case of the complex number because ironically, perhaps the complex numbers are less complex than the real number as they're ultimately code. Okay, but so you have. So that's a Banach algebra, the sister algebra.
00:09:38.198 - 00:10:51.628, Speaker A: Complex numbers are a sister algebra. What's the sister algebra? Well, the most straightforward definition is you look at the Hilbert space. Everyone know what a Hilbert space is. If it's finite dimensional, then it's what sometimes is called linear product space. And with this over either the real or the complex numbers, perhaps in most undergraduate mathematics, you think of the Hilbert space and inner product space over the real numbers, just as well be over the complex numbers. So if you have the, for instance, the algebra of complex numbers itself, or, sorry, the vector space of complex numbers, it's a complex vector space, then what is the natural inner product that makes it a Hilbert space? Are you familiar with that? Yeah. How do you make the real or the complex numbers into an inner product space? Well, are you familiar, have you studied linear algebra? Yes.
00:10:51.628 - 00:11:24.456, Speaker A: Okay. Well, sometimes. So on Friday when I was talking about linear algebra, I meant over an arbitrary field without talking about specializing derivative complex numbers. But usually in a linear algebra course, one looks also at the special case, if not exclusively, then at least also at the case of real or complex numbers. Right? Usually real numbers, maybe. Okay, so if you have a real inner product space, well, the real numbers are an example, I guess, of a real inner product space. Right? Okay.
00:11:24.456 - 00:11:54.064, Speaker A: But for something to be in inner product space, there has to be an inner product. So what's the inner product? Sorry. Yes. Yeah. Okay. In fact, you could be even more. Somehow you could make it into a little rhyme and say the inner product is the product.
00:11:54.064 - 00:12:51.806, Speaker A: Yeah, okay, right. Well, yes, but you don't have to be, you don't have to go into gory details. It's, a product is usually bilinear. An inner product is by axioms and an ordinary product. Well, if we know what the ordinary product is, and it just happens that the ordinary product satisfies the axioms for an inner product if you consider the real numbers as vectors. Right, okay, now, are you familiar with the case of the complex numbers? All right, so now that's sort of a very concise statement. Is that clear to everyone? What, what is your name? Hy.
00:12:51.806 - 00:13:30.408, Speaker A: All right, does everyone understand what hy said? I mean, what did he mean when he said, you take the complex conjugate? Yes? Yeah. Right. So you have two complex numbers. You consider them as vectors in this one dimensional complex vector space, and you don't just multiply them. That would be some kind of bilinear product, but it wouldn't be what's called linear product. It wouldn't be a Hilbert space, your inner product. But as you said, if you take, you have, you have to choose your convention.
00:13:30.408 - 00:15:58.780, Speaker A: You take the first one or the second one to take the complex conjugate of it, and then you take the, then you take the product, okay? And that satisfies the axioms for inner product in the complex vector space, okay? When in finite dimensions, in the norm associated with the inner product, and that's called the two norm, you take the norm of a vector, you take the norm square, no, you take the inner product of a vector with itself, and then you call that the square of the norm. Okay, define the norm as the square root. One of the axioms is, of course, this is positive, even non zero, if the vector is non zero, okay, so that's how you get complex numbers to be of Hilbert space. And what's the algebra of operators? Linear operators on this Hilbert space. Are you familiar with linear operators? Okay, but on Friday I was talking about linear algebra, right? Already in linear algebra, you have linear operators, right? You have matrices, but they're not, they also have linear operators, and they're not quite the same thing. So if you have a vector space, then you don't have matrices automatically, but you do have linear operators. They're maps from the vector space into itself, which remind me of your name, I think on Friday you told me your name, but Chris, Chris, I'm sorry, I'm not very good at things at all, but, okay, so linear operators, a map from the space into itself, considered as sets, and then it preserves structure, which means the scale modification, it's compatible in a natural way with scalar, multiplication of vectors by number, by scalars, and it's compatible with addition in the natural sense.
00:15:58.780 - 00:16:55.408, Speaker A: So it takes the sum of vectors into it, into the sum of what it takes the two vectors into. Does that make sense? Okay, well, that's a linear operator. That's called a linear operator, either between two vector spaces, linear operator or linear map, linear transformation, many different names and, okay, but you have two of them, you can compose them. Also, they're related to matrices. Do you remember? You probably studied it at one point, but do you remember how linear operators are related to matrices? Yeah, exactly. The rule for multiplying matrices comes from the natural multiplication of operators where you just first do one and you do the other, you operate with one, then you operate with the next one. That's called the product of the two operator.
00:16:55.408 - 00:17:55.364, Speaker A: But then every operator, if you fix a basis for the vector space, let's just say it's, we're talking about the case where the vector space, just one vector space. At the moment, you're talking about linear operators from a vector space into itself. All right? If you fix the basis, say it's finite dimensionals, you have a finite base. Well, then you have every operator has a matrix, right? And you can tell from the matrix how the operator acts on a vector, because the vectors have matrices, too, if you like, they have column matrices. That's usual convention. You multiply a matrix, a column matrix by a square matrix on the left. And if it's a square matrix of an operator, and what you get is the column vector of the operator acting on the original vector.
00:17:55.364 - 00:18:43.840, Speaker A: Okay? So if you look at, if you interpret the matrices this way as corresponding to linear operators, the matrix multiplication corresponds to multiplying, to composing, composing the operators considered as maps from the space into itself. Right? Okay. But then this is how you get matrix multiplication. No one in their right mind would think of any, would be, would have any other reason to modify matrices the way you have to learn it. Right? That's the proper way to multiply matrices is to just modify the. And actually, that's, that's interesting. People do that.
00:18:43.840 - 00:19:04.884, Speaker A: Okay. It's called sure. Multiplication. Sure is famous. S c h u R. Okay. But so the.
00:19:04.884 - 00:19:38.424, Speaker A: Yes, well, I would say that it's, that's an interesting question. Oh, okay. Right. Yeah. Okay. You know, I could hear that just fine, but, you know, that's, maybe that recently I got hearing aids. Okay.
00:19:38.424 - 00:20:13.476, Speaker A: And maybe they're even more than I need. Okay, but Rishi, how was that the second time? Of course there's noise in the room. Yeah, sorry, can you repeat the question? Oh, yeah. Okay, well, I always hesitate to steal someone's thunder. To try to steal someone thunder, but. Okay, so what was the. Okay.
00:20:13.476 - 00:21:15.960, Speaker A: Or non, non linear? So in this course, we're basically going to be talking about linear operations. Okay? And maybe that's the question to think about. Why, why do we just look at linear operators? But for one thing, we're looking at vector spaces, right? And normally, when you look at a mathematical structure, if you're looking at maps from it into itself, the natural thing at square one when you start off is to look at maps which preserve all the structure of your space, okay? So if it's a set, then it doesn't even make sense to say it's a nonlinear map, right? Map from a set into itself. If it's a vector space and you have a map into itself. Well, one reason linear maps are interesting beyond the context where you assume the map is linear. Well, this is what calculus notices. Newton.
00:21:15.960 - 00:22:43.344, Speaker A: So Newton and Leibniz, lots of people noticed at a certain stage that the nonlinear map can be approximated by a linear map, right? And again, when I was at the Institute for Advanced Study, I remember trying to convince one of the visitors, visiting members who was specialized in english, english literature. I tried to convince her that, but this was true, that you could approximate a linear, non arbitrary map, a linear, non linear map with just a curve like this, you approximate it by a straight line, and that would be fine. She wasn't, she was worried about that. Okay? And now I could see ways to be worried because it only works at that one point, right? And maybe I didn't say that clearly enough, okay. Otherwise it gets pretty bad pretty quickly, right? Okay. But anyway, that's a good, it's a good question because nonlinear phenomena are all over the place. But the functional analysis and operator algebra theory sometimes looks at the, consider it a nonlinear setting, but in a very careful way.
00:22:43.344 - 00:24:29.684, Speaker A: You may have to be careful not to go beyond what you can manage. Okay, well, thank you, by the way, you could look into this yourself. But the particular question of by the way, so what's an algebra? How do linear mappings make an algebra come into being an algebra? Well, you can. Multiplication of two of linear mapping is just composition, which makes sense for nonlinear maps, too, right? But if you add, if you add, if you want to add two linear maps, then it works quite well because you get a linear map, you add them vector wise, right? You add the values for each vector, and that gives you a new linear map. Now, of course, if it's two nonlinear maps and you add them in the same way, then you get a another map, which of course, will also, in general, be nonlinear. Well, for instance, if one of them is minus the other, then you get zero, right? Well, okay, but I wouldn't be, I wouldn't boast about that, but I wouldn't crow about that especially. No, the point is that you get a algebra of linear maps.
00:24:29.684 - 00:25:32.458, Speaker A: And this algebra is so, for instance, if it's a finite dimensional vector space of, say, dimension two, then what's the algebra of linear maps in terms which you can visualize? Well, you choose a basis with two elements. Then you can have square matrices representing the operators. Those are called two by two matrices sometimes, right? Okay, and multiply two by two matrices. Those form an algebra. And that's if the scalars are real or complex, then that's an example of a sea star algebra. It was a suitable star operation, which is the, well, I'm talking about the axiomatic approach to system algorithms, where you just say you have a star operation, certain property in the Hilbert space. Every bounded linear operator bounded is automatic.
00:25:32.458 - 00:26:34.544, Speaker A: In the finite dimensional case. Every bounded linear operator has what's called an angel. In fact, for every finite dimensional vector space, every linear operator from the space into itself has an adjoint as a space, as an operator, from the dual space into itself. But over the real or complex numbers, there's a very close relationship between the original space and the dual space, almost the same thing. So the adjoint operator in general, in the dual space, exists in the original space. And you call that a star, call that the star or the adjoint. And that is, well, you can extract these properties and you get the abstract axioms versus algebra as they're outlined in the textbook.
00:26:34.544 - 00:28:28.314, Speaker A: And then getting onto a century ago now, in the early 1940s, Gelfand and Nimar showed that the axiomatic that you can characterize sea structure by the, looking at the properties that the concrete algebras in the Hilbert space, algebras are linear operators in Hilbert space, the natural properties that they have, and taking those as axioms. So you have an abstract algebra, usually, as I said, the complex numbers are less complex. So you look at a complex algebra, which is a vector space with multiplication suitable property, and the norm, and you assume it has a norm and a star operation, all with certain properties. These will be detailed in the textbook, and they prove that if you have a abstract, it's called a star algebra, with, and it has a, well, it has to be with the norm. It's supposed to be a norm, which is tied in with the product structure. And the star structure has a sister algebra property, as well as a normed algebra property. Go look at all these axioms for the norm star algebra, including that it should be complete as a bandic space.
00:28:28.314 - 00:29:49.454, Speaker A: Okay? But this, this, this abstract star algebra is isomorphic to, means all structure gets carried over to a concrete algebra of operators on the Hilbert space. So star algebra of operators on the Hilbert space means that for every one, if you have an operator in the, in the algebra, then the adjoint operators also in the algebra. Okay, but this, this is, um, and I think this, this was, well, yes. Question, you got some data on your earlier question already? No. Yes. Okay, well, let me, may I just, I like to, I mean, I'm supposed to be registering participation in the course, right? So if you meaning good questions and so on and well, let's say questions and it's a matter of opinion how good they are. But what, so, okay, fine.
00:29:49.454 - 00:30:51.738, Speaker A: For the question or for the name? Okay, fine, but now you've got me on tenderhooks. Okay, all right. But I would like to welcome, in general, I would like to welcome questions. The sum total of material, if you like to be covered in the course, is flexible. And that's not so much the point as somehow getting people interested in the subject and getting some idea that there's something going on. Okay, so, um, and, and there, um, there really are, um, um, this, this area of mathematics which is less than 100 years old. Before mentioning Gelfand and Nimark, um, I should have mentioned von Neumann.
00:30:51.738 - 00:32:34.734, Speaker A: Well, if you sound like there's some name dropping in the emails that you've received, repeating ones from last year, but the subject doesn't change enormously from one year to the next, right? Actually sometimes it does. One of the main sort of items in this course is what's called classification of Sears there algebra. And what does classification mean? That's one of the things I want to talk about at various levels, going back to what James Glimm did 60 or some 60 years ago, to what has been done very recently, various classes of sister algebras. A really nice classification has been a thing. And you might say what is the purpose of this? Well, one theory which I've seen advanced is that the main purpose of the brain is to classify things, okay? Sorting things. And so if someone shows that something can be done in that line, that direction, then we should be grateful, right, instead of just straining to do it. Sometimes it might not be possible.
00:32:34.734 - 00:33:59.364, Speaker A: Well, of course, sometimes classification is not necessarily a good thing, but if there's we and them, too much of we and they, and that's not always good, but. Well, so where does the first classification for seast algebras start to click in? Well, what about these matrix algebra? There's one by one matrices and two by two matrices, say over the complex numbers and three by three matrices and four by four. Okay. These are our algebra, c star algebra. Let's consider them just as algebras for the time being. In other words, some complex vector spaces with multiplication, of course, intertwining within natural, intertwining between the addition and modification. Well, you have to prove it that it's true.
00:33:59.364 - 00:34:49.593, Speaker A: It's true. If you look at ten by ten matrices and eleven by eleven matrices as algebras, these are different. Okay, that's a good exercise. And actually, there's an easy way of doing it. You want me to give you a hint or. Yes, sorry, what? Yeah. Okay, well, if you're for any two, for any class, any kind of mathematical structure, then if you have two examples of it, it makes sense to say, well, different is sort of slang.
00:34:49.593 - 00:35:39.114, Speaker A: It makes sense to say that they're isomorphic or not isomorphic, which is greek for same shape, okay? But it goes beyond the shape. It means they're essentially the same, okay, isomorphic, everyone seen the word before? So isomorphic vector spaces is a theorem that says that isomorphic vector vector spaces are isomorphic if and only if they have the same dimension, right? Okay, and that's a theorem. Every vector space has a dimension. In fact, on Friday I was talking about linear algebra. That's one, that's one statement of the main fact about linear algebra, that dimension exists. Okay? Of course, fake dimension exists. Much easier to prove that.
00:35:39.114 - 00:36:48.384, Speaker A: And it has the property, the fundamental property of dimension. It's additive on direct sum, but it's still true for fake dimension, that two vector spaces are isomorphic if they have the same fake dimension. But it's a good exercise to see if that's any easier to prove. I think it might be. Well, okay, but by the way, this comment about the invention of a vector space and their isomorphic if and only if dimension is the same, I'm slipping this in, sort of trying to slip this in unobtrusively as a hint for the exercise about the matrix algebra. Okay, anyone, how many people are catching on to that? As a hint, as a possible hint. Okay, so the idea is we have these square matrices, algebras of square matrices, maybe ten by 1011 by eleven, and we want to know if they're isomorphic as algebra.
00:36:48.384 - 00:38:19.294, Speaker A: That means essentially the same. There should be a map between them, which is bijective, as I said, map and one to one and on to injective and surjective. And furthermore, both it and its inverse preserve our additive and preserve modification, okay? And preserve also scalar modification too, preserve all the structures. If that's true for two algebras over the scalar field, then they're said to be isomorphic, okay? But in particular, if they're isomorphic as algebras, okay, let me go ahead. And you're going to have trouble enough as it is doing all the exercises, so, or even doing some of them, I mean, even picking out some to do. So let's, let me, by the way, I hope you take seriously mister Rao's advice how to approach the problems, okay? He's given some very nice suggestions how to do problems. You can make up your own problems if you want, okay? But if two algebra, isomorphic as algebras does have this linear isomorphism between them, preserves product.
00:38:19.294 - 00:39:05.424, Speaker A: Well, in particular it's a linear isomorphism, right? And the algebra among, if you just look, forget temporarily about the product. What is an algebra? An algebra is a vector space for the product. If you forget about the product, it's just a vector space. And if they're isomorphic is algebra, they're going to be isomorphic as vector spaces, right? Because it's a bilinear map, sorry, it's a bijective map which is linear in both directions. Okay, so that's a vector, that's a, and preserves product, but in particular preserves the linear structure. So the vector spaces are isomorphic. And we know when vector spaces are isomorphic, right.
00:39:05.424 - 00:40:15.014, Speaker A: You just have to compute the dimension. So what's the dimension of the vector space of ten by ten matrices? Okay, well, I've got two answers now. Do you have an answer? Okay, yes, that's two to one. Now it's like a tennis game. Okay, that's topical because we just had a nice tennis final on the weekend. Well, the, a whole bunch of them, of course, I'm not suggesting everyone gets glued to their television set the way I sort of, the way we do, but the, okay, and, oh, okay, ten by ten is 100. What about eleven by eleven? And now we're getting, we're sort of coming across the phenomenon that in mathematics, not only is there a conceptual challenge, but sometimes there's a technical challenge as well.
00:40:15.014 - 00:41:15.624, Speaker A: Okay, so now nobody, and furthermore, when I was in school, we learned our multiplication tables up to a twelve, okay? So I know what eleven times eleven is, but anyway, we don't have to think very hard to guess that it's going to be bigger than 100, right? Strictly bigger than 100. All right, so the dimensions are different, therefore they're not even isomorphic as vector spaces. So there's certainly not isomorphic as algebra. So this is a, so we've classified, we've shown how to classify matrix algebra. So, algebras of linear operators on finite dimensional vector spaces over a fixed field, the classification is. Well, the vector spaces themselves are classified by the dimension, which is 0123 and so on. Zero for the zero vector space.
00:41:15.624 - 00:42:24.154, Speaker A: And it's essentially the same number. It's essentially the same classification for the matrix algebra, the algebra operators, except you say zero one four. Now my, now my training clicks in, right? Yes, zero, one. 4916. Okay, so that's fine. All right, but so this is the beginning of sea strummer classification, and this is an analogous classification which people may be, may have come across, okay, for what are called groups, finite groups, finite simple groups, which means there are no. How many people know what the subgroup of a group is? Okay, how about normal subgroup? How many people know what a normal subgroup of a group is? Okay, good.
00:42:24.154 - 00:42:49.998, Speaker A: Then suppose there are no normal subgroups aside from the one element subgroup, the trivial subgroup and the whole group. Those are subgroups. Okay, so they're always. Those are. By the way, that might just be just a single subgroup. Again, always say it's two subgroups, because the whole group might be trivial anyway, if those are. If only.
00:42:49.998 - 00:43:25.734, Speaker A: If you only have the trivial subgroups, sometimes the whole group is called the trivial subgroup, then it's called a simple group. And if it's a finite group, well, some finite groups are automatically simple, right? Like if you have a. The number of elements is prime, then I guess it's a simple group, right? It's even automatically commutative. Everyone come across that. That's a good exercise. If you want to, you could do that. You could hand that in.
00:43:25.734 - 00:44:04.834, Speaker A: Don't tell Mister Lau I said. I said so. Okay, so. Well, but as soon as you get to be not a prime, then actually, that's an interesting question. If it's not a prime number, is there always more than one group? Well, probably there is. That could be a exercise in itself to decide if it's not a prime number. There are always more than one group without many elements with that order, if you like.
00:44:04.834 - 00:45:05.924, Speaker A: Okay, well, but certainly there, it's known. For instance, if it's four elements, then up to isomorphism, there are two groups, right? The four elements, and they're both billionaires, both communities. You know, to me the word Abelian is real, because I visited the University of Oslo, and there, the math department is in Abel hus Abul Haus. Okay, so by the way, there's an Abel prize. How many people have heard of that? Good, it's a word. It's interesting. It almost rhymes with Nobel.
00:45:05.924 - 00:45:41.014, Speaker A: Right. It's hoped that it will be a similar stature awarded by the king of Norway for one of the Nobel Prize. One of the prizes called the Nobel Prize Prize is also awarded by the king, the monarch of Norway. Well, so what I'm leading up to is how many people are familiar with the little red book. Well, that's. Is this really passe? Completely passe. Mao's little red book.
00:45:41.014 - 00:46:05.542, Speaker A: Okay, well, you know, I'm glad to hear it. If it is. If it's gone, that's fine. Fine with me, I think. Don't tell him I. Don't tell him I said. Okay, but there's a big red book, the big Red book, which is called.
00:46:05.542 - 00:46:34.222, Speaker A: What do you suppose the title of this big red book is called? The Atlas of finite simple groups. Okay? That's it. That's the classification of finite simple groups. Okay? You can look it up, be in the library. You might not be able to take it down from the shelf. I mean, if it's not. Even if you were allowed to, it might be.
00:46:34.222 - 00:46:41.422, Speaker A: Might be quite heavy. Yeah, it is. Well, it's about this thick and this large. By that large. It's something like that. I've seen it. Okay.
00:46:41.422 - 00:47:02.444, Speaker A: I haven't tried to lift it, but it's. But it's that. So what it is, is a list of finite simple groups. And what we just did was make a list of finite dimensional sea star algebra. Because you can take direct sums, but it's finite dimensional simple sea star algebras, if you like. Okay. We made a list.
00:47:02.444 - 00:47:24.144, Speaker A: And that. That was pretty short work. But the classification of finite simple groups, some people are still checking it, okay. They want to be sure it's correct, because it's not that. It's thousands and thousands of pages to establish it. Right. And they're not all in this book.
00:47:24.144 - 00:48:00.156, Speaker A: I don't think there'll be room in the. However large it is, I don't think there'll be room for the proof. The proof is maybe still unfolding. I'm not, obviously, I'm not an expert, all right, but. Okay, so what's my point? Well, my point is, if you. It's not just finite dimensional, simple sea structural algebra that can be classified. It's almost.
00:48:00.156 - 00:49:03.034, Speaker A: Now, this is a vast overstatement, okay? But it's, to all intents and purposes, to the man in the street, it's all simple sea star algorithm. Okay. I won't be surprised if some experts, they leave the room. But that's something I hope to convey during the term how close to being true this is. And it's such a vast collection of algebra that even the, the mind even goes into overload just trying to fathom what it means to classify them. Okay, and so in other words, there's no such thing as a list of. There's a list, there's a list of finite dimensional simplicity calculus.
00:49:03.034 - 00:49:37.694, Speaker A: There's finite, there's number zero, number one, number two, number three, and so on. It's just like for finite simple groups. Finite simple group number zero. It's not the one with zero elements as a server might be the one with the finite simple group number one, finite simple root number two is a list, a discrete list, accountably many of them, and they're just listed. You have to check that every group is isomorphic to one of the groups in the list. Every financial group and the one and the ones in the list are all different. Otherwise it wouldn't be really a classification.
00:49:37.694 - 00:50:35.236, Speaker A: But, and for some, for some classes of sis algebra, some classes of simple sea algorithm, you can do the same thing. It won't be, won't necessarily be countable, but to be a nice parameter, and you can just associate with a family with the parameter, you can associate algebras in this, in this class, and, and every algebra in the class is isomorphic to one of these and to only one. So you have, so you've classified the algebra by parameters. I mentioned Wim's name already. Well, that's what he did in his PhD thesis. He showed in fact that there's a nice class of sister algebras that can be classified by, by a parameter. In fact, the algebras are given as they're constructed.
00:50:35.236 - 00:51:36.248, Speaker A: You can construct the family of algebras according to this parameter, which is by the way, a point in the counter set. Everyone know what the counter set is, okay, zero one, the set, zero one, sequences of zeros and ones. In the computer age, everyone should know what the counter set is. But, so for every point, for every sequence of zeros and ones, Gwim constructed a seastar algebra. And then turns out that every seast algebra in a certain class can be defined maximatically. Every seastor algebra in that class is isomorphic to one and exactly one of these of the algebra of the, so that you have a canter set, a nice counter set of isomorphism classes, well that's just as good a classification as a finite simple group or a matrix algebra. You have a nice parameter, well behaved parameter.
00:51:36.248 - 00:52:27.974, Speaker A: It doesn't, doesn't have to be discrete. It's a continuous parameter. And furthermore, if you have the same thing was done by, a little later by a team of people. I made a reference in my email from January due to the rotations, okay? Rotations on the circle to every one of those who associate an algebra. And if you look at the rotation from zero to PI, all those rotations give you, well, in the irrational case, irrational multiple of two, PI. It's going to be a simple algebra, simple algorithm. And look at this abstractly defined class of algebra.
00:52:27.974 - 00:53:09.104, Speaker A: Every one of them is isomorphic to one and only one number. So the one parameter set, gee, I'm glad I brought my own chalk, but don't have time to fish it out. Here's a piece which is an 8th of an inch long. That would be enough for an 8th of a minute. Is that okay? So if you look at the cantor's end, you leave out, I like this french way of writing closed intervals. The open interval is the opposite. Okay, so then you, it's the middle third set.
00:53:09.104 - 00:53:38.824, Speaker A: So you remove all the middle third sets. That's the counter set. Okay, so that's the glim, that's the glim algebra. And then the other parameter space goes from zero to one half. Both of these are label the set of isomorphism classes. It's a nice list. Okay, but that, but in general, you can't expect to have a nice list of the isomorphism class because the classification is not smooth while we have a classification like this.
00:53:38.824 - 00:54:17.184, Speaker A: And so what it is, that's, that's what this course is about. Okay, well, I think I won't overstay my, I won't try to overstay my welcome. We do have an hour. Right? Okay, well let's. And that means there'll be someone counting at the bit outside. Yeah, but just think about that. What are we actually looking for? Well, I've given you the hint that we're looking for k zero, okay?
