00:00:01.200 - 00:00:02.248, Speaker A: Hello, Sheldon.
00:00:02.398 - 00:00:03.154, Speaker B: Hello.
00:00:04.734 - 00:00:05.926, Speaker C: How are you doing?
00:00:06.110 - 00:00:08.554, Speaker B: Pretty good, thank you.
00:00:15.854 - 00:01:03.984, Speaker A: There was one part of proof from last week to finish theorem on, I called it controlled finite interpolation. And there was a reason to call it controlled and finite because when h is given as an rkhs on a set x, then f in x, f in h, sorry. Is a function on x with values in c now in reverse order, if a function is given.
00:01:05.004 - 00:01:12.464, Speaker C: So now f from x to c is given.
00:01:14.604 - 00:02:23.084, Speaker A: And we were looking for some conditions such that this condition guarantees that f is in h. It's kind of important theorem with many applications. And the one we saw, the controlled finite interpolation, a part of it, I mean part two indeed, which is mostly used, part C also use in particular in the study of multiplier algebra, but also for the function f. But part two is kind of the heart of this theorem. I remind you about part two. And we will see one more time why it is called controlled and finite interpolation says that there is a constant c such that for every finite set f.
00:02:26.184 - 00:02:32.444, Speaker C: X n in x. So that's the meaning of finite interpolation.
00:02:32.904 - 00:02:34.976, Speaker A: And there is a function x.
00:02:35.080 - 00:02:55.708, Speaker C: There is h in the space h such that h at point x k is equal to f at point x k from one up to n at.
00:02:55.756 - 00:03:00.204, Speaker A: N points h coincides with f. And.
00:03:00.244 - 00:03:10.204, Speaker C: Moreover, and moreover, the norm is controlled.
00:03:13.624 - 00:03:47.174, Speaker A: So that's the meaning of controlled finite interpolation, two things. And it's to see the importance of this result. Note that many functional spaces that we consider the ambient space x, the space on which the functions are defined, is usually big, is not even countable. For example, we saw rkhs on the.
00:03:47.514 - 00:03:51.854, Speaker C: Open unit desk rdspace, Bergman space, diversion space.
00:03:52.834 - 00:04:05.054, Speaker A: So x over there is d. And the theorem says that choose any finite number of points.
00:04:05.914 - 00:04:08.090, Speaker C: I mean, it can be, there is.
00:04:08.122 - 00:04:09.282, Speaker A: No restriction on z.
00:04:09.338 - 00:04:10.694, Speaker C: One up to zn.
00:04:11.554 - 00:04:46.406, Speaker A: And then if you can find a function h, usually we go for polynomials, but in this case polynomials are not the best choice because with polynomials we can do this interpolation. But the other one we are not sure. If you can find an interpolating function whose norm is bounded about, then the function you started with is in this space. So from something finite, you go to the whole space, not only separately from finite.
00:04:46.470 - 00:04:51.318, Speaker C: Usually we go to a sequence is something separable.
00:04:51.406 - 00:04:59.054, Speaker A: But here d is not even countable. So from a finite number of points on d, we go to the whole.
00:04:59.174 - 00:05:04.394, Speaker C: Of t. And this is what helps us in application.
00:05:05.294 - 00:05:11.514, Speaker A: We did one implies three, three implies two. And now it remains just two.
00:05:12.614 - 00:05:16.234, Speaker C: I mean, this possibility implies one.
00:05:18.654 - 00:05:20.990, Speaker A: In other words, why? This implies the function.
00:05:21.022 - 00:05:22.382, Speaker C: There is a function which is in.
00:05:22.398 - 00:05:39.590, Speaker A: This space, and here is the point. Instead of sequences, we use nets for our limiting process. So the assumption says that for any finite set f, there is a function.
00:05:39.662 - 00:05:46.918, Speaker C: H. There is a function h, which I will denote it, but hf such.
00:05:46.966 - 00:05:50.318, Speaker A: That with this interpolation is possible, and.
00:05:50.446 - 00:05:53.234, Speaker C: The norm is controlled by c.
00:05:55.974 - 00:06:18.436, Speaker A: H. F is not necessarily the best function in this space. I mean the best in terms of the norm, which does the job if we saw before in previous courses, if we project f to the space created by those kernel functions, the notation we.
00:06:18.500 - 00:06:37.184, Speaker C: Used h. So h of is the span of kx in f. In other words, span of kx one to kx.
00:06:38.764 - 00:06:51.872, Speaker A: If you project into this space which is close, being finite, you obtain the function which does the interpolation and also its norm is smaller.
00:06:52.008 - 00:07:00.644, Speaker C: So if I call g of f is the projection into h of this function hf.
00:07:03.144 - 00:07:06.912, Speaker A: Then gf at each point.
00:07:07.008 - 00:07:12.174, Speaker C: X k is equal to hf at eight point x k. And this is.
00:07:14.194 - 00:07:16.474, Speaker A: We know before that is equal to.
00:07:16.514 - 00:07:19.810, Speaker C: F of x k from one up.
00:07:19.842 - 00:07:22.338, Speaker A: To n. And also we know because.
00:07:22.386 - 00:07:27.178, Speaker C: Of the projection, the normal gf is.
00:07:27.226 - 00:07:28.818, Speaker A: Less than or equal to the normal.
00:07:28.866 - 00:07:31.098, Speaker C: Hf, because we project it into a.
00:07:31.186 - 00:07:48.798, Speaker A: Smaller space and this is less than or equal to c. That is why we switch. And instead of using hf, we use gf. And now define m to be the.
00:07:48.846 - 00:07:52.274, Speaker C: Supremum of all gf.
00:07:53.134 - 00:08:02.514, Speaker A: When f ranges over x and f, we know this supremum exists and is.
00:08:02.554 - 00:08:04.214, Speaker C: Less than or equal to c.
00:08:08.714 - 00:08:13.538, Speaker A: Now, the claim is that this gf, so.
00:08:13.586 - 00:08:14.174, Speaker C: Claim.
00:08:17.474 - 00:08:22.210, Speaker A: Gf, when f is a subset.
00:08:22.242 - 00:08:23.854, Speaker C: Of x and finite.
00:08:26.094 - 00:08:49.646, Speaker A: Is a Cauchy net. And we know that, I mean, there was a proposition at the beginning of this chapter. It's a cauchy net which converges to f. So let's verify.
00:08:49.710 - 00:08:51.634, Speaker C: Why is it a caution net?
00:08:53.614 - 00:09:02.262, Speaker A: I mean, at the beginning we show that the cauchin is convergent. But here it is easy to see that if it converges, there is no other option.
00:09:02.318 - 00:09:04.974, Speaker C: It has to converge to f. But.
00:09:05.014 - 00:09:11.674, Speaker A: First of all, why is it a cosinet, by definition of supremum, even.
00:09:14.054 - 00:09:14.950, Speaker C: Epsilon.
00:09:15.102 - 00:09:28.842, Speaker A: Bigger than zero, that exists at f zero, such that this quantity is close to m? I mean, as much as we wish, because it's a supremum.
00:09:28.898 - 00:09:30.674, Speaker C: So we can get closer and closer.
00:09:30.714 - 00:09:35.442, Speaker A: To m. And for convenience, I choose.
00:09:35.498 - 00:09:45.294, Speaker C: F zero such that the nor of gf zero is it squared here. No, it's not squared.
00:09:45.924 - 00:10:18.008, Speaker A: It's for sure is less than or equal to m bigger than or equal to. We can put whatever we wish here, but strictly less than m. And the authors choose m minus epsilon squared. If you choose m minus epsilon, it's also true. I mean, we obtain at the end something which goes to zero, but epsilon squared is two, is to obtain something a little bit easier at the end.
00:10:18.096 - 00:10:19.444, Speaker C: It'S not that essential.
00:10:20.464 - 00:11:02.146, Speaker A: Now, what can we say if f is finite but bigger than f, zero contains more point. Namely, I want to show that with capital f the approximation is better. And to do this, put it here to see better. Now note that if I consider Gof, recall the definition of grf. It's a function.
00:11:02.250 - 00:11:03.614, Speaker C: It's here, indeed.
00:11:06.974 - 00:11:22.942, Speaker A: It'S a function which is norm bounded, bounded by n. But I need this property here. It does the interpolation for all x in the finite space f, and f contains f zero.
00:11:23.118 - 00:11:26.302, Speaker C: So g of x, f of x.
00:11:26.478 - 00:11:31.954, Speaker A: Is equal to f of x for all x in f, in particular.
00:11:35.874 - 00:11:36.258, Speaker C: For.
00:11:36.306 - 00:11:52.906, Speaker A: All x in f zero, because it's smaller. And therefore, if I project this, therefore, when I project this gf into hf.
00:11:52.970 - 00:11:55.854, Speaker C: Zero, I obtain gf zero.
00:11:59.574 - 00:12:25.298, Speaker A: Remember, we had a theorem that when we project into the space created by kernel function, we obtain a function does the interpolation, and also it is unique. So it doesn't matter with which function which does the interpolation. To start, there are infinitely many of them. But when you project, you obtain the.
00:12:25.346 - 00:12:27.778, Speaker C: The one which has the minimum norm.
00:12:27.946 - 00:12:31.842, Speaker A: So phf zero of gf is equal.
00:12:31.898 - 00:12:48.054, Speaker C: To g of f zero. And therefore, because of the projection property, this implies that gf minus gf zero is orthogonal to gf zero. That's because of this projection.
00:12:50.934 - 00:12:53.142, Speaker A: And now you note that I can.
00:12:53.198 - 00:13:15.914, Speaker C: Write gf equal to gf minus gf zero plus gf zero. And therefore norm of gf squared is, by orthogonality normal gf minus gf zero squared plus norm of gf squared.
00:13:22.204 - 00:13:36.544, Speaker A: And also note that norm of gf zero is less than or equal to the norm of gf because of the projection.
00:13:38.364 - 00:13:39.504, Speaker C: Projection.
00:13:41.384 - 00:13:51.904, Speaker A: And this is less than or equal to m. That's the definition of f of m. And what was our choice of f zero?
00:13:52.024 - 00:13:53.856, Speaker C: This is bigger than or equal to.
00:13:53.920 - 00:13:56.884, Speaker A: M minus something, which we choose.
00:13:59.504 - 00:14:01.616, Speaker C: M minus epsilon squared.
00:14:01.760 - 00:14:08.864, Speaker A: And now the effect of this is seen here. Here is the effect. Therefore, gf.
00:14:15.004 - 00:14:27.100, Speaker C: Minus gf squared is normal gf squared minus gf zero squared, a.
00:14:27.132 - 00:14:49.844, Speaker A: Two minus b two c a minus ba plus b less than or equal to this one. I can replace it by two times the suprema.
00:14:50.584 - 00:14:52.240, Speaker C: All of them is less, are less.
00:14:52.272 - 00:14:58.400, Speaker A: Than or equal to m, and this one, because of the inequality that you.
00:14:58.432 - 00:15:00.694, Speaker C: See here, because of this one.
00:15:05.434 - 00:15:17.134, Speaker A: Is less than or equal to epsilon squared. Therefore.
00:15:20.394 - 00:15:30.234, Speaker C: Therefore, gf minus gf zero is less than or equal to a constant times epsilon.
00:15:32.974 - 00:16:04.794, Speaker A: For any f which contains f zero. Now, if you consider two f, let's say f one and f two such that both of them contain f zero. Then you repeat this inequality two wise, and you easily see that gf one.
00:16:05.534 - 00:16:12.474, Speaker C: Minus gf two is less than or equal to gf minus gf zero.
00:16:14.414 - 00:16:15.154, Speaker A: And.
00:16:17.214 - 00:16:21.764, Speaker C: Gf one minus gf zero, gf two minus gsu.
00:16:26.264 - 00:16:30.160, Speaker A: And each of them is like big o of epsilon.
00:16:30.352 - 00:16:42.124, Speaker C: So it gives us two times root two m. It's a constant c. It's not the same c that we had before, but one write is ct times epsilon.
00:16:44.984 - 00:16:46.444, Speaker A: And thus.
00:16:49.254 - 00:16:57.914, Speaker C: This shows that gf, when f is in x finite, is cauchy.
00:17:00.614 - 00:17:07.462, Speaker A: By proposition that we had at the very beginning of this chapter, there is.
00:17:07.478 - 00:17:29.184, Speaker C: A function g in h. I have to mention here Cauchy in h such that lean gf is equal to g. That by the first proposition we know.
00:17:29.224 - 00:17:33.120, Speaker A: That convergence in norm implies the convergence.
00:17:33.192 - 00:17:45.074, Speaker C: In at each point. Therefore, for any x in x limit gf of x is equal to gx.
00:17:51.254 - 00:17:55.310, Speaker A: But as soon as f contains the.
00:17:55.342 - 00:18:07.754, Speaker C: Point x, if x is enough, then g f of x is finite, doesn't change.
00:18:08.414 - 00:18:10.494, Speaker A: When I say finite, I mean with.
00:18:10.534 - 00:18:33.434, Speaker C: Respect to our parameter, which is f. F is our variable here. And therefore, for any x in x and g of x is equal to f of x. Conclusion f is in h. This is what we wanted from the very beginning.
00:18:40.124 - 00:18:56.940, Speaker A: It's kind of surprising theorem. And there is a very nice corollary here to show on one hand the mystery, on the other hand the power of this theorem.
00:18:57.012 - 00:18:58.704, Speaker C: And look at this corollary.
00:19:01.944 - 00:19:06.680, Speaker A: Let f define and d with values.
00:19:06.712 - 00:19:13.284, Speaker C: In c be a function. No, no other restriction for the time be a function.
00:19:23.224 - 00:19:28.828, Speaker A: Assume that this is the assumption we need to apply part two.
00:19:28.996 - 00:19:30.144, Speaker C: Assume that.
00:19:32.084 - 00:19:38.064, Speaker A: There is a constant c.
00:19:38.444 - 00:19:41.264, Speaker C: Strictly bigger than zero such that.
00:19:48.684 - 00:19:53.054, Speaker A: K of z and w defined by this.
00:19:53.094 - 00:19:58.798, Speaker C: Formula C two one minus z w.
00:19:58.926 - 00:20:04.594, Speaker A: Bar minus f z f w bar.
00:20:05.214 - 00:20:23.634, Speaker C: Is a kernel function. And, oh, it's better to say kernel function on d z and w vary.
00:20:26.014 - 00:20:44.394, Speaker A: You see, it's in a sense, it's a question in linear algebra. What does this mean? Kernel function means that fix endpoint z one up to zn form the grammy.
00:20:45.144 - 00:21:08.364, Speaker C: The grammar matrix kzizj. And that is something positive. So let me, let me mention here. This means that for any z one up to zn, indeed, the matrix k z I z j.
00:21:11.884 - 00:21:12.700, Speaker A: Is bigger than.
00:21:12.732 - 00:21:13.984, Speaker C: Or equal to zero.
00:21:18.764 - 00:21:27.956, Speaker A: So, it's a verification in linear algebra. And now you see no mention of complex analysis techniques.
00:21:27.980 - 00:21:30.756, Speaker C: And all we have seen up to here.
00:21:30.820 - 00:21:33.344, Speaker A: And now look at the conclusion then.
00:21:34.524 - 00:21:35.624, Speaker C: Conclusion.
00:21:39.264 - 00:21:41.404, Speaker A: F is analytic on t.
00:21:48.224 - 00:21:49.088, Speaker C: So, from.
00:21:49.136 - 00:22:12.396, Speaker A: Something in pure linear algebra, you conclude that the function is analytic, that the derivative exists, and more than that, moreover, the sum n from one o zero.
00:22:12.500 - 00:22:25.104, Speaker C: To infinity, I can call it a n. A n mod squared is finite, where an are Taylor coefficients of f.
00:22:39.944 - 00:23:01.180, Speaker A: So, we have two rather unexpected results in complex analysis from an assumption in linear algebra. Just by verifying something is positive, you.
00:23:01.212 - 00:23:03.940, Speaker C: Conclude that the function is analytic and.
00:23:04.132 - 00:23:08.172, Speaker A: Its Taylor coefficient or square summable. Quite.
00:23:08.308 - 00:23:10.064, Speaker C: I mean, fantastic results.
00:23:10.524 - 00:23:19.860, Speaker A: Really a surprise. And you can do this with all functions, with all rkhs that you have.
00:23:19.892 - 00:23:20.984, Speaker C: Seen up to now.
00:23:21.984 - 00:23:24.056, Speaker D: I was just about to ask a question.
00:23:24.160 - 00:23:24.792, Speaker C: Does this.
00:23:24.888 - 00:23:27.884, Speaker D: Do they have similar results for the by disk?
00:23:28.984 - 00:23:46.248, Speaker A: It's in absolutely the theorem. Look, it's very general. There is. I didn't mention the whole statement of theorem here. It's an x. No restriction on x. X could.
00:23:46.248 - 00:23:48.364, Speaker A: Here I choose x to be d.
00:23:49.464 - 00:23:51.444, Speaker C: You can take it to Vd two.
00:23:53.504 - 00:24:01.124, Speaker A: It works in any rkhs, and that's the real power of this theorem.
00:24:01.784 - 00:24:07.004, Speaker D: Okay. That kernel would look the same. The only difference would be the functions.
00:24:07.544 - 00:24:26.234, Speaker A: The kernel here, instead of c two over one minus z u bar, it will be c or c two. That doesn't matter. But in denominator, you'll have one minus z. One w 1 bar times one minus z two w bar two.
00:24:28.334 - 00:24:29.326, Speaker C: It will be like that.
00:24:29.350 - 00:24:31.782, Speaker A: And over there, fz one z two.
00:24:31.838 - 00:24:36.474, Speaker C: F w one w two r. It will be like that.
00:24:37.974 - 00:24:41.726, Speaker D: Okay. Okay. Thanks.
00:24:41.870 - 00:24:44.434, Speaker A: You're welcome. It works in any space.
00:24:45.724 - 00:24:57.236, Speaker B: I just want to note that it's not even obvious to me. I agree. This looks like a surprising result, because it's not even obvious to me that the hypotheses implied that f is a continuous function on the.
00:24:57.300 - 00:24:58.844, Speaker C: Absolutely right.
00:24:59.004 - 00:24:59.900, Speaker A: Absolutely not.
00:24:59.932 - 00:25:01.204, Speaker B: Assuming that at all.
00:25:01.364 - 00:25:29.494, Speaker A: Yeah. Positivity has proved to be a very strong tool. And in our focus program, there was a talk by Jura Ljubarsky. He talked a little bit about Gabor frames. And there is a recent work by Dominique Guillaud. He also kind of marginally talked about it. Dominique Guillaud and Apurwa Kare.
00:25:29.954 - 00:25:31.218, Speaker C: And two more.
00:25:31.306 - 00:26:02.394, Speaker A: Two more also work on this. But a whole group of people on frame theory work on positivity. And the new notion that they have is total positivity. And they extract a lot of information after that. So at the first glance, as Sheldon mentioned, I mean, you just have positivity. You don't expect even the function to be continuous, but a lot more comes out of it. And here is one phenomenon that we witness.
00:26:02.394 - 00:26:30.840, Speaker A: If you look at their work, they have a very nice recent survey, about 70, 80 pages. They explained about total positivity. And you see, they just start from this. All the minors, I mean, the determinant of k by k, all of them, not just principal ones. If they are positive, they call it the total positive, and they extract many.
00:26:30.872 - 00:26:32.564, Speaker C: Many information from that.
00:26:36.584 - 00:26:45.984, Speaker A: Okay, that finishes our chapter four. And now we start chapter chapter three.
00:26:46.024 - 00:26:46.328, Speaker C: Sorry.
00:26:46.376 - 00:27:04.614, Speaker A: We start our chapter four, which is indeed totally linear algebra. It's a lot of interesting stuff, but it's kind of, I can say abstract linear algebra, but it's better to say beautiful linear algebra.
00:27:04.654 - 00:27:06.102, Speaker C: It's a lot of interesting stuff in.
00:27:06.118 - 00:27:33.574, Speaker A: This chapter, and it's mainly on trolesky. And sure result sometimes instead of sure, we call it Hadamard. But we arrived that. Let's start with Cholesky factorization to do this one observation.
00:27:34.794 - 00:27:39.134, Speaker C: So, question.
00:27:44.154 - 00:27:51.414, Speaker A: Assume that the two by two matrix, whose first element is zero.
00:27:52.004 - 00:27:55.944, Speaker C: And here we have BCT, is positive.
00:27:58.124 - 00:27:59.660, Speaker A: So you have a matrix a, b.
00:27:59.692 - 00:28:01.460, Speaker C: C, d, which is positive.
00:28:01.612 - 00:28:03.340, Speaker A: But the first element is zero.
00:28:03.412 - 00:28:05.024, Speaker C: A is equal to zero.
00:28:06.244 - 00:28:34.216, Speaker A: What can you say about the other elements? It's not a difficult question just to use the definition of being positive positivity.
00:28:34.280 - 00:28:37.096, Speaker C: And we obtain the conclusion.
00:28:37.280 - 00:28:46.424, Speaker A: And here is another point that what I emphasized before shows its importance. P being positive.
00:28:52.964 - 00:29:03.604, Speaker C: Is equivalent to p of x and X inner product is.
00:29:03.684 - 00:29:20.844, Speaker A: Bigger than or zero for all x in c two, not r two. I emphasized on this before, but its importance, you will see here, for any x in c two, the inner product.
00:29:20.924 - 00:29:24.236, Speaker C: Of Px and X has to be positive.
00:29:24.420 - 00:29:29.172, Speaker A: And now we choose general.
00:29:29.268 - 00:29:35.864, Speaker C: X in c two is of the form alpha and beta, where alpha, beta.
00:29:36.024 - 00:29:42.568, Speaker A: R and c. And then let's see, what is px inner product with x.
00:29:42.616 - 00:29:55.564, Speaker C: It's. It's not difficult to see that px is. I write it here. It's.
00:29:58.424 - 00:30:05.484, Speaker A: Alpha squared times zero. That's the first element, which is zero plus beta. Alpha bar.
00:30:07.344 - 00:30:09.924, Speaker C: Times b plus.
00:30:12.304 - 00:30:13.604, Speaker A: Beta bar.
00:30:14.664 - 00:30:22.484, Speaker C: Alpha times c plus beta squared in modulus times t.
00:30:40.604 - 00:30:50.784, Speaker A: Well, to it's it's, it's very straightforward to see that b has to be c bar and d has to be positive.
00:30:51.444 - 00:30:55.566, Speaker C: Like special choice of alpha and beta.
00:30:55.710 - 00:31:17.554, Speaker A: But more than that, more than that, I choose beta such that it's unimodular. And it's in a sense, the argument of B beta times.
00:31:18.454 - 00:31:24.854, Speaker C: Is it beta or beta beta pki.
00:31:25.794 - 00:31:32.738, Speaker A: So beta times b. Usually we choose this beta times b.
00:31:32.786 - 00:31:36.154, Speaker C: To be absolute value of b, but.
00:31:36.194 - 00:31:42.794, Speaker A: Also we can choose it to be. If we add PI to the argument, we can choose beta such that beta.
00:31:42.834 - 00:31:45.894, Speaker C: Times b is minus absolute value of b.
00:31:50.934 - 00:32:00.394, Speaker A: And it will happen here too, I think I.
00:32:02.254 - 00:32:06.614, Speaker C: Okay, so what does this give to me?
00:32:06.654 - 00:32:21.694, Speaker A: This gives the first one is zero and the second will give me minus. Yeah, alpha alpha.
00:32:21.994 - 00:32:28.218, Speaker C: I choose to be in R. To simplify, really doesn't matter to be that complicated.
00:32:28.306 - 00:32:32.074, Speaker A: Here I get two alpha absolute value.
00:32:32.154 - 00:32:33.014, Speaker C: Of b.
00:32:34.754 - 00:32:38.054, Speaker A: Because in absolute value both.
00:32:38.094 - 00:32:50.074, Speaker C: Of them are the same. And then beta is chosen to remove the argument and plus.
00:32:52.414 - 00:32:55.270, Speaker A: Just d. This.
00:32:55.302 - 00:32:57.434, Speaker C: Is bigger than equal to zero.
00:32:59.094 - 00:33:16.810, Speaker A: I divided no, no, no. Didn't do anything. This is just what I obtain if I choose alpha to be real and beta unimodular such that beta times b is minus mode b.
00:33:16.922 - 00:33:20.562, Speaker C: This is a standard trick that we.
00:33:20.618 - 00:33:25.914, Speaker A: Do when we want to prove the generalized Cauchy Schwarz inequality too.
00:33:25.994 - 00:33:29.774, Speaker C: This is not something very strange standard technique.
00:33:30.914 - 00:33:56.810, Speaker A: Why we do this? Because when you look at the last inequality here, you see immediately that alpha is free. So alpha can go to plus infinity. And if b is not equal to zero, then there is a problem here. Inequality will bigger than or equal to.
00:33:56.842 - 00:33:58.334, Speaker C: Zero will not be true.
00:33:58.774 - 00:34:06.526, Speaker A: So since this is true for for all alpha in R, immediately from this.
00:34:06.590 - 00:34:14.514, Speaker C: We obtain b equal to zero and c also equal to zero and d bigger than or equal to zero.
00:34:15.414 - 00:34:20.742, Speaker A: Therefore that, that's the end. Therefore, if you have a matrix zero.
00:34:20.878 - 00:34:30.594, Speaker C: B c, d bigger than or equal to zero, then b is equal to c equal to zero, and d also.
00:34:33.934 - 00:34:36.158, Speaker A: Of course, this is, this is reversible.
00:34:36.246 - 00:34:38.014, Speaker C: But I don't need that.
00:34:38.054 - 00:34:50.074, Speaker A: But it's distributed true. You can easily generalize this, because the definition.
00:34:52.074 - 00:35:06.894, Speaker C: Of positivity. So the lemma is here. If you have p equal to pij, n by n matrix is positive.
00:35:09.034 - 00:35:12.254, Speaker A: And one element on the diagonal is zero.
00:35:12.714 - 00:35:17.384, Speaker C: And pkk is equal to zero.
00:35:19.244 - 00:35:28.620, Speaker A: Then everything on the column of pkk and on the row of pkk has to be zero.
00:35:28.812 - 00:35:42.144, Speaker C: Then pik is equal to zero for all I and pkj equal to zero for.
00:35:47.764 - 00:35:51.620, Speaker A: So if this is your p, which.
00:35:51.652 - 00:35:54.224, Speaker C: Is bigger than or equal to zero.
00:35:54.564 - 00:36:11.416, Speaker A: And on the diagonal, these elements say is equal to zero. Everything on this diagonal and on this row, on the, on this column and.
00:36:11.440 - 00:36:14.964, Speaker C: On this row is equal to zero.
00:36:22.384 - 00:36:53.298, Speaker A: And the proof is just we saw a bound. It is true that in the general case, when we write px inner product with x x x is in cn, but you can consider all the coordinate equal to zero except two of them. The case one and another one. And then you obtain a two by two matrix and apply the previous result to that two by two matrix and deduce that everything here is zero.
00:36:53.466 - 00:36:55.414, Speaker C: Everything here is zero two.
00:36:57.274 - 00:37:00.800, Speaker A: So it's a nice observation, but it.
00:37:00.832 - 00:37:05.724, Speaker C: Will help us to prove next theorem.
00:37:06.544 - 00:37:08.324, Speaker A: Is it clear or.
00:37:09.704 - 00:37:36.064, Speaker C: Okay, Trm Cholesky let pij n by n matrix be positive.
00:37:37.124 - 00:37:52.564, Speaker A: And now we assume that the pkk, the element on diagonal, is not zero, and it either either say not equal to zero, but we know that it has to be bigger than or equal to zero.
00:37:52.604 - 00:37:58.784, Speaker C: Therefore this is equivalent to say that pkk is strictly bigger than zero. It's the same.
00:37:59.964 - 00:38:05.064, Speaker A: Then define the following matrix.
00:38:09.684 - 00:38:42.104, Speaker C: There's no specific name for it, but the ij element of this is pij minus is pik pkj divided by pkk PI j from one up to, I call it q.
00:38:45.804 - 00:38:49.762, Speaker A: So we define a matrix q like that. Then q is positive.
00:38:49.898 - 00:38:54.974, Speaker C: The conclusion, the main observation is that q is also positive.
00:39:01.274 - 00:39:09.066, Speaker A: It is not clear why this is something important, but gradually we see, and even today we will see some application.
00:39:09.130 - 00:39:14.974, Speaker C: Of this trolexi observation. But there are more which will come in future.
00:39:22.784 - 00:39:31.744, Speaker A: Well before, before giving the proof, let's.
00:39:31.784 - 00:39:35.484, Speaker C: Do some discussion or remarks.
00:39:39.144 - 00:40:12.732, Speaker A: When you have two matrix a and b, and we do the product. There are several ways to look at this product. Usually, I mean in, at the very beginning we say that c, which is the product of a and b, is a matrix such that the ij element is a kbkj. When the summation is over k with.
00:40:12.788 - 00:40:18.692, Speaker C: Appropriate, I mean parameters k from one up to depending on the dimension.
00:40:18.748 - 00:40:21.988, Speaker A: That's the thing we do at the very beginning, which is good.
00:40:22.036 - 00:40:24.420, Speaker C: We can throw a lot of stuff.
00:40:24.532 - 00:40:36.644, Speaker A: Even with this original definition. But that's not the only way that we can look at the product of a and b. Let's provide some more, which I mean.
00:40:36.944 - 00:40:42.560, Speaker C: More insight, which is equivalent to what I mentioned here.
00:40:42.672 - 00:40:51.736, Speaker A: Sometimes we need to do something about the columns of C. So we need to know what are the columns of.
00:40:51.760 - 00:41:01.324, Speaker C: C. So we look at C as a matrix of this one column one, column two, and the last column.
00:41:01.404 - 00:41:06.412, Speaker A: What are these columns? In these games we do the same.
00:41:06.588 - 00:41:15.104, Speaker C: For b, so a times b. It would be a times the first column.
00:41:18.004 - 00:41:20.824, Speaker A: I use c one for column one.
00:41:21.444 - 00:41:28.184, Speaker C: And c two for column two. But these are the columns of B.
00:41:30.004 - 00:41:32.716, Speaker A: And the result, the first column is.
00:41:32.780 - 00:41:42.184, Speaker C: Ac one, the second is ac two, and the last one is acn. That's one way to look at this product.
00:41:48.444 - 00:42:09.336, Speaker A: Another way. Still in this observation, when we say ac one again, by the main definition, ac one is a linear combination of the columns of a which are gathered here. Ac two is another linear combination of.
00:42:09.360 - 00:42:14.724, Speaker C: The columns of a which are gathered there, and so on up to the last one.
00:42:15.724 - 00:42:23.564, Speaker A: If we are interested in the rows of c, then we need to consider.
00:42:23.684 - 00:42:35.948, Speaker C: The roles of matrix a. So here is r1.02 reals rm times.
00:42:36.036 - 00:42:41.632, Speaker A: B and the product. It will be r one times b.
00:42:41.688 - 00:42:49.072, Speaker C: As the first row, two reals times b second row and rn times b.
00:42:49.168 - 00:42:59.496, Speaker A: Last row again, r one. B is a linear combination of the rows of b which are gathered here.
00:42:59.600 - 00:43:04.644, Speaker C: The same for r to b, and the last one is the same. So it's.
00:43:07.884 - 00:43:16.692, Speaker A: The second interpretation. Another one which I mean was kind of new for me, but it's very.
00:43:16.788 - 00:43:21.624, Speaker C: Interesting and we will need it. Here.
00:43:24.884 - 00:43:29.060, Speaker A: We look at a as a.
00:43:29.092 - 00:43:43.364, Speaker C: Column matrix c one up to say cn and b as a row matrix.
00:43:46.184 - 00:43:58.768, Speaker A: And note that it has to be bn, so n columns and n row. And the product is precisely the product.
00:43:58.856 - 00:44:06.784, Speaker C: Of c one b one plus c two b two plus cnbn.
00:44:08.524 - 00:44:13.724, Speaker A: Note that r one rho one rho.
00:44:13.764 - 00:44:15.424, Speaker C: N. Sorry for that.
00:44:22.604 - 00:44:24.464, Speaker A: Columns time times.
00:44:26.264 - 00:44:29.124, Speaker C: So r1 up to rn.
00:44:35.784 - 00:44:43.720, Speaker A: And note that each time when you multiply CI times ri, you multiply a.
00:44:43.752 - 00:44:49.564, Speaker C: Column by a row and the result is a matrix.
00:44:51.844 - 00:45:13.620, Speaker A: Usually due to the scalar product, we do it in the other direction. We multiply a row by a column and we obtain a complex number. But here we need, in the reverse order, c times r. Why did I mention this point of view? Because when we go back to the.
00:45:13.652 - 00:45:20.804, Speaker C: Theorem, you might say where does this come from? What is this?
00:45:22.864 - 00:45:48.904, Speaker A: What is Pik pkj divided by pkk? Pkk is a constant, doesn't depend on I and j. We can take it out, but what would be pik times pkj? And here it is immediate.
00:45:49.964 - 00:45:51.184, Speaker C: Observe that.
00:45:55.644 - 00:45:56.904, Speaker A: The matrix.
00:45:58.844 - 00:46:04.504, Speaker C: Pik pkj pk.
00:46:05.844 - 00:46:07.978, Speaker A: Is just one over pk.
00:46:08.156 - 00:46:08.954, Speaker C: That's.
00:46:11.934 - 00:46:22.634, Speaker A: The constant we take out. And what remains is just v star. When v is the case column of.
00:46:23.094 - 00:46:24.514, Speaker C: The matrix b.
00:46:36.674 - 00:46:40.986, Speaker A: Recall that pkj is.
00:46:41.010 - 00:46:52.134, Speaker C: Equal to pj k r. So it's one over pkk.
00:46:53.394 - 00:46:54.934, Speaker A: The case column.
00:46:58.274 - 00:47:08.994, Speaker C: Here is p one k p two k pk here, the last one is p and k.
00:47:12.574 - 00:47:22.366, Speaker A: Times. When we do v star, either do start to this. So it will be p one k bar, p two k bar.
00:47:22.430 - 00:47:26.474, Speaker C: But using this, you can say that is pk one.
00:47:33.814 - 00:47:37.074, Speaker A: So if you do this, you obtain.
00:47:37.414 - 00:47:40.158, Speaker C: The matrix you're looking for.
00:47:40.326 - 00:47:55.824, Speaker A: There is another reason which I mentioned this up to here. It's all the linear algebra notation that we use. There is another notation that we use mostly in operator theory.
00:47:56.164 - 00:47:56.904, Speaker C: So.
00:48:08.444 - 00:48:19.096, Speaker A: If h is a Hilbert space and x and y are two elements.
00:48:19.160 - 00:48:26.960, Speaker C: Of h, then x tensor y is.
00:48:26.992 - 00:48:43.104, Speaker A: An operator on H defined by this simple formula. The action of x tensor y on element z is the inner product of.
00:48:43.264 - 00:48:46.694, Speaker C: Z and y times x.
00:48:46.814 - 00:48:50.550, Speaker A: So it's finite rank.
00:48:50.582 - 00:48:57.590, Speaker C: Indeed, the rank is, if x is not zero and y is not zero, the rank is equal to one is.
00:48:57.702 - 00:49:03.286, Speaker A: Usually one rank operator zero or one rank operator on H. So z in.
00:49:03.310 - 00:49:10.194, Speaker C: A product with y and then times x.
00:49:14.534 - 00:49:31.446, Speaker A: So what is the connection between x tensor y and v star or cr as I defined here? Well, you have a column c, you have a rho I, you multiply by them and you obtain a matrix, say.
00:49:31.510 - 00:49:32.998, Speaker C: N by n matrix.
00:49:33.166 - 00:49:39.194, Speaker A: But this matrix, you can consider it as an operator on C.
00:49:40.934 - 00:49:42.942, Speaker C: So we come back here.
00:49:43.118 - 00:50:57.594, Speaker A: So if you have c and r a column, a row, the whole thing here is an n by n matrix. And now what happens if this acts on a z and z is in C? Okay, I want to make a connection between cr acting on z and x tensor y. To make it more clear, note that for all vectors, we consider them as column vector. So for x is true, it's no problem. We can say that our c is equal to some x, but for our r, we should say it's y transpose. Or, I mean, if you want to consider the, the conjugate two, you can say that r is equal to Y star.
00:50:58.644 - 00:51:00.544, Speaker C: There's no problem with that.
00:51:00.884 - 00:51:16.028, Speaker A: And now what is so X Y star is well defined. What is the action of X Y star on Z? Note that X is a vector like this.
00:51:16.116 - 00:51:18.144, Speaker C: Y is a vector like this.
00:51:19.324 - 00:51:43.520, Speaker A: So X Y star is a matrix. And then that matrix act on another matrix z, which is still out this form. It's just a matter of calculation. There is nothing mysterious about it. Write the component of X, the component.
00:51:43.552 - 00:51:45.524, Speaker C: Of Y, and the component of Z.
00:51:46.144 - 00:51:57.424, Speaker A: Do the all the products and simplify. And I mean, if you do the calculation, you see that this is nothing but the operator that we have in operator theory.
00:51:57.924 - 00:52:02.664, Speaker C: Acting on that, it's precisely the same thing.
00:52:05.644 - 00:52:09.424, Speaker A: Okay, so we will see this in.
00:52:12.404 - 00:52:18.604, Speaker C: The proof of trulexity. Any question up to here.
00:52:22.184 - 00:52:25.160, Speaker A: Okay, so let's have a break.
00:52:25.232 - 00:52:33.104, Speaker C: And after the break, we continue with the proof of trulexity.
