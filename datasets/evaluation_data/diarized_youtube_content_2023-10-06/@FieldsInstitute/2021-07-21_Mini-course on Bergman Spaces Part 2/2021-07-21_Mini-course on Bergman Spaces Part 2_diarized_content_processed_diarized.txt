00:00:00.640 - 00:00:23.274, Speaker A: Despite all my calendars and high tech gadgetries, I forgot what day it was. So I think today is Wednesday, and we have three talks today. Stefan Richter, Herve Kefalek, and Meredith Sargent. So we'll start with Stefan Richter's second talk on Bergman spaces.
00:00:25.174 - 00:00:56.824, Speaker B: All right, thank you, Bill. I'm going to share the slides of my talk here right now. That way there'll be some benefit for those people who showed up early. So today I want to talk about something that distinguishes the Bergmann space very much from the hardy space or the Dirichlet space. I want to talk about invariant subspaces of infinite index. So hopefully I can forward my slides. And also mostly I'll be talking about the Hilbert space situation.
00:00:56.824 - 00:01:51.328, Speaker B: And if I do mention something that's more general, it will be the banner space situation, but it will not cover any of the spaces with P less than zero. All right, let's start with the invariant subspace problem. So it's an open problem to determine whether that whenever you take a complex Hilbert space of dimension bigger than one, and you take a bounded linear transformation t on the silver space, then is there a closed subspace of H that's invariant for t. So t of x is in M for every x and m, and that M is non trivial. So of course, if m was zero or the whole space, then this would be true. And the question is whether this is always the case for any operator. All right, that's the open problem.
00:01:51.328 - 00:02:29.770, Speaker B: It's known to be faults for some wano spaces. That won't be important today. So there's a theorem that's called the sandwich that I will call the sandwich theorem. So let me explain what that is. So we write like Tom did, the lattice of T is the collection of all invariant subspaces of an operator t. And when I say invariant subspace subspaces are always automatically going to be closed in the norm topology. And let's say b is the Bergmann shift multiplication by z on the Bergman two space.
00:02:29.770 - 00:03:45.838, Speaker B: And then in 1985, Apostol, Berkovich, Feuersz and Piercy proved that if you could answer this one particular question about the invariant subspaces of the Bergmann shift, then the invariant subspace problem would have a positive answer. That is, every operator on a Hilbert space would have a non trivial invariant subspace. So the question is that given two invariant subspaces of the of the Bergman shift such that one is contained in another, and the difference of the two spaces has dimension bigger than one, so the co dimension of m in n is bigger than one. Then can you prove that there's an invariant subspace of the Bergmann shift that's properly between between the two? If you can, then the invariant subspace has a positive answer. So we'll call it the sandwich theorem, because you have the bottom and the top, the m and the n, and the good stuff you're looking for is what's in between them. So this is a question that's slightly harder to answer about if you wanted to answer it about any operator, but you just have one operator here where you have to answer the question, and in particular it's the operator we're interested in in this mini course. All right, so let me try.
00:03:45.838 - 00:04:23.476, Speaker B: So, okay, so sandwich theorem, you can say it could hold for some other operators too. It does not hold for the unilateral shift. So instead of b, you take the unilateral shift multiplication by z on h two. Then we do know that between any two invariant subspaces, you can find another one. But it doesn't tell us about the invariant subspace problem at all. But there are lots of other operators that have the same property. And so I'll say the sandwich theorem holds for a particular operator.
00:04:23.476 - 00:05:16.674, Speaker B: If whenever you can answer this type of a question, you have a positive answer to the invariant subspace problem. So the type of operators we'll consider will be multiplication by z on some Hilbert space of functions. Then we'll say h minus n is the author complement of the subspace n and m, and t is bounded below if t of f is greater than c times f for all f and h. And I'll always assume that, because then it's easy to prove that for any invariant subspace t of m, as I said, is a closed subspace of m for any invariant subspace. And then I can define the index of an invariant subspace. So the index is just the co dimension of tm in M. So tm is contained in M and so M mod Tm.
00:05:16.674 - 00:06:08.804, Speaker B: The dimension of that is the index of the invariant subspace. And if you think about the Hilbert space situation, then when you take t and you restrict it to m, then what I'm looking at here, M minus TM is just the author complement of the range, and then we know that's the same as the kernel of the air joint. So that's the dimension of the kernel of t restricted to m. So that's what the index is. So you can define an index, the index for banner space operators by this expression on the left. But for Hilbert space operators, you have these things available. All right? And so what Apostle Berkovich, Feuers, and Piercey observed was that if you have an invariant subspace of infinite index, then the sandwich theorem holds for this operator.
00:06:08.804 - 00:07:11.364, Speaker B: So the first thing I want to do is I want to explain why that is, and then I want to see, I want to explain how to prove this average theorem for, for the Bermuda space. All right? So we need to just go back and do a little bit of elementary Hilbert space operator theory. So if you have an operator on a Hilbert space that takes a Hilbert space M, into itself, then let's say you have a subspace l, and you can decompose m into l plus l purp, direct some l purp. And then you can write this operator r as a two by two operator matrix with entries a, b, c and D. And so, for example, c down here in the 21 entry goes from l to l perp. And the nice thing about this is you can look at pictures now when you're looking for invariant subspaces. So this subspace L would be an invariant subspace for this operator R if the two one entry here is equal to zero.
00:07:11.364 - 00:07:33.208, Speaker B: So I indicated that's the only thing that takes l to anything outside of. And so if that's zero, you have an invariant subspace. And I indicated by this, I put stars here. That means I don't really care what, what these operators are, the a, b and D. But it's important to have a zero here. This sandwich property is a little more complicated. So now you have three invariant subspaces.
00:07:33.208 - 00:07:55.744, Speaker B: M contained an L contained an n and the invariant. And, but then that means your operator looks like this. So you have the m, that's the first invariant subspace. There's a zero underneath the first star. Then the l is the second subspace. You have this two by two, four zeros under the two by two case up here. This zero won't matter for L being invariant.
00:07:55.744 - 00:09:16.142, Speaker B: And then similarly, n is invariant because you have three zeros here, and so it's invariant for this three by three space up there. All right? So by looking at these kind of operator matrices, you can c invariant subspaces. All right, there's a theorem from Caritas, and I'm stating here a version that I found in a paper by Apostol, Berkovich, Voyage and Piercy, that says that if you take a bounded operator on a Hilbert space that's bounded below and such that its adjoint has an infinite dimensional kernel, then the conclusion is that any Hilbert space operator that has norm less than this lower bound of c. For any such operator, there is an invariant subspace such that your operator r looks like this with respect to the decomposition of L being invariant for R and the a sitting down here in the two two spot. And so there's a couple of remarks here. So the first thing is, when I state this theorem, of course, I have to say every Hilbert space operator, I'll say on a separable Hilbert space, that's the only kind of Hilbert spaces that matter for the invariant subspace problem. But that comes with its own Hilbert space.
00:09:16.142 - 00:09:48.910, Speaker B: So that would be on some Hilbert space K or something like that. And of course, K may not be a subspace of the original space M at all. So when I write this, I don't really mean that the a is down there, but I mean there's an operator that's unitarily equivalent to a that acts on some subspace l purp takes l purp into itself. So I don't actually mean a, but I mean unitarily equivalent. But I'll just write a. I won't distinguish it. And then if you ever look up Keratus's theorem in the literature, it's typically stated for r instead of r.
00:09:48.910 - 00:10:32.164, Speaker B: So I've done that here for my presentation. So typically, it says that the dimension of the kernel of the operator is infinite and the operator itself is ontor and those conditions are equivalent. And then it doesn't have specifically the norm on this operator. And then also, Caradis's theorem is originally about banner space operators, and instead of unitary equivalents, it actually gives you similarity, which is also enough for the invariant subspace problem. But if you, that's where apostle Berkovich and feuers just state their theorem for the Hilbert space situation. And so they get this theorem anyway. So the proof of that is just about a half a page in abstract Hilbert space, and it's not very instructive to look at it.
00:10:32.164 - 00:11:25.980, Speaker B: It's nice if you know your Hilbert space theory, but it starts out like saying something like, well, take this infinite dimensional kernel of r and divide it into two infinite dimensional subspaces, and take a unitary operator that takes some part of this space that a works on and map it onto one part of this and so on. So you don't really see how this works out in detail. It's just that construction, and it's well understood since the 1960s. So Karidius's theorem is not the first theorem of this type. Okay, so now let's use Keratis's theorem to prove this observation of Apostol Berkovich four. Jason Piercey. So remember, we want to take an operator t on a Hilbert space that's bounded below, and we want to have an invariant subspace with infinite index.
00:11:25.980 - 00:12:12.152, Speaker B: And I want to say that if I have such a subspace, then the sandwich theorem holds. So if I have such a subspace with infinite index, I take for t, then I take t restricted to m. And now remember, the index of this invariant subspace is the dimension of the kernel of the adjoint of t restricted to m. That was one of the equivalent definitions of the index of the subspace. So this r satisfies that the dimension of the kernel of R star is infinite dimensional. And of course, if t is bounded below, then r is bounded below, because r is just the restriction of t to n. And so that means that when I consider the r keradis theorem applies.
00:12:12.152 - 00:13:02.682, Speaker B: So now I started out with the t r was t restricted to m. So that's my picture there. The m is invariant for t, so there's a zero here, and then I plug in that I can put any operator up to a scalar multiple as a two by two entry here for the r. So I expand this a little bit, and so up to a scalar multiple. Any operator a can sit here. So if I now want to prove that a has an invariant subspace, I can scale it, I can put it here. And an invariant subspace of a, as I showed you earlier, means a has, if I blow up a as a two by two matrix, there would have to be a zero down here, and then that picture corresponds to the other picture I had the intermediate invariant subspace between the two invariant subspaces that I have here.
00:13:02.682 - 00:14:04.450, Speaker B: So the going from the invariant subspaces with infinite index to the invariant subspace problem is just this keratocyst theorem. And so I will focus on the existence of invariant subspaces with infinite index. So if I told you earlier that for the unilateral shift you don't have keratus the sandwich theorem, and for the Bergman shift, you do. So for the Bergman shift you should have invariant subspaces of infinite index, while for the unilateral shift you don't. So let's investigate the index of an invariant subspace. So if we take the spaces we discussed here, hp or Bergman Lp spaces, or the Dirichlet space t multiplication by z, and let's take a sequence of points in the unit disk, there might be a zero sequence of your space that you started out with. If it is, I will deny, denote by I of lambda this zero based invariant subspace.
00:14:04.450 - 00:14:51.168, Speaker B: So that's all the functions in this space age that are zero at the points lambda n. Also, if you take a function in your space, then bracket f will be the smallest invariant subspace that contains f, so that contains f z f z squared f, and so on. And then linear combinations of these things, and the closure of that, that's the bracket fold. All right? And you get a very simple theorem here. Zero is the only invariant subspace of index zero. Remember, the index was the co dimension of z, m and m. So if you say, for example, if your invariant subspace contains a function that's not zero at zero, then zm, anything in zm will be zero at zero.
00:14:51.168 - 00:15:16.054, Speaker B: So m minus zm is at least one dimensional. So the only way that could be zero is you take the zero invariant subspace. For single functions, bracket f always has index one. You look at this here. So anything of the type zf, z squared f and so on, that's in z times bracket f. And so the only thing that's possibly not in there is the f. And so the index can be at most one.
00:15:16.054 - 00:15:58.966, Speaker B: But if f is not zero, then it's at least one also. So it's one. It's also very easy to prove that zero based invariant subspaces always have index one. So if you want to start out looking for invariant subspaces that don't have index one, then the first obvious examples of invariant subspace do have index one. Another thing that's not very difficult to prove is that the index of the closed linear span of two invariant subspaces is less than or equal to the sum of the indices. So if you want to construct an invariant subspace of index two, what you would do is you look at two cyclic invariant subspaces and see whether their span has index one or index two. All right.
00:15:58.966 - 00:16:39.082, Speaker B: Well, let's look at the HP situation first. So Berlin proved in 1948 that if you look at any non zero invariant subspace of HP, it's of the type some inner function phi times HP, and polynomials are dense in HP. So it's then easy to see that the bracket phi is the phi Times HP. So every invariant subspace is of the type bracket phi. So every invariant subspace in the HP situation has index one. For the duringly space, Alan Schiels and I proved that it's also the case. Any invariant subspace that's non zero has index one.
00:16:39.082 - 00:17:39.146, Speaker B: Oops. And now for the Bergman space. With Alexandra Alleman and Bill Ross, we proved in 1998 that if you take any of these Bergman spaces between p between one and infinity, let's say you take two functions, f and g, and we want to look at the ratio of the two. So I assume g is not identically equal to zero. If it turns out that the ratio of the two has finite non tangential limits on the set of positive measure on the circle, then the index of the span of these two is one. So if you'd like to construct an invariant subspace of index two, then you have to take two functions such that the ratio doesn't have non tangential limit on any set of positive measure. So that means you have to work really hard to find invariant subspaces of larger index larger than one.
00:17:39.146 - 00:18:33.254, Speaker B: So it's no wonder that it took until 1985 with the theorem of Apostol, Bercovitch, Voyage and Piercey to discover that something like that can happen. So with the easy functions that you start out with, that will never happen. All right, so in 1985, Apostol, Berkovich, Feuers and Piercey then showed that in the Bergmann space any index occurs. Hiddenmount was the first one in 1993 to give explicit examples of this, at least for the case of finite index. And then a little later, we worked out more precisely in Hindman's examples with Christian, SAP and Hokan. Together we were able to prove that the index can be infinite with these explicit examples, and that it also works in the banner space situation. So that's something that wasn't known here before.
00:18:33.254 - 00:19:24.466, Speaker B: But the idea goes back to Heidenbaum, is that he was looking at SAP's work on sampling and interpolating sequences. And using that work, one can tell that there are interpolating sequence, two interpolating sequences whose union is sampling. Now, eventually there'll be a mini course on sampling and interpolation. So I'm not going to even define this here, and it's, I'm not going to pursue this in this talk what sampling and interpolation are, but let's just go with these words. Then. It turns out that the closed linear span of these zero based invariant subspaces is already obtained by taking just the sum, the vector sum of these two things. So these have sort of, these spaces are at a positive angle and the vector sum is already closed.
00:19:24.466 - 00:20:02.878, Speaker B: And so you don't have to take the closure here. And then it's fairly easy to prove that since each of these has index one. The vector sum has index two. So that's what Hindenbaum's idea was based upon. So of course that's very nice. It tells you what these subspaces are where this happens, but it doesn't tell you, for example, which other operators have invariant subspaces within infinite index, because we don't have Seibes theorem available in other spaces. And it also uses the concept of sampling, which is a little bit of overkill, which I want to explain now.
00:20:02.878 - 00:21:24.768, Speaker B: So what I want to do is I want to actually prove or outline the proof of this first theorem here and show you that you get by with, will use interpolating, but you can get by with dominating sequences. All right, so what are we after? We're looking for unusual invariant subspaces in the sense we're looking for these invariant subspaces with infinite index. So one type of unusual invariant subspace comes up when you ask whether operators have spectral synthesis. So the question is, does an operator a have spectral synthesis? That means that does every invariant subspace of a contain a nonzero eigenvector? So for example, if your Hilbert space is finite dimensional, then you know by the John canonical form for the operator, you can decompose the operator into these Jordan blocks. Each restriction to invariant subspace corresponds to one of these lambdas that you have on the diagonal there, which is an eigenvalue. So in finite dimensions, every operator has spectral synthesis. If you go to infinite dimensional situation, then the unilateral shift is an operator that has no eigenvalues, and so trivially it doesn't have spectral synthesis.
00:21:24.768 - 00:22:23.358, Speaker B: So that doesn't seem fair to look at such operators. But if you look at the adjoint of the unilateral shift, then Tom and his talk showed us that the adjoint of multiplication by z always has plenty of eigenvectors, namely each reproducing kernel is an eigenvector for the air joint. So when you look at S star applied to S lambda, S lambda is the z kernel, the reproducing kernel for, for the hardy space h two, then you get lambda bar times s lambda. So each lambda bar in the open unit disk is an eigenvalue. And the question now for whether a star has spectral synthesis makes some sense because you have plenty of eigenvalues. And the question would be, does every invariant subspace of S star contain an eigenvector? And the answer that Berlin came up with, and that's why he actually wrote his paper in the first place, was one of the questions he wanted to answer is the answer is no. S star does not have spectral synthesis.
00:22:23.358 - 00:23:13.884, Speaker B: Namely, there are these singular inner functions. They don't have any zeros in the open unit disk. And then there can be no reproducing kernel that's orthogonal to the invariant subspace generated by the singular inner function. And that would just be the reproducing kernel would just be in the author complement of that invariant subspace, which is invariant for the adjoint operator. So one of the questions that Birling answered was that S star does not have spectral sentences. Well, a few years after Borling considered which diagonal normal operators have spectral synthesis. So he asked the question, let's take a bunch of points, lambda lambda I, and put them on the diagonal of some operator that acts from l two to l two.
00:23:13.884 - 00:23:58.684, Speaker B: And he was wondering, does such an operator always have spectral synthesis? So obviously this has plenty of eigenvectors. Each basis vector en is an eigenvector for this normal operator n. And Werma answered his question. And the condition that he came up with later came up in a paper by Brown, Shields and Zeller. And so I've combined these two theorems on this slide. So, first of all, Wrmer's theorem, the way I've stated it here, condition five says that the other four conditions are equivalent to the fact that n does not have spectral synthesis. So that means n does have an invariant subspace that has no, that contains no eigenvector.
00:23:58.684 - 00:24:38.606, Speaker B: So in fact, N has some unusual Invariant subspace. So that's the kind of thing we're looking for here. And so the conditions that turn out to be equivalent to this is. So the first one here says that almost EVery Point on the unit circle, there's a subsequence of the LambDA n's that converges non tangentially to that point. So remember the non tangential approach regions? I had them yesterday. They look like this. So this theorem is saying that for almost every point on the unit circle, you have to have infinitely many of the Lambda N's inside of this non tangential approach region so that you can get points that converge to it.
00:24:38.606 - 00:25:19.566, Speaker B: So that has to happen for almost every point on the circle. So these Lambda ends have to be all over the place. All right? So that's the first condition. The second condition is that every age infinity function takes its supremum norm, the supremum over all the absolute values of f of z over the open unit disk on these lambda n's. So that's the same as just taking the supremum on the lambda n. So obviously the supremum over the lambda n is smaller than this. But, for example, if your lambda n's form a Blaschke sequence, then we know there's a blasker product that's zero at all of these points.
00:25:19.566 - 00:26:07.464, Speaker B: Then the supremum on the right hand side here would be zero, but the blasker product is not zero. So there would be a function that where this is not satisfied. But the condition says for every function, this would have to be satisfied. So Blasker sequences will never satisfy this, and that's in line with the first condition. So perhaps the equivalence of these first two conditions intuitively makes some sense, because we know that for functions in h infinity, we can always take radial limits almost everywhere on the circle, and we get an l infinity function whose essential norm is the same as this h infinity norm. And so we should be able to take these non tangential limits and get the infinity norm. And so that's why we need these points in almost every non tangential approach region.
00:26:07.464 - 00:26:53.154, Speaker B: All right, condition three is a condition that's sort of more interesting. I'll discuss that in a moment in more detail. But that says that for each lambda in the disk, there's a sequence an in little l, one such that you can write the value of q of lambda for every polynomial somehow, in terms of the lambda n's. And the animation and condition four is easily seen to be equivalent to condition three. I'll show you that in a second. But that says something very similar, that for every lambda in the disk, you can get this q of lambda by taking the inner product of q of n with this. So let's look at the equivalence of three and four.
00:26:53.154 - 00:27:38.512, Speaker B: So, first of all, I'm going to go back to the page in a moment. But since you have five equivalent conditions, it makes sense to give a name to this kind of concept. So we call a sequence dominating if it satisfies these conditions. All right, so the equivalence between three and four four is the condition that every polynomial, for every point lambda, there are vectors x and y and l two, such that when you look at q of n xy in a product, you get q of lambda. Well, if you're an l two, it's a sequence given by sequence xn and sequence yn. Then you look at q of n applied to x. You had q was a diagonal lambda n.
00:27:38.512 - 00:28:09.904, Speaker B: I mean, n is a diagonal operator. Q of n is a diagonal operator. So this gives you the sequence q of lambda n xn. So if you take inner product with the the yns, you just get that q of n xy is this infinite sum. And of course, if xn and yn are l two, then the product gives you a little l one sequence, and that gives you the q of lambda. And then conversely, any l one sequence can be factored as a product of two l two sequences. So clearly conditions three and four are equivalent.
00:28:09.904 - 00:28:56.726, Speaker B: And then condition three and four are equivalent to the fact that you don't have spectral synthesis. And in fact, the invariant subspace that doesn't contain any eigenvalues, eigenvectors is the one invariant, the singly invariant generated invariant subspace bracket x generated by this vector x. But that'll just be motivation for us. So we won't need to know how to actually prove that. All right, now, the real action was going in. The implication between two and three. Remember, condition two was that somehow we have these sequences that accumulate just about everywhere on the circle.
00:28:56.726 - 00:29:25.644, Speaker B: You have this isometry condition here, and then in condition three, that is where there exists something. So for every lambda there is an an. So there's the existence that comes in. So that will give us the invariant subspaces later on. So that's the important direction for us. This was something gets constructed. So condition two says that the operator from h infinity to little l infinity that takes a function h to h of lambda n is isometrics that you take the soup over.
00:29:25.644 - 00:30:27.766, Speaker B: This is the same as the soup in h infinity. Now, a little bit of functional analysis tells you that when you look at the quotient space of l one on the unit circle and h 10, that's the h one functions that are zero at the origin. Then that has dual space h infinity. The dual of l one is l infinity. And then you know that the dual of a quotient space is the annihilator of the space that you have down here. And the annihilator of h 10 is just h infinity, h 10 being the functions in h one that are zero at the origin. And you can define an operator w that takes little l one into l one mod h 10 by just taking an l one sequence an to the equivalence classes of the type summation a and p lambda, and where the p lambda n's are the Poisson kernels, the Poisson kernels have l one norm one.
00:30:27.766 - 00:31:09.144, Speaker B: So this is obviously something of l one norm bounded by the l one norm of the ans, little l one norm of the ans, and the quotient norm makes it smaller. So the w is a contraction, and it turns out its adjoint is this operator v that I had over here. So the calculation is done here. So if you look at w applied to a vector a in little l one, and now you take the dual action with an h in h infinity. So then wa is in l one mod h one. And so the w of a is just the sum here with the Poisson kernels, and you can take the sum out. You have absolute convergence.
00:31:09.144 - 00:31:40.162, Speaker B: You just have the Poisson kernels, and they evaluate the h at the lambda n. And so you get the sum an h lambda n. So that's the hn here. So that's the v of h in the l one little l, one l infinity duality. So that proves that w stars is v. And since we know that v was isometric, it means w is isometric. That means w has closed range, and so w has closed range.
00:31:40.162 - 00:31:50.130, Speaker B: And, and then you conclude that w in fact has to be on to, because w is one to one. And then the range of w has to be dense.
00:31:50.162 - 00:31:50.300, Speaker C: So?
00:31:50.322 - 00:32:29.444, Speaker B: So it's dense and onto. So w is onto. And once you know that, then it's easy to finish the proof. You pick a lambda in the disk, and because the w is onto, you take a Poisson kernel with base point lambda, and you find a sequence a and a little l, one such that w of a is the p lambda. And that does what you wanted to do. That was the conclusion. So the existence of this a and in fact, then the existence of these invariant subspaces follows from this miraculous fact that this operator v is isometric.
00:32:29.444 - 00:33:06.338, Speaker B: And that means its pre adjoint is onto. All right, now, as a corollary to what I just did is I can spice this up a little bit. I can take a sequence of sequences. So let's lambda k for each fixed k now be a dominating sequence. So now I have a double doubly index subscripts here. So lambda nk, and each of them are dominating. And for each of them, I take the diagonal normal operator with those lambda n's on the diagonal.
00:33:06.338 - 00:33:32.782, Speaker B: Actually, I put lambda n bars on there. That sequence will be dominating if and only if the lambda ends are dominating. And then I take the direct sum of orthogonal direct sum of these operators nk. So the nk's were the ones that showed up in Wehrma's theorem a moment ago. For each nk, I get a vector x, k and Yk such that I have this condition four. I think it was satisfied. And now all of these spaces are orthogonal to one another.
00:33:32.782 - 00:34:29.466, Speaker B: So it's clear then that when I look at these sequences that I get, I have orthogonality. So if k doesn't equal j, then I just get zero all the time, because these yjs are going to be orthogonal to everything that I can get from looking at p of n x k. All right, so that's the corollary we get out of Wehrma's theorem. And I have one slide I wish I didn't have to do here, but I'll blame Apostol, Berkovich, Voriazh and Piercey for that. But I want to connect this diagonal normal operator back to the Bergmann shift. And what I'm saying here is that I can get a subspace of infinite index if I can get a similar statement for the adjoint of the Bergman shift. So over here, what I've proven is, for this diagonal normal operator, if I can get something like this, and I'll take lambda equals zero now, so I can get get this for every lambda.
00:34:29.466 - 00:35:12.072, Speaker B: So I'll take, in particular, I can get it for lambda equals zero. So instead of the n, if I can get mz star there, then I have a subspace of infinite index. So if I, this is precisely this condition. And now I need to have some vectors gj and fk, that satisfies this with an m z star there instead of n. And then I claim that that means this has infinite index. Well, what I have to do is, after proof, there's a connection of infinitely many linearly independent vectors in n minus c n. So, and I claim so, the Fn's span, the n, and the gjs here are the ones that I used to prove that I have infinite index.
00:35:12.072 - 00:35:49.414, Speaker B: So, I look at the projection onto n of the gjs, and I prove that they are linearly independent in here. So let me, both of these are rather simple. Now, with these conditions, let me do the first condition. So, let's say you have a linear combination of the p and g j that's equal to zero. Then you can take, of course, the pn out of the sum. And that means that the Aj Gj is orthogonal to n. If I look at the sum Aj Gj and I take the inner product with fk, the fk is an n.
00:35:49.414 - 00:36:28.294, Speaker B: Then I get zero. On the other hand, by linearity, I can take the a j's out and gjfk, that was equal to one. By my condition here, that when I take polynomial p equals one, then I get one or zero, depending on whether j is equal to k, so I get ak, and so each ak has to be zero. So these are linearly independent. And then the other condition is a similar proof. Now I'm using this with polynomials that are zero at the origin. All right, now how do I make the connection between the diagonal normal operator and the backward Berggruen shift? Well, a sequence is orthonormal if it satisfies the pythagorean theorem.
00:36:28.294 - 00:37:00.548, Speaker B: We can loosen that up a little bit. We call a sequence a res sequence. If there are constants little c and big c such that the norms of a n u n summation a and un are comparable to the little l, two norms of this. So you have the constants here. When both of these constants were one, they would be orthogonal. So resequence in certain sense is almost orthogonal. So if you take, in particular, if you take all the ans to be zero except for one of them, you take that one to be one.
00:37:00.548 - 00:37:44.474, Speaker B: Then you see that the norm of each of these vectors, un, would have to be independently of n bounded above and below by these constants. So it makes sense to look at un's that have unit norm. So if I take a normalized reproducing kernel like shell, and yesterday I'll call capital k sub z to be the normalized Bergmann kernel. So I take the Bergmann kernel k z of w, and divide it by its norm. It's a unit vector. Now, and so here's the form of it for the Bergmann space. And I say that sequence lambda n is an interpolating sequence for l two a if and only if, these normalized Bergmann kernels form a resequence.
00:37:44.474 - 00:38:35.114, Speaker B: And usually the condition of interpolating sequence is stated for some sort of an air joint operator. But today I'm just doing the opposite of what people are usually doing. I consider the air joints. So, interpolating sequence then means somehow some operator that takes a sequence en in little l two to the sequence un is bounded above and below. All right, so now suppose we have a interpolating sequence for l two a, and we have this diagonal normal operator with the lambda n bars on the diagonal, and we look at the subspace k. That's the linear span of these normalized reproducing kernels. That just turns out to be the author complement of the zero based invariant subspace with the lambdas.
00:38:35.114 - 00:39:20.502, Speaker B: But it's a subspace of l two. And now this operator s that I had induced by this reset sequence property that takes en to un or the k lambda n here. That is an operator that was bounded above and below. And if I take my range to be just the span of these k lambda n's, then it's an invertible operator. And it turns out that when you look at sn and m z s, you get the same thing. So if I check it on one of these orthonormal vectors, n e, n is lambda bar en, sen is k lambda n. So it's lambda bar k lambda n.
00:39:20.502 - 00:40:26.388, Speaker B: And over here I get sen is k lambda n and m z. While I'm applying mz star to a reproducing kernel, I get lambda bar times the reproducing kernel. So what I get is that this normal operator, this diagonal normal operator, is similar to multiplication by z star, restricted to the subspace h here. And that means I can take my vectors xi and yj that I had constructed for the diagonal normal operator and get them over here and get my gis and fjs that I needed to construct these spaces with infinite index. And so, as a corollary to this construction, we get back to the Bergmann space. Now, if there is an interpolating sequence for the Bergmann space that is a countable infinite union of mutually disjoint dominating sequences, then invariant subspaces with infinite index must exist. All right? And so it should be no surprise now that where this was headed, that such sequences do exist in the Bergmann space and that they don't exist in the Hardy space.
00:40:26.388 - 00:40:55.568, Speaker B: So that's one big difference between Bergman and Hardy spaces. So let me explain how that works. Construction of such sequences. So here's the points in the picture. So I choose a sequence of radii rn, one minus one over n. So these rns will converge to one. And then on each of these circles that have radius rn, I choose n equally spaced points.
00:40:55.568 - 00:41:40.336, Speaker B: So I take the nth roots of unity rn times e to the ij over n. So the picture is like this. These are all the points that I'll look at for a moment. And on each circle of radius rn, I have n points equally spaced. And then what I'm going to do is I don't use all of these circles. I will explain how to choose a sequence nk going to infinity, such that when I choose all the points on each of these circles, that I'll select, but not all of the circles, but then take infinitely many of these circles, then that's going to be my sequence, that's going to do the things that I said it needed to do. So let's see how this works out.
00:41:40.336 - 00:42:21.204, Speaker B: So the first thing is that no matter what I choose for this subsequence nk, every point on the unit circle is going to be non tangential limit of points from this. So here's a typical point on the circle on the boundary and it's non tangential approach region. And now if I make the angle big enough, the way I've chosen my points is that the distance of a particular circle to the boundary is about the distance between any two points that lie on a circle. So on a circle I have n points. So the distance will be two PI over n times whatever the radius is. Rn. So rn is approximately one.
00:42:21.204 - 00:43:01.654, Speaker B: So the distance between two points on the circle will be approximately two PI over n, and the distance to the boundary is one over n. So they're comparable. So if I make the angle, the opening angle big enough, I will always have at least one point in there. So I always have an infinite number of points converging to the boundary point. So every point is a non tangential limit. And also, if I have chosen a subsequence of circles, then I can write it as infinitely many subsequents, each of which are dominating. Because for example, let's say this is the subsequence that I have, I can take for my first sequence, capital lambda one.
00:43:01.654 - 00:44:13.386, Speaker B: I can take all the points on this first circle, all the points on the third circle, all the points on the fifth circle, and then capital lambda two could be all the points on the second circle, all the points on the 8th circle, all the points on the 16th circle, skip a few, and then the third one will be whatever I have left. So I can divide it up into infinitely many subsequences that are dominating. So I only have to show that I can find such a sequence that is interpolating for l two a. And so the first step in that is that I can't say much about, is an application of Carlisen's h infinity interpolation theorem and the Goethe Tuplitz theorem. Tom, in his talk mentioned that Marshall and Sandberg proved characterized the interpolating sequences for the Dirichlet space. And one of the big takeaways from their paper was that when you look at interpolating sequences for the multiplier algebra, it's the same as the interpolating sequences for the Hilbert space with constants. And that's true in my generality.
00:44:13.386 - 00:45:06.614, Speaker B: So anytime you have a setup where you have a Hilbert space with the multiplier algebra, the interpolating sequences for the multiplier algebra are the same as the interpolating sequences for the Hilbert space. No, not the same. Any interpolating sequence for the multiplier algebra is interpolating sequence for the Hilbert space. And so for the Bergmann space, h infinity is the multiplier algebra. And so, on each particular circle, you can use Carlson's theorem to prove that you have the condition satisfied with the fixed constant. That doesn't depend on the circle that you're on. So on each particular circle, if you keep the n fixed, then you get the interpolation condition with fixed constants.
00:45:06.614 - 00:46:03.028, Speaker B: All right? And now the crux of the matter is that if you change from the nth circle to the nth circle, you can sort of take an arbitrary linear combination of vectors of the normalized reproducing kernels on the nth circle and arbitrary vector linear combination of vector on the nth circle. You can make that inner product arbitrarily small compared to the size of the vectors by just choosing the m large enough. So the second circle far enough out. So that's where the crux of the matter is. And so I've written out the proof of that over here. And so, if you look at the inner product of two reproducing kernels. And so I've written ZJ now for the lambdas on the nth circle and wi's for the lambdas on the m circle.
00:46:03.028 - 00:46:23.476, Speaker B: To drop the double subscript, the normalized reproducing kernel. So I have the one over the norm on the top. So that gives me one minus CJ squared on the top. And I have the reproducing kernel terms. On the denominator. One minus Cj squared is one minus Cj times one plus Cj. So that's less than two times one minus CJ.
00:46:23.476 - 00:46:51.642, Speaker B: And one minus CJ was one over n. And so that's less than two over n. Same for the w's, so that's less than four over n times m. And then in the denominator, I say let's do away with the wj. I just get a number here for the z I's that gets closer to the unit circle, and that makes this smaller. And if I take the absolute value of the zj's, I make that even smaller. That makes the expression bigger.
00:46:51.642 - 00:47:26.244, Speaker B: But I have a square here. So one minus zi, that is the z I is one of the circle with the n. So that's one over n. So I get one over n squared here, cancelling so I get four times n over m. And now if I take my arbitrary linear combinations of these normalized reproducing kernels, I just do the worst possible thing. I take the inner product of them, I go in with absolute values, I take out the sums, I use the estimate that I had up here that's independent of I and j. Now, so then again, I use the Cauchy Schwarz inequality.
00:47:26.244 - 00:48:07.246, Speaker B: I use the worst possible thing I can do. I say summation. Absolute AI is less than or equal to square root of n times AI, the l two norm of the AI's, and same for this. So now I get an m squared of m here, but I do have an m. So when I do the cancellation, I have a square root of m left, and I have an n here, which could be big. But the question was, can I choose the m large enough to make this as small as I like? And I see this works. If instead I take the Hardy kernel, the Zeger kernel here, then that reduces the powers.
00:48:07.246 - 00:48:36.292, Speaker B: So for the Zeger kernel, I just get the first power. I get a square root on the top here. And what that does is I get square root of n over m here. And what happens then over here is the square root of n over m cancels with the m, and I don't have a square root of m. And this proof fails. And this proof will work, actually, if you're invariant, if your kernel has a power bigger than one here. So it works in wider generality than just the Bergmann space.
00:48:36.292 - 00:49:46.874, Speaker B: And I haven't written out the proof now of how to go from here to get actually the interpolating sequences. But that should be clear as a technicality. So now, as I'm ending with a corollary to this, so this shows then that you have invariant subspaces of infinite index. It doesn't really show exactly where they are. There was this proof in this part of the proven Wormer's theorem, where we get this existence of these sequences an, and we didn't really know, we did know it had something to do with dominating sequences. And if you look carefully at that proof, what you can prove is that if you have a zero sequence for l two a that happens to be dominating, then it turns out you can choose a dominating subsequence that happens to be interpolating, and then you can find an invariant subspace of infinite index that contains the zero based invariant subspace based by these lambda N's. So it gives you a little bit of an idea of where to look for invariant subspaces of infinite index.
00:49:46.874 - 00:50:46.410, Speaker B: With Alexandru Allemann and Carl Sandberg, we proved that if you have an interpolating sequence, interpolating sequences are always zero sequences. But if you start out with an interpolating sequence, then it's dominating if and only if you have this property. Unfortunately, if you just have a zero sequence, it turns out there are zero sequences that accumulate non tangentially only on sets of measure zero, and they still have this property. So there's no converse in the general setting. And then my last slide is, I want to end with an open question. The question that we were trying to answer is that give a necessary and sufficient condition on the function f in the Bergmann space such that every invariant subspace that contains f has index one. So that would be kind of.
00:50:46.410 - 00:51:39.216, Speaker B: We were looking for a condition that would somehow say, in some sense, if f is a nice function. In some sense. So we know, for example, if the function is in h infinity, then it will have this property. In fact, we can do better. We can prove that if the function is in h infinity of some subset of the unit disk, and this subset, I'll explain here, is a union of stals regions over some set of positive measure on the circle. So it's a Stoltz star over a subset of positive measure. And if your function is bounded in that, and if this set of positive measures satisfies this well circalizing condition, again, that the complementary arcs satisfy this finiteness, then we can show that every invariant subspace that contains f has index one.
00:51:39.216 - 00:51:50.184, Speaker B: But unfortunately, without this condition, this theorem is not true. So it's false. So we were unable to answer this question, as far as I know, is still open.
00:51:50.724 - 00:51:51.060, Speaker A: So.
00:51:51.092 - 00:51:54.864, Speaker B: All right, that was the last slide of my talk, so I'll end here.
00:51:56.484 - 00:52:16.314, Speaker A: Oh, let's thank stefan for a wonderful talk. All right, you guys know the drill. Now, either post a question in the chat or just shout it out. There are over 100 of you, so I can't monitor everything.
00:52:16.394 - 00:52:17.014, Speaker B: So.
00:52:30.274 - 00:52:42.454, Speaker A: And stefan posted his. For those of you who showed up to class late, stefan posted his slides in the chat so you can catch up on what you missed.
00:52:46.424 - 00:52:47.512, Speaker C: Hello? Please.
00:52:47.568 - 00:52:48.952, Speaker A: Yes, come on, please.
00:52:49.128 - 00:52:50.284, Speaker C: I have a question.
00:52:51.304 - 00:52:52.484, Speaker A: Please go ahead.
00:52:52.944 - 00:53:20.148, Speaker C: Yes, hello, rector. Thank you for your very nice lecture. I have a question. When you talk about Carlosson interpolation sequence, exactly .3 in your proof, .3, I didn't understand how it came. It comes to select colors of sequence.
00:53:20.148 - 00:53:21.532, Speaker C: Yes, it has.
00:53:21.668 - 00:54:16.996, Speaker B: Right. So what I'm saying here is that on each of these circles, individually, there's only finitely many points. So in a certain sense, obviously, it's an interpolating sequence because you just have finitely many points. But you can look at the Carlasson that comes up in the condition of Carlisson's theorem and look at the constants that are there. So the conditions in Carlson's theorem are that the points have to be separated in the pseudo hyperbolic metric. So that means that if I just look at these points on the circle, the distance between any two such points has to be bounded below by a constant that is comparable to the distance times the distance to the boundary, which is precisely what I have here. So at each of these circles, I get the same constant in the separation.
00:54:16.996 - 00:54:48.092, Speaker B: So if I look at the distance from this point to this point on this circle with respect to the pseudo hyperbolic distance, it's the same as over here. It's the same constant. And the second part in Coliseum's theorem is the coliseum measure condition. And there's the same kind of thing that you look at the number of points in the coliseum boxes. And again, you get the same constants for each of these circles. And so that's why I wanted to go a little bit quick here. But for each of these circles, you get the same constants.
00:54:48.092 - 00:54:58.230, Speaker B: And that's how you get that. Independent of the n, the constants are the same for each of these layers that I end up with.
00:54:58.382 - 00:55:01.674, Speaker C: Yes. Thank you very much. Thank you. Thank you very much.
00:55:04.214 - 00:55:31.774, Speaker A: Any other quick questions before we have to move on? Otherwise, I'm sure Stefan will answer your fan mail if you have anything else you want to add. All right, so let's take a five. Oh, let's thank Stefan for again. So let's take a quick break while we get set up for the next talk. Herba, are you around somewhere?
