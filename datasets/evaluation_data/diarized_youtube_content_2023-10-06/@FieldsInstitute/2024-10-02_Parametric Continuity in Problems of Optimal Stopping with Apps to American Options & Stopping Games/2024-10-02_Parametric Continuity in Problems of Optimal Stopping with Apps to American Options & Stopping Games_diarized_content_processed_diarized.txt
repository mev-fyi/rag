00:00:02.280 - 00:00:23.465, Speaker A: Morning, everyone. Welcome back to the day two of the Eastern Conference in Math Finance. And without further ado, I will introduce our first invited speaker of the morning, and that is Stephen Campbell. He is currently at Columbia University in the Department of Statistics. And today he will be telling us about parametric continuity in optimal stopping. Steven, take it away.
00:00:23.505 - 00:00:55.985, Speaker B: Thank you so much. Thanks everyone for coming in the morning and thank you very much for the invitation. It's a real pleasure to be here. So today I'm going to talk about parametric continuity and problems of optimal stopping. And because this is a Math Finance conference, I'm going to also look at some applications to American options and stopping games. This is some ongoing work that I have with two PhD students at Columbia, Georgie Gates Gorey and Richard Groenwal, and their advisor, who everybody knows, Yanis Karatsis. So let me just set up the initial optimal stopping problem in some generality.
00:00:55.985 - 00:01:48.283, Speaker B: Most of you will probably have seen some version of this here. We're going to be looking at optimal stopping problems that are parameterized by some alpha in a topological space A. And so the standard optimal stopping problem begins with, you have a sequence of rewards here going to be denoted by Y. And I'm going to try to maximize my expected payoff by stopping in some set of feasible times. And so for each alpha, we're given a reward process, a terminal horizon. Okay, the reward process satisfies some kind of conditions, some boundariness conditions, and we're also going to be given a set of admissible stopping times for our filtration. So, okay, to convince you that this is not too different than a standard setup, let's look at a simple example where you're dealing with a Markovian problem.
00:01:48.283 - 00:02:43.945, Speaker B: So here the parameter alpha is just the initial data that you're given for the problem. So you have some Markov process X that started at time T and state X, and you're trying to maximize your payoff before sometime T minus t. So it's a finite horizon optimal stopping problem where you have a fixed terminal horizon, the natural filtration of your Markov process, and a reward process which is given by some continuous gain function of your state. So, of course, this is not the only parametric setup. You can enlarge your space to include additional parameters that determine the cost function G or the reward function G, or the behavior of the Markov process. So you can think about making geometric Brownian motion, one of the parameters being the volatility of the Brownian motion. Additionally, from a game setting, you may want to have a parameter that kind of governs the interaction with the population that you're, you're playing the game with.
00:02:43.945 - 00:03:21.141, Speaker B: So this is the kind of setup. And so here's a very incomplete list about the optimal stopping literature. So this is totally non exhaustive and somewhat biased by the content that you're going to see that follows. So many people have worked on optimal stopping theory, including Sergeyev, Kuratsis, Lamberton and Davis. And people have also in recent years been looking at games. So actually some form of stopping game goes quite far back like Dinkin games, zero sum stopper games go back to people like Bismuth, Friedman, Krylov and Morimoto. And in recent years, say from 2016, 2017, there's been a lot of work in mean field games of optimal stopping.
00:03:21.141 - 00:03:51.085, Speaker B: So Marcel has done some work in this area. Daniel Lacquer and Rene Carmona have looked at this stuff from a PDE approach. There's this guy, Charles Bertucci that also did something around the same time and Nizar has also been working on problems of this type. So you have some mean field type optimal stopping problems that he has looked at recently as well. And so yeah, very non exhaustive list. Many people have contributed. So it's a very rich literature and hopefully I've convinced you that many people are kind of interested in this topic.
00:03:51.085 - 00:04:50.151, Speaker B: So let me just move on from there to give you an overview of the main idea of optimal stopping. So kind of core to solving an optimal stopping problem is this idea of a Snell envelope of your reward process. So it's the essential supremum of the conditional expectation of your reward based on the stopping policy, your optimal stopping policy. And so we have this Snell envelope Z and we're going to denote by A and M the finite variation and Martingale parts of its Dubmeyer decomposition. Standard theory tells you that the optimal stopping time is the first time that your word process is equal to your Snell envelope. And this is intuitive if you haven't seen this before, because it's basically saying that the, your best time to stop is when the future looks just as bright, the present looks just as bright as the future. So the Snell envelope is like your belief about how good things can possibly be.
00:04:50.151 - 00:05:39.614, Speaker B: And your reward process is the reward you can get today. So once today looks about as good as it's ever going to get, you stop your you choose to stop. So this tau is your tau star is your optimal stopping time. And this sigma star, which is the first time that your finite variation part increments is the largest. Okay, under some conditions here, in our case, this person is going to be continuous under the assumptions that we've made under the conditions that this time is finite or it coincides with the smallest optimal stopping time. This time here, the first time that it jumps, is going to be the largest possible optimal stopping time for your problem. Okay? So a fundamental result that's critical to the analysis that I'm going to show you today is this result of Davis and Kuratsis that dates back about 30 years ago.
00:05:39.614 - 00:06:39.655, Speaker B: Exactly 30 years ago, actually. So what they managed to do is they managed to create a direct link between an optimal stop, optimal stopping problems, and a pathwise optimization over time. So the result basically says here that if I optimize overall stopping times, it's the same as taking an expected value of a pathwise maximum of my reward, but penalized by the Martingale part of the Snell envelope. So they make this relationship. So kind of a mortal who only needs can use, you know, stopping times, can match the reward of the profit up to a penalty, up to this Martingale penalty. And moreover, not only do they manage to match the reward, but the optimal stopping time actually attains the supremum on the right hand side. So the optimal, if you do this pathways optimization, which you get back, the smallest value will actually be the supremum and the sigma stop become the largest time will under the conditions that it's actually optimized.
00:06:39.655 - 00:07:45.969, Speaker B: So this is a very nice link to between optimal stopping and traditional optimization. And this is going to form the core idea of what we do. The idea is if we want to get some results in optimal stopping, we can think about more classical approaches to getting these results in traditional optimization theory and use those to pass back and forth. Here is, let's say our main abstract result. So if we assume that you're mapping to your terminal time is continuous and the penalized process is continuous in your parameters on the graph of the correspondence here, then if you fix a sequence that converges to a parameter, you have this notion of lower semi continuity of your smallest optimal stopping time and this upper bound as well. In particular, if your smallest and largest stopping times coincide, then you have this continuity type result for your smallest optimal stopping time. In addition to this, we can derive some continuity properties of the value function.
00:07:45.969 - 00:08:16.323, Speaker B: But this is less important because typically you can show this using kind of standard arguments. Can you remind us T alpha capital T alpha would be like the terminal horizon of your problem. So in the Markovian case it could be Like t minus T, like capital T minus little T. You can also fix it. If you'd like to fix it, you can fix it. So not all problems will have a time dependent terminal horizon. Alpha is some topological parameter.
00:08:16.323 - 00:09:00.185, Speaker B: Yes. So, yeah, so this is the setting. We get some kind of continuity property of the smallest optimal stopping time. And one of our main applications that we'd have in mind is trying to look at this with applications to games. So when you're looking at games, usually you want to incorporate some notion of population interaction either into your reward or the dynamics of your state. And often this interaction is introduced through the law of your stopping time or your state or the reward that you get the distribution of these times or rewards. So for concreteness, let me just assume that we have a mean field of homogenous players and reward processes that are bounded.
00:09:00.185 - 00:09:38.871, Speaker B: But this can be relaxed. Basically, I just want to work on a space of laws that is weakly compact otherwise and convex. But otherwise this can be relaxed in main directions. So a representative in your game is going to face an objective of this type where you're going to maximize the reward, but the reward will depend on the law, the distribution of the population. And so the distribution here is going to be on purpose in this weekly compact set. And the idea of solving the game is that we're going to look for a fixed point where we take an input measure and we look to see what is the law of my best response to that input measure. And I look for a fixed point.
00:09:38.871 - 00:10:51.285, Speaker B: So I can say that this is going to be an equilibrium of Nash type for the game. So perhaps not surprisingly, the approach to doing this is what is often done in optimal stopping games. You want to use a continuity type fixed point theorem. The idea is, let me try to get continuity of my mapping from my control to the laws and then use the fact that I have a weakly compact convex set and apply Schader's theorem to get a fixed point. So of course, because we have this strong notion of continuity, almost sure convergence and continuity of the processes under the assumption that this is continuous and the largest and smallest stopping times coincide, which they do for many problems, weak convergence of the measures corresponds to weak convergence of the laws. And then an application of Schader's theorem gives us that there exists an equilibrium for this game. So I should say maybe I've convinced you that this is great, but there's this sneaky thing going on here that there's the Martingale part of the Snell envelope and we want to kind of make sure that this is not, we're not assuming the conclusion basically here.
00:10:51.285 - 00:11:48.017, Speaker B: So one natural question to say is that if the reward person is continuous, can we actually cope for the continuity of the Martingale part of the Snell envelope, which so far has been quite abstract. Well, we can look at Markovian problems. So if we have a parameter space that depends on the initial state of our process and potentially some other parameter, then the Snell envelope can often be expressed as the value function composed with the state process. And so the Snell envelope itself can inherit continuity from the continuity of the state process, which we're assuming here is easier to show, which naturally it would be, and the continuity of the value function. And so these ones are easier to show directly than the abstract continuity of the Martingale part of the Snell envelope. And then you can look at this result related to the proof of Formender's theorem, Norris's lemma, which says that if your Snell envelope is close. Okay, I'm brushing a lot of things into the rug.
00:11:48.017 - 00:12:32.885, Speaker B: This is a heuristic argument. If the Snell envelope is close, then the Martingale part ought to also be close with very, very high probability. So this gives you some confidence that if you can show continuity of the value function and continuity of the state process, you should be reasonably confident that the Martingale part of your Snell envelope should also be continuous. Now let me look at narrow in on a very particular example. So let's look at a Markovian problem where you have exactly one dimensional diffusion and you're governed, and the parameters that you're interested in are time, the initial position of the process, and some other parameter that takes values in a separable, locally compact housed or space. So Richard likes to keep things very general. So we're going to try to be, try to be general here.
00:12:32.885 - 00:13:18.051, Speaker B: So we're going to maximize our gain, which depends on our state and the time of our process. And also the this topological parameter theta, where G for simplicity is going to be some continuous function, continuously differentiable function. So I'm putting zero here for the last coordinate to mean that if I differentiate G with respect to the first or the second variable, the derivative remains. The derivatives remain continuous in the third variable. And we're going to fix some finite stopping time for finite terminal horizon for simplicity, where we have this form for our process X. As a technical matter, I also want the generator acting on G to not be zero on a full measure set. But this can also be worked around and really even when you make this assumption, you only require it in the stopping region.
00:13:18.051 - 00:14:40.545, Speaker B: But to try to make everything completely transparent, let me just throw this up on the board and ask you to grant me this much. So, yeah, this can be relaxed in various directions, but here is a result where things can be made completely explicit and hopefully I can convince you that the assumptions we're making here are relatively standard. So if I assume that the value function is continuous and I have a stopping rule that's of threshold type, so most stopping rules and standard optimal stopping problems are of this threshold type. So you stop when you hit some boundaries, and you have either an increasing or decreasing boundary, which actually is the most common type of boundary and the one that most people can deal with in the literature, or you have some very weak notion of the continuity of these functions and they're consistent in your parameters, then there exists a possibly weak solution version of the vector process of your reward and the Martingale part of your snow envelope that is continuous in the parameters. And moreover, the smallest optimal stopping time coincides with the largest for your for your problem. So these boundary assumptions are relatively standard, and many techniques in the literature have been developed to kind of prove them. So there's a very nice work by Tiziano, de Angelis and Stabil where they actually try to extract local Lipschitz regularity of the, of the boundaries by using some probabilistic arguments.
00:14:40.545 - 00:15:12.815, Speaker B: And then Montenacity is also a very standard thing that you can find for many problems. For instance, the American put. So, so here are some applications of this work related to finance in particular. So we can look at a simple market. So let's just say we have effectively a geometric binary motion. But I'm going to allow the volatility to depend on time here, and I'm starting at some state S, and I'm going to assume the interest rate in the market is zero. I'm of course going to assume that A is strictly positive and bounded.
00:15:12.815 - 00:16:01.455, Speaker B: And I'm going to look at a payoff from my optimal stopping problem that is a C2 function that satisfies this second differentiable condition so that I satisfy the generator property that I pointed out to you before. So here's my optimal stopping problem, and this is going to be the price of my finite horizon American option. If, for instance, G has a bounded derivative, then if you look in the textbook of Nizar, then you'll see that the pricing function is going to be Lipschitz in the state variable and as a consequence differentiable almost everywhere. So why Do I care about this? Often when I'm looking at American options, one thing that I want to do is I want to solve a pde. It's a free brand new pde. And I would like to show the continuous differentiability of my value function. And so there are ways that people try to do this.
00:16:01.455 - 00:17:12.783, Speaker B: What we're going to do here is we're going to leverage the continuity of the optimal stopping time in order to get this done, along with the differential the bounded derivative of G, which gives us that we have differentiability almost everywhere. So standard theory will give us that the optimal stopping time for your initial data is of this type. It's the first time that your value function is equal to the gain. And for the sake of simplicity, let's again assume that the stopping region is of threshold type, like in the American put. For many common problems like the American put, these are monotone and continuous and there's actually a very tight relationship where if you have monotone boundaries and the continuity of the boundaries, they're almost closely equivalent to some work of pesky on this kind of the type of discontinuities boundaries can have. So under our assumption, under the assumptions here, our theory gives us the almost sure consistency of the optimal stopping times, in the sense that if I take a sequence of initial data converging, then my optimal stopping times for that initial data are also going to converge almost surely. So as a consequence, we can try now to differentiate our value function P or price.
00:17:12.783 - 00:18:15.191, Speaker B: And noticing that our geometric Brownian motion emits a differentiable flow, we can use, we can pass to the limit here and argue that the derivative looks like the derivative of the gain evaluated at the process. The optimally stopped process times the differentiable flow here again evaluate at the optimal stopping time. And so by some continuity arguments and dominating convergence theorem, we can argue that the right hand side is continuous as a function of ts. And as a consequence our price is actually is going to have the derivative of our price is actually going to have a jointly continuous modification. So importantly, this can be used to get this smooth fit solution to the usual free boundary PDE for the price, and also derive integral equations that characterize the free boundary B1 and B0. As a consequence. So kind of a standard approach would be you look at your stopping problem, you try to get some initial properties, show that it's a threshold type, get some continuity of the value function by standard arguments, and then look at the structure.
00:18:15.191 - 00:18:52.755, Speaker B: For instance, if the value function is decreasing in time and you have a gain that depends only on your state. You're going to get these monotonicity properties and then as a consequence you can apply these kind of results here. So now for the game, let's look at the exact same market. So again we're going to have no interest rate, R is equal to zero. And we're going to look at this market under the risk control measure and we're going to keep the time dependent volatility, where A is going to be determined in a moment. So the idea is that A is going to govern the interaction with the population. So this is a problem.
00:18:52.755 - 00:19:51.035, Speaker B: The problem that I'm trying to pose to you is one where I want to price an option, but there's going to be some interaction with the population in the dynamics of the price that then determine the value of the option. So if I have an infinite horizon American option with a twice continuously differentiable unbounded payoff G, then this of course is going to be the price of my option at time zero. So being a little bit clever here, we're going to resort to some tools and optimal stopping to characterize the structure of the solution to this problem. So by time change, it's possible to verify that the optimal stopping time for this problem is the first time that your stock price process enters a stopping region. And importantly that this stopping region, or interestingly here kind of constructed to be a simple example, this stopping region is independent of the volatility. The idea is we do a time changes infinite horizon problem. We can kind of absorb this away and the stopping region is independent of the volatility time.
00:19:51.035 - 00:21:00.783, Speaker B: So let's suppose again looking at this mean field approach, imagine there's infinitely many traders and we'd like this volatility to be determined by the behavior of those traders in the market. So one natural conceptualization of this interaction is we have a baseline volatility sigma and we add to that volatility the cumulative distribution function or the cumulative exercise time distribution of the agents in the population. The idea being that as more traders exercise their options in the market, the volatility is going to going to increase for the option. So there's a natural type of interaction, but we can accommodate other forms. For instance, as long as A is continuous with respect to weak convergence or almost ever continuous with respect to weak convergence of the measures, then this is, this is going to be just fine. There's some technical assumptions here so that it fits exactly into our framework, but these are easily verifiable because for this problem, because of the structure, the value Function is actually going to be the convex of the concave envelope of the gain function. So you just look at your gain function, you look at what the concave envelope is, and you can immediately check whether or not these boundary assumptions are going to be satisfied.
00:21:00.783 - 00:21:53.891, Speaker B: Basically, we want the boundary to have measure zero and we don't want any isolated points. We'd like our stopping region to be the closure of its interior. So stepping back, what do our results give under these assumptions? We get for free the weak law, the weak continuity in law of our stopping time with respect to the weak convergence of the measures defined by the cdff. And moreover, we get the equilibrium result. So the existence of optimal distribution such that the best response to that distribution is optimal for the stopping problem and it governs the dynamics of the stock price process. So we conclude by this here that there's an equilibrium exercise time distribution for the mean field of traders in the market. So I can I see a price, Everybody else is seeing a price of the options.
00:21:53.891 - 00:22:22.285, Speaker B: They're going to try to exercise their options in the optimal way, and when they exercise, it's going to affect the distribution of the price. And there's a stable point for the system of agents. So here's a picture. I concoct ag and I solve it. So here we have a function G. It's like a composition of sigmoid type functions. And then here is the equilibrium value function, which because of this time change argument is still going to be the concave envelope of our gain.
00:22:22.285 - 00:23:11.753, Speaker B: And then I numerically solve by doing a fixed point iteration for the stopping time distribution in equilibrium. And so we get some stable point to converge to stable point. And here is the stable point that we get the equilibrium exercise distribution of the agents in the system. When in particular, I start the process close to the minimum here and I have my baseline volatility of a half. So if you were to change this around, if you change the initial position of initial data of the stopping problem, you're going to get a little bit different distribution. But this is kind of natural to expect. Okay, so as a summary, what have we done? We've tried to leverage this dual characterization of the optimal stopping problem to connect consistency of optimal stopping times to path properties of the reward process and its null envelope.
00:23:11.753 - 00:23:47.275, Speaker B: So we can use this consistency to derive the existence of Nash equilibria for a broad class of stopping games. And we try to make this practical by forcing some Markovian assumptions and some assumptions on the structure of the problem to establish the necessary path properties that we're looking for. And in addition, using this setup, we propose two applications of results, including the continuous differentiability of American option prices and the existence of equilibria in problems of pricing with population interaction. So thank you a lot. Thank you very much for coming to this talk. And here's a list of references.
