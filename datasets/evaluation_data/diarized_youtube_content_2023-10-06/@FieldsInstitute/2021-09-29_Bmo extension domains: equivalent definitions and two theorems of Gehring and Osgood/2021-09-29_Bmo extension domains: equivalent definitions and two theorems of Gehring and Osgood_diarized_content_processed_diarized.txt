00:00:03.520 - 00:00:20.594, Speaker A: All right, so thank you, everybody, for coming to the contributed talks for the BMO week. It's my pleasure to introduce Amaz Butayev, who's going to be speaking to us on BMO extension domains, equivalent definitions, and two theorems of Goering and Osgoode. Thank you, Amaz.
00:00:21.814 - 00:00:55.164, Speaker B: Well, thank you, Ryan. I'd like to thank the organizers for the invitation. Well, I'm going to talk about the BMO extension domain and the two theorems of Germany and Osgoode. This is a joint work with Gallia. Daphne. Yeah, so that's it. So I don't want to spend much time on like defining the BMO space, but I want to bring your attention to the following fact, that when we're talking about the BMO space on a cube, everything is, is clear and unambiguous.
00:00:55.164 - 00:01:32.480, Speaker B: We're talking about the functions living on the cube. That's why, first of all, we assume that these functions are integral on the cube. So when it comes then the usual definition of the BMO. But when we go to the whole space, when this is the RN, we have to give up the integrability. Because if you assume that the function is b one, l one, then you're missing all the l infinity functions, I mean, most of them. So we're giving up the integrability. We only assume that it's local integral function and we control the mean oscillation well.
00:01:32.480 - 00:02:36.120, Speaker B: So, and this is the archetypal example of the demo function log x. And what happens to the log x is if we just look at this integrability, what happens to it is if, say we look at the cube sar intervals on r. So if we look at what happens, say, on this interval, and if we look at, happens like on that interval. So we see that the variations, because this function is in BMO, the variation from the, from the mean here as well as the, you know, so there is a mean here, and the variation here on average are all bounded. That's because log is in Pmo. But if we now just look at the average values themselves, right? So on this interval, and on that interval, we'll look at the average values. So then here the average value is like this, and they're approximately.
00:02:36.120 - 00:03:18.376, Speaker B: It's like this. And the further this interval will go, the higher the average will become. It's not just the function itself is growing. The averages over unit cubes are growing. And so this is something that we wouldn't like to have. We would like to have some, some sort of control on the averages and so what I'm trying to say is that be multiple functions with a more uniform control of l one norms. But over, over unit cubes, say, is if you want to retain this control, you naturally get into the BMO space that was introduced by David Goldberg.
00:03:18.376 - 00:04:50.250, Speaker B: It's called the little bmo rn. That space is sometimes called local, sometimes localized. We prefer the term inhomogeneous BMO space. It consists of functions that are in a big beam or on rm and such, that apart from the control of the mean oscillation, would you want to control the average values of the function on cubes of size one and bigger? All right, so then, so this type of questions and this type of spaces are naturally suited to some problems in partial differential equations or some questions in geometry, if you are trying to solve the PDE's on unbounded domains. And so typically, what happens if you, if you, if you're solving the ad u equals f, and if ad operator is a differential operator that's completely homogeneous, say Laplacian, then you can talk about the right hand side being BMO. But if this ad is a non homogeneous operator, if it's, it has lower order derivatives or it has some, you know, constant terms. If the symbol has a constant term, then a natural setting would be to consider the f to be in the little BmO, because big BMO is not suited well for these type of problems.
00:04:50.250 - 00:05:32.584, Speaker B: So this is the reasoning from the PDE's. And the little BMO is also well suited to do the analysis of manifold, say, when you want things to be splittable and localizable. So BMO lacks that. That's why the little BMO is more useful. So, okay, so we figure out what the little BMO on Rn is. So what about the domains? So we can consider the BMO class on any domain, basically, and we can consider the subclasses of little bmos with a parameter lambda. So what's a big bmo, first of all? So the big bmo is what it should be.
00:05:32.584 - 00:06:29.844, Speaker B: So it's the class of local integrable functions on that set on the domain omega, such that the mean oscillation is bounded. The thing is, you want to deal with the cubes or balls in that domain that just lie inside the domain. So for example, if this is your domain omega, the cubes that you want to operate with should be the cubes inside of the domain. So you don't want to talk about the cubes in rm that are not properly inside the omega. I mean, you can do it and you're talking about different types of bmos. Okay, so what about the bmo lambda? Again, what is lambda? Before I go there? So, in this definition of Goldberg, we fix the unit cube that was one instead of this one. You can choose ten or 0.1
00:06:29.844 - 00:07:25.274, Speaker B: or 55. The norms will change the numeric values, but the class b mole will stay the same no matter which constant you fix. So it's not important. So now when we, when, when it comes to the domains and when we're talking about the little bmos on the domains, this thing becomes a factor. So that's why we will define like a scale of bmo subspecies, little bmo lambda spaces as the local integral functions such that on cubes of small size, small in the sense of lambda, which is controlling the mean oscillation, and on large cubes, we want to control the averages in addition to that. Yes. So feel free to ask questions if you have any.
00:07:25.274 - 00:08:09.484, Speaker B: So, this is the definition so far. So this is what we want to look at. So these are the function spaces now. So, main questions that motivated our research and people who, these things before us were the extension problems. So for, for big bmo, the question is what? The question what BMO extension domain is, is basically the question what is what? The conditions on omega, on the domain omega that guarantee that any bmo omega function can be extended to a bmorm function. Is there a way to ensure that, assuming some geometric conditions on that. And we can ask the same question for the beam o lambda.
00:08:09.484 - 00:09:10.924, Speaker B: So what conditions on omega ensure that the Bmo lambda, any Bmo lambda function, can be extended to a global Bmo on rn? Okay, so the first question has been answered a long time ago was Peter Jones, who proved the following. So omega is a BMO extension domain if and only if there is constancy such that for all pairs of points xy in the domain, we have the following relations. So there is one metric, k omega, and there is another metric, j omega. And k is controlled by j. So what are the k's and k? So k and j are here. So this is the definition of k. To measure the distance between x and y in this way, you're taking the curve gamma.
00:09:10.924 - 00:09:45.898, Speaker B: This gamma curve connects x and y. So the endpoints of the rectifiable curve inside the omega are x and y's. And then you compute this integral. Oh, it's a line integral over that curve. And then you minimize such a computation. And this is the k omega distance between x and y any domain has. So this is a metric on any domain.
00:09:45.898 - 00:10:29.564, Speaker B: So the j omega looks even more complicated. So it's a half of the logarithm of the product of two things. So it's the distance between x and y relative to the distance from the x to the boundary and y to the boundary. You multiply them out, you take the square root, and then you take the logarithm of that, okay? So that may be confusing. So I want to give the, I want to spend some time giving the, the pictorial description how you compute this thing. So for example, x, k omega for x and y. And suppose this is the point x, and this is the point y that you have.
00:10:29.564 - 00:11:26.804, Speaker B: Suppose we want to figure out like get an idea of what is the distance between x and y. So what you do, you connect it with, with them, with some curve. And so the curve should be like that, so that you stay away from both boundaries as much as you can. Then you do the following. So you look at the ball that's like from this x to the boundary, okay? So that's like that, right? Now you look at the next ball close to the boundary like that, right? And you keep doing that. So where you're far away from the boundary, you can increase the ball, okay? Like this. And somewhere your ball has to be really small like that.
00:11:26.804 - 00:12:14.964, Speaker B: So once you cover this curve by the balls, the total that sit inside the domain omega, you can just count this balls. And this will give you the number that is more or less. So it's the number of balls covering gamma. And so this number of balls more or less approximates it gives you the idea of how big K omega is or how small it is. Right? So j omega. So what's the, yeah, so the k omega distance is called the quasi hyperbolic metric. So that was introduced by Gerrigan, Palker, late seventies.
00:12:14.964 - 00:12:55.740, Speaker B: And so what they should, they also introduced the distance ratio metric, which is this. And the way you calculate that is simple. Well, it's x and this is, yeah, so you start with the same balls like this. And yeah, this ball should be like touching the boundary or just touch the boundary. And after that, what you're doing, you're ignoring the domain completely. You connect this by a straight line like this. And what you do next is the starting ball.
00:12:55.740 - 00:13:15.252, Speaker B: You double it, it's twice as big. This one, you double it, it's twice as big. Then you double again. And then you double again. The moment you intersect, you stop doubling. And then you count the number of balls you have. So it's 12345.
00:13:15.252 - 00:14:05.454, Speaker B: So that thing is approximately five for this x and y, and my very rough computation. So this is the idea behind this k and j. And so that gives you a way to figure out, like, how big k and j are. Just looking at think it's more or less geometric. But there is a better way to give the intuition, I mean, to figure out if the omega is good or not. Following the definition of uniform domain, the omega is called uniform if there are constants a and b, such that any pair of points, x, y and z omega, can be connected by a rectifiable curve gamma, such that this is the arc length. The arc length is less than a multiple of the ambient euclidean distance.
00:14:05.454 - 00:14:53.904, Speaker B: And for any point here, you have that relation. So that as you go along the curve gamma, the distance from the point to the boundary must be increasing relative to the arc length. Okay, so the first condition is called the quasi convexity, and this condition is called the John condition. Yep. So, pictorially, do I have this? Yeah. So, pictorially, that means that if we again talk about this, I'll put this y here and x here. So a good curve, the curve that satisfies these two conditions, is the one that lies here.
00:14:53.904 - 00:15:25.344, Speaker B: And not only the curve lies in the omega, but also you have a cone. Okay, so let me make my life simpler. Like, first, I'll explain what happens, like, if you have x and y here. Okay, so this is x and this is y. A very good curve. Is this one a straight segment? And not just the curve, but also a cone that contains it lies inside the omega. The.
00:15:25.344 - 00:16:46.022, Speaker B: The aperture of the cone is that b constant. And because it's a straight segment, the quasi convexity constant a is one you cannot get by using the straight segments. If one of the points is here and another is here, then your curve must be, like, actually curvy. So you will have a quasi convexity coefficient greater than one, and you will need to choose b, also very small, because it's really hard to fit this, this curve along with the, with the, with the cone around it. Okay, so that's it. So what's the Gehrig Osgood theories? So what Geriat and Osgood showed, I mean, let's just remind ourselves that Jones proved that the BMO extension domains are the domains where the k metric is majorized by the J metric. And so what Gary Nosgood showed, first of all, the first result is that if you have any points x, y and omega, and they can be joined by a curve that satisfies this uniformity condition, that quasi convexity.
00:16:46.022 - 00:17:28.734, Speaker B: And the John condition, then you have this kj inequality, which by Jones means that then you have BMO extension, if you have this situation for all x and y's. And the second theorem of Germany Osgood is about the other way around. So, suppose that you have KJ inequality. Can you deduce that the domain is uniform? Well, almost. Well, actually, exactly. But the theorem is the following. Suppose that you have two points in omega and you have a geodesic, the one that minimizes that k distance.
00:17:28.734 - 00:18:51.404, Speaker B: And if you have the KJ inequality holding for all the points along that geodesic, then the gamma will satisfy the uniformity condition with some a and b that depend only on the c in the hypothesis. So it looks strange and like slightly different. It's from the previous theorem, because here you're asking not just that the K J satisfied at the end points, but they are satisfied along the whole curve. But if you check the proof that, the proof really heavily relies on that, on this assumption. And. Okay, now, so Jones and Gerryman Osgood showed that omega is a bmo extension domain if and only if it's essentially uniform. So now what can we say about the localized, inhomogeneous bmo lambda spaces? Right? So the intuition that we talked about at the very beginning, that bmo lambda spaces, they give you an extra control of the averages along the whole space, somehow hints into the direction that these extension domains for these functions must be some sort of local versions of the uniform domains.
00:18:51.404 - 00:19:26.204, Speaker B: All right, and so, but which local version? There are at least two that we can state. So, the first one is mid nineties. The definition of Heron and Koscala is the following. So this is paraphrasing. So, given delta domain, omega is called delta locally uniform. If you have quasi convexity and you have the John condition, but they hold only for the points that are close to each other. It's a pretty natural, localized version of the usual uniformity condition.
00:19:26.204 - 00:20:18.116, Speaker B: Now, but there was another definition due to Jones himself. And that definition historically appeared even before the heron and Koskala in the paper. In Jones paper, enacted. So, where he defines the so called epsilon delta domain, this is a domain such that for any two points that are delta close to each other, you can connect them with a curve. You have quasi convexity, with some epsilon, you have some sort of john condition. But you see, compare this thing to that one. The distance that Jones is using, these are the absolute values.
00:20:18.116 - 00:21:10.768, Speaker B: So it's the ambient euclidean distance. And so the definition of Heron and Costco, this cone condition, the John condition, is based on the arc length. So these are two different local versions. Although Heron and Costco, in their paper, they say that the Jones Epsilon delta domains are the same class of domains that they consider that local uniform. Unfortunately, they don't cite any result, and it's hard to find the proof of that fact. So it's probably folklore, but we had to reprove it. All right, so assuming that, like, whatever local definition there is, so what's the local version of the KJ inequality? So, let's just recall again, Gerry Osgood.
00:21:10.768 - 00:21:57.746, Speaker B: So this is Gerry Osgood theorems. Look, the first theorem stays valid. I mean, it tells you nothing about the x and Y dependence. So you can, like, if your x and y are close enough, so if x minus Y is less than delta, then you can still apply Garmin Osgoode, and you can still get the control of this by that. Which means that if omega is a delta local uniform in the sense of heron and cos, then it will satisfy the inequality k less or equal than one plus j, up to a constant for x and y close to each other. This is good. The other way around fails, though.
00:21:57.746 - 00:22:51.894, Speaker B: So if now we have x and y that are close to each other, if we have these things in mind, and suppose that we have this inequality true the endpoints. The second theorem of Gary Osgood doesn't help us, because the hypothesis of the second theorem is that you have k and j inequality holding for all points along the gamma. And so what can happen is like, hypothetically, this. You can think of that, that you have two points close to each other, x and y, really close, less than delta. But there is some boundary, some huge piece of the boundary here. All that is the complement. And then the only curve that you have connecting x and y goes like that.
00:22:51.894 - 00:23:10.916, Speaker B: And then you get two points. Here's u. So here is v, and they are really far away from each other. Okay? So this u and V may get far away. You have no control, and there is no guarantee that you'll get what you want. So, is the way around. And the answer is yes.
00:23:10.916 - 00:23:54.334, Speaker B: So that's what our first result is, that there is a modification of the German OsGo theorem that works for delta local uniform domains. So what you need is the following. So suppose that the gamma is a geodesic through the given points x and y. Suppose all you have is the KJ inequality for really close points only. And in addition to that, this is a crucial the endpoints x and y are close to each other, like even closer than delta. That's delta over 100 c squared. This c is the c.
00:23:54.334 - 00:24:21.094, Speaker B: Then you can guarantee that the uniformity conditions hold. Okay. And we have an example that shows that without this extra condition, you cannot guarantee anything. And the example is very simple. You can just look at the infinite strip, pick up two points. X here, y here. The quasi hyperbolic geodesic is a straight line for any two points that are close to each other.
00:24:21.094 - 00:25:16.434, Speaker B: You can feel fit the the cone into the domain, but this domain doesn't. But that infinite gamma will not satisfy the uniformity condition. This is the improvement of the gerios good. The variation of Geri Osgood that works for the local setting. So there is another thing about Gerry Osgood theorem for the Epsilon delta domains. So why do we even have to talk about this? So again, if we look back, remember the first theorem of Gerinosgo that worked really well when we talked about locally uniform domains in the sense of heronkoscula are not that good for the Epsilon delta domains. Because the assumption here is the gamma satisfies the uniformity condition.
00:25:16.434 - 00:25:54.806, Speaker B: The uniformity condition is with respect to the arc length. So it's lxyz. And if we want to talk about the epsilon delta domains, this is where the John condition is not with the arc length. This is when you have the distance. But fortunately, things are not that bad. I mean, the gerios proof doesn't hold because you need to modify it. And so what you get still is the following results.
00:25:54.806 - 00:26:47.308, Speaker B: So if you have two points, x and y, they close delta away from each other. They can be joined by a curve satisfying the epsilon uniformity condition of Jones. Then you still get the k less or equal than j plus one times some constant. And here is the interesting thing, that this constant, in this case, it becomes dependent on the dimension so higher the dimension, bigger the constant is in the case of Gernowski theorem one, there is no dependence on the dimension. This new phenomenon is because of the new definition of the uniformity that we use. All that helps us to. Yeah, so one more thing is that all I'm talking about, like is stated in this, in the case of rn, but it can, without much difficulty be transferred to the metric measure spaces.
00:26:47.308 - 00:27:54.264, Speaker B: But there is still some work to be done. But so what hints is that this n, in the case of the general metric measure spaces with doubling measure will be replaced by the doubling constant, or the geometric doubling constant, if you wish. Yeah, so all that helps us to figure to establish the following, that if you have omega, that is delta local uniform, it's Heron Kosko definition this is the same as Epsilon delta domain. And so this equivalence is a folklore result state in theorem. So then we show that this two, all of them like this, and that both equivalent to the local to the condition in terms of k and j. And all these equivalences is something that helps us to finally establish that all these things happen if and only if it is a BMO extension domain for the Goldbergs BMO, if that lambda is small enough. Okay, so, yeah, that's all then.
00:27:54.264 - 00:27:55.524, Speaker B: Thank you so much.
00:27:57.864 - 00:28:24.412, Speaker A: Thank the speaker. Are there any questions for almas? All right, thank you. Let's welcome the next speaker, Nikos Kalmukis, who's going to speak to us about polymorphic semigroups and Saracen's characterization of vanishing and mean oscillation.
00:28:24.588 - 00:29:14.030, Speaker C: Thank you. Thank you very much. And thanks to the organizers for giving this opportunity to give this small talk. So, yeah, I'm going to speak about this work that, in fact, is a joint work with Vasilis vascaluganis from the Aristotle University of Saloniki. So, in the first couple of slides, I'm going to talk about, I mean, general bmo stuff that probably you've had at least a couple of times today, but repetition does not have, I guess. And so, let's say that we're in the unit circle. T denotes the unit circle, and then the BMO, as you have seen, is this function of the space of l.
00:29:14.030 - 00:29:49.474, Speaker C: One functions from the unit circle such that. Okay, first, we give this intermediate definition that we have this bracket of f with appendix I is just the mean value of f over I, and the BMO space is a space of functions such that this seminar, let's say by star, which is the supremum over the absolute value of f and minus its value I. The average of all this is uniformly bounded for all interface.
00:29:49.934 - 00:29:50.794, Speaker B: Okay.
00:29:52.294 - 00:30:43.258, Speaker C: And I mean, in this talk, we will be particularly interested in the analytic BMO space. So, that is, functions that are BMO on the boundary, and therefore, extension inside is an analytic function, and we will denote it by BMO a for analytic. Then we have the corresponding vanishing condition, which says that if this oscillation of the function tends to zero as the length of the interval becomes smaller and smaller, we say that the function is of vanishing in oscillation. And if, again, the Poisson extension is analytic in the unibrisk, say that it's analytical vanishing municipalation, or VMOA for.
00:30:43.306 - 00:30:43.894, Speaker B: So.
00:30:46.194 - 00:32:04.206, Speaker C: I mean, up until now, I guess we have plenty of motivation to study BMO type spaces. But let me give you my quick motivation, my quick review on the subject is that, okay, BMO type spaces play a very important role in the function theory, in the human disk in particular. Not only, but also. So here I will state things about analytic functions, but I mean, almost everything that they say has some real counterpart. So, first of all, if Hp is the classical hard space of analytic functions, so functions analytic in the unit disk, which have, with the restrictions on smaller circles, are uniformly integrable, then the dual of the VMOA experience can be identified canonically with the hard space h one. The dual of h one can be identified with BMoA. And then the sega projection, which maps a function to its analytic completion, is bounded from anything to BMW.
00:32:04.206 - 00:33:29.600, Speaker C: So that's, that places the space BMW in very central picture in function here then, because in this talk we will also mention a very related space, which is the block space. So, in fact, the block space is the space that plays the role of BMW when we are dealing with Bergman space theory. So, suppose that we have the Bergman space ap in D, which is just analytic functions in the unit disk, which are Lp integrable with respect to the. Then the same statements hold. If we put instead of the BMO a space, the block space, which is just functions, which have this growth on their derivative, and we replace vmy by the little block space, which are functions, so that the derivative has this small o growth near the boundary. Now, I would like to come to the theorem that was, that is the motivation behind all this I'm going to talk about today. So, we have this theorem of Saracen by 1975, which has the following characterization of functions of vanishing minisculation.
00:33:29.600 - 00:34:47.438, Speaker C: Josarason says that if we start with a function in BMOA, then this function is of vanishing minocillation, if and only if its rotations converge in the BMOA norm to the original function. As the angle of the rotation tends to zero, so we rotate the function a little bit, and then we are very close in DMy norm to the original function. If this happens, if and only if this happens, the function is organic mean oscillation. Let's keeping this in mind just, just to say that just by one reading the proof, it convinced himself that the same holds for the block space in the little block space. So, if you start with a function in the block space, then it become belongs in the little block space. You can only convert to original function in the block norm this time, of course. Okay, but then, I mean, if we think about this theorem for a second, we realize that we have introduced an object from, from another area of mathematics, which is called holomorphic semigroups.
00:34:47.438 - 00:35:47.474, Speaker C: So a holomorphic semigroup is a one parameter family of holomorphic self maps of the unit disks. So 50 maps d to itself such that the first, the first one f zero is the identity. We have the semi group law. So the composition of t with p's is a function p s, and we have some kind of continuity condition that explicitly that 50 converts to the identity uniformly of compact sets as t goes to zero. So the reason why I'm saying this is that of course, in the theorem of Sars, the exponentials e to the I t times z is a semi group, holomorphic semi. But we will see this again in a second. For the moment, I would like also to say that there is an equivalent description of holomorphic SEM groups as solutions of a co c problem.
00:35:47.474 - 00:37:12.884, Speaker C: So if one can look, this is a non trivial piece of theory. But one can describe holomorphic semigroups as solutions of this initial value problem, where g is some holomorphic function such that to the solution, I mean the z has to be picked in such a way so that the solution of this initial value problem has solutions for all positive times t. So one can prove it's not trivial, of course, but one can prove that these descriptions equivalents the function g, which is uniquely associated with the holomorphic semgroup is usually called infinitesimal generator of the semgroup. Now, from the dynamical viewpoint of the semi groups, we can infer that all members of the semi group have a unique common dendrool point, which is called the dendroid point of the same group. And we make a rough distinction, a rough classification of semigroups to the ones that have the dual wolf point. I'm sorry, there is an experiment. The draw wolf point can belong also to the boundary, so there should be a bound here.
00:37:12.884 - 00:38:56.654, Speaker C: So we make this rough distinction to the semi groups such that they have a dendrite which point inside the unidisk which we call elliptic, and the semigroups semi groups which have the boundary which we call non elliptic. So notice what I said before, that rotations are form a semi group, in fact a group of holomorphic semi group, and in fact it's elliptic semi group with dendrites point the origin. Now that, now that, now that we have both pieces of information, we have the theory of holomorphic semi groups, we have the characterization of Sarsun. It is a natural question, of course, to ask the following so are there any other semigroups, holomorphic semi groups, such that we can replace, that they can replace the rotation is in Saracen's theorem. So, can we give a characterization? A characterization can be interpreted, of course, in many ways. But what I mean here is in terms of the infinitesimal generator g of semi groups, such that if I put them in Saracen's theorem, the statement is the same. So the function is in BMW is a BMW function is in vMware, and only f composed to this semi group converges to f in the norm of Bmoa, as the parameter t of the semi group goes to zero.
00:38:56.654 - 00:40:09.958, Speaker C: And of course a similar question we can ask for the block space, right? So the problem, as I said, I think it's quite natural, and it has been already investigated by some people. In particular, there are two nice papers of Blasco, contreras, Diaz, Madrigal, Martinez, Topadenikis and Sesquakis, where they study this problem so briefly. What they do is the following. They introduce, first of all, the maximum subspace of strong continuity associated to a semi group, which is which is. So x will be either the BMoA or the block structure. I use it here for convenience, is the set of functions such that f composed t converges to the function f as the parameter which is zero. And I mean it is proved that this space always contains the little version of the space.
00:40:09.958 - 00:41:40.828, Speaker C: So I'm referring to either VMi, the little block space, and this is strictly contained in the space X. Now this result, it was by Blas quetal for the block space, and then by some other argument by Anderson, Jovis and Smith, the BMR space. So in fact, our main problem now can be formulated as a problem of minimality. So what is the function, the holomorphic semigroups pt such that this maximum subspace achieves minimality in this chain of inclusions, and I mean, they prove a characterization of this, of this space as follows. So let semi group the spaces x and x zero as before, and blast petal prove that this maximum subspace can be described as the image of an operator that I will introduce in second plus constant functions intersection with the space itself and the closure. Now the operator is a quite well known operator. It's called the generalized Voltaire operator, and in fact is integral of the function against some symbol.
00:41:40.828 - 00:42:58.378, Speaker C: Now the symbol of course depends on the semi group and is defined in terms of the infinitesimal generator. So if not really very important, to understand here how the symbol depends on the infinitesimal generator, just keep in mind that the infinitesimal generator produces a symbol, this symbol produces an operator. And the image of operator describes the maximum subspace of strong continuity. So they have some kind of description of, of the maximum subspace. And this allows them to give the following two sufficient conditions for the minimality of the maximum subspace. So first, they introduce this vanishing logarithmic block condition, which says that if the reciprocal of the infinitesimal generator has some, has this little growth near the boundary, it's little over this function, then the maximum subspace strong continuity for the block space is in fact minimal. It's delivering block space, and they have a corresponding condition about the VMO interspace.
00:42:58.378 - 00:43:45.704, Speaker C: So this is a condition which we call vanishing logarithmic minocylation condition. And it involves again the integral of the reciprocal of the infinitesimal generator, or on a Carl, on Carlson boxes. So I have, I have a picture here for the Carlson box, but I guess most of you know what the carousel box is. So they have this sufficient condition. Notice that we have one direction only, and this condition in fact covers a large class of semi loops. So there are a lot of same groups that satisfy this condition. And hence this theorem of Sarasota holds if we put these same groups at the place of the patients.
00:43:45.704 - 00:44:50.416, Speaker C: But I want to stress here that there are some questions that are left open by the results of blasphemy. So in fact, it can be verified that there are no non elliptic semigroups that satisfy either of the two conditions. So if you start with the semi group which is not elliptic, it always fails to satisfy the condition of blasphemy, either of the two conditions. So in particular, is there some non elliptic SEM group which can be used in Saracen's theorem so that the maximum subspace is minimum. This is not clear at this point. Another thing is that one can verify by hand quite easily that the vanishing logarithmic in oscillation condition implies the vanishing logarithmic block condition. But is there an example of a semicircle which satisfies the one but not the other? And the third condition is of course, maybe the third question is of course the most natural one.
00:44:50.416 - 00:45:50.634, Speaker C: Are the implications in the theorem that I mentioned of blast petrol reversible? So, on this problem, we worked with Vasilis, and we have the following theorem which says that take a semigroup with anything decimal generator g, the following are equivalent. The maximum subspace for the BMOA space is the vanishing initiation space. So we have the minimality and g satisfies the condition vanishing logarithmic minoculation. So in fact we can reverse the implication in the theorem of minasquital. But not only this, these are further equivalent to everything else. So to that, the maximal subspace on the little blocks on the block space is the little block space, and that g satisfies the vanishing logarithmic log. So everything turns out to be equivalent.
00:45:50.634 - 00:46:38.486, Speaker C: And maybe one could guess that from the theorem of blasphemy that this a and b would be equivalent and c and D would be also equivalent. But it's quite impressive. I think that quite interesting that all of the four conditions are equivalent. So we can answer to our previous questions. In particular, there exists no non elliptic semi groups that achieve minimality of the maximum subspace, and the conditions vLMo and VLB are equivalent. So how much time do I have? Do I have like five minutes?
00:46:38.550 - 00:46:40.262, Speaker A: Maybe like seven minutes.
00:46:40.438 - 00:47:33.868, Speaker C: Great. So that's the main theory. I just want to give some hints about how one would go about proving this thing. I will start maybe with the, with the fact that b and D are equivalent, which seems a bit counterintuitive. It seems a bit counterintuitive until you bring in mind this theorem of Pomeranian, which says that for a univalent function, being in the little block space is equivalent to being in the VMware space. So of course this theorem does not say anything in our case. But with Vasilis we managed to prove a weighted version of this result, which is relevant for problem.
00:47:33.868 - 00:48:18.294, Speaker C: So suppose we have a positive c one function in the unit disk, a weight function which satisfies the following regularity condition. So the modulus of the gradient times one minus z modulus of z squared is controlled by a constant times the weight. And this constant has to be strictly smaller than one. So if we have such a weight, then the following are equivalent. For a univalent function, the derivative has this growth near the boundary, and we have this weighted BMO type condition again. Si. Here is a correlation box.
00:48:18.294 - 00:49:47.390, Speaker C: And then this together with the fact, yeah, this together with the fact that this symbol gamma that is induced by the semi group is univalent. If you write everything down, you can see that it gives you the equivalence between the vl b and real mo conditions, where of course omega in this case is. Sorry, is this logarithmic weight. So that, that is that. And then of course we have to talk about the other implications. So let me just for convenience, let me introduce this BMW log space weighted BMW space with the space of functions such that we have, I mean, this quantity that defines BMW functions weighted by this, of the reciprocal of the length of the interval square, and of course, the corresponding, the corresponding vanishing condition. So as the length of the interval goes to zero, this quantity becomes zero, defines the vanishing minoculation logarithmic space.
00:49:47.390 - 00:51:10.604, Speaker C: Then we have the following lemma which says, take a function g which is in space Bmoa, but it's not in the vanishing logarithmic vanishing mean moa base. Then we can construct the function f in Bmoa such that the image of f under the operator tg, this generalized operator that I defined before, is a BMI function, but it's not a VMI function. So that's, that's the central technical, let's say lemma in our construction that allow us to prove our results. And I will show you why, because suppose we want to prove, if you remember that if, if the maximal subspace is the VMOA space, then gamma must satisfy this balancing min oscillation logarithmic balance in oscillation condition, which is equivalent to saying that gamma belongs to this vMware long spreads. So suppose that this does not happen. So gamma is not in this space, but yet we have this, this equality. Then it can be proven that when ft is elliptic, then gamma is necessarily a BMW function.
00:51:10.604 - 00:52:03.924, Speaker C: Therefore, we can apply our lemma for the function gamma, and we can construct a function f as in the lemma. Then immediately t gamma of f is in the maximal subspace of strong continuity by the theorem of blaspheme, which is, it says that this space is just the image of t gamma, practically, but by construction is not in the VMI space. And that concludes the proof, because we constructed an element inside this difference, we have reached a contradiction. So this takes care of the elliptic case. Well, the non elliptic case is a bit more tricky, but not that much. It requires some more arguments. But this is just to give you an idea.
00:52:03.924 - 00:52:21.824, Speaker C: It requires. Next. So here I have some references, in particular, the two works of Blaspetal, the work of Anderson, Jordan and preprint on the subject. Thank you very much.
00:52:25.404 - 00:52:31.604, Speaker A: Let's thank the speaker. Thank you very much. Does anyone have any questions?
