00:00:00.360 - 00:01:20.555, Speaker A: First I would like to thank all the organizers for this great opportunity which is my great honor and my great pleasure. So here In Shanghai it's 1:00:30, so it's quite late. So I will use this pre recorded video this afternoon and as you can notice actually after I recorded this I noticed this 45 degree watermark, you know from this stupid Chinese software with my name in Chinese and my mobile phone. So please just forgive this watermark. Okay, I will discuss some of our recent work on quantum advantages on computing, adaptation and metrology. Among many different types of qubits, photons have certain advantages and disadvantages. They are fast flying, have very weak interactions with environment, so they are very robust to travel over long distance.
00:01:20.555 - 00:02:37.433, Speaker A: But photons also have very weak interactions with other photons. So this is also a major disadvantage for quantum computing, a reason why many people think photonics is ugly duckly of quantum computing. Another drawback about photons is that they can get lost easily either in the source or in the propagation or in the detection. This is a learning point that will relate to all the three topics I will talk today in collection, meteorology and computing in the field of quantum communications. About six years ago we launched the first quantum science satellite missions which has now been combined with optical fibers on ground to form a space flowed integrated quantum network. You can see this recently published view for overview based on the satellite. We have demonstrated satellite ground colon key distribution, entanglement distribution and ground to satellite Quantum teleportation over 1000 km using the satellite as a trust relay.
00:02:37.433 - 00:04:17.937, Speaker A: Intercontinental QKD between Beijing and Vietnam was also done with a distance of 7,600 km actually in July. We have now launched the second nanosatellite for Quantum Communications with a weight of 23 kilogram but will be a few hundred times more efficient in generating the secure key. So you may ask why developing the very costly satellite? We have been using optical fibers for our Internet and we are very happy. So why is optical fiber not good enough for the qkd? The reason behind this project is related to the key drawback of photons being easily lost as I just mentioned earlier. So say if you have perfect single photon source operating at a 10 GHz reputation rate, sending it through a 1000 kilometer fiber, then you will have to wait for three centuries to get one photon in output. In classical communications we can use amplifiers in every tens of kilometers, but in quantum the non cloning theorem does not allow noiseless amplifier of the quantum signals. So the satellite based quantum communications can take advantage of the nearly vacuum environment in outer space, Whereas the absorption is negligible and so the main loss is a diffraction loss.
00:04:17.937 - 00:05:13.565, Speaker A: So at a distance of say 2000 km, the effective photon loss is reduced by a huge effect of about 10 to the power of 35. Satellite does not actively overcome photon loss. It mitigates the photon loss by creating a new ultra load loss channel. One way to overcome photon loss is quantum analog correction or quantum computation. A point which I will come back the end of my talk. Another much easier and very much relevant communications way is called content teleportation. Especially if you would like to travel for long distance, for example from Canada to Tokyo.
00:05:13.565 - 00:06:11.025, Speaker A: By definition, it's lossless travel of a particle from one location to a remote place. The idea is to loyally share entangled pair between two locations and then the photon to be transmitted performs a joint projection with one of the entangled pair. Then boom. The unknown quantum state will appear in the distant photon in the output. We have seen many impressive experiments on so called long distance teleportations. However, actually the teleportation only happen in the local optical table with a few meters. Then the teleported photon is Transmitted through, for example the 143 kilometer or 1000 kilometer equally lossy channel.
00:06:11.025 - 00:07:31.249, Speaker A: So in the end the photon survival rate remains the same or actually even lower due to the probabilistic bell state measurement. And you can look at these two figures as a compare. So we could define a new term which we maybe we can call quantum calculation or advantage. Where if we can send a photon and it has a better survival probability than using direct transmission. And the standard way to do, you know, for the teleportation advantage should we should first priorly distribute intangible photons with high halogen efficiency between Alice and Bob Bob, which means if Alice, you know, has one photon, then definitely, you know, Bob should also get one correlated photon. So we need to distribute over long distance high halo efficiency intangible photons. But if we simply send one of intangible photons directly to Bob, you know, to the remote Bob, then this photon loss.
00:07:31.249 - 00:08:30.399, Speaker A: Again one way to overcome this problem is that if we can do this, you know, when the photon is just about to arrive at a bulb, we have this magic quantum non demolition measurement which means to see a photon but without disjoint. So how can we implement the Q and D? Interestingly, quantum telepathic itself is qnd. Let's see this left figure. If there's a single phone coming in, assume we have a Ideal entanglement source here, then there will be a double click and the incoming photon, unknown state, will be faithfully transported teleported to output. If there's no incoming photon, then there's no double click and we will know that. So in principle this interpretation protocol should work. But there are two problems.
00:08:30.399 - 00:09:24.451, Speaker A: So here we assume ideal entangled pairs. But as PDC is not a perfect entanglement, it's probabilistic, meaning there's a probability of P emitting one pair and half of a P square emitting two pairs. So they can be unwanted terms adding up the desired turns where each of these SPDC emit one pair. So fortunately we can list all these signal and noise turns and by doing some mathematics, we can suppress the noise by decreasing the brightness of the second pair. And we have optimal success rate. When the equivalent distance is about 100 kilometers, we still have about 80% halogen efficiency. That's the first problem.
00:09:24.451 - 00:10:41.815, Speaker A: The second issue is that we need to maximize the halogen efficiency of the entangled photons in order to optimize the saturation success rate. So we need to develop a SPDC source with near unity collection efficiency results sacrificing the photon indistinguishability. And previously entangled photons were usually collected from the intersections of the two down converted rings, which are however not in a good Gaussian shape and cannot perfectly couple into a single mole fiber. In 2016, we engineered SPDC show that the two rings are converged into two separate circular beams, which is more favorable for the single mode collection. And finally there's unwanted spectral correlation, which you can see from the joint spectral correlation. The phase matching here should be adjusted so that you know, it becomes circular, which means this know the frequency is uncorrelated. So in 2018 we created SPDC Source with simultaneously 100% efficiency and interesting mobility.
00:10:41.815 - 00:11:40.995, Speaker A: So combine all these together. Recently we have performed such experiment, telepathic experiment. You can see the setup here. And the final telepathic efficiency, you know, without, without discharging any experimental loss imperfections, you know, is about 6.2%, which is six times higher than the 1% by direct transmission. So these things are promising. But you know, some people call the challenge this result, you know, much like Pan Zhang and his colleagues, you know, use imperfect fidelity in Google's Sigma experiment and you know, use that to shorten the classical simulation time.
00:11:40.995 - 00:12:38.987, Speaker A: So our teleportation fidelity in the end is also not 100%. It's about, you know, 80, 82%, 81%. So the classical player can use a better strategy. You know, first you clone the single photon into two to three photons with this fidelity. Then you know, send the two or three photons and in the end, you know, the efficiency can get a enhancement. And you know what is the best strategy? You know, reading this fairly old paper by Nicholas Jason and Sarge Massa, you know, we can have this equation. So the optimal fidelity equals to this formula M is the number of cloned particles.
00:12:38.987 - 00:13:57.675, Speaker A: So even if we take into account of this strategy, then the classical transmission efficiency will be enhanced by a fact of maximum 3. You know, as the bond is for two photons, the fidelity is 0.86. So even, you know, if we set up a bond of 3, the efficiency is increased by 3, we are still 2 times higher than the classical optimal strategy. Now let's move to the second topic, quantum computing. Well, as we mentioned, photonics were usually considered as ugly duckly mainly because of absence of two of photon photon synodicate. There are two remarkable insights which have greatly shaped our view. One is in 2001, you know, when Melbourne proved that linear optical quantum computing is in principle possible, although with a downturning high precision and huge overhead.
00:13:57.675 - 00:14:49.679, Speaker A: Today, with later developments, especially the class state quantum computing, the overhead is greatly reduced by a factor of three order magnitude. But it is still very very out of the reach of the current technology. Despite some of the bold claim from e.g. psy quantum. And another remarkable result, and a very surprising result was from Scott Allison and his student Alex. They designed a quantum device called boson sampling and proves that it can solve a problem that no classical computer can solve in a reasonable amount of time. So boson sampling experiment is much, much simpler than the KLM scheme.
00:14:49.679 - 00:16:08.285, Speaker A: We just need a length of quantized source, large interferometer and the detection is purely passive. So the biggest headache in kom such as active feeder forward and very demanding requirement of efficiency and fidelity. And also in other schemes shown photo induction is removed. And the boson sampling scheme set a very attractive goal in the community. Achievable goals, the first step to self improvement. So while the boson sampling proposal only require, you know, a high quality 50 to 100 photons, about 6 order magnitude lower length than for example running a 9 trivial shores algorithm, they can actually provide more compelling theoretical complexity. Theoretical evidence for the quantum computational speed up because the mathematical problem related to sampling in computational complexity language is much more difficult than for example factoring.
00:16:08.285 - 00:16:49.995, Speaker A: And so this protocol attracts a lot a lot of people, including myself. But when we started to seriously consider a large scale boson sampling. In 2013, we collected the best figures of merits in single photon source in a filameter and a detection. And if we assume we can compatibly put them together, so do 30 photon coins conduit would be 10 to the power of 100. And why this is so difficult? Let's hear what Scott said about the challenges.
00:16:50.455 - 00:17:22.425, Speaker B: You know, so what is the difficulty in scaling this up? Why haven't you know, the quantum optics people done it with 30 photons yet? Okay, well the central difficulty is that, you know, you need all n photons to arrive at your photo that detectors at the same time. Okay. That's the only way that you're going to observe an N photon interference effect. If you had super duper reliable, you know, single photon sources. Okay. Unfortunately those don't exist yet.
00:17:23.285 - 00:18:05.975, Speaker A: So Scott is right. We need to develop a super durable reliable single phone source. And ideally, you know, it should simultaneously fulfill the following checklist. The generation, collection and detection should have high efficiency, emission should have near zero multiprotum probability. And the photons should be identical to each other in all degrees of freedom. You know, usually experimentally, you know, achieving this checklist individually is easier. But to get them all working together simultaneously is much more difficult.
00:18:05.975 - 00:19:06.055, Speaker A: And this is not only in boson sampling, but also in all systems of quantum computing experiments. Because he have to build five or seven different chances criteria. And some of these criteria can even contradictory to each other. For example, the coherence and the computability is usually, you know, they don't like each other. So first we need to fix the indistinguishability of the single quantum source. And for quantum dot source, you know, it usually rely on the method of optical extension. And the previously the predominantly used method is non resonant excision which caused uncontrolled tiny jitter and electric field fluctuation which fundamentally limited the two photo interference visibility below 60%.
00:19:06.055 - 00:20:14.165, Speaker A: To solve this problem, we developed pulsed resonance station which directly drive the two level system and so avoided the pine jitter. And as you can see, the PI pass laser power is only a few nanowatts, which is 4 order magnitude lower than non resonant extension. So with this the photon indistinguity was increased to 66.5%. The next problem is photon collection. So to make emission predominantly directional, we couples emitter in a micro pillar cavity. Emitter lifetime is shortened from about 700 picosecond off resonance to about 80ps on resonance with a preserved fact of about 10 so the extraction efficiency is increased from 1% at the bulk structure to about 66%. We have better number now compared to five years ago.
00:20:14.165 - 00:20:56.115, Speaker A: So this is encouraging because in the extract efficiency. But extract efficiency is not everything. It's not the end of the story. So the use of lesson extension which made the photon quality almost perfect, came with a price. Why? Because a photon laser is exactly the same wavelength as a single photons. So we have to use polarization filtering to suppress the laser leakage, which at the same time caused an efficiency loss of at least 50%. So we designed a new protocol that solves two problems simultaneously.
00:20:56.115 - 00:21:56.305, Speaker A: Instead of using a symmetrical cavity, we employ elliptical micropillar cavity that split the cavity mode into two non degenerative linearly polarized modes. We put the emitter on resonance with the horizontal cavity mode, so most of the generated photons will be funneled into the horizontal. Second, we excite with vertical and collect with horizontal. So there's no photon loss due to a polarization filtering. You can see this left figure is the actual elliptical micro pellock sample we fabricated. So finally we can generate polarized single photons with simultaneously high efficiency purity and indexing ability. In principle we can put them all together, but much more engineering work should be done to make them, you know, perfect.
00:21:56.305 - 00:22:35.765, Speaker A: Now how about interferometer? It also has. It also must simultaneously fulfill a number of checklists. High transmission rate because every photon matters. Random metrics because we need to ensure the problem is hard connectivity so that the metrics cannot be reduced to smaller ones. We have tried different types of interferometers. One is by combining the micro optics with intermolecule force. And the feature here is high transmission rate over 99%.
00:22:35.765 - 00:23:26.745, Speaker A: The second one is time being encoded fiber loop. Following an early proposal by our much missed friend Johnson Darling and his co workers, the feature here is fully electronically programmable. The loss however is higher, about 20% per loop. So at that time we cannot afford too many loops. We tried four to five this year. Xenadoo has a much clever scheme that they use only three loops to induce non local entanglement. So in 2019 we scaled up to 20 photons and 60 mode three dimensional in filament which have output state space dimension up to 10 to the power of 14.
00:23:26.745 - 00:24:30.925, Speaker A: So currently we continue to optimize a single photon source. We have now more than 15 million single photon count per second with system efficiency of about 72%. And by the way, this surpassed loss tolerant sludge code scheme raised by Taylor Rudolph and his colleagues. Considering purely bottom loss, not as a depot Alyssa error. So originally you know why we push for the 20 to 30 regime photon regime, because this was shown early, should be sufficient to show quantum speed up. However, in 2017, classical algorithm improvement raised Sabat to about 50 photons. So how to go beyond 50? So interestingly, there's a highly efficient scheme called Gaussian boson sampling.
00:24:30.925 - 00:25:38.515, Speaker A: And here's an important feature is while most sub previous multiphoton experiments with SPDC were afraid of the multipair emission, you know, many of the audience who perform experiment with spdc, you know, we have to make sure the laser power is, you know, not too strong so that the P is usually smaller than a few percent. And those experiments can only exploit the tip of iceberg of spdc. However, GBS very nicely overcome this limitation. How it works. So in the original boson sampling we can see that there are many possible paths that leads to a certain multi photon coincidence. For example, from input 1234 to output 1347, there are more than 20,000 possible paths adding up together. So in GBS, instead of using single photons as input, GBS uses squeeze vacuum.
00:25:38.515 - 00:27:13.549, Speaker A: And squeeze vacuum is just a concurrent superposition of different even photon number state with a fixed phase relation from k equal to zero photons, two photons, four photons, six photon and so on. So the GBS can, you know, what a GPS brings in on top of the different path superpositions which I just showed in the in the previous slide is that there's also different input photon number combinations. For example, if we detect four photons in the output, it can come from different paths, but also it can come from combinations of different photon numbers. For example, the first one have four photons A equals 2 2, the second 1000 or you know, 2200 or 2 002. So there's also photo number superpositions. And you know, this is the way I find it convenient to explain GBS for discrete level a discrete variable optics people for continuous value group, you know, they have more subtle understandings. So now from the idea to experimental implementations, in the first experiment we sent 50 single squeezed data into 100 mode in filameter.
00:27:13.549 - 00:28:21.105, Speaker A: And this output and in the second one in the field meter is increased to 144 modes. And from idea to experiments is a number of new challenges, including the source in the field meter phase locking which I explain in detail. So first we need 50 single mode squeeze state and they should have simultaneously high level of indistinguishability. And efficiency. So we use a custom designed pulse laser with a reputation rate of 250 kilohertz instead of the routinely used about 80 megawatts to generate much higher peaker power. We use the deformable mirror to compensate the high order dispersion in this commercial laser. So we made a lot of effort in how to make the redesign and to tailor the commercial laser into our machine.
00:28:21.105 - 00:29:12.223, Speaker A: And you can see the intensity, the output spectrum is transform limited. You can see the intensity in the phase data. So the laser is split into 25 pairs and focus on 25 PKDP crystals. And you can see the, you know, frequency uncorrelated. And we also borrow idea from laser and we design stimulated squeeze light. And so we double pass the laser into the crystal to make more efficient use of the laser power. So in this way we generate the squeeze light source with higher providedness, purity and efficiency as you can see from these two plots.
00:29:12.223 - 00:30:36.585, Speaker A: And we can in the future we can scale to even higher order of stimulated spdc. And interestingly, when we are developing the simulated SPDC squeeze light, there's actually a spin off application on silver sensitive phase measurement. So the stimulated squeezer can actually be seen as a non linear interference which can measure an unknown phase and gives unconditional scalable and robust advantage. The method here follows the thin scaling as a known state to reach the Heisenberg limit. But the actual performance is much more robust than the known state because it has high tolerance for the external noise to the photon loss. For example, to actually show unconditional advantage for about 10 photons, you know, for the known state, it must have very high about 90% efficiency. But using this skin, you know, they can tolerate up to 99% photon loss.
00:30:36.585 - 00:31:56.575, Speaker A: So in our initial experiment we have directly observed about six fold enhancement above the shoulder noise limit and the fish information extracted by photon. And this is without discounting for any photo loss and imperfections and can even outperforms ideal 5 photon known state ideal meaning it's created deterministically and measured, you know, with 100% efficiency. And then there's no loss in the, in the circuit. Now back to the gps. So unlike the original boson sampling where there's no phase relation between the source, so the GPS require phase control of all bottom number states from the crystals to the interfield meter. So we develop active phase blocking for each optical path consists of 20 meter optical fiber and 2 meter free space. And the overall drift is controlled within 15 nanometers, which is only 1% of the wavelength of the photons.
00:31:56.575 - 00:32:57.705, Speaker A: So the whole setup looks like this. You know, for the first version, this is the commercial made laser system to corner light source. Each are collected into optic fiber which are wind along the PSO for face locking. And now then goes into this in a few meter. Actually the design of the geometry here is precisely what we arrange in actual optical table. So this is the most compact way we can, we can think about. And the second version we use stimulated squeezed light which you can see from here.
00:32:57.705 - 00:34:08.813, Speaker A: And in a few meter goes to three dimensional because there's no more mode. So this is animation that reproduce actual object table. And because of the COVID 19, you know, we. We haven't been able to travel for the in the. In the past three years. And I hope in the near future, you know, our lab can welcome all of you to visit. The optic output photon distribution at a different laser power is shown here.
00:34:08.813 - 00:34:53.055, Speaker A: So at the highest laser power, the maximum photo click is 113. We also measure low power data where the maximum Click is about 20. So everyone is welcome to play with our raw data. So because for 20 photons you know, you can just play with your laptop. But if the photonum is more than 40, then you have to rent a supercomputer and tens of gigabytes. Raw data is shown in this link and has been made available. The verification of boson assembling is perhaps even more challenging than experiment and have checked ongoing effort.
00:34:53.055 - 00:36:02.105, Speaker A: Unlike Shor's algorithm, a full certification of GBS is itself intractable for classical computation at least you know, we believe today. So what we can do at the moment is to first calibrate the experimental parameters and to provide evidence the quantum nature of the PBR device. To rule out possible hypothesis and possible hypotheses include thermal state with thermal state distinguishable photons and so on, which can result from photo loss or you know, mode mismatch. So in the GBS there's no perfect one single matrix that can fully validate the data, but allows a collection of analysis tools. And one of these is called a Bayesian test. So we define test strength delta H. If delta H is positive.
00:36:02.105 - 00:37:26.421, Speaker A: Well, it depends on, you know, the definition on the sign that it means experimental samples are more likely to be generated by GBS machine sense mockup. So we can look up our system. We first investigate a subsystem with fewer output bosonic mode. Say, you know, in total 144 mode. But we first look at the 80 and then we increase to 90100 from the data we can show, we can not only rule out the thermal and distinctionable mock up in the subsystem, but also we can see that the strength of the validation becomes stronger when subsystem mode number increase. So it's reasonable to infer that as a validation will succeed in the supremacy regime where we can not compute directly. Another useful intuition which we prove experimentally you can see is to testable gene data is that due to the classical computing power, we usually test the region which have smaller photo number which is left slope of this photo number distribution.
00:37:26.421 - 00:38:59.215, Speaker A: Here this region shares the same setup as higher photo number experiment the light slope, but effectively surface more photon loss higher, you know, photon loss. So in the condition of the thin input photon numbers, so we can deduce that at higher photon region, you know, the hypothesis that related to photon loss will be lowered out with even higher confidence as also proved in these two figures. So as we load in the first paper, we hope this work will inspire new selective effort to verify large scale gps, improve the classical simulation strategies. So we share our data and hope to inspire new efforts. And indeed there are a lot of interesting classroom algorithms improvement and and spoof one is from Google. The idea is to artificially generate Markov samples based on only one order and two order correlations. And they have shown better total value distance on small scale subsystems but didn't show high order correlation as an example and also didn't show for larger systems.
00:38:59.215 - 00:40:35.625, Speaker A: So we are also making progress in improving the calculation of the metrics and hopefully it will beat the mock up the second. The most recent spoof attack is from Nicholas, so we will give a talk later. So they came up with the squash state which is maximally similar to our targets squeeze state and shows that as a squash state can have very similar high order correlation. And they also try even more tailored tech by modifying the squeezing parameters, you know, different to the actual experimental parameters and to check whether it can generate higher Bayesian test score since experimental samples. So for Joseon 2, the data, you know, the experiment sample score is higher and we we have also include the more precise quantitative by calibrating the thermal noise in the system and we can show you as in the meta data point. The advantage of the external samples can get even higher. And as pointed out by many people, quantum supremacy is not a single shot achievement, but have to be defined over time to eliminate loopholes because smart people will come up with better and better classical algorithms.
00:40:35.625 - 00:41:36.785, Speaker A: This is in spirit very similar to the Bell test history which lasted for 40 years and this still ongoing efforts, for example to eliminate the freedom of choice, free will, something like that. Maybe people think some of these are paranoid. One last point I would like to mention is that GPS machine is partially programmable. We demonstrate last year. So because of phase amplitude and amplitude of the input speed states is also included in the matrix and they can be tuned so we can try to solve different mathematical problems. And finally you know what's next. We hope to make in a Fermi fully tunable and to find possible applications demonstrate for tolerant GKP code and there's a next round of quantum supremacy.
00:41:36.785 - 00:42:58.433, Speaker A: As recently talked by Scott Allison a lot is that, you know, they should have much higher fidelity, lower loss and more valuable. One candidate we are interested in is IQP using either superconducting system or you know, atomic arrays and on photons. I think is the next biggest challenge is how to develop deterministic photon photon scenario which is a new introduction of my research, my future research and my group is also interested in investigations of fundamental problems in quantum mechanics. For example, recently we rule out real number quantum theory and in addition our group also reproduced Google Sigma result with larger size with 56 and 60. So this comes to the end of my talk. Many people have made significant contributions to this work over the years. We have received support from many colleagues and friends and I'm just talking presenting the work by the Leo Kilos are my students and postdoctors who did the hardest part of the work.
00:42:58.433 - 00:43:10.425, Speaker A: And it has been a great fun to work with these brilliant minds. And with that I would like to thank you for your attention. Thank you very much.
