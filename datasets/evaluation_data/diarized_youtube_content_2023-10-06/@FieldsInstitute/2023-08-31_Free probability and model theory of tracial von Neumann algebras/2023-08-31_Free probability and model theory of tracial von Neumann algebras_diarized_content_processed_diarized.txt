00:00:00.320 - 00:00:44.150, Speaker A: So our next speaker is David Jackel, who will speak on free probability and model theory of tracial clinoim and algebras. Okay, thank you very much. Actually, Sree told me to change my title to factor so that all the talks in today would be two on factors. But whether something is a factor is not that relevant to this talk, unfortunately. I guess you guys have all seen the von Neumann algebras before in several talks. What I want to emphasize here, as I recap the definition, is that we can view these as a non commutative analog of probability space. We have a trace of annoyment algebra.
00:00:44.150 - 00:01:34.634, Speaker A: We have a trace. It's unital, it's tracial, it's positive, it's faithful, it's normal. And when you have a probability space and you have the l infinity of this probability space, gives you a motivating example for this, right? So commutative tracial annoyment algebras, if they're separable, will all have this form. And so you can view the expectation in that case as an example of a trace. And so, in the general setting, we consider the trace on the von Neumann algebra as an analog of the expectation. And the elements of the von Neumann algebra are analogs of l infinity. And l infinity of the probability space represents random variables, right? So the elements of your m are random variables in a non commutative sense, and the trace is the expectation.
00:01:34.634 - 00:02:53.384, Speaker A: And so, free probability theory, that came a lot from the work of. But I mean, there's many ideas about non commutative probability that have been around in the physics literature for a while, like studying and fermionic analogs of things. And so you really want to just use our knowledge of classical probability theory in order to motivate new results and see what happens in the non commutative case and how things might be the same or different. So here's one problem that we want to consider. So, as in Ben's talk earlier, you could think about isomorphisms between your operator algebras. That map specified tuples to another specified tuple, right? So we're basically looking at, you know, certain generators like that. And our, our goal right now is to figure out how could, you know, what's the right notion of two tuples from your operator algebra behaving the same? Now, first attempt, you could try, well, you can look at what happens in classical probability, and you can say, well, okay, there's, you could look at whether they have the same probability distribution in the classical case.
00:02:53.384 - 00:03:40.474, Speaker A: And since your random variables are bounded, they're in l infinity. The probability distribution requires you to test against all c zero functions. But if your measure is compactly supported, it suffices to check on it on polynomials, because those are dense by the Sohn Weierstrass theorem. And so having the same probability distribution could just be reduced to saying that the expectation of any polynomials in x and y are the same. But then it's also equivalent in classical probability to say that they're approximately conjugate by automorphisms. So if you have a probability space, you have two tuples of random variables with the same distribution. You can find sequences of automorphisms which will approximately move the x to the y.
00:03:40.474 - 00:05:00.042, Speaker A: And the reason for this is basically you consider the range of possible values of x and y, and you partition the range in some nice way, and then the corresponding subsets of the probability space where the values of x land in a specific set, or the values of y land in a specific set, these will have the same measure. And so then you just find, find some transformation moving these sets to each other. And if you choose the partition fine enough, then this allows you to build an automorphism so that x composed with alpha and y are within epsilon. So in this case, this kind of approximate automorphic equivalence in classical probability is just detected by the probability distribution. So in the non commutative setting, there has been a lot of work which uses the non commutative moments, right? Replacing this expectation of p of x equals expectation of p of y, where now you use non commutative polynomials, or in fact star polynomials in these tuples, x and y. And so we often use this as a definition of agreeing in non commutative law or non commutative distribution. But in a non commutative setting, the equivalence between two and three here fails quite drastically.
00:05:00.042 - 00:06:05.854, Speaker A: You can have two things which have the same non commutative moments, but don't behave the same. And this is kind of. So if you forgot about the particular generators and you looked at the algebras, then what this would be saying is that you can find two embeddings, two trace preserving embeddings of some algebra into m which are not approximately conjugate by automorphism. And so there's many examples or ways to construct such embeddings that are not conjugate by automorphism. So here I'll consider, like in Sree's talk, the property gamma, right? And we could consider, we're going to consider x and y to generate copies of a free group phonomenalgen. So you could take x to be the tuple of unitaries for the generators of the free group. And so if you embedded l into its ultra product by the diagonal embedding, and I realize my fonts don't match here, but these fn's are supposed to all be the same font.
00:06:05.854 - 00:06:47.778, Speaker A: Yeah. So there's an embedding, the diagonal embedding of L of fn into l power. This is not going to have anything that commutes with it because of the lack of property gamma, that of fn. But I can find another embedding of it that does have a bunch of stuff that computes with it. And the reason for that is, well, I could just take l ten l infinity, and this will be con embeddable. And if it's con embeddable, then it must embed into the ultra product of L of fn, because the ultra product of L of Fn contains the ultra power of r. So anything that's con embeddable embeds into there.
00:06:47.778 - 00:07:52.944, Speaker A: So therefore, now I've found another embedding of l into there, where it's actually an embedding of L tensor l infinity. So this copy of L has a bunch of stuff that commutes with it, right? And there's no way to even approximately conjugate these copies of L by automorphisms, because if you took the one that didn't have anything that commutes with it, it actually doesn't have anything that approximately commutes with it either. And so you couldn't approximate it by automorphic conjugates of the other one. And so this is saying, okay, the non commutative moments are not enough to determine the behavior of this random variable up to automorphisms, okay? But there is something in model theory which does tell us how to distinguish them up to approximate automorphism or something of that nature. Right? And so this is the type in model theory. So I think the previous two talks avoided talking about formulas in model theory. So I'm going to recap these a bit.
00:07:52.944 - 00:08:38.414, Speaker A: So when we're talking about the type, instead of just testing the non commutative moments, we have to test other quantities. We have to do other measurements of your x in order to figure out what it does. And that is measurements that involve sup and inf operations over unit both. So the formulas in model theory are built up recursively through various operations. So you start with basic formulas. And so for the, for the language of tracial von Neumann algebras, these basic formulas are formulas of the form real part of trace of some non commutative polynomial. Okay, those are the, those are the things we've just talked about before with these non commutative moments.
00:08:38.414 - 00:09:35.290, Speaker A: Then what else can you do? Well, you can take formulas that you already have, like several formulas, and then you can apply some continuous function after them. And we're thinking of this here as analogous with applying in discrete logic, you can apply logical connectives like and, and or, and we're replacing those things with continuous functions. So we're taking our real valued formulas, applying continuous function to them. Okay? And now here's the part where we have sup's and infs, right? So in where in classical logic, you use quantifiers, like for all, and there exists in the continuous logic, this real value logic, you use sup. And if and in the soup and inf, you're supposed to take the soup and inf over appropriate domains that are specified as part of your language. So, for tracial von Neumann algebra is what you're allowed to do. Your domains are operator norm balls.
00:09:35.290 - 00:10:29.562, Speaker A: And so you're allowed to take the sup or inf over some variable in your formula and over an operator norm ball of whichever radius you want. So, if you have a formula in n plus one variables, and then you take the sup or inf in one of the variables over the operator norm ball, you get a formula in n variables. And then by iterating these operations, you construct all these formulas. So each formula is then something that you can evaluate on a tuple from your von Neumann algebra. So you just plug in your specific x into the formula. Each soup and inf operation is interpreted as a soup or inf over the operator norm ball in your specific m. And of course, the evaluation of the formula very much depends on what the ambient algebra is.
00:10:29.562 - 00:12:15.392, Speaker A: Because if you take the sup over the alvator norm ball, right? And if you did this over a larger algebra, then something else could happen then compared to the smaller algebra. And then when you iterate the soups and infs and alternate between them, there's not even any monotonicity for a general formula under the inclusions of the algebra, right? So if you choose a bigger algebra or a different, or just a different ambient algebra, or even if you have your copy of x sitting inside the m in a different way, then you can get different answers for these sup's and amps. So, for instance, if you had some formula that involved commutators, then this formula could detect whether there exist things that approximately commute with your x's right. And so in particular, formulas would distinguish the example that I had before, because there would be some formula saying that the inf over the operator norm ball of the commutator of x and y is zero, meaning then there that if you take the inf over y, then that would mean that there's things that approximately commute with x. So now when we look at these formulas, then we can define the type of your tuple, and the type is just a mapping that records the values of all these formulas. So just like with probability distribution, probability distribution could be represented as a linear functional that takes in an input function as a test function and then spits out its expectation. And then in the same way you can take the vector space of formulas and then have a mapping from the formula to the real numbers, which just is the evaluation of the formula at the between x and this is what we call the type of this tuple.
00:12:15.392 - 00:12:55.344, Speaker A: Now in logic there's also types over relative to a specific set, and I'll get to that later. But here this would be types relative to the entity. So just like the space and probability distributions, I want to consider the space of types. So the space of types, and I'm not exactly sure who first used this notation, but they often use the letter s for the space of types. And we want to have a topology on the space of types as well. Just like for when we talk about probability distributions, we talk about weak star topology on the space of probability distribution. We want the same for the types.
00:12:55.344 - 00:14:00.884, Speaker A: Since this is linear functionals on a certain vector space, then there's a weak star topology. We use that weak star topology on the space of types for everything that comes from a certain operator norm ball. And maybe if you want to extend it to the whole space of types and not assume that you have a specific bound on the operator norm of the x's, then what you do is that. I think what works well is to consider inductive limit topology from the SNR. If I have a tuple of operator of operators and their norm is less than or equal to r, then they have a type and their type is in this space s and r, right? And then if you consider the union over all r, that's the entire space of types. And I'm going to say set as open in this space if and only if the intersection with each of the snrs is open in there. In the weak startupology, this definition may be a bit to absorb any questions about this.
00:14:00.884 - 00:15:16.044, Speaker A: Okay, so before I get to return to the question of automorphisms that I started with. One more detail about this is that, okay, so we have the space of formulas, right? And then we took a dual space of it by, by looking at these types. These types are elements of the dual space. But then if you have a formula, then that, again, can be viewed as a continuous function on the space of types, right? Because this is sort of like the embedding of x into x double dual, right, where x is the space of formula. The space of formulas is not, is not complete, right? I mean, we don't, we have to define a norm on it, right? But once you define a norm on it, it's not really complete. And we actually want to have something that's a bit, that's a nice completion of the space. And so this is the notion of definable predicate.
00:15:16.044 - 00:16:03.082, Speaker A: So basically, a definable predicate is something that can be approximated by the formulas. And, and these definable predicates represent an appropriate completion. So the continuous functions on the space of types will correspond exactly to these definable predicates. So formulas will be special cases of this. These define all predicates are somehow limits of formulas, and you can make the following definition for them. All right, so a definable predicate, you could just, there's a couple approaches to defining it. I mean, rather than defining it as like, just abstractly taking a completion of the space of formulas, I'm just going to define it as a collection of functions.
00:16:03.082 - 00:17:01.584, Speaker A: So, for each von Neumann algebra m, you want to be able to evaluate your definable predicate. So you have a collection of functions from m to the n to r for each m. But then the approximation by formulas is that for every epsilon and for every r, there exists a formula which approximates, there exists a formula p, which approximates phi uniformly on the product of operator norm ball with radius r. And this approximation is uniform over all mix. So if you have some, some object like this, that is like a uniform limit of these formulas, and that's the definable predicate. And so we really want to think about this topologically as well. The space of types is some nice compact housedorf space, and this space of, and the space of definable predicates, is this the space of continuous functions on the space.
00:17:01.584 - 00:17:58.784, Speaker A: Okay, so now, coming back to this question of automorphism, the types do detect the behavior of x up to automorphism. And here what I'm going to say is, yeah, so there are some issues to deal with. Well, do you want an actual automorphism on the entire thing, or just an automorphism on some large space. But what you can do that's pretty nice, is that you can take your m. Any m has an elementary extension in which the types precisely correspond to automorphism orbits. So you can find an elementary extension in which the two things have the same type if and only if they're conjugate by an automorphism. So I guess I didn't define ele, I didn't say out loud the definition of elementary extension.
00:17:58.784 - 00:19:22.708, Speaker A: But elementary extension means that if you take any formula and you evaluate it on the things that come from m, then if you use n as the ambient algebra, you get the same answer as if you use m as the ambient algebra. So in particular, right, if you're doing the sup and inf operations, taking the sup over m and taking the soup over n, give you the same answer if you're plugging in constants that come from m. So these types do actually capture the behavior of variables up to automorphism. And for that reason, they might want to view these as an analog of the probability distributions, right? So, coming back to the classical case, earlier, we saw in the classical case that actually, if you have the same probability distribution, and you don't need to use any formulas of SuP's and ifs, you just need to know the expectation of each polynomial, then in classical probability, if they have, if they have the same distribution, then they will be approximately conjugate by automorphisms. And so therefore they will have the same time. And so in model theoretic terminology, what this means testing the moments with any polynomial, these generate these quantifier free formulas. So these are these basic formulas.
00:19:22.708 - 00:20:23.374, Speaker A: If you take the basic formulas and apply continuous functions, these are the quantifier free. And so, having the same non commutative moments is just saying the quantifier free type is the same, and saying that in classical probability, if they have the same quantifier free type, they're approximately conjugate by automorphism. That will mean that if they have the same quantifier free type, they have the same complete type. And so, in order to know how the x and y behave with respect to all these formulas involving sup and infinity, you actually don't have to test any of the formulas with supine. All you have to do is test the quantifier free formulas, and that uniquely determines their behavior on, all right. And this is expressing the fact that this classical probability space, classical diffuse probability space, emits quantifier elimination. It actually tells you that any definable predicate in this, if you evaluate it on this l infinity space, is going to have to agree with a quantifier free definable predicate, which is like a limit of the quantifier free formulas.
00:20:23.374 - 00:21:21.032, Speaker A: And so basically, the reason why non commutative probability is more complicated is because in classical probability there's quantifier elimination. So basically, lots of stuff that you would do with SuP's and ifs becomes trivial and you can kind of just reduce it to something much simpler. The non commutative case, we don't have that luxury. And that makes stuff a lot more interesting. And so in particular, if we're trying to develop non commutative probability theory, it has, we have to take account of these facts. And so it might actually be better, instead of just looking at non commutative moments of something, to look at the complete type as an analog of the non commutative, as a non commutative analog of the probability distribution. So in particular, we would want to take concepts from classical probability like the Wasserstein distance and optimal couplings.
00:21:21.032 - 00:22:15.304, Speaker A: I'll tell you about these in a moment, as well as free entropy. So in Sri's talk, he alluded to this notion of free entropy, of measuring the volume of the space of matrix approximations for your x. And so you can actually develop an analog of that, where instead of just considering approximations with non commutative moments, you actually use all these formulas, not just the quantifier free forms. So this is the goal of my talk. There's actually several papers this is based on, which are kind of developing many of these ideas from classical and then non commutative probability into this setting of, of complete touch. Okay, so several things that will come up. One thing that comes up several times in what I'm about to pretend are these notions of algebraic indefinable closure.
00:22:15.304 - 00:23:08.734, Speaker A: So this might seem like a detour from what I said before, but you'll see later how this comes in. So earlier I was talking about automorphism orbits, right? When are two tuples equivalent by an automorphism, et cetera, right? And now you can also do this relative to some subset a. So if you have some subset a, you can kind of do all the concepts that I said relative to this a. So you can take formulas instead of just regular formulas where you take non commutative polynomials in x. You also allow the non commutative polynomials to contain constants from the set a. And so you kind of view this as like, I guess, non commutative polynomials with coefficients from a. And so you take those polynomials, you use them to build basic formulas, right? And you construct the formulas recursively with the sup and inf operations, right? But now all these formulas are formulas that can contain constants from a.
00:23:08.734 - 00:24:31.406, Speaker A: And so then analogously, you define the type over a as the mapping from this set of formulas with coefficients from a into the real numbers. Um, and so now, uh, if you consider, um, yeah, if you consider this, uh, question of conjugacy by automorphisms, right? If you have some subset of n, and let's assume that this n has this nice strongly homogeneous property that the types correspond to automorphism orbits. Well now you could ask yourself, well, suppose I consider automorphisms that fix the set a point wise, then what else has to be fixed by those automorphisms? And if you consider this analogous thing for fields, this is exactly like the galaw closure of the subfield generated by a. And then there's a similar notion of algebraic closure. The algebraic closure is a set of things where every automorphism that fixes a doesn't necessarily fix x, but it can't move x around too much. Automorphisms that fix a will, if you apply them to x, it will generate an orbit which is compact. So if you replace compact with finite, and you thought about the field case, then this is actually like legitimately talking about being algebraic.
00:24:31.406 - 00:25:26.916, Speaker A: But since we're in the metric setting, we don't just care if the set is finite. What we really care about is whether it's finite up to epsilon for every epsilon, which means that it's compact or totally bound. So these are the definable in algebraic closure. And then if you think about this in terms of types, right, the type, the real set of realizations of the type over a corresponds to this automorphism orbit under the things that fix a. And so then saying that it's in the algebraic closure means that x is the unique realization of its type over a. And then being in the algebraic, the algebraic closure means that the realization set of realizations of the type of x over a is compact. Okay, now I think there's not been that much study of definable in algebraic closures for Anumian algebra.
00:25:26.916 - 00:26:20.564, Speaker A: So I wanted to give a couple propositions to illustrate what these things behave like. So first of all, uh, let's say I'm going to take a, I'm going to consider a, to be a subalgebra, right? Not just a subset, because turns out the definable and algebraic closures are automatically von Neumann algebras. So I really only care about the case if a is von Neumann algebra already. So let's take a subalgebra of m and then the definable closures contained in the relative biconmuton. And this is an immediate observation. The reason is that you can get automorphisms of the von Neumann algebra that come from conjugation by unitary. And if you take a unitary in the commutant of a inside of m, then that automorphism induced by that unitary will fix a point wise.
00:26:20.564 - 00:27:02.184, Speaker A: And so therefore, if you're in the definable closure, then every unitary in the relative commutant is forced to fix this x under conjugation. And as you know, any von Neumann algebra is generated by unitaries, in fact spanned by unitaries. And so if I show, so if I take an x, if x commutes with every, with all the unitaries in the relative commutant, then therefore x actually has to commute with everything in the relative commutant, and so therefore it's in the relative bicommutant. Yes.
00:27:09.564 - 00:27:13.780, Speaker B: Maybe I'm being silly here, but it's kind of a closer, it's just equal to the diagonal.
00:27:13.892 - 00:28:19.504, Speaker A: Yeah, so that's a good example. Let me write this with chalk. Right. So let's say that you have m and you embed it into the tolsr product. So then the definable closure inside the ultra product of m will just be m itself. And the reason for this is a general fact that the definable closure of something has to be contained inside of any elementary sub model that contains it. Actually, here's a fact which I guess I didn't put in my slides, but fact, if you have this m which is like a, like saturated and strongly homogeneous in s above, then the algebraic closure in m of some a is actually to the intersection of all, intersection of all n, which is elementary sub model of m, which contains a.
00:28:19.504 - 00:28:28.492, Speaker A: So this is another way to think about the algebraic closure. Any elementary sub model is forced to contain the algebraic closure.
00:28:28.628 - 00:28:51.024, Speaker B: So one of the mark that I would like to make is that there's a famous open question of soline copper. That relative biconcutant of the diagonal embedding inside the ultra power is equal to diagonal bedding, then is it true that it's isomotive?
00:28:53.164 - 00:29:32.654, Speaker A: Well, the relative, okay, the relative bicon, yeah, I mean, I don't know the answer to that question, but that is a nice connection. Okay, in your observation, did you mean dcl? Oh, yeah. Okay, so, sorry. I knew there would be some type of somewhere, but I didn't, I looked through it and I couldn't find the typo. So. Yeah, the algebraic closure in m of a. So it always, so, as another example, right? If you have a maximal abelian subalgebra, then the relative commutant of it is itself.
00:29:32.654 - 00:30:21.354, Speaker A: And so the relative bicommutant of it is also itself, and the definable closure will then be equal to itself. Now here's a, well, I want, I want there to be an actual automorphism of the entire m, right? And I'm not necessarily assuming the continuum hypothesis, so I'm not countably saturated. Yeah, I mean, both, they're both countable. Yeah. If you made it much more saturated, I suppose this will be true. Okay. Anyway, so here's another example where, here's an example where the definable closure can be much larger than the original m.
00:30:21.354 - 00:31:14.284, Speaker A: And so this actually has to do with rigidity. So suppose that I have a inside m, and I'm assuming that it's an irreducible sub factor, meaning that the relative commutant of a inside m is trivial. These are just the complex numbers. And I'm assuming that it has factorial gap, which means that there's nothing that approximately commutes with a in m. And so then the conclusion is that if you take the normalizer in m of a, right, so the normalizer is a set of unitaries such that u, a u star equals a. That normalizer is contained inside the algebraic closure. And actually the commutator subgroup of the normalizer is contained inside the definable closure.
00:31:14.284 - 00:31:46.766, Speaker A: And here's why. Okay, so here I'm going to reason just from being fixed under automorphism. I mean, there's another reason I could reason and prove this more formally in terms of the, like the formulas and the types. But let me just do it with this galaw perspective. So, assume that I make an elementary extension where it's homogeneous. And suppose that I have u, which is in the normalizer. And suppose then I have an automorphism fixing a.
00:31:46.766 - 00:32:24.594, Speaker A: So if I have an automorphism fixing a, then I want to consider, want to consider what is alpha U. So now we look at alpha of u and alpha of u. If we conjugate an element in a. Well, a is fixed by alpha. So I could rewrite this as alpha of a, or I could write it as alpha of U star au that's assumed to be an a. So that's also fixed by alpha. Right? So if I have something in a, then the conjugation of it by alpha of u is the same as the conjugation of it by u.
00:32:24.594 - 00:33:04.654, Speaker A: And so that actually if you rearrange this, this means that alpha u star u or alpha u inverse u commutes with everything in a. And by our assumption about the spectral gap and having trivial relative commutant, that will mean that it's a scalar multiple of the identity. Um, and so say it's lambda of u times U. And so then in particular the, the set of realizations of the type of U or the automorphism orbit of U is contained inside scalar multiples of U. And so in particular it's compact. So there, that means it's in the algebraic order. Yeah.
00:33:04.654 - 00:33:51.450, Speaker A: Yes, yes, that's a good point. But if it has spectral gaps, then this will be true, because it will be true for any elementary extension that it will have a trivial relative combaton as well. Yeah, yeah. Well, I mean, you could say so that whatever the n is, right, the n will be contained in some ultra power of m, right. And commuton in there will still be trivial because I assume this has spectral gap. Yeah. Right.
00:33:51.450 - 00:34:29.214, Speaker A: And now the second part of the proof. Why is the commutative subgroup in the, in the definable closure? Well, if I have this automorphism, right? And if each U is mapped to a scalar multiple of itself, it's an automorphism, so it respects multiplication. And so that will imply that this lambda sub u is a character, right? Lambda sub U v is lambda U times Lambda V. And so it's a, it's a homomorphism from your group into a commutative group. And so it thus vanishes on the commutator subgroup. So if, if it's in the commutator subgroup, then actually it's a unique realization of its type of array. And so it's in the definable query.
00:34:29.214 - 00:35:35.290, Speaker A: So there are many, so I should say there's many examples to which this proposition applies, because, for instance, suppose that a has property t, and then you take a properly outer action of some group on the property t thing to generate your m. And then, then in this case, the definable or say of the algebraic closure will be the entire m. And recently in the work of Joanna, Jimba, Osun and Sun, they okay, whatever. They had this paper on, a very important paper on property t. And in particular they showed that you can have basically any group that you want actual by having an outer action of any group that you want upon something with property T. And so then that will construct a lot of examples like this. So the group, the group acting on your thing will generate elements of the normalizer.
00:35:35.290 - 00:36:42.734, Speaker A: And then if the thing has property t and your action is properly outer, then it will be irreducible and it will have this. So I'm gonna, this is the last thing I'll say about like this construction per se, but I think this is a good result to motivate, like trying to study connections between rigidity and von Neumann algebras versus these notions from models. Okay, so now I'm going to talk about kind of the analogs of optimal transport theory for these types, right? So remember I was saying the types, the complete type is a good analog for probability distribution. And then I want to take ideas from classical probability and implement them for the types. In particular, there's this notion of Wasserstein distance. And already in past work, such as by song, people have studied probability spaces from a model theoretic perspective. And song in particular studied the L1 Wasserstein distance in this framework.
00:36:42.734 - 00:37:54.114, Speaker A: Now I'm going to study the L two Wasserstein distance here, but in the non commutative setting. And this Wasserstein distance actually coincides with a more general model theoretic concept, which is a so called d metric on the space of types. What is this d metric? Well, you just consider your two types, I'll call them mu and nuke. So I have two types. The distance between them is the infamum of the distance of x and y, where x and y are any realizations of these types. Now, if you think about a situation where the types correspond automorphism orbits and it's saturated enough, then this distance between types, you could think of it as how close are these two automorphism orbits? Right? You look at your orbits and like what's the minimum distance between these two orbits? All right, so that's the Wasserstein distance between these things. Now, in classical probability theory, the Wasserstein distance gives you something that gives you the same as the weakstar topology on the space of probability distributions.
00:37:54.114 - 00:39:01.554, Speaker A: And this is actually not going to be true in a non commutative setting. So one way that you can see this is a continuous version of Wilnarzewski theorem, and it's saying the Wasserstein distance gives you the weakstar topology if and only if the theory that you're studying is not categorical. I'm not going to define what that means, but this is something which is true for the commutative probability spaces. And it's very much related to this fact that any two things with the same quantifier free type are approximately conjugate by automorphism. So this is very closely related to that and to the quantifier elimination of the classical probability space. But most of the time, if you read the papers of Goldwing, Hart and Sinclair, and the papers of far, Hart and Sherman, like they show, there's not, it's impossible for any two one factor to have this property. And so in particular for, for that theory, you can't have the washer Stein distance agree everywhere with the weakstart topology.
00:39:01.554 - 00:39:51.634, Speaker A: But you could also ask a more refined question. So what if the two topologies agree at some point, meaning that every neighborhood of the point in one topology contains a neighborhood in the other topology. And if that's true, that corresponds to, then that point is called a principal type. And it's an open question at this point to figure out what the principal types are. So it's true if your x generates an amenable algebra, then its type will be a principal type. But other than that, we don't really know much, at least not that I'm aware. So, in the quantifier free setting earlier in the paper with Gangbo Nonman Shia, and it looks like I forgot a comma, the actenco there with another typo.
00:39:51.634 - 00:40:52.714, Speaker A: But anyway, we studied the Wasserstein distance on the space of quantifier free types. And in that case we showed if you're looking in the conventible setting, then being a principal quantifier free type is actually equivalent to generating something amenable. This doesn't, the same proof doesn't automatically work for the setting of complete types. So there's a lot more to do here to figure out what's going on. But this is a good open question to think about. Can you figure, can you classify? Or if not classify? I mean, at least figure out some better understanding of what could be principal types for the theory of sum to one factor. Okay, so yeah, so looking at these two topologies, we see some things where the classical theory and the non commutative theory are pretty different.
00:40:52.714 - 00:42:01.056, Speaker A: But what I'm going to say next is something where you can succeed in emulating the classical theory to some extent, and that's in looking at a kind of dual characterization of this Wasserstein distance. So I'm going to express the Wasserstein distance in an equivalent way by kind of testing the values of certain definable predicates on each of the two types. Now, in order to do this, there's a couple of, a little bit of notation. The first, since we're trying to minimize the l two distance between x and y, and x and y are assumed to have a particular type, mu and nu. Well, the l two norm of X and the l two norm of Y are then fixed because they're just determined by what the type is. And so then, in order to make the l two distance as small as possible, the only thing that I need to do is make the inner product as large as possible. Because if I expand out the l two norm squared, right, I expand that out, then I have a norm of x squared plus norm of y squared minus twice the real part of the inner product.
00:42:01.056 - 00:42:25.634, Speaker A: And I guess I wrote inner product there, but you should really say the real part of the inner product. Okay. And I think that applies throughout the talk. When I write inner product here, you can just assume the real part of the inner product. Okay, so then this c of mu nu, I'm just going to define this to be the soup of the real part of inner product of x. And. Yeah.
00:42:25.634 - 00:43:28.630, Speaker A: And saturation will imply that this supremum is achieved. Okay? And so now we want to characterize what is the supremum. So, in the classical case, you have the following theorem. So say that you have probability distributions which correspond to types in the classical probability space. So then you can characterize this c of mu nu, this like maximal inner product between copies of x and y. You can characterize it as the infimum of integral of fdmu plus integral of g d nu over pairs of convex functions f and g, that satisfy this certain inequality, right? And I mean, this is kind of magical, right? Because this is not like this. F and g, these are just functions of one variable, right? And I'm just testing integrating f with respect to mu and integrating g with respect to nu.
00:43:28.630 - 00:44:43.176, Speaker A: And then if I test all the convex functions that satisfy such an inequality like this, then this will capture, sorry, this will capture this optimal inner product. So, with gangbo and naminish, Liaksenko, we studied this for quantifier free types for the tracial von Neumann algebras, and we had some theorem of that. But what I did recently in the paper, which I posted last week, is the version for complete types. So in the version for complete types, what do you do? You kind of have the same type of statement. You have the infamum of a phi of mu and c of nu. So this just means, okay, the pairing between the definable predicate and the type, because definable predicates are continuous functions on the space of types, right? So that's the analog or at least an analog of this. Right? So you take the infamous of this and you consider definable predicates that are convex, right? So the definable predicate is a function that you evaluate on tuples, and you just demand that that's a convex function.
00:44:43.176 - 00:45:36.694, Speaker A: And if it's, if it's convex and if it satisfies this inequality, although right here, maybe there's a little bit of defect here. And then I'm only assuming this inequality on a product of operator norm balls. And, but this is basically like an analog of the previous theorem. I guess another defect or difference at least, is that when I'm talking about definable predicates. If I were looking at that in the classical probability case, the definable predicates don't always come from point wise functions f and g on rn. So in a sense, this theorem doesn't exactly mimic, mimic this case, because it's very difficult to understand what would be the analog points in the non commutative setting. But it is a pretty similar result.
00:45:36.694 - 00:46:14.614, Speaker A: And I'll remark, I guess I'll keep this short for the sake of time, but basically this theorem works out better for the complete types than it does just for the quantifier free types. I mean, and I think, you know, this is one of the motivations for trying to study the version for complete type is that some things work out better, and this is one of them. Okay, so I may or may not have time to even get to entropy at all, but I do want to tell you about the convex definable predicates. Yes.
00:46:15.194 - 00:46:29.454, Speaker B: Question about the definable closure. Do you prove something like if you have savage n of n, then the one boundary entropy of n in the presence of.
00:46:34.164 - 00:46:38.704, Speaker A: That's a question that's relevant at a different point in this talk, like later on.
00:46:40.644 - 00:46:47.744, Speaker B: Okay, so, so do you prove it or not? You'll find out.
00:46:48.364 - 00:47:10.684, Speaker A: Ask me the question later. This is, this is not right point to ask this question. Okay. Anyway. Okay, so how do we study these convex definable predicates? So you might remember a function is convex. Say a function on a Hilbert space is convex. If in, say, okay, say a lower semi continuous convex function.
00:47:10.684 - 00:48:40.554, Speaker A: This happens if and only if for each point there is a, some vector, right? Or there's some, say, hyperplane that is underneath the graph of your function. So we want to prove a similar result for the convex definable predicates. And what happens actually in this case is that you can find, not only can you find a vector which is a subgradient which gives you this floating hyperplane, but actually you can find this vector in the definable closure of x, the point where you're, where you're trying to kind of look at the point where you're considering differentiating your function. And so what is the reason for this? Well, okay, so first of all, we pass to an elementary extension that's sufficiently nice. And if you look at all the vectors y that satisfy such an inequality, the y's that satisfy this inequality are going to form a closed convex set. And then if you have a closed convex set in the Hilbert space, there's a unique point of a minimum of minimal norm. Now if you consider automorphisms of your thing that fix x, then, and well, if it's an automorphism, then this function phi is also invariant under automorphism.
00:48:40.554 - 00:50:13.918, Speaker A: So if the automorphism fixes x, then this convex set of vectors y, this convex set of subgradient vectors, will be invariant under the automorphism. And so in particular, the point of minimal altunar has to be fixed by the automorphism as well, which means it will be in the definable closure. Another consequence of this is that these convex definable predicates satisfy a sort of Jensen's inequality type statement, which is the following, that if you have a, if you have n, which is an elementary extension of m, or equivalently m is an elementary sub model of n, then f of the conditional expectation of something onto m is less than or equal to f. And the reason for this is the following, that say I, let say I pick a point z. I want to compare the values of phi on z and on the conditional expectation of z. And, well, I pick a sub gradient vector y as in the previous slide, that's in L two of the definable closure of x, and then I use that subgradient condition at the point x. And because the definable closure has to be contained inside any elementary sub model that contains x, then this y actually comes from m.
00:50:13.918 - 00:51:19.874, Speaker A: And so if I take the inner product of y and z minus x, this will be zero, because y comes from m and because z minus x is z minus the conditional expectation of it onto mistake. And so you get this kind of Jensen's inequality. And so if you think about Jensen's inequality, Jensen's inequality in general is like a statement about, about averaging. And in this case, if you have this conditional expectation, I don't know for sure, this conditional expectation can be given by averaging. But somehow the role of like choosing the minimal thing in l two norm kind of takes, replaces the philosophy of averaging in this setting. But you kind of get remnants of what you would think from classical convex functions in here. So here's another type of statement that comes out from this reasoning of kind of using classical convex functions to using ideas from classical convex analysis and applying them to these definable predicates.
00:51:19.874 - 00:52:33.544, Speaker A: So if you have an optimal coupling of two types, meaning like x and y, are, have the types mu and nu, respectively, and they realize the closest distance, then if you take a strict convex combination of x and y, then that thing, the definable closure of that thing has to generate everything that came from x and y. So the definable closure of x and y has to be the same as the definable closure of the strict convex combination. And in a sense, this is kind of an analog of optimal transport theory. Because in optimal transport theory, classically, this is, this is true. If you have an optimal coupling, then for anything that's strictly in the middle, you can, you can express all, each of them as a functions of the other ones. And this idea of being in the definable closure is in a sense something like expressing that it's a function, that one thing is a function of the other one. Now, I'm actually going to make another claim here that actually x and y are not only in the definable closure, they can be expressed by applying definable functions to x and to this thing.
00:52:33.544 - 00:53:22.836, Speaker A: This is actually a general statement. That's true for facial von Neumann algebras. And I believe the same argument would work for continuous model theory. In general, if your structures come sort of come have a, if your metric spaces are basically subsets of some Hilbert space, if your language is some sort of Hilbert space with additional structure, which I think in tracial von Neumann algebra, it is because you have the inner product coming from this trace. So the statement is that anything which is in the definable closure can be expressed as a definable function of the set that you're taking the definable closure of. Now, I haven't told you. Yeah.
00:53:22.836 - 00:54:37.754, Speaker A: So up at the top, I guess I have the definition of definable function. The definable function means that I have a, that it's a function that I can evaluate on arbitrary tuples. And then the distance between f of y or, sorry, f of x and y will be a definable predicate. And so this theorem is essentially saying the following, like, suppose, suppose that I have something in the definable closure, right? That's just okay for my particular a, there's an x which is in the definable closure. But then I'm saying that you can actually make a continuous selection. If you, if you replaced, instead of having particular a one, a two, etcetera that were constants, you could, if you allowed those things to vary, you can actually find a continuous selection of something in the definable closure of, say, y's, which at that particular choice of a will give you the particular element z, which is in the definable closure of that. And the way that this works, well, first of all, kind of by general facts about model theory there, and like, reasoning with these definable predicates and stuff.
00:54:37.754 - 00:55:44.144, Speaker A: Well, if the distance is the definable predicate, then so then the inner product should also be definable predicate. And if it's definable predicate over a, then that will mean that you can actually express this as a definable, some definable predicate that you can evaluate on arbitrary points. But now you take this definable predicate and evaluate it on x and a one, a two, etcetera, right? And so this is true, you know, for this particular, a one, a two, this, this definable predicate agrees with the inner product of x and z. And now if I add this phi and I, and if I differentiate it with respect to x, then I'll get back the vector z, the gradient with respect to x. This thing will be z. So what I really want to do is to say that I can arrange my phi to not just the, any definable predicate, but one which is globally differentiable with respect to x. And so then I'll use the gradient with respect to x as this definable function to achieve this result.
00:55:44.144 - 00:56:09.184, Speaker A: And let's. Yeah, I have not much time left. Okay. But so I may go quickly here. I'll try to say the basic ideas out loud. If you don't have time to read everything on the slides, that's fine, you can read them later. But the idea here is I take an arbitrary definable predicate and then I want to somehow regularize it and make it smooth.
00:56:09.184 - 00:57:06.494, Speaker A: So I want to arrange that it's differentiable and that the gradient is also a definable function. How can I do that? Well, if you have functions on an infinite dimensional space, you can't necessarily use the same tools in finite dimensions. Like finite dimensions, you take a function, and you convolve it with a smooth, compactly supported function. And then this will make it smoother. And this is how you get arbitrary continuous functions to be approximated by smooth functions in infinite dimensional space. You cannot exactly do that, but you can do something else, which is in fact a sort of regularization where you use supes and infs to kind of force your function to be semi convex and semi concave, meaning it's sort of, you force your function to kind of be smooth enough that you can touch it from above and below by parabolas and this. And once you do that, then it will, then it will be forced to be a c one function with a lift, its gradient.
00:57:06.494 - 00:57:54.950, Speaker A: So using such a method like this, you can actually show the following theorem that you can approximate any definable predicate by one, which is one which is like a definably c one function, right? It has a gradient, which is a definable predicate and which is. Which is. And the way that you do this is some kind of formulas with soups and inks. So I have a formula of original formula phi in variables x and y. And then I want to find another formula which is going to be kind of c one as a function of x. You can do it in the following way by taking, like, some soup and some int. Okay, now, don't worry specifically about this formula.
00:57:54.950 - 00:58:30.004, Speaker A: It's just some quadratic thing. It will force it to be semi convex and semiconcave. And then from there, you kind of reason with convexity. And you keep arguing until you show that it's differentiable and that the gradient is a definable predicate. And the definability of it really comes from the fact that this function has a Lipschitz gradient. And so if you look at the limit to define directional derivatives, then this limit will occur uniformly. And this uniformity is what allows you to kind of get this thing to be defined.
00:58:30.004 - 00:59:48.554, Speaker A: So take away from this, maybe too fast to get the details of this last part. But the point that, say, convex analysis and optimal transport theory may naturally go together and combine with this continuous model theory. Because if you're doing these operations on convex functions, like doing these inf convolution and sup convolutions, or if you're doing legendre transform of convex function, that's a soup. And so it's very natural to say that if you took formulas in continuous model theory and then do these operations to them, this will create other formulas because you're just using sup. And inf operations. And the other claim is that using these types is a good framework to make optimal transport theory work. Um, and another takeaway from the previous result that I stated here, this actually shows that there's a lot of definable functions and something which wasn't really clear before, like, you know, you can write the definition and prove properties of definable functions, but how do you know these things actually exist? Well, earlier I showed there's examples, right, with, where, with this spectral gap where you get a lot of things in the definable closure that weren't in the original algebra.
00:59:48.554 - 01:00:38.154, Speaker A: And then more recently I said, well, everything in the definable closure can be realized as a definable function applied to some elements from the algebra. So in particular, there has to be enough definable functions that you can actually express all these things as a definable functions. Right. And before this, I wasn't aware of any examples where you know of a definable function where, where you even know that the output of the function isn't in the von Eumann algebra of the input point, but this shows there actually has to be a pretty large amount of definable functions. Right? So I'm out of time. I have more to say about the entropy and adapting that to the model theoretic setting, but you can ask me afterward if you want to learn more about that.
