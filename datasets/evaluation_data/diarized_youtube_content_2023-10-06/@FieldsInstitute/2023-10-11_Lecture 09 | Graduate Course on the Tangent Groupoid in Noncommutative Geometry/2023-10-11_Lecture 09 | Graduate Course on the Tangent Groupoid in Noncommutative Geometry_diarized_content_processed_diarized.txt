00:00:01.320 - 00:00:45.534, Speaker A: We'll continue today with sorts of mathematics that we were doing on Thursday all those days ago, talking about the tangent groupoid of euclidean space, at least for a little bit. We'll glue together charts to talk about the tangent groupoid of a manifold. But on Thursday, we'll also look at the tangent group point of a manifold in some more satisfying, intrinsic. I don't know if it's satisfying or not in some intrinsic way as well. All right, so let's just see what happens. Oh, and we'll talk about scalability, scalable operators. We'll get back to all of that stuff, too.
00:00:45.534 - 00:02:04.274, Speaker A: So we had defined this thing not for all M so far, but for basically rm. But the way we put it, we'd used some, a little bit of group theory and everything works because the M's that we were considering have a free transitive action of RN we're calling rna, which I'm coming to regret now that we're starting to talk about operators, because everywhere up to this point, a was an operator. But anyway, fix that in the notes at some point. So that's the state of play. And we built this thing as an example of what's called a transformation groupoid, except that we just very slightly changed the notation. Anyway, it's basically quadruples like this. So two m's from m and one a from a and a t.
00:02:04.274 - 00:04:25.504, Speaker A: And the condition is that you can get to of the second m from the first one by adding t times a like that. Okay? And the t's come from the real line, and the m's come from n, and the a comes from a. If you have an open set inside of m, then we can slightly extend our definition just by requiring in the above formula that the m's belong to you. That's simple, t like this. Now, we've defined the tangent group void for a whole bunch of manifolds, sort of. And the last thing that we talked about last time was something about diffeomorphisms. And that last thing extends to this modestly new context of tangent groupoids of open sets, in the obvious way that if u, let's say in w, in rn, in mister are open, and if you can get from one to another by some change of coordinates, by some diffeomorphism, then you can manufacture out of this diffeomorphism from tu big tu, that is, to big tv.
00:04:25.504 - 00:05:32.884, Speaker A: So in the notes, I can't remember if I did this in class, but in the notes I used a big, big fat f like this. Cw is also a diffumorphism, where this thing is defined in the following way. There are two things you might want to say separately for the sake of clarity. Namely, you might want to say what happens when t is not zero? And what happens when t is zero? And if t is not zero, then you're supposed to do the following thing, which I don't quite have room for here. You see, if t is not zero, there's a formula for a in terms of the m's, which we can take advantage of. It's not really an independent thing. That is the formula for a if t is not zero.
00:05:32.884 - 00:05:57.050, Speaker A: So this formula that I'm writing down would make sense when t is not zero. So you just apply f's. Everywhere. There's an m two, let's apply f to it. There's another m two and an m one. Let's apply f to them. Running out of room.
00:05:57.050 - 00:06:26.254, Speaker A: There's another f two, excuse me, m two, let's apply f to it. And then there's a t. So that's a legitimate formula. Sorry for squashing it over multiple lines. If t is equal to zero, then things look a little bit different in the sense that two m's are the same. And you cannot figure out what the a is in terms of the m. So you've got to do something different with the a.
00:06:26.254 - 00:07:35.904, Speaker A: And what you do, well, the m is no problem, you just apply f to it. What you do is you apply the derivative at m of f to a like this. So this is all what you're supposed to do if t is zero, obviously. Anyway, these formulas, which are disjointed, you know, you give a formula in two bits, like when you make Frankenstein something for the head and something for the rest of Frankenstein, despite that Frankenstein ish nature of the definition, this thing here is a diffie morphism, coordinate change, if you like, smooth coordinate change. And it's compatible, despite our little venture into the world of calculus here with derivatives and so on, it's compatible with this underlying category theory structure, with this underlying idea of composition that we talked about last time. So this is not just a diffeomorphism and, and isomorphism. Isomorphic, functor.
00:07:35.904 - 00:08:21.638, Speaker A: Diffiemorphic. Well, functor, the fact that it's a diffeomorphism means in particular that it's an equivalence, indeed, in isomorphism categories. So all of the algebraic structure that the tangent groupoid has, which you might think depended on addition and scalar multiplication, all of the things we use to build the tangent groupoid, all of that algebraic structure is preserved by an arbitrary diffumorphism. So this is what we said last time when u is equal to w was equal to all of rn. And nothing that we said last time when u was equal to w which was equal to rn. Nothing really depended on u and w being all of rn. Exactly the same formulas work, same appeal to Taylor's theorem works.
00:08:21.638 - 00:09:47.464, Speaker A: But boom, you're in business. So no difference to the proof to prove this very slightly more elaborate statement than we had last time, but now you're in business if you want, if you have any manifold on which calculus works or any smooth manifold, I mean, you could do this whatever your personal opinion of a man. I mean, however you personally define manifolds. But we're imagining here that these manifolds are matrizable as topological spaces, a couple of space. Some people are happy with non hazdoff manifolds, some people are happy with host of manifolds which are not para compact. Okay, fine, you can, if you are happy with those, you can do this construction and you'll get one of those out of it. But if you start off with a decent, God fearing manifold, then that's what you'll get out of this construction.
00:09:47.464 - 00:10:44.040, Speaker A: So we're just going to use local CoOrDINates to figure out how to make t of m into a manifold. First of all, we have to decide on what t of M is as a set. And for that we'll take inspiration from what we saw earlier on. I like to stick in a zero like this, but of course, I'm just crossing with a one element set. That's the tangent bundle of m times a one point set, if you want optional. And we just take a disjoint union inspired by what we did of this space. M times m times the non zero real numbers, which you're supposed to think of itself as a disjoint union of copies of m times m parameterized by the non zero t's.
00:10:44.040 - 00:11:36.804, Speaker A: So we'll just define this just as a set. To begin with, this thing is going to end up being a nice, smooth, connected manifold. Connected, if M is connected. But to begin with, it's just a set. Well, it's more than a set, because those groupoid operations that we discussed last time make perfect sense here. It's also a groupoid. So in order to define a groupoid, you need to have these maps called source and range, and they need to end up in the same place that's going to be m times r.
00:11:36.804 - 00:12:21.582, Speaker A: And what these maps do is what they did before but now we're in a slightly more general situation. So there are two possibilities. Maybe you have an element of t of m, which is just a tangent vector, comma zero in my notation here. So we'll just send that to m zero. Or maybe you have a point in the groupoid which is one of these triples. We're in this situation here. We'll just send that which we're doing here, source to m one t, and the same basic thing with a range map.
00:12:21.582 - 00:13:07.458, Speaker A: Well, no difference here and down here we'll just pick out m two instead of m one. That's not the only thing you're supposed to do. You're supposed to define a composition law. But it's just going to be the obvious composition law, namely concatenation of, well, effectively pairs m two s m one s. So m two m three, comma m two composed with m two, comma m one. Those are composable morphisms, because the source of one is the range of the other. Anyway, the composition is m three, comma m one, comma t in the t not equal to zero case.
00:13:07.458 - 00:14:41.346, Speaker A: And in the other case, you're just adding tangent vectors just like we did before. Okay? And the only thing that remains is to make this thing into a mount to catch up, so to speak, so that we know just the same for manifolds that we knew for rn. The only thing that remains is to make this into an honest Smooth manifold instead of some weird disjoint union, often two n dimensional manifold with a two n plus one dimensional manifold. So it's all a little funny and it's just an exercise to see that you can define the smooth manifold structure in the following way. Those unique smooth manifold structure on the algebraical set, theoretical thing, we just defined t of m and we could just nail down what it is in some pedestrian way. First of all, maybe the easiest thing to say is that this fellow here, this part of the tangent groupoid is an Open set. Of course, if you're defining a manifold, first of all, you have to define a topological space.
00:14:41.346 - 00:15:32.978, Speaker A: And so this is going to be a big open set. It's an open set according to the assertion of the theorem in a manifold. So it's a manifold in its own right. And what this is as a manifold is just the product of m times m times r. In other words, it's just the U has its standard structure with the standard product manifold. So this fellow, of course, has to be closed set. That's the law.
00:15:32.978 - 00:17:34.318, Speaker A: And it's also a sub manifold it's part of the theorem, just like it is in the standard Rn case with its usual structure as a manifold. But we're not quite done yet, because it could still just be the disjoint union of these things. And as long as you don't insist your manifolds have constant dimension, that would still be a manifold. So the third condition says that locally, what happens is exactly what happens over here. So if u is an open set, if f mapping u to w is a diffumorphism squeezing that n, and what's w? It's an open subset of rn to with I'm not sure an open subset, then the thing that we've just defined is going to be diffimopsum. That's part of the theory, yes. Well, I guess, yeah, but then tu can define it, correct? This thing here, tu is not an action group, it's an open subset of an action group.
00:17:34.318 - 00:19:00.594, Speaker A: Yeah, yes, apply to, yeah, so it all depends on how sophisticated you are, and I think you're too sophisticated for what I've written down here. What I mean by this is the following thing. The derivative at the point n of f in the direction a is d dt t is equal to zero of the following vector value function of t f of m plus t a. So that, let's just agree for this definition, that is the, for this discussion on this board, that that is the definition of the derivative. Of course, if I'm now going to go to this fancy world of manifolds, then we have to have a discussion, and we would have to have a discussion if we weren't all such sophisticated people, about what a tangent vector is at a point in a manifold. And a tangent vector at a point in a manifold is one of a number of things. As you know.
00:19:00.594 - 00:19:59.344, Speaker A: You can define it in terms of curves, you can define it in terms of derivations, however you define it. Of course, you have to adapt this definition appropriately, and you have to check that this definition is consistent with however you personally enjoy to define the tangent space of a manifold at a point. Anyway, let's just finish this then. Big fucking find from t u to tw. What do I mean by big t of u? Well, I mean that part of big t of m where which is mapped by the source and range maps into u times r instead of just m times r. So it's the analog of what's written down here. Maybe I should write that down in a moment, is also a diffumoffsen.
00:19:59.344 - 00:21:40.414, Speaker A: Oh, maybe I should say add one thing here just to insist that this thing is an open set so we can talk about diffumorphisms and just a little bit of discussion. Yeah, so as I said, t u is going to be the union of tangent bundle of u cross zero with u times u times r cross like that. And that is exactly the same thing as all of those points. Maybe we'll now call them gamma in tm. So gamma means either, where is it up here, either a tangent vector comma zero, or ammm one m two m one t. Whatever it is, you can apply the source to it, or you can apply the range to it. And the requirement here is you end up in u.
00:21:40.414 - 00:23:00.194, Speaker A: And now, of course, big f applied to a tangent vector at some point m. Maybe I'll call it u comma zero, is supposed to be the derivative at the point u of f evaluated on x u comma zero. And so indeed, on the one side, because we're talking about abstract manifolds, we have to think about the derivative du of f abstractly, not concretely like I did here. And on the other side, after we've applied du to f and evaluated at xu, we have to regard the result as a concrete vector, because that's what is a constituent in t comma t of w. Did I call it w everywhere? Yes. So, okay, you're going to make me do that. Yeah.
00:23:00.194 - 00:24:32.684, Speaker A: Can you say again please? Yes. So x u, where? Put it this way, for every tangent vector in the tangent space at u of manifold m or manifold here. Okay, so this is supposed to be some function or linear functional which sends functions on m or near on u to find near m for the real numbers, and it satisfies Leibniz's rule like this. This is one way of defining an abstract tangent of activity, which is, if you've seen this for the very first time, not especially helpful, because, wow. Bam, there it is. But any such fellow, any such xu, comes in the following way. For each x u, there is some curve to find on a little neighborhood around the diagonal.
00:24:32.684 - 00:25:41.992, Speaker A: Given x u, there is a curve such that the formula for Xu is xu of f is d dt. Now, it looks a bit like what we had before f of c of t, like that. So you can think of every tangent vector more or less as we thought of a tangent vector here. The bad news is that different curves can give rise to the same tangent vector. So although this is very good way of concretely thinking about tangent vectors for calculations, it's not so good to define the tangent space in terms of equivalence classes of curves, because then you have to mess around and ask, what is the equivalence relation? Okay, but that's what we're talking about here. Maybe I'll just add somewhere at the top here, because I didn't put it in. Each time you have a tangent vector, there exists some c, but it's not unique, such as.
00:25:41.992 - 00:26:25.168, Speaker A: This is true. Yes. Great. Oh, speaking of advertisements, I was bragging about the YouTube videos to this class on Thursday night at dinner, and I wanted to show people. I was bragging to the number of YouTube views which has exceeded 300 for lecture one. And so I whipped out my phone and I started to, you know, find the videos. These damn things have ads in the front.
00:26:25.168 - 00:26:50.028, Speaker A: The first thing you find is that this lecture class is brought to you by chips Ahoy. Cookies. Do I get a cut of that? Do I get a single damn cookie out of this? No. Chips ahoy. What's going. What's the deal? I don't know. Thousand views or something.
00:26:50.028 - 00:27:07.548, Speaker A: Wow. Yeah. Somebody told me that 300 is like the threshold. After 300, you, you know, you start to collect royalties. Maybe, you know, one cookie to begin with, but, you know, it's still. Yeah. So I don't know.
00:27:07.548 - 00:27:22.840, Speaker A: Someone's. Someone's making out big time from these lectures. Good. So that's just a little. We'll come, of course, back to manifolds. I have some ambitions. Maybe we'll verge.
00:27:22.840 - 00:27:53.734, Speaker A: We'll wander off in the direction of the tier singer index theorem a little bit, for example, we'll do some actual topology, but that's not the immediate task at hand. So this is just to look ahead, and it's just to set the tangent groupoid into its final resting context. There will be a tangent groupoid for every smooth manifold for your thing. Yeah. Wow. Impressive. That's a lot of cookies.
00:27:53.734 - 00:28:35.228, Speaker A: Yeah. Good. Now we'll run out of space. Maybe we'll continue here. Let's go back to operators. We did a little calculation last time which put differential operators into contact with the tangent groupoid. And I think I certainly made a typo in the notes, which is now corrected, but the correction is not published yet.
00:28:35.228 - 00:29:49.184, Speaker A: But I may also have made a misstatement in the lectures, just deliberately to drive up views and get more ad revenue. So let's just deal with that and take it, you know, learn from my mistake. So here's what we did. We looked at a differential operator like this, maybe of order r on rn, some manifold of the type we've discussing over there in the left. It's basically just rn what we did is we managed to define a family of operators on the source fibers of the tangent group mode by the following procedure. First of all, we did something that's very familiar to us by now, and that is that we just rescaled in a sort of easy way, t d by t. Just by multiplying by t to the r.
00:29:49.184 - 00:30:54.144, Speaker A: It's not a very big thing. And then we built a whole bunch of operators called DMT, one for almost every source fiber. Maybe I'll do the whole formula for all source fibers by the following things. So here is the smooth functions on a source fiber on which our operator DMT is supposed to act. And the way this is defined is we just make the natural identification of this fellow with M. We'll put our DT here and then define DMT to be what it takes to make this diagram commute. And what are these natural identifications? Well, if you take a triple.
00:30:54.144 - 00:31:13.338, Speaker A: So first of all, there are two different prescriptions. So let's consider first of all the case where t is not zero. If you take a triple quadruple where t is not zero. M 02:00 a.m.. One, t and t is not zero. Like we said over here, the a is actually determined by m two and m one. So you can forget about it.
00:31:13.338 - 00:31:46.802, Speaker A: And what the natural identification of mt with m is, you just project onto m two. That's the natural identification. We call that thing phi mt. And phi mt is the map of composition with phi mt. This doesn't work when t is equal to zero, because the map phi Mt is not a diffiemophism anymore. It's just a constant map. So you have to do something else, which is also what we also discussed.
00:31:46.802 - 00:32:40.178, Speaker A: So if you happen to be at t equals zero as we are now, then the right thing to identify, or more or less the right thing to identify with, is just a like this. Instead of projecting onto the m two factor, we project onto the a factor. And that projection we called psi m zero. And this is the composition with psi m zero. This is what you do when t is equal to zero. And so up here we're defining the operator d zero t like that. And what do I put down here? What I put is our friend that you get by dropping lower order terms in your differential operator and evaluating the coefficients at n.
00:32:40.178 - 00:33:09.474, Speaker A: So this is a constant coefficient differential operator. Oh, thank you very much. Not sure where that came from. Thank you. So now all of the DMTs have been defined. And what we showed is that these guys things constituted a smooth family. Of operators.
00:33:09.474 - 00:34:51.843, Speaker A: So all of these fellows altogether act on all of the source fibers that there are, which is this many. We showed that these operators vary smoothly with t and m and also they vary equivariantly. That's something that makes sense given the groupoid structure. This is a smooth equivariant family of operators. Linear operators on the spaces, on the, on the smooth functions on compactly supported functions on the sauce phase. And the way we did it is we chose a systematic and uniform way of looking at all of the source fibers at once. Not this Frankenstein way where some source fibers are identified with a and some source fibers are identified with m.
00:34:51.843 - 00:36:24.554, Speaker A: There's a way of identifying all source fibers all at once with just a, just by projecting onto the a factor that's diffumorphism. And now you can calculate what happens to DMT. Of course it's an operator up here. But in the same way that these pictures suggest using thetas instead of phis you can identify the DMTs with operators just on functions on this a, which of course is just rn and what the operator is. And so now this may or may not be a small correction from what I said last time. So it's a alpha of m plus t a the alpha dx. So the alpha, oops, I missed off the.
00:36:24.554 - 00:37:09.108, Speaker A: Let me try and put that somewhere. The power of t. Good. Yeah. So what I wrote last time, at least in the notes and maybe on the board, is not quite this where you see a here I wrote a minus m possibly, and that's wrong. When you actually do the calculation, you get this fellow doesn't change the fact that this is a smooth equivariant family. It's just different formula.
00:37:09.108 - 00:38:13.372, Speaker A: And yeah, that's what it is. But we could do something a little different, which is what I probably should have done, probably will do in a revision of the notes. If we defined theta in a different way, let's make a big display of it. So I said that the standard projection formula where you project onto m two is not a diffumorphism when m when t is equal to zero. And that is absolutely true. Nonetheless there is a way of identifying all of these source fibers with m just by doing what I'm about to write down. So here's an m 02:00 a.m.
00:38:13.372 - 00:39:12.260, Speaker A: One or m, I guess t. Let's just pick off m plus a. So basically it's exactly the identification we looked at above. Except I've now translated by m. Very modest change, but this has the effect of slightly improving this formula. Well, not because you have m's instead of a's, but because you have the operators that we've become accustomed to thinking about. These ones exactly the same thing as before, except now it's our friend am plus t.
00:39:12.260 - 00:40:06.104, Speaker A: X minus MdL. I should call this the variable here, x. This has this extra translation built in, not surprisingly, because we translate it and those are the operators that we saw before. We previously had a collection of operators that we called DMT and we got them through our rescaling operators like this. There we go. It's coordinate. X is the coordinate function.
00:40:06.104 - 00:41:06.764, Speaker A: The n coordinate functions from rn to a, from a to r. Excuse me, I could have, maybe I should have called it a, I guess because there were x's here. I felt there should be an x here as well. Okay, so this is a minor adjustment which doesn't alter the. Except for the fact that I made a typo last time, the verity of anything I said before. But it's better because it directly links with the actual operators that we were talking about last time, not last time, but forever. And now we're slightly better positioned to think about rescaling because we have at least these particular operators in view that we didn't have before.
00:41:06.764 - 00:41:57.194, Speaker A: So we could build some rescaling operators. We could call them rm lambda, if you like. And they act on these tangents, on these source fibers. And in a moment we'll glue them together and they'll just be one r lambda. But just to be consistent with what's over there. So what's something in the source fiber? It's an m 02:00 a.m.. One t.
00:41:57.194 - 00:42:35.830, Speaker A: Like that. And what do I want to put on the other side of the equals? Well, I want to put something on the other side of the equals, which sort of corresponds to our traditional mts. And a good thing to put is the following. And now we can. Maybe I should call these little r's because these are maps on spaces rather than maps on functions. For the moment. Well, I have to write down something which is in the tangent groupoid and I want to do some rescaling.
00:42:35.830 - 00:43:50.874, Speaker A: So I'll stick in a lambda there. But if, if I put a lambda here, the condition for the, for the eventual quadruple to be in the tangent group, or it forces me to put lambda inverse t over here. So really where we're ending up is lambda inverse t. Not because we're not going to keep this. You'd like me to call it alf yeah, that's just like, no, I'm not going to use this notation for long. So that means you'll only be confused for a short time. Of course you these are just part of one map which is defined on all of t of m all at once by the same formula, which could be defined by the same formula.
00:43:50.874 - 00:45:14.294, Speaker A: So this thing is evidently because, got to say it, but we're imagining, like we have been imagining, that lambda is positive. This thing is evidently a diffeomorphism, and it's also a functor. This is an automorphism of the category tm, so it pays attention to the smooth structure, and for that matter, the category structure tm. Of course, we here, we're discussing the case where m is rn, but you could do the same thing for any m and the same, an analogous thing for any m, and the same basic facts will be true thanks to the following little calculation. It's just an exercise. This thing doesn't monkey around with m one and m two. So not only does it map t of m to t of m, it also maps t of u into t of u.
00:45:14.294 - 00:46:51.214, Speaker A: And so you can ask the following question, how is this compatible with diffumorphisms? And the answer is perfectly you can scale and apply big fan in one order, or you can scale and apply, I guess these are compositions, big f in the other order when you get exactly the same thing. So if we wanted to, we could go back to that theorem where we glued together all of the tier views to get a t of a manifold. And that would give us perfectly respectable rescaling functor and diffumorph some r lambda on any t of m. If you're itching for something more intrinsic, not built out of coordinate patches and so on, then just patience will do that on Thursday. But for now we're just mucking around with coordinate patches. All right, good. So now let's define big, which is what I wanted to do at the beginning.
00:46:51.214 - 00:47:59.194, Speaker A: Big rescaling operators, linear operators on functions, just by composition, I guess. Yeah, call this rm lambda. I forgot to put t in the notation, so now I'll even drop the m and just call this r lambda. What this would really be a map between is just following what's written up. There is a map from big m sub m lambda inverse t, to functions on big m, sub m comma lambda times lambda inverse of t, or just t. So it I've switched things around. I think I said that rather poorly.
00:47:59.194 - 00:49:07.754, Speaker A: I switched things around by using for t down here. What should have been lambda inverse of t up there, and then the old t would be lambda times the new t. So it all works out. So these are our rescaling operators and they're compatible with our old rm lambda or omega lambda. We were calling them via these maps, theta. The old rescaling operators acted over here. And the theta maps compare functions on M, which is where the old rescaling maps acted.
00:49:07.754 - 00:50:44.544, Speaker A: With theta maps, those functions, functions on mt mmt, which is where the new rescaling maps operated, and the obvious diagram commutes. All right, if we don't use theta to make an identification with c c infinity of m, we don't use theta to identify this source fiber with m. If instead we use phi, that's the projection onto the first factor, the m two factor, then what the rescaling maps look like is something completely different. They look like the identity. For now, I'm just trying to, let's just let for the moment to be non zero. So here's the map that we've just been discussing. If we make the sort of boring identification, or maybe it's the interesting identification, I'm not sure.
00:50:44.544 - 00:51:48.824, Speaker A: Using projection, that's just m onto the first factor in the definition of the tangent group, or it's just projection onto little m two. We were calling that thing in this situation Phi m lambda t star. Then what you need to put here is just the identity. It's kind of nice. So from a certain point of view, the rescaling operators don't do anything violent at all. Okay, so this formula that I'm now trying to write down successfully, now this formula is easy. This is a formula that we've seen before.
00:51:48.824 - 00:52:35.674, Speaker A: And of course it's easy because what DT looks like downstairs, just as an operator on M is, it's just you multiply t to the minus r, excuse me, t to the plus r times dt. And what the rescaling operator is, is, well, nothing at all. And so the formula is sort of evident. I mean, the reason you have a lambda to the minus r here is that dt is not just d, dt is t to the r times d. But the dependence is all in on t is all in that initial fact. So this is a sort of trivial scaling relation from the point of view of this new gauge that we've created for ourselves. And of course this extends to all operators by smoothness, by smooth annuity.
00:52:35.674 - 00:54:10.964, Speaker A: And so we have this type of relation not just for t, not zero, but when we define those operators d d sub m zero will still have this relation which is evident from the actual explicit formula that we had for those guys. All right, so I mean who can resist? Now doing the following thing. So from this fact here, we just did a little experiment with these to inspire us. But from this fact here we have a completely, not completely new, a new, a way of looking at what a scaling family is in that, in the sense that we were talking about until just a week ago, a scaling family of operators. And now we reach this point where there's a terrible conflict of notation which I shall resolve in the notes in a different way because I kind of like the letter a for operators. I'm just going to insist on that in the notes. But because we have a's already somewhere up there, let me just call the operators p or something instead.
00:54:10.964 - 00:55:08.782, Speaker A: So what's written on this line, except for the fact that I've called it p instead of a, is something that we've been discussing about for weeks, namely scaling families. Scaling families of operators of some degree, which we can't call m anymore either. So we're calling it r. Just need a bigger Alphabet. I don't know why the French didn't, you know, when the French got rid of the king, they also got rid of all of these crazy imperial units. But you kept the same Alphabet with an awkward 26 letters. You should have expanded it to a more humane hundred letters at the same time.
00:55:08.782 - 00:55:58.578, Speaker A: You know, why not then, okay, but they didn't, the French didn't do it. They missed their chance. And now, now we're all paying, we're all paying the price for that. This is exactly the same thing as a. As a what? Well, these are supposed to be smooth families of operators built in to the definition of scaling ness. Definition of what a scaling family is, is some kind of extendability. So rather than put that, putting that in later, let's just do it all at once.
00:55:58.578 - 00:57:17.450, Speaker A: Now I'm going to talk about the extended family right now because as soon as you have the pts for t not zero, you have all of these operators, PMT, t equal to zero. So why not just put them in to begin with? It's the same thing as a smooth equivariant in the sense we've been talking about family of operators, maybe I should say just to emphasize we want all of our operators to be properly supported and also continuous in this distribution. Usual cc infinity topology probably supported continuous operators. So it looks like I'm defining a whole bunch of operators now pMt. Now this parameter p is in here right from the very beginning but remember, equivariance means that as long as t is not zero, all of these pmts for a given fixed t are the same. So there's really only one p for each non zero t. And when you get to t is equal to zero, there's one p for each little m.
00:57:17.450 - 00:58:57.644, Speaker A: And what equivariance means is that it's translation invariant. Anyway, you get a bunch of operators like this on the source fibers, or if you like, a single operator with some properties on the functions on all of the tangent groupoid. And now I'm not distinguishing between t equals zero and t not zero. This is all possible t's and all possible m's and r's such that, well, our lambda p m t, our lambda inverse is equal to lambda to the minus r. So I've had the degree here, pm lambda t plus k m lambda t, and ran out of room. So here we're talking about a smooth family of smoothing operators, just like before, basically adjoining operators continuously. Are there examples of non inventory? You mean in nature, real life? Yeah, like I'm just kind of curious.
00:58:57.644 - 00:59:24.024, Speaker A: Yeah, I'm sort of wrestling. I mean, all of the operators that happen to emerge from this theory, the PMTs are, as we've seen, automat. I mean they are a joint. But that's a theorem. It's tempting to say, well, let's just put a joint ability in from the beginning, and then that theorem will go away. Well, it doesn't go away. You have to, at a certain point you have to check that some construction really does produce an adjoinable operator.
00:59:24.024 - 00:59:53.236, Speaker A: I would say that, yeah. I mean, after worrying about joint ability for the last six months, I would tell you, don't worry about joint ability. Yeah, nothing, no good thing will come of it. That's my experience. So I feel like I finally conquered a joint ability with the argument I showed you a couple of times ago. But I don't, some things just aren't worth thinking about. And I think this issue of adjointability might be one of them.
00:59:53.236 - 01:00:42.646, Speaker A: I mean, you have to prove a theorem, but then you should forget about it. I mean, where some of us are were sister algebraics, and so we're interested in adjoints for that reason. It's not like, and having an adjoint is a type of continuity properties, as you alluded to. And so it's, you know, it's not, it's not nothing. It's worth, it is worth thinking about a little bit, but you can sort of fall down a rabbit hole, I think, as I did with this issue, and I'm counseling you not to do that. All right, so we could just define a joint ability in this way using the rescaling operators we've just defined, which come from this multiplication by lambda operation that we, we have still at the top of the board there. Okay.
01:00:42.646 - 01:01:33.356, Speaker A: And this thing being diffeomorphism invariant, gives rise to what I suppose you could call a canonical diffiemorphism invariant. Action of the group r plus r plus the multiplicative group r plus on the tangent group point t of m. No matter what m is, any kind of smooth manifold will do, doesn't depend on the coordinates, even though apparently you're doing scalar multiplication in some of these pictures, you're doing scalar multiplication. And so we have a notion of smoothing, excuse me, scalable operator, therefore scaling family. Therefore we have an opera notion of scale, Abel operator. And all of the theorems we've proved up to this point now work on any manifold, one or two points. As we saw, it's convenient, or at least convenient, depending on which point you're talking about.
01:01:33.356 - 01:02:26.104, Speaker A: It may be essential to assume that the operators are compactly, but with that, all of the theorems carry through just exactly as before. Excuse me, I had a question. Yeah. So when we are defining, so when we define a manifold, there will be open sets and other stuff. So about the operators that we have defined, I mean, like, we can have the sheep of metamorphic functions, do these operators also form a sheep in any way? Or something like that happens. So first of all, we're in the world of c infinity in this class, chief of smooth functions, do they form sheaf? No, you can't restrict a pseudo differentiable or a scalable operator to an open set. It will sort of spill out of that open set, but not by much.
01:02:26.104 - 01:03:14.354, Speaker A: The simple answer to your question is no. Scalable operators don't form a sheaf, but they, and the almost ness is kind of important, right? If you have, for example, inside of scalable operators, there are smoothing operators. Smoothing operators on a manifold don't form a sheep, or on rn don't form a sheep. You can't restrict a smoothing operator to an open subset, because when you apply the smoothing operator to a function in the open subset, of course it could become something not supported in that open subject. Okay, thank you. Okay, good. Properly supported helps, but it doesn't quite.
01:03:14.354 - 01:03:59.054, Speaker A: What it says is that each, you know, if you have a certain set u and you're looking smooth, compactly supported functions on you and let's say you had a compact closure. Then you can find another compactly, another open set with compact closure v such that a maps cc infinity of u into cc infinity of v. You can do that kind of thing, but U is not necessarily equal to V. And if you insist that U is v, then by a well known theorem, in fact, you are talking about differential operators. So it's a bit of a puzzle here, but it's because it's only a little bit of a puzzle. It's still interesting to think about to what extent these really do form a sheath. They form a sheaf modular smoothing operators.
01:03:59.054 - 01:04:40.192, Speaker A: The set of the algebra of pseudo differential, as they say complete symbols, is a sheath can be made into a sheath. Good. I wanted sort of not sure where to go from here. Let me think about what I wanted to say. My notes today are extremely truncated, thanks to thanksgiving. Any further questions before we go on? So we did two things so far today. We talked about how to talk about the tangent group point of a manifold.
01:04:40.192 - 01:05:22.058, Speaker A: And then we said, basically back to RN and we looked again at operators on rn and we made a small correction to what was said on Thursday. And after having made that correction, we saw that this idea of scaling that appears up here is perfectly compatible with the old type of scaling. And so we can therefore transport our notion of scale label operators scaling families to scalable operators right over. And so now, just in an instant, the whole theory extends in a coordinate free way to manifolds. It's kind of nice. Just like that. Okay, now let us go back to.
01:05:22.058 - 01:06:12.124, Speaker A: Let's go. Having just said manifolds, let's dwell in manifolds for a little while. Oh no, there is something I wanted to say. So let me, um, let me say that something was nagging somewhere in the back of my mind, and it's come to the fore. So we defined at the beginning of this semester what is a scaling family of operators. And we just carried over that definition to the world of the tangent groupoid, where it's arguably a little tidier. There are no rescaling operators, really anymore, except these rather trivial r of lambdas.
01:06:12.124 - 01:07:09.284, Speaker A: And. Yeah, and then you have the whole theory on anything, any manifold. It's diffeomorphism, invariant, et cetera, et cetera. But I want to slightly adjust this definition since we're making a, a new start and we're defining scalable operators again, let's do it right this time, rather than wrong as we did it before. I'm not saying we have to correct anything that we did before. But we should choose a different definition, which is just a little easier to work with. Z for zoom maybe I'm defining now a vector field.
01:07:09.284 - 01:08:06.484, Speaker A: All vector fields are given by differentiation, and this one is no different. Here's this scalar multiplication that we were talking about today. So instead of writing lambda, I'll write e to the u, in which case maybe I should call this u as well, and this one u. So that's now if I take a fixed element in the tangent group or gamma, one of those quadruples, if we're talking about rn, and if I multiply by e to the u, of course I get another point in the tangent group order, which varies smoothly with u. How do I know it varies smoothly? Well, you check in local coordinates in some u, in some big open set big u, and there it was certainly smooth because we have the formula for it somewhere. It's still on the board, so you can differentiate. Now this thing is a vector field.
01:08:06.484 - 01:09:46.678, Speaker A: So what's going on here is that f is a smooth function on t of m and gamma is just a point in t of M. This defines a vector field z on the manifold t of m, and in particular it's an operator on functions on t of m. And what should I like to say? I should like to reformulate this definition over here as follows. So this obviously from the definition, this is the vector field whose associated one parameter group of diplomorphisms is multiplication by e to the u in the sense that we described. If you have a vector field, you may or may not be able to integrate it over all time to get a flow. But if you can, that flow is unique, and in this case you obviously can. And so this is that flow.
01:09:46.678 - 01:11:25.864, Speaker A: And so this is the generator of the rescaling motions that we were talking about before, sometimes called the zoom action, hence z. Now let's do the following thing, which we did a little bit, I think, last time, but now let's be a bit more serious about it. Here's a family of operators piece of table or maybe I should be pointing at this line, here's a family of operators on all of the source fibers of the tangent groupoid m, sub MT piece of MTx on the m teeth source fiber. If I have a family of operators for what I'm about to write down, equivariance is of no issue and neither really is properly supported or continuous. But let's throw those in anyway because we want to always be thinking about that. I have a collection of operators like this. So the MT operator acts on the source fiber associated to mt.
01:11:25.864 - 01:12:42.940, Speaker A: Then rather than writing down all of these subscripts and keeping track of a whole family of operators like this all at once, let's just glue them together like we know we can and make a single operator big p. And the way big p works is like this. When you want to act on a function, you look at the point at which you want to evaluate the acted function and you ask what is the source of this point? And whatever the source is, there is a corresponding operator, because we have a whole bunch of operators there. So let's just take f and restrict it to the source fiber m, sub mt. And now evaluate p sub s gamma on it like this. What should maybe be consistent with, with my notation, right, source gamma there. And after I've done that evaluation, of course, I'll get another function on the source fiber.
01:12:42.940 - 01:13:45.194, Speaker A: And in that source fiber is gamma because the source of gamma is s of gamma. So I can evaluate this thing on gamma like that. And I've glued together all of the operators into a single operator big p. Maybe it's not evident to people with bad eyesight like me that this is a big p. So this big p maps smooth, compact, supported, or arbitrary smooth functions, whichever you like, on the tangent report itself. Okay? And now we can express, we can also go backwards. When is a big p? When does a big p come from a family of p, sub mts? Well, it has, depending on how many of these adjectives you put in, you have to worry about equivariance.
01:13:45.194 - 01:14:18.174, Speaker A: What equivariance means, let's not do that right now. Properly supported. Well, that just means properly supported. Still continuous. That just means continuous. Still, what makes an operator a family of operators is that it actually commutes with multiplication by functions which are pulled back from m times r via the source map. So if you have a function h of gamma and it only depends on the source of gamma, then that function has the property that p of hf is h of pf.
01:14:18.174 - 01:14:49.460, Speaker A: That's what it means to be. Because in this formula, when you restrict to a source fiber, h just becomes a constant. And by definition of linearity, you can move out the constant. Okay, so the, the operators big p, which are families of operators, are easy to nail down. They're just the operators which are linear over c infinity functions on m times r. So you think of c infinity functions on tm is a module over c infinity functions on m times r by the source map. And we're just talking about those linear guys.
01:14:49.460 - 01:16:13.916, Speaker A: So there's a nice way of thinking about what a family is, and it's not that hard to think about what an equivariant family is, but that's beside the point at the moment. And now let's just rephrase our definition of scaling family in this language. It if I happen to have an operator on functions on the tangent groupoid, which is a family of operators, then I'll say it's a scaling family. If. Well, let me just write down the infinitesimal version of this condition here. Namely, I'll take this condition and then I'll differentiate with respect to lambda. Set lambda is equal to one.
01:16:13.916 - 01:16:48.584, Speaker A: Or if you like, I'll set lambda to be e to the u and then differentiate with respect to u and set u equal to zero. And then what I'll get is z coming out. You know what? Well, if I called that big z, it would look like the integers. Maybe I can't do that. I'll stick with this type of z. Think about maybe I'll choose a different letter so I can write big whatever it is here. So the notation subject to revision.
01:16:48.584 - 01:17:50.434, Speaker A: I think we've reached the point where the, we've seen I've made all possible mistakes regarding notation that I'm likely to make so I can go back and fix them all systematically. This that I've written down is the derivative of the left hand side of this key scaling relation with respect to lambda evaluated at lambda is equal to one, or the derivative with respect to lambda equals e to the u evaluated at u equal to zero. And so now what happens when you differentiate the other side with respect to lambda? Well, the only lambda you have to worry about is this one here, because this doesn't appear when you talk about the families. This is just minus r times p plus k. Well, yeah, don't like that k. It's what it looks like. Good k.
01:17:50.434 - 01:18:53.706, Speaker A: Where this thing, big k is a smooth family. Smoothing operators, kind of nice. It looks almost algebraic. Oh, why is this equivalent? Why is this infinitesimal version equivalent to the global version that we have on the other board? Well, going in one direction is not very difficult. If you want to integrate, that's not terribly difficult. If you have this condition, then you can easily obtain this condition by integration going the other way. Differentiation, I mean, differentiation is supposed to be easier than integration.
01:18:53.706 - 01:19:41.974, Speaker A: But the question arises, as we saw in this setup, whether indeed this smoothing family is differentiable with respect to lambda. So it is differentiable with respect to lambda. That's a theorem of Bob and Eric. But it's not obviously differentiable with respect to lambda. That's a slightly worrisome adventure into the land of the back category theorem, as we discussed. So let's obviate the need for back category by building differentiability into the definition, by talking not about this operator here as a function of lambda, but rather the derivative of this with respect to lambda. If you differentiate this with respect to lambda and then set lambda equal to zero, that's what that k is up there.
01:19:41.974 - 01:20:18.860, Speaker A: So we're taking advantage here, or we're building into the definition, the one that's above my head. The fact that in the old definition, that the big k, m lambda, t is actually differentiable with respect to lambda. That is to say, you have a smooth family of smoothing operators, not just as m and t vary, but also as lambda varies within the positive real numbers. When lambda goes to zero, that's. I mean, zero is not a positive real number. We're not saying there's any smoothness that lambda is equal to zero. But for lambda inside the set of positive lambdas, this should be a smooth function of lambda.
01:20:18.860 - 01:20:39.090, Speaker A: It is a smooth function of lambda. By Bob and Eric's theorem, and taking derivatives, we end up with this equivalent definition. And it looks okay. It looks kind of suggestive. It's suggestively simple. Great. That was one extra thing I wanted to say.
01:20:39.090 - 01:21:14.954, Speaker A: We have ten more minutes, so let me just make a little start. It's a light lecture today, but let's take advantage of the rare opportunity to look ahead. To look ahead. I mean, some of this just notation, right? We didn't actually do anything different. The only thing that's a little bit substantial here is that we made the strategic decision to make k differentiable with respect to lambda. So we can write the definition in this way. That's the only thing substantial that happens so far.
01:21:14.954 - 01:22:41.026, Speaker A: But if you have a simple formula like this, sometimes it suggests things, and hopefully we'll come back to that, will take this suggestive formula, which is at the top, and make some use of its suggestiveness. That's a word. Okay, what I want to do for a little bit today, not long, because we just have a few minutes and more thoroughly on Thursdays, think about this tangent group point in more than one other way. So we made the assertion, I made the assertion earlier on, that t of m you can make t of m no matter what m is. Any smooth manifold gives rise to a t of m. And I said it's a smooth manifold. So if it's a smooth manifold, it has smooth functions on it.
01:22:41.026 - 01:24:02.530, Speaker A: Apart from being a topological space, I have some exercises which describe the topology of this thing in a dead easy way. But apart from being a topological space, so what makes a smooth manifold? A smooth manifold is there's a certain sheaf of smooth functions with some properties. And so you might ask, what are these smooth functions? And it's kind of interesting. Well, maybe we can put it this way. I forgot to say this, but these maps, which are supposed to be of key importance, the source and the range maps, which map t of m to m times r, certainly in the case of rn, and then by local coordinate calculations for any m and ner, these are smooth maps. In fact, they're submersive smooth maps. And so the law says that any composition like this has to be a smooth function.
01:24:02.530 - 01:24:35.654, Speaker A: If you compose a smooth map with a smooth map, it's a smooth map. So these are among the smooth functions, okay? But they are not all of the smooth functions. Because remember, when t is equal to zero, the source and range maps agree. You have an element of the source fiber over t equals zero. It's tangent vectors pointing up like this. And the source and the range maps both take that tangent vector down to its root, little m. And so all of these functions are constant on all of the tangent space fibers.
01:24:35.654 - 01:25:28.364, Speaker A: The fibers at t is equal to zero of the tangent groupoid. So we didn't build all of the fibers of the tangent, excuse me, we didn't build all of the smooth functions on t of m yet. But there's a lovely construction which you can build is worth thinking about, which is a little different. Suppose you have a function on m which is smooth. Well, you could get one of these guys just by composing, but you could also do something else which is a little bit different. Let's call it, I don't know, just whittle or something. Or maybe delta is good.
01:25:28.364 - 01:26:07.024, Speaker A: No, I don't like that. I just call it twiddle. Let's not invest in this notation. So this will be a function of gamma, which is an element of tangent groupoid. And let's do the following thing. In fact, let's just do it first when t is equal to zero. Logical to do it this way, let's look at this difference, which I'm trying to write down correctly.
01:26:07.024 - 01:27:17.244, Speaker A: So that is a smooth function of gamma by what's written above. And it's not a terribly interesting smooth function of gamma yet, because when t is equal to zero, it doesn't do anything better than this guy here, when t is equal to zero, this function is identically zero. But since this function is identically zero, when t is equal to zero, you can divide it by t. If you have a smooth function on a manifold like this, which is equipped with a submission to r like this one is, and if this function vanishes identically when t is equal to zero, then by Taylor's theorem, it's t times some other smooth function. Another way of saying that is you can take that function and divide by t, and the result is still a smooth function everywhere, including where t is equal to zero. So let's call that the Taylor Taylor's extension. And by Taylor's theorem, this is smooth on t of m.
01:27:17.244 - 01:28:27.234, Speaker A: And that's an interesting function. It's sort of strange. You take a function and you write down all of these sort of difference quotients attached to the function, and it becomes a smooth function on all of the manifold. Let's just maybe remember, just do a little scratch, one little scratch calculation, and then we'll stop. If we're in the rn case and you take the, you want to know what is the range of an m 02:00 a.m.. One? T? The answer is that it's m two. But the answer also is that this is equal to m one plus s of t times a.
01:28:27.234 - 01:29:29.374, Speaker A: So this thing, which is no longer above, I guess, is f tilde of m 02:00 a.m.. One tilde is just f of m one plus ta minus f of m one divided by t. That makes it look even more like a difference quotient, which is what I was trying to write down. And the fact that we'll, well, a little bit more on next time, is that these fellows want you to find this way. And the ones you define this way. That's it pretty much. These are essentially, that is just, yeah, maybe I'll put it this way.
01:29:29.374 - 01:30:23.526, Speaker A: They generate the smooth functions. So there's one exotic construction, which is this funny difference quotient construction, and then that's it. I mean, if you have two smooth functions on a manifold and you add them together, it's supposed to be a smooth function. If you have two smooth functions on a manifold, f one and f two, and if you have a third function, h, and you form, which is a function of two variables, and you form h of f one comma f two, that's also by law still a smooth function. So once you've thrown certain numbers of smooth functions into the club, you're obliged to let you know how it is you're obliged to let in everyone else. And so all of those other guys are in the space of smooth functions generated by the ones that I've discussed here. And that's it.
01:30:23.526 - 01:30:53.480, Speaker A: You don't get any more. So every function is some function of n variables, and you put into the end slots of that function of n variables, n functions which either look like case one or case two. And now we've. Well, I shouldn't have said n d variables. And now for various D's, you get as d varies. You get all smooth functions in that way, and you can figure out what d the biggest d has to be. We shall take this a little bit further next time.
01:30:53.480 - 01:31:05.754, Speaker A: And using in the spirit of algebraic geometry, we'll build the tangent groupoid from the ring of smooth functions and. Yeah, we'll see where that takes us. Thank you.
