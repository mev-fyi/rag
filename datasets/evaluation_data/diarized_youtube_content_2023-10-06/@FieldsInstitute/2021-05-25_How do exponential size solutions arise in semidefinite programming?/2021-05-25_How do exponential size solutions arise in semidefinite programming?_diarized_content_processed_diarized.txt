00:00:01.120 - 00:00:59.312, Speaker A: Okay, so the talk is about how do exponential size solutions come up in semi definite programming? This is joint work with Alex Duzov, who is my PhD student and very excited about this work. So I hope you are going to like it. So let's start with linear programming feasibility as a warm up. So the question I'm asking is, given this linear system of inequalities, is there an x which is a feasible solution? And here the a is a matrix, b is a vector, and they are given by integer entries. And I only want to know whether there exists a solution to this linear system of inequalities. And we know that if there is a solution, then there is one which has polynomial size. So if this problem has a feasible solution, then there is a feasible rational x bar in which the entries have numerator and denominator that are not too large.
00:00:59.312 - 00:01:53.304, Speaker A: In particular, the size is at most n times log n times log l, where n is the number of variables and l is the largest entry in a and b. And how do I get to this result? I'm going to use Kramer's rule at an extreme point of this linear program or linear inequality system. So the corollary of this suppose I just want to know whether there exists a feasible solution. Then a way to do that is I'm going to compute one. And that's reasonable because I can describe that solution in polynomial space. Let's go to SDP semi definite programming feasibility. In that case, I'm going to be curious whether there exists an x solution to this linear matrix inequality.
00:01:53.304 - 00:02:54.034, Speaker A: Here, the AI and the b are symmetric matrices, and the s greater or equal than zero for any s matrix means that this s is symmetric and positive semi definite. This is a far reaching generalization of linear programming. There are many, many papers, and the point that I would like to address is that it may have solutions which have to be of exponential size. This is shown by this classic example of Hachian. So we have m variables, and we start out with saying that xm has to be greater or equal than two, and the after that I keep squaring the variables. I say that x one is greater or equal than x two squared, x two greater or equal than x three squared, and so on and so on. And if I do a little algebra, then by induction I can see that the x one has to be greater or equal than two to the two to the m minus one, where m is the number of variables.
00:02:54.034 - 00:03:35.914, Speaker A: What does it say about the size of x? I take the log and I see that the log of two to the two to the m minus one is two to the m minus one. So the size of x has to be at least two to the m minus one. In other words, even the size is exponentially large. And this is a quadratic system of inequalities. But at the same time, I can write it as an SDP. I can write these quadratic inequalities by two by two linear matrix inequalities by putting the xi on the diagonal and the xi plus one on the off diagonal element. Okay, so I should stop here and see if there is a question.
00:03:38.434 - 00:03:52.058, Speaker B: So the question. So the form of the SDP on the fever slide. And this is different in term of, like, structure, right?
00:03:52.226 - 00:04:21.284, Speaker A: Yes. So this is a block structured SDP, right? Because I have to put together a bunch of small sdps, a bunch of two by two sdps. But at the same time, I can make this, this bunch of small sdps into a large SDB, because I can take these two by two blocks and I can put them onto the, to be the diagonal block of a large matrix. So I can, I can rewrite this in a standard form.
00:04:22.104 - 00:04:22.752, Speaker B: Okay?
00:04:22.848 - 00:05:11.624, Speaker A: So just like, just like linear programs have a standard form, sdps can be put into a standard form. So if I want to see what goes on in this problem, I can picture it. So instead of saying that the X three is greater or greater than two, I say, I'm going to say that the X three is between this upper and lower bound, and after that, the x one greater or greater than x two squared, x two greater or greater than x three squared. So we can see that the X one grows quite drastically when the X three grows. Now we come to this major open problem, which is, is SDP feasibility in P. And this is a major open problem. It's open even for quadratic constraints.
00:05:11.624 - 00:06:18.584, Speaker A: And what is the main dilemma there in sdps? If I have a solution which has to be of exponential size, then how do I prove in polynomial time that exponential PSi solution exists? I mean, I cannot do it by simply writing it down, because even that would take exponential time just to write it down, let alone computing it. So that's the major dilemma. And this is an open problem that has been open for 30, 40 years, depending how we count. And I should emphasize that this is even open for quadratic constraints. So to some extent, we can view this as a very fundamental problem as to what is the difference between linear and nonlinear, because quadratic is just a little bit more difficult than linear. So we can see that this problem already shows up in quadratic problems, which are only a little bit more difficult than linear ones. Now, from this major open problem, we can derive two open questions.
00:06:18.584 - 00:07:46.250, Speaker A: The first question is, is it possible to represent these large solutions in polynomial space? So can I somehow convince myself that a solution is really a solution, even though actually writing it down would take exponential space? And the Ho Chian example? It gives us some hope, because in order to convince myself that the two to the two to the m minus one is in the solution, I don't have to write down the two to the two to the m minus one. I can just do some kind of symbolic manipulation. I can see that two to the two to the m minus one, with some suitable x two and with some suitable x three and so on, is a feasible solution to that linear matrix inequality, or to that quadratic system of inequalities, without having to write then the two to the two to the m minus one, which would take a lot of space. So in other words, the system provides a certificate by doing some symbolic computation. The second question is, are large solutions common in semidefinite programs? Now to that, the answer seems to be a no. There are several reasons for that. The first is that they don't come up in linear programs, linear programs that of course sdps.
00:07:46.250 - 00:08:28.364, Speaker A: And they also do not come up in typical sdps in the literature. So for example, in the Mexico SDP, which is our typical favorite SDP, they don't show up. On the other hand, we can also eliminate them, even in the Hodgson problem, by a very slight change. So, for example, what I can do is I can replace the inequality xm greater or equal than two by this new inequality. When the two is replaced by two plus xm plus one, where xm plus one is a new variable, I can make the xm plus one to be something negative. Let's say I can take it minus three. Then the XM doesn't have to be larger than negative one.
00:08:28.364 - 00:09:11.954, Speaker A: And then if I continue this chain of reasoning, then I can see that the x one doesn't have to be large anymore. Now, the other way of getting rid of the large solutions is I can do some kind of random linear transformation. I can replace the x by gx, where g is some random dense matrix. So I can do this linear change of variables. And then the Hodgkian problem becomes a complete mess. So if I do a linear change of variables in an SDP, it is still going to be an SDP, but it becomes a very messy one, and it just won't be able to see any kind of structure anymore. So I should stop here and see if there is a question.
00:09:11.954 - 00:09:27.834, Speaker A: So, but I can kind of guess or state that apparently it seems like the common constant is that large variables in SDPs are rare, so they don't really happen a lot. So I would like to see, is there any question.
00:09:30.334 - 00:09:45.574, Speaker B: If you do a chain of variables for the cations example, you can solve it to get the solution which is not exponential large or.
00:09:46.194 - 00:10:16.834, Speaker A: That's a good question. It's. One would have to try it. One would have to try it. It's a good question whether if you have that nicely structured, I mean, if you put that into Moses, I think it's going to just say that there's an overflow because you cannot store that large numbers. But it's a good question whether if you do this random change of variables, is it going to be something better? Okay, that would be interesting to try actually.
00:10:18.214 - 00:10:18.954, Speaker B: Right.
00:10:20.454 - 00:10:53.634, Speaker A: So the main result is, which is informal, that actually every strictly feasible SDP can be made into a Hotchie anti SDP, as follows. We need a little bit of background. It's not going to be too much. I'm going to look at this dual system, which is y positive. Some definite AI dot y is equal to zero for every I. So this is kind of like a dual SDP. And I'm going to look at the singularity degree of this dual SDP.
00:10:53.634 - 00:11:53.314, Speaker A: What is singularity degree? It's the minimum number of so called phase reduction steps to certify what is the maximum rank positive semi definite matrix in this dual system. Now if you know what is facial reduction, that's great. Now if you don't, then that's not a problem either. The only thing we have to know is that key is a non negative integer parameter and it's less or equal than one when the SDP is a linear program. Of course I can make the test DP into a linear program, for example, by, by making the matrices diagonal. So this is all the background that we need. And we are also going to assume one more thing, that the SDP is strictly feasible so there exists an x so that this linear matrix inequality holds strictly, so that this affine combination of matrices is positive definite.
00:11:53.314 - 00:12:36.290, Speaker A: So I'm going to assume that. And it turns out that these assumptions are going to be minimal. And here's our informal theorem one. So after a linear change of variables, x is replaced by mx, where m is a suitable invertible matrix if x is strictly feasible and x k is large. Then this following chain of inequalities holds. Where the alpha exponents are bounded between two extremes, the upper bound is at most two, which is to be expected, because that's what happens in Hodgkin's example. And the lower bound is larger than one.
00:12:36.290 - 00:13:20.604, Speaker A: On every one of the alpha's, it's k divided by k minus one, k minus one divided by k minus two, and so on. It goes all the way up to two. So the d j and the alpha j are constants that depend on the AI and on b, and then the large m minus k variables that we consider to be fixed. So, in other words, after a linear change of variables, I can create a hot xion type SDP from any strictly feasible SDP. That's the gist of the main result, and the assumptions that we make are minimal. I'm not going to go into every detail, but that's in the paper. So I should stop here and see if there is a question.
00:13:24.704 - 00:13:32.844, Speaker B: One question is the matrix M is like, you just show that it exists. A matrix M. Right.
00:13:33.184 - 00:13:58.894, Speaker A: So this is a constructive proof. So we can show how to construct the m. But to do it computationally, we would have to solve SDPs in exact arithmetic, which in general, we don't know how to do. So it's a constructive proof. On the other hand, there is going to be some concrete examples in which the m is just an identity matrix, so we don't have to do anything at all. And the SDP is going to be automatically in this hoxheon type shape.
00:14:00.234 - 00:14:03.654, Speaker B: But those examples, it's rare.
00:14:04.834 - 00:14:40.554, Speaker A: There is only two of them, and one of them is something that we created. So it comes from something that's concrete. It comes from polynomial optimization, which people have studied. And we show that even in that example, this hierarchy shows up without the change of variables. The other one is due to a computer scientist, Ryan O'Donnell, and he also came up with an SDP that has large solutions, and we can show that his SDP fits into our framework.
00:14:41.554 - 00:14:53.374, Speaker B: Okay. And Alpha, Alpha J here need to be alpha K. It needs to be a rational number, or it can be real numbers.
00:14:54.114 - 00:15:09.844, Speaker A: That's a good question. You see, I think they have to be rational. I think. Yes, yes, yes, yes, they are going to be rational. I haven't even thought about that. But if I think about it a little bit, they have to be rational. Yes.
00:15:10.704 - 00:15:15.324, Speaker B: And de can be just positive, real numbers.
00:15:15.704 - 00:15:22.176, Speaker A: Yes. They have to be positive. Otherwise, this doesn't make too much sense.
00:15:22.320 - 00:15:25.752, Speaker B: But they are rational as well.
00:15:25.808 - 00:15:28.684, Speaker A: Or you can choose them to be rational.
00:15:29.104 - 00:15:29.704, Speaker B: Okay?
00:15:29.784 - 00:16:12.304, Speaker A: You can choose them to be rational, and that's just because the rationales are dense, you know, in the set of real numbers. Right, but the alphas are, can be rational, can be chosen to be rational. Yeah. So the corollary, if you start playing with those inequalities, we start to see what happens if the alpha j's are at the upper bound. So all alpha j are equal to two. Then I can derive that the x one is greater, equal than constant times x k to the power of two to the k minus one, which is exactly what happens in Ho Chihan's example. Now, if the alpha j is at the lower bound, the lower bounds are strictly bigger than one.
00:16:12.304 - 00:16:26.232, Speaker A: Remember that. Then I can derive that the x one is greater, equal than constant times x k to the power of k. So let's see what happens.
00:16:26.408 - 00:16:27.434, Speaker B: One more questions.
00:16:27.504 - 00:16:28.014, Speaker A: Yep.
00:16:28.134 - 00:16:37.502, Speaker B: So in order to get exponential large variables, and you need xk to be greater than one, right?
00:16:37.598 - 00:16:38.062, Speaker A: Yes.
00:16:38.158 - 00:16:40.502, Speaker B: You need to put that constraints right?
00:16:40.558 - 00:16:41.634, Speaker A: Yes. Yes.
00:16:41.974 - 00:16:42.878, Speaker B: Otherwise, not.
00:16:42.926 - 00:17:54.994, Speaker A: Okay, so the theorem says that this is true if the x k is large enough, right? Which kind of makes sense, because even the, I mean, remember that even in the Hoxhean example, if I replace the xm greater or than two by just saying that the xm is greater than one half, then the, the whole logic collapses and the x one doesn't have to be large anymore. So let's look at whether these, these extreme possibilities can really occur. So the worst case example is the Ho chi and SDP. I'm going to look at this, I'm going to demonstrate this for four variables. So there is this five by five linear matrix inequality. So this matrix has to be positive, semi definite, and I can look at the red subdeterminants, the blue subdeterminants, and the green subdeterminants, meaning red subdeterminant is the subdeterminant which has three red corners, the blue is, which has three blue corners, and so on. And the red corners imply that x one is greater or greater than x two squared.
00:17:54.994 - 00:19:03.752, Speaker A: Then x two greater or equal than x three squared, and x three greater or equal than x four squared. So the exponents are the largest possible that can come out of the theorem. Now let's see whether we can get the exponents to be minimal. And let's look at this mild SDP here again, I look at the red subdeterminants, blue subdeterminants, and green subdeterminants. And I get the inequalities x one times x three greater or equal than x two squared, x two, x four greater or equal than x three squared, and x three greater or equal than x four squared. Now, these are not so nice as in the Hachian example, because I have xixj on the left hand side. However, I can do a little algebra and I can derive inequalities in which the x, the left hand side only has an xj in it, and the right hand side x as xj plus one.
00:19:03.752 - 00:19:56.564, Speaker A: So, specifically, I can derive x one greater than x two to the power of four thirds, x two greater equal than x three to the power of three over two, and x three greater equal than x four squared. So in this, the exponents are the minimal that can happen in my theorem. So both extremes can be achieved, both the upper bound and the lower bond can be achieved. And if I want to picture this just with three variables, and I added the normalization x three between two and zero, just for normalization, to make things better visible, then I see that in the Hachian example, the x one grows much faster when the x three grows, as opposed to the mild SDP. So I should stop here and see if there's any question.
00:19:58.544 - 00:20:11.824, Speaker B: There is one question. So if you go from the worst case to the my case, and how many combinations that you get?
00:20:13.164 - 00:20:30.844, Speaker A: Yeah, that's a good question. You can count it. I mean, there is going to be the result stated. There's going to be a recursion later on in which we are going to find out how to compute the alpha J's. So I think one can count it. I think one can count it.
00:20:33.464 - 00:20:40.844, Speaker B: Okay, so the numbers would be finite, but could be exponential large, right?
00:20:42.104 - 00:20:51.924, Speaker A: I think they could be exponentially large, exponentially many possibilities for the exponents, but there is going to be definitely finitely many possibilities.
00:20:54.404 - 00:20:55.264, Speaker B: All right.
00:20:57.724 - 00:22:02.016, Speaker A: The proof idea, so further is this issue of reformulating, in other words, changing variables. So there is this crucial part of the result that we have to change the variables in order to get to the, to an SDP in which I can find this ho chi type hierarchy. So I'm going to change variables to get an SDP, which I'm going to call SDP prime. Second, from this SDP prime, I'm going to derive messy quadratic inequalities such as these. So here we have, this is quadratic, but instead of just having x one, I have x one plus a linear combination of things, which are larger numbered, larger index than x one, so x two, x three, and so on. So we can derive such messy quadratic inequalities, and in the messy quadratic inequalities I can clean them up. In other words, I can suppress the larger numbered variables, I can suppress the x two and the x three, and I can suppress the x five.
00:22:02.016 - 00:22:58.484, Speaker A: So I'm going to get something like x one, x four bigger than constant times x two squared when x k is large. So the point is to suppress the larger indexed variables. And then finally I can do some elimination procedure to make sure that the inaccurate is look like x j greater or equal than constant times xj plus one to the power of alpha j plus one for every j. And after that we can find a recursion to compute the exponents. So these are the main steps. The first step, which is the, probably the most important one that I can reformulate my SDP into this SDP prime, which has this special format. The first constraint matrix has this small identity matrix in the upper left corner and lots of zeros elsewhere.
00:22:58.484 - 00:23:46.946, Speaker A: And the second constraint matrix has arbitrary stuff in the first or one rows and columns and then a small identity matrix. Then it keeps going like that. So in general, in the ith constraint matrix, the first one plus plus plus ri minus one rows and columns on arbitrary. And then there's a small identity matrix and the sizes of these identity matrices are positive so that the r is r1 through rk. And then there is some other matrices here, xi times AI prime plus b prime. And those don't really matter what they look like, not to be 100% precise. To get to this normal form, the only thing that I did is not just a linear change of variables, but I was a simulated transformation.
00:23:46.946 - 00:24:34.250, Speaker A: So I was permitted to replace the matrices AI by t transpose ait that t is some invertible matrix. So to show the mean idea here, I'm going to show how from this SDP prime I can derive massive quadratic inaccurate. So I'm going to show this by an example. So I'm going to focus on the x two, x three and x four. And then there is this dot dot dot. The dot dot dot means that there is some combination of the x five times a five prime, and so on and so on. And I'm going to look at these circled entries which are going to give me a two by two subdeterminant.
00:24:34.250 - 00:25:22.170, Speaker A: And I know that since this matrix is positive definite, the two by two subdeterminant has to be positive. So I'm going to look at this two by two subdeterminant, which is x two plus two x three plus five x four plus dot dot, dot, dot, dot dot means a combination of x five and so on and so on. And then it's multiplied by x four plus dot dot dot. That comes from this lower right entry. And then the off diagonal entry gives me four x three plus seven x four plus dot dot dot squared. So from a positive definite matrix I can very naturally derive quadratic inequalities just by looking at two by two subdeterminants. I don't even have to look, ever.
00:25:22.170 - 00:26:01.014, Speaker A: A three by three subdetermines, it's not even necessary. And the dot dot, dot, as I said before, means a combination of higher numbered variables. And the cleaned up version of this, I can suppress the higher number terms. So I'm going to get from the x two plus two, x three plus five, x four plus dot dot dot, I'm going to get x two and then from the x four plus dot dot dot I'm going to get x four. And then from the right hand side I'm going to get constant times x three squared. And this whole thing is going to be true only if x k is large enough. So that's the basic idea.
00:26:01.014 - 00:26:04.534, Speaker A: So I should stop here and see if there's a question.
00:26:07.154 - 00:26:17.142, Speaker B: So in order to get the form, or the form SDP from, you need to assume the problem to be tricky. Feasible?
00:26:17.238 - 00:26:40.434, Speaker A: Yes, yes. Well, no, I mean, to, just to, just to reformulate the problem, you don't have to assume that the problem is strictly feasible, but to derive the inequalities for that, you do have to assume that the problem is strictly feasible. So that reformulation into the SDP prime format, that works for any SDP.
00:26:40.734 - 00:26:53.674, Speaker B: Right. So here is the coefficient alpha case. It's computed. Can you go to next page?
00:26:53.834 - 00:27:00.214, Speaker A: Next page. This one, yeah.
00:27:01.794 - 00:27:16.300, Speaker B: So the matrix t here again is not like is contracted or just, just show it exists. A matrix transformation.
00:27:16.372 - 00:27:38.324, Speaker A: Again, it's the same story. So in order to compute the t is the same story as to compute the m. So I know that it exists, I can give a constructive proof, and I wasn't, that to actually compute it, we have to solve sdps in exact arithmetic. But again, there is going to be some concrete examples in which you don't need a t and you don't need the m.
00:27:41.664 - 00:27:42.564, Speaker B: Okay?
00:27:46.184 - 00:28:02.444, Speaker A: So in other words, the, from, from a strict SDP one can derive quadratic inequalities just by looking at two by two subdeterminants now. Okay, does, does this make sense? Are there any more questions?
00:28:04.184 - 00:28:05.684, Speaker B: No, it's clear.
00:28:06.224 - 00:28:37.834, Speaker A: Okay. So then how do I compute the alpha J plus one exponents? There's a nice recursion for that. In particular, I can compute this by this recursion going backwards. So first I compute the alpha k and then the alpha k minus one and so on. So in general, the alpha j plus one is computed as two minus one over a product of the alpha J plus two, alpha J plus three, and so on. Alpha t J plus one. That tj plus one is a number that I'm going to define a little bit in a hand waving manner.
00:28:37.834 - 00:29:17.444, Speaker A: And. But let's notice that firstly, this is something like continued fractions. So this is continued fractions are something that we know from number theory. And it's kind of interesting that they show up in SDP. And pj plus one is the index of our rightmost block in the linear matrix inequality, where the x j plus one shows up. Again, this is still a bit of a hand waving, but I'm going to give some very concrete examples. So what I can see from this definition is that suppose I shift the xj plus one to the right, then the tj plus one is going to increase.
00:29:17.444 - 00:29:55.816, Speaker A: And if I do that, then I'm going to multiply more terms in this denominator, and then the alpha j plus one is also going to increase. Right? So that's the general idea. And I'm going to show this specifically to an example. So we start with this mild example, with this mild SDP. So this matrix has to be positive, semi definite. I left out the curly inequality just to save some space. And from this linear matrix inequality I managed to derive the exponents four thirds, three over two.
00:29:55.816 - 00:30:22.386, Speaker A: Two. In other words, the x one is greater or equal than x two to the power of four thirds. X two greater or equal than x three to the three over two, and then x three greater or equal than x four squared. Now let's shift the x two a little bit to the right. Okay. Then see what happens. Then the four thirds became five thirds.
00:30:22.386 - 00:31:14.804, Speaker A: So as soon as I shifted the x two to the right, its exponent went up, and then I'm going to again shift the x two to the right and its exponent again went up. So the alpha two became two. So if I go from the left towards the right, the inequality x one greater equal than x two to the power of four thirds became x one greater, equivalent to the power of x two to the power of five thirds, and then that became x one greater equal than x two squared. So shifting the x two to the right made the exponent larger. It was actually monotonically increasing. So again, I should stop here and see if there is a question.
00:31:17.824 - 00:31:30.806, Speaker B: So from the contraction of alpha k, alpha alpha j, we can see it gives us rational numbers, but it's also related to the location of x, right?
00:31:30.950 - 00:31:32.474, Speaker A: Yes, yes.
00:31:34.014 - 00:31:37.794, Speaker B: So how, how is it related to that one?
00:31:39.934 - 00:32:07.454, Speaker A: Well, that was in the previous one you see here. So the tj plus one is the position of the xj plus one is the right most position of the xj plus one. So when, when the tj plus one gets larger, then that means that I'm going to multiply more alpha j's in this denominator. And that means that the alpha j plus one is going to get larger.
00:32:08.874 - 00:32:19.006, Speaker B: But you don't have the formula for a constant behind in front of x j plus one.
00:32:19.150 - 00:32:56.234, Speaker A: No, that we haven't computed. I think that would be probably messier. Or it's probably not that. I mean, maybe one can do that too. That's such a good question. Maybe one can compute exponent the formulas for the DJ's as well. Okay, so now I'm going to look at a connection to Fourier mods in elimination, which is something that we know from linear programming.
00:32:56.234 - 00:33:51.724, Speaker A: And this connection is interesting because sdps are of course, very non linear. So we can view this whole process of deriving these inequalities as a variant of Fourier Modskin delimination or as an application of Fourier mosquito elimination. So let's start, I'm going to show this through an example. So suppose I start with these quadratic inequalities. These quadratic inequalities can be made into linear in linear inequalities because I can take the logarithm on both sides and I can call the log of xi to be vi. So the x one, x three becomes y one plus y three, the x two squared becomes two by two, and so on. So I took the log, and from the quadratic inequalities I obtained linear inequalities.
00:33:51.724 - 00:34:42.184, Speaker A: And then on this I can do some elimination. I can add positive multiples of an inequality to another inequality. So for example, if I add one half times the last inequality to the middle inequality, I get y two greater or equal than three over two times y three. And then this linear inequality can be translated back to the quadratic space and I get x two greater equal than xt to the power of three over two. So this is, this is not necessarily extremely important because I really know how to compute the exponents. But it's an interesting connection that computing these exponents is a little bit the same thing as Fourier and botskin elimination.
00:34:43.564 - 00:34:46.012, Speaker B: Next question before you move on.
00:34:46.188 - 00:34:46.796, Speaker A: Yep.
00:34:46.900 - 00:35:01.564, Speaker B: So here if you have a constant in front of x, you know, the Royans have the inequalities here. Then you take a log. Is it, the inequalities is going to.
00:35:01.604 - 00:35:07.504, Speaker A: Have a constant, then it's going to be a constant. Yes, but you can still do the same thing.
00:35:07.804 - 00:35:12.064, Speaker B: But then, then the constant is going to be unlock right terms.
00:35:12.724 - 00:35:13.668, Speaker A: That's right.
00:35:13.836 - 00:35:16.784, Speaker B: Yeah. So it locks smaller than.
00:35:17.244 - 00:35:18.076, Speaker A: Yeah.
00:35:18.260 - 00:35:24.104, Speaker B: Constant. It's, and it's going to be like three.
00:35:24.844 - 00:35:26.956, Speaker A: That's correct. Yeah, yeah, yeah, yeah.
00:35:27.100 - 00:35:30.464, Speaker B: So that makes the numbers getting smaller.
00:35:30.854 - 00:35:31.634, Speaker A: Yes.
00:35:32.974 - 00:35:53.302, Speaker B: So in that case, it also affects the solutions in terms of like the size of the solution because, you know, because when you, you make the numbers getting smaller, it also like the solution also getting smaller.
00:35:53.478 - 00:35:53.862, Speaker A: Right.
00:35:53.918 - 00:35:54.514, Speaker B: So.
00:35:56.634 - 00:35:59.570, Speaker A: Yeah, but, but what is, what's wrong with that?
00:35:59.642 - 00:36:27.502, Speaker B: So what I'm saying here is because we're looking at some solution which may be exponentially large, but if you, it probably depend on the coefficient of the problems. But we take chain of variable for the log, then the coefficient also like, also need to be changed from, let's say a to log a.
00:36:27.638 - 00:36:31.314, Speaker A: Yes, that's true. That's true. That's, that's, that's absolutely true. Yeah.
00:36:34.134 - 00:36:36.486, Speaker B: So, um, but that's okay.
00:36:36.510 - 00:36:42.714, Speaker A: I mean, then after, at the end, when I translate everything back to the, to the x space, I'm going to get the same results.
00:36:43.734 - 00:36:44.582, Speaker B: Okay. Yeah.
00:36:44.678 - 00:37:39.884, Speaker A: There's like a one to one correspondence. It's just kind of, just kind of interesting to see that we can do this by flying mode skin elimination. So now we come to this question, do we need this change of variables? Which is kind of like a question that comes up a lot. Okay, so why do you need this change of variables? This looks so unnatural. So first I want to argue that this is natural, because if I do a change of variables, a linear change of variables, that makes a complete mess even out of the hachian example. After that, I will not be able to prove any kind of hierarchy or large variables, even in the Hachian example. So if I'm permitted to do this change of variables, then, so as if I do that in the Hodgkian example, then I might have to do the inverse operation to undo this mess.
00:37:39.884 - 00:38:06.140, Speaker A: That I created. But sometimes it turns out that actually I don't have to do any kind of change of variables. And I'm going to show two concrete examples for that. So first, it's a classic example. Suppose I want to minimize an even degree polynomial. It's a univariate polynomial. And what I can do is I can rewrite this as SVP using the sum of squares technique.
00:38:06.140 - 00:38:43.188, Speaker A: And I'm going to look at the dual problem, and I'm only going to show the case when n is equal to three. So the univariate polynomial is degree six. So then I get an SDP, a dual SDP, which looks something like this. And I can see that this problem is automatically in our SDP prime format. So in other words, the vi six has a coefficient matrix so that the one is in the upper left corner. And then vi four has some arbitrary stuff in the first one row and column, and there is a small identity matrix here. And the vi two continues like that.
00:38:43.188 - 00:39:37.364, Speaker A: It has some arbitrary stuff in the first two rows and columns, and there's a small identity matrix there. So in other words, this problem is exactly in the form of SDP prime, without having to do any kind of change of variables. And the corollary is that a vi, which has two n components, is feasible in that dual SDP. Then there is this nice hierarchy among the vi I's. So the vi two n is upper bound, is lower bounded by a power of minus two, and so on and so on. And if I combine those inequalities, then I get theta y two n is greater or equal than y two to the power of n. And it's interesting that this problem has been looked at for quite some time, but somehow this doesn't seem to be known.
00:39:37.364 - 00:39:41.564, Speaker A: Okay, other questions.
00:39:42.184 - 00:39:50.404, Speaker B: Do you require y two to be greater than one, or how y two constrain on y two?
00:39:51.024 - 00:40:06.594, Speaker A: No, you don't. You don't. So in this case, you don't need. This whole story of xk has to be large enough, because this is so clean that we don't need any of that, any of those complications.
00:40:07.134 - 00:40:18.374, Speaker B: Then is it not clear if the y two n is exponential large, right. It depends on y two, right?
00:40:18.414 - 00:41:22.756, Speaker A: So it's lower bounded by the nth power of y two. And of course, if the y two is large, then divide two n is also going to be much larger. So the other example comes from computer science. Suppose that I want to verify, want to certify that this polynomial, which is a linear polynomial, is non negative for all x y that lives in k, where k is some very simple set. So I'm not going to go into every detail, but k is a simple enough set so that one can convince himself or herself that this linear polynomial is indeed non negative for all the x y which are in that case set. And the resulting SDP is going to look like this. Without going into too many details, I'm going to define the eij to be the ij unit matrix, and then I will see that it's exactly in the form of SDP prime.
00:41:22.756 - 00:41:49.444, Speaker A: So SDP prime is that normalized SDP. And so this SDP is in that form without having to do any change of variables, any similar transformations and anything like that. So it essentially yields Hoxheans example. So this is a second problem from the literature in which we can show that it's already in that HTTP prime format without having to do any kind of change of variables.
00:41:51.684 - 00:41:55.220, Speaker B: So this example is look like separation.
00:41:55.332 - 00:42:00.224, Speaker A: Theorem, or separation theorem. Sorry, what do you, what do you mean?
00:42:00.884 - 00:42:09.612, Speaker B: So it means that we need to separate the set k from, you know, the origin or whatever.
00:42:09.708 - 00:42:53.114, Speaker A: No, no, no, it's much simpler than that. It's just basically you want to prove that a polynomial is non negative for all x y, which lives in a certain set, right? So the sum of squares technique, if you don't have constraints, it just wants to certify that the polynomial is non negative. And the way it does it is that you write it, you write that polynomial as a sum of squares, you write it as sum of I f I x squared. If you can do that, then you can be sure that that polynomial is indeed non negative. And if you have a constraint, then you have to somewhat take that constraint into, into account.
00:42:56.014 - 00:43:03.554, Speaker B: Yeah, okay, I have not thought about this, but we'll see.
00:43:04.534 - 00:44:03.594, Speaker A: Okay, so then here's, now we come to a point which we are quite curious about. The question is, how do we certify exponential size solutions in polynomial space without computing them? So here's the idea. So look at this reformulated problem again, right? So this is SDP prime. Now, suppose that I'm given x k plus one, and so on xm. So we are given these m minus k lost variables so that I know that there exists x one through x k. So this problem is strictly feasible. Now, how can I convince myself that this problem is strictly feasible? In other words, how do I convince myself that the x one through xk actually exists? So what I can do is I can prove that the x one through xk exists without having to compute them.
00:44:03.594 - 00:44:47.782, Speaker A: So how do I do that? Well, I could compute them in reverse order, starting with x k and then x k minus one, x k minus two, and so on. And the way I do it, I'm going to make larger and larger lower right corners of this matrix to be positive definite. So we start with this matrix, the sum of Xia prime plus B prime. So we start with this z matrix. And I know that this corner is positive definite. Right. And after that I'm going to add the multiple of the Ak prime.
00:44:47.782 - 00:46:00.814, Speaker A: So I'm going to add xk times Ak prime and xk is sufficiently large and the result is going to be a matrix so that this lower right corner is positive definite. So I made this lower right corner, which is positive definite in the z to be a little bit larger, and it is still positive definite. And then I'm going to add the multiple of the AK minus one prime to make an even larger corner to be positive definite. So I'm going to make this corner to be positive definitely, and so on and so on. And I know that I can do that because the AK prime is an identity matrix, which is here. So if I add a sufficiently large multiple of the AK prime to this matrix, then this lower right corner is going to be positive definite. So the structure of the AI primes makes it clear that I can carry out this computation, but I don't actually have to numerically carry out the computation because I know just from the shape of the AI prime that this computation can be carried out.
00:46:00.814 - 00:46:08.094, Speaker A: So I don't know whether this makes sense.
00:46:09.594 - 00:46:13.814, Speaker B: So this actually comes from the shoe complement, right?
00:46:14.714 - 00:46:35.384, Speaker A: Exactly. You can, you can view the shoe complement, but, but actually one doesn't even have to go to Schur complement. One can just go to the definition of positive definiteness, that for every u, the U transpose a U is positive, is positive as long as the U is non zero.
00:46:35.804 - 00:46:42.052, Speaker B: Right. You don't need to compute it by show compliment, but it's just by contraction. You can do that.
00:46:42.148 - 00:47:00.844, Speaker A: Yeah, yeah, yeah. So the shape of the matrices makes it clear that these x one through x k existing. And I don't have to carry out the numerical computation. I can just prove on with pencil and paper and I can see that these x one through xk, they actually exist.
00:47:01.904 - 00:47:02.684, Speaker B: Yeah.
00:47:03.464 - 00:47:49.524, Speaker A: And then the question that comes up is that, is it true that every SDP which has large solutions is in this regularized form? And perhaps I have to permit a simulated transformation, but that doesn't affect the size of the variable so this is a big question, and this is what we think could be really interesting to help to solve that problem. I mean, can we certify, every time that we have a large solution, can we certify that large solution in polynomial space? In this particular special case? We can do it. Can we do it every time? That's the question. Okay, other questions.
00:47:53.184 - 00:48:04.324, Speaker B: So the large solution, because you become adding a large number every time you compute it, or.
00:48:06.664 - 00:48:59.374, Speaker A: Because the large solutions are because because of this whole machinery that we developed in the paper, so that you have those two by two sub determinants. So anytime there is a strictly feasible solution and the x k is large, that two by two subdeterminant has to be positive, and then we can push through that whole argument. So the large solutions happen because of that theorem. But the point is that the x one through x k are the variables which are large. And the point is that to convince myself that they exist, I don't have to compute them. Okay, so the conclusion is that we have this issue that in sdps we have exponentially large solutions. And this goes back to the famous Hachian example.
00:48:59.374 - 00:49:53.114, Speaker A: And our main result is that we can derive a Hachian type hierarchy among the leading variables in every strictly feasible SDP after we do this linear change of variables. And we can compute the exponents using this NIST formula, which is just like continued fractions. And we give a partial answer to this question, how do we represent exponential PSI solutions in polynomial space? And the point is that every known SDP which has large solutions is in our normal form, which is SDP prime. And we have a paper which is posted in the archive. Comments and suggestions are definitely very welcome. So thank you for your attention, and I'm happy to answer more questions.
00:50:04.994 - 00:50:07.226, Speaker B: I think I asked enough questions.
00:50:07.330 - 00:50:09.554, Speaker A: Okay, well, so thank you so much.
