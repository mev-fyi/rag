00:00:00.200 - 00:01:06.144, Speaker A: Thank you very much for reminding. Okay, sorry about that. So we are, last time we were talking about self avoiding random work, and what we did, we proved the theorem of Duminil, Kapien and Smirnov about the connectivity constant, about approximate rate of the number of self avoiding random bulk, specifically on hexagonal lattice of the length. And our main tool last time was this paraphernalia. So this strange contraption, which turned out to be essentially discrete analytic. And the problem with it was that it wasn't discrete analytic enough. So it wasn't enough to achieve what we wanted to achieve, but it was enough to prove the theorem.
00:01:06.144 - 00:02:03.922, Speaker A: And the main tool there was the skillema, which essentially established discrete cache Riemann equation. So we already discussed why it was discrete cache Riemann equation, because it implies certain form of Mary RSA. And so now I will try to clarify what it is we try to achieve. Well, rather they try to achieve, but back many years ago, and it's still an open question mostly. And for this, we'll need to talk about more general models, what is called in statistical physics often models, and the first often model is quite famous. It was invented by Ernstesen, a chemist. And the model essentially is the following.
00:02:03.922 - 00:02:54.404, Speaker A: So it can be defined on any final graph. So you look at any final graph with vertices v and edges e, and we look at all the configurations, spins on this graph. So configurations are just functions from vertices to minus one one. So you assign spin negative one on one on each of the vertices, and then you have parameter which from the physics point of view is the inverse temperature. So this is essentially one over temperature, whatever the temperature is. So just, well, the intuitive meaning of this is that when bit is really large, everything is kind of frozen, the temperature is near zero. When bit is very small, there is a lot of chaos.
00:02:54.404 - 00:03:54.814, Speaker A: And then what is the probability of each configuration? So you just look at this weight of the configuration. So you look at all the guys which are adjacent, multiply the spins, multiply by beta and exponentiate, and this would be the weight of your configuration, and then evaluate to normalize to make it a probability measure. This is a really standard thing there. This is called partition function. So this is the same partition function we already encountered in self avoiding clock. Okay, so now it's really easy to see that this probability is the same as this probability. So essentially, if two sigmas are the same, the product is one.
00:03:54.814 - 00:04:53.588, Speaker A: So this is just increases the weight of configuration. So you only need to, since the number of edges stays the same, you only need to count the edges where you are different, whereas the product is minus one. So when the spins are different on two sides of the edge. And so you need to multiply by two here because, well, first you take all of them to be one, and then you subtract the ones where the weight is negative one. And so this is just another form of the same. And that would help us with the next approach, which is called loop representation. So what is loop representation is the following.
00:04:53.588 - 00:05:45.794, Speaker A: What you do you, well, I did it on hexagonal lattice, but you can do it on any planar lattice. So you look at the spins plus or minuses, and then you surround each class, each connected cluster of the spins by a loop. And then it's very easy to see that actually probability of each configuration of loop. So now, okay, so again, let's recapture a bit. Our configurations used to be just assigning spins on the vertices. And here we go to the configuration bin. Collections of loops on the lattice.
00:05:45.794 - 00:06:51.634, Speaker A: Well, if you look carefully, it's actually collection of loops on the dual lighters. But let's not be too pedantic here. And the weight of configuration is just x to the length of all the loops here where x is e to the minus two beta. Indeed, where would you get this guy? The things where spins are different, you get it from one of the loop points. So the weight of your configuration would indeed be your e to the minus two beta to the power length of all the loops that are included in this weighting. And then of course you need to divide by normalized constant to make it a probability measure. So you need to divide by the total sum of all possible weights.
00:06:51.634 - 00:07:44.428, Speaker A: Okay, so this is what isn't model is for me. So again, original async model is a spin configuration. And then we have loop representation. Later today, we'll actually have yet another loop representation of the same model, which would give a slightly different scale and limit, but they would be related. Okay, so now let's generalize this approach to easing, and this is called often model from rigorously speaking. What I will describe is the loop version of n model. There is often representation of easing model also, and it's related similarly to the loop model.
00:07:44.428 - 00:08:45.940, Speaker A: But there are some technicalities which I don't want to discuss it not that important. That was invented by Stanley, a physicist. And the weight of configuration is now the following. So you have two parameters, n and x, and then the weight of configuration is n to the total number of loops multiplied by nice, to the length of the curves. And again, as usual, you need to divide by the normalizing factor and valve. As I mentioned, this model originally was related to spin being not zero dimension as we saw before in using one and minus one is zero dimensional sphere. The spins here live on n minus one dimensional sphere.
00:08:45.940 - 00:09:43.404, Speaker A: But again, I don't want to talk about all these technicalities. Okay, let me explain that some of the things we already saw before. So what happens when n is one and x is one? So the weight of each model is, of each configuration is the same. This would be just a loop representation of percolation, right? So for percolation, remember we independently assign weight to any, assign color rather to any face of the graph. So here, all the loop configurations would have the same probability. So this is, this would be percolation. But when x is positive and n is still one, this would be this one.
00:09:43.404 - 00:10:30.874, Speaker A: This would be easy model for. Because this is just, we have the same probability. Right? So now what happens when n is equal to zero the moment you have a loop? Look at this. Your configuration has zero probability. So this would be a model of configuration of curves. Well, and I'm cheating here slightly, I didn't yet tell you how to make this into a configuration where you can have curves, but a slight stretch of imagination that this would be self avoidant block. So no loops are allowed.
00:10:30.874 - 00:11:40.516, Speaker A: And then the weight of each curve configuration is proportional to just x to its length. And then there was a conjecture, there is still a conjecture wide open by two physicists and Bernard Nienko, that if at least n is between zero and two, including zero and two, then they exist. A conformally invariant scaling limit for specific value of x. For critical value of x. So this is one over square root of two minus plus square root of two minus n. So you notice that when n is equal to zero, this is exactly the constant we saw last time, rather than versus inverse of the connectivity constant. And they say, so they exist limit for this and for all this, for x bigger than x, critical.
00:11:40.516 - 00:12:59.554, Speaker A: These are different scaling limits. And in terms of slico, if you again, and I will try to explain how to make it into interface model, but a bit later. But in terms of Slic kappa, in the first case, in the critical case, this kappa is four PI over two, PI minus r cosine of minus sign over two. Well, this is impossible to digest, but trust me that it's a number which is less or equal than four. Now let me remind you that when we are less or equal than four, we have a simple curve. So the limit of this configuration essentially consists of well, if you want, a full configuration consists of some sort of loops which are simple, not self touching, not so in the case when x is bigger than x, critical. So in this particular case you have kappa, which is bigger, equals and four, so only for n equal to actually two, you have simple curves.
00:12:59.554 - 00:13:25.372, Speaker A: For larger cases you have self touch and even space filling curves. So this is called dense region. Exactly. Because of what I explained, the scaling limit would be very dense. And here it's a dilute region. And the conjectures that I am stating is for hexagonal lattice. So there was a question in the chat.
00:13:25.372 - 00:14:14.504, Speaker A: It is for hexagonal lattice. The critical constant dependent on this critical constant depends on the lattice. But the second part of the conjecture, the existence of the limit, is universal. So this should be true for annulitis and for the corresponding critical value of effects. Okay, so now let me actually open the chat. Okay, so these are two regimes. And now let me try to explain what is mathematically known about these two regimes.
00:14:14.504 - 00:15:22.492, Speaker A: So there would be a slight generalization of our discussion last time and a few more conjectures. So let me for now look at the dilute region. So what happens at the critical temperature? So as before, when I say a belongs to the boundary of Ohm, again, things like that, I will talk about medial lattice. So remember, what is medial lattice? You have lattice, the original lattice. Then in the middle of each edge you put a vertex and you join these two vertices, new vertices, on the medial lattice by medial edge, if and only if the corresponding edges meet at a lightest. So at the original lattice point. So let me remind you what happens for hexagonal lattice.
00:15:22.492 - 00:16:24.104, Speaker A: So this is medial lattice, this is medial lattice. For hexagonal lattice, it's agony. Okay, so what we do now to really discuss situation is the following. We take a point, an edge on the boundary of our fixed, simply connected domain and consider the following configurations. Configuration consists of some collection of loops plus a path from a to some point z inside the function. So there is a fancy physics term for doing this, but combinatorially this is really just this, we just add to our original configurations. We also allow some fun pass and the length of this path will be counted.
00:16:24.104 - 00:17:28.254, Speaker A: So in the weight function here, the length of the path will be counted, but it's not counted in the number of loops. So if you look at it this way, it's kind of clear that this is z zero. You get self avoiding clocks inside the domains, right? So you just look at passes at all the paths from a inside the domain, okay, so what is the paraphernionic absorbable here? So the one, remember that for n equal to zero for self avoiding work really helped us last time. And this is, well, that's the following. So let's fix n and x, and the observable is the following. So you look at the probability of the weight of the probability of the configuration. So n to the number of loops, x to the number of loops, and x to the number of edges.
00:17:28.254 - 00:18:29.734, Speaker A: This is not exciting. What is exciting is the following. You look at the winding of the curves from a to point z. So for a fixed middle edge z, you look at all the configurations where you have curve going exactly from edge a to z, exactly like we did last time. So you look at configurations where you have curves going from a to z, and then there are loops everywhere around. And then you look at the winding of the curve as he goes from a to z, but this only curve. So again, there are loops, and then there is one curve, one arc, and this arc has to end at z.
00:18:29.734 - 00:19:30.644, Speaker A: And then you take it with a weight, the weight, well, it's somewhat complicated. K would be one over two, PI, r cosine of n over two, and sigma is one quarter plus three. K over two looks horrible. But hopefully it would be clear after the kilemma that actually, well, this is just the formula you have to get. So again, like in self avoiding work case, the sigma was not obvious at all. But then we just looked at the, and it turned out that for the condition of ki lemma to hold, we need exactly this sigma and exactly the sex. The same thing happens here.
00:19:30.644 - 00:20:25.794, Speaker A: So this is our new key lemma for orphan model. If you have this horrible critical situation also it's a critical situation then for any vertex with neighboring vertices, medial vertices. So I just pick you up. So that's exactly what I drew here. So this is exactly the same pictures we had for self avoiding fork. We have exactly the same formula, so exactly the same cache. Riemann equation holds that p minus v f of p plus q minus r, f of q plus r minus v f of r is just zero.
00:20:25.794 - 00:21:51.976, Speaker A: Again, ki lemma, as we already discussed last time, is not enough to establish any sort of discrete analyticity, but it's a good first start. So let me explain first the proof of this exciting lemma, and then we will talk about what is really known. So you see, for proof, I really don't want to spend too much time on discussing the same technical details, but the proof is exactly the same. As for self avoiding, work with one key difference. So, let me return to self avoiding work for a moment. So, in self avoiding work, we considered two cases and grouped them as first case is we have paths going through pq and r and all other q. So here we'll do exactly the same.
00:21:51.976 - 00:22:44.884, Speaker A: But now we are allowed loops. Remember, for self awareness, specific thing was that there are no loops, so it was just a path. So the path, if it goes through all three adjacent edges, it has to either go here and return, or go this way and return. So no choice. But when n is equal to zero, first case is more exciting because you also have the following possibility, that you have a path which just ends at p, but then you have a loop which goes through the rest of the curve. So it's simply a separate thing. It's now not part of the curve.
00:22:44.884 - 00:23:30.988, Speaker A: It doesn't end at q or at r. It still ends at p, the curve. But then there is yet another loop which would actually increase or decrease depending on where n is, the probability of the configuration. So the probability of this configuration is multiplied by n. You have one extra loop, you have exactly the same number of edges, but you have one extra loop. And so let me just show you the, quickly show you the computation here. So what are the contributions of gamma one and gamma two and gamma three when you combine them? So the beginning is the same.
00:23:30.988 - 00:24:11.044, Speaker A: It's exactly the contributions of the gamma one and gamma two. So you look at the rotation from a to q to my a to r. Length is exactly the same because you have exactly the same number of edges. But here is a next return. Notice this special factor m. That's because of what I just explained, that the number of loops increased by one. So the weight of the configuration is multiplied by n.
00:24:11.044 - 00:25:20.066, Speaker A: This configuration is n times more likely than this or this. And so this extra n would actually help, because after you do the same trick as you did for self avoiding work, you take out this common factor. So x is the same, p minus v. You factor it out in the hopes of, well, then you divide q minus v by p minus r minus v by t minus v, and you get exactly the same factors, and then you get n, which didn't exist for sulfur Boyd work because, well, it was zero, so that's easy. And now, well, that's a leap of trust. But I don't want to do this computation. If you take this specific k and then this specific sigma, that would make this expression to be zero.
00:25:20.066 - 00:25:59.464, Speaker A: So that's how you compute the sigma. You just solve this equation that this plus this plus this is equal to zero. That's all. And by the way, this is how physicists arrived to the same conclusion. They looked at very similar things. They didn't look at it as a computation for discrete analytic functions. They looked at it from their point of view, from conformal field theory point of view, but the computation was essentially the same.
00:25:59.464 - 00:27:04.564, Speaker A: But now, amazingly, case two after that is exactly the same as self avoiding work. Of course, you have three configurations like this. So let me return to self avoiding work. So you have these three configurations and you just sum them up. They have the same probability. There are no extra loops introduced here, because again, the first configuration, gamma one, you simply visit just one of the edges, and then to visit the second you just add bit here or there. Okay, so now as we saw for self avoiding work, this lemma tells us exactly that when you look at the dual lighters.
00:27:04.564 - 00:28:57.474, Speaker A: So we let me return to self avoiding clock and look at the dual lightest, then discrete integral over every closed contour in the dual lightest, lightest consisting of now faces is zero. And exactly the same computation as in the dual light, as in the self avoiding fog, shows that this is not enough to uniquely determine f, because there are essentially two thirds of the two sorts as many linear equations for the values of the function f as there are variables. Well, a problem, but there is a conjecture, and the conjecture is the following, that actually fits when you start squeezing your lattice. It has a limit, and that's the conjecture that Smirnov and Duminil Kapin were working on when they proved this connectivity constantly. So the conjecture, which is still wide open in general case, is the following. So suppose that omega is a simply connected domain, a is a boundary point of omega, and phi is a conformal map from omega to the upper half plane, which near a behaves like this. So it's just some rotation of z minus a to put it to the real line, plus some ib, plus, well, some bounded analytic function.
00:28:57.474 - 00:30:41.002, Speaker A: And let us now consider lightest approximation omega delta to zam ion omega. And here I put by a fairly general lattice, because original conjecture was stated for a class of light as well, which are periodic and regular. Then it started including all what is called isaridal lighters, something that we will see later in the course, much later, but now it includes, well, essentially any good lightest, whatever good is. So there are. So essentially you can the conjectures that you can embed any embeddable lattice in the plane, or any embeddable sequence of lattices, such as the conjecture would call but anyway, so what happens is that this paraphernionic absorbal itself converges to the derivative of this map to the power exactly sigma, where sigma is what worked for the key lemma. Well, you need to rescale and the rescaling constant is again supposed to be delta to the minus signal. So amazingly, for easing this stroke for n is equal to one and critical easing it is true for essentially any light.
00:30:41.002 - 00:31:32.700, Speaker A: So again, as I said, x here can change the sliders dependent. But the conjecture holds for now it's known for any either radial light. This is due to Czilka, and I should have put chilka here. I will do it when I prepare nodes for, for publishing. Okay, so this is a conjecture and then this is a serum which we will prove later in the course. Simply don't yet have tolsto we need some probability here. Any questions so far? Okay, if not, let me.
00:31:32.700 - 00:32:16.886, Speaker A: Container is yet another model, this ports model. And again, this is a model where lots of things would be unknown. So the pots model was invented by Renfrew pots. And to motivate this, let us return to easyc. So again you have a spin configuration. The probability is just this. So now you will see why I would want to really switch from sigmas.
00:32:16.886 - 00:33:24.224, Speaker A: Sorry. So here I will use lambdas for the spin, because, well, we already had sigmas use both for spin configuration, the last part, and for critical constant in the periphery absorbable. So here this would be even more prominent. So I decided to switch to lambdas for the configuration. So now probability of this configuration gamma would be just valve, what we described. Now suppose that instead of two spins minus one, one you have q spins, and then, well, the formula is exactly the same, just the spin can be taken not to be just one or negative one. There are few possible values for the spin.
00:33:24.224 - 00:34:22.578, Speaker A: So here the natural graph to discuss is actually the normal square lattice, but it works for other graphs. And well, lots of the things I will discuss would work in fairly general situation. But again, natural graph is zero for, for the reasons you will see in a moment. Now here I want to add boundary conditions. So what happened in our fan models are essentially, we didn't talk about boundary at all. So the only role of was to provide this point a and restrict where we can what part of infinite graph we consider. So here the boundary would play crucial role.
00:34:22.578 - 00:35:03.080, Speaker A: So let's talk a bit about also the boundary condition is the following. You fix the spins on the boundary. That's it. So nothing too exciting happens just on your boundary. You have fixed spins and you only consider configurations with given spins. And again, there are few possible values of the spin. And now let us talk about other configurations.
00:35:03.080 - 00:36:05.854, Speaker A: This is called random cluster model or fortune Castellan model, which is due to cease fortune and Pieter Castelli, hope I pronounced his name correctly. So here configurations actually functions on edges. So instead of assigning spins to the vertices, you open or close edges. Okay, so the interpretation here is the following. If you assign zero, you consider this edge closed and you, if you assign one, you consider this edge open. And then you look at all the clusters of open edges. So all the clusters of things which can be connected by open edges.
00:36:05.854 - 00:37:10.964, Speaker A: And then representation is probability of given configurations. The following depends on two parameters, p and q, where q would be. As we will see in a moment, the same q is in q state box model. You take p to the power number of the open edges and you take one minus p to the number of closed edges, right? So when I write this, it's simply the number of open edges, right, zero is four close. So each open edge would contribute one. And then you multiply by q to the power number of connected clusters of open edges. And here, as usual, I forgot to divide by normalizing factor ZPQ to make it a probability measure.
00:37:10.964 - 00:38:03.804, Speaker A: So when you look at one q equals to one, you again get a percolation model. Because what happens here? So let's look at this. You just take all the configurations with the same weight. So you open with the weight proportional to p. So this is actually more general percolation than the one we described in the previous part. So here the probability you assign you open a close edge. This probability is p and one p.
00:38:03.804 - 00:38:47.136, Speaker A: So you don't necessarily throw a fair coin to open or close an edge. Instead you just throw this coin which gives probability p to opening and probability one minus p to close. Okay, so this is a percolation. But let me point out that this is again slightly different percolation than the one we considered before. This is percolation on edges. And so this is a typical configuration of something like this. So you see, you open some edges, you close some edges and you have a lot of clusters.
00:38:47.136 - 00:40:09.324, Speaker A: Here you see there is one huge cluster. So this is one huge cluster, so and lots of smaller ones. Now how do introduce boundary conditions here? So basically you fix condition of all edges between vertices on the boundary. So if you have two boundary vertices and there is an edge joining them, then you prescribe it's open or it's closed. And for different boundary conditions you have different prescriptions so that's all, that's how you assign boundary conditions here. But if you have an edge which goes from boundary vertex to non boundary one, that's still up for graphs that still can be open or closed. And then when you look at this number of clusters, you don't want to count clusters which are already formed on the boundary.
00:40:09.324 - 00:40:47.884, Speaker A: It's not interesting. They're already there. So you count the number of clusters not intersecting boundary. So that would be the weight of configuration with boundary conditions. So again, you take all this, but only for edges which do not know anything about boundary. Well, sorry, you do it only for clusters which don't know anything about boundary, and for edges which are not totally inside the boundary. Okay, so that's your boundary condition and so that's your probability.
00:40:47.884 - 00:42:10.304, Speaker A: And now I want to explain why these two models are essentially the same. And that would actually help us understand Potts model rigorously and prove something there. So this is another fancy probabilistic term to couple two things. Couple two random things means you define them on the same probability space. And so here we consider essentially two configurations together. So configurations are lambda configuration on non boundary vertices. And then you start opening, closing edges on the vision, not boundary edges.
00:42:10.304 - 00:43:18.444, Speaker A: Okay. And then another misplaced here. And then this would be our new model. So probability of, actually that was. Okay, probability of a configuration. So again, the configuration is assignment of both edges and spins of enclosing is one of the normalizing function. Probability of, well, p to the number of open edges, one minus p to the number of closed edges.
00:43:18.444 - 00:44:10.942, Speaker A: So far, nothing like nothing yet participates in coupling. But then that's where lambda comes in. You look at all the edges and your weight of the model is the following. The product of this should be zero. What does it mean? So you want to have the following. If lambda of x is equal to lambda of I, then the edge between them can be the open or closed. But if lambda of x not equal to lambda in your configuration, then you have to close the edge between them.
00:44:10.942 - 00:45:08.864, Speaker A: You cannot have open edges which join things of different spins. So that's the model. Okay, so again, you only consider configurations of all this. For instance, it means that you only consider configuration configurations where you have open edges only between vertices of the same spin. Okay, and so, well, that's what I wrote here. One way to achieve is the following. You select value of each vertex and then only assign lambda, which can only join vertices of the same one.
00:45:08.864 - 00:45:52.114, Speaker A: And then the weight of the configuration is again this. So if you only consider good assignments so you don't consider assignments which break this rule. Then the weight of configuration is really just this. You just look at the p to the number of open edges, one minus p to the number of closed edges. And now this is Ethereum of Robert Edwards and Alan Sokal. So they are physicists. But the theorem is, well, it's a rigorous theorem.
00:45:52.114 - 00:47:01.804, Speaker A: So remember that this thing, well has parameters p and q. Our original pot model had beta of this inverse temperature. So let's relate them. Suppose that p is one minus e to the minus two beta and the probability. So in our new model, the one which we just described, the probability of each fixed configuration on vertices, which is the following. So you look at all configurations of vertices and edges for which lambda is equal to lambda naught. So you look at all possible configurations.
00:47:01.804 - 00:48:36.124, Speaker A: We join only things of the same spin. And then the probability is exactly Qpod's probability e to the minus two beta sum of things with different spins, well, with normalizing cost. And on the other hand, for each edge configuration, probability of this edge configuration would be exactly fortune Castellan probability. So again, you sum up over all edge configurations where edges of all sort of lightest edge configurations, lambda and at where z, and you just arrive to this probability. So in other words, if you choose add according to Fk cluster model and then assign values from one to q to each cluster uniformly at random, you get pots model. Because again, originally when we looked at it, you see what we did? We did the following. We started with assigning just spins and then discuss incompatible configuration of edges.
00:48:36.124 - 00:49:25.244, Speaker A: What this theorem tells us that we can do opposite. We can first assign the edges and then do configurations of spins again uniformly at random. And you get both, because again, you start with this, you get to this, and then you specialize to the fixed lambda you get bought. Okay, so the proof, I think is logical to do after a ten minute break. So it's usually a ten minute break. I will see you at 11:00 I will stop the recording for now.
