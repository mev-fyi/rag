00:00:01.680 - 00:00:51.912, Speaker A: It's mostly stabilizing in terms of people entering the virtual talk, so that's great. I'd like to welcome everybody to the vector and Fields machine learning, applications and advances seminar. This is our first seminar of 2022, and many of you who have been here for previous years will know that it's a traditional at Vector to have some of our newest faculty members give talks. And it's my pleasure to welcome two of our relatively new faculty members. The first I'll introduce is not really new to University of Toronto. He's definitely been in the community and has many colleagues and friends in Toronto and at the hospitals. This is Mike Brudnow.
00:00:51.912 - 00:01:50.100, Speaker A: He is a professor in department of computer science at University of Toronto and he's also the chief data scientist for the University Health Network. And like both of our speakers today, he is a member at Vector and he's also scientific director at HPC for Health, which is a private computing cloud for Ontario hospitals. So I'm only giving a very brief bio today because we're gonna do short intros, but most recently, or maybe not that recently, but his latest degrees were PhD at Stanford in the computer science department, and then he did postdoctoral fellowships at UC Berkeley and a visiting scientist at MIT for a while. And he's received the Ontario Early Researcher award and the Sloan Fellowship, as well as the Outstanding Young Canadian Computer Scientists award. So, Mike, you are speaking first, correct?
00:01:50.292 - 00:01:51.348, Speaker B: Yep, I am.
00:01:51.476 - 00:02:02.544, Speaker A: Thumbs up. So why don't we go with you and then I'll introduce our second speaker, kind of as he's coming online. Welcome, doctor Michael Bruno.
00:02:02.844 - 00:02:44.434, Speaker B: Thank you. And hopefully no wrong one. Hopefully my screen is sharing properly. Everybody can see my screen. So thanks for having me at this seminar series. It's a bit weird to be introduced as a new anything in Toronto after having been here for 16 years, but I'm relatively new to vector as a faculty, so it's a pleasure to be here to give this talk. And the talk will be relatively light on the machine learning, on the hardcore machine learning, and more heavier on human factors.
00:02:44.434 - 00:04:11.840, Speaker B: And I will talk more about what are the key considerations as we look to moving some of the ML into actual clinical deployments. Because when you're doing this really, machine learning that works well with the human workflow will be much more successful than machine learning that doesn't. The accuracy of the model is important as well, but is often not nearly as important as to whether you built the model into the human factors in a way that the clinician will actually want to use it. So we've all heard about the promise of what AI or machine learning will do in medicine. It can lead to accurate diagnosis, faster workflows, and more equitable, and more fair and just better, better overall healthcare for patients, and maybe better workflows for the doctors. But the reality is that how AI is often deployed in hospitals leads it to be underutilized. It's poorly designed, that makes it difficult to use, and also it's very hard to transfer the learnings from one hospital or healthcare system to another just because the data is so disparate and the workflows are so, so different.
00:04:11.840 - 00:05:39.274, Speaker B: This could even be applied to workflows within a single hospital, between two different clinics, or even two different physicians within a single clinic. And sort of the goals of our work is to really, when we were thinking about designing ML, to design it around the clinical pathways, and thinking of the how to think about the system user, how to think about what the physician is doing and or the clinician, and how their work is going to be affected by whatever machine learning intervention that we want to put in. Because very often what we see is very successful machine learning systems that have amazing aurocs, or precision recall trade offs, and which are not really useful to the physician based on how their data is acquired. For example, the data comes in way too late for the system to be actually useful. And another important thing here that I want to mention, it's really important to try to solve one problem at a time. I'm a bit afraid of systems that try to solve all of precision health, or machine learning that solves all of medicine. And even for things which are relatively generic, we are trying to really approach them one clinic at a time or one workflow at a time.
00:05:39.274 - 00:06:19.022, Speaker B: So in the first portion of this talk, I want to talk about how we're working to integrate AI into electronic health records. And I'll make a further note that this is really for taking clinic notes. There's lots of other ways that AI can be integrated into electronic health records, but we wanted to narrow the scope. And as we went on, we narrowed the scope even further to just some specific clinics. And if you know anything about electronic health records, you probably are aware that doctors hate them. There is lots of press about how bad electronic health records really are. They're horrible.
00:06:19.022 - 00:07:25.334, Speaker B: You know, there's things in New York Times, New York Times Magazine, or New England Journal of medicine. This is one of my favorite papers, where they talk about how to make EHRs work better and how to remove fields that don't really need to be documented from forms that used to previously require them, or something from a few years ago in the New Yorker. Why doctors hate their computers. It's really amazing how the electronic health record is something that doctors absolutely hate. At the same time, it's something that's completely necessary because we need to have good documentation of clinical encounters in order to not just do the ML, but just for the healthcare system to function. So obviously, AI is changing how we interact with computers. Things like predictive typing on phones or just audio controls have had a huge impact on.
00:07:25.334 - 00:08:13.904, Speaker B: On our interaction with devices in our daily lives. One of the things we want to think of is how this can be transferred into the clinical realm. To start off with, we actually ended up doing a survey. We did a survey of 66 Ontario physicians. Now, this is not an unbiased sample by any stretch, but we made the survey available through the Ontario Physician association that it was available to quite a number of people. This is still going to be people who are interested in electronic medical records and data in general. So 76% said that they type into EHR systems during the clinical visit.
00:08:13.904 - 00:09:27.806, Speaker B: And about half of those 32% said that their current practice impairs the interaction with the patient. So typing and talking to the patient at the same time is not something that's that easy for a physician, and is actually often set up in a way that makes it difficult for them to maintain eye contact. Most of them thought that things was the ability to take photos and draw during their. During their visits. During their visits with their patients, they varied a little bit more about whether they want to write on a tablet, some like that feature, others didn't, and on whether they want to have video recording, where some felt there could be an invasion of privacy to do video recording. But one of the things that they all agreed on is obviously that it's very important that whatever things that they have any kind of decision support or other additional tools that are built in need to be integrated with the EHR systems. And this is actually one of the big difficulties when deploying AI in the clinic.
00:09:27.806 - 00:10:08.150, Speaker B: You build an additional system that means the physician has to take their data, which they already have to put into the HR system, copy it into your machine learning system, run it there, look at the result, copy that back into the EHR system. And that extra step is going to really reduce the utilization of any system that you put in by a significant amount. It's just extra work that the physicians don't want to do. They hate double entry or triple entry of data. They're likely already doing double entry. So your ML system is going to become triple entry. We also asked physicians about what are some of the key features they want in the tools.
00:10:08.150 - 00:10:54.404, Speaker B: So, for example, many of them spoke about the ability to edit previous medical notes or taking photos and how these were very useful. And while recording video was a little bit more split. And we also asked them about what they're comfortable with already. And obviously, most of them are very comfortable with taking photos a bit less. So drawing on a tablet, but mostly we're still okay. Recording video they were even less comfortable with. Voice recognition was fine while writing medical notes was a bit of a more concern, but they were about half were willing to give us something like this, a shot.
00:10:54.404 - 00:11:51.364, Speaker B: So really, the dream of a lot of these positions, and the dream that sort of, to some extent, being proposed by several commercial entities, is that, well, we have a patient encounter the patient, they're talking to each other. That creates a speech signal. We do speech recognition and get some kind of text. Then we do speaker diarization or speaker recognition, and have a conversation, do named entity recognition, slot filling, and get them like symptom, time period, and medication. And all of that goes through classification and summarization, and we get nice medical record. Chief complaint. 25 year old male presents with a complaint of headache.
00:11:51.364 - 00:13:09.774, Speaker B: Patient relates history of headache for past two days. So that's the dream. All of this happens completely automatically, based on a machine learning model that's trained to go from speech to notes. Now, in reality, we're not ready for that. Each component of this pipeline is not good enough, and more work is needed to build the pipeline. The intuition, we had that, well, what if a physician had a physical device? Could we can use it for additional nodes, images, pictures, and decision support, and also to help them complete some steps in here where we're not really sure what this, what in order, what the system should be doing. So this led to the development of phenopad, a system that we have built and we have actually deployed as a research study at the hospital for Sick Children, where we added, we put in the exam room a mobile device, a microphone array attached to a raspberry PI microphone array was used to enable better speaker diarization because we could get audio directionality, as well as giving a surface book and pencil to the physician.
00:13:09.774 - 00:14:09.454, Speaker B: You can see the physician is using it here to draw while or take notes while talking to the patient. And the advantage of the surface book was that it had both a keyboard interface and a stylus interface. And really, a physician was free to use whichever one they felt more comfortable with. And these devices controlled each other. There's control via Bluetooth, and all the data would flow into our own secure server inside the hospital where the data analysis was happening. So on the phenopad, we were doing handwriting drawing, taking photos, recording videos, and typing, while the Raspberry PI would do audio recording and beamforming, computational processing and data storage. So, together, these gave you all of these key features of building a machine learning system for clinical note taking.
00:14:09.454 - 00:14:56.554, Speaker B: This is how it actually looked like in practice. This conversation. This is a sample conversation, not a real clinical one, but this is the speech. It's diarized. Who is talking when? Here, you can actually see on the right, the system identifies key clinical terms that come up. The physician can easily select yes by just clicking on one of these with a stylus or a mouse to say that's something that actually is true and accurate representation of something we talked about. Rather than a spurious, you know, a spurious term that was misidentified, they can also easily add an additional term that's not relevant, that was not caught by either by the audio system.
00:14:56.554 - 00:15:43.620, Speaker B: The system has a different screen for doing. For doing clinical decision support. It gives you other suggested features to look for that may have been missed. And the differential diagnosis. Now, these have all been picked from genetics, and this is because our initial deployment was in the genetics clinic. And this actually talks to the fact that we need to customize the system for every single clinic where we deploy it. So the differential diagnosis for, you know, for somebody who has pharyngitis, fever, fatigue, and anorexia, and pain in a general practitioner's office should not include familial.
00:15:43.620 - 00:16:30.648, Speaker B: Well, it could include, but is not likely to be familial, lifelong persistent fever. But that actually is a reasonable differential if you're seeing somebody in the genetics clinic. Phenopad also allowed for note taking with a stylus. And there was handwriting identification, which was then transferred into notes. And there was an interface that was built to allow for identification of ICD ten terms. So billing terms that can then be easily selected and added to the patient medical record. And the idea was to simplify the billing process, as this is one of the biggest requests of physicians.
00:16:30.648 - 00:17:19.268, Speaker B: This is something that they really are interested in simplifying where they spend a lot of their current time. Now, as we were building, this was much more than just like a pipeline of existing things. There were aspects where we developed our own machine learning capabilities. So for named entity recognition, Arian or Bobby built a tool that would allow for identification of phenotypes using both ontology and the machine learning. Something where we built embeddings of the ontology into high dimensional space and related the terms of the ontology to each other. So you can see, for example, this ontology where these are child relationships. So cancer is a subtype of phenotypic abnormality.
00:17:19.268 - 00:18:10.304, Speaker B: Abnormality is a subtype of phenotypic abnormality. And retinal neoplasm, or cancer of the retina is a child of both cancer and of retina abnormality. So in reality, to get the embedding for retinal neoplasm, what you have to do is you add up it and all of its parents to figure out what the embedding is. So this is a new kind of machine learning that we had to do developed to do identification of named entities in clinical text. We also built a method to do abbreviation disambiguation. So a lot of abbreviations have multiple different meanings. So when you see IVF in the clinical text, it could mean intravenous feeding, it could mean intravenous fluids, it could mean in vitro fertilization.
00:18:10.304 - 00:18:54.664, Speaker B: I think there may be one more. And actually differentiating which one it is requires machine learning. And this is machine learning for which very little training data exists. So we built a semi supervised method that does this by supplementing available terms with other related ontological entities. And this is something that just recently got published in Nature comms. This phenopad system, we actually evaluated in 25 sessions at sickkids, where a physician would come in and use phenopad to take their notes for that specific clinic visit. And the results were very positive.
00:18:54.664 - 00:19:45.896, Speaker B: The patients, 90% indicated that they have more eye contact with the doctors and that the doctors aren't really distracted by the tool like they may be by the electronic medical records. 80% of the patients felt more natural and comfortable when the clinician was using phenopad rather than the regular computer for the, you know. So there were also advantages that it's out of the way for both patients and clinicians. I apologize, my animation is not working properly. It improved the physical positioning and ability for the clinician to engage with patient. The clinician would be able to face the patient more directly. And they actually said that the notes that they were producing were with a much better quality.
00:19:45.896 - 00:20:43.024, Speaker B: Some of the disadvantages of a system like phenopad multiple times. They noted that performance was not good enough for what they wanted. So there was a lot of, from the voice recognition, from, there were mistakes from the named entity recognition from the speech, from the audio analysis. And part of this is that we were not able to use the best and the most high performance models, especially for audio, because we could not send our data to commercial cloud. So all of the tools that we built had to be trained and deployed locally. Physicians also complained that they were not used to writing on the tablet, and some did not like that experience, so wanted to stick with the computer form factor. And many noted that writing on a tablet is less efficient than typing, which probably is true for those.
00:20:43.024 - 00:22:11.534, Speaker B: So, just to summarize, AI enabled note taking, we wanted to use technology to assist patient interaction, capture everything, notes, speech, and video, generate records, things that are easy to browse, locate important information, automatically provide clinical decision support on the fly. And really, there needs to be EMR integration for a tool, even like phenopath, to be truly useful to the physicians. So, very quickly talking about the sepkan project, something where we're working on to integrate AI into emergency department workflows with something called machine learning, medical directives. And it starts with this little video. So a kid rolling down and then slamming into a wall, and at the very end of the video, you hear somebody saying, call 911. So the question comes up, what will happen to this kid once that kid presumably is taken to the local hospital emergency department and is seen there? Under the current emergency department workflow, the kid would be assessed by a triage nurse. They would take blood pressure, respiratory, respiration, age, symptoms, enter the triage data into electronic health record, and then the patient would be told to wait until an assessment room is available.
00:22:11.534 - 00:22:48.714, Speaker B: After the assessment room is available, the patient goes into the assessment room and waits for a physician to come. The physician comes, looks at the patient, and says, okay, well, it looks like you've hurt your arm. Let's go get an x ray. You go get an x ray, come back, wait for the physician to show, come back to you. And the physician says, okay, we looked at the arm that may be fractured. Maybe it's not fractured. And correspondingly, we will have appropriate the results or appropriate diagnosis and treatment.
00:22:48.714 - 00:23:49.904, Speaker B: So really, this is very inefficient. Based on the initial HR triage data, one could pretty easily conclude that maybe we should be getting an x ray, and maybe an x ray could be gotten instead of sitting around and waiting in the waiting room and then in the assessment room. So nursing directives is a tool that's currently being used to short circuit this process. So nursing directives basically means a doctor can sign over their privileges to do something to a nurse. So doctors could agree a nurse can order an x ray for us, and then the nurse will be able to order that x ray, the triage nurse, and that way the doctor will get the patient already with the result. The problem with this is that there's only a specific set number of nursing directors that could be in place in one time. People do not have the capacity to.
00:23:49.904 - 00:24:59.874, Speaker B: The nurses do not have the capacity to handle any more than a specific number because otherwise, basically, it makes them into doctors. They have to be able to think about all of these possible diagnoses, all of these possible conditions at the same time. It turned out that based on the triage note, we can identify some of the most common ed conditions. Things like pneumonias, urinary tract infections, clinical significant infections. So basically things that require a blood test and forearm buccal flexors, which is just one type of x ray, that one type of injury that requires a hand and lower arm x ray. So these are the number of patients we saw at sickkids over the course of a year with these diagnoses and looking at the triage data, vital signs and so on, including some text around reason for visiting complaint. Multiple types of machine learning were able to have very good positive predictive values for, for all of these, for pneumonias, for utis, for CSI's and so on.
00:24:59.874 - 00:25:53.414, Speaker B: So we actually achieved very good ppv's compared to the clinical ppv benchmarks. And this is just basically how good physicians are at identifying them. And the reason is that we're just really looking at a different place in the rock. So with machine learning, we're looking somewhere here where we're looking for a really good true positive rate, while our false positive rate is so. So while the physician clinicians can't really tolerate any false positive, so they're looking like she's flipped, but on the very tail end of the curve. So because clinicians can't really false, false negatives. So this is just examples of curves in terms of.
00:25:53.414 - 00:26:47.336, Speaker B: For things like predicting a diagnosis of buccal fracture and predicting the need for wrist x rays or forearm x rays. One of the things to notice is that the second curve is a lot better. We're much better at predicting whether a patient will need an x ray as to whether they actually broke their wrist, because obviously you need the x ray in order to determine whether the wrist is broken. There are lots of other injuries which will have the same symptoms. Deploying machine learning medical directives. The way it would work is obviously a machine learning model would be able to take the output of the triage data and identify confirmatory tests, have the patient to wait for results and be assessed by the healthcare provider. If a machine learning medical director model doesn't trigger somebody goes into current standard of care, and they're just seen as they would be.
00:26:47.336 - 00:27:57.124, Speaker B: Naturally, as part of doing this, we obviously look for things like bias and explainability. So we looked at every single one of the features and look if there are gender biases in our ability to predict certain diagnoses and tests, and there seems to be a bit of a bias with urinary tract infection. So we're very carefully looking into that, make sure that we can create a machine learning model that is not going to perform worse for certain large class of the population, and also using shap values to explain to the physician what it is that's making us think about the fact that this patient needs to get a specific testing. So there needs to be, I think, a lot of this until people are much more comfortable with machine learning, ordering specific testing. So overall, the idea of ML medical directives is like nursing directives. They help order appropriate testing without actually diagnosing the patient. They can predict the test much better than they can the cause of the why the test would need to be ordered.
00:27:57.124 - 00:28:56.334, Speaker B: And the idea is that with MLMD, we're hoping that low risk clinical operations can be automated while the high risk ones are still requiring human care, human intervention. We've estimated impact of deploying this would be about 2 hours saved, waiting for about 20% of the emergency department visits based on the time points of how long it takes currently for patients to be seen and to have the right testing ordered. So it could be a significant time saver if we're able to get this deployed. Right now, the biggest bottleneck is simply legal, legal liability for something like machine learning medical directives. So finally, just want to acknowledge the people who did all the hard work. The phenopad work was led by Jishwan Wang and with the abbreviation ambiguation, and named entire recognition by Marta Skretti and Arian Arbabi. And the medical machine learning medical directive was led by Devin Singh in collaboration with Andrew Goldenberg's team.
00:28:56.334 - 00:28:57.734, Speaker B: Thank you.
00:28:59.374 - 00:29:38.876, Speaker A: Thank you, Mike. So I think we have some time for some questions. So I'll just open it to the floor if anybody wants to put their hand up or their virtual hand up, or you can also come on camera. I can get it started while people get their questions ready. So one thing I was wondering, Mike, is I noticed that in the pheno pad, the surface tablet was doing a lot of work. There was all these activities being performed on it. But you had a completely separate device for the mic array.
00:29:38.876 - 00:29:50.424, Speaker A: And I was wondering, is that because the beam forming and source separation is just so important to that application? Is that why you isolated that on one device?
00:29:51.484 - 00:30:45.514, Speaker B: Two things. One, beam forming does help a bit with, with the speaker diarization, and especially because we don't have that many training samples from the patients before they show up, and they can be a little bit further from the device. The second is we noticed that the microphones that are built into the surface at least, but I think in general, into laptops are actually pretty poor quality. They're often facing towards the speaker, towards how I'm right now speaking to my computer and getting data from that side. There are speakers on the other side of the device. The quality dropped significantly and the accuracy of all the downstream steps fell dramatically when we had poor quality audio data. So we did identify that as a need.
00:30:45.514 - 00:31:10.884, Speaker B: And obviously, it's not an easy decision. Having one more device in the exam room means it's that much more difficult to set up a room to be used. We couldn't just, for example, give the, the device to the physician, say, walk around with it. They had to have specific rooms that were preset up, which is where we did all of our testing of the device and all of the clinical exams.
00:31:12.264 - 00:31:21.576, Speaker A: Quick follow up. Is that raspberry PI doing the speech recognition because you mentioned not being able to send it out to the cloud, or is that being done somewhere else?
00:31:21.720 - 00:31:55.744, Speaker B: It's being done on a server, but on a server in the hospital, the data center. The raspberry PI is just controlling the. Controlling the speaker and communicating via bluetooth to the phenopha, to the tablet, to the windows surface machine, which is in order to enable easy ability to turn it off. So again, there were some times when the physician would say, we want to turn it off, or the patient would say, I would like this not to be captured. And there would be a button on the user interface to turn off the audio at that point ones.
00:31:56.404 - 00:32:12.104, Speaker A: Okay, does anybody else have any questions? Okay, I see homa with a hand up.
00:32:12.644 - 00:32:28.374, Speaker C: Sure. Thanks a lot for the, for the interesting talk. I have a question about the nursing directive and ML directive. So do you expect that the ML directive performs better than the nurse stereotype assessing a patient?
00:32:29.434 - 00:33:19.330, Speaker B: So that I think the answer is the scale isn't the scale, you know, if at sick kids, for example, there is no nursing directive for ordering x rays, but there is a nursing directive for treating wounds. So nurses are allowed to prescribe antibiotics and apply them to open wounds. So that's to save the time of waiting for a physician to allow for something like this. The issue is the number. If you can have 1234 nursing directives, and nurses will be managing it just fine. Once the number of nursing directives becomes overly low, like, you know, goes into the dozens, their ability to perform. Each one of them goes down.
00:33:19.330 - 00:33:39.254, Speaker B: When they see the patient, they have to remember, oh, there's a nursing director for this type of patient. Yes, this patient meets all of the eligibility criteria. They have to go through basically a checklist, at least in their mind, but probably on paper as well. Like, yes, the patient has this. Yes, the patient has this. Yes, the patient does not have this. So, you know, and so.
00:33:39.254 - 00:33:53.904, Speaker B: And that it becomes trickier to just do en masse. So while nurses are very competent in doing individual directives, probably better than our machine learning algorithm can be, scaling would be an issue.
00:33:54.844 - 00:33:55.944, Speaker C: Thanks a lot.
00:33:59.364 - 00:34:05.284, Speaker A: Okay, I think it's a good time to move on to our next speaker. Mike, would you still be able to take questions in the chat if people.
00:34:05.404 - 00:34:06.424, Speaker B: Absolutely.
00:34:07.484 - 00:34:10.004, Speaker A: Great. So just pop any more questions into the chat.
