00:00:00.160 - 00:01:23.934, Speaker A: Let me start with an advertisement. If you're subscribed to the paying version of this class, then you won't see this advertisement. But since you guys didn't pay anything, you have to watch the ads to continue. There is a workshop on non commutative geometry at the end of the semester, this semester here, which is being organized by Masud Kalkali and me. And this, we made some space. We have a sort of temporary semi schedule, and there will be some space for little mini student presentations. Just the most modest little talks, if you're interested, available exist.
00:01:23.934 - 00:01:55.074, Speaker A: I'll just say student presentations if you're interested. I thought this was the perfect place to advertise this. I looked on the, on Google, what's it, YouTube. The first lecture had something like 250 views. Most of those are probably bots, I guess, but then it went down 90 and 50 or something. But anyway, there's a bunch of people out there on the Internet who a little bit curious, who may see this advertisement. And you know how it is with ads.
00:01:55.074 - 00:02:50.842, Speaker A: You just need like 1% of the, of the punters to bite and you're in business anyway. If you're interested, please email me and Masood, here's Masood. Masood masoudoud at UWO. Thank you. And me, the USA. I have lots of email addresses and all email goes to the same place. So don't worry if you use a different email address, you don't have to resend it to this address.
00:02:50.842 - 00:03:36.546, Speaker A: Okay, so that's the advertisement. Oh, we're trying to get everything sorted out ASAP. So let's say you have to do this before the end of the day on Monday. This feels time zone. Eastern FDT feels daylight time. Yeah.
00:03:36.610 - 00:03:38.042, Speaker B: How long have these talks?
00:03:38.218 - 00:04:19.334, Speaker A: That depends a little bit on logistics. There are lots of things in play right now, as Jamie knows, so the schedule is a little bit in flux and we're trying to nail everything down. They will be short between ten and 20 minutes, but it depends on how many of you guys bite. Purchase one of these valuable slots, and it depends on some other things. How many of the speakers are ravaged by COVID? All sorts of things are unknown in these dark times that we live in. All right, very good. We were doing.
00:04:19.334 - 00:05:04.856, Speaker A: Now the lecture is going to begin. So if you muted the ad, then, well, you have to unmute because you won't hear me saying that. But no more ads. Okay. We did a couple of calculations last time, and the calculations involve this famous scaling relation and in particular iterations of the scaling relation and there's one more calculation of that time to type, which I'm now going to describe to you. I'm going to describe the end result of the calculation. I'm not going to do the calculation because I don't want to open up those wounds from the last time.
00:05:04.856 - 00:06:29.524, Speaker A: All of those iterated formulas and so on, it's the same. So I'll make one comment about this calculation in due course. And anyway, so this is built on following idea of what I'll call a symbol family of order, say m. And let's, for us, we're just going to take m to be an integer. And what it is, it's like those scaling families that we discussed before, but then you throw away everything except what you have at t is equal to zero, which is a slightly funny thing to do because what you have at t is equal to zero in a scaling family is what is sort of implicitly supposed to exist, but which is not given to you initially. So okay, we'll see how this works out is what. It's a family of operators, a smooth family, let's call them s, the symbol.
00:06:29.524 - 00:07:26.204, Speaker A: So these are operators on rn, let's say, and there's a whole bunch of them. There's one for each point in rn like that. And they should be a smooth family in the sense we're discussing. There's no t variable here, no t parameter. And these guys also have to have a rescaling property, which is to say that for every lambda bigger than zero, if you rescale by lambda, the operators not t but s omega. You might remember that when I defined r lambda, it just means you're rescaling around zero. So omega equal to zero.
00:07:26.204 - 00:08:23.270, Speaker A: What you get is just s omega bag up to a fact, which is this degree, I should say degree plus some error term as usual, which will depend on omega, of course it will sound everything. I think everything is the way I want it. So we're rescaling just around zero. Yeah, yeah. If you're a geometer, then there's one of these on each tangent vector space. I missed off a crucial thing. Okay, but let me, I'll come back to that in a moment.
00:08:23.270 - 00:09:58.512, Speaker A: Let me just write down the thing, which is probably obvious if you look at all of these operators as omega varies. It's a smooth family of smoothing operators like we discussed last time, so that you could have guessed and maybe what you might not have guessed, although if you're a geometer you probably would have, is that all of these operators here are supposed to be translation invariant. Yeah, that's the definition. And the reason for sticking in translation invariances, that if you start off with one of our scaling families, s sub t, excuse me, we were calling them a's, a sub t. And if you build those operators a sub omega t out of the s sub t's by rescaling the s sub t's, and then if you extend to t is equal to zero, which it's part of the definition that you're allowed to do, then those extended operators are automatically translation invariant. Why are they automatically translation invariant? Well, if you dutifully did the homeworks in the notes, you'll have seen why. It's just a little trick to see that those guys are translation invariant.
00:09:58.512 - 00:10:51.920, Speaker A: So the operators that come to us at t equals zero from all of the discussion we've been making are always translation invariant. We saw this in real life when we looked at differential operators. When we took a differential operator d of order m, and then when we defined d sub t to be t to the m times d, we saw that that was a scaling family. It gave rise to a bunch of operators, d sub omega zero, which is one of these. And what d sub omega zero was, you took the differential operator, you froze the coefficients at omega, and then you drop the lower order terms. Whether or not you drop the lower order terms after you've frozen the coefficients, you obviously have a translation invariant operator. So that doesn't prove that you always get a translation invariant operator, but it indicates that you do in that case.
00:10:51.920 - 00:11:12.450, Speaker A: In fact, in all of the other cases we looked at, you get a translation invariant operator, and you can easily prove it in general, as discussed in the famous notes. Okay, so that's why we put translation invariance here. Nope. On the nose. On the nose, actually. Literally. Exactly.
00:11:12.450 - 00:12:25.716, Speaker A: Translation invariant. Great. So each one of these scaling families of order m, degree m. Trying to teach myself to call these guys. They're more like graded things. There's a specific number that you attach to a scaling family gives a rise to a symbol family, which we'll refer to as the symbol of the family. And if you're keeping track because you know something about pseudo differential operators and differential operators, we're talking here about principal symbols.
00:12:25.716 - 00:13:16.948, Speaker A: I'll show you an example in a moment. I guess we already saw one. I spoke about differential operators, and this notion of symbol corresponds to principal symbol. So s of omega is just what you get by applying this rescaling operation and then extending by continuity to zero. Like that. Hope also good. So yeah, let's just look at one other example.
00:13:16.948 - 00:14:42.704, Speaker A: Maybe it's a little more convincing than before. We looked at a couple of examples of convolution operators and in this example I'm just going to stick them together. So one of the things we looked at was the function one over x. And we saw that to get a reasonable theory so we don't have to worry about infinity, we should also put in a cutoff function like that. We also looked at the logarithm function like that. Again, stick in a cutoff function so you could add them together. And in order to make us scaling family, we saw that you should divide by one of the absolute value of t.
00:14:42.704 - 00:15:15.580, Speaker A: That has to do with the relationship between the way you rescale integral kernels after you do a rescaling. This is some Radon licker dim factor which disappears. So because we're talking about the at operators, the one over t occurs there and it's common to both. And I'm not quite done yet. What we saw is that this first thing is a degree zero scaling family. And the second thing is a degree one scaling family. Excuse me, correction, typo.
00:15:15.580 - 00:16:03.780, Speaker A: A degree minus one scaling family and you're not allowed to add scaling families unless they have the same degree. So in order to make things match up, you have to promote this from a degree minus one scaling family to a degree zero scaling family. Otherwise you're not allowed to do the plus right here. Okay? And we saw the way to do that is just to multiply by t. So this thing here is a degree zero scaling family and so it has a degree zero symbol. And in the degree symbol what you're going to do is. Well, you have to do this rescaling.
00:16:03.780 - 00:16:39.684, Speaker A: You have to build the omega t's first of all, and then evaluate at t is equal to zero. When you do the rescaling, two things happen. First of all, this one over the absolute value of t disappears. It's a radon negative factor and it just disappears. The second thing is that all of the x over t's become x's. But this t isn't going anywhere, that t just stays. So after you do the rescaling and you look at the operators a omega t, what they look like is that the radaron nicodem factor is gone and all of the x over t's have just become x's.
00:16:39.684 - 00:17:47.726, Speaker A: But this t isn't part of an x over t, it's just t and so just stays. And so the operator, a omega t, looks like convolution by, forget about, let me just write that down, is convolution by, as I say, the one over t go away. And the first, and the, sorry, the one over absolute t goes away. The x over t just becomes x. So you have something like this, but then t log x sigma x, that's what a sub omega t is. And so when you evaluate that t is equal to zero, the lower order term goes away. So this construction, um, doesn't, doesn't see the lower order terminal in this example any more than it did in this, in the partial differential operator example.
00:17:47.726 - 00:18:26.844, Speaker A: Both cases, the lower order terms go away. And there's no avoiding that. You may say to yourself, well, do something different here. How about square root of two? I don't know. How about something else? Well, there's no avoiding that. And that's the content of the next theorem, which says that if you start off with a scalable operator, let's do it this way. Let's not talk about operators yet.
00:18:26.844 - 00:19:31.064, Speaker A: Let's talk about scaling families. If a t is an order degree scaling family, M doesn't really matter what it is. It has to be something for it to be a scaling family. And suppose one of the operators in this family, according to the scaling relation, doesn't matter which one you choose. Suppose one of these operators is a smoothing operator. It's given by a smooth integral kernel, not non smooth things like this, but a smooth integral kernel. Then, when you make this symbol family, which is a symbol family of degree m, it consists entirely of smoothing operators.
00:19:31.064 - 00:20:41.284, Speaker A: It's a smooth family of smoothing. What that says is that if you, from the point of view of scalable operators, if you start off with any scale Abel operators, any one of these things up here for t equals one. And now you try to manufacture a really cunning scale ing family from that scale Abel operator. It doesn't matter what cunning construction you get, by the time you get to t is equal to zero, the, the symbol that you get is always the same. Not exactly, but up to smoothing operators. Okay, so it's not obvious if you start off with a scale Abel operator, which means there's a corresponding scale in family, and then you evaluate that scale ing family at t equals zero to get a symbol family. The symbol doesn't obviously depend just on the original operator, because you built it out of the corresponding scale ing family.
00:20:41.284 - 00:21:42.228, Speaker A: It's not obvious that it just depends on the scale Abel operated in you started with. But that's the way it is, at least up to smoothing operators. Each scalable operator of order M has an order m symbol, degree m, principal symbol, which is one of these things here, thanks to this result. How do you prove this result, Nigel? Well, it's a consequence of the scaling relation. And because we went through two exercises with the scaling relation, and because I don't want to write down all of those formulas again, I should have fit this into the last lecture, but I didn't. I just refer you to the notes where this thing is proved. Just to make it a little bit enticing, let me just say there's one interesting quirk here, which is that the case? So now you start to do the proof.
00:21:42.228 - 00:22:24.644, Speaker A: And we saw that negative order operators are sort of different than positive order operators in respect of some details when it comes to, for example, integral kernels. And this is kind of the same. The case where the degree is bigger than minus n is the thing that sort of comes easiest. So it's sort of opposite of what we were talking about before. We said if the degree is less, the order is less than minus n, then the integral kernel is a continuous function and so on. So we made some conclusion out of the order being less than minus n. But here, if it's bigger, it turns out that the argument's a little bit easier.
00:22:24.644 - 00:23:27.964, Speaker A: And the whole argument can be reduced by considering pushing up the degree and lowering the degree. Considering something like this, for example, del alpha a t don't even have to do it like this. But this is the sort of thing that we saw before talking about scaling families. So I should say something like t for the alpha plus beta. So this is a composition of three scaling families. And so it's also a scaling family of degree alpha plus beta plus m. And by studying this family, which you can make to have degree bigger than minus n, doing this one first and then, and then integrating, you can recover the general result.
00:23:27.964 - 00:23:50.604, Speaker A: So it's just a little interesting that it goes the other way around. Maybe that's enough to encourage you to look at the notes. It's not difficult, it's just cumbersome. And the fact of the matter is, there are also some other cumbersome arguments today. Sorry about that. But they're different cumbersome arguments. So just one annoying argument per day, please.
00:23:51.074 - 00:24:01.322, Speaker B: Why is p equals zero? I mean, why is zero single dotted? Special places where something happens? We don't seem to be using the boot property in the real numbers very much.
00:24:01.458 - 00:24:51.974, Speaker A: Well, what we're doing is we're stretching out, as we saw originally, that these operators, these rescaling operators, r of t have the effect on functions of stretching them out to make their graphs more spread out, make the functions more slowly varying. So we're looking at the infinite stretching limit in which all functions become asymptotic to constant functions. Yeah, that just means we're stretching around some other point. We're treating, if you like, we're treating rn not as a vector space, but as an affine space. So we're not paying particular attention to any origin. And according to the usual fashions of mathematics, we're doing so by considering all points as origins all at once. And yes, so zero is, t equals zero is not in the multiplicative group of positive real numbers.
00:24:51.974 - 00:25:06.714, Speaker A: It's some sort of limiting case of a positive real number. And that's why it's special. T equals two. Not interesting. T equals one. I mean, you didn't do anything. T equals one.
00:25:06.714 - 00:25:31.360, Speaker A: Yeah, I mean, I guess that's, it's special. It's kind of important. Sometimes the best thing you can do is nothing at all. And. Okay, so t equals one is the next most important after t equals zero. All right, so we'll talk more about symbols. Oh, I guess we're going to talk more about symbols instantaneously.
00:25:31.360 - 00:26:44.904, Speaker A: Yeah. So here's a theorem that I won't be proving. You don't even have to take my word for it. Read the notes, you'll see it's not a, it's not a super crazy thing. So if you believe the theorem, now you can talk of the degree m, if you like, to include this word, principle, symbol, family of a, order, and scalable operator. If you have an operator a which is scalable of order m, it may be extended to a scaling family of order m. That's the definition.
00:26:44.904 - 00:27:18.334, Speaker A: Like this one up here. We extended it in this way. Then you can evaluate, after you do the a omega t thing, then you can evaluate t equals zero and you get a symbol. And what you get only depends on the a one that you first thought of. Up to smoothing operators, because two different families extending a one will have the property that their difference is zero. At t equals one, and therefore at t zero, you get a bunch of smoothing operators. So this thing is well defined.
00:27:18.334 - 00:27:33.846, Speaker A: This is always a little bit annoying, but well defined. Modulo smoothing operators, smoothing families. Do you have a question?
00:27:33.990 - 00:27:51.554, Speaker B: Yeah, I think if I have a between m minus one scalable operator, I can take m scalable operator. And what you're saying is the symbol will actually be a split m. Yeah.
00:27:51.914 - 00:28:14.450, Speaker A: Right. If you take an order like you see here, somewhere here. If you take an order minus one operator, you can. It's also an order zero operator. It's a filtration, not a grading. But if you treat it as an order zero operator and then calculate its order zero symbol, then you'll get zero. Or a smoothing operator, its order minus one symbol exists because it, sorry, degree minus one symbol exists.
00:28:14.450 - 00:29:09.046, Speaker A: And that's something interesting, namely, it's this operator without the table. This operator. Yep, same thing. Yeah. All right, so let's call this sim of a. So it's actually a collection of operators not parameterized by t, but parameterized by points of rn. The main theme of today's lecture is to figure out something which goes in the reverse direction.
00:29:09.046 - 00:29:54.544, Speaker A: Suppose you have a symbol. Can you build an operator from a symbol? Well, yeah, if the, if we're talking about differential operators, the symbol order m, symbol of an order m, partial differential operator is just all of those operators you get, like I said, by dropping the lower order terms and freezing the coefficients at omega. Or you could do it the other way around. But once you have all of those frozen coefficients at all of the omegas, you can treat the coefficients as actually functions of omega. And so you can reconstitute an operator. It's not the operator you started with because you dropped the lower order terms and they're gone. They're not coming back, just like the logarithm here is not coming back after you pass to the symbol, it's just gone.
00:29:54.544 - 00:30:32.682, Speaker A: But you get something. And what you get is an operator, which if you then take its symbol again, you'll get back the symbol you started with. So what you see in the world of differential operators is a process called up, which goes from symbols to operators. And it has the property that op of symbol of D is not d. But on the other hand, symbol of op of symbol is symbol. So it's a left or right one sided something or other inverse, but not an inverse in the other direction. No, that's not the right way.
00:30:32.682 - 00:31:38.854, Speaker A: You'll see almost instantaneously what we're going to do. And it's a little bit different from that. Okay, but so the main, in fact, let me, so the main topic or construction goes like this. You start off with a symbol family, maybe in degree m. And so this arrow is what I want to discuss. And what's going to come out of this is a scaling family. So this will be a collection of operators parameterized by nonzero numbers, just like we know.
00:31:38.854 - 00:33:38.534, Speaker A: So this is now a degree scaling firm. And like I said if you do this, you compose them the right way. You get back exactly to where you started. Simulation of up of s is s. And this construction pairs nicely with the following little lemma was my favorite eraser here, which says the following thing. If you have a degree m scaling family, and if its symbol is just the zero family, theres nothing there. Then you can divide by t.
00:33:38.534 - 00:34:25.080, Speaker A: There exists another scaling family, which obviously is going to have to be one over t times a of t. So with the property that t times this scaling family is this one. So it has to be this. And so the assertion is that this is our scaling family of degree m minus one. We saw this the other way around. If you have a scaling family of degree m minus one, and you multiply it by t, then it's the scaling family of degree m. It's not a terribly interesting scaling family of degree M because its symbol is obviously zero.
00:34:25.080 - 00:34:51.564, Speaker A: But it is a scaling family of degree M. And you can just reverse it. Why can you reverse it? Well, you can reverse it because we're not doing what those guys out there are doing. I'm not talking about eating all of those cakes and having fun. I'm talking about sea star algebras. We're talking about smooth functions, not continuous functions. And if you have a smooth function, an f of zero is zero, then it's x times another smooth function.
00:34:51.564 - 00:35:20.768, Speaker A: That's Taylor's theorem, and that's all that's involved here. Everything in the whole discussion is smooth functions, smooth functions, everywhere. The domains are smooth functions. The families are smooth families. You can just use Taylor's theorem to produce, well, what needs to be produced is the extension from this guy here. Let's call this guy bt. We get a whole bunch of operators b omega t, and then we need to produce the operators b omega zero, the symbol of that scale Scalene family.
00:35:20.768 - 00:36:04.176, Speaker A: And you, you produce it just by, by taylor's theorem. Okay, everything is smooth. And so everything has a smooth extension even after you divide by t because of vanishing at zero. Well, here I'm just, the simple version of the lemma would be exactly. But if, if you have any smoothing symbol family you can extend it to, it comes from a family of a scaling family, which also consists of smoothing operators. And if you like, you can make it zero at t equals one. You can do fiddles like that.
00:36:04.176 - 00:36:52.434, Speaker A: But here I'm imagining, just for simplicity, that this is exactly true, because then I can write down the proof and just say Taylor's theorem. Exactly. There exists a smooth family bt with the property that t times bt is 18. Or another way of saying that because in the definition of a smoothing family, t is always non zero. This thing is the smoothing family. The crucial thing is to be able to show there is this extension, smooth or continuous, but smooth extension to t equals zero. That's the difficult part of the whole theory.
00:36:52.434 - 00:37:00.934, Speaker A: And that comes from Taylor's theory. Yep.
00:37:02.674 - 00:37:09.474, Speaker B: For now then, these derivatives, maybe the derivatives in a different direction, aren't the same.
00:37:11.854 - 00:37:59.550, Speaker A: Well, there, I mean, the Taylor's theorem we're looking here, the operative thing we're looking at here is just functions in the variable t. I mean, what's actually happening is we're looking at functions in various variables, t x, one up to xn. And we're saying that if such a function in many variables vanishes identically when t is equal to zero, it is t times the c infinity function in those n plus one variables. There's really only one direction in play here, which is the t direction. Now comes the fun part. I didn't tell you what op is yet. That's not the fun part.
00:37:59.550 - 00:39:26.164, Speaker A: But let's just leave that for a moment and have fun here. And just temporarily, let's just indicate the degree by putting a little subscript mix in the hope that this will add clarity to the following discussion. I mean, maybe not. Maybe that's not going to help. Usually adding subscripts doesn't lead to clarity. Anyway, reminded of some, first of all, I was reminded of some funny anecdotes related to subscripts, triple subscripts, actually. But then I was reminded I'm on the Internet, so I'm not going to tell you what they are.
00:39:26.164 - 00:40:09.504, Speaker A: I'll tell you later. Okay. Right. So let's say you start with a scaling family of degree. Maybe you actually started, maybe you were really interested, like we all really are, just in a single operator, a scale Abel operator of order M. But the first thing you have to do with a scale ABl operator of order M is to turn it into a family before you can talk about it in this lecture. So then you have a family like this.
00:40:09.504 - 00:40:56.026, Speaker A: So you could take a this family, and you could take its symbol, agree M. And then if there is a construction, which I haven't told you what it is yet, but I will soon, which produces, out of a symbol, an operator pointing to the wrong thing should have been pointed to that thing. Where is it? Here, this thing. Then you can form this difference. And this difference is a Symbol. It's a difference of two Symbol families. Excuse me, scaling families of order m.
00:40:56.026 - 00:41:51.860, Speaker A: So you're allowed to do that. You're allowed to take the difference and it vanishes identically at t is equal to zero. The symbol of this thing is zero because of that relation on the bottom board over there, symbol of op as opposed to op of symbol is zero. So this thing here, let's maybe call the original family, see if this is a good idea. Maybe I'll just call it a, changed my mind. This difference is actually t times some family of order m minus one. That's its symbol vanishes.
00:41:51.860 - 00:43:27.864, Speaker A: So it's t times the lower order family. Sorry. I think you can see where I'm going. We could take this m minus one family and subtract from it the family we get by applying the Ott construction to the symbol in degree m minus one of the family m minus one. And this is a family whose symbol is zero. So it's tied times the family of degree m minus two, and so on. And you can just keep going as many times as you like and you obtain a sort of expansion for a, which I'm just going to write in this way.
00:43:27.864 - 00:44:41.226, Speaker A: You apply the operator construction, which I didn't tell you what it is, to a certain symbol family of degree m, which is just the symbol family of a. And the difference is just t times up m minus one of some other symbol family of degree m minus one. This is the symbol family of this am one here and so on. So we have to have a discussion about what et cetera means at some point. But you could, if you wanted, postpone that discussion by just doing this enough times so that you end up down here with an operator, a symbol, excuse me, a scaling family of order less than minus the dimension n. In that case, we kind of know what it is. It's given by a family of integral kernels, and they're all continuous integral kernels.
00:44:41.226 - 00:45:29.964, Speaker A: You may still want to know more about these continuous integral kernels, but not if you're one of those people out there. If you're a C star, algebraic, it's just a bunch of compact operators. So you're happy you figured out exactly what every scaling family looks like up to a family of compact operators. And if we can master the et cetera here, maybe we can improve that and figure out what every scaling family looks like up to a smoothing family of smoothing operators. That would be our version of the Sea Star algebra statement involving compact operators. Okay, that's what's going to happen. And so this entire subject consisting of these mysterious scaling families which we didn't exactly know what they were, although we studied some examples, comes down to just one construction.
00:45:29.964 - 00:45:50.194, Speaker A: If we understand this construction, we understand everything. It's over as far as scalable operators are concerned. We know what they all look like, up to smoothing operators so far, up to compact operators, but up to smoothing operators when we get serious, just like.
00:45:52.654 - 00:45:58.994, Speaker B: Why would you, is the reason why you think to maybe get some sort of expansion like this becoming differential operators.
00:46:05.854 - 00:46:56.886, Speaker A: So of course, this from Eric and Bob's point of view, van Erp and Junken's point of view, that they already knew about pseudo differential operators. And what we're doing here is a construction that is completely understood in the pseudo differential world. We're just doing it in a, in a different, with different clothes on. So the idea that there are these so called symbol expansions is central to the whole practicality of pseudodifferential calculus. The fact you can use it to solve problems comes from the fact that every operator can be understood successively up to arbitrary accuracy, so to speak, by some mechanical process involving op and sim. Yeah, it's like a Taylor series. It's exactly as you can tell from Taylor's theorem up there, it's exactly like a Taylor series.
00:46:56.886 - 00:47:15.834, Speaker A: And as in the theory of Taylor series, if you have a Taylor series, there's always a C infinity function, which it is the Taylor series of. That's a famous theorem of Burrell, not Armand, but the Borrell senior, I guess. And so that's what's going to happen here. That's what's going to resolve this, et cetera.
00:47:15.954 - 00:47:17.494, Speaker B: This is sort of like how you.
00:47:18.714 - 00:47:25.114, Speaker A: Yeah, exactly. Yeah, that's exactly what we're going to do on, in the next lecture once we've mastered this.
00:47:25.194 - 00:47:46.030, Speaker B: So like just, I'm sorry, this is quite another kind of question. But no, no, at this point you're saying we're trying to reconstruct pseudo dimensional operators. So is the guiding principle that whatever we constructed should have like a, whatever we should be able to take asymptotic.
00:47:46.102 - 00:48:01.884, Speaker A: Yes, that's the guiding principle. That's not what actually happens. In the most extreme generalizations, things go a little bit wrong. But that's certainly the principle. That's the idea. And when it goes wrong, you have to cope with that.
00:48:02.224 - 00:48:08.564, Speaker B: But the idea is if we have asymptotic respect or asymptotic, then we have.
00:48:09.064 - 00:48:20.376, Speaker A: Yep, that's exactly what's going to happen in the next lecture. Not today. We have to. Yeah, we'll see how far we get. We have to resolve this, et cetera, we have to prove Burrell's theorem, if you like, in this context, at some point.
00:48:20.440 - 00:48:24.684, Speaker B: But in general, like, if one could think that a good calculus is one that has.
00:48:25.884 - 00:48:44.252, Speaker A: Yeah, it's like. I mean, what makes calculus good? It's that you can calculate with it. It's very practical. You can. I mean, this is the Isaac Newton's secret weapon, that everything's a gigantic polynomial. So you can calculate everything about everything if you're Isaac Newton. And it's the same thing here.
00:48:44.252 - 00:49:07.464, Speaker A: Everything is a gigantic polynomial. Well, it's not exactly a polynomial, because sooner or later, after a finite number of terms, these guys are. It's just going to go on and on forever. But these guys are going to have negative orders. It looks like a power series, but it also is a bit like. A bit more like studying Laurent series than power series, if you take these coefficients into account.
00:49:08.924 - 00:49:14.944, Speaker C: So just. Excuse me. So, once you have defined open seam, we are trying to construct.
00:49:15.244 - 00:49:17.664, Speaker A: Can you repeat the question, please? I wasn't.
00:49:18.604 - 00:49:35.708, Speaker C: We have defined open sim. We are trying to define a kind of Taylor function or everywhere c infinity differential c. I mean. I mean, everywhere differential function. A function, which. I mean, c infinity function with it. Right.
00:49:35.708 - 00:49:41.584, Speaker C: I mean, we are taking q from the Taylor series, and we are trying to construct a series out of it. A kind of power series.
00:49:42.404 - 00:49:43.664, Speaker A: Yeah, we're not.
00:49:46.524 - 00:49:47.308, Speaker C: So that.
00:49:47.436 - 00:50:09.756, Speaker A: Yeah, it's. Every function can be represented. It's not completely true, but more or less, according to Isaac Newton, every function can be represented by a power series. Every pseudo differential operator can be represented by some kind of power series as well. That's going to be relevant to what we're doing. We're not going to study, I don't know, some kind of crazy functional calculus. We're not going to attempt to construct.
00:50:09.756 - 00:50:19.410, Speaker A: Well, we could, but it's another story. Cosine of. Of a. Using some calculus like this. I don't know if that's the direction you're asking about. Sorry, you're up there.
00:50:19.522 - 00:50:22.066, Speaker C: No, no, it's fine. It's fine. It's fine.
00:50:22.210 - 00:50:22.594, Speaker A: Okay.
00:50:22.634 - 00:50:24.834, Speaker C: I was just trying to understand what's happening.
00:50:24.994 - 00:50:32.818, Speaker A: Okay. So, of course, I haven't yet. I mean, done the main thing, which has to be done, which tells us to tell you what RP's is. But get to that in a moment. So it's.
00:50:32.866 - 00:50:40.506, Speaker C: It's a kind of indeterminate in. I mean, like we define polynomials in. Yeah. Right, right.
00:50:40.650 - 00:50:43.858, Speaker A: I mean, we need to come up with a definite construction.
00:50:43.946 - 00:50:45.090, Speaker C: Right, right.
00:50:45.242 - 00:51:12.588, Speaker A: And all I'm pointing out that is that whatever the construction is, as soon as you have it, there's this very simple conceptual consequence that everything can be expanded in a series like Isaac Newton and everyone else in the last 300 years after him. You have to worry about, you know, it's our job now to worry about what et cetera means. In the olden days, you didn't have to do that. You could just write et cetera. But now we have to. Okay, we'll worry about that. Jamie, you had a question?
00:51:12.676 - 00:51:15.824, Speaker B: I was going to say what happens if any of those.
00:51:18.484 - 00:51:47.802, Speaker A: You can just, you can expand an operator. Yeah, maybe you're not satisfied with compact operators. Maybe you're satisfied with integral operators, where the integral kernel is 57 times differentiable. Then you have to go a bit further. Maybe you're only going to be satisfied when you get to integral kernels, which are smoothing kernels. Then you have to go a lot further, all the way to infinity. And then you have to worry about in a more serious way, what this etc.
00:51:47.802 - 00:52:16.624, Speaker A: Means. So we'll get there. All right, so that's where we're headed all of a sudden. This weird abstract theory involving scaling families and scale Abel operators. All of a sudden it's sort of changed, hasn't it? And it looks like it's become a sort of practical tool that you could actually calculate with. As long as we can understand what this crazy op is, once we have that, we can expand everything as series. Looks like we're in good shape.
00:52:16.624 - 00:53:37.434, Speaker A: And we'll see a good example of that practicality in the next lecture. But that today our goal is to understand this construction. Okay, let's see now where to go. Maybe we'll just keep this main goal. I could do this and just tell you what office is right here. So let s be one of these symbol families, just a bunch of operators which are more or less homogeneous of degree m. So there's a small technical detail involved in the following construction.
00:53:37.434 - 00:54:12.904, Speaker A: Okay, let me. We'll get to it. Let me not discuss it just yet. I don't need it to write down this formula. So I want to take this family and I want to make a scaling family out of it. And that means I have to tell you what these operators are. We're all non zero t and then we'll be in business.
00:54:12.904 - 00:54:50.524, Speaker A: And I'll tell you what they are by telling you how they act on a function. I'm just a little bit worried that doing this in the wrong order. I'll do it this way. Okay. Oh. So I want to explain how this operator, which I haven't yet defined, acts on a function. So I'll tell you what its value is at a point, x.
00:54:50.524 - 00:55:37.640, Speaker A: And here's what I'll do. I'll involve x in the rescaling and in the choice of the operator from this family. Oh, too many inverses. And then I'll apply this to f, and then I'll evaluate at x. Yes, I think I'm happy with that. Yeah, it's a formula and it feels kind of right. We get one operator for each t.
00:55:37.640 - 00:56:14.124, Speaker A: For example, let t just be one for a moment, that maybe I should have done that first. Talk about a single, as it happens, scalable operator by setting t is equal to one. If I were to set t equal to one, these rescaling operators would just be the identity and you could just forget about them. In fact, that's such a nice thing to do. I'm just going to write it out. This just looks so much nicer. This is just s x over f evaluated at x like that.
00:56:14.124 - 00:56:33.898, Speaker A: That's a little bit easier. The rest just involves some rescaling. Let's not worry about that. This is the basic idea. What you see in this formula down here, which has indeed the right number of parentheses, although not of the right size. So we take all of the s's. We want to build a single operator out of a bunch of s's, and clearly we have to use all of them.
00:56:33.898 - 00:56:35.854, Speaker A: And this is how we do it.
00:56:36.954 - 00:56:48.214, Speaker B: Yeah, maybe I'm misunderstanding it, but it seems kind of strange. So like, in some sense, I can think of this as like a family of operators.
00:56:49.434 - 00:56:49.958, Speaker A: Yep.
00:56:50.026 - 00:56:51.838, Speaker B: A function on the base.
00:56:51.926 - 00:56:52.590, Speaker A: Yep.
00:56:52.742 - 00:57:02.894, Speaker B: And this t equals one. I'm just applying this should I thinking like I'm pulling that back into the tangent model?
00:57:03.054 - 00:57:40.050, Speaker A: No, you should think that you are choosing at each point an identification of the tangent space at that point, at least a ball in the tangent space with a ball with a ball around that point in the manifold. We're definitely taking advantage right here of the fact that the, the manifold, if you like, is rn this construction, this thing called op is not going to be a totally canonical thing given to us by, you know, a manifold loving God. This is something which depends on coordinates. So that's why you don't like it.
00:57:40.162 - 00:57:41.122, Speaker B: It's not that I don't like it.
00:57:41.138 - 00:57:57.710, Speaker A: I just like, okay, okay. It feels about right. You've got all of these sss that you s omegas and you have to use them all at once. Somehow. I mean, why not like this? Yeah. Yes. Yeah.
00:57:57.710 - 00:58:12.554, Speaker A: We'll get to exactly what. And. Yeah. And what comes out of this definitely depends on the choice of coordinates. It's not coordinate independent. So sue me. All right.
00:58:12.554 - 00:58:36.900, Speaker A: Say again? No power of t in this. Not yet, no, but I haven't. I just made a definition. Yeah. If you look at the. It's gone now. But we were looking at some examples involving one over x and the logarithm and so on.
00:58:36.900 - 00:59:09.486, Speaker A: And when we built a scaling family out of such a thing, all you're supposed to do is just divide by t. Just do some rescaling. If you think about how you go from the function one over x times sigma of x to the operator, which is convolution by that function, which is a scalable operator of order zero, to a scaling family of order zero, the only thing you have to do is divide by t. In other words, you have to rescale. And there's a one over absolute value of t. That's just some radon nicotine factor. Just.
00:59:09.486 - 01:00:05.822, Speaker A: It's because I didn't express the thing intrinsically using densities. Okay. All right. But now, of course, there needs to be a theorem somewhere. Let's put it in some prominent place, maybe over here. Razor. The theorem is that this thing, or all of these things together, is a scaling family of degree.
01:00:05.822 - 01:01:02.054, Speaker A: Whatever it is. We started with M. So this thing that we wrote down before, this defines the scalable operator of order. Nice. Once again, the S sub X's are translation invariant operators. So if we remember, I know we said we weren't supposed to do it, but let's just remember the Fourier transform for a moment. We know what the S sub X's are.
01:01:02.054 - 01:01:53.804, Speaker A: They're just convolution operators, and they can be easily understood by taking Fourier transform. Under Fourier transform, the S sub X's are just multiplication operators. So we have a family of multiplication operators here, depending on X. And to signify that we took a Fourier transform, let's call the variable xi, which is the multiplication operator variable and not x. And you see that as soon as you have a family like this, you get a function of this type. Well, there are some details about is it really a function or is it a distribution? Okay, with some details to work out. But in fact, it turns out this is always a legitimate c infinity function in both of its variables, and it satisfies those funny estimates that occurs from.
01:01:53.804 - 01:03:05.234, Speaker A: That can be proved very easily, because everything is translation invariant. It's so easy to work with this formula, this formula here, the one that I'm pointing at, defining the operator a, gives an operator which is exactly of that integral operator form involving the Fourier transform that we talked about before. All of a sudden, we're right on the doorstep of pseudo differential operator theory. These guys, these formulas, these operators that we're now building, which are scalable operators of order m, are examples of integral operators of exactly the kind that Hermander defined and Cohen and Nahrenberg and so on, defined, codified. So it didn't take us long, but here we, and here we are back in the world of pseudo differential operators. If we take, if we do this thing here, which is following the winding path of Fourier. So far, so good.
01:03:05.234 - 01:03:41.944, Speaker A: Now we got to check something. We have this family. We want to know. It's a scaling family. So we have to build those operators which look like this. Start off with these ones, and then after we've built them, we have to show there's a smooth extension to t equals zero. And then we have to check this scaling law.
01:03:41.944 - 01:04:04.474, Speaker A: So let's, I mean, before, first thing we have to do is calculate what this damn thing is. And it's clearly just a calculation because everything's given by a formula. And so it's not going to be that hard. There is an inverse. Thank you. That's a typo. Thank you.
01:04:04.474 - 01:04:32.694, Speaker A: I do mean opposite. Thank you very much. Although that would lead to disaster if I try to calculate this thing. Let's just stick this in. I guess I have to do that now. There's no room to tell you what it is. It's done.
01:04:32.694 - 01:05:08.154, Speaker A: This is equal to, well, what's going to be involved. These s sub x operators are obviously going well. There's going to be a formula just involving in the ends, maybe some rescalings, and the s sub x occupation, it's only a question of figuring it out. And you don't have to guess because you can just calculate. But it helps to know what happens when you do calculate. This is the direction in which you're headed. That's nice.
01:05:08.154 - 01:05:54.464, Speaker A: So there's an obvious extension of this to t is equal to zero. So if this is correct, this is just a calculation, we'll see how much time I have or just how much strength I have. As with the previous calculation, the beginning of the lecture, this is actually in the notes of today's lecture already because I had to calculate it to check. It really worked. And so if I don't prove it, you can either prove it yourself, you just grind it out. And since I told you the answer, you'll know when you made a mistake. And so you just have to keep making mistakes repeatedly until by accident, you get this formula and then, you know, then you'll, then you'll have proof.
01:05:54.464 - 01:07:31.360, Speaker A: And so the corollary is that there is a, an obvious smooth extension recalling these things. Let's introduce the usual notation. So these guys we would call op asks of omega t, namely this thing up here, this thing. So t is not zero, of course, t is equal to zero, and the extension is obvious what the extension is. So if this is a scaling family, which we don't yet know that it is, if it is a scaling family, and then you calculate its symbol, what you'll get is the original family. That's clear from the formula, quite possibly. Oh, thank you.
01:07:31.360 - 01:08:38.080, Speaker A: Yes, thank you. I guess I was going to write that. The next thing we have to check is that. But it is another lemma, it's another calculation. So now we have these operators, which I'm going to call op s of omega t, and we need to check the scaling law. And it's just a calculation and it's going to involve the scaling law for the symbol, that's for sure. So let's write r lambda s omega r lambda minus lambda to the minus m s omega.
01:08:38.080 - 01:09:05.390, Speaker A: So this is the two, these are the two things, I've subtracted them, which are supposed to be equal modular smoothing operators in the definition of a symbol. So let's call the difference k lambda omega. So let's just imagine we'll do this one lambda at a time. So let's just temporarily fix lambda. It doesn't have to be positive or negative. This is just algebra. Sorry, bigger or one and less than one.
01:09:05.390 - 01:10:05.482, Speaker A: This is just algebra. Then back to the putative scaling relation for these fellows. Well, this is supposed to be lambda to the minus m up s omega lambda t plus a smoothing operator. And in order to tell you what the smoothing operator, it's just an algebraic formula. I'm going to write down, in order to tell you what it is, I have to write it in this way. I have to plug in a function and evaluate the function at x like that. So the same thing here, because the error term, which is going to be a smoothing operator, well, you'll see what it is.
01:10:05.482 - 01:10:49.034, Speaker A: When I remember exactly what it is. I think I do remember what it is. I'm going to take k, and k has two indices, a lambda and an omega. Excuse me, traditionally in this class dictates that we put the omega first. So I just changed the notation there. And the correct thing is lambda times t times omega minus x, like that of f evaluated at x. Yeah, so I screwed something up.
01:10:49.034 - 01:12:11.746, Speaker A: And meaning there should be a comma here, and I can't remember whether it was lambda or lambda. See, I guess it's just k lambda, because that's what we're doing, scaling by. If you believe this, then looks like we're in business, because it's easy to check that this funny thing, which is a sort of hybrid of, I mean, there's a whole bunch of smoothing operators here, and we're not just applying one smoothing operator to f and then evaluating at x, we're choosing x first and then deciding which smoothing operator to apply. So the formula for this, if you write the smoothing operator as a little k of x, comma y, the formula for this is going to be a little bit complicated, but for sure it's going to be a smoothing operator, c infinity and all possible variables. Because we're dealing with smoothing families, we're insisting that the k's, these big k's, the ones which appear up here when you write them down with integral kernels depending on omega and lambda. The integral kernels are c infinity, functions of omega and lambda and x and y. So whatever kind of crazy, ridiculous thing you write down like this, it's going to be represented when you work it all out.
01:12:11.746 - 01:13:01.644, Speaker A: There's going to be some integral kernel for this thing, which is a c infinity function of x and y. All right? So that says the scaling relation is true. Whole modular scaling operators. And either I did the calculation correctly and in which case, here's the answer, and you just have to do the calculation correctly as well, or you have to duplicate exactly the mistakes I made, in which case you'll also get this formula. You know, this is just a calculation, and I haven't decided if I have courage to do either one of these calculations directly in front of you, even though they're written down right here, and I could just read my notes like that. It's just formulaic. Yeah, it's not, it's, yeah, it's just algebra.
01:13:01.644 - 01:13:28.258, Speaker A: I've, over the years, I used to say it's just algebra when I was young like you, but now it's, it's not like that. You can spend decades finding the right formula. Algebra is much harder. You know, after, after decades, a piece of mathematics eventually becomes algebra. That's what everything eventually becomes after it's polished and refined and made more and more beautiful. But this is not like that. This is just calculation.
01:13:28.258 - 01:15:27.994, Speaker A: Maybe that's the right way of saying it, not algebra. All right, nearly there. It looks like with these two lemmas, it looks like everything is just done. I mean, what is a scaling family? It's supposed to be a smooth family and it's supposed to have, and it's supposed to be an extension, which there obviously is, and the scaling relation is supposed to be true. But there's one small detail that, that we've overlooked, which is a little bit annoying, and it has to do with where is it? This formula here, this formula involves x's in two places, and it may occur to you at some point to ask yourself, is this a properly supported operator? And the answer is no, it's not necessarily a properly supported operator. So this thing, let's go all the way back to the a that we were looking at over there. Even this one is not necessarily properly supported, because the formula for the support of this operators is going to involve the formula somehow for the support of all of the S's at once.
01:15:27.994 - 01:16:45.886, Speaker A: And what we assumed about the S of x's is that they were uniformly properly supported over compact subsets of X, not arbitrary subsets of X. And in order to get around that, we need to assume just a little bit, some little thing to deal with this annoying issue. And so this, the nicest thing to deal with is to say that this family here is compactly supported in Omega. So it's just a compact number of operators. Each s omega is a translation invariant operator. I'm not talking about it being compactly supported. I mean the number of s omegas which are non zero, that's supposed to be a compact number of omegas for which that's true, in which case, because most of these guys are zero, they're going to be uniformly completely uniformly properly supported.
01:16:45.886 - 01:18:02.044, Speaker A: And that's really what you need is, let's call it completely uniformly uniformly uniformly completely uniformly properly supported. Here's a completely uniformly properly supported family. If you have any family at all, any family of S's to begin with, any symbol family, you can always modify it by a family of smoothing operators so that smoothing family of smoothing operators so that it becomes completely uniformly properly supported. It's not a big deal. It's just, you know, here's the lawyer, the lawyers are speaking here. It's just a little blemish. And I think the instincts of right thinking people like me, the best instinct is just to assume compact support, and then this annoying problem goes away.
01:18:02.044 - 01:18:53.924, Speaker A: Although you need much less than that to make the problem go away. Okay, so I'm just being honest. When you do the calculation, everything works just beautifully, except for this minor, minor blemish that if you started out with an arbitrary family, which had some ridiculous support condition, then you would end up with a little bit of a problem. If you had all of these s omegas, which are convolution operators, and if you multiplied all of the convoluting functions distributions, I should say by the same sigma of x like we saw in our examples, then done, you would have automatically made everything uniformly, completely uniformly properly supported. And you wouldn't have changed anything except at the tails where everything is smoothing operators. And there's nothing really big at stake here. It's just annoying.
01:18:53.924 - 01:19:25.434, Speaker A: There you go. But if you're only dealing with a compact number of omegas, then everything is fine. Okay. And now I have to just. Any questions? Maybe that's, maybe it's time for a little discussion. Yeah, okay, fine. That's a legitimate question.
01:19:25.434 - 01:19:38.962, Speaker A: Completely. So let's put in somewhere. Might be up there again. Step forward.
01:19:39.018 - 01:19:41.334, Speaker B: What's the definition of example?
01:19:42.274 - 01:20:18.314, Speaker A: Well, there is no definition of this term. Let me say compactly supported. I think what I've written down here and what I said settles the matter. Compactly supported by a compactly supported family. I mean, a family for which, for all except a couple compact number of omegas, s omega is exactly zero. That's what compactly supported means. And now, as for this horrible, completely uniformly properly supported family, let's just for the record, write down what that means.
01:20:18.314 - 01:21:27.744, Speaker A: So there's the asterisk. I mean, for every phi compact support, there is some psi also compactly supported, such that if you take these operators in the family and you look at just this piece of them, then this piece is equal to Phi s omega psi. And the other way around, too. This one you get for free. This one is, you don't have to worry about this in our particular case. But, but I mean, it's not this type of failure of proper support which is at issue. It's this one at the bottom here.
01:21:27.744 - 01:22:06.512, Speaker A: Anyway. So this is supposed to be true for all Omega or whatever the index set is here? Yeah, we're calling it Omega. You have to choose the Phi and the Psi first, and then the same Psi is supposed to work for every single little omega. You're not allowed to have Psi depend on first choosing a compact set of omegas the same size has to work for all of them all at once. And I'm not recommending that we ever think about this notion, I'm recommending that we just say that. We just remember that if the family is compactly supported, it's easy. Everything just works.
01:22:06.512 - 01:22:27.374, Speaker A: And if the family is not compactly supported, just a warning that something might go wrong. And you have to have ridiculous support, proper supports, for something to go wrong. But you never know, maybe, you know, you might encounter that situation. Go for it.
01:22:28.194 - 01:22:30.574, Speaker B: Is the negative minus x, actually.
01:22:32.514 - 01:22:41.258, Speaker A: Yes. Thank you very much. Yes. Yeah. Very good. Yeah. And, I mean, instinct should also tell you that.
01:22:41.258 - 01:23:01.154, Speaker A: But I'm glad it matched up with actual calculation. It's possible I said it the right way and just wrote it wrong. Yes. Thank you. And Jacob checked it, which is far more reliable than my notes, probably. So. So, yeah.
01:23:01.154 - 01:23:25.838, Speaker A: Anyway, it's true. Yes. Yeah, yeah, yeah, yeah. All right. So this is the construction. And if there wasn't a construction like this, we'd be way off base. We'd be far, far from, or at least some distance away from actual pseudo differential operators.
01:23:25.838 - 01:24:02.202, Speaker A: So we'd have to do something to change the definitions. But fortunately, everything works out beautifully. There is this construction called op, which produces examples of scaling families and in particular, setting t equals one scale abl operators. The extra good news is that when you actually work out what op s sub one is, what the scalable operator is, that's given by this formula. As long as you allow yourself the Fourier transform, you see, it is just a multiplication operator. The recipe is you take a function f. First you Fourier transform to make f hat of psi.
01:24:02.202 - 01:24:56.822, Speaker A: Then you multiply by this thing, and then you take the inverse Fourier transform, and what's left is something which depends on just xi along. Did I say that right? Yeah, because you're integrating out x when you do that, not integrating out x, you're integrating out psi. Excuse me, I'm going to write that down because I've managed to confuse myself. A of fucking of x is, well, the experts tell us we're supposed to put in a one over two PI n, and then the integral of a of x psi f hat psi running out of space e to the I psi x xi. I guess that's what I mean. Yeah.
01:24:56.998 - 01:25:04.714, Speaker B: So this like Taylor expansion, give me the polyhomogeneous expansion.
01:25:05.854 - 01:25:47.558, Speaker A: Yes. What happens? So we're talking here about what is op of s evaluated at one. And this is exactly the operator you get. And the xi you get. The function a of xi you get here is a function which is asymptotically homogeneous of order m. What do I want to say about that? Which turns out to be the same thing as saying it is a homogeneous function of order m plus a Schwarz function, at least away from zero. If you start to add in the lower terms, as we were discussing, just to understand what is a general scalable operator through that series construction, which has now disappeared, then you'll get a function here still.
01:25:47.558 - 01:26:14.284, Speaker A: Clearly you'll get a function here still, because you set t equals one, which is just a sum of homogeneous terms of lower and lower order plus a little schwarz error. As we'll see. We will spend just a little bit more time. We will actually take Fourier transform of s, omega and ops, just for the record. But we're peeking ahead. You can see how it's going to work out. Good to know roughly where we're going.
01:26:14.284 - 01:26:45.570, Speaker A: Okay, so there are five minutes left. I'm not going to do the calculations because Jacob did it. You also don't need to do the calculations because Jacob did. So we can stop five minutes early, have a martini. Oh, I have a question. You take. Oh, no, I don't think that's true.
01:26:45.570 - 01:26:57.344, Speaker A: Not necessarily. I thought about that. That'd be nice. But I don't think that's necessarily true because the operator could have some weird, horrible proper support to begin with. I couldn't see that it was true at any rate.
01:27:00.044 - 01:27:06.884, Speaker B: Yeah, this thing doesn't quite come around then, because we needed an assumption at the very beginning can make certain.
01:27:07.044 - 01:27:34.674, Speaker A: Yeah. So the exact theorem which would precede that asymptotic expansion is that every compactly supported scalable operator has an asymptotic expansion whose terms are individually also, if you like, compactly supported, if you want to deal with the general operator partitions of unity. But this procedure would work within the world of, as it's written within the world of compactly supported operators. I think that's no big deal.
01:27:41.914 - 01:27:54.210, Speaker B: To justify why there. Is it reasonable to say, like, okay, I know you said it's not an exact sequence, but we have like the order m minus one going into the order m, and then we have this symbol.
01:27:54.282 - 01:28:25.450, Speaker A: Oh, so that, that is an exact sequence. Yeah. Okay, you're right. I had this asymptotic expansion fixated in my mind. But what you're describing is indeed an exact sequence. There is a map, the symbol map, from scalable operators to scaling families. Let's say scaling families, modular smoothing operators, order m scaling families, and the kernel is precisely the order m minus one operators.
01:28:25.450 - 01:28:29.594, Speaker A: So that's true in this plain vanilla version of the calculus.
01:28:29.634 - 01:28:31.242, Speaker B: Yeah, that's sort of the content of.
01:28:31.258 - 01:28:35.766, Speaker A: That level upstairs yeah, yeah, exactly. Yeah.
01:28:35.790 - 01:28:42.314, Speaker B: And so in that sense, I'm kind of satisfied that you wouldn't actually get like a canonical splitting because in general, probably.
01:28:43.294 - 01:29:22.494, Speaker A: Yeah, that's exactly right. But geometrically, what's going on is that the rns that you have at t is equal to zero are tangent spaces. And the rns that you have when t is not zero are just coordinate patches in the manifold. So they're similar, but they're not the same. And you have to make that identification to do anything. Like wherever this formula is, here it is, right? This is an operator acting on a tangent space made to act on a function which lives on the manifold. So there's something wrong with this formula on, on a general manifold, there has to be some extra notation telling you how to identify tangent vectors with points in the manifold.
01:29:24.394 - 01:29:52.084, Speaker C: Excuse me, sir, I had a question. So when you are saying that there is a map, say, to the scaling family of operators of degree m, so what is the structure there? Is it a sheep or is it some kind of, I mean, what is the structure? To have a manic, to have a map to that kind of a family and a kernel, so there has to be. So what is the, when you say, when we're talking about the exact sequence.
01:29:52.704 - 01:30:00.468, Speaker A: Yeah. Are you asking, are these topological algebras of some sort? When you say structure, is that the.
01:30:00.476 - 01:30:08.452, Speaker C: Kind of structure, when you spoke about the kernel or something like that? So you need to have some group structure. I mean, some structure.
01:30:08.508 - 01:30:22.488, Speaker A: Oh, I'm just talking, I'm talking about the vector space of all scale Abel operators of order m form a vector space. Yeah. There's a linear map from that vector space to the vector space degree m. Simple families.
01:30:22.636 - 01:30:23.056, Speaker C: Thank you.
01:30:23.080 - 01:30:33.724, Speaker A: It's onto, and the kernel is precisely the vector space of order m minus one operators. That's what's going on. So I don't know where to look. But anyway, that's what's going on.
01:30:34.504 - 01:30:35.840, Speaker C: Thank you. Thank you.
01:30:36.032 - 01:30:44.544, Speaker A: Okay, good. Thank you very much. Miller time for this class. We shall continue on Tuesday.
