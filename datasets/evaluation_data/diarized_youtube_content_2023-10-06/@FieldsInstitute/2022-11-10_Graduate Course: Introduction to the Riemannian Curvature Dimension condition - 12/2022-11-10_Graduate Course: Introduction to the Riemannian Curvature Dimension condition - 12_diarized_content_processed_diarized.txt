00:00:00.200 - 00:00:04.814, Speaker A: Find, you know, d plus g nabla.
00:00:04.854 - 00:00:07.674, Speaker B: F and d minus g nabla f.
00:00:08.454 - 00:00:12.718, Speaker A: So this was the limit as epsilon.
00:00:12.806 - 00:00:23.594, Speaker B: Going to zero from above. And this from below of the same quantity. And the quantity is at the minimal we cap of f plus epsilon g.
00:00:25.134 - 00:00:28.830, Speaker A: Squared minus the minimum weak applicant of.
00:00:28.862 - 00:00:58.538, Speaker B: F squared divided by two epsilon. And then I also gave a definition. I said that x is infinitesimal, Burton, if w twelve is inverted. And then basically I conclude the lecture.
00:00:58.586 - 00:01:03.258, Speaker A: By, you know, stating a result concerning.
00:01:03.306 - 00:01:18.466, Speaker B: How this d plus d minus g nabla f, you know, reads in the case of infinitesimal libertarian spaces. And basically the calculus rules they obey are those of, you know, grad f grad g modeling. So the theorem proposition, whatever does the.
00:01:18.490 - 00:01:28.254, Speaker A: Following say, x is infinitesimal libertian. Okay, then.
00:01:31.074 - 00:01:36.546, Speaker B: A few things are true. So first of all, d plus g.
00:01:36.570 - 00:01:40.574, Speaker A: Nullah f is the same as d minus g nullah.
00:01:42.514 - 00:01:53.064, Speaker B: And the map that takes, you know, fg into this, you know, same quantity is bilinear and symmetric.
00:01:54.804 - 00:02:01.700, Speaker A: Bilinear symmetric and satisfies a few things.
00:02:01.732 - 00:02:06.964, Speaker B: I will call it. So call it once you know, that this binary symmetric, call it, you know.
00:02:07.004 - 00:02:08.064, Speaker A: Dfdg.
00:02:11.023 - 00:02:28.543, Speaker B: Because now it's linear symmetric and satisfies, you know, cautious warts, what looks like cautious Marx equality.
00:02:28.663 - 00:02:30.791, Speaker A: If f is equal to g, of.
00:02:30.807 - 00:02:37.904, Speaker B: Course this is almost everywhere it is local.
00:02:38.204 - 00:02:42.332, Speaker A: Namely, what is d f?
00:02:42.468 - 00:02:45.468, Speaker B: D g is equal to d f.
00:02:45.516 - 00:02:50.940, Speaker A: Prime, dg prime m almost everywhere on.
00:02:50.972 - 00:03:30.264, Speaker B: The set where f is equal to f prime and g is equal to g prime satisfies the chain rule. And here phi is lip sheets. And just, you know, let me also add the c one on r. It's not necessary, just lip sheets, sufficient. But if I drop the c one assumption, I do work a little bit more and I don't want to do this. And then d f one f two.
00:03:30.844 - 00:03:43.344, Speaker A: Dg is equal to f plus f dg.
00:03:46.084 - 00:03:53.020, Speaker B: And finally, and finally, integration by parts for the Laplacian. If, you know, the integral of g.
00:03:53.092 - 00:03:59.634, Speaker A: Laplacian f dm is equal to minus the integral df dg.
00:04:02.054 - 00:04:02.678, Speaker C: Okay?
00:04:02.766 - 00:04:08.854, Speaker B: And this of course requires more than f living just in the suburb space. Of course, f should have a Laplacian.
00:04:08.894 - 00:04:27.250, Speaker A: In order to make sense to this expression. So let me write this, okay, okay, I come to the, so perhaps one.
00:04:27.282 - 00:04:33.362, Speaker B: Comment, okay, one thing that one could say is, okay, w one two is.
00:04:33.378 - 00:04:35.658, Speaker A: In Bert, I polarize.
00:04:35.786 - 00:05:07.702, Speaker B: And when I polarize, I get a scalar product end of the proof. So that's, however this is. I mean, there is some polarization going on in this proof, but it is harder than just this. And the problem is that what a priori is not clear. It will be clear after this, it will be proven. But a priority is absolutely not clear that if w twelve is Hilbert, I don't know a priori where the map that takes f and returns, you know, say, the integral over a certain given set. Nicholas? Sorry.
00:05:07.838 - 00:05:08.390, Speaker A: Yes.
00:05:08.502 - 00:05:16.080, Speaker B: The right blackboard. Yes, I did not notice. Maybe it's this.
00:05:16.232 - 00:05:17.484, Speaker A: Yeah, good, thanks.
00:05:19.464 - 00:05:31.952, Speaker B: So I was saying the w twelve is silver tells that this map depicts f and returns the interval of the minimal recap of squared over the whole space, that this is a quadratic form.
00:05:32.088 - 00:05:35.720, Speaker A: Okay, now what is a priori not.
00:05:35.752 - 00:05:42.004, Speaker B: Clear is whether it remains a quadratic form when I integrate only over a certain border set.
00:05:42.184 - 00:05:43.076, Speaker A: Okay?
00:05:43.260 - 00:05:45.612, Speaker B: So whether, if you want, whether, you.
00:05:45.628 - 00:05:50.932, Speaker A: Know, so being the global level means.
00:05:50.988 - 00:06:17.544, Speaker B: Implying, being, you know, Hilbert, even at a local level, or if you wish, at a point wise level, as in the statement of the proposition, okay, in some sense, being Gilbertian has not a variational characterization. So this is not immediately obvious. And basically, proving this statement is more or less like proving that this is a quadratic form. And this is strictly related to proving the Leibniz formula.
00:06:17.664 - 00:06:18.160, Speaker C: Okay?
00:06:18.232 - 00:06:33.164, Speaker B: So, in fact, we will first prove the Leibniz formula, and then. And then. And then prove, conclude the statement. Okay, the proof of the statement. All right, so it's not really obvious, but I mean, but it can be done. It's not, not even that complicated.
00:06:34.284 - 00:06:41.344, Speaker A: Let me first observe the following. Okay, so, first of all, we know.
00:06:42.444 - 00:06:53.784, Speaker B: That'S by definition, or what we have seen the other, the last lecture, that d minus g is less or equal than d plus g almost everywhere for every fng, right.
00:06:54.804 - 00:06:58.012, Speaker A: That we know, okay, and we also.
00:06:58.068 - 00:07:07.858, Speaker B: Know that this guy is. Okay, actually, yeah. Is the limit as epsilon goes to zero of that expression from above and the other from below.
00:07:08.026 - 00:07:10.654, Speaker A: Now, if, if.
00:07:11.554 - 00:07:13.370, Speaker B: Now if x is infinitely.
00:07:13.442 - 00:07:16.882, Speaker A: Burton, I know that this guy is.
00:07:16.898 - 00:07:18.454, Speaker B: A quadratic form, right?
00:07:19.114 - 00:07:23.614, Speaker A: That's, you know, that's my assumption, basically quadratic, right?
00:07:25.134 - 00:07:30.702, Speaker B: In particular, it's smooth, okay? And in particular, I can take the derivatives from the left and from the.
00:07:30.718 - 00:07:34.274, Speaker A: Right at every point, and these derivatives exist, right?
00:07:34.614 - 00:07:35.734, Speaker C: So, right.
00:07:35.814 - 00:07:39.806, Speaker B: Is that clear? So this implies that, this implies that the, you know, if you want, that.
00:07:39.830 - 00:07:43.246, Speaker A: The limit as epsilon goes to zero.
00:07:43.390 - 00:08:06.580, Speaker B: Of trigger energy of f plus epsilon, g minus chig energy of f divided by two divided by epsilon. Sorry, this exists. Make sense if you want, here, this is the quadratic form. So you can polarize, right? So this limit is, okay, I can, maybe I can give a name to this limit. This is the.
00:08:06.692 - 00:08:10.508, Speaker A: I don't know, let's call it this way, fg, ch.
00:08:10.676 - 00:08:12.796, Speaker B: This color product in terms of quadratic.
00:08:12.820 - 00:08:15.144, Speaker A: Commodities make sense.
00:08:16.884 - 00:08:17.624, Speaker C: Right?
00:08:18.304 - 00:08:19.712, Speaker B: But this, you know, but on the.
00:08:19.728 - 00:08:32.088, Speaker A: Other hand, d plus, you know, the integral of this guy, in dm, this is equal to the limit as epsilon.
00:08:32.136 - 00:08:34.404, Speaker B: Goes to zero from above of that expression.
00:08:40.824 - 00:08:41.376, Speaker C: Right?
00:08:41.480 - 00:08:43.284, Speaker B: And the integral of d minus.
00:08:46.924 - 00:08:47.236, Speaker C: Is.
00:08:47.260 - 00:08:51.224, Speaker B: The limit as epsilon goes to zero from below of the same expression.
00:08:52.484 - 00:08:53.484, Speaker A: Makes sense, right?
00:08:53.524 - 00:09:07.124, Speaker B: Just because that's by definition, right? And some dominant convergence theorem, right? The domain convergence I can bring limiting epsilon inside and outside a integral easily.
00:09:07.244 - 00:09:09.224, Speaker A: So I get this right?
00:09:09.984 - 00:09:24.484, Speaker B: Now, these two are the same because this limit exists. Therefore these integrals are the same. But I had, you know, an inequality between, between the integrands and this forces equality to be true almost everywhere.
00:09:26.784 - 00:09:27.524, Speaker C: Right?
00:09:30.184 - 00:09:32.200, Speaker A: So that's the first step, okay?
00:09:32.312 - 00:09:34.088, Speaker B: Let me call this guy this, you.
00:09:34.096 - 00:09:36.718, Speaker A: Know, let me call this quantity, let.
00:09:36.726 - 00:09:37.334, Speaker B: Me give it a name.
00:09:37.374 - 00:09:41.638, Speaker A: Let me call it Dg nabla, f, okay?
00:09:41.766 - 00:09:49.982, Speaker B: I don't want still to use this expression because this is too symmetric. I don't want to make confusion, okay? I don't want to choose. So as an intended step, let me.
00:09:49.998 - 00:09:58.014, Speaker A: Call it this way, okay? Now notice the following in g, even.
00:09:58.054 - 00:10:06.414, Speaker B: Before knowing this, the map that takes g and returns. So the amount that takes g and returns d plus g, null f.
00:10:08.434 - 00:10:08.746, Speaker A: I.
00:10:08.770 - 00:10:21.698, Speaker B: Claim this is convex. If you think for a moment, convex in the, you know, almost every word sense. This is a map that takes sombol.
00:10:21.746 - 00:10:25.574, Speaker A: Functions and returns l one functions, okay?
00:10:26.114 - 00:10:30.966, Speaker B: So why is it convex? Well, because this is the derivative, the.
00:10:30.990 - 00:10:35.390, Speaker A: Right derivative of a convex function, the trigger energy, right?
00:10:35.582 - 00:10:41.114, Speaker B: Or if you want the minimum weak upper gradient to be, to be more precise, and the derivative of a convex function is always complex.
00:10:42.294 - 00:10:43.382, Speaker A: Makes sense what I'm saying.
00:10:43.438 - 00:10:54.486, Speaker B: So if you want, let me be a bit more explicit. So how much is it? D plus one minus lambda g one plus lambda g two.
00:10:54.630 - 00:10:57.094, Speaker A: Well, this is the limit as epsilon.
00:10:57.134 - 00:11:03.554, Speaker B: Goes to zero of what differential of f plus epsilon. M equal g lambda just for brevity.
00:11:04.094 - 00:11:08.678, Speaker A: Squared minus differential of f squared over.
00:11:08.766 - 00:11:10.794, Speaker B: Whatever two epsilon, right?
00:11:11.454 - 00:11:13.510, Speaker A: But now, for any given epsilon, for.
00:11:13.542 - 00:11:18.142, Speaker B: Any given epsilon, this expression, you know, is convex in this variable.
00:11:18.238 - 00:11:21.850, Speaker A: Makes sense because, because, you know, there is nothing here.
00:11:21.962 - 00:11:22.930, Speaker B: There is nothing here.
00:11:23.002 - 00:11:26.138, Speaker A: This is a convex function, as we already said, right?
00:11:26.186 - 00:11:37.774, Speaker B: So this expression is convex. So, you know, you apply the convex inequality, then you have a convex inequality for every epsilon that passes through the emitter and tells you a convex inequality at this level. Um.
00:11:41.594 - 00:11:44.414, Speaker A: Makes sense what I'm saying, right?
00:11:45.394 - 00:11:56.454, Speaker B: So this is convex. And of course, it's also and positive one homogeneous. And, you know, you know, d plus lambda, g, nabla, f is equal to lambda.
00:11:56.614 - 00:12:03.154, Speaker A: D plus g, nabla, f, whatever, lambda positive, just by scaling.
00:12:03.574 - 00:12:04.022, Speaker C: It's a.
00:12:04.038 - 00:12:10.634, Speaker B: Right, it's. It's a. It's a derivative. The derivative is always, you know, one homogeneous in the differential variety.
00:12:11.134 - 00:12:12.034, Speaker A: Makes sense.
00:12:13.594 - 00:12:16.906, Speaker B: So this guy is convex and positive one homogeneous.
00:12:17.010 - 00:12:35.814, Speaker A: Of course, d minus is concave and one homogeneous for the same reasons. It's concave, concave, and positively one homogeneous.
00:12:40.734 - 00:12:41.310, Speaker C: Right?
00:12:41.422 - 00:12:43.354, Speaker B: But then d plus is equal to d minus.
00:12:45.134 - 00:12:45.542, Speaker C: So.
00:12:45.598 - 00:13:00.102, Speaker A: So the map that takes g and returns dg, this is both convex and concave. So it's a fine and it is positive one homogeneous.
00:13:00.158 - 00:13:02.754, Speaker B: And also, you know, negative one homogeneous. So this is linear.
00:13:06.794 - 00:13:07.586, Speaker C: Okay?
00:13:07.730 - 00:13:09.362, Speaker B: So, you know, half of this is.
00:13:09.378 - 00:13:10.554, Speaker A: Done in some sense, right?
00:13:10.634 - 00:13:12.094, Speaker B: So it's linear in one variable.
00:13:15.954 - 00:13:18.094, Speaker A: Right? Now.
00:13:22.234 - 00:13:22.974, Speaker C: This.
00:13:23.554 - 00:13:24.734, Speaker B: This is obvious.
00:13:26.954 - 00:13:27.866, Speaker A: Because if you look.
00:13:27.890 - 00:13:40.944, Speaker B: At the definition, this is obviously from the analogous property of minimal weak upper gradients. You remember that if f and f prime agree on a certain set, then their minimal weak upper guidance also agree.
00:13:40.984 - 00:13:41.964, Speaker A: On the same set.
00:13:42.984 - 00:13:51.032, Speaker B: Now, you write the definition, okay? Perhaps I should say, of dg, you know, nabla f is equal to dg prime.
00:13:51.208 - 00:13:52.144, Speaker A: Nabla f prime.
00:13:52.184 - 00:13:53.560, Speaker B: I mean, desensitism.
00:13:53.752 - 00:13:59.078, Speaker A: So on this set, you agree just by.
00:13:59.126 - 00:14:04.134, Speaker B: By, you know, by definition, in fact, also the plus and the minus are local in the very same set.
00:14:04.174 - 00:14:08.634, Speaker A: All right? So this is proved. So that's okay in some sense.
00:14:10.054 - 00:14:10.694, Speaker B: Let's check.
00:14:10.734 - 00:14:11.198, Speaker C: Let's check.
00:14:11.246 - 00:14:28.768, Speaker B: Cautious wax. Okay? Cautious wax is also obvious. So why is that the case? Well, what it is the, let's say D plus g. I mean, let's have a look, actually. You know. Yeah, let's have a look, perhaps to that expression.
00:14:28.816 - 00:14:33.564, Speaker A: So that's the limit in epsilon of.
00:14:34.344 - 00:14:47.928, Speaker B: Actually. Yeah. Of the difference quotients. So, but let me say then that this is less or equal than the lenin. Let me expand the minimum weak upgrade over there. So that is, you know, df plus epsilon.
00:14:47.976 - 00:14:49.874, Speaker A: Dg, right?
00:14:50.374 - 00:15:13.190, Speaker B: Squared minus dx squared. Squared divided by two. Epsilon, you agree? I should be a little bit careful of what I mean by this. One possibility is take any sequence as epsilon goes to zero and you have a sequence of l one functions.
00:15:13.302 - 00:15:17.656, Speaker A: Take the point was limit of that sequence, any sequence.
00:15:17.680 - 00:15:21.204, Speaker B: So this is going to be less or equal than any limit you would obtain in this way.
00:15:21.744 - 00:15:25.192, Speaker A: Okay, if I don't do this, I.
00:15:25.208 - 00:15:29.400, Speaker B: Should be, I should explain what it is the essentially mean for extramurally of functions.
00:15:29.472 - 00:15:33.240, Speaker A: Okay, okay. Now you expand, you expand.
00:15:33.272 - 00:15:57.494, Speaker B: This is equal. Now you expand. You know, the df disappears inside in front of the dg squared. That is still an epsilon does not simplify. The only thing that remains of order, of the correct order is the FDG. And then, and then D minus is bounded from below by the, you know, by the same trick. So the cautious part is obtained.
00:15:57.494 - 00:16:00.162, Speaker B: So that's, that's okay.
00:16:00.338 - 00:16:01.094, Speaker C: Um.
00:16:02.114 - 00:16:03.374, Speaker A: So that's okay.
00:16:03.834 - 00:16:16.600, Speaker B: What about this? What if you plug g equal f, you get, you know, uh, the differential of f plus epsilon f is the same as the differential f times one plus epsilon.
00:16:16.712 - 00:16:20.124, Speaker A: And so you get this, right.
00:16:21.944 - 00:16:31.168, Speaker C: Sorry, sorry.
00:16:31.256 - 00:16:33.844, Speaker B: You say, you think, you say that's something wrong.
00:16:36.264 - 00:16:37.504, Speaker A: Yes, that's okay, right?
00:16:37.584 - 00:16:38.524, Speaker B: I mean these.
00:16:39.504 - 00:16:40.284, Speaker A: Sorry.
00:16:42.224 - 00:16:49.088, Speaker B: No, no, if, if. No problem.
00:16:49.136 - 00:16:54.816, Speaker A: Okay, other questions. Yeah, don't feel afraid to, to ask questions or whatever. That's okay.
00:16:54.920 - 00:16:55.644, Speaker C: Uh.
00:16:57.104 - 00:17:03.052, Speaker B: Okay, so I've done this, this, and this. Let me prove, let me prove the chain rule.
00:17:03.188 - 00:17:03.964, Speaker A: Okay.
00:17:04.124 - 00:17:26.968, Speaker B: Notice that we already know, we already know that if f, if I is leapface and c one. And actually, and if g is, let me produce it. So I already know that phi composition g is w twelve. If g is in w twelve and phi is c one and liches, right? That's that, that we already proved.
00:17:27.116 - 00:17:31.344, Speaker A: Okay, so at least this makes sense, right?
00:17:31.424 - 00:17:34.564, Speaker B: Because I can, you know, there is a suburb quantity over here.
00:17:35.824 - 00:17:36.564, Speaker C: Now.
00:17:39.664 - 00:17:41.204, Speaker B: If phi is linear.
00:17:43.224 - 00:17:44.284, Speaker A: This holds.
00:17:45.184 - 00:18:01.034, Speaker B: Because I already proved, actually, let me, let me, I'm making confusion. F and g. So let's check whether, whether this chain rule holds. Let me write this way. I claim that this is equal to phi, prime composition DG.
00:18:02.654 - 00:18:07.134, Speaker A: All right, now if phi is linear, I know this.
00:18:07.254 - 00:18:09.830, Speaker B: And I know this because by linearity.
00:18:09.862 - 00:18:17.966, Speaker A: Of this guy, if phi is a constant, I also know this, because if.
00:18:17.990 - 00:18:19.794, Speaker B: I is constant, this is zero.
00:18:21.024 - 00:18:21.872, Speaker A: All right?
00:18:22.048 - 00:18:34.312, Speaker B: And so I need just to check that the differential of a constant function applied to grad f is zero. But that you can check, you know, the difference. The minimum weak comprehensive of f plus.
00:18:34.368 - 00:18:42.496, Speaker A: Constant is the same as the minion weak of f. Okay? That's, doesn't really change, okay?
00:18:42.640 - 00:19:20.850, Speaker B: So if you are bothered by the fact that adding a constant f is not anymore in w twelve, feel free to see that the reference measure is finite. Say m is probability. So, you know, I add a constant to a function, still remains in it. Okay, this, of course is irrelevant. I mean, all these arguments are extremely local in nature, and so the global mass is not really a point. A way to handle this from the technical level is to work with the definition of weak upper gain given by test plans where the integrability of f plays no role. And there you see that constant functions are always in this over class s two with zero upper gradient for trivial reasons.
00:19:20.922 - 00:19:21.654, Speaker A: Okay.
00:19:23.554 - 00:19:25.514, Speaker B: So, so this is true also.
00:19:25.594 - 00:19:27.810, Speaker A: If I is constant, right?
00:19:28.002 - 00:19:30.042, Speaker B: So bilinearity is true if I is.
00:19:30.058 - 00:19:35.734, Speaker A: A fine, okay, so by locality.
00:19:37.834 - 00:19:38.194, Speaker C: By.
00:19:38.234 - 00:20:00.850, Speaker B: Locality, this is also true if phi is piecewise of phi, right? If phi is a function, you know, goes like this. So I can divide error in a family, accountable family of intervals, you know, and, you know, PI in each of these is a fine. Okay, as far as these global ellipses, by, you know, by locality, this works.
00:20:00.962 - 00:20:04.178, Speaker A: Okay, so now I want to pass the limit.
00:20:04.346 - 00:20:07.894, Speaker B: And what I want to, and I want, what I notice is that.
00:20:20.284 - 00:20:22.104, Speaker A: And what I notice is that.
00:20:23.764 - 00:20:33.104, Speaker B: So if phi is c one ellipses, then there exists a sequence, phi n of piecewise, of fine.
00:20:40.064 - 00:20:40.960, Speaker A: And global, and.
00:20:40.992 - 00:21:12.004, Speaker B: The uniform ellipses, functions that is so converging uniformly to phi and such that phi and prime converges also in some sense uniform. You at least point twice to five prime for almost every z. Five prime of z.
00:21:12.824 - 00:21:13.684, Speaker C: Okay.
00:21:15.504 - 00:21:46.500, Speaker B: Let me say for every z, okay, for every z, for every z. Except accountable, let me go this way. Except accountable number, I have to, you know, these are piecewise, I'm fine. So on some points that will be not differentiable, forget about those points. On the rest, they converge, you know, if you want even uniformly. So what am I doing here? I'm just saying I have a globally c one elliptic function.
00:21:46.532 - 00:21:50.548, Speaker A: I just divide our uniform in interval.
00:21:50.596 - 00:21:51.908, Speaker B: And then each of these interval, you.
00:21:51.916 - 00:21:55.844, Speaker A: Know, I take the finite interpolation, I pick this sequence.
00:21:56.464 - 00:21:57.324, Speaker C: Okay?
00:21:59.544 - 00:22:01.536, Speaker B: Okay, now let, so I know that.
00:22:01.560 - 00:22:08.764, Speaker A: This is true for every n, this is true. Right?
00:22:11.664 - 00:22:21.144, Speaker B: Let's check what happens, what happens on both sides. So here I have a fixed l one function, and I multiply it by.
00:22:21.724 - 00:22:25.708, Speaker A: A function which is almost, almost point.
00:22:25.756 - 00:22:34.716, Speaker B: Wise convergent to five prime composition. Okay, this goes, this goes, in fact, I have point wise convergence. So this goes to phi prime composition.
00:22:34.780 - 00:22:38.556, Speaker A: G. Except, you know, except possibly, you.
00:22:38.580 - 00:22:48.552, Speaker B: Know, if you want on g minus one of, you know, countable numbers, call it ln, except on g minus one. On this pre major, I don't know.
00:22:48.568 - 00:22:50.964, Speaker A: What is happening, right?
00:22:51.464 - 00:22:54.400, Speaker B: And this pre made a priori, a priority.
00:22:54.432 - 00:22:58.584, Speaker A: This pre major could have positive measure a priori.
00:22:58.744 - 00:22:59.524, Speaker C: Okay?
00:23:02.864 - 00:23:03.688, Speaker A: But in fact.
00:23:03.776 - 00:23:07.640, Speaker B: So there are a couple of ways to handle this, this problem.
00:23:07.752 - 00:23:11.724, Speaker A: So either I prove that on this.
00:23:11.764 - 00:23:16.744, Speaker B: Pre image, the differential of g is equal to zero almost everywhere.
00:23:18.604 - 00:23:19.012, Speaker C: At this.
00:23:19.028 - 00:23:22.252, Speaker B: In fact, I know, right? Because on the primage of any given.
00:23:22.308 - 00:23:24.564, Speaker A: Point, you know, for any, perhaps any.
00:23:24.604 - 00:23:25.836, Speaker B: Point, let me point it out.
00:23:25.860 - 00:23:28.692, Speaker A: So on, you know, on.
00:23:28.868 - 00:23:36.498, Speaker B: So I claim that this is zero. On the preimage of any singleton z.
00:23:36.636 - 00:23:40.166, Speaker A: Whenever is that real, this guy is zero.
00:23:40.350 - 00:23:41.606, Speaker B: How do I know this?
00:23:41.750 - 00:23:45.206, Speaker A: Well, because on this set, g is.
00:23:45.230 - 00:24:06.368, Speaker B: Identically equal to the function constant or equal to z. And the constant function z has zero minimum wake up agreement. So on this side, this is zero. But if this is zero on this set, it is zero on the pre image, almost surely in deeper image of any countable set, just by, you know, countable unit.
00:24:06.496 - 00:24:08.284, Speaker C: Okay? So.
00:24:09.984 - 00:24:16.004, Speaker B: On this set to work, I might not have convergence of differentials of the riva. This is identically zero.
00:24:18.224 - 00:24:19.084, Speaker A: Okay?
00:24:21.144 - 00:24:41.040, Speaker B: And for the same reason, the same set, also this left hand side is identically zero. Because we proved, because we proved, we know that the differential, you know, we know that the different, the mean. Sorry, I speak, say differential. Of course, I mean minimal, weak upper gradient, we proved that this is equal to five. Prime composition, g dg.
00:24:41.152 - 00:24:41.804, Speaker A: Right?
00:24:43.624 - 00:24:46.504, Speaker B: So, so whenever this is zero. So this is zero.
00:24:46.664 - 00:24:50.592, Speaker A: Okay, so on this set where I.
00:24:50.608 - 00:24:54.724, Speaker B: Don'T have convergence of derivatives, because derivatives do not exist, both sides are zero.
00:24:56.144 - 00:25:02.708, Speaker A: Okay, please. But n is not, by the way, I chose.
00:25:02.836 - 00:25:21.852, Speaker B: Yes, this was enough. So I have piecewise, a fine function, each one, you know, it's, you know, I'm really doing, you know, the series. I even, you know, if necessary, I also avoid picking intervals that accumulate and then just, I'm really picking uniform partitions.
00:25:21.908 - 00:25:22.252, Speaker A: Whatever.
00:25:22.308 - 00:25:24.520, Speaker C: Okay, no problem.
00:25:24.632 - 00:25:43.124, Speaker B: No problem. So that's. So what I'm saying here is that on this set, which is fixed, once the chosen sequence has been picked, on this set, I have the both sides are zero almost everywhere. So in particular, I have convergence because they convert to zero.
00:25:45.944 - 00:25:46.472, Speaker C: Okay?
00:25:46.528 - 00:25:52.974, Speaker B: So all this proves that this right hand side converges to five prime composition.
00:25:53.054 - 00:26:03.126, Speaker A: G d g n, say inner one. This is l one function, and I'm.
00:26:03.150 - 00:26:07.314, Speaker B: Multiplying this l one function by a sequence of uniformly bounded functions.
00:26:08.094 - 00:26:08.954, Speaker A: Okay?
00:26:09.574 - 00:26:11.494, Speaker B: They are converging almost everywhere.
00:26:11.654 - 00:26:17.614, Speaker C: Okay. All right. Okay.
00:26:18.154 - 00:26:36.494, Speaker B: Perhaps another comment that I want to make. There's another way of handling this issue of non existence of the reality non convergence is that I can arrange a little bit the functions phi n, so that this set g minus one of n has zero measure.
00:26:37.394 - 00:26:38.818, Speaker A: This I can always do.
00:26:38.946 - 00:26:54.436, Speaker B: How do I do? Well, here I have a lot of wiggling room in where to choose these points of non differentiabilities of the function. It is clear that for almost, you know, g one. So the measure of the primage of.
00:26:54.460 - 00:26:58.548, Speaker A: A singleton, this is zero for every.
00:26:58.596 - 00:27:20.512, Speaker B: Z except a countable number. Just because these sets are all Borel and disjoint, and you cannot have more than countable sets of positive measure if the total measure is finite or sigma finite.
00:27:20.608 - 00:27:24.544, Speaker A: Okay, so if by chance you happen.
00:27:24.584 - 00:27:48.656, Speaker B: To have chosen a point z where g minus one as positive measure, just change it a little bit to find a point with zero mass and durtal. Okay, so in other case I get this convergence. So it remains to prove that this goes to, goes in l one to the differential of phi composition g nabla.
00:27:48.680 - 00:27:52.656, Speaker A: F. Right, but this is true because.
00:27:52.760 - 00:28:00.464, Speaker B: So what's the difference? So what's the difference between it's by continuing. So basically, so what's the difference between any write this way.
00:28:00.504 - 00:28:12.874, Speaker A: Dg nabla f minus dg prime nabla, f. Well, bilinearity in g, this is.
00:28:12.914 - 00:28:15.706, Speaker B: D of g minus g prime nabla.
00:28:15.730 - 00:28:16.294, Speaker C: F.
00:28:18.554 - 00:28:21.738, Speaker A: Okay, and now by Cauchysvarts, I.
00:28:21.746 - 00:28:23.426, Speaker B: Mean, you know, you know, that kind.
00:28:23.450 - 00:28:29.134, Speaker A: Of cautious parts, right?
00:28:32.264 - 00:28:56.032, Speaker B: Okay, so I apply this in our case, and what I get is that the disparence between phi n composition g null f minus, this is, you know, bounded by the minimum weak upper gate.
00:28:56.088 - 00:29:03.224, Speaker A: Of phi minus phi n, composition g d f. Right?
00:29:04.804 - 00:29:13.864, Speaker B: And this is bound in turn. This is bounded by phi prime minus phi m prime composition g d g d f.
00:29:19.644 - 00:29:21.900, Speaker A: But now, as before, except.
00:29:21.932 - 00:29:29.910, Speaker B: On accountable number where of zs, where. Anyway, you know, whose premise this is zero. These guys are uniformly bounded and point.
00:29:29.942 - 00:29:32.434, Speaker A: Twice converge to zero.
00:29:35.374 - 00:29:36.234, Speaker C: Okay?
00:29:37.294 - 00:29:39.510, Speaker B: But I don't know where this converge to zero. This is zero.
00:29:39.582 - 00:29:42.846, Speaker A: So in any case, this guy is.
00:29:42.870 - 00:29:52.150, Speaker B: Converging to zero almost surely. But this is uniformly bounded. This is nr one, so converges to zero one. So I converted one in here.
00:29:52.342 - 00:30:02.874, Speaker A: So I proved the chain rule. Okay? Oh, by the way, by the way.
00:30:04.854 - 00:30:36.230, Speaker B: The work, let me clarify with this, the work that we do improving this proposition. I mean, even if we knew now these theory of modules that I presented a couple of weeks ago in my talk, the work should still be there. So it's not that, you know, I'm now doing some job that is in fact, you know, in some sense becomes old the moment I introduced the concept of actual differential. That kind of, this kind of proof should still be there. So we are, in some sense, making now some job that we should do anyway, do later on.
00:30:36.262 - 00:30:42.554, Speaker A: Okay? So I proved the chain rule, all right?
00:30:42.954 - 00:30:46.894, Speaker B: Now, Leibniz rule, you know, Leibniz rule is a consequence of chain rule.
00:30:47.194 - 00:30:48.962, Speaker A: You know, they don't tell us, but.
00:30:48.978 - 00:30:58.730, Speaker B: That'S what it is. Just, there are various ways of doing this.
00:30:58.802 - 00:31:02.494, Speaker A: So one is, I guess so this is that, okay?
00:31:04.594 - 00:31:24.434, Speaker B: One is to. Is to just do what it is you can. I mean, as soon as you put a non linearity, you know, basically any non linearity, you are done. So, one possibility is to work with the square. The other one is to work with the log.
00:31:25.214 - 00:31:26.514, Speaker A: So what is this guy?
00:31:27.214 - 00:31:41.400, Speaker B: I have two ways of computing this. Either I first apply the chn rule, or I. Or I first spun the square. Let me first apply the chain rule. If I first apply the chain rule to, you know, this is the square of this sublevel function.
00:31:41.432 - 00:31:42.720, Speaker A: G one plus g two.
00:31:42.912 - 00:31:51.204, Speaker B: So, by the chain rule, this is twice g one plus g two. The differential of g one plus g two.
00:31:51.504 - 00:31:54.128, Speaker A: Not black. Okay?
00:31:54.256 - 00:32:08.560, Speaker B: Which, of course, I continue. This is. Well, I don't write what it is, okay? Or I first expand the square, and this is then equal differential of what is g one squared plus g two squared plus twice g one.
00:32:08.592 - 00:32:11.844, Speaker A: G two applied to naval.
00:32:13.144 - 00:32:17.764, Speaker B: Okay, now, in both cases, I use the linearity of the differential.
00:32:18.904 - 00:32:19.552, Speaker C: And.
00:32:19.688 - 00:32:25.280, Speaker B: And then, and then let me, let me use the linearity. And then here I still have the term g one squared and g two.
00:32:25.312 - 00:32:29.794, Speaker A: Squared, to which I apply the chain rule, right?
00:32:29.914 - 00:32:40.746, Speaker B: So this is equal to twice g one. The differential of g one applied to grad f plus twice g two. The differential of g two applied to.
00:32:40.770 - 00:32:45.738, Speaker A: Grade f and then plus twice differential.
00:32:45.786 - 00:32:54.090, Speaker B: Of g one g two. Now, you use the linearity of the differential.
00:32:54.122 - 00:32:54.266, Speaker C: Yeah.
00:32:54.290 - 00:33:00.832, Speaker B: You span, you span the. The product and you get the, you know, this guy. And this guy simplifies out and you.
00:33:00.848 - 00:33:03.564, Speaker A: Remain with the lab instrument. Okay?
00:33:07.304 - 00:33:08.124, Speaker C: Okay.
00:33:10.664 - 00:33:11.344, Speaker A: All right.
00:33:11.464 - 00:33:28.064, Speaker B: So it remains, or perhaps I should pay a little bit, just a little bit of attention. So this should be. So when you apply the, the leibniz rule, the function should not only be subreddit, but also bounded. So what I'm using here.
00:33:28.484 - 00:33:31.028, Speaker A: So let me perhaps make a comment.
00:33:31.156 - 00:33:54.916, Speaker B: The square is not elliptic function. I cannot apply the chain rule with phi equals z squared, because phi is not globally riches. So what is there doing in practice? I do. First of all, I need to work, and for a good reason. I mean, even on rd, the square of a w twelve function is not the w twelve. U have problems of integrability of both the functions and degrading. So what I actually should have said previously, I forgot I said it.
00:33:54.940 - 00:33:57.924, Speaker A: Now, g one and two are bounded, okay?
00:33:58.084 - 00:34:11.108, Speaker B: And if they are bounded, you know, phi squared is, you know, is lip sheets on their image. And when you are out of their image, do whatever you want to smoothen them out, to smoothen them out and making globally c one ellipses.
00:34:11.196 - 00:34:14.563, Speaker C: All right. Okay.
00:34:17.543 - 00:35:00.664, Speaker B: It'S now time to prove symmetry, which is, you know, the hardest part in some sense of this. So symmetry comes after I prove two claims. The first claim is that there is symmetry at the integrated level. And the second claim is. Well, I guess, well, I guess it's just this. I guess it's just this claim.
00:35:01.324 - 00:35:12.724, Speaker A: Okay, suppose for a moment that we know this. So, yeah, suppose that we know this then. So then.
00:35:14.984 - 00:35:31.322, Speaker B: I'm assuming I already know this. Then what I get is that, is that actually I can prove this easily. There's no need of bothering. So this is true because the Chig energy is a quadratic form. This is a global statement.
00:35:31.488 - 00:35:32.366, Speaker A: Okay?
00:35:32.550 - 00:35:36.390, Speaker B: This is on one side, this is equal to what? This is equal to the limb.
00:35:36.462 - 00:35:38.942, Speaker A: Let me, this is actually.
00:35:38.998 - 00:35:44.886, Speaker B: So in both cases, let me, let me say in both cases, then maybe I expand a little bit. But in both cases, you know, this.
00:35:44.910 - 00:35:47.646, Speaker A: Is what previously I indicated in this way.
00:35:47.670 - 00:35:52.758, Speaker B: Now this is the scatter product, you know, induced by the global chivalry, which.
00:35:52.766 - 00:35:55.554, Speaker A: Is a quadratic form. Okay.
00:35:57.834 - 00:35:59.574, Speaker B: So, so this is true.
00:36:00.514 - 00:36:01.194, Speaker C: Make sense?
00:36:01.274 - 00:36:07.774, Speaker B: I could, I could write, I could expand a little bit more if you want. And notice that. And notice that.
00:36:08.394 - 00:36:11.858, Speaker A: So on one side I have, I.
00:36:11.866 - 00:36:21.330, Speaker B: Guess if you want this. So actually, let me write an identity. Let me write an identity. The trigger energy of g plus epsilon, f minus sugar energy of f, of.
00:36:21.362 - 00:36:25.382, Speaker A: G divided by epsilon minus epsilon, the.
00:36:25.398 - 00:36:54.254, Speaker B: Trigger energy of f. This is equal. I'm proving, you know, if you want, I'm reproving with this identity, the symmetry of a scalar product induced by a quadratic form. This is an identity that goes for every FNG.
00:36:54.914 - 00:36:59.546, Speaker A: It's basically the parallelogram identity, or a.
00:36:59.570 - 00:37:02.090, Speaker B: Consequence of the parallelogram identity written in disguise.
00:37:02.242 - 00:37:05.722, Speaker A: Okay, now if you believe in these.
00:37:05.778 - 00:37:15.594, Speaker B: And you should, I mean, this falls from quadraticity. I mean, it's a, you know, there's no analysis in here, just algebra. Basically, when you let epsilon go to zero in here, you get this.
00:37:16.574 - 00:37:18.150, Speaker A: No, you get this. Sorry.
00:37:18.342 - 00:37:20.510, Speaker B: Here, this object is basically is linear.
00:37:20.542 - 00:37:22.014, Speaker A: In f. So you get this and.
00:37:22.054 - 00:37:23.174, Speaker B: You get epsilon go to zero.
00:37:23.214 - 00:37:24.714, Speaker A: Here you get that one.
00:37:25.974 - 00:37:29.074, Speaker B: Okay, so you have, you have that symmetry.
00:37:29.534 - 00:37:31.554, Speaker C: All right, now.
00:37:38.654 - 00:37:50.914, Speaker B: That I have symmetry, let me think for a second. If I can do things.
00:37:55.494 - 00:37:56.394, Speaker A: All right.
00:37:56.734 - 00:38:11.614, Speaker B: If I have symmetry, no, no, I have to still work a little bit. Okay, symmetry. And let me, so another claim.
00:38:13.434 - 00:38:13.746, Speaker C: Is.
00:38:13.770 - 00:38:22.654, Speaker B: That I have the integrated chain rule. So the integral of differential of g.
00:38:22.994 - 00:38:28.130, Speaker A: Applied to grad PI composition f with.
00:38:28.162 - 00:38:34.414, Speaker B: The same assumptions on phi. So phi is globally c one. And Lipschitz, this is the integral of phi, prime.
00:38:38.034 - 00:38:40.374, Speaker A: Composition f d g.
00:38:45.714 - 00:38:46.002, Speaker C: I.
00:38:46.018 - 00:38:57.818, Speaker B: Cannot deduce this from the symmetry and this rule for the differential, because if I use the symmetry, then I get, okay, differential of PI composite. I mean, let me show you. So this, what I know for sure.
00:38:57.866 - 00:39:00.244, Speaker A: Is that, I mean this is, this.
00:39:00.284 - 00:39:04.396, Speaker B: Nabla g. But then these, for these, I know the chain, the, you know.
00:39:04.420 - 00:39:09.140, Speaker A: The chain rule, but then I still.
00:39:09.172 - 00:40:03.934, Speaker B: Don'T know that this is equal to this because I know the quality of these things only when just of their integral. I still don't know the point wise identity, right? If I knew the pointwise identity, I would have put bilinearity and symmetry, but I still don't know. So I still have to work a little bit to prove this. But let me, let me show you how the proof completes if I know this. Now, if you know this, what I would do is the following is fix say a function h, which is say lipsticks and bounded with bounded support, maybe. And let's look at the functional that takes f in w twelve and returns the integral of f of h dx squared. My dream is proving that this function is a quadratic form.
00:40:03.934 - 00:40:07.814, Speaker B: I know this by assumption if h is identically one.
00:40:08.194 - 00:40:09.242, Speaker A: But I want to prove that this.
00:40:09.258 - 00:40:10.298, Speaker B: Is still a quadratic form for any.
00:40:10.346 - 00:40:14.322, Speaker A: H, okay, which is, I mean, if.
00:40:14.338 - 00:40:20.856, Speaker B: I do this, that would be the same as, you know, integrating over a certain set by approximation, right? So that would be a localized version.
00:40:20.880 - 00:40:26.160, Speaker A: Of the global quadraticity that I begin right now.
00:40:26.272 - 00:40:27.264, Speaker B: What is the, this?
00:40:27.344 - 00:40:30.416, Speaker A: Well, this is equal to, let me write this way.
00:40:30.440 - 00:40:33.604, Speaker B: So this is the differential of h times f.
00:40:36.904 - 00:40:37.764, Speaker A: Minus.
00:40:39.704 - 00:40:42.204, Speaker B: F differential of h.
00:40:45.124 - 00:40:55.580, Speaker A: Right here I'm applying, I am applying the Leibniz rule. Okay, you can of course, you know.
00:40:55.612 - 00:41:23.104, Speaker B: Be annoyed a little bit by the fact that I didn't say anything about f here on the leibnizor, we should be in l infinity. So if you want, let's pick f in w twelve and l infinity. If you want it's not really necessary because in this case h is lip sheets with bounded support. So uh, h times f remains in w twelve even if f is in w twelve. But that's, you know, let me put this integral assumption f. So by the lambda tool, once I drop this out, I get this.
00:41:24.564 - 00:41:29.780, Speaker A: Okay, now let me rewrite this.
00:41:29.852 - 00:41:36.196, Speaker B: Actually, let me rewrite this third, this right hand side in polo. So by the chain rule, this is.
00:41:36.220 - 00:41:39.064, Speaker A: Equal to, you know, grad, f squared.
00:41:39.104 - 00:41:51.976, Speaker B: Over two by the general, the level of gradients. And this is equal to, so differential of f squared over two. Nabla.
00:41:52.000 - 00:42:05.342, Speaker A: H. Okay, now here's the thing. The map that takes f and returns this expression, this first integral is bilinear, right?
00:42:05.478 - 00:42:10.438, Speaker B: Because, sorry. Is a quadratic form, what I meant. So it's linear on this entry and.
00:42:10.446 - 00:42:14.430, Speaker A: Is linear also on this entry basically, right?
00:42:14.542 - 00:42:16.350, Speaker B: Yeah, on this entry by d symmetry.
00:42:16.382 - 00:42:18.566, Speaker A: And the linearity on g makes sense.
00:42:18.590 - 00:42:19.394, Speaker B: What I'm saying.
00:42:21.094 - 00:42:21.834, Speaker C: Right?
00:42:23.134 - 00:42:26.274, Speaker B: And the map that. So, so this is a quadratic form.
00:42:27.614 - 00:42:28.434, Speaker A: Yeah.
00:42:29.594 - 00:42:30.334, Speaker C: Right.
00:42:30.834 - 00:42:42.978, Speaker B: Because it's linear on both entries in some sense. And what about this? Well, this is not psychic form. I mean, there's an x squared and then after f squared, I do something which is linear in f because on the left hand side, this is linear.
00:42:43.146 - 00:42:44.774, Speaker A: So this is also quadratic form.
00:42:45.754 - 00:42:47.454, Speaker B: So this is a quadratic form.
00:42:49.954 - 00:42:53.410, Speaker A: Make sense, right?
00:42:53.562 - 00:43:00.884, Speaker B: And once it is a quadratic form on this space of sublep and bounded functions by truncation, you see that is a quadratic form on the space of.
00:43:01.184 - 00:43:04.364, Speaker A: The whole w twelve, right?
00:43:05.344 - 00:43:20.296, Speaker B: And if you want by polarization after this quadratic form, you conclude, you conclude that the integral of h d f n g is equal to the integral of h d g. Nabla?
00:43:20.320 - 00:43:28.264, Speaker A: F, right, for every f g and h, right?
00:43:28.964 - 00:43:43.864, Speaker B: But once you have, you know, these, these two l one functions, they agree once you integrate them against any lips and bounded function and these forces, these forces dg, nabla, f, the integrals to be the same.
00:43:44.284 - 00:43:45.624, Speaker A: Make sense what I'm saying.
00:43:49.084 - 00:43:49.660, Speaker C: Right?
00:43:49.772 - 00:43:55.524, Speaker B: So, so this proves that once, this proves at once linearity and symmetry.
00:44:00.984 - 00:44:01.584, Speaker C: Right?
00:44:01.704 - 00:44:06.364, Speaker B: Because once the symmetric, it was linear in one entry, becomes linear also in the other one.
00:44:07.224 - 00:44:08.124, Speaker A: Okay.
00:44:10.624 - 00:44:52.204, Speaker B: Let me, let me now just prove this. And then we come back to our claim. This is trivial actually, because so if so, what is the issue? If I pick v, sorry, what was our Laplacian? Our Laplacian was the element of minimum norm in the sub differential of the trigger energy, right? So let me pick v in the.
00:44:52.244 - 00:44:56.308, Speaker A: Sub differential of the trigger energy, left okay.
00:44:56.476 - 00:45:14.214, Speaker B: And what I wonder is what is the integral or actually what is the scalar product of v and epsilon g, this carpal in a two. And what I know is that, you know, the trigger energy of f plus, this is less or equal than the trigger energy of f plus epsilon g.
00:45:17.074 - 00:45:17.866, Speaker C: Right?
00:45:18.050 - 00:45:18.906, Speaker A: That's what I know.
00:45:18.970 - 00:45:33.290, Speaker B: So for every g and every epsilon, this is true. So what I get is that epsilon vg, some probability two, is less or equal than, you know, trigger energy f plus epsilon g minus trigger energy of.
00:45:33.322 - 00:45:36.414, Speaker A: F divided by g by epsilon.
00:45:37.254 - 00:45:53.022, Speaker B: No justice. So I'm really going through once again the same proof of the weak integration by parse formula. But now with a better understanding of what this is, because now, if epsilon is positive, if epsilon is positive, I.
00:45:53.038 - 00:46:00.154, Speaker A: Can divide by this and then take the limit as epsilon goes to zero, right?
00:46:00.574 - 00:46:11.974, Speaker B: But this is equal. Now I know exactly what it is. This is equal dfdg, right? I should have a problem with a sign somewhere.
00:46:14.554 - 00:46:16.018, Speaker A: No, that's okay. That's okay.
00:46:16.066 - 00:46:17.054, Speaker B: Yes. Yeah.
00:46:18.714 - 00:46:19.194, Speaker C: Okay.
00:46:19.234 - 00:46:27.134, Speaker B: And then I could then either play with epsilon negative, or just notice that, you know, both sides of this inequality are linear in G.
00:46:28.894 - 00:46:29.358, Speaker C: And if I.
00:46:29.366 - 00:46:37.598, Speaker B: Have an inequality between two linear things, that this must be an equality. You cannot have one linear function g replace g with minus g. That's the.
00:46:37.606 - 00:46:44.270, Speaker A: Easy, okay, and then when we get this right, so for any, in fact.
00:46:44.302 - 00:46:58.164, Speaker B: For any element, for any element in this differential of the trigger energy, what I get is that, you know, v scalar, the integral you want of vg is equal to the inter of dfdg.
00:46:59.944 - 00:47:03.672, Speaker A: So this proves in particular that v is unique, right?
00:47:03.728 - 00:47:06.604, Speaker B: And that must be exactly minus the Laplacian and.
00:47:09.104 - 00:47:13.752, Speaker A: Okay, so Italy remains to prove this claim. Let me prove this.
00:47:13.888 - 00:47:27.052, Speaker B: And then we take a breath. This requires just a little bit of work.
00:47:27.148 - 00:47:32.692, Speaker A: So first of all, I notice, I notice that the map, so notice that.
00:47:32.708 - 00:47:35.484, Speaker B: The map that takes f and returns.
00:47:35.604 - 00:47:44.804, Speaker A: D plus g null f. So this is actually the integral, this the m, this is equal.
00:47:44.924 - 00:47:57.054, Speaker B: So we have already noticed that this is equal to the inf over the positive epsilon of trigger energy of f plus epsilon g minus chigger energy of f divided by epsilon, right?
00:48:00.634 - 00:48:04.658, Speaker A: Now for any fixed epsilon, this expression.
00:48:04.706 - 00:48:13.774, Speaker B: Is continuous in the solver f norm, the solver f norm of f plus epsilon g, the solver norm of f. I mean the gradient part of the sub.
00:48:14.974 - 00:48:18.190, Speaker A: So this quantity is the inf of.
00:48:18.222 - 00:48:30.914, Speaker B: A family of continuous functions. So this map is upper semi continuous. Of course, for the same reason.
00:48:32.614 - 00:48:32.998, Speaker C: This.
00:48:33.046 - 00:48:40.474, Speaker A: Guy is lower semicontinuous okay?
00:48:40.874 - 00:48:42.746, Speaker B: So we know that these two agree.
00:48:42.850 - 00:48:52.694, Speaker A: So we conclude that the map that takes f and returns the integral of dg Nabl, this is continuous.
00:48:54.714 - 00:48:55.574, Speaker C: Okay?
00:48:59.954 - 00:49:04.094, Speaker A: And there's one other thing that we know is that.
00:49:04.874 - 00:49:25.278, Speaker B: I don't know if it is linear. I don't, but I know that it is positive, homogeneous. So I claim so, or if you want, I claim that dg nabla alpha.
00:49:25.326 - 00:49:32.794, Speaker A: F. I claim that this is equal to alpha digi nabla f. And why is this the case?
00:49:33.374 - 00:49:43.214, Speaker B: Well, I basically observe this is a consequence of the following identity. The minimal weak upper gradient of alpha.
00:49:43.294 - 00:49:47.406, Speaker A: F plus epsilon g squared minus the.
00:49:47.430 - 00:49:49.674, Speaker B: Minimum weak upper gradient of alpha f.
00:49:50.134 - 00:49:55.022, Speaker A: Squared divided by two epsilon. This is equal.
00:49:55.078 - 00:50:03.898, Speaker B: I mean, if you take alpha out in both cases and you see what this comes out, this is the minimum we get of f plus epsilon over.
00:50:03.946 - 00:50:16.490, Speaker A: Alpha g squared minus the minimum weak appropriate of x squared divided by twice epsilon over. Also, just for trivial reasons, okay?
00:50:16.682 - 00:50:18.226, Speaker B: But now, when you let epsilon go.
00:50:18.250 - 00:50:20.658, Speaker A: To zero, here, you get this.
00:50:20.786 - 00:50:21.762, Speaker B: If you get epsilon, go to.
00:50:21.778 - 00:50:23.014, Speaker A: Here you get this.
00:50:25.334 - 00:50:39.894, Speaker B: So have this, you know. Yeah. This homogeneity in f, then of course, even, you know, and much like, much like, if I add. So if I add a constant.
00:50:39.934 - 00:50:45.286, Speaker A: So dg nabl f plus constant is.
00:50:45.310 - 00:50:53.056, Speaker B: Equal to dg nabl f because, because the, for trivial reasons, because adding a.
00:50:53.080 - 00:50:56.680, Speaker A: Constant to f does nothing in terms.
00:50:56.712 - 00:51:03.560, Speaker B: Of minimum weak upper gradients. And now I do the same trick as I've done before for proving the chain rule.
00:51:03.752 - 00:51:08.968, Speaker A: So the, so the, what I just.
00:51:09.016 - 00:51:23.454, Speaker B: Proved, if you want, is that dg nabla phi composition f is equal to phi prime composition f d g f for phi, which is affine.
00:51:24.754 - 00:51:25.614, Speaker A: Okay?
00:51:26.194 - 00:51:29.814, Speaker B: Then by locality, this is also true if phi is piecewise affine.
00:51:30.794 - 00:51:31.654, Speaker A: Okay.
00:51:32.274 - 00:51:47.264, Speaker B: And now I would like to argue by approximation and conclude that this is true, you know, for every phi c one. But my approximation requires a continuity statement. And I don't have a point wise continuity for this quantity, but I have point wise continuity for the integrated quantity.
00:51:49.084 - 00:51:50.572, Speaker C: Right? So.
00:51:50.668 - 00:51:56.532, Speaker B: So now I get, I get the chain rule, the desired chain rule, even at the level of gradients in some sense.
00:51:56.708 - 00:52:02.344, Speaker A: Okay. And this concludes the proof. All right.
00:52:03.524 - 00:52:16.404, Speaker B: Okay, good time to take a few minutes break. Sorry. Can you explain the last step again? This was too fast.
00:52:17.064 - 00:52:17.804, Speaker A: Sure.
00:52:18.624 - 00:52:20.824, Speaker B: Are you, are you up to here we fire.
00:52:20.904 - 00:52:21.736, Speaker A: Fine.
00:52:21.920 - 00:52:22.424, Speaker B: Yes.
00:52:22.504 - 00:52:22.856, Speaker A: Yeah.
00:52:22.920 - 00:52:59.504, Speaker B: Okay, fine. Piecewise, I find. Okay, and then you do what? So then I know, what I know is that for every five piecewise, I find, I know that this. Let me actually let me write this way. PI, nice. Composition f, you agree? For every sequence, phi n of piecewise are fine. Now, if we pick phi n converging twelve phi in the same way that we, we got it before.
00:53:02.944 - 00:53:03.488, Speaker C: We get.
00:53:03.536 - 00:53:37.384, Speaker B: That, phi n, composition f, will converge in w twelve to phi composition f, right? Because they converge in l two and the minimal weak per unit of the difference is equal. Or if you want, actually is equal to phi prime minus phi m prime composition f, minimum, we can bring it over.
00:53:44.124 - 00:53:44.652, Speaker A: Right?
00:53:44.748 - 00:54:03.644, Speaker B: And this expression, this expression goes to zero in l two. That's my claim. If you want. And this goes to zero two, because minimal we complex f is in l two. These functions are uniformly bounded because phi and d, phi ns are uniform ellipses.
00:54:23.644 - 00:54:25.676, Speaker A: Okay. Okay.
00:54:25.780 - 00:54:34.052, Speaker B: And then there is this issue of points of non differentiability that can be handled as before. So in the case, in the pre image of any countable set, this minimum recapping is zero.
00:54:34.108 - 00:54:36.300, Speaker A: So this causes no problem.
00:54:36.372 - 00:54:44.704, Speaker B: So this, this, if you want, this function is converging to zero point wise almost everywhere. Let's put it this way.
00:54:51.034 - 00:54:51.894, Speaker A: All right?
00:54:52.714 - 00:55:03.946, Speaker B: And then you want to use like Kashi Schwartz or something? No, no. Now I want to use the continuity that they proved before. So now, given that phi n, composition f, goes to phi composition f in.
00:55:03.970 - 00:55:06.294, Speaker A: W twelve, this.
00:55:09.074 - 00:55:15.616, Speaker B: By continuity. Yeah, I cannot use, cos, because I don't have, I don't have linearity in the gradient, but I can use this.
00:55:15.640 - 00:55:18.364, Speaker A: Trick, the continuity is still there.
00:55:23.344 - 00:55:29.484, Speaker B: And of course, on the right hand side, things are just simple, right? Because, I mean, the gradient of it never appears.
00:55:32.944 - 00:55:33.816, Speaker A: Okay, got it.
00:55:33.840 - 00:55:34.608, Speaker C: Thank you.
00:55:34.776 - 00:55:35.604, Speaker A: Okay.
00:55:35.944 - 00:55:36.560, Speaker C: No, sure.
00:55:36.592 - 00:55:39.964, Speaker B: I mean, sometimes I go maybe a bit too fast.
00:55:40.764 - 00:55:41.544, Speaker A: That's.
00:56:13.224 - 00:56:13.640, Speaker C: All right.
00:56:13.672 - 00:56:28.324, Speaker B: I guess we can, we can start again. So now I want to use this newfound technology, if you wish, of integration by parts and the alike.
00:56:31.764 - 00:56:33.756, Speaker A: And I want to use it in.
00:56:33.780 - 00:56:37.144, Speaker B: Order to better understand the properties of the heat flow.
00:56:39.964 - 00:56:43.524, Speaker A: In particular, it should be clear now.
00:56:43.564 - 00:56:48.624, Speaker B: That infinitesimal bertianity is linked to linearity of the heat flow.
00:56:50.324 - 00:56:55.556, Speaker A: And so the next question is, okay, if I have a CDC space and.
00:56:55.580 - 00:56:56.676, Speaker B: I know that the heat flow is.
00:56:56.700 - 00:56:58.888, Speaker A: Linear, what else can I say?
00:56:58.936 - 00:57:14.168, Speaker B: I mean, I should have now a very romanian like situation, and I want to have a Romania like property then. And. Okay, so the path from now to the end of the series of lecture.
00:57:14.216 - 00:57:17.328, Speaker A: Is, you know, well, on one side.
00:57:17.496 - 00:57:28.780, Speaker B: We continue to develop calculus tools, and on the other side, we want to use these calculus tools to better understand both the analysis and the geometry, actually, geometry of the underlying space.
00:57:28.852 - 00:57:35.772, Speaker A: Okay, now, this won't be weak.
00:57:35.908 - 00:58:10.924, Speaker B: Okay. Still more work is needed. And so, I mean, let's, let's continue. And the one thing, actually, perhaps, let me, let me point out the following. I mentioned the first order differentiation formula. If you want this horizontal and vertical derivatives, that is something that has to do with plans representing gradients. You remember, if PI represents the gradient of f, then whenever I differentiate g direction PI, I know what the result is.
00:58:10.924 - 00:58:31.722, Speaker B: I still have not given you any statement concerning existence of such plants represent ingredients. And I, you know, I will do now. And that's basically the only, the only case where we will use this concept of plant representing gradients. And here is the theorem, if you.
00:58:31.738 - 00:58:32.294, Speaker C: Want.
00:58:34.634 - 00:59:20.346, Speaker B: If you want. This is a metric version of brine mechanic theorem. This has been proved by Ambrosio Savari myself. And the same goes as follows. Let me cheat. Let me, let me, let me say x compact. It's a terrible assumption, it's totally unneeded, but, you know, will save my life or my time at least.
00:59:20.450 - 00:59:25.762, Speaker A: So say vector compound, let, you know.
00:59:25.898 - 00:59:33.162, Speaker B: Mu zero, mu one, two given measures. And let's say, and let's say that.
00:59:33.298 - 00:59:37.372, Speaker A: N PI, you know, actually, let me.
00:59:37.388 - 00:59:39.104, Speaker B: Put this way, and muti.
00:59:41.764 - 00:59:42.492, Speaker C: Aw to.
00:59:42.508 - 01:00:03.804, Speaker B: Judisic, you know, connecting them, you know, from zero to mu one. Now, let phi and let, yeah, let phi be a control of its potential so it maximizes the dual problem.
01:00:04.864 - 01:00:08.592, Speaker A: And also, and let PI, you know.
01:00:08.608 - 01:00:10.568, Speaker B: This is a probability measure on this.
01:00:10.616 - 01:00:15.256, Speaker A: Space, you know, pass on x, be.
01:00:15.280 - 01:00:16.164, Speaker B: A lifting.
01:00:21.184 - 01:00:22.324, Speaker A: Of muti.
01:00:30.804 - 01:00:31.684, Speaker C: Assume.
01:00:31.804 - 01:00:58.120, Speaker B: Okay, so far, so far, I basically made no assumptions on this space. On the metric measure space XDM. Here is the crucial assumption, which is a very heavy assumption. You will not have this assumption on arbitrary metric measure space, but assume that the density of mu t is bounded by some constant times m. For every.
01:00:58.152 - 01:00:58.724, Speaker C: T.
01:01:00.784 - 01:01:10.872, Speaker A: There exists c such that. Okay, so for some reason, these measures.
01:01:10.928 - 01:01:13.656, Speaker B: Along, you know, this geodesic, they all have boundary density.
01:01:13.800 - 01:01:16.984, Speaker A: Okay, notice that, of course, this assumption.
01:01:17.064 - 01:01:23.724, Speaker B: Speaks about the relation which, between the reference measure m and at least this busset tangent basic.
01:01:25.584 - 01:01:30.804, Speaker A: Well, then, PI represents.
01:01:33.904 - 01:01:36.724, Speaker B: The gradient, you know, gradient of minus PI.
01:01:40.784 - 01:01:41.684, Speaker A: Okay.
01:01:43.824 - 01:02:03.682, Speaker B: I'll prove this in a second. Actually, the proof is not surprisingly simple. Um, but I want to comment on the statement first. So what it is that Brunier mechanics theorem tell. So, suppose x was remaining manifold. And, you know, this was a Judisc of measures made of, you know, whatever smooth densities you want. But then PI.
01:02:03.682 - 01:02:18.634, Speaker B: But now in the smooth, notice, you know, remark in the smooth case, we knew that.
01:02:21.614 - 01:02:24.502, Speaker A: PI would be the push forward.
01:02:24.598 - 01:02:40.514, Speaker B: Let me put this way. The push forward of mu zero via the map that takes where f is the map. You know, the curve that takes t and returns f t of x. This is nothing but the exponential map with base x of minus t grad PI.
01:02:41.494 - 01:02:43.154, Speaker A: So that's what Maccant has.
01:02:44.294 - 01:03:07.374, Speaker B: It tells you that if I is a controlled potential and everything is sufficiently smooth, then you are going from mu zero to mu one, following this map. So you're sitting at x. Where do you shoot x? Well, you shoot x in direction of minus grad phi. So f times t is exactly x with base x of minus t grad phi, x minus t times.
01:03:09.674 - 01:03:15.154, Speaker A: Okay? So, in particular, the derivative at time.
01:03:15.234 - 01:03:17.082, Speaker B: T equals zero of this curve.
01:03:17.178 - 01:03:21.050, Speaker A: You know, f prime at times zero at X.
01:03:21.162 - 01:03:23.614, Speaker B: This is equal to minus graphi at X.
01:03:26.834 - 01:03:27.266, Speaker C: Right?
01:03:27.330 - 01:03:31.122, Speaker A: That's a corollary, a very partial information.
01:03:31.218 - 01:03:35.054, Speaker B: With respect to what Mechan tells. But this is true. And this is what this is telling.
01:03:36.954 - 01:03:37.814, Speaker C: Okay?
01:03:41.274 - 01:03:51.690, Speaker B: In fact, something more could be said. In fact, one could also say. One could also say, and. And I could also say that the.
01:03:51.722 - 01:03:56.122, Speaker A: Distance between gamma zero and gamma one.
01:03:56.218 - 01:03:58.146, Speaker B: Is equal to the minimal weak upgrade.
01:03:58.210 - 01:04:07.700, Speaker A: Of phi at gamma zero for PI, almost every gamma, that's also something will.
01:04:07.732 - 01:04:21.984, Speaker B: Come with the proof. So Meccan and Brenier, they also tell you, you know, knowing the base point, they know where you are sending this point. As I mentioned, you're sending it to x minus graphite.
01:04:22.444 - 01:04:24.640, Speaker A: So they know exactly where you're sending it.
01:04:24.812 - 01:04:36.944, Speaker B: Now, here, you don't know exactly where you're sending it. The conclusion is not that PI is induced by a map. I don't know that. But I know at least that the distance that every point travels depend only on the.
01:04:36.984 - 01:04:40.280, Speaker A: Depends only on the base point, okay?
01:04:40.472 - 01:04:59.456, Speaker B: And there's a good reason why I don't know more than this. Because, say in r two with the l infinity norm, or on any banach space with not a strictly convex norm, you can find examples of, you know, judicial boat where you don't have optimal map, but you just have optimal plans. And you know, but you know the distance.
01:04:59.560 - 01:05:06.688, Speaker A: You know this and the structure. All right, if you know, I don't know if you.
01:05:06.776 - 01:05:37.374, Speaker B: So how many of you have experience with the l one optimal transportation with cost equal distance. So if you have experience with that, you might, you might have heard that in one optimal transportation, you, you not only have mass, but also plans. Because, you know, the optimality in the dual program tells you only the direction where you're moving, but not how far you're moving. Okay, this is the dual situation. My coated distance squared. But maybe, maybe my distance is not strictly convex, so I know how far I'm moving, but maybe not the direction.
01:05:37.454 - 01:05:40.114, Speaker A: Okay, so that's anyway proof.
01:05:43.594 - 01:05:51.738, Speaker B: So what I know, what does it mean that phi is a control of its potential and that PI is a lifting of mu t. So I know that, you know, I know that e.
01:05:51.786 - 01:05:54.658, Speaker A: Zero comma e one four PI.
01:05:54.706 - 01:05:57.134, Speaker B: This is an optimum, optimum plan.
01:06:04.594 - 01:06:05.170, Speaker C: Right?
01:06:05.282 - 01:06:13.654, Speaker B: Should I prove this? Maybe I should, right? So why, so why is this true? So what is the integral, let me copy this.
01:06:13.694 - 01:06:16.910, Speaker A: What is the integral of gamma?
01:06:16.942 - 01:06:28.718, Speaker B: T dot square dt d PI of gamma. What is the kinetic energy of this plan? Well, this is certainly greater or equal than the integral of distance squared gamma zero gamma one.
01:06:28.766 - 01:06:29.070, Speaker C: Right?
01:06:29.142 - 01:06:32.006, Speaker A: D PI of gamma by just because.
01:06:32.070 - 01:06:36.782, Speaker B: The distance with two points, you know, the energy boundary distance.
01:06:36.878 - 01:06:40.270, Speaker A: Okay, on the other hand, given that.
01:06:40.302 - 01:06:45.686, Speaker B: PI is the lifting of this measure, of this carbon muti, this is equal to the intra, from zero to one.
01:06:45.830 - 01:06:56.878, Speaker A: Of mu t dot squared dt by the definition of lifting, okay? But muti was a geodesic, and geodesics.
01:06:56.966 - 01:07:06.124, Speaker B: At constant speed and speed equal to the distance between their endpoints. So this is equal to w two squared mu zero mu one.
01:07:09.384 - 01:07:10.124, Speaker A: Right?
01:07:11.904 - 01:07:13.992, Speaker B: So the cost of this plan, which.
01:07:14.008 - 01:07:16.816, Speaker A: Is this quantity, is less or equal.
01:07:16.840 - 01:07:21.216, Speaker B: Than the square distance and therefore it's.
01:07:21.240 - 01:07:23.004, Speaker A: Equal, cannot be strictly smaller.
01:07:24.784 - 01:07:56.414, Speaker B: So, so this, so this claim is proved. Now, this is optimal. PI is the control of its potential. Therefore. Therefore, what I know is that, is that, you know, by the fundamental theorem, optimal transport is zero and one four PI. So this is concentrated on the c separate differential of I.
01:07:58.794 - 01:07:59.534, Speaker C: Right?
01:08:01.554 - 01:08:12.414, Speaker B: So for PI, in other words, for PI, almost every gamma. I know that, I know that the following is true.
01:08:14.314 - 01:08:16.954, Speaker A: Well, I know that phi at gamma.
01:08:17.034 - 01:08:42.221, Speaker B: Zero is equal to five c actually minus phi c gamma one, with this plus distance squared, gamma zero gamma one over two. Let's say that cost is distance squared over two. If you, otherwise, I mean, you have to decide what c is. For me, c is this square over two. The proof change is nothing if you just, just a matter of paying attention to it.
01:08:42.277 - 01:08:42.953, Speaker A: Okay?
01:08:43.653 - 01:09:02.844, Speaker B: So, okay, now, on the other hand, on the other end, of course I know, of course I know that phi at any other point, say, for instance, gamma t is less or equal than PI. C at gamma one plus distance squared at gamma t, gamma one divided by two, right? Just by the duality formula, right?
01:09:02.884 - 01:09:10.144, Speaker A: Phi is secant k. So phi is the inf of it's. Okay, so, so phi.
01:09:10.444 - 01:09:26.833, Speaker B: I'm saying that phi is equal to five cc, right? So because of this, because of this, PI at any x is less or equal, actually is equal if you want to the inf over y of minus five c of y plus distance squared x y.
01:09:27.693 - 01:09:29.033, Speaker C: So just.
01:09:30.813 - 01:09:38.573, Speaker B: Okay, well, but then, but then the difference between these two, phi at gamma zero minus phi at gamma.
01:09:38.613 - 01:09:42.562, Speaker A: T, this is greater or equal then.
01:09:42.738 - 01:09:43.722, Speaker B: Okay, what is it?
01:09:43.738 - 01:09:47.002, Speaker A: I have, I have distance squared gamma.
01:09:47.058 - 01:10:08.434, Speaker B: Zero gamma one multiplied by what? By one half minus one minus t squared over two. Gamma is. So the distance between gamma t and gamma one. I know what it is. Is one minus t. This distance, right, goes there. So this is something that I can compute.
01:10:08.434 - 01:10:18.782, Speaker B: Okay, but what it is, this will make confusion. Well, I mean, this is something that we should be able to compute. And this will give us, you know.
01:10:18.798 - 01:10:23.462, Speaker A: In particular, let me write, let me write here.
01:10:23.598 - 01:10:27.966, Speaker B: The conclusion, the conclusion is that the.
01:10:27.990 - 01:10:31.634, Speaker A: Limit of the integral of phi.
01:10:33.754 - 01:10:42.698, Speaker B: Gamma zero minus phi gamma t divided by t in d PI, this is greater or equal. This is greater or equal than the.
01:10:42.746 - 01:10:51.374, Speaker A: Integral of distance squared, actually greater than w two squared mu zero mu one, right?
01:10:52.074 - 01:10:58.570, Speaker B: Because now when I, when you expand the squares, so this is one half minus one half minus t squared over two.
01:10:58.602 - 01:11:08.578, Speaker A: And then I have plus t, right? So this simplified, I divide by t. I integrate, right?
01:11:08.706 - 01:11:13.314, Speaker B: And then this t squared divided by t, I still have a t, which disappears in the limit.
01:11:13.434 - 01:11:15.122, Speaker A: And what I get when I take.
01:11:15.138 - 01:11:28.834, Speaker B: The limb is just, it's just that this line is greater or equal than the integral distance squared. The PI, which is, as we mentioned, is w two squared. Okay, so this is one inequality. Another inequality is.
01:11:32.014 - 01:11:32.446, Speaker C: All right.
01:11:32.470 - 01:11:36.914, Speaker B: Oh, yes. It's even simpler. So, on the other hand, I know that.
01:11:42.974 - 01:11:50.110, Speaker A: Yeah, I know that phi of Y, for this very same reason over.
01:11:50.142 - 01:11:55.728, Speaker B: Here, phi of, actually phi of x PI of x minus phi at gamma.
01:11:55.776 - 01:11:57.880, Speaker A: Zero for this reason.
01:11:57.952 - 01:11:59.664, Speaker B: So this is less or equal.
01:11:59.784 - 01:12:02.800, Speaker A: I use, you know, I use y.
01:12:02.872 - 01:12:06.360, Speaker B: Equals gamma one in this to bound.
01:12:06.392 - 01:12:07.644, Speaker A: From above this quantity.
01:12:08.224 - 01:12:42.168, Speaker B: While for y equals gamma one, I have this equal to three. So this is less or equal than. And as before, five c drops. And what I get is distance squared x gamma one divided by two minus distance squared gamma zero gamma one divided by two. Okay, well, then this is less or equal. You know, this is the difference. So one half, well, actually, distance x gamma one minus distance gamma zero gamma.
01:12:42.216 - 01:12:49.802, Speaker A: One times the sum of g two divided by one. Distance x gamma one plus distance, gamma.
01:12:49.858 - 01:13:00.734, Speaker B: Zero, gamma one, okay, but now this is of course triangular. This is bounded from above by the distance between x and gamma zero. And then I have times this quantity.
01:13:02.994 - 01:13:03.734, Speaker A: Right?
01:13:04.794 - 01:13:13.514, Speaker B: So if you divide by this quantity and you let x go to gamma Zero, what you get is that the lim.
01:13:13.554 - 01:13:20.560, Speaker A: So let me give a name. Well, this name, so as x goes.
01:13:20.592 - 01:13:26.752, Speaker B: To Gamma Zero of phi, of x minus phi, of gamma zero divided by.
01:13:26.768 - 01:13:29.764, Speaker A: The distance between x and gamma zero.
01:13:31.984 - 01:13:37.288, Speaker B: This, let me take also the positive part. This I will call it, you know.
01:13:37.336 - 01:13:45.412, Speaker A: The upper slope PI, or actually this way, let's see, at gamma zero this.
01:13:45.468 - 01:13:51.184, Speaker B: Quantity is less or equal than the distance between gamma zero and gamma one.
01:13:55.804 - 01:13:56.544, Speaker A: Agree?
01:13:56.924 - 01:14:05.974, Speaker B: Because when x goes to gamma zero, the distance between x and gamma one will converge to this between gamma zero and gamma one. So twice the same quantity and I get square.
01:14:06.044 - 01:14:06.774, Speaker A: Okay.
01:14:09.554 - 01:14:29.546, Speaker B: Okay, now it's time for remark. If, you know, f is Lipschitz, we are on a compass space. If epsilon, then the minimal weak upper.
01:14:29.570 - 01:14:46.190, Speaker A: Gate of faith is less or equal than this sort of slope almost everywhere. Okay, what we knew, what we know.
01:14:46.222 - 01:14:53.034, Speaker B: If you want to read, is that the minimal weak upper gradient, the sovereign quantity is less or equal than the asymptotic ellipsis constant.
01:14:53.694 - 01:14:54.110, Speaker C: Okay?
01:14:54.142 - 01:14:56.366, Speaker B: That comes from the relaxation point of view.
01:14:56.470 - 01:14:57.184, Speaker A: Okay?
01:14:57.334 - 01:15:10.304, Speaker B: Now I want to say that it's regarding something potentially quite smaller than the asymptotic constant because I'm only taking in some sense a one sided variation. And I'm also fixing, you know, one of the two points.
01:15:10.924 - 01:15:13.940, Speaker A: Okay, now why is this true?
01:15:14.052 - 01:15:35.574, Speaker B: Well this is true. I can prove this by using the weak upper gain, the property. The point is that if f is lip sheets and gamma, notice that if gamma is absolutely continuous, then I still know that t, I mean these, I already used f gamma t is absolutely continuous.
01:15:38.074 - 01:15:38.814, Speaker A: Right?
01:15:39.634 - 01:15:49.602, Speaker B: And what I claim is that the derivative, this derivative is, you know, bounded.
01:15:49.618 - 01:15:52.798, Speaker A: From above is less or equal than.
01:15:52.966 - 01:15:55.030, Speaker B: The upper slope of f. If you.
01:15:55.062 - 01:15:58.126, Speaker A: Want at gamma t by times gamma.
01:15:58.150 - 01:16:08.954, Speaker B: T dot for almost everything. And why is this true for.
01:16:11.374 - 01:16:11.734, Speaker C: Well.
01:16:11.774 - 01:16:13.994, Speaker B: For, I guess for trivial reasons.
01:16:20.114 - 01:16:20.570, Speaker C: Actually.
01:16:20.642 - 01:16:27.974, Speaker A: Is in modulus, is in module. Okay?
01:16:30.754 - 01:16:37.414, Speaker B: But it's just because an absolute continuous function, you know, on the real line is almost everywhere differentiable. So.
01:16:43.854 - 01:16:46.454, Speaker A: You know, take, you know, take.
01:16:46.494 - 01:16:55.114, Speaker B: A point of differentiability. Take t differentiability point of, of this one t.
01:16:59.494 - 01:17:00.234, Speaker A: Right?
01:17:00.854 - 01:17:03.862, Speaker B: Now if the derivative is positive, you.
01:17:03.878 - 01:17:07.270, Speaker A: Know, if the derivative is positive, well.
01:17:07.302 - 01:17:10.290, Speaker B: Then this derivative, what is this? This is, this guy is, you know.
01:17:10.342 - 01:17:13.506, Speaker A: The limb as h goes to zero.
01:17:13.650 - 01:17:18.002, Speaker B: Of f gamma t plus h minus.
01:17:18.058 - 01:17:21.654, Speaker A: F, gamma t divided by h, right?
01:17:21.954 - 01:17:23.618, Speaker B: But if the derivative is positive, I.
01:17:23.626 - 01:17:26.774, Speaker A: Can also put, you know.
01:17:29.754 - 01:17:30.450, Speaker C: Right?
01:17:30.602 - 01:17:39.686, Speaker B: And this is, and this is the, you know, this is bounded from wo by this lobe, you know, then you divide. So then you divide distance gamma t.
01:17:39.710 - 01:17:44.070, Speaker A: Plus h gamma t distance divided by.
01:17:44.102 - 01:17:57.278, Speaker B: H. And you're done, right? What if the derivative is negative? Well, if the derivative is negative, you do the same, except that, except that you write that this is, this derivative is equal to the limit when h.
01:17:57.326 - 01:18:00.806, Speaker A: Goes to zero from below, right, of.
01:18:00.830 - 01:18:02.074, Speaker B: The same expression.
01:18:07.194 - 01:18:07.934, Speaker A: Right?
01:18:09.514 - 01:18:15.234, Speaker B: But, but when you take the absolute.
01:18:15.274 - 01:18:18.370, Speaker A: Value of this thing, you are, you are.
01:18:18.482 - 01:18:24.734, Speaker B: So then the, you know, modulus of the derivative is equal to this.
01:18:26.474 - 01:18:26.858, Speaker C: Right?
01:18:26.906 - 01:18:28.218, Speaker B: And then again, you take the positive.
01:18:28.266 - 01:18:31.354, Speaker A: Part and you're done. Okay.
01:18:34.014 - 01:18:54.126, Speaker B: So, so this proves my claim, right? Because I proved, yeah, that this upper slope for Lipschitz functions. For Lipschitz functions, this upper slope is in fact an upper gradient. So that's the job of the upper gradient along any absolutely continuous curve. And since clearly this upper slope is.
01:18:54.150 - 01:18:57.586, Speaker A: Bounded by the global leaches constantly, my.
01:18:57.610 - 01:19:25.490, Speaker B: Space is compact, so it has finite measure. This global is in l two. It's in. So this is in fact, a weak upper gradient. This is way more than a weak upper gradient. So our minimal weak upper gradient is smaller than that. Okay, so, so let me write, what is it that we proved here? So we proved that the minimal weak approgriate of phi in gamma zero, this is less or equal than the distance between gamma zero and gamma one for PI.
01:19:25.490 - 01:19:26.654, Speaker B: Almost every gamma.
01:19:30.554 - 01:19:31.730, Speaker A: End of the proof.
01:19:31.922 - 01:19:34.266, Speaker B: We have three things. We approve three things.
01:19:34.410 - 01:19:42.130, Speaker A: We prove this, we prove this and we prove this, right?
01:19:42.202 - 01:20:03.594, Speaker B: So let me just make, you know, take this all together. So this limit, this is what we want to control. If I want to prove my claim.
01:20:03.934 - 01:20:10.038, Speaker A: About plan representing gradients, this Guy is.
01:20:10.086 - 01:20:15.374, Speaker B: Greater or equal than, well, w two.
01:20:15.414 - 01:20:16.074, Speaker A: Squared.
01:20:18.634 - 01:20:25.370, Speaker B: But this is greater or equal, actually. Well, this is well greater or equal. Then let me put this way. So I have one half w two.
01:20:25.402 - 01:20:31.290, Speaker A: Squared plus this Quantity over here, which.
01:20:31.322 - 01:20:51.494, Speaker B: Is, you know, one half the limit as t goes to zero. But this Quantity is constant in T. So I'm writing limit t as a very complicated way. But this is, you know, a Constant.
01:20:51.534 - 01:20:55.154, Speaker A: Quantity because, because of this, right?
01:20:55.574 - 01:20:56.054, Speaker C: Right.
01:20:56.134 - 01:20:59.054, Speaker A: If you want the energy, the kinetic.
01:20:59.094 - 01:21:24.494, Speaker B: Energy for any s, for almost every s, this KinetiC Energy is equal to the square, the speed of the curve. But the square speed of the curve is always constantly equal to the square distance between the endpoints. So if I'm taking the Average or not doesn't matter. And now this Quantity, this is equal to the, you know, one half, if you want inter this chord, gamma zero, gamma 1d PI.
01:21:24.954 - 01:21:27.450, Speaker A: But now I use this, and then.
01:21:27.482 - 01:21:28.682, Speaker B: I get that this is greater or.
01:21:28.698 - 01:21:42.724, Speaker A: Equal than one half the inter of the minimum we compreh, zero d PI plus one half. This limb soup, which is what is.
01:21:42.764 - 01:21:51.704, Speaker B: Required by, you know, plants representing gradients. Given that here there is phi at gamma zero minus five, gamma t represents the gradient of minus five.
01:21:54.564 - 01:21:57.204, Speaker A: Okay, if you want this minus phi.
01:21:57.244 - 01:22:14.244, Speaker B: At gamma t minus minus phi gamma zero. Okay, and what about this last statement? Well, so these inequality cannot be strict.
01:22:16.584 - 01:22:16.992, Speaker C: Right?
01:22:17.048 - 01:22:35.214, Speaker B: Because in plan representing gradients, you know, no inequality can be strict. So if this inequality cannot be strict, it means. It means that this inequality must be an equality for PI, almost every gamma, which is precisely what.
01:22:38.914 - 01:22:39.774, Speaker A: All right.
01:22:43.594 - 01:22:44.454, Speaker C: Okay.
01:22:52.354 - 01:23:13.342, Speaker B: Yeah, I guess in our paper, you will not find that this paper, this statement written in this way. But, you know, that's basically what we proved. This statement is there. But anyway, I'm restating SEM, I mentioned this because maybe you look but precisely the same statement, but, you know, you won't find it there.
01:23:13.438 - 01:23:14.422, Speaker C: Okay, sorry.
01:23:14.478 - 01:23:15.954, Speaker A: Question, please.
01:23:16.894 - 01:23:19.254, Speaker B: Where did you use that mu t.
01:23:19.334 - 01:23:22.750, Speaker A: Was less than some constant times m. Say that again.
01:23:22.782 - 01:23:27.310, Speaker B: Sorry. Where did you use that mu? T is less than or equal than.
01:23:27.342 - 01:23:29.440, Speaker A: C times m. Oh, you're right.
01:23:29.472 - 01:23:39.880, Speaker B: What did I use? Good question. What do they use on the fact that PI, in order for PI. Good point. Never, I guess, except in here. So, in order to say thank you.
01:23:39.912 - 01:23:41.704, Speaker A: I mean, thank you, in order to.
01:23:41.744 - 01:23:54.404, Speaker B: Say that PI represents something, PI should be a test plan. And in order for PI to be a test plan, it's margin has to be uniformly bounded.
01:23:55.844 - 01:23:57.984, Speaker A: Oh, okay. Okay.
01:23:58.884 - 01:24:08.024, Speaker B: There is also another point. Thanks. I should have clarified this. So, at least I used the fact that mu zero is absolutely continuous.
01:24:09.764 - 01:24:10.704, Speaker C: Namely.
01:24:11.004 - 01:24:21.148, Speaker B: When I integrate the minimal weak approval of phi, you know, composed at gamma zero in d PI. Thank you, Vitaly, for this question. So, if I write this integer, I.
01:24:21.156 - 01:24:24.102, Speaker A: Should be sure that this intra is well defined, right?
01:24:24.198 - 01:24:44.214, Speaker B: And the minimal weak upper ingredient is only defined m almost everywhere. So if I know nothing about e zero push or PI, I cannot write this. You see what I mean? So if I have. So I guess. I guess this is a general factor.
01:24:44.254 - 01:24:47.398, Speaker A: If I have a function f, well.
01:24:47.446 - 01:25:13.854, Speaker B: Defined, you know, mu almost everywhere. And I want to, you know, and I want to, you know, compute f composition t. And I ask whether this is well defined in order for this to be well defined new almost everywhere, I should be sure that people show new is absolutely continuous with respect to me.
01:25:15.534 - 01:25:17.874, Speaker A: Make sense this, right?
01:25:18.374 - 01:25:20.358, Speaker B: Because if I have a set of.
01:25:20.406 - 01:25:24.302, Speaker A: Positive new measure that is sent in.
01:25:24.318 - 01:25:27.874, Speaker B: An agreeable set of mu, then I have a problem. Define this.
01:25:29.654 - 01:25:30.342, Speaker A: Makes sense.
01:25:30.438 - 01:25:54.564, Speaker B: So I'm using here in order to write this. Thanks. Vitaly should have been more explicit. In order to write this, I should have, you know, I must be sure that e zero before PI is absolutely continuous with respect to m, which is where this function is defined. And of course it is, because PI is a lifting of our family of measures. And in particular.
01:25:56.544 - 01:25:57.604, Speaker A: You have a question.
01:25:58.904 - 01:26:10.380, Speaker B: If you are on a romanian manifold. Yes. Just assume that mu zero is absolutely the volume measure. This gives you that mutilating is absolutely continuous for a short time.
01:26:10.532 - 01:26:10.860, Speaker C: Yes.
01:26:10.892 - 01:26:19.292, Speaker B: In fact, for every time up to, you know. Yes. Not one, of course, because you can get to the delta.
01:26:19.308 - 01:26:20.184, Speaker A: Yes, of course.
01:26:20.924 - 01:26:28.876, Speaker B: When, when do you have like a lip sheets map? Because I guess that the reason why inequality odds is that the exponential map.
01:26:28.900 - 01:26:30.184, Speaker A: Is sort of leap sheets.
01:26:30.904 - 01:26:31.448, Speaker C: Haha.
01:26:31.496 - 01:26:39.192, Speaker B: No, it's because of Richie Kurk. No, the optimal map, you know, the optimal map is never leaves. It's never.
01:26:39.328 - 01:26:40.084, Speaker A: Okay.
01:26:42.784 - 01:27:22.344, Speaker B: You know, it's in our time to prove that the ultimate plan is induced by a map. So getting that the map is continuous would be another regularity, you know, and then leap sheets another regularity, forget. Okay, you get that sort of statement whenever, if you have a geodesic and you sit at the interpolate, say, the midpoint measure, and you look at the optimum map from that midpoint measure to the initial and finite measures, then in the romanian setting, this would be a local elliptic map, but they're for totally different reasons in some sense. No, that has to do so.
01:27:22.934 - 01:27:26.994, Speaker A: Okay, now it comes, I mean, you.
01:27:27.374 - 01:27:39.954, Speaker B: I was going the direction. Let me present the result in the metric setting. Without proof, I will not prove this. This is too long to prove. This is a proposition that's a useful, an extremely useful proposition by Raya.
01:27:42.534 - 01:27:48.234, Speaker A: Patio around 2011.
01:27:48.754 - 01:27:49.534, Speaker C: Um.
01:27:52.434 - 01:27:54.122, Speaker B: And the, and the statement is.
01:27:54.138 - 01:28:07.362, Speaker A: This, let x be CD k infinity space. And again, compact.
01:28:07.538 - 01:28:09.734, Speaker B: Absolutely not needed. But let me say compact.
01:28:12.434 - 01:28:15.194, Speaker A: Let's also.
01:28:16.934 - 01:28:19.198, Speaker B: Mu zero and nu one.
01:28:19.366 - 01:28:22.286, Speaker A: The probability measures on x that are.
01:28:22.350 - 01:28:24.954, Speaker B: You know, bounded from above by some constant.
01:28:27.734 - 01:28:33.822, Speaker A: Okay, now we have two measures bounded by a constant, and the space which.
01:28:33.838 - 01:28:43.934, Speaker B: Is CD can feed. Well, then what Raya tells instead. Then there exists a w two judisic.
01:28:48.474 - 01:28:55.734, Speaker A: You know, from mu zero to mu one. So that two things are true.
01:28:59.474 - 01:29:04.054, Speaker B: First of all, the convexity inequality holds. So, in particular, you know, the entropy.
01:29:04.954 - 01:29:07.492, Speaker A: Of mu tilde is less or the.
01:29:07.508 - 01:29:36.424, Speaker B: Same inequality that, you know, comes with the pro, with the definition of CDC space. What is this minus k over two t one minus t w two squared? And second, I mean, this loss permit has by definition. So Raya tells you something more. Second is that you can choose such.
01:29:36.924 - 01:29:40.544, Speaker A: That this holds true for every t.
01:29:42.964 - 01:29:43.944, Speaker B: For some.
01:29:46.364 - 01:30:02.224, Speaker A: C prime. Okay, so let me comment. So, first of all, in the.
01:30:02.344 - 01:30:20.272, Speaker B: If you follow Rajara's proof. Now, I'm not really. I'm being a little bit dishonest in here. So he gives, he tells you exactly who is the c prime in terms of the statement. And if you. And that c prime is the same c prime that you will get on the remaining manifold. If you have.
01:30:20.272 - 01:30:28.010, Speaker B: If you have, you pick the only judicia mechanic tells you that there is only one connecting two measures with bounded.
01:30:28.042 - 01:30:30.786, Speaker A: Density on a space on a manifold.
01:30:30.810 - 01:30:44.974, Speaker B: Which is compact, and there's some units. So in particular, you have a control on how far these densities, these densities are. And you have a lower bound on the Ricci. Then you get, you know, an upper bound on the intermediate density, and Royalla tells you exactly the sharp upper bound.
01:30:45.674 - 01:30:51.134, Speaker A: Okay, so first thing, second thing.
01:30:53.774 - 01:30:54.086, Speaker C: And.
01:30:54.110 - 01:30:56.886, Speaker B: Perhaps I want to make another comment in direction of what you were saying, luca.
01:30:56.910 - 01:31:00.894, Speaker A: So I think you had an inequality.
01:31:00.934 - 01:31:34.760, Speaker B: In your mind, which is, which was going in the wrong direction in some sense. What you want from the optimal map in order for this to be true is that the optimal map does not shrink stuff too much. If the optimal map is the most regular map, the constant map, then, you know, t push forward, you know, constant pushover. Any measure is a direct mass. So this typically tends to be not true. What you want is that they must remain spread out from below, which is exactly. This spreading out is exactly what should come with loss turbulence.
01:31:34.872 - 01:31:40.096, Speaker A: Okay, now, this theorem would be trivial.
01:31:40.240 - 01:32:03.674, Speaker B: If we knew that, you know, that we had a geodesic whose lifting is concentrated on jurisdictions that do not intersect. So say so idea. So, rough idea. Very rough idea. Very, very rough idea, at least as a why this could be very rough idea. Or perhaps it's a comment in this. So say so.
01:32:03.794 - 01:32:06.574, Speaker A: Let PI be the lifting.
01:32:12.174 - 01:32:12.670, Speaker C: Of the.
01:32:12.702 - 01:32:17.114, Speaker A: Lost rule Bilanese jurisic, meaning, you know.
01:32:19.454 - 01:32:24.394, Speaker B: The one that exists because of the loss of bileni conditions, CD k infinity condition.
01:32:25.054 - 01:32:27.674, Speaker A: And let's assume, and let's pretend.
01:32:32.014 - 01:32:32.398, Speaker C: Such.
01:32:32.446 - 01:32:34.594, Speaker A: That be such that.
01:32:36.374 - 01:33:04.174, Speaker B: For every t in zero one, let's say at least intermediate the map that takes gamma and returns gamma t is PI injected. Is PI essentially injected, injected. So what does it mean? It means that PI is concentrated, let me say like this, so actually, let me say like this, so that I don't have to introduce. So let me say that there exists.
01:33:04.834 - 01:33:10.934, Speaker A: Gamma set of curves Borel.
01:33:13.834 - 01:33:42.062, Speaker B: And such that, you know, PI of gamma is one. So PI is concentrated on this gamma and such that et restricted to gamma is injected. So this means that whenever, you know, I have a certain initial distribution, I find a distribution, I have this plan.
01:33:42.198 - 01:33:45.302, Speaker A: That is the lifting of the jurisdiction.
01:33:45.318 - 01:33:51.594, Speaker B: And for some reason I am so lucky that these jurisdicts, whenever I look at m time t, I always find different points.
01:33:52.254 - 01:33:54.726, Speaker A: Okay, so this means that if I.
01:33:54.750 - 01:34:02.604, Speaker B: Want to look at the density et pujam at some point in sense that density only comes from this judicial.
01:34:03.784 - 01:34:06.992, Speaker A: Okay, and let me also pretend, let.
01:34:07.008 - 01:34:21.644, Speaker B: Me also pretend assumes this is absolutely, absolutely not true. But assume also that, you know, for every, for every a in Borrell here.
01:34:23.644 - 01:34:28.464, Speaker A: Borrell with PI of a positive.
01:34:29.564 - 01:34:34.420, Speaker B: If I look, if I look at the, you know, the, you know, the.
01:34:34.452 - 01:34:37.404, Speaker A: Curve if you want, if I take.
01:34:37.444 - 01:34:46.304, Speaker B: My plan, I restrict it to a and then I normalize this guy and then I look at its marginals.
01:34:49.224 - 01:34:49.536, Speaker C: This.
01:34:49.560 - 01:35:03.920, Speaker B: Is also a loss room with an issue. This, what am I saying? I say, look, this plan, you know, this plan is such that whenever I look at its margin, at its marginal.
01:35:03.952 - 01:35:08.208, Speaker A: Satisfied inequality in one, okay, I'm saying.
01:35:08.256 - 01:35:35.034, Speaker B: Look, pick some jurisdicts, take a bunch of these jurisdictions of positive measure, forget about all the others and rescale the restrict and rescale the plan. Now you have another plan, leaving on the space of judicial it's an optimal plan between its marginals, because the support of this plan, new plan is contained in your support. And assume that the entropy still convex along the jurisdiction.
01:35:36.814 - 01:35:39.494, Speaker A: Okay, well, if you have all of.
01:35:39.534 - 01:35:59.650, Speaker B: This, it's not hard to check. It's not hard to check that. It is not hard to check that not only you are that convexity of routine one, but you have the better.
01:35:59.722 - 01:36:04.258, Speaker A: Convexity, which is a log of root.
01:36:04.306 - 01:36:16.390, Speaker B: So called, you know, lateral tm, this is et PI. Then log rho t in gamma t is actually less or equal than one.
01:36:16.422 - 01:36:24.430, Speaker A: Minus t. Log rho zero in gamma zero plus t times log rho one.
01:36:24.622 - 01:36:35.854, Speaker B: In gamma one minus k over two t one minus t distance for the gamma zero, gamma one or PI or most derivative.
01:36:44.634 - 01:36:47.774, Speaker A: Let me comment 1 second, this inequality. First of all.
01:36:50.994 - 01:36:54.854, Speaker B: It should be clear that this inequality is the point wise version of one.
01:36:55.754 - 01:36:58.914, Speaker A: If you have this and you integrate.
01:36:58.954 - 01:37:12.754, Speaker B: With respect to PI, you get exactly, you know, if you integrate here with respect to PI. Let's, let's look, for instance, on the left hand side, integral of log, what is this log? Rho t in gamma t d PI of gamma.
01:37:14.374 - 01:37:14.990, Speaker A: What it is?
01:37:15.022 - 01:37:21.134, Speaker B: This, this is equal to the integral of log of rho t d e.
01:37:21.174 - 01:37:37.684, Speaker A: T push over PI. E t push over PI. But et push rho PI is rho tm, right? So this is equal to the entropy of et PI. Makes sense, right?
01:37:37.724 - 01:37:46.064, Speaker B: And the same for rho zero and rho one, because here there is gamma zero, gamma one. And if I integrate distance squared, the PI is optimal, I get w two squared.
01:37:46.764 - 01:37:49.564, Speaker A: Okay, so this is the point wise.
01:37:49.604 - 01:38:10.104, Speaker B: Version of one, and therefore it's of course stronger. I mean, if you have this and you integrate, you get one. But of course, a priori, one does not imply this pointwise version. In order to get from one to this point wise version, I had to assume basically that whenever I restrict my plan, I still remain a plan that satisfies the CD cause.
01:38:10.964 - 01:38:11.824, Speaker A: Okay.
01:38:14.084 - 01:38:15.844, Speaker B: Now say that.
01:38:16.004 - 01:38:18.076, Speaker A: Say that you have a plan that.
01:38:18.100 - 01:38:27.168, Speaker B: Satisfies this, an optimal plan that satisfies this. Well then, you see, you can use this inequality to get a bound from above on the log of rho t.
01:38:27.296 - 01:38:32.984, Speaker A: And therefore on rho t, right? Because so logo.
01:38:33.024 - 01:38:34.672, Speaker B: So rho zero and rho one were.
01:38:34.768 - 01:38:38.600, Speaker A: Bounded from above by some constant, right?
01:38:38.752 - 01:38:43.632, Speaker B: So this quantity and this quantity are uniformly bounded in gamma, in t, in.
01:38:43.648 - 01:38:45.884, Speaker A: Whatever, okay.
01:38:47.604 - 01:39:04.984, Speaker B: Now this quantity k over two t times one minus t distance squared is also bounded from over some constant. If k is positive, you can just drop this term. If k is negative, well then pick the absolute value of k, and here you put the diameter of the space, and still this is uniformly bounded.
01:39:06.724 - 01:39:07.584, Speaker A: Okay?
01:39:08.204 - 01:39:10.140, Speaker B: So in any case, if you have.
01:39:10.172 - 01:39:16.312, Speaker A: A plan PI for which this is true, well, then you have found, you.
01:39:16.328 - 01:39:18.600, Speaker B: Know, an upper bound on the density.
01:39:18.632 - 01:39:28.480, Speaker A: Of root t, which is exactly what you want, right? Okay, now perhaps here is a good.
01:39:28.512 - 01:39:42.576, Speaker B: Point to make a comment. So, first of all, in the romanian setting, when you prove this term Vorness theorem, that a lower bound on the rich curvature implies convexity of the entropy.
01:39:42.680 - 01:39:45.032, Speaker A: What you prove is actually this inequality.
01:39:45.208 - 01:39:46.712, Speaker B: And then you integrate this inequality and.
01:39:46.728 - 01:39:50.444, Speaker A: You get, and you get the convexity, okay?
01:39:50.824 - 01:40:12.150, Speaker B: So now of course you can wonder, well, but if this is the useful inequality, or in any case, if this is stronger than that, why didn't last proven? Villainy use this inequality to define this decay space. And the answer is, of course, in the stability problem. This. If you start from this, you don't know whether, you know, this is not stable under weak convergence of jurisdictions. You don't know that.
01:40:12.302 - 01:40:12.646, Speaker C: Okay?
01:40:12.670 - 01:40:14.462, Speaker B: So you cannot use that.
01:40:14.558 - 01:40:15.274, Speaker C: Okay.
01:40:17.294 - 01:40:34.918, Speaker B: So the lower reach bound. And here comes, you know, the final comment in your question. So, the lower reach bound, really, what tells you is this inequality. So the density at the intermediate times is bounded from all by, you know, density in a very quantified way. The density initial time. And this is gives. Gives, in particular, gives the bound of.
01:40:34.966 - 01:40:35.992, Speaker A: Interest in this case.
01:40:36.118 - 01:41:14.072, Speaker B: Okay, now, what did Rajala do? Okay, so he knew that it could not get. I mean, exactly this. And there are several difficulties that occur in handling this. One of those is that this very first assumption might be not true. What could be true, in general, if you start, you know, from a density or a density there is that your plan does something like this? I don't know. And there are many jurisdictions that pass on the same. Actually, really on the same point there.
01:41:14.072 - 01:41:26.920, Speaker B: There are still sufficiently spread out that the entropy is, you know, convex, but there is a little bit of overlapping. And if there is a little bit of overlapping, it means that the density, say, a rho, one r at this.
01:41:26.952 - 01:41:29.702, Speaker A: Point, you know, comes from the needs.
01:41:29.758 - 01:41:38.310, Speaker B: From, you know, several densities at the start, final point. So you cannot hope for an inequality so smooth in such. And so nice to be. To be true.
01:41:38.422 - 01:41:40.950, Speaker A: Okay, so that's one of the problems.
01:41:41.142 - 01:41:54.594, Speaker B: And the other problem, of course, is that is that, okay, you have just the integrated inequality. You have no idea. You have no idea whether. Where the. Whether you can localize which. Okay, it's a quite. I mean, it's the same sort of conceptual problem that is in India stored the idea.
01:41:54.594 - 01:42:31.684, Speaker B: But then this requires some computations. I will not show you the computation. I mean, once you get the idea and the good understanding, optimal transport, this computation at least the advantage that are easy to follow, hard to find, to be honest. But, you know, if you pick Rajara's paper and, you know, and you sit down, it's a couple of pages, and you sit down quietly and you read them in, say, an afternoon, you should be able to, you know, come out. I have no idea how he managed to get that, because, I mean, it's sort of miraculous, but at least, you know, the paper is well written and nice to read. So the point is this what he says, that actually. So, actually it tells you exactly who is the judiciary.
01:42:31.684 - 01:42:33.924, Speaker B: And the judiciary is made like this.
01:42:34.664 - 01:42:36.364, Speaker A: Take mu zero, take mu one.
01:42:36.664 - 01:43:14.044, Speaker B: First of all, you don't know whether there is only one Judisic or many. Those two vietnamese don't tell you anything about this. So in particular, you can have many midpoints. Well, you pick among all the midpoints, you pick the one which minimizes the entropy. By convecting lower semiconductor, there should be such one. Okay, so you start, you have your, you know, your mu zero, mu one, you have your entropies, you have many midpoints, and you pick the one that minimizes the entry, and then you pick the one that minimizes, you know, one fourth, you minimize the entropy barrier, three fourths, you minimize the entropy over here and you continue like this. And now you build dyadic judies.
01:43:14.044 - 01:43:51.314, Speaker B: And if you want, by lower semi continuity, then you get, I mean, you can extend this to azure basic and by lower semi continuity and optimality of these midpoints that you previously chosen, you know, that you have this convexity of the entropy satisfied. So one holds for free. And then what it does is, okay, let's suppose, suppose that for some reason this is not true for this c prime that comes in some sense from the remaining anatomy. Well then what I do is I remove, say that in this region the density was too high. Well then what I do, I pick my optimal plan PI. I take out all of these logistics.
01:43:52.214 - 01:43:54.342, Speaker A: Okay, given that I took out all.
01:43:54.358 - 01:44:02.798, Speaker B: The jurisdiction, now the initial and finite mass will not be anymore minus zero, zero one. So I look at the initial and final distributions of the set of judiciary.
01:44:02.806 - 01:44:05.310, Speaker A: That I took out, and for that.
01:44:05.382 - 01:44:53.110, Speaker B: I applied a lot to Bilani. I picked the lot to Bilani judicial in some sense that this judiciary that I chose still add, you know, an initial and finite bound that they still had, because rho zero one, you know, the finite density. But they were, you know, instead of concentrating a lot to create a density higher than c or c prime, well, any, give me, give me another, you know, another way of interpolating between them in order that the entropy stays low. Because if the density is high, the entropy is high. But I want, you know, if density is uniform, the entropy must be high for sure. Well, photony gave me another, you know, another plan. And if I use this new plan and together with what I had previously, basically something with less entropy in that.
01:44:53.142 - 01:44:55.182, Speaker A: Point to where there was concentration, I.
01:44:55.198 - 01:45:09.992, Speaker B: Mean, this is roughly the idea. Okay, I understand this very rough. I don't pretend you to, you know, get the point, but at least, you know, maybe some more. I mean, if you ever want to read paper. These words that I mentioned maybe can help you. Guiding. Guiding your.
01:45:09.992 - 01:45:12.528, Speaker B: Your intuition. Anyway, just to close.
01:45:12.616 - 01:45:12.888, Speaker C: So.
01:45:12.936 - 01:45:21.004, Speaker B: So what we will do, I will not do, but I will use a big box. So I apply to the magnetic bern theorem.
01:45:21.584 - 01:45:23.648, Speaker A: So on CD completely spaces.
01:45:23.776 - 01:45:31.244, Speaker B: We have many test plans, you know, coming from vast and udesic, that represents the gradient of control potential.
01:45:31.664 - 01:45:32.600, Speaker A: Okay.
01:45:32.792 - 01:45:40.662, Speaker B: And on Friday, we will use this to get what is called the Evi property of the. Of the gradient flow of the heat flow.
01:45:40.758 - 01:45:41.070, Speaker C: Okay.
01:45:41.102 - 01:45:45.246, Speaker B: We get contractivity in busses. Time for the. For the heat flow.
01:45:45.350 - 01:45:52.914, Speaker A: Okay, that's all. If there are no questions, see you on Friday.
01:45:54.454 - 01:45:54.814, Speaker C: All right.
