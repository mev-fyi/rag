00:00:00.480 - 00:01:01.324, Speaker A: We may start. Hello everybody. Thank you for coming back to this course. We took a major step last time by proving the Moore's theorem, which enables us having any k which is bigger than or equal to zero onset, which sometimes we write omega for the set, sometimes x. In the next chapter. Indeed, I will mostly write x due to interpolation. Whenever such a k is given, then Mohr theorem says that there is a reproducing kernel Hilbert space age, which we denote it by h of k because its kernel is precisely k, and based on a simple proposition that we had before the Moore's theorem.
00:01:01.324 - 00:02:17.904, Speaker A: This construction is unique. Indeed, there is just one rths with this property. And now we started already to study the effects of k on h. The first one, I mean, I mentioned as recall the first one was that if k is a continuous function, omega times omega to c is continuous. It implies that every f in h of k is continuous. And indeed, we use the result in the proof which by itself is somehow that if x n, or better to say xi when I is in set I is a net and xi or x converges to x in omega. We assume that omega has its topology.
00:02:17.904 - 00:03:13.746, Speaker A: So it's a generalized way of thinking about convergence here. If you have a convergent net in x, then the corresponding kernel function kx converges to kx in the space H. So this is something that we used in the proof to show that the elements of H are continuous, but by itself is important to recall. Now we continue this from here. We have new result. The first one is about taking the conjugation. If k is given and k is bigger than or equal to zero, with the meaning we gave.
00:03:13.746 - 00:03:50.674, Speaker A: I mean, every n by n matrices formed with n distinct nodes is a positive matrix. Then we can define L of X and y to be kx and Y bar. It's very easy to see that L is also positive. I will show in the proof of next proposition, L is also positive. It's easy to verify. We will do. And so we can talk about H of L.
00:03:50.674 - 00:05:13.894, Speaker A: So what is the relation between h of L and h of k? Because of the bar, it is easy to guess, or it's expected to have, that this is precisely equal to h of k bar. I mean, we take conjugate of all elements of h of k and we obtain another space. This is true, it's a good expectation, and I prove it in this proposition. Let k from. Let's switch to x. X in c be a kernel function, which I briefly wrote as this one bigger than or equal to zero, it means equivalent to say it's a kernel function. Then l equal to k bar is also a kernel function function.
00:05:13.894 - 00:06:09.834, Speaker A: And we have what is written here. The space generated by l is the same as space generated by k. Take bore of all the elements. Moreover, we can have a map, a very natural map c defined from one to the other. For example, from h of k to h of l. Sending f to f bar is a surjective. Conjugate linear isometry is kind of trivial from the definition.
00:06:09.834 - 00:06:49.778, Speaker A: Conjugate linear also kind of trivial from the definition. But the isometry part, it has to be verified. You shouldn't say that when we calculate the norm, the normal f is equal to the normal f bar. Of course, for many, many function spaces and spaces that we consider, this is the case because I mean there is an absolute value somewhere. Usually we use an integral formula and in the integral formula there is an absolute value. So there is no difference between f and f bar. But in general we do not know.
00:06:49.778 - 00:07:55.248, Speaker A: And this is something that has to be verified. Proof first, yl is a kernel function. For this, let x one up to xn be in x distinct and correspondingly n scalars not necessarily any alpha one up to alpha n in c. Then what can we say about the sum I and j from one to n alpha I alpha j l of x I and x j? And we need to put bar somewhere, either on the first one or the second one. Doesn't matter. Sometimes it is more convenient to put than the first one, but it's the same. We need to show that this combination is positive.
00:07:55.248 - 00:08:49.414, Speaker A: That's the meaning of being a kernel function. And immediately we see that this is equal to some a and j from one up to n alpha bar alpha j k of x I and x j. That's the bar. That's the, the definition of l is k bar. And now I take bar out means that sum a and j, both of them from one to n, the whole thing bar and the whole combination inside the parenthesis. By the assumption that k is a kernel function. By this assumption inside is positive.
00:08:49.414 - 00:09:36.444, Speaker A: So the whole thing here is positive. So this means that l is a kernel function. That's very easy. And note that little l of X, which is defined by the formula k of Y l. That's the definition. So this is the definition of little l of X. And by assumption this is k bar of y and x.
00:09:36.444 - 00:10:35.324, Speaker A: So this is assumption. And again I use one more time, the definition this is equal to little k x at point y bar. For every y in space x. Again, this is the definition little k. And we see that immediately little lx is equal to little kx bar for every x. That's the simplest manifestation of what is claimed here. What is claimed here is that the element of Hl are the conjugate of element of Hk.
00:10:35.324 - 00:11:54.854, Speaker A: So here is one incidence of that. L of X is an element of space Hl and k of X is an element of Hk. And by definition we obtain this and the rest is just linear combination of both sides. And then taking the limit as analyst two, here is the way to do it. So that's one thing to do the other thing, which indeed is an outcome of this. What is the inner product of lx and l x prime, say in h of l? So let's calculate this is lx at the point x prime, the meaning of kernel function which is equal to the capital L x prime and x. That's the defining relation between capital l and small l.
00:11:54.854 - 00:13:10.774, Speaker A: By definition, this is k of x prime, x bar. And we do the same identities we did for l in reverse order for kick. So this is little k x at point x prime with a bar, and that is kx kx prime in h of k bar. And having the bar, we can change the order, right, kx prime kx in h of k. This is an important identity, the relation between h of L and h of the inner product in HL and the inner product in Hk. I write it again, the pax formula. So L index x in a product with l x prime in h, well, is equal to the order reversed kx prime, kx inner product in Hk.
00:13:10.774 - 00:14:36.504, Speaker A: This is a consequence of indeed this identity and also will help us to establish this. Let's see how we can do that. First step is the definition of c on a dense set, precisely as we did in Mohr's theorem. So, definition, we define c of the sum j from one to nice alpha j k, say yj to be sum j from one to n bar. Of this, say alpha j bar and kyj bar is l y j. That's my definition. How to to show that? I mean, this is well defined because this is an element of this space, but the representation is not necessarily unique.
00:14:36.504 - 00:15:23.436, Speaker A: I mean f, which is of the form j from one to n alpha j k y. It might also be of the form a from one to m beta. I say k z I with another set of points. We have seen incidents of this before. If there is a linear dependence between kernel functions, this is possible like the third and fourth example, in Sobel of spaces. In the third, one k zero was equal to k one equal to zero, both of them. So two linear dependence here.
00:15:23.436 - 00:16:00.194, Speaker A: And in the fourth, one k zero was equal to k one. So one one dependence. And many more combinations in more sophisticated spaces are possible. So, briefly speaking, f might have several combinations like this. And when we define c, we might use this combination. So it gives us one c of f with this one and another formula with this one. And in the general setting, there is no clue that these two are the same.
00:16:00.194 - 00:16:47.162, Speaker A: If not, it's not a good definition. And we saw this indeed in Moore's proof too, and we showed that it's well defined. There is no problem in the general setting. You need to consider an identity like this, which is like this, and calculate cof using the first formula. Also cof with the second formula, and show that they are really equal. With some techniques. One of the techniques which in many, many occasions help us is to show that there is an isometry between the function which is here and the function which is in the image.
00:16:47.162 - 00:18:45.530, Speaker A: In other words, if I can show that the nor of j from one to n alpha j k y in the first space is equal to the norm of its image, which I wrote as sum j from one to n alpha j or l over j in h, if I can show this for any combination, any linear combination with the different alpha j, that is enough because, because you take the difference here and you have a combination such that it's equal to zero, so its image is equal to zero, which means that in the combination cof, call it c one of minus c, two of its norm is equal to zero. So they are equal to each other. So if, whenever you show an isometry like this, I emphasize this because it will happen in future lectures too. Whenever you have an isometer like this, the definition is fine and we can proceed. And now it is easy to show this isometry based on, based on this, the green or this one, or based on this box formula, it is easy to arrive at what I want. So this box formula is also consequence of the previous one is useful for us. Let's do, I mean, let's calculate the norm squared.
00:18:45.530 - 00:20:06.338, Speaker A: So sigma j from one to n alpha j k y j squared in h of k is equal to, well, its inner product with itself j from one to n alpha jkij and sigma I use another index I from one to n alpha I k y. I need. It's linear. We can write it as sum over I and j alpha I bar alpha j k y j inner product with kyi in h of k, which we know it's equal to. There it is the inner product of the corresponding function in l, but the order reversed. So this is equal to. I can put identity here.
00:20:06.338 - 00:20:55.414, Speaker A: Indeed, it's better to put it there. Alpha I power alpha j l y I l y j in h. Well, and I do the same operation in reverse order means that I put the coefficients inside and sum. So now I have a sum I from one to n alpha I bar l o y. And alpha j goes inside. It becomes conjugate sum j from one to n l y j in h of l. And these are the same.
00:20:55.414 - 00:22:26.246, Speaker A: So it's become sum j from one to n alpha j bar l o y norm square. And this is precisely what I wanted to obtain, the isometry, the isometry, this one, which is what I wanted indeed, to prove c from h of k to h of l, mapping f to fr and such that cof in hk in Hl is equal to normal f in Hq. I did not do for all elements. But I'm not far away from the end, because what I proved here is that this is true for all f. The form sum alpha jkyj. And the good thing is that we know this, call it w. As in most theorem, w is dense in this space and its image is also densing h of l.
00:22:26.246 - 00:23:36.384, Speaker A: And the rest is just taking a limiting argument and using the fact that convergence in norm implies point phase convergence. In other words, you can see that an arbitrary f in h of k, not necessarily of this form, but there is a sequence, I call it fn in w, such that f goes in f in h of k. W is the collection of all functions of this form. And by exam, I'm sorry for that. We know that. We know that fn is a Cauchy sequence to end. So cofn is also a Cauchy sequence.
00:23:36.384 - 00:24:38.434, Speaker A: And then the fact that cofn converges with my apologies, implies that. And the fact that cofn is equal to fn bar, this we have already shown both of them, implies that. And also, don't forget this, that c of f is equal to f bar. I mean, it's a little detailed that I leave it for you to complete. But the bottom line is the fact that convergence in nor imply pointwise convergence. And that is why we obtained this one here. Yeah.
00:24:38.434 - 00:25:41.034, Speaker A: Is this space Hl isometrically isomorphic to the dual of Hk, we can make a mapping, I mean, it's not mentioned in this way. In the proof. But we have, if you wait, not our next example, not the third one. The third example that the self proposition that I will study today is about rkhs induced by inner product, and in that we will see precisely what you mentioned. So wait a little bit. We'll arrive at that result hopefully. Hopefully today our next rkhs is rkhs induced by just one function.
00:25:41.034 - 00:26:51.814, Speaker A: We saw this before, but there is one more item in the result that I want to highlight. So, rkhs by one function or by a function. We will need this in a future lecture too. So proposition x is a set, and f is any function on x with this value in c f not identically equal to zero, just a non zero function, even it might be nonzero at one single point and zero everywhere else. So, and define so put k of x and y to be f of x f. Then we saw before. K is a kernel function.
00:26:51.814 - 00:27:52.844, Speaker A: At least I mentioned this. What is new is that, and h of k is indeed a one dimensional space, is indeed is equal to c times f one dimensional vector space. And in this space for sure, f is there normal f in this space is equal to one. It's normalized that way. We will need this result in future. Even though it's elementary. That k is a kernel function is very easy.
00:27:52.844 - 00:28:44.044, Speaker A: As before, we fix some points and study what is sum I j from one to n alpha I bar alpha j k x I xj. What is this? So we have the formula for k. We plug it there. So sum alpha bar alpha j f of xi f of x j bar. That's the definition of k. And now we immediately see that this sum can be written as the sum I from one to m alpha bar a from xi. The whole thing, absolute value squared, which is positive.
00:28:44.044 - 00:29:35.904, Speaker A: So it's a kernel function. And what is the little k here? Little k at point y k of y by definition is k y and x which is equal to f of x bar by definition. This is true for every x.com as our argument in the space. So the same identity can be written this way. K of y is equal to a constant. No, wait a minute.
00:29:35.904 - 00:30:07.920, Speaker A: There is something. Oh, I see, my mistake. I see there is something wrong. There shouldn't be bar on this. My mistake is precisely here. My mistake is that k of x y is x here, y there. So it will be f y bar, and x is the argument.
00:30:07.920 - 00:31:10.814, Speaker A: So this identity can be written also as ky as a function is equal to a constant times the function f. So it's just one function with different multiples. And if you remember the space w, the space w was the space of all linear combinations, alpha j, k y j, with different values of alpha j and y j. But it doesn't help that much. I mean, if you consider different values of yj, it gives you different coefficients here, and you multiply by alpha j, that's another coefficient. And sum them up is still another one. So at the end of the day, when you consider all different yj and alpha j, you obtain nothing but c times f.
00:31:10.814 - 00:32:06.564, Speaker A: And c times f is already closed. So the space is precisely this one. It's one dimensional. So h of this kernel function is precisely cf. And how can we calculate the normal f in this space? Very interesting trick. I mean, playing with all different identities we have seen by now. On one hand, we know that k of yy is equal to kyy, k by ky, which is equal to the norm of ky squared in the space.
00:32:06.564 - 00:32:49.574, Speaker A: And now let's see what each side gives us for the normal ky. Ky is calculated. Here is the norm of f y bar as a coefficient time f. So the coefficient comes out and the function stays. And this is precisely what we want to calculate. We want to calculate normal f. So that's what this part gives us.
00:32:49.574 - 00:33:38.364, Speaker A: And now let us, let's see, what is k of y by definition, the very original definition, k of y is f of y, f of y bar. At the very beginning, we defined k of x and y to be f of x, f of y bar. So it's absolute value f of y bar. And now compare these two using this identity. You immediately see that. So, absolute value of f of y squared, normal f squared is equal to f of y squared. And now this is the, for any y, this is the point.
00:33:38.364 - 00:34:56.064, Speaker A: You see, I emphasized, f is not identically equal to zero. So at least there is one point such that f of y is not zero. And we can eliminate f of y from both sides and deduce that normal f is equal to one. It's what we wanted to prove, the very specific rths, but it will appear in our discussion near future, too. Another example is an interesting one with applications in economy. The mean function. We define k on zero, infinity times itself with values in r, by a very simple formula, k of x and y to be the minimum of x and y.
00:34:56.064 - 00:35:49.994, Speaker A: So k leaves here, and that's the frontier in this, in this neighborhood. Here, k is equal to x, and here k is equal to y. And on the diagonal, they coincide immediately. You see that I haven't shown that it's a kernel function yet. I need to do a little bit of work. But at least in advance, you can say that h of k consists of continuous functions, because k is continuous. We'll see some of its element later on.
00:35:49.994 - 00:36:50.004, Speaker A: But at least from k continuous on zero infinity, we can immediately, you see that the importance of that theorem, without knowing what is in this space in advance, we can tell that h of k consists of continuous functions. That's the power of that theorem. But before arriving at this, we need to show that k is a kernel. It's not as straightforward as the examples before. We need to do a little bit of work. And for that, let me introduce this matrix Gn, which is an n by n matrix. All of its elements are one.
00:36:50.004 - 00:37:52.584, Speaker A: We need this one. And the lemma about this is that lemma and jn is positive, bigger than or equal to zero, and has eigenvalues. And sigma. Sigma is a notation which we use in operative theory. But in matrix analysis, it's just eigenvalues. Sigma of Gn is just zero and one, but with different orders. The eigenvalue at one has multiple.
00:37:52.584 - 00:38:28.940, Speaker A: Let me read here. Okay. It's not mentioned in the in dilemma, but the order of one is one, and the order multiplicity of one is one. But the multiplicity of zero is n minus one. This is, it's not explicitly mentioned. So multiplicity of this one is n minus one. This one is equal to n.
00:38:28.940 - 00:38:59.804, Speaker A: It's not explicitly mentioned in the theorem. And there are many ways to prove this lemma. It's usually the case with resulting linear algebra. Many elegant proofs exist to show the same thing. For positivity, the most straightforward is, as before, to calculate that sum. Robert, can I interrupt? I'm misunderstanding something. This is the matrix of all ones.
00:38:59.804 - 00:39:17.252, Speaker A: Yes. Aren't the eigenvalues zero and. Nice? Yes, thank you. Yes. N zero and n. Yes indeed. Yes indeed.
00:39:17.252 - 00:40:31.774, Speaker A: I will add in the, in the, in the proof I added here myself by, by hand, I written the determinant of lambda minus jn is lambda exponent n minus one times lambda minus n, which makes this correction. Thank you for pointing this out. So it's zero and n. So the proof for positivity is, I mean, the straightforward proof that we use all the time for any v in c m or here rn. But we have to do it for c n. Indeed, even, even if the matrix is real, for cn, what is the sum sum I from one to n j front? If that sum is precisely that sum is precisely gn of v inner product with b in c n. True.
00:40:31.774 - 00:41:44.674, Speaker A: And if you write it more explicitly, this is a ing from one to n, then alpha I bar alpha g bar times the ig element of jn, which is all the time equal to one. And oh no, bar here. And we see immediately that as we saw before, this is the absolute value of one of them, for example, j from one to j squared in absolute value, which is positive. So this shows that it is a positive. And to show that it has just two eigenvalues. One way, as I said, is to calculate determinant of lambda I minus jm. And it's an easy exercise to see that this is lambda exponent n minus one times lambda minus n.
00:41:44.674 - 00:42:16.824, Speaker A: So I let you to do it directly. I mean, it's not something difficult. Another one is to give explicitly the eigenvalues and eigenvectors. For this matrix. It's easy. V one all its element coordinate equal to one. And we see that gn of v one is equal to n v one.
00:42:16.824 - 00:43:23.174, Speaker A: So it shows that n is an eigenvalue and v one is an eigenvector. And to show that we have also zero as an eigenvalue, as an eigenvalue, I give you other vectors, v two. You put one here and minus one there and zero for the rest. You shift this down and finally you arrive at one minus one or n minus one of them, clearly independent. And for each of these you immediately see that gn vk is equal to zero. So they are in kernel and show that the kernel has dimension n minus one. The other dimension is for the eigenvalue created by v one.
00:43:23.174 - 00:45:07.442, Speaker A: Why do we need, this is for the kernel created by the min function. And here is the proposition k defined on zero infinity times zero infinity to r by k x y is a kernel function. So what do I need to prove? I need to consider the endpoints. So let x one, x two and x n be in the space x x is here, zero up to infinity. And I need to consider the matrix k x one x I x j this n by n, and show that this is bigger than or equal to zero. And what is this matrix? It depends. If x comparing x one and x two comparing to them, which one is smaller and k of x one xix would be eventually either Xi or Xg.
00:45:07.442 - 00:46:11.124, Speaker A: So I have a matrix like this, and the component ej here is eventually either Xi or Xg, depending which one is smaller. So to give it a bit of order, we can say, without loss of generality, x one, less than x two less than x three, and up to x n. And even if they are equal, it really doesn't matter. But why without loss of generality in general? And it's an important point to consider this. When we look at the original definition, we do not assume this. Indeed, there is no ordering. When we consider an arbitrary x, we consider endpoint.
00:46:11.124 - 00:46:51.064, Speaker A: And sometimes we emphasize that they are the distinct. Here, if, even if they are not distinct, that's not a big deal. But still we cannot assume in advance that x one is less than x two, x less than x three. So there is no ordering. But why? Still I can say that without loss of generality, this is true. This is again based on our knowledge of linear algebra. And because if p is a permutation matrix, it means that it's a matrix.
00:46:51.064 - 00:47:34.766, Speaker A: Of this form, all elements are zero, except at the intersection of two rows and two columns. Here, we put one here, one here, and of course on the diagonal is equal to one. Here is zero, here is zero. On the diagonal one, and all other elements are equal to zero. So it's kind of permutation between either row I, rho j, or column I. Column J. I can, I can call it pij for further reference.
00:47:34.766 - 00:49:09.524, Speaker A: So the question is for you, if you're not familiar with this matrix, what is pij multiplied by a matrix? A from the left side, or we multiply from the from the right side. What is the consequence? It is easy to convince yourself that if you multiply from the left side, the outcome is the same matrix, except that you change the row I with rho j, you just change the ordering. And the same thing with the other one. If you multiply from the right side, then the columns are changed, but just column I with column j. And another thing to notice is that pij inverse is the same as pij, and of course is equal to pij self adjoint. And the final thing we need here is that a is bigger than or equal to zero if and only if pij API is positive. In other words, you can multiply it from both sides, left and right, by pij with any I and j you wish.
00:49:09.524 - 00:50:23.942, Speaker A: And that's the clue. That's the clue for what I wrote here. That's the clue for what I wrote here. Of course, at the very beginning, x one might not be the smallest one, but we apply this observation here. We multiply our matrix by pij with right choice of I and j, such that we can bring the smallest one at the very beginning. And what would be the consequence of this if we do this? If x one is the minimum of x one up to xn and we apply this transformation, then k x one, xi xj. This n by n matrix will start like this, because it's the smallest one here.
00:50:23.942 - 00:51:20.460, Speaker A: Still we do not know, you can see that later. So by bringing x one to the very beginning with this transformation, we have this formula, and then we do similarly with the second smaller one, we multiply from both sides. And note that if we change, say, row I with row j here, it really doesn't matter because it's x one and the same for column column I and column j, then we change it. This is x one. So it doesn't affect this part, which is already taken care of. We improve the rest. And so we continue up to the end and we obtain x one less than or equal to x two less than or equal to xn.
00:51:20.460 - 00:53:02.644, Speaker A: And if we do all of this, we will see that at the end, when we do all this modification, our matrix has this form. Then here is x two, then about this is x three, and we continue up to xn here, in other words, and this broken line is a constant. And finally we have x n here. And this way it's easier to show that we have a positive matrix, because you immediately see that if x one is less than or equal to x two less than or equal to x, and I can write this matrix as x one times jn. So if I subtract x one from the previous matrix, this is equal to plus plus. What if I move this to the other side to see what is my matrix? Here x one times jn is a matrix, all of its elements are x one. So here I would have zero, and here x two minus x one.
00:53:02.644 - 00:53:58.848, Speaker A: After this, x three minus x one, and finally xn minus x one. And now you see, up to here it wasn't that clear why I consider zero up to infinity. Why just positive ones? It's more transparent here. Now, jn is positive, we multiply by a positive coefficient. So the whole thing is positive. This one is also positive by induction. I mean, we start with n equal to one.
00:53:58.848 - 00:54:39.180, Speaker A: N equal to one is just k x I, x j. If we consider one by one, it's just x one. And again, this is another indication why we can see the zero to infinity. It's something positive. So we have a positive matrix. Here we have a positive matrix, there we add them up. And conclusion is that kxi xj for any n is positive.
00:54:39.180 - 00:55:05.224, Speaker A: So we have a kernel function now we want to. I mean, I will have a break now. After the break, we try to detect some of the elements of this space. It.
