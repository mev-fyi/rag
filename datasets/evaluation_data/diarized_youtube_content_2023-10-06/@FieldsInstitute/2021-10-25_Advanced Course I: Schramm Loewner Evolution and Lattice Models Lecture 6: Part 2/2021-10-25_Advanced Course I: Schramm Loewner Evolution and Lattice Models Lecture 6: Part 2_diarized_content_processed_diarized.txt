00:00:03.000 - 00:01:28.344, Speaker A: Welcome back everybody. And now, as I promised, we'll look at more general theorems on martingale conversions. So remember the question that the meta examples that we had in the beginning, when can a martingale be generated by one function which is f infinity measurable? So here we'll partially answer this question, and then we'll have necessary insufficient condition a bit later. And so first we start with a situation when you have a martingale which is continuous on continuous time, which is uniformly LP bounded. So not only these expectations fine, they uniformly bounded. And of course p is bigger than one, as promised. And the claim is that in this situation you can find x infinity which belongs to LP, and infinity measurable such that xt converges to x infinity, both almost surely and in LP that only happens for Pls infinity.
00:01:28.344 - 00:02:25.464, Speaker A: You can't assume that bounded martingale uniformly, conversely, that cannot happen. But again, this is reminiscent of situation in hard space situation. And finally, that's what we want, that xt can be restored from x infinity by taking conditional expectations. So if you are uniformly in Lp, there is an Lp function in infinity which generates your multi care. Again, if time is continuous, you need to assume continuity. First, let us remark that of course it's enough to prove everything for p less than infinity, because l infinity of course is subset as Lp, and almost true limit of function from l infinity of course belongs to l infinity. And we never claim that there is a convergence on l infinity.
00:02:25.464 - 00:03:49.618, Speaker A: So in the rest of the proof we will assume that p is less than infinity, but bigger than one. Now what we would do, we would use duality exactly as in card space. We'll take subsequence xdk, which converges Lp weekly, meaning for every Lq function integral of this with respect to xdk converges somewhere, and it converges to sum x, since all of the xtk are x is also infinity measurable. So far, again, we just know that x is a weak limit of xdk. If we would show that this x does not depend on choice of tk, that would immediately imply convergence NLP and the fact that it belongs to lp. Because again, weak limit by definition is an element of Lp. And to establish this, we actually start with proving this, that xt xt is expectation of x and with respect to x t.
00:03:49.618 - 00:04:41.224, Speaker A: So let us fix s and let's take a belonging to fs for sum s. So s is not a stopping time. Here s is just a time s. Then characteristic function of a belongs to Lq. So if you want to compute integral of a x, this is just the limit of integrals of xdk of a, right? So because by definition of a convergence, so this is the same as integral over a of x, tk dp. And then for tk bigger than s, you have this identity. So when tk becomes bigger than s, surprise, surprise, this integral becomes equal to this.
00:04:41.224 - 00:05:38.004, Speaker A: So what we just proved, we just proved just from the convergence that excess is expectation of x with respect to fs. Okay, now, f infinity is the minimal sigma algebra which contains all the fs. And, you know, expectation of x with respect to any of this sigma algebra fs, then it's the standard thinking matrix measure theory. This unique level almost surely defines x. And so by compactness, of course, xt itself weakly converges to x. Again, not yet LP converges, but so far it can go just quickly. And we have this identity.
00:05:38.004 - 00:06:19.138, Speaker A: So this is a somewhat twisted way to see it. First, we start with proving this identity, which just follows from the convergence. This implies uniqueness of x. This in turn implies weak convergence to x. Okay, so now let us try to get almost show and LP conversions. Let us define variation of x at infinity. At the process of omega is simply lim sub minus limit.
00:06:19.138 - 00:07:13.484, Speaker A: We want this to be zero. We know that it's almost surely fine, because by one of the previous corollaries, lim soup of absolute value of x t is bounded. Also, probability that it's bigger than epsilon is bounded by the probability that limp of epsilon follows. Bigger than epsilon over two. It's bounded by two of epsilon, integral of x infinity in DP. Okay, so now how does a proof proceed for hard space? In this situation, you say, okay, suppose that you are in a dense subset of continuous functions. Then for continuous functions, you have much nicer conversions.
00:07:13.484 - 00:08:11.754, Speaker A: And then this maximum inequality would imply everything. So here the role of continuous function would be played by predictable functions. So the functions which are fs measurable for sum s. So the functions which are just depend on what happens up to time s. So this is d and d is dense nlp again, because f infinity is generated by all the sigma fs. So this is dense in lp. So what does it mean? It means that for every epsilon, you can find j epsilon such that expectation of x minus j epsilon to the p is bounded by epsilon to the two p.
00:08:11.754 - 00:09:25.944, Speaker A: So which means, remember, we want this to be small. I want expectation of x minus j epsilon is bounded by epsilon squared by standard young inequality. Now observe that omega of j epsilon is zero, because of course, lim sub of x t is equal to limp of xt is excess. Remember, if x was g, then everything would stop at time s because g is the first measurable, there would be no change. And now we write it as so probability that omega S is bigger than epsilon is the same as probability that omega of x minus j epsilon is bigger than epsilon. Subtracting j epsilon doesn't affect the limit. And now we can use this estimate to see that this is bounded by two over epsilon times epsilon squared, which is two epsilon.
00:09:25.944 - 00:10:20.694, Speaker A: So probability that variation of omega X is big, it's actually, well, small. So omega X is actually equal to zero almost surely. Because again, probability that is bigger than epsilon is bounded by two epsilon. And so the limit of xt exists almost surely. But is it x? So for this you'll need some extra steps. But first let us establish that actually you have lp convergence to x. So here things are easy.
00:10:20.694 - 00:11:13.414, Speaker A: The difference between xt and x in LP is bounded by the difference between xt and j epsilon. Also NLP and the difference between J Epsilon XLP. Well, that's standard. Now, what is the difference between xt and j epsilon, at least for t bigger than s? It's expectation of xt minus j epsilon with respect to ft to the power p. Then you take expectation, take it to the power one over p plus this difference. So, but again, this is bounded by j epsilon minus x in Lp. So here I should remove t.
00:11:13.414 - 00:11:43.994, Speaker A: So this j epsilon to the p with respect to ft. So this thing also bounded by this j epsilon x. Again. So this is again another application of young inequality. So this is bounded by two epsilon squared. So lp norm of this happily times to zero. And then the question is, well, this is all great.
00:11:43.994 - 00:13:45.394, Speaker A: Why is yeah, equal to x. One way to see it is to say, okay, xt converges to x in Lp. So there is a subsequence which converges almost surely. But this, this is because there exists x t n convergence to x almost surely, which means that of. And this. So just to notice that this of course finishes. You are muted.
00:13:45.394 - 00:14:30.074, Speaker A: Thank you. I am back. Sorry for this interruption. Let me try to share. Okay, my screen again. So, okay, so actually I got interrupted in a good moment. We just finished showing that the proof of the theorem, that there is an easy way to see it that actually xt converges to almost surely to x, which is lp limit of x t's.
00:14:30.074 - 00:15:44.612, Speaker A: Now let us try to show it by another way, which is more complicated, but which will lead to a much more general setting, so we call family of processes or a process Xt uniformly integrable. If two conditions hold. First is that for every epsilon. So now, of course, my computer decided to act up not just my Internet connection. Well, it's one of these days. Okay, so two conditions here we assume that if measure of the set is small, then integral over the set of xt is less than epsilon. And second is that supremum of all the integrals.
00:15:44.612 - 00:16:35.594, Speaker A: So they're uniform integrals. Expectation of x T is bounded of infinity. Now, if you're an analyst, condition two is unfamiliar to you, because for all normal measures that we like and use, say, for libyac measure, for any non atomic measure, condition one implies condition two. You just subdivide, you're set into small subsets of size delta. And then this condition implies that supreme of integrals is small. But in probability, they want to deal with abstract measures. So that's why there are these two conditions.
00:16:35.594 - 00:17:52.914, Speaker A: Now, observe that if you are uniformly bounded in LP like we had in our theorem, then you are uniformly integral simply by chapter of inequality. Indeed, integral over set of small measure by junk is bounded by probability of to the power of one over q times Lp norm of xt on e. But of course, this is bounded by delta to the power one over q supreme of this expectation. So as long as the supreme is bounded, you are uniform integral. And this uniform integrability condition would give us everything. Now, one thing we all learned in our analysis course is that if you have uniformly integrable family and it converges to some y, almost surely, then it also converges to this y in l one that the norm turns to zero. That's actually the right condition for converters.
00:17:52.914 - 00:18:41.086, Speaker A: And here I outline the proof. So this integral of the difference is bounded by. Well, first you look at the probability that you are less or equal than epsilon, and then x n minus y is less or equal than epsilon. This is bounded by epsilon times this probability. Because x n minus y on the set is bounded by epsilon plus the probability that you are bigger, it should be y. So, probability that x and minus y is bigger than epsilon. And this in turn goes to zero.
00:18:41.086 - 00:19:26.994, Speaker A: Probability that Ui bigger than epsilon goes to zero. That's. Well, that's what almost show convergence guarantees you, because almost show convergence implies convergence in probability. And so this thing is bounded by epsilon for large n, and this thing is bounded by epsilon. So for large n, this is bounded by two epsilon. That's exactly what we wanted. And of course, this observation implies that since this already goes to zero in Lp in our theorem, then x ram is equal to y because we know that xt also converts to y.
00:19:26.994 - 00:20:24.824, Speaker A: Let me consistency right, right here. Okay, it turns out that actually uniform integrability is the key to understanding all these convergence theorems. So the next series of theorem is due to dup. And we will finally answer in if and only if terms. The question of van Martingale is generated by a single function. And the question about convergence first is, well, it's dupes martingale convergence. And here there is no uniformity.
00:20:24.824 - 00:21:28.044, Speaker A: So suppose that xt is a submartingale which is as usual, continuous for continuous types, and suppose that it has bounded suprema. Then there exists limit on the shore. So remember, we just know that lim Sop is bounded. But what I claim is that there exists limit. And again here it's as you will see from the proof, it's enough to assume that supreme of expectation of positive part of x, a maximum of x t is zero, is bounded. But it's easy to convince yourself that this is the same. And of course, same is true for super martingales xt with supreme of now xt minus minimum of x t and zero finite.
00:21:28.044 - 00:22:26.404, Speaker A: And this, all you need to do is consider submariting -60 so dupes maximum convergence theorem is true for super martingales as long as you assume subsequences. But again, for submartingales, this is actually equivalent to this. For super martingales, this is equivalent to this. Okay, so proof is actually not that easy. It relies on following claim, and I was hesitant to include it here. But actually this is so important for understanding lots of things going on with Martingale that I will include. Okay, so let's look at a function f defined on r plus, or maybe on if the time is discrete on natural numbers.
00:22:26.404 - 00:23:17.472, Speaker A: Let's just consider some partitions. So let's consider finally many times d f is a partition and s one would be the first time you are above b. Then s two would be the first time bigger than s one when you are below a. And in general for odd numbers, you want to be the first time above b after you hit after a, and this is below a. So let's look at example here. So here is this is t one, t two, t three, and so on. And at the time t one, you are still below a.
00:23:17.472 - 00:24:26.604, Speaker A: The first time when you are above b, is the time t two. Then the first time after this, when you are below a, is t five and t six is s three here, because again, you are immediately after a and then convention. Minimum of empty set is TD. So if this was your last visit above b or below a, that's it. After that your time is td. And now this d of f of ab, this is supreme over all partitions of f of the interval ab of these guys, dff ab, this is maximum of n such that s to n is less than td. Remember, when there is nothing left, when you don't down cross anymore, you just say, okay, this would be td.
00:24:26.604 - 00:26:24.004, Speaker A: So this is number of crossings from b two a. So this is the number of times you cross from above b to below a. And the amazing lemma, which of course is also due to loop, is that if you have a super martingale, then expectation of this guy is actually bounded by the expectation of by the supreme of x t minus b plus, expectation of xt minus b plus. That's why it defined this way, to be inherently stable, to be inherently finite. Okay, so notice that each Sk is a stopping time. And let us define Ak, which is omega such that sk less than td. So that Sk really exists that you really made val four even one up crossing for, or rather for even one down crossing forward once up crossing, okay, over two times.
00:26:24.004 - 00:28:04.274, Speaker A: So this is a case, this is an element of fsk. Notice that of course Ak is a subset of Ak. Sorry. Rather a k plus one is subset of Ak. If you crossed k times on a 20 on a to n, again, by definition, that's because it seems that I understand. Okay, I'm connected back. Okay, so now let us just integrate x as two n minus one minus b over the set a to n minus one.
00:28:04.274 - 00:28:54.364, Speaker A: Remember, a to n minus one belongs to f s to n minus one. So by submartingale property, this integral is bounded by this integral. Because conditional expectation of x s to n is big or equal than x s to n minus one. But this integral of course is bigger equal than a. You jumped over b, but let's bounce this. So x s to n is actually less than a on n on a to n plus. You also have this guy on a to n minus one minus a to n, this integral left.
00:28:54.364 - 00:29:42.744, Speaker A: So you separate your set a to n minus one to subset a to n, where x s to n is less than a, and the rest y is just this integral. But on this set, x s to n is equal actually td. And so what we get is that b minus a. So let's put it to this side. We get that b minus a probability of a to n is bounded by the integral of x td minus b. And to make it a nice inequality, let's put a little plus here. It's still an equality, right? Because it would be true even without plus without maximum.
00:29:42.744 - 00:30:29.024, Speaker A: So we can do that. Now, a ton is the set where the number of crossings is bigger than n. These things, they disjoint for different n, of course. So let's now sum it up and we get exactly this again by lake a principle. This is bounded by the supreme. And again, we just checked it for arbitrary repetition. So far, arbitrary repetition is bounded supreme was bounded.
00:30:29.024 - 00:31:28.450, Speaker A: End of story. Okay, and now, using this exciting theorem, we are ready to give a very weak proof of our exciting loop theorem that the limit exists. Suppose that it's not almost show convergent. Then there exists some a and b, such that limb width is less than a and limb soup is bigger than b with positive probability. So the number of crossings of Ab is actually infinite with positive probability. But you see expectation of this number of crossings, on the other hand, finite. That's a contradiction.
00:31:28.450 - 00:32:22.974, Speaker A: That's it. So that's all there is to it. Again, so just clever use of finite optional stopping times here. And now with this exciting development, we are ready to answer the question of when is xt generated by some x infinity? So when xt is equal to expectation of x infinity with respect to ft. Okay, so the next theorem, again by dup, is that the following are equivalent. Okay, if I wrote tfie, then of course, I didn't need to write equivalent. So let me raise e.
00:32:22.974 - 00:33:22.104, Speaker A: So, let us take a Martin galaxy, assumed to be continuous time, and the phonical equivalent, first, that xt converges null one when t goes to infinity. Second is our condition that expectation of x infinity with respect to ft is equal to xt. And third is that x t is uniformly integral. So the moment you are uniformly integrable, it means that you are generated by this, and you converge in l one to this function xt infinity. Okay, let's prove everything. So two implies three. Well, this is obviously because x infinity itself was absolutely continuous.
00:33:22.104 - 00:34:13.674, Speaker A: Every integrable function is absolutely continuous. Absolutely continuously integrable, sorry. Absolutely integral. So which means that the properties, well, for every epsilon exist delta, they carry out two functions, x t. Of course, because integral of x t over any set is the same as integral of x infinity. So if integral of x infinity is small, integral of x t is small, one implies two. Well, that's also easy, because expectation of x infinity with respect to ft, since we have convergence in l one, this is just expectation of x t plus s with respect to st, and this expectation was xt.
00:34:13.674 - 00:34:57.874, Speaker A: So that's also easier. So one implies two that's trivial, and the non trivial part is three implies one. So if you are uniformly integrable, then you have an l one limit. But by dupe it won't be too complicated, because we know that by dup exists. Almost surely that's by dope theorem. But now you have uniform integrability. You know that uniform integrability, in the case of uniform integrability, all mature convergence easily implies l one conversions, and then we are done.
00:34:57.874 - 00:36:09.514, Speaker A: So this is the condition. If your martingale is uniformly integrable, it's representable, and vice versa. And now we are ready to prove this general optional stopping times here. So suppose that you have a martingale and you have two stopping times. Well, first thing is that if t is bounded, then expectation of excess, sorry, conditional expectation of activity with respect to fs is excess. So this is generalization of our discrete stopping time theorem, where we had exactly the same condition. But now if X is uniformly integrable, then excess is actually equal to expectation of x infinity with respect to fs, and in the process it's also equal.
00:36:09.514 - 00:37:57.904, Speaker A: Okay, so let's return to the proof of this general optional stocking time theorem. So before this unintended break, we saw that our notion of uniform integrability allows us to say that if your martingale is uniformly integrable, then optional stopping time theorem calls for any stopping times. And first, let us note that the first part was about bounded stopping times implied by the second part, because XT is uniformly integrable for bounded times, because XT is expectation of xm with respect to ft. So if your times are bounded, then you have this final thing. Now, to prove two, it's enough to prove just this part, because then excess is expectation. Remember, we just pass the expectations through ft. So this is the same as expectation of X C with respect to fs.
00:37:57.904 - 00:39:22.394, Speaker A: Okay? So all we are proving now is that this is equal to this. Now observe that the family of expectation of x infinity with respect to b, where b is any subsequent algebra of infinity, is uniformly integral, because again, they are just restrictions of functions that can infinity. The integrals are the same. Now this by with the optional stopping time theorem, which we had before, we know this identity for sk, which are bounded and finally valid. Moreover, if Sk's have finitely many values and one of them is infinity, then this is still true, because what is it? So when I is just t, one, t, two, t and infinity. This is just finite index. Martingale.
00:39:22.394 - 00:40:10.094, Speaker A: It doesn't matter how I call the indexes. I can call them 1235 or I can call them one, two, five, infinity. That's still the same. Martin Gal. So this identity holds even for escape, which I allowed to have infinite to take infinite values, but they have finitely many values. Now, assume that sk has this finitely valid stopping time, since converges to s from above, such a sequence does exist. Notice that fs is subset of fsk, because sk is bigger than s.
00:40:10.094 - 00:40:42.734, Speaker A: And here is the short proof. We know that for any set from fs we have, I intersect with s, intersect with c, belongs to ft. So this means that a intersect with s, k less or equal than t. It's the same as a intersect with s, less or equal than t intersects with the scale, less or equal than t, because this implies this. But this already belongs to ft. And this belongs to ft. So this intersection of two sets, which belongs to ft.
00:40:42.734 - 00:41:26.934, Speaker A: So this intersection belongs to ft. So the set a itself belongs to fsk. Okay, so what do we have? A is an fs, a is fsk. So integral over a of x x k is the same as the integral over a of x infinity. Because again, xsk is expectation of x s infinity with respect to f. Now, this converges almost surely, and that's why we use continuity. And that's uniformly integral.
00:41:26.934 - 00:42:19.604, Speaker A: So integral over a of xs is the limit of integrals of a x s k, which is the no integral of a, of x infinity. So it means that x s is expectation of x infinity with respect to fs. That's the end of the story. Okay, so here you should probably ask, well, this is all great, but why? Why can't we just say that this is always true? Two stopping times, expectation of biggest stopping value of your marginal biggest stopping time. At a smaller time, that would be your martingale at the smallest stopping time. Let me show you an example where it's not true. So this is non example.
00:42:19.604 - 00:43:01.708, Speaker A: As you know, this xt expectation of brownian motion with minus t over two, this is Martin G. This is not expectations are exponential. Since it's exponentially, it's a positive martingale. And you know that bt minus t over two almost surely tends to minus infinity. That was one of our theorems that bt over t actually tends to zero. So bt minus t over one half t tends to minus infinity. So this tends to minus infinity.
00:43:01.708 - 00:43:45.474, Speaker A: Then x t tends to zero almost surely. Too much single tends to zero almost surely. And then let t be the first time that xt is less or equal than alpha, and alpha is less than one t is good enough, stopping time less than infinity almost surely, because we tend to zero. So eventually we will be less than alpha. But expectation of x at time t is just alpha, because this is the time we reached alpha. So we became alpha, which is not equal to expectation of x naught, which is just one. X naught is identically one.
00:43:45.474 - 00:44:47.338, Speaker A: So really we do need some conditions here. Now, since we have optional stopping time theorem, we can talk about stopped Mars and gaps. So let me run a bit late now because there were some interactions, and while I have Internet connection, let me keep going. We'll have slightly shorter break. Okay, so let me define stopped martingale. So x t is a martingale, t is a stopping time xtt as before, would be martingale, which is just x t up to time t, and then it's just x capital t. So this is a process which is measurable with respect to this exciting sigma algebra.
00:44:47.338 - 00:45:52.584, Speaker A: T's at time little t, this is just f t intersect with f t. So I claim that this stopped martingale is indeed a martingale, and ft was uniformly integrable. So is the stopped martingale. Okay, let's play with it. First, let's assume that x t is uniform integral, and let's define capital s to be minimum of t and capital t. It's stopping time s is less or equal than t. So by optional stopping time theorem xt at time t.
00:45:52.584 - 00:46:58.260, Speaker A: Well, this is excess. By definition, this expectation of xt with respect to fi sigma algebraic, this is minimum of TfT mutiny. So this excesses expectation of xt and with respect to something. And so xt is just uniformly integral. Now, if x t was any marginal, let us fix s. Then for up to the time, it's uniformly integrable. So, well, here, sorry for the notation.
00:46:58.260 - 00:47:55.444, Speaker A: We always have s less than t here, for some reason, I have t less than s. If t is less than s, then x t t is expectation of xst with respect to ft. Again, because we know that this is true for uniformly integral martingales and up to time s, you are uniform integral. That's all. Okay, so now we are ready to move on to the quadratic variation. And so here, this is, I think, a good time to take a break. So let's take a seven minute break, and at least I'll try to be back at noon.
00:47:55.444 - 00:48:02.404, Speaker A: Toronto time use Internet works. Okay, see you in seven minutes. Bye.
