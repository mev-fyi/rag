00:00:01.200 - 00:00:23.601, Speaker A: Okay, so let's get started. I remind you, for those who need a grade, which includes, I think everybody, almost everybody in this room, the assignment is due tomorrow night. You should have received a crowd mark link for that. If you didn't, you should let me know right away. There's no ta, so I'm grading it myself. I'm gonna do it within two weeks. I will.
00:00:23.601 - 00:00:48.907, Speaker A: I have prepared assignment two. From now on, assignments two through five are shorter. They're five problems each. And I will post assignment two tomorrow. Normally the rest of them are due every two weeks, but because we have reading week next week, Assignment 2 will be due three weeks from tomorrow and then all the rest are two weeks after that. And again next week is reading week. So there's no lectures next week.
00:00:48.907 - 00:01:19.473, Speaker A: Right. We have today and we have Wednesday and then we meet again two weeks from today. Are there any questions about any of that? I'll have office hours over reading week? Yeah. The office hours for this course are Fridays at 10:30 to 11:30 or by appointment. Okay, so we were. We were talking about curvature chapter, chapter four of docarmo. And that I said was assumed to be a review.
00:01:19.473 - 00:01:50.669, Speaker A: So I was stating a lot of things without proof, all of them are sort of trivial to check. Or at least even if it's not trivial, you can read the chapter on your own. There's a couple more things that I didn't get to. So we're going to finish that today. We're talking about Ricci curvature, scalar curvature, and then I'm going to show you that curvature basically can be interpreted as the, as the failure of covariant differentiation along two different curves to commute right, like on a parameterized surface. So we're going to prove that result. It's in chapter four, because we're going to use that right away.
00:01:50.669 - 00:02:32.155, Speaker A: After that, about 20 minutes or 25 minutes into the lecture, we're going to start talking about Jacobi fields for the rest of this week and probably one more lecture after reading week. And Jacobi fields are special types of vector fields along a geodesic. So you fix a geodesic and you look at a particular type of vector field on that geodesic, and that gives you information about the manifold, about the metric. Okay, so that's what we're going to be doing. So before we do that, let's just finish the rest of the stuff about curvature. Let's talk about Ricci curvature. So MG is Riemannian, and remember we have a Riemann curvature tensor.
00:02:32.155 - 00:03:55.033, Speaker A: So with Riemann curvature tensorflow r. This is a 04 tensor, right? There's two equivalent ways of describing as a 1, 3 tensor or 0, 4 tensor. Let's let x and Y be vector fields. We define RIC stands for RICCI of XY to be Gil R of Ei x y e l for any local frame E1 up to en of the tangent bundle. So remember, what does this mean? Like for example, a coordinate frame would be okay, and then this, this GIL is just is the inverse matrix of GJ gil which is G of eiel. Okay, so these are locally defined on some open set vector fields which are everywhere linearly independent. For example, I can take a coordinate chart in The D by DXIs are going to be a local frame.
00:03:55.033 - 00:04:42.145, Speaker A: Then I get the components for the metric tensor in that local frame. I can take the inverse which is invertible because this is a positive definite symm matrix. And then I compute this guy. So we need to show that this is independent of frame. So we need to show this is independent of the choice of local frame. Do this calculation because everyone should see it once. So suppose E1 tilde up to en tilde is another local frame.
00:04:42.145 - 00:05:32.391, Speaker A: So really what am I doing here? I'm defining for x and y vector fields. This is going to be a smooth function real valued function on M. Given a choice of x and Y, it's going to be a smooth function on M. And what I'm saying is if you want to evaluate this function at a point, pick a local frame whose domain includes that point and calculate this function. So let's, let's see what happens here. There exists an invertible n by n matrix of functions P such that EI tilde is P I m E M. There's a sum over M.
00:05:32.391 - 00:06:06.263, Speaker A: I'm going to break the Einstein convention here. In this calculation I'm going to just write everything in subscripts. It'll be easier to follow. But there's a sum over M here. And then what happens if we compute GI j tilde? Well, this is going to be G of EI tilde EJ tilde. And then if I replace the E tildes by the e's, I'm going to get a pimp JK gmk because I'll have a G of EM with ek. So if you think about this in terms of matrix multiplication, there's a sum over M and there's a sum over K.
00:06:06.263 - 00:07:08.197, Speaker A: It says that the matrix for G tilde, the ijth entry is given by this. Well, that's going to be the I entry of P, and then it's going to be a matrix multiplication of P with G with P transpose, right? Because I really want this to be kj to be matrix multiplication. So then if I take the inverse of this equation, this tells me I get P transpose inverse, G inverse P inverse. So let's compute what G tilde il times R of EI tilde XY e l tilde is. So I'm going to put in the fact that the G tilde inverse is this. So it's going to be P transpose inverse ia g inverse ab P inverse. And then from here I'm going to get a P.
00:07:08.197 - 00:08:04.133, Speaker A: Let's do it slowly. R of pik ek X y and P L M E m. So I can pull those out because this is a tensor and the transpose of the inverse is the inverse of the transpose. So this is P inverse AI g inverse A b P inverse bl pik plm R of ek XY em And let's look at this pairs with this. There's a sum over I to give us delta aka. And this one pairs with this one. There's a sum over L to give us delta bm, which means I can replace A by k and B by m.
00:08:04.133 - 00:09:06.613, Speaker A: So this is going to be G inverse km, which is the same as G upper KM R e k x y em. Okay? So it's independent of the choice of local frame. Because I compute it in the tilde frame or in the non tilde frame, I get the same answer. So note if E1 up to EN is a local orthonormal frame, which we can always do by running the gram Schmidt process on a local frame, just we can ensure that it's a coordinate frame. Then Ricci of xy is just the sum from k equals 1 up to n r of ek xy ek. Because in this case this is just delta km. If it's an orthonormal frame, okay? And we also know that because the Riemann curvature tensor is A is A is a tensor.
00:09:06.613 - 00:10:09.355, Speaker A: If I replace X or y by. If I multiply X or y by a function, it comes right out. We also have that Ricci of fx, Y equals Ricci of X, FY equals F Ricci of XY for all F and C infinity of M. And clearly it's bilinear in X and Y as well, because R is bilinear and VG is bilinear. So it's a smooth smooth 02 tensor. It's something that takes two vector fields and gives you a function and at every point it's a, it's a, it's a bilinear form on the tangent space. By the way, remember we talked about the fact that there are various different conventions for the, how we do define the Riemann curvature tensorflow.
00:10:09.355 - 00:10:49.985, Speaker A: So depending on how people choose to define the Riemann curvature tensor, that will affect their definition of the Ricci tensor. But everyone agrees on what the Ricci tensor should be in, you know, qualitatively. So in other words, this definition that I gave is exactly the definition. It has to be so that if you take the sphere which has positive sectional curvature, you get the Ricci tensor is positive. It's a positive definite symmetric bilinear form. Okay, so you might see other people contracting the Ricci to get the Ricci they would contract the Riemann on the first and third, right? And that's because they have another convention for the Riemann tensor. Also, why do we contract on first and fourth? Right.
00:10:49.985 - 00:11:35.887, Speaker A: If we contract it on the first two with the metric, we're going to get zero because it's anti symmetric in the first two. So if we contract it with something symmetric we have to get zero and if we contract it on the last two, we'll get zero. So it has to be one of these guys and one of these guys. And then because of the symmetries of the Riemann curvature tensor, all those choices, 1 3, 1 4, 23 or 2 4, they just differ by a sign. They're either equal or they differ by a sign. Okay, and this is like I said, the one we choose because we want the Ricci curvature of the sphere to be positive. Okay? So the next thing to say about the Ricci tensor, oh, by the way docarmo defines it, you know, he also has this issue that he picks the wrong sign for the, for the Riemann curvature tensor.
00:11:35.887 - 00:12:01.697, Speaker A: Non standard definition. But independent of that, he also puts a 1 over n minus 1 in here, which is very nonstandard. I've not seen that anywhere else. Okay. There's a, there's a method to his madness. There's a reason I can see why he did it, but no one else does it. So that means we have to be very careful in the rest of the book whenever we use the Ricci tensor to be aware that he's got this other factor of 1 over N minus 1 in front of there.
00:12:01.697 - 00:12:57.505, Speaker A: And we're not going to have that. Maybe I'll just write it down. So note Del Carmel's definition, definition of Ricci tensor is 1 over n minus 1 times the standard definition. Okay? And N is the dimension. Okay, so the first thing, or not the first thing, the. The main thing that's important about the Ricci tensor is that it's symmetric. So lemma, the Ricci Tensor is a symmetric 2,02 tensor.
00:12:57.505 - 00:13:54.737, Speaker A: Let's prove that. Actually, before I prove that, let me just say in local coordinates, in local coordinates, Ricci is rjk dxj tensor dxk, where rjk is Ricci of djk. So it's R I, J, K, L, G I, L. So this is a standard thing that's done in Riemannian geometry. We're using R also for the. When we're writing the Ricci tensor in coordinates, but it's clear because it has two indices, it can't be anything else. When it has four indices, it's the Riemann curvature tensor.
00:13:54.737 - 00:14:50.407, Speaker A: When it has two, it's the Ricci tensor. What? I'm going to define another thing in a few minutes called the scalar curvature, which is a function and some people use R for that, with no indices because it's a function and that makes sense. But then, then it's clashing with the notation that I said. Capital R with no decoration just means the Riemann tensor, right? That's why I'm writing this one. I wrote this as Ric because there's something else that's R of xy. Okay? So let's prove that the Ricci tensor is symmetric I, E Ricci of XY equals Ricci of yx for all X and Y. So we just need to show that the components in any coordinate frame are symmetric.
00:14:50.407 - 00:15:35.263, Speaker A: So let's compute rkj. This is R I, K, J, L, G I, L. And let's use the Bianchi identity, which says that I can, if I cyclically permute the first three and in fact any three of them because of the symmetries. But if I cyclically permute the first three and add I get zero. So I can use the Bianchi identity and replace this by minus R, K, J, R, J, I, K, l, G, I, l. And now the first term is going to be zero, because this is Q and I and l. And this is symmetric in I and L, right? So for every pair, if I equals L, I automatically have zero.
00:15:35.263 - 00:16:48.765, Speaker A: And then if I have an I not equal to L, when I, you know, I'll have two, two terms, one with the I here and the L here, and one with the l and the I And that's they're going to be negative opposites of each other, and these are going to be the same. So this is going to be zero. And here I'm going to use the symmetries, the skew symmetry in the first two indices, and that's just rjk. Okay? So it follows from the Bianchi identity and the symmetry in the first pair of indices that this Ricci tensor is symmetric. Okay, but because it's symmetric, it's almost exactly like the metric itself. The metric g is a symmetric 02 tensor, but that one's also positive definite, right? There's no reason for this bilinear form to be positive definite. So, but since Ricci is a symmetric bilinear form, symmetric 02 tensor, just like the metric, we can compare them.
00:16:48.765 - 00:17:59.225, Speaker A: They live in the same space. So we make the following definition. Mg is called Einstein, an Einstein metric, if and only if the Ricci tensor is some constant times the metric, where lambda is a constant. This makes sense to impose this condition because both sides are symmetric bilinear forms. In fact, you can make it even more general. You can ask for lambda just to be some smooth function, right? It doesn't have to be a constant. But here's a remarkable One can show that if Ricci is lambda G for lambda, a smooth function, and the dimension of M is at least 3, then Lambda is constant.
00:17:59.225 - 00:18:26.995, Speaker A: That's not at all obvious. It uses what's called this second Bianchi identity, the contracted second Bianchi identity. It's not a hard thing to do, but because it's not going to be relevant for us, I'm not going to do it. I just wanted to point it out, right? You could say, well, why do you make this a constant? It could just be a function, but it's actually not obvious. But it can't be a function if the dimension is at least 3. If the dimension is 2, it could be a function. Those definitely show up.
00:18:26.995 - 00:19:13.215, Speaker A: And in fact, in dimension two, the Ricci tensor is always a function times the metric. That's not obvious either, but it's true. So here's an example of an Einstein metric. Suppose mg has constant sectional curvature, little K. So we know that that means that R I, j, K L from last class is k times Gil gjk minus Gik gjl and I told you there are plenty. There are manifolds with constant sectional curvature. Let's compute the Ricci tensor.
00:19:13.215 - 00:19:49.065, Speaker A: Rjk I have to contract this thing with G upper. Illustration, if I have G upper il times G lower il that's the trace of the identity matrix. That's just N, it's a dimension. So this is N times gjk. And then here if I contract G upper il, I'll get a delta IJ here, so I get gjk. So I get k times n minus one gjk. And now you see why Docarmo wanted to put a one over n minus one in his Ricci tense.
00:19:49.065 - 00:21:01.325, Speaker A: Because he wanted, he wanted, if you have constant sectional curvature K, he wanted Ricci to be K times G. Anyway, so any constant sectional curvature metric is Einstein, but there exists lots of Einstein metrics which are not constant sectional curvature. And again, this is not a course on Einstein metrics, but I'll just give you an example. The complex projective space CPN with the Fubini Studi metric. This is the Fubini Studi metric. And if you know anything about Kalar geometry, this is the sort of standard metric on standard Kaylor metric. On cpn, this is Einstein, but not constant scalar sectional constant section.
00:21:01.325 - 00:22:14.565, Speaker A: Okay? And Einstein metrics are a very important class of Riemannian metrics that have. That show up a lot. But as I said, it's not the focus of the course. I want to give one more definition, which is the scalar curvature. The scalar curvature of G is a smooth function on the manifold given by S is GJK RJK. So GJK Ricci of EJ EK for any local frame E1 up to EN. So the proof that this is independent of the choice of local frame is exactly the same.
00:22:14.565 - 00:23:19.135, Speaker A: Basically, whenever you have a, an EJ in ek downstairs and a G upper jk, and the rest is a tensor, that's going to be independent of the frame. So this is equal to the sum From k equals 1 up to n Ricci of Ek Ek for an orthonormal frame, okay? And this is a function, right? We've, we've taken another contraction. We've gotten rid of all the indices and we've lost. Each time we go from Riemann to Ricci to scalar curvature, we lose more information and we throw away some information. And again, Docarmo defines s to be 1 over n times the standard definition. And you'll see why he does it here. Here's an example.
00:23:19.135 - 00:24:28.685, Speaker A: Suppose mg is Einstein with Ricci as lambda G. Well, that means that the scalar curvature is N, lambda is constant. Okay? Normally you get a function for an Einstein metric, you get a constant, and we know that every constant sectional curvature metric is Einstein. So it will have constant sectional curvature. But of Course, there exists constant scalar curvature metrics which are not Einstein. And if you're interested in learning more about constant scalar curvature, for example, look up the Yamabi problem. We don't have time to talk about this.
00:24:28.685 - 00:25:18.979, Speaker A: We could teach a whole course on the Yamabi problem. But it basically says if you have a Romanian metric, let's say on a compact manifold, can you always find a metric which is conformal to it, which is some positive function times it, that has constant scalar curvature? And this was a big open problem that was finally solved in the, in the early 80s by Rick Shane in the affirmative, that essentially you can always do that. Yep. Metric. Yeah, yeah. So saying something is an Einstein manifold is a little bit unclear, right? That means it's a, that there's a particular Riemannian metric on it, which is an Einstein metric. Okay? So, for example, the round metric on the sphere is constant sectional curvature, so it's Einstein.
00:25:18.979 - 00:26:21.371, Speaker A: But if you take some other metric on the sphere, it won't be Einstein in general. Okay? So that was a sort of brief introduction to other kinds of curvature. They are going to show up later in the course. For example, we're going to prove at some point the Bonnet Myers theorem, which says if you have a complete, which we haven't defined yet, if you have a complete Riemannian metric with Ricci curvature, strictly positive in some precise sense, then it has to be compact, right? So that we're going to be using these curvatures as assumptions on the metric. And the result is that if you have a metric with certain assumptions on curvature, then you get global topological consequences as a result. And let me just say in words without writing it down, there's geometric meaning to these things, right? So one thing you can do, and I'm not going to do it just because this is really not the point of the course, you can show that if you take the. Remember we talked about Riemannian normal coordinates as sort of being the best kind of coordinates centered at a certain point.
00:26:21.371 - 00:27:14.153, Speaker A: And in those coordinates, the first derivatives of the gijs all vanish at that point. So you can show that if you took the GIJ as a smooth function on the domain of this coordinate chart, in reminding normal coordinates and you expanded in a Taylor series, the linear term vanishes because all the first derivatives vanish. And basically the vanishing of the quadratic term is equivalent to the vanishing of the curvature tensor. So a way to think about the curvature tensor is that the manifold is the metric in normal coordinates centered at a point, the metric agrees with a Euclidean metric up to third order in the Taylor series, if and only if the curvature vanishes. And you can do similar things with the Ricci curvature and the scalar curvature in terms of volumes of geodesic balls and things like that. But that's not about. That's not what we want to talk about.
00:27:14.153 - 00:27:57.715, Speaker A: So let me end the discussion of Chapter 4 with the following lemma, which shows you that Riemann curvature is related to the failure of covariant differentiation along two different curves to commute. So let's let sigma be a parameterized surface. We talked about this a few lectures ago. Remember, this is some, some set in R2. Most of the time it's going to be some open set. It's going to be a nice set most of the time. So let V be a vector field along sigma.
00:27:57.715 - 00:29:10.933, Speaker A: Then what we can do is we can covariantly differentiate V along in the t direction for this parameterized surface or along S, and we can do those in either order and we can ask what the difference is. And turns out that the difference is D sigma R of D sigma ds, D sigma dt apply to V. It's what we want to prove. So Riemann curvature. Curvature is the obstruction to the covariant differentiation along two different curves to commute. Okay, so this is just a calculation. It's, it's not that interesting, but I'm going to do it because we're going to see, first of all, it's going to remind us how covariant differentiation along the curve works.
00:29:10.933 - 00:30:03.749, Speaker A: We're going to be using that a lot this week. And also we're going to see that actually we're going to use the symmetry limit. And maybe that's later today. No, we're not going to use the symmetry lemma here, but let's do this calculation. So everything here is a vector field along sigma, right? Because we know that if we have a vector field long sigma, we covariate, differentiate in the S or the t direction. We get a vector field along sigma and this as well. What does this mean? If you, if you want to evaluate this at a point S comma t, then that's a tangent vector at the point sigma of st, and so is this, and so is this.
00:30:03.749 - 00:31:07.685, Speaker A: And therefore we can act by this to get a tangent vector at the point sigma of st. So let's let x1 up to xn be local coordinates in a neighborhood of sigma of st. We're going to show this is true for all st and a, right? Then we can write v of st as vi of st d by DXI at sigma of st, where the vi's. Well, let's say vi of st are smooth. So they're defined in a neighborhood. I don't want to spend too much time worrying about the domains of these things, right? But we're looking at a small enough piece of a so that sigma of ST lies in the. In the domain of this coordinate chart.
00:31:07.685 - 00:31:54.645, Speaker A: And then we know that if we compute dt of V, what do we do? This is a function. So we differentiate this function with respect to t and we do nothing to the vector field there. And then we apply dt to D by dxi. So now let's take DS of this, DS of dt of V. So first there's this function of S and T. I have to differentiate it with respect to S and do nothing to the vector field. Then I, I do nothing to the function and I differentiate the vector field.
00:31:54.645 - 00:33:11.375, Speaker A: And now I have these terms. So I have a DVI ES dt D by dxi plus VI DS dt d by dxi. And we're going to compute the difference DS DTV minus dt dsv. And if you look at the terms where there's only one derivative, the sum of these two terms is symmetric in S and T, right? So when I swap the S and t and subtract, those are going to cancel. And also this first term is symmetric in S and t because the functions are smooth, so they're C2, so the mixed partials are equal. So when I swap S and t and subtract, all of these will cancel with the corresponding terms in the in the other one. And all I'm going to be left with is vi DS dt D by dxi minus vi dt DS V by dxi.
00:33:11.375 - 00:34:08.915, Speaker A: So we need to calculate those guys. So recall that D Sigma DT is going to be DXK DT D by DXK. So we're writing Sigma of ST is some X1 of ST up to XN of ST. The coordinate representation for sigma, right? This is really the coordinate representation. So this is D sigma dt and D sigma DS is dxl DS d by dxl. So now let's calculate. We want to calculate this and then again and then swap the S&T's.
00:34:08.915 - 00:34:51.415, Speaker A: So we have DT of D by DXI. Remember when you're. When you're applying covariant differentiation on a vector field along the curve, which is the restriction of a vector field defined on an open neighborhood. This is nabla D sigma dt D by dxi velocity velocity vector of that curve. And that if I put d sigma dt is this. These are functions, they come out. So this is dxk dt times nabla d by dxk D by dxi.
00:34:51.415 - 00:35:56.465, Speaker A: Okay, so now let's take DS of this. So now I have a function of S and t here. So first I have to differentiate that function and do nothing to this guy. And then I leave that one untouched and I differentiate nabla d sigma DS of D by dxk d by dxi. And I do the same thing here. D sigma DS is this. So this term is dxk dt dxl DS nabla d by dxl of nabla d by DX k D by dxi okay, so again, what I want to do here is swap the S and t and take the difference.
00:35:56.465 - 00:36:36.619, Speaker A: This is symmetric in S and t. This one is not symmetric in s and t. That's exactly where we're going to get the curvature. So if we do that difference DS dtv minus dt DSV is equal to. So I have a little vi from this. So I factor out the vi and I have this expression. As I said, when I take this difference, that's going to go away.
00:36:36.619 - 00:37:51.935, Speaker A: So I'm going to write this thing and then minus this, with the s and t swapped. So I'm going to have dxk dt dxl DS Nabla d by dxl nabla d by dxk d by dxi minus dxkds dxl dt. Okay, and now I want to combine these terms. So you see, what I should do now is swap the roles of k and l here, because these are just dummy indices of summation. So I'm going to swap the roles of k and l in the second term. And now these are the same, right? I can pull those out. So this is equal to vi dx k dt dxlds.
00:37:51.935 - 00:38:55.195, Speaker A: And what's left is partial l partial k on xi minus partial k partial l on xi. And the bracket of those two guys is zero because they're coordinate vector fields. So this is r l k I j d by dxj. And then that is equal to vi dx k dt dx l DS R of D by dxl D by dx k applied to D by dx j. And I can bring these guys in now, right? When I bring this guy in here, I'll get a d sigma ds. When I bring this guy in here, I'll get a d sigma dt. So this is vi r of d sigma DS D sigma dt D by D X.
00:38:55.195 - 00:39:22.845, Speaker A: This is. This is an I. Sorry. Because it acts on I. And then again, I can bring this through, and that's R of D sigma ds D sigma dt applied to V. Okay, so if we commute covariant differentiation along a curve, we pick up a curvature term like this. We're going to use this in the next few minutes.
00:39:22.845 - 00:40:16.837, Speaker A: Are there any questions about that? All right, so that, that completes chapter four of Do Carmo. There are some things I didn't do there, and there's a lot of things I didn't prove. Right, but you should go back and read it, because as I said, apart from this lemma, which I'm not assuming people have seen before, although it's just a calculation, the rest of chapter four, I'd like to assume as a review, since I know it's not for many of you. You should fill in the gaps. But we're moving on to chapter five. And now we're going to slow down again like we did in chapter three. And we're going to do everything very careful, very carefully.
00:40:16.837 - 00:41:41.193, Speaker A: So this is the topic of Jacobi fields. As I said, these are special kinds of vector fields along a geodesic. Okay? So this will be our first relation between geodesics and curvature. For the rest of the term, really, we're going to be investigating the relationship between geodesics and curvature. Okay, so what we'll see by the end of Wednesday's lecture, we'll see that if LP is a two dimensional subspace of tpm, then the sectional curvature of lp, this is some real number, right? That depends on P and the two dimensional subspace lp. This determines, I'll put in quotation marks, how fast geodesics which start at P and are tangent to LP spread apart. So this is not a precise statement yet.
00:41:41.193 - 00:42:20.991, Speaker A: The goal of the first half of this chapter is to make this precise. But what do I mean? Let me just draw a picture. There's the point P. My picture is already two dimensional, right? So this is, let's say this is lp, which is a two dimensional subspace of tpm. I can look at geodesics, different geodesics which come out of P, which are initially tangent to lp. And the question is, as you move along, are these geodesics getting sort of spreading out or are they coming back in on themselves? And we know what happens in RN with the Euclidean metric, they never meet again, right? They just. They're straight lines.
00:42:20.991 - 00:43:00.125, Speaker A: Two straight lines will only intersect unless they're the same line they're only going to intersect. Two straight lines passing through P in RN are either the same one or they only intersect at P. But we know on the sphere that the geodesics are great circles. So if I take two geodesics starting from the north pole, they will meet again at the south pole, right? So those geodesics, they're spreading out initially and they're coming back. So we're going to see that curvature is affecting the behavior of geodesics starting at a point. Okay, we're going to make this statement that's in quotation marks. We're going to make it quantitatively precise.
00:43:00.125 - 00:44:03.019, Speaker A: So for us, a Jacobi field will be a vector field along a geodesic gamma that is defined, that satisfies a particular, a particular ode. Linear ode and linear odes are the nicest differential equations you can come across. Right? So we like those. And this is going to be coming from the exponential map. That's why it's related to geodesic. It's a vector field along a geodesic and we're going to define it a Jacobi field by using the exponential map. So one thing is Jacobi fields are going to tell us some information about the geodesic that they're on.
00:44:03.019 - 00:45:21.845, Speaker A: But also they're also related to singularities or critical points. Critical points of the exponential map, that is points in TPM where the push forward is singular. Okay, the push forward, let's say V in TPM where this guy is singular. So if I compute the push forward XP at some point V, we know that If V is 0, this is the identity, so it's definitely invertible. Right? And if V is close to zero, this will be invertible. That's coming from the fact that the inverse function theorem tells that near 0x is a diffeomorphism and the differential of diffeomorphism is invertible. So we know that for V sufficiently close to zero, this will be invertible.
00:45:21.845 - 00:46:25.985, Speaker A: But in general, for any V that's in the domain of X, this is some linear map and it goes from n dimensions to n dimensions. So it fails to be invertible exactly when it has a kernel, a non trivial kernel, right? And those are going to be detected also by these Jacobi fields, we're going to be able to characterize exactly when does the exponential map become fail to be invertible in terms of the existence of a Jacobi field with certain properties. But I haven't told you yet what a Jacobi field is. Okay? So that's, that's the goal of this whole week now and probably a little bit after the reading week as well. Okay, so what we're going to do is we're going to revisit a construction that we had when we, when we proved the Gauss lemma. I'll remind you how that works. And we're going to see that from there we get a vector field along a geodesic which satisfies a particular ode.
00:46:25.985 - 00:48:13.945, Speaker A: And then that's going to motivate us to study. Well, let's look at solutions to this ode. What do those solutions tell us about the geometry? Okay, so that's the goal. So let's let MG be Riemannian and let P be a point in M. In the proof of the Gauss lemma, we saw that if XP is defined at some vector V, right? If V is in the domain of xp, that domain is some open set, open neighborhood of tpm, star shaped open neighborhood of tpm. And we take sigma of ST to be x p of t v of s for 0t to 1 and minus epsilon S to epsilon, where V is a smooth curve in the tangent space. Curve with V of 0 is this vector V and V prime of 0 is some W which is in the tangent space to TPM at V, which is canonically tpm.
00:48:13.945 - 00:49:25.445, Speaker A: Then. So if we, if we took a smooth curve V with these properties and we looked at x p of t times V of S, we get a parameterized surface and the push forward at V applied to W is equal to D Sigma DS at 01. Okay, so let me remind you how that, how that went. So let's see how that goes. Review of this calculation. What is D Sigma DS at 01? I want to differentiate with respect to S at s equals 0 and t equals 1, then I can put t equals 1. Already I'm taking a partial derivative here and I just want to differentiate with respect to S at s equals 0 sigma of s1 and that's d by ds at s equals 0 of xp of.
00:49:25.445 - 00:49:52.995, Speaker A: Let's go back to the definition of sigma of S and T. It's x p t versus. So if I put t equals one, I just get XPVs. And now that's a curve in TPM. And then I apply the X map to that. And then I take the velocity at S equals zero of that curve. So that's equal to the push forward at.
00:49:52.995 - 00:50:27.625, Speaker A: Well what happens when I put S equals 0 there I get V apply to V prime. Let's just put W this is V prime at zero. Okay, So I just reproved that for you. Let me draw a picture. So this is tpm. We draw this picture so many times. Xp, this is M, this is P.
00:50:27.625 - 00:51:13.815, Speaker A: We have this curve V of S. And this is V, which is v of 0. And its velocity at time 0 is w. And then this gets mapped and use the same colors. This is XP of V of S. So this is. So the pink curve here is x P of V of S.
00:51:13.815 - 00:51:59.415, Speaker A: And this vector here is expanded P star at V of W, which is just the Sigma DS at 01. Okay. Just the velocity of this, this image curve. And we can look at. We know that the straight lines through the origin, the straight lines through the origin in TPM get mapped onto geodesics emanating from P. So these are geodesics. The blue lines are geodesics.
00:51:59.415 - 00:52:38.135, Speaker A: And so you see that the size of this tangent vector, its norm, is somehow measuring how fast we're moving along this red curve in the manifold. Okay. And here, this red curve is sort of for every point S, we have V of S. And then XV of S is. Sorry, X, V of S is a geodesic. So this blue, if this is, if this corresponds to time S, then this is T times V of S as we vary T. And then we get exp.
00:52:38.135 - 00:52:52.539, Speaker A: Yes. The colors really aren't showing up on the video. Yeah, I know. There's nothing I can do about that. The color doesn't show up on the video. But if it was all white, then it wouldn't help you any better. Right.
00:52:52.539 - 00:53:23.261, Speaker A: So this is the best I can do it for as long as you know. Yeah, yeah, I know. I wish I could do something about it. Okay, so. So with that picture in mind. So the length x P star at V applied to W, that's a tangent vector. I can take its length.
00:53:23.261 - 00:54:37.999, Speaker A: That's just the length of D Sigma DS at 01. This is the length of the velocity of the curve. S goes to sigma of s comma 1 at s equals 0. Intuitively measures the rate of spreading of the geodesics. T goes to sigma of st, which is x P of t versus. Okay, starting from P with initial velocity versus starting from P with initial velocity V of S. Right.
00:54:37.999 - 00:55:16.523, Speaker A: This, this thing here, T goes to. This is exactly a geodesic. This curve T goes to x P. T of versus is a geodesic with initial velocity versus. Okay, so again in the picture, we're saying that as we, as we're following this red curve here, we're getting These, these radial blue lines which are getting mapped to geodesics. And if these geodesics are going to be spreading apart quickly, that means this green vector is going to be big in length. The velocity of this blue, this red curve, which is the image of this one, should be large.
00:55:16.523 - 00:56:44.725, Speaker A: Okay, this is all again, motivation. So we'll see that this is related to the sectional curvature of lp, where LP is the span of V and W. Okay, I guess strictly speaking here I didn't take V and W to be linearly independent, but if they're not, there's nothing to do. Okay, so that's, that's still motivation. I want to do one more, say one more thing about motivation before we actually get started. So also, if xp star of VW, if this length is 0 for w not equal to 0, then the push forward, which goes from TV TPM, which again is canonically TPM to T x p of v M is singular. This is a, this is a linear map.
00:56:44.725 - 00:57:14.825, Speaker A: And these are both, both N dimensional. So it's going to be not invertible. It's going to be singular if and only if there's something in the kernel. And that's if and only if there's some non0w. So when you plug this in here, you get the zero vector. So the length of this vector is telling us, you see how the geodesics are spreading. It's also telling us if the exponential map is singular.
00:57:14.825 - 00:58:48.427, Speaker A: Okay, so that's sort of it for the motivation. What I want to do now is take this construction and just generalize it a little bit, because what we've done here is we've put t equals 1, we've done this at s equals 0, but for t equals 1 and there's no reason for us to stick t equals 1. So we're going to compute this for s equals 0, but for any T, that's going to give us a vector field along the geodesic and then we're going to see that that satisfies a nice ode. All right, so recall, we had sigma of ST is exp p of t, V of S. Okay? So more generally, now let's consider D Sigma DS at 0T. So before we had T equals 1. Okay? And now by the same calculation, this is exp star at TV because why is this TV? This is T V of S evaluated at 0T.
00:58:48.427 - 00:59:21.945, Speaker A: I plug in. Sorry, I plug in. S is 0. So V of 0 is V and I plug in T is T. And this is acting on TW because this is D by DS at S equals 0 of TV of S, right? So the T comes through v prime of 0 is W. So that formula which is circled on the left board, the only difference when I evaluate this at t instead of at one is that there's this. There's these t's here, right? And you can see when t equals one, we recover what we had before.
00:59:21.945 - 00:59:55.514, Speaker A: So this is a vector field along sigma. So restricting. Well, it's a vector field. Let's say. Let me go to the other board. We know that D sigma DS is a vector field along sigma. So if I restrict it to S equals zero, it's going to be a vector field along sigma of 0t.
00:59:55.514 - 01:01:03.327, Speaker A: So d sigma ds is a vector field along sigma. So d sigma ds at 0t is a vector field along the curve sigma of 0t, which is x p of TV. This is the geodesic gamma V of T. Okay? It's a geodesic with initial point t and initial velocity v. So I've constructed a vector field along the geodesic and I claim this vector field satisfies a particularly nice linear ode. Let's derive that right now. Okay, so just for ease of notation, let j of T be D Sigma DS0T.
01:01:03.327 - 01:02:51.495, Speaker A: So J is a vector field along gamma gamma V. Okay? So the claim is j satisfies a linear ode. We're going to find out what that ode is, and that's going to be the Jacobi equation, and then we're going to look at all solutions to that equation. So first of all, since the σ dt is the velocity vector field of the geodesic, T goes to X t, X p t v of S, which is sigma of st, we have dt of d sigma, DT equals zero, right? This just says zero acceleration, right? Let's just review what we're saying here. For any S, V of S is some vector in tpm, and the map that sends T to x p T of that vector is a geodesic. And therefore if I, if I look at its velocity vector field and then I covariantly differentiate its velocity vector field along the direction of that curve, I'm going to get zero. Okay, now what we usually do in geometry, whatever you have, you just differentiate everything in sight, right? And see what happens.
01:02:51.495 - 01:03:29.773, Speaker A: So this guy is identically 0 for all t and S. So that means if I differentiate, again, covariant differentiation is linear. So it's going to take, it's going to take zero to zero. So I just differentiated that in the S direction. But now by the symmetry lemma, which was from several lectures ago when we were doing the Gauss lemma, I can. Oops, what am I doing? No, not yet. By the lemma from earlier today.
01:03:29.773 - 01:04:20.631, Speaker A: Lemma from earlier today, I can write this as dt of DS D sigma dt plus r of D sigma DS D sigma dt applied to d sigma dt. Okay, so the capital V in that lemma from earlier today, I'm using D sigma dt as V. That's a vector field along the surface. So I was able to swap these guys at the cost of this curvature term. Now I use the symmetry lemma, which says that here I can swap the T and the S. That's what. Exactly what the symmetry lemma says.
01:04:20.631 - 01:05:09.313, Speaker A: So this is dt of dt of d sigma ds plus r of d sigma ds d sigma dt, d sigma dt. And now let s equal 0. This is. This is true for all s and T for all s t in the domain of this parameterized surface. So Let s be 0J of t is d sigma ds at 0t, and gamma prime of t is d sigma dt at 0t. And I was calling it gamma sub v there. Let's call it gamma.
01:05:09.313 - 01:05:34.905, Speaker A: That's the geodesic. Okay, so when I plug in S equals zero here, what do I get? I get dt of dt of j. Because when I plug in S equals zero. This is j plus r of. Again, this is a J gamma prime. Gamma prime equals zero. Okay, this is called the Jacobi equation.
01:05:34.905 - 01:07:16.973, Speaker A: See, what we've done is we started with this particularly nice parameterized surface which is sweeping out these geodesics. And we saw that when you look at the sigma DS for that parameterized surface, and then you set s equal to zero, you get a vector field along the geodesic which satisfies this ode. So that's the motivation for studying this equation. So definition. Let gamma from 0a, let's say to MB, a geodesic on M. A vector field j along the geodesic Gamma is called the Jacobi field if it satisfies the Jacobi equation, Jacobi equation, and again, let's write it out again. This is dt squared j plus r of j Gamma prime.
01:07:16.973 - 01:07:42.015, Speaker A: Gamma prime equals zero for all t in. For all t in zero A. Okay, so this is a vector field along gamma, we can covariantly differentiate it. Once in the direction of gamma, we get another vector field along gamma. We can do it again. That's a vector field along gamma. And again, what is this? This is a vector field along gamma such that when you Evaluate it at T.
01:07:42.015 - 01:08:01.755, Speaker A: It's R of J of T, gamma prime of T applied to gamma prime of t. And that's fine because R is a tensor. These are all tangent vectors at gamma of t. And I can, I can evaluate that. Okay, that was. Now we know what the equation is. So what we want to do for the rest of today and Wednesday is to say what can we.
01:08:01.755 - 01:09:00.943, Speaker A: What can we say about that ode? Well, first of all, it's not clear yet that it's actually a linear ode. It almost is clear. But we're using covariant differentiation and we're, We've got this curvature tensor. Let's actually verify in what sense this is really a linear ode second order. So let's see that the Jacobi equation is a linear second order ODE for J. So the metric is fixed and the geodesic is fixed, right? And we're looking for vector fields along that fixed geodesic which satisfy this equation. So let's let P Be gamma of 0, the initial point of this geodesic.
01:09:00.943 - 01:10:16.043, Speaker A: And let E1 up to en be an orthonormal basis of TPM by parallel transport along gamma, which is an isometry. All parallel transport is an isometry. We get for all t in 0a, an orthonormal basis e 1 of t up to en of t for T Gamma T of M such that DT of EI is 0 on 0a. And that's because we obtain these EIs by parallel transport right along gamma. That means that if you covariantly differentiate them in the direction of gamma, you get zero. So the picture, we have our curve gamma, which is a geodesic. I haven't used that.
01:10:16.043 - 01:10:47.825, Speaker A: It's a geodesic. This is the point P. We start with some orthonormal basis at P and we. And for every time T gamma of T, we're going to have E1 of T, 2 of T, et cetera. And they're going to be, they're going to be orthonormal for all T. Because parallel transport is an isometry. We proved that long ago.
01:10:47.825 - 01:11:41.945, Speaker A: Okay, so hence for any vector field j along gamma, we can write J of T is the sum From I equals 1 up to n F I of T E I of T. Right? Because for every, for every time T, this is a tangent vector to M at gamma of T. That's what it means to be a vector field along gamma. And that's a basis. The E1 of T up to En of T is a Basis for t gamma T of M. So there's going to be some coefficients here which are going to be smooth functions. If J is a smooth vector field, right? And if J is just sort of piecewise smooth, then those will be just piecewise smooth.
01:11:41.945 - 01:12:40.955, Speaker A: Am I doing like seven minutes? Okay, so let's compute dt of J. I have to covariantly differentiate that's linear. So the sum will come through. And then if I covariate differentiate a function times a vector field, I have to take the time derivative of that function times the vector field plus zero. And this is because dt of vi is zero. Right? The next term here would have been fi times dt of vi, but those are zero right here. So when I covariantly differentiate, if I use a frame along gamma which is obtained by parallel transport, then the vector field along that along that curve is just given by these n functions.
01:12:40.955 - 01:13:22.985, Speaker A: And covariantly differentiating along the curve just means taking the time derivative of these functions. So I do it again. Dt squared of J. Again it's going to be D squared fi dt squared di because the other term will be zero again. So now let's go and plug this into the Jacobi equation. What is R of j? Gamma prime gamma prime. Well, that's going to be R of J gamma prime gamma prime inner product EI times ei sum over I.
01:13:22.985 - 01:13:57.835, Speaker A: Because this is an orthonormal frame. And that's going to be sum over I R of j gamma prime gamma prime ei, ei. And that's going to be I. Put J is fk. This is F k E K sum over k. So this becomes the sum over I and K. The Fs come out R of EK gamma prime gamma prime EI, EI.
01:13:57.835 - 01:14:29.005, Speaker A: So this thing here only depends on the frame I chose and the geodesic. It doesn't depend on j. So let's call this aki. Okay, this is a function of T. Maybe I shouldn't call it a. Let's call it BKI. It's a function from 0a to R.
01:14:29.005 - 01:15:07.227, Speaker A: And it's as smooth as the gamma as gamma. Gamma is a geodesic, so this is smooth. Okay, now let's put together the Jacobi equation. And this will probably be a good place to stop. Maybe. So dt squared j plus R of j gamma prime gamma prime equals zero is equivalent to. I have this expression for dt squared J in terms of the eis.
01:15:07.227 - 01:16:16.905, Speaker A: And I have this expression for R of J gamma prime gamma prime. So this is going to be D squared Fi dt squared plus the sum over K FK bki all of this times EI sum over I equals zero. Okay, but again, these are linearly independent. So the only way this is going to be zero is we need this as equal to zero for all I. And the B's are known information that comes from the geodesic, which is fixed. So this is a system of n second order linear odes for f1 up to fn. Okay? And we know from the theory of od's because it's linear, as long as we prescribe the initial values of the Fs and their first derivatives, there's going to exist a unique smooth solution.
01:16:16.905 - 01:17:22.545, Speaker A: Let me write that down. So, from the Ode theorem, if we prescribe initial conditions, which are the fis at time 0 and the DFIDT's at time 0, then there exists a unique smooth solution. Okay, so let's translate back to getting rid of the of the local frame. What does it mean? What's this equivalent to? If you know all of these, that's the same as prescribing j at time 0. Right? Because the fi is at time 0, we've chosen the frame at the first at p. So prescribing the fi's at time 0 is the same as prescribing j at time 0. And this is the same as prescribing dtj at time 0.
01:17:22.545 - 01:18:42.801, Speaker A: So to summarize the Jacobi equation, so let gamma be a geodesic. The Jacobi equation is for a vector field J along gamma is dt squared j plus R of j gamma prime. Gamma prime equals zero. And there exists a unique smooth solution given J of 0 and DTJ at 0, which is in TPM. P is gamma of 0. Okay? So if we prescribe the initial value of the Jacobi field and the initial value of its first covariant derivative in the direction of the geodesic gamma, then there will be a solution and there'll be only one, and it's smooth. Okay, so that is probably where we should.
01:18:42.801 - 01:19:29.735, Speaker A: Oh well, let me just make one more column and we'll stop there. So also from ode theory, you can show basically the solutions. It's a linear equation. So if you have a linear combination of solutions, you get a solution and you get a 2N dimensional vector space of solutions. And that space can be, for example, described by saying I can take the solution where all of these are zero except one of them and all of these are zero. That's going to give me n different solutions there and I get n more solutions by taking all of those to be zero and all of these but one to be zero. And you can show, and it's easy to see that any solution is going to be a linear combination of one of those two n basic solutions, and those are independent.
01:19:29.735 - 01:21:21.759, Speaker A: So by the od, by general ode theory, since the equation is linear, the space of Jacobi fields along gamma has dimension 2n. So I, e, say we take j to be such that ji of 0 is ei and dt j is 0, and you let take ki be such that k I of 0 is 0 and dtk I at 0 is ei. Then j 1 up to jn and k 1 up to k n are a basis for the space of Jacobi fields for the space Jacobi fields along Gamma, right? It's easy to see that any, any Jacobi field will be a linear combination of those. And then you just have to argue that those are linearly independent. And that's not hard to show. Okay, so next time, well, I'm out of time. Okay, so we'll, we'll see what these are good for.
01:21:21.759 - 01:21:59.085, Speaker A: We'll see what is the geometric meaning of Jacobi fields. And we've already hinted with that long motivational discussion that they should be related to spreading of geodesics and they should also be related to singularities of the exponential map, critical points of the exponential map. So we're going to do all of those things. Probably. I can finish chapter three if I don't finish it on Wednesday. I'm just going to need about 20 minutes after reading week to finish it and then we're going to move on to chapter six.
