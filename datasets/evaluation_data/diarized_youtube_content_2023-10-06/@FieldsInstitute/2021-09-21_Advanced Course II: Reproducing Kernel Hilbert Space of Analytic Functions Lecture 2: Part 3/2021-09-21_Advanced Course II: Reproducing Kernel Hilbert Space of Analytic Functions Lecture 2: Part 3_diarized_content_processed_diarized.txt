00:00:00.360 - 00:01:28.644, Speaker A: And it's the relation between a space age and its closed subspaces. Indeed, in examples we saw by now at least four examples, this has been seen without explicitly being mentioned. So, to have an abstract framework with what I want to explain, let age be an Rkhs and m in edge a closed subspace when m it's a closed subspace. So everybody knows that by itself it's a Hilbert space. But here it's more the evaluation priori are continuous on m two, because they are continuous on H. We know that when we have a function and we restrict the domain of the definition, if the initial function is continuous, then the restriction is continuous too. We know that to distinguish between the two cases, when I write ex, I want to say ex in the space h or in the space m.
00:01:28.644 - 00:02:19.786, Speaker A: Let me add the h up here. So this is evaluation from h to r or to c in the complex case, and we map f into f of x for every f in H. But same token, we can have exactly from M to C. And in this case, even though the formula is the same, but for every f is in m. So, but when f is in M. Now use the fact that m is in h two. So if with respect to bigger space H, our evaluation is continuous.
00:02:19.786 - 00:03:27.196, Speaker A: This means that either we write it this way or write it f of x. This is less than or equal a constant. We know what is the constant, but I mean call it c. Then the normal f in h for any f is in H. In particular, if f is in M, then a priori f is in h two. So we can say evaluation xm of f, which is the same as f of x. Here we can also say it's h of f for this f, and this is less than or equal to c normal f in h by this formula.
00:03:27.196 - 00:04:16.042, Speaker A: And note that normal f in h is the same as normal f in M because it's a closed subspace and they use the same norm. Now if I compare this with this immediately implies that exm is also continuous and also its norm is less. I mean the norm. That's not my point now, but we can say that this less than or equal to c, which was the norm of ex H. That's not my main concern. My main concern is that the it's automatically continuous. Therefore we have reproducing kernels.
00:04:16.042 - 00:05:22.272, Speaker A: But what in this space, if we are in h, then ex h f f of x is the inner product of f with a kernel in space h. And to distinguish with the next case, as we did an h here, let me add an h here. This is very common notation. And similarly, if we are in m, this is true for f in s and x mof m. Yeah, I can write m or h. Both are acceptable because we are in the same space. It's closed subspace.
00:05:22.272 - 00:06:52.428, Speaker A: It really doesn't matter if I put h or m here. The reason I emphasize for h or m is that when we go to the study of Dobron Grovniak spaces, they are not the same and we need to be careful over there. So, but for the time being they are the same. And the question is that what is the relation between the two kernel? Before giving the answer, let me convince you that you have seen this before. For example, in example three, you consider the space h consisting of the function f such that f is absolutely continuous, and just f prime is in L two. In example either one or two, both of them are good. For example, in example one I call it m.
00:06:52.428 - 00:07:48.430, Speaker A: Now, before I call it h, but with this notation I call it m. You can see that f such that f is absolutely continuous. F prime is in L two, and f of zero is equal to f of one equal to zero. So it is clear that m is a closed subspace of h. You can even prove it directly. And in both cases, without raising this general question, I obtain the kernel of this and the kernel of this independently. And even, I mean, when you look at the formula you have, you see the formula for both kernels, even though if you look at the formula, there is no clue.
00:07:48.430 - 00:08:35.624, Speaker A: Y are like that. I think I have one of them at least here. This is kernel for the third one. That's the kernel for h, no restriction. T plus one, x plus one is, is this kernel. And so with my new notation I can say that this is kxh. And when I go back to the first example, it's not mentioned.
00:08:35.624 - 00:09:56.022, Speaker A: Ah, here it is. And with my new notation, this is Kxm. So looking at these formulas, there is absolutely no clue what is the relation between kxh and kxm. And the answer I give, even though it's very short proof, is kind of trivial. It will not be useful if we do not have a formula for the projection. Here is the answer. Since M is a closed subspace of h, we can talk about Pm orthogonal projection onto M, projection of h onto m, and this is the clue to find the answer.
00:09:56.022 - 00:11:08.294, Speaker A: Here is the answer theorem. Under the above conditions, Kxm is the projection onto m of k xh. It looks a simple formula and it is indeed a simple formula, but it's a bit misleading, because as I said, this will be a good formula to use, successful formula, if we know what Pm is. But if we just use the abstract point of view, it doesn't help us that much. The proof is extremely easy. Proof is extremely easy. We use the fact that for any f in m, the projection is identity, Pm is equal to f.
00:11:08.294 - 00:11:29.954, Speaker A: And then. Now let us start. Let f being in M. Therefore f is also in h. It's better to say fix x. X is fixed. And then here is the trick.
00:11:29.954 - 00:12:36.364, Speaker A: We know that f of x is equal to the inner product of f with the kernel in H. That's true, because it's in h. And that's the definition of kernel. The trick here is that now, instead of f, I write pmf, because pmf is equal to f. And to emphasize we are in H. We treat this as an element of h, this as an element of h. And now I use the fact that Pm is equal to Pm.
00:12:36.364 - 00:13:13.430, Speaker A: And orthogonal projection is self adjoint. So I put m on the other component. So Fpmkh. And here, even though I can write h, but let me write m. Now we are in the space m, because in the previous case, here and here, both elements were in h, even though one of them was indeed m. But both were in h. That's correct.
00:13:13.430 - 00:14:06.574, Speaker A: Now, with moving p m on the other side, both are in m. That is why I can write the inner product in M. And that's the end of the story, because f of x is also equal to f, kx m in m, and the kernel is unique. You remember the Rees representation theorem. It tells us that the representing element for a functional is unique. Or I put it this way, kernel is unique. So it shows that kx m is nothing but projection of kxh.
00:14:06.574 - 00:15:05.014, Speaker A: That's the end of the story. So even though I haven't obtained here, even though I don't have a formula for PM, it can be obtained. But I haven't obtained yet. But I directly obtain kxh here and kxm here. And the theorem tells us that if I project kh, I obtain the formula. A good exercise is to find this. First, find an explicit formula for PM in this setting, and then using this formula, show that, or verify that the projection of kh is really km.
00:15:05.014 - 00:16:06.404, Speaker A: That's, that's a very good exercise to do another thing. Let me see. Okay, there is another use for the projection, which in some applications we apply this result trm. We saw in the previous two examples that an explicit formula for PM is not available. But indeed we can use the theorem which I wanted to state to obtain a formula for PM. If we know the kernels. And here is a theorem.
00:16:06.404 - 00:17:07.864, Speaker A: H is an rkhs, m is in h is a closed subspace. Then PM is well defined from h to h. Orthogonal projection. But note that the range of Pm is equal to m. We project each element into m and the question is what is the formula? And here is the formula. Then for each f is in h. Here is the formula for the projection p m of f at the point x is f.
00:17:07.864 - 00:17:55.564, Speaker A: Inner product with kxm. So compare this theorem with the previous one. You need to have a piece of information if you have the projection in advance. If PM is given and you know it, then you can use this formula to obtain a relation between the kernels. From kernel of Kh you can find the kernel km. If not, you have the kernels like the one we did before. You can use this formula to obtain a formula for Pm.
00:17:55.564 - 00:19:25.584, Speaker A: So any piece of information from one side gives you the other side. And as in the previous case, the proof is easy too, because we know that Pm of f is in M. Being in M means that. Yeah, being in M means that pmf at the point x is equal to pmf and inner product with the kernel, but the kernel on m. I can put m or h here, but let's stick to m. And now use again the fact that m is self adjoint. I can write this as f, take pm on the other side and note that now I have to write h because this time it is true that the second component stays in M, but the first one is in h in general.
00:19:25.584 - 00:20:04.872, Speaker A: But even at the beginning here, if you write h, that is okay. In this setting there is no problem. But there are some settings that we need to be careful about M and H. And we are almost done because KXM is in M and its projection is the same. So that's the end of the story. Proof. Proof of both cases are easy.
00:20:04.872 - 00:20:42.358, Speaker A: I mean, if you know what to do. But the only thing is that which type of information we have. There is something in chat over line one, Sheldon taking m two b as an example. Yes, yes. Yeah, absolutely. As Sheldon said, in that case, I mean, comparing example two and example three, we have an explicit formula for the orthogonal projection. So in this case we have pm.
00:20:42.358 - 00:21:33.064, Speaker A: And when we have pm, we can use this formula above. We can use this formula to obtain one kernel or another. So as I said, it depends what type of information we have. If, as Sheldon mentioned, we can easily distinguish our projection, then the life is easy and we obtain the things which are missing. So well, if this ends this section two, there's the relation between kernel of a space and its subspaces. And you can do the same exercise between. I mean, you have four examples and at least three of them are nested.
00:21:33.064 - 00:22:36.084, Speaker A: Example four is different, but example one, two and three, they are nested. So you have three projections from one to two to two to three, and from one to three in the order that is appropriate. And in each case you can find the projection and the relation between the kernels. Our next section is a long one because it's about another series of examples reproducing kernel Hilbert spaces on the open unit. This I just start the beginning band but leave the rest for the next week. Here is the idea. The open unit disk hole d is the space of all holomorphic function on the open unit disk.
00:22:36.084 - 00:23:11.792, Speaker A: This is really huge space. And that is why we go into subspaces of this. And subspaces means put some restriction. And the easiest way for us to put restriction is to put restriction on TaYLor coefficients. Since f is defined on d. EaCh f has the representation sum n from zero to infinity. An, Zn and Ans are Taylor coefficient of f.
00:23:11.792 - 00:24:01.824, Speaker A: You know, at least two formulas to find them either by derivative or by integral cauchy integral. But I mean, at the end of the day you have a formula for n. And now the spaces we define are of this form, I call it h. And then I will age to some specific examples. All functions f such that the sum n from zero up to infinity, absolute value of an squared with some weights here and call it omega n is finite. Omega N is something strictly positive. And we want to study at least three spaces of this type.
00:24:01.824 - 00:25:09.624, Speaker A: If omega n is equal to one, we obtain the classical Hardy space, which we denote by h two. If omega n is equal to n plus one, we obtain Dirichlet space. And for the notation is d. And finally, if omega n is one over n plus one, that's the classical Bergman space. And in fact the space with which the whole theory of rkhs started. In most books when they write a two, but sometimes they write b 22 for Bergman space, I mean, whatever the notation is, these are our main concerns. We consider this spaces their kernel.
00:25:09.624 - 00:26:03.384, Speaker A: Moreover, in the second case we will go a bit further and study D zeta, the local derivative spaces, and more generally D muted Dirichlet spaces. I will give the definition of these two cases too. There is a reason we do that and the reason will be more transparent. Indeed, at the end of our course that why we need these spaces. And in the case of h two, we will go to subspaces, I mean the section just we had and the subspaces of this. If we consider closed subspaces, this gives us K theta, which are called model spaces. That's one important category that we will consider.
00:26:03.384 - 00:26:51.940, Speaker A: And if we do not consider closeness, I mean just arbitrary vector subspace with some properties. It leads us to Hb spaces, which are called Dobranche Wozniak spaces. We will not be able to go into the detail of all of this. And for each topic there is indeed a section in our focus program for Dirichlet spaces is already done the same for Bergmann spaces, but for dobronzhgrovniak spaces it's coming. For spaces is down too. You can look at the videos that are available on the fields institute website. But we will touch again the basic formula that we need in this case.
00:26:51.940 - 00:27:34.060, Speaker A: And going from classical setting to some more examples, and we give the definitions, obtain the kernels whenever possible. I emphasize this, it's not always possible. It's interesting. We know that kernel exists, but we do not have a formula for that in some rkhs. So that's why I give these examples. This is indeed the case in DMU, we know that they are Rkhs, but we do not have a formula for the kernel. And on contrary, in HBsP we know that they are Rkhs.
00:27:34.060 - 00:28:46.094, Speaker A: We have a formula for kernel, but we don't have formula for the norm, an explicit formula. So these are interesting examples that a bit more involved, but little by little we will get there. That's my plan for next week. And after having this plan, I mean, I hope I have time also to cover a very important example which is called PW payload winner space, which has applications in signal processing. And it's not an easy example to describe because for that we need to know a little bit about the theory of entire functions of exponential type and also about plancheural theory for l two functions. It's a good part of the harmonic analysis course that we give at the graduate level. But still, I will try to describe what PW space is, because it's a very important space with many, many applications in mathematics and in engineering.
00:28:46.094 - 00:29:23.244, Speaker A: So this is my plan for, I mean, at least one week to come, and hopefully by the end of next week examples are over and we can go into more abstract theory of rkhs. Well, thank you very much. I am sorry about the technical difficulties we faced. I try to resolve this by next week and I can lecture from my own office. If there is any question, I'm available. If not, thank everybody.
