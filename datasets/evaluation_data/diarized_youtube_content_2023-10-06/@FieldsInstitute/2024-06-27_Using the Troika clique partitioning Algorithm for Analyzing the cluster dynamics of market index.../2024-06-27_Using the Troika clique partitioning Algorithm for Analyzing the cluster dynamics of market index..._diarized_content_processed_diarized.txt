00:00:38.960 - 00:01:46.750, Speaker A: Join clusters using a clustering algorithm to maximize the sum of all edge weights. So for instance, given an input network on the top right, let's say after solving using a clique partitioning algorithm, the output should look something like this, with five clear disjoint clusters. And the cleat partitioning problem, also known as the CP problem, is NP hard, meaning that finding an exact solution is computationally infeasible for larger graphs. And it's also a fundamental problem for network analysis, like with applications across various fields, such as like biology, social networks, and also finance. Let me give you an example here. So how would you partition this graph? I'd say so that your goal here is to group nodes into clusters, into disjoint clusters, where to maximize the sum of all edge weights, the red edge represent. I guess it's kind of hard to see, but yeah, the red edges represent negative one, and then there's also blue edges, uh, which represent one.
00:01:46.750 - 00:02:35.910, Speaker A: And I'd say it's probably pretty hard to do it by the I. But if we simply just group all the nodes into just one cluster, it's actually going to equal to an objective value of four, which isn't the best solution we can find here. Now, this is the output. After running the cleat partitioning algorithm, we can see four disjoint clusters, which results in an objective value of 16 instead, which is much better than what we had previously. Before I introduce the mathematical formulation for the problem, I'll quickly go over the concept of integer programming, also referred to as IP, using a simple financial problem. So, suppose we have an objective to maximize the returns for a portfolio with only stocks and bonds. There's two decisions we need to make.
00:02:35.910 - 00:03:01.856, Speaker A: How much we want to invest in stocks, which is defined as s, and how much to invest in bonds, which define as bhdem. And here's the integer programming formulation for the problem. The returns was defined as 1.2s plus one one b. And there are three constraints to consider. So first, the total investment limit, meaning that you can't invest more than $100,000. 2nd, non negativity and also diversification.
00:03:01.856 - 00:04:01.834, Speaker A: And lastly, integer constraint, meaning that the investments must be whole numbers. And this is what distinguishes integer programming from linear programming. There are many possible solutions that satisfy the constraints. However, the optimal solution is to just invest $20,000 in bonds and $80,000 in stocks, which results in 116,000 in return. This is just to maximize the return given the constraints. Now I'll briefly go over the integer programming formulation for the clique partitioning problem, we define a variable xi j, for example, to be a binary variable zero if nodes I and j are in the same cluster, and one if they are not, and the objective function here is to maximize the sum of the weight of all edges within the clusters represented by RP. The constraints here look pretty complicated, I would say.
00:04:01.834 - 00:05:13.730, Speaker A: However, they're just there for sort of consistency within the clusters to ensure that, like for example, if two nodes share a cluster and a third node doesn't and a third node doesn't, then they must be in different cluster and differently. If, let's say, two nodes shares the same cluster and a third node also share the cluster, then they all must be mathematically represented by these inequalities. So before I introduce the Troika algorithm, it's very important to understand the existing algorithms as well. For the CP problem, these can be broadly categorized into exact and heuristic methods. So exact methods can include solving the integer programming formulation using commercial solver like Guarobi to find exact solution, but it can be computationally expensive for larger graphs. There's also an exact branch and count method that explores facet inequalities introduced in 2019, but it can only be applied to a subset of the problems. On the other hand, there's heuristic methods which are designed to provide good enough solution within a reasonable amount of time.
00:05:13.730 - 00:06:13.964, Speaker A: So these include the combo algorithm, a well known algorithm for modularity maximization, but it can also be applied to the cleave partitioning problem. There's also the merge defy mimetic clique partitioning algorithm, which is a hybrid algorithm introduced in 2021, which also seemed to provide very good results. Despite the amazing progress they made, there is still optimality and also efficiency limitations which schroika is trying to address. Schroika is a Python algorithm inspired by the Bayer modularity maximization algorithm. The name troika means three horse carriage, and which reflects the algorithm's approach of a no triple branch and cut method. The goal here is to achieve a close to optimal solution within a reasonable amount of time for small to mid sized networks. We start off by branching on a set of three nodes to explore the feasible space based on logical disjunction.
00:06:13.964 - 00:07:26.550, Speaker A: The algorithm then uses linear programming relaxation, which is just integer programming without the integer constraint, to establish the upper bounds, and uses a heuristic search, the combo algorithm I mentioned previously as the lower bound. This approach helps in efficiently cutting down the feasible space as we go. We can then leverage the CP problem solution to interpret stock return correlations within market index in this project, specifically the S and P 500 index. So the cleave partitioning problem maximizes the width and cluster edge weights over all possible no partitions, and therefore can successfully identify groups of positively correlated stocks. This analysis uses sign networks, each generated from a pruned correlation matrix on the constituents after applying the Fisher transformation and also setting a threshold of two standard deviation. In these networks, the constituents are represented as nodes and the correlations between them as edges. And by utilizing the partitions returned by troika, we can evaluate the changes in the correlation with an SMP 500 over the last five years.
00:07:26.550 - 00:08:42.000, Speaker A: So here are the visualized results, spanning from 2019 to 2023, with different colors representing different clusters. And in 2019 specifically, we observed one major cluster, which is in blue, comprising of 330 stocks, and a smaller cluster in orange if it shows correctly. And then in 2020, there is a significant shift to one dominant cluster of 471 stocks, likely due to the result of the pandemic. And then in 2021, the structure had reverted to sort of like a 2019 like pattern, but there's actually eleven clusters scattered around, which is one big cluster being the blue one. And then finally, in 2022, as well as in 2023, the market sort of shifted to sort of like two primary clusters, showing a relatively balanced market, especially in 2023. So, to understand the inner workings of Troika, let's break down the process step by step. So first, Troika begins by solving the linear programming relaxation of the CP problem for a, let's say a given network.
00:08:42.000 - 00:09:34.466, Speaker A: This gives us an upper bound on the optimal solution, and then from the LP solution, we select a node triple, let's say node I, j and k, which do not satisfy the triangular constraints as shown previously. Then next we create two branches. So in the left branch, we extend the LP model by adding additional constraint. The constraint enforces that node I, j and k to belong in the same cluster and in the right branch, on the other hand, we extend the LP model with another constraint. This constraint, on the other hand, enforces that at least two out of the three nodes have to be in different cluster. So for each branch, we again solve the respective extended LP models. And if the solution is still fractional, or the upper bound and the lower bound haven't converged yet, we keep going and apply the same branching strategy as we go.
00:09:34.466 - 00:10:53.350, Speaker A: So the process just continues iteratively, and then adding constraint, creating new branches until all variables in the LP model either become integer or the upper bound lower bound converge, thus resulting in a feasible or basically near optimal solution. So in order to achieve greater efficiency and also scalability, I implemented a few techniques to speed up the algorithm. We'll go more in depth today, specifically on this graph pre processing. So charcute first reduces the input network by applying preprocessing step to simplify the graph and also reduce the number of variables and constraints. For example, in the graph on the left here, the pendant node four is connected to its sole neighbor node zero, and after reduction, node four and its edges are replaced by self loop at node zero that has the same weight as the original edge right there. And similarly the graph on the left here, there's two distinct positive cliques, one comprising of zero, one and two, and one comprising of three, four and five on the right. And after post processing, these respective positive cliques are then encapsulated into two nodes, each with a weighted self loop.
00:10:53.350 - 00:11:39.050, Speaker A: To evaluate the performance of troika, I conducted experiments using three distinct type of data sets, Abr graphs, which stands for aggregation of binary relations MCF graphs, which stands for manufacturing cell formation problems, and burebosi algorithm graphs to model real network structures. These instances are also used across the literature. For the cleave partitioning problem. We evaluated each method based on solution quality and computational time. All experiments were run using Python and the guerrobi solver. For comparison, the other two methods compared our guerrobe IP formulation and the combo heuristic algorithm. Because of the readiness in the Python environment.
00:11:39.050 - 00:12:36.146, Speaker A: Here are bar graphs that show the logarithmic solve time of each method on the datasets on the right. On the legend, we can see that RP stands for groupie IP method, and then which is in blue, troika is in red, and combo is in green. But do note that across all three data sets in terms of solve time, combo outperformed the other two methods by a lot, which is actually expected because of its heuristic nature. So for Abr graph specifically, Troika is actually remarkably efficient solving problems on average 5.8 times faster than the Garobi solver. And in the case of MCF graphs, Trico is also able to outperform grobi IP formulation and twelve out of the 31 networks tested, while also achieving global optimality. And when applied to biobasi album graphs, Troika solves problem on average 14.8
00:12:36.146 - 00:13:50.262, Speaker A: times faster than grovy. This efficiency allows troika to handle more complex and also larger real world networks, uh, relative to the exact integer programming formulation. And here are the scatter plots that compare the objective value retrieved, uh, from the partitions of the three different methods, uh, with the solutions requ returned by RP being in blue. Uh, the global optimal solution in the case of MCF graphs, uh, Schroika in red consistently achieves higher objective values compared to combo. The partitions return are quite close to best possible and also outperforming, specifically in the case of CN 97 test case. Here, combo did not manage to reach a solution very close to the optimal at all, but Troika was able to achieve a solution much closer to the global optimal, which shows its effectiveness in this specific context. And when applied to vertebracy algo graphs, Tricia outperforms the combo algorithm again on average being 99% optimal versus 97.5%
00:13:50.262 - 00:14:53.026, Speaker A: optimal. In summary, troika consistently retrieves close to optimal partition with higher solution quality than combo, while achieving much faster solve time than the exact integer programming formulation solved using guerrobi, being 5.827 and 14.8 times faster than gurobi API formulation on the three data sets, and also 2.2% more optimal than combo on the MCF data set and Barabasi alba graphs. Overall, Trica demonstrates the balance of efficiency and also optimality as shown in the experiments. And however, do consider that when maximum optimality is required, I recommend using the exact method like solving it using groby commercial solver, and in the case where speed is extremely important, then I recommend using heuristic algorithm like combo, which is more suitable in that case.
00:14:53.026 - 00:15:08.680, Speaker A: But when both speed and accuracy matters, shortcut can be an ideal algorithm for the clique partitioning problem, specifically for small to medium sized network of up to around 5000 edges. And this concludes my presentation. Thank you for listening.
00:15:09.340 - 00:15:15.240, Speaker B: Thank you. Thank you Boris. We have time for one or two questions.
00:15:18.340 - 00:15:38.820, Speaker C: Thanks for the talk. You may have mentioned it, but in your troika technique. So Karobi, we know the solution is optimal because they use branch and bound or some variation of branch and bound in your technique, how do you guarantee that you got optimality and not just a good solution?
00:15:40.360 - 00:16:20.530, Speaker A: In a case where optimality matters the most, we actually look for the global optimality even when we're sort of like going through the branch and cut. So we don't cut off any non like feasible space, we only cut out the non feasible space as we go. There's many techniques I implemented here, like these are all techniques to ensure that the feasible space is still there. So then, like when we're looking, when we're branching on like subproblems and different constraints, we make sure that the optimal, we can achieve, still achieve the global optimality in the case when time allows. Yeah.
00:16:29.390 - 00:16:37.822, Speaker D: Just a question on the portfolio exercise. So if I understand correctly, you do a correlation matrix, right? So you.
00:16:37.846 - 00:16:39.130, Speaker A: Oh yeah, the.
00:16:40.430 - 00:17:48.370, Speaker D: And yeah, essentially the key point is that you observe these big blobs. This is, I think is relatively well known and is related to the fact that, let's say 50% of the variability of returns is due to a common factor, right? So there is a one market factor that essentially explains roughly 50% of the variability. So what people typically do, I mean, this is suggestion actually is to do the following. So you do a principal component analysis and you project on the eigenspace, orthogonal to the first eigenvector, okay? So essentially you remove the first factor and then you do the click partitioning or the partition. Then you will see much, much nicer groups and much well separated. While if you do directly on the road, on the raw return, so not in excess with respect to the market factor, then you tend to have this very compact structure, which is not very informative, because there is this market factor that explains a lot of variability. So the suggestion is to repeat this analysis, but, you know, not on the return, but on the residual of the return when you regress against the first factor.
00:17:48.370 - 00:17:49.610, Speaker D: That's.
00:17:50.550 - 00:17:52.410, Speaker A: Thank you, thank you for the recommendation.
00:17:54.790 - 00:18:06.090, Speaker D: Then you will see sectors, you will see financial sector, for example, economic sector, that's. There are a lot of work about that using different methods. You will see energy, I don't know, financial and so on and so forth.
00:18:11.200 - 00:18:29.664, Speaker B: Are there any other questions or comments? Okay, so we are pretty much on time and we will meet in 30 minutes. We go for the coffee break and we will meet back in 30 minutes at 340. Thank you again, boys.
00:18:29.752 - 00:18:30.040, Speaker A: Thank you.
