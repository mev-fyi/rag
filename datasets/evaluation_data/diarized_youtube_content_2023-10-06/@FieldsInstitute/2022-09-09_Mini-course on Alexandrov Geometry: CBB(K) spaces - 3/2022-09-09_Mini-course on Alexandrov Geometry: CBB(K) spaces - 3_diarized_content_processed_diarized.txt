00:00:00.880 - 00:00:12.634, Speaker A: Oh, yeah, good. Okay. All right, so let's continue with lectures of Anton petroleum on CBB case. Anton, please.
00:00:13.134 - 00:01:18.994, Speaker B: Thank you. So today I will talk about gradient flow, and first let me say a couple of words about prehistory of this technique in geometry. Oops, what's happened? Okay, so prehistory of this technique in geometry. And I think everything started from Vladimir Sherifudinov, the so called Sherfudinov retraction. Just a second. What's happening? Okay, copy paste doesn't work. What? I don't know.
00:01:18.994 - 00:01:51.774, Speaker B: It worked. Just. Oh, yeah. What's happening? Okay, let me try to start over. Okay, maybe now it will work. No, it doesn't. Okay.
00:01:51.774 - 00:02:20.598, Speaker B: Okay, so let me try to just show a hoodie. Enough. Retraction. Retraction. And it started from 1970. So this idea is very, I mean, powerful and has many applications in many places. So before.
00:02:20.598 - 00:03:19.070, Speaker B: Beforehand, let me remind that there is a closest. What's happening, Anton?
00:03:19.102 - 00:03:26.890, Speaker A: Are you speaking? Because I don't think we. Well, sorry.
00:03:26.922 - 00:03:30.026, Speaker B: Yes, I was speaking, but should I start over?
00:03:30.170 - 00:03:32.494, Speaker A: We didn't hear your sound.
00:03:35.194 - 00:04:13.572, Speaker B: I don't know what's happening. It seems like I made some comments and they start to execute. So I will start over. So I'm talking about sheriff wooden infrastructure. And it's a generalization of certain construction that's called closest point projection. Point projection, which is useful in Hilbert space and euclidean space, of course. And it works perfectly well in cad zero space, by the way.
00:04:13.572 - 00:05:28.254, Speaker B: And the idea is that if you have a convex body, for any point, you can find the closest point, right? I'm sure everybody saw this in action, right? It's a useful construction. Now, what to do if you don't have. If your space is not as nice as cat zero or as euclidean space. So there is a generalization which requires a bit more. Instead of one convex body, you can see the one parameter of convex bodies, right? So you nested nested family of convex body in your space, and then you can apply closest point projection, despite it doesn't have nice properties, as nice property as before, because your space, ambient space is not as nice and iterate it. But then you make partition better, iterate again and pass to limit. And the limit map is cultural Fujinov retraction.
00:05:28.254 - 00:06:03.514, Speaker B: It's a family of maps. Phi t, that maps wall space to the convex body with parameter t. So zero, k one, two, and so on. K one, okay? And it has nice properties. It's most important that as well as the closest point projection. This is a short map. Short map is another way to say that it's one Lipschitz map.
00:06:03.514 - 00:07:14.060, Speaker B: Okay, so, this is again 1977, done by Vladimir Shervudinov. And this was beginning of applying kind of gradient flow in geometric setting. The application Srofudinov made is he proved that in non compact manifold with non negative curvature, non negative sectional curvature, any two souls are isometric. If you know what it is. If you don't know what it is, don't worry. Okay, so, now, the next development was done by several people. So, it was like the application of gradient flow in spaces with Kouch bounded below was done by Grigory Perelman and me.
00:07:14.060 - 00:08:39.894, Speaker B: And I don't remember yours, but it was about my thesis. And at the beginning, we thought of it as a nice technical tool that simplifies the proof of existence of quasi, so called quasi geodesics in Alexander spaces. But after a while, it turned out that this tool is more like, is more powerful than actually the actual quasi geodesic for which we developed it. Okay, so, for cats spaces, the technique was developed by Jurgen Joost and Uwe Meyer pretty much the same time, little later. And after that, there was some generalization and reunification of the methods that were done by Alexander Lichak, Shin ichi Okta and Giuseppe Savara. Okay, so what's, what's the idea? All right, first of all, Alexander of space. Alexander of spaces have many semiconcave functions.
00:08:39.894 - 00:09:57.332, Speaker B: Functions. So, semiconcave means that roughly, f double prime is less or equal than some constant, which is local. For example, the meaning of this inequality I described last time, that means that if you take composition of f with unit speed geodesic and subtract c t squared over two, then this function is concave. Okay, so this is, this is just meaning of this shortcut. Okay, I forget t here to include t here. Okay, so we have plenty of concave function. In fact, distance function is semi concave in the sense, except for the.
00:09:57.332 - 00:11:06.382, Speaker B: For the origin. And for these functions, we can define gradient flow. Okay, so, before going to definition of gradient flow, I have to say a couple of words about tangent space. So, if you have, the construction is the same for lower curvature bound, for upper curvature bound, and it works for any space with sufficient amount of geodesics. You start with a point. You can see the old geodesics that start from the point. And as we know, we have well defined angle between these geodesic, the set of those geodesics denoted by geodesic space of direction at point p.
00:11:06.382 - 00:11:54.944, Speaker B: That's my point p. And it comes with angle and we know that this angle defines a metric. After that, you pass to completion of the space. Sigma is Sigma p is completion version of this one. And this called space of directions. And finally, the tangent cone at the point is the cone, euclidean cone over Sigma P. So that's the definition.
00:11:54.944 - 00:12:41.974, Speaker B: And it's a cone. The space of the tangent space of space with low curvature boundaries has non negative curvature, in the sense of Alexandrov. But at the same time, it might be not intrinsic space, so it might be not a length space. There are counter examples, so you might have to be careful with those. But of course, for nice spaces, it is right. For a nice, say spaces. This is the case.
00:12:41.974 - 00:13:34.416, Speaker B: Now, let us try to face next problem. If I have a. Let's forget about Alexander geometry for a while. Let us think that we have a function from euclidean space to ride, and we want, which is concave, and we want to generalize gradient flow. So, gradient flow is a solution of equation x dot equal gradient of f at point x. So this equation has perfect sense in smooth category, if f is smooth. But we don't want to assume this.
00:13:34.416 - 00:15:02.764, Speaker B: We want to assume that it's only concave, say semi concave or lambda concave doesn't matter, right? And if we work in euclidean space, we could, we could proceed the following way. We could smooth smooth fish, apply gradient flow flow and then pass to the limit, okay? And it produces what we want. It's producing, like you have to prove something here, right? So that the result, the limit map, limit family of maps. The solutions of limit solutions do not depend on the choice of smoothings, but it's possible to prove. And this is a working idea for euclidean space. Unfortunately, it doesn't work for Alexander space. The reason is simple, that our space, our original space, instead of euclidean space, we have a map from Alexander space to real line, right? And our space itself is very non smooth.
00:15:02.764 - 00:16:00.764, Speaker B: And we need to find a way around. And there are several approaches. So I will describe, naturally, I will describe the approach I like, right? But there is alternative approach and people from analysis like the approach better, right? So what is the idea? Right. So you say that. Let me remind what's the gradient like v is equal to gradient at point x of f. That means that vx is equal to dx f of x, right? So let us try to make sense of it. First of all, differential of the function has perfect sense in.
00:16:00.764 - 00:16:18.378, Speaker B: In the Alexander, the x of f at v, right. Have too many x. Yeah, right, right. Like. Okay, let me. Yeah, let me put p here, right. Yeah, sorry for this.
00:16:18.378 - 00:16:57.674, Speaker B: Yes, this was one x wasn't connected with the other x. Let's see. Are you good now? I mean, is it better? Yeah, yeah, yeah. Okay, so, first of all, the differential has perfect sense. We have a cone. If, since the function is semiconcave, for any geodesic passing through from the point p, I can, the function on this geodesic is concave like a semiconcave. In particular, there is derivative of this function at the beginning.
00:16:57.674 - 00:17:50.776, Speaker B: So in this derivative, it simply defines the differential at geodesic directions and can be extended naturally to all directions. So I assume that, sorry, I forget to say that I assume that f is Lipschitz. In this case, it's particularly easy to extend. Also, you can generalize it. Okay, now, scalar product, I use here scalar product, which usually associated with euclidean space. But you can still do this, right? You have a vector v, you have vector x, you can measure angle, like I call them vectors, but actually elements of tangent cone, right? So it's there. No, there is no linear structure, but still there is an angle.
00:17:50.776 - 00:19:37.004, Speaker B: And I simply define the scalar product s distance to zero from v times distance to zero from x times cosine alpha, as we learn it in school, right? Finally, there is a problem that, the bigger problem, that this equality never holds, right? Whenever I take scalar product of the type defined here, this never happens, right? So it almost never happened, right? Except for exceptional cases. So, instead of this, we exchange this inequality, to exchange this equality to an inequality great or equal. Okay? And plus, we add one more condition, is equal to DPF of v. So these two things define, is a definition of a gradient p. So it's the definition of this identity, okay, how unique is that? It is unique. Like, you can prove many things about this, right? It's uniquely defined and, right. And in fact, maybe I should say that this condition makes it unique.
00:19:37.004 - 00:20:42.950, Speaker B: But even without this condition, it works, right? So, like, if you simply. I will talk about this later. Why? This is not important. This equality is not important, is not important. But, right, you have to prove certain things, right? If the function is semi concave, then gradient uniquely defined. And after that you can consider equation, instead of writing x dot equal xf, you write, you write x plus write the right derivative of a point, right? Moving the velocity of the speed of the point at some time is equal to gradient of the function at this point. So let me give some example of like how the.
00:20:42.950 - 00:21:32.604, Speaker B: And right, now, after that, you have to take introduction to ordinary differential equation and repeat everything from the introduction like up to existence and uniqueness of solutions. And what you get that future. Future is uniquely defined. Defined. So you don't know the past. I will, I will give example, right? So for example, you can see the real line and my function is f is equal to minus absolute value of x. That's a perfect concave function.
00:21:32.604 - 00:22:43.234, Speaker B: How the gradient flow look like, if you start with negative point, it moves with unit speed to the right. If you start with positive point, it moves with unit speed to the left. If you, if you at zero, you stand at zero, right? And also it gives you example when past is not uniquely defined. If you stay at zero, you don't know what happened before, right? How long did you stay at zero? And when it turned like was it before negative or positive? Or maybe you stand at zero forever, right? So, questions, comments about this. So, once you, once you did it for Euclid, like you can think that it's a special trick to work directly with semiconcave function on euclidean space. But once you made the proof on euclidean space, it's rather straightforward to generalize it to curvature. Spaces with curvature bounded below.
00:22:43.234 - 00:23:54.044, Speaker B: Spaces with curvature bounded above. Roughly, you need existence of angles and little bit and just minus structure from, from the spaces, right? Okay, so now you can maybe. Questions? So we define gradient and we define gradient curves. And we may consider also gradient flow, right? So this is simply, we may start with a point. Each point produce can be used as an initial data for our equation. And we get a gradient curve x of t. That solves our equation x plus equal gradient xf.
00:23:54.044 - 00:24:48.700, Speaker B: Okay? And we may start with another point and get another solution. And the map x maps to xt, right. You may see a synchrony x zero. Right, is our flow for time t. So this, this flow is, defines an action of semigroup plus right on the space. And it has number of properties. So, and most of important is the distance estimates, which I will sketch a little bit.
00:24:48.700 - 00:26:00.394, Speaker B: I mean, I don't want to write them completely. So that's our next aim is to make distance estimate. So let me start with the most important one. So, assume I take two points, x zero, y zero, and I follow gradient flow for time t and get xt yt. So I assume my function f is lambda concave, meaning that f double prime less lambda. Then distance from x of t to y of t is less than e to lambda t times x of zero, y of zero. So we get that gradient flow.
00:26:00.394 - 00:27:06.774, Speaker B: In other words, we get that gradient flow for time t is e to the lambda t Lipschitz map. Okay, so that's one of important properties. For example, if your function was concave, then gradient flow is not expanding, right distance, non expanding map. So there are other distance estimates. You may say that if f is nearly close to g, then a gradient flow for f is nearly, let's say, very close to gradient flow of. And most importantly, that you, you can get explicit bounds, which in fact, exact bounds for for this closeness. Or you may find estimates for different parameters.
00:27:06.774 - 00:28:56.034, Speaker B: You can flow flow along one, float for time for the same flow for time tower and for time t, and estimate this distance in terms of this distance. Okay, but these are not important, right? Like as usual in comparison geometry, all these estimators say the following, that your function is lambda concave, and maybe your space is CBB Kappa. Then behavior of gradient flow is better than in model space for lambda, like for function, which has equality here, right? With equality here for the function, right? So. And as I said that these among most useful estimates. But most importantly, you have a calculus to make such estimates for your, in your case. Questions? Comments? So, next I will try to give an illustration for this method. I want to show how to prove splitting theorem using gradient flow.
00:28:56.034 - 00:30:28.216, Speaker B: So what splitting theorem says that assume l has curvature bounded below by zero, it's complete length space and has a line. So line means both sides infinite geodesic. And once more geodesic for us means that minimizing geodesic, right? Then l is isometric to the product R L cross l prime. And in fact, the geodesic goes into the like. Our geodesic is the r factor of this product. This, by the way, is splitting theorem is like. Has a great history, right? It started from Canfossen, it was generalized by topanograph, it was generalized by two.
00:30:28.216 - 00:31:43.224, Speaker B: The high dimensions proved it in dimension two, it generalized to spaces with non negative rich ecology by Chigger and Gromo. And after that it was yet generalized to to the space time by Eschenburg. I don't know, like you kind of a lot of development of romanian geometry actually can be seen in this little theory, right? Okay, no questions about statements. Okay, so how the proof goes, first of all. So I assume that I have my gamma is my both sides infinite geodesic. And for any geodesic I can like, for any geodesic ray, I can consider the boozyman function. So I may take gamma of t and can see the function f, which is limit.
00:31:43.224 - 00:32:45.204, Speaker B: It goes to infinity distance from gamma of t to a point p minus, distance from gamma of t. Actually, why should I write this minus t? Okay, so this is when t increase. This is non monotonic function. So it has a limit. And this function has perfect meaning. Right? And let's write h for the same thing for limit as t goes to minus infinity. So the same definition for, let me just say plus t here.
00:32:45.204 - 00:33:31.554, Speaker B: Okay? Right. And that's another boozyman function for approaching infinity in the opposite direction. So the fact that our jadisic is minimizing implies that f plus h is greater or equal to zero. If it would be strictly smaller somewhere, then the geodesic would fail to be minimizing. That's a sufficiently large interval. Clear? Right. Okay, now I want to prove opposite inequality.
00:33:31.554 - 00:34:06.116, Speaker B: I want to prove that f plus h is less or equal to zero. Why is it so here I will apply comparison. I have to apply comparison. Here I have to apply the fact that my space is cbb zero. And I will apply development comparison for two reasons. First of all, it's visual. And second, I didn't talk about this.
00:34:06.116 - 00:35:27.664, Speaker B: I decided to save time and didn't talk about this type of comparison last time. So what's the development comparison is like? Assuming I have a geodesic gamma in my matrix space, l, and I have some point that usually I don't want it to lie on the gamma. Then I can construct another curve in the comparison space. In our case is euclidean plane gamma tweedle and p twiddle such that these distances for t is exactly the same as this distance for all t's. And gamma twiddle is unit speed speed. Okay, so such, such a, such a curve called development of gamma. And the comparison says that if you start with the geodesic, actually you can apply it to any unit speed curve in the, in the space.
00:35:27.664 - 00:36:19.356, Speaker B: If you apply to geodesic in the CBB zero space, then the development is locally concave. So it looks like here on the picture, and it cannot look like this. This is forbidden. Okay, again, this is if and only if condition. So the space is CBB zero if and only if every geodesic has convex, locally convex development. Okay, now let us try to apply this. In our case, we have a line and I choose some point p and apply development for this.
00:36:19.356 - 00:37:07.154, Speaker B: Right now I have this, this was picture in our space l. And now I have picture in, in the plane, I have convex curve like this. And if it was, if this curve, it must be convex. But if it's not a line, then it's really easy to see that there is a horde to this curve. That passed through pie. So this is the same as this one, this is the same as that one. And this part is the same as length of this arc.
00:37:07.154 - 00:38:02.068, Speaker B: And we get a contradiction, right? We know that one plus two must be greater than three by triangle inequality. But on the other hand, here, we know that one plus two must be smaller than three by the same triangle inequality, right? So what we get from here is that if I fix a point, then. So, first of all, I get what I promised, right? I get that f plus h is small or equal zero at any point. So f of p, h of p is smaller than zero. So it's for any point the same. But actually I get more, right, if I fix a point.
00:38:02.196 - 00:38:09.064, Speaker A: Sorry, Anton, you showed that this development must be aligned. Why does this imply this inequality?
00:38:13.704 - 00:38:36.684, Speaker B: Right, so you. Because. Because this is the case in the plane, right? Right. When you. Right, I skipped this part, right, but I mean, I showed it. It's a line. And evidently, the sum of boozieman functions in the plane is vanishing.
00:38:36.684 - 00:39:27.214, Speaker B: So it must be the same in the space l, right? Okay, so we get this inequality, but we get little bit more. We get that if I fix a point, then the distance to points on one line are exactly the same as they would like. They would be in the plane, right? Kind of a. Kind of strong condition. Okay, so then, last thing, right? So, like, finally, I have to use gradient flow. So, I have two functions, f and h. By construction, they both of them concave.
00:39:27.214 - 00:40:03.508, Speaker B: They are limit of a certain type of limit of distance functions. And these distance functions, getting more and more like zero concave. In the limit, I get concave functions. I know that f plus h is zero. So that means that f and h are fine. They not only concave, but also convex. So, if I restrict this function to any geodesic, I get a linear function, okay? So in particular, I get.
00:40:03.508 - 00:40:41.120, Speaker B: And I can apply the flow for time t with respect to h. And it's really easy to prove once everything is done. If you apply definition, that's the same as flow for time minus t of f. Sorry, sorry, sorry. Composed, it's equal to identity, right, sorry. That's also plus. Okay, so these two flows invert each other.
00:40:41.120 - 00:41:34.784, Speaker B: In particular, we know that not only future of this flow uniquely defined, but past also, right? So, combining all these together, right, we have functions. So we have. We have our line gamma. We have level sets of our functions, f and hook. Each of this level set is convex. Plus. I have a gradient flow that moves from one set to another and back so one way is flow for h, another way is flow for f.
00:41:34.784 - 00:42:33.672, Speaker B: So both of these flows, since the functions are concave, both of these flow do not increase the distances. So and they invert inverse one is inverse of another. So they actually preserve distance. So I get the real action that moves left and right. And then it's pretty much the end of proof, right? So like I have a line passing through every point, right? So I have, I mean, pretty much I get coordinates, right? I can, I can think that this is l. This convex set of course is Alexander space with curvature, with non negative curvature. And then for any point I can find a point here such that flowing for some time t or maybe backwards for some time t, I get any point on the space.
00:42:33.672 - 00:43:56.244, Speaker B: So the distance estimate showed me that the metric here is exactly as in the product l cross r. So questions? Comments? I hope, I hope you get an idea how to how useful gradient flow is. And let me say that the gradient flow is useful not only in lower curvature bound, but also in upper curvature bound. Also the most interesting applications coming in lower curvature bound, right? So that like in say gradient flow for distance function in the upper curvature bound, you simply go into the origin along the geodesic, okay? And for lower curvature bound, it's something new and interesting. You run in a way of point like for some curve, even if, even if Jedi Z doesn't exist right in this direction. So it's like I have one remaining topic, it's gradient exponent. So if you have questions, please ask.
00:43:57.464 - 00:44:14.330, Speaker C: Now, I have a question you mentioned in the introduction of the topic Chigurh Grimall, which uses only a Ricci curvature assumption. Is there some weaker assumption than CBB that could also give you splitting theorem here?
00:44:14.472 - 00:44:22.790, Speaker B: So Ricci curvature of course is weaker assumption, right. But it is roughly analog of sectional curvature. Yeah.
00:44:22.862 - 00:44:30.694, Speaker C: So in the, in the metric space set, in this weaker setting, is there an analog to reach a curvature assumption.
00:44:30.734 - 00:44:39.154, Speaker B: That, that you could use in the metric sense? Of course there is a like recently development.
00:44:39.944 - 00:44:41.124, Speaker C: Oh yeah, sure.
00:44:41.624 - 00:44:46.360, Speaker B: And I actually, I'm not sure about status of splitting theorem.
00:44:46.512 - 00:44:49.164, Speaker A: Oh yeah, there is, there is one.
00:44:50.104 - 00:45:05.924, Speaker C: Yeah, I think there is in that, in that, in that setting. But there's nothing that sort of suggests itself immediately from the technology that you have a simple weakening of the CBB.
00:45:09.604 - 00:45:45.198, Speaker B: Well, the idea of, I think the proof I gave is closer to the proof of Chigurh Gromo, right. The original idea was like a little different. And by the way, I wanted to remark that Milker also proved it. It's also, let me remind that it was also the first statement in Alexander Geometry for dimension greater than two. Right. The splitting theorem. So it's kind of.
00:45:45.198 - 00:45:49.114, Speaker B: Yeah, sorry. But. Yeah, sorry.
00:45:49.454 - 00:46:27.344, Speaker D: If I may add a comment, if I may add a comment. So it is correct that the splitting theorem in these synthetic lower Ricci bound setting holds. Perhaps, let me advertise that if I will be able, I will prove this during my course here at the feed Cc that will start this Friday. As for what concerns, the structure of the proof, as Anton said, is basically the same. So still the Boseman function and gradient flow plays a major role. Technically speaking, it's a bit more complicated. There is also the Bockman inequality involved to basically get information on the essence.
00:46:27.344 - 00:46:31.544, Speaker D: But the structure of the proof is really what Anton mentioned.
00:46:32.204 - 00:47:07.104, Speaker B: Okay. Okay. Thank you very much. So, more questions? Comments? Okay, so it's not the case. Let me go to the last topic. It's gradient exponent. And let me first say what we are fighting with, right? So, in romanian settings, we used to have exponential map, exponential map from tangent space to the manifold.
00:47:07.104 - 00:48:22.834, Speaker B: And of course, everybody knows what it is, right? And in Alexander setting, such map is not well defined by simple reason. You choose a direction at the point p, and there might be no geodesic in this direction, right? No geodesic. And so the exponent, like if it's a vector V exponent p of v is not defined. But still you want to go in this direction and still you want to make it as nice as possible, right? So you want a curve that goes in this direction and pretend to be geodesic as close as possible. And gradient exponent, which usually denoted by this way, is a good solution for this. So let me quickly say what it is. So I will cheat a little bit.
00:48:22.834 - 00:49:55.654, Speaker B: Let me start with the function f is distance function. I will cheat a little bit and after that I will tell where I am cheating, right? So let me start with the function distance square over two. That's a function from my space to real line, and it's f is one concave, assuming l has curvature bounded below by zero, right? But something like this, a little bit weaker, works for all kouch bounds. So I can apply gradient flow. I take a flow for time t from l to itself, and I know that this flow is e to the t lipschitz, as I mentioned before. Right? So this, by the way, I should say when I, when I said about this, this is essentially Picard theorem, except you, instead of equality, you use inequality there, right? Like, not, not Picard theorem. But essentially it repeats the proof of Picard theorem in ordinance differential equation there of this statement.
00:49:55.654 - 00:50:43.958, Speaker B: So, we have this Lipschitz map, but I want it to be short. I want it to be one Lipschitz. And it's easy to arrange. I just multiply my first space by e to t, right? And so this is a short map now, okay. And then I consider the case when t goes to infinity. So in this case, in this case, this space converged to tangent space at point p. Okay.
00:50:43.958 - 00:51:28.984, Speaker B: And this stays the same. And in the limit, I get shot map from tangent space to l. And this map is called gradient exponent at the point p. So first of all, it works in for lower curvature bound zero. For other curvature bounds, you have to be more like. This doesn't work directly this idea with the rescaling and passing through the limit, but you have to apply some other way to define it. But still you can apply, you can construct a gradient exponent for any lower curvature bound.
00:51:28.984 - 00:52:40.626, Speaker B: So I was cheating here, because in general, that's not true. That rescaling copy of my space converge to tangent space, but for reasonable spaces, say for, if the spaces have finite dimension, finite dimensions, that's true. So for most applications, it's correct. And the last statement that gradient exponent actually defined is true always, right? So that's one way. So. And gradient exponent is really exponent, right? So if you have a point p, some have point q, you have a geodesic, you may find the tangent vector to this geodesic. Then gradient exponent at p of v v is equal to q, right? So in other words, it can use this logarithm.
00:52:40.626 - 00:53:41.014, Speaker B: So that's from q to v is called logarithm at p. And that behaves very reasonable. Plus, it is short map. Plus it has nice property. If you have some point q and you can see the so called radial curve like it's gamma of t equal gradient exponent at p of t v. Assuming that v is one, right, then you have distance estimates from point q the same as it would be geodesic, the same distance estimate as for jeddesic that you shoot in this direction. So it's like, yeah, it's a good thing to.
00:53:41.014 - 00:54:06.834, Speaker B: Good exchange for exponent. And maybe I have, I want to like if no questions, I will show one example how to apply it in some, I mean, some little application. I can show some little application. Is there other questions about.
00:54:08.214 - 00:54:22.074, Speaker D: I have a couple, actually. One, if it's possible to see an example of what happens discrete exponents say, if you hit the boundary, say with an angle of an Alexander, say a square or something.
00:54:23.044 - 00:54:46.892, Speaker B: Very good question, right. I assume I have a half plane, right? And I have point here, and I gradient exponent from the point, right. As far as the geodesics, it's just usual exponent. Once you hit the boundary, you start to slide along the boundary. So what actually happened? You, you may think that you go further and project to the boundary.
00:54:46.908 - 00:54:48.316, Speaker D: Okay, so we let speed, right?
00:54:48.380 - 00:55:23.844, Speaker B: So, so, right. You, after that, right, after, after first singular point, right, after, after, geodesic doesn't exist anymore. You have to slow down, but you still go there. If you get, if you hit a critical point like this one on the boundary, you stay, you stand, right? So you, after, so they go on side slowly, but still, still, of course, the way I defined, right. If you just take usual, usual exponent and after project, that's, that's for, for the right. Okay. Okay.
00:55:23.884 - 00:55:35.268, Speaker D: I have another question. I don't know if this is something you wanted to talk about, or, but I've read, I think, in your papers about the notion of quasi geodesic. Is this any way related?
00:55:35.436 - 00:56:07.648, Speaker B: That's. Yeah. Right. So, in the construction of quasi geodesic, we used gradient curve for short time after that, again, gradient curve, gradient curve, gradient curve. And applying quite complicated limit procedure, we were able to construct a quasi geodesic. But once more, that quasi geodesic work only in finite dimensional spaces. And gradient work always, and they are much simpler.
00:56:07.648 - 00:56:56.536, Speaker B: And in terms of applications, they are better, right? So actually. Okay, yeah, yeah. So, more questions. So if not the case, let me just quickly say one thing. There is an old question in Alexander geometry, that boundary boundary of Alexander space space is Alexander space. So that means that boundary with intrinsic metric, induced intrinsic metric of Alexander space is Alexander space with the same k, which bound, right. So that's open question.
00:56:56.536 - 00:58:25.786, Speaker B: I think that's first question, which formulated an Alexander geometry in dimension higher than two, right? And still open. I think it's due to Yuri Bharaga, but I'm not sure, maybe if he's here, he could correct me. Yuri Buraga. Okay, but until it's proved, any partial result is interesting, right? Any, any color of the statement is interesting. And one calorie of this statement, if that sigma has curvature bounded below by one, then dimension m, right? Then the boundary of sigma has area less than area of m minus one dimensional sphere, right? That if it, if the statement, if the conjecture is true, then it easily follow from one, from another, but without assuming that it's true, you can prove the statement. So this, you can prove this partial result. Okay, how to do this? You just take your space sigma.
00:58:25.786 - 00:59:31.726, Speaker B: So that's your boundary. You choose point that on maximal distance from the boundary and apply gradient exponent. And then you have to prove something, right? So the gradient exponent maps half sphere to sigma, and the boundary of half sphere is like, okay, gradient exponent. The gradient exponent of the boundary contains boundary of sigma and its distance non expanded. So there might be, by the way, there might be creases, right? So the image of boundary might look like this, right? But it covers all boundary. All right? And in particular, the area of the boundary is smaller than area of boundary of half sphere. And let's finish the proof.
00:59:31.726 - 00:59:44.214, Speaker B: So I am on time, ended on time. Right. Question, I mean, should I finish? I'm not sure. Like I was told that you are strict with time.
00:59:44.514 - 00:59:46.690, Speaker A: Sorry, can I make a comment here?
00:59:46.882 - 00:59:47.546, Speaker B: Sure.
00:59:47.690 - 00:59:54.514, Speaker A: I mean, you don't actually know that it's a half sphere. What the space of direction at this point p is. It could be smaller.
00:59:54.554 - 01:00:12.796, Speaker B: Right, right. Yeah. Right. You have to work a little bit. But there is a short map from, from the sphere to this. It's like you roughly can assume that it's space of direction is a sphere, right. It's a generic point.
01:00:12.796 - 01:00:17.864, Speaker B: It is so, yes, yeah. And by approximation you can do this.
01:00:19.204 - 01:00:22.244, Speaker A: Also, the fact that it's, the image.
01:00:22.284 - 01:00:38.094, Speaker B: Contains a boundary is not obviously not obvious. Right. You have to work. But I mean, I showed that, I think. Yeah, I showed the idea how to use the gradient exponent. Right. At least you can prove something new and interesting.
01:00:38.094 - 01:00:48.814, Speaker B: Right? More comments, questions? Not the case.
01:01:14.494 - 01:01:31.398, Speaker A: If there are no more questions, let's thank Anton. And so Anton has another lecture tomorrow. This is not the last one. It was about dimension theory.
01:01:31.446 - 01:01:32.434, Speaker B: Right, right.
01:01:33.494 - 01:01:34.994, Speaker A: So at the same time.
01:01:35.574 - 01:01:37.270, Speaker B: Okay, see you tomorrow.
01:01:37.342 - 01:01:39.486, Speaker A: Thank you. See you tomorrow. Bye.
