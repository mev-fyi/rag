00:00:24.520 - 00:01:47.188, Speaker A: Chapter five we start a new chapter now on operations. On kernels operations. We have seen marginally some occasions of this, in particular when we add two kernels together. Now we want to do it more systematically and see when can we multiply or take the difference of two kernels and what is the space we obtain. That's the purpose of this. The first section is something standard that we do, usually in structures which are real, but we want to create something which is complex. So if you have h which is an rkhs on x, but real valued it simply.
00:01:47.188 - 00:02:30.964, Speaker A: I mean, this is the most natural way to do it. Consider h plus ih, which means all the functions f plus ig. Then f is in f and g. Both are in h. This is called complexification, and it's easy to see that w is a vector space. But we need to also define the inner product and the inner product. What is inner product of f one plus I f two and g one plus I g two in w.
00:02:30.964 - 00:03:54.142, Speaker A: And simply we ask ourselves what we want this to be. We want to be an inner product, so we should be able to expand. And when we expand, it will be f one inner product, g one in w plus f one inner product of ig two in w. What we take out this becomes minus I and then plus if two in a product with g one in w and I comes out becomes plus I. And finally, the if two inner product with ig two in w, I and I from both components come out, it's I times minus I gives us minus. So if we want to have an inner product, it has to satisfy this property, this identity. So, and now f one, g one, f mean all of this inner product even though they are in w, but they should be compatible with the one on h.
00:03:54.142 - 00:03:59.470, Speaker A: So we want this to be f one. G one.
00:03:59.502 - 00:04:02.794, Speaker B: Excuse me, I think the last minus sign should be a plus.
00:04:04.154 - 00:04:17.874, Speaker A: Yes, yes. Thank you. This I times minus I. Absolutely. Thank you. Yes. And now we want this to coincide with what we have before.
00:04:17.874 - 00:05:07.114, Speaker A: So, inner product of f one and g one is this in w is this. We want this to be the same as the one in h and the same. A little bit of space here. This we want to be minus I, f one, g two in h plus I, f in h and the same with plus f two and g two in h. We want this to be true. And now we consider this as our definition. We define this to be equal to this as our definition.
00:05:07.114 - 00:06:03.968, Speaker A: Definition. And then the rest is just a matter of calculation to show that this is a good definition. It means that it satisfies all the properties that that we want to behold and they are held. And in particular, we see that the norm. So the rest is just some verification. I leave it to the student to do it. And in particular, you see that the norm of this element in w squared, based on the definition, is the norm, the first one in h squared, plus the norm of the second norm in h squared.
00:06:03.968 - 00:07:21.850, Speaker A: This is what we want to be true indeed. And also, again about the kernel, what is the f one plus if two inner product with ky, the kernel that we have for h? So this is a real function in w, based on the above definition, is f one, k one in h plus I, f two and ky in h. That's our definition. And based on the property of the kernel function, the first one is f one evaluated at y plus I f two evaluated at y. So in other words, if I call this function f for any f in w, f in a product with ky is f at point y. Kernel doesn't change. It's the same.
00:07:21.850 - 00:08:29.322, Speaker A: We have, we have the same k as before. It's just the space which is complexified. This is something very standard that we do in other contexts too. That's in the context of rths, the next topic, which is about differences and sums plus and minus minuses. There is, there is a result which is used in the proof of the next theorem, which merits to be highlighted by itself. And in a sense, even though the proof is easy, it is used many, many times even in research articles. And it's a powerful tool just for rkhs and its generalization, which is rk p is reproducing kernel in a sense Banach spaces.
00:08:29.322 - 00:09:15.238, Speaker A: So I mentioned it as a lemma. Suppose that h and k are given, I mean reproducing Kendall Hilbert spaces. I mean, both are given and you have a mapping t which is defined from h to k. Of course, linear. Everything is linear. So Joseph mapping is well defined. It means that if f in h is given, then to f belongs to kick.
00:09:15.238 - 00:10:33.058, Speaker A: So it seems nothing extraordinary up to here. What is amazing is that then t is automatically bounded. If you have a mapping, linear mapping, which is just well defined, then boundedness is a gift. It comes automatically. And the proof is based on the closed graph theorem here. It's a powerful tool from functional analysis. It says that if you have a sequence in h and f is also in h and g is in k and f, these are all assumptions.
00:10:33.058 - 00:11:57.384, Speaker A: Fn goes to f in h and t goes to g in k, so from this hypothesis, you can show that tf is equal to g. Then the whole thing, if we can show this, which means that the graph is closed, that's precisely the same thing, but written in mathematical language and more explicitly, then this implies that t is bounded. That's the theorem. Closed graph theorem. If the graph is closed, then the mapping is bounded. And now, how can we verify this? That's the power of rkhs. Well, f, well, if n goes to f implies that if n of x goes.
00:11:57.384 - 00:13:08.464, Speaker A: There is something missing here. Please let me, I mentioned this, but I'd gone back and complete it later on. There is one, there's one hypothesis which is missing here. I'm sorry for that. For any x in x, and the same is for k. I mean, t f n x goes to g of x. No, sorry, I tell you why I wasn't careful enough here.
00:13:08.464 - 00:13:54.154, Speaker A: Then there is something here to, to conclude that tf at all point x equal to g of x for every x. And so tf is equal to g. This I forgot. And you'll see in the application why I forgot, because our mapping is the inclusion mapping. And so when it's inclusion, you can easily remove this t and c f n also goes to gr and conclude. But there is a generalization, and I mean, I should come back and give you a more precise formulation. I'm sorry for that.
00:13:54.154 - 00:14:37.884, Speaker A: So back in, not, not next week, next time I will complete this. But still, this theorem, this theorem is. This lemma is a very powerful lemma. Just one hypothesis is missing. If jus is well defined, you can go further and show that your mapping is bounded. And I forgot this because my mapping is just inclusion and I can easily go to the end. Again, my apologies for that.
00:14:37.884 - 00:16:05.524, Speaker A: So our first theorem is for the inclusion, which gives us the difference later on. Here is the theorem. X is set k k one and k two. Both of them are from x times x with values in c. These are kernels. And then here is the conclusion. Then h of k one is a subset of h of k two if and only if if and only if there exists a constant circle such that the relation between kernels k one is less than or equal to c two, k two.
00:16:05.524 - 00:17:01.754, Speaker A: So up to here, it's just an inclusion. Inclusion of two spaces is equivalent to the condition on the kernels. And I mentioned before the break, k one less than or equal to a constant k two is a huge restriction. You have to verify it on all n by n matrices with n equal to one, two, three. No restriction on n and no restriction on our notes. X one up to xm. So we have many, many grammy many many matrices to verify that this matrix is less than or equal to c times that matrix infinitely many.
00:17:01.754 - 00:18:15.638, Speaker A: Well, this is for the first part, but we can say a little bit more about c. Moreover, the norm of f in space h two or hk two, we can simply write two here, which means in the second space is less than or equal to the same constant c normal f in h one in the first space for Olaf in h one or hk one. So the inclusion map is bounded by c. The best constant is indeed the norm of the inclusion map. That will be shown in the proof. I give you the complete proof and you will see what went wrong when I wanted to prove the previous lemma. Now first, assume that the constant c exists.
00:18:15.638 - 00:19:55.364, Speaker A: Assume what happened. Assume that c bigger than or equal to exists, and well k one is less than or equal to constant times k two. Now, if f is in h of k one and we can normalize it, it really doesn't matter. I will tell you what happens if we do not normalize it. Then by the famous theorem that I used even today a couple of times, I can say that f of x f of y bar is less than or equal to one here, which is the norm of f one times k one, x and y. You see, the effect of one is here. Otherwise, here you have normal f one norm one of f squared.
00:19:55.364 - 00:21:05.344, Speaker A: So this is from the previous famous result that I mentioned several times. By the theorem, I think it was three point, either 3.1 or 3.11. I forgot the name, the exact numbering, but the theorem we use several times, and our assumption is that k one is also majorized by k two. So put this together, we see that f of x f is less than or equal to c two, k two x and y. And one more time, the same theorem. The same theorem says that f is in h of k two, and also its norm is majorized in this space is majorized by c.
00:21:05.344 - 00:22:04.382, Speaker A: For a general f, if f is in h of k one, you can consider f divided by normal f in the space. Now it is normalized and you obtain the same conclusion. F is in the second space, and also its norm in the second space is controlled by c. In other words, normal f in the second space, you multiply both sides by. This is controlled by c times normal in h one. That's half of the theorem. But even in this half I used the trm to 3.1
00:22:04.382 - 00:22:43.254, Speaker A: or triple, .11 maybe two y's. And even before the break I use it several times. And now for the converse. For the converse, just assume that h of k one is a subset of h of k two. And consider the mapping for which in the lemma I faced some problem. The mapping from h of k one to h of k two.
00:22:43.254 - 00:24:05.274, Speaker A: Just the inclusion map. It's well defined because h of k one is a subset of h of k two. And now for this it's trivial to apply the closed graph theorem if you have fn in the domain which goes to f in h of k one, and in the range Tof, which is f, goes to g in h of k two. Now we can use the fact that convergence in norm implies convergence point wise. So this implies that f goes to f of x for every x and this one. This one implies that tfn of x, which is f n of x, goes to g of x for every x, and therefore for every x, f of x is equal to g of x. So f is equal to g.
00:24:05.274 - 00:25:40.544, Speaker A: In other words, g is equal to t of and so close graph theorem implies that, say by close graph theorem, t is bounded, t is bounded. And so I can write, I can write norm of tf is less than or equal to a constant t normal f. But note that this is in two and this is in one. But t of f is equal to f. So normal f in two is less than or equal to a constant normal f in one for any f in h one, for any f is in h one or f. Ok, it's not over yet. What, what I did is that from just h of k one, what was my assumption? That's not a mistake.
00:25:40.544 - 00:26:40.744, Speaker A: My assumption in the second part was that h of k one is a subset of h of k two. And from this assumption I concluded that a norm of h two is less than this constant normal f in h one. So this is what I have shown up to now. But my goal is more than that. My goal is also to show that the kernels are k one is controlled by the kernel k two. And for this there is a trick. And it's a nice trick, but it's not a trivial one.
00:26:40.744 - 00:28:21.844, Speaker A: Here is the way we do it. So remember, our equal goal is to show that k one is less than or equal to a constant k two. So how we can do that? Again, we need to consider what is the meaning of this? And for this we consider endpoint in x and alpha one. Alpha n is in C. And for simplicity we write ky one for the reproducing kernel of the first space, k one I x and y and k y two for the second space. So these are our kernels. Now, what can I say about this combination? Sum alpha I bar alpha j k one x I x j.
00:28:21.844 - 00:29:19.194, Speaker A: Well, of course I can write it I j from one up to n alpha I bar alpha j. This is the same thing as little k at point xg. But in the first space k one evaluated at point xi. And usually, usually, and this is true, I write something here, but I will erase it. Write this as k xj one evaluated at the point xi. So it will be kxi at the .1 in the first space, h one or hk one.
00:29:19.194 - 00:30:16.894, Speaker A: This is perfectly fine. This is true. But at the same time, at the same time, use the fact that our main hypothesis that k hk one hk one is in hk two. So this element, this element, it is true that which is in hk, it's also in hk two. So I consider it as an element of the second space, a function evaluated at point xi. So I can write k two at point xi in the second space. That's the trick.
00:30:16.894 - 00:31:37.580, Speaker A: This is something that if you don't do it, it doesn't give you the result you are looking for. So the main point is here, switching from space one to a space two. And then the rest is mechanical work. You take the sum inside j from one up to not twice alpha j k xj one, sigma I from one up to n alpha I k x I in space two h k two. And I mean, if you wish, you can take the absolute value, but it's not needed because we know this combination is already positive. So apply Cauchy Schwartz. So it's less than or equal to the norm of this times the norm of this.
00:31:37.580 - 00:32:34.204, Speaker A: But both norms in the second space, sum j from one up to n alpha jkxj one. And here I from one up to n alpha I k xi two. And now it's time that I need this inequality, which I already proved, this inequality for any element of the first space. Of course, we can calculate the norm in the second space, but the norm in the second space is controlled by the norm in the first space. And I use it. I use it here. This is an element of the first space, but the norm is computed in the second space.
00:32:34.204 - 00:33:40.954, Speaker A: So I can write this less than or equal to, I mean, the constant that we have before constant c, which is the norm of t, the norm of the same object, alpha j kxj one. But this time in the first space and the second doesn't change. So this allows us to go from two. Oh, this is not good. This allows us to go from two to one. And now look at this. And the one that we started with this.
00:33:40.954 - 00:35:22.414, Speaker A: It's very easy to see that if you square this quantity and use the fact that this is the kernel, you obtain the same thing that we have at the beginning. In other words, we have shown that the norm of the sum j from one up to n alpha jkxj one. The one we have before is this one squared less than or equal to c. And I repeat it here one times. And I repeat this one k x one squared. And now we can remove one of this and just obtain the sum j from one up to n alpha jkxj one less than or equal to c alpha I j I think two is forgotten. From some point on, there is a two which is forgotten here, forgotten here, forgotten here two.
00:35:22.414 - 00:36:46.304, Speaker A: Well, almost at the end. I already said that. I already said that. If you square this, you obtain the combination that we have here in the yellow rectangle with k one here. So if you do this here, if you square both sides, then this one gives us the sum I j from one up to n alpha I bar alpha j k one x one x I n x j less than or equal to c two. And this one gives us alpha I bar alpha j k two xi and x. And this is precisely the meaning of k one is less than or equal to a constant k two.
00:36:46.304 - 00:37:43.984, Speaker A: And you see, the constant c equal to the norm of t works. So that's the end of proof. The proof is nice one, but it really doesn't work if you do not apply this trick here. The harder proof is here that instead of considering this as an element of h one, consider it as an element of h two. And so apply the kernel of the second space, and then the rest is somehow mechanical. And you go up to the end. A special case which is very important is the contraction when c is equal to one.
00:37:43.984 - 00:40:13.954, Speaker A: In this case, we say that h one is contractively included in h two two. So if h one is a subset of h two, but more than that norm of h in two is less than or equal to normal h in one for any element of the smaller space. In this case, we say h one is contractedly contained in h two. And the previous theorem gives a complete characterization of this. So corollary as before, h one and h two rkhs on space x and with kernels with kernels respectively, k one and k two, then h one very nice characterization is, and track tv contain h two if and only if. This is difference k two minus k one is positive. It's the same as k one is less than or equal to k two.
00:40:13.954 - 00:41:36.586, Speaker A: But it's more practical to write it that way. And we didn't go into this example in detail, but we just mention it as an example. But we had a week on this topic on the branch of Niac spaces. For these spaces, we consider b as an element of h infinity, but nor is less than or equal to one. So, in the unit ball, in the closed unit ball of h infinity, and the kernel of this space, I mean, it's called the space Hb. The kernel is one minus bzw four, one minus z w bar. Indeed, there are three different ways to define Hb spaces.
00:41:36.586 - 00:42:17.954, Speaker A: And there is a nice article of Joseph Paul on this. One method is to consider with this kernel, show that k. I mean, so we start with this definition, it's a little bit of work, but we show that k is bigger than or equal to zero. And therefore by Mohr theorem, it gives a space h of k. And this is precisely our space hp. That's one way to define it. So, starting from kernel, we get this space, if we do it that way.
00:42:17.954 - 00:43:43.534, Speaker A: Well, you need a little bit of work to distinguish some of its element and develop some of the properties of Hp another way, which I mean, we adopted in our book. And Sarasohn has a very, very nice book of Hardy Hilbert spaces. And indeed, this is in his way, he invented this approach. We use two operator to define Hp as a function space. And then we show that is an rkhs and its kernel is given by this one. This second approach is better, and you easily prove, more easily prove some of the properties of Hp spaces. The reason I mentioned this here, the reason is that, I mean, I just mentioned as a result that Hb is contractedly contained in the hardest base h two.
00:43:43.534 - 00:44:39.420, Speaker A: It's one of the famous examples of contract leave inclusion. So they are there. And so nef. Well, we can put it this way, nor what does this mean? It's an Hp is a subset of h two, but more than that, normal f in Hb is less than or equal to the normal f in h two for any f in the smaller space. That's the meaning of contractive inclusion. I mean, don't go into the detail of this. It's beyond the scope of a course.
00:44:39.420 - 00:45:46.864, Speaker A: But if you're interested, and as I said, there is book of Saracen, and there was a week on this topic, courses given by Dantimotin can go there and watch his lectures. These are all explained over there. That finishes the part about inclusion and differences. We can continue and talk about some. Now, what do we obtain if we add two kernels? I think I have enough time to explain a little bit. But the proof we will see here is a theorem, same hypothesis as before. H one and h two rkh on x with kernels k one and k two.
00:45:46.864 - 00:46:48.714, Speaker A: And define or put k is equal to k one plus k two. Well, if you add two positive kernel function and the result is positive, there is no extra condition here. But still there is a question. What is h of k? Then h of k is what you imagine it has to be, is the set of all f one plus f two, such that f one is in h one, and if two is in h two. But be careful here the representation is not unique. I mean different elements, different f one and different f two. You may add them together, but obtain the same object.
00:46:48.714 - 00:48:13.466, Speaker A: So if for any f in h k, it is true that f can be written as f one plus f two. But what can we say about the norm of f squared? This is equal to the minimum of all possible representation. All f is equal to f one plus f two. F one is in h one, f two is in h two, and the norm is the norm of the first one in the first space. Square close the norm of the second one in the second space. Either put one here or h one or h two squared. So you cannot say that the norm of f squared is equal to the norm of f one squared plus normal f two squared, because of, I mean, non uniqueness of the representation.
00:48:13.466 - 00:49:10.008, Speaker A: But you can take the in femum and instead of in femom I wrote minimum. It's attained. And now let's go to proof. Consider the direct sum of h one and h two. This is the ordered pairs, pairs f one and f two. With the usual way to define the inner product in this space means that the pair f one, f two, its inner product with g one, g two. It's component by component.
00:49:10.008 - 00:50:10.404, Speaker A: This one with this one, but they are in space h one and f two, inner product with g two. They are in h two, and this gives the inner product in the direction. So it's easy to. The rest is to prove that this is well defined and we obtain a Hilbert space and the norm. Moreover, in this space in h one plus h two, the norm of pair f one, f two squared is equal to the norm of the first component squared. Plus the norm of second component squared. Good.
00:50:10.404 - 00:51:44.084, Speaker A: And now consider this null space. I mean, we will see why it's n, why it's null space. The set of all ordered pace of the form f minus f. When f is in intersection of h one and h two, this is minus. So we consider this, it's easy to see that it is a vector space, I mean closed under addition and scalar multiplication. It's also norm closed, because if fn and minus fn is in n and converges to the pair f and g, then by the definition of convergence, then the first fn should go to f in h one, and the second component, which is minus fn should go to g in h two. That's the meaning of this convergence.
00:51:44.084 - 00:52:45.390, Speaker A: And therefore this implies that f is equal to minus g. But f was in h one, g was in h two. So this is in h one intersection h two, and therefore the pair f and g is the same as the pair f minus f stays in n. So it's a closed subspace of h. N is a closed subspace. So h one plus h two. Therefore I can write, I can write h one plus h two.
00:52:45.390 - 00:53:56.924, Speaker A: I can decompose it as n plus. This is orthogonal decomposition and perp. So any element f one and f two, which I consider I can write it uniquely as an element of the node n, which means f minus f plus another element h one and h two. This is in, this is in n per and this is in n. This is possible. So why is this helping us here? Because I can consider the mapping gamma. Now you see, why is it called n? The mapping gamma from h one plus h two to a vector space, which is called, I call it h.
00:53:56.924 - 00:55:19.104, Speaker A: For the time being, there is no structure on it. It's just the sum of element of h one and h two. Note that h one, it's element of functions, and x complex function, the same thing as this one. So when you add a function here to a function here, it's an element of the family of all functions defined on x x with values on c, and we call this h. So the definition is like just simply f one and f two, you send it to f one plus f two. That's the definition of h. No structure on h yet, except that it's a vector space and the kernel or the null space, better to say nola space of gamma is precisely what we called n, because it's all f one and f two, such that f one plus f two two is equal to zero.
00:55:19.104 - 00:56:17.636, Speaker A: When this is the case f two is minus f one. So we can write it as f minus f, and f is at the same time in h one and in h two. This is the space we called n. So the null space is n. And therefore we can say that if we restrict gamma from n plus to h, we obtain a mapping which is bijective. True, because now the nodes, it's linear, and now the null space is just zero. So we obtain something which is bijective.
00:56:17.636 - 00:57:43.196, Speaker A: And I explained this before in, I mean, in the middle of our course, that it happens sometimes that we have two sets and a bijection between them. Say we call this set, say x, this set y, and there is a structure on the first space, and this map is bijective. And we can use this bijective map to transfer the structure of x to this, to y. So we define a structure here just by copying. In other words, you take, for example, if the structure is an inner product, what is the inner product of two elements? Here, bijective. You go back to the first space by your map, and you obtain two elements here. There you calculate the inner product of these two elements and define the inner product of these two to be the same, and for any other operation, the same.
00:57:43.196 - 00:58:22.204, Speaker A: For example, if you want to define a group, the same thing, vectors, whatever is here by the bijection, can be transferred to the other space. Well, we are in the middle of proof still. I need probably ten more minutes to finish this. But it's four two. Let's have a break and come back after the break to finish the proof. I have a quick question, please. I didn't catch that remark that you made about the lemma.
00:58:22.204 - 00:58:49.114, Speaker A: Was it true? Under certain condition, other conditions, yes. Yeah, I remember. I mean, after the dilemma. Let me share again to show you. I failed to prove the lemma because of, was it on this or on the other one? I think it was in.
00:58:53.174 - 00:58:53.486, Speaker B: Here.
00:58:53.510 - 00:59:45.394, Speaker A: It is. I have to show this true, but I couldn't. And the reason is that I need more information on TFN at the point X and its relation to FN. I didn't add in this lemma. I wanted to generalize. And I tell you now I can tell you why, because I remember in the application I didn't need this, because TFN was precisely equal to FN. So if fn goes to f, and if n goes to g, f has to be equal to g.
00:59:45.394 - 01:00:55.874, Speaker A: The general version I needed at t of f, n was just a multiplication. So tf was, say, h times f, something like that. And now, again, you see, there is a connection between tf at point x and f is h of x, f of x, and I. Again, I can apply the closed graph theorem in the following sense. If fn goes to f and tfn goes to g, then here I can say that f of x goes to f of x. What can I say about tfn of x? T of X? Here is the definition is h of X. F goes to h of x, f of x.
01:00:55.874 - 01:01:37.952, Speaker A: On the other hand, I assume that it goes to g. Also, it goes to g of x, and therefore g is equal to h times f. In other words, is equal to t of f. And therefore, I can apply the closed graph theorem in this general setting which I mentioned here. The thing I forgot is to somehow connect tf to f. I think the.
01:01:37.968 - 01:01:53.324, Speaker B: Way to do that is to have the hypothesis that the map that takes f to t f evaluated at x is continuous on h for every x in the domain space of k.
01:01:56.704 - 01:02:02.924, Speaker A: So for any x in x.
01:02:05.944 - 01:02:16.084, Speaker B: F goes to tf evaluated at x. You need for that to be continuous on h. That's the missing hypothesis.
01:02:16.524 - 01:02:33.284, Speaker A: Okay. And then f n goes to f. Oh, yes. Then we can go. And, of course, it works for. For this multiplier.
01:02:33.404 - 01:02:35.420, Speaker B: For that. And if t is the identity also.
01:02:35.452 - 01:02:43.128, Speaker A: Yeah. For inclusion, it works for multipliers. It works. So, yes, this is. This is a good one. I didn't think of this. Thank you.
01:02:43.128 - 01:03:04.604, Speaker A: Thank you, Sheldon. I will write this correctly for the next time. So the only yes. The point evaluation is continuous with respect to a structure of h. Are there obvious examples where this would fail without either of those conditions?
01:03:05.064 - 01:03:14.128, Speaker B: Yes. So, because otherwise, even if you take h equal to k, if you don't add any hypotheses, you're claiming that every linear map is bound.
01:03:14.296 - 01:03:27.184, Speaker A: Okay. Yeah. Yeah. Okay, thanks. Thank you indeed. And thank you, Sheldon.
