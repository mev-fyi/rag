00:00:02.600 - 00:00:37.414, Speaker A: Thank you. Hello everybody. After a break for Thanksgiving week, we continue. The next topic in the book is about parcel frames. But since we talked about kernels, I do the Moore's theorem today. And if time remains, I go back to this. It's a very important topic that we need to cover, and many, many function spaces come out of this result.
00:00:37.414 - 00:01:42.574, Speaker A: So we talk about r theorem. To prepare for that theorem, let's have an observation. First. Let me make it. Observation is a lemma about reproducing Colonel Hilbert spaces. Let h be an rkhs on a set omega, and you fix some points. Let x one up to x n be in omega n distinct.
00:01:42.574 - 00:02:51.604, Speaker A: Then you can form a matrix, uh, with this point and the kernel of the Hilbert space. And here is our matrix. You can see that the n by n matrix, ok, at point Xi and xj and I and j, both of them run from one to n. And note that when your points, I mean x one up to xn, change, the matrix changes too. But for each fix x one up to xn, you have a matrix. The result is very simple. Then this matrix is positive, is bigger than or equal to zero.
00:02:51.604 - 00:03:50.260, Speaker A: The proof is indeed very simple, but the observation is fundamental for what we want to do later on proof. How do we show that something a matrix is positive? We make that double sum, and we need to show that that double sum is positive. Here is a double sum. Let alpha one up to alpha n be complex number. And considering complex is important, you cannot just stay with real numbers. Then the sum over a and j here you need to write alpha I, beta j and put the conjugate in one of them. Some books, put it on alpha I, some alpha j.
00:03:50.260 - 00:05:25.584, Speaker A: It really doesn't matter. To get a, I mean, nicer formula, I put an alpha I first here and then k xi and xj. The thing I mentioned before, that this matrix is positive, is equivalent to say that this combination is bigger than or equal to zero for any choice of alpha one up to alpha n. But then to show this, just recall the definition of capital k at point x y. This is the inner product of the kernel at the function xj, inner product at the kernel at the point x y in the space h. That was the very definition of capital K which we had a few weeks ago. And as soon as you made this observation, the rest is straightforward because then did I write it correctly? Yeah, I mean, I should write kj here and ki over there because this is kj at point xi.
00:05:25.584 - 00:06:41.868, Speaker A: Let me mention here, this is kxj at point xi, which is equal to this one. And now you see why I put the bar on alpha I, even though it really doesn't matter, because now I can take alpha I inside on the second variable and obtain alpha j in the, in the first one, alpha I in the second one, k x I. And this is just a norm of sigma I from one to n alpha ykiri h squared. And the norm is positive. That's the end of proof. And also the proof has a byproduct for strict positivity. That's the definition of being positive.
00:06:41.868 - 00:08:05.724, Speaker A: Sometimes we need to study also, being strictly bigger than zero and strictly bigger than zero means the matrix is this sum is strictly bigger than zero, except for the zero element. And here you see that, here you see that this is strictly bigger than zero, except that there is a combination of kxi. If there is xi and alpha I, such that this combination is equal to zero. Then you see that your sum is double sum is equal to zero. Then the double sum that we have above is identically equal to zero, and we do not have a strict positivity. In other words, a kernel function at points. For any choice of the points, any finite number of them should be linearly independent.
00:08:05.724 - 00:09:05.304, Speaker A: So I can write it this way. K that we have about is strictly positive if and only if. For any distinct collection of the points, the set kx one up to k x ten is independent. We have simply. Sir? Yeah? Have we defined what is a kernel function? No, but I say this, I do this. Forgive the motivation for that definition. Now, up to here, I have started with age and I say that the kernel has these properties.
00:09:05.304 - 00:09:40.004, Speaker A: The kernel of h here is my the kernel has this property. And then I go backward. Instead of starting with age, I start with a function which satisfies this property. So this part is just observation and motivation. True. So for kernel function is which satisfy the properties of the lemma. Yeah.
00:09:40.004 - 00:10:21.274, Speaker A: The proof shows that if h is given, then the reproducing kernel satisfies these properties. First, it's bigger than or equal to zero. And the meaning of bigger than or equal zero is precisely from the language of linear algebra. I explicitly gave an n by n matrix. This n by n matrix is always bigger than or equal to zero. This is from linear algebra, and it's not strictly bigger than zero if there is a dependence relation. We have seen examples of this before.
00:10:21.274 - 00:11:20.234, Speaker A: First example is in the Sobolef space. For example, the first one, we had some conditions, and the boundary condition was that f of zero equal to f of one equal to zero. And as you see here, the kernel at .0 is identical to the zero function and the same for kernel at. .1 each one of this is enough to show that, for example, this is just for one point, the set is dependent or the other one. So in this case, our reproducing kernel is not strictly positive.
00:11:20.234 - 00:12:33.534, Speaker A: We had another example in which, again in the subalarf space, I forgot which number was that, but the condition was that f zero is equal to f zero, one not equal to zero, just they are equal. Maybe it was in the exercises I gave. And in this case, this means that the reproducing Keller at, .1 they gave the same value and therefore k zero one times k zero plus minus one times k one is equal to zero, another dependence relation. So even in this case, our reproducing kernel is not is positive. I mean, in this case, of course we can say that in this example, the capital k is bigger than or equal to zero. Even in this case we can say that it's bigger than or equal to zero, but it's not strictly bigger because of this relation that we have.
00:12:33.534 - 00:14:17.864, Speaker A: But if we go to the hardest base setting, h equal to h two, our kernel kz at point w, or usually we write kw at point z, is the cauchy g one minus z w bar is this, I mean the kernel, is it k? Of course, all the time bigger than or equal to zero. But can we say that k is also strictly bigger than zero? To verify this, we need to answer this question. Is there a collection of points, say w one, w two and w n in d, and a collection of scalars in c such that these are distinct, such that alpha one kernel at point w one plus alpha two plus alpha n kernel at point w n equal to zero is. We need to answer this and, well, there are several ways to answer this question. One of them, which is one of them is direct, it's doable. I leave it to you. I mean, you have the formula here for kw.
00:14:17.864 - 00:15:43.948, Speaker A: Put kw one kw n and. Go ahead, that's direct way. So you do it as an exercise. The second method, if this is the case, for every f in h two, we should have f inner product with alpha one kw one plus alpha n k w n in h two is equal to zero because the second component is identically equal to zero. And now if we apply the meaning of kw one kw n, this is equivalent to say that alpha 1 bar if at point w one plus alpha n bar if at point wn equal to zero for every. And now every here helps us choose specific functions to help us. I mean, very judicial choice of f such that with that particular f, for example, we can show that alpha one is equal to zero, another f for which we can show that alpha two equal to zero, and so on.
00:15:43.948 - 00:16:21.986, Speaker A: And you have seen this before. These are the polynomials that we use in approximation. I mean, you can see the f of w. For example, you want to show that alpha one is equal to zero. Consider the polynomial. You forget the first term, the second term, w minus w two up to w minus wn, and you even do not need to normalize it. But if you want to have a normalized polynomial, divide by its value at point w one.
00:16:21.986 - 00:17:20.730, Speaker A: And here is the point that you see, this assumption is important. Distinct, we do not divide by zero. What is the properties of this polynomial? First of all, f at point w two and w three, and wn is equal to zero. And second, because of our normalization, f at point w one is equal to one. And now for every f plug f here, if you plug this particular f, you see that alpha 1 bar times one plus alpha two bar times zero alpha and quantity zero is equal to zero. So immediately alpha one is equal to zero. And you continue the same procedure to show that alpha two is equal to zero two.
00:17:20.730 - 00:18:27.544, Speaker A: So the same, you can continue and conclude that all the coefficients are zero. And so in this case, the difference between this case, the hardy space and the previous cases, the two sub spaces that we saw. It is true that all the time that k is bigger than or equal to zero. But here we can say even strictly bigger than zero. So the answer is yes, and with the same reasoning indeed, I invite you to show that for the Dirichlet space and for the Bergmann space, we have the same conclusion. It's strictly bigger than the observation we made about the reproducing kernel of a Gilbert space gear. Now we use this to go back forth.
00:18:27.544 - 00:19:08.264, Speaker A: In other words, we do reverse engineering. Here is the definition. Omega is set k. You see, the function k is given now, not the Hilbert state k defined. An omega times omega with values in c, is given. It's a function. We say that k is a kernel function.
00:19:08.264 - 00:20:26.024, Speaker A: If k is bigger than or equal to zero, what does this mean? I repeat the definition already had before. If, for every collection of points, if every x one up to x n in the set omega distinct, for every collection of scalars, alpha one up to alpha nc, the matrix. Okay, the ij element of this matrix is k evaluated at x I xj n by n. It's even better to. I have two y's for all for all. It's better even to add here for all n bigger than or equal to one for one, two, three. I mean, all dimensions, the matrix is bigger than or equal to zero.
00:20:26.024 - 00:20:59.134, Speaker A: So there is a lot of liberty here. The dimension is free, can be dimension one, two. For the matrix one, two, three, any n, and the choice of the points is also arbitrary. I wrote it this way. I wrote it this way, so I don't need this. Sorry. But if you want to write it more explicitly, you know how to write it.
00:20:59.134 - 00:22:24.574, Speaker A: For any collection of the point x one up to xn, the matrix is strictly positive, which means, I mean, since we came up to here, let me add that this means that the sum alpha alpha jk x j or x j really doesn't matter, is bigger than or equal to zero for all alpha I. That's the meaning of being positive. Now we can imagine why I gave such a definition. Remember, we had a theorem two weeks ago. The theorem was like this. If you have so recall two rkhs h one with kernel k one and h two with kernel k two to rkhs and k one is equal to k two. The kernels are equal.
00:22:24.574 - 00:23:12.014, Speaker A: Then h one is equal to h two in the strong sense, which means they are equal as a set, and moreover, they are equal as Hilbert spaces. The Hilbert space, there is an isometry for every f in h one. The norm of f in h one is equal to the norm of f in h two. So that's what I mean by a strong equality. So in this case, the spaces are given and we see that they are uniquely determined by the candles. That's good. Now we want to go backward or look at this from, from the exterior.
00:23:12.014 - 00:23:59.280, Speaker A: A space is not given. K is given a function which is a kernel function which is positive by the definition that we had the power. And there are many easy examples of k which satisfy this property. As a very elementary example, I mean really elementary. Any set omega and consider any function from omega to c. Any function, no restriction. And consider define k of x y by f of x f of y.
00:23:59.280 - 00:25:23.754, Speaker A: Part that's my definition can easily show that this k is a kernel function. Indeed, the proof is very simple. What does this mean? What is the sum I from one to m sum j from one to n alpha I alpha j k xix. And put bar, for example, on this one. This is equal to some alpha I alpha j bar f of x y f of x j bar and this is nothing but some alpha I f of x I mod square, which is positive all the time. So you see, it's not difficult to create a kernel function. This is one of the simplest examples.
00:25:23.754 - 00:27:37.034, Speaker A: So, parallel to this observation, which says that rkhs are uniquely determined by the reproducing catnip, you may ask, if k is given, is there the question, okay, given, is there a Hilbert space agent Hilbert space which is actually a reproducing Kennedy space, such that evaluation functions are continuous. Therefore, we have a kernel, and we can talk about little k x, that's the kernel function at point x. So kernel function at point x evaluated at y is our capital k for a, for h, but such that, such, ah, such that is already up there, such that ex are continuous. And this is precisely the k which is already given. Or did I write it correctly? No, I should write y and x. In other words, if k is given, can we, how can we construct a reproducing kernel Hilbert space such that the kernel is precisely k which is given? And Moore theorem is indeed the answer to this question, and a very good answer. It means a constructive answer.
00:27:37.034 - 00:29:10.286, Speaker A: Not only Moore says that the answer is yes, but only he gives a procedure which is very natural procedure indeed to construct the Hilbert space age. So I write the theorem and then give the proof. Proof is long, but a very good proof, because by looking at the proof, you see several techniques, several interesting facts in the proof. So please bear with me, and we go together step by step. All the I write a theorem for reference, and then I give the proof, let omega be a set and let from Omega to Omega to c be a kernel function. Then it's an existence theorem. So I mean, all of us as a mathematician, we know that the existence theorem are important.
00:29:10.286 - 00:30:56.716, Speaker A: Then there exists and rkhs h all functions on omega such that k which is given before is the reproducing kernel, is the probability kernel of hook and exist. So that's the message, the main message of this theorem. You start with k and then you construct h. In several applications, indeed, the construction which is given in the proof is not used. Just the fact that something exists is enough for us. And one of the celebrated examples is the Dobranche of Niagara space. If you had the chance to listen to Joe Ball talk a few weeks ago, maybe it was last week or two weeks ago, he talked about hb spaces.
00:30:56.716 - 00:31:56.224, Speaker A: And the way he gave the definition was he gave the reproducing kernel. In other words, he gave a k which was positive, and then conclude that there is a function for functional space for that, and the function space is precisely space. We will see this in more detail later on. Proof. As I said, it's a long proof, but please bear with me. Let me open another screen to look. We had an observation before, so recall it was a very simple result, but that result helped us here.
00:31:56.224 - 00:33:05.384, Speaker A: If h is an rkhs with kernel, okay, then the family, or the span of the family span little kx omega mean the collection of all the producing function at points. And then you make linear, finite linear combination with them. This set is large enough in the sense that is dense in h. That's a result that we saw a few weeks ago, maybe two weeks ago. And Moore said that, okay, we want to either find or construct h. We don't have h yet, but what we have. Look at the theorem.
00:33:05.384 - 00:34:16.264, Speaker A: In the theorem, k is given capital k. So we can restrict in one argument and let the other argument to be free. And that gives us kx. In other words, we define kx at point y by k of x and y, or k rw k y x like that. We define it that way because capital k is given. And if we define it that way and eventually succeed to construct our h, which we do not know yet what it is, this little kx would be the linear combination would be dense in this space. So let's go backward.
00:34:16.264 - 00:34:48.635, Speaker A: We define little kx. We also define w to be span of these functions. We have all of this. We know that this has to be dense in our eventual space. So we start from here. And here is his procedure. The definition is the first step.
00:34:48.635 - 00:35:26.164, Speaker A: The second step, more defined an inner product on w. This is not straightforward. We need to work a little bit on it to show that it's really an inner product. There is no problem. And indeed the major part of the proof is precisely here. Then we do a completion. Every inner product can be completed to give us a Hilbert space.
00:35:26.164 - 00:36:21.774, Speaker A: And that Hilbert space, the abstract completion, is not necessarily a family of functions on the, on the set omega, but we need to modify it to obtain a family of functions on omega. We will see how to do that. And that is the space we are looking for. This is the our rkhs. So that's the road to follow. So for the time being w is given. It's a collection of functions on omega, because each kx is a function.
00:36:21.774 - 00:37:31.736, Speaker A: When you do linear combination, it still is a function on omega. So the first step, how to define an inner product on w. So we may do naively as follow we will see what is the problem. May say that let's f and g being w. Therefore, for f there is alpha one up to alpha n complex number. And there are points x one up to x n in omega such that f is alpha one little k x one plus alpha alpha two. And finally alpha n little k xn.
00:37:31.736 - 00:38:17.094, Speaker A: That's true. No problem with that. And the same for g there is, call it beta one up to beta m and another set of points one up to ignite m such that g is equal to beta one. K at point y, one plus beta mk at point ym. Absolutely no problem. Up to here, two f and g are defined by such combination. And then we proceed to to define the inner product.
00:38:17.094 - 00:40:00.984, Speaker A: How do we find f and g at point w? We haven't, I haven't defined it yet, but let's see what we need here. So it's sum from I from one up to n alpha one, alpha ikxy. And then for g j from one up to m sum beta j k y g. Okay, I want to define this since I want to have a linear product. It has to be linear with respect to the first component and conjugate linear with respect to the second component. So at the end of the day, this should be equal to sum I from one to n j from one to n alpha I beta j bar k x y k y j in w. And what else? I want to, to have alpha beta j.
00:40:00.984 - 00:40:47.000, Speaker A: I also want the little k to be the reproducing kernel. If I want this to be true, then I have no choice but this inner product to be k of yj xi. We saw this before. If a kernel, it has to be, this inner product has to be equal to this. So I would have a good definition. If f and g satisfies this property, I need that. Therefore, I mean, adopt this as my definition.
00:40:47.000 - 00:41:42.990, Speaker A: So I define f and g, inner product of f with g in w, to be this sum. Forget about the calculation that I did before. I mean, they were just justification for this definition. So I use this as my definition. And you should immediately tell me what's the problem as definition. I mean, I have f, I have g. So I define as the.
00:41:42.990 - 00:42:08.174, Speaker A: But there is a problem here. Chat whether it is well defined. Yes, in other words, well defined. This means that f is represented by this combination. It's not unique. It might have another representation. So I write behind this with another color.
00:42:08.174 - 00:43:23.700, Speaker A: So it might be equal, also equal to alpha one prime, kx one prime, and alpha, say p prime, not necessarily even the same number of points, kxp prime and the same for g. G might be beta one k y, one prime up to beta, say q prime for both, another set of scalars, another set of points. So if I choose the red one, I see this also has to be equal. And I don't know this I have to verify, is it also equal to sum from one up to pj from one up to q alpha I prime beta j prime bar k at point y j prime xi prime. That's the big question. If this is not the case, my definition is not a good definition. It doesn't make sense.
00:43:23.700 - 00:44:47.698, Speaker A: And that's, I can tell it's pretty much the heart of proof is precisely here. This verification that w is well defined, the rest is automatic. How? Why is this well defined? So go back to what we had for f. F on one hand is equal to alpha, one, kx, one two alpha and kx, and on another hand for another set of scalars and points. But whatever is the representation, the value f at any point is well defined. So for any y in the set omega f at point y, is this function at point y or this function at point y, they have to be equal. And the evaluation of that function at point y is alpha one, k, y at point x one plus alpha n capital k y at point x one.
00:44:47.698 - 00:45:36.970, Speaker A: This is the evaluation of this function at point y and the evaluation of the other at. .1. This is an important identity, very important identity. Alpha one prime, Kyx one prime, alpha p prime, capital k, y, x and tri. That's the key to success, this identity. So remember this, it is true that you may have many different representation, like the one here or the one there. But when you evaluate at points different points of omega, you should get the same value.
00:45:36.970 - 00:46:19.182, Speaker A: That is why I have this combination equal to this combination. And now I can show last index should be p rather than n for the first one, is n for the second, because we don't have necessarily the same number of points in the second representation. Yes. So shouldn't it be x p prime? Oh yes, yes, thank you. Here p prime. Thank you. And now using this identity, which I wrote for f, we have the same identity for g, I can show this identity.
00:46:19.182 - 00:46:56.930, Speaker A: The answer is yes. And let's see how we do this. It's not that difficult. Now let's start with the first one. The sum I from one up to m, j from one up to m. I have to be very careful here not to mix the different indexes that we have. Kyjxi that's the combination I have here at my initial definition with the representation I chose.
00:46:56.930 - 00:48:18.296, Speaker A: But I should show that for any other representation, I obtain the same number. We have identity here. And to do this, I keep the summation over j, so beta j bar, and keep the summation over I inside. You see, the summation is over I and the purple identity here with y replaced by yj. I can go from this sum to the second sum. So that is why here I can say sum j from one up to m beta j bar, and replace this sum by sum I from one up to p alpha I prime, k here, xi prime at the same point. And the point here is yj point.
00:48:18.296 - 00:49:16.300, Speaker A: Repeat. So it's identity above that we have for point y. I write it for point y j and put it there. So that's one step. Now I change the order of summation. I keep I at the beginning and then j inside theta j bar, k, y, j x, I prime and do the same thing. No, knowing that, since k is positive, if you consider two by two matrices with two points, you see that when I change the order here, I have to consider the complex conjugate.
00:49:16.300 - 00:50:18.580, Speaker A: In other words, this is equal to the same thing here, the same thing here. And here is kxi prime, yj bar. If you change the order of the arguments, you need to put a bar over there. And now take the bar outside over the bracket. And what can you say about, what can you say about beta j, k xi prime, yj? There is a bar here. And you see, now our argument, which is fixed, is this one, and the summation is respected, beta j. And this is a representation for the second function.
00:50:18.580 - 00:51:25.218, Speaker A: So it's the evaluation of the second function, g at point x, j prime as this one. When we did this move, when we did this move, this was the evaluation of the function. Well, f at point yj, just keep in mind, and this is without the bar. This is the evaluation of the function g at point x, I prime. So if you use the second representation for g, this is sum. Was it n or m? J is from one to m. The second representation j is from one to q beta j prime, k xi prime, yj prime.
00:51:25.218 - 00:52:33.034, Speaker A: And there is a bar here. I made a mistake here. When I took this sum out, I continued to write n, but it's pull. So the sum a from one up to p alpha I prime. But now I put bar back inside again. Almost finished. This is q beta j prime bar and k bar, which I write it as yj prime, xi prime, and sum I from one to p, sum j from one to q alpha I prime beta j prime bar ky j prime xi.
00:52:33.034 - 00:53:24.294, Speaker A: This is precisely what I wanted here. If you compare the first and the last, this is precisely what I want here. And that is why my definition is a good definition. Doesn't matter which representation you use, either for f or for g. At the end of the day, when you do this summation, you the same number. So f in a product with g in w is well defined point. This part, as I said, is the most important part of the, of the proof of Moore's theorem.
00:53:24.294 - 00:54:31.402, Speaker A: Before the break, let's make one observation, one further observation out of this fact that it doesn't depend on the which representation we use for f or g. Given f in w for g, choose ky. This is an element of w too, with the definition we gave. And you see why we gave this definition. What is the inner product of f and g, or better, the same, the inner product of f and k, y and w. Well, the definition is here. This is the sum, except that the second sum, j from one to m, there is just one in j is from one to one.
00:54:31.402 - 00:55:18.214, Speaker A: I mean, we can use any representation that we like when it's already ky by itself is a representation, and ky is equal to one times ky. It's funny, but that's the way it is. We show that it's very defined. So it's sum I from one up to n. That's a representation for f, j from one to one, alpha, beta, j bar. But beta j is just one of them, just beta one k at point y j x PI. So it's, I simplify a I from one to n alpha I.
00:55:18.214 - 00:56:19.804, Speaker A: Beta one is one k, one k y, one c y and x I. And you have seen this combination before. You have seen this combination before. This is precisely the evaluation of f at point, point y. So that in other words, the, the inner product is defined. And it's well defined in such a way that for any f in w and for any point in the set omega f evaluated at point y is equal to inner product of f and k w and k y. And this shouldn't be a surprise.
00:56:19.804 - 00:56:50.504, Speaker A: We want this to happen. This is our goal. At the end of the day, it's not fully realized yet because, well, some properties have to be checked, checked yet that this is an inner product. It's not completely verified yet. That's one thing. And more important than that, w is not complete. So we need to do more work.
00:56:50.504 - 00:57:00.864, Speaker A: And, well, we continue after the break for the rest. It's 58. Ten minute break and we come back.
