00:00:01.120 - 00:00:35.722, Speaker A: So I will talk to you about zero shot human eye coordination in Hanabi and rich ride. And I've put two terms here into brackets, into parentheses, because human eye coordination in Hanabi are examples. But the concepts I'm going to touch upon are extremely general and can be applied above and beyond those settings. And I will use the next 30 minutes to try and convince you that this frontier, human eye coordination, is one of the key new frontiers for machine learning. And I also like to show you some of the progress we've made in this direction. As Richard said, please interrupt. I think communication is an crucial factor of coordinating with others.
00:00:35.722 - 00:01:22.270, Speaker A: So ask questions as much as you can, even if you don't get to entire all of the material, that is fine. So the outline for this talk is as follows. We'll start out by talking about theory of mind and the Hanabi challenge, then introduce the formal concept of zeroshot coordination and our initial method in this area called otherplay. And then in the part two of the talk, I talked about rich Rider, which is our brand new method that allows us to find diverse solutions, including those that can coordinate with other agents at test time. So, as we all know, artificial intelligence has made huge amounts of progress in the last 2030 years. And in particular, we've beaten humans at a lot of games. It doesn't matter if you think about, go about chess or dota or even poker.
00:01:22.270 - 00:02:25.664, Speaker A: In all of these settings, AI agents now beat the best human players. However, there's one thing these games have in common. They're fully adversarial. So this begs the question, this is great progress, but is this really our goal? Is our goal as AI scientists, really to make as many human players as possible? Feel sad about losing two superior AI algorithms? And I would argue, no, this is probably not our ultimate goal as scientists. And in particular, I think what we really want to achieve in the long term is to build AI agents that can work with humans, support them, coordinate with them, and act in teams. And it just so happens that the science underlying cooperation, communication, coordination, is fundamentally different from those tools required for adversarial settings. So we have to move away from trying to beat humans, and instead we have to start cooperating, coordinate with humans, and I'll outline the talk how we can make a step in that direction.
00:02:25.664 - 00:03:02.564, Speaker A: So what actually is human eye coordination? For a start, it's clearly a multi agent problem, because I have at least the human and the eye agent. Secondly, it's fully cooperative, because by definition, I'm trying to help the human as an AI agent. However, commonly things about the environment will be unknown. So this could be the reward function of the human, but it could also be other facets of the environment that are unknown. Therefore it's a partially observable problem. Secondly, clearly we can't pre agree the action to take in every possible state with this human. And that means we have to coordinate in one way or another in a zero short fashion, which I'll formalize later.
00:03:02.564 - 00:03:49.610, Speaker A: These coordination problems require something called theory of mind, and that is the ability to reason or intention action of others. To illustrate this, imagine you're cycling down the road in London and you see a person next to a bus stop raise their hand as a human. Immediately you know that there must be a bus behind you, and that's because you see the person raising their hand. They must be signaling to the bus driver, therefore there's a bus behind me. This ability of interpreting the actions of others when observing them, and also of taking action and informative when observed by others, is what I summarize this theory of mind. And this is one of the crucial facets that are required when we're coordinating, but not when we're competing. More broadly speaking, I think as a community we have to make progress on the underlying fundamental principles of coordination.
00:03:49.610 - 00:04:28.104, Speaker A: So I hope that in some tens of years from now we'll publish the fundamental principles of coordination. How we're going to do this little step in that direction is by looking at reinforcement learning. So that's going to be our framework, and in particular in reinforcement learning, we have an agent that interacts with the environment. The agent receives observations o and rewards r and then takes actions u. After each action, the environment transitions to a new state s prime, and the transition function is normally static. This means that the agent can learn by interacting with the environment over and over again. And the goal of this agent is to maximize the total return per episode written down here j that is discounted across time.
00:04:28.104 - 00:05:10.394, Speaker A: Because we care about machine learning algorithms that can be applied in high dimensional complex settings, we will use deep neural networks to map from observations that the agent receives to distributions over actions. And normally these neural networks are parameterized by weights, theta that we indicate here. Because we care about multi agent learning, clearly one agent is enough. So instead we'll have a number of agents. We'll have the red agent with their own observation, rewards and actions, the blue agent and the green agent. So this is called multi agent learning, or mal for short. What makes this difficult is that the next state does not just condition on the action taken by the red agent, but instead it depends on the actions taken by all of the agents in the environment.
00:05:10.394 - 00:05:51.556, Speaker A: So this is the most general setting where each agent has their own observation function. Your function takes their own action. However, because I'm interested in cooperation, coordination, I will restrict myself to so called decentralized partial observer Markov decision processes, or decpomMdps, in which the reward functions between all these agents are identical. That means all agents are trying to operate together to maximize the shared team reward. Now we've outlined what we're trying to achieve. We have our method framework, reinforcement learning. Now we need a problem setting, and luckily there's a fantastic game that can be used to test all of these ideas, and that's called Hanabi in particular.
00:05:51.556 - 00:06:30.008, Speaker A: If you haven't heard of Hanabi before, it's something we introduce as a new challenge for the machine learning community, because instead of being adversarial, it's a team game. So all of the members in the team are trying to cooperate together and maximize the joint team reward. So it's fully cooperative and partially observable, and it's entirely focused on theory of mind. So it's a perfect test bed here. And as a twist, as you can see in this picture, when I am playing the team with you, I can't see my own cards. So to figure out what I can do, what I need to do next in Hanabi, I have to interpret your actions and I have to communicate to you through my actions. I'll give you a very brief rundown now of how Hanabi works.
00:06:30.008 - 00:07:04.878, Speaker A: In particular, it's a type of cooperative counting, so it's like a team based solitaire, where we're trying to play these cards in a legal order, starting with the one for each of the colors and ending with a five. So I can see my partner's cards. I don't know my own cards, and then the partner gives me a hint so the partner can point out sets of cards. So for example, all these cards are ones. Question is, what should I do now? Clearly, all the ones at this point are playable, so I can just play any of the ones. And indeed that was a legal move. Secondly, though the partner says this card is red and a priori, this card being red does not tell me anything about the world.
00:07:04.878 - 00:07:50.630, Speaker A: However, as a human who has theory of mind, I can ask myself, why would that person point out that this card is red? Clearly it must be useful. Maybe this card is playable, and indeed I can now play that second card. It turned out to be the red tooth so I think this illustrates a little bit of the reason it needs to happen. In Hanabi, it's all about theory of mind, understanding the actions of others. The issue is that in the community, this is usually treated as a self play problem setting. And what this means is that we're optimizing a team of AI agents during training, and then require that this team does well as the same team at test time. And this is a great idea in two player zero sum settings, where finding any Nash equilibrium at training time is sufficient to do well at test time, because Nash equilibria interchangeable.
00:07:50.630 - 00:08:27.382, Speaker A: In contrast, in cooperative settings, they are perfect self play policies which do not cooperate well with any other agent in the world. And this is shown here. So in particular, what we've done is we've trained eleven different self play policies and we're now evaluating them. In cross play. That means we take agent number one and we test it with all other independently trained agents. So this is agent 01234 up to number eleven. And as you can see, agent number one obtains highest scores when paired with itself, but does not play well with any other copy of itself.
00:08:27.382 - 00:09:07.014, Speaker A: So in an independently trained version. And that's because these agents in Hanabi learn arbitrary conventions. In particular, they use things like hinting red or yellow to mean to play the newest card completely independent of the actual meaning of the information provided in the game. And similarly, they use white and blue to mean that the latest card should be discarded. So these conventions are extremely efficient, but entirely different from human conventions. As you saw before, humans would typically point to a certain card using color or rank, and that card is playable in the conventions developed by our AI agents. Red does not have anything to do with the actual card being played.
00:09:07.014 - 00:09:44.386, Speaker A: Red, no matter what card is being hinted at, a different card is playable. And the problem is that those are arbitrary conventions, a priority. There's no reason why you would use red to mean play rather than using blue to mean play. So the result of this is that on the diagonal, the self play score is around 24, but the cross play score is really low. It's around three points in a setting. So that's the problem. If we can't coordinate with independently trained copies of ourselves, how are we expected to coordinate with, with humans at test time? We address this issue by introducing new setting called zero shot coordination.
00:09:44.386 - 00:10:47.084, Speaker A: In zero shot coordination, two teams are trained independently, but have to then do well when matched together in cross play. So we're training the argentinian team and the brazilian team independently. But at test time, we're requiring that those two teams work well together. And the idea is that if I can work well with an independently trained team, I can might, I might also be able to work well with a human team at test time. And our question we're asking is, if the coaches can agree on a training strategy before training starts, what should this strategy be to maximize the expected cross play score? What's interesting here is that we can verify easily after training that we failed at achieving good cross play. However, we're not allowed to maximize the cross play objective during training because by definition, it can only be evaluated at the end of training. So the brazilian and argentinian team are separated by a wall during training, when they're being optimized, and only once training is finished, can we then ask the question, how did they do? And there's no known algorithm that maximizes cost play in general.
00:10:47.084 - 00:11:24.504, Speaker A: I will next present other play. Our recent paper that was done with a set of fantastic collaborators across Facebook guide research and was published at ICM in 2020. So here we formalize this zero coordination setting by looking at independent AI designers that have to construct agents for a set of x unknown decommissioned piece. And the crucial thing is they train independently and get tested together in a, in a cosplay setting without being able to coordinate any further. The question is, what should this decision making will be that they use?
00:11:25.324 - 00:11:26.580, Speaker B: Can I ask a question, Jacob?
00:11:26.652 - 00:11:27.516, Speaker A: Yes, please.
00:11:27.700 - 00:11:35.744, Speaker B: Can you say a little bit more about what they do know about each other? How are they different than each other? And what do they know about how they're the same or different?
00:11:36.284 - 00:12:26.986, Speaker A: So you and I can exactly agree on a learning rule, but then we have to run that learning rule on independent sets of the problem settings we're given, right? So you and me get access to the same environments, but once we've been given access, we no longer can communicate up to the point where we deploy our policies. So you can think about the setting where we get the same simulator, we get exactly the same problem setting, but we have to train our policies independently, and then they get evaluated in cosplay. But we can agree on a learning rule. The question is, what should that learning will be? Let me maybe illustrate this here. Here's a simple problem instance of this task. You and the random stranger have to coordinate by choosing one of the ten levers shown on the left hand side. And if you pick the same lever, you get the reward.
00:12:26.986 - 00:12:41.346, Speaker A: Both of us get the reward that's shown next to the lever. However, otherwise, reward is zero. So these nine levers pay 1.0 this lever pays 0.9 and we have a single attempt. And the task description is common knowledge. So everyone knows that.
00:12:41.346 - 00:12:54.264, Speaker A: Everyone knows the task description at infinity. And the question is, what do you do in this setting? Right? What lever do you play at this point? Does anyone have an answer? What would you play if you faced with this problem?
00:13:06.964 - 00:13:13.704, Speaker B: There's some answers in the chat. I don't know if you can see it. A couple of saying 9.9.
00:13:13.844 - 00:13:32.984, Speaker A: Great, so that's exactly what I would play. And how did we get there? So in particular, what humans can do is humans can reason through the decision making process. We don't just find a solution, we reason over the solutions. So here I replace myself. I imagine I have an imaginary twin that can go through the same reasoning process as me. So imagine that we both pick an arbitrary 1.0 lever.
00:13:32.984 - 00:13:49.614, Speaker A: Clearly we can't coordinate because we pick different ones. Instead. What happened if my max return and myself both picked 0.9 lever? So we have this kind of reasoning ability. And the question is, how can we bring this kind of reasoning ability to our AI agents? Because software agents would pick the 1.01 of the 1.0 liters.
00:13:49.614 - 00:14:34.500, Speaker A: And the key idea here is that we are going to use symmetries. So symmetries are properties of the decommission. These are functions that map from states to states, from actions to actions, observations to observations, but they leave the entire, the transition tree of the decommodp unchanged. So the transition function, word function, observation function, look exactly identical under the permutation of the different nodes of the tree. And therefore the symmetries can also be applied to policies. In particular, because I can apply it to the states and the actions, I can think of a policy PI prime. That's the symmetry applied to a policy by just having the policy act and observe in this permuted way.
00:14:34.500 - 00:15:23.680, Speaker A: Now this is very abstract, so let me illustrate this a little bit with a simple example where we have a robot that exists in the 2d world based on an x and y coordinate, and can move up down, left right, because the goal is in the middle. This means that there is an axis of symmetry. We can invert the Y and the x axis. However, if we just invert the y axis and leave the action space unchanged, we no longer have an equivalence. Because moving upwards, the green arrow when in the top right corner moves me away from the goal, but moving upwards in the bottom right corner moves me towards the goal. So to establish equivalence, I have to actually both invert the axis and have to exchange the yellow and green action. Taking the yellow action in the bottom right is equivalent to taking the green action in the top right.
00:15:23.680 - 00:16:22.234, Speaker A: And that's why symmetries in general act in both the state space observations and the action space. Once we have those symmetries, we can introduce equivalents, and in particular, we can define a new objective that's no longer self play, but instead quote our other play objective. So in self play, we could optimize the joint policy PI by arbitrary optimizing the two sides of the policy PI one and PI two instead. In other play, I require that I am robust to arbitrary symmetries being broken differently for myself. So this is equivalent to saying that I can only specify my imaginary twin, my partner, up to the equivalence class. If I think PI two is a great policy, I have to be robust to my partner playing Phi of PI two at any position, at any point in time. And what this does is that it maximizes, it finds the policy class of equivalent policies that maximize the expected return when two agents play independent policies from that equivalence class.
00:16:22.234 - 00:16:50.232, Speaker A: And that's the other player objective. And crucially, this can be implemented on top of any arbitrary deep URL algorithm, once you know the symmetries. Hi Jacob, can I ask a question please? What if you don't know the symmetries? I mean, that's very hard to discover in most graphical models. This is a fantastic question, Chris. We will get to that in the second part of the presentation if we get there. So that's the rich writing part of this. And I will say we haven't solved it.
00:16:50.232 - 00:17:18.544, Speaker A: This is still somewhat of an open problem, which is why I said that we don't have a general algorithm that solves this problem setting. But I think we have to understand the fundamentals here. I think it's pretty early days, really good question. Thank you. So what does other play look like pictorially? In particular, we still have two different teams training independently, the brazilian team and the argentinian team. However, now at training time, each agent gets trained with a randomly permuted version of itself at every episode. This is indicated by this five acting on the second half of the team.
00:17:18.544 - 00:18:06.334, Speaker A: And then they still get tested in cross play by matching the two halves. What are the symmetries in Hanabi? While in general this is hard to find out. In Hanabi, the symmetries are the colors. All the colors are equivalent, so the arbitrary labels, and we can permute them. So what does other play do in Hanabi? Let's imagine one example here where we have Phi exchanging the yellow and the white colors, one of the agent observes the world where the first card is a yellow one, and the agent can hint to the partner by saying that their first card is yellow, pointing to that card, and then that yellow two here is playable under the permuted view that is every time different. For one of the agents. The white one is playable because we exchange yellow and white, and the agent can still hint to that first card by hinting to the color, only that this time the color is white.
00:18:06.334 - 00:18:33.326, Speaker A: So I now have to hint to this by saying this first card is white. And what this means, I have to find policies that are robust to symmetry breaking. So what does this actually do? Probably, as you expected in the leave a game, it will now find the human like policy. The same 0.9 that the people on this meeting actually discovered is what other play will learn in this game. What does that, what does that mean? It means that at self play, at training time, self play actually does better. It gets 1.0
00:18:33.326 - 00:19:00.494, Speaker A: in expectation, but at test time, other players get 0.90.9 points, while soft play gets 0.11 points in expectation. In Hanab, we also increased the cross play by a lot. So, in particular, if we now look at the comparison between self play and other play, we add 13 points for the vanilla method. And even if we add more regularization methods, we still add four points. Most crucially, though, all of this actually leads to much more human like policies.
00:19:00.494 - 00:19:34.036, Speaker A: So here we show the other playbot performance on the y axis and the self playbot on the x axis. Each of the blue dots is one set of cards, one deck. And what you can see is that for the vast majority of cases, the other playbot does much, much better when paired with a human than the self playbot. In particular, we get around 16 points here when paired with the human, compared to around six points for the self playbots. And these humans were told nothing. It's a single game of an army. What it shows you is that other play gives us an initial step towards understanding the fundamental principles of coordination.
00:19:34.036 - 00:20:21.244, Speaker A: However, as Chris pointed out, one of the challenges is that we had to have access to the symmetries and other play per se. It's only applicable when these symmetries are present. So a big open question is, can we learn zero coordination policies without requiring prior access to the symmetries? I will next present an initial step in that direction that's called rich rider, which is a general method for finding diverse solutions. By following the eigenvectors of the Hessian and this sounds extremely unrelated, but I hope I can make the connection to the zero shock coordination problem at the end of this. So the underlying idea is that when a measure becomes a target, it ceases to be a good measure. That's like our zero shock coordination objective. If we train directly for that objective, then it becomes soft and we've nullified that objective itself.
00:20:21.244 - 00:21:23.100, Speaker A: So let's take a bit of a step back. How do we normalize loss functions? How do we normally optimize loss functions? We follow the gradient if we're doing deep learning, and this is extremely cheap to compute locally reduced loss, great track record. However, we don't get any insight into what kind of solution was found using SGD, and also it will bias us towards those solutions that are easy to learn, and those are typically associated with a high curvature when starting optimization process. So the first question is, how can we find the diversity of different types of solution using optimization? And our answer is that rather than following the gradient, which gives you a single direction, we should follow the eigenvectors of the Hessian, which we call ridges, and in particular those that have negative curvature when starting at a saddle. So here's some cross surface and you have a saddle, and here's the first eigenvector of the Hessian, and here's the second eigenvector of the Hessian. They both obviously comply with the eigenvalue equation. And what's interesting here is because the Hessian h is a function of theta.
00:21:23.100 - 00:21:58.244, Speaker A: That means that the eigenvectors and eigenvalues are implicitly also functions of theta. You might wonder, why is that a good idea to follow an eigenvector of the Hessian? And the key insight is that this is a locally loss reducing set of directions. So in particular, imagine we started saddle, the gradient is locally zero. We take a step along the first eigenvector direction, get to some new set of parameters, and then we calculate the updated. So this eigenvector pointed in this direction. We take a small step, so we have a smooth continuation of this ion vector. We can find the new ion vector at this new parameter set, theta t plus one.
00:21:58.244 - 00:23:09.184, Speaker A: And we can now follow this eigenvector again, that second red step. The question is what happened to our loss function? And to understand this, we first have to compute the gradient at our updated parameter step and to first order, because this was an eigenvector of the Hessian, the grid is nothing but minus alpha lambda I times the eigenvector itself. And this means that to first order after the second step, when we move in this red arrow here, you can see that the inner product between the red arrow and the green arrow is going to be lambda times alpha squared times the inner product between these two eigenvectors. So what this means is that all of the different eigenvectors with negative eigenvalues are locally loss producing directions, as long as the eigenvectors are changing smoothly, slowly, because then the inner product between these two eigenvectors is greater than zero. And that means that rich writing our algorithm, rather than having one direction, which is the gradient, it has a set of orthogonal loss reducing directions. What we then do is that we start at a specific location in parameter space that I don't have time to explain, and we branch out. We're turning optimization into search.
00:23:09.184 - 00:23:47.186, Speaker A: We branch out and follow all of the different ion vectors, negative curvature at the different ridges, always updating the rigid every time step, and then repeat the branching process when we stop making progress. I know, reducing the loss function anymore after a while. Crucially, while we're doing this, we can index all the different solutions by the ridges that were followed so far. So here we follow the first eigenvector, so that's a zero there, and then follow the first one again. So this is theta zero, zero that solution. And when we're done, we have lots of leaves of the tree, which are the solutions to the problem, and they are indexed by the fingerprint I how they were produced. Now, this is expensive, there's nothing we can do about it.
00:23:47.186 - 00:24:22.082, Speaker A: It's always going to be more expensive. And we also have to compute the full Hessian, the spectrum of the Hessian. But we have ways to get around this using approximate versions of rich writing and using hessian vector products. So it's quite scalable in fact, and you have to evaluate many branches, but we can use the fact that symmetries need to repeat that values to reduce number branches that have to be explored in this setting. And obviously we don't have a shown track record yet because it's brand new. So let's illustrate what this does in a toy problem. So here's a cost surface where we're trying to get to the dark blue regions of negative loss, and we're starting near the center.
00:24:22.082 - 00:25:03.702, Speaker A: And crucially, what Sud is going to do stochastic in descent is going to fall down the steepest curvature direction and end up in this local optima at this side and that side. So these are not good values, but they are where SGD would end up. And indeed rich riding will also get there with the first with the steepest ridges. So the blue and the green both end up in those same places as SGD would. But if you follow the less steep direction, the first, the second eyeing vector in one direction actually curves around. You can see how we're following this eigenvector here, and then we end up getting to this negative loss region outside the plot. And similarly the orange one actually curves around, then stops making progress.
00:25:03.702 - 00:25:49.024, Speaker A: And then we branch. And at the branch the steepest directional leads us all the way to the loss reducing region. So hopefully this illustrates a little bit of the intuition. Now, what I promised was that we can actually use this for addressing the zero shock coordination issues when we're not given access to the symmetries. And the main insight here is that if we have a coordination problem, and there are repeated solutions that are equivalences, then those symmetries and equivalences will lead to repeated eigenvalues. And crucially, when we have repeated eigenvalues, the ordering of those different eigenvectors, the ridges across repeated eigenvalues, is inconsistent across different runs. It's arbitrary symmetry breaking that happens when you order the eigen spaces.
00:25:49.024 - 00:26:23.750, Speaker A: In contrast, we can coordinate reliably on unique directions. And this means that in this specific setting, we can use rich rider to solve zero shell coordination when not giving access to the symmetries of the problem. So what's shown here is we run the rich writing algorithm some number of times and we look at the outcome. That is what happens if you follow the steepest ridge up to the 9th eigenvector. And what you can see is there's one direction that's very consistent. We always obtain the same solution 0.6 solution on the first bridge.
00:26:23.750 - 00:27:05.384, Speaker A: In contrast, the solution is associated with the 1.0 subspace. The eigenspace are arbitrarily shuffled across different runs, and this means that they are the expected payout, because now when we're doing cross pay between the different ridges is much lower than it is for the consistent direction. And if both agents, if we can agree on this protocol, then we can agree during training that we will all find the same consistent direction during the optimization process. So we can solve zero shell coordination without having access to the symmetries. So this is very encouraging. Obviously it's still early days, but it shows you that there are some underlying fundamental principles that can be used to address zero shock coordination.
00:27:05.384 - 00:27:37.798, Speaker A: In the paper we do a lot more. I don't have time to talk about this at this point. But we show that there is a connection between supervised learning and generalization, and supervised learning and zero shot coordination. In multi agent problems, we also look at diversity. We look at, in reinforcement learning by looking at a tabular problem, look at diversity for, for solutions in MNIST, and we do some work on out of distribution generalization. We also have some, as I said, approximate algorithms for this that are quite scalable, and some nice theory. Either way, I think this is just very early days for both social coordination and for rich writing.
00:27:37.798 - 00:27:59.664, Speaker A: And there's a huge amount of work to do, which is why I think it's an exciting time to be looking at this. And hopefully in the future we can address a lot of the tasks in this world. And who knows, maybe we'll find all the solutions to all of the problems, and ideally without using all of the GPU's. So with that, I would like to thank you all for listening and open the floor to any more questions.
00:28:03.524 - 00:28:08.824, Speaker B: Okay, so that's great. Thanks so much, that was excellent. We have time for a couple questions.
00:28:10.604 - 00:28:28.760, Speaker C: So, this is Baskar speaking. I have a question. So you explained an algorithm, but I was wondering if there's an actual optimization problem that justified essentially this algorithm in the end. So, is it possible to formulate this.
00:28:28.792 - 00:28:41.104, Speaker A: As an optimization problem, this being rich writer or other play? So we've covered, I've tried to put two things into half an hour. So, which part of the presentation, when you say this.
00:28:43.964 - 00:28:57.276, Speaker C: I guess you're talking about coordination. Right. And then I thought that I was expecting to see an optimization problem, and after that, to see an algorithm that would solve this optimization problem. But you, you seem to have been straight into, into an algorithm.
00:28:57.420 - 00:28:57.748, Speaker A: Yeah.
00:28:57.796 - 00:29:01.540, Speaker C: And I suspect optimization problem.
00:29:01.692 - 00:29:30.150, Speaker A: Yeah. So, so far, I have a paper coming out that formalized optimization problem more rigorously, mathematically. So far, we only have a description. So this is not a mathematical formulation yet of the, of the problem. Right. So we have a description of it, which is you have independent AI designers having to agree on an algorithm, and then training independently on the problem instances, and then evaluating afterwards in cosplay. Right.
00:29:30.150 - 00:30:11.904, Speaker A: So we can put that, that into. Mathematician in particular, what this comes down to is you can imagine that you're training on a problem instance without being given access to consistent labels for states and actions and observations. So all you're given is the structure of the deck form DP. You and me are given the structure of the DEC form DP, but we have to independently optimize our policies and then evaluate and cross play and the question is, what should be optimization algorithm that we use to maximize the cross based score? And as I said so far, this was in the paper. We actually do this in a verbal introduction. We don't have a mathematically rigorous formulation of this yet, but I have a paper coming out soon that does that.
00:30:15.404 - 00:30:16.652, Speaker C: Okay, thank you.
00:30:16.788 - 00:30:36.352, Speaker A: And then other play, actually, I can also tell you this now, other play with a bit of a fix is a Nash equilibrium of that game where you and me have to decide on a learning algorithm together. So, thank you.
00:30:36.448 - 00:30:47.084, Speaker B: There's a question that showed up in the chat. So it says from Jonah, do you have any analysis of how other play performs for one shot or K shot coordination in addition to just zero shot?
00:30:48.024 - 00:31:47.588, Speaker A: Well, so I think when I say zero shot, this might be a little bit misleading, but K shot is a special instance of zero coordination, because my episodes are extensive. So nothing says, I still think the right way to formulate K shot is by thinking about extensive episodes that take K steps, because that's the general framing. So another play, that's great. I mean, Hanabi, you know, for a start, has many times that we have 80 time steps, so we should adapt to each other, right? So I think the correct. I mean, personally, I think the best way to formalize what we actually want in kshot, what assumptions make is that it's the zero shot assumption, but over an extensive episode of case steps, and then other players will, in turn do the right thing. But there are also a lot of open questions. So this is maybe saying, I think this is, in general, the right formalism.
00:31:47.588 - 00:32:02.804, Speaker A: I think there's still open questions around what the right solution mechanism is. But it does. I mean, Hanabi has 80 steps, so that gives you k equals 80, basically, because you can adapt to each other across the game.
