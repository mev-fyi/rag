00:00:00.160 - 00:01:30.622, Speaker A: The complexity issues concerning some of the problems we're looking at, whether we want to think about them as partial matrix completion problems, or we want to think about them as finding realizations. So, completion of a partial euclidean distance matrix, by the definition of euclidean distance matrix, corresponds to finding whether exists a euclidean realization for the partial set of entries that are given. And those realizations will automatically provide you at least one set of distances for the remaining entries of the matrix. And these are variations depending on whether the rank of the matrix is specified, or you're only told that it's an euclidean distance matrix. These are different questions, but they all fall under the general question of finding realizations or determining the existence of realizations. And the kinds of problems that we care about, as we have already talked about, is whether the number of realizations is finite, whether there in general exists a realization, whether it's finite, whether it's a, whether there's a single realization. And then you might also care about whether the set of realizations is what it looks like.
00:01:30.622 - 00:02:42.702, Speaker A: So what is the dimension of the set of realizations, the solutions to this partial matrix completion problem, or distance given set of distances, whether a graph can be realized with those given set of distances. Whichever that problem is, you might care about what are the set of realizations. Is it usually called loosely conduct a realization space or a configuration space, whether it's connected, whether it's convex under some parameterization, whether it has singularities, and all of these questions, all relating to the realizability or the set of realizations of this constraint system. And so far we have sort of restricted ourselves almost entirely to distances. Later on, we'll consider more general geometric constraints, which also you can treat in. I mean, of course, you don't have this close connection with the EDM cone, or in fact any kind of normed cone, because it's not a distance at all. But we do have polynomials, just like we have distance polynomials.
00:02:42.702 - 00:03:58.380, Speaker A: We have polynomials associated with these other constraints, although we don't have things like Kalimanger conditions and so forth. We could instead talk about other such conditions, and we'll talk about other such equivalences, something like this Cayleymanger syzygys. There'll be other syzygies. So again, we have this two ways of thinking about the problem, either solving the set of polynomial equations that represents the constraints, or solving this other polynomial system, such as the Kalimanger system, to directly come up with the distances. So either way, you can think about it. And one of the key things, as we have already seen, is understanding dependences between these constraints. And so the idea that realize existence and dimension and all of these things, including notions of dependence and independence, can be dealt with in a generic way based on the graph of the constraint system rather than the actual values of the field over which we are looking at these polynomials or constraints.
00:03:58.380 - 00:05:03.700, Speaker A: So the actual distances in the first case. So that's the other sort of, sort of cross cutting theme, the cross cutting theme of genericity, and the cross cutting theme of detecting dependences between constraints, both generically, non generically. So we will think about, these are the general types of questions we are interested in, and all of them have algorithmic variants, in fact. So the question is, you're given a distance partial distance matrix, find a realization or completion of this distance matrix, and that's an algorithmic problem. And we can talk about the complexity of this algorithmic problem. We can talk about other notions of complexity, such as the complexity of this algebraic set s. We could ask, you know what, like we said, in the case of three decomposable graphs, we actually have a quadratically radically solvable system.
00:05:03.700 - 00:05:51.518, Speaker A: So the solutions actually belong in some field power of field extensions. There could be other Sort of algebraic Properties of this s that are of interest. So that's another notion of complexity, not a complex time or space complexity of the algorithm for finding the realization, but a kind of complexity measure of the set of solutions. So we'll talk about that as well. So these are the questions. So essentially, by the way, this is taken from the handbook, and now that we've finished the tour, we can sort of focus on specific chapters. I'm just going to order my lectures in some way.
00:05:51.518 - 00:06:49.604, Speaker A: I mean, the order is just pretty random at this point. I mean, it's a partial order, really, and the linear ordering of the partial order is my choice. It could have been done differently, but this is how I'm choosing to do it. I'm kind of hoping that some of the, we are going to be relying now more and more on what Tony talks about in his class, specifically notions of the d dimensional rigidity, metroid and different types of metroids. So I hope that he would have advanced to the point where we can then talk about those things. So the key thing that, as I said, is going to run through the whole thing is the computational complexity of these problems. So we have to start out with sort of a baseline understanding of how difficult these problems are.
00:06:49.604 - 00:07:33.382, Speaker A: So if you think about. So for the moment, let's say we're not caring about the complex case and we in most applications, you want real solutions. And so the very basic question is, okay, if you're given a polynomial system, in this case, it's a quadratic system. It's a distance polynomial system that we have seen so far, but it could be some similar polynomials representing geometric constraints. You can think of it as the Cayley manga polynomials, if you want. Basically, those are equivalent systems, even though one seems to have very high degree and the other one quadratic degree. Essentially, solving one gives solutions to the other, so their equivalent system.
00:07:33.382 - 00:08:31.192, Speaker A: So whichever way you think about it, you're looking for real solutions. In the Caylee Manger case, you're actually looking for non negative solutions because the distances are all non negative. So if you want to do this, and the general question of solving polynomial systems over the reals. So, in other words, you've given a set of polynomial systems, and you're asking, does there exist a real solution to this system of equations? If this hadn't been the reals and had been the natural numbers, then it's called solving a polynomial system over the natural numbers is an undecidable problem. So that means there's no computer that can do this in finite amount of time. Let me repeat the problem again. The input is a polynomial system with integer coefficients, let's say.
00:08:31.192 - 00:09:01.474, Speaker A: And not even the natural numbers. I said natural numbers. I meant integers. So you're given integer coefficients, and you're asked if there exists a solution in the integers. Okay, that problem is undecidable. So even the linear version of the problem is called linear diophantine equations, system of equations, problem that's already undecided. So the.
00:09:01.474 - 00:09:46.404, Speaker A: I can find the reference. You can look it up, and you can find the reference. So the question then was, you know, in the beginning of the previous century, whether this would be this. How about over the reals? If you're looking for solutions over the years, does it still have that very difficult, that difficulty? Is it undecidable? And the nice theorem by Tarski basically shows that, you know, solving, asking, answering questions about existence of real solutions. In fact, I'm saying existence. But that's a single existential quantifier. You can extend this to questions about, with, you know, the predicate quantifier free part being a system of polynomial equations.
00:09:46.404 - 00:10:39.648, Speaker A: And then you can have any number of quantifiers for all there exists, for all that exists, whatever, a statement about these polynomials. So, for every value of this variable, there exists a value of this variable such that for every value of this variable, so forth. So that's called the theory of the reals. And I was taking the very simplest version of that, which is just a single existential quantifier. That question over the integers is undecidable. But over the years, the whole thing, the whole theory of real closed fields is decidable. And the first algorithms were given, sort of reasonable algorithms were given by Collins called the cylindrical algebraic decomposition algorithm that actually goes, and if you give an arbitrary quantified formula over the reals, it will actually go and do something and try to find out, find whether it's true or not.
00:10:39.648 - 00:11:23.654, Speaker A: And if it is true and there is a solution, it will give it. However, the complexity of these algorithms is tremendous. Okay, so it's, you know, the best, absolute best in any way of thinking about it you can hope for is NP hard. And it's double exponential. It's not in NP, and it's not in NP. Well, it's in NP in a weird way, because your witness is a real number, right? So that's, let's not get into the arcana of slight differences between complexity classes depending on whether it's over the reals or over the natural numbers. But essentially it's, you can say it's, you're not going to get better than NP.
00:11:23.654 - 00:12:21.024, Speaker A: And if you're actually doing the full thing, the full theory of the reals, it's complete for the class. Exponential space. You know, exponential space is like, if you want to, the time simulation is something like two to the space. So, you know, it's like double exponential time complexity. So these problems are extremely hard. And since we have already seen that finding existence of a solution can be expressed over the complexes and equivalently over the years using the positive and stalins, that's to ideal membership questions. Then the ideal membership, you know, the corresponding algorithms, the best algorithms are grobna basis type algorithms over the, over the complex numbers, those because of this basic underlying inherent complexity, even grobna bases or worried characteristics, that or any or any one of those is going to take a long time.
00:12:21.024 - 00:13:26.034, Speaker A: Okay? So we have to start with that understanding of how difficult these problems are. So then we can proceed to see, you know, whether we can improve things, you know, because we're looking at the generic version. Is it somehow easier? And since we don't really care about the exact value, we just care about the structure of the polynomial system, sort of the combinatorial structure, and not really the actual values, maybe it's a little bit easier, you know, then we can ask, but we have to have this baseline understanding of how long it takes. Okay, so now I'm going to just go over a table here of the complexity, the known complexity for rigidity and realization. And rigidity, as I said, is a very closely related rigid just means that you have finitely many solutions. So it's zero dimensional. And locally, that solution has no nearby solutions.
00:13:26.034 - 00:14:23.908, Speaker A: So you think of that as that realization as being rigid. So the word rigid just basically corresponds to saying there are finitely many realizations. And so the asking whether something is rigid is asking, does there exist finitely many realizations? Realizability means, does there exist a realization and infinitesimal rigidity. Hopefully, you guys have seen in Tony's class, which is in a generic setting, in many cases, using this Asimov Roth result, you can essentially convert, you can linearize the polynomial system with distance constraint system, which is a quadratic system. You can linearize it. And in the generic case, you know, the existence of a solution can be treated as a kind of linear independence of the, of the linearized system. So that's infinitesimal rigidity.
00:14:23.908 - 00:15:18.018, Speaker A: So, um, so what I'm talking about here is, uh, whether something is generically rigid, which means, if you didn't care about the actual entries in the main, in the, in the polynomial system, is it the case that it has finitely many solutions? And it turns out that this is equivalent to infinitesimal rigidity in many cases. Generically speaking, speaking, in rigidity and infinitesimal rigidity are the same. And rigidity means, I'm not talking in generic terms. Rigidity means, I'm actually asking, here's the polynomial system. You tell me whether it has polynomial, I mean, here's a distance, let's say distance constraint system for the moment, and tell me whether it has finitely many solutions. So, in fact, sometimes there are variants of this. So the question would be not that you tell me whether it has finitely many solutions.
00:15:18.018 - 00:16:20.880, Speaker A: I'll even give you a solution if you want, and you tell me whether that solution is locally unique. In other words, there's no other solution in its neighborhood. So, small variance, I'm just giving you the general theory. I mean, general representative questions and their algorithmic complexity, not so much the small variance of it. So, essentially, for d equals two, which means if you have, you know, you're looking for realizations in two dimensions, the generic rigidity problem turns out to be equivalent to the infinite infinitesimal rigidity problem, and that's a polynomial time problem. So you can do it in many, you can think of it in many ways. So the, I don't know if you guys have seen the sparsity metroid definite, the sparsity two v minus, you've seen Lamand's theorem.
00:16:20.880 - 00:17:38.577, Speaker A: So for d equals two, generic rigidity is the property of the graph, and that property has an underlying sparsity condition associated with it, which is the two, three sparsity meteoroid. So you can check for that. So you can check Laman's condition, basically using an adaptation of a very common algorithm that's known in combinatorial optimization to always produce ease in polynomial time integer solutions, which is called the network flow algorithm, and a variant of that network flow algorithm, which is the attractive name called pebble games. Pebble games are what specialization of the network flow algorithm automatically. It's basically a linear programming algorithm for a very special case. And the linear relax, I mean, the relaxation of the integrality constraint for linear programs, which, the integrality constraint is typically necessary to solve a combinat, to think of the problem as a combinatorial meaningful, the problem as giving a combinatorially meaningful solution. That relaxation of the integrality constraint doesn't need to be done because automatically the solution will be integral.
00:17:38.577 - 00:18:54.716, Speaker A: Okay, so, and that's underlying, that's the key thing that's underlying the existence of these polynomial time pebble game algorithms. It's the same theory. It goes far back, I think at least as far back as Edmund's saying that the linear programming polytopes happen to have integral solutions, and that's the reason we have these polynomial time algorithms. And there are some deep connections between these linear programming polytope integral vertices of linear programming polytopes and matroids, which, as we know, are also connected to having greedy algorithms, matroid submodular functions whose optimizations is easy and overall a notion of discrete convexity. So all of these problems have polynomial time algorithms. And then, but however, if you focus on, I mean, if you don't look at the genetic situation and you actually ask, does there exist finitely many solutions to this, then it becomes relatively hard. Even in the situation where you are given a, in fact, the question, you sort of remove the question of finding a solution from the problem.
00:18:54.716 - 00:19:28.014, Speaker A: You say, I give you a solution. Here's a solution. You tell me whether this solution is locally unique, right? So in other words, there's no nearby solutions. And if you write, assert this statement that this solution has no nearby solutions, that's itself a co NP statement. And you can see, you can show that it's a co NP hard problem. Okay, I'm not putting the references here. You know, you'll find the references, I'll maybe later on add some references to this.
00:19:28.014 - 00:20:48.324, Speaker A: Now, as far as realizability goes, which means does there exist a solution? And typically, I don't know of a way to just test existence of a solution without actually finding one. It's typically the case that in algorithms and complexity testing, existence is constructive. So in other words, when you, if you're able to answer the question does that exist? You usually can constructively produce an answer. There are in proofs, when you're writing a proof, you know, there's, you know, the probabilistic method in combinatorics is very famous for showing existence of something, but not necessarily constructing it. But as far as algorithmic, you know, the algorithm goes, you know, any algorithm that sort of determines existence of something, typically with modifications, will, you'll be able to construct that thing. Okay, so in this case, Sax, I mentioned it during a lecture. Sachs has shown that realizability checking existence of realizations is NP hard for the simple reason which we can already see with triangle decomposable graphs.
00:20:48.324 - 00:21:53.848, Speaker A: In fact, it's even NP hard for Hanneberg zero extension graphs. Okay, so it NP hard for everything. And the key idea here, it's a reduction from three sat for those of you who have some idea of complete and how to show NP hardness. So essentially what it's saying is that if you are finding a solution, remember when we find these Henneberg zero solutions or realizations for Henneberg zero extension graphs, or three decomposable graphs at every point, at every stage your placing a point or finding this coordinates of one point. And then if you think of it as a triangularized polynomial system, you're substituting that solution into the next equation, and that equation now becomes a single quadratic equation, one variable, and then you can solve it and so forth. So at every stage you are placing one point. Now that one point, because it's a quadratic, can have two possible solutions if you want to determine existence of a solution, if you happen to choose the wrong path.
00:21:53.848 - 00:22:42.640, Speaker A: So in other words, path that in the future will not have a solution. Then, because some discriminant becomes less than or equal to zero or something vanishes, then you're now stuck. So essentially, in order to find the existing solution, I'm giving you the intuitive reason. How this NP completeness proof goes through is that essentially you would have to, you know, somehow decide which of the two paths to take at each stage. So out of these two to the n, if there are n points to be placed, there are two to the n possible paths you can take and the particular path that you're taking. Once somebody gives you that path, someone tells me, okay, at this stage you choose this solution. At this stage you choose this solution, the left solution, right solution, whatever they just tell you, left right, left right, left right.
00:22:42.640 - 00:23:29.948, Speaker A: That is what is called a witness. That's a proof or certificate that shows that it's in NP. So essentially, once somebody gives you that certificate, you can simply solve the sequence of quadratics and choose the right solution at each stage and keep going. Okay? So therefore it's NP hard realizability. It's closely tied to the fact that there are many possible solutions and you're going to have to chase through them to find one because all of them could eventually, at the last stage, not exist. Right? So that's already for two, and it doesn't really change as we go to higher d. Let's look for a second for higher dimensions.
00:23:29.948 - 00:24:21.712, Speaker A: So if you look at generic rigidity, there is no obvious, because there is no equivalence to a sparsity matriarch. So the equivalence of Lamand's condition where points have two degrees of freedom and there are three euclidean trivial isometries in 2d. So you have this two three count. The corresponding count in three dimensions would be every point has three degrees of freedom and there are six euclidean trivial. So you want to throw those out. So the corresponding count you would expect to be three six, but there is no sparsity matriid for the three six count. It's relatively easy to establish that two three has a sparsity matriarch, but three six does not.
00:24:21.712 - 00:25:30.580, Speaker A: So obviously you're not going to just do the sparsity to do this. So the pebble game type algorithms are all going to be limited. They may work on some special class of graphs, but not in general. Okay, so in general, you're going to deal somehow directly with the rank in the rigidity metroid. And one way to do that is to write the rigidity matrix as a matrix of indeterminates. Maybe I can at this point just draw something. Okay, so, okay, so you can, for example, so you have your rigidity matrix.
00:25:30.580 - 00:26:15.134, Speaker A: I mean, someone has to alert me if what I'm saying doesn't make sense. I think, I'm pretty sure that you've already seen Asimov, roth, you've seen rigidity matrices and so on and so forth. Okay, so you have your rigidity matrix, you know, PI minus PJ. Pj minus PI, whatever, for an edge, ij. And, you know, you, you have probably been looking at individual frameworks. So this PI and PJ are points with actual coordinates. They have actual, some set of coordinates living over the years, let's say.
00:26:15.134 - 00:27:32.998, Speaker A: And now. So you have an actual matrix here, and you can check the rank of this matrix to make sure that there are sufficiently many independent rows to determine infinitesimal rigidity, which is the same as rigidity, because if these points are generic, in the generic case, and that's what we're looking at. So if you have a generic framework, which is a particular solution, so someone gives you a solution, and they're asking, is it locally rigid? Then all you have to do, because in the generic case by Asimov Roth. So you basically implies that generic, generic for generic frameworks, rigidity is if and only if, infinite decimal rigidity, and infinite decimal rigidity is just rank of this, this exact matrix that I've drawn. So you can simply check this rank, and that's, you know, order, whatever, you know, n to the 2.59, whatever, you know. For those of you who know about gaussian elimination or solving polynomial systems, in order to check that you actually get a full upper triangular matrix.
00:27:32.998 - 00:27:48.550, Speaker A: There's so many ways you can check rank. Okay? So this, that's the. So let's just say order n cubed. So that's certainly in polynomial time. Now an alternate. So that's for a particular framework. I'm giving you a particular framework.
00:27:48.550 - 00:28:34.960, Speaker A: Now, that's, in general, not quite easy to determine whether a framework is generic unless somebody asserts to you that it's generic. You know, even testing whether a framework is generic is not straightforward. So the alternative way to do this is just not worry about a framework, okay? And you simply say, I'm going to make these indeterminates. So now you have a polynomial system. So this PI, pj, if it has these j entries, PI, you just give them some name. I mean, give them some variables. X one through x d, x one I through xdi.
00:28:34.960 - 00:29:31.480, Speaker A: And this guy is x one j two, x three, x dj, right? So you give them indeterminate, you give them value. So now you have a polynomial. I mean, the determinantal polynomial determinant is a polynomial. And so basically, I mean, there is small things that I'm glossing over, which is there's going to be some set of columns, I mean, whatever, d plus one, choose two columns or something that you'll have to fix somehow, because otherwise you won't have a square matrix and it's called, there are ways of doing this called pinning, because you know that this, for it to be rigid, you're going to have something like these as number DV minus d plus one. Choose two rows here, but there'll be dv columns here. So somehow you're going to make it a square matrix and check the determinant of polynomial. You have to massage it a little bit.
00:29:31.480 - 00:30:06.846, Speaker A: It's sort of equivalent of saying somehow I have to fix some small number. Remember when the triangle decomposable graph, we always started in order to realize that we started out at the first point at the origin, and we said, okay, the next point, let me put it at zero or d plus one. D, one, two, zero. So in other words, I've fixed these guys, but I know it doesn't matter because eventually my solution, I don't care. All the trivial euclidean motions, rotations and translations, I don't care about. So I just may as well fix them in the beginning and fixing them is not going to affect solvability. And that's the kind of thing.
00:30:06.846 - 00:30:48.864, Speaker A: So you can sort of fix d plus one, choose two of these. So you actually have a square matrix and you have a determinantal polynomial. So that, and you want to check whether that determinant polynomial is not identically zero. That's what genericity is. So that's what generic rigidity is. Generic rigidity is the same as saying that this determinantal polynomial generically rigid if and only if this determinant polynomial is not identically zero. So these are the edges of your graph, and for the vertices, you put, I'm assuming everybody has seen the rigidity matrix.
00:30:48.864 - 00:32:00.354, Speaker A: Now we make the rigidity matrix of indeterminates, and you want to check whether this determinant polynomial is not identically zero. If it is not identically zero, then you have something that generically has finitely many solutions. Now how do you check whether a determinantal polynomial is not identically zero? So you do that by using very simple algorithm. You simply insert some random values for the indeterminates, random assignment of values to the indeterminates. Or in other words, you're choosing random points and there's actually a formal result that says, you know, it's intuitively very clear, right? If this is not identically zero, then the polynomial is going, is going to be almost everywhere non zero, except for a few roots. The number of roots is very small compared to the number of non roots. So if this is not identically zero, a random assignment of values is going to give you something that's not zero.
00:32:00.354 - 00:32:39.446, Speaker A: Determinant polynomial evaluated at this random assignment is going to evaluate to something that's not zero. And if it's identically zero, it's going to be zero everywhere. So it'll relate to zero. So if the polynomial is not identically zero, with high probability, you will correctly give the right answer. You could be unlucky and somehow end up in one of those crazy zeros, but that's only unlucky. So your chances of making an error if the determinant polynomial is not identically zero is very small. And if the determinant polynomial is identically zero, you'll never make an error.
00:32:39.446 - 00:33:34.174, Speaker A: So that is called that class. And checking once that value is given, putting it in and checking the determinant is only going to take you this much time. Okay, so that class of algorithms, which are with a random choice of witness or certificate, take polynomial time to verify, is called class RP. So that's basically, I'm spending a little more time on this class than the other ones, because NP and P and all that are very common classes, and this is a little less common. So, so let's go back. So, essentially, this generic rigidity is in RP, which is a sub class of NP. NP is in relation to RP.
00:33:34.174 - 00:34:07.244, Speaker A: You know, you're not. So you don't have a randomized way of picking the witness. You actually have to search through or something if you want a deterministic algorithm. So in this case, we're lucky that a random witness or a random choice will actually answer the question correctly with high probability. So that's the class RPG. So this is randomized polynomial, and then rigidity and all that don't really change exactly the same whether D equals two or higher D. And in terms of.
00:34:07.244 - 00:34:41.843, Speaker A: You can ignore this for now. Just think of it as Rp. Okay, so for specific case of D equals three, we know a few things. And, I mean, I haven't probably listed everything we know. There are some special cases we know. You know, for example, in G squared graphs, graphs that have the property that you start out with the graph, and every time there are, there is a path of two edges, you add an edge there. So in other words, you take the incidence matrix of the original graph and you square it.
00:34:41.843 - 00:35:30.474, Speaker A: That's why it's called a g squared graph. So if a g squared graphs, for example, it's known that the three six actually gives you a matroid. And it's the rigidity metroid and then there's similar situations for other graphs that arise from other types of systems, like body hinge systems and so forth. You can sort of convert them into your distance constraint graphs. And those graphs also have these properties. So there are whole slew of special classes where you can essentially follow the same plan of using a sparsity metroid and trying some kind of network flow based algorithms. I have to say, the network flow based algorithms are not going to be simple payable games anymore.
00:35:30.474 - 00:36:10.816, Speaker A: They're going to be a little more complex here. And some of those questions are still open. So you have, without these conditions. So if you're actually looking at general graphs, we have some bounds on the rank of the rigidity metroid. So that gives you sufficiency conditions. So certain tests on the graph that will sort of tell you that when the graph is, if that condition fails, then the graph is definitely not rigid. However, if the condition passes, you don't quite know whether the graph is rigid or not.
00:36:10.816 - 00:36:56.236, Speaker A: Something else might happen that will create a dependence against, come to these dependences, that there is a dependence between the edges that you do not detect in these simple ways. So the upper bound on rank is essentially saying the same thing. So there's a way in which I can upper bound the rank. So if I compute the rank and it somehow turns out to be more than sort of, so that upper bounding gives you a one sided error. So in other words, if the algorithm says yes, the thing is rigid, you may be wrong. But if the algorithm says no, the thing is not rigid, then you're right, always right. So this is for the general graph case.
00:36:56.236 - 00:37:40.186, Speaker A: I mean, the other cases that I mentioned before, g squared and all that, those are different, different style. We also know that some, you know, for example, one simple upper bound is, you know, it wasn't easy to prove. But simple upper bound is that the upper, the rank is almost, is always less than or equal to the size of every maximum three six power subgraph. This is obvious in 2D. But here the, and we know that in 2D because it's a sparsity matriarch. Every two three, maximal two three, six sparse subgraph, which is maximal independent sets of this matriarch, they're all of the same size here, that's not the case, it's not a matrix. So they could all have different sizes.
00:37:40.186 - 00:38:14.852, Speaker A: So what this is saying is that you can take the minimum, one of those minimum sized, one of the maximal three six bar subgraphs, and the rank of this rigidity metroid is guaranteed, of this graph is guaranteed to be less than or equal to that. Okay, so there's some. So this gives you, like I said, a one sided error test. They can be arbitrarily bad, I have to say. So the bounds can be arbitrarily bad. And so that means you could be making errors in all kinds of graphs. So this is for the general graphs.
00:38:14.852 - 00:39:13.304, Speaker A: Everything else is the same. I just wanted to expand a little bit more on the special cases for the D equals two concerning realizability, where we said it's NP hard or NPR complete NP super R just means if you allow me a witness that's in the reels, then it's in NP. That's all I'm saying here. Okay, so special cases we talked about yesterday. If it turns out that you don't have to explicitly give a rank condition and simply say that the euclidean distance matrix is negative, semi definite, and nothing else, you don't mention anything else. And it so turns out that just those constraints are sufficient to uniquely specify a completion or uniquely specify a realization. Completion, realization, same thing, no difference.
00:39:13.304 - 00:40:04.708, Speaker A: And in that case, you can use semi definite programming to. That's how, because that's the only constraint. There's no extra rank constraint. And so if you happen to know that the set of distances that you have been given represents a universally rigid framework or universally rigid realization, then you have a polynomial time algorithm to find it so. And special examples of those are this D plus one laps or augmented triangle decomposable graphs. They're all special cases of these universally rigid ones. And in fact, they're what are called generically universally rigid, because in general, universal rigidity is not a generic property, which means you might have some framework neighborhoods of the same set of distances which are universally rigid and others which are not.
00:40:04.708 - 00:41:11.640, Speaker A: But this class of graphs is known to not have that aberrant property. In other words, every generic framework is universally rigid or not. Okay, so I mean, it's genetically universally rigid. This graph either. If it has a solution, it has a single solution, period. Then we can also extend this, and this will take us to the very first topic we're going to discuss, which is doctor plans is a general approach to realizing graphs by decomposing them into rigid subgraphs or global rigid subgraphs, depending on what you care about. If you care about just, you don't really care about the problem that causes this NP hardness, which is that you don't know which branch of the solution to search through in order to ensure there's a realization at the end but if you're, for example, somebody told you which branch to search through, like we talked about before, someone said, choose the right one, right realization of these two realizations here, the left one over here, so forth.
00:41:11.640 - 00:42:16.840, Speaker A: It's sometimes called a type of a realization or a chirality or orientation. You'll hear all of these. So if someone gives you a type of the realization, then that's equivalent, more or less, to having this augmentation, this extra edge that this d plus one layeration graphs have, or this augmented triangle decomposable graphs have, which have the effect of making them globally rigid. Okay, so essentially, it's one way or the other. If you can, at any given point in this solution process, decide on one path to take in whichever way. If you do that, and you have what is called a constant size Dr. Plan, then you actually have a polynomial time algorithm, the only then, the only still roadblock that's left, which is determining whether the graph is rigid, whether the graph that you happen to be decomposing into, whether those are rigid.
00:42:16.840 - 00:43:36.352, Speaker A: And you can go get around this in two ways. You can say, well, I can say that there's a sparsity metroid that rejects rigidity for this, which is in the two dimensional case. Or you can simply say, well, there's an independently interesting question as to whether there is a constant size doctor plan. So let's just assume I have a test for rigidity. Someone has an oracle that will simply answer correctly the question whether something is rigid or not, and then ask for the complexity of doing the decomposition, and after doing the decomposition, actually solving those polynomial systems that come in this block triangular form of the decomposition. And how long does that take? Okay, so you have these basic complexity ideas, and as I said, this is not all the variants of the problem, but you need to have a baseline understanding of how difficult these things are. So if you can have a rule of thumb, you have a sort of starting point so that when you're actually looking at a specific algorithm for a specific problem, you have a basic expectation of, you know, you can't possibly do better than this, right? Okay, so that, so the remainder of the course, I am planning to use the, you know, handbook as a general guideline.
00:43:36.352 - 00:44:30.434, Speaker A: As I said, the handbook is written in a handbook fashion. So it doesn't have proofs, but it does have all the references, pretty much all the references you need. So we will be going into those references. So when I say I'm going to talk about a particular chapter, I just mean the general topics of that chapter and maybe a few other topics that are referred to in that chapter. And we will do some proofs and get into more detail on those. So this is chapter six is what we will start out, six and seven, pretty much, which is the idea of extending, extending this triangle decomposable graph way of realizing algorithms, which is also the triangularization, ensuring quadratics at each stage. Maybe not a single quadratic, but a constant number of quadratics at each stage, independent of the size of the problem.
00:44:30.434 - 00:45:19.704, Speaker A: Constant independent of the size of the problem. So we'll talk about that first, and that will take us, you know, there's a lot of theory there with not just the actual decomposition into rigid, but also, you know, detecting sparsity. I'm not sure if Tony has talked about the pebble game. I think he said he was going to let me do that so I can talk about network flow pebble game. Detecting the sparse subgraphs, I mean sparse. Using the sparsity matriarch to detect the two, three sparse sub graphs, or any other sparse dime a troid that happens to work, any other sparsity count that happens to work. And we'll talk about the actual complexity of solving after we have done the decomposition.
00:45:19.704 - 00:45:49.974, Speaker A: That's chapter seven. This will take us a couple of weeks. And then we will sort of a natural offshoot of this realization after decomposition is this notion of Kayley configuration spaces. And that will lead us into the non rigid graphs. Graphs that actually move so have a one dimensional or higher dimensional solution space. And then the question of describing the complexity of that solution space. That goes into the realm of kinematics.
00:45:49.974 - 00:46:18.686, Speaker A: And we talk about that. And that leads us to other certain topics that are related to Kaylee configuration spaces, such as flattenability of graphs and other. And in other distance, not euclidean, non euclidean distances. This is also. Now here comes the problem. I didn't quite know how to order these four topics. They're all kind of equally related to each other.
00:46:18.686 - 00:47:15.160, Speaker A: There are arguments to say that I should talk about the kinematics ones immediately after this. There are arguments to say that I should talk about, you know, metric cones and other things related to other norms before I talk about universal rigidity. You could do these four chapters in any order you want, really. I mean these three chapters in any order you want after this chapter. But we'll choose one ordering. So universal rigidity, this idea, because this flatten ability relates to lots of the questions about projections of the EDM cone onto graphs and what they look like and all this other stuff some of the questions that some students, Faye and others were asking yesterday. So it naturally leads to this notion of universal rigidity and what those graphs correspond to in terms of their projections and fibers for the clinian distance cone.
00:47:15.160 - 00:48:25.284, Speaker A: So we'll talk about that. And the notion of stress matrices and positive semi definite stress matrices becomes a very important tool here. So we will definitely talk about, I'm not quite sure what we'll do first after this, but we will spend some time on this and then these three topics. This metric corner, metrics based embeddings maybe is a little bit away from the rest of the topics, but I think it should not be. People should be looking more closely at metrics based embeddings which have way more combinatorial optimization applications. So other metrics, tree metrics and other things have been used for approximation algorithms, for np hard problems, and are also related to this, using the semi definite programs for semi definite programs, semi definite relaxations of combinatorial optimization programs to solve combinatorial optimization problems. The max cut is a classic example, so we'll spend some time on that.
00:48:25.284 - 00:49:27.132, Speaker A: And maybe in this context also talk a little more about submodular functions. How sub modular functions are related to submodularity is a sort of discrete version of convexity, and so it almost has a direct relationship to matroids and semi definite programming. So we'll talk about, maybe if we have time, we'll talk about that. I might reorder this, I might do after this, go to this, and then go to that, and then go to this, maybe something like that. And then finally we go beyond distance constraints to other types of constraints, incidences and so forth. And the general idea of geometry theorem proving. Geometry theorem proving is the most general notion of dependence in these contexts that we are talking about, where the theorem is a polynomial that's dependent on the axiom polynomials.
00:49:27.132 - 00:50:47.444, Speaker A: Just like if you think of the dependent distance in a, in an over constrained graph dependence edge being a distance that is determined once the other distances are determined, you can think of that polynomial as belonging to the ideal generated by the other polynomials. A geometry theorem is a polynomial that belongs in the ideal generated by the axiom polynomials. So, which are themselves geometric conditions or constraints. So we'll talk about, and there, once you go into that realm, you know, mostly people tend to not, you know, they cannot deal with generic, you know, defining what is generic and non generic becomes much harder to do. There is a, there's this general general art of defining genericity using what are called pure conditions but pure conditions often are some abstract algebraic conditions that don't have a clear geometric meaning offhand. So the art of sort of coming up with geometric meanings for these pure conditions and saying, ah, okay, that's kind of behavior, geometric behavior. I'm not considering that generic.
00:50:47.444 - 00:51:37.364, Speaker A: Okay, that's non generic. So then you can sort of a priori, sort of rule out certain types of non genericities and then say, okay, now maybe there is a combinatorial way of dealing with this, right? And that turns out to be quite a hard thing to do to get meaningful answers. You know, people struggle with this. And so we will do a few of those trying to define genericity for certain types of other types of things that are usually used that you see mainly in geometry theorem proving, but don't seem to have an underlying rigidity theory associated with them. So because of this difficulty of defining generosity. So we may get to that point. So I think that's the plan.
00:51:37.364 - 00:52:13.270, Speaker A: If you have any questions, I'll take them right now. Funereal silence. So I'll stop share. So we have six minutes. I mean, I can probably take one or two questions that maybe I wasn't able to answer last time. There's some question in the chat plans again. Yes, I'm going to.
00:52:13.270 - 00:52:40.988, Speaker A: Oh, now you mean Jack. Just what you said on the table slide. Oh, I see. Okay, so wait, let me go in order here. What is NPR complexity? Sean? NPR complexity. You know what NP is, right? It's got, it's all problems where it's got a polynomial time. Verifiable proof.
00:52:40.988 - 00:53:26.268, Speaker A: Yeah, yeah, right. You got that? So, but the proof, when they talk about the proof, they mean something over the integers. It's a string of bits. It's a polynomial length string of bits. Now, if you think of solving polynomials over the reals or complex numbers, a standard kind of witness to show existence of a root is the root itself. So someone gives you the root, you simply plug it into the polynomial and check whether it's the root. Okay? And that's in fact the proof that solving, I mean, showing the existence of real solutions to polynomial equations is actually in NP.
00:53:26.268 - 00:54:02.616, Speaker A: But now the root is no longer a string of integers. I mean, string of bits. It's an infinite string of bits. Okay, so there's this whole theory developed by bloom, shoeb and smale bss. It's called bloom, shoe, Ben snail to deal with this idea that, you know, it should still be NP, really? And. But now I'm going to define algebra. This, this notion of NP over the reals, okay? So they define an entire complexity theory over the reals.
00:54:02.616 - 00:54:33.564, Speaker A: So kind of an algebraic complexity theory. And so it belongs in that. So it's a technique. It's a, you know, you can think of it as NP, okay, and real. The next one is related to real number complexity. There are two streams of this work on real number complexity, which is, you know, one is just. What is the complexity of a real number? So if I give you a real number, say square root of two, right? Yeah.
00:54:33.564 - 00:55:21.166, Speaker A: So if there is an algorithm that generates the bits really fast, one after the other, it doesn't matter. So there is a notion of real number complexity where it's related to Kolmogorov complexity. I don't know if you have heard about it. If the speed of generating the bits is one after the other, that's the complexity of the real number. So PI, for example, or square root of two or whatever, and there's this other strand of real number complexity, so to speak, is the one that I just mentioned by bloom, shub and smale, who don't really worry about the complexity of real numbers per se. They say, well, let's assume there's a magical computer that can store real numbers, okay? So. And it can do complex computation on real numbers.
00:55:21.166 - 00:55:55.734, Speaker A: I'm just giving it to you that adding two real numbers takes one step, okay? And if, with that assumption, what do we get? Can we build a complexity theory with that assumption? So that's another strand of real number complexity. So the first one that I mentioned is called, is by a group started out by a guy called Kerry Koh. And then it's sort of, it's got its own branch of people doing that. And then the second one is this blom Schuben Smith. Yeah. For example, x plus y is counted as one operation, no matter what digits. Yes.
00:55:55.734 - 00:56:50.534, Speaker A: That's Blum Schubert smell style. Okay, then I have to say, you have to be careful. When you write a paper, algorithms paper, you have to say what you're talking about, because the normal way in which a computer does real arithmetic is something called finite precision arithmetic. So in other words, you have to then argue about errors, right? You say, I'm storing only this many bits to represent the real number. And then, you know what can happen when I make mistakes, when there are small errors, rounding errors, and you have to deal, then that goes into the realm of numerical analysis, semi numerical algorithms and so on and so forth. That's finite precision complexity of real arithmetic. But the Bloomshoeben snail thing says, hey, I don't want to deal with all that ugliness, I'll just treat r1 number as one thing somehow that I magically store, and I go, okay, okay.
00:56:50.534 - 00:57:39.254, Speaker A: So there's genericity and then doctor plans. Okay, so let me go quickly to the screen that you want to see. We're going to spend the next several lectures on this, Jack. So the question was, so what was your question? What is the doctor plan? Or why am I saying this? I was just wondering if you could repeat what you were saying about them here. Yes. Okay, so a doctor plan is a generalization of this triangular, so this triangularized system that we had for triangle decompose tree decomposable graphs. Right.
00:57:39.254 - 00:58:19.726, Speaker A: A constant size doctor plan will give you a similar block triangularization of the polynomial system instead of an actual triangularization, where every time, when you're eventually solving, you will get one more quadratic equation and one additional variable. Now, you'll get a small system of quadratics in a small number of variables. Okay, that makes sense. And that number is going to be constant size regardless of the size of the entire thing. Yeah. Okay, so that's what I mean by it's order. One size doctor plan.
00:58:19.726 - 00:59:34.872, Speaker A: Okay? And if in addition, you have certain, you know, guarantees, such as having a rigidity oracle, or, you know, having a polynomial time rigidity detector, such as based on sparsity Metroid, and you know that, you know, these graphs have, allow you in some way to disambiguate between the two, the finitely many possible realizations at any given stage. So that could again be through multiple means. One could be that it's globally rigid, as in the case of sort of universally rigid, as in the case of dlateration graphs, the subgraph, I mean, or it could be that, or it could be that you have one more piece of information. In other words, the witness actually tells you which of these constantly many solutions you should pick at this stage because it only cares about whether that particular solution exists. Okay, so any one of, if anyone, if these, these guarantees are given, then you have a polynomial time algorithm for realizability. Okay, that's good. Thank you.
00:59:34.872 - 01:00:07.524, Speaker A: Okay. Yeah. So anyway, so we're going to move through this, I probably, chapter six. This, this portion will take us, I would say, at least two weeks. And then this chapter will take us another week or two. And then I probably will jump here and there'll be some guest lectures. By the way, by the way, I'm going to be contacting all of you or several of you to give some guest lectures.
01:00:07.524 - 01:00:47.554, Speaker A: So for this, for example, I'm hoping to have a guest lecturer. This is many things that I have put in here. This includes some of the sum of square stuff and connection to semi definite programming, plus sort of Newton polytopes, number of roots, counting, and the kind of things that people care about who are doing kinematics. So it includes a lot of things. So even though it's one line here, that might take like a couple of weeks, and then after that, maybe I will go to universal rigidity, which will require going back and thinking a little more of the cones and stuff. And then here, maybe that's the order in which I do it.
