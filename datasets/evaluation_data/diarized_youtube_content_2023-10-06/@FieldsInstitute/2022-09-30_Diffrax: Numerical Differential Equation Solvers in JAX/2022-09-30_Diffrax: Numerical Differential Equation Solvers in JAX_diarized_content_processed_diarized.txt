00:00:01.040 - 00:00:23.070, Speaker A: Cool. All right. Hey, everyone. It's great to be here. So, yeah, obviously I'm going to say a few things about DeFrax and about Drax as well. And also, I don't think we're going to be under huge amounts of time pressure for this talk, so I'd really encourage everyone to just jump in and ask questions if at any point anything isn't clear. Or conversely, you know, don't, because I think we're a bit behind schedule, and then everyone can get to lunch on time.
00:00:23.070 - 00:01:00.894, Speaker A: So, you know, do a dope is easy fit. Um, so, yeah, defacts. What am I talking about? What's going on? Um, so takeaways from this talk, these are the things I want you to have come away from this talk knowing, um, one that there's this new tool you can use. Um, it's called deFrax, and I think it's really cool and hopefully it will be of some use to you. Um, and so in particular, I'm going to be showing off a few code snippets, uh, of using diffracts, and also some comparisons against other libraries. Um, and then also, you know, I'll talk a little bit about some of the new innovations, some of the new mathematics going on inside this is, you know, this is not merely an implementation, implementation of differential equation solvers. There is something new going on here.
00:01:00.894 - 00:01:41.582, Speaker A: And then actually, we're going to see that this actually motivates quite a lot of open questions in numerical differential equations. And I think this is actually pretty fascinating because, of course, numerical differential equations always, you know, this is a very, very old field, and it turns out that actually, some of the new ideas here sort of open up a lot of things that we just haven't considered before. So, okay, first of all, the first question you might ask is why. Why am I sitting here telling you about a new tool? Um, given that, obviously we have differential equation solvers and have had many implementations for a long time. Um, and basically what it comes down to is, uh, these modern machine learning frameworks that. That have been appearing. Um, so the.
00:01:41.582 - 00:02:24.214, Speaker A: The ones I sort of emphasize here, these are Pytorch and Julia and Jax. Uh, for those of you who aren't familiar, by the way, Jax, this is one of the big machine learning frameworks. Um, I realize it's not quite as well advertised as Pytorch. Um, but in any case, the point is, is that we have these big machine learning frameworks and everyone is using them to train their neural networks and so on. Um, but it would be really, really cool if we can sit down and write some differential equation solvers or non linear optimizers or whatever, you know, a whole scientific computing stack, um, in these frameworks, because then we can auto differentiate through them and get parameter gradients. We can auto parallelize them and just, you know, shard them onto like multiple accelerators or put them onto a GPU and just, you know, to have them be faster in the future, first place than traditional methods. There's a lot of advantages to this starting point.
00:02:24.214 - 00:02:56.438, Speaker A: Okay, so without introduction, without motivation out of the way, let's jump right in and I'll show you a little code snippet of what does it look like? Show me the API. What does it actually look like to solve an ordinary differential equation using diffracts? So step one, import a bunch of stuff. And those of you who haven't seen Jax, by the way, this is always quite a popular selling point. It has a numpy like API, so it's quite familiar to many folks. And then I'm going to just write down exponential decay. So this is the vector field. This is the right hand side of my ode, exponential decay.
00:02:56.438 - 00:03:16.594, Speaker A: And then I'm just going to declare this as an ode. And exactly what's going on here, by the way, there's actually something very precise and in particular that's going on here. And we'll return to this term thing in a little bit later. But for now it's an ode. And I write down my solvent. So dot in prints five, four, so very standard bronchocutter method. And then I say, please start it with, say, these two scalar values, evolve them independently under exponential decay.
00:03:16.594 - 00:03:40.754, Speaker A: And then I get my solution. So I just plug all of that in. So that's more or less what it looks like. And in many respects this is a little bit like sort of playing with Lego. You write out the things you want and then you plug it into the solvent and magic happens. For those of you who have already got some extensive Jax background, this is not a DSL. This is not something built on top of Jax.
00:03:40.754 - 00:04:10.992, Speaker A: This is actually something that very much at the same level as Jack's. So for example, if you ever wanted to use this solver manually, then this solver object has solver equals to five. This has a step method on it. You can do solver step and make a single step yourself. So I emphasize this is not a DSL. And actually you can very much go around and start playing inside the guts and inside the internals and start doing things with this yourself, you don't have to plug the right thing into the right hole. Okay, so that's my OD snippet, nice and simple.
00:04:10.992 - 00:04:27.480, Speaker A: Hopefully. That looks cool. Let's go a little bit further. This is where we start cutting into what this term thing is. So again, I'm going to import a whole bunch of things, and then we'll see each of these getting used as we go down. So let's try and solve an SDE this time. So, okay, this is my exponential decay.
00:04:27.480 - 00:04:41.698, Speaker A: That's the drift. And then I write down the diffusion. This is just a little bit of noise. And then I write down my brownian motion. And I need an explicit source of randomness. This is one of the really cool things we have with Jax. Um, is that the, the randomness is something you really explicitly control.
00:04:41.698 - 00:05:00.330, Speaker A: It doesn't just sort of appear to ether. It's not a stateful prng in the background. It's something you said for yourself. Um, and this, this gives you great reproducibility of results and things like that. So, so we set, we sit down and say, this is my brownian motion. And this brownian motion, by the way, this is not some probabilistic object at this point. We've now embedded a particular random key in it, pseudo random number generator in it.
00:05:00.330 - 00:05:27.626, Speaker A: Um, and we can now deterministically generate an entire continuous time brownian path at any single point in time, given this object. And we'll get the same evaluation every time we get single point, same point. And then I wrap all that up and I say, okay, here's my, here's my drift. That goes in an ode term. It is an ode like quantity. And then I have put my diffusion and my brownian motion, I couple those together and I say, this diffusion, this is with respect to this brownian motion. I put all this together in my term.
00:05:27.626 - 00:06:01.094, Speaker A: I write down my solver, and again, I just plug it through the same API as before. And this is where you see that this ode term and this control term start to take on quite precise meanings. So emphasize this again later. But to explain it just right now, when I write down ode term drift, what I'm really saying is drift, dt, right? And this control term, what I'm really saying is diffusion, d brownian motion, dw, if you wish. And that's what a term is in this language is, it's a way of putting together this control. Inspector Field. And again, I'll turn to this later.
00:06:01.094 - 00:06:33.188, Speaker A: So again, code snippet, sde we can solve them diffract us these things. Fine. Um, the, the obvious question you might have at this point is, okay, versus, versus what, you know, versus all of the other things that are sitting and floating around in the ecosystem in particular. So torchdpq. Torch TPQ, obviously, this is, uh, this is the gorilla in the room. This is what pretty much everyone is using, certainly in like, you know, neural differential equations and so on. Um, how do we compare? Well, first of all, diffracts is much, much faster.
00:06:33.188 - 00:07:06.226, Speaker A: Um, and I, I write down 200 times faster. I've actually seen larger speed ups than that if you write down the right problem. Um, this, the implication of this, basically is that, so, so the reason for this rather, um, is that tortdq, uh, this is written in Python. Um, and this means that you are sort of in some sense, bound to the slowness of python. You, you back out after Pytorch. And so each individual operation is fast, but you still pay the overhead of the python interpreter. And this is sufficient to really slow you down when you're doing these kind of scientific computing problems where you've just got lots and lots of little operations.
00:07:06.226 - 00:07:27.716, Speaker A: Hydrogen actually doesn't give you that much feeder. We've got loads of more features, lots of things. I'll talk about that on my next series of bullet points. And also it's actually actively maintained, which is, I think, also usually a benefit. Okay, so when I talk about more features, what am I talking about? So I'm talking about OD and SD solvers. We've already seen both of those. And then also slightly more obscure.
00:07:27.716 - 00:07:53.934, Speaker A: I'm talking about controlled differential equation solvers. Um, and for those of you who aren't familiar with the controlled differential equation, don't worry, that will be my next slide. Maybe the one after, um, and then we'll see how actually controlled differential equations are giving us the, the language to express, uh, this, the, the way defract works. Right. And what's going on with these, these term objects? Um, lots of different solvers. You know, you've got all of the things you actually need to do your, do your job in practice. I order solvers, implicit solvers, and practice over blah, blah, blah, blah, blah.
00:07:53.934 - 00:08:19.130, Speaker A: Um, lots of different ways of doing, uh, uh, saving output. Maybe you just want the final time, maybe you want particular times, maybe you want a dense solution. That is to say, you want to have the output of your solver be a continuous time spline and be able to evaluate that at any point in time and get an accurate numerical solution. Um, multiple methods for backpropagation. Uh, I think in the previous talk we saw some discussion of the adjoint method. Right. Um, of course, you know that you want to back propagate through a differential equation.
00:08:19.130 - 00:08:59.324, Speaker A: You want to, you know, differentiate with respect to your, the parameters of your model or something like that. Um, then you've got multiple different ways of doing this. Famously, you have optimize and discretize, which is this continuous time backwards differential equation or something like say discretize and optimize, where you actually work your way through the internals of the solver. Um, and just quick soapbox moment for, uh, since we're on this point. Um, back propagates through the interns of the solver is like 95% of the time the choice you actually want. Just things like adaptive step size controllers. Um, so you open up your textbook and, you know, numerical methods, and how do you adapt the step size of, say, an ode solver? It'll probably tell you, oh, use an eye controller or also known as a deadbeat controller.
00:08:59.324 - 00:09:28.300, Speaker A: This is fine for some problems in practice. Actually, for a lot of problems, you can do a lot better. PI controllers in particular are much, much better. And so we support this as well. So PI controllers are particularly useful for stiff systems and they're particularly useful for SDE solvers as well. And so these more powerful set plus controllers are available as well. Um, event handling discontinuities, um, things like, uh, I've mentioned before that, um, building on top of these, uh, auto, parallel GPU capable libraries, uh, mean that we just sort of get things like this for free.
00:09:28.300 - 00:09:55.530, Speaker A: And indeed this stories too. Uh, when I say diffract supports distributed computing, what I'm really saying is Jax supports distributed computing and, and defracs is built on top of that. Um, and then of course there's this composability with the best of the Jax ecosystem, which is sort of growing and flourishing. And people are watching all kinds of cool things happening in Jax. Um, and then defects interfaces nicely with all of these. Um, okay, so that's versus torchDPQ, which is probably the one that I don't know if people are already doing. This is probably what, I don't know, maybe half of you are doing.
00:09:55.530 - 00:10:22.794, Speaker A: Um, but you know, there are options. How do we compare? So let's just say Sci-Fi well, first of all, this is sort of an easy comparison, right? Diffracts actually supports auto differentiation, it supports auto parallels, and it supports GPU's. None of those, none of those things are true on Sci-Fi it's much faster and again, loads more features. We just saw that. Now, of course, we come to. This is one talking point everyone brings up versus Julia. Right? Again, I bet there's probably a decent number of folks in the room using Julia.
00:10:22.794 - 00:10:42.154, Speaker A: How do we compare to differential regressions? Jl so the facts that. Well, first of all, reliable auto differentiation. Those of you who have used Julia in the Julia community probably know there's been quite sort of like a one after the other switching of different order differentiation systems. This hasn't happened yet in Julia. Um, I hope it will in time. I love Julia. The language, it's, it's stunningly, stunningly cool.
00:10:42.154 - 00:11:07.870, Speaker A: Um, but right now, you know, for solving the problems of today, it's not there yet. And so for that reason, I really like diffracts and these, these Python based libraries, um, it's easily extensible, so I'm not going to talk too much about this today. Um, but if diffracts, obviously comes with these solvers that we've just seen, we saw Euler's method, we saw dormant prints. Um, but, you know, you can write down your own solvers as well. Diffracts allows you to just write down your own solvers and plug them straight in. And that works. Um, we've got simpler internals.
00:11:07.870 - 00:11:35.012, Speaker A: In large part. This is because of this ode SDE, sort of like, uh, unification that I'm going to be talking a bit about, uh, later and which we've already seen a little bit of so far, this whole term system. Um, and then obviously, I'm also realistic about the fact that many folks quite like python, quite like numpy. And so defect sort of has this, this, uh, sort of familiarity for many people. Um, so that's, that's, uh, you know, not me having a go at existing libraries. Existing libraries have been amazingly cool. I've actually worked on many of these libraries before.
00:11:35.012 - 00:12:14.274, Speaker A: And so to me, this is now simply about taking the next step and then trying to solve the problems that we could not solve before and sort of work around the pain points that we've suffered before. And so there's this one big point I've mentioned, Ode SDE unification. What's going on with this? This is what I describe as being the really cool stuff. So everything up until this point has been me telling you, what can diffract do? What can diffract do for you? Here it is as a tool that you may wish to use because it's fast, because it's autodifferentiable because it supports this and the other. Right. But, okay, so, you know, software dev, hat off, mathematician. Hat on.
00:12:14.274 - 00:12:34.518, Speaker A: What's going on here in the internals. Why is this possible? So ods and sds are reduced to a controlled differential equation and solved in the same unified way. So what do I mean by that? Well, let's just write down an ODE. There we go. OdE, nothing complicated. And now let's write down an SDE. Again, SDE, nothing complicated.
00:12:34.518 - 00:13:18.002, Speaker A: It's just a generic ode and a generic SDe. These are both special cases of a controlled differential equation, which is an object that looks like this. So you can see that if we look at the ode up at the top, this is taking x of t equals t. If I have this continuous path x, and if this continuous path x is just the identity function x of t equals t, then in fact, my controlled differential equation actually just reduces down to an ordinary differential equation. Um, meanwhile, if my, if my x of t, it was equal t comma w of t, there's to say if x was vector value. Just to take two values is to take t the identity and to take w of t, the evaluation of my brownian motion at a particular point. Um, then, then, actually, why then recover is an SD.
00:13:18.002 - 00:13:56.642, Speaker A: And we've actually generalized from, from, from the ODSD case to the control to differential equation case in terms of, like, the precise mathematics of what's going on here, I'm not really going to talk about that today. Those of you are wondering, how is this actually justified? The short answer is rough path theory, because indeed, you know, there are sort of like, you know, theoretical niceties to be, to be sorted out when you write down something like this. But, okay, what does this mean in terms of the numerics? Well, you know, forget about the continuous time limit. Why does this work in the numerical case? Well, we write down Euler's method for node e. It looks like this explicit Euler method, very simple. And if I write down the explicit Euler method for SDE's like this, again, very simple. You see that they share common structure.
00:13:56.642 - 00:14:29.614, Speaker A: These in some sense, these are very, very similar. Right. Obviously, the drift term is the same in both cases. But the thing I really want to emphasize is the similarity of the ode to the diffusion term of the SD. I want to emphasize a similarity across here where you can see that actually, the only thing that has really changed here is that instead of doing ticket n plus one minus t, instead of doing this difference in time, we're doing a difference in brownian motion instead. We've got this w in there instead. And so there's this sort of, if you wish, forget about the continuous time limit, focus on numerics.
00:14:29.614 - 00:14:42.468, Speaker A: We see a commonality here that we may wish to exploit. So there we go. Same equations as before. I've just gone and put them up at the top. What is this commonality? Let's be precise. So each term, this is the terms we saw earlier. Oee term control terminal.
00:14:42.468 - 00:15:12.576, Speaker A: So the terms we saw earlier, they have a vector field. That's that f or f one or f two, right? This is the evaluation of a vector field at a point. We have a control, that's that thing on the right, that's a more precise, precisely, you might say it's like a delta control or something like that. It's a, it's a difference in your control across some interval. And then we have a bilinear interaction between them. So in the case of this ode up at the top, that bilingual interaction is f. F is just, you know, this, you can think of this as being an array valued or vector valued or matrix value or anything like that.
00:15:12.576 - 00:15:41.608, Speaker A: It doesn't have to be a scalar, right? But nonetheless, this is, this is that, this is like an array scalar product or a matrix scalar product or vector scalar product or scalar scalar product domain, any of these. This is a bilinear operation. Same is true in the drift term down here in the SD and now over here in the, in the diffusion term. Typically what we think of this as is, as a matrix vector product. My, my vector field is a matrix and my, my brownian motion is a vector, and we have a matrix vector product between them. So we have a bilinear operation. And so when I write down ode term, this is what I'm saying.
00:15:41.608 - 00:16:21.204, Speaker A: I'm saying that, um, I have my vector field, my control is time, and my bilingual operation is that of scalar multiplication. When I write do control term, I'm saying I've got my vector field, I've got my control, my bilinear operation is a matrix vector product. And this actually gives us an opportunity to do things beyond middle mathematics, but actually also the computational efficiency. Because what if my matrix exhibits special structure? Maybe my matrix is banded, maybe my matrix is diagonal, something like this. We can write down more efficient matrix vector product algorithms, and indeed we can do that. So in this case, weekly diagonal control time. What this is saying is that f exhibits so called weak diagonal structure, which is simply to say that as a matrix, it is diagonal, and we can start.
00:16:21.204 - 00:16:50.062, Speaker A: You get the idea. You can fill out these gaps, and if you need to, again, implement your own. The facts is extensible. You can write down your own terms if you ever need to express something of this nature with some custom bilinear interactions. So, at the numerical level, this is what's going on. And I think, so far, so simple. So what are the implications of this? Well, first of all, simple code, right? This is, I mean, obviously this is super cool for me as a library author, in that, you know, I get to write less code.
00:16:50.062 - 00:17:27.354, Speaker A: I can write down my Euler method once for controls differential equation and have it applied to an ode, have it applied to an SD, have it applied to anything else. So that's nice for me. I also want to emphasize that this is hopefully nice for everyone downstream. I want, you know, diffracts is not the final word in differential equation solving libraries, right? You know, we had touchdb three, we had scipy, etcetera. We had diffracts now, and I'm sure when, you know, another year, five year, whenever, you know, someone will come up with some other amazingly cool idea and they will write their own differential equation solving library. And I really hope that, you know, folks doing so can take advantage of these innovations and, you know, build on top of these same ideas. Is it shouldn't just be me who takes advantage of simple code.
00:17:27.354 - 00:17:59.574, Speaker A: I've emphasized the fact that this is the solvers. So I mentioned, like, writing down Euler's method just once, or, you know, dormant prints just once or something, but actually also the entire vest of everything else that interacts with differential equation solving. So this includes things like, say, adjoin methods. So these ways of back propagating through a differential equation, you want to write down your backwards in time differential equation. This is kind of complicated. If you can write it down once for the unified version, then maybe you don't have to write it down multiple times for the OD, for the SDE, for the CD, for the PDE, for the whatever, whatever, whatever. Things like advanced use cases.
00:17:59.574 - 00:18:28.022, Speaker A: So if you want to smush together a control term and an SDE term and a stochastic term, you can do that. You want to start using some more esoteric solver like Nimar Victoire, which decomposes your matrix vector product into a sum of vector dot products, you can do that. And again, you can sort of write these things out. Oh, also, I'm realizing, by the way, I've been using this word control quite a lot. This is not in the sense of control theory. Maybe it's worth just emphasizing this quickly. My control here, this is some exogenous forcing function that I've been given.
00:18:28.022 - 00:18:49.262, Speaker A: This is at Dx or DW or whatever appearing on the right hand side. So this is not in the sense of optimal control. This is just another separate meaning of the word control. Some people use the word driver instead. It's extensible. So I've been talking today a lot about odes and sds. This was certainly the motivating idea behind diffracts.
00:18:49.262 - 00:19:33.374, Speaker A: Diffracts started as a research project for can we solve ods and sds in a unified way? Answer, yes, we can, it turns out. But actually, you know, folks have then looked at this and gone, oh, but I'm trying to solve a PDE. But I'm trying to solve some problem on a manifold or whatever, right? And the good news is that in many cases, lots of these can be reduced down to an OD or to an SD or something. So maybe you have a PD and you semi discretize it to an OD, or you have a manifold, and what you really need to do is just solve an OD and then project down to a manifold after every step. And so again, I've spoken to people who've used diffracts and have then built this on top of it, where they've written their own solver that handles these extra cases. So again, extensible. And now we get to, I think, possibly one of the most exciting parts of the talk, which is open questions.
00:19:33.374 - 00:20:01.534, Speaker A: I've been talking about all these things that we've done, but this motivates a lot of things that we haven't done and I don't know the answers to. So things like, more like broadening this unification, right? So we've got odes in there, we've got SDE's in there. We've got folks starting to figure out manifolds. We've got folks starting to figure out PDE's. I've got a very toy example making this work for semi explicit daes. I'm working with differential order work equations. That is, I'm chatting with someone at the moment about extending this to delay differential equations.
00:20:01.534 - 00:20:47.758, Speaker A: And again, the hope is we can bring all of these things into one big, broad umbrella. And so that if you want to solve a stochastic, delay differential algebraic equation, sort of smudge these things together that you don't need to go out and write a custom solver that handles that one case, it should emerge naturally as a composition of these different ideas. Um, things like controlled differential equation and rough differential equation solvers. So, um, for those who aren't familiar, by the way, rough differential equation is just another generalization of controlled differential equation. Um, all of this came out of this idea of, let's take ods, let's take sds, let's think of them as cdes, and just treat them in that, in that language, and then, you know, internally lower them to a CD every time you see an OD or one SD. But maybe we can work with this language directly. Maybe we can, um, come up with, uh, controls differential equation solves that really make sense.
00:20:47.758 - 00:21:15.814, Speaker A: Um, sort of, you know, ab initia as a controlled differential equation solver. Um, uh, and I think it's probably, it's one of these things where, you know, people are writing down new numerical differential equation solvers all the time, right? Very often they'll say, oh, I consider the SD. Oh, I consider the OD. And I sort of put it to you to maybe consider the CD, start with that, and then get both ods and sds for free. SDE usage of ode theory. This is hugely important. This is such a cool thing because we're writing down this sort of unified way of doing things.
00:21:15.814 - 00:21:38.480, Speaker A: And ode theory, numerical ode theory has lots of things in it. We've got things like how to start at steady state. We got things like how do you choose your initial step states in smart ways. But then we wrote down OD's. We had to generalize these to handle the controls differential equation case, usually in a kind of the obvious way. But now, because we've done that, SDE's get all of this for free as well. And I think there's a lot of theoretical concepts there that just haven't been explored.
00:21:38.480 - 00:22:04.146, Speaker A: You know, what does it really mean to start, like solving an SDE too steady state in this particular way? Dot it, Dot. You get the idea. I'm speaking now very precisely to the rough path there is in the room. Maybe there's not many of you, but what does it mean to back propagate for non geometric rough differential equation? We have our joint methods for geometric rough differential equations. This is as general as I know it to be to differentiate through a differential equation. I don't know what it means to differentiate through a non geometric graphic differential equation. I would love to know.
00:22:04.146 - 00:22:36.554, Speaker A: Um, and then also through these alternate generalizations. So, um, I've generalized from OD. So I've generalized ods and sds to CDs in this one particular way. Um, but there's other ways of doing it if you, those of you who have the midpoint method fixed in your head, if you think about the explicit midpoint method, um, then it actually turns out we can improve upon this. If you change the way you evaluate your brownian motion and sort of generalize this in a slightly different way, you actually get higher order weak convergence in some cases. And this is, as far as I know, just not in a literature way. It's just one of these things that sort of pops out of thinking about things in this way.
00:22:36.554 - 00:23:12.876, Speaker A: And so with that, I am now wrapping up my talk. So that's everything about diffracts, and I hope this was really cool. And so I do have one final quick advert. Probably like half of the people in the room have seen this anyway, but nonetheless on neural differential equations. So, as I think my very first slide said, I am recently ex University of Oxford, recently at narrow Google X. And in particular, this means that, you know, as many people, I wrote a PhD thesis, and this PhD thesis is meant to be a textbook on neural differential equations. So if you find yourself just needing a reference text, you know, just go and point your new student at this paragraph or that section or whatever, then maybe this is helpful to you.
00:23:12.876 - 00:23:33.824, Speaker A: It's. So we cover neural ods, scds, sds and so on. And if these new material, for example, on training sds as gans, um, numerical methods, obviously, this is the thing I've been talking about today. Um, and so, for example, um, the, the textbook also includes things like reversible differential equation solvers. Again, not something I've had time to talk about today. Again, something new, something cool. Um, things like Android methods.
00:23:33.824 - 00:23:57.160, Speaker A: Um, I swear, if I see more people, like trying to prove back formation through an Od by like, resetting for lagrangian, um, and like, doing some lagrangian multiplies, like, this is so painful. You write down like three pages of proofs to try and, uh, prove your, your backpropagation. It's possible to do this in just five lines. Um, it's actually really, really simple. And so there's, there's some new proofs in there that make that quite nice. Um, and then things like symbolic regression. Those of you, especially coming out of engineering, you're thinking of something like Cindy.
00:23:57.160 - 00:24:11.208, Speaker A: Um, actually, we can do a lot better than Cindy. Um, we've got ways to combine your lods and vectorized evolution. Again, new material in the thesis and hopefully interesting as a textbook as well. So, um, with that advert out the way. Uh, I think this is. Now brings me to my final slide. So, Defrax.
00:24:11.208 - 00:24:19.656, Speaker A: It exists. It's on GitHub. You can go find it. Uh, it has more GitHub stars than then this. By now, I think you can tell these slides are slightly old. It's my thesis. It's available on archive.
00:24:19.656 - 00:24:28.624, Speaker A: It's online. Go find it. And then finally, if you ever want to reach out to me, you can find me at either of the above. So on that note, thanks, guys. Thanks for listening, and I'll take any questions.
