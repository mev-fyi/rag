00:00:00.400 - 00:00:59.408, Speaker A: Let's start where we stopped last time. I'd like to do another example of a framework in the plane, and this is kind of the famous triangular prism framework. You can see it pictured here. It's got two triangular faces and they're connected by three bars. And we'd like to study the pure condition. Remember that's a polynomial in the brackets that vanishes. When this framework has a motion, has an infinitesimal motion, that's not a rigid motion, and this framework is isostatic in the sense that it's generically rigid, but in special positions.
00:00:59.408 - 00:01:50.122, Speaker A: Then where the pure condition vanishes, we're going to find that it actually has motions. And so what we do, as before, we tie down two of the vertices using three edges, because we're in the plane. And you can think of saying that a is a fixed vertex and b is kind of constrained to line a one dimensional space. So we've removed both translations and rotations from our consideration. And then we write down this extended rigidity matrix. And there's a bit of a convention here that I want to remind you of. A minus x refers to the position of a minus, the position of x, and the positions of a and x are in two reals.
00:01:50.122 - 00:02:43.524, Speaker A: So a minus x is really a two vector. So it's spread across two columns here. So this is a two by two matrix here in the top left corner. And then you've got the rest of the matrix here and it's got a nice diagonal structure, block structure here. And so it's determinant is the determinant of the top two by two matrix, which turns out to be equal to the determinant of a certain three by three matrix, where a is one column, x and y are the other two columns. So that's one of our brackets. This next two by two matrix is another bracket, a bz, the second 3rd matrix here is the determinant a, b, c, and then we have the determinant of this big portion in the bottom right.
00:02:43.524 - 00:03:59.156, Speaker A: And if you look at that for a while, you'll realize that this determinant here is actually the determinant of, it's the pure condition for a triangle with three tie downs. Well, maybe not the pure condition, but it's the full determinant of Mgt where, where g is just a triangle and you've got three tie downs. So you've got def as a triangle and d is tied down to a, e is tied down to c, and f is tied down to b. And what we know is that such a determinant, such a polynomial is going to be given by the pure polynomial for the triangle, which is just def and then we multiply by the, by the portion of the pure condition that refers to the tie downs. And usually we factor those out. These are these black things here that, the brackets that we factor out. But here we're not going to factor them out because we're actually interested in the full determinant.
00:03:59.156 - 00:04:56.452, Speaker A: And that determinant, sorry, that portion of the determinant is encoding when the tie downs are dependent as lines in projective space. So the tie downs da this bar, ce this bar and bf this bar. When are they dependent in p two as lines? Well, they're dependent in p two when they're coincident, when they intersect in a common point. And that's one of the things that we developed a bracket condition for last time. We actually wrote down that bracket condition directly. And so this was the bracket condition. And so we can just put that in as the, as the rest of the determinant.
00:04:56.452 - 00:05:46.248, Speaker A: And so you get this expression here that determines when you have an actual motion of this isostatic framework, which I think is pretty cool. Okay, so I want to just talk about a few generalities now which I find very interesting. These are things that I learned from Walter's paper with Tony and Bern, the survey paper that's out on their website now. So if you have a framework in, let's say, n space. So this is g sitting here. I've drawn it like a plane, but I'm thinking of it as n space then. And if you take a vertex outside of rn, so think of it in rn plus one, then you can kind of join that vertex to all the vertices of g.
00:05:46.248 - 00:06:53.804, Speaker A: And we call that a new framework, the cone over G. And White and Whiteley proved that coning doesn't change the rigidity of a framework. And in fact, you can even slide these vertices that are currently in RN, you can slide them along their, they're kind of coning lines as well and still preserve the rigidity, I believe, which is super cool. Okay, so I guess what I want to do next is I want to address how does coning change the pure condition. And it's very simple. If you've got the pure condition of your framework G and RNA, and you want to find the pure condition for the coned setup, then all you have to do is in every bracket appearing in CG, you just tack in an extra v inside that bracket. You make the bracket one bigger and that's it.
00:06:53.804 - 00:08:08.064, Speaker A: And this is assuming that the vertex that you're coning over, that it remains fixed so that there's no infinitesimal motions of that vertex that are allowed, it's somehow tied down. And I'll say that Jessica Sidman and Ashley Wheeler and I found a way to consistently cone not just the bracket expressions, but the grassman Cayley factorizations for those bracket expressions. And I won't say too much more about it. It's pretty simple. If you've got a join of a bunch of elements, then you just tackle in an extra v as an element of the join, and you get a new Grassman Cayley expression that refers to the join of v with, with these guys. And if you had a meet of two extensors, x meet, y, then you do this extra little bit. You do x meet, and then the thing you meet with is the join of y and v.
00:08:08.064 - 00:08:59.188, Speaker A: Okay, I'm going to skip, actually, the application of that. I'll just say that what we did, um, for that was we wrote down Grassman Cayley expressions for brackets that characterize when d plus four points lie on a rational normal curve. So if you know what that means, great. If not, then maybe you've heard of the twisted cubic in p three. That's a famous curve in three space. And you could ask, when do seven points lie on that twisted cubic? And this characterization is quite nice. What do you do? You take, fix one of the seven points.
00:08:59.188 - 00:09:48.700, Speaker A: So pick that and cone over that point to the other six points, and project those lines that you get joining the points, and, and take those lines and intersect them with a hyperplane with a plane. You get six points on the plane. And the seven points that you started with lay on a twisted cubic. If and only if, the six points that you get lie on a conic. And you can then, you know Grassman Cayley expressions to check when six points lanakonic from Pascal's theorem. And this has to happen for all the different projections. There's seven of them from seven points to the other six.
00:09:48.700 - 00:10:22.152, Speaker A: And this characterizes when seven points lie on the twisted cubic. And we did this in all dimensions, which is kind of cool. And the, the work, leveraged work by Kaminata and Schoeffler, who worked at the bracket level, and we worked at the Grassman cable. Okay, I did say something about this. So anyway, that's all I want to say. Okay. But similar in some ways to coning is a much kind of reduced kind of coning.
00:10:22.152 - 00:10:57.066, Speaker A: The zero Hennenberg move that we've talked about before in lots of talks. So if you've got your framework g here and you want to add in a vertex in a zero Henneberg move, just add this extra vertex and join it to two other vertices by edges. And zero Henneberg moves preserve rigidity. And you might say, well, how does the pure condition change? And it changes in a very nice way. You just. Just. It's just v ab for this triangle.
00:10:57.066 - 00:11:42.864, Speaker A: Right. So, yeah, anyway, so it just multiplies your pure condition by an extra factor, one extra bracket. And I thought for a little while that it should be easy to work on one Henneberg moves, too. Right. Like, how hard could it be? Turns out it's pretty hard, because essentially, if you could understand how pure conditions for one Hennenberg moves work, then you would understand the full situation of rigidity, certainly in the plane and in other places, too, to some extent. So this is actually, I think, harder. So we're not going to.
00:11:42.864 - 00:11:47.784, Speaker A: We're not going to delve into that right now, though. It could be an interesting project.
00:11:50.084 - 00:12:04.388, Speaker B: Okay, Will, can I ask a question? Yeah. What do you mean by zero Hennenberg and one Hennenberg? So a zero Hennenberg seems to be you have an edge and you add a vertex.
00:12:04.516 - 00:12:09.356, Speaker A: That's right. And one Henneberg, is that you just.
00:12:09.380 - 00:12:12.788, Speaker B: Add a vertex connected to two others, or is that a one extension?
00:12:12.956 - 00:12:24.484, Speaker A: It's a one extension. That's what I mean by I think you've got an edge and then you connect, you draw three new. Three new edges.
00:12:25.424 - 00:12:28.284, Speaker B: Okay. So that's usually called a Heinenberg two, I think.
00:12:30.944 - 00:12:38.200, Speaker C: Yeah. So just, will, again, do you need that edge between a and b?
00:12:38.352 - 00:12:39.056, Speaker B: Yeah, that was.
00:12:39.160 - 00:12:41.606, Speaker C: What do you call the zero Henneberg extension?
00:12:41.800 - 00:12:43.186, Speaker A: Yes, you'll need that.
00:12:43.330 - 00:12:47.298, Speaker C: Okay. So it's not just sum. Two vertices. Okay.
00:12:47.426 - 00:12:53.562, Speaker A: No, you have to start with an edge, a, b and g. Otherwise you don't really get a triangle here. And so.
00:12:53.618 - 00:12:55.574, Speaker C: That's right. Yeah.
00:12:56.234 - 00:13:37.664, Speaker A: Yeah, good point. Sorry, I think I probably misstated that. And maybe I'll just. I'll dodge Sean's question for now, because I'm not sure exactly what I mean by when Henneberg moves. I guess I mean that, you know, an edge split or a result about infinitesimal rigidity in the plane, that it's preserved by these two moves. Zero Hennenberg moves and one Henneberg moves. And that you can kind of build up every, every rigid graph from, like, essentially a line segment by a combination of these moves.
00:13:39.164 - 00:13:55.780, Speaker B: You need something more general than the zero Hennenberg that you got here. You need to be able to add a vertex connected to any two vertices. Not just. It doesn't just have to be the connected by an edge. It has to be slightly stronger, slightly more general.
00:13:55.972 - 00:14:07.414, Speaker A: So even here I think things are more complicated, although I think it's worth maybe worth trying to like, understand, like how the pure condition behaves for zero hannibal.
00:14:08.114 - 00:14:11.410, Speaker B: Is anything known at all or not by me?
00:14:11.602 - 00:14:26.082, Speaker A: Okay, I'm already on thin ice here. Yeah. Okay, this is where I'm stepping out just a little bit beyond whatever the literature, and I'm getting in trouble. So.
00:14:26.258 - 00:14:38.426, Speaker B: No, no, I'm guessing probably nothing is known because I think you are right, the triangle does make things very nice and doable. So that does seem to be the key element. Thanks for answering my questions. Thank you.
00:14:38.450 - 00:15:04.864, Speaker A: Okay, no problem. Okay, so we could go on. I've got two other examples, and these are in three space. So now we're shifting up to spatial structures. And so I thought we'd start with the tetrahedron. And there's two ways to do the tetrahedron. One is to say, hey, the tetrahedron is a cone over a triangle.
00:15:04.864 - 00:15:42.164, Speaker A: And we already figured out the pure condition for a triangle. So can't we just tack in an extra vertex for the cone vertex and into our bracket for the pure condition for the triangle, which is ABC? And then you'd have ABCD would be done. And that's true. Alternatively, you can do it from scratch. You could tie down a, you need three tie downs to tie down a in three dimensional space. And then you get two tie downs on b and one tie down on c. And then.
00:15:42.164 - 00:16:42.848, Speaker A: And then you get actually a block upper triangular matrix for the extended rigidity matrix, or augmented rigidity matrix, as I was calling it. And then you can calculate what the determinants of all these matrices are. And just like before when we had two by two determinants that also have the same values as three by three determinants, here I've got a three by three determinant that has the same value as a certain four by four determinant whose columns are a, x, y and z. And these positions of a are four entries. Because now we're in three dimensional projective space, right? So there's four components for a. Or you can think of a as just being like the x, y and z coordinates, and then a one tacked on, if you want to kind of place everything in the finite part of the projected plane. Okay, so this is like the simplest example.
00:16:42.848 - 00:17:31.598, Speaker A: Let's look at the next simplest example, which is not so simple, which is just the prism, but in three dimensions now. So in black, I've kind of drawn that prism. There's one of the triangular faces, ABC, and I've got the other triangular face def here. And they're connected by three bars as before. But that's not isostatic in 3d. If you take 3 volts minus six here, it's not equal to the nine edges, it's equal to twelve. So we have to add three bracing bars in order to brace the prism so that it becomes rigid, or generically rigid, at least.
00:17:31.598 - 00:18:15.660, Speaker A: And so I put these three red bars here. I've joined a to e, c to f and b to d, and now I've got a isostatic framework. So it's generically rigid, but in some special positions it might admit non trivial motions. And we just go ahead and we tie down a and we tie down b and we tie down c. We need six tie downs in three space in order to kind of eliminate the six rigid motions. And, and then we write down the augmented rigidity matrix and we get something, again, a block trial triangular form. But unfortunately, it's a little bit tricky.
00:18:15.660 - 00:19:26.598, Speaker A: So here you get just the usual four bracket for a tie down, and here's another four bracket and another four bracket. And these are kind of things that are relatively easy to see from the block structure, and they're also the portion of the pure condition that we're going to throw away because it's got tie downs in it. So really we're interested in the determinant of this lower right matrix. And unfortunately, this doesn't have a block structure quite so nicely, and it's not possible to reorder the rows so that it does have a block structure. I tried. So things are problematic here. So how are we going to find the determinant of this lower right matrix? Well, again, if you look at it closely, what you find is that it's the augmented matrix for this triangle def, where each of the points de and f have been tied down to two other points like this.
00:19:26.598 - 00:20:26.964, Speaker A: So I've got definitely, and then I've got a, now thought of as a tie down vertex, and it's connected to d and e, and b is a tie down vertex connected to DNF, etcetera. And what we'd like is actually the full determinant of the augmented matrix for this. Right, that's the lower right matrix. And what we know is that it's the portion of the determinant that comes from the tie downs times the portion that comes from the pure condition for the triangle, and the pure condition for the triangle. You can work this out in three space is just one in three space. The pure condition for the triangle is one. Then all we have to do is figure out what's the kind of portion of the pure condition that comes from the tie down.
00:20:26.964 - 00:21:41.774, Speaker A: And there's a very beautiful theorem in a paper by White and Whiteley that says that this pure condition for the tie down is a bracket polynomial. And it's measuring when the six lines here, the six lines are the lines spanned by a and D and a and et cetera. All these green line segments get extended to lines. And when do those six lines, when are they independent? When are they dependent in three space? And I thought about this for quite a while when her six lines dependent in three space. And I was somewhat despairing of what was going on here. And then I remembered that about four years ago, I gave a talk in a special session, and in fact, I think it was organized by Mira and Jessica Sidman, who was at the SIaM conference in Atlanta. And I had explained there that there was something called the super bracket that I learned from Neil White's survey paper in the handbook.
00:21:41.774 - 00:22:09.574, Speaker A: And that super bracket comes up in robotics, and it measures when six lines in three space are dependent. It was like, oh, great, let me just look at that super bracket. And so let me show you the super bracket. Oops, there's the super bracket here. It's this crazy bracket expression. So this is a line, a one, a two. So a one and a two are points in three space.
00:22:09.574 - 00:23:00.468, Speaker A: B one, b two is another line, etcetera. And this symbol denotes a bracket polynomial with 24 terms. So it's a very compact way to write down a very large bracket polynomial. And there's this dotted notation, which if you read various papers in this subject, you'll see dotted notation a lot. What this means is that there's a term here that you can just read off, but you're also allowed to interchange d one and d two. You're allowed to interchange those, and you're allowed to interchange or substitute e one for e two and act using a permutation for those. So you've got two permutations here, each of order two, and they generate four brackets.
00:23:00.468 - 00:23:21.284, Speaker A: So this line here really represents four brackets. This next term represents four more brackets here. There's three permutations, each of order two. So that represents eight brackets and eight more brackets. And that's the 24 terms. Okay, so that's a lot of terms, but we have special lines. We don't have just general lines.
00:23:21.284 - 00:24:05.194, Speaker A: We've got ad and then another line, BD. So they share points, point, things like that. And if you work it all out, it turns out that 22 of the brackets vanish. And so you only get two brackets left over. And this is the portion of the pure condition that comes from the tie down. And if you go back to what we were doing before, this is exactly the pure condition for the braced prism. And this agrees, then, with the table that's given in white and white lace scion paper, where they write this pure condition down.
00:24:05.194 - 00:24:54.064, Speaker A: So, that's a nice computation. Now, I want to turn to, what does it mean? On one hand, we know what it means. It means that these six lines are dependent in three space. But I'd really like to get a Grassman Cayley interpretation for this bracket polynomial. And this bracket polynomial, remember, is telling us when the braced prism has motions back to the polynomial, we need this polynomial to vanish. So we need these two terms to be equal. And so I can divide one term by the other and say, well, that quotient has to be one.
00:24:54.064 - 00:25:23.694, Speaker A: Okay, so that's how we get this set up. And then if you stare at this long enough. I keep saying that. Sorry about that. But if you spend enough time thinking about such things, you realize, oh, I could kind of reorder things, and they look kind of interesting. Here I've got, like, ACF appearing in the top, and I've got an ACF factor also appearing in the bottom. And then I've got kind of b and D.
00:25:23.694 - 00:26:48.718, Speaker A: And this is. Now I'm going to kind of pull out a bit of magic that I learned from Susanna Pell, that I learned this in her algorithm for Kaley factorization. And her algorithm is kind of designed for working in p two in the plane, not in three space. But if you generalize the algorithm appropriately, and I claim this is the appropriate generalization, then you can see that what you should be doing is taking this acf term, which is a plane, right? It's spanned by three points, and this bd term, which is a line, and you should intersect that plane with the line, and that gives you a point in three space. And you do the same thing for these other terms. And then what we do is we replace this bracket, Bacf with the distance from b to a distance between these two points, and you get this product of oriented distance ratios. And notice that this goes from b to a and then from a to d, thinking of this as kind of going from b to d, and you pass through a on your way, and then you go from d to c passing through b.
00:26:48.718 - 00:27:39.350, Speaker A: And then you go from c back to b, where you started with c. On your way, you get a kind of triangular path going from b to d to c and back to b with these capital letter points on the a inches of the triangle and appels algorithm suggests that what you should do is on this triangle. Sorry, I didn't draw a triangle here. On this triangle, you can kind of take two of the sides and compress them into one edge kind of shortcuts. So that's what we're going to do now. We're going to take this, this portion that goes from d to c and from c to b. We're going to go from d to b, directly passing through a new point, z.
00:27:39.350 - 00:28:17.758, Speaker A: And kind of the magic is to express what that new point is. And so I've written down the Cayley expression for that new point. And then you get this product of distance ratios, which is supposed to be equal to one. And how could that happen? You're going from b to a divided by a to d, and going backwards from d to z divided by z to b. And this can only happen if z is actually equal to a. So a must be equal to this expression, but it's also equal to a different expression. And let's see here.
00:28:17.758 - 00:28:56.422, Speaker A: It's equal to this expression. Both expressions. A is on this plane and also on the line BD. Z is on the line BD. So they're both on the line BD. The real question is, is z on the plane as well? And so you can measure that by asking whether ZacF, there should be brackets around this is equal to zero, whether the join of z with a c and f is zero. That can only happen if z is actually on the plane generated by ACNF.
00:28:56.422 - 00:29:49.244, Speaker A: And then you can just expand that using this, plug it in here, expand everything out. You get this nice Grassman KLA expression, and then you can actually expand it using the meet and join operations. And I did. And you get minus the pure condition, you get almost exactly the same polynomial, and certainly up to vanishing, it's the same. And I should just say all this is kind of ad hoc. I cooked it up before these lectures. So I don't know how general all this kind of approach is, but I do have to warn you, I put this point in red because it's not at all clear that c is well defined as a point.
00:29:49.244 - 00:30:38.724, Speaker A: We're working in p three, so the ambient space is a four space, and c is a point, and d is a point. So this is a line, and it's supposed to intersect another line. But in four space, two lines generically don't intersect. So what's going on? Like, this meet should already be zero here. And the answer is, actually, this calculation here has to take place in a plane spanned by B, C and D. And there's ways to make that explicit and even put it into terms of Grassman Cayley expressions in four space, like coning and other stuff. But I don't want to go into that.
00:30:38.724 - 00:31:27.574, Speaker A: I just wanted to point out that we actually have a Grassman Cayley expression that explains what the heck we mean when we talk about the six lines being dependent. And this actually, I think, is actually part of the challenge of working with Grossman Kailey expressions. Or maybe I should say with working with bracket polynomials, is interpreting what the heck they mean after the fact, sometimes pretty hard. Okay. Okay. Just a few more remarks about graftsmen Kaylee factorization to set us up for Wu's algorithm. So not every bracket polynomial admits a Grassman Kaylee factorization.
00:31:27.574 - 00:32:03.764, Speaker A: So it's possible that my efforts before were fetal. It turns out they weren't. But I was trying to factor this pure condition. It might not have had a Grassman Kali factorization. Here's a simple example that I got from Appel's thesis. But she cites an example, I think, Jude White, which says that this simple bracket polynomial doesn't admit a Grassman Kalia factorization. But if you multiply it by a little by another bracket, than it does.
00:32:03.764 - 00:33:35.814, Speaker A: And in fact, that's kind of the generic general behavior that occurs. Walter and Bernsteinfels proved that if you start with the bracket polynomial, then that bracket polynomial may or may not factor as a Grassman Kayley expression. But there's always a bracket monomial so that you can multiply by, say, the product of a bunch of brackets, m, multiply p by that, and then you do get a Grassman KLA factorization. And the way I'd interpret that is saying that the bracket condition, that the bracket polynomial p vanishes, is equivalent to grass michele expression under the side condition that says that m is not zero. Right. And that side condition is particularly simple because it's just saying a bunch of bracket brackets themselves aren't allowed to vanish, and they would only vanish in a degeneracy situation where kind of four points, lay on a, lay on a plane in three space, that kind of thing. So it says that if you're trying to interpret this bracket polynomial at least up to some non degeneracies, you can write it down as a Grassman Cayley expression.
00:33:35.814 - 00:34:24.414, Speaker A: Now, to actually do that algorithmically and kind of produce it isn't so easy. Maybe that's being, it's very hard, actually. Maybe the better way to say, and this same thing with these side conditions happens also when we use those binomial proof techniques. It didn't show up in the example that I showed you guys last time, where you multiply the two kind of bracket polynomials down the left side and down the right side and set them equal, cancel the polynomials. But, but sometimes it, it does turn out that, um, that you have to say, well, some of these brackets aren't allowed to be zero. Otherwise, though, you can't cancel them. Maybe that's the best way to say it.
00:34:24.414 - 00:35:29.824, Speaker A: Okay. Um, so let me shift gears and say something about Wu's method. So this is my first time hearing about Wu's method in this class. So I learned about Wu's method in this class and in preparing for these lectures, and I should say right at the outset, you know, that that makes me a neophyte in terms of Wu's method. And I found it a bit heavy going. I found several papers on Wu's method in the literature, and it's a bit unfortunate. There seems to be situations where people kind of skim over the details, and so you don't quite get the full sense of what whose method is, or they go into all the details, and sometimes in language that isn't quite common in commutative algebra, at least.
00:35:29.824 - 00:36:32.394, Speaker A: So I found the reading somewhat heavy going. So I'm sure this is just my kind of lack of understanding, but I'm going to try my best to explain what Woo's method is for you. So, Woo's method is a technique for proving statements in elementary geometry. So that's what we're interested in. And the approach is really kind of appealing. He uses polynomials to encode the hypothesis of each hypothesis of a geometric statement, and he uses a polynomial c to encode the conclusion of a geometric statement. So you've got these hypotheses, polynomials h one through ht, encoding the hypotheses, and then a conclusion, polynomial c encoding the conclusion.
00:36:32.394 - 00:37:54.214, Speaker A: And if it turns out that whenever you have the hypotheses vanishing, then the conclusion also vanishes, then that would prove your theorem. So there's a technique then in algebraic geometry to check whether this variety, cut out by the vanishing of these hypotheses polynomials, is contained in this hyper surface. Well, in commutative algebra, we'd say, well, this happens precisely when c is in the square root of, sorry, the radical ideal generated by h one through ht. And here I'm assuming that we're working over an algebraically closed field, and that just means that some power of C is in this ideal generated by h one through HD. So that's kind of what we're checking. And in my reading of Wu's method, people don't actually use this power. I've not seen anyone use this radical.
00:37:54.214 - 00:38:55.064, Speaker A: Instead, what they do is they try to aim to show that C itself is in the hypothesis polynomial. And certainly if that was true, then, then you would have this inclusion, and c equals zero would follow from h one through ht being zero. But that's just what they're aiming for. They actually usually fail to get that, and they get something slightly weaker. And what they prove instead is they find a polynomial g. So the g times c is in the ideal h one, three ht. And so, as long as g evaluates to be non zero, then the theorem holds for the geometric situation that's parameterized by x one through x one, x two, all the variables.
00:38:55.064 - 00:39:42.848, Speaker A: So this is like a side condition. G represents a side condition, saying, so long as that side condition isn't satisfied, you can think of it maybe as non degeneracy. Then the conclusion follows from the hypothesis. And so this is analogous to what we were just talking about, but side conditions in Grassman K factorization and in binomial proofs. Okay, so let me do one example just to show you kind of what I mean, because I sketched out the method, but I didn't really give enough detail. So we're going to look at this theorem. It says that if you have a parallelogram, a, b, c d, then the intersection of the two diagonals, ac and bd, intersecting at, .0
00:39:42.848 - 00:40:13.214, Speaker A: then o is equidistant from a and from c. It's a pretty simple geometric statement. But. But what we do is we start assigning variables to coordinator the various points. So B gets a pair of variable coordinates. Bear with me for a moment. The labeling seems totally nuts, but there's a method to the madness.
00:40:13.214 - 00:40:46.244, Speaker A: B gets a pair of variables, c gets a pair of variables, a gets some d, etcetera. So everybody gets their own variables. And, and then we notice that, hey, the theorem itself is actually invariant under translation and rotation. Right. Doesn't matter. If we use a rigid motion, it doesn't change the theorem. So we could use a rigid motion to kind of tie down a and put it at the origin, zero, zero, and tie down b so that b is on the x axis, let's say.
00:40:46.244 - 00:41:26.904, Speaker A: And so a bunch of these terms disappear or at least become zero. Excuse me for a moment. So here, now you've got this picture. A is zero, zero. B is U ten, c is u two, U 3D has got x's, o is x's. Okay, so now let's look at trying to encode the hypotheses. I want to encode that ab is parallel to Cd, that where these two sides are sides of a parallelogram, so they should be parallel.
00:41:26.904 - 00:42:32.884, Speaker A: One way to do that is to calculate the slope of ab to zero and make sure that the slope of Cd is also equal to zero. And if you do that, you get this polynomial here, the h one. Similarly, you can check that ad and Bc have the same slopes and encode that using a polynomial, you could ensure that o lies on Bd. You could do that, say, by calculating the slope of boundaries and then calculating the slope of Bd and saying, hey, they have to be equal. And similarly, o lies on Ac. And then the conclusion polynomial is saying that the distance from a to o is equal to the distance from o to c. And you get those just by, you know, the usual sum of squares, and we'll write that to them.
00:42:32.884 - 00:43:24.802, Speaker A: Okay, so that's our conclusion polynomial. And you'll notice that the polynomials have this nice property that the first polynomial only involves x one. The second polynomial involves x one and x two. And what I was kind of hoping for was that the third polynomial would involve only x one, x two and x three, that we would have essentially some kind of triangular system here in the x variables. And that's not the case, but we could make it the case. So that's the next thing. So the next step is that, typically, the hypotheses you write down are not triangular in the sense of only adding one more variable each time.
00:43:24.802 - 00:44:15.214, Speaker A: But what you can do is you can, for instance, there's an x four in both of these two polynomials, h three and h four. And so I could essentially use one of the equations and substitute for x four. Or a different way you could say it is, say, take this leading coefficient of h four and multiply it by h three. Take the red leading coefficient for h three and multiply by h four. And now they have exactly the same leading coefficient in terms of x four. And so if you subtract the two polynomials, you should get rid of the x four s, and that could become our new h three with x four removed. So that gives us this triangulated system.
00:44:15.214 - 00:45:03.774, Speaker A: Don't look over the equations, but the main thing that you're supposed to be getting here is that these, you've got this kind of triangular system with you adding one variable each time. And then. So I've kind of shown you one way to try to triangulate a system using this kind of substitution, if you wish. Or you could think of it in grobner basies, we would have called this an s polynomial. We did a computation like that. But unfortunately, it turns out that you can't always triangulate a system like this. You can write down systems that you can't triangulate using these techniques.
00:45:03.774 - 00:46:04.564, Speaker A: Instead, what you might have to do is factor some of these polynomials. For instance, if you had x four appearing to the second power here, then you might be in a situation where you're kind of like, oh, I have to substitute for it. I'd have to solve for x four. And that's not so easy. I'd like to try to factor this polynomial. And factoring that polynomial in x four means factoring over an extension field, say, of the rails, by all the other variables. So you're working over a field and you're trying to factor polynomials, and all of a sudden you get into complicated territory where we're doing kind of sophisticated questions in algebraic geometry, really about whether you, whether certain varieties are reducible or irreducible over the wheels.
00:46:04.564 - 00:46:56.200, Speaker A: Pretty complicated stuff. Just so it's not, it's not so simple in those cases. So I'll just say that if this variety is not irreducible, then you're going to fail to triangulate, and then you have to factor and consider each of the components in this variety separately. That becomes complicated. Okay, but we managed to triangulate. So what do we do next? So we'd like to reduce the conclusion against each of these hypothesis polynomials. And essentially, the idea is you're going to use h four to get rid of the x four in the conclusion, h three to get rid of the x three in the conclusion.
00:46:56.200 - 00:47:34.894, Speaker A: Etcetera, h two and h one. They're all going to kind of get rid of the x's. And eventually you're just going to be left with a polynomial, hopefully, in the US. And that polynomial, if it's non zero, then your theorem just isn't true. But if it is zero, then your theorem is proved. Essentially, you've shown that at C is a polynomial combination of h one through h four. Actually, there's a little bit of this side condition business as well.
00:47:34.894 - 00:48:27.486, Speaker A: So let me just show you that. How do you reduce c? Well, here's c. And what you do, we're going to try to use h four to reduce c. And so h four has the x four in it. So I'd like to kind of get rid of the x four in C using a reduction. So what am I going to do? I'm going to multiply C by this blue leading coefficient of the x four, and I'm going to subtract this red leading coefficient of c times h four from the resulting polynomial. So I take minus U two c, and if you wish, you can kind of expand minus two uc as a copy of two u three h four.
00:48:27.486 - 00:49:05.492, Speaker A: So I've taken a copy of this multiple of h four, and these two polynomials have the same leading term. So this contributes something to the polynomial c. And then there's some kind of remainder. And the remainder doesn't involve any x four s, which is very nice. In fact, it's this remainder. And then you would kind of try to further reduce the remainder term here. Using h three, you get rid of the x three's at the potential cost of substituting some x two's into the problem or x one's.
00:49:05.492 - 00:50:01.106, Speaker A: Then you get rid of the x two's using the h two, and then you get rid of the x one's in the final remainder by using h one. And what we'll do each time is multiply c by one of the leading coefficients by the leading coefficient of the one of the hypothesis polynomials. And in fact, here things are a little simple. You only have to multiply by the leading coefficient once. But in some situations where x four, for instance, appears to a higher power, you may end up having to multiply multiple times by the leading coefficient. And this whole algorithmic setup is, I think, due to mathematician writ. And one complication in all these things which I haven't really spelled out here is that we're working over the real field here.
00:50:01.106 - 00:50:40.094, Speaker A: We're not working over the complex numbers. We'd like to be able to work just with real setups. And so algebraic geometry over the real numbers is its own complication, its own complicated kind of body. And I'm just going to refer you to Faye's lectures next week. But that, so what do you do now that we've reduced c. Well, you finish the reduction. So after you've reduced c, you've got leading the powers of leading coefficients of the various hypotheses polynomials.
00:50:40.094 - 00:51:22.510, Speaker A: Not sure what the four and the one here are doing. Sorry about that time. C. And this is equal to a polynomial combination of the hypotheses plus some kind of remainder term. And then the point is that if the remainder term is zero, then you've got exactly what you wanted. You've got this g over here, times c is a linear combination of the h's. And so c should be zero when the h's are all zero, at least subject to the leading coefficients here, not zero.
00:51:22.510 - 00:52:34.138, Speaker A: The leading terms, I guess I should call them the leading coefficients, really. The leading coefficients are the side conditions. So if this remainder is zero, and it is in our example, then your theorem is true, subject to these side conditions. And if it's, if it's not zero, then your theorem is actually generically false, is my interpretation of that. So this works pretty well for simple geometric statements like the one that we're just working with. But there are some complications, and some of the complications involve, for instance, problems triangulating the system. And in that kind of situation, you may end up having to do significant amounts of computation to reduce the hypotheses to appropriate subsets that you then have to verify the condition over each of the components of the hypothesis variety, etcetera.
00:52:34.138 - 00:53:15.462, Speaker A: So it's kind of complicated thing, potentially, but when it works, it works well. That's what I'll say. There used to be an advertisement for certain beer in the Maritimes in Canada, and they said, those who like it, like it a lot. I'm not sure exactly what that was supposed to mean, but I would say that when Wu's method works well, it works really well. When it doesn't work well, should maybe look for a different method. So. Okay, that's what I had to say about Wu's method.
00:53:15.462 - 00:53:21.434, Speaker A: Miri, you probably had some other ideas about what should be said about Woo's method.
00:53:22.134 - 00:53:44.636, Speaker C: Well, yeah, this. An example or two would have been good, but you didn't have time. It's okay. The other thing was this idea of the characteristics set thing, you know, on the triangular set, whatever. Maybe we might have some time to look at it, maybe later, at some point.
00:53:44.700 - 00:53:51.684, Speaker A: So are those, these given by these triangularizations, like.
00:53:51.764 - 00:53:52.864, Speaker C: Yeah, I think.
00:53:55.444 - 00:53:57.908, Speaker A: Were they. The leading terms of this went out.
00:53:57.956 - 00:54:03.864, Speaker C: A little, went a little fast for me here, too. Parse the variables and stuff, but.
00:54:09.844 - 00:54:16.068, Speaker A: Okay, we'll talk about it at some time. That would be fun. I'd like to understand more about Woo's method.
00:54:16.196 - 00:54:18.812, Speaker C: Okay, we'll do that. Yeah.
00:54:18.868 - 00:54:28.844, Speaker A: Okay. So that's. That's the end of what I had to say. Thank you for your time. And thanks for giving me the chance to talk.
