00:00:00.400 - 00:01:27.864, Speaker A: All right, let's start. Disoriented by this music stand. Not being a musician, don't really know how it works. So even though we're trying to avoid excessive reliance on this thing, today will be Fourier transform day, so that we can sort out a few things. So what we want to do is understand what happens at t is equal to zero. In other words, understand these symbol families. And through a process that I began to explain to you last time, we want to create some at least interesting scalable operators.
00:01:27.864 - 00:02:34.653, Speaker A: We need a few operators just to get the whole theory off the ground in a reasonable way. I mean, there are differential operators, but if there were only differential operators, we'd be wasting a lot of energy on this nonsense. And so what we want to construct are a whole bunch of operators, e depending on some parameter n. You could make this parameter n a real parameter parameter. But the easiest, laziest thing for us to do is to let n be an even integer. That's just because there's a natural order two operator out there, and e one is just going to be the Laplacian, and e two is going to be the square of the Laplacian and so on. So we're still just working on rn.
00:02:34.653 - 00:03:30.044, Speaker A: Oh, well, you could do that, but then that just requires a bit more energy. I mean, how exactly are you going to take the square root? I realize there are too many ends in this picture, so let's change this index to k, see if that works, and these operators are going to behave in the following way. First of all, e zero is just going to be the identity operator. We don't really care if it's not exactly the identity operator, but I guess you could always make it the identity operator. And en guess I didn't write it down. There are three lines I have to fit in three properties. So maybe I'll just make a bit more space here.
00:03:30.044 - 00:04:28.564, Speaker A: So this operator, en, or rather ek, should have scaling order, the order we've been talking about. Like I said, of two k. E zero will just be the identity, and then there could all be zero, except for the e zero. At the moment, the crucial thing is they form a sort of group. So that ek, composed with el is e k plus l. Well, that's a little hard to manage on the nose, but if you throw in a smoothing operator, then you're in the territory of our theory. Yeah, you can.
00:04:28.564 - 00:04:49.982, Speaker A: So I really want, secretly, I want to do the easy thing, which is to let e one, excuse me, e two be the Laplacian. So e one would be a sort of square root of the Laplacian, according to the rules that I wrote down here. And for sure, you can build a square root of the Laplacian in the world.
00:04:50.128 - 00:04:51.314, Speaker B: My question to the full face.
00:04:51.354 - 00:04:51.754, Speaker A: Oh, sorry.
00:04:51.794 - 00:04:56.094, Speaker B: The scaling order of the lampars would be two.
00:04:56.634 - 00:05:10.458, Speaker A: Oh, shit. I see that. The way I've. I have too many twos. Yeah, I see. Thank you. I was answering the wrong.
00:05:10.458 - 00:05:26.914, Speaker A: A different. Some other question. Yeah, maybe I don't. Yeah. Just because I'm too lazy to take the square. Build the square root of the Laplacian. Okay.
00:05:26.914 - 00:05:54.946, Speaker A: Since you're students, you can build this. Take the square root of the Laplacian by yourselves. Yeah. It's not a big deal. All you need, all I really want, are enough operators to make the theory tick in a reasonable way. And this is enough operators. And if you really want square root, if you want to index these fellows by real numbers, integers, rational numbers, knock yourself out, whatever you like.
00:05:54.946 - 00:06:12.708, Speaker A: That's fine. But this is sort of the minimum. Okay. Yeah. The fact that they're even is neither here nor there. If it was multiples of three, there should be enough of them so that there are really high order operators in this world, and also really low order operators, and they're invertible there. The high order.
00:06:12.708 - 00:07:23.264, Speaker A: I mean, they're mutually parametrics, I guess, is the right way of saying it. Okay. That's what we're going to use the Fourier transform for, just to see that we can understand what's happening at t is equal to zero. Understand symbol families well enough that we can proceed and make this construction. If nothing else, it's a sort of test of the theory, isn't it? I mean, if you can't construct anything at all, then why bother? But you can. Look, we have all of these wonderful operators, all of them called e or easy, because, well, these fellows are going to be easy translation invariant operators. Okay, so let's start with a symbol family, not particularly relevant at this point, whether or not it's compactly supported.
00:07:23.264 - 00:08:44.480, Speaker A: We're going to be discussing a little bit of Fourier transform in order to tidy up the last thing I was talking about on Tuesday, and after we've tidied up the last thing that we talked about on Tuesday, then we'll tidy up the next to last thing that we talked about on Tuesday. I guess I talked about on Tuesday, which was some theorem which said that if the symbol consists of invertible operators, the operator is invertible, more or less modular smoothing operators. So we'll go back to that in due course, that will be used to construct these guys. And so the grand conclusion of the class today will be that these guys exist. All right, so what we want to do is take the Fourier transforms of the operators s in order to try to understand them better. If we were working in one of these fancy situations where there isn't a really good Fourier transform, or as good a Fourier transform as there is in Rn, then we'd have to work a lot harder. At this particular point, there's no getting around it.
00:08:44.480 - 00:09:33.132, Speaker A: But fortunately, because of Fourier transform, this is not going to be too painful. So we're going to take not a but s omega. And I want to apply it to a function like you do in Fourier theory, which is the function e for the minus I x psi. Psi is another point in Rn. So when I write x times psi, I mean the dot product of x with xi. So the thing that I applied s to is a smooth function of x on Rn, and there's one such smooth function for each psi. It's obviously not a compactly supported function, but these are properly supported operators, these s omegas.
00:09:33.132 - 00:10:27.832, Speaker A: So apart from mapping, like I wrote up there, smooth, compactly supported functions to smooth, compactly supported functions, they naturally extend to maps of smooth functions to smooth functions. And that's what we just did. We applied s omega to the smooth, but not necessarily compactly supported function, not at all compactly supported function, which is this exponential. And now I just want to evaluate zero like that. So, once again, back to this proper support business. If I'm evaluating at zero like this in order to make sense of this expression, given that s omega has compact support, all I need to do is multiply this function by a bump function, which is identically equal to one in some huge ball around zero, and then I'll be able to make sense of this. And the point is that the hugeness of the ball is irrelevant.
00:10:27.832 - 00:10:49.264, Speaker A: After the ball exceeds a certain size, it doesn't matter what the bump function is. Yes, sorry, zero there. Yes. So this thing is a function on rn, a function of x. And so I'm going to evaluate this thing, which is a function of x at. .0 c is at this point a parameter, and c is a parameter.
00:10:49.264 - 00:11:13.394, Speaker A: But we can think of this as a recipe for a function that we might call sigma, and it depends on omega. Sorry. Like that. So all I tried to convince you is that this is sort of, you know, logical. Is this in your way? I don't know where to put this thing. Maybe I can just go down. Oh, it does.
00:11:13.394 - 00:11:22.474, Speaker A: Great. Now I can lecture at a lower height. And if I want to sit down and play the cello. Okay.
00:11:24.734 - 00:11:28.214, Speaker B: Transform variable. And this, I mean, x is the variable that.
00:11:33.714 - 00:11:56.164, Speaker A: Yeah. So this here, this is a certain function of x, so to speak. So if you like, you think we can think of it as lying in c infinity of Rn. And as I just applied, discussed, a. Sorry. S does in fact act on infinity Rn. Okay.
00:11:56.164 - 00:13:23.204, Speaker A: And so this thing here that we just built is just a function, a smooth function of xi, and for that matter, omega, because we're dealing with smooth families. So this is the Fourier transform, if you like, of the operator s. If you think of the operator big s as being convolution by some sort of generalized function, little s, then this is the Fourier transform of that generalized function, that distribution, little less. That's what we're talking about here. And because every function can be decomposed according to Fourier theory, Montreal formula into these modes, e to the ixi. In principle, once you know these fellows, you know what is the operator s, you can reconstruct s omega applied to any function f. Let's say a smooth, compactly supported function f, by decomposing f into Fourier modes, writing down its Fourier transform, writing down the Plancherelle formula for f, and then just applying omega to these, and you can pass the s omega, so to speak, through the Planche r integral, because s is a continuous function, continuous operator.
00:13:23.204 - 00:13:51.868, Speaker A: And the integral, the Fourier integral is really nice because Fourier transform of a smooth, compactly supported function is a Schwarz function. I shall remind you about Schwarz functions in due course. So, once we know these fellows, we know everything. That's the, the idea. And so we might as well get to know these guys. And so the question is, what kind of a function is this? We had an operator before, and now it's just a function that seems like progress. And for that matter, it's a smooth function.
00:13:51.868 - 00:14:18.008, Speaker A: That's even nicer. No weird singularities. If you look at the, as we've seen, the integral kernels defining the sorts of operators we're interested in, they have all sorts of singularities. We looked at one example, which was log of the absolute value of x on the real line. Convolution with that, well, that's, you know, that's no good. And even worse, one over x. We looked at convolution with one over x.
00:14:18.008 - 00:15:54.130, Speaker A: That's even worse because one over x is not integrable. But all these guys are just functions. So we want to know what kind of functions are they? And what we have to go with is the notorious scaling relation. I don't think I wrote down what degree this family is, but suppose it has degree mix. Then the scaling relation says that when you do this, you get back the operator s omega pretty much up to this factor, and there's a small error, which is a smoothing operator. So this is some smoothing smooth like family of smoothing operators. And I'm going to invoke here a fact which I didn't build into the definition of symbol families or any of other of the families that we've been looking at.
00:15:54.130 - 00:16:55.424, Speaker A: You don't have to because you can prove what I'm about to write down. The proof is, requires a certain amount of exercise, requires a little, as far as I'm aware, and Bob and Erica are aware, a little excursion into the bear category theorem and so on. Terrible, right? Yeah, yeah. It took me a while to figure out what they meant, but eventually I did. Anyway, it's a fact that this guy here, this family, where you look at all of the omegas and all of the lambdas, I mean, these, these scaling factors are all supposed to be positive. So this also is a smooth family of smoothing operators. So it depends smoothly k, that is, k lambda depends smoothly on lambda.
00:16:55.424 - 00:17:46.564, Speaker A: So if you were to write the integral kernel of big k omega lambda as little k omega lambda of XY, then you'd have a function of four variables, omega, lambda, x and y, and it's c infinity in all of them, all at once. That's what's going on. And that's a fact that was observed by Bob and Eric. And the proof is explained in a series of exercises in. Maybe it's the exercises to this lecture or the previous lecture, I don't remember, or the next lecture, I don't know. I wrote it and I can't remember where I wrote it. Or for that matter, if it's in the exercises that you currently have access to.
00:17:46.564 - 00:18:30.774, Speaker A: And we're just going to take this for granted. Why are we going to take this for granted? Well, we're going to take this for granted because in a little while, when the crowds are clamoring for it, we're going to introduce the tangent groupoid, when you can just bear no longer not knowing what the tangent groupoid is. And when we introduce the tangent groupoid, we shall redefine what is symbol family and a scaling family and scalable operators and so on. And we'll build in, because you can do it in a nice way on the tangent group. We'll build in this basic fact as an assumption from the beginning. We won't get any different theory because of this fact proved by Bob and Eric. So I'm just going to borrow this now.
00:18:30.774 - 00:19:14.234, Speaker A: But for those who are keen on the bear category theorem and such things, the argument which proves this is in the exercises somewhere. Yes, I can see that it's a little confusing to begin with. I mean, our assumption was that this was smooth and omega alone, but in fact, it's also smooth in lambda. Yeah.
00:19:22.814 - 00:19:27.750, Speaker B: Shouldn't be a problem. And on the other side, you just translated, how is that going to ever create a problem?
00:19:27.902 - 00:19:28.714, Speaker A: Yeah.
00:19:29.254 - 00:19:30.354, Speaker B: Differentiated.
00:19:31.034 - 00:19:48.334, Speaker A: Yep. Yep. It. It sure seems like there should be a one liner for this. It's not, as far as I'm aware, not just algebra. Yeah, I don't know how to prove this without. Without the back ethereum.
00:19:48.334 - 00:20:49.234, Speaker A: Yep. Yeah. You use the back category theorem just to get going. I mean, the first thing you might ask, let's assume the family is compactly supported in Omega, the original family, that is to say, the original family s. Then the first thing you might ask for is whether these k omega lambdas are all given by little integral kernel functions, little k, comma, sub omega, lambda, of x y. You might ask, are those functions uniformly bounded in x and y and omega and lambda, at least over compact sets? You might ask that, and that's not obvious, but back category tells you that there's some interval of lambda space where those things are uniformly bounded, and then you're off to the races. Well.
00:20:49.234 - 00:21:12.734, Speaker A: Right, yeah. Category theorem, even choice that we dig.
00:21:12.774 - 00:21:14.806, Speaker B: Down, drill down into the.
00:21:14.950 - 00:21:38.446, Speaker A: Wow, that's. Those people are extremists. Yeah. Right, right. Yeah. You know, there are lots of things out there that are going to kill you. I think the bear category theorem is not going to kill you.
00:21:38.446 - 00:22:26.956, Speaker A: Just don't worry about it. Use it whenever you want to use it. Okay. So thanks to this, what you can do is you can take the relation at the top, and you can think of it as a family of relations, a smooth family of relations in some sense, as lambda varies, and you can differentiate it with respect to lambda, and then you can set. And it's best to think of lambda as e to the t. So think of lambda as e to the t, and then differentiate with respect to t, and then set t is equal to zero, and that derivative is going to exist. And what it will tell you by differentiating the r lambdas, you'll get the notorious.
00:22:26.956 - 00:23:25.634, Speaker A: I can't call it e because I just used e there. So let me c kind of looks like e, I'm going to call it with v s omega and these k's which don't have a lambda. This is some new family of k's which you get by differentiating the old family of k's with respect to lambda and then setting which is e to the t. So you're really differentiating with respect to t and then setting t equal to zero. So that's what the k omegas are. They're the derivatives of this family in lambda equals e to the t at t is equal to zero. And what's v? V is the friend of many people in many different parts of the world.
00:23:25.634 - 00:24:18.614, Speaker A: It's the Euler vector field where you sum not from one to infinity, stuff at n like that. This is the vector field which generates the flow on rn, which is just scalar multiplication by e to the t. This is the differentiated form of the scaling relation. And this guy here in the other direction certainly implies the scaling relation which is at the top just by integration. So this is a better way of saying what is the scaling relation? And now, well, you have to know what is the Euler vector field. But you don't have to worry about these R lambdas anymore. The formulas tend to get a little bit easier.
00:24:18.614 - 00:25:18.138, Speaker A: And we can do this not just for symbol families, which is what we're discussing now. You can actually do it for these scaling families as well. But the way we have things set up as of today makes the formula just a little complicated to express when you're discussing the whole theory of scaling families and scalable operators, which is why we didn't do it yet. When we introduce the tangent groupoid, finally, thank heavens, we'll have it at some point, then we'll see that there's an extremely simple way of writing down what this is, not just on the t equals zero part of the story, but on the whole story. And we can just define pseudo differential operators, scaling scalable operators by this formula. And then it's just, it's nicer and it's stronger and you don't need the back category theorem anymore. Yeah, yeah, yeah.
00:25:18.138 - 00:25:40.194, Speaker A: And we would have done it from start, except it's just, it's a little messy to, I mean, the whole definition of scalable operators was already a little messy because we had those r omega lambdas and then we an r omega t's. And then we had to worry about what happens at t is equal to zero. And it was all had a multi component aspect to it, this definition. So we'll improve on that. Yeah.
00:25:45.174 - 00:25:47.194, Speaker B: Definitely worry about a fair category.
00:25:47.554 - 00:26:11.374, Speaker A: I would say that. Yeah, it. I mean, it would. It'll be exactly the same theorem, thanks to the theory, thanks to the bear category theorem, if you assume smoothness from the beginning. And I would say that in retrospect, they should have done it, but they didn't. But. Okay, there you go.
00:26:13.254 - 00:26:24.754, Speaker B: Go between these two things. You just apply it to a function, translate it, and instead you got lambda 14 to the minus b. And that will determine.
00:26:27.254 - 00:27:16.766, Speaker A: Yeah. Right. If you take r lambda and let lambda be e to the t and then differentiate r lambda with respect to t, what you get is a new operator, which is the composition of r lambda with the application of this vector field v. So in other words, the r lambdas are the, if you think of them as just acting on rn, they're the one parameter family of diffeomorphisms, the flow generated by the single vector field v. In the language of differential equations. Yeah. Which is exactly the formula for lambda, since lambda is our lambda, since lambda is e to the t, they're like homogeneous functions of order minus m.
00:27:16.766 - 00:28:18.522, Speaker A: Except even though this is the actual Euler vector field, not some weird Euler like vector field, there's still an error term. And that's the way it is, this formula. Yeah, this formula is exactly for constant coefficient homogeneous operators of degree m, but not beyond that. All right, good. Now you can take this relation, this one here, and just apply it to e to the I like we were doing over here, and you'll get a formula. And here's the formula that you get. It's kind of nice.
00:28:18.522 - 00:28:51.714, Speaker A: It just says that I have this language here, sigma. On the right hand board, there's a small change in sign like this. Yes. I should call this omega. And then there's some little thing, maybe we'll call it kappa of omega two. Maybe kappa omega. Lambda of omega.
00:28:51.714 - 00:29:23.602, Speaker A: Sorry. There we go. Well, I haven't done what I'm. This. It's not clear what this refers to. It refers here to the theorem of van Earp and Jankin. Don't need the Euler vector field.
00:29:23.602 - 00:30:24.194, Speaker A: I'm using here the actual rescaling. I will soon differentiating with respect to lambda. But this is a straight translation of that law at the top there, with one additional fact, which is that this error term here. So this fellow, which is the Fourier transform of k, defined exactly like this, but using k instead. This thing is now going to be smooth in Omega and Lambda. So this is what the, the back category theorem buys us smooth. What family of functions in the Schwarz space of rm or smooth family of Schwarz functions.
00:30:24.194 - 00:31:20.174, Speaker A: Because what we're doing here is we're taking with these little kappas, the Fourier transforms in the sense of the left hand board of these operators, big kappa of omega. I should have been pointing up there, operators big kappa of omega, lambda. Those are smooth in omega and lambda. They're smooth, compactly supported functions in omega and lambda. And when you take the Fourier transform of a smooth, compactly supported function, you get one of these Schwarz functions. Let's just remember that the short space is just functions. Let's just call them f's, with the property that for all indices alpha and beta, if you take the largest possible value overall, let's say these are functions of psi.
00:31:20.174 - 00:32:02.214, Speaker A: And what am I supposed to put here? Psi to the alpha, partial to the beta, f at sine. This is finite. So these functions, you can differentiate as many times as you like, and then you can multiply them by any old polynomial, and they're still bounded. That's what a schwarz function is. Okay? And each one of these is some kind of seminar on this. It is a seminar on this space of functions. And when I say smooth here, I mean smooth as a map into this vector space equipped with all of these different seminars.
00:32:02.214 - 00:33:22.316, Speaker A: And now we'll do the analog over in Fourier land of this thing here. And it's a nice way of stating this, which is to say that away from zero, let's say for, let's just move outside of the zero vector by a certain amount, for reasons that you'll see, in fact. Okay, fine. It doesn't have to be one. Here's what I want to look at. I want to look at the sigmas that we have defined on the left hand board. And I want to ask, how big are they compared to this homogeneous function? So psi to the mix is a function which has this homogeneity that you see on the top, on the nose.
00:33:22.316 - 00:34:32.158, Speaker A: Of course, psi to the m may not be quite a smooth function at psi equals zero, but otherwise everything is fine. This thing is equal to m times sigma of omega psi plus a Schwarz function. And in the sense we were just describing, smoothly varying with omega. I guess I could have given it a name. So all of these functions are homogeneous functions with a little bit of an error thrown in, plus a Schwarz function, because homogeneous functions can have singularities of zero. When I say Schwarz function, I mean restriction to the complement of a ball of a Schwarz function. Otherwise the theorem can't be quite true.
00:34:32.158 - 00:35:01.284, Speaker A: If I'm as not a non negative even integer or something like that. Those three times. This guy is smooth. Yeah, just like this. Oh shit. There is a stupid error. And.
00:35:01.284 - 00:35:37.784, Speaker A: But, but not there. Here's the. I'm combining my apologies into the proof into the statement of the lemma. The proof of it. That's what I should have written. My apologies. We're almost there, v.
00:35:41.204 - 00:35:44.740, Speaker B: Yeah, we do that all along.
00:35:44.812 - 00:36:03.640, Speaker A: Yeah. Yeah. Well, yeah, in a manner of speaking. I knew it too. Anyway, now I think we're in business. So now it's a straight translation of this. No.
00:36:03.640 - 00:36:57.220, Speaker A: Okay. Why did it become plus m and not minus m? And before there was a lambda inverse? And now it's okay, you do the math. And this is how it works out. All right. We want to take this just one step further in order to understand these functions. And the one step further was what half appeared in the garbled version of this lemma. So let's see if we can do this properly.
00:36:57.220 - 00:37:49.244, Speaker A: Now, of course, this statement here no longer requires this. You can just get rid of that. It's still a true statement, but you do require it here. Now I want to take Sigma of Psi and divide by this function here. And now let's think of a way to state this. So what I would like to say is that this basically doesn't depend on the size of sigma. We can normalize not sigma, but psi.
00:37:49.244 - 00:38:36.636, Speaker A: We can normalize psi. So it has length one. And nothing is going to change here up to a Schwarz function. I guess the right way of saying that is that there is some smooth family of functions, sigma zero. So these are defined on the unit sphere. So that, hey, this guy here only depends. That's sigma zero, sigma omega zero, zero, omega zero omega.
00:38:36.636 - 00:39:33.434, Speaker A: Like that, up to a little error, which is this type of error over here. So the final conclusion is that each one of these symbol functions is an actual homogeneous function of order m, namely that sigma zero function times the norm function plus the Schwarz function.
00:39:38.014 - 00:39:43.074, Speaker B: What are some nice families of sigma omegas that we should be keeping in mind?
00:39:47.354 - 00:40:14.994, Speaker A: You take any smooth function on the sphere. You define sigma omega to be that smooth function of psi. You define sigma omega by means of this formula. That's what the theory is built out of. Yeah, the. I mean, the obviously polynomial, homogeneous polynomial functions fit into this context. And for those, you don't need the Schwarz error function.
00:40:14.994 - 00:40:56.368, Speaker A: Okay. Yeah, it's like saying that, but it's, I don't know, a little bit better. It's saying it actually is a homogeneous function plus a little bit extra. I mean, all of these laws remain lords. If you, if you take the basic objects, the s omegas up here, the original operators, you just add to them a bunch of smoothing operators. So there's always going to be some, according to the way we've defined things, there's always going to be an error term in all of these formulas. But the point is that it's never worse than this.
00:40:56.368 - 00:41:05.344, Speaker A: And there really is at the end of the rainbow, a beautiful smooth function, smooth, homogeneous function, which is determining everything up to a tiny, tiny error.
00:41:14.044 - 00:41:22.504, Speaker B: Is there some sense in which significant operators or Schwarzman are actually small? I mean, I know that morally we understand them as small errors, but.
00:41:24.924 - 00:42:04.664, Speaker A: If you're measuring. So let's just think about what we're doing here. We're attaching now to any scalable operator of order m, a principal symbol, so to speak, which is such a function as this. You know what it would be a brilliant idea to call this not sigma zero, but Sigma M like that. Okay. Somewhere it could be really big. Yeah.
00:42:06.644 - 00:42:16.460, Speaker B: But the only way we can detect it is by, we can only get rid of it. The noise is by going to infinity, where it disappears.
00:42:16.572 - 00:42:17.304, Speaker A: Yep.
00:42:17.644 - 00:42:28.392, Speaker B: At the ground zero, we can be anything we want that you're allowed to throw in any function you want. So the only way to detect it is that infinity.
00:42:28.568 - 00:42:47.204, Speaker A: Right. So it's true, therefore, that a short smoothing operator need not be small in some traditional sense of small. It could send some innocent, tiny little bump function to 100 billion kajillion times itself. And it could still be a smoothing operator for doing that.
00:42:48.024 - 00:42:49.244, Speaker B: That's what I mean by.
00:42:50.184 - 00:43:19.390, Speaker A: Yeah. So now let's think about what we've done. Suppose you have a scalable operator of order m. Then it may be extended to a scaling family of degree m. That's, after all, the definition. And that thing gives rise to a symbol family of degree m. And once you have the symbol family, what you're talking about modulo these smoothing operator perturbations, is a single homogeneous function of degree m, or in other words, a single function on the sphere.
00:43:19.390 - 00:44:04.358, Speaker A: I mean a single family of functions parameterized by omega. So the principal symbol, which is the family s, omega modular smoothing operators is exactly the same thing as we'll see in just a moment, as a smooth family of smooth functions on the n minus one sphere, the unit sphere in Rn. And if you had a smoothing operator to begin with, it of course, would be a scalable operator of order m, and this thing would be zero. Fine. If this thing is zero, then we know that this scalable operator actually is an operator of order m minus one. And so we can repeat and attach to an operator of order n minus one. It's m minus one principal symbol, which is defined in exactly the same way except for m minus one.
00:44:04.358 - 00:44:41.624, Speaker A: And if you started off with a smoothing operator, it too would be zero. Et cetera, et cetera, et cetera, when you take the space of all scalable operators, in other words, the space of all pseudo differential, classical pseudo differential operators, it's possible to put on it various topologies. And the good ones tend to come from measuring the size of these functions here. And in that point of view, all of the smoothing operators in the closure of zero. So in that sense, they're small, they're all limits of zero. Can't be smaller than that.
00:44:42.644 - 00:44:50.464, Speaker B: What about this field as like the event arising from a black hole?
00:44:58.324 - 00:45:43.894, Speaker A: No, no, you're in the wrong world, because we took the Fourier transform, the inside this sphere is this space of size, or all possible size. If you like, you can put this sphere that I'm now talking about as the celestial compactification, the spherical compactification of Rn, which Rn. The Rn which is where all of the xi's live, and inside that space where all of the KSis live, or on that space where all of the KSIS lives. This symbol, sigma omega, is a perfectly respectable function. Smooth function, singularity live. Back in, I don't know, configuration space, so to speak, not in size space. So, sorry.
00:45:50.314 - 00:46:02.918, Speaker B: But we're allowed to add to it any smooth function in line. And then somehow that's supposed to be meaningful, right? Like it's substance controlled at infinity.
00:46:02.966 - 00:46:42.574, Speaker A: Like, oh, which the functions we're talking about of xi are homogeneous functions of xi outside of a little region around zero where homogeneity is problematic. And up to Schwarz function. What is up to a Schwarz function mean? It just means you take an actual homogeneous function and you add to it a Schwarz function. That's what we're talking about. You normalize it. Yeah. If you divide it by psi to the m where m is the degree, then.
00:46:42.574 - 00:47:28.394, Speaker A: Well, if it was homogeneous of degree m, then you'd get a function on a sphere which would tell you everything about the polynomial. Okay, good. Oh, yeah. So maybe we should just say a little bit about how this is proved. Right? That's me. Well, you can just take this function as a small problem with this quotient xi equal to zero. So forget about that.
00:47:28.394 - 00:48:25.834, Speaker A: Still a smooth function on the rest of rn and you just differentiate it with respect to this Euler vector field. When you differentiate it with respect to the Euler vector field, this is what you get as far as sigma is concerned. And as far as the norm of psi to the m function is concerned. You also know what you're going to get the same thing. And by the, what's it called, the product quotient formula for derivatives, what you see is that when you take the derivative of this thing, the only thing that survives is this Schwarz function, the derivative of this Schwarz function divided by psi to the m. So you just made it even more Schwarz by dividing it by psi to the m. So this is a function whose derivative with respect to the Euler vector field is a Schwarz function.
00:48:25.834 - 00:48:59.906, Speaker A: Let me say that again. This quotient function is a smooth function of psi. And when you differentiate it with respect to the Euler vector field, what you get is a Schwarz function. That doesn't mean that this quotient is a Schwarz function. You wouldn't want it to be a Schwarz function because it's supposed to be converging to this thing. But it does mean that if you subtract off this. Well, first of all, because the derivative of the quotient is a Schwarz function, that means that there are radial limits of the quotient.
00:48:59.906 - 00:49:51.174, Speaker A: The function is varying so slowly. All of the derivatives are varying so slowly, like in the definition of Schwarz function, that for sure this function is going to have radial limits. In other words, if you take sigma omega of lambda psi divided by the norm of lambda psi to the m, and you let lambda go to infinity, there will be a limit. And you define that limit to be this function. And if you subtract off this function, now you have a function which is a Schwarz function whose limits at zero, excuse me, at infinity, are all zero. And now it's a little exercise that if you have a function whose derivatives with respect to v are Schwarz functions, and if the limits of the functions are all zero to infinity, then indeed it was a Schwarz function to begin with. So that's how this is proved.
00:49:51.174 - 00:50:44.404, Speaker A: Just calculus. And now we know almost exactly what these s omegas are. If you believe that sigma tells you everything about s, and this tells you everything about sigma, then you're in good shape. Okay, there's just one more thing that you might want to think about, which is this. Oh yeah, let me say one more thing. Apropos of our previous discussion of small operators, suppose you had a symbol family, big s, from which you got this family of functions, little sigma and from those little sigmas, you built the other functions on spheres, little sigma m. Suppose after you did all of that, you ended up with the zero function.
00:50:44.404 - 00:51:25.174, Speaker A: Suppose sigma, sub m comma omega, was the zero family of functions. What then? Well, if these functions are zero, then this function sigma is going to be a Schwarz function. And if little sigma is a Schwarz, I mean, it'll be this norm of psi to the m times a Schwarz function. But that's just another Schwarz function. So little sigma will be a Schwarz function. And now you can check very easily that that means that s omega is a smoothing operator. So if the m degree, degree m symbol is zero, then indeed, you're talking about Schwartz.
00:51:25.174 - 00:52:26.800, Speaker A: You're talking about smoothing operators in this context. Okay, so the symbol, this degree m principal symbol, let's give it a, let's just do that. Give it a nice fancy name. Well, there's a whole bunch of operators, one for each omega, so maybe we should call it a family. If this family is zero from, we started with a big s omega family at the top, then it's because the big s omega family was smoothing operators. And we saw that if we had two symbols families whose difference were smoothing operators, we've trained ourselves to think that those two families are exactly the same because we couldn't nail everything down to any accuracy better than modular smoothing operators like that. So the symbol, in other words, is telling us everything.
00:52:26.800 - 00:52:36.084, Speaker A: The degree m symbol of a degree m symbol family tells us everything about that degree m symbol family, up to smoothing operators.
00:52:41.204 - 00:52:43.944, Speaker B: One. Then you're critically order m, right?
00:52:44.484 - 00:53:32.238, Speaker A: No, for scale apple operators. But here we're talking about something a little different. We're at t equals zero, and the scaling law for these fellows is a little more strict because there's a version of the scaling law like you see it at the top. If you're talking about scale ing families, there's a whole one parameter family of operators. There's an extra parameter t, which fits into the picture. And you're not comparing in the scaling relation the operators at a given value of t to the operators over here at the same given value of t, you're comparing the operator or the function in this case at the value t to the operator or function in this case of the value lambda times t. So you can't get as much as stronger conclusions out of that.
00:53:32.238 - 00:54:06.654, Speaker A: In fact, we know for sure that the symbol of a scalable operator only sees of order m, only sees the operator, so to speak, in degree m. If you add an order m minus one symbol excuse me. If you add an order m minus one scalable operator to an order m operator, and then you ask what is the degree m symbol? The answer is that the thing you added, the order m minus one operator, doesn't contribute to that symbol solution operator, have the order m cannot.
00:54:06.774 - 00:54:15.314, Speaker B: Okay, but that's, so you said it's sigma sigma m zero. Then it was moving. Right, but I think you want Sigma m zero for all.
00:54:17.174 - 00:55:07.954, Speaker A: So now you're, now we're asking the question, what did I say? I have no idea what I said. I just blather on you. Yes. If sigma m is zero, so sigma m is something that you can attach to a degree m symbol family, and a degree m symbol family is something you can attach to an order m scalable operator. If Sigma M is identically zero for all of these omegas, then the symbol family Big s of omega is a family of smoothing operators. And if that happens, then this order m operator that you started with, whose symbol is what we're discussing, is actually an order m minus one operator, if and only if.
00:55:10.214 - 00:55:12.902, Speaker B: That's what I said, that an order m minus one is.
00:55:13.078 - 00:55:49.656, Speaker A: Well, now we're discussing what you said, and I have equally no idea about that. You know, let's agree that we were. All right, okay. If you're talking about families, these famous scaling families, yes. If you're talking about single operators, scale abel operators, no. You can add an order three operator to an order four operator scalable operator, and the result is an order four scalable operator, and the principal symbol, a degree four symbol of that sum, will not see the order three bit that you added.
00:55:49.800 - 00:55:52.724, Speaker B: So the point is, if you start with the scalable operator.
00:56:00.864 - 00:57:03.034, Speaker A: Yeah, let's just, I can see that an example is going to, I don't know if it'll help, but it's certainly required and we'll see. So here's a differential operator of order M. Okay, suppose you now pass to the corresponding symbol operators s omega degree m symbol operators. So these are operators, the translation invariant operators. There's one for each omega. And what they are, as we discussed before, is you just take the top degree parts of this family and you make the coefficient functions into numbers by evaluating it, omega, like I'm now doing. And now you can look at the corresponding symbol function, let's call it this guy here, sigma m omega.
00:57:03.034 - 00:57:41.744, Speaker A: In this particular case, there are no smoothing operator things to worry about. There's no difference at all between this function on the sphere and the actual function. But let's just write it this way. Anyway, let's maybe was to indicate that we're talking about an element on the sphere. Let's write it like that. So what it is, is the same sum of the same coefficients. And all you do is you take I times psi as it happens.
00:57:41.744 - 00:58:00.998, Speaker A: Oh, and I have to normalize this in a moment. I guess I can do it right here over xi, and you raise it to the power alpha. So xi is actually a vector. Psi one, psi two, psi three, and so on. And alpha is actually a multi index. Alpha one, alpha two, alpha three. And so you raise psi one to the alpha one, th power, etcetera.
00:58:00.998 - 00:58:32.764, Speaker A: That is what this function is that we're talking about. And this function sees everything that there is to know about these operators. Absolutely everything, because we don't have to worry about these smoothing operator bits and pieces here. But the symbol doesn't see everything that there is to know about this operator. It only sees the, so to speak, top degree part of this operator. So two functions can have the same, two operators up here can have the same symbol function for sure, but it just means that they only, they can only differ in the lower degree terms. Lower order terms.
00:58:32.764 - 00:58:40.004, Speaker A: Does that help? Yes, exactly.
00:58:47.504 - 00:58:49.524, Speaker B: The leading coefficient is.
00:58:54.564 - 00:59:14.468, Speaker A: Yeah, I mean, there are all sorts of annoyances there. Like, you want all degree homogeneous degree m operators to be a vector space. That's a reasonable requirement. Now zero's in there, and now zero has every degree. So it's, you know, this is what we have lawyers for. Figure out this stuff.
00:59:14.516 - 00:59:15.660, Speaker B: Yeah, well, that's what we have.
00:59:15.692 - 01:00:06.574, Speaker A: Category theorem or category theorem? I don't know. All right, good. So, yeah, there's one coda to this discussion, which is kind of interesting. So what else? Well, there's a sort of converse question that you might want to ask, which is, given a symbol function, sigma m comma omega six, principal symbol degree m, symbol family, can you find an operator which it comes from? Well, this would work for a polynomial function, but wouldn't work for a.
01:00:09.434 - 01:00:11.014, Speaker B: Okay, it's a polynomial.
01:00:18.334 - 01:01:25.174, Speaker A: Yeah. So given this smooth family of functions, can we find a smooth family of operators? That's the question that you might want to know if you can find it. It's by the discussion we just had. It's unique up to smoothing operators. So that's for sure. And the only problem is, can you actually build these guys? Okay, so the obvious thing to do is some sort of inverse Fourier transform something or other is to build an operator according to a formula like this. So this is the formula that we saw a long time ago.
01:01:25.174 - 01:02:28.328, Speaker A: Where in which we'll put here the function sigma m omega. So, let's define a function now, sigma omega smooth. And let's suppose that it's defined in such a way that sigma omega of psi over psi to the m is equal to sigma m omega psi over psi to the m. Sorry, no m required. Let's say away from zero. So, given a principal symbol, let's first of all make a symbol function by homogeneity. And, well, you have to make it smooth near zero, because all symbols, like we said at the very beginning, are smooth functions like it's still up there.
01:02:28.328 - 01:02:52.634, Speaker A: So we do something else when psi is less than or equal to one to define the function. I don't care. Okay. Yeah. I'm just saying, if you have a function on a sphere, it always extends to a smooth, homogeneous function, except at zero, possibly, in which case you have to do some fiddle. That's all we're doing. Okay.
01:02:52.634 - 01:03:37.418, Speaker A: And then once you've done that, you can correct my mistake here. I can correct my mistake. I'll just put here sigma of xi, and then f hat of xi, and then e to the I x psi d xi. And I know I'm on the right track because we had a long, you know, we were peaking at home under this book. And so we know that this is basically the right formula. For example, in the case of a polynomial function, we know for sure that this gives us a differential operator. This reconstructs d, except for the lower order terms.
01:03:37.418 - 01:04:20.316, Speaker A: It reconstructs the top order part of d just from the function. So we know we're on the right track. It's doing more or less the right thing. Okay. And now you can ask, what happens when you calculate the symbol attached to this operator? First of all, what happens when you calculate sigma omega attached to t omega? And then what happens when you calculate this leading order term, this principal part? The answer is, you get back exactly what you started with. This. Exactly inverts what we were discussing.
01:04:20.316 - 01:05:12.846, Speaker A: You have to be a little bit careful for a reason that I'm about to write down. But apart from this care that you have to take right now, we're done. The only issue is that t omega need not be properly supported. And we made an agreement that all of our operators would be properly supported. Because it's not properly supported, you have to be a little more careful than I need it to be up here when it comes to applying t omega to this exponential function. So you have to ask yourself, does it really make sense to apply t omega to this exponential function? Well, yeah, you can make sense of it in a variety of reasonable ways using the idea of the dual of the short space. But we really want a properly supported family.
01:05:12.846 - 01:05:44.070, Speaker A: So let's just fiddle with these operators a little bit, and then we'll be done here. Maybe. Yeah, go here first. Oops. So, I'm trying to build an s. This basically is the formula for s done, except for the fact that this may or may not be a compactly support, excuse me, properly supported operator. It all depends on, if you like, what is the inverse Fourier transform of this sigma, and that may not be a compactly supported distribution.
01:05:44.070 - 01:06:35.300, Speaker A: So you just got to fiddle a little bit more and invoke just a little more stuff. Let's take this integral here. I'm really getting into putting the one over two PI to the n. I feel like I learned something in this class, which is to always put that in there. So here's what we were proposing for our. Yeah, some people decide to split the difference and put one over the square root of two PI to the nice in both. But right thinking people like me and Hormand do it this way.
01:06:35.300 - 01:06:39.892, Speaker A: I mean, I'm doing it this way because I'm just copying him. But right thinking people like Hormanda do it.
01:06:39.908 - 01:06:44.504, Speaker B: The same advantage I've often had, this.
01:06:46.404 - 01:07:13.204, Speaker A: I like the fact that at least one of these formulas is just simple. Now, no pies at all. And the way it's done, you can also stick PiS up here. I mean, representation theorists do that, God forbid. But then the Fourier transform of differentiation is multiplication by two PI I times. You know, you don't want to do that. I think this is what Hormer does, is completely defensible.
01:07:13.204 - 01:07:29.944, Speaker A: I like what he does. You have to. It's a bit sad that two PI is not one, but two PI to be one. Yeah. Yeah. Well, that can cause other problems. Okay, so I'm just going to rewrite this formula here.
01:07:29.944 - 01:08:10.452, Speaker A: This is the formula that we were writing. And let's just insert the definition of the Fourier transform, this little, um, calculation. We'll also use one other time. I mean, sort of overkill here, but let's just continue anyway. Fourier transform involves another integral, and what you're supposed to integrate is f of y against an exponential. So there's going to be an f of y. I guess I should put that at the end.
01:08:10.452 - 01:08:53.544, Speaker A: And then there's going to be an e to the. I should have been a plus here. So x minus y psi, f of y dy. And you have to be just slightly careful about this. There's nothing wrong with this y integral, and there's nothing wrong after you've done the y integral with doing the xi integral. But if you do them the other order, you get into trouble. Okay, so this tells you what to do.
01:08:53.544 - 01:09:33.566, Speaker A: I would say we want to define operators s omega. So disregarding what I just told you was problematic, which is changing the order of integration. Let's do it anyway. A moment. If you integrate over psi first and save the integration over y till later, what you see is that the integral kernel, so to speak, for this operator is the integral of sigma omega of psi times this exponential factor integrated over all psi, which. Okay, but that's what it is. And that may or may not be a compactly supported function of x minus y.
01:09:33.566 - 01:10:19.174, Speaker A: But you can always just stick in a bump function. Let's just do that. I can't call the bump function sigma because we already did that. And now let's inspired by that, let's just do this. This is a perfectly respectable operator. This is a continuous, properly supported translation invariant operator satisfying the scaling relation that we had before. In fact, it's just a small smoothing operator away from the original guy.
01:10:19.174 - 01:10:38.374, Speaker A: And of course, phi should be some function which is compactly supported in one in the neighborhood of zero, as usual. Yeah, just be sure.
01:10:39.874 - 01:10:57.064, Speaker B: All this purpose of all this thing, just to give an explicit description of the symbols and show that it corresponds to. But for people who don't know anything about formulas, theory, if you want to take not a necessary part of the theory.
01:11:00.244 - 01:11:37.220, Speaker A: It'S, we had a little goal, which I hope we'll still get to, which was to construct certain operators en, and that goal will be necessary to advance the theory a little bit further. And so we do need to know what we are now discussing, which is that certain constructions can be made. We're going to. So we're just about to answer the question of whether or not you can construct an s from a sigma. And the answer is yes. And once we've done that, we'll have a way of constructing operators. And once we have a way of constructing operators, we'll apply it to construct these famous operators e.
01:11:37.220 - 01:11:41.004, Speaker A: And then we'll be done. Yeah, and we're almost there.
01:11:41.124 - 01:11:49.304, Speaker B: Is this sort of like another sort of exact same problem? Operators, symbols and.
01:11:50.414 - 01:12:09.566, Speaker A: Yeah, we'll come back to that. Yeah, there's an exact sequence in the background and we'll, for those who are sea star people, we'll actually make sea star algebras out of that sequence, maybe as soon as Tuesday, the next election. The answer to your question is. Yeah, maybe it's a dumb question, but.
01:12:09.590 - 01:12:16.714, Speaker B: Like, it's just, I know that somehow there's like this. Extensions, characterization.
01:12:19.634 - 01:12:51.434, Speaker A: Let's discuss that another time. I would like to. I just want to get to the point here somewhere. Maybe over here. I guess we're. Now we can more or less be done with these things. So this operator, you can tidy it up a little bit if you want to.
01:12:51.434 - 01:13:28.034, Speaker A: What this operator is, is you can take the Fourier transform of this xi function. Let me see. Get the notation right. Yeah. I need another greek letter ETA. Now I can put in sigma Eta here like that. And now I can integrate the ETA.
01:13:28.034 - 01:13:51.560, Speaker A: So think of this thing as just another symbol function. New sigma of psi. I guess I should have been an omega there. Okay, now over here, there's a mistake. Yeah, yeah. Yes. Thank you.
01:13:51.560 - 01:14:36.724, Speaker A: Sorry. Yep, yep. And that will appear in the rest of this formula. What's left is e to the I x, c f hat of c. So this integral is just another way of writing that integral over there. So this is a new function, sigma. And the little lemma, which is an Exercise in Calculus, again, is that this function here.
01:14:36.724 - 01:15:31.444, Speaker A: Guess I'll write it out rather than give it a name, is exactly what you started with. Plus a Schwarz function, depending smoothly on omega, of course. No, no, because I forgot it. There you go. I was feeling kind of smug about that, but okay, put me in my place. Okay. So the big integral is just a remanipulation, that's a word of the former big integral.
01:15:31.444 - 01:16:00.094, Speaker A: And it puts s in the same form that t was, except with a new symbol function. I don't know, sigma prime. And the fact of the matter is that sigma prime, minus sigma is Schwarz functions. So we got the correct operator almost on the nose. We got a good operator except modular Schwarz functions. This is a little interesting. It's not the compact supportedness of phi which is relevant here.
01:16:00.094 - 01:16:46.684, Speaker A: What's relevant about phi is it is identically one in a neighborhood of zero, or at least all of its derivatives of zero are zero. So it's a little bit tricky to see that this really works. But it's another exercise, calculus. So if you have a sigma m degree, m principal symbol, you can make, it's not unique, but you can make some sigma out of it in such a way as the previous relation in some previous lemma is holes. And then from that sigma, you can build an operator t. Not quite good enough, usually, because t will not be properly supported. And what you have to do is just change sigma w here, sigma omega, I mean by some other Schwarz function, and then everything is fine.
01:16:46.684 - 01:17:29.168, Speaker A: You get an operator which is manifestly, properly supported. So there's no difference modulo smoothing operators between principal symbol functions Sigma m omega and symbol families s omega. They're one and the same thing, modulo on the one hand SCHWarTZ well, modulo in the case of symbol families, families of smoothing operators. Okay, now finally, let's quickly, about ten minutes to go, just enough to wrap this up. So there are a bunch of exercises here. It's good review of Fourier theory to do this. Of course, what we actually want to do, I just build those operators em.
01:17:29.168 - 01:18:52.594, Speaker A: And you don't need the big general theory just to build the operators em because you can just specialized to some particular functions. But it's good to know that this is the general story. And now let's go back to where we were in the last lecture. Somewhere, I guess up here. And with the benefit of all of this burdensome notation, we can rewrite a theorem that we had last time. And now what it says is that suppose you have an operator a which is scalable, has ordered m, and suppose it's. So now what I want to do is take the symbol attached to a, the Fourier transform, and then take the degree principal symbol just like we were talking about.
01:18:52.594 - 01:19:54.110, Speaker A: That's a family of functions on spheres parameterized by omega. And suppose that this guy here is nowhere vanishing. That is to say, for no omega and for no vector on the unit sphere, do you have Sigma m omega? Of that vector equal to zero, it's always everywhere invertible. Okay. Oh, we now need to put in some u's and v's like we had last time. So here, we'll just put it, put in a u like that. Suppose it's nowhere zero, just over some u.
01:19:54.110 - 01:20:36.598, Speaker A: Okay. Then the conclusion is that there, this is the strongest possible version of the theorem. Well, I decided against it. Let's do what we did last time. For every v which has compact closure inside of you, you don't need this clause, but I'm putting it in to make my life simple. There is a scalable scabble, scalable operator of order minus m with. And it's exactly what we were talking about before, one minus.
01:20:36.598 - 01:21:30.688, Speaker A: What am I have to give it a name? B one minus ab, phi times ab minus the identity. Or do it the other way around, phi times b a minus the identity. These guys here are smoothing operators, and this should be true for all phi, as long as the support of phi sits inside of vlog, you don't actually need the v. This would be true. You could just drop v and then put at the very end here u, and then it would still be true. But it's a little easier to prove it this way. Okay, the way it stands, b is allowed to depend on v, but in fact it doesn't depend much on v.
01:21:30.688 - 01:22:14.344, Speaker A: And so you can patch together by a partition of unity and get one b that works for all v all at once. Okay, so let's, yes, we have five positive plus five minutes, so there's a chance to get this done. Yes, exactly. Yes. Yeah, we've reached another sort of high point. We had the high point of the definition of Bob in Eric, like in Feynman's lectures. And now this from a practical point of view is what pseudo differential operators were built for in the first place.
01:22:14.344 - 01:23:45.496, Speaker A: Well you can certainly choose B, whose principal order minus m degree minus m symbol is just one over the symbol of a because that's just some other smooth family. Well, at least for omega in U, in fact, because let's be a little bit careful because we would, we want to deal with smooth families on all of our n. Maybe we should say here just Omega in v. So on v we're just going to define, we're going to define first the principal symbol family for b to be this guy. Then we'll take this guy and just extend it arbitrarily to a smooth family of functions on all of rn, which you can always do. And then we'll build, having built the symbol function, we'll build the operator like we were just discussing, which is the collection of, this is a degree minus m symbol family of operators. And then we'll build the operator op of that symbol family op of t and we'll be in business.
01:23:45.496 - 01:24:33.344, Speaker A: We'll have an actual scalable operator. Okay. And it will have this property, not exactly, but in the following sort of limited form. These won't necessarily be smoothing operators, but they'll be order minus one operators. That's the best you can do because you know, we're doing the whole construction just by looking at principal symbols. And when you look at principal symbols you're not going to see anything below top order. So you couldn't possibly hope for a better statement than this.
01:24:33.344 - 01:25:25.288, Speaker A: That's all this method gives us for now. And now you need just to finish a little bit of algebra. And yes, and because we're working over a compact set, we can certainly assume that this operator is compactly supported. So this symbol family is a compactly supported symbol family. So all of the theory we were discussing before will be in play. And now let's be inspired by the following calculation. We're trying to invert a by b, and we did a pretty good job.
01:25:25.288 - 01:26:01.726, Speaker A: We inverted a by b up to order operators of order minus one. But in order to do better, we want to write down a formula for the actual inverse of a in terms of the data that you see here. And of course, what will that involve? B is sort of an inverse of a. But to actually invert it will also need to understand this operator, a b inverse. Take this fellow, which is b inverse, a inverse, and you multiply it by b. Then you get a inverse. I'm just speaking in quotation land here.
01:26:01.726 - 01:27:02.134, Speaker A: And so this multiplied by b is going to be the real inverse of a. Okay, modulus smoothing operators and so on. So, but what is this thing? This is identity minus identity minus ab inverse, which is identity plus identity minus ab plus identity minus ab squared plus and so on. Okay, now we can stop the quotation marks. This is a formula for a b inverse whenever it makes sense. And if you take this series and whenever it makes sense and you multiply it by b, then you'll get an operator which is inverse to a whenever it makes sense. Where does it make sense? Well, this guy here is order minus one.
01:27:02.134 - 01:27:42.514, Speaker A: To be accurate, it's order minus one after you multiply by a cutoff function. This guy here is order minus two. Let's put a little asterisk there. After you multiply by a cutoff function, and so on. So they add up, and they don't add up as they stand. But I can take each of these operators, as discussed, and I can modify it by a smoothing operator. And after I've modified each of them by a smoothing operator, I will get a series which really does converge.
01:27:42.514 - 01:28:28.102, Speaker A: You can plug in, in the sense that when you put in a function f, you'll get out evaluate at a point, you'll get out a convergent series, and we'll define in that way a properly supported continuous scalable operator, exactly as we were discussing. So, in fact, inverting modulo order minus one was enough. It's like power series. If you have a formal power series, and if it starts with a one, then one over the formal power series exists. You don't have to worry about the other terms. There's some recursive technique, which is this technique here, which allows you to write down what the inverse is. So you take a, you build b using today's discussion of inferior theory.
01:28:28.102 - 01:29:07.220, Speaker A: Once you have b, you build a times b minus the identity. That's an order minus one operator, and then you square it in qubit, and so on. And you have a series of operators of decreasing orders. So it does converge. In this asymptotic sense. There is an operator c, which is the asymptotic sum of this series in the sense that we discussed before, and you take that operator c, which you've built, and you multiply it, like we just said, by b, and then you get a true parametrics for a, not everywhere, but over this set vlog. And that's how it goes.
01:29:07.220 - 01:30:13.964, Speaker A: And so you can apply this whole thing to the Laplace operator, and you can construct, so to speak, one over the Laplace operator of parametrics. Actually, the Laplace operator being translation invariant, you don't have to go through most of these steps. You can just work at the level of symbols, translation invariant operators. But once you do so, you'll get a perfectly respectable operator, e minus one, which is just the parametrics of e one, which we agree is going to be the Laplacian, and e minus two will just be the square of e minus one, and so on and so forth. And then you'll get a family of operators which have this sort of group property, group property modulo scaling smoothing operators that we started the lecture with. So our theory does include a bunch of non trivial operators in negative degree. We already had the differential operators in positive degree, but now there's a bunch of operators in arbitrarily big negative degree, which are substantial in the sense that they're more or less the inverses of differential operators in positive degree.
01:30:13.964 - 01:30:29.024, Speaker A: What are we going to do with this interesting family of operators? Well, we'll do sister algebra theory next week with these operators here. End of the lesson. Thank you. Any questions?
