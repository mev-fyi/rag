00:00:21.680 - 00:01:26.910, Speaker A: Well, we continue with tensor product. It's time to, to use the same techniques. I mean, the universal property that I explained before the break to obtain many interesting further properties of tensor product. Here is one of them. Corollary, let u one, u two, and v one v two be vector spaces and two linear mapping s defined from u one to v one and the other one call it t from u two two. Both of them be linear. Then there is a unique map.
00:01:26.910 - 00:03:15.864, Speaker A: Then there is a unique linear map, and let's call it l. Now, from u one tensor u two to v one tensor v two, such that l of u one tensor u two is equal to well u one is here is mapped by s to v one, and u two is there is mapped by t to v two. So s of u one tensor t of u two, and well, this is an element of v one tensor. Again, if such a map exists, since this element create a space span of them, is the tensor product u one tensor v two. If l is well defined, it is unique. So the main question is that why is this well defined? And the trick is the same trick that we applied before. But since we have the universal property, we don't need to go into all details.
00:03:15.864 - 00:04:38.817, Speaker A: Just consider b from u one cartesian product, but not tensor product cartesian product u one u two into v one tensor v two, defined by this. This is well defined b of u one u two. An arbitrary element of this we define to be s of u one tensor product t of u. This is well defined, absolutely, absolutely no problem here. And where do I use the fact that s and t both of them are linear? They are reflected here. S and t linear implies that b is bilinear. So you have a bilinear map from u cartesian product u one contezual product u two into a space.
00:04:38.817 - 00:05:56.144, Speaker A: This is our space w and by the universal property. So therefore, by the, by the universal property b tilde is well defined. It's well defined. And note that b tilde was from u one tensor u two to v one tensor v two, which is our w. And b tilde is the same thing that we call in the announcement of tier. So that's the end of proof and notation. We write s tensor t for l.
00:05:56.144 - 00:07:36.254, Speaker A: So in other words, if s is defined from u one up to v one and t is defined from u two up to u two, then s tensor t is defined from u one tensor u two to v one tensor e two, and it maps the element little you want. And so u two the map of the first element s of u one tensor map of the second element t of u two. Special case. That spatial case happens a lot in function spaces. What if v one is equal to v two is equal c. So in this case we need to understand what is c tensor c? And well, each element, what is, what is a basic element, the basic element u times v, then, um, is in c and v is in C. We can write it because they are scalar, we can write it as uv times one tensor one.
00:07:36.254 - 00:08:40.222, Speaker A: So every, and then we add them up with different scalars. Every x for every x in c tends to c x has the general representation j from one up to n u j tensor j, but it will be j from one to n u j v j. Because these are scalars, one tends one. So at the end of days is a lambda times an element one tensor. And now we can forget about this and say that this is usually done in all books, say that c tensor c is the same as c. This is equivalent to say that every element has this representation. But then we forget about one tensor one.
00:08:40.222 - 00:10:01.684, Speaker A: It has nothing to do with it. When it doesn't do anything, it's just an annotation which is there. So keeping this, we can call it convention. So keeping this convention in mind, if you have s from u one capital u one, to see a functional linear function, and t also from u two to c, then s tensor t would be a linear functional from u one tensor u two to c tensor c, which we might see. So it will be another linear functional, and the definition is such that s tensor t acting on the basic element u one tensor u two. I should write s of u one tensor s of u two. But based on the above convention, I just write s of u one times t of u two.
00:10:01.684 - 00:12:19.926, Speaker A: So that's one special but important case. Another property of this, which indeed I will need in the next session of the book, is the Haml basis for utensil. And here is the theorem. So for hanal basis. So u and v are vector spaces with Hamilton basis enf, and then this set, which is when we can, we can say e tensor f, but more explicitly then e tensor f, when e runs over capital e and f runs over capital f. This is a basis, or is a hammer basis basis for utensil. What we have done up to now, we, based on what we discussed up to now, we know that this set spans the whole space.
00:12:19.926 - 00:13:35.296, Speaker A: So the question is for being independent. So, proof that span, I mean, let me call this, there's no notation for that. Let me, let me call it a, that the span of a is equal to u tension v is, well, I say trivial. Trivial for us, because we have discussed a lot about it. The question is about being independent. So assume that j from one up to n lambda j. And now we need to add lambda j.
00:13:35.296 - 00:15:01.818, Speaker A: Because we did not consider all u tensor v, we just considered e tensor f with a collection of e and collection of f, not all of them. So lambda j is needed here, lambda j ej tensor f j is equal to zero. And our goal, that lambda j is in c and e j is in e f j is in f. Don't you need different indices on the e's and the f's? Here? I explained this because we need to be careful here. You mean I write eifj? Yes, that's one way to do it. Another way, which is based on the comment there is repetition is possible among ej or fj. For example, we might have Sheldon mentioned, we might have something like this, say e times.
00:15:01.818 - 00:15:55.898, Speaker A: Well, how can I write such that. Oh, I use f prime e f one element, e times f prime another element, and e prime times f prime. These are three different elements. But you know, you see at the, with my notation, this is e one, tensor f one. This is e two, tensor f two, and this is e three, tensor f three. And I multiply this by lambda one, lambda two, and this one by lambda three. So it is possible that, for example, e one be equal to e two and be the same element.
00:15:55.898 - 00:16:38.174, Speaker A: The way I wrote it, I just assumed that e is in capital e and f is in capital f and the same here. Okay, thank you. You're welcome. So, ei is in e f j, ej is in e, fj is in f. But we might have some repetition. And the way to do it is, is based on what we did before. I mean, the tensor product of two linear mapping.
00:16:38.174 - 00:17:59.394, Speaker A: So we know that since e is a hamlet basis for u, there is a mapping phi from u to c such that e at point e one is equal to zero and equal. No, no, sorry. Equal to one at this element is equal to one and phi at. Here is the problem. When I write ej equal to zero for j not equal to one. There is a reason here I didn't write for j between two and. Nice.
00:17:59.394 - 00:18:41.474, Speaker A: There is a notation problem. I agree. I have to think what is the right way to do it. What I mean, here there is a fee such that at point e, for example, here is equal to one and at point e prime is equal to zero and at all other point of the basis. We can put it this way. I know how to do it. Now, phi of e is equal to zero for all e is in e except e one.
00:18:41.474 - 00:20:02.746, Speaker A: I think that works now. And similarly for f one. F is a hammer basis for v. Therefore there is psi from v to c such that psi of f one is equal to one and psi of f equal to zero for any f in our set, except the one we choose before, except f one. And now. Now consider phi tension psi from u tensor v to c. What can we say about phi tensor psi for any element of the form I mean u tensor v.
00:20:02.746 - 00:21:28.914, Speaker A: By definition, this is phi of u sine of v. That's by definition. Therefore phi tensor psi of e, one tensor f one is equal to phi of e, one psi of f one which is equal to one, and for any other element, for any e and f such that either e not equal to e one or f not equal to f one phi tensor psi at point e tensor f is phi of e sine of f. And then one of these is equal to zero. So either this is equal to zero or this one is equal to zero. So one of them is zero, so it's equal to zero. The mapping, and now we apply it here.
00:21:28.914 - 00:22:50.624, Speaker A: We apply it precisely there. We apply phi tensor psi to this sum. So therefore v tension sine of sum zero. And this is equal to, I forgot, lambda j sum j from one up to n lambda j phi tends to psi, ej tends to fj. And this sum is again equal to the first one is lambda one times one, and for the rest, by the property we saw here, the rest is zero. So the conclusion is that lambda one is equal to zero. And we continue the same, the same property, the same form of reasoning.
00:22:50.624 - 00:24:24.614, Speaker A: So similarly, lambda two, lambda n is equal to n. So they are, they are dependent, independent, and we are. So if you have Haml bases of the ambient spaces, then you tensor product them. You obtain a hammer basis for the whole product. And immediately from this corollary, before corollary, recall that the dimension of u cartesian product v is the dimension of u plus the dimension of v. And now corollary immediately tells us that the dimension of u tensor product v is the dimension of u times the dimension of v. So somehow it's a bigger space.
00:24:24.614 - 00:25:51.366, Speaker A: Then this is done. You want, and, well, I wrote this as a corollary, but I mean, let me leave it as a simple exercise for the student. Exercise. U and v vector spaces and just we assume that the second component v, one up to vn are linearly independent. So we assume that v one up to vn linearly independent and x. But assume that some j from one to n u j tends to vj is equal to zero. That's the consequence of independency.
00:25:51.366 - 00:26:50.344, Speaker A: In general, if we have a summation like this equal to zero, we cannot say anything more. I mean, just the linear combination is here. But now, thanks to this property, we can conclude that then u one equal to u two u n equal to zero. Of course, the other way around is trivial. So that is another advantage of working with independent sets. Another observation after this that's really interesting. When we go to functional spaces, assume that v is equal to cn.
00:26:50.344 - 00:29:05.884, Speaker A: Then what is u tensor product v or u tensor product cn? Well, we know that cn is the span of the canonical product e one up to en the canonical basis. I'm sorry, ej, all of the components are zero, except the j's element, which is equal to one. And therefore every element, based on the previous exercise, every element of this, every x in this space has a unique representation of this form x equal to u, one tends to e one plus un tension en, because first of all, it expands the space, so it has to be of this form. And second, independency gives us the uniqueness. This is not quite a vector of dimension n, but we can use this to say that we have a mapping from u tensor cn to un cartesian product of u n times, which maps the element sum j from one up to n u, j densor ej to the element u one up to u. We succeed here because of the unique uniqueness, we have a unique representation. So we can do this map and we obtain this correspondence.
00:29:05.884 - 00:30:48.674, Speaker A: This is a bijective map. Or in technical terms, we can say, if you want to use the technical terms, u tensor cn is isometric. I think it's better to say isomorphic, because there is no topology involved. Now, isomorphic to un. And, well, sometimes we abuse the notation and simply write u tensor c n is equal to un, but equality is, I mean, the way I explained about even, even more in infinite dimensional case. And you need to be a bit careful here. When a topology come into picture, when they can see the u tensor product l two, well, we know that l two is also spanned by e one e two.
00:30:48.674 - 00:32:11.002, Speaker A: So every element here, any x in u tensor l two, can be represented uniquely as the sum j from one up to infinity. This time uj tensor ej true. And this is equal, but equality in the sense I explained above, the vector u one, u two, and so on. Indeed, it's not equality, it's isomorphic to that. And that is why they write. I mean, we can say that u tensor l two is equal to u cartesian u cartesian u infinitely many times, or if you want pj from one up to infinity. Well, uj, when uj is equal to u up to here, there is no problem.
00:32:11.002 - 00:33:42.972, Speaker A: But when we put a topology on this, for example, when we put in nor it's not for all elements of the product, we have some restrictions. For example, one of the restrictions that we will see before, you will not see in this my mini course, but you will see it next week when Michael Hartz talks about the worry everson space, not all elements here, the elements which satisfy the norm of uj squared is less than infinity. That's the meaning of utensil. But why do we have this isomorphism is based on the theorem we had above. Final comment about this, which is also interesting, is this one. Assume that l, the mapping from um to v is given, and then consider the identity mapping. So I from space cn into c, so identity as before, but before we had map t map s and before s tensor t.
00:33:42.972 - 00:35:06.862, Speaker A: Now we form l tensor I from u tensor c n to v tensor cn, precisely as before. And how did it work? It mapped an element here to element here with the operation of this. What is an element of u tensor cn? Well, we saw that for an element here, we can write u one, u two up to un. But if you want to be very specific and more precise, it's indeed u one tensor e one plus u two tensor e two plus un tensor enable. But we write it in this way. And what is the mapping? On the first component l applies, and on the second component e one up to en I applies, but I is the identity. So this is mapped to the element l of u one tensor e one plus le two e two l u two tensor e two up to l un tensor en.
00:35:06.862 - 00:36:34.374, Speaker A: And again, we write this as l of u one l of u two l of u. So it's a mapping. We can now forget about tensor and say that we have a mapping l, uh, or I mean l tilde, if you call or stick to the same notation that we had before, from un to the to vn, which maps this element to this element. I mentioned this because, as I said for sure, next week you will see several occasions when the speaker will talk about consider well, Hilbert space or Hardy space h two. And then they consider harder space h two tensor l two, or hardiest phase h two tensor cn. You will see incidences like this. And that's the preparation for next week to finish this part.
00:36:34.374 - 00:37:32.754, Speaker A: I mean, up to here it was based on my note, there is one proposition in the book, proposition 4.9. Well, up to here I consider Hilbert's vector spaces when we go into inner product spaces. So let me add this. I need to say this before the proposition. Inner vector spaces, you have u and there is an inner product in U and the same for v. You have v and there is an inner product in V. Sometimes, to distinguish, I put a u here and put a v there, and then you form the tensor product u and v.
00:37:32.754 - 00:39:50.634, Speaker A: And the question is that if you can say there are two basic elements here, say u one tensor v one and u two tensor v two, can we define an inner product here? Well, the natural choice which comes to your mind is this one. We define the inner product and that's the definition to be the inner product u one and u two in u times the inner product of v one and v two in v. The same problem that we had before shows its face. Why is this well defined? Probably in this format it is not clear why we have a problem. But again, how do you define the the inner product of two arbitrary elements, x and y, when x and y are in a u tensor v? To do this well, x has a representation the same for y and for x inner product v. You can write j from one and for y I from one up to. Nice.
00:39:50.634 - 00:40:40.232, Speaker A: Use the linearity, that's the way we define it. Take this out. PI from one up to M. I think I wrote about M and then you end up with basic elements. And now apply the definition above. Uj with Ui prime and vj with v I prime. This happens in U, this happens in V.
00:40:40.232 - 00:41:06.174, Speaker A: All the operation is correct. The only problem is that this is not unique. We have seen this problem before. This representation is not unique. X might have other representation, but y. At the end of the day, doesn't matter which representation we use, we always obtain the same number. There is no problem.
00:41:06.174 - 00:42:03.284, Speaker A: So there is no problem. And our definition is okay, why is this the case? And I think you know how to deal with this now. I mean, we have had several incidences. You can use the universal mapping property, or forget about universal mapping property, which means that forgetting about the proof of the universal mapping property, you define this on the scalar product, on the cartesian product of U and v, and then obtain the null space. And I mean, you know how to do it. I've done it at least twice today and show that if we go to the quotient space, this is well defined. So it's not clear at the, at the beginning, but at the end it is well defined.
00:42:03.284 - 00:42:49.424, Speaker A: One important property that it's explained in the book, and it connects to the topic before that section, the shore product. Note that there are several steps to be taken here. One step. I just gave you the hint and I asked you to follow it. Why is this well defined? I didn't go into detail why I told you how to do it. Now assume that it is well defined. Why is this an inner product? We have four properties to investigate.
00:42:49.424 - 00:43:57.274, Speaker A: The first one is to explore the linearity that's in the definition. I mean, we define it that way. Linear with respect to the first component, conjugate linear with respect to the second component. That is clear. No problem. The main difficulty is in this one, why x and X is positive for and why? If X and X is equal to zero, this implies that X equals to zero. Why is to answer to these two questions, we need short product and explain a little bit, what is x? I mean, it has a representation like this, some j from one up to nujvj.
00:43:57.274 - 00:46:31.764, Speaker A: And let's see, what is x times x. So j from one to n, I from one to n. It's the same thing. Ui tends vilinearity. It's uj, vj and ui tensor vi and by definition this is uj, ui times does does this product tells you something? And why is this positive? Temporarily, forget about, say, vjvi, just consider sum j from one up to n, sum I from one to n u j inner product ui. This is the same thing as we call this vector Gramion. Vector Gramion of u, one up to un is the matrix whose elements are inner product of ui and uj.
00:46:31.764 - 00:47:19.828, Speaker A: Now the order is reversed, is uj and un. So it's Grammy and transport that that really doesn't matter. So it's gramion. If you want transpose here applied on the vector eta, inner product with eta, where eta is the vector one, one up to one. All the components are one. In the general, when you put a vector with component eta one, eta two, eta n, here we have eta I, eta j bar or bar on the other one. It depends how we write it.
00:47:19.828 - 00:47:54.048, Speaker A: But here eta I is equal to one and you do not see it. So it's g transpose eta inner probe, that is eta, and we know g is positive. So g transpose is also positive. So this is positive and the same thing. So this is one thing exactly. By the same reasoning, the sound j from one. What was that? Vj? V j.
00:47:54.048 - 00:48:47.004, Speaker A: Vijay, it's, I call this gramion of u. It's gramion of v transpose eta and eta, which is bigger than or equal to zero. So, so you have, uh, let me write grammar of v two. Vijay. Both of them are n by n matrices. That's another reason. So back to here.
00:48:47.004 - 00:49:46.634, Speaker A: Why, why this is, this combination is bigger than or is equal to zero. Well, look at this and look at this. You have two matrices, both of them are positive. Schur theorem. By short, we know that uivj, no ui, uj and vi inner protag vj. Just component by component, we multiply is bigger than or equal to zero. And let's call this g.
00:49:46.634 - 00:50:22.954, Speaker A: And this is nothing. But this is nothing but g eta and eta. Maybe g transpose. Yeah, g transpose eta and eta, which is bigger than or equal to zero. Absolutely not obvious. If we forget about the short product. So it's the short product, which tells us that we had a quote question to answer.
00:50:22.954 - 00:51:01.264, Speaker A: We had a question to answer. Yeah, that's the short by short theorem, we can show that the inner product of x and x is bigger than or equal to zero. Also, again, by the same theorem, we can answer this. Instead of answering this, I do this. If x not equal to zero, then inner product of x and x is strictly bigger than zero. We can show this. And this is hidden here.
00:51:01.264 - 00:52:05.660, Speaker A: If x is not equal to zero, then go back there. The representation, representation was. I use the representation for x. Here is the representation I use. You remember we had, before the break, there is a representation in which n is minimal, and when n is minimal, uj's are independent and vj are independent. I use it here. So I use n minimal, which implies that uj tensor vj is u one up to un independent and the same for v, one up to vn.
00:52:05.660 - 00:53:01.114, Speaker A: We had this before. Consequence of that when they are independent g, the gramion of U will be strictly positive and the same for the gramion of V strictly positive. And the theorem of shor, we show that if you have two matrices both strictly positive, then g of. I mean, let me call it uv, it will be strictly positive. Did I call it Guv here? I mean, I simply call it G, but we can call it guv. Guv is strictly positive. And this is the same thing as this implies that if you can see the, the inner product of X and X.
00:53:01.114 - 00:53:59.464, Speaker A: It's nothing but Guv, transpose of eta with eta and that will be strictly positive. Done. So you see everything, I mean comes together very neatly here and we can show by using techniques from tensor product and techniques from the short theorem to show that the inner product is well defined here and we have an inner product space. Be careful before the next break. If you start with two Hilbert space h one and h two, of course you can form h one tensor h two as inner product spaces the way define the power. But this is not necessarily complete. It's an inner product not necessarily complete even though h one and h two are complete.
00:53:59.464 - 00:54:28.224, Speaker A: But we can do the completion and the completion is also denoted by h one tensor h two. So this is something doesn't cause any problem. But sometimes we need to be careful about this. Well let's, let's have another break. We have covered all the negative.
