00:00:00.160 - 00:00:52.620, Speaker A: Thank you, Manuel. All right, we can start. Questions? Comments? So, today, today we collect what we learned so far about heat flow and differential calculus integration by parts of metric, measure spaces, and start really working on this class of RCD kinetics basis. Then I'm going to define later on. For the moment, let me recollect this setting. So, questions? So, let's start with the CDK infinity space. For simplicity.
00:00:52.620 - 00:01:51.012, Speaker A: In my proofs, I will always, perhaps in my statement, so that I don't cheat, I will assume sometimes compactness, just, just for simplicity, although that is not, is not necessary. All right, so recall that we have defined, we have defined, well, at least one heat flow. So one version. So, first version, I mean, we have two equivalent definitions of heat flow. So, heat flow can be seen as. So, you know, the gradient flow of the trigger energy in the space with respect to the distance l two and the gradient flow of the relative entropy with respect to the geometry of the distance w two. Okay, now that's also, in general, CDC and PV spaces.
00:01:51.012 - 00:02:51.316, Speaker A: And we are seeing, and we are seeing that, in fact, this viewpoint is, makes it evident that this heat flow is, in general, nonlinear. Okay? And I've already made a comment that, you know, on remaining manifold seed flow is linear. It seems like, you know, a reasonable idea to see what happens, what we can, what we can, you know, learn about, you know, these spaces, if we make the final assumption that the heat flow is linear. And perhaps I should also add that the heat flow is stable with respect to measure granovas dot convergence. So the linearity of the heat flow, or in a sense, whatever condition you put on the heat flow passes to the limit and provides a stable notion. So it makes sense to study these spaces where the heat cd confining, where the heat flow is also linear. So, because this is still a stable class of spaces now.
00:02:51.316 - 00:03:50.840, Speaker A: And if you remember a couple of lectures ago, I said, okay, well, as nice or natural as this idea can be seen, the point is not only throwing definitions, but actually proving statements about the definition that are thrown on the table. So what can we say about a CD Kanpur space or heat flow rate or whatever else, if we further assume that the heat flow is near. Okay, and today we start seeing this. And let me start discussing two crucial formulas. So, I have two propositions. Both are basically from my papers with ambrosia and Savare in, I guess, 2001. So x CDK infinity, let me say compact, it's not needed.
00:03:50.840 - 00:05:18.712, Speaker A: But, you know, and let, and let, you know, peak a probability density, which is, you know, also in a two so probability density. So it's non negative. So we know, we know, in particular, from the mass preservation and the weak maximum principle for the heat flow, we know that if we consider the gradient flow trajectory, okay, the heat, let me say the heat, say in l two, starting from f, then we know that FT is always a probability measure, probability density, sorry, for any positive time. Okay. And we also know that the curve that takes t and returns mu t, which is ftm, this is, you know, locally absolutely continuous with respect to the distance w two, right? Because it's also a gradient flow of the relative entropy. So in particular, this is true. Okay, now, the first formula that I want to study is.
00:05:18.712 - 00:06:13.642, Speaker A: So, is the following. So if this curve is absolutely continuous in a metric sense, for every measure, new probability measure on x, the curve, the map that takes t and returns, say, one half the squared of arsenic, distance between mu t and u is, you know, this is absolutely continuous locally, absolutely. This, on the interval, let's say zero plus continuous, right? Absolutely continuous curve. I'm post composing with a distance function squared. Okay? So this is absolute continuous, is differentiable almost everywhere. Okay? And I can tell you, is this derivative, this derivative, and that's the formula. Then dt of one half w two squared ut nu.
00:06:13.642 - 00:07:09.614, Speaker A: This is equal. So let me write it this way. Actually, no, let me write this way. So, this is equal to the integral of phi t laplace of ft dm for almost every t where phi t is a control of its potential. From mutating. I'm getting a formula here. All right, let me first of all read this formula, and then we prove this.
00:07:09.614 - 00:07:45.132, Speaker A: First of all, I'm telling that a certain derivative, I mean, I'm giving a formula for almost every bit the existence of the derivative. I've already commented. So, by absolute continuity. Now let's look at the right hand side, and let's notice that given that I assumed the space to be compact, any control of its potential, in particular, is a distance squared over two function must be Lipschitz. Okay? Global lip sheets, and also globally bounded. Okay, so the Laplacian of ft is an l two function. This is bound, is, in particular, m is finite, say probability.
00:07:45.132 - 00:08:19.838, Speaker A: So the fact that this is, well, you know, is an integral function poses no problem. And I did not quantify, I did not make any quantification on phi, on phi t. And this formula is true for any, if I, there is no statement about uniqueness of controlled potentials in general. Okay? But whatever you pick, the formula holds. Okay? So the function can change, but not the value of data proof. And the proof is trivial. I mean, if only all the proofs were like this.
00:08:19.838 - 00:09:28.884, Speaker A: Okay, pick a point e where the derivative exists. Almost every does the job and pick any control potential fighting. Okay, then we know by, you know, by definition of control is potential. We know that, you know, we get equality in the dual problem, right? So, we know that one w two squared mu t nu is equal to the integral of phi t d mu t plus the integral of phi t. On the other hand, the couple, sorry, phi tc. The couple phi t, phi tc is an admissible couple in the dual formulation of the optimal transport rule. So, for any other time, say for instance, e plus h, we have that this is greater or equal than this, right? So, so, you know, let's make the difference.
00:09:28.884 - 00:10:25.414, Speaker A: This simplifies. And this is, you know, mu t plus h. Remember, the mu t is, you know, ftm, right? And then use the fact that, you know, dt ft is equal to the Laplacian of ft to make sense, right? If you want, if you want add, I did tell the right derivative of ft is equal to the plus ft for every t. But if you don't want to use that result, you say, you know, let's say that this equation is also satisfied for almost every t. So, pick a t where this derivative exists and is equal to the Laplacian, and where this function is differentiable. Pick the t and the newta. Okay, all right.
00:10:25.414 - 00:11:33.294, Speaker A: I made no use of the linearity of the heat rule in this proof second key formula. And again, this comes from my work with ambrosia and Savare. And, okay, perhaps I should also mention a paper with Ambrosio, myself, Mondino and Rajana, where these things have been worked out in the non compact case, where a bit more complicated, but I will stick to the compact case. So again, xcd completely. And now I will compute a totally different kind of derivative. I pick, let's say that say s to nu s is a w to Judysig of bounded compression. So, new s is less or equal than some constant time reference measure m for every s in zero one.
00:11:33.294 - 00:12:25.338, Speaker A: And let me also assume, let me also assume that that new zero is bounded from below by some other constant. Okay, and let me also assume, so now this guy, so this guy is say, well, let's write, let's write. Mu s is writing eta s m. And let me also assume that eta zero. So the density of nu zero is suble, right, it's a regularity. I'm adding a lot of regularity assumptions on this. And let's compute well.
00:12:25.338 - 00:12:55.264, Speaker A: And. Well, then let me compute the leanings as s goes to zero of the. You know, let me take the sort of right derivative at zero of the relative entropy. This derivative exists. That's the derivative. I'm just taking the mean first of now. Well, this is greater or equal than the integral.
00:12:55.264 - 00:13:46.812, Speaker A: I don't know if there should be a minus. I guess, way minus d minus eta zeta graph phi d. You know, where phi is a control of its potential, you know, from, you know, for BJD six, if you want, from u zero to nu one. You remember this. This quantity d minus f and g is something that is always well defined for any couple of sober functions. So, as before, phi, given that phi is a control, which potentially these lip sheets, because my space is compact. So lip is in particular sublet.
00:13:46.812 - 00:14:05.428, Speaker A: Right? So, so this expression is well defined. Eta zero is sober by assumption. So, okay, all right. And. And this, this. I mean, this formula is really crucial. I cannot underline enough how important it is.
00:14:05.428 - 00:14:31.644, Speaker A: And. Yeah, okay. Well, you would see in the applications why this is working. So let's prove this. Let me compute this guy. Let me notice that, you know, the entropy of nu s minus the entropy of nu zero divided by s. Well, this is equal, you know, by, you know, by sort of, by definition, this is.
00:14:31.644 - 00:15:21.814, Speaker A: Let me write u eta s minus u eta zero divided by s, where u, you know, u of z is z log z. Right? By definition. Right. Now, u is a complex function. So at any point, I have the inequality. So this is an equality, if you want, that holds even without integrals. Right? I'm just using the u at b minus u at a is greater or equal than u prime at a times b minus a.
00:15:21.814 - 00:16:39.414, Speaker A: That's, you know, direct consequence of complexity. Okay, all right, well, then, now I pick. So I lift the gdc in w two, and, you know, and I get a test plan. Pipe PI is a lifting of nu s. Okay? So it leaves is a probability measure on the space of curves with nu s as marginals and energy, which is the same as the energy. Okay? Now, and notice that given that I am lifting a geodesic with bounded compression, Barayara's theorem, or lemma, that we made last time, PI represents the gradient of minus five in the sense that we discussed the other side. And on the other hand, what do I get in here? So, let me.
00:16:39.414 - 00:17:00.066, Speaker A: Let me, you know, let me continue here. Yeah. This is the integral of u prime, eta zero. Let me write it this way. In gamma s minus u prime, eta zero, gamma zero, d PI of gamma. This quantity is the same as this. I'm just.
00:17:00.066 - 00:18:11.334, Speaker A: I just used, you know, the fact that the marginals of PI are the etas makes sense, right? What's the integral of this function at gamma? S is equal to the integral of the same function, des PI. But e s push over PI is new s, right? Makes sense. R divided by s divided by s. Okay, so when we take the line inf over here, you know, we want is greater equal than the limit as s goes to zero of this expression, right? But now we can use the formula for, you know, the first differentiation formula. The horizontal and vertical derivative done the other time. And this is greater or equal than the integral of d minus of u prime, eta naught, applied to, you know, nabla, if you want minus five, let me write this way. And then this should be evaluated in gamma, not d PI.
00:18:11.334 - 00:18:34.944, Speaker A: But. So this is eta zero here. All right. Okay. But now we are done, right? So this. I mean, I have one homogeneity in this guy, or perhaps, let me, let me keep it this way. Make it this way.
00:18:34.944 - 00:18:56.620, Speaker A: Positive modulity. If I don't make any further assumption, and I have. I have the chain rule over here. So u is z log z. U prime is log z plus one. And when I differentiate one other time, this is, you know, u second. You know, this is like, by the chain rule, right? This is u second eta zero.
00:18:56.620 - 00:19:21.922, Speaker A: And then I have d minus, you know, eta zero. Grad, I mean, the correct thing, it has zero, but you second is 1. Second is one over z. So this simplifies. Okay, all right. Okay. So we have these two formulas that are valid on an arbitrary cd.
00:19:21.922 - 00:19:44.614, Speaker A: Okay? Perhaps I should say on an arbitrary CD infinity space. In fact, it's not that I ever used, really. This is the continuity condition. To be honest, I didn't really use. I mean, as far as the assumptions are true, you know, uh, about regularity of f zero or regularity of u zero. These formulas are valid. Okay.
00:19:44.614 - 00:20:13.668, Speaker A: Um, now look how close they. This. These two things are. Um, this guy and this guy. If only, if only calculus in our space was Hilbert like, you know, you see this expression? So this is, you know, if I integrate by parts, this is basically up to a sign. This is the differential of phi applied to the gradient over here is swooped so up to a sine. Okay, there is a differential of phi applied to the.
00:20:13.668 - 00:21:50.832, Speaker A: Sorry, differential of the density of the measure applied to the gradient of PI. Okay? And I guess, let me write, let me write it. So if we combine these two, we get the following result here, and this is the starting point, if you want, of the theory of RCD spaces. So let, and now I really need cdk infinity. Okay, this, I don't need that. Put, for simplicity, with linearly flow with, okay, infinitesimal burden then. And also, and also let, let's also, let say mu zero and mu b probability measures, let me say with bounded densities.
00:21:50.832 - 00:23:06.584, Speaker A: So they both have, you know, yeah, density bounded from both. And let me also say that mu zero is also density bounded from below. Okay? Then for almost everything, we have the derivative of the square distance between mu t and nu plus the entropy of mu t plus k over two. The distance squared between ut and u is less or equal than the entropy. Okay, this is an inequality is a pretty important one. I mean, the rest of, I mean, I first proved this inequality, and then for the rest of today's lecture, I basically study, study this differential inequality that encodes a lot of information. So let's prove it.
00:23:06.584 - 00:23:54.378, Speaker A: I'm just stating the certain inequalities. Okay, so let's, the proof. Now, with all the tools that were done, you know, that we've been so far, the proof is, I mean, very easy. Um, why is that? Well, you know, I, first of all, I can compute this derivative at almost every. So we can use that formula and, and notice, and recall, recall, perhaps first of all, notice the following, that for every t positive, if I call, okay, I didn't say perhaps, I didn't say with muti. Muti is the, in the heat flow starting from mu zero. That, of course, you can interpret either as gradient flow of the trigger energy starting from the density of mu zero, or as degrading flow derivative entropy.
00:23:54.378 - 00:25:25.320, Speaker A: Okay? Now, what I wanted to say is that for every t bigger than zero, of course, mu t as a density, and this density, by the weak maximum principle, this density is bounded from below by a constant and from the above by another constant, right? Because these bounds are preserved by the, you know, by the weak maximum principle. And moreover, fp is also sober, right? Why sober? Well, because if I do, the gradient flow of the Chigger energy doesn't matter where I start. For every positive time, I end up in the domain of the Chigurh energy, which is the solar space, okay? And moreover, for every t I use that, I can use Raya's lemma that exists a Judisic, and let me call it new say, st. So t here is a parameter, is a, you know, index, is the new. And s is the index for w two. Julie sic from nu t to nu. Okay, with bounded compression and such that the convexity inequality is true.
00:25:25.320 - 00:26:18.284, Speaker A: So the entropy of new stuff is less or equal than one minus s, the entropy of mu t plus s, the entropy of nu. And then there is the correlation factor. What is minus t? One minus t, the square distance. Right? But ayara tells that there is. I can select a juridic, I mean, if I have two measures with bounded densities and I'm sitting on a cd, on a cd k infinity space, say compact, well, then I can find a vast ngc connecting them such that not only the convex inequality is true. Okay, that's lost turbulability by definition of DQ affinity. But I can select this, I mean, so smartly that boundary compression.
00:26:18.284 - 00:27:22.784, Speaker A: Okay, well then what do I do? Well, then I collect what I know and I'm done. So I also, you know, peak 50. Can't throw it potentially potential from, you know, nu t nu. Well, then this formula holds. And well, this is equal to, you know, to this formula that now, now I am in imperial liberty of space. So now I can write this graphite gravity. Yeah, right.
00:27:22.784 - 00:28:08.684, Speaker A: This quantity is something that we have defined at the other time and is not disability. And on the other hand, that the limit as s goes to zero of this entropy, what it is a new st minus. The entropy at nu t t is basically nu zero t divided by s. This is greater or equal. Then what is this that the formula here is telling me in this infinitesimal albertian case where I can, you know, the expression is linear and symmetric. This is equal minus f grad f t grad phi t. See, now these two are the same.
00:28:08.684 - 00:29:23.104, Speaker A: In general, they are not. But in predesimal bertil, it tells me that these two things are the same. Okay, and, but I am done because. Because this linen is also less or equal than, I mean, just by the convexity inequality, this is less or equal to the entropy, you know, at the value s equals one minus the entropy at value s equals zero and minus the correction term k over two would distance good. I mean, you just, it's a direct consequence of the convexity inequality, if you want. What I'm telling, if k is equal to zero, what I'm telling is that, is that if I have a convex function and I compute the right derivative at any point, this is bounded from above by, you know, the difference quotient, you know, later time. Okay, end of the book.
00:29:23.104 - 00:30:16.516, Speaker A: Because this derivative is less or equal than this lenin. But this de mean is less or equal than this difference. It's really, you know, there's no cheating. Okay, now that inequality, the questions now that inequality that they brought over there is so that has not been. Okay, it's not been adapted to the current time zone. Okay, so what was I saying? Yes, that in a code, that differential code is so important that this is the name. And let's study a little bit.
00:30:16.516 - 00:31:04.170, Speaker A: The properties result, the finish, if you want. Now, I hope I forget everything about cd k infinity spaces and matric measure spaces. And I go back to discuss the notion of gradient fluid metric setting. So, let x d be a metric space and e a function from x. For simplicity, I put in non negative and lower semicontist. It should be there. No, negativity is just, you know, for me, because I'm listening.
00:31:04.170 - 00:31:46.054, Speaker A: And then, and then I say that the curve is an avi, you know, an avi k gradient flow trajectory. If so, this is a curve, defines anal. So it should be continuous. Continue on. Zero plus infinity. Absolutely continuous on the open outline locally. Absolutely.
00:31:46.054 - 00:32:28.090, Speaker A: And for every y in x, we have the dt. I mean, that inequality over there should be true. Okay. Evi stands for evolution variation inequality. Okay. Is a variation inequality, okay. Concerns evolution decay.
00:32:28.090 - 00:33:42.484, Speaker A: Is this okay? Now, why gradient flow? I mean, this doesn't look a priori gradient flow at all. I'll comment about this in a second. So, the definition has been introduced by Ambrosio Savre myself, in our book on gradient flows, and has been extensively studied, especially by Savare and collaborators. And I will recall some of the results that they got in a moment. First of all, well, perhaps first of all, informally, why, what has this to do with gradient flows? And as usually, it's a good idea to start from the smooth case, you know, rd with smooth functions. Smooth and convex function function, you know, example, I mean, motivational example. So, let me from rd to r be smooth and convex.
00:33:42.484 - 00:35:09.470, Speaker A: And remember that if e is convex, then grad, e at x is the only, you know, is the only vector, vector v for which, you know, in Rd, if you want, for which in tangent at x rd, which is rd, such that the energy at x plus v scalar product y minus x is less or equal than the energy at y. Whatever. Yeah. Well, now notice that. So I can further observe that this quantity, if I am on Rd and Rd, you know, on a birthday space, this quantity is also equal to the derivative. Sorry, no, this is. Wait, so in some sense, so xt is a gradient flow, okay, if and only if I have that.
00:35:09.470 - 00:35:49.614, Speaker A: The energy of x t plus, actually. Yeah, plus this way. So x t prime, x t minus y is less or equal than the energy y for every y and t. I just replace v. If you fix a t where x, say, you know, say the x is a new curve, everything is 60. Then x t prime is minus the gradient of if and only if for every y results. And I swapped x and y here to take into account them, this equivalent character for if is convex, of course.
00:35:49.614 - 00:36:44.834, Speaker A: Well, now, an observation is that, okay, but this guy, this guy is the derivative of one half x t minus y squared. And now, so you see, I characterize being a gradient flow via means that have a matrix analog saying that x t is a gradient plus if and only if the energy of x t plus this derivative is less trigger than the energy of y for every y and t. Okay, this is, you know, makes sense in the matrix space. And that's the definition above. I mean, for these convex. I mean, if you put k convex, you get this correction term over here because you have a correction term in k over here. But that's, you know, not conceptually.
00:36:44.834 - 00:37:44.822, Speaker A: You know, there's nothing conceptual about that. Okay, very well. So these, if you do, perhaps a warning if you try, I mean, an interesting warning, if you try to do this on a d where the distance comes from a norm, this equation, this inequality does not characterize gradient. Here we use, in an essential way, the fact that the distance comes from a norm so that we can differentiate. We can differentiate things, you know? Okay, try, if you wish, and see that something goes wrong. In some sense goes wrong, the same thing that goes wrong. You know, we had to use the inverteanity in that computation of the derivative of the energy and computational derivative of the distance in order to get a certain linearity, which is exactly what goes wrong if things are fixed.
00:37:44.822 - 00:38:24.426, Speaker A: Okay. All right, so. So motivated by this, we say that. That a car with a, you know, Navy IK gradient flow trajectory provided this inequality set. Okay, now, there is at least one thing which is a little bit unpleasant here, and namely is the department. Basically, you have to decide in advance the value of k. So you question yourself whether, you know, the car is an area k grain flow, and the, you know, and the k appears in the definition, whereas for gradient flows, in the energy, the sepia says there is no k, right? And, well, that's it.
00:38:24.426 - 00:39:11.156, Speaker A: Okay. I mean, we can work out, in fact, strictly speaking, I could remove this k, but I will not. Okay, so that's it. And in some sense, and in some sense, the reason for this and this, if you want, is a bit evident, or more evident from this example, is the following. This inequality encodes the fact that, or if you want, the fact that this, that for every x, pick a smooth function e and wonder whether for any x there exists v such that this is true for every y. If this happens to be true, then your function is convex. Because what I'm telling is that, is that, you know, for every x, you can find an hyperplane that is touching from below the graph of the function.
00:39:11.156 - 00:41:17.052, Speaker A: And if you think about this a little bit, you see that this implies the convexity of the function, okay? So in some sense, this inequality is encoding, in some sense, is encoding the convex theory, okay? And also because of this fact that we are differentiating distance squared and imposing the certain nice properties, it is also encoding the fact that some hibertian condition of molescades, in some sense, okay? So in some sense, we do expect that this inequality also metric setting encodes some k convexity, property of the energy, and some in burtianity at small level. And we'll see later on in with sense, okay? All right, let me perhaps add a couple of basic considerations about these evis. Then we take a small break. So, remark or proposition. The following are equivalent. So xt is, you know, an avi, a gradient flow trajectory for the functionality two, the inequality. So, distance squared xs y minus distance squared x dy divided by two plus the integral t, two s of the energy of x plus k over two distance called xr y.
00:41:17.052 - 00:42:40.444, Speaker A: Doctor, this is less or equal than s minus the energy of y. And this should be true for every y and for every non negative tns. Okay, so what I'm, what I'm saying here, what I'm gaining here by integrating in time is defined. And now this inequality is true for every tns. Okay, three, you know, same as, you know, the original one. This is star, but with, you know, limbs, limb soup as h goes to zero or distance squared xt plus h y minus distance squared xdy divided by two h in place of dt, this transport, and four, same as above. But, but, but for only, but only for y belonging to a certain d dance.
00:42:40.444 - 00:43:32.044, Speaker A: Okay, all right, let's, let me prove this. So the fact that one implies two comes just by integrating this. I've just integrated this inequality. That is an inequality concerning the derivative of an absolutely continuous function. I'm integrating from t to s. Okay, I can extend these up to t equals zero, even t equals zero, just by first bracketing it for t positive and then letting t go to zero. And the left hand side is continuous in t.
00:43:32.044 - 00:44:14.422, Speaker A: So that's okay. Now, two implies three, because just write two with s which is equal t plus h divided by h and let h go to zero. And you get, you get what you want. Okay, you need to use, you need to use a fatuous lemma to, you know, pass the, you know, the average integral. I'm using the fact that the average integral from t to t plus h of e xr, the limit of these, as h goes to zero. This is greater, equal than the energy. Actually, I'm not, you don't even need photo, sorry.
00:44:14.422 - 00:44:48.770, Speaker A: You just need the lower semiconductor. Okay. And you get this. Now, the fact that three implies one, what is obvious, right? Actually, three, let me, let me, same as star, but, but for every t greater than zero, end with. Okay, let me be more precise. Three, I'm asking. So this right derivative, taken as lim soap, is a point where it's not the derivative, you know, that exists almost everywhere.
00:44:48.770 - 00:45:23.228, Speaker A: Just because the curve is absolute continuous for every function I can compute is that derivative. And what I'm telling is that this right derivative at any t greater or greater than zero is, I mean, satisfies that inequality, right? And why this is true? Well, because the integrated version is true for every tnt. So you just write s equals t plus h divided by h and let h go to zero. Okay, so three is also, I was not precise. So c also in this sense, three also in this sense. Now, the value three plus one is trivial because x t is absolutely continuous locally. So this write different quotient for almost every t is equal to the derivative.
00:45:23.228 - 00:46:08.854, Speaker A: So that's okay. And what about the fact that, um, sorry, dense, of course, should have been more precise density. So let's speak about four. What does it mean that I'm requiring the inequality for, not for every y, but for every y density. It means that is density energy. Remember this density b inside x is dense in energy. If you know, for every y in x, there exists a sequence yn inside d such that yn converges to y.
00:46:08.854 - 00:46:45.680, Speaker A: So that's density. Density energy means that not only this is true, but the energy of yn is converging to the energy of y. Okay, sometimes. So, in general, the lower semic continuity of the energy only gives you an inequality here. I'm asking this, you know, to be true with inequality, right? Well, but now with this definition, it's clear that if I know this, for every y, when I said density pass to the integral formulation, pass to the two for that y. And then. And then you see.
00:46:45.680 - 00:47:13.700, Speaker A: You see that two. Both sides of two are continuous in y as far as. Also the energy is continuous. Or if you want, write two for yn. Fix y, pick y n, as in here, write two for that yn and pass n to the n to the limit. Distances converge because there I don't. I'm not taking any more derivative of distances.
00:47:13.700 - 00:47:38.714, Speaker A: So, you know, as far as the base point convergence and energy is converted because. Because of density. Okay, okay. Let's take a few minutes break, and then. And then we see actual, you know, serious properties of the avi's unique. Okay? If x naught is equal to y naught, then x t is equal to waiting for everything. Okay? Right? So please.
00:47:38.714 - 00:47:52.666, Speaker A: It's true. It's true. It is obvious energy is finite at some point. I think you're right. Yes, you're right. Yes, you're right. Yes, of course.
00:47:52.666 - 00:48:09.034, Speaker A: You're right. Yes, of course. What had in mind is that in a few lemmas, I will prove some a priori estimates. So I will tell you how small it is actually. But of course, you're totally fine. Thank you. I don't need to make assumption.
00:48:09.034 - 00:48:46.846, Speaker A: So you have this quota three destiny, right? Now you remember that for convex functions on general Banach spaces, the gradient flows are not necessarily unique. So if you want a corollary of this, is that not every gradient flows of convex functions on manuk spaces typically don't satisfy the. Otherwise, we have a problem here. There are various ways of proving this. And in fact, to be honest, that this exists of Judia is not necessary. But it makes the proof, I think, more transparent. So I will keep it.
00:48:46.846 - 00:49:09.314, Speaker A: And anyway, it's always satisfied in my. In my situations. So what is the deductible? Well, a fix. Well, first of all, we know. We know that the matter takes t and the times this distance is absolutely continuous locally at least, because xt and yt are absolutely continuous curves. So there's no, you know, there's no problem in the regularity of this. So we basically use grombos lemma.
00:49:09.314 - 00:50:06.094, Speaker A: Okay? And what I would like to do, what I would like to do is say k equals zero. Just for simplicity, I would like to write this for the curve x t with the point yt as reference point. Then revert the roles of yt and xt and add things up. Then the energies of yt and xt will simplify and they will get a differential inequality for the distance squared problem. Is the problem is that in general, you know, in a general metric space I don't. So the problem is that I don't know whether, you know, these sort of lab needs short holes where this is equal or not to what it is. Ds in s equal t of distance squared xs yt over two plus the same derivative of distance squared y m x t y s.
00:50:06.094 - 00:50:38.824, Speaker A: Right. I would like, you know, some, I mean if this formula were true, right, I basically the plan that I just stated would work. Okay, now there are a few ways of handling this. The fact that this formula is not true, as I wish. And one way is the following. Pixt and let and let's say s into, you know, gamma ds. If you want biodesic from x t to y s two y t.
00:50:38.824 - 00:51:37.550, Speaker A: And write this in the reversion with this right derivative with as a charge of point, you pick the midpoint of digit. So let, we want to let and pick and let just for brevity, say empty the midpoint, this is gamma t one r. Okay, then what we know, let me write for brevity this d plus dt of distance squared xt. What s in s equal t x m t divided by two plus the energy of xt plus k over two. The distance squared xt empty is less or equal than the energy of empty. Where am I going? Wait a second. Let me think.
00:51:37.550 - 00:52:06.042, Speaker A: 1 second. I was forgetting assumption. So assume also, actually, I mean this is not an assumption. So assume also that e is k. So basically convex. In fact, it's not an assumption. In a second, we'll see.
00:52:06.042 - 00:52:59.444, Speaker A: This will be true. Okay. And epic gamma diuretic for which the density convexity of the response is. Careful. I mean, I was trying to be minimal in the assumptions and of course I, you know, I removed a few too many, but uh. Okay, so, okay, well, so given that I have, I'm assuming that this curve is where this convexity, o, this energy in the midpoint is less or equal than one half the energy of xt plus the energy at yt. And then I have a correction term which is what it is, minus k over two, k over eight, I guess distance squared xt yt.
00:52:59.444 - 00:53:48.444, Speaker A: Right. So, and now I write the same for y's. For the carbon here I used the fact that xt was an evi gradient flow. I do the same for yt and then I get the d plus ds in s equal t on the distance curve between y s and the same point. Empty. This is the midpoint between x t and y t plus the energy at yt plus k over two distance squared yt empty. Which, by the way, okay, we commented this is less or equal than d one, the same thing that is over there.
00:53:48.444 - 00:55:00.944, Speaker A: I made a bit concerned that there are too many cancellations, I guess. Okay. Anyway, so now the observation is this, the observation is this, let me, let me, okay. Is this, is that mt is a minimizer, minimizer for, you know, distance squared xt dot plus distance squared y t dot. If you have two points on a metric space, you look at the function distance square from one point from plus distance squared to the other point for three reasons. Any midpoint minimizes this quantity for, you know, triangular quad, basically. Okay, so, because given that this is true, if I compute, you know, so what I'm saying, what I'm saying if you want, is that, is that, is that the distance squared xt mt plus distance squared ytmt.
00:55:00.944 - 00:55:40.924, Speaker A: Well, this, I know how much it is, right, because this is the midpoint. So the distance is one half of the distance. A distance squared is one four. So this is one half distance scored xtyt. And wait a second. But for any other, you know, but, you know, in general, in general, distance squared between x s and m t plus distance squared y s and m t. This typically is greater equal than one half distance scored x s y s.
00:55:40.924 - 00:56:32.154, Speaker A: If you want, by triangular equality, this is less or equal. Then distance between x s and m t plus mt y. You expand the squares, use young inequality, and you get this inequality. Okay, all right, but then, but then you see, okay, now it's just algebra, right? So d plus, or the derivative, derived derivative as s equal t of distance squared xs y s over two. Because of this, this is less or equal. Then the sum of the two derivatives of distance squared xs empty, you know, divided by two plus the symmetric term. Okay, now you plug it to an equality.
00:56:32.154 - 00:57:12.024, Speaker A: You pay more attention than what they did to handle the terms in k. That here there is something wrong. But say if k equals zero, if k equals zero, you get, um, you get basically that this derivative is less than zero, which is the end of the proof, okay? In general, you get, in general, you get the derivative. In general, you get, you should get the derivative is less or equal than minus two k. Uh, distance squared or minus, don't know something. Okay, but I mean, the conclusion works. Okay, okay.
00:57:12.024 - 00:58:04.984, Speaker A: I mean, there's nothing beside, you know, checking the correct coefficient in front. Okay, so we have contractivity, a huge property. Another property is the following. And, okay, yes, yes. And this perhaps will, you know, this is my excuse for making, for forgetting an assumption over there. And this is another proposition. This is a result by Daenerys Avare around, I think, 2008, if I got it to remember.
00:58:04.984 - 00:59:36.900, Speaker A: And the theorem is this. So say so let. So, so say that for every, suppose that for every x in x, there exists an avi k gradient flow trajectory, or e, and assume also that. And assume that for every x y in the domain of the energy, there exists a Judisic connecting them. Then for every gamma from zero one to x, geodesic with finite energy at the marginals. So the energy gamma naught and the energy gamma one is finite, the curve t, you know, we have the convexity in the convexity. So, so this is a metric.
00:59:36.900 - 01:00:45.654, Speaker A: So you remember before, when discussing the heuristic behind the evi, I've mentioned that EVi encodes at least, you know, in the smooth setting, the vi encodes both the convexity of the energy and some hilbertianity of the underlying space. This gives a rigorous recipient of what it means that encodes the convexity of the energy, right? As soon as there are jurisdictions, okay, connecting points with finite energy, you do have that along any such logistics, you have decay convexity, okay? So in particular, if you have EVik flows, you have more convexity than what you actually typically get from the k convexity. You see, you also get, you also get, in particular, let me just mention this for the case k equals zero. In particular, in particular, the map that takes t and returns the energy along gamma t is actually convex. It's really convex. This is more than what we typically ask for JudesI convexity. Remember, for JudiSsI convexity, we only typically say that the energy at gamma t is less or equal than one minus t.
01:00:45.654 - 01:01:37.070, Speaker A: The energy at gamma naught plus t times the energy gamma one right? For every t in zero, one right. And I remarked that this is less than the convex, the actual convex. This is just saying that, you know, this is zero, this is one. And I'm drawing the graph of an energy satisfying this assumption, right? So, you know, this inequality is just telling that the graph of the energy is below this line, okay? And, you know, for technical reasons related to this, basically, typically to the stability of this property, one just requires this. And you can say, well, but it's not complex along, you know, these points. Well, and the answer to this, well, between this point, there would be maybe another curve that is a jurisic, along which maybe the carbon actually still just like this, etc. Etc.
01:01:37.070 - 01:02:04.254, Speaker A: So, so, so, actually saying that this is convex, you know, is more. Okay, and why do I get convexity? Well, because, you know, look at the sub interval. The sub interval is that you dc connecting points with finite energy. But then, you know, I should have the convection equal even along that units. Okay. And the proof, luckily, is trivial. The statement is not, it's deep, but the proof is triggered.
01:02:04.254 - 01:02:57.768, Speaker A: Let me do the case k equals zero. What do I do? Well, I pick, you know, let so fix x y or say what? X? Yeah, actually fix a geodistic, fix x and y and let, you know, gamma be adequate. Xy with finite energy gamma b adequistic connecting them. And now we write the. So y is adjudic. So there is. So let me check the convexity inequality at t time one r.
01:02:57.768 - 01:03:55.464, Speaker A: Okay, for different times is the same. Let me write this for time. And so there exists a curve, let's call it zs, an evi gradient flow zero gradient flow trajectory starting from, starting from gamma one r. All right, well, let me take definition two. What do I know? So I use the equivalent property, two for the curve for this gradient flow trajectory peaking as reference point y once I pick x. And the other time I pick, I pick y. Look at what happens.
01:03:55.464 - 01:04:35.804, Speaker A: So I get that, you know, I know that the distance scored between zs and x minus the distance scored between, you know, gamma one r and x divided by two plus the average zero s of the energy z r. The r, this is less or equal than the energy at x s times. Yes. Right. Actually, let me do this. Okay. And of course, now I do the same with y.
01:04:35.804 - 01:05:18.114, Speaker A: Gamma one half y over two s plus actually the same quantity. This is less or equal than the energy of y, right? Well, but now I add up these two inequalities and I let s go to zero. And I use an s before I use the same. This thing, actually, I use, if you want, I use the. What is that? I use, I use the. I don't even need actually, because this quantity. So the sum of these two is positive.
01:05:18.114 - 01:06:19.360, Speaker A: And this sum of these two is positive because the point gamma one is a minimizer for distance squared from x plus distance squared from y. Makes sense. Here I have distance squared from an arbitrary point zs to x plus distance square zx two y minus this gamma one half x. Gamma. This is always possible because this guy. Zero x. Okay? So when you add these things up, what you get, you get twice the average interval from zero to s of the energy as zr is less or equal than the energy at x plus the energy y, you let s go to zero, you use the lower semi continuity of the energy, you know, and that's the convex denominator, okay? At our arbitrary tradition, so that, that gives the convexity, okay.
01:06:19.360 - 01:08:17.074, Speaker A: So whenever, so if you have many evi gradient flows starting from any point, then you actually, then you actually get, then you actually get convexity. Okay, now, I gave you two different definitions of gradient flows in metric spaces. I should really, I should really compare them. So let x t be an evik gradient flow trajectory for the function e. Then for any epsilon positive, the curve that takes t and returns x t plus epsilon is Lipschitz and an ed gradient flow, getting flow in the energy dissipation sense. And in particular, this last result is due by sabre. Second, the car that takes t and returns xt or e to the what e to the kt slope of the energy.
01:08:17.074 - 01:09:27.892, Speaker A: These are non increasing. And three, we do have the following year priority estimates that I never remember. So yes, we have e to the kt. And again, first I will comment this, and then I go to the proof this is true for every y and t. And here I should tell you with this ik, ik of t is nothing, but is the primitive. It's a concise way of writing this e to this k. You should think that.
01:09:27.892 - 01:09:59.774, Speaker A: So in the case k equals zero. So k equals zero I, zero of t is just t, okay? And it just grows, grows like t. So think of this as this inequality as you know it st. Okay, so let me comment this. So, first of all, one tells you, tells you, you know, that there is a compatibility. You know, whenever you have an Evi gradient flow, then you have also a gradient flow in the sense of energy dc ratio, okay. Two is a regularization property.
01:09:59.774 - 01:10:42.912, Speaker A: And three are, these are priority estimates. And these are priority estimates. I want to stress that these are as powerful. In fact, they are equivalent to the typical a priori estimates that you get for gradient flows of k convex functions on inverse basis. So basically, even though now we are in the metric setting, if we encode the gradient flow interpretation through the evi, then you really get in sometimes the same properties that you get on eberspaces. And let me comment on this. So, so these estimates are given in terms only of the distance of the initial point to an arbitrary, you know, reference point that we chose.
01:10:42.912 - 01:11:35.818, Speaker A: Okay? In particular, on the right hand side, on the right hand side, there is a distance, there is not an energy. And estimates, you know, for the gradient flow trajectories, in the sense of energy dissipation at the, you know, the left hand side or right hand side where it is, there was just the energy at the initial point, right? And because of that, we could only speak really of gradient flows in that sense for, you know, initial points that add finite energy. Otherwise the condition basically trivializes, okay? While here we are getting rid of the energy and we just have the distance. Okay? And we will see that because of this. This is a crucial technical tool, technical fact that allows us to speak about evolves starting from points with infinite energy. Okay, first remark, then another reminder. And this is typical in calculus of variation.
01:11:35.818 - 01:12:21.494, Speaker A: You have a function on a space and as soon as you have that, you have three times sort of level of regularity. The lower level of regularity is that the points belong to the space and improvement regularity is the point belongs to the space and it has finite energy. And the third level of regularity is it has finite energy and also finite slope, okay? And here we are not distance and so, belonging to the space in some sense having finite energy and finite slope. And look at what happens when t goes to zero. So this goes to, one is a constant. This, I mean, goes to zero, st. So I'm saying that t times the energy remains controlled and t squared times the square slope remains, remains controlled.
01:12:21.494 - 01:12:50.156, Speaker A: Okay? Now, for instance, if you think, I don't know, if you, I mean, this is really, you know, very common. If you think of heat flow, say in l two, then you have l two functions, is where the function is defined. Some function w twelve, where the function where the change is finite and functions with Laplacian l two. That is where this loop is finite. There are these three levels. Or if you have, you work with sets of finite perimeters. You have arbitrary sets, sets with finite perimeter.
01:12:50.156 - 01:13:47.924, Speaker A: And typically you have this local minima or quasi minima of the perimeter that are those basically where the slope is finite in some sense. I mean, that is more. But you only kind of goes in the direction. Okay, let's do this. So this lipstick gravity is quite easy because for every epsilon, this curve is still an evi gradient flow trajectory. You just, if you rescaling time, forwarding time, there is nothing that, you know, previous to the gradient, right? So, so, um. Aha.
01:13:47.924 - 01:14:56.734, Speaker A: Now, now I got into trouble. Let me say, okay, so what I want to say, what I want to say, what I want to conclude is that then the distance between x, t plus h actually s plus h and s. I want to say that this is less or even the distance or e to the kt the distance between, okay, s minus is only x t plus epsilon and xt for every t less than s. Okay, what I want to say is this, right, because I have two evi gradient flow trajectories, right? Let me start from the point x t or from the point x t plus epsilon, and let me flow for a time equal to s minus t. S is bigger than t. Then xt goes to excess, xt plus epsilon goes to excess plus epsilon, and I discontractivity, right? So pick a point. So pick a point t where this, where the speed is finite.
01:14:56.734 - 01:15:54.936, Speaker A: X t is an absolute, is a local, absolutely continuous. So for almost every t, it has a matrix speed. Okay, fixed such t. And then, and then, and then, you know, fixed t so 40, such that, uh, the limb as epsilon goes to zero of the distance x t plus epsilon x t divided by epsilon is finite. If you want fix this, then what this inequality is telling is that, is that for every s bigger than t, this, this limb soup is also, you know, if you want, if you want the metric speed, let me put this with the metric speed is less reliable than this lim soup. For almost every s bigger than t. Makes sense, right? But if the matrix speed is bounded, the curve is limited, right? Just, just contracting.
01:15:54.936 - 01:16:39.716, Speaker A: Sometimes it just goes lower and slower. Of course, I slightly cheated, because I only know this if I have this assumption about the existence of jurisdicts, at least in the previous lemma. So, if you want, add that assumption if you wish, or find a way that is possible to prove this contractivity estimate, even without the existence of judicial, I mean, any of the two works. But let me sometimes be a little bit fuzzy here. So, add the assumption if you wish. I mean, these assumptions, the existence of Judith in the domain of the energy, it will always be the case for us. Now, let's prove that these Evi gradient flows, that they do satisfy the energy dissipation, which seems a bit crazy.
01:16:39.716 - 01:17:50.548, Speaker A: We have this. The CVI is really is a way of controlling the derivative of the distance scored in terms of the energy. And this actually implies that this energy dissipation formula holds. So, let me prove, let me prove in this, in the case k equals zero, there is no additional complication except in writing formulas, but no concept of complication improving that these are, you know, the dissipation formula for the energy works. Now, first of all, notice that if I rearrange the terms in the evi, in the formulation of the, of Dvi, what I get is that the energy of xt minus the energy of y for every y divided by the distance between x t and y, this is less or equal. If k is equal to zero, this is less or equal than one over twice the distance between x t and y times dt of distance squared xt for every y and almost everything. You agree? This really comes just by rearranging terms in Dvi.
01:17:50.548 - 01:18:38.044, Speaker A: Okay, one half distance squared, derivative of distance squared is less or equal than energy o y minus. So I'm just whooping the. Okay, maybe there's a minus. Okay, well, but this is, this, you know, by this rule, this is equal for almost every t minus, the derivative of x t y, which of course, is bounded from above by x t naught. Make sense, right? So in particular, so this quantity where every y, this quantity is bound, you know, for almost everything, is bounded by that quantity. So I can, which is positive. So let me take the positive part in here.
01:18:38.044 - 01:18:53.946, Speaker A: Positive part, positive part. This is positive. And what I get now, I get, I let, I send y to x t. I cheat a bit about fixing p and y. Swapping priorities. This, for almost every t, is almost ideal. Priority should depend on y.
01:18:53.946 - 01:19:59.594, Speaker A: But in fact, for every t for which this is metric, speed is well defined, is inequality holds with the rim soup. Okay? So I know that for almost every t, this inequality holds for every y. And now I let y go to xt. And what I conclude is that this loop of e at x t is bounded from above by the speed for almost everything. And that's, you know, the first piece of information, a second piece of information is, well, of course, if I now pick here instead the y equal to xs, okay? What you get is that the energy of x t minus the energy of x s divided by the distance between the two is bounded by the speed, okay? But now if you swap this, basically, you're telling it. This is bounded by the max of these two, okay? And this is bounded by, you know, if t and s are bigger than some epsilon, this is by the, what we already proved about local. This is uniformly bounded.
01:19:59.594 - 01:20:44.926, Speaker A: So I'm proving that the map that takes t and returns e of xt is locally. Lipsticks make sense. Because as soon as I pick t and s bigger than epsilon, this is actually liquid, okay? So it is locally. So it is differentiable almost everywhere. So I can compute minus dt of the energy of x t, okay? This derivative is the limit as h goes to zero, of what I do. I do the energy of xt minus the energy xt plus h, because there is a minus. So the h, in some sense, goes with the minus sign.
01:20:44.926 - 01:21:28.594, Speaker A: And then I divide by the distance between these two. I multiply by the distance between these two divided by h. Okay, well, then, of course. Well, let me, you know, this is less or equal. Let me take you the positive part. And when h goes to zero, this. And if t is a point where the matrix speed exists, this is, you know, the bounded, followed by the slope times the metric speed, right? So, Bianca, inequality, this is bounded by the speed squared, one half plus one half the slope squared, right, for almost everything.
01:21:28.594 - 01:22:49.014, Speaker A: Okay, uh, so, and this is, and this is the second inequality, people. So I have one. Okay, now it comes the third and last inequality. So, the energy dissipation formula is something that relates the dissipation of the energy, the metric speed and the slope, and basically tells that the three agree up to scores and factors, one half and so on and so forth. So, I'm writing down three inequalities that combine the Gibbs, this, and the last thing I could is this. Pick, what do you pick? Peak y equal to xt in this. And you get the distance squared between x s and x t divided by two, plus actually is less or equal.
01:22:49.014 - 01:23:42.952, Speaker A: Then the integral from t to s of the energy of xs minus the energy of xr. In the r case, zero. So that term will, okay, this is equal. Let me rewrite this. This is equal to the interval from zero to one of the energy of actually, of s minus t is the factor. And then I have the energy of what I have x plus h, time s minus t minus the energy of x t. Like this, correct? No, no, it is not.
01:23:42.952 - 01:23:55.248, Speaker A: Wait a second. No, no, sorry, sorry, sorry, sorry. I mean, I mean, sorry. So here I should have, sorry. I pick, sorry. Y equal x. Sorry.
01:23:55.248 - 01:24:15.614, Speaker A: Y equal x t in two. Y equal x t. Sorry, sorry. Y equal x t. So this term drops out of this term, squared xs xt. This term gets out. So I get the energy of x t minus the integral, the energy of xt minus the integral of this, sorry.
01:24:15.614 - 01:25:04.758, Speaker A: Now I, you know, I rescale this integral, you know, I change the variable, and this is the interval of energy, x t minus the energy of x at t plus s minus t times h. If you want the h. Now, if you divide by s minus t squared and you let s go to t, what you get is that the spin squared over two is less or equal. You know that the limb when h goes to zero. H is a terrible name. Here, let me. So let me put r here.
01:25:04.758 - 01:26:36.092, Speaker A: R limit as h goes to zero of the interval between zero and one of energy of x t minus the energy of x t plus h times r divided by h in the ring. Right? But now, but now by, you know, if you pick a point when, where this guy, where the energy as a function of time, energy of x t is differentiable, you see that this is really, this is really equal, this is really equal to minus the derivative of the energy of x t for almost everything, right? And this is the third inequality. Okay, there is a factor two somewhere. There should be, there should be a factor one half, sorry, one answer. One half counts. Because basically this, this is basically, you know, should think this is kind of, kind of the derivative of ext times r, right? So when you integrate r from zero to one, you get one half, okay, so, so this is the third inequality. You know, the speed is bounded from above by the dissipation.
01:26:36.092 - 01:27:24.674, Speaker A: The dissipation is bounded from above by a combination of speed and slope, but the slope is bounded by the speed. Put these three together and the claim is proved. All right, now I had to prove that the functional is decreased or non increasing in time. And now this is obvious by the energy distribution formula and that the slope up to this exponential factor is also decreasing in time. But at least for almost every t, I know that the slope is equal to the speed and the speed decreases in time. Right? Because of this. So up to the, some, almost everywhere that I should handle, I sort of almost proved that the slope is decreasing.
01:27:24.674 - 01:28:11.934, Speaker A: Okay, let me conclude the proof the of the a priori estimates. Again, I do this in the case k equals zero. But the point is this. Let me compute, let me compute t squared over two, this log squared in x t I k. Now given that, forget about this, in t this is decreasing. This is less or equal than the integral from zero to t of s. Sorry, of s slope squared in excess.
01:28:11.934 - 01:28:54.374, Speaker A: In fact, I proved that for almost every s smaller than t, this is bigger than this point. Right? So then you integrate s and you get this, okay, but I also know that this is the derivative of the energy, right? By what we just proved. So this is equal to the interval from zero to t of s times. Okay, I should write, okay, I should write this minus the energy of excess. But given that I'm a little bit lazy and I want to, I fear of making confusion with the integration by parse formula. Let me write like this. This term does not depend on s.
01:28:54.374 - 01:29:27.834, Speaker A: So I can add this. And I have the advantage that when s is equal to z, I want to degree by plus. And then when s is equal to zero, this part, this term is zero for the boundary term. And when s is equal to t, this term is zero. So when I integrate by parts, I don't get boundary terms. And this is just, so this is just the inter, from zero to t of the energy of x, t minus the energy of excess ds. This correct? No, should be minus success.
01:29:27.834 - 01:30:11.518, Speaker A: Why is that? Should have, I should have the other sign. Why is that the case? Yes, because I integrated web parts, of course. Okay. Okay. Now what do I use? Okay, now I use Dvi. DvI is telling me so. And you know, forget about the integral.
01:30:11.518 - 01:31:01.890, Speaker A: I know, let me write it here, what I'm using so that you see it more clearly. I'm using the fact that ds of distance squared xsy over two plus the energy at excess. This is less trigger than the energy y for any y and almost every x. This is true, right? So, so the energy at excess is bounded from above by, so I still have this term, this I didn't even touch. This is bounded from over the energy at y plus ds of this distance called xsy over to ds. Sorry. Minus, thanks.
01:31:01.890 - 01:31:31.934, Speaker A: Minus, thanks. Okay. Okay. Now this is less regarding what? Okay, this guy and this guy do not depend on s. So this is just t times the energy at y minus the energy at x t. And here I'm integrating this distance squared. So this is, I have this zero because of this minus, the zero term comes with a plus.
01:31:31.934 - 01:31:58.794, Speaker A: So plus distance squared x zero, y over two minus some distance squared, which is positive. And therefore I forget. So I have less of the point. And you see this inequality with k equals zero is precisely this. You know, I have, uh, no, uh, wait a second. No, I, um, wait a second. I keep the term, but I keep the term.
01:31:58.794 - 01:33:10.264, Speaker A: I, I don't throw it away, x t minus y divided by, sorry, this is equal, I throw away, I don't throw away this last term. So I have the distance scored x t y over two. If I bring to the left comes with a plus, where this I bring to the left, it comes, you know, with a correct sign as written here, plus t squared over two, slope squared. This is bounded from above by this guy over here. Okay, okay. Let me perhaps, let me conclude, and let me just conclude with the following observation, and this will be in some sense the starting point, okay, perhaps a starting point of next Friday's lecture. So an observation is the following corollary.
01:33:10.264 - 01:34:48.596, Speaker A: So let you know e from our matrix space to zero plus infinity be such that for every x belonging to some d, then seen x not in energy, there exists a gradient flow, an evi, a gradient flow trajectory starting from x. So assume that you have existence of evik gradient protection not for any initial point in the space, but just on a dense set. Well, then existence is in place for every initial point. All right. And, okay, how, I mean, so how would I like to prove this result? Well, what I would like to say is, okay, look, pick x in x and x n in this dense set, converging in x, right. For each of these, I have x and t, right. The corresponding trajectory by the contractivity estimates say that the space has jurisdicts.
01:34:48.596 - 01:36:29.894, Speaker A: If you want sufficient jurisdictions, then by the contractivity estimates, I know that the distance between x and t and xmt is bounded by what is e to the minus kt, the distance between xn and xm, right? This is true for every t, right? So whenever the initial points are converging, the corresponding Evik gradient flow trajectories are converging locally uniformly somewhere. I mean, look at my space is complete, they convert locally uniform, okay, so exists, you know, locally uniform limit, call it xt. And clearly this is a curve that starts from x, okay, also, clearly, clearly satisfies too, right? Because if you write this for every nice right, and then you pass the limit in n, you see that there is, you know, the distances converge for the energy. I typically only have lower semicontinuity, but the energy is, you know, on the left hand side of the neutron, the lessoric. So, but to allow me to pass the limit on the energy, okay, so for trivial reasons, any locally, actually, any point wise limit of evi gradient plus satisfies inequality. Okay, so then why I did not, you know, stated this corollary, 1 second after having given the definition of evi, because a priori, it is not trivial. And I should work a little bit to prove that the limit curve x t is absolutely continuous.
01:36:29.894 - 01:37:20.584, Speaker A: This is just a uniform limit. So in principle, just a continuous curve. Okay, how do I get absolute continuity where from these are priori estimates, which I raised, okay, because thanks to these a priori estimates, I know in particular that the energy, you know, fix any y with on the space. Forget about this positive term. Forget about this positive term if you wish, or I mean, keep whatever you want and fix y with finite energy so that this is not minus infinity. And what you get, what you get is basically the soup in n of the energy of x. And t is finite for every t positive.
01:37:20.584 - 01:38:14.106, Speaker A: But now from the energy dissipation formula, you know, that the energy at x and t is equal to the energy at x and s plus the integral from t to s of the speed squared of this car. Okay, so this is non negative. Let me throw that out. This is bounded by some constant. And you get, you know, the uniform bound in n. You fix t and s positive, right? And you get a uniform bound in n of the energy of these curves in that interval. Now, uniform bound, you know, this, on the only two energy, this passes to the limit.
01:38:14.106 - 01:38:36.264, Speaker A: And thus the delimit card is also absolutely. Okay. That is local. Okay. So some work of this form has to be, you know, done in order to be sure that you have, you know, an absolutely continuous car. And. All right, so there is one last thing that I wanted, that I wanted to mention today, perhaps.
01:38:36.264 - 01:39:21.796, Speaker A: Let me just draw a picture. And then Friday, we start from this picture, because I've mentioned that. So, just to wrap things up, we have two notions of gradient flows of k convex function. One which is based on the energy dissipation equality, and the other one, which is based on the, you know, this evolution, variational inequality. And whenever, whatever definition you have, typically, typically, you are concerned with existence, uniqueness and stability. Stability. Now, I mentioned that the.
01:39:21.796 - 01:39:49.974, Speaker A: For the, you know, for this concept of gradient flow, of convex function, we typically have existence. Existence. Well, we don't have existence for DVI, okay? Because if we had existence, you know, for Evi gradient flows, these Evi gradient flows would also be ed gradient flows. So they should be contractible and unique. But we have examples of convex functions on balance base where you. We don't have uniqueness. So in particular, typically, we don't have existence.
01:39:49.974 - 01:40:12.386, Speaker A: Conversely, for the ed gradient flow, we typically don't have uniqueness, as just mentioned. But for dvis, we get. Actually, we have more. We have contractivity. Okay, what about stability? We have already mentioned the stability of these and the uniform, you know, k convexity property. And we will see that, in fact, even there is also stability at the level of DVI. This is.
01:40:12.386 - 01:40:38.204, Speaker A: We will mention, we will mention at the beginning of Friday's lecture. So there are two different ways of working with gradient flows. In some sense, the basic theory works with this. But if you are in a situation where you also have existence for Dvi, k gradient flow, then that point of view typically works better. Okay, when is available? Okay. Okay. So, end of math, a couple of communications.
01:40:38.204 - 01:41:12.960, Speaker A: So, next week, there will be no lecture because there's the conference on aspects of riccibangs and perhaps let me make explicit something that I mentioned to a few of you informally. So, for those of you who. I mean, you and whoever else is going to take the exam, or we need to take the exam for this course, the exam will consist basically on a presentation. So few weeks, we arrange things. A few weeks before the exam, the student comes to me and says, I want to do the exam. And then I give you to read something concerning the topic of the course. Could be the functions, could be Evi gradient flows.
01:41:12.960 - 01:41:25.424, Speaker A: Could be, you know, some topic. I give him something to read and then a representation of, say, half an hour or something like this on the topic. Okay? And that's it. See you on Friday.
