00:00:00.760 - 00:01:39.062, Speaker A: Hello, I'm Stefano Luzzato from the Abdus Salam International center for Theoretical Physics, and I'm going to talk about some recent work together with some collaborators. Aligol Makani from Iran, currently in Brazil, Edward Kujinan Komlan, Edward Kujinan from Benin, currently in Austria, and Pavel Pilacek from Poland. So quite an international team effort. And we'll talk about some work in the quadratic family. This is an area that I've been working on for a very long time, and I want to talk about some regulus numerics calculations that we've carried out, and also some possibly take advantage of the expertise on this online symposium on machine learning and dynamical systems to get some feedback about whether there may be some machine learning approaches that could work well in this context. In particular, my talk is going to be focused on the themes. The first one is the main theme, the main motivation for this work, which is the study of the real quadratic family, which I will define precisely later, and in particular the set of parameters for which this family exhibits stochastic like dynamics.
00:01:39.062 - 00:02:41.434, Speaker A: I will say a little bit more about what this means later on. And this is something that the quadratic family has been studied for decades, and it's one of the most interesting examples of dynamical systems, and there's still a lot of interesting open questions there. In particular, there's a lot of the dynamics of particular parameters in this family, which are determined by the dynamics of the critical point. Sometimes you need to know the infinite orbit, but even finite pieces of orbit can give a lot of information. And therefore it is interesting to study the properties of, of these orbits, which is what we're going to do. The second theme that comes from this is really the statistics and possibly some machine learning approach to understand the results of what we get. In particular, the results I'm going to talk about will mean that we chop interesting regional parameter space into millions of sub intervals.
00:02:41.434 - 00:04:00.794, Speaker A: And it seems natural, therefore, that there might be some kind of big data or machine learning approaches, which we could use to understand the dynamics of these various properties, or somehow how these various sub intervals can be grouped in some ways. So that would be, for me, in this particular context of the symposium, one of the main motivations for this talk. Finally, I will also finish the talk with some comments about the actual technical aspects of the work, the algorithm and numerical procedures that we use to obtain our results, and to iterate the critical .1 of the interests of this approach is that it seems quite general, and we applied for the critical points. But one of the key focuses is the fact that it is about iterating a point for many different parameter values. And that is quite different from iterating a point for a single parameter value. And so we need some new approaches and some new ideas to do deal with this.
00:04:00.794 - 00:04:43.626, Speaker A: As I said, the focus is the quadratic family. So let me recall quickly the quadratic family. This is a family of one dimensional maps given by the very simple quadratic formula a minus x squared. There are some various variations, but essentially it all boils down to the same thing. And as you change the parameter a, there's a remarkable, a variety of dynamical phenomena can occur. For some ranges of parameter everything is understood and things are fairly clear. For example, for a less than -0.25
00:04:43.626 - 00:05:20.134, Speaker A: all the points just go to infinity. There's no real currents, nothing interesting. For a greater than two, the dynamics is not completely trivial because there is a cantor set, an invariant cantor set, which is bounded, but it has zero measure. And almost every point also just converges to infinity. So there's no real recurrence on any positive measure set. So these two areas of parameter space are fairly well understood and we will not worry about them. This leaves the parameters, the parameter interval between -0.25
00:05:20.134 - 00:05:50.904, Speaker A: and two. And for every parameter in this, there is a interval, an invariant interval in which all the dynamics happen. So outside this interval, all the points just go to infinity. And so this is the interval. This allows us to focus our attention on this interval to understand the interesting dynamics that may happen. And again, within this interval, we can distinguish various regimes in the initial part of this interval from a between 0.25 and 0.75.
00:05:50.904 - 00:06:42.308, Speaker A: The situation is very simple. There's a unique attracting fixed point and everything converges to this fixed point. As a increases, something very interesting happens, which is that there is so called period doubling bifurcations in which the fixed point becomes a, bifurcates into a period two attracting orbit, and then this period two bifurcates into period four attracting orbit. This is represented in the well known bifurcation diagram here. As you can see in the region up to 0.75, this shows that there's a single, the position of the single attracting periodic orbit for each parameter. All the other points converge to this as at 0.75,
00:06:42.308 - 00:07:40.432, Speaker A: there's this bifurcation and you get these two branches which are actually two branches of the same period, two periodic orbit. And then this period two periodic orbit branches into period four periodic orbit, and this one into period eight. And these are actually, these first few period doubling bifurcations can be explicitly seen as you increase further, it becomes, of course, difficult to see these numerically. But it has been proved, following the pioneering work of Feigenbaum in the 1970s, he showed that actually there is an infinite sequence of period doubling bifurcations. And he studied very much the, he was interested in the parameters at which this occurs. And here is the table with the first few period doubling bifurcations. And as you can see, they start occurring very, very rapidly, one after the other.
00:07:40.432 - 00:09:02.852, Speaker A: And in fact, it is now proved that this cascade converges to a certain parameter, a star which is just a little bit bigger than 1.401. And so at this point, this is called the Feigenbaum parameter, and it is the limit of the period doubling cascade. And so we can say that up to this parameter, again, everything is relatively simple, because you, you just have a unique periodic attractor which attracts almost every point, even though this periodic attractor might have very high period. And what is left with really is the remaining region of parameter space, which we call omega, which is from the Feigenbaum parameter to the parameter two. And one of the most extraordinary things is the richness of dynamics in this parameter region, where literally almost anything can happen. There are a myriad number of really dynamical counter examples that can be constructed of very pathological and very intriguing behavior. And this is the parameter on which we still do not know too much, although many, many things are known.
00:09:02.852 - 00:10:06.444, Speaker A: In particular, let me mention that not anything can happen with positive probability. So I will say in a second, in other words, with positive measuring parameter space, it turns out there's only two phenomena. So there is some structure in this madness. And in particular, the two kinds of dynamics that can occur mostly are so called regular and stochastic dynamics. And there's a theorem of Lubitsch 2002, which builds on several previous theorems of himself and other people that says precisely that, that the only phenomena that can occur with positive measuring parameter space are these two phenomena of regular and stochastic behavior, although there's many, many other different kinds of dynamics that can occur even for infinite number of parameter values, but always with positive measure, positive probability. So let me say a little bit about what we mean by regular and stochastic. So we say that f is regular if almost every point converges to an attracting periodic orbit.
00:10:06.444 - 00:11:17.034, Speaker A: And we say f is stochastic if there exists an invariant methylic probability measure mu, which is absolutely continuous with the spectral object. And although this is not immediately intuitive, what this also means in practice is that these maps are kind of chaotic in a very precise sense, that they have sensitive dependence on initial condition, but they also satisfy some very good long term average behavior. So, from a strictly mathematical topological point of view, the dynamics are very, very different in these two cases, although sometimes it's very hard to distinguish the two numerically. However, our interest will be in defining these two sets, omega minus and omega plus. So, Lubitsch theorem says that the union of these two sets have full measure in omega, in our set omega. And the first real milestone in the study of this is way before Lubitsch's theorem. Jacobsson in 1981 showed that omega plus has positive measure, rather contains a set of positive measure.
00:11:17.034 - 00:13:21.838, Speaker A: And the proof showed that this contains a noise, nowhere dense set of positive measures. So, modelling a kind of cantor set of positive measure, he was actually trying to prove that it had zero measure, and came up with this remarkable theorem, which has been generalized very much in many different settings. A little bit later, almost 20 years later, Gracchik and Shantek and Lubitsch at the same time proved that omega minus is open and dense, which implying in particular, obviously, that has also positive measure, and implying also that omega plus is nowhere dense. So the set of stochastic parameters is a nowhere dense set of positive measure, which is very remarkable and interesting structure. And I would say that probably until now, we have been trying, with some success, to understand what it is about the properties of stochastic dynamics that arises for positive sets of positive measure. An interesting question, which is what I want to kind of focus on today, which is the focus of the work that I'm going to talk about today, is, what is the measure of omega minus and omega plus? So, once we've established that both of these have positive measure, it is a natural question to ask, what is the relative measure of each one? This might or might not have any real mathematical motivation or mathematical significance, but for me, personally, personally, the challenge is to actually be able to measure it. So I think perhaps the appropriate question is, can we find the measure of omega minus omega plus? Is there any way to establish what these measures? This turns out to be extremely difficult.
00:13:21.838 - 00:14:20.640, Speaker A: There does not seem to be any analytic way to do this, as far as we know. Interestingly, there does not even seem to be any heuristics as to what this measure should be. There's no heuristic as to whether the measure should mostly whether most of the measure should belong to omega plus omega minus. So this is not at all clear. And maybe trying to compute these measures can also lead to some understanding of why we will get the results we will get. So there are some partial results in this direction. And let me mention that there is a result of Tucker and Wilczek in 2009, in which they studied the measure of omega minus, which are the regular parameters, which is an open and dense subset.
00:14:20.640 - 00:15:07.312, Speaker A: So this is a union of intervals, right? And they show that about 10% of the parameter space we consider is formed by at least 10% is formed by regular intervals. Remember, the parameter is roughly 1.4 till two. So the parameter length of the parameter interval is 0.6. So the actual measure they establish is 0.06. I should mention that they actually do the color calculations for the logistic family, which is a different parameterization of a unimodal family, which is various, which is another quadratic family. However, there is a standard, there is a kind of conjugacy between the two families.
00:15:07.312 - 00:16:04.456, Speaker A: So in principle, one could calculate exactly what this means, and roughly it means the same I have not done, or nobody has actually done, that. It would mean going through several millions intervals and checking what intervals in the quadratic family they correspond to. But this does not have much distortion, this conjugacy. So roughly, we expect that it would be pretty much the same result. So given that they found 10%, yes. So the proof is they actually, it's a computer assisted proof, and they actually compute about 5 million intervals, corresponding to attracting periodic orbits of periods, up to some very large periods, some of the largest, I think, about 30,000. And so some of these intervals are obviously very, very small.
00:16:04.456 - 00:17:17.640, Speaker A: And according to the authors, it really would be difficult to push these techniques beyond that, because the intervals become extremely small and the computational cost becomes extremely large. So we do not know, of course, this is still a finite number of intervals. It is not clear what the remaining parameters belong to. So when you remove this 10%, the still leaves about 90% of parameters unaccounted for, which may be stochastic or may be regular. And we have no idea, no idea. A little bit earlier, I myself with a colleague, Hiroki Takahashi, in 2006, we looked at Omega plus I to get some estimates for omega plus some explicit estimates. So I should say that the paper of Jacobsson, and also all the generalizations in the eighties and nineties and 2000, or in various contexts, they do not give any explicit estimates.
00:17:17.640 - 00:18:04.520, Speaker A: So in this paper, we were able to get an explicit estimate, which is very small indeed. We can apply our calculation only to a very, very tiny interval near two. The parameter two is a kind of a special interval in the quadratic family. It's a Chebyshev polynomial smoothly conjugate to the 10th map. And this gives it a lot of special properties. And what we prove here is that 98% of the intervals in this small parameter interval are stochastic. And this of course gives an overall bound of ten to the -5000 for the stochastic interval.
00:18:04.520 - 00:19:00.544, Speaker A: In general, this is of course very small number, although I like to joke that it's smaller numbers. And of course it is not really does not still give any clue about the 0.54 measure of parameters. However, it was the first time that we got an explicit estimate. And I want to mention here that this is not just a matter of going through Jacobson's theorem or one of its generalizations and keeping track of the constants in those papers. You always need to assume that you have a good parameter value. We wanted to set up a procedure that did not require that.
00:19:00.544 - 00:20:27.802, Speaker A: So a procedure, in order to be able to generalize this paper in the future, we set up a procedure in which we obtain a formula that is based on a bunch of computable constants. So you need to be able to take a general parameter interval anywhere in Omega, some small parameter interval, compute a bunch of constants related to that interval, and if they satisfy some formal conditions between them, to plug them into the form formula that would give the lower bound for the measure of stochastic parameters in that interval. And this is what we do. Now, in this particular case, where we take this particular parameter interval near two, we are able to compute these constants analytically, so we can plug them into the formula, and that's how we get the 98%. But in general, they need to be computed numerically. And so I want to talk about which constants, because this is the numerics that I'm going to present, are part of this bigger project. And to motivate these constants, I just want to mention that one of the key characteristics that we use to know that a belongs is stochastic parameters.
00:20:27.802 - 00:21:08.674, Speaker A: And this is down to this classical theorem of Col. Ekman from 1983, that says that if you consider the critical value, so zero is the critical point for every parameter value. So Fa of zero is called the critical value, which is the image of the critical point point. And you look at the orbit of this critical value, and you take the derivative along this orbit, then the derivative along this orbit. There's a mistake here. This should be one over n log of this derivative. And what I mean is that this is going exponentially fast.
00:21:08.674 - 00:21:50.618, Speaker A: Okay, so, in fact, this theorem actually is designed stated even though this is not how I meant to state. It is also true. There is a more recent result of Shen, the collaborator, whose name I cannot remember right now. I'm so sorry. Which shows that if the derivative, it's almost correct. So if this ly min is actually bigger than some constant, there exists some constant such that if this is bigger than some contents, then the same conclusion holds. In particular, if this goes to infinity, then the same conclusion holds.
00:21:50.618 - 00:22:28.566, Speaker A: So there are much more general results than Col. Ekman's result. But this is what we'll use. The fact that you need exponential growth of the derivative to get stochastic behavior. Yes, so I mentioned here that there's some weaker conditions that apply in this case. So the computable constants we need are based on these two properties. So, first of all, I recall that zero is the critical point for every parameter value.
00:22:28.566 - 00:23:40.544, Speaker A: This is one of the advantages of this particular parameterization. If we fix some delta greater than zero and let delta minus delta delta be a neighborhood of the critical point, then we define the following property which we call uniform expansion. Outside delta, this is a property which may or may not be satisfied. And this property says that there exists a positive lambda such that for every k and every x which belongs to the image of this critical neighborhood. In other words, we take x close to the critical value, and then as long as the further images of x up to time k minus one do not belong to delta, then this implies that the derivative is exponentially growing for every parameter in omega. So this is a property that may or may not be satisfied, depending obviously on the parameter omega, depending on the size of delta. If delta is too small, this is unlikely to hold.
00:23:40.544 - 00:24:46.030, Speaker A: Remember that this is a very non trivial property, because delta is very small. So staying outside delta does not guarantee that this overall derivative picks up some very, very small derivatives of the order of delta. But what this is saying is that really, the average amount of time that this orbit is spending outside delta must be spending more time in regions where the derivative is big than in regions where the derivative is small. And when you look at it like that, it's really amazing that this could hold it tall. However, it does hold in very many situations, and even in some cases for delta, very small. In fact, if the parameter a is collectman for a single parameter, this property actually holds for some lambda and for any arbitrarily small delta, as long as you put a constant c here, or in fact, no, if you start in some the critical value. You don't even need the constant c.
00:24:46.030 - 00:25:29.574, Speaker A: So this remarkably holds for any delta. Of course, once you enlarge and you even if it holds for one parameter value, you now take a little parameter neighborhood of that parameter value. This does not generally mean that it will continue to hold for this parameter value. So this is a non trivial condition, non trivial to verify, but it does occur. And the second, for the second condition, we introduce this notation where we let cn of a be the image of the critical point for a particular parameter a. So we are interested in looking at this orbit of the critical point for different parameters. So it's natural to introduce this notation where this is a function of the parameter.
00:25:29.574 - 00:26:09.580, Speaker A: And we also introduce this notation where we let omega n be the set of the images of the critical points for all the different parameter values in a. Notice that this is clearly the image of the critical point depends smoothly on the parameter a. So this itself is an interval here, omega n. So this interval is an interval in phase space. And we're interested in certain properties of this interval. And in particular, particular, we just define the following notion. We say that n is an escape time for omega.
00:26:09.580 - 00:27:14.430, Speaker A: If the images of omega I do not intersect delta up to time n, and at time n, the size of this is greater than or equal square root of delta, square root of delta, which is a way of saying that it's a order of magnitude bigger than the critical neighborhood itself, which is of length two delta. So the notion of escape time has some history. It comes from work, very interesting and pioneering work of Benedict Scarlison on generalization of Jacobson's theorem. I will not explain so much the motivation, but it is crucial. It kind of says that if you think about what omega n is, is the set of critical point, the image of the critical points. The fact that it's big means that at time Big N, not all these points can fall into the critical neighborhood delta. In fact, most of these points necessarily must fall outside delta.
00:27:14.430 - 00:28:24.236, Speaker A: So it's a way of saying, without even knowing precisely where this interaction is, the fact that you cannot have too many points falling inside the critical neighborhood. So the heuristic statement of the big project, that is a joint project of myself with Alegor Makani and Pavel Pilates, and this is in progress. And these computations are going to, to present us some steps, some little pieces of the puzzle that we need for this is that if both conditions hold for some n, and the escape time is for some n sufficiently large, then omega contains many stochastic parameters. So the Formula I mentioned about computable constants. So the actual statement is, is a little bit more technical. There are some other constants involved, but heuristically, really what I've just said, these two conditions are really the fundamental ones. So as you can see, there is, the parameter interval is crucial.
00:28:24.236 - 00:29:16.998, Speaker A: The size of delta, the fact that you have this lambda, and the escape time n with these properties. So these are constants. And the idea is that you should be able to compute all these constants and compute these properties. And then the formula tells us that if this holds, and if n is sufficiently large, then there should be a very large percentage of parameters in Omega which should be stochastic. Just a quick remark. Notice that a key feature of this is that some of these conditions go in opposite directions. So if you take delta very small small, it makes it more unlikely to satisfy the uniform expansion outside delta, because you're allowing this orbit to come very close to the critical point.
00:29:16.998 - 00:30:16.506, Speaker A: However, it makes it easier to get an escape time because it makes it easier to guarantee that you stay outside and that you go to a certain size. So delta small kind of makes one of these definitions easier, but the other condition hardened. They have to be both satisfied simultaneously. So that shows that as you vary delta, you will need to find some optimal range. It's not a question being able to say, let's take sufficiently small or sufficiently large. You need to take some optimal range because you need to balance these two conditions. Similarly, in terms of the size of omega, if omega is small, this is good to get the uniform expansion outside delta, because it needs to be satisfied for less parameter values.
00:30:16.506 - 00:31:33.298, Speaker A: It's also good for the first condition here, where you need to stay out outside delta for the first nitrates. But of course, it's bad for the last condition of the escape times, because the image needs to be big. So again, the size of omega, it's not like we can make sure that we can just take a sufficiently small neighborhood of an interval of a particular parameter a. It needs to be sufficiently small for something to work, but sufficiently large for something else to work. So this is part of what makes this whole computable and numerical approach to this result quite complex. Interestingly, there is some previous work, so joint work of myself in 2008 with Daekubu, Mishai, Kovac and Pilates, and then some refinement with Kolmakani and Pilot, in which we develop rigorous numerical procedures for the first condition, the uniform expansion outside delta, it looks at first sight like it would not be amenable to numerical computations because k can be arbitrary. Indeed, it can be arbitrary.
00:31:33.298 - 00:32:48.794, Speaker A: But that does not mean that we need actually to check for the, for a infinite. We need to check for infinite orbits, because we can construct a partition of a space outside delta and construct a directed graph that gives all possible, basically combinatorics, and take the minimum possible derivatives of all these options and use some directed graphs algorithm to actually show that there are some loops whose average derivatives is big, and so that you can actually numerically verify uniform expansion outside delta. And in those papers, we do that, especially in the last paper in 2016. We go through the parameter interval and calculate the values of lambdas and deltas, which work for many different parameter intervals. So what we want to focus on in this paper is the second condition, condition, which is the escape times for sub intervals of omega. So now, before explaining the techniques that we use to do this numerical computation, let me explain give our results. So, we are dealing with this for simplicity.
00:32:48.794 - 00:33:44.314, Speaker A: We take the parameter 1.42, because Feigenbaum parameter is not completely explicitly known, but it's very close to 1.4. And what we're going to do is define a partition of omega that is based on two numbers, delta and n zero. For our particular results, we take delta equal ten to the minus three and n 00:25 but the computations work exactly in the same way for different values of delta and different n zero. And I will say later what, what we mean by dynamically defined how we define this partition. But once we have this partition, each element of this partition would be one of two kinds. So we define a collection of sub intervals of elements of the partition p, and we call it p.
00:33:44.314 - 00:34:37.458, Speaker A: Are those elements that has a verified escape time for some n bigger than equal and zero. So, simply by verified, I mean simply that we can rigorously compute and this escape time, we can prove that this particular interval has an escape time. So delta, our choice of delta, comes of course into the definition of escape time. And our n zero is just a way of putting a lower bound. So, because, as I said, we want to verify that we have very large escape times, then we just set a more or less arbitrary lower bound, and we just check for escape times that occur after time n zero. And then we define just the p minus is just a complement. So every element for which we cannot verify this for whatever reason.
00:34:37.458 - 00:35:19.534, Speaker A: I will explain later how we decide that. Sometimes because, because the interval is just too small, sometimes because we get into some technical computational issues, sometimes because at some point we just want to stop the calculation. And then all the intervals for which we've not been able to verify an escape time get put into p minus. So p minus is not intrinsically defined, but just as the complement of p. So what I want to explain now is exactly what statistics, what we get for this p, some properties of this element p. So first of all, we have the following result. The partition p that we construct is formed by almost 4 million intervals.
00:35:19.534 - 00:36:17.674, Speaker A: In fact, 3960 9763 the measure we use this notation as the measure of the partition, of course is 0.6 because it's a of the space omega. And then what we are particularly interested is these intervals. Which intervals belong to p? Which intervals belong to p? P. So of these, about 36% in number of these intervals, and most importantly 90% in measure belong to p. So the cardinality of the set of intervals in p is 36% of the cardinality of p. But most importantly, the measure of p is almost 90% of the total measure of the interval.
00:36:17.674 - 00:37:42.544, Speaker A: And this is a very, very promising result. Of course, this does not say, that does not say anything about the stochastic parameters themselves, but indications, as I mentioned above from the analytic part of the results is that if you have, if an interval has an escape time, if both of the conditions hold, if you have the uniform expansion outside delta, and if you have an escape time for some sufficiently large n, then you're very likely to have a large proportion of those parameter intervals as stochastic intervals. So this, at the moment we get almost 90% of these, and therefore of course p minus is just a complement, so it is about less than 10%. So let me make a remark that very interesting. Although we've not done the full computations, we've done some partial computations, and it turns out that choosing bigger lower bound for the escape times does not actually seem to significantly change the measure. This is very stable, and there are some heuristic arguments in favor of this conclusion. And in future we'll do some calculations for bigger and zero to the same calculation and see what we get.
00:37:42.544 - 00:38:44.000, Speaker A: So I kind of informally state a conjecture, as I said before, that most parameters in p belong to omega plus, which would point in the direction of most of the 90%. So notice how these figures match correspond very well to the results I mentioned before from tackle and wilshak about the measure of regular parameters being about 10%. So these numerics also point in that direction. So in fact, let me just before we focus our main results about p, but let me say something about p. Again, this really corresponds very, very close to the results of, of tackle and wheelchair on the measured periodic windows. The number is not such an important criteria. In fact, because of the way we do a construction, these are not connected components.
00:38:44.000 - 00:39:43.000, Speaker A: So if we merge adjacent intervals, the number of connected components is much less. So the number is not such a significant, such a significant factor. But once we take the connected components, what looks like a non trivial observation is that there's a lot of the measure belongs to large connected components. In particular, the three largest connected components take up 30% of the total measure of p. Just three, just the three components formed by 10% of the total integration. The 100 largest connected components take up 90% of the total of the total measure. And I observe also that the ten largest components match up very well with the non periodic windows.
00:39:43.000 - 00:40:54.034, Speaker A: So in the kind of, this kind of squeeze version of the bifurcation diagram, and there are these big, so called macroscopic, in some sense windows regions which belong to omega minus, which are also computed, which have been computed in particular by Tucker and Wilshire, also by other people before. And our connected components, unsurprisingly of course, match up with that. It might be interesting to combine this approach with daggers of tackle and Wilczek from 2009. And maybe we could compare the connected components that we get with our construction, in which we do not prove the existence of attacking periodic orbits. But we kind of define the intervals in p minus by saying that we do not have escaping times. But they may be, it might be interesting to combine these two up approaches. So main focus is actually p.
00:40:54.034 - 00:41:38.308, Speaker A: This is what we are most interested in. This takes up 90% of the measure. And so what I want to talk about are various statistics of these intervals. And this will lead up to my question about what we can really say and what this means. So all of these intervals in p, they have an escape time at time end. So from the point of view of original motivation, they are intervals which look promising from the, in terms of how many stochastic parameters they may be. But what we want to focus here is how different estimates apply to different these intervals.
00:41:38.308 - 00:43:07.428, Speaker A: In particular, what is the size of these particular intervals? And what we get is quite interesting distribution here. So this pie chart shows the proportion of intervals which has size of various different orders of magnitude. So you can see that it is fairly evenly distributed in orders of magnitude. There is a small measure of only 5% of the intervals which are large in some sense bigger than ten to the minus three, and the others are much smaller, about 20% of intervals have sizes between ten to the minus three and ten to the minus 430 percent, between ten to the minus four and ten to the minus five and so on. And there's even a 5% which are really small, but still not a negligible amount in terms of measure that have the width of these intervals is less than ten to the minus seven. So this just arises out of the chopping procedure. And we, to be honest, don't really know what this is, just data that we have.
00:43:07.428 - 00:43:59.876, Speaker A: We don't really know what conclusions or what this means, particularly whether it is more or less so surprising than if they were all kind of small. However, we have other statistics. So another interesting. Yes. So here let me say that we don't really have the same situation as in the regular intervals where you can merge adjacent ones, because here they may be adjacent intervals, but generally they will have different escape time. So part of the chopping procedure, which I'll explain later, means that at some point we stop iterating one, but we chop it from another one, adjacent one, that we continue iterating, it has a different escape time. So they really should be thought of as separate intervals and it wouldn't make sense to merge them.
00:43:59.876 - 00:44:51.144, Speaker A: So in some sense here, these, these sizes do have a bit more of an intrinsic meaning for the intervals. A second interesting fact is what about the escape times? So we know that all the escape times are bigger than equal to 25 because that's how we chose an n zero. But some intervals may have significantly bigger escape times. And here we have the distribution, and as you can see, half the intervals escape exactly at time 25. This is, we're talking about million, 1.2 million intervals. So it's a lot of intervals that we have iterated and they all escape at the first available opportunity.
00:44:51.144 - 00:45:52.604, Speaker A: This also, there is mathematical heuristic reasons that support this. Basically, when you do basically what happens, which is very remarkable and very non trivial, is that at any one time, most intervals in the construction are escaping, kind of because we chop things and stay outside. And so most intervals which are still, which are still which eventually becoming people also which have not already been placed in p minus, they are kind of in a state of perpetual escape, which means that as soon as we put down the threshold, so we say, okay, at n 00:25 let's see. And then suddenly half the interval are immediately escaping at this time. And then you get something that you can kind of expect, which is those that are not. Many of them escape immediately afterwards. And then you have a slightly decreasing proportion with larger and larger scales.
00:45:52.604 - 00:47:28.368, Speaker A: Interesting observation is that again, some informal computations seem to indicate that we would get a very similar distribution if we set n zero bigger. For example, if we set n zero equals 30 or 40, we expect to get something very similar where about half the intervals are still escaping as soon as possible. And then you get this kind of decreasing proportion of parameters escaping at large intervals. Given that, it is also interesting to know what the size is at the escape times, because if you remember, the condition of the escape times is that omega began. So the size of the image of omega time began must be bigger than square root of delta. The question is, what is the actual size when they escape? Is it bigger just a little bit? Is it two times is a different order of magnitude? And it turns out, and to be honest, this I find really quite remarkable and quite surprising, is that the distribution of sizes that basically the sizes are almost always really macroscopic compared to delta or square root of delta, which is the minimum size that allowed to be the smallest, 11% are less than 0.5, which is already something much bigger than square root of delta.
00:47:28.368 - 00:47:59.844, Speaker A: So I made a comment here. Notice that they called it delta is ten to the minus three. And therefore, in most cases, as we see here, omega n is much bigger than the square root of delta, which is 0.03. We don't even, there's such few intervals which are of that order of magnitude that we don't even include them in the bar chart. We have the bar chart in terms of these kind of macroscopic sizes, whether it's between 0.5 and one, between one and 1.51.51. .52.
00:47:59.844 - 00:49:04.044, Speaker A: There's even a bunch of intervals which are almost of the order when they escape almost of the order of the whole interval themselves. And it is initially kind of intuitive, not counter intuitive, that this happens in the published version of the paper. We have a discussion about this, and we also have a table in which we follow one particular interval in the procedure and show how it can escape with such a large image. It is actually very instructive to see that. And I think even experts in one dimensional dynamics would not immediately feel that. It is obvious that for so many parameter intervals, variables for 9% of the whole measure parameters, they escape at time bigger than five with such a big escaping component. So these are the three.
00:49:04.044 - 00:50:06.942, Speaker A: We also have in the published paper some other quantities that we compute. But I will not go into the technicalities. I think these are the interesting, the most interesting ones here. The size of all the value of n, the size of omega n. So when we, as I said before, there's no we, what we are missing here is some way to correlate these three values. For example, we do not know if these intervals, for example, which are particularly largely large, whether they all have similar values of big n, what the distribution of n is restricted, you know, conditional to being the size, or what the size of omega is conditional to these. So there's a lot of interesting questions that can be made about the correlations between these.
00:50:06.942 - 00:51:09.664, Speaker A: So I will discuss this as some of the open questions. And there seem to be at least two possible directions. One of them I have already mentioned, which is the original motivation of this, which is just to calculate the actual measure of stochastic parameters. This involves verifying the uniform expansivity, which is the first condition, repeating the calculation to possibly get larger values of n, zero. And the final step would be then to insert this into the formula to make sure the formula works with these values, and to get the measure of the stochastic parameters. So, this direction of research is the project that I mentioned above, which is something that is really has been going on for a long time, and is our main motivation for producing this. The second direction, which might be more interesting in the context of this symposium, is really related to the statistics, which is not something we anticipated.
00:51:09.664 - 00:51:47.204, Speaker A: And, I don't know, it might be very exciting. And it is about EStABlishinG POSsible correlation between these various quantities. As I mentioned, the size of omega, the size of omega n. We do not know how to do that. We tied a little bit using some classical statistical methods, but none of us are experts in that. So in the end, we gave up. We could ask whether there's a correlation between the size of omega, size of omega n, the value of n, or whether there is no correlation between these and other quantities which we compute in the paper.
00:51:47.204 - 00:52:50.430, Speaker A: And so, really, a question, a big question for me to other experts, is whether there is some way we have these 4 million intervals. We have data associated to each of these 4 million intervals. So from the little that I know about big data and machine learning are precisely trying to draw conclusions from this big data. Yeah, I think it would be extremely interesting to be able to see if there's anything we can say about this particular data that we have. So, these are the open questions, two directions, one more classical and one more new. That kind of comes from this work. Okay, so let me now move to the final part of the talk, which is talking about the actual computations that we do.
00:52:50.430 - 00:53:40.612, Speaker A: So for those people who are not so much interested in the techniques, the numerical techniques, you can stop here. I've discussed the motivation and the results that we get. Okay. There is, however, some very interesting procedures that need to be created for doing the rigorous numerics, and there are more or less two main steps in the computation. The first one has to do with actually iterating parameter intervals. So our goal is the effective computation of omega n, because that to check escaping component, you need to check that this omega n does not intersect the critical region delta. So you need to compute omega to sufficiently good approximation.
00:53:40.612 - 00:54:38.174, Speaker A: And then you need to compute its size at some time begins. So both of these steps in escape type boil down to just computing this interval omega n. Now, computing the images of a single parameter value is a relatively standard procedure. Even the rigorous computation, you cannot of course compute it with infinite precision. But using interval arithmetic, you can get a rigorous enclosure of the image of this critical point where the upper and lower bounds are given by representable numbers. So using arbitrary precision for the computations, you can actually get this enclosure to have this computation to any desired accuracy if you want. However, things get a little bit trickier when we are trying to do it for a range of parameter values.
00:54:38.174 - 00:55:32.634, Speaker A: What we are trying to do here is actually iterate each point fa of zero by its own corresponding parameter value. So how do we do that? Well, we think of cn as a map from omega to omega n. It takes the parameter value and sends it to the corresponding cn of a. Now if this map is monotone, so omega is an interval in parameter sPace. Omega N is an interval in the phase space of the dynamics. It is the union of the image of the critical points. If this map is monotone on these two intervals, then that means that all we need to do is calculate the images of the endpoints.
00:55:32.634 - 00:56:08.364, Speaker A: And if we want the image of the endpoints, then we can just use. It is just as we said, finding enclosures for the single. So we take the endpoints of omega, parameter a and parameter b. We calculate the images, and if it's monotone, we know that omega n, the image of omega is contained between the endpoints. So that gives us enclosures, that gives us upper and lower bounds. If we compute dual arbitrary precision enclosure of the endpoints, we obtain both inner and outer enclosures for the interval omega. Nice.
00:56:08.364 - 00:57:06.098, Speaker A: The problem is that this is not necessarily monotone. There's no reason, in fact, that this should be monotone in general, which means that it could the image of omega that the endpoint there might be a fold in this image Cn from omega to omega n. And the images of the endpoints may be very close, but that does not mean that omega N is very small, for example. And so the way we need to tackle that is we need to check at every stage the monotonicity of Cn. So this is how we approach the problem is to check conditions for monotonicity of Cn. So the map Cn is actually differentiable and because it depends smoothly on the parameter. And so the monotonicity follows if the derivative with respect to the parameter is different from zero for every parameter.
00:57:06.098 - 00:57:53.496, Speaker A: So this is what we need to check. And how do we check that numerically? Well, notice first of all that for n equals zero, we have c zero of a is just the critical value, right? If you remember the definition of cn of a four, n equals zero. This is just the critical value. And by the definition of the map which is fa of x a minus x squared, the image of zero and x is equal to zero. This is just actually a. So the map c zero that maps the parameter interval to the interval in phase space numerically has exactly the same value. This is sort of critical value of the map size fa numerically has the same value of a.
00:57:53.496 - 00:58:27.884, Speaker A: But of course, these are two a's that belong to different spaces. This is an a in parameter space. And this is the same numerical value of a point, which however is in the interval where the dynamics happens. But because of this, we have that c prime zero of a is equal to one, which is therefore different from zero. So of course, here you get an explicit map c zero. And so for n equals zero, we have that this condition is satisfied. And then we have also an inductive relation.
00:58:27.884 - 00:59:55.628, Speaker A: Because of the form of the map, it turns out that we can calculate the derivative with respect to the parameter is actually equal to minus two times the position of the point c, and minus minus one of a times the derivative with respect to a of the point n minus one of a plus one. So, can we use this to verify monotonicity? Well, notice that what we need to make sure that c prime n of a is different from zero for the whole interval omega is we need to know these values for the whole interval omega. And all this means is that we need to have a legal enclosure for all the possible values of the derivative at time n minus one, and the legal enclosure for cn minus one of a, which just means that we have calculated omega n minus one. So you can see that this is a simple inductive argument in which we have for n equals zero. We have both for n equals zero. We have that we know exactly the numerical value of omega zero because it's the collection of all these c zero. We know the numerical value of c prime zero of a for all a in omega.
00:59:55.628 - 01:00:42.780, Speaker A: And then we can therefore compute some enclosures for the derivative, for the derivative of c one and use that to then calculate using the monotonicity of c one. We can use that to show that c one is monotone. Because here we get that this is different from zero for every a, using the fact that a is between 1.4 and two, c prime of a is equal to one. So we can get that this is always different from zero. And therefore, using the monotonicity, we can calculate c one of a and therefore calculating c one of a. We can get a bound for c one and so on.
01:00:42.780 - 01:01:47.122, Speaker A: And we can find, we can use an inductive procedure to verify at each step the monotonicity at the same time as we calculate omega. Okay, so this gives some idea of how we can, how, what we need to do to be able to get rigorous enclosures. We need this formula for the derivative with respect to the parameter. We need to check the map is monotone at every stage. So yes, so we need to impose this condition on the parameter intervals. We iterate, right? So this becomes a kind of condition on the call on the calculation, if we want to keep iterating an interval of parameter, this condition that c that zero is not one of the elements of the derivative, we can define omega prime n to be the enclosure, the set of all possible derivatives with respect to the parameter. If this is not satisfied and it fails.
01:01:47.122 - 01:02:20.832, Speaker A: And if it fails, we just take for example omega and put it in p minus and forget about it. And this is one of the ways in which we kind of abandon an interval omega and give up on it having a chance of belonging to p. So this is one of the main steps of the calculation. It's iterate parameter intervals. The next important step is chopping them. That's how we construct dynamically. So remember that I said that the partition p is dynamically defined.
01:02:20.832 - 01:03:12.788, Speaker A: In order to maximize the measure parameters. In some initial calculations that we did, we predefined a partition element partition. So we just took a partition of PDF into maybe 6000 or even 60,000 or even 600,000 partition elements of equal size. And then we did these calculations and checked whether they would escape. And it turns out that there is a huge difference in the effectiveness of these two. So you get a very, very large failure rate if you just take a fixed partition, whereas having dynamically defined, having them dynamically defined, where you. We create a chopping procedure so that it intervals intersect delta.
01:03:12.788 - 01:04:05.160, Speaker A: And therefore, before, for example, time n zero, therefore, in principle, would fail the escape time condition. At this point, a lot of this interval might fall outside delta, so it's worth chopping. And only the little piece that falls inside delta, we give up on and put in p minus, and the rest, we continue iterating. And this turns out to be one of the crucial element which allows us to get a very large measure proportion of measure belonging to p. So, to do this, we need to have a chopping procedure. When omega hits delta, some part may fall outside the delta, as I said. So we want to iterate it further, and so we want to chop the original interval omega, in such a way as to separate the parts of omega that fall inside delta and those that fall outside delta.
01:04:05.160 - 01:04:38.130, Speaker A: Which means that we need to find the points that fall on the boundary points of delta. And this is actually relatively standard. It needs to be incorporated into the algorithm. But the method itself, we just use a bisection method. So once you fall inside delta, we bisect, and we look at the two sub intervals that we get, and we look at the bisection point, actually. So we do not. We do not.
01:04:38.130 - 01:05:42.504, Speaker A: A key point is that we do not reiterate the intervals, which is a computational expensive, but we just iterate the point that we use to bisect, and we look at whether this point falls inside delta or outside delta. And based on this, we carry out more bisections, and we see if it falls inside delta, outside delta. And in this way, we converge very quickly to a point which allows us to optimize the amount of measure which we throw away, because it falls, because it may fall inside delta. So this. So, the construction of the partition p uses these two steps, basically, combination of iteration and chopping. What is not completely initially obvious is that several decisions need to be made, including when to stop iterating. So the way we set this up, formally, is the following.
01:05:42.504 - 01:06:34.374, Speaker A: We have basically, at any moment, we have a partition that is formed by the union of three collections of intervals, which together form a partition of the whole interval omega. So p consists of the intervals we're looking for, which are those which have an escape time at some time n zero. So once an interval is in p, we do not iterate it any further. It's fixed. It's fixed together with all its properties that we know, including the escape time and its size and so on. P consists of the intervals which at some point we have decided I will go through some of the reasons we've decided are unlikely to to have an escape. So we decide to stop iterating them.
01:06:34.374 - 01:07:25.808, Speaker A: And again, these do not get iterated any further. And PQ is the queue, which is a set of intervals that still need to be iterated. These intervals, once we iterate them and chop them, we take various actions depending on certain features, which I will briefly sketch, and we decide to perhaps chop them into sub intervals. Some of these go become p if they have an escape time. Some of them get abandoned, and we put in p minus and some of them back in the queue for further iteration. So we start with just the trivial intervals. So this is a completely standard automat.
01:07:25.808 - 01:08:40.138, Speaker A: The whole partition is completely dynamically defined in the sense that we start just with the interval omega, which is in the queue, which means we iterate it. Then we iterate and chop and move sub intervals into p plus or p minus, as I will explain. And then at every stage, if, as I would say, in fact, in fact in our particular case, we end up with a situation where the queue is empty. So we complete the procedure, but there is always the option if the computation time is very large, or for whatever reason, you need to stop to just take all the remaining intervals in the queue and put them in p minus, because the only definition of p minus is that we cannot guarantee an escape. So in the end, we empty the queue and we end up with the partition p, which is p p union p. And so, as I said, we need to make, in particular, two particular choices when to move intervals into P or P. And then we also may need to make a choice at which intervals we prioritize for iteration, because intervals are iterate one at a time.
01:08:40.138 - 01:09:32.399, Speaker A: And so when you now we put in hundreds of thousands of intervals into the queue, and we need to pick one to iterate it. And so we need to make a kind of choice about how to do that. So I will make some brief remarks about that. Let me explain a little bit the principle of how we process the queue. So, if we have an interval omega and PQ, the first thing we do is check how big it is, and we impose a threshold, which we call w min. In our particular case, we choose this as ten to the minus ten. And if this interval is too small, then we just forget it, because we decide that it's not worth the computational time or the computational cost.
01:09:32.399 - 01:10:22.294, Speaker A: And anyway, we can throw away lots of these intervals and it doesn't really add up too much in terms of measure. So we let omega and p stop iterating. So if this interval is big, sufficiently big, then we need to take some actions and we consider various possible scenarios. In particular, there are three classes of situations. The first one is if we iterate and we actually establish some n such that, which is the first time we look at the first time that this interval intersects the critical neighborhood delta. So it stays outside up to time n and then intersects delta. So this is potentially an escape time, because this first part is the condition for the escape time.
01:10:22.294 - 01:11:11.998, Speaker A: So if n is bigger than n zero and the escape time condition hold, which means that omega n is bigger than square root of delta, then we're done. We have an interval that satisfies the properties we need, in particular in terms of n zero, and we have an escape. So we just let this omega in p plus, and from then on this interval is in p plus. Whatever, it's fixed, we no longer iterate it, and this n becomes the big n, the escape time. Alternatively, it may be either that the escape time conditions do not hold or n is not bigger than n zero. In this case, we have still this intersection with delta. And so we chop, this is where the chopping procedure that I just described comes in.
01:11:11.998 - 01:11:53.510, Speaker A: And we, we chop it into up to three intervals, because we might have one part of the interval. So these left and right parts of the intervals at time n, they do not intersect delta. And Omega Delta is an interval that intersects delta. We do it in a rigorous way to guarantee this condition. Here, of course, when we say that this intersects delta, we know that the outer enclosure intersects delta. Strictly speaking, it may be that the interval itself does not intersect delta, but we treat it as though it did, because it may. And here also, there are some.
01:11:53.510 - 01:12:29.980, Speaker A: We need to be careful, make rigorous estimate. The only thing we need to make sure we guarantee is that this condition is satisfied. So we use the chopping procedure, we use the outer and enclosure of these intervals to make guarantee that this is not intersect delta. And then we take this part, which possibly does intersect delta, and we let into p minus. So we abandon it. And then we look at these two sides, these two intervals on either side. Of course, if they're too small, according to the criteria threshold we had before, then again we abandon them and we put them p minus.
01:12:29.980 - 01:13:06.208, Speaker A: If they're not too small, then we put them back in PQ. We know that up to this particular time n, they do not intersect delta. So we still have a hope to iterate them and have an escape at a later time. So we put them back in the queue, and at some point later, we will iterate them again. Yes. Notice, of course, that these may be. Not all of these may be non empty, right? So it may be that omega L and omega L are both empty.
01:13:06.208 - 01:14:01.084, Speaker A: It may be that the whole interval falls into delta, and then we just have one interval that falls into p minus. So they may be empty. Okay? But this is one possibility, and we explain how to deal with that. The other possibility is that we may have some technical or computational issues, in particular, of two kinds. The obvious one is that, if you remember, I said that one of the conditions was that the c prime, sorry, this should be c prime of n of a is equal to zero for some a and omega. This may be because it actually is equal to zero, or it may be because of omega overestimates to do with interval arithmetic and the calculations. But in any case, if the c prime n is equal to zero, then we cannot iterate anymore, because that is a condition for us to continue.
01:14:01.084 - 01:14:52.404, Speaker A: But still, this seems to happen enough for us not to want to throw away the whole interval again. So what we do in this case is we bisect omega, and we just put the two subtract intervals back in the queue. And it's possible that later on, when we iterate them, they're smaller intervals. And we may avoid this possibility that the derivative is equal to zero, or it may happen for one of these intervals instead of the other, so that we have saved some of the measure. Another rare and less significant computational issue, which, however, can happen, is basically that the enclosures that we get from our resolutions are larger than the actual distance between the endpoints. Which means that we have no idea how big omega actually is. We don't really have any control.
01:14:52.404 - 01:15:39.804, Speaker A: And at this point, there's nothing really you can do except go back to scratch and increase the resolution of our computations. But this can only happen if only omega is very small, because we already have very high precision, so it's not worth doing. So in this case, we basically have a similar situation to whether omega is too small. It's below the threshold, and we just abandon these intervals and leave them in p. Finally, the last situation which can occur, and which does occur, is that we keep iterating omega and it never intersects delta and it never hits delta. So, in principle, this could go on an infinitum. We don't want that.
01:15:39.804 - 01:16:26.500, Speaker A: This does happen. For example, if we are inside one of those macroscopic regular periodic windows. It may happen that these critical points are all converging to some attracting periodic orbit which, whose orbit does not intersect delta if delta is too small. So we just need to put some threshold which allows us to stop the iteration. In our case, we choose n max equals 200, so that if omega is iterated 200 times without hitting delta, and without any of the other conditions happening, then we just stop. And of course, in general, they will not have an escape time. At this point, we could check whether it has an escape time, but generally it does not.
01:16:26.500 - 01:17:11.710, Speaker A: And at this point, we therefore generally put omega in p in this kind of situation. So these are the three cases. This covers all possible cases. The introduction of these kind of, what Pavel calls safeguards, guarantees that this process was actually finished, because every interval, if it's too small, we put it in p minus. If we delay too many times, we put it in p minus. So this guarantees that there is a finite number of calculations that are done on each interval, and it means that we eventually conclude the process. Yeah.
01:17:11.710 - 01:18:15.564, Speaker A: So the final comment about how to choose elements in the queue, there's several criteria. What we do in our case, it doesn't really matter, because we, in our calculation, we actually, the queue does eventually become empty. So we manage to go through these various steps for all intervals. But our criteria is that at each point we choose one of the intervals that has been iterated least number of times. This guarantees that there is not a huge gap between intervals which have iterated, been iterated more times or less times. So the ones that have been iterated less and put in the queue are prioritized so that we iterate those and see what happens with those first. And it also means that if some point you want to stop the calculation, put all the intervals in the queue in p, then you are kind of not too unbalanced in terms of the number of iterations of this interval.
01:18:15.564 - 01:18:55.200, Speaker A: So these are choices, there are several choices that can be made which are somewhat arbitrary, but as long as they work, they work. And finally, let me conclude this talk mentioning that we have a web interface for these calculations. So you can actually go the. Yes, so the web interface is based on the fact that there is no. We have been able. There is actually a number of algorithms which are involved in these various steps. The chopping, the iterating, the putting in the queue, the choosing in the queue.
01:18:55.200 - 01:19:55.878, Speaker A: And I, I really want to give credit to Pavel Pilarczyk, our co author, who has been mainly responsible for the technical and computational part of this paper. He has done a really extraordinary job in being able to automate how all these algorithms talk to each other. So there is no manual input except at the very beginning where we need to choose various variables that have role in the decisions that need to be taken in various steps. And because of this, there's actually a very nice web interface we can, you can see on Pavel's webpage in which everybody can actually do this calculations and explore modify relevant parameters. Let me show a little bit what this web interface looks like. So there is a short version and a long version depending on how many parameters you want. So this is the short version.
01:19:55.878 - 01:20:45.014, Speaker A: So as you can see, you can specify the left and right endpoints of the parameter interval. So this is where we choose the parameter intervals that you want to iterate. This is where you choose delta. This is 0.001, which is ten to the minus three, which is what we use in our calculations. This is just a safeguard to make sure that this does not run over time and does not get into some infinite, you know, does not take up too much time of the calculations of the server. This is the number n zero, which is the number of the minimum number of iterates, which we consider for an escape time.
01:20:45.014 - 01:21:18.654, Speaker A: And this is, as I said, a square root of delta, which is 0.03, square root of 0.001 which is the minimum size of an escape. So of course these are the, at least except for 14. So these five are really the minimum number of variables which we need to specify. These are the defaults that we use in our calculation. And when you run this, if you press learn on the web interface, you get something that looks like this.
01:21:18.654 - 01:22:01.256, Speaker A: So you can see that the conclusions of this is that in this parameter interval between 1.91 and 1.92 end up with 99.5% of parameters are stochastic intervals. The number of these intervals is 138. So the total measure is this, the lower bound, which is small because we take a very small interval. And the number of remaining intervals in p is 1663, which is much larger than 38, but takes up much much less measure.
01:22:01.256 - 01:22:48.060, Speaker A: And this took 5 seconds. So it gives just the fundamental information. We could also get the actual list of the stochastic intervals, for example. But for the moment that the quantity we are most interested in knowing is the proportion of stochastic intervals. Before we finish, let me just also show the extended input. Here you can choose many more variables, let's say this for experts. So you can choose the binary precision of the computation, you can choose the number of bisection steps.
01:22:48.060 - 01:23:40.414, Speaker A: When you. You chop and you want to find the lesions that fall inside delta, you can choose a whole bunch of other things. The number of intervals to process and various things. So the number, the learning time and. Yes, and the number of decimal digits in the output. So some of them just have to do with the display, but you can choose pretty much all the parameters that you want to choose, and you can actually get this. So we hope that this can be useful tool for other people that might be interesting in exploring very specific and very small regions of palamer space.
01:23:40.414 - 01:24:16.612, Speaker A: And I will say very little about the actual numerics and the algorithms. I refer to the original paper because this gets very technical. There are several interesting points about it, though. So the paper is published in chaos. You can find it at this link here, or. Or you can just search for it on the archive from these rigorous numerics for critical orbits in the quadratic family. And so with this, I conclude my talk.
01:24:16.612 - 01:24:21.604, Speaker A: And I want like to thank you all very much for listening. Thank you.
