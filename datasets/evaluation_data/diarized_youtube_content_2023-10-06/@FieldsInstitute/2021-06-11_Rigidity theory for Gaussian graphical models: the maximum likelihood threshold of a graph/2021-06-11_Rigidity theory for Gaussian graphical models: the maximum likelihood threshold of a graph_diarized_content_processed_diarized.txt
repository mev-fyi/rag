00:00:00.160 - 00:00:44.214, Speaker A: So I'm going to talk about some work that I guess is currently ongoing that started at this thematic semester program at the Fields Institute. And this is joint work with Sean Dewar, Shlomo Gortler, Tony Nixon, Mira Sithrom and Louis Theran. So just give you a brief overview. Like the very broad motivating problem here is to fit a model to a relatively small data set. And so in particular this means fewer data points or observations than random variables. And specifically this is going to be about gaussian graphical models. And these are just families of multivariate normal distributions that satisfy independence constraints that are given by a graph.
00:00:44.214 - 00:01:36.122, Speaker A: And sort of the broad goal here is to use the combinatorics of the graph of a particular graphical model to determine how few observations are needed to be able to actually fit the graphical model. So you want to figure out how small of a dataset you can get away with. And I guess the broad take home message of this talk is going to be that rigidity theory, which is I guess one of the themes of this whole semester program offers many tools for this. And I should say behind the scenes here, and I'll mention this, but not going to details, there's sort of a connection with low rank matrix completion. I'll briefly mention that. And so yeah, that's sort of the connection with like the theme of the conference specifically. So, okay, to start from the beginning, so, normal distribution definitions.
00:01:36.122 - 00:03:03.144, Speaker A: So if you let mu be some vector in r to the v and sigma be a v by v positive definite matrix, then the multivariate normal distribution with mean mu and covariance sigma is the probability distribution function with this density function. And given a graph with vertex at v and edge set e, the gaussian graphical model m, sub g consists of all multivariate normal distributions and new sigma with set of random variables v, such that the inverse covariance matrix has a zero entry corresponding to the non edges of your graph g. So for example, if this is your graph, the random variables are indexed by one, two, three and four. The corresponding gaussian graphical model just consists of all multivariate normal distributions whose inverse covariance matrices have this sparsity pattern. So there's no one three edge, there's no one three entry of the inverse covariance matrix. And then the more like the interpretation of this constraint, this just says that, um, an edge not being present in the graph just means that the corresponding random variables are conditionally independent given all of the other random variables. So in this example, one and three are conditionally independent, given two and four.
00:03:03.144 - 00:04:28.284, Speaker A: And these arise in a number of different places. The sort of application that I have in the back of my head through all this is genomics, because that, that's the situation where you're in this high dimensional setting where you wind up actually having more, or you could have more random variables and observations. Okay, so let's, yeah, the specific problem want to think about here is maximum likelihood estimation. And the idea is, suppose we're given a graph and data points x one through xn that are supposedly independently and identically distributed from some distribution in this graphical model. So, one way to fit the model to this data is to, you know, take the maximum likelihood estimate, and the maximum likelihood estimate is the solution to this optimization problem, if it exists. So we are optimizing over the mean and covariance matrix of a normal distribution in this model, and the objective function is just the product of the density functions evaluated each of the observations. So we have this objective function and we're optimizing it over all mean and covariance matrices, where the covariance matrix satisfies this condition to be in the graphical model.
00:04:28.284 - 00:05:43.130, Speaker A: And this can be found via convex optimization. In fact, here's the alternate formulation of this that's more obviously a convex optimization thing. So in particular, if you know, mu, hat and s are the sample mean and covariance matrices of your data. So in particular, this is mu hat, and this is your sample covariance matrix, then the maximum likelihood estimate in this graphical model for your data exists if and only if this convex optimization problem can be solved. And all that has happened here from the previous slide is we've basically just taken the logarithm of the objective function on the previous slide and done a little bit of manipulation, renamed some variables, and basically the matrix satisfying, or the optimum of this convex program is the inverse covariance matrix of the maximum likelihood estimate. And the mean of it is just going to be the sample mean. But the point of writing this is that if you look at the dual certificate for feasibility, it gives you a nice condition on whether or not your optimum exists.
00:05:43.130 - 00:06:24.208, Speaker A: So in particular, this is the result of Dempster in 1972 that says that the maximum likelihood estimate exists if and only if there's some positive definite matrix a that agrees with your sample covariance matrix on all the diagonal entries and on all the entries corresponding to edges of g. You have your data set, you have your graphical model. You want to know if the maximum likelihood estimate in the model exists given your data set. And here is a very nice certificate. Or not a certificate. It's a condition for existence, the anomaly. It just says, take your sample covariance matrix.
00:06:24.208 - 00:07:31.504, Speaker A: Can you wiggle the entries corresponding to the non edges of your graph to get something positive definite? So now, yeah, as I said, this the reason, if you like convex optimization, the reason that this is a feasibility certificate is because this is just a feasible point in the dual convex optimization program. In Dempster's paper, this is actually just proven using just straight calculus. One thing to note, so, in general, the rank of your sample covariance is almost surely going to be your number of data points. If you have at least as many data points as you have random variables, this will be feasible. The maximum likelihood estimate will exist because just take a to be equal s s is going to be positive definite. So the covariant, the sample covariance itself is your feasibility certificate. Things get a little more interesting when the number of data points is less than the number of random variables you have.
00:07:31.504 - 00:08:41.754, Speaker A: So in particular, the maximum likelihood might estimate might exist. It might not and will define the maximum likelihood threshold of a graph to be the minimum n such that the maximum likelihood estimate exists almost surely given end data points. So, yeah, I should say, if there are any questions, please feel free to interrupt, and maybe I'll just pause here for a few seconds to see if anyone has a question. All right? Yeah, not all this. Go on. So just using Dempster's result here, we can rephrase now this definition of the maximum Leslie threshold in a purely, um, you know, linear algebra and combinatorics kind of way. So it's equivalent to that previous definition to say that the maximum likelihood threshold of g is the minimum r, such that for almost every positive semi definite matrix of rank r, there exists a positive definite matrix a that agrees with that positive 70 definite matrix of rank r on all the diagonal entries and on all entries corresponding to edges of g.
00:08:41.754 - 00:09:41.034, Speaker A: So immediately from this definition, we can see that the maximum likelihood threshold of a complete graph kn is n. And then we can use that to say, well, if g has a k clique, then the maximum likelihood threshold is at least k. And this is maybe easier to see in an example. So here's a graph. If I wanted to, if I, if I, if I wanted to investigate whether or not the maximum likelihood threshold of this graph is one, what I would need to do is I would need to take a, I would need to show that for almost every positive semi definite matrix of rank one, I can adjust only the entries corresponding to the non edges, namely these red entries. To turn this into something full rank and still positive semidefinite, a positive definite matrix. But if this is rank one, then in particular this two by two principal minor is going to be zero.
00:09:41.034 - 00:10:34.640, Speaker A: And no matter what I do to these two off diagonal entries, this principal minor will always be zero. So no matter what, this full matrix is going to have a two by two minor principal minor, that's zero. So this cannot be a full rank positive definite matrix. And the fact that I have this fully specified two by two submatrix here is because I have the complete graph on two vertices. I have a complete graph on two vertices. So this generalizes this argument, and it tells you that if you have a clique of size kick, your maximum likelihood threshold is at least k. So this was actually noticed in 1993 by Boolean, and he proved something more.
00:10:34.640 - 00:11:27.114, Speaker A: He also showed that not only is the clique number of a graph a lower bound for the maximum likelihood threshold, the tree width plus one is an upper bound. And so in particular this would show, for example, that for chordal graphs, the maximum likelihood threshold is the click number. Both of these bounds are far from tight, and they're also both NP hard to compute. So this raises the question, can we get some better bounds on the maximum likelihood threshold of a graph? And we can. And rigidity theory offers some really nice ways of thinking about this. So before I can tell you some better bounds, I need to just introduce the basics of rigidity theory. There will be more rigidity theory definitions later.
00:11:27.114 - 00:12:00.706, Speaker A: I'll just introduce stuff as I go. But anyway, the fundamental main definition of rigidity theory is that of a bar and joint framework in d dimensions. And this just consists of a graph and a map that sends the vertices into RD. So just think about this as like you're physically building a graph in d dimensional space, and we'll say that such a framework is independent. If the edge lengths can be independently perturbed. This is maybe easier to see. In an example.
00:12:00.706 - 00:13:04.622, Speaker A: Look at this graph in the, in the center, or this framework rather. So this is a physical construction of this force cycle with a chord in two dimensional space. I claim that the framework is independent because I can lengthen or shorten any of the edges while keeping the lengths of all the other edges the same. So for example, if I wanted to lengthen, say, the diagonal, I could just sort of shear this square into a rhombus, making the diagonal longer but keeping all the other edge lengths the same. If I wanted to say lengthen this edge, I could think about sort of like rotating this edge here around this vertex, keeping the length of everything else the same, but sort of stretching this as I do that. And you can do that for any edge. So this framework is independent, but it turns out that, well, you can also talk about graphs being independent because of the following theorem of Asimov and Roth.
00:13:04.622 - 00:14:08.204, Speaker A: It just says that. So given a graph g, if your framework, if your way of putting the vertices into d dimensional space is sufficiently generic, then whether the framework GP is independent doesn't actually depend on G. So in other words, if I give you a graph and I give you some dimension of space, if you were to randomly plop down the vertices in that dimension of space, then either with probability one, that framework is going to be independent, or that framework is not going to be independent. So you can really talk about independence of a graph in d dimensional space. And with that in mind, we'll say that a graph G is generically independent in RD if the framework GP is independent for all generic choices of vertex placements. Okay, so with this in mind, let's make the following definition. The generic completion rank of g, denoted gcr g, is the minimum d, such that g is generically independent in r to the d minus one.
00:14:08.204 - 00:15:13.424, Speaker A: Now the reason it's called the generic completion rank, and here's sort of the connection to the theme of the rest of the workshop. The generic completion rank is also the minimum k, such that every generic partial symmetric matrix whose missing entries correspond to the non edges of g can be completed to rank k. This is really like a matrix completion parameter of a particular pattern of knowns and unknowns. But the reason that one would care about this from a maximum likelihood threshold perspective is because this gives you an upper bound of the maximum likelihood threshold. So this was shown by Caroline Uller in 2012 and Elizabeth Gross. And I should say, yeah, Caroline Uller showed this in this like matrix completion perspective, and Elizabeth Groth and Seth Sullivan in 2018 showed actually that you have this rigidity theoretic interpretation. And this upper bound is very, it's tighter than a boolean upper bound of tree width plus one, but it's also nice because it can be computed in randomized polynomial time.
00:15:13.424 - 00:16:03.966, Speaker A: And it'll be great if that bound were sharp, but it's not. And I guess, first I should say the public, this pre print came out long before 2018. So for, I think in like 2016 or something. So for a while, you know, it was, you know, people weren't sure if maximum likelihood threshold was actually just equal to the generic completion rank people suspected? No. But in 2019, black women and Zinn proved that the maximum likelihood thresholds of k 55 is four, but the generic completion rank is five. And not only that, the maximum likelihood threshold can be arbitrarily smaller than the generic completion rank. In particular, the maximum likelihood threshold of knn grows linearly with n, whereas the generic completion rank grows quadratically.
00:16:03.966 - 00:16:53.284, Speaker A: So these two parameters can be pretty far apart. Yeah, and so for the rest of this talk, I'm going to tell you about some new stuff, starting with a complete, like, sort of rigidity theoretic way to talk about maximum likelihood threshold. Again, in order to do that, I'm going to need to introduce some more rigidity theory definitions. So let GP and let GQ be frameworks in two different dimensional spaces, RD and re, and we're going to consider this equality. So these are just, you know, pairs of points, or distances between pairs of points, you know, being equal. If this holds for all edges of g, then we will say that these frameworks are equivalent. If it holds, moreover, for all pairs of vertices, not just those connected by an edge, then we'll say those frameworks are congruent.
00:16:53.284 - 00:17:36.504, Speaker A: Right? So equivalence implies congruence. The converse is not in general true. A framework gp in rk, we'll say, has full affine span if the image of the vertex set under this function p affinely spans rk. So here's another rigidity theoretic characterization of what the maximum likelihood threshold really means. Let g be a graph with n vertices. Then the maximum likelihood threshold of g is the smallest d, such that every generic framework in r to the d minus one is equivalent to a framework in r to the n minus one with full affine spanish. Let's look at this in an example.
00:17:36.504 - 00:18:59.454, Speaker A: So look to the so, yeah, both of these are representations of the graph, the four cycle, one in one dimensional space and one in two dimensional space. So I claim that the maximum likelihood threshold of the four cycle here is three. So in order to prove that to you, according to this theorem, what I need to show is that if I take any generic framework on the square in two dimensions, so we have this offset d and d minus one in two dimensions, I have to show that it is equivalent to a different framework in three dimensions where your vertices actually span the full space, and that you can hopefully, I can hopefully convince you of, just intuitively. So if I take this generic framework on the square in two dimensions, if I sort of pick this up into three dimensional space, I can think about folding this vertex out of the plane. So if I were to treat these edges, say, as iron bars that are free to move around the vertices, then I can imagine just folding, say, this vertex out of my computer screen into three dimensional space. And that finally spans three dimensional space. So this tells me that the maximum likelihood threshold is at most two.
00:18:59.454 - 00:20:05.084, Speaker A: Now, in order to show that it's exactly two, I need to argue to you that, well, there exists generic frameworks in r one that are not equivalent to frameworks in r three that affinely span things. And actually here's an example. So I have this four cycle on the line where the length of one of the edges is equal to the sum of the lengths of all the other three. So no matter what dimensional space I try to put this framework in, these four vertices will always have to sit on the line. So, any questions? All right, so this tells me that the maximum likely threshold is strictly more than two. So with this rigidity theoretic way of understanding maximum likelihood thresholds at our disposal, we can now just import a bunch of stuff from rigidity theory. And I'm going to tell you, I'm going to give you some examples of that.
00:20:05.084 - 00:20:56.754, Speaker A: Before I can do that, I need to introduce a few more rigidity theoretic definitions. So one says that a framework GP is universally rigid if GP and GQ are congruent whenever they're equivalent. So in particular, note that p and Q can go into different dimensional spaces. If I add the constraint that p and q have to be in the same dimensional space, then I get global rigidity. So GP is globally rigid if GP and GQ are congruent whenever they are equivalent and then also live in the same dimension. This is a slightly stronger condition than universal rigidity, and I can go a little bit weaker and talk about local rigidity. So GP is locally rigid if GP and GQ are congruent whenever they are equivalent in the same dimension and sufficiently close.
00:20:56.754 - 00:22:00.924, Speaker A: And maybe a better way to think about local rigidity is this is what your intuitive notion of rigidity would be if you just treat a framework as like a physical construction of your graph in d dimensional space with iron bars for edges that are free to move about the vertices. So for example, here's a framework in two dimensional space. It is not locally rigid. And to see this, note that again, treating the edges as iron bars, I can shear this rectangle and get a framework that is still equivalent because I didn't change the lengths of any edges. But it's not congruent because the distances between, say, the diagonal vertices changed. If I were to look at this framework right here, this is locally rigid, but it's not globally rigid. And, you know, maybe you can convince yourself that as long as you're staying in the plane, there's no way to flex or deform this structure without breaking, bending or disconnecting any of the edges.
00:22:00.924 - 00:22:44.584, Speaker A: But it's not globally rigid because I can sort of like reflect this half of the triangle onto the inside. And this is going to give me an equivalent framework that's not congruent, right? I didn't change the lengths of any of the edges, but I did change the distance between these two vertices. So equivalent but not congruent, but it's not, so it's not globally rigid. Here is a framework in one dimension that is globally rigid but not universally rigid. So note, or maybe take on faith, depending on how hard you want to think about this, there is no equivalent framework in r1 that's not congruent.
00:22:46.524 - 00:22:47.052, Speaker B: But.
00:22:47.148 - 00:23:37.832, Speaker A: There are equivalent frameworks in r two and r three that are not congruent. So for example, this framework I actually obtained just by continuing to shear this rectangle all the way down until all four of the vertices lie on the line. So if I bring this out of the line, I get to something in two dimensions that's equivalent but not congruent. So this is globally rigid, but it's not universally rigid. And then finally, for something universally rigid, if I take this framework on the line, it's universally rigid, because no matter what dimensional space I put this in, these four vertices are always going to be in a line, and the distances are always going to be constrained to be what they are here. So local and global rigidity are generic properties of the graph in the same way that independence was a generic property of the graph. But universal rigidity is not.
00:23:37.832 - 00:24:33.104, Speaker A: And to see that universal rigidity is not, these two frameworks are on the same graph. One is universally rigid, one is not. You can perturb the vertices and convince yourself that this even holds generically. Um. And the reason, um, from a maximum likelihood threshold perspective that we would be interested in universal rigidity is because if G has an open set of frameworks in R to the d minus one that are all universally rigid, then the maximum likelihood threshold of g is strictly greater than d. And this is something that we can actually get a good bit of mileage out of via this. Results of connolly, Gortler and the ran in 2020, which says that g is generically globally rigid in r to the d minus one if and only if there exists an open set of configurations on g in r to the d minus one that are all universally rigid.
00:24:33.104 - 00:25:28.252, Speaker A: And this immediately gives us that if a subgraph of g on at least d plus one vertices is generically globally rigid and r to the d minus one, then the maximum likelihood threshold of g is strictly greater than d. So here are some implications of this for maximum likelihood thresholds. So first is it gives us a lower bound on the maximum likelihood threshold that generalizes Boole's result, saying that the clique number is the lower bound for the maximum algorithm threshold. So in particular, cliques are generic. You know, a clique on a, I'm gonna get, I hope I don't think the numbers wrong here. A clique on D plus one vertices is generically globally rigid in D minus one dimensional space. So, but there are other graphs that are also.
00:25:28.252 - 00:26:12.794, Speaker A: So we have a stronger lower bound or sharper lower bound than bools for maximum monthly threshold. And we can also get a, I should say we don't know how computable this new lower bound is. We can get a slightly weaker lower bound that is computable in polynomial time. So in particular, you can test in polynomial time whether or not a graph is generically globally rigid. But we don't know how to. If you can test in polynomial time to find like a globally rigid subgraph of the highest possible dimension, but some other implications. G has fewer than nine vertices, then the maximum likelihood threshold is equal to the generic completion rank.
00:26:12.794 - 00:27:41.734, Speaker A: And also if the generic completion rank is less than or equal to four, or the maximum likelihood threshold is less than or equal to three, then the maximum likelihood threshold and generic completion ranks are equal. And these two results together tell us that Bleckerman and Zinn's counterexample to this equality always holding, is actually like the smallest you can get get no matter how you want to define smallest. So with this at our disposal, we can immediately prove some kind of nice things about cases where maximum, like threshold and generic completion rank are equal. So I'll give you an example here and then tell you about something else. So it's not too difficult, once you're familiar with the basics of rigidity theory, to convince yourself that if g is a graph with n vertices, then it's independent in r one in one dimensional space if and only if it has no cycles, and that it's globally rigid in r one if and only if it's too connected. These two facts together tell us that if the generic completion rank of g is three, then its maximum likelihood threshold is three. So in particular, if the generic completion rank is three, this means that you have to, the smallest dimensional space in which you're independent is two.
00:27:41.734 - 00:28:17.304, Speaker A: So in particular you're not independent in one dimensional space. So you have a cycle. Cycles are globally rigid in r one. So via our bound on the maximum likelihood threshold, in terms of this global rigidity, the maximum likelihood threshold of g is strictly greater than two. But then by, you know, and gross and solvents bound, the maximum likelihood threshold is less than or equal to the generic completion rank. So the maximum likelihood threshold actually has to be three. And a similar argument actually holds for generic completion rank four.
00:28:17.304 - 00:29:20.654, Speaker A: Um, so using this result of Bergen Jordan, which says that, you know, if g is three connected and minimally dependent in r two, then it's globally rigid in r two, we can sort of use this and then also a sort of like gluing construction using positive definite, positive semidefinite stresses to prove that if the generic completion rank is four, then the maximum likelihood threshold is four. You know, I won't go into details, but sort of the big picture argument just looks kind of exactly like this. And then I guess in the last like nine minutes I'm going to talk about some sort of like other directions we are going or want to go. One is related to something called the weak maximum likelihood threshold. So this is a weaker form of the maximum likelihood threshold. Unsurprisingly, it's defined as follows. Given a graph g, the weak maximum likelihood threshold of g denotes the minimum n such that the maximum likelihood estimate in mg, given n data points, exists with positive probability.
00:29:20.654 - 00:30:08.158, Speaker A: So the only difference between this and the strong maximum likely threshold is here. Instead of in the regular maximum likely threshold, positive probability would be replaced by almost surely. So it's not too difficult to see that the weak maximum molecular threshold is one if and only if g has no edges. Um, so we can show actually that if the weak maximum molecular threshold is two, then there exists an orientation of the edges of g, yielding the order diagram of a partially ordered set. And we believe the converse of this is true. Um, and we think we know how to prove it. Um, but actually, like, you know, hammering out all the details is hard.
00:30:08.158 - 00:31:12.096, Speaker A: Although I don't know, we're really close. We have a ton of computational evidence to support this, but as of now, that's still a conjecture. But one thing that we really nice about if we could prove that conjecture is that it would imply that computing the weak maximum threshold, or graph is np hard in general. And this, this just stems from the fact that, you know, telling if a graph can be oriented like the order diagram of partially ordered set is np hard. So this would tell you that testing whether or not your weak maximum threshold is two is np hard. And then using a construction called coning due to Whiteley and I think maybe connolly too, you can sort of take weak maximum like threshold two up to three using basically all the same combinatorics. Um, yeah, and then um, finally I'll, I'll, I'll end by mentioning some, um, yeah, other things that are like further in the future.
00:31:12.096 - 00:32:25.144, Speaker A: Um, so first of all, we don't have an algorithm for computing maximum likelihood thresholds and weak maximum likelihood thresholds. Um, you can, you know, do various things to convince yourself that, um, you know, weak maximum likelihood threshold of a graph has to be the maximum. Like the threshold of the graph has to be at least a certain number, you know, just by like, you know, generating random PSD matrices of a particular rank and then seeing if you can complete it, you know, by forgetting all of the entities corresponding to non edges to a positive definite matrix. So, you know, there's just some semi definite programming that you can do to sort of convince yourself that the maximum threshold is at least some number. But this is not, this is, you know, this is just, you know, this can give you, this can tell you that you're greater than a certain number, but it can't really tell you that you're less than a certain number. We also really don't understand very well this phenomenon of the maximum likelihood threshold in the general completion rank being different. So, or at least in cases aside from bipartite graphs.
00:32:25.144 - 00:33:18.780, Speaker A: So yeah, black, Herman and Zinn basically, you know, solve the problem for bipartite graphs. Those are well understood. Those are also well understood on the rigidity theory side in terms of stress matrices. But we don't know if you can get differences between the max. We start between maximum likely threshold and our completion rank for examples that aren't just in some way from complete bipartite graphs. As far as like, so, you know, we already have a way to bound the maximum likelihood threshold from above using Uhler's bound, this generic completion rank. It would be nice to figure out if our lower bound in terms of like, you know, the maximum d such that you have a subgraph that's globally rigid in d dimensions.
00:33:18.780 - 00:34:47.064, Speaker A: It would be interesting to see if you can actually compute that efficiently. We don't know yet, and you know, this, this gives you an upper bound on tree with plus one, sorry, a lower bound on tree with plus one. So, you know, this could also be interesting just for separate, like graph theory reasons. Sean Dewar, one of the co authors on this, actually has also been doing some stuff, bounding the maximum likelihood threshold in terms of the genus of a graph, you know, like that being like, you know, the minimum genus of a surface that you can embed the graph on without having any edge crossings. And then also there are other sorts of like rigidity theory setups that could maybe be useful for related problems. So for example, there's a paper of Schultz, Seracold and Theran that studies coordinated rigidity and gives very nice characterization of when you have rigidity, when you constrain certain edge lengths to be equal to each other. It seems like this should be useful for understanding the maximum likelihood thresholds of color gaussian graphical models, which are kind of like the same analog for graphical models, where instead of having edges the same length, you have entries of the matrix that have to be the same.
00:34:47.064 - 00:35:12.964, Speaker A: And then finally it would be interesting to see too, if there are any subfields of rigidity theory that could be used to understand maximum, like the thresholds of directed graph directed gaussian graphical models. So you can say something about causality. So anyway, yeah, that is all I have. We don't have a pre print yet, but hopefully within the next few weeks we will. So yeah, thank you for your attention.
00:35:14.704 - 00:35:19.724, Speaker C: Great Daniel, thank you very much for the nice talk. Any questions or comments?
00:35:20.904 - 00:35:33.934, Speaker D: Yes, are there any applications of tensegrity theory where you constrain the lengths to be no longer, no longer or no shorter or anything like that? That's very close to universal rigidity.
00:35:34.994 - 00:36:02.034, Speaker A: I mean our main tool really is like stress matrices. So it would seem like, and there are times when we do care about the signs in the stress matrices. So it would seem that things you know about tensegrities would be useful. I can't say anything like more precise than that confidently, but I think there should be something. Yeah.
00:36:03.534 - 00:36:20.474, Speaker D: Okay. Because almost every universally rigid thing can be weakened. The conditions can be weakened to be upper and lower bounds instead of just equal lengths.
00:36:22.394 - 00:36:24.534, Speaker A: So that would be super stability.
00:36:25.034 - 00:36:26.214, Speaker D: That's what I mean.
00:36:27.314 - 00:36:27.650, Speaker A: Right?
00:36:27.682 - 00:36:35.414, Speaker B: This would correspond to, let's say you have, you have your data, your sample data is not exact, your sample data is only one sided.
00:36:42.474 - 00:36:48.014, Speaker A: I have to think to see if it like translates so directly, but that seems likely.
00:36:51.384 - 00:37:08.324, Speaker B: So it would be like an MLT problem, but you didn't have equality data your data was inequality. That's hopefully what the correspondence would be to attend segre. So you have a completion, but the numbers that are even there, you can go in one direction for free.
00:37:10.064 - 00:37:32.304, Speaker A: I would think too also, I guess in a lot of our arguments when we were trying to glue things together in a certain way, like we cared about like the signs that were possible on like certain entries of the stress matrices and I guess intensegrities, that's like also something you care about, right? Like your stresses are only allowed to have particular signs on particular entries.
00:37:33.644 - 00:37:41.416, Speaker D: You almost get it for free. You know, just what happens to be the sign of the stress matrix.
00:37:41.580 - 00:37:42.444, Speaker A: I see.
00:37:50.144 - 00:37:52.864, Speaker C: Other questions or comments maybe.
00:37:52.904 - 00:37:59.044, Speaker E: I have an elementary question. Why would maximum likelihood threshold be important?
00:38:01.384 - 00:38:40.190, Speaker A: Well, so you can know how few observations you might need in a particular example or. Yeah, I mean, and then, you know, there are particular sort of like, I mean, yeah, and I guess knowing about, you know, maximum likelihood thresholds of certain families of graphs could potentially lead to like algorithms for searching through them and knowing that you'll be able to like actually fit one of them to data. Or, I mean, are you thinking about like compared to like graph lasso or something? Like why would you be thinking about like a specific graph? Is that sort of.
00:38:40.382 - 00:38:48.750, Speaker E: No. Okay, so in this case, like when you are doing the experiment, you actually need only that number of samples. You don't need more.
00:38:48.942 - 00:39:43.474, Speaker A: Can you think, like there are instances where you can only get so many, right? Like, so in a lot of like genomic applications, when you're trying to understand, you know, like a gene interaction network, you'll have like, you know, potentially tens of thousands of random variables, but only, you know, like a few hundred data points. And so you would want to know, like, is that enough to conclude anything or like in a lot of, um. And this, this isn't exactly gaussian graphical models, but like in a lot of phylogenetics studies, you really only have one sample and you're trying to like, you know, fit like, you know, when you want to fit like a evolutionary history to like a bunch of species you only observe today, right? Like you can't go back in time and I mean, you can look at fossils and stuff, but I guess in a lot of instances, like you only have a snapshot of today. So you have one data point to fit your model to.
00:39:46.814 - 00:39:47.914, Speaker E: Okay, thanks.
00:39:49.054 - 00:39:50.086, Speaker A: Maybe a quick.
00:39:50.230 - 00:40:01.718, Speaker C: Was there a question if I. I'm not sure I understood how you connected. So semi definite matrix completion to rigidity theory. So that, I mean, the technical bridge here seems very.
00:40:01.886 - 00:40:25.670, Speaker A: Yeah. So I was not precise here, really. Or I wasn't. I sort of rushed through this, but. So, and it's not semi definite matrix completion that's, that's connected. It's just like completing a symmetric matrix. So the idea here is generic completion rank, which gives us an upper bound for the maximum likelihood threshold, has the following interpretation.
00:40:25.670 - 00:40:44.990, Speaker A: It is the minimum k such that every generic partial symmetric matrix whose missing entries correspond to the non edges of g can be completed to rank k. Is that sure? Okay.
00:40:45.022 - 00:40:51.046, Speaker C: No, it's just I'm not sure I see the rigidity as how. Connect this to the rigidity.
00:40:51.230 - 00:41:17.990, Speaker A: Oh, well, okay. So. And it turns out, and this was, I mean this, this. So, so it's in the paper of gross and Sullivan. They actually like prove this. But this, this also sort of follows just from coning that this is actually equal to um, one more than the minimum dimension in which your graph is independent in that corresponding rigidity. Matriid.
00:41:18.102 - 00:41:25.354, Speaker C: Yeah. Okay. So I believe it's the bridge between these two results that seems really fantastic, but quite puzzling.
00:41:28.014 - 00:41:28.794, Speaker A: Thanks.
00:41:30.294 - 00:41:32.474, Speaker C: Any further questions or comments?
00:41:35.534 - 00:41:43.662, Speaker E: Yes. So will this, will the slides be available somewhere? Or maybe we should really wait for the preprint. If we want to check more later.
00:41:43.718 - 00:41:46.214, Speaker A: On I can send you the slides. Just send me an email.
00:41:46.994 - 00:41:48.018, Speaker E: Okay, thanks.
00:41:48.106 - 00:41:55.426, Speaker A: Yes, send me email. Send them to you. Great.
00:41:55.490 - 00:42:01.834, Speaker C: So I guess we're done for the day and we'll continue tomorrow morning at 11:00 a.m. thank you.
