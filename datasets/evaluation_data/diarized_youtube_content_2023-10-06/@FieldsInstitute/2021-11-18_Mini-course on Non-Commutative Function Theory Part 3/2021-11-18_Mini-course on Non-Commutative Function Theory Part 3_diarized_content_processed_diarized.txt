00:00:00.520 - 00:00:01.128, Speaker A: Yes.
00:00:01.238 - 00:00:11.446, Speaker B: Okay. The other thing that means is I'm not able to see the chat while I'm talking, so if there's some question in the chat. Yeah, thanks.
00:00:11.550 - 00:00:13.994, Speaker A: I love you. Now you're ready to start?
00:00:14.574 - 00:00:15.566, Speaker B: Yeah, I'm ready.
00:00:15.710 - 00:00:28.578, Speaker A: Okay. Good morning, good afternoon. Good evening, everybody. Welcome to the third or fourth day of this week and we continue with Mike's lecture on non commutative function theory.
00:00:28.766 - 00:00:56.884, Speaker B: Bye. Please. Thanks, shivat. Okay, so this is the last of the three lectures. In this one, I think I won't try to sort of derive anything or prove much of anything. I'm just going to sort of tell you some things that are true. I do want to pick up one end from the end of the talk yesterday.
00:00:56.884 - 00:01:58.702, Speaker B: So yesterday we had looked at, so I introduced this space of square summable NC power series. This is sort of a Hilbert space in the natural way, just with the l two inner product on coefficients, and this sort of, obviously some kind of analog of the hardy space. Because if you just did this in one variable, you got the hardy space, you know, in one variable, functions of square summable power series converge in the disk. So we showed that these series will converge well in the row ball and the column ball and this spectral radius ball that we introduced. So in particular, what we found from estimating the series was that we have like in the classical setting, bounded point evaluation. So these f's are Nc functions on say the row ball. And so our estimates on the power series showed that I get these continuous maps from the Hilbert space into n by n matrices by just evaluating at some level.
00:01:58.702 - 00:02:53.668, Speaker B: And if we then hit that Fw with a bilinear form, we get a scalar and then we get bounded point evaluations. So by the Ries theorem, we got these kernel functions. So we have in the ball marks Vinikoff language, a CPNC kernel, because there's a natural sort of positivity, these condition disobey. And so what I also showed you that if you restrict these kernels to the ball, to the row ball at level one, which is just the equity and ball in CD, we get the jury Arvisson kernel. I'm going to talk more about that connection later in this lecture, but I do want to make a couple more comments that flow out of what we said about the kernel by just writing this out in coefficients. What you can do is you can write down the series expansion for the kernel. It's in the Hilbert space.
00:02:53.668 - 00:03:19.044, Speaker B: So these coefficients here will be square summable. I have chosen last time I took adjoints here, because I wanted to arrive at that Cp and C formulation here. I want to do something different. I'll just take complex conjugates of the whole thing. So this w bar, again, it's not an adjoint, right? It's just the entry wise complex conjugate without the transpose. And the same thing with the bars on the vectors. I mean, those conjugate the entries.
00:03:19.044 - 00:04:01.420, Speaker B: And so if I write it that way, what you can do is, if you think about this, I can write this as in the inside chronicler tensor product of w's with Z's. And on the outside, I have these vectors, tensor. In this identity n is just the identity map from the n by n matrices to itself. And what we have inside here is just really sort of an Nc geometric series. The same computation I did for the Drury Iverson kernel shows that this is really just one minus w bar z inverse. I'm using this sort of abbreviated notation. So my one minus w bar z is really, it's two copies of the identity running out of room here.
00:04:01.420 - 00:04:36.974, Speaker B: Sorry. This w bar z notation, to save too many symbols, is really just a sum wj bar tensor chronicler, tensor product zj. That's what that notation means. And so what you have here is something in the middle that looks very, very much like the Zego kernel we're familiar with. This is now some matrix valued thing. And then I sort of trace out an inner product out in the w variable, but leave the z variable alone. So when I'm all done, I get a function of z.
00:04:36.974 - 00:05:25.440, Speaker B: And so this last expression that's highlighted in the blue here, is a prototype for what's called an Nc rational function. And this particular expression here, where, again, the w, I'm thinking of, the w is fixed, the x and the u are fixed vectors, and the z is the variable. And so this is a particular sort of representation of this function of z. And it's called a descriptor realization, which is, comes from maybe the engineering literature. But this is points to sort of a. I want to say a couple things very quickly about NC rational functions, because what happens is, this is, in the NC setting, irrational functions, these. The point I want to make is that these reproducing kernels are rational in the appropriate sense.
00:05:25.440 - 00:06:13.474, Speaker B: So I just want to introduce a few pieces of terminology about rational functions before I move on. So these kinds of rational functions are extremely important in other aspects of non commutative function theory that I haven't talked about, especially in problems connected to engineering, about matrix inequalities and convexity and things like that. So, the work of Helton and Klep and McCullough and some of the things that Ryan talked about yesterday. So, we have a monic linear pencil. So again, the x's are going to be these Nc variables here. And again, this notation, really, when I substitute in this, is going to be identity tensor identity minus the sum aj tensor xjs. That's how I evaluate an expression on this variable.
00:06:13.474 - 00:07:03.804, Speaker B: Products are really chronic or tensor products. So, the idea is, I fix some coefficients, matrices of some fixed size, and then I can, I can view this as a sort of NC function and NC polynomial, but with matrix coefficients rather than scalar coefficients. So, that's a monoclonal pencil, which, don't worry about the terms. So what's a rational function? Well, an NC rational expression is sort of what you would imagine it would be. So, if you think about ordinary rational functions, the way you make them is you start with polynomials, you call those rational, and then you start inverting things. And anything that you can start with, by starting with a polynomial and taking inverses, and taking more products and sums and algebraic operations, you generate this sort of field of functions, which is called the rational functions. And so, in the NC world, we'll simply do the same thing.
00:07:03.804 - 00:07:47.448, Speaker B: So, without being sort of too careful about it, I'll just say an NC rational expression is just any syntactically correct expression built from polynomials and inverses. So I can take like x squared minus y inverse plus y squared, and then inverse that, and then so on. So I can build very complicated. So, just by combining parentheses and polynomials and inverses, and so build just some finite expression like that. That's called a rational expression. And then the idea is, this will define an n function, at least where it makes sense. So the idea is, well, the domain is simply the set where it's defined.
00:07:47.448 - 00:08:20.066, Speaker B: So, the point is, I just look at, in this case, for this expression to be defined, I would need x squared minus y to be invertible. And then I would also need this expression that appears inside the inverse. I need that to be invertible. And so it's just the collection of all the things that make all the inverses that you're required to take exist. Okay? So that's just a rational expression, and it's got some domain conceivably could be empty. I mean, if you did things wrong. I mean, so the point is, I mean, zero inverse is syntactically correct rational expression.
00:08:20.066 - 00:09:01.800, Speaker B: Because I started with a polynomial and I threw in an inverse. But of course, this has empty domain, but that's okay. So that's what an NC rational function is, more or less. And so, a thing that you should be aware of is that, I mean, sort of like what happens in. Well, may not ramble too much, but I mean, you can have, you can have different rational expressions that sort of represent the same function. So a simple example would be something like this. So if I look at x times one minus y x inverse, well, by a piece of algebra that I think is very familiar, this is the same thing as one minus xy inverse times x, at least on some domain where this makes sense.
00:09:01.800 - 00:09:52.244, Speaker B: So if x times x and y are both contractive or something. So these are two different, I mean, formally as rational expressions, as polynomials and inverse and so forth. These are two different expressions, but they, they have an overlapping domain. There's at least some domain where that they have in common, and on that common domain, they're equal. So, strictly speaking, when you want to talk about an NC rational function, I won't sort of state things too precisely, but roughly speaking, what you have to deal with are equivalence classes of rational expressions. And so these two rational expressions would belong to the same equivalence class, because they there exists, and their domains have non empty intersection, and on that intersection, they're equal. So really, an NC rational function, properly speaking, should be an equivalent class of rational expressions.
00:09:52.244 - 00:10:39.740, Speaker B: But I won't worry too much about the details. The important thing is that NC rational expressions, or NC rational functions, once you tidy up the details, all have very elegant sort of, well, not shouldn't they all have, but there's one important caveat. But they have convenient representations. And so the reason I introduce linear pencils is that's what I need to describe them. So this is a theorem due essentially to Schutzenberger in the 1960s, which predates any kind of non commutative function theories. This actually comes from the world of automata and formal languages and things that seem quite far removed from what we're doing. But really that there were people who were concerned with formal power series and manipulating formal power series with sort of very abstract coefficients.
00:10:39.740 - 00:11:21.926, Speaker B: That's where this comes from for our purposes. What I want to say is, if I have a rational expression and make the importance and assumption that zero belongs to its domain, and again, that need not be satisfied. If I try to take the inverse of a commutator, then, for example, zero is not in the domain of that thing. But if I have zero in the domain, then it admits a descriptor realization. So I have this linear pencil in the middle. So again, if I write this out longhand, this is c star, one minus ax, inverse b. And again, this is really a sum, a tensor xj like that, and then the c and the b act only in the a, and I get some coefficients.
00:11:21.926 - 00:12:04.236, Speaker B: So the point is, if I have a rational expression which is regular at the origin, then it emits what's called a descriptor realization. So descriptor realization is just, that's what this expression is called for some monoclonal pencil a and some vectors, and these descriptor real. And in fact, the converse is true. So if I have a descriptor realization, then at least in some neighborhood of the origin, this coincides with some NC rational expression. And this can all be proved. This is all proved very formally, just by manipulations with power series, formal power series, but just a few bits of terminology. So once you have a realization, sorry, a descriptor realization, they're very far from unique.
00:12:04.236 - 00:13:12.300, Speaker B: The same rational expression can have many different descriptor realizations. But what you can do is when they exist, there always exists a minimal one, because as soon as I have a realization for some r, this pencil, these coefficients a have some size n, and I can look over all the possible realizations, and there must obviously be one of minimal size, because the size is a positive integer. So the good thing about these is that even though they're not unique, the minimal realization is going to be unique up to a similarity, because the a, b and c, again, can't be uniquely determined, because again, I can conjugate the a, b and c all by the right similarity and get a new a, b and c, which is really just a change of basis. But the point is, for a minimal realization up to a change of basis, it's unique. And that's an extremely useful fact. And then there's an important result of voltage, which is only much more recent, that the minimal realization is somehow the correct one, because I talked about equivalence classes and overlapping domains and all that. But the point is that the minimal realization has the maximal domain of any rational expression that it represents.
00:13:12.300 - 00:13:49.154, Speaker B: So the minimal realization is somehow the right one to use. And maybe one more comment. Polynomials themselves, of course, are rational expressions. They just don't have any inverses in them. And if you think about if you're representing a polynomial, well, the point is this, near the origin, this linear pencil can be expanded in a geometric series. And so the point is, I can write this as c star, a sum a to the alpha x to the alpha b. Or if I can write that as I can bring the coefficients inside, this looks like the computations we were doing with the kernels.
00:13:49.154 - 00:14:27.650, Speaker B: But notice that if I started with a polynomial, then this series is zeros after some point, which tells you that these numbers, these coefficients, these entries of the a to the alphas will be zero. So one way for that to happen is, of course, if the a's are jointly nil potent, if the a's themselves, the powers of the a's for large alpha are all zero. And the point is, if you have a minimal realization, the converse is true. So if I look at a minimal realization for a polynomial like this, the A's are going to be jointly nil potent. And so then the point is it's then obviously a polynomial. So I'm not going to use this for anything going on. I'm going to switch gears in a minute.
00:14:27.650 - 00:15:26.874, Speaker B: But anyway, even dealing with polynomials and rational functions in this space, access to these realizations and properties of the minimal realization are extremely important in proving things. One reason for this is that if you think about how you would try to generalize proofs that you're familiar with in the one variable setting in the hardy space. If I want to prove something about a polynomial, well, an easy thing to do is you just factor the polynomial and then try to prove things about the factors and so on. In the NC setting, factoring polynomials is obviously much more fraught. I mean, you can sort of factor them, but factorizations aren't unique, and you don't have these sort of simple linear factors anymore. And so arguments in one variable that involve factorizations of polynomials are unlikely to succeed in the multivariable set. In this NC setting, it turns out a good substitute for that is to work with realizations, and realizations are sort of the right tool.
00:15:26.874 - 00:16:04.694, Speaker B: So I'll mention that. So anyway, from what I said before, the point is these NC kernels, because I just wrote them down as having descriptor realizations are rational functions. And in fact, we can go further. So this is actually a fairly recent theorem of Rob Martin and Elia Shamovich and myself. Not so hard to prove, but you can ask then, which NC rational functions belong to this NC Hardy space. So, in the disk setting, in the classical h two, of course, it's easy to see which rational functions belong in the Hardy space. The answer is they just don't have any poles in the closed disk.
00:16:04.694 - 00:16:57.664, Speaker B: You can prove that easily by doing integral estimates in the l two norm and in the NC setting, of course, we don't have integrals or boundary values in any known useful sense. So you have to come about it in a different way. And the right way to approach it is realizations. So we can characterize the NC rational functions that are in the, that have square summable series by their realizations. So the theorem says what's written there. So if r is an NC rational function, and I take its minimal minimal descriptor realization, and again, the point is the minimal one is always the correct one to use. If I take its minimal descriptor realization, then I can say that r belongs to the Hardy space, if and only if the spectral radius of that pencil, the coefficients a there, the joint spectral radius from the last lecture, if and only if the spectral radius of a is strictly less than one.
00:16:57.664 - 00:17:36.310, Speaker B: And then if you combine that with Pepescu's Rhoda strang theorem, which I also mentioned, that means you can, by a similarity, move the a over to a w bar for a rho contraction. And that means since the minimal realization, I mean, again, I can move it by a similarity, it's still minimal realization, since a will be similar to a row contraction. What that means is I can re express, I can change the basis and re express my rational function in this form. Well, w is a point in the row ball. In other words, it's a kernel. So the conclusion is, and we saw already just by an algebraic form of the kernels, that the kernels are rational functions. And this is a converse.
00:17:36.310 - 00:18:10.134, Speaker B: What this says is then that the reproducing kernels for the NC Hardy space are exactly the same as the rational functions that are in the space. So said. That way, it sounds a little strange, because if you think about the hardy space in the disk, this is of course not true. I mean, the kernels are just, you know, these one over one minus. So in the classical setting, the kernels are just, you know, the zego kernel, like that, which is a rational function. But certainly not every rational function is a zego kernel. Every rational function in the space is a linear combination of zego kernels.
00:18:10.134 - 00:18:57.054, Speaker B: But the way to see that things are correct is to remember that our reproducing kernels here are reproducing kernels at all the matrix levels simultaneously. And remember, in the Hardy space, we could use the functional calculus in h two of the disk. What we could do is I could take an n by n matrix z of norm one, and I can still evaluate Hardy space functions at matrix points, and I can still have these sort of matrix, let me call it w. Sorry. I can still evaluate matrix points just like I did before using the functional calculus. I still get these bounded evaluations of entries. And so, in the classical Hardy space, I do have still these reproducing kernels for these matrix points.
00:18:57.054 - 00:19:23.514, Speaker B: And this theorem is, then the proof goes through identically. This theorem is true in the Hardy space. So every rational heart function in the Hardy space, classically, well, is not the reproducing kernel at a point in the disk, but is a reproducing kernel of this sort of matrix type. And so that's how you sort of correct things. It's not a deep fact by any means, but it's. But it sort of explains what's going on here. So it's not quite as weird as it would seem.
00:19:23.514 - 00:19:45.880, Speaker B: Okay, so I think that's all I want to say about rational functions. I guess I should. Sorry. If I'll advertise Rob Martin's talk later this afternoon. He's going to talk in more detail about things you can do with rational functions in this space. You can even do in this NC hardy space. I won't talk about it, but you can do some version of the de Braun Rovniak construction.
00:19:45.880 - 00:20:44.074, Speaker B: So there are de bronze Rovniac spaces and the de Bronze Rovniak spaces, for these rational functions are sort of, can be analyzed successfully. And Rob will talk about some of that. But the last topic of these lectures, and an important one, is the issue of multipliers. And so, in several of the focus weeks already in this focus program, we've looked at various spaces. We've looked at the Hardy space and the Bergman space, the diraclay space, Debrange Rovniak spaces, and many others. And in all of these function spaces, a question that people are, people like us who work in function spaces, ask is, what are the multipliers? And of course, the multiplier just means that phi f is in your space whenever f is in your space. When f's are analytic, the fees you expect to be analytic.
00:20:44.074 - 00:21:09.054, Speaker B: Analysis with kernels tells you the fees have to be bounded. Sometimes they're all the bounded fees, like in the Hardy and Bergman case, sometimes. So not all the bounded p's, but. Okay, so we want to analyze and discover what the multipliers are. And it turns out, for this NC Hardy space, this has a decent answer to begin with. Let's start with some very, very the simplest possible multipliers. And let me go back here.
00:21:09.054 - 00:22:02.464, Speaker B: So, in all of these spaces, especially Hardy Bergman and Dirichlet, a thing that, in fact, in the lectures that we saw already on these spaces, I mean, the thing that the simplest possible multiplier is multiplication by z in the Hardy space, that's the shift operator. You have a Bergman shift, the Dirichlet shift, and we ask lots of operator theoretic questions about the shift operator. So a natural place to start in the NC setting will be with NC versions of the shifts. Well, okay, so in our NC hardy space, we have an orthonormal basis of monomials, just like in the classical hardy space. And then we have, well, we don't have one shift operator anymore. Since I have d variables, I have of them, I can choose each one of the coordinate functions, each one of the in turn, and think about shifting in that coordinates. And I also have a choice to make, because, again, I'm non commutative, whether I want to shift on the left or on the right.
00:22:02.464 - 00:22:25.140, Speaker B: And I'll talk mostly about the left. And then you can sort of believe that everything I say has a kind of mirror image statement for right shifts. And once you figure out what happens on the left, it's sort of kind of transparently has a right version. So let's talk about the left. So I have these left shifts. So I'll write this l sub j for the left shift by zJ. So I just take my power series, I multiply by zj.
00:22:25.140 - 00:23:11.044, Speaker B: So the monomial z alpha gets shifted to the monomial z j alpha. So the word alpha, I append a j at the beginning of the word. And these are sometimes called left creation operators if you work in sort of the fox based language, which I won't bother with. But what you see from this power series is exactly what you see from, you know, the usual case of the shift in the hardy space is that each lj is an isometry, simply because I have the same coefficient c alpha. It's just that in this case, I don't have all the monomials appearing anymore. I only have the monomials that appear. So the range of Lj is just the span of those monomials that start with Jack.
00:23:11.044 - 00:24:14.848, Speaker B: So what that means is, if I took li instead, well, then I would get the span of the monomial z I alpha. And if I and j are different, these two sets of monomials are all mutually orthogonal, because they're all orthogonal to each other. So what that tells you is these are isometries, but it's a system of isometries that has orthogonal ranges and I can package this condition of isometries with orthogonal ranges into a single identity like that. Li star Lj is the chronic or delta, I should say chronic or delta, I guess, times the identity operator in H. But it's a system of isometries with orthogonal ranges. And the other thing that this tells you is if I arrange the l's into a row, and then I put the l stars in a column like that, then this is the d by d identity operator. So what that tells you is if I arrange the l's into a row, that single row is an isometry.
00:24:14.848 - 00:24:51.238, Speaker B: So we call L a row isometry. So now you can take single isometries, you think about what happens for a single isometry. You have a wool decomposition, there's shifts in unitaries and so on and so forth. You can start asking all the same questions for row isometries. And so Gelu talked about some of that in his talk, and many of the big initial theorems about row isometries are due to him. So I'll call this the left d shift, this d variable shift, orthogonal ranges. And then I'll call an NC function a left multiplier.
00:24:51.238 - 00:25:38.986, Speaker B: If, simply, we have this condition that looks like exactly the definition of a multiplier. In the usual case, if I just multiply by f by phi on the left, it stays in the space. Analogously, of course, there's a definition of a right multiplier. So the natural question is, what are the left multipliers, which NC functions give us? Left multipliers. Of course, from what I just said, each of the zjs is a left multiplier, and therefore, polynomials in the z's are left multipliers because they just get a polynomial and a bounded operator. And then by the definition, what you get are sort of limits of polynomials. If I take limits of these things, if I think of these operators, if.
00:25:38.986 - 00:26:27.056, Speaker B: Well, I have to sort of think about how you want to take limits, but what you believe is some kind of weak limits of polynomials should still be multipliers, whatever that means. But you get a good answer. And the good answer is the following. So phi is a left multiplier of the NC Hardy space, if and only if. Well, it's the best answer you could hope for. Phi is bounded, well, bounded, in which domain? Now, we do have to be careful about the domain, because I said these nc power, these L two power series converge while they converge in the row ball, but also in the column ball, more generally in the spectral radius ball. And the reason to favor the row ball, finally, is given by this theorem, because the point is that the norm of the multiplier, the multiplier is bounded if and only if it's bounded in the row ball, specifically, so, not in the spectral radius ball or the column ball, but row ball.
00:26:27.056 - 00:27:07.632, Speaker B: So, left multipliers are the things that are bounded in the row ball. And even better, just like in the hardy space, the norm of the multiplier is exactly the sup norm over the row ball, the h infinity norm, in other words. And so in pepest original formulation. So here again, it wasn't literally in the space of Nc function formalism, because the thing that happens, which I haven't mentioned, but we have these evaluations, but I've been talking about them, where the disease are matrices. But in fact, this also works for things at the infinite level. So I can take tuples of operators that have road norm of less than one. They still have this functional calculus.
00:27:07.632 - 00:27:59.796, Speaker B: And so what Pepescu proved was that, in fact, this is the soup not over just matrices, but overall operators. And with a tiny bit of extra work, using Pepescu's dilation theorem, you can show that, in fact, enough to take the soup just over matrices, just over the finite levels. So the multipliers are what you hope they would be. They're bounded functions on the correct domain, and the correct domain is row ball, if I work on the left. And moreover, these abstract row isometries, these shift operators, these were studied for a long time by Pepescu and by Davidson, Pitts and others. This object called the free semigroup algebra. So you take these ls, you take polynomials in them, and then you take weak operator limits of those things, and that gives you some kind of abstract analog of the weak operator algebra generated by the weak operator closed.
00:27:59.796 - 00:28:39.952, Speaker B: So the weakly closed algebra generated by the shift, which we know to be sort of completely, isometrically isomorphic to h infinity by the functional calculus. And the point is, the same thing happens here. So you have a completely isometric isomorphism between these operator algebra, the abstract operator algebra generated by these, the d shift, and the sort of more concrete thing of these left multipliers. And so that, that all works the way it does in the disk. So we sort of know what the multipliers are. I stated this here for the left. And maybe as a quick warning, you can ask, well, there should be an analogous thing on the right, but if you stare at it for a minute, the left and right multipliers are not the same set of functions.
00:28:39.952 - 00:29:22.396, Speaker B: So you need a different theorem here on the right. And the point is that the row ball is not the right domain. And maybe I'll leave it as a puzzle for you if you're interested to figure out what the right domain is. But the left and right multipliers are not the same. And maybe, well, in the interest of time, I may not want to go through in this example in too much detail, but here's the idea. If I look at, say in two variables, if I look at these monomials, z one to the n z two, corresponding to l one to the n l two, these have orthogonal ranges. As I wet n range over the integers, you can see that just by looking at the words.
00:29:22.396 - 00:30:15.234, Speaker B: So what that's going to mean is that if I take a square summable sequence of coefficients, say of norm one, if I look at this operator, this sum c one l one to the nl two, this is also an isometry, precisely because these things have orthogonal ranges. And therefore this means this function, if I let f of z one, z two be this sum CnZ one to the n z two, this is a left multiplier. It's an isometry. And the point is, if I like, I can think of this as h of z one times z two. Now, my cn's are just any square summable sequence, norm one. So this h is really any function in the usual hardy space. If I take any function in the usual hardy space of z one and multiply by z two, that's a bounded left multiplier.
00:30:15.234 - 00:31:13.908, Speaker B: But if I look at the same function h of z one z two, and think of it as acting on the right, then what will happen is this will be bounded as a right multiple if and only if this function in one variable is actually bounded is h infinity. So if you then take any h two function that's not bounded, I get an example by this construction of a left multiplier that's not a right multiplier. There is a simple relationship between them, but in the interest of time, I want to move on. But you can sort of straighten this out and figure out what's going on. Basically, what you have to do is just reverse the order of all the words is a sort of transpose map that sends alpha to writing it in the opposite order. That transpose map on the monomials induces a unitary of the Hilbert space. And then basically what you get is the right multipliers, or the images of left multipliers under this sort of word reversal map.
00:31:13.908 - 00:31:54.784, Speaker B: Then you can patch everything up that way. Okay, but I'd like to move on. So an important thing that I mentioned at the end of last time was again, the connection with the Drury Arvisson space. And that was if I took an F in the NC Hardy space and restricted it to level one, I got something in the Drury Arvisson space. And moreover, this map plays well with norms. The norm is this restriction map is a partial isometry. And conversely, every f in the Drury Arverson space is a has a Hilbert space norm preserving sort of free lift.
00:31:54.784 - 00:32:44.672, Speaker B: I can extend it to the balls at higher levels while preserving the Hilbert space norm. And then the next theorem is a result, which is again, I'm stating in this NC language, in this NC function theory language, which wasn't how it was originally formulated. It was originally formulated in terms of just this free semi group algebra. But the point is, the same thing happens for the multipliers. If I take multipliers, what this theorem of this Davidson and Pitts theorem says, if I take a multiplier of the NC Hardy space, which I'll write maybe is NCH infinity a left multiplier and restrict it to level one, that's a multiplier of the Drury Arvisson space. That restriction map is a complete as a complete contraction on operator algebras, and it's even a complete quotient map. So even more importantly, the converse happens.
00:32:44.672 - 00:33:22.882, Speaker B: So again, I can lift. And this is much less obvious why this would be true. And I'll even say the obvious choice of lifting doesn't work. But what happens is, if I take a contractive multiplier of the juryarvisin space, there exists an nc function up in the row ball whose super norm is the same as the norm of the hardee's of the jury Rus multiplier. So I have a lift, so I have a norm preserving free lift so I can do this. So the same thing, this thing that I said in the Hilbert space norm, this lifting business also works in the multiplier norm, though that it works in the multiplier norm is much less obvious. And I'm not going to prove this.
00:33:22.882 - 00:34:37.784, Speaker B: The proof of this converse, you could do it in a couple different ways, probably, but the right way to do it is via realizations. So not the descriptor realizations of rational functions that I talked about before, but a related thing called an ABCD realization, which I'll write down for the people who have seen it before. So something that looks like a real descriptor realization, although the a's don't have to be matrices anymore, they can be operators in some Hilbert space. But basically by showing that your multipliers in both the commutative and non commutative settings have realizations, you can prove this lifting theorem. But this now gives you a sort of viable method for maybe passing back and forth between theorems about these nc multipliers, these bounded functions in the row ball and the Drury Arbus in space. And I'll come back to this in a minute. So to get there, the next big thing I want to talk about is a Berling theorem, because we have these shift operators now, and the most famous theorem about the shift operator is the Berlin theorem, which tells you what its invariant subspaces are.
00:34:37.784 - 00:35:20.994, Speaker B: So we can do the same thing. We can ask for invariant subspaces of the. Well, really, we want to ask them about joint invariant subspaces, invariant subspaces for all the left shifts simultaneously, which quickly reduces to just saying, is the same thing as the invariant subspaces for all the multipliers simultaneously. So we'll say, just like we talk about invariant subspaces in the Hardy space or the Bergman space or duroque space or wherever, to say a closed subspace is left invariant if it's invariant under all left multipliers. So phi f is in the space whenever f is in the space. That's a left invariant subspace. Of course, I could also talk about right invariant subspaces, and I'll get sort of mirror image theorems about those.
00:35:20.994 - 00:36:14.374, Speaker B: Similarly, we can make a definition just like as we do in the commutative setting, we talk about a cyclic vector. So a cyclic vector is one where, if I take the vector taking any individual function, I can look at, say, the left invariant subspace it generates. So I can take all left multipliers on f and then close that up and I get a space, the invariant subspace generated by f. You say f is cyclic if that invariant subspace is the whole space, in other words, multipliers times f or dense in f, or in other words, maybe I didn't use this notation elsewhere, but the cyclic invariant subspace generated by f is the whole space. So I'll state a simple version of the NC Birling theorem. This isn't the full version. Cause here I'm making the extra hypothesis that m is cyclic.
00:36:14.374 - 00:36:45.044, Speaker B: There's still a version for general subspaces. I don't need to require them to be cyclic. I should mention that, of course, in the classical theorem, in the Hardy space in h two, every invariant subspace is cyclic. That's not the case anymore. The NC setting. But I'll say in the cyclic case, what happens, this is a theorem discovered independent. Well, again, not just the cyclic case, the general theorem was discovered independently by Arias and Pepescu, and then later by Davidson and Pitts.
00:36:45.044 - 00:37:11.970, Speaker B: But in the cyclic case, what it says, if I have a cyclic, okay, now I have to be careful about left and right. Here's what I'm going to do. I stated here about this is sort of left cyclic, but I could talk about white cyclic as well. Let me state the theorem in the write case. So write invariant. So here I'm looking at the write invariant subspace, so f times write multipliers. The idea is that one way to say Berling's theorem is that the invariant subspace is the range of an isometry.
00:37:11.970 - 00:37:49.484, Speaker B: And that's what happens here. And the point is, if you keep track of left and right, a right invariant, some space will be in the range of a left isometric multiple. And it's sort of clear how that should work. Because if I look at theta times f, then everything, if I want to generate an invariant subspace, well, if I multiply this thing on the right, then that's still in the range of theta. So if I multiply, so the range of a left multiplication operator will be right invariant. And so that's why I have to be careful about left and right. Similarly, if I want a left invariant subspace, it's going be the range of a right invariant multiplier.
00:37:49.484 - 00:38:28.230, Speaker B: But just like in Berlin's theorem, it's an isometric multiplier. So ranges, so invariant subspaces are ranges of isometries. In the cyclic cases, just the range of a single isometry. In the non cyclic case, I would have a row isometry, and then my invariant subspace would look like a row times a column, and the whole row together would be a row isometry. But that's the Berlin theorem. In the cyclic case, I won't prove this, but actually, I mean, if you think about the operator theoretic proof of Burling's theorem, what do you do? You take your, you take your shift, you should restrict the shift operator to the invariant subspace, then apply the mold decomposition. And it's basically the same thing here.
00:38:28.230 - 00:39:07.100, Speaker B: So there's a Popescus version of the mold decomposition for the row isometry lets you prove this. So I won't say about the proof, but it runs by familiar steps. Okay, well, you get an immediate corollary of this, which is an inner outer factorization, because all I do is I apply the Berlin theorem to the right invariant subspace generated by f. So a write multiplier. Close that up. If I apply the Berlin theorem to the write invariant subspace generated by f, it says that space is equal to some theta times h two nc. But of course, f itself is in this space.
00:39:07.100 - 00:39:57.944, Speaker B: So f looks like theta times something. And then, by the usual business, you get that this f is a cyclic, the capital f is a cyclic vector. So just like in the classical setting, f factors as an isometry, an isometric multiplier times a cyclic vector. And so you can think of the isometric multiplier as an Nc inner function and the cyclic vector as an nc outer function. Although right now, this is totally abstract and operator theoretic, and there's no function, the theory here yet, but this is, I mean, has a strong claim on being called some kind of inner outer factorization. So if you've done this now, you can sort of ask several questions. I mean, certainly the obvious questions are going to be, is there any sort of function theoretic content to these inner outer factors? And I'll get to that in a second.
00:39:57.944 - 00:40:43.460, Speaker B: But before I get to that, let's notice that combining everything I've just said, you get a factorization theorem in the, in the purely commutative realm for the Drury Arverson space, because it then says the following. It says, if I take a theorem, a function in the Drury Arvisson space, I can factor it as some theta times f, where theta is a contractive multiplier. Now, it's contractive in general. It won't be inner in the sense that it won't be an isometric multiplier. It turns out, in the Drury Arvisson space, you don't have any isometric multipliers except for the constants in several variables. But nonetheless, you can write it as a contracted multiplier times a cyclical vectors and some kind of outer function. And moreover, the norm of the cyclic vector is the same as the norm of the original f.
00:40:43.460 - 00:41:28.314, Speaker B: And the proof consists of just combining everything I just said. So the point is, you take f in your Drury Arbisson space, you lift it to a function in the NC Hardy space. You factor that function by the Berling theorem as some theta of z times some z f wiggle of z. That's the Nc birling theorem. And then when you restrict back down to the ball at level one, I get some multiplier here, which is, since this was an isometry, it's contractive. And the restriction map will give me another contractive thing, and I'll get some f tilde there. And you can just check that if I restrict a cyclic vector, I get a cyclic vector.
00:41:28.314 - 00:41:57.724, Speaker B: And. And then that's the factorization. And this works also, not just for the Hardy space, but for also complete pick spaces, which that connection. I'll refer to Michael Hartz's lectures next week. But this is a simple example of a way that you can prove something about this commutative setting, about commutative function spaces, via this free lift business. And the point is, the free lifts respect both the Hilbert space norm and the multiplier norm. That's why this hangs together.
00:41:57.724 - 00:42:46.304, Speaker B: You can do more things. You can talk about nebula pick interpolation and stuff like that, but I think I'll suppress that and move on. You can also do this, I'll mention, not for a single f, but if I have a column full of f's, you can again write this as a sort of column multiplier times a cyclic f, and you can apply that to factorization further factorization theorems and so forth. But anyway, the point is, this is a way that you can prove things in the commutative setting. And this happens across the theory, even in sort of parts of the engineering literature that I mentioned about realizations for rational functions. You can prove things about ordinary functions, of commuting variables by proving things in the NC realm and then restricting to the ball at level one. So this happens in many places across the subject.
00:42:46.304 - 00:43:57.384, Speaker B: And I think it's part of the reason that it's interesting. Not just the NC stuff is interesting in its own right, but it can tell you sort of things about the community world. Okay, so let me, in the seven minutes I have left, revisit this Nc inner outer factorization and ask if we can interpret it or get any sort of more general function theoretic information rather than just abstract operator information. And what kind of information would we want? Well, obviously, in the h two case, if I write f as its inner outer factorization, classically, a thing that we know is that, for example, the zero information about f is all contained in the inner factor. The zero information is contained in the inner factor. And so this leads us to ask questions about, say, zero sets of NC functions. And so let me tell you something there, actually, before I do that.
00:43:57.384 - 00:44:36.264, Speaker B: Well, yeah, yeah. You can ask. So you can ask, sort of, what are the inner functions? I mean, how do they look like? And so in the cyclic case, sorry, in the h two case, we know that these are generally inner functions. In the sense that the boundary values are equal to one almost everywhere on the circle. And then you can ask, what do the outer functions look like? And of course, you have this Reese representation of the outer function as a Herglitz integral, et cetera. And then the inner functions factor into Blaschke products and singular. And you get detailed information about the function, theoretically.
00:44:36.264 - 00:45:23.344, Speaker B: So you can ask, can you characterize the outer functions? I'll say ahead of time right now, in the NC case, we really have no good intrinsic description of function. Theoretically, what the inner functions really look like and what the outer functions really look like. This is nearly a wide open problem. We can say a little bit if we specialize to, say, polynomials are rational functions. And so this is the result of Robinelli and I. So if I stick to NC rational functions or polynomials, then you can say a few elementary things that you hope would be true. The first is that if I look at the NC inner outer factorization of a rational function, or polynomial, then both the inner and the outer factors are themselves rational.
00:45:23.344 - 00:46:02.888, Speaker B: And in fact, if my, if I start with a polynomial, the outer factor is also polynomial. You can prove that using older things. You can prove that using Pepescu's version of Ferris theorem. But anyway, if I have a polynomial and I factor it, the outer factor is a polynomial, and the degree doesn't go up just like it would, just like what happens in the classical case. And also in the polynomial and the rational case, we can in fact characterize what the NC outer functions look like. If you think about the Hardy space, it's easy to characterize the polynomials that are outer for the Hardy space, the router, if and only if they have no zeros in the disk. And the answer turns out to be the same here.
00:46:02.888 - 00:46:30.944, Speaker B: So if I have a polynomial or a rational function, again, just polynomials and rational functions. For polynomials and rational functions, these things will be cyclic if and only if they have no zeros in the row ball. And no zeros means that the matrix f of z is non singular. The determinant is not zero anywhere in the row ball. So that's a satisfying theorem and not overly difficult to prove. Maybe. But then you think about how you prove this in the, in the hardy space.
00:46:30.944 - 00:47:17.704, Speaker B: And in the hardy space, this is easy to prove because I just factor the polynomial. And if I want to prove it cyclic, it suffices to prove just each individual factor is cyclic. And for a single linear factor, you could just bare handedly prove that it's cyclic. I mean, you can just write down an approximating sequence in your done, as I mentioned before, a polynomial in the NC setting. I mean, you can't factor into linear factors like this, and you can sort of factor it, but not in a helpful way, probably. So, I mean, it's a reasonable conjecture and you believe it and it ends up being true, but proving it that somehow you have to really invoke some completely different set of ideas. And not surprisingly, the completely different set of ideas you need are realizations.
00:47:17.704 - 00:48:10.220, Speaker B: Realizations. But one extra ingredient, because even if you think about what this says, that if you think about how in the hardy space, the inner router Blaschke singular outer factorization works, saying that this condition is sufficient is really saying that a polynomial can't have a singular interfactor. So somehow in the NC case, if you're proving that, you're proving polynomials can't have singular interfactors, whatever that means. So to tidy that up, let me in the last couple of minutes say a word about zero sets. So what should a zero set be in the NC setting? And it turns out, well, you can investigate different notions, but the right one, at least for our purposes, turns out to be this. So I'm going to look at not just places where f of Z is a singular matrix, but I want to keep track of a direction in which it's nonsingular. So I'm going to look at a vector Y that's orthogonal to the range of f of z.
00:48:10.220 - 00:48:37.684, Speaker B: So a pair zy is a detailed zero of f. Of course I want y to be a non zero vector. Here, a detailed zero if y is orthogonal to the range of f. This is set up for right invariant subspaces. Of course, there's also a left invariant version where you just look at the other side. And so the left detailed zero set will simply be all the pairs again, y, where y is, or non zero vectors of these zeros. So this is the analog of a zero set of an c rational function.
00:48:37.684 - 00:49:50.290, Speaker B: It's easy to check that if I look at the zeros of f, if I have a detailed zero of f, then it's also a detailed zero of f times phi for any phi on the right, which means that if f is cyclic, then it can't have any detailed zeros, right? Because any detailed zero of f will be inherited by the invariant subspace, just like class. And also it tells you that if I have an inner outer factorization, then the inner factor carries all of the zero information of f. You quickly see from this formula that f has a detailed zero at zy, if and only if the inner factor does. So this is sort of satisfying. The inner factor has all the zero information, but still, I mean, it's not like we have a Blaschki product that we know how to extract it. So it's still really a wide open question of how you extract the zero information in a useful way. But we can say one thing that's sort of, I think, kind of surprising, that in the NC case, you nonetheless have a Blaschke singular outer factorization, where I have to be a little bit careful about how to define these.
00:49:50.290 - 00:50:18.908, Speaker B: And so I'm not a time, I'm not going to try to do this too. But basically this f is going to be an outer thing. This S will be a singular inner thing in the sense that it's an isometric multiplier. That's why I call it inner. But S has no zeros in S has no zeros. I shouldn't say determinant. The thing that I have to be careful of here is it's not strictly an NC.
00:50:18.908 - 00:50:58.174, Speaker B: I need to look at the infinite level. I need the word worry about zeros at possibly operator points in the functional calculus. And so I'm not going to give you a careful definition of what the Blaschke thing is, but basically what it is, is it's an inner function that determines its own zero set, so that the invariant subspace generated by b is exactly b times h two nc. So there's no sort of singular. Anyway, you can make sense of it. And so using this fact, and you can prove that polynomials can't have singular interfactors, and that's how you land on that. So I'm out of time.
00:50:58.174 - 00:51:21.204, Speaker B: So I haven't stated this theorem quite precisely because I have to give you a careful definition. But this works. But I'll say, in terms of open problems, it's still completely wide open to give any kind of intrinsic function theoretic characterization of what any of these factors are, I think it'll be very interesting to do this. And so, anyway, I'm out of time, so I'll stop here. Thank you.
00:51:22.704 - 00:51:38.024, Speaker A: Thank you indeed. Let's thank Mike first. Any question or remark for Mike. Yeah, Mike, it's Stefan here.
00:51:38.144 - 00:51:39.112, Speaker B: Hi, Stefan.
00:51:39.288 - 00:51:47.284, Speaker A: I have a question. This last theorem, can one just get a version where the s and the b are switched? Is that, is there a reason why the b is on the left?
00:51:48.904 - 00:52:16.340, Speaker B: Somehow, the way this is set up, the b is. Yeah. Could you get a version where the S and the b are switched. I mean, the way that we prove it, it kind of forces the B to be on the left. But I don't see a reason why there couldn't be a switched version. But you would have to sort of prove it differently. Yeah, that's a good question.
00:52:16.340 - 00:52:30.264, Speaker B: I don't know. Yeah, the way the proof runs, it sort of. It forces it to be on the left, but it's not clear to me that. That. Yeah, I don't know. That's a good question.
00:52:32.084 - 00:52:37.604, Speaker A: Thanks. Further question or comments?
00:52:38.784 - 00:53:16.384, Speaker B: Actually, let me just say, I mean, in reference to Stefan's question, I mean, the way this is set up, you proved, I mean, you look at this by looking at, of course, the right invariant subspace generated by F. So what you do need to have is that the, it's certainly what you're going to have. This, with the inner factor on the left and the outer factor on the right, there's a companion version. I mean, you could reverse everything where you. You do the left invariant subspace instead and you look at right zeros. But in that case, I mean, if you reversed everything, you'd have a sort of an FSB like that, but still you'd have the flash key all the way on the outside and the singular sandwiched in the middle. So whether.
00:53:16.384 - 00:53:21.644, Speaker B: Whether you could have this theorem with those two reversed. Yeah, I don't know.
00:53:31.524 - 00:53:44.044, Speaker A: Is there any other comment or question for Mike? If not, let's thank him again for this wonderful series of lectures.
