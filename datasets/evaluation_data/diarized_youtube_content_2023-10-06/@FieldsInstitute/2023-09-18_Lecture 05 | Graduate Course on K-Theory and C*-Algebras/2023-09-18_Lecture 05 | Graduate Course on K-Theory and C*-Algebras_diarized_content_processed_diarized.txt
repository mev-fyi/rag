00:00:01.680 - 00:00:34.704, Speaker A: Good afternoon. I think there's lots of light up here, so I hope you, I hope you can see me now. I can't see everyone very clearly in the audience because I don't think this lighting is up to scratch. There are no windows, but. Okay. So it could be worse, I guess. So what I'm going to do today, I think I'll go over the.
00:00:34.704 - 00:00:56.554, Speaker A: Go through people's names, go through the list of names again. But was that a little brighter? She was, yeah. Okay. Someone found that's lighting up the aisles. I don't think that was. Anyway, more light than the subject, the better. I don't know if you want to be referred to as the subject.
00:00:56.554 - 00:01:39.164, Speaker A: Maybe that's better than the object. I don't know. Well, but the object. The object of the course. What I was planning to do is start off with discussing a homework exercise, namely along in the theme, in the theme of there only being one theorem in linear algebra over an arbitrary field. Forget about the canonical Jordan form or canonical rational form, because that's not linear algebra, that's ring theory. Okay, so it's very nice, but it's.
00:01:39.164 - 00:02:36.438, Speaker A: Linear algebra is over a field, linear operators and vector spaces. But just what's true that if you look at polynomials with coefficients in a field, then that's a principal ideal domain. And you can, if you take the vector space with a linear transformation, then it does give you a module over the, over that ring. If it's a finitely generated vector space, then it'll be a finitely generated module and it could even be infinitely, even could be an infinite dimensional vector space. There's the clue. This applies also to infinite dimensional vector spaces. If there is a single vector together with the or finite number of vectors, which together with the which together with the operator, generate all vectors.
00:02:36.438 - 00:03:06.226, Speaker A: So that's a. It's a rudimentary infinite dimensional theorem. Yes. Goes back to our problem. What kind of problem? Like our solutions to the problem. Well, yeah. Well, by the way, the problem set was only a suggestion, so I couldn't really take up that particular, those particular problems you're supposed to.
00:03:06.226 - 00:03:39.694, Speaker A: I mean, one problem you were supposed to consider is the one I'm just going to talk about now. That's. But I don't think that was included in Mister Laos suggested list. It was just saying that an example. He was giving another example of two linear algebra theorems which are trivially equivalent. Okay? I mean one of them is well known. It's the fact that dimension exists, right? The number of elements in the basis behaves like a dimension.
00:03:39.694 - 00:04:26.654, Speaker A: Take a direct sum of two vector spaces, then the union of the two bases is in a natural sense as a basis. And so the dimension is additive. That's one theorem about vector spaces. And the another one is what? I was just directly related to the course, namely the k one. Sorry, k zero of a field is what? Yeah, it's a group, right? It's a billion group. And in this case, it's the group of integers. And this is supposed to allow you to prove that in a trivial way, that dimension is number of elements in the basis is additive.
00:04:26.654 - 00:05:22.604, Speaker A: And Rex sums, in other words, and in particular is independent of the basis. And just to give you an I was hoping, I'm still hoping. I haven't seen what people have been handing in. As of yesterday, someone had already given me a list of exercises, quite a few pages. But today I'm hoping. I've been too busy with a workshop to look at my email yet. But anyway, I was hoping that someone might, for instance, take the theorem, the basic linear algebra non trivial theorem within a finitely generated vector space.
00:05:22.604 - 00:05:55.842, Speaker A: Injective for linear transformation implies surjective. That's one statement. And the other statement is you just interchange injective and surjective. And it's still true. Okay, and so how do you go back and forth between those two? How do you interchange injective and surject? Well, for finitely generated vector spaces, there's a simple way to do that. How many people know how to do it? Yes, you're Abu. Well, I don't have to.
00:05:55.842 - 00:06:44.010, Speaker A: Have to. I'm wanting to keep track of people's name. Are you Abu Sabbak? All right, well, at least I'm right that I should know your name, right? That's called second order. Second order rightness. This is a second order theorem in linear algebra that all first order terms, all non trivial first order terms are trivially equivalent. And so, Sambek, what were you going to suggest? Well, okay, but this is sort of changing ground. I mean, you're saying you want to prove that injective implies surjective and that.
00:06:44.010 - 00:07:20.430, Speaker A: And so you're saying take some other non trivial. Of course you're allowed to take any, any other non trivial theorem and use it to prove this. But I was suggesting a single, a particular other non trivial theorem, namely, where you just interchange injective and surjective. Okay, you have to look twice. Have to look twice to make sure there are different statements. Right? So one is injective, implies surjective, and the other surjective implies injective. And now you could go by way of some third non trivial theorem, but in fact, you can go back and forth.
00:07:20.430 - 00:09:04.004, Speaker A: There's a simple operation with finite dimensional, finitely generated vector spaces, which, which allows you to interchange injective and surjective. So how many people think they might be able to suggest what I have in mind? To put their finger on what I have in mind? Yes, Daniel? Well, okay, I'm very happy to hear that answer, because that's the other sort of new fundamental non trivial theorem. Okay? We're adding to the number of non trivial theorems. I don't know if linear algebra students how will be thankful to us, but, but anyway, so you're suggesting a technique of index. I would suggest that there's a more elementary way of doing it, and it is important that the ultimate, that the vector space finitely generated. But is everyone familiar with what the dual of a vector space is? It's, you took linear functionals, and there are vector space, linear mappings from the vector space into the underlying field, the basic field, okay? And in general, it's an infinite dimensional vector space, then, and the dimension of the, of the dual space will not be the same. Okay? At least if it's a countably infinite dimensional vector space, then the dual vector space does not have countable dimension, doesn't have accountable basis.
00:09:04.004 - 00:10:31.634, Speaker A: But if it's a finitely generated vector space, let's not talk about a base, let's not talk about the dimension, because that's one existence of dimension with important properties. That's one of the non trivial terms. Okay? You don't want to start running risk computing or having a circular argument, but, okay, so now, not only so, if you have a, you have a vector space, two vector spaces, we would say just v and v, because we're talking about transformation from the space into itself. The dual space is v star. Then, first of all, if it's w, v going to w, then you have, well, v star the opposite direction. Because if you have a linear functional, linear functional on the f is the field, have a linear functional on w, then you compose it with the map, you get a linear functional on v. So it goes from v star, goes from w star to v star, okay? So, and furthermore, if you take the go, take the double star, and that's still, you have the same theorem, right? But it reverses again.
00:10:31.634 - 00:11:19.940, Speaker A: And in fact, in the finitely generated case, v is canonically isomorphic to v double star. It's always contained in the finite dimensional case. It's on and w is the same as w double star. But if you look at t and then t star. So t double star is very, is equal to t in the finite dimensional case. But the point is that you look at t and t makes sense for t to be injected, right? Well, that's exactly the same as saying that t star is surjected. And the thing that, and furthermore, these are exercises.
00:11:19.940 - 00:12:01.480, Speaker A: If t is, t is surjective, that's the same as saying that t star is, is injective. So saying both, saying that the t is both injective and surjective. Well, we're not talking about both. We're not talking about both, but we're saying that injective implies surjective. Okay? Injective implies surjective, but then going down to t, that says you can interchange the two words. So that says that surjective implies injective, but that's an arbitrary, we're talking about an arbitrary t. The t star is an arbitrary of arbitrary transformation, okay? And this is not the same, not canonically the same as w is isomorphic.
00:12:01.480 - 00:12:29.000, Speaker A: There is an isomorphism non unique. But the point is, it's an arbitrary finitely generated vector space, okay? So we're saying that if it's true for even the dimensions the same, we don't have to worry about that for arbitrarily finitely generated vector spaces. Injectivity. Surjectivity implies injectivity. Then it's, then it's inferior. It's the same, it's the corresponding family. And the two words get flipped.
00:12:29.000 - 00:13:15.418, Speaker A: Injective and teductive get flipped. So there's an example of essentially trivial proof, just a bit of hand waving. It's a good exercise. Okay? I'd say, even if I've gone over it, it's worth writing out. And you could even do it by this afternoon if you haven't got handed hidden exercises. I don't want to insist, by the way, too much on deadlines, but I forget I said that. All right, so that's an example of what this multifaceted exercise is, the fundamental theorem of linear algebra in quotation marks, because I think someone even at one year managed to locate such a name, the Internet.
00:13:15.418 - 00:14:00.826, Speaker A: And something is called that, maybe rank nullity or something. But the slightly unfortunate choice of it's supposed to be something bombastic, which summarizes all of linear algebra in a single statement. And the fact is, and the trouble is that all the non trivial facts, they look a lot, they look very different, especially when you start talking about index. So Daniel, what you were thinking of is that, that the vector space is finitely generated if and only if every linear operator has an index. And then in that case, it means the kernel and the co kernel are finitely generated. Then in that case, in the finitely generated case, the index is always what? Yes. Okay.
00:14:00.826 - 00:15:39.074, Speaker A: And that's, that's equivalent to this range nullity theorem, if you like, or it's equivalent to any one of these other non trivial theorems for arbitration. And so the one I'm suggesting is that k zero of a field, when you putting up for consideration this k zero of a field is isomorphic to the group of integers, okay? Just a group without any order function. And so there, so there are two parts to the exercise to prove this is true. But we've discussed why it's true, right, to compute the semigroup in terms of direct sums of the finitely generated vector spaces, which is the same as finitely generated projective modules. So we just take as a given that you look at the semi group isomorphism classes of finitely generated vector spaces with direct sum, and you notice that up to isomorphism, it doesn't matter how you, in which order you take the direct sum, or if you take three, it doesn't matter how you bracket two of them. Okay, so that's a semigroup, a billion semi group. When you take what's called the universal enveloping group.
00:15:39.074 - 00:16:16.794, Speaker A: Some people associate the name Groten Deek to that construction. But it was discovered relatively general groups 20 years before Groten Dieck used it. His. Groten Dieck's claim to fame for his justifiable. Justifiable claim to fame is that he used the developing group to give you the k group. And in the setting of algebraic geometry, it was revolutionary what he could do with it. So if people talk about the growth in the group just as a way of going from a semi group to a group, then he'll be rolling in his grave, no doubt about that.
00:16:16.794 - 00:17:42.816, Speaker A: But, okay, but, and in the case of the, if we proving, if we're doing this calculation, then you get that the semi group is plus including zero. And the developing group of that, we've known from an early, early in our education that the enveloping group is the integers, right? Okay. And so the question now is to go backwards. Does this imply that the Murray van Neumann semigroup, say, defined in terms of module, module version, means finitely generated vector spaces? Because this is the simplest way of defining, you take finitely generated vector spaces and take direct sum. This is isomorphic to z plus. Okay? Sometimes people write z greater than or equal to zero, I mean, including zero. If you're taking the enveloping group, it doesn't matter too much.
00:17:42.816 - 00:18:18.454, Speaker A: Whether you include the zero in the semi group, it doesn't matter at all. Semi group doesn't have to have a zero, whereas a group has to have a zero. You take away the zero from the semigroup, then the group just puts. Developing group just puts it back in. Okay, so this is. This is a one part of the. Of the sort of very ambitious bird's eye view of linear algebra that I've been trying to promote.
00:18:18.454 - 00:18:48.094, Speaker A: Okay, so that's what I wanted to. That's how I wanted to start off. But let me go through names, and please excuse any awkwardness, the pronunciation, locating people. So. Abigail Arsenault. Thank you. Alexander bowling.
00:18:48.094 - 00:19:20.674, Speaker A: Thank you. Jing kuan kao. Jack Ceroni. Is that good pronunciation? Should it be Cerrone or Cerrone in English? Yeah. Okay, so the trouble is, sometimes I can coach too much on other languages, and I do it like you never. No one should ever say Einstein. Right? They might think that you're inviting them to go for a beer.
00:19:20.674 - 00:19:49.824, Speaker A: I don't know. Okay. But on the other hand, if it were 100% English, sort of transliterated, it would be to Einstein. It's a difficult world to negotiate. The world of science. Or science students, I guess, too. All right.
00:19:49.824 - 00:20:17.944, Speaker A: Yuj Cheng. Carissa Cho Lee. Oh, thank you. Siddharth Dagar. Okay. Daniel Dima. Emil Shredfeld.
00:20:17.944 - 00:21:00.524, Speaker A: Sorry, takes me a while to scan. It's really intimidating when Google says how much it took to display a whole screen full of sources, some number of microseconds. Well, now, maybe with me it should be milliseconds, but I don't know. That's okay. Justin Foos. Thank you, Jermaine. Charles Griffin.
00:21:00.524 - 00:21:18.824, Speaker A: Thank you. Yes. 10 may gor. Thank you. Henry Hua Huang. Thank you. Sorry.
00:21:18.824 - 00:21:33.016, Speaker A: Yeah. Okay. What do you usually use? Okay, I'm sorry, I still can't. It's a large room. I still can't catch everything. Little noise. That's just an excuse, I guess.
00:21:33.016 - 00:22:09.944, Speaker A: But if you come up afterwards, I mean, at some point, you're asking a question of what we're thinking of. Jal al Kasub. Thank you. Alexander Kolk. Jal Chang Lin Yifei Lu. Thank you. Ruth McCutch.
00:22:09.944 - 00:22:31.304, Speaker A: Thank you. Abu bakr mohidin. Thank you. Madariso mulaisho. Ruben Navisarjan. Sorry. Oh, yeah.
00:22:31.304 - 00:22:57.624, Speaker A: Thank you. Sorry, Jonathan. You know, I never got the right pronunciation of ng, so. Okay, good. And Rishi Prakash. Thank you. How are you? Okay, good.
00:22:57.624 - 00:23:30.200, Speaker A: Bo Tao Su. I have a former student named Su, but also the. And his name comes up in sister algebra theory a lot. So it'll come up in the course when I'm reporting on recent classification theory. So after this, I was trying to go into Wim's theory of classification. Okay. For matrix algebra, generalized matrix algebra, Internet tensor products of matrix algebra.
00:23:30.200 - 00:24:09.944, Speaker A: But there's a whole story going from there very far into the future, very far into the field of simple seaster algebra. And there's a, if the, there's something called Jiang sulgebra. If the tensor product of a given one with that is it doesn't change it, then the k zero and k one and traces classify it. Okay. They tell you what it is if it has to. If you have a, it has to be what's called an amenable algebra. And simple, but otherwise.
00:24:09.944 - 00:24:44.634, Speaker A: Okay, so this is something that it's not in the book because the book is too old. Okay, well, the book has af algebras, which was the very next step after Gwyn UhF. All right, so see if I can find where I was. Okay, that's easy. By the way, Jang is j I a n g Su is sql. I've been in China a number of times, and there's Shanghai. It's a whole province.
00:24:44.634 - 00:25:00.914, Speaker A: And then north of it is. What do you suppose the next province is? Jiangsu. Okay. The Shanghai. We have had a meeting, been at a lot of meetings in Shanghai. Very important center for sister algebra. So it's a, it's well located.
00:25:00.914 - 00:25:44.514, Speaker A: Okay. And by the way, in physics, having two algebras close together, that's how you join the physical system. So somehow, Shanghai and Jiangsu, it's almost as if they form a joint system, which is okay. But sometimes my imagination gets better on me, I'm afraid. Okay, so she, she, she draw soon. She draws. Thank you, Lawrence.
00:25:44.514 - 00:26:00.478, Speaker A: Sorry, Larry. Fine. Amy Wang. Thank you. Congo Wang. Thank you. Shao dong.
00:26:00.478 - 00:26:37.774, Speaker A: Wei ji. Cam Wong. Thank you, Gg. Okay, thank you. All right, so we've already talked about matrix algebra. Algebra. Square matrices form an algebra over the field that forms the entries.
00:26:37.774 - 00:27:26.648, Speaker A: Entries with complex matrices, with complex entries. Then they form a complex algebra. In fact, it's seized algebra with a natural norm and star operations. But the classification theory for Bim algebras and even the general Af algebras, we don't need to do it at a siesta algebra level. We can just talk about pure algebra. Looking at the finite conventional matrix algebra, as algebra, just algebra, infinite direct sum. And then you have, you have two such algebras.
00:27:26.648 - 00:28:07.034, Speaker A: Finite direct sum matrix algebra. This is what battle he did. So he looked at finite direct sum of matrix algebras. Then you mapped it in some way into another finite direct sum matrix algebra. Maybe with, doesn't matter so much, but maybe with an injective map. So it's larger algebra. And then another larger algebra again, which is a finite matrix algebra of various sizes, probably most likely the size getting larger.
00:28:07.034 - 00:29:01.224, Speaker A: If there's only one matrix algebra at each place, then the size has to get larger or the algebra has to stay the same at every stage. I mean, that's just saying. If you have a sequence of numbers, natural numbers, it either has to get strictly larger all the time, or it's the same arbitrarily large. Arbitrarily large. If it's real numbers increasing, it doesn't have to get arbitrarily large without continuing without natural numbers. Then if it doesn't get arbitrarily large at increasing sequence, then after a certain point, it's always the same. Okay, but, so that's how, well, but then it's finite sums.
00:29:01.224 - 00:29:30.674, Speaker A: They don't always have to get large. The inductive limit is simple. In the case it's simple, meaning no non trivial two sided ideals. Then the individual matrix algebras have to get large. At least if this is unit will map. So this is taking a unit into the unit and so on. Let's assume that.
00:29:30.674 - 00:30:18.164, Speaker A: Okay, um, well then, then you have increasing sequence of algebra. Just like an increasing sequence of sets. It makes sense to take the limit, pass to the limit. Okay, the infinite union of increasing sets. Now, strictly speaking, you have to give a definition of it. If it's an increasing sequence of subsets of a given set, of a fixed, of a set, you know what it is, then, then you can just take the, you don't need to define, don't need to construct the union of the increasing sequence. It's just there, okay? That's one of the, that's how set theory works.
00:30:18.164 - 00:31:28.986, Speaker A: And if there were something called a larger set, there are larger sets that contain everything. Then you're all your increasing sets along the way, they'd be contained in this big, big set. So you could take the union in the usual way. But you can't do that. We discussed earlier why there's no such thing as the largest set, right? Because if having being a set means you have a cardinal number and you can do cardinal arithmetic, and whenever you take an exponential of one cardinal, one cardinal, not equal to one or zero by another one, you get something which is strictly larger, so there's no largest cardinal. So, but let's, let's not worry about how you construct the abstract, the union of an increasing sequence of ceps. By the way, it's the same problem for the disjoint union.
00:31:28.986 - 00:32:45.458, Speaker A: And maybe people are even more familiar with the disjoint union. And in fact, if you, I guess you could probably use the disjoint union to construct the union. Let's not worry about that. So that's what these algebras are. They're the Af approximately finite dimensional algebra, the non sea star version. Let's take the union, because once you have the union, finite stages have all have compatible algebra structures. So it's pretty light work to extend the common finite stage algebra structure to the union.
00:32:45.458 - 00:33:40.076, Speaker A: Okay? Because any elements, algebraic operations involve finitely many elements, and the finitely many elements always belong to a certain finite state. You already know how to operate on them, combine them, and it doesn't matter which finite stage you take, because the finite stages are compatible. All right, so what did Glynn do? Am I on the camera? Okay. All right. Yeah. Okay, I go this way, this way. Things are improving.
00:33:40.076 - 00:34:52.424, Speaker A: The first day I had the feedback, but that seems to have got ticked. All right. So I was going to say, what did Glim do? Well, did an enormous number of things. He did fundamental seast algebra work in a number of directions. And then this was his PhD thesis, the UHF uniformly hyperfinance, that's way of expressing the union of finite dimensional. So he did this, and then he, this was his thesis. Then he did something, he said a lot of fundamental work on what are called type one Cs algebra, which if you look at UHF algebras, if you include the finite matrix algebras, and those are type one, it turns out, and the otherwise, otherwise they're not type roman numeral one.
00:34:52.424 - 00:35:59.964, Speaker A: That's a fundamental bifurcation in mathematics between them. So for seast algebras, if it's commutative, then it's type one, if it's simple and not finite dimensional or not, the compact operators, and it's not type one. So he had a definitive paper on type one CSR algebra, finding a whole lot of equivalent positions, quite surprisingly different. One of these algebra referred to me in the class of pipeline algebra, and then he, with Arthur Jaffe, he collaborated in a series of papers on quantum field theory using sea star algebra. Construct quantum field work with me and how many people have studied physics? Okay, well, indirectly, if we were studying mathematics, we're studying physics. Okay. But it's very.
00:35:59.964 - 00:36:51.138, Speaker A: And recently, in recent years, well, maybe for a long time, mathematics has boiled over into physics. I mentioned earlier, maybe the first day, one example, the not polynomial of the late mathematics skills medalist von Jones. It was first very ordinary knots. You could distinguish the knot in your left shoe from the knot in your right shoe, assuming that you did the mirror image knots. And that was the first time those two knots had been distinguished. Okay. Well, there was no way of visualizing it.
00:36:51.138 - 00:37:36.432, Speaker A: An inductive way of calculating it using. Projecting a knot onto a plane, keeping track of when two lines cross in the plane, which one is on top. Okay, so that was not the crossing to keep track of the crossings. And you can compute the polynomial. Well, whereas something called the Alexander polynomial, it was very easy to understand geometrically, and it didn't distinguish these two left and right knots, left in right versions of the not. But the. By the way, I have a left right comment in my email.
00:37:36.432 - 00:38:06.364, Speaker A: Right. That the left module construction of k zero gives you the same answer as the right module construction. And the way you prove that is to show they're both equivalent to doing it with idempotence. Item potence are. There's no left right. Okay, so, um. Well, what Witten did, and he got a fields medal for that part, for that is, um.
00:38:06.364 - 00:38:35.314, Speaker A: By the way, some of you probably know this story as well as I do, but what he did was find a quantum field theory. We're talking on the subject of quantum field theory. We found a quantum field theory method of calculating the Jones polynomial, okay? And it was quite simple. You have a. You don't have to project the knot into a plane. It's just a sitting in three dimensional space. Okay? But it's a curve, a closed curve in three dimensional space.
00:38:35.314 - 00:39:01.354, Speaker A: Well, three dimensional space is full of a quantum field. So, roughly speaking, you just integrate the quantum field around the knot and then take the trace. You get an operator. So you take the trace of it. The trace is what we're going to be talking about with UHF algebras, by the way. That's the secret of classifying UhF algebras, because the trace computes the k zero group. Okay, that's.
00:39:01.354 - 00:39:18.046, Speaker A: That's not how Dixmier said it. It was almost what he said, but he. He referred to the Murray. He said that. He pointed out that the, um. The, um, trace computes the Murray phenomenon semigroup for uhfl. Trace of an idempotent gives you.
00:39:18.046 - 00:40:04.272, Speaker A: The two projections are Murray phenomenon equivalent, if they have the same traits, okay. And the same phenomenal traits. By the way, what are the UHF algebras where you take, um, that's the case where, where all these algebras are just a single irreg summoned. Okay? And because if you have one finite dimensional algebra contained in another, and it's just the same as, if the same as the second one is, the first one is mn one. The second one is m. N one time utensil, m. And no, it's definitely going to be the first way you see it is, it's a larger matrix algebra.
00:40:04.272 - 00:41:02.942, Speaker A: If the occlusion preserves the unit. And with the larger matrix algebra, which is, which is m, sub m, then m is a multiple of n one. So it's n one times n two. Okay, but then you have any matrix algebra which over not, it's not a prime number, the order is not prime, but you call the order or the size. If it's not prime. If it's a product of two numbers, then what can you say? Because if you have mn one and two, then you could ask about how does that, how is that related to, well, first of all, it will contain, it will contain mn one, but how will it be? And it will contain mn two, but how is it related to the both of them together? Anyone hasn't studied this? Anyone who doesn't already know it wants to tell me. That's a funny question.
00:41:02.942 - 00:41:26.962, Speaker A: Right. Okay. All right. Well, it's isomorphic to a very canonical construction coming from mn one and mn two. And by the way, how does it contain, well, it's just, okay, I'll just say it right out. Mn one, tensor as an algebra. Mn two, tensor product algebra.
00:41:26.962 - 00:42:16.354, Speaker A: You take the tensor product vector space and then the natural product, you extend the product in a natural way, coordinate wise, component wise product, you have two elementary tensors. Then you multiply the first components and multiply the second components and take the tensor product. That's that. And then you extend it by linearity. That's the tensor product algorithm, and it's a larger, you get it's isomorphic to a lot of matrix algebra that contains, what's the, what's the inclusion, what's the inclusion? Well, if you look at Mn one tensor, the unit of all things a little, a tensor one, that's a sub algebra. That's a subset of the tensor product and it's a subalgebra and it's isomorphic to its, well, it's equal to mn one tensor one. Okay, which is isomorphic to mn one.
00:42:16.354 - 00:43:03.402, Speaker A: Is that clear? Okay, well, what Gwim did was look at an increasing sequence, a single matrix algebra, at every stage. But what I, way I want to look at it is look at it as an increasing sequence of finite tensorflow. The second 1st matrix algebra is mn one. The second one is tensor product of two because it's larger, and the third one is even larger. So you have mn and a three. Okay? And then you just keep on going. Okay.
00:43:03.402 - 00:43:45.736, Speaker A: And then you take the sort of, those dots mean you take the union. Okay? Make sense? So that's, that somehow already constructs the union, increasing union. Okay? Because you say you can define it. You take, all, of, you take all finite tensor products, a one tensor, a two tensor, a three tensor one, and then, and so on, you can always repeat the one. You can repeat infra, any ones. That's how a tensor product, that's how the infinite tensor product is defined. Infinite tensorflow, vector spaces.
00:43:45.736 - 00:44:13.300, Speaker A: If you take a fixed vector of each vector space and you can find the infinite tensor product by saying it should tail off finite the number of components and tail off with the same one later. Okay, that's the infinite tensor product. Yes, yes, yes, yes. Of course. If they're. Well, I'm just getting to the prime. I said that I'm getting to the prime numbers now.
00:44:13.300 - 00:44:37.000, Speaker A: Okay, that's a very good question, because we've seen here that product is very important. Right. By the way, you don't, you don't want too many zeros. Well, as soon as you have a zero, then all bets are off. Okay? Because it's a zero algebra, and so everything will collapse. But you don't mind having a one, the n one. Each n could be one, because that gives you a one by one matrix, which is fine.
00:44:37.000 - 00:45:11.196, Speaker A: Doesn't, you don't see much. You don't see them very much, but they're there. Okay? And I, as I said, William didn't, he didn't include the finite dimensional matrix algebra, the individual one, where at some stage would all be, after some stage will all be m one. Okay? So I want to include the ones where you take m one. Always after some stage, include those. But in general, not, I mean, most of them will not have the arbitrary sequences of numbers, but you. Exactly.
00:45:11.196 - 00:47:18.388, Speaker A: But the point is that this sequence of numbers is not unique, because the, it's only the finite tensor product, only the product n one, n two. That's important if the individual numbers are not important. So the point is that you use, every natural number has a prime power factorization. Okay? And that's unique. And so if you have these infinite tensor products, an arbitrary infinite tensor product, an arbitrary infinite tensor class, there's a canonical way to express it, okay? Which takes into account that if you just do it in a random way, then the orders of the matrix algebra components are not unique. Okay? So what do you do, by the way? Why am I talking about different ways of expressing these algebras? We've already said what the class of algebras is that we want to, that we want to talk about. Right? And the glimm's theorem, we can state Wimm's theorem now without worrying anymore about analyzing the algorithm, because assuming we have digested the definition of k zero as a group, then if we have a and b are uhf, okay? And a isomorphic to be taking the unit to the unit, this is equivalent to saying k zero a is isomorphic, actually, k zero.
00:47:18.388 - 00:48:03.424, Speaker A: Yeah. K zero a is isomorphic to k zero b, except that we want them, we wanted to take the ISO. There's a special class of the canonical class. If we define in terms of modules, then you have the algebra itself. Okay? And if it's defined in terms of item potent, you take the unit, and both cases, that's a special element of k zero. So we'll take, we'll take, we could just call it the special element k zero a, and then star, if you like, is isomorphic. So I could go, I can call this the class of the unit, or I could call it this, the algebra a itself as a left ideal.
00:48:03.424 - 00:48:38.974, Speaker A: All right, then that's isomorphic. In other words, as a group isomorphism, taking the special element to the special element. Okay, so that's the theorem. That's Finn's theorem stated in modern, little more modern language. Okay? Yes. Are those. Well, don't forget that it contains the tensor product.
00:48:38.974 - 00:49:29.608, Speaker A: Well, okay, well, that's a good question that comes up with direct sum already, right? When we're defining the k zero group, you take direct, start off with modules in our setting. Then you take direct sum, abstract direct sum. Now, that is not commutative order. If you take ordered pairs in one set with the first component, one set, and second component in the other, it's not the same as reversing it, right? Different. It's apples and oranges. But if you talk about isomorphism, then a direct sum b is isomorphic to b, direct sum a. Okay, but, but we're not talking about isomorphism classes, okay? We're just talking about constructing algebras.
00:49:29.608 - 00:49:57.112, Speaker A: It's a good question. But, but it's don't, don't read too much into the construction the first day. Okay, so this is just a construction. You get an algebra. It contains all the individual things, so it's non abelian unless all the n's are one. Okay? But then, so it's a non commutative algebra. But then if you, it turns out you can classify these algebras in a very simple way.
00:49:57.112 - 00:50:53.064, Speaker A: There's a, in fact, the, this is true. And one reason to study the things more is that the, in fact, the caso groups are subgroups of the rational numbers. They're subgroups of the group of rational numbers containing the number one. They contain the number one, okay? And in fact they're isomorphic in a way which preserves the number one if and only if they're equal. Okay? So that's, anyway, I hope I'm not saying too much too fast. This is, and if you look up, glimpse paper, it won't quite look like this. Okay? Geeksme's paper will start talking about subgroups of few, but he doesn't talk about the case of a group, whereas he does talk about marine Quilber.
00:50:53.064 - 00:51:37.174, Speaker A: So, but I, this is a very important term. It started, it was one of the fundamental theorems in the whole subject of operator algebras, which is a pure subject, and it's a pure mathematics subject studied for its own sake, but also for many applications. I think I mentioned the, not so much an application to theoretical computer science, quantum computing, but the other way around. So a clearly extremely close connection with a lot of fields of mathematics. All right. But, but there probably be someone coming in. Thank you.
