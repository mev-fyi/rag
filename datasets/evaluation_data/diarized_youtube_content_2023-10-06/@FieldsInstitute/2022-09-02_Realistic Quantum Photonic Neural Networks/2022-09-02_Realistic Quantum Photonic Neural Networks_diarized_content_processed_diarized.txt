00:00:00.560 - 00:00:44.710, Speaker A: Hi everyone. It's great to be here. I've enjoyed listening to all the talks so far and I'm excited to tell you all about what I've been working on for the past year or so at Queen's University with the Quantum Nanophotonics Lab and Shastri Lab. So I'm just going to get right started and start telling you all about these networks that can learn to become fundamental components of emerging quantum technologies. And in fact, they can learn to do all sorts of things. They have reconfigurability and parameterization built right into the design, allowing them to perform tasks in quantum simulation, communications and computation. And even outside of the quantum regime, they have potential applications in image processing and even things as abstract as fraud detection.
00:00:44.710 - 00:01:48.380, Speaker A: However, there is one caveat to all this, and that is all of these proposed applications have been proposed assuming that the networks are operating ideally. And that's where I came in, because I was fascinated by the idea of these networks and all their potential applications. But I wanted to know if this was all too good to be true or if we can take something realistic from this. So to start looking down that path, we can establish some criteria for practical quantum photonic neural networks. The first of those being low latency and ultra low operational powers as necessary for all of those previous applications. And the good news is, by choosing the photonics platform, we've already met these criteria as we can leverage the maturity of photonic integrated circuits. Now, the most important criterion to consider is that these networks need to maintain the efficient operation with realistic imperfections, including photon routing imbalances, non uniform photon losses, and most significantly, weak nonlinear photon photon interactions.
00:01:48.380 - 00:03:02.294, Speaker A: So, keeping all of this in mind, this is the roadmap for my talk today and for understanding realistic quantum photonic neural networks. So I'm going to draw from an example in quantum communications, keeping in mind that in principle, if these networks are realistic for one application, they have the potential to do a lot more. I'm going to start by looking at the network architecture and how they are trained, then add some relevant imperfections and ultimately analyze the realistic performance. So, starting off here with the example, this is a diagram of a two way quantum repeater where the idea is to establish a communication channel between these two people. Now, the way this is done is each of these four purple quantum repeater nodes generates a pair of entangled photons and sends them in either direction to Bell state analyzers that perform what is called entanglement swapping. So the idea is that two photons come into the Bell State analyzer, and it projects it to the computational basis, and that creates this widespread entanglement between the photons on either end here that ultimately establishes the communication channel as desired. So the most important part of this scheme is the Bell state analyzer.
00:03:02.294 - 00:03:34.162, Speaker A: So I'll take a closer look at that here. By definition, this component is designed to project the maximally entangled Bell states to the computational basis for measurement. And more specifically, it allows you to perfectly discriminate between all four Bell states. And one other thing to note is that if you run it in reverse, you can also use it to generate Bell states. But keeping projection in mind, one thing here is that all these states are written in terms of qubits. And I've been talking about photons. So one way that we can encode these photonic qubits is with dual rail encoding.
00:03:34.162 - 00:04:22.380, Speaker A: So for each qubit, we have two spatial modes or two wave guides. And when the photon's in the upper mode, you have your zero state, whereas if it's in the bottom, you have your one state. And as an example, we can see for this uppermost input output pair what these states look like in terms of the photons. Now, to quantify the performance of the Bell state analyzer, we can first label it with the system transfer function S, and then taking S and applying it to an input state that produces some output. And we simply compare what is produced to the ideal output that we're trying to generate. And doing that, over all four input output state pairs, we have the unconditional fidelity. And what this is, in other words, is basically the chance that you get the correct output for any given input.
00:04:22.380 - 00:05:20.620, Speaker A: And the important thing to note with this measure that we chose is it includes imperfections intrinsically within it. It's diminished as a result of imperfections like loss and everything like that. So it gives you the full picture of how often your network is actually working. So the natural question to follow this is, how can we realize the Bell state analyzer using these photonic architectures? And one approach is with a linear programmable nanophotonic processor as imaged here. However, back in 2015, when this one was fabricated, they were only able to achieve an unconditional fidelity of 0.466. And in fact, even if this network was completely perfect, this linear optics was completely perfect as designed, you can only get to an unconditional fidelity of 0.5. Now, the reason for this is there's no interaction generated between the photonic qubits, and you're unable to realize a deterministic Bell state analyzer as a result.
00:05:20.620 - 00:06:13.044, Speaker A: Now, keeping this upper limit of 0.5 in mind, we can zoom back out to this repeater diagram. And what we notice is that if you needed, for example, 10 Bell State Analyzers to realize this scheme, the success rate of the overall scheme drops to 0.1%. Now, obviously that's not practical for this technology, but if we use an ideal quantum photonic neural network, we are able to realize this deterministic Bell state analyzer by adding that nonlinear piece that creates the necessary interaction. And while this is great, it doesn't really tell us what we want to know because it's saying that it's going to be perfect. And we know nothing in life ever is. So what we really want to do is look at the realistic version of the network and fill in this gap in between to see if we can reach some non negligible success rates for technologies like this.
00:06:13.044 - 00:06:52.172, Speaker A: And that's what I'm going to build up to here. Starting off with the network architecture. So what we have here are two meshes of maxenter interferometers where each interferometer consists of two phase shifters and two 50, 50 directional couplers. Now, the idea of these interferometers is to generate interference between the photons. So we can see, sort of by this cartoon, that one photon comes in and then in superposition, they have some probabilities of moving throughout the networks, throughout the network in different routes. And you eventually have some probability of having one or two photons in each mode. Now, this is just the linear piece.
00:06:52.172 - 00:07:28.750, Speaker A: And naturally for a neural network, this gives us the linear layers of the network, but it's still the same as what I showed before. That linear circuit. What we need to add to it, to give it the interaction between the qubits is this nonlinear component. So in this case it's a Kerr nonlinearity. And the idea is that if one photon comes in, it leaves with no phase change. But if two come in, they each leave with a phase change of phi. And this is the piece that creates that necessary interaction between the qubits to realize a deterministic Bell state analyzer.
00:07:28.750 - 00:08:32.400, Speaker A: And it's also analogous to the activation function of a conventional neural network and thus provides the learning capabilities and allows us to train it. Now, speaking of that, what we can do is again call this entire network by S and then define the network error or unconditional infidelity, which is just the the opposite of the fidelity. And the idea is to minimize that by varying all the phase shifts throughout all the interferometers in the network, and that's how it's trained to do a specific task. So with the architecture and the training methods in mind, we can now turn to incorporating some relevant imperfections into the model. Starting off with those that arise due to fabrication, and most specifically, how photons may be lost as they propagate due to scattering or being absorbed as they propagate through the network. And we treat more realistic losses by treating them as non uniform. So as we can see in this image here, the losses are centered at some value, alpha wg.
00:08:32.400 - 00:09:11.020, Speaker A: But for each max enter interferometer throughout the mesh and throughout the network, there's a slightly different value of loss chosen. In this example you can see it's more likely to lose a photon traveling through the upper part of the mesh compared to the lower part. And another thing to note here is that the state of the art for silicon on insulator at 1550 nanometers is 0.3 decibels per centimeter of losses. We also consider imperfect directional coupler splitting ratio. So ideally all of the directional couplers are going to be 5050 in the network, but in reality you might have 4753 for example, or some other imbalances. So those are also treated in the model.
00:09:11.020 - 00:10:05.014, Speaker A: So now with this in the model for the networks, we ran many simulations for a variety of losses. And these are some of the preliminary training trials for a two layer network. And just to show how this figure works, we can follow the pink line, for example, for state of the art losses, and see that the network at the beginning of the training, the network error is around 1. And then as we work through the iterations and the parameters are varied, that error decreases by order of magnitude to where it converges at the pink crosses there. Now, as expected, we see that the network error that can be achieved decreases for lower losses. But something that's more unexpected is this variation in the solutions for losses at 0.01 decibels per centimeter, as more typical of silicon nitride platforms or better.
00:10:05.014 - 00:10:43.360, Speaker A: And we can only assume that these that this variation is due to the fact that the losses are now low enough that it's the imbalances due to the non uniformities that are dominating the imperfections here. Whereas for state of the art losses are worse. It's just the sheer unavoidable losses that are dominating. If we increase the network size now to four layers, we see that variations start to go away. And the reason for this is twofold. First of all, by Increasing the network size, there's a higher probability of losing the photons. And also there are more parameters to learn with, so the network is better able to account for the imbalances.
00:10:43.360 - 00:11:31.620, Speaker A: Now this is interesting, but it doesn't tell us the full story. What we really want to know from this piece of the analysis is how does the network work or how efficient is it as a function of the losses? And that's what we can see here for a two layer network in a couple of different ways. The first is offline training. So the idea here is that you have a chip and you perform a simulation off the chip, assuming that everything is ideal, and then upload that solution to the chip. This is compared to in situ training where you'd be training on chip in the presence of all the imperfections and imbalances. Now, the in situ results are actually what are shown here in the training trials. And a training trial is deemed successful if it's better than the lower bound of the offline curve.
00:11:31.620 - 00:12:46.594, Speaker A: And it's not, as we can see from the results on the right here, it's not only better than the offline curve, the in situ results are able to jump up towards the loss limit, where the loss limit is the best that we can do. We, we're always going to have some unavoidable photon loss in the circuit, but the loss limit is the case where all the losses are balanced and all of the directional couplers are 50, 50. And just to highlight this learning here, I've given the numbers for an example for state of the art losses where the fidelity is around 90% in the offline training method. But when learning in the presence of all the imperfections and how to account for them, that fidelity increases to 96%. Now, to summarize these results, we can look at the fidelity as a function of the network size for all the losses shown in the color bar. And most notably, we see that with current currently available technology and state of the art losses greater than 90% fidelity can be maintained for network sizes up to six layers. Now, in the future, as losses are improved or silicon nitride platforms become more common, for example, we also see that the base size of two layers may not be optimal.
00:12:46.594 - 00:13:34.820, Speaker A: It may be better to have larger networks that better account for other imbalances and imperfections. So that concludes the look at losses and imbalance and the fabrication imperfections. But it leaves the most important consideration, namely the weak nonlinearities. So if you recall this current nonlinearity that I mentioned before, we have this phase shift in the two photon case. Now, ideally that's a PI phase shift, and unfortunately that's something that has yet to be achieved for a single photon Kerr nonlinearity. It has been demonstrated for other nonlinearities, such as electromagnetically induced transparency. However, these nonlinearities are typically incompatible with most quantum information processing because they contribute to excess photon loss.
00:13:34.820 - 00:14:29.532, Speaker A: Now, more compatible approaches include chiral scattering from quantum emitters and integrated nanophotonic cavities, both of which have been theoretically proposed to achieve effective nonlinear phase shifts approaching PI. So we come to the conclusion that we're going to get close to PI pretty soon, but we're not going to get all the way there. So we want to know what's the weakest nonlinear phase shift that we can still maintain efficient operation with. Now, to start answering that question, we can look at the fidelity as a function of this phase shift again for the offline training, in situ training and showing the loss limit again. However, now the in situ training results have been broken up into the mean of all the successful trials and the best case. So here on the right, I've given an example for PI over 2. And we can see that there's a clear separation in these plateaus in the successes of the training.
00:14:29.532 - 00:15:24.826, Speaker A: So the best case is isolated. And when we do that, what we find is this unexpected monotonic non monotonic relationship in the best case in situ result with an additional peak at PI over 2, which is half the ideal non linearity. So the caveat is that there's a separation between the mean and the best case. So where it's much harder to train the network to the best case result. Um, but that's a small price to pay considering that you achieve the same loss limited performance as you would with the ideal nonlinearity once you find that solution. And again, we can see the learning from the in situ training where the For PI over 2, again the offline fidelity is around 56%, which increases to 96% when trained in situ. In the best case, which is within 0.5%
00:15:24.826 - 00:16:31.120, Speaker A: of the loss limit result. We can look at this plot similarly for a network of four layers and see that of course, as the network size increases, the loss limit lowers because there are more losses, we see that the trainability improves, the mean curve tends to approach the best case, so it's easier to get to the best case when training the network. But most significantly, there's a much wider domain in the effect of nonlinear phase shift that reaches loss limited performance. Namely in this case, all the phase shifts greater than or equal to PI over 4. Similarly, to summarize these results, we can look at the fidelity for the in situ best case as a function of the network size. And what becomes evident in this figure is that for a given amount of losses and a given nonlinear phase shift, there is an optimal network size. If we follow the red triangles, for example for PI over 4, we see that an increase infidelity from 2 to 3 layers up to loss limited performance and then adding subsequent subsequent layers after that only works to hinder the performance and decrease the fidelity.
00:16:31.120 - 00:17:27.530, Speaker A: So with this in mind, we can map out the optimal sizes for each nonlinear phase shift down to as low as an order of magnitude less than the ideal phase shift at PI over 10 where loss limited performance and greater than 90% fidelity is achieved for a network size of six lessons layers. This doesn't continue forever unfortunately as we see that non linearity as Weakest PI over 100 essentially acts as a linear optical Bell state analyzer, maintaining fidelity around 0.5 regardless of the size. So that concludes this roadmap. But there's one more piece I want to go to and that's that I've treated the network entirely as a black box thus far, but it can be interesting and revealing to take closer look inside. So this is a diagram of a two layer network with a nonlinear phase shift of PI over 4 and state of the art losses. And it's been colored to show how the photons are routed and interfered throughout the network.
00:17:27.530 - 00:18:16.940, Speaker A: Where orange shows the probability of having one photon in a given mode, while purple shows that for two photons. And what we see is the Bell state is taken in at the input, but towards the middle there there's this reddish color and that is showing that there's some probability of having one photon in each mode and some probability of having two. And because of the principles of quantum mechanics, both of those cases are acted on at the same time at the nonlinearity. So the network has learned during the training how to route and interfere the photons in such a way that it's getting the most out of the weak resources that it has available to it, eventually achieving an unconditional fidelity of 0.825 in this case. Now we're not at the optimal network size for this non linearity, so there is 13.8% left over to some of the other imperfections.
00:18:16.940 - 00:19:33.788, Speaker A: And one other thing to note is that even if you sum those, you still have another piece left over that will always be there, which is the unavoidable photon loss. So to recap everything Realistic quantum photonic neural networks perform near deterministic quantum information processing. They're reconfigurable to a variety of applications and built on the mature photonics platform. They learn to account for fabrication imperfections such as imperfect directional coupler splitting ratios and non uniform photon loss. And most importantly, they can still optimize in the presence of weak nonlinearities. So now, as promised, we filled in the middle part of this graph from early in the presentation, where we can see that for a nonlinearity as weak as an order of magnitude less than the ideal at the optimal network size, there is a success rate for the entire scheme of around 37%, which is over 300 times what is possible even with perfect linear optics. And one other thing to note is that if you have a scheme that uses fault tolerant protocols and can be conditioned on the success of measuring photons in specific modes at the output, all of these success rates increase to above 0.999.
00:19:33.788 - 00:19:51.780, Speaker A: And if you would like to know more about that or look more at the analysis, you can check out the paper we recently put up on the archive. And with that altogether, I just want to reiterate how these networks have the potential to realize some of the key components of emerging quantum technologies. Thank you all for list.
