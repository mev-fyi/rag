00:00:00.360 - 00:01:01.714, Speaker A: Discussion or whatever. I mean, I know that, for instance, last lecture have been a little bit fast. So if you feel a little bit lost, just, you know, tell this and we can discuss. Right, so where were we on with this Monday? So, Monday I discussed the identification of the two notions of sovereign functions that we discussed. I presented, I've stated Kuvada's lemma and I showed how this plays a key role in this identification and also in the identification of the heat flow as gradient flow, both of the Chiga energy and of the relative entropy. Then I did not really prove Quadas lemma, but I showed how that how the statement follows from statement about Hoflux formula and David Hogov equation, okay, which is a purely metric statement, not metric measure statement in some sense. And the statement was the following.
00:01:01.714 - 00:02:06.002, Speaker A: And this is a statement. So in the general matrix setting, this is due to Ambrose, savare and myself, and independently by Goslan, Roberto and Samson. So there is a little bit of, I mean, the only difference between these two discussions is that they assume the space to be locally compact. But this assumption is in fact easily seemed to be not relevant. And the theorem tells the following. I mean, I give you a statement, I mean the same that they gave on Monday, which is sufficient for Kuvada's lemma, although this template is quite robust and also more generally xd metric. I don't think you need anything f, say lip sheets and bounded on x.
00:02:06.002 - 00:03:34.164, Speaker A: So from x to r. And let me define qtf well, at x, I mean write this weight. So this is the infilmum over y's of this guy, where this is nothing but f of y plus distance squared x y over two t. So this is the half flux formula. You know, if you, if you are an Aldi industry, then say q zero f equal f, then the ellipsis constant of qTA is bounded by, I guess something like twice the ellipsis constant of f, or maybe four, some constant, okay, should be two. But let me put four the map that takes t and returns qtf has a map with value. So it takes values on Cbx is absolutely continuous and is actually lips.
00:03:34.164 - 00:04:24.118, Speaker A: So this space has the subnorm. Okay, so Lipschitz means, you know, lips. Now in particular, given that, I mean, if I'm leaptist with values in the subnome for every x, the map that takes x and return, sorry, the map that takes t and runs QTf at x is leap sheets, you know, for, for trivial reasons. And I can wonder what the derivative is. And this statement is the DT sorry. QTF plus the asymptotic ellipses constant squared of QTF divided by two at x. This is less or equal than zero for almost everything, right? So that's the steps.
00:04:24.118 - 00:05:27.812, Speaker A: Okay, it's a bit rough. I mean like one can improve a little bit. This statement, for instance, is almost every, in fact is for every t except accountable number or something like that. But you know, for us this is sufficient, right? So in some sense, if here I had a quality, this would really tell me that in general, the hoplux formula produces solutions of the Hamilton Jacobi equation, right? In some sense, in some metric sense, proof. So, it is useful to let me introduce the following two functions. D minus tx and d plus dx. This is the imp of the living watt of distance x y n among all yn's minimizing sequences.
00:05:27.812 - 00:06:10.138, Speaker A: Minimizing for ftx. And this is just the soup of the helium superfuel the same way. Okay, so imagine, so what am I saying? Imagine the space is compact, so minima of f dx exists. I could have possibly multiple minima, okay? And d minus is the least distance between x and the minimum. And d plus is the maximum distance between x and the minimum. If I don't have compactness, I don't know if the minimum exists. I have to deal with minimizing c.
00:06:10.138 - 00:07:44.444, Speaker A: Okay, so here are a couple of trivialities. So first of all, let me bound, let me bound the plus. I claim that this guy is less than. So claim if you want, this is less or equal than twice t. The ellipse is constant of f uniform. Okay, and why, why is the decays? Well, I guess if yn is minimizing, so what it is that I, so say that yn is minimized, okay, by diagonalization. This soup is a max, okay, so say that when is a sequence, realizing that this the d plus, so I have that f of y n, I have the lim soup, if you want, of f y n plus distance squared x y n over two t less or equal, then f of x, right? And certainly, and certainly this guy of course is also greater or equal than if you want f of x minus t times, sorry, minus the ellipsis constant of f times the distance between x and y n.
00:07:44.444 - 00:08:30.654, Speaker A: Okay, limb soup plus this guy. Okay, so what I get, what I get is that, is that, what is that I get, I'm getting a little bit messy. So, so I get the distance square. So d at the end, I get this, the dt plus of x squared over two t is less or equal than the ellipsis constant of f times d plus x, which is this. Does it make sense? This guy, you know, this guy has a limit. This conversion to d class. Okay, so this limb soup.
00:08:30.654 - 00:08:49.582, Speaker A: So also, this is a limit if you want, as a limit, because the sum converges to qtf. Okay? So this is a limit. This is a limit. I'm bounding this from below by this guy. Okay? I can play with the super old limit. So I have no problems in carrying left and right with a minus. I get this, which is.
00:08:49.582 - 00:09:27.828, Speaker A: This makes sense, by the way, from now on, given that. So if I, if I write minimizing sequences, I, you know, I make a mess in the blackboard. I will cheat and work just in the compact case. Okay? All the proofs are, you know, easily adaptable if you replace minima with minimizing sequence without, without basically any effort. Okay? The only thing that basically you have to observe is that whenever you have minimizing sequence, if this distance has a limit, then also the function has a limit. And so you can, you know, split limb soups and lemon, everything is additive. Okay? So that's the first remark.
00:09:27.828 - 00:10:13.074, Speaker A: The second claim is the following. The second claim is that. So, f edx minus qtf at x, this is always greater or equal than zero and less or equal than, I guess, something like four times t. Delete this constant of f. Okay. For every x. Okay, now these inequality is obvious because in the definition of q, so qt is clearly a decrease in function, right? So, so, because f t, if you let t increase capital ft decreases.
00:10:13.074 - 00:10:28.942, Speaker A: So if you take the infancy that is smaller than something else, the inf will be. That's right. In particular here, if you want, you can always pick y equal x. Yeah. That's an acceptable competitor. So y equal x. This disappears.
00:10:28.942 - 00:11:00.546, Speaker A: You get f of x, but qt is the inf. So qt is less or equal than f. Makes sense. So this is proved. Now, what about this? Well, we've just proved, we've just proved that this inequality. All right. Now, this means that, in fact, the QTF at X is the inf, not among all Y's on the whole space, but just among all the Y's in the ball centered at X and radius.
00:11:00.546 - 00:11:46.856, Speaker A: Say something a little bit bigger than this. Say, for instance, I don't know, four times or three times. Four times t lip f. Make sense of capital f. Actually, let me write this. F of y plus distance squared xy divided by two t makes sense, right? There's no need of going behind this ball, because any minimizing sequence will eventually, you know, you know, be in, okay, but that was the point. Look, just, so, so let me, so for, let me, let me state it differently for, so fix x bar.
00:11:46.856 - 00:12:35.214, Speaker A: And notice that, what am I proving? No, sorry, I was going to prove something else. Let me think 1 second. So this is true but irrelevant for this purpose. So let me, so I mean, sorry, I'm, this is true but irrelevant. Let me actually observe the following f of y. So this difference, so qtf lx. So this is, let me put this way, this is equal to the soup in y of f of x minus f of y minus distance squared xy divided by two t.
00:12:35.214 - 00:13:31.354, Speaker A: All right, now this is less or equal than, so the soup over y of the elliptic constant of f times the distance between x and y minus distance squared xy over two t, right? Okay, but now, now this is a parabola, right? So, so these, so the soup, the soup over z in r of l z minus z squared over two t. This, we know what it is. It's something like tl or two tl, something like this, I mean we take the derivative, take the maximum. So, so whatever is, so you see here, this is super. Yeah. This z is the distance between x and y, right? So this is proved some constant. Makes sense.
00:13:31.354 - 00:14:31.154, Speaker A: Okay, now, now actually the other claim, and this was useful for this. So the claim now is that, is that the lips is constant. I mean, one holes, such as one holes. This, okay, why one holds one also, because as I was saying, qtf can be realized not, but not just by taking the inf over the whole space, but just taking the info over some ball. More generally, more generally. For, let me put this way. So for fixed, say x bar, if I want to explore, and let me say x belonging to the ball centered in x bar and with radius, I don't know, something like t times the ellipsis constant of f.
00:14:31.154 - 00:15:23.578, Speaker A: Okay, we have that the q d f at x is the info over y belongs into some ball of radius, some constant, I don't know, for a, some number, t leapf universal number that I can compute. If only I were not lazy. And this is f of y plus distance squared xy over two t. Makes sense what I'm saying, right. For any given point, the inf is realized over the ball of radius number times t lip. Okay, now if I let this point vary over a ball of radius t lip f, we center in some given ball in some given point, up to possibly enlarging the number a little bit. This is what happens.
00:15:23.578 - 00:16:05.654, Speaker A: Okay. Now, why, this is why this is relevant. Because on a ball, you know, on, you know, look, the function y to distance scored xy over two t on the ball. On the ball, you know, b constant t lip f centered in x. This is. This is c prime Lipschitz, c prime. So this is, what's the lipstick constant of this thing? The reference constant of distance squared at a point is basically a distance from the base point.
00:16:05.654 - 00:16:38.554, Speaker A: Okay? So here there is a divided by division by t. But I'm looking to the ball of radius t times something. So this should be c prime lipo for some c prime. Makes sense what I'm saying. Is this true? Yes, it should be true. Okay. Right.
00:16:38.554 - 00:17:12.482, Speaker A: So if I scale things correctly, right? So the function. So look, look at the function. I mean, fix x. The function distance squared from x at a certain point y as lips is constant, which is bounded from above by the distance from x, right? If I divide it by two t, divide the local lips, is constant. It's bounded by distance from x divided by t. But if my distance from x is bounded by constant t, Lipschitz, you know, this t simplifies with the t denominator. And I get, okay, now, f is not a fixed point.
00:17:12.482 - 00:18:19.864, Speaker A: Is a point which varies over a ball of radius. Again, t times d function centered some point x bar. Okay, so, so, so, you know, and that, and that's also, you know, I can, if I take this big enough, this ball contains b t, what it is, say, b two, t, for instance, lip, f bar, x. Make sense? Okay. And now is the conclusion of the argument. Basically, I'm looking, I'm looking at this function in this ball, okay? Over this ball, this function is obtained by taking infinite in y of these functions. But these functions over this ball or this ball are uniform ellipses, right? So on this ball, this function is uniformly lip sheets.
00:18:19.864 - 00:18:46.148, Speaker A: Okay, makes sense. So the inf of a uniform elliptic family of functions is uniformly lip sheets. Okay? So perhaps this proves something. Let me put, actually, let me be more precise. I proved a local lip syncularity. So let me put this way. The asymptote is constant of this is less or equal than, I don't know, some constant lips of f.
00:18:46.148 - 00:19:06.644, Speaker A: Okay? In principle, in principle, if I don't know that. Sorry. In principle, if I don't know that the space is a, is a length space or a judiasic space. I don't know whether a local, if it's control, implies a global if it's control. But still. Okay, sorry. Question.
00:19:06.644 - 00:19:18.444, Speaker A: You should have some t in the denominator in this. Lipstick constants. Why don't you. No, I don't. I don't, vitaly. I don't. Because of this trick.
00:19:18.444 - 00:19:26.184, Speaker A: Because this trick over here. So let me repeat. Let me repeat the argument. So. Oh, sorry. I get it. Yes.
00:19:26.184 - 00:19:42.130, Speaker A: All right. I'm looking on a small ball, right? So there is a t in the radius of the ball. Okay? Yes, yes. Which, you know, can be as small as I wish. As far. You know, as far as I'm interested in the local. That is sufficient.
00:19:42.130 - 00:19:56.806, Speaker A: Of course, if I'm on a judisic space, you know, I can interpret. You know, I can integrate the local and get a global lips control. But, you know, for now. Okay. Okay, thanks. Sure. So, I proved this.
00:19:56.806 - 00:20:10.702, Speaker A: I mean proved. I gave an idea of why this is true. Now let's actually compute. So, let me see. Yeah, let's prove. I know. Okay.
00:20:10.702 - 00:20:46.974, Speaker A: Let's go on now and prove. Improve the actual oplux formula and the rest of the proof. So. And I need. I need some remarks. So, clean the map. You know, X into D minus TX is lower semicontuous.
00:20:46.974 - 00:21:20.984, Speaker A: X to D plus tx is upper semi continuous. And this is triggered by the organization. Okay, if you. If you have. Imagine in the compact case, imagine you have, let's put the upper semi continuity. Imagine you have xn converging to x and y n minimizer or Ftxyn. Imagine that everything is compact.
00:21:20.984 - 00:22:21.178, Speaker A: So imagine that yn is converging to some y. And imagine that y n is a minimizer chosen so that the distance between x n and yn is equal to d plus dx. Well, then, it's not hard to prove that if, given the Dewaner minimizers, everything is compact, everything is continuous. Y is a minimizer, is a minimizer for, you know, the limit function f t x, you know, dot. So yn was a minimizer for xn. Okay? And it follows that y is a minimizer for x. But therefore, you know, clearly this guy is converging to distance x y because x n converges to x and y n converges to y and y is minimized.
00:22:21.178 - 00:23:04.556, Speaker A: Maybe there is another one where the distance is bigger, but this is a minimum. Okay? So this proves the upper semicontinity of d plus. And analogously, the lower semicontinuity of d minus follows right? Now. Interestingly. Interestingly, d plus and d minus are often actually the same function. So the claim now is that d minus dx certainly is less or equal than d plus dx. Lnx right, that is obvious.
00:23:04.556 - 00:24:35.656, Speaker A: But this is also less or equal than d minus s x. You know, if s is bigger than t, okay. In particular, d minus tx equals d plus tx for every t except the most accountable number. Okay, let me prove this. So this is obvious, of course. So let's go improving this and what it is that I do well, I pick, you know, you know, pick, you know, yt minimize for, you know, capital f dx dot, so that the distance between x and y t equals d plus dxy d plus t. And also pick y s minimizer for this other guy so that the distance within x and y minus sx, right? If the space is locally compact, you know, sufficiently compact, you can do otherwise peak minimizing sequences.
00:24:35.656 - 00:24:58.972, Speaker A: Okay, and now just notice that, you know, unit yt minimizes this. What does it mean? This means that f of y t plus distance squared. But let me write directly dt plus squared. Actually, let me write this one. Distance squared x y t divided by two t. This is better. This does better than what y s does for the same function.
00:24:58.972 - 00:25:48.934, Speaker A: This is better than f of y s plus distance squared x y s divided by two t. On the other hand, y s also minimizes something. So let me write it. F of y s plus the distance squared x y s divided by two s. This is less or equal than what yt does here. X y t divided by two s, right? You sum this, the functions get away. And, you know, you use the fact that one over t minus one over s is positive to conclude, okay, so, so this proves, this proves this inequality.
00:25:48.934 - 00:26:26.682, Speaker A: And now you have two functions. Both are monoton, you see? And they are in some sense one, you know, contained in the other in some sense. But you know that the monoton function from r to r must have at most accountable number of discontinuities, right? Because every discontinuity should be a jump. Every jump should contain some rational point. And these rational points, you know, are all different from the other because the jumps are, you know, disjoint. And therefore, therefore, and therefore, at any point of continuity of both these guys, these guys agree. Make sense.
00:26:26.682 - 00:26:55.974, Speaker A: I think the picture here helps, you know, more than you know. Or perhaps, perhaps. So, so if t. So if, if a t is continuity, continuity, let's say for d plus x, then what it is that we have. So we have the d minus. So d. Let me see.
00:26:55.974 - 00:27:40.246, Speaker A: Okay, okay, I get the point. From the above, it follows that d plus dx is always less or equal than the limit. As s goes to t from above. Of d minus sx, right? And it's greater or equal than d minus tx. Okay, so if I have a point of continuity for d minus, this guy is equal to this, and this is also equal to dt. And similarly for, you know, continuity point of plus makes sense, right? So we have these functions that are more increasingly increasing. Sometimes they make jumps.
00:27:40.246 - 00:29:09.232, Speaker A: And d minus basically stands here and d plus tends here on the jumps. But on the rest of the points, these are, these are make sense. All right, further claim, the further claim is that the map that takes t and returns qtf at x for any fixed x, this map is local ellipses on the open interval with derivative, which is equal to minus. Let me write this way, or maybe two distance for almost everything. Okay, so I'm not claiming anything about regularity at time t equals zero in some sense. This is taken care of by an estimate that I erased, I'm afraid. Yes, well, I wrote an estimate before that.
00:29:09.232 - 00:29:44.150, Speaker A: Now I erased. That takes into account zero. But now this, this is another computation. Okay, well, let's compute this. So let's do qsfx minus qtf x. Let me take as before, yt minimizer for this and y's minimizer for this. Okay? But if yt minimizes this, this quantity is less or equal, you know, I pick yt on both sides, you know, for both terms.
00:29:44.150 - 00:30:54.096, Speaker A: And this is less or equal than the distance squared yt x divided by two s minus distance squared ytx divided by two t. Makes sense because the function, the f part of yt gets cancelled, right? All right, but this is equal to, you know, this is equal to distance squared x y t times what divided two t s and the other you have t minus s. All right, well, but, uh, that's this. So lip discontinuity now follows local lips discontinuity because, you know, this guy is uniformly bounded, right, by, um, you know, d, this d plus. Now this is bounded by d plus dx. And d plus was bounded by some t like times, the diphthis constant, something like this. So it's locally formally bounded.
00:30:54.096 - 00:31:18.070, Speaker A: And this is, you know, as far as s and t are far from zero. This provides a localitis regularity. Of course, this is a one sided locality, but I can swap snt here. This totally symmetric. And so what also, what I also get if I divide it by s minus t. And let s go to zero. I get the delim soup.
00:31:18.070 - 00:32:09.262, Speaker A: Sorry, s go to t of this difference quotient. This is less or equal than d plus t x times you know, times, I guess minus one over two t squared, right? Makes sense. But now I, is that okay? But I can do the same, I can do the same for, with bounds from below. Let me, let me show you so that you're convinced that I'm not cheating. So this gives a bound from above. For the bound from below, I start observing that qsf minus q t x f x. This is greater or equal.
00:32:09.262 - 00:33:11.544, Speaker A: Now I pick y s. In some sense, this is greater equal than distance squared y squared x divided by two s minus distance. For the ysx divided by two t, right? And this is distance squared x y s times the same thing as before, t minus s divided two t's. So if I divide it once again by s minus t, and let you know as s goes to, actually, I could pick s goes to t, to the lyme. This difference quotient is greater or equal than, again, the same quantity as before. Actually, you know, when s goes to t y. So this quantity is greater or equal than d minus sx.
00:33:11.544 - 00:34:12.944, Speaker A: But then I use the lower semi continuity of d minus. And then I get d minus t x squared times minus. Okay, and then, and then, and then, and then if I, if I divide by s minus t with s smaller than t, I get the other inequalities. Okay, does it make sense what I'm saying here? Notice that I use the semi continuity of d minus even in t, even in t, which follows by the same argument that I used before I showed the semi continuity in x. But this is in x and p. Does it make sense what I'm saying? All right, so this is proved, right. Okay, now to conclude, to conclude the proof, or almost conclude the proof, let me just notice that.
00:34:12.944 - 00:35:26.086, Speaker A: So I computed the time derivative of qt is given by this distance function. Basically. And so now the claim, now the claim is that, is that the asymptotic Lipschitz constant of qtf at x, this is bounded from above by dt plus x divided by t. And you see that if I prove this, I proved this inequality, right, because this derivative is equal to d squared over two t squared. But leaps is constant squared divided by two is less or equal than this quantity. So, so this closeness, all right, and, well, let's compute it. So let me, let me pick z and y if you wish.
00:35:26.086 - 00:36:15.894, Speaker A: Actually, let me say x one and x two close to x. And let me pick say y two minimizer for this. And so, this is bounded from above by the simplex is always the same formula this transcode y two x or y two argument of f t x two. So divided by two t. This is x one minus this squared y to x two divided by two tail by the usual, you know, y two is optimally near. It's possibly not optimally near. So I get a bound from above.
00:36:15.894 - 00:36:53.024, Speaker A: Now let me continue. This is less or equal than, you know, a squared minus B squared, difference of squares. This is bounded from above by, you know, let me say distance x one. Let me do all the steps. Y two, x one minus distance y two, x two times distance y two, x one plus distance y two, x two divided by two t. This is equal. Actually, this is equal.
00:36:53.024 - 00:38:33.150, Speaker A: I've done nothing. But now, this difference, this difference by triangle inequality is bounded from above by the distance between x one and x two, right? And what about here? So this is bounded from above. So let me write. So this is bounded from above by the distance between x one and x two, again, plus the distance between y two and x two, right? So this is bounded by, let me write it here. Twice d plus t of x two plus the distance between x two and x one divided by two t, right? Because this guy, this Guy of course, is less or equal than d plus y two is a minimize. All right, now divide by this quantity and take the lean soup as x one and x two go to x. So you think that the limb soup as x one and x two both go to x of qtf, x one minus qtf of x two divided by the distance x one, x two.
00:38:33.150 - 00:39:03.564, Speaker A: This is less or equal than what? So this, I divide, this goes to zero. This I use the upper semi continuity of d plus. This is less or equal than dt plus of x divided by t. But now this expression is totally symmetric in x one in x two. So I can take, if I take the absolute value here, if you wish. If I replace x one and x two, you know, I swap them, get an inequality for the upper for the absolute value. Makes sense.
00:39:03.564 - 00:39:59.250, Speaker A: So, so I print my claim, right, this is, this is just this name suit. Okay, all right, very well. So what happened? We are done. It's just now a matter of, you know, so this is proved, as I mentioned, because I compute exactly the derivative, and then I show that that derivative is bounded bounds from above, this absent total constant. So this is proved. The only thing that remains to be proven is the lipid circularity of the function with values in Cb. Okay, then the problem is that I can have issues at time t equals zero, because my estimates typically tend to blow up principle.
00:39:59.250 - 00:41:20.174, Speaker A: But I'm saved by this equation. Now that I know what is the derivative, I'm saved by that equation. Because, because if you want, or if you wish, by this, because I know, this is telling me, this is telling me, if you want, look at this, that the model for any x, this is equal to d, sorry, d x divided by two t squared squared, right? But we know that this guy is bounded from above by some constant times t, the elliptic function of f. So this is lesser equal than some universal constant times the ellipses squared of f. Right? And now I'm done. Because, because, so this is telling me that for any x, the cardiac x t and the transit f of x, in fact, not only is local elliptic in here, but in fact, I mean, it extends the elliptic gravity up to zero, the uniform up to zero. You see what I mean? The derivative is uniformly bounded and also uniform in x.
00:41:20.174 - 00:41:44.744, Speaker A: Right. And so, and so we are done. Now, this tells us the desired uniformity is given. Right. How much is the difference between, say, f and q t of f at some point? Well, it's at most t times, you know, this, for any point. So the estimate is uniform. Okay, so, end of the proof.
00:41:44.744 - 00:42:45.164, Speaker A: Okay? Yeah, I guess this could be, if there are no questions, this could be a good moment to take a break. Five minutes. And then, and then, and then we resume and we completely change topic. All right. No, not yet. Okay, Robin, that's better. It.
00:42:45.164 - 00:47:54.768, Speaker A: All right, it's a good time now to stop for a second and wonder a little bit what we have proved so far and basically what we have learned so far about CD community spaces. And I would say, I mean, we can summarize what I've done during this course, but in these bullet points. So, first of all, on arbitrary metric measure spaces, no need of a cd k infinity assumption. Sublet functions can be defined either by relaxation or via weak upper gradients. And the two approaches coincide. Okay? One speaks the l two language, the other speaks dw two language, but they, you know, in an arbitrary metric measure space, they give the same result. Second, and DFCD is crucial, there is a well defined and stable.
00:47:54.768 - 00:48:51.948, Speaker A: Stable with respect to measure gravascular convergence notion of heat flow and heat flow satisfies in a natural way, sort of it equation. I mean, we wrote this, okay? Solutions of the heat flow satisfy this. Here stability comes once again from optimal transport point of view, much like the, you know, CDK infinity condition is stable thanks to this optimal transport approach. And while the equation, the PvE comes from the more sublux point of view, so again, there is this interplay between Wasserstein and Lebeck geometry, if you wish. And, and, yeah, and, yeah, and this heat flow can be seen as gradient flow in two, a priori, extremely different, but in fact equivalent in a very arbitrary framework sense. Okay, lastly, we have learned that the heat flow in general is not linear. And it's not linear because there is no need for our.
00:48:51.948 - 00:49:51.976, Speaker A: This is totally invisible from the western point of view. By the way, you will never see linearity of the heat flow by looking at the gradient flow of the entropy. Once I heard Jan Brany making the following joke. There is an alien civilization that studies the heat flow at the beginning, not as gradient flow of the Dirichlet energy, but as gradient flow of the entropy in the vast world. And then some mathematician in alien space proves that the heat flow is linear. And of course, it wins the fields medal, right? Because that's, there's absolutely no reason at all, if you look at the wasteland point of view, to realize that the heat equation is actually linear. Okay? If you look at the back point of view, that is evident and is evidently linked to the fact that the Chig energy is a quadratic function, or if you wish, to, the fact that w twelve is inverse space, okay, this, of course, is the infrared dimensional analog of if I have a smooth function over rd, if you take its gradient, is linear, if and only if the functional is a quadratic form.
00:49:51.976 - 00:50:25.372, Speaker A: Okay, that's equivalent. All right. Okay. Now we know by experience that typically having a linear Laplacian is a good thing, okay? So perhaps it could be a good idea to have a look to the space where the depression is linked. Okay. And our chances of actually, you know, in fact, I could, you know, give now the definition and say, okay, space is a linear cd kinetic space if the Laplacian is linear, okay. And by the stability property, in some sense that, you know, for free.
00:50:25.372 - 00:51:24.584, Speaker A: For free. I mean, after all the work we've done so far, improve the stability of this class, also of spaces. But now the question is, okay, what do I do with this class? So how can I, you know, handle, you know, geometry or icing this space, what do I actually earn, you know, from the assumption of linear to the Laplacian? So, in order to make any concrete use of this assumption, we need, that I'm going to make maybe Monday or next Friday is we need to sharpen our calculus rules. Okay, so far, so, so far, our best understanding of the Laplacian is sitting in the weak integration by parts formula, right? This we know, but this is clearly, in some sense, does not really see any. I cannot really use the linear Laplacian linear to get anything interesting. Okay. In some sense, what I want to do today is to improve this into.
00:51:24.584 - 00:52:01.524, Speaker A: I really want to integrate by parts. So my next goal is to understand this. Okay. And perhaps understand this right hand side. Okay. And because of this, it's better to take a step back and have a look to the standard, or perhaps not standard, but the smooth physical case. And so this gives us, will give us an idea on how to define this object in purely metric measure spaces.
00:52:01.524 - 00:52:38.348, Speaker A: So, let's say. So let's say we have a fizzler manifold, or in fact, everything here lives on a tangent and cotangent space. So let's just pick, you know, RD, we don't know this norm may or may not compromise color. Probably, I don't know. Okay. And take f from this space into r's boot. Then who is the differential of f at some point? Well, this is, you know, is well defined, regardless of the number.
00:52:38.348 - 00:53:27.228, Speaker A: This is, this is, you know, you can define it. For instance, as, you know, this is the element, let me put this way, let me be precise. This is the element of the dual of this guy, which of course, I should think as you know, the dual over d, because rd and the tangent of x is, they're all the same. It identified, you know, given by dfx. Applied to a certain vector, v is the limit as epsilon goes to zero or as t goes to zero of f at x plus tv minus f at x divided by t. Right. The norm has no ruling.
00:53:27.228 - 00:54:23.104, Speaker A: Okay, now who's the gradient? Now here the matrix comes into play. Okay, the differential is something living in the cotangent space. The gradient is something that lives in the tangent space and is related to the differential, okay? And it comes from a, you know, you have seen this in Banach space theory for sure. So whenever you have a Banach space, right, you have the dual and you know, the every vector, you know, for every. Let me write it. So, for every, say, v in the Banach space that exists, possibly not unique, l in v star, such that l of v is equal to the norm of v squared, and the norm of l is the same as the normal v, right? You see that the guy or one of the guys realizing the normal v. Okay, this l in general is not unique.
00:54:23.104 - 00:54:51.088, Speaker A: And this possibly multivalued map that takes v and returns this set of L's is in fact a single valued and linear if and only if. These are in inverse space, in which case this is nothing but the rich isomorphism. Okay, now we can do the same. In fact, in fact, in fact, this is the same. Notice that this. I mean, it is convenient to encode both of these in the same inequality. Notice that.
00:54:51.088 - 00:55:49.704, Speaker A: Actually, let me write. So this is the same as saying that l of v is greater or equal than one half norm of v scored, plus one half normal star scored. Notice that this inequality always holds if you have an element v of a Banach space element of dual, you know, by definition of dual normal young inequality, this inequality is always true. Right? So if the opposite holds, I'm really encoding both of these, you see? Make sense if you want. This is the same argument that we used when introduced the Georges notion of metric gradient flow. Right? So this was the starting observation. Okay, all right, so, so, coming back on our fizzler setting, so gradef at x.
00:55:49.704 - 00:56:27.536, Speaker A: Now, in general, this is not a single value guy. This is the set of. Okay, now, okay, should. Okay, now, b is a guy in the tangent. So, in TXRD, which is Rd, such that, you know, the differential of f v is greater or equal than one half the normal. The differential squared at x, I could say plus one as the normal v squared. Rd was coming with a norm.
00:56:27.536 - 00:57:13.488, Speaker A: So Rd star as a norm, the dual norm. It is the norm that I'm using to, you know, in here to compute this differential. And I'm saying, you know, that now, v is a gradient, if, you know, I mean, the same idea, except that I'm sort of swapping tangent and cotangent. Okay, notice that this is always non empty and convex. I mean, for any fixed x, this is a convex set. Okay, make sense. Let me try to convince you with an example that, in general, gradients are not uniquely defined.
00:57:13.488 - 00:57:52.012, Speaker A: And I guess that the example, we've already seen it. So take the l infinity norm, one of two, and take f of x one, x two to be equal to x one. So this function increases when you go to the right. Okay, but given that I'm taking the l infinity normal, now, wonder who is degraded of this function. Of course, the first guess is, you know, say, at zero degradient should be the vector, you know, pointing to the right. That's. That's another choice of gradient.
00:57:52.012 - 00:58:22.886, Speaker A: But then, you know, because I just have to go to right. But on the other hand. On the other hand, say the vector one one, this is the same normal as this. And the derivative of the function along this vector is the same as this. So why not, you know, or why not this, you know, in general, in general, grade is equal to, in this case, in this case is equal to what it is, the set of vectors, or therefore one alpha with alpha, you know, less than one. Okay, you can check that. This is the case.
00:58:22.886 - 00:58:54.894, Speaker A: Okay. This should be, you know, should explain why, why we don't expect uniqueness. All right. Now, so we were going. So my goal is to understand, at least in the physical setting, what this differential of a differential, g grad f. Okay, now, if grad f is not single value, this is not a single valued thing. So the best I can do, the best I can do is to define.
00:58:54.894 - 00:59:47.866, Speaker A: So this would be a multivalued map. And I could define d plus of g grad f and d minus g grad f, say, at a point x. You know, this would be the sup among dv in grad f of the differential of g at x applied to v. And this is of course the inf of the same expression. Now let me perhaps point out that if you open a book on physiogeometry, typically you don't see this sort of discussion because, because typically it is assumed some smoothness assumption on the Rom is imposed. In particular, the norm is typically taken uniformly at least. Well, typically uniformly convex and uniformly smooth.
00:59:47.866 - 01:00:26.654, Speaker A: But in particular, typically norm is taken as strictly convex. And if the norm is strictly convex, this guy is a singleton. Exercise. Exercise. If this norm is strictly convex, then grade f x is a single one. I mean, it's really an easy exercise. Just sit down and think about the statement and the definition of gradient, and you're done.
01:00:26.654 - 01:01:17.094, Speaker A: Okay, so it's a little bit of a warning. You will not find this, but in the nosmo setting with the elephant is certainly measure space. So it is something we have to deal with. So let's be prepared. Okay, now here is, here is a relevant observation. There can be seen as yet again, one observation linking, if you want the analysis in the independent variable with analysis in the dependent variable, or l two and w two word or DeLorean and Lagrangian point of view. Remark.
01:01:17.094 - 01:02:37.620, Speaker A: D plus f d g is the limit as epsilon goes to zero of the norm of the differential of nd. Minus is the same as delete as epsilon go to zero from below. Let's discuss a little bit what I just brought. First of all, first of all, notice that, notice that the map that takes f and returns the differential of f, say, at some point x, the norm, I should say this, is convex, right? At any given point, right, is convex non negative. So also, also, this is complex. Or if you want, another thing happens as the differential, at some point is linear. I post compose this linear function with the convex function, one half norm squared.
01:02:37.620 - 01:03:29.092, Speaker A: So this is convex. Any convex function admits directional derivatives, right? So, given that this is convex for any g, you know, for any, if you want f and g, if I take, if I take, you know, the limit. So this limit from the left, from the, what is, you know, from above and from below, this limit exists. This limit exists because this limb, for instance, this limit, as epsilon goes to zero. This is the same as the info over positive epsilon. And this is the same as the soup over negative epsilon. Does it make sense what I'm saying? Right? I'm just saying, look, I have a convex function.
01:03:29.092 - 01:03:47.164, Speaker A: You know, fix f and g, and look at the graph of the convex. So this. So if you want this map, I put this way. So the map that takes. Yeah. Epsilon and returns one half differential of f plus epsilon g squared at x. This is convex.
01:03:47.164 - 01:04:14.652, Speaker A: Um, let me, let me draw its graph. And, you know, the graph could be something like, I don't know, should be like this, for instance. I don't know. I have no idea what the graph looks like. I know that this convex. And now I look at this derivative in zero, the right derivative, and the left derivative in zero. The left derivative is this expression over here.
01:04:14.652 - 01:04:58.694, Speaker A: And the right derivative is this expression over here. They both exist because the, you know, the difference quotients, you know, here, you see, as far as epsilon goes to zero from above, the difference quotient decreases. So this limb exists because it's an inf. And as epsilon goes to zero from below, the limit from below exists because it's the same of a soup. Make sense? Okay, so at least the right hand side that were defined. Now, let me prove that equality holds in there. And you see here, I'm perfecting in the dependent variable, where I'm computing this.
01:04:58.694 - 01:05:21.254, Speaker A: Or when I compute the differential g along v, I'm perturbing in the independent variable. So this is horizontal derivation. This is vertical derivation. Well, I mean, let me actually. Okay, let me actually prove. So let me notice the following. So I have the inequality.
01:05:21.254 - 01:05:57.304, Speaker A: So peak v in gradient at some point. Then what I know, I know certainly that the differential of f applied to v is greater or equal than one half this norm squared. I mean, everything is evaluated at x. But let me avoid writing this. This is by definition of gradient. On the other hand, I can differentiate the function f plus epsilon g at d. And here I know that you know, this is an arbitrary function, an arbitrary vector.
01:05:57.304 - 01:06:21.964, Speaker A: I know that there is an inequality. Please. For the b plus and the b minus. Yeah, there's no reason why they like, they could both be like. No, take. Yes. So, it's simple.
01:06:21.964 - 01:07:02.484, Speaker A: It's simple. Take f from r to r convex. Then the limb say, as epsilon goes to zero of f x plus, I don't know, epsilon minus f of x divided by epsilon. What I'm saying is that the right derivative is equal to the inf over epsilon positive on the same expression. You agree? Actually, let me put this way. Let me put this way. So, the map, this map, this map, this map is monotone.
01:07:02.484 - 01:07:29.914, Speaker A: This is a non decrease increasing. Maybe not strictly, but increasing. Okay, so, so now you see, if I divide, you know, I don't know whether it is single value that's zero in some sense, but I can take the inf on positive or the super negative. Okay. Okay. Other questions. Yeah, don't feel afraid to ask.
01:07:29.914 - 01:08:17.430, Speaker A: Now, what do I do? I take the difference of these two expressions, and certainly what I get, you know, the differential a is linear. And what I get is that epsilon g, sorry, epsilon, the differential of g apply. The v is less or equal than the differential of f. This norm square disappears minus the differential of f squared divided by true. This is true for every f, g, epsilon and v gradient of f. Now, if epsilon is positive, I can divide by epsilon. I let epsilon go to zero.
01:08:17.430 - 01:08:54.534, Speaker A: And what I prove is this inequality, you see, because I bound that from above, the differential g applied to v for any v in the gradient. Okay? And if epsilon is negative, when I divide by epsilon, I get the other inequality. So when I get the epsilon, I get this. Okay, right. Exercise. Prove equality. Hint, use compactness.
01:08:54.534 - 01:09:52.740, Speaker A: Okay, so that's in the, in the, in the smooth field is a smooth fizzler setting. Now, let's see how we can translate this knowledge to the nosmooth setting. Well, first of all, first of all, okay, perhaps here is the, before I. Listen, this here is a starting observation. In principle, in the nosmus setting, I might be in trouble in defining what a differential or a gradient is. So I might be in trouble in, you know, a priority defining these objects in order to define this. But these things are easier to, you know, I have a notion of normal differential.
01:09:52.740 - 01:10:46.114, Speaker A: That's the mean of weak upper green. So the first thing that I will do is I will take, you know, these right hand sides, in some sense, as definitions for d plus g, d minus g applied to grad f, even in a setting where I don't know what dg is or grad f is, but, you know, just as a formal expression. And that should, you know, be related to integration by parts in sense make sense. So that's the first thing. And, okay, that should be related to, you know, what is the differential of g in the direction of grad f, even though for the moment, I don't know what the differential of g is, I don't know what grade f. So now metric measure spaces. And the starting observation is so now xdm.
01:10:46.114 - 01:11:45.092, Speaker A: Now take fng sub. Actually, let me consider the map that takes sobo functions f and returns one half minimal weak up squared of f in l one. This is a function in l one xm, if you wish. This map is convex, you know, m almost everywhere convex in some sense, meaning that whenever you write the, you know, the convexity inequality, the convexity inequality holds the right hand side, I don't have a number of a function, but, you know, the right hand sides, you know, I have the correct inequality almost everywhere. Make sense what I'm saying. Let me write it actually, so, so that it's clear what I mean. So what is one minus t? F plus tg squared.
01:11:45.092 - 01:13:10.764, Speaker A: This is less or equal than one minus t. Differential of f squared plus t over two differential of g squared. M almost everywhere should be this, right? So it's convex in this sense. This inequality holds. Okay, now, if this inequality, also by the same argument basically written over here, but rather than for, you know, r value function for l, one variable function, uh, the limit as epsilon goes to zero of df plus epsilon, g squared minus df squared divided by two epsilon. This exists both in l one and almost everywhere, right? And I call it d plus g nubla f. And of course, the limb, as epsilon goes to zero of the same expression, by definition, will be d minus g in the blood.
01:13:10.764 - 01:14:24.730, Speaker A: Make sense. So this sigma, let me perhaps argue a little bit more about what this limit exists. When epsilon decreases, this expression decreases almost everywhere. We have a decreasing sequence of functions, okay? But you can check that they are all bounded from below by they uniformly bounded from below by, um, well, for instance, by the value of this expression for some epsilon negative make sense. If you put it in the other way, the map that takes epsilon and returns this with velocity one is increasing, right? So, so when you let epsilon go to zero from, you know, from above, you know, it is uniformly bounded from below by the value at epsilon equal one and uniformly bounded from below by the value at epsilon equal, minus one and plus I have convergence almost everywhere. So I have convergence both in one. Makes sense here.
01:14:24.730 - 01:14:52.798, Speaker A: I should, if I wanted to, if I wanted to write here inf over epsilon, you know, over positive epsilon, I should be a little bit careful. And writing that inf is the essential infimum. So if you have familiarity with the concept of essential infimum, that's what you should put here. You should put it here. It's another familiarity. Forget what I just said. All right, now notice that by the monotonicity in epsilon of this, the, you know, notation stands.
01:14:52.798 - 01:15:41.154, Speaker A: So at least this is less, or even this, you know, I'm almost there at least. Okay. Okay. Now the question I want to address is, do this expression have anything to do with the concept of differentiating g in the direction of grad fucking? So that's the next question. So here I used in some sense the trick, if you want, of taking those right hand side as definitions. But now I want to sometimes try to re establish the equality between the left and right hand side. But of course, I need some concept of, you know, gradient of it.
01:15:41.154 - 01:16:54.090, Speaker A: And in fact, there is such a concept, without going to the theory of modules, that maybe some of you already, you know, seen in the recent talks that I gave. And the starting observation is this, let f be sobolech and PI be test. Then we know for every t positive, this integral is less or equal, sorry, not g. Right? This is true for every t, you know, between zero and one, right? For t equal one. That's part of the definition. For t is more than one. What I do, I apply, you know, the definition of the function to the restricted one, which is the test.
01:16:54.090 - 01:17:52.414, Speaker A: Okay, so now this left, this right hand side, it is certainly less or equal. Let me apply young inequality. This is less or equal than one half plus one of the double integral, right? Let me divide by t. Let me take the lean soup. And what I get is that the lim sub st goes to zero of the integral of f gamma t minus f gamma zero divided by t in d PI of gamma. Of course, this is bounded by what? Okay, let's have a look. Let's have a look at this expression.
01:17:52.414 - 01:18:33.474, Speaker A: Let me use pubini to exchange the integrals. This is basically this guy. Let me, let me write in green. So this guy is equal to the double integral. So the average interval from zero to t of the integral of d f squared in, you know, with respect to the measure, you know, let me write it with rho s dm ds, where, where rho s is the density of e s. Push over PI makes sense by, you know, the change of arrival formula. This would be like integrating, you know, this is gamma s.
01:18:33.474 - 01:19:19.342, Speaker A: So when I take something with respect to gamma s is the same as integrated function d e s push over PI. PI is test. So the marginals are uniformly bounded in l infinity. So in particular, you know, the density. Okay, now what happens to these densities when s goes to zero? When s goes to zero, the measures rho sm converge weakly to rho zero m rho zero m in the sense of measures. This follows from the fact, well, perhaps, let me notice. So notice that.
01:19:19.342 - 01:20:08.084, Speaker A: So the claim, the first claim is that e s push for PI goes weakly to e zero push over PI. And why is this true? Because PI is concentrated on continuous curves. What does this mean? This means that what I claim is that the integral of phi composition es d PI, goes to the integral of phi composition is zero d PI for every phi continuous and bounded on x, right? But this is trivially true. Phi is continuous and bounded because the integrand is uniformly bounded and point wise. You know, phi in gamma S goes to phi in gamma zero because phi is continuous. So dominated convergence gives them so. So this is true as soon as PI is concentrated on continuous curves.
01:20:08.084 - 01:20:57.824, Speaker A: But moreover, given that it is test these densities, these densities are uniformly boundary and infinity. And now it's an exercise to check that if I ever uniformly bounded sequence in an infinity that is converging weakly in the sense of measures, it also converges weakly status. So, in duality with l one, if you want to apply egorov, okay, so this to say that this integral has a limit when s goes to zero. When t goes to zero, then this goes, know goes to the integral of d f squared. If you want gamma naught d PI of gamma. So I have a limit here. I have no idea whether, whether I do have a limit in here or not.
01:20:57.824 - 01:21:50.754, Speaker A: So let me put a limb soup, one half limb soup, st goes to zero. This, you know, kinetic energy, if you wish, average the kinetic energy. Okay, so this inequality is an equality that is always true. Wherever I pick. You should think of this like this. Whenever I pick a function f and I try to differentiate in the direction of any given test plan, what I obtain is something that is bounded from robot is a constant. And you should compare this, you should compare this to the expression that we have seen in this moot, that the differential of x applied to v is less or equal than one half norm of the differential squared plus one half norm of v squared.
01:21:50.754 - 01:23:23.064, Speaker A: It's the same thing, right? I mean, same. I mean, it's humid, is that okay? But in the physical setting, what it is that we have done, as soon as we realized that this is always true, we said that v was a gradient if the other inequality holds. So I do the same definition. So f is suble, PI is test. I say that PI represents the gradient of f if the other inequality holds. And here I still keep the limb soup. It's a definition.
01:23:23.064 - 01:24:29.934, Speaker A: It's just, you know, I'm imitating what I would have done in the, in the smooth physio setting. Well, if you want, another theorem is this. And this will be crucial. This will be crucial when, when studying better, studying the heat flow in the nosmooth setting, even in order to get laplacian comparison estimates. And this is crucial, really, to make computations in the metric measure spaces. It has a trivial proof, but it is crucial. And this theorem, as much as all the discussion that I'm making now, comes from my paper on the differential structure of matrix measure species.
01:24:29.934 - 01:25:07.784, Speaker A: Remember 2009, parts of this was also, it's also appeared in earlier papers with Luigi and Giuseppe. But then this discussion comes from here. The theme is this. Say you have two subtlety functions, and say that PI represents the gradient of f. Okay, so that's inequality also. And now, so PI instance PI is defined in terms of what happens when you differentiate f f along PI. Now try to differentiate g long PI.
01:25:07.784 - 01:25:58.504, Speaker A: We can look at the, you know, integral of gamma t minus gamma naught divided by t d PI of gamma. Okay, let me write it twice. I would like to look at the limit of this expression as t goes to zero. I don't know if the limit exists, so I can only get no lm inf, which is of course less or even sub. So far, I did nothing. Now, the interesting thing is that this is bounded from above by the integral d plus g. Nabla f, that might not be PI, and bounded from below by the inter d minus.
01:25:58.504 - 01:26:25.324, Speaker A: Right? So, so. And you see that this is really an analog of what I just erased over there. So this is the horizontal derivative. I differentiate g direction of PI. And there is something where test plans don't appear at all. Just minimum weak, apparently. Okay, proof.
01:26:25.324 - 01:27:13.600, Speaker A: And the proof is as trivial as the one that I just showed you before. In the, in the smooth setting in the linear case is this one. Right? What I know, I know that the limit of the integral of f gamma t minus f gamma naught divided by t d PI is greater or equal than one half the interval df squared gamma naught d PI plus one half the lean soup of the kinetic energy. Let me, you know, okay. On the other hand, by the remark, you know, by the. That is always true. I know that if I apply.
01:27:13.600 - 01:29:04.524, Speaker A: No, if I take the sober function f plus epsilon g, and applied to this function, I applied what I know for every sobel function and every test plan, this is less or equal than one half the minimum weak upper grade of f plus epsilon g plus the kinetic energy, right? And then, as before, I take this and I subtract this. When I subtract the inequality, since that's going the correct direction, the kinetic energy part drops away. And what I get is that the lim soup of epsilon, the integral of gamma t minus g gamma naught divided by t d PI. This is less rico than actually, remember this way that the integral of minimal weak upper gain of f plus epsilon g squared minus d f squared divided by 2d PI in gamma naught d PI, right? Then as before, if epsilon is positive, so this is true for area f g and epsilon, as soon as PI is a test plan that represents the gradient in particular value for every epsilon. So if epsilon is positive, I can divide by epsilon. And when I get this dim soup, which is what I'm interested to bound for a ball is less or equal than this expression divided by epsilon for every epsilon, right? And so I let epsilon go to zero, I use DL1 convergence of this expression to this, and then that epsilon is negative. When I divide, I get the other inequality.
01:29:04.524 - 01:29:53.160, Speaker A: All right, end of the proof. Now, let me just. I know that my time is over, but let me just. Let me just give a definition and a statement so that you know where we are going to go next lecture on Monday. Sorry, it's not 2009. I think it's more 2012. 2009 is the first paper on the heat flow.
01:29:53.160 - 01:30:50.364, Speaker A: So the definition is this same paper. We say that x is in predesimal burtian f w twelve. Is it birth. Okay, so the idea behind this definition is. Well, I mean, what we discussed, right? So this typically is Banach, but being Hilbert should be related to some Hilbert property of the, you know, sober functions and should be related to linearity of delaplascian, which is something that we would like. Now, here is the theorem that I just state, but I want to, you know, to. Just to show you how calculus works on infinitesimal liberty and spaces.
01:30:50.364 - 01:31:31.340, Speaker A: And the statement is the following. Let x be infinitesimal. Burton. Then a few interesting things happen then. So d plus g. Nabla. F is equal to d minus g Nabla, f for every Fng, almost everywhere for every Fng and the map sending.
01:31:31.340 - 01:32:51.684, Speaker A: So let me call, let me call this guy, let me call this guy DFdG. And, you know, the map that takes fg and returns this Dfdg is bilinear symmetric and satisfies. And, you know, satisfies the following. So I want to tell you that now I have the right of calling this the FDG because of the following properties. So I have a cautious, vast inequality. I have a chain rule. So if I see one ellipse and I have the largest rule last in the other guy, f 2d here, f one and f two should be also bounded.
01:32:51.684 - 01:33:48.680, Speaker A: And so believe. And what else? And, okay, yes, and, and, and if f has a Laplacian and g is sublev, then I have the integration by parts, g Laplacian FDM. This is actually equal to minus the FDG, right? So this is, you know, the improvement in some sense of the integration by parse formula that I was hinting before, I don't have a less or equal now. Now I really have an equality and this really what it should be. And this expression is bilinear, okay, if you want. It is clear that this expression on the left hand side is linear in g, because just introduce them. In general, it's not linear enough, because the Laplacian is not, is not linear.
01:33:48.680 - 01:34:40.714, Speaker A: But once I know this bilinearity and symmetry of this, then the negative. Okay, so in some sense, this theorem tells us that what concerns the first order, you can take this way. So what was the first order? Calculus, calculus on infinite space. It really looks like a calculus on the minor many for some of the functions. Okay, let me just conclude with a comment. So, one consequence, one consequence of that statement is that, in particular, if I look at this theorem of horizontal vertical derivatives, only in free decimal spaces, what I deduce is that the limit as t goes to zero of the integral of gamma t minus gamma naught divided by t in d PI, this is equal. Now, because, you know, bounded from bone blue, but two things that are the same.
01:34:40.714 - 01:35:45.156, Speaker A: Okay, so this derivative exists, which is incredible, right? So we are taking the derivative, right, a directional derivative, and we are able to prove that this derivative exists. Okay, this is really, you know, the, you know, one of the cognizance of the theorem from now on. Okay, let me, let me perhaps comment we something which is miraculous on the fact that this expression is symmetric. And if you pause for a second and wonder, even in this mood setting. This is something that is not really obvious. So take two functions, smooth functions, f and g, a point and take, you know, you have f, you have g, and now you differentiate f in the direction of grad g and you get a number, and then you do something completely different, namely, you differentiate g in direction of graph. These two are the same, okay? Because in both cases, it's graph g.
01:35:45.156 - 01:36:10.274, Speaker A: And the reason they are the same has to do with the fact that you are walking on a Hilver space. If you try to do this on a feeder setting, this is not the case, you know, because one expression is linear in f, the other expression is linear in G. But in general, it is not bilinear. So, so they cannot be equal or, you know, they cannot be equal. But if you are, you know, on a very minor manifold, then they are the same. And this is, you know, in some sense, the. The counterpart in the metric measure setting of these identities.
01:36:10.274 - 01:36:53.142, Speaker A: Right hand side is symmetric in fng, even if the left hand side doesn't absolutely not look like that. I'm taking gene differentiated grade f. Okay, perhaps I did not mention I gave a definition of plant representing gradients. I did not state any existent results. Of course, if there were no such plants, the theory would be quite empty. In fact, there are such plants. But I'm not proving a general statement because in France we'd be interested in basically just one particular case of such plant representative gradients, the one coming from optimal transport.
01:36:53.142 - 01:37:18.074, Speaker A: But I will discuss this later on. So, we don't care about really planned representative gradient for arbitrary sombre function, but just for control potentials where optimal transport and cd condition will give us the result. Okay, so for today, that's all. And we will assume on Monday. I guess. Goodbye to the line audience.
