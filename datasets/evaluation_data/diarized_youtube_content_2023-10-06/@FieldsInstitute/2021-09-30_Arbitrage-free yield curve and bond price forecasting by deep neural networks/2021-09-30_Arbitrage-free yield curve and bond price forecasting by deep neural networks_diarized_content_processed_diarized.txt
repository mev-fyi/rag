00:00:00.520 - 00:00:47.428, Speaker A: Professor Cody Hyman is an associate professor and chair of the Department of Mathematics and Statistics at Concordia University. He is the director of the Mathematical and Computational Finance Program there at Concordia and also co founder of the Machine Learning, Quantitative Finance and Business Analytics FinML answer Create program. His research interests include mathematical, computational finance, probability, stochastic analysis, filtering and control, term structure models, energy and commodity markets, and insurance mathematics. His research on machine learning, algorithms and applications in finance have appeared in risks and the Journal of Machine Learning. And today Cody is going to tell us all about arbitrage, free yield curve and bond price forecasting by deep neural nets. So Cody, the floor is all yours and you can start sharing your screen.
00:00:47.596 - 00:01:33.516, Speaker B: Thank you. Thank you, Sebastian. And thanks to all the organizers for the invitation. This is the first time I've given this talk, so please excuse me if it's not perfect. I will give you one piece of advice, is don't be chair of a big department during a global pandemic. But maybe some other people here know that advice already, if you can possibly avoid it. But in any case, this is joint work really, with my recently graduated PhD student Shangao, and he's working in Toronto, but the papers haven't been put up yet.
00:01:33.516 - 00:02:47.636, Speaker B: But you can find his thesis on our thesis depository if you want, but I'm just going to launch right into it. Feel free to stop me if there's any problems with the sharing or if you have any questions. So, you know, machine learning, obviously, it's becoming bigger and bigger in finance for a few years now. And one of the ways you can look at it is, you know, another way to maybe predict some asset prices or some other financial variables you're interested in. But what I'm going to look at is bond prices or just generally term structure models. So if you do this using some arbitrary machine learning, you know, algorithm, deep neural nets or reinforcement learning or whatever, to predict whatever kind of asset price you want, there's no guarantee that this is going to be consistent in any way with our classical mathematical finance theory of, you know, arbitrage. No, arbitrage and arbitrage pricing and these kinds of things.
00:02:47.636 - 00:03:24.784, Speaker B: Now, I'm kind of a classical guy in this respect, so I want these things. And I know if you put transaction costs and, you know, you have to make. Make a relaxations of all of these things. But I like to, you know, when I'm trying out new stuff, stick to the classical ideas a little bit and. And see where we can go from there. And then we'll make the model a bit more complicated. So, in this paper I had with my previous PhD student, Anastasi Kratzios, who went on to do a PhD in Zurich, or, sorry, a postdoc in Zurich, and I think he's in Basel now.
00:03:24.784 - 00:04:13.990, Speaker B: We came up with a regularization approach for model selection in a generalized HGM framework. And I presented something like this back in the CMS winter meeting that was in Niagara Falls, I think. And it was much more complicated. And Tom heard told me it was the most complicated thing he'd ever seen, something like that during the break. But we spent some time refining it. We've taken out a lot of the topology, and, you know, that's fine. Now it's almost classical regularization approach for model selection, which you can really build a lot on top of.
00:04:13.990 - 00:05:09.966, Speaker B: And the theoretical results that justified are in this paper with Anastasi, which was in risks last year. I actually do a little advertisement. He's really was a superstar, Anastasi. We had another paper that's in the Journal of Machine Learning Research that you might have seen me talk about many times at conferences, and he's doing really interesting stuff now without me, but that's great. So we have this general framework for learning, an arbitrage free factor model that is similar to a factor model that's from some pre specified class of alternatives. Okay? So you can think of it as in just the simple term structure stuff that I'm going to, to present today. You know, you want to predict the yield curve or predict bond prices or whatever, five days ahead.
00:05:09.966 - 00:05:52.004, Speaker B: And, you know, you have a factor model which maybe is based on Nelson Siegel, or maybe it's something else. Maybe it's a affine term structure model, whatever. But what we do is, you know, you get a model that fits really well. Like, you know, just, it doesn't have to be arbitrage free. It doesn't have to be anything, you know, fancy. And then you have a class of models that includes the one you fit as a special case. And we want to add a penalty term that chooses the closest model within this class that, you know, is arbitrage free.
00:05:52.004 - 00:06:34.814, Speaker B: So there's this penalty now you can, you can weight the penalty to make it, you know, very strong or a bit weaker. And we can have discussions of what it means to be approximate arbitrage approximately arbitrage free. And there's some discussion of that in the paper with. So the intuition is basically, you have a model that you fit, okay, maybe it's a, maybe this is a Nelson Siegel type model. This is models from the class. And this is some loss function, will depend on the application. And here we have an arbitrage free penalty.
00:06:34.814 - 00:07:22.446, Speaker B: And this lambda is a certain parameter that you can make bigger or smaller depending on how much you want to force the arbitrage free stuff. And the easiest thing to think of, when we originally were thinking of this, we were thinking of volatility services, all kinds of other things. But primarily we've been sticking with term structure models for term structure of interest rates. And so let's say you had a Nelson Siegel model. You can go fit your Nelson SIEGEL model every day, or you can get a dynamic Nelson SIegel model, or you can get the Spencen model. You can do all kinds of things that will fit the model and arbitrage free versions of it if you want. Okay.
00:07:22.446 - 00:08:01.026, Speaker B: But, you know, we want to impose a arbitrage free structure and build the model this way. So in this paper with Anastasi, we did various predictive models. You can go look that up, it's in risks if you want. And we use some german bond data. And, you know, what we found was what we were doing was really good on the long end of the curve, we thought, but it wasn't as good on the short end. Now, that's a kind of weird result. But we wanted to go a little bit further.
00:08:01.026 - 00:08:23.716, Speaker B: And with my new PhD student, newer at the time, but he's graduated now, Shangao, we, you know, got some us bond data by the treasuries and corporates and we built kind of a more robust model that has some different features from the one that we previously considered.
00:08:23.900 - 00:08:25.836, Speaker C: Sorry, may I ask you some question?
00:08:26.020 - 00:08:26.628, Speaker B: Yeah, sure.
00:08:26.676 - 00:08:40.024, Speaker C: So just background information. I'm sorry to interrupt. So when you say the fatal well, so the, what is your target, his target data or today data are the awesome theoretical model. So what mean of the fading?
00:08:40.454 - 00:09:19.094, Speaker B: Well, so, I mean, we looked at, I'll get more into that later. But we look, in the original paper, we looked at just the, you know, mean absolute or mean squared error of the predicted yields versus the observed yields. But in this paper and Shank's thesis, we look at a lot of different metrics. We look at a hit rate. So, you know, you're within. We, we do some prediction, not just on yields, but also on prices. And we look, if you're like within, you know, for $100 of face value, you're within twenty five cents of the price.
00:09:19.094 - 00:10:24.812, Speaker B: Maybe, I don't know, the bid ask spread, if there's any bond traders in the room, the bid ask spread of a, of a corporate bond or a treasury, you know, what it is in terms of price. I know people like to think in terms of yield, but there's a nonlinear transformation when you take a yield to a price, and especially with a coupon bond, it becomes pretty complicated the relationship. So we look at two different methods. One where we're predicting on yield curves, one where we're predicting on bond prices directly doing the estimation. Okay, so I'll get into the different metrics and things. So, let's say we had this dynamic Nelson SIeGel model that's given in Diebold and Lee. We could use this as our factor model if we think this fits the observed data well, we could take arbitrage free Nelson SIegel with an autoregressive model from Angen Piazz.
00:10:24.812 - 00:11:11.416, Speaker B: I can't say that, probably, but the X where the X's are, you know, the level, slope and curvature are kind of state variables. Or we could take an affine term structure model, maybe the one given in Duffy and can or Christiansen and others, or some of my affine term structure stuff. But, you know, we could take any kind of model we want and we could fit it. And if we think it's a good fit, you know, that's great. But, you know, some of these models aren't arbitrage free necessarily. And, you know, dynamically predicting over time, it isn't always good. So, and if you want to do, you could do different things with filtering.
00:11:11.416 - 00:11:46.878, Speaker B: But this is a particular approach that we're taking. So, just to fix some notation, I'm going to assume almost everybody here knows what is an HGM type model. We're looking at the forward curve. Okay? So we've got the parameterization with the tau is the time to maturity. And we're going to assume just in this because we need a finite dimensional model, first of all. Okay? So we're going to assume that this is basically separable. So there's some underlying stochastic factor model, XT, and there's a loading parameter beta.
00:11:46.878 - 00:12:19.934, Speaker B: Okay? And it's separable in, in terms of time to maturity. Okay? So we'll just take a vasocheck model, start there. We can add jumps later if people want to. All right. But I don't start anything adding jumps. And, you know, we have the form of x. And we can figure out what, based on this kind of assumption, what the dynamics of the forward rate are in terms of these parameters, kappa bar, theta bar.
00:12:19.934 - 00:13:17.928, Speaker B: Okay? And the loading parameter will take kind of like a Nelson Siegel vector basis. And, you know, well, it is exactly that. It spans, you know, the span of this basis. All right? And our forward rates are just going to be given in this way. Now, the yield curve is just going to end up, in this formulation, being given in this way after integrating the loadings, okay? So Bjork and Spencen show that this is a complete space if the drift and volatility lie in the span of the basis functions or the tangent space, okay? Then the forward rate process will evolve in time in this. So this is important. You have to stay in this space over time.
00:13:17.928 - 00:14:07.450, Speaker B: Okay? That's why we kind of pick this stuff. And, you know, simple exercise from mathematical finance is we need to figure out the drift restriction to make this an arbitrage free model, okay? So, you know, given these assumptions, we can figure out this kind of lambda. It's lambda. And process here needs to be zero. Then this is arbitrage free, okay? Under the risk neutral measure, of course. So we have a whole bunch of bond data, corporate bond data, we just took from trace. We have us treasury data that's a little bit augmented with some proprietary data from a service.
00:14:07.450 - 00:15:23.138, Speaker B: I don't want to discuss no free advertising today, but each day, for example, for the treasuries, there's going to be about 60 or 84 bonds, 60 to 84 bonds trading. Okay? We need to do some filtering. So we want fixed coupons, we don't want callable bonds, we don't want convertible bonds, we want to get rid of some outliers. And we don't really want bonds with maturity less than three months because that will get weird. And basically, we're just going to do our prediction with four features, the clean price, coupon rate, coupon frequency, and remaining time to maturity, which is all I need to calculate the yield, um, for these bonds. Okay? So after you do that, you get about 67 daily observations where we've kind of in proportion. We select, you know, a subset in some sense, and we want to keep the proportion in each bucket kind of constant over the trading so you don't get kind of weird effects because of different numbers of bonds.
00:15:23.138 - 00:16:22.114, Speaker B: So, you know, and there's also dropout and, and different kind of things. If you wanted to get fancy, you could restrict yourself to on the run bonds and all kinds of various things, but this would further restrict your data set. Okay, so basically you do Nelson Siegel kind of fitting, all right? And we figure out this optimal decay pattern from our data set, and then we take this decay pattern or parameter and we fix it and then we do some sequential fixing to get the optimal x. Okay. The state variables, we extract these. And so then in this Nelson Siegel model, our arbitrage free kind of. This is the arbitrage free Nelson seagull model.
00:16:22.114 - 00:16:56.940, Speaker B: You end up with the bond price given by the model. And then we just want to minimize the fit, obviously, or the mean squared errors to get the factors or the state variables across the bonds that we are dealing with. And so you do this every day to kind of bootstrap out your yield curve. Okay. There's a nice description in Carmona's book of how to do this. I won't get into it, but. Or one of Carmona's books.
00:16:56.940 - 00:17:46.730, Speaker B: Anyway, you get this picture. And so, a slice this way is the yield curve on a particular day, and then this way is our data set over time. And I sent this picture to Sebastian, and he posted it on LinkedIn, and I saw a nice little trace along here, which would be kind of like the short rate if this went to zero. I wonder, wow, did he put that in some kind of machine vision program or something? Or is he like a super PDF hacker? But apparently he traced it so, by hand, which, you know, it's pretty good still. But in any case, so this would be like a long time. This would, if you went all the way to zero maturity, this would be the tender, this would be the, the short rate. Okay, so this is what you get this.
00:17:46.730 - 00:18:11.856, Speaker B: So we just, this is kind of pre processing. You get nice kind of yield curves every day. They have different shapes. You know, you get inverted ones and humped and all of these other kinds of yield curve shapes over time. And then we pull out the state variables. Okay, as well. So you can see the level kind of variable from Nelson Siegel is x one.
00:18:11.856 - 00:19:13.362, Speaker B: It's in blue up here, and then the slope is the orange one and the green one is the curvature. Now, obviously, these are not constant over time. So this is a dynamic model, and we just get, we just get these parameters and state variables out. Okay? But what we're going to do is we're going to use filter based sequential methods to estimate bond yields and bond prices, given these parameters. And then we're going to do the estimation of the parameters where we're going to model them with deep neural networks. Okay? So if you go back and you look at what the Kappa, the theta and the sigma are here, I mean, these are the kind of parameters we're going to model with deep neural networks. Okay? And so this is, you know, the, the approach and we're going to use the Kalman filter and the extended Kalman filter.
00:19:13.362 - 00:20:12.032, Speaker B: But for the Kalman filter, for the prediction, we're going to use yields. So we, we've got the yield curve from this method. For the extended Kalman filter, we can predict directly on price, and we're also going to use the particle filter to predict on price, bond price, okay? So, you know, we want to predict the yield using the estimated state variable, okay? And we have some variants of some noise term here. My very first paper, I was doing something similar for commodity prices with Kalman filters and different kind of estimation procedures. And someone said that adding on this noise was going to introduce arbitrage. And I very nicely proved that it didn't, but they didn't care. But in any case, you don't really care because we're going to do arbitrage free regularization.
00:20:12.032 - 00:21:14.054, Speaker B: So you can think of this as a statistical kind of thing that we need to use the Kalman filter. Or you could think about it as some noise in the data or whatever else, okay? But it's not going to introduce any arbitrage. So we're going to predict the bond prices, okay? So the Y's are bond prices that we observe or the yields if we're going to use the Kelman filter, okay, so this would be for extended Kelman filter or particle filter. And this up here is for the Kalman filter, okay? So, you know, we can figure out the conditional mean invariance one step ahead. And the Kalman filter is really nice. We have our prediction step, okay, which we, we can use for prediction. And then we have the measurement step, okay? Which updates based on the innovation in the data.
00:21:14.054 - 00:21:58.804, Speaker B: These, the innovation process is the difference between your prediction and your observation. K is the gain. And this is really nice kind of stuff. But I'm going to go pretty fast through this because this is not what the talk is about. The extended Kelman filter is just an approximation that you can use for, for nonlinear observations, okay? Basically Taylor expansion. So the particle filter is a little more complicated. It involves sampling the particles from an empirical distribution, and we're going to assume there's a multivariate generalized gaussian distribution.
00:21:58.804 - 00:22:27.658, Speaker B: So we're not going to just assume gaussian. And, you know, we can figure out the density. And, you know, it's, it's a bit complicated. And you have to use basically MCMC to sample from this. And there's all kinds of, you know, things you have to do. But I can see I'm going a little slow. So maybe I won't go through all the details of the particle filter.
00:22:27.658 - 00:23:39.738, Speaker B: We use this sequential importance resampling, which is kind of one popular method to do this. And basically the one thing I'll just kind of point out is when we're doing this, we use, we were calculating at the same time the extended Kelman filter, and we're using the measurement and updating equations from the extended Kalman filter to get the posterior particles and posterior covariance. And then we get the sample, or sample particles here. Okay, so there's three different filtering methods, okay, which are pretty classical, that can be applied here. And of course you have to update the weights and we do some normalization and, you know, you can figure out the number of offspring and so on. But let's get to how we're going to parameterize this by the recurrent neural networks. I see.
00:23:39.738 - 00:23:44.418, Speaker B: Maybe I should have the chat open. Somebody is asking a question.
00:23:44.546 - 00:23:48.022, Speaker A: Yeah, we can leave the questions to the end. That's how we normally.
00:23:48.118 - 00:24:37.654, Speaker B: Yeah, so that's fine. Okay. So we have our filter based, we use a filter based recurrent neural network for our prediction. Okay, so the model structure, you have input layer, residual layer, state layer and filter layer, and the model parameters, these are kind of interpretable parameters from how we set up the model. We have our state parameters x and or, sorry, the kappa, theta or state parameters, and we have our state variables x, but then we have some uninterpretable parameters, which are the network parameters. And it's a sequential model. You can use the LSTM, which is pretty standard.
00:24:37.654 - 00:25:12.940, Speaker B: But what we're doing is we're learning the state parameters from the historical data. Okay? So this is what we're trying to learn from the historical data. And then we want to also learn the error parameters from the prediction. And then in the filtering, we update the state variable using what we just learned for the state parameters and predict the next state and the predicted yields or predicted prices using the predicted x. Okay. And the measurement error. Okay.
00:25:12.940 - 00:25:53.030, Speaker B: Using the observation y, or we measure the error using the observation yt plus one. Okay. So there's some details on the architecture we're using here. Okay, I don't want to go into it too much. There's a nice picture coming up soon. But you know, you have the input layer has this kind of architecture and you have your hidden values and it's connected. And then we have our state layer, which has three dense layers, okay.
00:25:53.030 - 00:26:13.666, Speaker B: And we use these activation functions. And this is, you know, the, you can read the thesis or the upcoming papers, if you. You want to see a bit more of the details. And then we have our residuals. Okay. And this is the. Sorry.
00:26:13.666 - 00:26:39.196, Speaker B: This is the kind of picture. So at, you know, a given time, your input is the output from the hidden layer. You know, your prediction, your state variable. And this is the r and the I are. That's what I always forget. But you have two h's. Anyway.
00:26:39.196 - 00:26:54.584, Speaker B: And anyway, it goes into our residual. Yeah, residual and input. Okay. And they come from, you know, they go into these. This kind of architecture. It outputs the u, which goes into the filter. This outputs the.
00:26:54.584 - 00:27:15.432, Speaker B: From the input layer. You. You end up getting the, the sigma, the theta and the k. And everything goes into the filter layer. And you start over. Okay, so this is kind of the architecture of the procedure. Okay.
00:27:15.432 - 00:28:11.834, Speaker B: And our loss function is just going to be the mean squared error of prediction error and the arbitrage free penalty. So this is across, you know, the whole yield curve and across time. Okay, so the regularization, the arbitrage free penalty is, if you want to think about it in terms of excess returns or this lambda, that makes the model arbitrage free. Okay. So it's across maturities and across times. Okay, so this is our loss function and our arbitrage free penalty. If you go back to the very first equation, it's a little bit different from, from what was in the paper with Anastasi.
00:28:11.834 - 00:28:43.182, Speaker B: And we could use in our loss function this hit rate. Okay, but we don't. Our loss function is just given by this. But we calculate this hit rate. So you want to be within the spread if you want. But I don't know what the spread is on bonds. I found a paper where they said it was maybe sometimes $0.45
00:28:43.182 - 00:29:13.174, Speaker B: or whatever, but this was a pretty old paper. So bonds, you know, bonds are not bonds. Bond markets are still pretty low tech in some sense. Some of them, you have to do these. I mean, a lot of the bond people from big pension funds are just calling people on the phone and asking for quotes. You can get some data off Bloomberg. You can use this request for quote system.
00:29:13.174 - 00:30:09.194, Speaker B: But it's, it's not like classical kind of model for a stock of liquid stock or anything, just because, you know, there's so many different bonds with different maturities, coupons, all kinds of things. So it's a bit of a fragmented market. And we were working with some people with industry to do this, but I don't want to get into that partnership right now. So in any case, we calculate this kind of hit rate measure. Okay. We also look at the mean absolute prediction error over all of our samples in the test set or the training set, and root mean squared error. And then just a simple benchmark is, okay.
00:30:09.194 - 00:31:15.774, Speaker B: The yield doesn't necessarily have to move a lot every day, so just uses a benchmark, yesterday's yield or five days ago yield your prediction. If you can't beat that, you may as well not do anything. Okay, so we look at one day ahead prediction, which is not that interesting because the yield curves don't move that much necessarily day to day. So five days ahead is more interesting. A lot of the papers, you'll see there's some papers from Duffy with two e's where they were doing a lot of the papers, they would do weekly or monthly kind of prediction. We did five days because of the data we had. So we have to batch our data into non overlapping sequences, and we, you know, look at these observations on Monday, Tuesday, Wednesday, Thursday, Friday.
00:31:15.774 - 00:32:02.404, Speaker B: And we're restricted by the amount of data we have for that. So we mix these distinct sequences as one training set instead of training them all separately. So, we want to compare the forecast results in terms of maturity across maturity into kind of different buckets, you know, short term, midterm and long term. And we train the three models under, or the three architectures. So, the architecture that I gave you had a filtering, you know, piece in, in it, but I didn't say which filter. So one of them is with the Kalman filter. One of them was with the extended Kalman filter.
00:32:02.404 - 00:32:25.224, Speaker B: One is with the particle filter. And then we also look at, you know, no arbitrage regularization and arbitrage regularization. Okay. And we did do some kind of study. This is just kind of binary, on or off. We did kind, some kind of study to see if you can optimize for the optimal lambda. But I'm not going to talk about that today.
00:32:25.224 - 00:33:07.806, Speaker B: So if you go, somebody's reorganizing their desk or something. I don't know. Anyway, if you go to the thesis, you can see that little extra table that I'm presenting here today. And there's some discussion of this in the paper with anastasi, but there's a different kind of parameterization of the lambdas, and you have asymptotic results, so it's not directly comparable. But in any case, what we find is, well, what we. How we do this. When we're doing the Kelman filter, we forecast the yield.
00:33:07.806 - 00:33:44.442, Speaker B: So then we want to measure using this forecasted yield. You transform that back to a price, and it's a nonlinear transform, and you want to see how you, you want to be able to compare. So we want to get prices from our predicted yields. Extended Kalman filter, you get your yield, okay, and prices, okay. And the particle filter only predicts the prices. And you have to do an additional step to extract the forecasted yields from the prices. So we use this Google Colab.
00:33:44.442 - 00:34:43.754, Speaker B: So we didn't use a big, big kind of computer to do this, and we just run it 30 to 60 epochs, and we find the kind of optimal results and the run times are comparable. Kalman filter runs in a couple of minutes. Extended Kalman filter just takes a few seconds to do one epoch, and then depends on the number of observations you have. So overall, it's a bit slower than the Kelman filter, but not much. And then the particle filter kind of increases exponentially with the number of particles you use. So you start with 300 particles, and it will take a few hours to do, like one epoch. Okay? So you could probably still train it overnight if you had better computers.
00:34:43.754 - 00:35:45.144, Speaker B: Now, this is just the convergence kind of thing of the loss function model loss for both the training and the test sets. It's not that interesting. I see some interesting questions in the chat, but I'm going to come to them later. So when we do this, so we, we look at our path that we get from the state variables, okay? Under the two different kind of approaches, one with no arbitrage regularization and one with arbitrage regularization. And, you know, we have the Kalman filter, the extended Kalman filter, and the particle filter. This is for the level, this is for the slope, this is for the curvature. Okay? And you see they're pretty, they're pretty close.
00:35:45.144 - 00:36:22.812, Speaker B: Okay. It's not the best way to look at this, but, but, you know, adding on the regularization for arbitrage free seems to improve it a little bit in terms of the level and the slope. Maybe there's some kind of drift after you. You know, you haven't trained your model for a while for the curvature, okay? But you have different performance from the Kalman filter and the particle filter as well. So you have to pick the one. That's right. But this is a more interesting kind of picture.
00:36:22.812 - 00:37:01.704, Speaker B: So here's a couple of different days yield curves that we forecast, okay? Five days ahead. And the blue is what ended up happening. And then these dotted lines correspond to what our forecast was five days before we observed this. And you can see with no arbitrage, regularization okay. On this date, it's a little bit. This is in percentage. It's a little bit.
00:37:01.704 - 00:37:20.082, Speaker B: I wouldn't say that. Good. All right. And then on, if you add the arbitrage regularization, it's close, but you get a little bit of problems as you get closer to maturity. Okay. And this is a nicer one. I like this picture better.
00:37:20.082 - 00:37:38.410, Speaker B: Right. Because it's a more interesting yield curve shape. So this is without arbitrage regularization. Okay. And this is with arbitrage regularization. So we kind of got a little bit away. The Kalman filter looks the best here.
00:37:38.410 - 00:38:34.554, Speaker B: Okay. But, you know, you get before five years maturity, in the mid tenors, you get a little bit of drift away, but it becomes much better on the long end of the curve. Okay, so this is when you're predicting on yield, the Kalman filter, but the extended Kalman filter and the particle filter, you're predicting on price. And you can kind of see this mean absolute prediction error. This is in beeps, you know, for one day ahead and five day ahead prediction, this is not for one particular point tenor on the curve. This is across the whole curve. So you get an average error across the whole curve of three to four beeps using the Kalman filter.
00:38:34.554 - 00:39:16.702, Speaker B: Okay? With no arbitrage regularization, and, you know, three to four beeps with arbitrage regularization. And we have the mean squared error, and it's five day. It's different. But, you know, we had to make some kind of transformation of the data to get this in beeps, so I won't get into that. So if you just look at the yield for the treasuries, this is in the testing set. Okay. Then in.
00:39:16.702 - 00:39:47.374, Speaker B: In the. This is where we're testing on price. Okay, so this is in. Says it's in dollars, but I think that might be cents or. Sorry, the yields. The yields are here. The prices, when it's in dollars, is here.
00:39:47.374 - 00:39:52.022, Speaker B: Okay, so the average, you know, one day.
00:39:52.078 - 00:39:55.566, Speaker A: Cody, just a quick, quick note. You got maybe five minutes. Minutes. And then.
00:39:55.590 - 00:40:03.910, Speaker B: Okay, yeah, it's fine. I'll speed up. Okay. And then you get the hit rates. Um, I like to hit. To look at the hit rates. Um, you can.
00:40:03.910 - 00:40:29.994, Speaker B: The benchmark, uh, for one day prediction, if you just use yesterday's yield, for today's yield, you get 80% hit rate. Okay. So it within ten cents of the price. Okay. Um, which, even when you go to five day, it becomes a little hard to outperform this benchmark. If you go to longer prediction intervals, you'll do it. I'm going to skip this.
00:40:29.994 - 00:41:08.860, Speaker B: This is the average excess return, which you can calculate from our penalty function. And this is just showing that this is, the particle filter is efficient in terms of the effective sample size, but I'll skip that again. And, you know, we get our distribution of our errors, we get a q q plot, which isn't great. Might indicate we need to have a slightly more complicated factor model with fatter tails. Okay. But I want to get to the corporates, so. Oh, actually, this is one really interesting thing, which I'd be interested in.
00:41:08.860 - 00:41:46.354, Speaker B: Feedback. So, when you are estimating the state parameters using the Kalman filter, all right, and this has no arbitrage free regularization, they're pretty stable for a while. But then it's like you have a regime change kind of thing. All right? And you see this, there's like at least three, two or three regime changes here. But once you put in the arbitrage regularization, the state parameters become very stable. So that's kind of interesting. Corporate bonds, we did ten issuers, and we look at five day ahead forecasting.
00:41:46.354 - 00:42:32.832, Speaker B: One day ahead is not interesting because we don't have enough data. And, you know, we're looking at the forecasting results. We showed that the predicted spread errors, we look at the predicted corporate yields minus the predicted treasury yields. Okay, so these are 14 beeps, kind of on average errors for five day ahead forecasting. So we don't have any kind of credit model on this, which is, you know, something that we would obviously want to put on for a corporate model. But after you're taking off the predicted treasuries, kind of what's left is, is credit. Anyway.
00:42:32.832 - 00:43:18.244, Speaker B: So this is just some description, and these are the results. So the mean absolute prediction error under the different models for apple, let's just look at Apple say, okay, it's, in terms of yield, it's about, the Kalman filter is the best, but it's predicting on yield. So you would expect that about five beeps. But if you predict on price, the particle filter becomes competitive and the hit rate, you know, five days ahead, you can predict the bond price across the curve. Okay. 65% of the time. Okay.
00:43:18.244 - 00:44:05.366, Speaker B: So we did it for these ten issuers that we had data for, and, you know, we got some results, and I'll just put the conclusions up there. I think it increase improves the performance for short time forecasting, the arbitrage free restriction. But I mean, short maturity forecasting, it obviously downgrades it. It's a constraint. So you add a constraint to something, it's going to downgrade it, but it doesn't downgrade it that much. And you can quantify these excess returns and it's pretty efficient to robust and stable this procedure. Okay, so that's about it.
00:44:05.366 - 00:44:08.926, Speaker B: I'll take any questions or I can look through the chat.
00:44:09.070 - 00:44:22.924, Speaker A: Great, thanks. Thanks, Cody. How about I go through and call upon those who had had questions to speak up? So I think, Hutang, you had a question. Would you like to ask it in person?
00:44:24.464 - 00:44:30.664, Speaker B: No, actually I got the answer. So one of the people in this meeting answered me. Thank you.
00:44:30.784 - 00:44:36.400, Speaker A: Okay, great. Brian? Yeah. Your question was probably answered as well.
00:44:36.432 - 00:44:36.944, Speaker B: In one of the.
00:44:36.984 - 00:45:00.824, Speaker C: Yeah, so there's some part, part of my question answered just in terms of the data window. So when you predict, say the five days from now or ten days from now or say even one year from now. So SEO model input, do you rely on the, any HR data, say beyond say today and yesterday? So water data you still need and SEO mod input.
00:45:02.324 - 00:45:38.590, Speaker B: Look, we train the model. I mean this is not how you would implement it. I think this is what you're asking. If I was in the bank, how would I implement it? I can't look into the future to train my model, obviously. So we would, we kind of use a rolling window of, you know, maybe the last couple years of data or something or, you know, 30 days if you want, or six months. And when you're going to retrain your model, you know, you retrain your model. But, but this way that we did it, we had a, you know, a training set and a test set.
00:45:38.590 - 00:46:03.706, Speaker B: And during the test set we didn't retrain at all. It's more kind of classical kind of stuff. So once we train the model, you know, we just work in the test set where after that, you know, it's, it's everything for, if you go back to the picture here, you can see like in here, we're not retraining them up. Yeah.
00:46:03.730 - 00:46:26.334, Speaker C: So I'm not looking for say you, I'm not asking you this. Or maybe my question I didn't ask clearly. So the one we used to model. So the. So, so the what? So we need to rely on some, some kind of input. For example, if I try to predict five days on the bond price or yield from today, so I still need to have some, some kind of input for my model. Right.
00:46:26.334 - 00:46:43.172, Speaker C: Either today yield or just either today you would combine with the, the, say the, the couple days in the past. And so I, I need to, I need to use the input right. For my model. So what is your model input? Is today yield, price or bond price. And they're trying to understand your model input.
00:46:43.348 - 00:47:02.504, Speaker B: Well, it's, to predict tomorrow, I need the predicted thing from the, let's say, the Kalman filter or whatever. So it's not using the, it's not using like the last few days of data or whatever. It's just using today's prediction.
00:47:03.244 - 00:47:03.992, Speaker C: Gotcha.
00:47:04.108 - 00:47:10.088, Speaker B: You know, the five day ahead. Yield or price or whatever else, or facts. Okay.
00:47:10.136 - 00:47:26.520, Speaker C: Okay. That will be the, I will view. That will be today information today in the bank. And we say that today observe market code or marketing information. Okay, so this is clear. So another, maybe just one more question. Say, if you predict, say they project on one year from now.
00:47:26.520 - 00:47:44.848, Speaker C: You know, the, the, I really like, say your model and you, you put restriction about our chart free. So how about the rating, say, one year from now? You know, the, for the bond on the, for certain sector. So even, even the rating. Right. And may have some kind of distribution. Do you have that kind of information or have you ever considered something like that?
00:47:44.896 - 00:47:49.416, Speaker B: If, if I understand your question, you want to put in like the ratings or something?
00:47:49.600 - 00:48:02.348, Speaker C: Yeah, yeah. Just say this FYI project one year from now on. The. So the one, just accuracy, right. So that you can tell another one. Just the, say for the, for the reading, reading information. Say the FF project one year from now.
00:48:02.348 - 00:48:09.420, Speaker C: I like to see the distribution, say, triple aaa on that kind of transition. So it looks like. No. Right. Okay, thank you.
00:48:09.572 - 00:48:10.836, Speaker B: There's no credit model.
00:48:10.900 - 00:48:12.624, Speaker C: There's no credit model.
00:48:14.604 - 00:48:39.328, Speaker B: The only way credit comes in is when I'm looking at corporates, I take off the predicted treasury yields. So I'm just looking at the side. Right. So, you know, if you want to add in extra factors, a credit factor, you can do this. It just has to be within the class of your factor model. I took a three factor. We took a three factor model corresponding to like, a dynamic Nelson Siegel.
00:48:39.328 - 00:48:48.008, Speaker B: And that's all we're using. So you want to put in extra stuff? You can put in extra stuff, but this is with no, essentially no credit features.
00:48:48.136 - 00:48:49.366, Speaker C: Gotcha. Yeah.
00:48:49.390 - 00:48:50.154, Speaker B: Thank you.
00:48:50.654 - 00:48:58.114, Speaker A: Thanks, Brian. Hamid Hilali, would you like to, you have, I guess, two, two questions here. Would you like to ask them?
00:48:58.774 - 00:49:02.070, Speaker B: Yes, I would rather ask the second question first.
00:49:02.182 - 00:49:03.158, Speaker A: Then please go ahead.
00:49:03.246 - 00:49:03.470, Speaker B: Yeah.
00:49:03.502 - 00:49:03.718, Speaker D: Yeah.
00:49:03.766 - 00:49:05.926, Speaker C: Thank you, Kodi, for the interesting presentation.
00:49:05.990 - 00:49:47.244, Speaker B: I would like to ask that, how can we include the changes, like changing policy rate into our model? I, I don't really. I mean, that's like economic macro kind of stuff, maybe. I don't really know anything about this. I'm more, you know, classical guy. But I was thinking about it a little bit, like, you know, I know there's all kinds of interesting stuff you can do. And I was looking at yield curve control that they're using, and everybody's talking. I listen to, like, odd lots or Bloomberg market surveillance podcasts, and they're always talking about these things.
00:49:47.244 - 00:50:30.634, Speaker B: And I was thinking about it, and, you know, you could probably put some of this into your factor model as constraints if you wanted to, but it's not something that I've tried to do in any way. But you could take a macro model as part of your factors, and as long as you can embed it in a class of models that might in some sense be arbitrage free, that you can then pick the closest arbitrage free to your factor model, which includes macro factors, policy rates, you know, whatever else, then I guess you could do it. But it's not something that we've tried to do. Thank you.
00:50:33.754 - 00:50:34.654, Speaker A: Philip.
00:50:36.594 - 00:51:05.604, Speaker D: Yeah, thank you. Yeah. So just regarding the forecasts, it wasn't clear to me, sort of what the day zero curves were. So you've got a certain curve at the beginning of your forecast, your testing set. Right. And then five days out, you've got these predictions, and you compared them to the observed in five days. But it would be really interesting to know whether the forecast and the observed are both, for example, above the starting curve or below the starting curve.
00:51:05.604 - 00:51:11.544, Speaker D: Like, you know, often the curve isn't that volatile. So maybe just holding a constant would have done.
00:51:12.484 - 00:51:53.334, Speaker B: I mean, you can. Well, that was our benchmark is basically taking today's curve is your prediction for tomorrow or five days from now. Right. And, you know, that's the problem is it's, it's oftentimes pretty close to the same. Right. So, yeah, if you want to get into this, as you getting directional kind of stuff or, you know, if your prediction is like above or below, maybe like some of the things we could look at would be metrics of, you know, the actual kind of number doesn't really matter. It's, it's in which buckets are you, above the current curve or below.
00:51:53.334 - 00:52:07.610, Speaker B: And the traders can put some trades on because of this, which I don't know what they would do. But, but you could look at this, but we didn't really think about it, is that, I don't know if that is answering.
00:52:07.722 - 00:52:12.538, Speaker D: Okay, so you didn't, you didn't really think about it, but you did include it in your benchmark, but it's not.
00:52:12.666 - 00:52:49.764, Speaker B: So then that's the benchmark. Right. It was like, you know, we get nice maps and blah, blah, blah, but then you get pretty good ones if you don't even do anything. Right. So you want to obviously beat that. Why should I implement a model that I have to train for hours, you know, blah, blah, blah, if it can't beat doing nothing right? And in some, for very short estimation, I mean, we'd have to look at the particular days and things like this more to get into this, because for one day ahead, I mean, it doesn't. You may as well just keep.
00:52:49.764 - 00:53:15.394, Speaker B: Keep today's yield for tomorrow, unless there's a policy meeting or something, micro thing, you know, I don't know. But for five day ahead, we can beat it over on average, you know, by a few percent. And this is with very few features, so we'd probably want to. If we were going to use this for something else. I mean, I think this is, like, one piece of a larger model that you would have. Right.
00:53:16.214 - 00:53:17.154, Speaker D: Thank you.
00:53:18.694 - 00:53:22.686, Speaker A: Hopefully, Philip, that answers your question. Yeah.
00:53:22.710 - 00:53:22.894, Speaker B: Good.
00:53:22.934 - 00:53:27.102, Speaker A: Okay, so, Jose Dion, we'll take this one last question, and then.
00:53:27.158 - 00:54:21.402, Speaker B: Yeah. Okay. Could you please elaborate more on the way the arbitrage free penalty is constructed? Yeah. Well, if you go back to this theorem, which is not that exciting a theorem, but it's just a standard kind of HGM kind of drift restriction relating to volatility and the drift that you have to have for the model to be arbitrage free. And then, you know, we. We integrate it over, you know, the time period and the curve that we're doing. So, you know, this function, and once you've got estimates of all of these kappas and x's and thetas, you could calculate this quantity empirically at different points on the curve.
00:54:21.402 - 00:54:56.470, Speaker B: Right? And so if we go to our loss function. Okay, so if we go to our loss function. So, you know, we're summing across j is across tenors, and I is across time. Okay. So for however much training data we're using and across all the tenors that we have data, we're calculating this. This lambda, and. Sorry.
00:54:56.470 - 00:55:19.510, Speaker B: And, you know, that's our penalty, the square root of this. And then we sum it up. Okay. You could do other things, but this is kind of what we do to approximate the theoretical thing that we need. And then, obviously, you're controlling. You put, like, a parameter to control how much you want to penalize by. Sure.
00:55:19.510 - 00:55:20.474, Speaker B: Thank you.
00:55:21.434 - 00:55:51.720, Speaker A: Okay. I think we're close to the end, but being the chair, I'd like to take the privilege to ask one little question. So you just mentioned the no arbitrage conditions, but for HDM model. But my understanding is that that's no arbitrage under the risk neutral measure. Right. So what is it that I'm a little bit confused about? What is the p evolution model that's being estimated and filtered and predicted versus what is the. What is.
00:55:51.720 - 00:55:54.856, Speaker A: Why there is this no arbitrage condition on those p evolutions?
00:55:55.000 - 00:56:05.824, Speaker B: Well, I mean, we're. We're working under a. No, an. Under a p measure or a q measure. Okay. I mean, this model is formulated under a q measure, but the idea.
00:56:05.864 - 00:56:08.404, Speaker A: You're making real predictions under p, though, right?
00:56:09.864 - 00:56:38.724, Speaker B: Well, they're bond prices, so they're just bond prices under p or q. It doesn't make any difference. So, you know, it's just, you formulate a general HTM model, and if you want the bond prices to be discounted, martingales. All right. You have to have this drift condition. Okay. We have a very general, I mean, you formulated as a very general.
00:56:38.724 - 00:56:41.132, Speaker B: Right.
00:56:41.188 - 00:57:09.064, Speaker A: I guess I'm thinking, I mean, on a particular day, if you want to compute the yield curve or the bond prices, then, yes, you have the drift condition on the evolution that you need in order to write down the. In order in order to compute those prices. But then this curve, as it evolves forward in time, that's a key real world evolution. And so I guess, I don't know. I may be slightly confused about the way that they're interacting, but we can talk about it offline. Cody, that's.
00:57:09.144 - 00:57:30.768, Speaker B: Yeah, I think we did something in the paper with anastasi because some referees had some, some questions about this. It involves, like, no free lunch with vanishing risk and stuff that I haven't looked at for a couple of years. But it's okay. I mean, I think.
00:57:30.896 - 00:57:31.632, Speaker A: Okay.
00:57:31.768 - 00:57:34.984, Speaker B: It's a question of the viewpoint and. Right.
00:57:35.884 - 00:57:44.516, Speaker A: All right, great. If there's any further tiny, tiny question, we could ask it, otherwise, we'll.
00:57:44.700 - 00:58:24.914, Speaker B: I see this Philip question. It's. It's under queue, but I mean, the point is you can use this to detect arbitrage in some sense, like arbitrage opportunities in the data. Maybe that was discussed a little bit in the paper with Anastasi. I skipped over it here. You can calculate these average excess returns, which are in different tenors that you end up getting from your predictions, and you could use this to develop some trading signals and things. But I don't get into that kind of stuff.
00:58:26.434 - 00:58:32.994, Speaker A: Okay, well, with that, thank you very much, Cody, for the interesting paper and discussions. So, virtual collapse all around.
