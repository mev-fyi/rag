00:00:06.840 - 00:00:08.154, Speaker A: Okay, I think it's done.
00:00:19.374 - 00:00:20.086, Speaker B: Okay.
00:00:20.190 - 00:01:25.354, Speaker C: So, ready to begin this session? It's my pleasure to introduce Professor Keith W. Hypal from systems design engineering professor at the University of Waterloo. He's an adjunct professor, but also distinguished professor emeritus. He leads the conflict analysis group, he's former president of the Academy of Science within the Royal Society of Canada, senior fellow of the center for International Government's innovation, fellow of the Basilisk School of International affairs, and his research interests fall within environmental system science and engineering. Professor Heipel has received many honors, including the. He's an Officer of the Order of Canada, Killen Prize in Engineering, Japan Society for Promotion of Science, Eminent Science Scientist Award, Joseph G. Wall Outstanding Career Award from EEE Systems man and Cybernetic Society, the IEEE System Command Cyberkinetic Society, Norbert Wiener Award, and so on.
00:01:25.354 - 00:01:31.674, Speaker C: There's a lot more, but I think that you get the idea. So please welcome professor.
00:01:38.454 - 00:02:08.914, Speaker A: Thank you, Christopher, for your introduction. Hello, everybody. Who are students here? Which ones are students? Okay, great. And the rest of us, faculty and staff. Okay, well, this is my first time I've given a lecture in four years. Had a stroke about four years ago, two very bad ones. But luckily I recovered and through exercise and better diet, getting better and better.
00:02:08.914 - 00:02:47.350, Speaker A: So it's real nice to try myself out again. So, since I finally in a class again, there are some students here, but we gotta have a test, right profs? We gotta have a quiz. So let me start this out with a little quiz. Since this is, I guess, the Fields Institute combined with UC, let's have a mathematical quiz. Who is often referred to as the father of geometry and often the ancient work elements, which was used in mathematical instruction up to the 20th century. Until the 20th century. Anybody can play.
00:02:47.350 - 00:02:58.114, Speaker A: Who said that? Somebody could. You're right. Yeah. Please come up with a friend here. You get a prize. Yeah.
00:03:01.054 - 00:03:01.430, Speaker B: Yeah.
00:03:01.462 - 00:03:14.604, Speaker A: Congratulations. Tell us your name and your research here, Thomas. I'm in engineering. What type? Ece.
00:03:15.664 - 00:03:16.680, Speaker B: Cryptography.
00:03:16.832 - 00:03:49.944, Speaker A: Cryptography, yeah. So I'm working on like a centralized communications medium because like, I guess like Marshall McLuhan said, the medium is a message, right. And famously didn't see the name, and I guess the end like. And I just thought, yeah, working on like medium using both like kind of a compute to peer architecture. Who is your super piper? I'm doctor. Excuse. So I'm like, master's.
00:03:49.944 - 00:04:05.914, Speaker A: Very good. Okay, well, let's. Good luck in your research. Thank you, Thomas. Yeah. Okay, so you got that right. Very important here at water.
00:04:05.914 - 00:04:11.614, Speaker A: Water cryptography, part of the quiz later. We'll deal a bit with that.
00:04:12.074 - 00:04:12.570, Speaker B: Okay.
00:04:12.602 - 00:04:18.254, Speaker A: Who introduced the decimal positional number system to the west, as well as the terms algorithm and algebra?
00:04:23.194 - 00:04:24.454, Speaker C: Al Kharismi.
00:04:25.364 - 00:04:26.116, Speaker A: Go ahead.
00:04:26.220 - 00:04:27.464, Speaker C: Al Khwarizmi.
00:04:28.164 - 00:04:30.864, Speaker A: Yeah, you're right. Can you say the whole name?
00:04:31.644 - 00:04:34.264, Speaker C: Muhammad ibn Musa al Khwarizmi.
00:04:34.724 - 00:04:35.108, Speaker B: Right.
00:04:35.156 - 00:04:47.904, Speaker A: Okay, great. Come on up here. I'll put this over here.
00:04:49.364 - 00:04:51.864, Speaker C: And expect primates over the table.
00:04:55.014 - 00:04:55.486, Speaker B: Okay.
00:04:55.510 - 00:05:02.558, Speaker A: Well, here we are. Congratulations. You get a pen, too. Oh, thank you.
00:05:02.726 - 00:05:07.054, Speaker B: Just a minute, everybody there, what color?
00:05:07.094 - 00:05:19.954, Speaker A: Green. Okay. Okay, tell us about your area of research. And Christopher.
00:05:20.884 - 00:05:33.744, Speaker C: Complex systems applying mathematics in unexpected places. Algebraic automata theory, but also artificial life. And I'm going to talk here about carbon currencies.
00:05:38.044 - 00:05:39.984, Speaker A: Let's have a round of applause.
00:05:42.284 - 00:05:42.884, Speaker B: Thank you.
00:05:42.924 - 00:05:45.544, Speaker A: So, one, one. Now it's time. Props, mercury, students.
00:05:49.744 - 00:05:50.120, Speaker B: Yeah.
00:05:50.152 - 00:06:07.680, Speaker A: So, as you know, Persians developed a lot of ideas in science and mathematics. I have a good friend from the United States. He was going to write a book about persian science, and I keep telling him, write in a worthwhile paper. I've seen enough of your technical papers on large scale systems, but it's quite.
00:06:07.712 - 00:06:10.644, Speaker B: A history, quite an illustrious history.
00:06:11.784 - 00:06:28.224, Speaker A: So let's try another one here. Okay, so algorithm is the last latinized version of his name. Algebra is latinized version of algebra from the title of one of his works, so really contributed a lot to humanity. This gentleman.
00:06:34.404 - 00:06:36.956, Speaker B: Here doesn't seem to be.
00:06:36.980 - 00:06:45.644, Speaker A: Worship to the errand. Okay. Who had been at cartesian coordinates.
00:06:48.304 - 00:06:49.444, Speaker C: Come on, students.
00:06:50.744 - 00:07:06.412, Speaker A: Oh, yeah. Ray. Yeah, please come up here. Renee. There we are. Great, congratulations.
00:07:06.588 - 00:07:07.100, Speaker B: Thank you.
00:07:07.132 - 00:07:08.484, Speaker A: Please introduce yourself.
00:07:08.644 - 00:07:19.100, Speaker D: Hi, I'm Deirdre Haskell, and I work in mathematical logic, specifically model theory. And I am the deputy director of the Fields institute. Just climb here today.
00:07:19.172 - 00:07:20.732, Speaker A: Oh, okay. Well, very nice to meet you.
00:07:20.788 - 00:07:21.924, Speaker D: I can say that. I'm sure.
00:07:22.004 - 00:07:23.532, Speaker A: What color pen would you like?
00:07:23.668 - 00:07:25.100, Speaker D: I'll go for green one as well.
00:07:25.212 - 00:07:56.208, Speaker A: Okay. Green's popular. I think that's a real nice combination field institute with ishi because I'm a real fan of mathematics and mathematicians. Once you can attach mathematics to it, then of course you can develop. You have a lot of really good sense you make out of whatever you're doing. Then you can develop a decision support system and make your decision technologies available to others. So, extremely important.
00:07:56.208 - 00:08:06.684, Speaker A: What a nice marriage of ideas at this conference. So who invented mechanical calculator and collaborated with Pierre de Fermat in the development of probability theory?
00:08:14.824 - 00:08:17.524, Speaker B: Couldn't hear you. These.
00:08:22.424 - 00:08:44.664, Speaker A: I don't think you said this, so gave me the answer. If you did so blaze. Pascal. We actually had the Pascal lectures at Waterloo. How many have been to a Pascal lecture? They usually bring in really good speakers over the years. Who invented infinitesimal calculus?
00:08:46.644 - 00:08:47.384, Speaker B: Who?
00:08:47.964 - 00:08:59.124, Speaker A: Lyman's. Right? Yeah. Right. Yeah. Please, come up here. I'm glad you did it. Not out of order, because most people say Newton first, but lively has actually had more impact.
00:08:59.784 - 00:09:01.960, Speaker B: Okay. And your name?
00:09:02.112 - 00:09:07.884, Speaker A: I'm Aniriza. Nice to meet you. Nice to meet you. Congratulations. What color pen would you like here?
00:09:08.784 - 00:09:12.960, Speaker B: Everyone went to green, but we got purple, too.
00:09:13.112 - 00:09:30.914, Speaker A: You're in mix department then, Theresa? I'm in civil and environmental engineering department, but I'm not a civil engineer. You're a mathematician? No, I'm an industrial engineer. A civil engineer. But you're a Persian. I'm a Persian, yeah. Okay. What color pen would you like here.
00:09:31.854 - 00:09:34.350, Speaker C: Since we're going to go out of green echo school.
00:09:34.462 - 00:09:38.398, Speaker A: Okay. Here's a nice blue one. Well, that's a nice green color, too, in a sense.
00:09:38.486 - 00:09:39.542, Speaker B: Yeah. Thank you.
00:09:39.598 - 00:09:58.420, Speaker A: You're welcome. Okay, tell us about your research, please. So, at our lab, we try to connect environmental impacts of air quality and climate change policies to human health and try to, like, create monetary values for.
00:09:58.452 - 00:10:01.972, Speaker B: Them and to inform policy and.
00:10:02.068 - 00:10:08.932, Speaker A: Sorry, Mary, that was rich. Department a. Civil and environmental. Civil environment. Who are you working with there? I'm working with Doctor Rebecca soy.
00:10:09.108 - 00:10:10.064, Speaker B: Very good.
00:10:10.364 - 00:10:12.224, Speaker A: Okay, well, let's hear for Miriam.
00:10:14.154 - 00:10:14.934, Speaker B: Thanks.
00:10:20.034 - 00:10:40.948, Speaker A: There's quite an interesting story here. Leibniz and Newton both independently developed calculus. Now we got something to learn from both of them when it comes to research. What is it about Leibniz's calculus? It was superior to Newton's. Leibniz is below from the left here. That's right. Exactly.
00:10:40.948 - 00:10:59.020, Speaker A: Very good. The notation. Leibniz had a much better notation. That's the notation, of course, that we use today with the differentials. Whereas Newton had that capital d, I remember, for a derivative. But there's something I really liked about Newton. He used the need for practical applications to develop his theory.
00:10:59.020 - 00:11:22.632, Speaker A: So, for example, his real interest was physics. And he had an intuitive idea how he thought physics works. And he knew that somehow he had to be able to model mathematically. The rate had changed. So he developed calculus to solve a practical problem. And so, to the mathematicians in the room, there are a lot of really tough practical problems to solve in climate change. So we need a lot of good mathematicians.
00:11:22.632 - 00:11:43.034, Speaker A: So there you go. Follow in the footsteps of Sir Isaac Newton. That's what he did. Who invented the concept of a phone function? And introduce or popularize the mathematical symbols f of x, PI, ie, gamma and sigma. Mathematicians really like this chapter.
00:11:46.014 - 00:11:48.118, Speaker B: Gauss, not ghost.
00:11:48.246 - 00:11:55.038, Speaker A: Well, I'm a fan of Gauss, too. I really like Gauss. And he's also from a german speaking country, Euler.
00:11:55.206 - 00:11:55.686, Speaker B: Euler.
00:11:55.750 - 00:11:56.514, Speaker A: That's great.
00:11:57.474 - 00:11:58.194, Speaker B: Okay.
00:11:58.314 - 00:12:09.074, Speaker A: There we are. Leonard. Oily Swiss. So, Chris, that was you? Or was he you, counselor? Chris. I already have a pen. You can get a pen for your wife.
00:12:09.154 - 00:12:09.814, Speaker B: Oh.
00:12:13.314 - 00:12:19.014, Speaker A: Okay. Okay. Very good. Congratulations. I like Euler a lot, too. He's a great mathematician.
00:12:26.934 - 00:12:27.886, Speaker B: Who pioneered the.
00:12:27.910 - 00:12:35.154, Speaker A: Application of operator theory to quantum mechanics, inspired by the game of poker, was a key figure in the development of game theory.
00:12:36.054 - 00:12:38.462, Speaker D: I'm going to guess von dawn.
00:12:38.558 - 00:12:55.894, Speaker A: That's exactly right. Don, please come up here. Von Neumann. He's one of my favorites. John von Neumann. Which color would you like?
00:12:58.154 - 00:13:02.986, Speaker D: I'm sorry, but I do want. I want the green one or maybe a blue one.
00:13:03.010 - 00:13:03.734, Speaker A: I'll just.
00:13:04.474 - 00:13:05.734, Speaker D: The green one's easier.
00:13:06.994 - 00:13:10.250, Speaker A: Okay, Donna, tell the group about your research, your current position.
00:13:10.402 - 00:13:32.018, Speaker D: She's professor in the school of planning. Secretly an economist. I was excused from the fold. And right now we're focusing on development of agent based land and housing market models and also understanding the environmental impacts of residential landscaping. And then probably something else I forgot.
00:13:32.106 - 00:13:32.826, Speaker B: Yes.
00:13:33.010 - 00:13:33.786, Speaker D: Thank you.
00:13:33.890 - 00:13:43.774, Speaker A: Well, thank you. Don was director of wiki for how many years? Four, I think. You were one of the founders, right?
00:13:45.614 - 00:13:51.074, Speaker D: I'm that person behind the scenes that still tells everybody else what to do, even though I don't have an official.
00:13:51.894 - 00:13:56.862, Speaker A: Yeah, I know. Yeah. Nice to see you again.
00:13:56.998 - 00:13:57.874, Speaker D: That's great.
00:14:03.334 - 00:14:16.734, Speaker A: Here he is, right here. John von Neumann. He's one of my favorite mathematicians. Very popular with engineers. Does anybody recall the title of the famous book he and somebody else wrote in a co author?
00:14:17.634 - 00:14:19.466, Speaker C: Oscar Morgenster.
00:14:19.490 - 00:14:26.414, Speaker A: Yeah, right. Yeah. Come up here, Christopher. How many children do you have? How much.
00:14:28.194 - 00:14:29.174, Speaker B: More pens?
00:14:31.434 - 00:14:39.698, Speaker C: Yeah, it's like. I can't actually know. I don't actually know. Theory of games and economic behavior.
00:14:39.746 - 00:14:41.204, Speaker A: Exactly. Very good.
00:14:41.244 - 00:14:42.260, Speaker C: Yeah, I tried to read that.
00:14:42.372 - 00:14:47.304, Speaker A: John von Eiman and Oscar Morgenstern. What color would you like?
00:14:49.764 - 00:14:53.704, Speaker C: Ok, I'm cheating because I am a mathematician.
00:14:57.084 - 00:14:58.784, Speaker D: Let's freeze right throughout.
00:14:59.684 - 00:15:01.064, Speaker B: Okay. Thank you.
00:15:06.664 - 00:15:22.244, Speaker A: That John von Neumann, he was hungarian. He was born in Budapest, but actually, the apartment that he lives in is now a bit of a museum, but he's very popular with engineers. John von Neumann. And he actually died of cancer, but does anybody know why?
00:15:24.304 - 00:15:25.088, Speaker B: Sorry.
00:15:25.256 - 00:15:27.024, Speaker C: Radiation radioactivity.
00:15:27.064 - 00:15:57.524, Speaker A: Who said that? Oh, yeah. That's close enough. So he went again. Come up here. So if you have a wife or a girlfriend, then you'll be fine. I'm back up here. How did he die of radioactivity? I think he was working on the Manhattan project or they used to take some of the famous scientists and mathematicians to watch the explosions in America.
00:15:57.524 - 00:16:40.234, Speaker A: I mean, there is a goose that is very aggressive, but it's right. What would you like? What color? Okay, I guess I'll take the. So is Amir right? Thomas. Thomas, right. Okay, great. Now we're getting closer to cryptography who is widely regarded as a father of theoretical computer science and artificial intelligence and played an instrumental role at Blushley park in breaking the Amiga accord. Imitation.
00:16:40.234 - 00:16:47.354, Speaker A: I know, the movie. Yeah. Remember the name of the person?
00:16:53.584 - 00:16:56.404, Speaker D: There's a. He has a test date after.
00:16:56.944 - 00:17:10.256, Speaker A: That's right, yes. Yeah, yeah, yeah, he got it. Come up here. That's right. Beloved persons are cleaning up what they.
00:17:10.280 - 00:17:12.944, Speaker B: Learned thousand five years ago.
00:17:12.984 - 00:17:18.044, Speaker A: They still remember, I guess. Okay, congratulations. What color would you like?
00:17:21.864 - 00:17:22.840, Speaker B: Okay, great.
00:17:22.992 - 00:17:45.884, Speaker A: Tell us what you're working on for your research. I'm working on climate mitigation, specifically on a technology called directory capture which starts CO2 from the air and on the ground. And I'm working mostly on the policy side of that technology and how you should make the decision under deep uncertainty. Yeah, my supervisor is Professor Hoytal and mister. Schwaizen.
00:17:49.184 - 00:17:49.544, Speaker B: Yeah.
00:17:49.584 - 00:17:56.044, Speaker A: Very important research topic. It out of hand. We gotta extract directly from the atmosphere.
00:17:57.744 - 00:18:02.704, Speaker B: Okay. Okay, great.
00:18:02.744 - 00:18:25.364, Speaker A: So here's right here. Alan Turing, a brilliant man. He ended up after the war at the University of Manchester. Or it was probably called Umisen University of Manchester Institute of Science and Technology. But there isn't a small park in Manchester. There's all along there. So I thought I'd go over and say hello, see how I was doing.
00:18:25.364 - 00:18:56.304, Speaker A: How many have seen that statue in Manchester? It's a little part not too far from the university. And that's a hungarian friend of mine, immer Rudas. He's president of a small university, a small technical university of Budapest. So we were speaking together there with Turing. And there, you see, he's holding an apple. Because he was so badly discriminated against, he got highly depressed and he committed suicide. He put cyanide in an apple and then ate the apple.
00:18:56.304 - 00:19:12.644, Speaker A: And guess who copied that? Where's the cyanide now? How many have seen the imitation game? I'm trying to figure out. Why did they call it the imitation game?
00:19:15.024 - 00:19:16.400, Speaker C: Because of the Turing test.
00:19:16.472 - 00:19:21.104, Speaker A: Well, one of the best. Maybe that.
00:19:21.144 - 00:19:22.856, Speaker B: Yeah. Okay.
00:19:22.880 - 00:19:37.832, Speaker D: I think the suggestion was that Turing himself, as probably somewhat autistic, was trying to imitate a human being because he didn't know how to be a human being, the same way that a computer tries to imitate a european of the Turin vest.
00:19:37.888 - 00:19:52.432, Speaker A: Right. Thank you. Which country achieved initial successes in deciphering the negro quote before passing on their work to the British? Turing was from the UK.
00:19:52.608 - 00:19:53.480, Speaker B: Well, it's not England.
00:19:53.512 - 00:20:06.500, Speaker A: It's another country. They were ahead of England. There's actually a movie about this. British secret agents were right across Germany before World War Two on german trains to this country. There are actually. Yeah, that's right. Poland.
00:20:06.500 - 00:20:14.144, Speaker A: There are actually three polish scientists here. They are here. I gave this talk once at Warsaw, so I had to, of course, bring Poland into it.
00:20:15.884 - 00:20:18.148, Speaker B: I had a meeting with the president.
00:20:18.196 - 00:20:24.076, Speaker A: Of the Polish Academy of Sciences. How many have seen the big birthday cake in Warsaw? That.
00:20:24.180 - 00:20:27.860, Speaker B: That huge soviet era building?
00:20:27.972 - 00:20:40.228, Speaker A: It's just massive. Right in the middle. Warsaw. It's still there. Okay, now we're getting closer to home. Who cracked the Lorenz code? More difficult than enigma. That was used by Hitler.
00:20:40.228 - 00:20:49.584, Speaker A: The german high command for high level strategic communications at Blushley park made major advancements in graph theory and move to University of Waterloo in 1962.
00:20:52.284 - 00:20:53.344, Speaker B: Who said that?
00:20:55.664 - 00:21:14.804, Speaker A: Yeah, you're right. Come on up here. You just have one wife right here for one of your children. Pick another pen. So it's William Tut. That's correct. There is right there.
00:21:14.804 - 00:21:26.104, Speaker A: One of the most famous mathematicians ever to come to water. Now, what was the most decisive battle of World War Two in the german russian war?
00:21:28.924 - 00:21:29.396, Speaker B: Kursk.
00:21:29.420 - 00:21:33.900, Speaker A: That's right, exactly. Kursk, more important than Stalingrad, actually had. Kursk.
00:21:33.932 - 00:21:36.596, Speaker B: K u R s K. They had.
00:21:36.620 - 00:21:43.984, Speaker A: Broken the german code. So the British were telling the Russians all the moves of what the Germans were planning to do, and they cursed. Salient.
00:21:50.104 - 00:21:51.044, Speaker B: Arcade.
00:21:51.864 - 00:21:54.204, Speaker D: University of Toronto. Sorry, that was down.
00:21:55.184 - 00:22:00.004, Speaker A: Yeah. University of Toronto. Yeah. Vienna.
00:22:00.904 - 00:22:01.328, Speaker B: Yeah.
00:22:01.376 - 00:22:16.174, Speaker A: So Waterloo now has the highest concentrations, of course, and it's very famous for its cryptography institute. Who has studied cryptography? Was that you, Thomas? Yeah. So basically it's like.
00:22:16.834 - 00:22:17.226, Speaker B: I think.
00:22:17.250 - 00:22:23.134, Speaker A: Yeah, we're like common focus optimization. I think we're, like, one of the few schools in, like, North America.
00:22:27.154 - 00:22:27.442, Speaker B: Yeah.
00:22:27.458 - 00:22:30.850, Speaker A: A very important area. And then climate change has been.
00:22:30.882 - 00:22:34.314, Speaker B: Become very important, too, because there'll be.
00:22:34.354 - 00:23:21.644, Speaker A: So much data that has to be collected by the different countries to show they're abiding by the international agreements, and then somehow breaking into their data or intercepting messages could be important. Now, I think that he's probably the greatest canadian war hero from World War two, really in time. And how many have seen his grave? Anybody seen it? Who's heard of west, West Mount Rose? That's where the covered bridge is. It's a little mennonite town just outside of Waterloo. You go to see the mennonite bridge there, the covered bridge. There's one church, just one little united church in the town. Go directly behind it, you'll see the grave site of Bill Tutt.
00:23:21.644 - 00:23:33.944, Speaker A: So there's a nice peaceful rescue there. No one accomplished so much in life. Okay, so I think there. So I guess that's the end of the quiz.
00:23:35.884 - 00:23:36.664, Speaker B: Yeah.
00:23:41.204 - 00:24:18.794, Speaker A: Switch to the other speech. Yeah, that one there. Yeah, that one there. Okay, now for this one, I have a copy of a paper that we published and I'll personally pass it out just because you're me. This is a personal copy to everybody if you'd like one.
00:24:31.234 - 00:24:32.014, Speaker B: Yeah.
00:24:35.194 - 00:25:14.902, Speaker A: So climate change, as you know, can involve a lot of conflict. And we were very interested in environmental conflicts and there wasn't really a good, good game theory technique applied directly to strategic conflicts. And I think you'll see the reasons why as I go on with the talk. So this is a paper that I wrote with two colleagues, Mark Fulwicker and Deeping Fang. See, there are co authors there on the paper. And actually this was one of four or five talks that I gave across Canada. I won something called the Romanovsky Medal from Royal Society Canada.
00:25:14.902 - 00:25:35.530, Speaker A: And part of that awarding, it actually has. There's funding there to pay for lecture tour. They want. Whoever wins it, they want to spread the message around. That was quite nice. Now, Romanowski, he was a metrologist, he worked in a theory of measurement for the National Research Council of Canada. And at that time he was very concerned about nuclear war.
00:25:35.530 - 00:25:50.400, Speaker A: It was a cold war days. So he started this medal for research in the environment. And then what he suspected has come back now, like a war in iron, this demon from the past, because of the russian behavior in Ukraine and elsewhere.
00:25:50.512 - 00:25:55.320, Speaker B: So that's, that's really sad that that's occurring now.
00:25:55.392 - 00:25:57.404, Speaker A: There's another medal since we have the.
00:25:57.864 - 00:26:01.432, Speaker B: Let's see, the Fields Institute here.
00:26:01.568 - 00:26:08.264, Speaker A: There's one called the singe medal. That's a medal the Royal Society has for mathematics. So I strongly encourage you to nominate.
00:26:08.304 - 00:26:10.488, Speaker B: People for that award because they want.
00:26:10.496 - 00:26:43.998, Speaker A: To get good nominations. Synge, I think he was a mathematician at University of Toronto many, many years ago. So what I decided to give today then, as an overview of the work we've done with this graph model. We wanted something that can work well in practice. And the order of the presentation is the same as the order of the paper. Everybody get a copy of the paper. Who wants one? So we were asking the question, how can water resources, environmental controversies be resolved? And like I say, there was not.
00:26:43.998 - 00:27:19.594, Speaker A: There was not. There wasn't a methodology from game theory available that you could apply directly to those problems. So we redesigned something, eventually called it the graph model for conflict resolution, to get strategic insights into conflicts so you can hopefully make better decisions. So the purpose of this talk is to put it into perspective. Try to point out the fundamental design associated capabilities by going through the historical development. You can see the little pointer there. Can you?
00:27:19.674 - 00:27:20.146, Speaker C: Yes.
00:27:20.250 - 00:27:48.814, Speaker A: So this is the genealogy graph that I drew. Let's get it in the paper there. I think about page 14. So up here, let's call them game theoretic methods. On the right here, I call it quantitative preferences, and over here, non quantitative. So here we have techniques, which is sometimes called classical game theory. There's cooperative game theory, extensive form, normal form, and more recently, we have these agent based models.
00:27:48.814 - 00:28:25.716, Speaker A: So over here, in order to use a lot of these techniques, you have to have cardinal utility functions. And that's why we took the word quantitative, whereas over here, it's just relative preference information. So, Christopher may ask me, Keith, after your talk, I'd like to take you for a tea or coffee. What would you like, coffee or tea? Well, I probably say, well, at this time of the day, I prefer to have tea, thank you. I probably wouldn't respond, saying that coffees are at 9.6 and teas were at 3.7. At the present time, you'd say you can get your own tea or coffee.
00:28:25.716 - 00:29:03.280, Speaker A: That's how you respond. But that's the type of information they assume with a lot of these methods over here. Now, we view some of these techniques, too, in some of our research in water and the environment, but not for strategic conflict. They don't work there. So we've done some work in compliance, compliance problems like with nuclear weapons. They want to check are they cheating or not? And now this is a very, very big problem, of course, in the environment, the whole area of compliance enforcement, environmental laws and regulations, you want to make sure that the people who sign the agreements actually abide by. The trouble with some of these approaches here is they get.
00:29:03.280 - 00:29:38.392, Speaker A: They get quite complicated, like extensive form gets complicated very quickly. So as I mentioned, the non quantitative methods, relative preference. And over here we assume cardinal preference. Here we can handle any finite number of decision makers and options here. Often it's a limited conflict size, and they might compare expected utilities. Over here we look at it more like chest thinking. Let's see, if I move here, my competitor may move there.
00:29:38.392 - 00:30:06.372, Speaker A: So you might be a large company, let's say that makes cars. Let me see, I'm going to reduce the price for this type of car. Let me ask the question, but what will my competitors do? Well, they may reduce the price of their cars too. And then you end up in a worse position because both sides then get less revenue from selling their cars. They got to think of the consequences of your move. So we think in terms of moves and counter moves, which is quite different from the methods over here. They often compare expected utilities.
00:30:06.372 - 00:30:31.464, Speaker A: I know when I think of conflict, I don't calculate expected utility in my mind here, the order of movements is often important. Here it can move in any order. So those are some of the key differences. The big one was the preference information assumed with these quantitative. Now, I should point out here that on both cases, both sides of this sheet here, they're highly mathematical, they're 100% mathematical.
00:30:31.804 - 00:30:34.564, Speaker B: But as I just mentioned, the quantitative.
00:30:34.644 - 00:30:59.352, Speaker A: Means for preference type and the non quantitative is relevant preference. So there is something called metagame. I'm just going to go back a few slides again. I wanted to just go down, let's just go down the left side here. So here we have down this, this, this branch here we have metagame analysis. There's something called drama theory, where they model conflict like a drama. And here we have the graph model.
00:30:59.368 - 00:31:02.352, Speaker B: For conflict resolution, and we have a.
00:31:02.368 - 00:31:28.724, Speaker A: Logical form to it. And we'll have a matrix, we have a matrix form to the model, two to write it down mathematically. I'll say a little bit about that as we go along. But it has a good solid mathematical basis, everything here, and that's so important because you have a good mathematical model. You can say a lot of things about the problem. You're looking at that hopefully, then it's confirmed by data too. So metagem analysis started by Nigel Howard.
00:31:28.724 - 00:31:35.952, Speaker A: There's his book done at the bottom. He was at Potterdoo for a few.
00:31:36.008 - 00:31:39.044, Speaker B: Years, but from the UK.
00:31:42.464 - 00:32:30.818, Speaker A: Well, I'm going to show you this game here, shown in normal form. We like to use a very simple conflict in this paper because we just wanted to get the basic ideas across. Like today was the meeting. I could have shown you a real complex, real world problem. This could apply to many different types, including phosphorus. Tell them about the problem, you applied it to the graft model classroom? Just very briefly, yeah, it was when I had the conflict resolution course for the term project, I applied the same methodology on the carbon pricing debate between the federal government and Alberta that finally they went to supreme court and it was a multilevel game. So I just used this methodology for modeling that decision making and it works out as a good multi level game.
00:32:30.818 - 00:32:57.348, Speaker A: Okay, great. Thank you and good luck in finishing everything. As you say, publisher Perish. But here's a very simple conflict. It actually has the same structure as prisoners dilemma. That's a very small conflict to reflect should I cooperate or not cooperate with somebody else? So here, cooperate. The decision maker agrees to significantly reduce greenhouse gas emissions.
00:32:57.348 - 00:33:27.898, Speaker A: Don't cooperate, does not agree to that. So here are very simple two players, US and China. The US is a role player here, and China is a column. So here is c for cooperating. So if China chooses c and the US chooses c, we get this state here called CC here. If they cooperate in China doesn't, it's a state CD. And just, just to make the explanation easier, although it might seem confusing, now, we give each state a number here, one, two, three and four.
00:33:27.898 - 00:33:53.320, Speaker A: Now, these other numbers here represent preference. So the first entry is a preference of the US, where a higher number means more preferred. More preferred. So here, the state BC, that's where the Americans don't cooperate. They don't spend much money in cleaning up, whereas American China does. They're cutting back significantly on the greenhouse gas releases and spending a lot of money. So that's also the least preferred for China.
00:33:53.320 - 00:34:26.964, Speaker A: Notice that has a one. So this is relevant preference because just in ordering of the states, like I say, a higher number is more preferred. Think of them being ordered for most preferred in this case. So we call that orbital preferences because it's an ordering. I'm going to use that as a basis of discussion, just to keep it simple. Trouble is, if I show a really complex one is then we get bogged down looking at the details of the history and what happened in the conflict. I'm not hard with what he called the breakdown of rationality.
00:34:26.964 - 00:34:30.640, Speaker A: So actually rationality, Nash equilibrium.
00:34:30.672 - 00:34:33.524, Speaker B: This produces predict state.
00:34:34.024 - 00:34:52.803, Speaker A: It misses, it fails to predict CC. So you look at these states here. Notice that DD, that's where they both don't cooperate. Not only has payoffs of two and two, whereas Cc has three and three. So it's more preferred by both players, but both, we call them decision maker. We like that word better.
00:34:54.223 - 00:34:56.615, Speaker B: Now this, this one here, if a.
00:34:56.639 - 00:35:09.455, Speaker A: Game were at DB. The United States controls movement in the rows. So if they went from a d to a c, don't cooperate, to cooperate, they go from a preference of two to a preference of one. So it's actually a, you know, out.
00:35:09.479 - 00:35:12.064, Speaker B: Of this improvement, whereas.
00:35:13.964 - 00:35:36.836, Speaker A: And likewise, if China, then they could go from a d here to c there. So if they went from d here to c here, we get the state DC, they go from preference of two also to one. So it's a disimprovement for them too. So there's no unilateral improvement by either player. So they probably stay there. And that's what Nash equilibrium predicts. DD.
00:35:36.836 - 00:35:38.252, Speaker A: And you can see that it clearly.
00:35:38.308 - 00:35:42.900, Speaker B: Misses the obvious solution here, CC, which.
00:35:42.932 - 00:36:38.974, Speaker A: Is where they both cooperate, and then they both benefit a lot because the greenhouse gas cutbacks, a lot of serious side effects there, hopefully eliminated. So Howard developed a number of stability definitions, I'll just say what they are verbally, but all this is really moves and counter moves, rather than taking expective values and that type of thing. So general meta rationality, every UI that's uno improvement by decision maker can be sanctioned by counter moves above the decision maker. Like I said, if one car company produces its prices for a certain model, then the other companies do the same, so they actually end up in worse position. They're better off not to move then. Symmetric meta rationality. Each unit lateral improvement by decision maker can be blocked by counterweise, an opponent and the focal decision maker cannot escape.
00:36:38.974 - 00:37:19.184, Speaker A: I can move its block and then I can't escape from that sanction. So here, GMR stands for gentleman irrational SMR, symmetric meta rational. So here, for example, notice us, if you were at CC, they could change their strategy selection here from cooperate to not cooperate. And notice it goes from payoff three to payoff of four. So that's a unilateral improvement. It's more preferred for the Americans, although I wish it weren't. But in this game that we set it up, it is.
00:37:19.184 - 00:37:20.984, Speaker A: Now, once we're here.
00:37:22.764 - 00:37:23.860, Speaker B: China could actually.
00:37:23.932 - 00:38:07.658, Speaker A: Change from a c to a d and change the preference from one to a two and move the game over to DD. Now, DD, notice for the Americans, there's a three here and a two here. The DD is less preferred by the United States to CC, so therefore they're sanctioned. So this is actually general meta rational and ends up sequential too, because they're sanctioned by a unilateral improvement by China. So I hope that sort of comes through. But there, that idea moves and counter moves. So you can see that this normal form, that box like structure there, is convenient for two decision maker cases, but becomes troublesome for conflicts with more than two decision makers.
00:38:07.658 - 00:38:20.134, Speaker A: It's a hard proposed auction forum. Let me show you this in table two. And here we have two decision makers, us and China has one overall option, cooperate.
00:38:21.134 - 00:38:21.518, Speaker B: Why?
00:38:21.566 - 00:38:50.106, Speaker A: Here means yes, cooperate, no means no, don't cooperate. And China has the same one. So they have cooperate where they cut back in their greenhouse gases. And we have the same four states here as we did in the game of normal forum. But it's written using this option forum. Now you can actually extend this list down as far as you like, any number of decision makers and options. So size does, so the size of the game, well, it does become a problem later on in certain algorithms because we don't want it to be too big.
00:38:50.106 - 00:39:34.104, Speaker A: So we're removing feasible states and things like that. But we can handle any sort of finite number of decision makers and options. So you can see it's a nice notation and say, well, what can I do? What can my opponent do? Those are the options. And then Neil Fraser and I, his PhD in mine, years ago, we wrote a book on this, we call it a conflict analysis, where we extended metagame analysis. And as I mentioned in the sequential stability, then I think I mentioned it briefly, that considers the idea of having credible sanctions. In other words, I'll block the other players improvement as long as it doesn't hurt me. And that just sounded very sensible.
00:39:34.104 - 00:40:16.456, Speaker A: So here's the game in option forum. And once again, let me repeat every table and figure given this on the overheads is also given in the paper here. Now just to show you the movement here. So here we are at state, let's say yy, that was CC sv take off rate. So the US has a unit improvement to here by going from a y to an end, you move from state one to state three. But then Chinese, you could go from a y to an, move the game from here over to here. And it's unit a lot of improvement for China, but forward is actually less preferred than one by the Americans, so they end up worse off, so therefore they're better off not to move.
00:40:16.456 - 00:40:47.584, Speaker A: And China can block it without hurting themselves. When there's more than one Ui, all uis have to be blocked to induce seq stability. When there are more than two decision makers, credible sanction can be generated by uis based upon uis among the sanctioning decision makers. So you can see the way this can become complicated. You got to keep track of things when there's no ui master ability there. There is also s eq. I was going to ask you a question, but I actually have the answer here, so I'll ask a question anyway.
00:40:47.584 - 00:41:10.564, Speaker A: Now, we have an existence theorem for seq. We can prove mathematically that all is predicted, at least one equilibrium. And why is that important? That's the nice thing about math, mathematics. You look at the properties of the models you're developing. Why is it important to be predicted least one equilibrium?
00:41:11.544 - 00:41:16.644, Speaker D: So you can get tenure in the economics department. No, I'm sorry, I don't know the answer.
00:41:16.984 - 00:42:04.540, Speaker A: Well, the answer is something always happens in reality, even if it's all late nuclear war. So if you have a game theory method that can't predict an equilibrium, you should take a more serious look at it. Is it realistic? This proof, I think, is given our 84 point, it does take a few pages to prove, but that's nice to know. We'll always get at least one equilibrium predictor, and they're usually more, they're often more than, because often there's more than one political compromised solution to a problem. Now, this tableau form was a nice way to do calculations by hand. I won't spend much time here, but basically this says us has the UI from one to three. China can go from three to four.
00:42:04.540 - 00:42:09.984, Speaker A: And notice four is less preferred than one. So it's just a quick way to do hand calculations.
00:42:12.844 - 00:42:13.584, Speaker B: Now.
00:42:14.204 - 00:42:49.918, Speaker A: So you can see that which have to know who are the decision makers, what are their options, what can they do, and what are their relative preferences. So preference information is always the one that's toughest to get. So we developed something called a preference tree. Now here the number stands for the option number. So what is most important for the Americans that the Chinese take option two and cut back into greenhouse gases? Next most important to them is they don't take their first option, which is to cut back. So if you follow this tree here.
00:42:50.086 - 00:42:52.166, Speaker B: It ends up, this is the ranking.
00:42:52.190 - 00:43:22.626, Speaker A: Of states for most, at least preferred. And you can actually do the calculations as shown down here too. The higher level gets a higher weight than the next level, another weight. So if you're dealing with a client, you would ask them what's most important to you in this conflict. Well, they would say, we'd like this to happen, and they might put conditions there too. We'd like this to happen, if that also happens. And so you can take care of that using preference statements.
00:43:22.626 - 00:43:23.970, Speaker A: And it's a really quick way then.
00:43:24.002 - 00:43:28.162, Speaker B: To order your states for most to least preferred.
00:43:28.258 - 00:44:00.080, Speaker A: And then it also allows you to order some by hand if some of them don't make sense. So in practice, we might want to know, what is independent behavior? How well can a decision maker do in a conflict and action independently based on his or her own interests and cooperation? Can I do even better by cooperating through others, through joining a coalition? Generally speaking, the answer is yes, but not always. But we recommend checking for both. Because if you can do better working.
00:44:00.112 - 00:44:03.680, Speaker B: With others, then why not do it now?
00:44:03.712 - 00:44:16.504, Speaker A: Hyperkins, this is a situation where misperception. Let me just see here. So I might go over here to Chris.
00:44:17.204 - 00:44:19.860, Speaker B: Chris. Okay, good.
00:44:19.892 - 00:44:49.444, Speaker A: You responded correctly. Sometimes when I go like that to somebody, I don't know, they think I'm going to hit them, they jump back, this thing's falling apart. So you misperceive what the intention is of the other player. One or more other players. Oops. Not working. Why would we need a phone there?
00:44:53.344 - 00:44:54.016, Speaker B: To call them?
00:44:54.040 - 00:44:55.600, Speaker C: To tell them the phone is broken.
00:44:55.712 - 00:44:58.924, Speaker A: Yeah. Tell them to get it right.
00:45:27.544 - 00:45:28.056, Speaker B: Open this.
00:45:28.080 - 00:45:34.576, Speaker A: Yes. I don't think it's that long.
00:45:34.720 - 00:45:36.524, Speaker D: I hope I didn't call anybody.
00:45:43.424 - 00:45:46.284, Speaker A: He's going to fall in after once. Okay.
00:45:47.504 - 00:45:47.880, Speaker B: Yeah.
00:45:47.912 - 00:46:20.342, Speaker A: So if Chris may mistakenly think, for example, I could throw our coffee over t when it might be the reverse. Right. So often you have misperceptions about preferences and intentions. And Neil fierce and I, we did an analysis of the cuban missile crisis in October 1962. We think it was the best analytical study ever done on the cuban missile crisis, and it was the hypergamy. The Russians, under premier Khrushchev, did not expect the Americans to react as strongly as they did and have enabled blockades. And the reason for that was because Americans performed so poorly at the Bay of Pigs.
00:46:20.342 - 00:46:45.274, Speaker A: So that's one level of misperception. And then we think the Americans were aware of that misperception. Russians. So when you, when you think of it as a hypergamic, as a really accurate model of what happened in the cuban missile crisis. So you could have any combinations of these misperceptions. And the answer to how to solve these are quite simple. Even if they have misperception, just, just analyze the game from the way they see it, even if it's incorrect.
00:46:45.274 - 00:47:07.230, Speaker A: So there, I mentioned different levels. The cuban missile crisis was the second level. And you can apply this, this methodology using those solution concepts to any type of hypergan. Now I'm going to get into the graph model, which is an extension of the conflict analysis approach.
00:47:07.382 - 00:47:09.486, Speaker B: We have two books on that one.
00:47:09.510 - 00:47:43.606, Speaker A: In 1993, another one in 2018. Now the 2018 is by Springer Nature. If you're at the University of Waterloo, hopefully Toronto too, you can download. Total download is free of charge. This book, the second book there, that's a nice explanation in the first three chapters of the book, and it's high at mathematical. And there I am with my co authors. I am Shu.
00:47:43.606 - 00:48:04.224, Speaker A: She was a PhD student at Mark Hildegra and I, oh, about 2011, I saw she graduated. Lee Ping Feng, he was a ph. D. Student Web, who graduated in 1988. Now he's at Toronto Metropolitan University. So if you'd like a copy for this support system to use for research, just email me, Ping. He'll tell you how to get at it.
00:48:04.224 - 00:48:32.964, Speaker A: So remember we had those states and the different cells in that box. Well, instead of a box, let's think of it as a graphic. And each player can cause the game to move from one state to another. So for example here, if Americans go from cooperate to don't cooperate, you go from a c to a d. That moves the game from state one to state three. They can move it from two to four, and China can move it from one to two and three to four.
00:48:34.224 - 00:48:34.952, Speaker B: So there we are.
00:48:34.968 - 00:48:42.974, Speaker A: On the left hand, we have a direction graph of the United States, and the preferences are given at the bottom there, or most, at least preferred for us here.
00:48:43.094 - 00:48:46.994, Speaker B: Here it is for China, that's a directed graph.
00:48:47.494 - 00:49:17.564, Speaker A: Now we can put the two together and become an integrated graph. You can imagine the real world if that very complicated situation. These graphs can get very messy. But the nice thing about graphs, island, if you have a graph, it means you can attach mathematics to attach algebraic equations. So that's nice. So here, notice there's a lot of information here, the movement of the two players and joint movements, they could do. So whenever you have information in a graph, you can use these matrices.
00:49:17.564 - 00:49:53.588, Speaker A: So that's what you decided to do. So vertices represent states and arcs movements. I could have irreversible moves too. So for example, if it's a war situation, once you drop the bombs, you can't take it back. So some might and you can have common boots. So here, for example, if they go from cooperate, don't cooperate, we assume it'd be very difficult for the Americans to change their mind. So that's, so we thought that was a bit like an irreversible move.
00:49:53.588 - 00:50:20.174, Speaker A: But a better example is a military one. In a common move, more than one decision making cause a conflict and move to the same common final state. So during the cold war, either us or the United States could launch a nuclear attack. The final state is nuclear winter. The reason we do this is because we want to simplify the game. Sometimes they get pretty, but they can't get larger practice. We have practical ways for removing.
00:50:22.034 - 00:50:23.174, Speaker B: These whole states.
00:50:23.514 - 00:50:48.494, Speaker A: Now the other thing we can do is we can handle intransitive preferences. At the top example, transfusion. I prefer coffee to tea, maybe because it'll keep me awake, but better tea. That quote because it's healthier. And then you might assume you prefer coffee to coke. That's transitive preferences. And in transitive you might prefer coke to coffee right before coke to coffee because you prefer this pint has something cold, for example.
00:50:48.494 - 00:51:13.128, Speaker A: Well you want to have a, you want to have a game buckle in handle trazoprophy. Why? Because they happen in practice so I don't think they should be discarded. Of course, if you have a utility function there it's automatically assuming transitivity and we think that's a little bit naive for some conflicts. So the decision makers value systems that determine interactive moves and counter moves among.
00:51:13.176 - 00:51:17.848, Speaker B: Decision makers and their express preferences among.
00:51:17.896 - 00:51:53.848, Speaker A: States and then conflicts. So I mentioned these already so I won't spend too much time here. But just looking at the states, most important to them is that China selects option two. So China pays for an expensive cleanup and they prefer one where they don't clean up and China has a similar one. Quite often you can have preference uncertainty. So we have a number of ways for modeling uncertainty of preferences. This first one was quite clever.
00:51:53.848 - 00:52:12.254, Speaker A: Kevin Lee, another PhD student of mine, ours, he said well, the type of uncertainty is unknown. You don't know what the preferences are. I don't know if you prefer coffee to tea for example, so you just don't know. And we could use fuzzy sets. You more or less prefer one state over another, gray sets where you have.
00:52:12.294 - 00:52:16.518, Speaker B: Numbers and interval numbers and probabilistic where.
00:52:16.526 - 00:52:50.034, Speaker A: You have probabilities attached to the preferences or cut any combinations of the above. And then we looked at something else called a degree of preference. I may be a very avid environmentalist and I greatly prefer that this company clean up explosion. So it's like a type of emotion. I greatly prefer this or this or greatly less prefer this than that. I greatly less prefer a messy environment compared to a clean one. So for example, China greatly prefers cc over DD.
00:52:50.034 - 00:53:28.466, Speaker A: Now what happens then with the solution concepts? Remember I said it's moves in counter moves? Well, those different names that I mentioned before, if the sanctions are based upon putting the other player in a greatly less preferred situation. Let me say that that's a strong solution concept because the player doesn't want to end up in a really, really bad position. So if all the solution concepts can be strongly sanctioned, the solution concept under consideration, we call it strong and it may be weak if it seeds one UI which is not strongly sanctioned. Well, this is just a summary of.
00:53:28.490 - 00:53:32.966, Speaker B: Some of the things that are saying about preference. I don't think I'll repeat it, but.
00:53:32.990 - 00:53:59.474, Speaker A: This is once again given in the paper course. So solution concept for stability concepts. This describes chest like human behavior under conflict. So this is consider the consequences of making a move before actually doing something. So this is decision maker. Palliative move is ultimately better off. So if all pulse removes can result in worst situation, the best choice is to.
00:53:59.474 - 00:54:34.226, Speaker A: So here's, here's the, here's the name on the left and we qualitatively say what these are about. There's Nash general meterration symmetric sequential symmetric limited move. We're in a situation you might have a very clever person who think many moves and counter moves into the future, or extremely clever. We can think into the infinite future in terms of moves and tolerant moves. That's eliminating case eliminate. So all these things are written down mathematically. And then of course you can program calculate them for a given conflict.
00:54:34.226 - 00:55:03.344, Speaker A: So let me give you one example of what it definitely looks at. So state s is sequentially stable for decision maker. I do not apply I as an element of this set here. If and for every unilateral improvement plus there that I has from s, let's call it s one. There exists at least one s two.
00:55:03.384 - 00:55:06.256, Speaker B: By the, by the other player, there's.
00:55:06.280 - 00:55:11.964, Speaker A: Two players ing sanctioned, it is less preferred to sanction. That's right.
00:55:14.584 - 00:55:15.800, Speaker B: And then when you have more than.
00:55:15.832 - 00:56:03.104, Speaker A: Two players, the other players n minus five, that's the set theory, subtraction. All the players except for I look at all the moves and counter moves they could make put together almost like a coalition. Well, I don't think I'll explain this one here. It's explained in the paper just to save a bit of time. And then let me just say a little bit about coalition analysis. Can decision makers far better by cooperation? So coalitions form in a given state according to whether or not it's advantageous for decision makers to do so. So we define coalitional solution concepts and one researcher with working with looking at Pareto collisional coalitional stability.
00:56:03.104 - 00:56:08.544, Speaker A: So here, going back to that simple prisoners dilemma game.
00:56:13.944 - 00:56:14.712, Speaker B: Let'S see here.
00:56:14.768 - 00:56:26.016, Speaker A: This is if the game moves from state four to state two. If the US goes for not cooperating.
00:56:26.040 - 00:56:31.984, Speaker B: To cooperate, notice that two is less preferred than four.
00:56:32.144 - 00:57:03.962, Speaker A: So probably wouldn't take that. Who puts them in, puts it in the worst position. Likewise here and likewise here. For China, probably wouldn't make a move that's going to hurt itself. And here, if they move together cooperatively to a joint unilateral improvement, it's better for both of them. So that's really especially a special case of Pareto optimality, a Pareto improvement on it, where both players have better off and over the worse. Then we also looked at the evolution of a conflict.
00:57:03.962 - 00:57:08.374, Speaker A: How could it evolve from a status quo situation? How do we want to define that?
00:57:08.874 - 00:57:10.042, Speaker B: And we take a look at the.
00:57:10.098 - 00:57:49.902, Speaker A: Unilateral moves players can make to trace how the conflict could evolve. In some cases, you may wish to look at what if they only use unilateral improvements? Can be, but we can look at that, too. And I just showed you this slide here. So, psychological factors in power. So I already mentioned hypergame analysis. We also did something with the japanese, something called attitude analysis. Tokyo, Tokyo Diagram Tokyo Institute of Technology professor by the name of Takehiro Inuhara, we looked at attitude analysis.
00:57:49.902 - 00:58:37.114, Speaker A: You would expect if you have a good attitude towards somebody, it might result in a better result. So any given game, I may have a positive attitude towards that player, a negative attitude towards another and the other. It doesn't matter neutral, but we can take that into account. And also hierarchical conflicts and power asymmetry. Already, I already explained this, I won't spend any time here. So, in most governments in the world, especially countries like France, China, Japan, they have a central government, provincial government, regional governments and cities look at the hierarchy. So in some of the research, we took that hierarchy into account to try to get a more accurate analog modeling of the problem.
00:58:37.114 - 00:59:13.900, Speaker A: As I mentioned before, the nice thing about a graph, you can store the information in matrices. Did I go through that definition before for sequential stability? Yeah, I did. Yeah. That was using Ozzy to explain that solution concept. But we can also write down our graph model using matrix algebra. We can redefine all the solution concepts, too. In that form.
00:59:13.900 - 00:59:31.304, Speaker A: It ends up that it's good for designing decision support systems, especially the engines, and it's good for theoretical development. So we had an IEEE paper in 2009 when we first put that forward. And then our 2018 book.
00:59:34.804 - 00:59:35.396, Speaker B: Deals with.
00:59:35.420 - 00:59:48.484, Speaker A: Both the logical and the matrix interpretation format for writing down the graph model for conflict resolution. So we have preference matrices, movement matrices, stability calculations.
00:59:56.024 - 00:59:57.960, Speaker B: Now, let me move on to what.
00:59:57.992 - 01:00:24.854, Speaker A: I'm calling systems perspective model, forward, inverse, and behavior. Well, first of all, let me say we're, we have a lot of theorems and proofs where we take a look at what is the relationship among solution concepts. Here we have the set of all states. The big square, GMR is the most general one. So actually, if it's s eq, it's also GMR. If it's SMR, it's also GM. And if it's national, it's also GMR.
01:00:24.854 - 01:00:38.784, Speaker A: I mentioned this existence theorem before. The reason why I stress this one, because we think it's pretty reasonable, players will only make credible, take credible sanctions, won't hurt themselves.
01:00:40.284 - 01:00:49.260, Speaker D: Can I ask a question about that slide? If you go back to that slide, yeah. No. Then what about equilibrium? Go down one slide.
01:00:49.372 - 01:00:50.024, Speaker A: Yeah.
01:00:50.604 - 01:01:10.574, Speaker D: So if we're. Is the idea that to, for a theorem or a theory to have a testable proposition, that it has to have a testable proposition, and that the existence of an equilibrium is a real world testable proposition.
01:01:12.394 - 01:01:15.026, Speaker A: So, in the real world, yeah.
01:01:15.090 - 01:01:20.654, Speaker D: So we were talking about why. Why is the existence of the equilibrium important?
01:01:21.194 - 01:01:25.634, Speaker A: Something always happens in the real world, that's for sure. Something always happens.
01:01:25.794 - 01:01:26.090, Speaker B: So.
01:01:26.122 - 01:01:32.934, Speaker D: But if we're taking a deep, complex system to view, we might be living in a non equilibrium system.
01:01:34.774 - 01:01:35.206, Speaker B: And.
01:01:35.270 - 01:01:39.046, Speaker A: Well, equilibrium here is different, maybe from the way you're thinking about it. I'm not sure.
01:01:39.190 - 01:01:50.214, Speaker D: I guess. Could I just maybe ask. You mentioned a couple, you mentioned the human missile crisis as a great real world example.
01:01:50.294 - 01:01:50.954, Speaker A: Yeah.
01:01:51.494 - 01:02:01.714, Speaker D: Can you think of other good real world examples where you see the equilibria from some of your models?
01:02:03.014 - 01:02:17.022, Speaker A: Yeah, we applied it to a lot of real world applications, not just ourselves, but other groups too. Like CMCR two is used by 122 groups in 29 countries. So we assume that they're all supplying it. Tough problems that they're looking at.
01:02:17.198 - 01:02:38.722, Speaker D: And do you have your favorite one or two examples where this model, everyone's always asking us for agent based models, like, when are you going to hit a home run on when are you going to hit a home run? What would be like that? So, just because we have you here now, I'm wondering if you can, you can remind us of some of your.
01:02:38.778 - 01:02:42.554, Speaker B: Really great home runs from the model.
01:02:42.594 - 01:02:58.848, Speaker A: Well, there's one talk I give on the Amara conflict that was over the pollution of the, of the groundwater by the chemical company. And they're. Nice result. It shows nicely what happened. And they might remember behind the scenes and the ministry of the Environment and.
01:02:58.856 - 01:03:01.744, Speaker B: The company made a side deal and.
01:03:01.864 - 01:03:09.524, Speaker A: We thought that this could happen because you could see an obvious coalition could form just from the structure of their preferences. And they did that.
01:03:10.864 - 01:03:11.200, Speaker B: Yeah.
01:03:11.232 - 01:03:11.528, Speaker A: Thanks.
01:03:11.576 - 01:03:38.320, Speaker D: It's just you've had such a successful career. I think some of the young people in the room don't know that. And just for young people in the room, when you have this case where you have a model and it actually works in the real world, that's a home run and it doesn't happen all the time. If you have more than one of those in your career, it's really something to celebrate. So sorry if I'm clattering.
01:03:38.472 - 01:04:05.394, Speaker A: There was one paper and I've forgotten the title of it, but we did it years ago, 20 years ago, and we did it while the conflict was ongoing. I think it was happening when the russian empire was collapsing. We did analysis of a certain situation there and then we actually sealed it, had somebody sign it, and then 15 or 20 years later, we took it out of that envelope, we sent it in for publication, and it didn't correctly predict what happened. I think it was to the polish academy science firm, one of theirs.
01:04:05.814 - 01:04:06.238, Speaker B: Yeah.
01:04:06.286 - 01:04:10.914, Speaker D: So it happens. Stick with it, everybody. Sometimes you can make it.
01:04:11.534 - 01:05:01.454, Speaker A: You'll be like Keith and like anything, I think the most important thing is you get a better understanding of the problem. This provides language of communication to discuss it back and forth. You do some analysis, some modeling, some analyses, and then you come up with the result and you try to think, is there something that can aid me, my decision making? If you're working, if you're a vice president in the company, that's why they computerized it. They use it now called non decision support systems rather than user friendly programs. And then for historians, you might ask the question, well, why did this bad result happen when something better probably could have happened? You can reanalyze the situation as it occurred at that point in time. So I think it'd be useful there, too.
01:05:04.434 - 01:05:19.914, Speaker D: So I guess this idea also, that a lot of the value in building a model isn't getting the answer then in identifying what your assumptions are and getting consensus on the assumptions and gathering all the information you have about incentives.
01:05:20.534 - 01:05:20.846, Speaker B: Yeah.
01:05:20.870 - 01:05:39.782, Speaker A: And obviously what you get from the model, you say, that's obvious. Well, that's good. That's what you want it to be. It was a good model. I remember once we were doing a little job for a large chemical company from the United Kingdom. We had a bunch of vice president in the room and then we said, well, here's something you might want to consider. Oh, my gosh, that's so obvious.
01:05:39.782 - 01:05:55.500, Speaker A: Why didn't we think of that? Just hit their heads like that. So my point out the obvious, which is good, just reconfirm that you're maybe thinking, well, but probably more often it points out something. Well, something that you missed, and what you missed might have been very obvious.
01:05:55.652 - 01:05:59.220, Speaker B: After the fact and where some more.
01:05:59.252 - 01:06:14.654, Speaker A: Work could be done. Could be done here. Don is. Maybe psychologists and sociologists could do some experiments for people where they test this out. Some people did way back. Look, we're stuck with the real world problem, so we just use large scale k studies.
01:06:16.434 - 01:06:17.930, Speaker D: We can make that work.
01:06:18.082 - 01:06:18.774, Speaker B: Yeah.
01:06:22.554 - 01:06:24.698, Speaker D: Sorry, I got you a little off track.
01:06:24.786 - 01:06:27.294, Speaker A: No, you didn't. Thank you for that question. That was a good question.
01:06:28.394 - 01:06:35.774, Speaker C: So there's. But there's no theorem if it's not. If it's not intransitive, you have this intransitive preference structure.
01:06:36.604 - 01:06:41.104, Speaker A: No, we have to assume transitivity, and that's right.
01:06:41.644 - 01:06:42.164, Speaker B: Yeah.
01:06:42.244 - 01:06:43.716, Speaker A: Right, Christopher. You're right.
01:06:43.900 - 01:06:44.704, Speaker B: Yeah.
01:06:46.124 - 01:07:14.204, Speaker A: But the thing is, if there isn't transit preference present, you can still do an analysis. But that theorem might not necessarily hold. But I think getting rid of intransitivity because it looks like it could be messy mathematically. It's probably not a good reason to do that. You want to have in your model assumptions that reflect the information and phenomena that's happening in the real world, hopefully. Right.
01:07:16.264 - 01:07:26.264, Speaker D: That came up in our last session. I just wanted to fight. We're getting a little bit close to the end of the slot here. 20 minutes.
01:07:26.344 - 01:07:28.024, Speaker A: Just put 15 to 20. Okay, thank you.
01:07:28.064 - 01:07:28.684, Speaker B: Yeah.
01:07:30.104 - 01:07:30.964, Speaker A: Okay.
01:07:31.384 - 01:07:31.776, Speaker B: Okay.
01:07:31.800 - 01:07:51.924, Speaker A: Let me. Let me mention these systems perspective as if it's into the future. So normally we go from left to right. Here we're the decision makers, options and relative preferences. Then we have an engine that calculates stability or the solution concepts. Then we end up with individual stability and also overall equilibrium. Now, equilibria doesn't mean tranquility and peace.
01:07:51.924 - 01:07:57.604, Speaker A: Maybe a better word is compromised. Maybe just compromised resolutions.
01:07:58.074 - 01:07:58.714, Speaker B: Yeah.
01:07:58.834 - 01:08:25.572, Speaker A: Now, here, then you might ask a question. So, notice there's a question mark here. You're trying to calculate these. Now, an inverse GMTr, you might ask. Well, I have a desired equilibrium or equilibrium here. The question mark here, what are the preferences that are needed by the decision maker to be able to reach that? So that's inverse engineering. That ends up being an AI type of problem because very highly complex.
01:08:25.572 - 01:09:09.321, Speaker A: And also you get large, enormous numbers of solutions, and then here is the behavioral GMCR. Given the input and the output, what is the pipe of taking the place in the middle here? Now, all these problems, these three problems we written down theoretically, but then you need really good algorithms in order to apply them to practice. Because of the complicated calculations, large number of results. We used to have one of the top people in the world here working in inverse engineering, Graham Gladwell. Everybody hear of that name, Graham Gladwell. I mean, he wrote a number of good books. There are many mechanical engineering types of problems.
01:09:09.321 - 01:09:30.652, Speaker A: His son is Malcolm Gladwell, a famous author in the United States. But he, of course, wasn't aware of this. But I would throw it as a challenge to people here. If you're looking at a certain type of decision technology, let's say decision system, think about the inverse engineering interpretation, the.
01:09:30.668 - 01:09:34.196, Speaker B: Inverse of your system, because you come.
01:09:34.220 - 01:09:47.408, Speaker A: Up with some very interesting mathematical problems that are going to be solved. Well, here are the behavioral GNCR here. We don't know what's under the ground, so we know the input, some brown penetrating radar, and we get a reflection.
01:09:47.456 - 01:09:48.896, Speaker B: Here to give them that input.
01:09:48.920 - 01:10:04.124, Speaker A: And that reflection, what's underneath the ground? Is there an ancient incaruin down there, for example, or some barrier nuclear waves? So I think I've already described this, so I won't spend much time here.
01:10:05.704 - 01:10:08.984, Speaker B: Now, the nice thing about this inverse problem, too.
01:10:09.024 - 01:10:26.148, Speaker A: If you have some preference information, you can still use this in the model. There might only be certain parts of the preferences you're not sure of in order to get a good result. And then a nice thing of having a mathematical description, no matter what decision.
01:10:26.236 - 01:10:29.292, Speaker B: System you're working with, is that then.
01:10:29.348 - 01:11:07.280, Speaker A: Hopefully you can put together the decision support system so others can use it. And here's a graph of what a good one might look at. So we have the input subsystem and analysis engine and output subsystem area. And hopefully that will provide aids for decision making. So, new expanding frontiers. Conflict is everywhere, even in the definition of sustainable development. You have so called development or expansion of human activities versus the present, the maintenance of nature, somehow not destroying it.
01:11:07.280 - 01:11:51.704, Speaker A: That's a conflict in itself, right inside the concept. And there are many conflicts to look at. And I was pleased that this particular workshop is dealing with climate change, greenhouse gases. But there are all sorts of messy environmental problems going on. Chemicals are the emerging concern, a huge problem. And I like the idea, though, having the practical application give the direction of where to take the mathematics, because then you get the right assumptions or actions in your mathematical model.
01:11:52.604 - 01:11:54.724, Speaker B: So here are some examples.
01:11:54.764 - 01:12:51.724, Speaker A: There are many case study areas that could be looked at. Fish farming, agriculture, sustainable development, climate change, really big ones. So lots of room for theoretical expansions, which I mentioned throughout my time. And in general, not just this model, but I think a wide range model for modeling both the physical and physical systems and societal systems aspects of problems in system design engineering. We like to think of societal environmental systems because you have to look at them of systems and systems interacting with each other. You can think of them being inhabited by decision makers within long systems. So we might want to learn how a decision maker may think based upon how they behave.
01:12:51.724 - 01:12:57.944, Speaker A: What are their actions? Can you purposely direct this thing to win many solutions.
01:13:02.164 - 01:13:02.948, Speaker B: For a higher.
01:13:02.996 - 01:13:26.330, Speaker A: I guess at the most basic level, can we have machines that think intelligently like people, such as learning and problem solving in GMCR, inverse GMCR computer determines how people should think to reach a desirable resolution in terms of preferences. So we think it has a key role to play in conflict analysis with extremely complicated calculations and huge numbers of.
01:13:26.362 - 01:13:30.534, Speaker B: Solutions and then just governors in general.
01:13:30.954 - 01:14:08.744, Speaker A: We like the concept of a system of systems perspective and we want to be integrative and adaptive. A lot of good management techniques come from the field of water resources management, integrated water resource management. We want to achieve desirable system goals such as resiliency, sustainability and fairness. We hope that will be present in systems that we help design or improve or maintain. I just have a couple of white slides at the end here. I mentioned this before, but let me say it again. So Sir Isaac Newton.
01:14:08.744 - 01:14:54.758, Speaker A: Newton, he was very interested how do physical models and bodies interact to invent a calculus to describe the rate of change. So I hope that this workshop here will inspire mathematicians to try to model some of these problems that go well beyond what I was talking about today in order to be able to address this monster called climate change. Now there's the COVID of an original program that is manuscript. I took that picture in a vault of the Royal Society of London. Has anybody here visited the Royal Society of London? I paid. You did Christian, did you get onto the vaults and see some of the manuscripts? No. Are you Mister Target? You went to the Swat Beaker room?
01:14:54.886 - 01:15:01.582, Speaker C: I just went to some conference there meeting and talks in the big auditorium. I didn't get to see any.
01:15:01.718 - 01:15:22.624, Speaker A: There's a conference right at that location. Yeah, yeah. Go down to the vault. And during World War Two was actually the location of the embassy of the Germany. And apparently one of the rooms has the biggest swastika in there for. I never saw that, but it goes on in the vaults. And that's a piece of the wood that came from the apple tree at Newton's farm.
01:15:22.624 - 01:15:40.494, Speaker A: I think many of you have seen the picture. He's sitting under an apple tree, falling off the tree. Aha. Gravity. Authentic piece of wood from there. And here's some of his scribbling on his manuscript by Sir Isaac Newton. So it's really amazing to go there.
01:15:40.494 - 01:16:02.214, Speaker A: And anybody can go, but you have to book well in advance in order to get in there. But really worthwhile seeing if you're going to London. So I think that's probably a good place to stop. I don't know if there are any other questions.
01:16:09.834 - 01:16:26.746, Speaker D: Oh, absolutely. I think we're just going to take questions. And if you guys are inspired with any research questions, just put them on the poster board downstairs, if anyone can read my writing. But questions q and a from the ground. Is there anyone online?
01:16:26.850 - 01:16:29.854, Speaker A: We have q one join on the left.
01:16:34.634 - 01:17:03.096, Speaker D: Can constraint in the system be added as conflict antimony? Using this draft, for example, this graphic model can be used to model the appropriate and price subject to weather changes. To model what crop yield and price subject to weather changes. For example, the price of price of a crop is dependent on yield, and yield is dependent on the weather changes.
01:17:03.160 - 01:17:03.648, Speaker A: Right.
01:17:03.776 - 01:17:04.804, Speaker B: So if.
01:17:05.584 - 01:17:14.384, Speaker D: If the precipitation is very low, we have to irrigate more, which can lead to large scale of groundwater extraction, which is not good.
01:17:14.464 - 01:17:17.604, Speaker A: So you write that down on some sort of causal graph?
01:17:18.274 - 01:17:18.650, Speaker B: Yeah.
01:17:18.682 - 01:17:27.094, Speaker D: So can the preference tree that you mentioned earlier be used here to study this cloud?
01:17:27.554 - 01:17:30.894, Speaker A: I'm not sure about the answer to that. Yeah.
01:17:38.474 - 01:18:24.764, Speaker D: A question that inverse process. So in your missile crisis analysis, your prediction for what the outcome would be if the action was to pursue war would be nuclear waiver. So in a modern scenario, where we're looking at another nuclear war and the predicted outcome would be mutually assured destruction, how do you use that end stage to try to make the decision makers not?
01:18:24.844 - 01:18:55.554, Speaker A: Well, you could ask the questions to reach a result where it is a nuclear war. How would the preferences of the decision makers have to change then? If you know the direction they should change, then you can use practical things in the real world to do it. Like some sort of payoffs. Right? Like, I'm not sure what happened with Iran and Saudi Arabia. That was nice to see an agreement there. The Chinese really pulled off a major cute coup. Was there any pavement behind the scenes, Kasra, to get those two talking to each other? So I think it was tremendous they did that.
01:19:07.064 - 01:19:07.884, Speaker B: Sorry.
01:19:20.184 - 01:19:22.464, Speaker A: Did you hear that, Chris? Part of it.
01:19:22.504 - 01:19:26.976, Speaker C: So China wants to have stability for their oil flow, so it's beneficial to China to have peace between them.
01:19:27.040 - 01:19:27.884, Speaker A: Right, right.
01:19:31.104 - 01:19:31.416, Speaker B: Yeah.
01:19:31.440 - 01:19:40.314, Speaker A: Regarding China, you mentioned, I think a more recent example is general relativity and.
01:19:48.134 - 01:19:51.434, Speaker C: General relativity and tensors were developed for.
01:19:56.294 - 01:20:02.074, Speaker A: Einstein in cooperation with a mathematician. Well, as german mathematician.
01:20:12.894 - 01:20:25.366, Speaker C: I wanted to ask, with the techniques you showed, they seem to be very algorithmic. And you would, like, compare these. You'd find in the preference order, a place that's acceptable to both, even though it's not.
01:20:25.430 - 01:20:26.654, Speaker A: They're both preferred.
01:20:26.814 - 01:20:31.910, Speaker C: Is there all, is there all software for all of these processes you've described already in existence?
01:20:32.062 - 01:20:40.502, Speaker A: Yeah. Right. Now, the system can't do everything I described. It doesn't do the inverse GMCR, for example, but it does do the forward process.
01:20:40.598 - 01:20:54.164, Speaker C: Okay, so if you know all the states and the preferences and either transitive or ordering, you can do this analysis and then say these would be the stable states or in the equilibria.
01:20:55.144 - 01:20:55.456, Speaker B: Yeah.
01:20:55.480 - 01:21:11.016, Speaker A: We can calculate the equilibrium using a wide range of solution concepts in the forward process, but the inverse one, we've written it down theoretically, mathematically, but it's. It's a nightmare to develop. Algorithms actually carry out the complicated calculations.
01:21:11.120 - 01:21:16.724, Speaker C: It doesn't seem that bad to me, but I don't know, I guess I have to look at the papers.
01:21:18.044 - 01:21:43.224, Speaker A: Well, you mentioned, like, you had a running temple of, like, the escape between China and us, and you had a comment that, you know, like kind of similar to dilemma. And I'm wondering, you know, for that sort of like, game, do you have like a play many rounds?
01:21:44.964 - 01:21:57.348, Speaker D: I think the question is, how does your conflict resolution graph theory model compare to the iterated prisoners dilemma? Is that the right question?
01:21:57.436 - 01:21:57.868, Speaker B: Yeah.
01:21:57.956 - 01:22:07.744, Speaker A: Yeah. I remember Anatole rackopart had that contest, didn't he? The Anatole rapper was somewhat aware of that left branch where I went down in his talk.
01:22:14.864 - 01:22:17.204, Speaker D: Maybe that's a question for a coffee break.
01:22:19.544 - 01:22:21.404, Speaker C: Thank you so much for this wonderful.
01:22:27.664 - 01:22:29.144, Speaker D: On behalf of Wiki.
