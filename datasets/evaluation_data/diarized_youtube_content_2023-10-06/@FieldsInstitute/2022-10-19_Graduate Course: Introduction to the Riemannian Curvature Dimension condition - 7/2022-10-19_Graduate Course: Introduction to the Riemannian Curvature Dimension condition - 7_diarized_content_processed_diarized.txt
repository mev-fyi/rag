00:00:00.240 - 00:00:51.926, Speaker A: More of a formal kind of intuition about how the evolution behaves and thinking. For instance, the mean curvature flow is the gradient flow of the area function with respect to the l two distance in some sense. But this is what this, while this sentence is in some sense only formal. Okay, so everything, you know, makes sense if everything is most. But it's not something that you, there is not such a rigorous theory in some sense behind this quote. While the kind of gradient flow that we are going to study now, so today is a crash course on gradient flows, is in some sense way better behaved situation where we deal with functional that are particularly nice and we work on metric spaces. And thanks to this, we will have a quite solid concept of gradient function.
00:00:51.926 - 00:00:58.394, Speaker A: Okay, so today we do gradient flow matrix basis, right? Um.
00:00:59.094 - 00:01:01.270, Speaker B: All right, so what is a gradient flow?
00:01:01.302 - 00:01:07.974, Speaker A: So a gradient flow trajectory in this moot case, you know, a gradient flow trajectory. So if you have given a function.
00:01:08.014 - 00:01:14.714, Speaker B: Say, on rd already, my m, any, for the few issue, let's say smooth, smooth.
00:01:17.314 - 00:01:26.014, Speaker A: Well, then a car gamma is a gradient flow trajectory provided solves gamma. T prime is equal, typically one plus the minus.
00:01:27.674 - 00:01:29.202, Speaker B: Of course, this is a convention you.
00:01:29.218 - 00:01:42.642, Speaker A: Could work with plus and change the sign to e. But from the point of view of calcul variation, what is most natural is that gradient flow tends to decrease the energy. Of course, if anything is most, and.
00:01:42.658 - 00:01:46.646, Speaker B: You differentiate gamma t in particular, this.
00:01:46.670 - 00:01:50.686, Speaker A: Is the way, this is the differential.
00:01:50.750 - 00:01:56.558, Speaker B: We apply to gamma t prime, which is minus square.
00:01:56.606 - 00:02:02.390, Speaker A: Gamma T, in particular, is less rigor than zero. So the energy decreases along the flow, among other things.
00:02:02.422 - 00:02:02.622, Speaker C: Okay.
00:02:02.638 - 00:02:05.870, Speaker A: Of course, decreases along many flows, but in particular, decreases along degree.
00:02:06.022 - 00:02:09.974, Speaker B: Okay, now, at the structure level, so what it is.
00:02:10.014 - 00:02:11.580, Speaker A: So now I want to, you know.
00:02:11.612 - 00:02:15.772, Speaker B: Interpret this od in a matrix setting.
00:02:15.868 - 00:02:23.940, Speaker A: Where I don't have derivatives. And of course, how can I do so? Well, I can do so, provided that I'm able to rewrite this in a way that has a metric analog.
00:02:24.012 - 00:02:24.604, Speaker B: Okay.
00:02:24.724 - 00:02:47.354, Speaker A: As usual. So we try to find a way to interpret our concepts in a weaker way so that we can generalize them. And perhaps before starting doing so, it's, you know, it's worth to take a moment and let's look at what it is. What are the structural elements in this OD? Well, there's certainly a car.
00:02:48.814 - 00:03:05.018, Speaker B: These are pivotal gradient flow. There's a function, certainly. And then there is this guy, the gradient. Okay, now if you think a moment.
00:03:05.066 - 00:03:26.974, Speaker A: And what you need to define a gradient. To define a gradient, you need a matrix. Okay, to define the differential you don't need any metric. There's smooth manifold. You have a smooth functional. You have a differential, right? But to define a gradient, you need some operation that brings the differential, which is a cotangent guy, and returns a gradient, which is a tangent guy. And this should be tangent, because the derivative of a curve is a tangent object.
00:03:26.974 - 00:03:37.194, Speaker A: So, it's some form of matric that allows you to dualize, in a sense, from differential to gradients is needed. So, I'm telling this because in some.
00:03:37.234 - 00:03:40.482, Speaker B: Sense, okay, maybe, maybe in ten years.
00:03:40.498 - 00:03:50.454, Speaker A: Or so, somebody will come with a more general, with a more general notion. But so, the ingredients that we have to speak about gradient flows in a metric setting will just be functional.
00:03:54.754 - 00:04:02.742, Speaker B: On a metric space. Given this, maybe we can try to.
00:04:02.758 - 00:04:09.674, Speaker A: Give a definition of gradient flow and perhaps, under some assumption, regular assumptions on either the function or the matrix space.
00:04:10.414 - 00:04:13.318, Speaker B: All right, now let's see.
00:04:13.366 - 00:04:21.794, Speaker A: How can we interpret this in a purely metric way where there is no differential structure? Well, by the way, this is all basically the geometry.
00:04:27.234 - 00:04:31.934, Speaker B: It's been revisited by Ambrosio, Savare and myself.
00:04:37.034 - 00:04:54.000, Speaker A: And basically, all of what I'm going to say is extracted from a book on gradient flows that we wrote like 15 years ago or so. And the idea is this. So, let me start once again from this, and actually, let me observe the following. Say that I have an arbitrary smooth.
00:04:54.032 - 00:04:54.604, Speaker B: Car.
00:04:57.184 - 00:04:59.564, Speaker A: And I evaluate the difference.
00:05:00.304 - 00:05:02.152, Speaker B: So, ETA is a smooth car, and.
00:05:02.168 - 00:05:09.644, Speaker A: I evaluate the difference between e at eta s and e at eta t. And. Or perhaps.
00:05:11.944 - 00:05:13.844, Speaker B: Okay, perhaps better this for now.
00:05:14.864 - 00:05:21.696, Speaker A: And, well, this is, of course, the integral is equal, actually is equal to minus the integral from t to s.
00:05:21.840 - 00:05:25.808, Speaker B: Of the differential of e at eta.
00:05:25.856 - 00:05:27.776, Speaker A: R, you know, or perhaps eta r.
00:05:27.800 - 00:05:36.164, Speaker B: I should say, applied to eta r prime the r chain rule.
00:05:37.944 - 00:05:45.604, Speaker A: Okay, now, this is less or equal. Of course, this is less or equal than the integral from t to s of the modulus of the differential.
00:05:48.534 - 00:05:48.942, Speaker C: Times.
00:05:48.998 - 00:05:52.558, Speaker B: The speed, the normal derivative.
00:05:52.686 - 00:05:53.354, Speaker C: Right?
00:05:53.894 - 00:06:00.254, Speaker A: And this is, of course, less or equal. Let me apply young inequality over here is less regular than one half the.
00:06:00.294 - 00:06:06.834, Speaker B: Integral from t to s of the speed squared plus.
00:06:08.614 - 00:06:11.514, Speaker A: De squared at eta.
00:06:16.304 - 00:06:18.192, Speaker B: This is obviously true, right?
00:06:18.368 - 00:06:28.784, Speaker A: Now, let me wonder, when is it that I have equality? So this quantity is always less bigger than this quantity. Now, when is it that I have equality? When it is that this guy is equal to this guy, say, for any.
00:06:28.904 - 00:06:30.688, Speaker B: T and s positive.
00:06:30.816 - 00:06:31.604, Speaker C: Okay.
00:06:31.904 - 00:06:34.304, Speaker B: And, well, I have to.
00:06:34.344 - 00:06:47.354, Speaker A: I need to have equality both here and here. Now, here it's easy. We have equality near if and al, if you know, you know, young inequality, if and all, if this number is equal to this number for almost every r, say, for any r, because everything.
00:06:47.394 - 00:06:53.014, Speaker B: Is most okay, and we have equality in here if and only if.
00:06:53.914 - 00:07:02.722, Speaker A: Basically the, you know, the, this vector is equal is proportional to minus the.
00:07:02.738 - 00:07:04.846, Speaker B: Gradient of e. You see?
00:07:04.990 - 00:07:07.558, Speaker A: I mean, I should realize the norm of the differential.
00:07:07.606 - 00:07:10.514, Speaker B: Basically, it should point in the correct direction.
00:07:11.134 - 00:07:12.070, Speaker C: Makes sense.
00:07:12.222 - 00:07:17.074, Speaker A: So these two together, so it's clear now that these two together, so equality.
00:07:18.534 - 00:07:22.354, Speaker B: You know, equality, if and only if.
00:07:23.294 - 00:07:31.510, Speaker A: Eta t prime is equal to minus the gradient of either t, say, for any t or almost every, I mean, anything is mood. So for everything.
00:07:31.622 - 00:07:34.966, Speaker B: Okay, now this of course makes me.
00:07:34.990 - 00:07:38.234, Speaker A: Happy, because I have no idea how to write this on a metric space.
00:07:38.694 - 00:07:40.974, Speaker B: But maybe this equality has some options.
00:07:41.054 - 00:07:46.358, Speaker A: This is more metric. There are no more vectors, but normal vectors. And then maybe, maybe this is handle.
00:07:46.486 - 00:07:51.110, Speaker B: Okay, very well. So this has been the starting point.
00:07:51.142 - 00:08:10.646, Speaker A: Of the georgian approach. And so, by the way, gradient flows in the metric setting are also called sometimes steepest descent curves. So in some sense, what this equality tells. But this is in some sense clear from the concept of, if you think a little bit about this equation, that.
00:08:10.670 - 00:08:12.542, Speaker B: A gradient flow trajectory.
00:08:12.598 - 00:08:18.154, Speaker A: Gradient flow trajectory is a curve along which in some sense, the functional decreases as fast as possible.
00:08:18.494 - 00:08:22.406, Speaker B: Okay? Provided of course, you can always decrease.
00:08:22.430 - 00:08:40.112, Speaker A: This twice as fast if you double the speed, but in some sense, provided you say that the speed of your car was to be equal to the normal degradient. Okay? So that fixes in some sense, you know, how fast you can go. And with that constraint, the gradient flow trajectory is a car for which this descent is the maximum.
00:08:40.208 - 00:08:42.564, Speaker B: Okay, very well.
00:08:43.504 - 00:08:53.268, Speaker A: Now let's see, how can we define this concept on a metric setting. Let me start from what would be the replacement for this guy? And the replacement for this guy is the concept of slope.
00:08:53.356 - 00:09:03.308, Speaker B: So let me define. So now let e from a matrix space to, well, I guess let's say.
00:09:03.396 - 00:09:06.464, Speaker A: R positive plus infinity, the lower semicontinuous.
00:09:08.044 - 00:09:10.220, Speaker B: Okay then.
00:09:10.332 - 00:09:19.382, Speaker A: Okay. And as usual, I define, I consider dov. This is the effective domain. This is the set where, you know.
00:09:19.438 - 00:09:25.794, Speaker B: E, the set where e is finite. Okay?
00:09:26.094 - 00:09:28.434, Speaker A: And I define the descending slope.
00:09:40.094 - 00:09:40.406, Speaker C: Is.
00:09:40.430 - 00:09:47.014, Speaker A: A functional which is defined from the domain of the energy, two non negative values.
00:09:49.634 - 00:10:17.970, Speaker B: And this defines this. The descending zone at any given point is the lim sub, as y goes to x of e of x minus e of y, positive part divided by the distance between x and y. Okay, let's have a look to this. Uh, to this formula for a moment.
00:10:18.162 - 00:10:29.346, Speaker A: So what it is that these measures? Well, first of all, if e is moves and we are on a b, then this is the modulus of the differential at x.
00:10:29.450 - 00:10:30.194, Speaker B: Okay.
00:10:30.354 - 00:10:34.858, Speaker A: Um, now more generally, so these measures.
00:10:34.946 - 00:10:37.500, Speaker B: In some sense, how fast I am.
00:10:37.532 - 00:10:39.980, Speaker A: Able to, when it is that. This is big. This is big.
00:10:40.012 - 00:10:42.180, Speaker B: If close to x, there are points.
00:10:42.252 - 00:11:04.872, Speaker A: Y with smaller value of the function. So this measures how fast I can decrease my function. This is always zero at a minimum. Perhaps this should be clear. If x is a local minimum or global minimum, if you wish, then this guy is zero. Because e of Y is always greater or equal than x. Your x minus, your y is negative.
00:11:04.872 - 00:11:06.284, Speaker A: This is the positive part.
00:11:07.824 - 00:11:14.840, Speaker B: This is so z plus is the max and zero.
00:11:15.032 - 00:11:15.884, Speaker C: Okay?
00:11:17.064 - 00:11:19.124, Speaker B: So this is always non negative. Okay?
00:11:19.624 - 00:11:26.656, Speaker A: And if x is a minimum, it is zero. So, for instance, for the function absolute value, this guy is zero at the.
00:11:26.680 - 00:11:29.604, Speaker B: Origin and one at any other point.
00:11:32.044 - 00:11:32.904, Speaker C: Okay.
00:11:34.204 - 00:12:03.122, Speaker A: Of course, you can wonder, okay, but why did you put, actually, why did the Georgie put the positive party other than the more natural absolute way? I mean, if I want to measure the local oscillation, in some sense of a functional absolute value, would be in some sense more intuitive. And the point is that, well, first of all, for gradient flows, I mean, I care only about how fast I can decrease, not how fast I can increase. So the concept I want to have.
00:12:03.138 - 00:12:05.266, Speaker B: In mind is, you know, the descending.
00:12:05.290 - 00:12:15.054, Speaker A: Slope, how fast I can go down. So that's why, that's why I don't put the absolute value. And there are, and for practical reasons, if I, if I leave you this exercise.
00:12:15.354 - 00:12:17.866, Speaker B: But basically, in any case, in any.
00:12:18.010 - 00:12:23.454, Speaker A: Case of interest, if you put the absolute value, you obtain something that is three del. So, exercise.
00:12:25.784 - 00:12:32.856, Speaker B: Let x be, say l, two of zero, one if you wish.
00:12:33.040 - 00:12:37.884, Speaker A: And e of f. F is a point of x, just the Dirichlet energy.
00:12:41.144 - 00:12:42.004, Speaker B: Okay?
00:12:42.784 - 00:12:43.524, Speaker A: Then.
00:12:46.624 - 00:12:56.150, Speaker B: The lean soup as g goes to f of e of f minus e of g, absolute value divided by.
00:12:56.182 - 00:12:57.394, Speaker A: The distance.
00:13:00.054 - 00:13:00.854, Speaker C: Even two.
00:13:00.934 - 00:13:02.194, Speaker A: This is identical.
00:13:06.174 - 00:13:10.798, Speaker C: Please. Yes.
00:13:10.846 - 00:13:13.034, Speaker A: Plus, yeah, so if.
00:13:13.694 - 00:13:14.262, Speaker C: Thanks.
00:13:14.358 - 00:13:17.418, Speaker A: I mean, if f is in h.
00:13:17.466 - 00:13:20.334, Speaker B: One plus infinity, otherwise, thanks.
00:13:25.554 - 00:13:26.374, Speaker C: Okay.
00:13:29.714 - 00:13:37.674, Speaker A: On the other hand, so this is quite easy. It's a little bit more complicated, but I invite you to try as usual, as I usually, as I always say.
00:13:37.714 - 00:13:39.802, Speaker B: I mean, exercises are not meant to.
00:13:39.818 - 00:13:41.558, Speaker A: Be solved, are meant to be attempted.
00:13:41.666 - 00:13:42.382, Speaker B: So try.
00:13:42.518 - 00:13:43.314, Speaker C: Okay.
00:13:43.974 - 00:13:46.646, Speaker A: It's like if you go to the gym and your, you know, instructor tells.
00:13:46.670 - 00:13:48.902, Speaker B: You do 100 push ups and you.
00:13:48.918 - 00:13:57.846, Speaker A: Just do ten, well, you should do that, right? So it's not that if you didn't do all of them, it didn't sort of help you. And on the other hand, the, you.
00:13:57.870 - 00:14:01.446, Speaker B: Know, this log of f, well, this is equal.
00:14:01.470 - 00:14:06.638, Speaker A: If you can compute, is a part of the exercise is the electronome of.
00:14:06.646 - 00:14:09.982, Speaker B: The Laplacian, meaning that it is, it.
00:14:09.998 - 00:14:13.542, Speaker A: Is plus infinity if f does not have a distribution of Laplace.
00:14:13.558 - 00:14:15.354, Speaker B: Or two, perhaps.
00:14:16.014 - 00:14:22.950, Speaker A: No, that's okay. And it is, it is this, if we, you know, if I have two, that this is this one.
00:14:23.022 - 00:14:30.926, Speaker B: Okay. Similar exercise, let x be, say, the.
00:14:30.950 - 00:14:39.510, Speaker A: Space of probability measures on, you know, zero one, if you wish. Zero one, of course, does not play any role here. Does not play any role here. You can pick rd, and that works the same.
00:14:39.622 - 00:14:44.554, Speaker B: And the relative entropy with respect to the back measure.
00:14:46.134 - 00:14:46.874, Speaker A: Then.
00:14:49.414 - 00:14:54.062, Speaker B: The limb soup, even in this case, if I, you know, do the.
00:14:54.078 - 00:15:07.514, Speaker A: Limb soup as nu goes to mu of the entropy of mu minus the entropy of new absolute value divided by, you know, the distance in the intended. In the w two sense, this is identically placing, okay.
00:15:08.014 - 00:15:10.154, Speaker B: But the slope computed.
00:15:13.414 - 00:15:14.166, Speaker A: By this formula.
00:15:14.190 - 00:15:21.326, Speaker B: Actually, the slope squared is the Fisher information. So this is equal to, you know.
00:15:21.350 - 00:15:26.444, Speaker A: Sometimes it is called I of mu, the Fisher information, which is the integral.
00:15:26.614 - 00:15:42.576, Speaker B: Of grad rho squared over rho. Okay, if mu is equal, rooted back plus infinity. Otherwise, in fact, the hard part of.
00:15:42.600 - 00:15:53.044, Speaker A: These exercises, which is, you know, this identity and this identity, we shall see. Actually, during the course of my lectures, you know, I will solve this. But meanwhile, try.
00:15:54.594 - 00:15:55.494, Speaker C: Okay.
00:15:58.954 - 00:16:02.106, Speaker B: All right, so, so, at least this.
00:16:02.130 - 00:16:05.066, Speaker A: Should convince you that you don't have to put the value, absolute value does.
00:16:05.090 - 00:16:14.162, Speaker B: Not work, but slope does. And now, okay, now I've told you.
00:16:14.218 - 00:16:28.706, Speaker A: Who is the replacement for this guy? Now, who is the replacement for this guy? And this is the concept of matrix pick. Let me give a definition. So, a curve gamma, say, defined on.
00:16:28.730 - 00:16:32.834, Speaker B: Zero one, with value on x. Matrix.
00:16:32.874 - 00:16:34.654, Speaker A: Space is absolutely continuous.
00:16:41.954 - 00:16:44.466, Speaker B: If there exists.
00:16:44.530 - 00:16:46.574, Speaker A: The function f, say, in l one.
00:16:47.424 - 00:16:52.184, Speaker B: Um, such that the distance between gamma.
00:16:52.224 - 00:16:54.280, Speaker A: T and gamma s is bounded from.
00:16:54.312 - 00:17:02.164, Speaker B: Above by the integer from t to s of f. For any, say, t less than s, even less than zero one.
00:17:08.744 - 00:17:09.524, Speaker C: All right.
00:17:11.224 - 00:17:15.170, Speaker B: If x is r, this boils down.
00:17:15.202 - 00:17:17.274, Speaker A: To the usual, you know, definition of.
00:17:17.314 - 00:17:22.602, Speaker B: Absolutely continuous function, say, from r to r. Okay?
00:17:22.738 - 00:17:25.974, Speaker A: But written in this way, makes sense for an arbitrary curve.
00:17:27.394 - 00:17:27.994, Speaker C: Okay?
00:17:28.074 - 00:17:46.884, Speaker A: Now, absolutely continuous functions are those that are different, that are a differentiable almost everywhere. And b, they are the inter of the derivative, basically. So they are a good starting point for starting taking derivatives. And this is the same for the level of curves, because of the following proposition.
00:17:49.344 - 00:18:03.776, Speaker B: So, let gamma be absolutely continuous. Then, first of all, the limit as.
00:18:03.840 - 00:18:06.652, Speaker A: H goes to zero of distance between.
00:18:06.708 - 00:18:14.984, Speaker B: Gamma t plus h and gamma t divided by h. Let me call it gamma t prime.
00:18:15.404 - 00:18:27.140, Speaker A: This exists for almost everything. Let me see.
00:18:27.212 - 00:18:34.240, Speaker B: One, two, this guy is in l one of zero, one of the domain.
00:18:34.272 - 00:18:45.964, Speaker A: Of definition of gamma. And three, it is the least l one function.
00:18:49.464 - 00:18:59.384, Speaker B: That can be put here. Yeah, here.
00:19:02.364 - 00:19:05.504, Speaker A: Least in the almost, almost every sense.
00:19:06.444 - 00:19:10.584, Speaker B: Indeed, no l one almost every verse.
00:19:12.484 - 00:19:16.020, Speaker A: That is to say that if you, for any function f that you can.
00:19:16.052 - 00:19:18.796, Speaker B: Put in here, this matrix speed is.
00:19:18.820 - 00:19:23.632, Speaker A: Less or even f almost everywhere. I mean, that's the best you can hope for. L1 functions.
00:19:23.788 - 00:19:28.464, Speaker B: Okay, so we are like the 8th lecture.
00:19:28.504 - 00:19:31.944, Speaker A: I guess this is the first derivative that we pick. Was about time.
00:19:32.024 - 00:19:32.764, Speaker C: Okay.
00:19:40.264 - 00:19:41.016, Speaker B: So, let's prove this.
00:19:41.040 - 00:19:56.104, Speaker A: Lemma, this proposition is not up. And how do you prove it?
00:19:58.364 - 00:19:59.664, Speaker B: Well, you pick.
00:20:02.244 - 00:20:03.984, Speaker A: Say a sequence xn.
00:20:04.364 - 00:20:05.104, Speaker C: Then.
00:20:06.924 - 00:20:10.508, Speaker A: Okay, don't remember. Okay, then say in, you know, in.
00:20:10.556 - 00:20:13.732, Speaker B: Gamma of the unit interval, if a.
00:20:13.748 - 00:20:32.464, Speaker A: Curve is absolutely so, I mean, I didn't assume this space to be separable, so I forgot. But that's not too bad, because. So, if gamma is absolutely continuous, certainly is continuous. So, the image of the interval is certainly a compact set, and therefore it's a variable. Pick a countable dense set in that separable guy, and consider the following functions.
00:20:32.544 - 00:20:40.064, Speaker B: The n of t is nothing but the distance between gamma t and x.
00:20:40.104 - 00:20:47.540, Speaker A: N. Post compose the curve with distance.
00:20:47.572 - 00:20:52.012, Speaker B: From x n. Now, it is clear.
00:20:52.108 - 00:20:58.460, Speaker A: That the distance, or it should be clear that the distance between gamma t and gamma s is equal to the.
00:20:58.492 - 00:21:10.224, Speaker B: Soup in n of the distance of dn. I guess s minus dmt every t less than s. Is it clear?
00:21:10.264 - 00:21:15.164, Speaker A: So, distance function is one. Leipzig. So, this inequality is obvious.
00:21:16.864 - 00:21:17.604, Speaker C: Right?
00:21:17.944 - 00:21:19.080, Speaker A: It's triangular inequality.
00:21:19.112 - 00:21:19.884, Speaker B: If you wish.
00:21:21.824 - 00:21:31.360, Speaker A: These other in, for this other inequality, you just use the density, right? You pick xn to be very close to, I mean, one of the two.
00:21:31.392 - 00:21:33.736, Speaker B: Guys, I don't remember, close to one.
00:21:33.760 - 00:21:36.044, Speaker A: You get the distance with the other one is minus the distance.
00:21:36.504 - 00:21:39.336, Speaker C: Okay, very well.
00:21:39.480 - 00:21:42.136, Speaker B: Now, because of this in particular, for.
00:21:42.160 - 00:21:44.272, Speaker A: Any, for any n, this quantity is.
00:21:44.288 - 00:21:46.816, Speaker B: Bounded from both by this quantity, which.
00:21:46.840 - 00:21:48.152, Speaker A: Is in turn bounded from all by.
00:21:48.168 - 00:21:52.884, Speaker B: The interval from t to s of f, right?
00:21:53.544 - 00:22:02.432, Speaker A: So, so, this function, so, t to d and t is absolutely continuous. This is a real value function. This is absolutely continuous.
00:22:02.448 - 00:22:04.896, Speaker B: In the classical, you can see yourself.
00:22:05.080 - 00:22:08.444, Speaker A: Make sense with, with with derivative.
00:22:11.104 - 00:22:11.488, Speaker C: Almost.
00:22:11.536 - 00:22:16.524, Speaker B: Every make sense.
00:22:19.064 - 00:22:19.804, Speaker C: Right?
00:22:20.704 - 00:22:27.904, Speaker A: Because, because the list DNS minus dnt is less or equal than d gamma.
00:22:27.944 - 00:22:32.894, Speaker B: T gamma s, which is less or equal than the interval from t to s, right?
00:22:33.014 - 00:22:33.874, Speaker A: So this.
00:22:37.214 - 00:22:49.390, Speaker B: Now, I define g to be the soup in n of d n y. G is a function well.
00:22:49.422 - 00:22:51.638, Speaker A: Defined almost everywhere and inner one.
00:22:51.726 - 00:22:52.214, Speaker B: Right.
00:22:52.334 - 00:22:55.246, Speaker A: G g, of course, g is less.
00:22:55.270 - 00:23:04.294, Speaker B: Or even f. Okay. Uh, perhaps, perhaps something this.
00:23:05.674 - 00:23:06.494, Speaker C: Okay.
00:23:07.354 - 00:23:15.034, Speaker B: And so g is in a one. Okay, now, okay.
00:23:15.114 - 00:23:21.306, Speaker A: Uh, now, I claim that the matrix speed is equal to g. And if.
00:23:21.330 - 00:23:25.690, Speaker B: I do so, I've done, because I.
00:23:25.722 - 00:23:32.506, Speaker A: Would have proved that dc is in l one, because g is in l one. And I also, given that g is.
00:23:32.530 - 00:23:34.426, Speaker B: Clearly less regular than f, the metric.
00:23:34.450 - 00:23:37.854, Speaker A: Speed would be less or equal than any f, you know, that you could put on the right hand side.
00:23:38.634 - 00:23:41.130, Speaker B: Okay, well, but how much is it.
00:23:41.162 - 00:23:49.932, Speaker A: The lim soup of, you know, as h goes to zero of the distance.
00:23:50.068 - 00:24:16.024, Speaker B: Gamma t plus h gamma t divided by h. Well, this is less or equal. Actually, this is equal to the limbs of h of the soup in n of dn. T plus h minus dn of t divided by h.
00:24:18.044 - 00:24:19.244, Speaker A: Because of this equal.
00:24:19.324 - 00:24:21.924, Speaker B: I just, for any t and h.
00:24:21.964 - 00:24:26.424, Speaker A: I just replaced the distance between the gamma t and gamma with the soup.
00:24:26.764 - 00:24:30.304, Speaker B: Okay, but, but, um.
00:24:32.084 - 00:24:34.148, Speaker A: Now, dn is an absolutely continuous function.
00:24:34.236 - 00:24:41.148, Speaker B: So this is equal to the limb soup with h of the soup in.
00:24:41.196 - 00:24:47.264, Speaker A: N of the integral from the average integral from t to t plus h of the derivative.
00:24:51.164 - 00:24:51.904, Speaker C: Right?
00:24:53.524 - 00:24:58.116, Speaker A: And I can certainly bring the soup inside. And this is less or equal so.
00:24:58.140 - 00:25:04.304, Speaker B: That the h of the average integral of g.
00:25:07.864 - 00:25:08.656, Speaker C: Right.
00:25:08.840 - 00:25:12.824, Speaker B: Makes sense. Okay, but this is equals, you know.
00:25:12.864 - 00:25:29.168, Speaker A: G is in n one. So this is equal by theorem. This is equal to g almost everywhere. So the rim soup of this difference quotient is bounded from o by g almost everywhere. This is not easy.
00:25:29.216 - 00:25:50.952, Speaker B: So don't get scared if you, if you don't succeed. What about the limit? The limit sh goes to zero of.
00:25:50.968 - 00:25:55.984, Speaker A: The same quantity v grammatical divided by.
00:25:56.064 - 00:26:05.716, Speaker B: H. Well, for any n, this is greater or equal than the limit over h of the distance.
00:26:05.860 - 00:26:08.584, Speaker A: The n t plus h.
00:26:16.524 - 00:26:17.412, Speaker B: All right.
00:26:17.588 - 00:26:41.744, Speaker A: But this, of course, is greater or equal. Actually, I can also do so like this. Just take one limit from one side. I mean, this is actually, no, wait a second. Yes, this is true for almost every, because of the differentiability points of, you know, this function is absolutely continuous. So it is almost everywhere differentiable.
00:26:41.784 - 00:26:45.560, Speaker B: So this limit is actually a limit.
00:26:45.592 - 00:26:57.322, Speaker A: For almost every t. Let me put this way. And this is equal. And this is equal to, I guess, the. No, wait a second. I'm in trouble. Oh, yes.
00:26:57.378 - 00:26:58.570, Speaker C: This. Okay.
00:26:58.642 - 00:27:01.254, Speaker A: And this is equal. This is equal to dm prime.
00:27:04.394 - 00:27:04.706, Speaker C: For.
00:27:04.730 - 00:27:08.454, Speaker B: Almost every t. You agree?
00:27:09.194 - 00:27:10.706, Speaker A: This function is absolutely continuous.
00:27:10.770 - 00:27:14.794, Speaker B: You know, the absolute value can take it outside. Right?
00:27:14.954 - 00:27:16.530, Speaker A: And. But this was true for every.
00:27:16.562 - 00:27:17.506, Speaker B: Nice.
00:27:17.680 - 00:27:19.582, Speaker A: And so I take the supine and.
00:27:19.598 - 00:27:23.074, Speaker B: I end the proof. Make sense?
00:27:25.294 - 00:27:26.154, Speaker C: Okay.
00:27:31.854 - 00:27:44.934, Speaker A: I just use this one lipstick function to basically reduce the problem to a problem in r where I know the derivatives exist. And then I just. This is a rather common procedure in nosmot analysis.
00:27:44.974 - 00:27:48.046, Speaker C: Okay. All right.
00:27:48.070 - 00:27:49.314, Speaker A: What is what I want to say?
00:27:51.614 - 00:27:55.234, Speaker B: Ah, yes, I have another exercise.
00:27:59.494 - 00:28:09.390, Speaker A: So this quantity is called matrix speed. Sometimes in some application, one wants to deal with cars with constant speed, say unit speed.
00:28:09.542 - 00:28:10.354, Speaker B: Okay?
00:28:11.094 - 00:28:37.234, Speaker A: In this case, it is good to know that you can always re parameterize an absolutely continuous curve in order to get a car with basically the same image, but constant speed. And this is something that, you know, in the Smooth Setting you all know how to do. And in fact, deconstruction can be pushed up to this Level of Generality. I leave you as an exercise. The formula. Getting the formula is more than half of the job in this case.
00:28:38.414 - 00:28:42.434, Speaker B: So let say gamma be absolutely continuous.
00:28:49.234 - 00:28:52.774, Speaker A: And define, you know, and then define. And define.
00:28:56.354 - 00:28:57.314, Speaker B: Say, a function.
00:28:57.354 - 00:28:58.734, Speaker A: Let me call capital t.
00:29:00.754 - 00:29:01.370, Speaker B: Capital t.
00:29:01.402 - 00:29:04.082, Speaker A: Goes from, you know, is defined on.
00:29:04.098 - 00:29:11.782, Speaker B: An interval whose length is this? This is what we certainly want to.
00:29:11.798 - 00:29:13.670, Speaker A: Call length of the carbon.
00:29:13.822 - 00:29:14.446, Speaker B: Okay?
00:29:14.550 - 00:29:23.766, Speaker A: So it's defined on the interval, you know, long as the length of the curve to zero one. This is my reparameterization Function in some sense.
00:29:23.910 - 00:29:25.074, Speaker B: And t of s.
00:29:27.654 - 00:29:28.582, Speaker A: Is defined as.
00:29:28.598 - 00:29:33.274, Speaker B: The inf among all t's.
00:29:37.354 - 00:29:38.334, Speaker A: Such that.
00:29:38.994 - 00:29:42.746, Speaker B: The integral from zero to t of.
00:29:42.810 - 00:29:48.214, Speaker A: Gamma prime r, doctor, is greater or equal than s.
00:29:51.874 - 00:29:52.734, Speaker B: Okay?
00:29:54.834 - 00:29:56.914, Speaker A: This is our reparametrization Function.
00:29:56.954 - 00:30:00.910, Speaker B: Then define Eta, EtA of S. Now.
00:30:00.942 - 00:30:08.514, Speaker A: Eta is a curve defined on this interval. And EtA of S is defined as gamma at t of s.
00:30:12.174 - 00:30:13.950, Speaker B: Okay, I.
00:30:13.982 - 00:30:18.474, Speaker A: Gave you the definition of Eta. And the exercise asks you to prove.
00:30:19.734 - 00:30:24.070, Speaker B: That EtA is absolutely continuous, okay?
00:30:24.102 - 00:30:33.064, Speaker A: Defined not on zero one, but, you know, and with. With matrix speed identically equal to one.
00:30:38.284 - 00:30:38.980, Speaker C: Okay?
00:30:39.092 - 00:31:08.724, Speaker A: With same image, okay. With same image of gamma. Here, the bordering technical issue is that this function, capital t in general, may be not continuous. Imagine what happens if you have your curve gamma, that starts, moves, then stands here for a little bit, and then moves again.
00:31:09.664 - 00:31:10.208, Speaker B: Okay?
00:31:10.256 - 00:31:31.664, Speaker A: When you want to re parameterize this curve with constant speed, you want to follow. And then in some sense, forget about this interval of time where the car was standing still and just move forward. So, so that's why, uh, the repetition map has to be defined in this way. It might be not continuous, but still the resulting curve is absolutely continuous and with magic speed, exactly equal to one.
00:31:32.364 - 00:31:43.524, Speaker C: Okay. All right. Okay. Um.
00:31:49.744 - 00:31:52.376, Speaker B: Now we are almost able to.
00:31:52.400 - 00:32:16.118, Speaker A: Define gradient flows or gradient flow trajectories. But before, in fact. In fact, the, the. Instead of the terminology that I just introduced would be sufficient. But in fact, we will not work with general lower semicontinuous functionals. We will work with convex functionals or geodesically convex function. These are a way better class of function we might better behave, and for which, in particular, the theory of gradient flows works particularly well.
00:32:16.118 - 00:32:30.754, Speaker A: So I won't dig at all into the more general theory. We'll just focus on the con, on the case of k convex functions. So I will write a definition, and I'm sure this will look familiar to you. So, let's say. So.
00:32:32.494 - 00:32:40.060, Speaker B: Say that e is a function from x, to say, sorry, r positive.
00:32:40.092 - 00:32:44.644, Speaker A: Plus infinity, lower semicontinos, we said that.
00:32:44.684 - 00:32:48.804, Speaker B: E is k convex or k geodesically convex.
00:32:48.844 - 00:32:53.544, Speaker A: But let me drop the geodesic term. K is a real parameter.
00:32:54.124 - 00:32:54.864, Speaker C: If.
00:32:56.644 - 00:33:27.496, Speaker B: For any x zero, x one in the domain of the energy that exists adequisic from x zero to x one, such that the energy at gamma t is less or equal than one minus t energy at gamma zero plus.
00:33:27.560 - 00:33:30.700, Speaker A: T energy gamma one. And I will stop here.
00:33:30.732 - 00:33:33.308, Speaker B: If we were speaking about convexity.
00:33:33.396 - 00:33:35.268, Speaker A: If I speak about k convexity, I.
00:33:35.276 - 00:33:52.940, Speaker B: Have to add the correction, which is this, right? For everything. So, of course, you see that this.
00:33:52.972 - 00:34:03.332, Speaker A: Definition is basically the same that I, we used for the loss true Milani condition, right? So the space CD k infinity, if and only if the entropy is k convex in this sense, in this space.
00:34:03.348 - 00:34:06.344, Speaker B: Of probability measures, right?
00:34:07.124 - 00:34:18.664, Speaker A: And of course, if I read this definition on RD, say, with k equals zero, I get back to the classical concept of convexity, where segments, you know, are interpreted as judicial. You know, these are segments.
00:34:20.663 - 00:34:23.471, Speaker B: Now, if you have a little bit.
00:34:23.487 - 00:34:52.844, Speaker A: Of familiarity with convex analysis, you know that, and perhaps you, or if you, or if you don't, you might have heard something when I introduced the concept of c concavity and c sub differential. So, in convex analysis, there is this magical fact that you can speak about differentials without taking derivatives or if you wish, the differential operator as a global interpretation.
00:34:53.984 - 00:35:04.404, Speaker B: Recall, perhaps, let me open a parenthesis. Recall if, say, e from rd to r is convex.
00:35:06.544 - 00:35:25.334, Speaker A: Let me say convex is mood. Just to. I mean, smooth is totally relevant. But let me just. Then, then, you know, v is the gradient of e. W is the differential of e at x. If and only.
00:35:27.714 - 00:35:29.802, Speaker B: E of x plus.
00:35:29.898 - 00:35:34.594, Speaker A: W applied to y minus x is less or equal than e or y for every.
00:35:34.634 - 00:35:39.686, Speaker B: Yeah, right.
00:35:39.750 - 00:35:50.646, Speaker A: So what in principle should be something that holds when y goes to x, thanks to the convexity in, you know, assumption becomes something global.
00:35:50.830 - 00:35:51.518, Speaker B: Okay.
00:35:51.646 - 00:36:06.470, Speaker A: And that of course is beautiful, because, you know, among other things, I mean, shows that the differential for convex function is a much more stable notion. Because you can start from this and prove that if a sequence of convex.
00:36:06.502 - 00:36:09.174, Speaker B: Function is converging point wise, then sub.
00:36:09.214 - 00:36:12.438, Speaker A: Differentials also basically converge point wise, kind of.
00:36:12.606 - 00:36:13.314, Speaker C: Please.
00:36:15.614 - 00:36:18.754, Speaker B: Smooth. Sorry.
00:36:22.934 - 00:36:23.794, Speaker C: Okay.
00:36:26.254 - 00:36:30.876, Speaker A: Now, it is easy to see this was just motivational, but it is.
00:36:30.900 - 00:36:34.260, Speaker B: Easy to see that if e is.
00:36:34.292 - 00:36:35.304, Speaker A: K convex.
00:36:40.044 - 00:36:40.784, Speaker C: Then.
00:36:42.524 - 00:36:43.704, Speaker A: The slope.
00:36:46.404 - 00:36:46.852, Speaker C: That I.
00:36:46.868 - 00:36:50.544, Speaker B: Previously defined, did I erase it?
00:36:53.284 - 00:36:54.068, Speaker A: Yes.
00:36:54.236 - 00:37:02.144, Speaker B: Okay, well, anyway, then the slope is actually equal. So this was, so the limb soup.
00:37:03.124 - 00:37:12.744, Speaker A: As y goes to x. So I rewrite it. E of x minus e of y, positive part divided by the distance between x and y.
00:37:13.764 - 00:37:17.340, Speaker B: This was the slope, right?
00:37:17.452 - 00:37:19.540, Speaker A: Well, this is actually equal to the.
00:37:19.572 - 00:37:20.224, Speaker C: Soup.
00:37:22.144 - 00:37:30.044, Speaker B: Over all y, not x of ex minus ey divided by the distance.
00:37:31.744 - 00:37:35.484, Speaker A: Plus k over two distance xy.
00:37:39.864 - 00:37:45.496, Speaker B: So it admits an interpretation as supremo rather than him.
00:37:45.520 - 00:37:49.724, Speaker A: So in some sense, you know, hope you see the analogy this above.
00:37:49.844 - 00:37:50.624, Speaker C: Okay.
00:37:53.484 - 00:37:54.412, Speaker B: I will prove this in a.
00:37:54.428 - 00:38:08.484, Speaker A: Second, but let me just point out that of course, this is way weaker than this, because the slope is basically a number, but the differential is a vector, or vector, if you wish. Some differential carries many more informations, but.
00:38:08.604 - 00:38:12.504, Speaker B: It'S sort of first think in the direction.
00:38:13.404 - 00:38:14.522, Speaker A: So let me prove this.
00:38:14.628 - 00:38:21.434, Speaker B: Um, of course, this is obvious, because.
00:38:21.774 - 00:38:25.302, Speaker A: If you take the limb soup when y goes to s, this will not.
00:38:25.438 - 00:38:27.990, Speaker B: Just play any role, right?
00:38:28.182 - 00:38:38.954, Speaker A: And so, instead of the limb soup of this quantity, when y goes to x is the same as the limb soup of this quantity, because this becomes more. And of course, the limb soup is less or even the soup.
00:38:40.154 - 00:38:41.642, Speaker B: Okay, so I'm just to prove the.
00:38:41.658 - 00:38:42.934, Speaker A: Other, the other inequality.
00:38:44.794 - 00:38:45.586, Speaker C: Given that I'm.
00:38:45.610 - 00:38:47.434, Speaker A: A bit lazy, I would do this for k equals zero.
00:38:47.474 - 00:38:50.654, Speaker B: But, you know, you will see that.
00:38:55.754 - 00:38:59.814, Speaker A: You will see that generalizes to general k. I mean, in a moment.
00:39:01.554 - 00:39:04.082, Speaker B: So what I have to prove, if.
00:39:04.098 - 00:39:06.290, Speaker A: I want to show this other inep.
00:39:06.322 - 00:39:12.516, Speaker B: Quantity, well, I have to prove that for any y, I can find the.
00:39:12.540 - 00:39:21.264, Speaker A: Sequence of points for which this difference quotient, say sequence of point going to x, that the different quotient is bounded from above by this pressure.
00:39:22.724 - 00:39:23.704, Speaker C: Makes sense.
00:39:24.044 - 00:39:27.100, Speaker A: Ok. And which points will I pick? Well, I pick the Judiatic along which.
00:39:27.172 - 00:39:30.044, Speaker B: That input is at this point, you pick.
00:39:30.084 - 00:39:45.864, Speaker A: So let me just, you know, I start from, you know, my function is convex. Say k equals zero. I start from that inequality. And what I conclude, I mean, just with manipulation, I get that e of, say, gamma zero minus e of gamma.
00:39:45.944 - 00:39:53.004, Speaker B: One divided by the distance gamma zero, gamma one is less or equal.
00:39:55.984 - 00:39:57.444, Speaker A: Let me see then.
00:39:58.804 - 00:40:02.964, Speaker B: Gamma zero minus e, gamma t divided.
00:40:03.124 - 00:40:06.984, Speaker A: T times the distance between gamma zero, gamma one.
00:40:11.124 - 00:40:13.532, Speaker B: I just, you know, forget about the.
00:40:13.548 - 00:40:20.304, Speaker A: Last term, you know, I just sort of rewritten things. And divided by t times distance gamma zero, gamma one.
00:40:22.004 - 00:40:22.904, Speaker B: Okay.
00:40:24.904 - 00:40:36.928, Speaker A: Now, gamma was a Jurisic between gamma zero and gamma one. So by a definition or a consequence of the definition, this is the same as the distance between gamma zero and.
00:40:36.936 - 00:40:41.244, Speaker B: Gamma t Gedi six constant speed.
00:40:43.104 - 00:40:52.984, Speaker A: But then, you see, you know, this is fixed. This is true for entities. This particular is also less relevant than in soup. T, say, goes to zero.
00:40:55.284 - 00:40:57.820, Speaker B: But of course, this quantity is less.
00:40:57.852 - 00:41:07.424, Speaker A: Or equal than this log of e at gamma zero. Because this gamma t's are just, you know, one family of points going, going to gamma zero.
00:41:09.204 - 00:41:11.148, Speaker C: Makes sense, right?
00:41:11.196 - 00:41:14.704, Speaker A: And here, you know, I had arbitrary point x, naught x.
00:41:16.064 - 00:41:21.844, Speaker C: Okay. Um.
00:41:22.984 - 00:41:24.912, Speaker B: All right, maybe that's, this is a.
00:41:25.008 - 00:41:26.696, Speaker A: Good moment to take a small break.
00:41:26.880 - 00:41:27.744, Speaker B: Five minutes.
00:41:27.864 - 00:41:29.472, Speaker A: And then, and then we see how.
00:41:29.528 - 00:41:33.520, Speaker B: Starting from this, from this formula, this.
00:41:33.632 - 00:41:37.520, Speaker A: Global version of this loop, we can quit.
00:41:37.672 - 00:41:43.544, Speaker C: I know, but, like, I feel like a month, it was like, especially when you're doing. Yeah, exactly.
00:41:48.364 - 00:42:01.900, Speaker B: All right, how's it going? Questions a lot, maybe been a bit fast, but if there are no questions.
00:42:01.932 - 00:42:02.624, Speaker A: Let me.
00:42:07.904 - 00:42:14.480, Speaker B: Let me perhaps move on. So the, if you, if you recall.
00:42:14.632 - 00:42:26.204, Speaker A: You know, the motivation, the initial motivation I gave for attempting a definition of gradient from the metric setting was based on the inequality like this.
00:42:29.344 - 00:42:34.864, Speaker B: Well, after a little bit of manipulation, I mean, this was.
00:42:38.404 - 00:42:38.964, Speaker C: In a small.
00:42:39.004 - 00:42:43.684, Speaker B: Setting, I wrote this inequality, among other.
00:42:43.724 - 00:42:53.356, Speaker A: Things, and then I used young inequality. But first of all, I used this inequality. Now, this inequality I could have, you know, written is because the object here were smooth.
00:42:53.540 - 00:42:53.940, Speaker C: Okay?
00:42:53.972 - 00:43:02.670, Speaker A: Of course, you should not expect the inequality of this form to be true for arbitrary functionals and arbitrary curves.
00:43:02.852 - 00:43:06.738, Speaker B: Okay? For instance, say that.
00:43:06.826 - 00:43:12.346, Speaker A: Say that we move to the metric setting. We replace this guy with a slope.
00:43:12.530 - 00:43:13.162, Speaker C: Okay?
00:43:13.258 - 00:43:24.258, Speaker A: This guy is a metric speed. And I wonder whether for any, I don't know, lower semi continuous functions, it is true that I can bound the difference of the energy by this integral along any curve.
00:43:24.386 - 00:43:25.210, Speaker C: Okay?
00:43:25.402 - 00:43:37.588, Speaker A: And the answer is clearly no. Clearly no, because what you, in fact, in some sense, in order for this to be true, you need some absolute continuity of the function, right? So you can take, you can easily find counterexamples, but, you know, take a.
00:43:37.596 - 00:43:44.356, Speaker B: Function that makes a jump, the slope is zero everywhere, literally at every point, okay?
00:43:44.460 - 00:43:53.828, Speaker A: But of course, there is no way that you can bound the difference, you know, of the function to different points by integrating the slope multiplied by some, you know, parameter speed.
00:43:53.916 - 00:44:02.878, Speaker B: Okay, there's no way. But if the function is convex, this works, okay? And this is a crucial, this is.
00:44:02.886 - 00:44:08.074, Speaker A: A crucial regularity property of convex functions.
00:44:13.454 - 00:44:15.766, Speaker B: And the dilemma, if you wish, the.
00:44:15.790 - 00:44:18.446, Speaker A: Key lemma that we justify our definition.
00:44:18.470 - 00:44:29.334, Speaker B: Of gradient flows is the following. Select e.
00:44:34.074 - 00:44:53.470, Speaker A: Be k convex and lower semicontinuous, and let's say gamma, be absolutely.
00:44:53.502 - 00:45:00.966, Speaker B: Continuous with, you know, image in the.
00:45:00.990 - 00:45:03.314, Speaker A: Domain of the entropy and the energy.
00:45:03.854 - 00:45:09.514, Speaker B: Okay, takes value where the energy is finite. Then.
00:45:12.734 - 00:45:14.582, Speaker A: I can bound the energy, you.
00:45:14.598 - 00:45:31.504, Speaker B: Know, gamma t, and of the energy gamma s. We are the integral from t two s of the state times log for every, say t less than s.
00:45:33.364 - 00:45:34.144, Speaker C: Hey.
00:45:35.844 - 00:45:37.164, Speaker A: I notice that, I'm not.
00:45:37.204 - 00:45:43.844, Speaker B: Stating, I'm not stating that this quantity is in l one, it can be plus infinity.
00:45:43.884 - 00:45:57.292, Speaker A: And of course, if the integral is plus infinity, I'm only happy because it's certainly greater or equal than the left hand side. However, if this function is in one, then I can bound, then I can.
00:45:57.308 - 00:45:58.224, Speaker B: Bound the difference.
00:46:04.244 - 00:46:06.484, Speaker A: Lower semi continuous is of what?
00:46:06.524 - 00:46:09.054, Speaker C: Sorry, clear definition assumed.
00:46:10.554 - 00:46:27.330, Speaker A: Ah, yes, you're right. Okay, yes, of course. So nobody ever cares about convex functions that are not lower semiconductors. That's the true, right? In some sense, for the same reason that nobody ever cares of linear functions that are not continuous. Yes, yes, absolutely, absolutely. But I have to say in the.
00:46:27.362 - 00:46:34.158, Speaker B: Proof, actually, you know, some things like these identities I've wrote here, it's true.
00:46:34.206 - 00:46:43.414, Speaker A: Even if I don't assume lower semicontinuity, that in any case I will always. But even if I don't, this inequality is true. But I don't know if this is true. If I remove lower semicontinuity, in fact.
00:46:43.454 - 00:46:44.834, Speaker B: I'm afraid it's false.
00:46:45.774 - 00:46:48.774, Speaker A: But, so the proof I know works with lower semiconductor.
00:46:48.814 - 00:46:49.514, Speaker C: Okay.
00:46:52.494 - 00:46:53.754, Speaker B: Okay, let's prove it.
00:46:56.334 - 00:47:38.594, Speaker A: First of all, I mean, idea, I mean, I will skip some detail, but let me, I want to in some sense convince you about the fact that this is true. So first of all, I can assume, you know, by the parameterization, can assume and assume that the speed of gamma is one. Use the reparabitization argument. And notice that by the chain, the chain rule in 1d in some sense. So these inequalities in biant and the reparametrization, that's, I think, what I want to say.
00:47:39.134 - 00:47:40.834, Speaker B: Okay, so.
00:47:42.574 - 00:47:54.134, Speaker A: I reparametized my constant speed via the exercise that I gave before and I, and I fall in that case. Okay, another thing is that it is sufficient to prove. So what I really want to prove is that need to prove.
00:47:58.794 - 00:48:12.734, Speaker B: That if, well, okay, I guess the map that takes t and returns this loop of e at gamma t is in a one.
00:48:14.474 - 00:48:15.214, Speaker C: Then.
00:48:18.734 - 00:48:22.486, Speaker A: Uh, the derivative then t.
00:48:22.590 - 00:48:26.314, Speaker B: To e gamma t is absolutely continuous.
00:48:27.414 - 00:48:37.070, Speaker A: I mean, if I prove this, I'm done. This, that's, that's, I guess what I want to say. Gamma sp, that's why there is no gamma dot in this.
00:48:37.182 - 00:48:40.514, Speaker B: Okay, and let's say that I've proved this.
00:48:42.574 - 00:48:50.486, Speaker A: Then, I guess once I've shown that this is absolutely continuous, I know that it is differentiable. This is a map from r to.
00:48:50.510 - 00:48:54.206, Speaker B: R. So I know that it is.
00:48:54.230 - 00:49:04.246, Speaker A: The integral of its derivative. So I know that e gamma t minus e gamma s is equal to the integral of the derivative. But now the derivative, you know, it's clear that if you know e, what.
00:49:04.270 - 00:49:14.942, Speaker B: Is gamma e plus h minus e gamma t divided by h. Actually, let.
00:49:14.958 - 00:49:34.034, Speaker A: Me say, let me do the, like this. Let me show the limit. Let me say h go to zero from below. So the h here is negative. And basically I can write this in this way. This of course is less or equal.
00:49:35.014 - 00:49:39.230, Speaker B: Than the limit when h goes to zero from below.
00:49:39.342 - 00:49:42.454, Speaker A: And now you see, I can let me multiply and divide by the distance.
00:49:42.494 - 00:49:46.566, Speaker B: Between gamma t and gamma t plus h. Okay?
00:49:46.630 - 00:49:57.314, Speaker A: And what I get, and what I get is, well, let me sup them say so, e gamma t minus e. Perhaps, let me take some space. Let me back here.
00:49:59.974 - 00:50:06.008, Speaker B: So e, what is dt gamma t.
00:50:06.176 - 00:50:13.016, Speaker A: Is equal to the limb when h goes to zero of the difference quotient, which is less or equal than the.
00:50:13.040 - 00:50:16.064, Speaker B: Limit when h goes to zero from.
00:50:16.104 - 00:50:22.240, Speaker A: Below of e gamma t minus e gamma t plus h divided by the.
00:50:22.272 - 00:50:22.964, Speaker B: Distance.
00:50:26.184 - 00:50:32.864, Speaker A: Times the distance. I mean, is the inequality, is the chain rule the usual thing, right?
00:50:33.524 - 00:50:34.384, Speaker C: Okay.
00:50:35.964 - 00:50:42.636, Speaker A: And of course, this inequality gets only stronger if I take the positive part of the right hand side. And the only thing that possibly had.
00:50:42.660 - 00:50:47.028, Speaker B: The wrong sign was this. But now this is bounded from above.
00:50:47.196 - 00:50:53.436, Speaker A: You know, this goes to the speed which is actually one, by the way, by the rapidization. And this goes, is bounded from both by this law.
00:50:53.460 - 00:50:56.194, Speaker B: For being gamma, you see?
00:50:58.254 - 00:51:02.070, Speaker A: So the only thing that I need to prove is the regularity property of this function.
00:51:02.142 - 00:51:02.914, Speaker B: Of course.
00:51:03.774 - 00:51:04.634, Speaker C: Okay.
00:51:06.494 - 00:51:09.342, Speaker B: But that I can do, because.
00:51:09.398 - 00:51:35.498, Speaker A: I can reduce the problem to a problem, you know, in real analysis. And what I do is I define.
00:51:35.546 - 00:51:41.106, Speaker B: Now the function f of t and g of t f of t is.
00:51:41.130 - 00:51:43.094, Speaker A: Just the energy gamma t.
00:51:45.894 - 00:51:46.406, Speaker C: And g.
00:51:46.430 - 00:51:58.354, Speaker B: Of t is the soup over s different from t of f of t minus f of s.
00:52:00.374 - 00:52:03.714, Speaker A: Divided by s minus t.
00:52:09.294 - 00:52:16.300, Speaker B: Right? Now notice, notice that again, my goal.
00:52:16.332 - 00:52:18.504, Speaker A: Is to prove that f is absolutely continuous.
00:52:20.084 - 00:52:23.116, Speaker B: This g of t is, okay, this.
00:52:23.140 - 00:52:34.864, Speaker A: Is less or equal. I mean, notice, so s minus, given that my curve has constant speed. One, the distance between gamma t and gamma s is bounded by s minus t.
00:52:36.604 - 00:52:37.344, Speaker B: Right?
00:52:37.984 - 00:52:44.552, Speaker A: And so, so this ratio is less or equal than the soup over s.
00:52:44.608 - 00:52:48.240, Speaker B: Different from t of e. Let me.
00:52:48.312 - 00:52:56.404, Speaker A: Take positive e gamma t minus e gamma s divided by the distance of the gamma t gamma s.
00:53:00.184 - 00:53:00.924, Speaker C: Right?
00:53:02.824 - 00:53:23.726, Speaker A: And, but if you recall the, this formula, the global formula for the slope, and you notice that gamma being absolutely continuous as a bounded image. So this quantity over here is bounded by some, you know, number. This is less or equal than the.
00:53:23.750 - 00:53:37.034, Speaker B: Slope at gamma t plus some cos. Make sense. Thanks.
00:53:37.154 - 00:53:39.934, Speaker A: Thanks to this global version of the.
00:53:40.554 - 00:53:41.414, Speaker C: Okay.
00:53:44.034 - 00:53:45.810, Speaker A: So now we have an exercise.
00:53:45.842 - 00:54:01.240, Speaker B: Area analysis, the exercises, so let you know, f be from zero one into r b says that lower semicontinius, it's.
00:54:01.272 - 00:54:07.688, Speaker A: Lower semicontinuous because it's the composition of a continuous cap with a lower semicontinuous function. So it is lower semicontinuous.
00:54:07.856 - 00:54:12.004, Speaker B: And put, you know, define g.
00:54:15.784 - 00:54:16.072, Speaker C: As.
00:54:16.088 - 00:54:19.720, Speaker B: In here, there's no, there's no e, there's no gamma.
00:54:19.752 - 00:54:21.232, Speaker A: This is a, you know, a function.
00:54:21.288 - 00:54:33.356, Speaker B: G that's defined from f and assume that g is in one, because if.
00:54:33.380 - 00:54:38.852, Speaker A: This guy is in l one, this bounds from above g. Okay? Or perhaps let me take a positive.
00:54:38.908 - 00:54:41.692, Speaker B: Part so that we don't, g is non negative.
00:54:41.748 - 00:54:43.384, Speaker A: So bounded from both is sufficient.
00:54:48.644 - 00:54:54.144, Speaker B: Prove that f is absolutely continuous.
00:54:57.784 - 00:54:59.448, Speaker A: I really have a function, function on.
00:54:59.456 - 00:55:04.952, Speaker B: The interval lower semicontinuous. I define this guy and prove that.
00:55:05.088 - 00:55:12.232, Speaker A: Is actually, actually absolutely okay. There are no matrix basis.
00:55:12.248 - 00:55:12.804, Speaker B: Now.
00:55:17.024 - 00:55:17.840, Speaker C: All right?
00:55:17.992 - 00:55:20.740, Speaker B: So if you don't prove it, well.
00:55:20.772 - 00:55:21.684, Speaker A: Believe me, this is true.
00:55:21.724 - 00:55:25.852, Speaker B: So in some sense, the conclusion, our lemma holds.
00:55:25.948 - 00:55:37.852, Speaker A: So this means that in the case of convex functions or k convex functions, we really have this bound, we really can bound the difference of energy in with the integral, with that integral.
00:55:37.948 - 00:55:56.284, Speaker B: Okay, so we can define, so they finish. So let.
00:56:03.704 - 00:56:05.376, Speaker A: Be k convex and lower.
00:56:05.400 - 00:56:09.872, Speaker B: Semicontinuous, or k convex, I can't have.
00:56:09.928 - 00:56:14.944, Speaker A: Two brakes and lower semicont. And we say.
00:56:18.164 - 00:56:27.276, Speaker B: That, say a car gamma, typically defined on a half line.
00:56:27.340 - 00:56:36.064, Speaker A: But you can easily modify the definition to let it be a bounded interval. Is a gradient.
00:56:40.064 - 00:56:42.044, Speaker B: Flow trajectory.
00:56:48.224 - 00:56:48.964, Speaker C: If.
00:56:51.024 - 00:56:54.724, Speaker B: The energy, well, first of all, if the.
00:56:55.024 - 00:57:01.520, Speaker A: Well, if the energy of gamma t, actually gamma is finite for every t.
00:57:01.592 - 00:57:18.468, Speaker B: For every t, and the energy at gamma t is equal to the energy, the gamma s plus one r. The integral from t two s of gamma.
00:57:18.516 - 00:57:22.100, Speaker A: Dot r squared plus the slope of.
00:57:22.132 - 00:57:27.264, Speaker B: E squared, gamma r. Doctor, for every.
00:57:30.224 - 00:57:31.084, Speaker C: Okay.
00:57:35.264 - 00:57:39.720, Speaker A: You can view that lemma as a moral justification that this definition makes.
00:57:39.752 - 00:57:44.024, Speaker B: Sense in some sense. It's carrying really information, okay?
00:57:44.144 - 00:57:59.588, Speaker A: Because this inequality, this inequality is always true. Actually, I should say if, perhaps I should specify if it is absolutely continuous.
00:57:59.676 - 00:58:02.224, Speaker B: It is absolutely continuous.
00:58:03.004 - 00:58:04.464, Speaker A: So that this makes sense.
00:58:05.044 - 00:58:05.904, Speaker C: Okay?
00:58:06.404 - 00:58:08.224, Speaker A: So this inequality is always true.
00:58:11.364 - 00:58:16.676, Speaker B: So the content of the definition is this inequality, right?
00:58:16.860 - 00:58:21.464, Speaker A: That tells that when you move forward in time, s is bigger than p.
00:58:21.774 - 00:58:24.874, Speaker B: You should decrease the functionality, right?
00:58:25.374 - 00:58:27.046, Speaker A: And you should decrease as much as.
00:58:27.070 - 00:58:31.062, Speaker B: You can because of at least this quantity.
00:58:31.238 - 00:58:38.142, Speaker A: Typically, you cannot stand still, because if you stand still, unless you are in a minimum, if you, this guy will.
00:58:38.158 - 00:58:45.430, Speaker B: Be positive and this guy will be zero, but then, you know, the energy will be constant.
00:58:45.542 - 00:58:48.992, Speaker A: And so you have this inequality, but not.
00:58:49.008 - 00:58:50.552, Speaker C: This makes sense.
00:58:50.608 - 00:58:51.444, Speaker B: Let's see.
00:58:55.944 - 00:58:58.688, Speaker A: And notice also that it is sufficient.
00:58:58.736 - 00:59:02.044, Speaker B: To ask, sufficient to check.
00:59:06.424 - 00:59:10.084, Speaker A: Because this is true. Always sufficient to check the case.
00:59:16.084 - 00:59:16.396, Speaker C: T.
00:59:16.420 - 00:59:24.464, Speaker B: Equals zero and nes, right, make sense.
00:59:27.444 - 00:59:32.916, Speaker A: This should be, I mean, if you think about it for one moment, this.
00:59:32.940 - 00:59:38.060, Speaker B: Should be clear because you have, suppose that, you know that this is true for.
00:59:38.172 - 00:59:39.812, Speaker A: From zero, from t equals zero and.
00:59:39.828 - 00:59:44.370, Speaker B: A certain size, and pick an intermediate time t, right?
00:59:44.482 - 00:59:55.490, Speaker A: Then. Then, I mean, the dissipation is additive. You know, there's a sort of a telescopic kind of thing that is going on. The dissipation from zero to s is the sum of the dissipation from zero to t plus.
00:59:55.522 - 00:59:58.498, Speaker B: That's. And.
00:59:58.666 - 01:00:08.404, Speaker A: Okay, so you have your sort of, by the way, this, for any point, if you have the other inequality from zero to s, you are forced at equality for all intermediate couples.
01:00:09.544 - 01:00:10.404, Speaker C: Okay?
01:00:13.584 - 01:00:50.184, Speaker A: In some applications, I mean, you can, I mean, the leader should hear. I mean, this, I mean, diverges a little bit. So for sure, you want the energy to be finite at positive t's, because otherwise you cannot be a gradient flow trajectory. If at least your energy is not finite. One can discuss whether imposing the energy to be finite at times zero, often one does not force this. But in this settings and sense, this is not a quite natural requirement, meaning that there are no existence or any kind of result. If you don't also ask that at t equals zero, the energy span, basically.
01:00:50.884 - 01:00:52.824, Speaker B: Okay, that's the definition.
01:00:54.004 - 01:01:06.440, Speaker A: Of course, it's easy to draw definitions. The hard part is to attach theorem to the definition. And in particular, in particular, we have just a convex function on a metric.
01:01:06.472 - 01:01:09.444, Speaker B: Space through this definition.
01:01:09.744 - 01:01:14.488, Speaker A: Do we have gradient flows? Yes. Or not? That's the first question. Do we have existence or not?
01:01:14.656 - 01:01:18.464, Speaker B: Now, luckily for me, even though this.
01:01:18.504 - 01:01:57.184, Speaker A: Question is conceptually very relevant, I actually don't need to know the answer to this question for the purpose of my talk of my series of lectures. So I can consent this at least I give you the theorem, but I will not prove it. And the theorem is this. And this is basically the job sheet. Let x be compact, the Georgie plus e k convex.
01:02:00.684 - 01:02:02.304, Speaker B: And lower semicontinuous.
01:02:07.964 - 01:02:08.396, Speaker C: Say.
01:02:08.460 - 01:02:11.424, Speaker A: X is a point in the domain of the energy.
01:02:13.924 - 01:02:28.994, Speaker B: Then there exists gradient flow trajectories starting from x gamma with gamma naught equal x.
01:02:32.774 - 01:02:33.634, Speaker C: Okay.
01:02:36.854 - 01:02:44.794, Speaker A: This compactness assumption can be. Actually, I could, I could, I could. I mean, or let me just say. Or sublevel. So v.
01:02:48.244 - 01:02:58.144, Speaker B: Are, you know, intersected bolts, compact. I mean, that's closed bolts.
01:03:04.644 - 01:03:14.024, Speaker A: This, the judge's proof works equally well in this case, but which does not add that much of a generic in some sense.
01:03:14.644 - 01:03:15.504, Speaker C: Okay.
01:03:17.964 - 01:03:19.252, Speaker B: I will not prove this theorem.
01:03:19.308 - 01:03:20.860, Speaker A: I will just tell you the basic.
01:03:20.892 - 01:03:21.904, Speaker B: Idea of the proof.
01:03:23.044 - 01:03:55.196, Speaker A: Well, or actually not the basic. The first step, let me see. And the first step is. This is to realize, let's go back once again to the smooth case. And how do you prove existence of gradient flows in smooth case? Of course, if you are on a v and the function e is smooth, then the gradient is a smooth vector field. You can apply Cauchy lipstick, but even, you know, even if you are in a D or maybe on a Hibbert space, and your function is only convex, then convexity is not sufficient. I mean, convexity does not imply leakage, regulatory, or vector field.
01:03:55.196 - 01:04:02.892, Speaker A: It all implies a certain kind of monotonicity. And existence is typically obtained in the following way.
01:04:03.068 - 01:04:09.634, Speaker B: So, by solve, basically solve the, what is called the implicit order scheme.
01:04:16.614 - 01:04:40.126, Speaker A: The idea is to say you fix a time parameter tau bigger than zero, and you, you, basically, you, you want to solve a time discretization of the gradient flow equation. Say that we are on Rd and, and find so, and recursively define say.
01:04:40.230 - 01:04:44.674, Speaker B: X to zero is our initial point x.
01:04:45.174 - 01:04:50.558, Speaker A: And if you define x to n, you define x to n plus one.
01:04:50.646 - 01:05:00.476, Speaker B: So that, such that the, what is the differential of e at x to m plus one actually agree.
01:05:00.540 - 01:05:07.020, Speaker A: Let me see, this should be equal to. Okay, to pay attention, a little bit of attention to the sign, let me think 1 second.
01:05:07.172 - 01:05:15.024, Speaker B: And actually n plus one x two n divided by two to the minus.
01:05:16.204 - 01:05:18.060, Speaker A: So suppose you are able to find.
01:05:18.172 - 01:05:20.144, Speaker B: X two n plus one such that this is true.
01:05:25.794 - 01:05:31.218, Speaker A: Yes. Then what you can do is, okay, I start from my point x. I.
01:05:31.226 - 01:05:33.402, Speaker B: Define x to zero and then x.
01:05:33.418 - 01:05:57.954, Speaker A: To one extra two, x to three, etcetera, etcetera. And this equation is a discretization of, you know, this is almost, you know, this is kind like x prime, equal minus gradient, right? This is sort of discretization in time of this equation. So you can hope that you iterate, you pass Togo to zero by some compactance, you get some limit, and, and then the limit is a gradient flow.
01:05:58.414 - 01:06:03.350, Speaker B: Okay, now, of course, I mean, I.
01:06:03.422 - 01:06:15.274, Speaker A: You know, moved with this case, the problem from, from solving this equation to first solving this. So I should show that this is solvable and then improving that the limit satisfactor.
01:06:15.734 - 01:06:20.366, Speaker B: But now, the interesting thing is that this is basically easily solvable in many.
01:06:20.390 - 01:06:22.286, Speaker A: Circumstances because of the variational nature.
01:06:22.390 - 01:06:31.766, Speaker B: So just pick, just minimize the energy plus.
01:06:31.870 - 01:06:33.422, Speaker A: Well, let me write directly in the.
01:06:33.438 - 01:06:44.614, Speaker B: Metric way, distance from the previous point divided by two. If you are saying RD, the minimize.
01:06:44.694 - 01:06:46.598, Speaker A: Any minimizer of this function, which should.
01:06:46.646 - 01:06:52.594, Speaker B: Exist by some, you know, minimal compactness argument satisfactorily.
01:06:53.174 - 01:06:54.034, Speaker C: Okay?
01:06:55.374 - 01:06:57.326, Speaker A: And the starting point of the George's.
01:06:57.350 - 01:06:59.190, Speaker B: Proof is, of course, what I can.
01:06:59.222 - 01:07:00.350, Speaker A: Minimize this even in matrix.
01:07:00.382 - 01:07:01.154, Speaker B: This is.
01:07:02.094 - 01:07:14.880, Speaker A: And maybe, you know, this sort of scheme can be, can be run even in the context, of course, in matrix setting, proving that the limit is actually gradient for trajectory is much harder and they will not produce. But, you know, that's the starting point.
01:07:14.992 - 01:07:15.724, Speaker C: Okay.
01:07:18.064 - 01:07:21.192, Speaker B: You know, other, other things, I will.
01:07:21.208 - 01:07:25.164, Speaker A: Not prove this, because luckily for me, lucky for me, I will not get this result.
01:07:30.904 - 01:07:32.244, Speaker B: I have existence.
01:07:32.984 - 01:07:39.266, Speaker A: But then the second question is, what about uniqueness? In particular, maybe, you know, you're familiar.
01:07:39.290 - 01:07:42.266, Speaker B: With the fact that on RD, or.
01:07:42.290 - 01:07:52.858, Speaker A: More generally my manifolds, the gradient flow of a convex function contracts distances. How many of you have never heard about this?
01:07:52.906 - 01:07:56.826, Speaker B: By the way, gradient flow of convex function contracts this.
01:07:56.850 - 01:08:05.154, Speaker A: Okay, very well. So let me put out this. Say that we are again on RD, if you wish, e and e is a smooth convex.
01:08:05.614 - 01:08:09.514, Speaker B: Let's say that I have two curves solving this equation.
01:08:13.614 - 01:08:24.674, Speaker A: So two gradient flow trajectories. And, well, let me, let me first of all, perhaps observe one crucial thing, one crucial property of the sub differential.
01:08:25.454 - 01:08:28.262, Speaker B: So let me pick, you know, let.
01:08:28.278 - 01:08:45.390, Speaker A: Me just, for brevity, let me put v equal gradi at x w equal gravity at y. So I have this inequality. That is true because v is the.
01:08:45.422 - 01:08:48.646, Speaker B: Gradient of the energy at x. I.
01:08:48.670 - 01:08:56.265, Speaker A: Also have the other inequality. The energy at y plus w dot x minus y is less or equal.
01:08:56.289 - 01:08:59.305, Speaker B: Than e of x, right?
01:08:59.409 - 01:09:03.265, Speaker A: If you add these two inequality up, the energy will disappear.
01:09:03.329 - 01:09:06.933, Speaker B: You know, simplify, and you remain with.
01:09:07.753 - 01:09:21.533, Speaker A: The bound, which is what is x minus y scalar product. V minus w is greater or equal than zero.
01:09:25.173 - 01:09:26.033, Speaker B: Okay?
01:09:27.173 - 01:09:31.341, Speaker A: And with that at the end, let's compute the derivative of the distance between.
01:09:31.397 - 01:09:34.353, Speaker B: Gamma t and theta t. Distance squared.
01:09:37.453 - 01:09:51.170, Speaker A: Well, everything is mouse. I can apply, you know, libraries, whatever. This is gamma t minus theta t, gamma t prime, minus eta t prime. I use my equation.
01:09:51.322 - 01:09:55.082, Speaker B: This is equal to minus gamma t.
01:09:55.218 - 01:10:02.174, Speaker A: Eta t, and then f grad e, gamma t minus grad eta t.
01:10:04.954 - 01:10:05.314, Speaker C: But.
01:10:05.354 - 01:10:06.374, Speaker A: This is a sign.
01:10:07.674 - 01:10:08.274, Speaker C: Okay?
01:10:08.354 - 01:10:14.414, Speaker A: So this is less or equal than zero for convex functions.
01:10:16.294 - 01:10:17.062, Speaker B: Okay?
01:10:17.198 - 01:10:25.714, Speaker A: So, in particular, gradient trajectory are unique because if I have two guys that start from the same point, you know, they must continue to be the same.
01:10:26.654 - 01:10:27.590, Speaker B: Okay?
01:10:27.782 - 01:10:42.754, Speaker A: And more generally, so more generally, if you play this trick with k convex, so, so, yeah, I mean, actually, what this tells, this tells that the distance between gamma t and, I mean, this is stronger than unit. This just tells you, an estimate.
01:10:45.414 - 01:10:45.782, Speaker C: Tells.
01:10:45.798 - 01:10:47.654, Speaker A: You that the flow contracts distances.
01:10:47.814 - 01:10:48.926, Speaker B: You know, you start from any two.
01:10:48.950 - 01:11:01.806, Speaker A: Points, any two gradient flows, trajectories, you know, they must be closer than initial points. If you, if you don't have a convex function, but a k convex, this is perturbed by, by an exponential factor.
01:11:01.870 - 01:11:05.454, Speaker B: E to the minus kt, okay?
01:11:05.494 - 01:11:08.634, Speaker A: So if k is negative, distances can increase.
01:11:09.264 - 01:11:09.816, Speaker C: Okay?
01:11:09.880 - 01:11:14.764, Speaker A: But, but still, you know, uniquely still there.
01:11:15.824 - 01:11:16.684, Speaker C: Okay?
01:11:17.904 - 01:11:19.920, Speaker A: Now this argument, in order to prove.
01:11:19.952 - 01:11:24.200, Speaker B: This, we needed to differentiate the distance squared and to use the fact that.
01:11:24.232 - 01:11:29.448, Speaker A: You know, this derivative is given by a scalar product. So this kind of thing is very.
01:11:29.616 - 01:11:33.044, Speaker B: Riemannian like and not very physical like.
01:11:33.664 - 01:11:57.574, Speaker A: In fact, there are counter examples to uniqueness if you move away from the riemannian setting. So, in general example.
01:12:02.334 - 01:12:04.354, Speaker B: Take asymmetric space.
01:12:04.654 - 01:12:07.542, Speaker A: Take just two reals with the elliptic.
01:12:07.598 - 01:12:19.262, Speaker B: Distance, and define a function e, you.
01:12:19.278 - 01:12:21.134, Speaker A: Know, e of x one, x two.
01:12:21.294 - 01:12:27.956, Speaker B: Will be just x one. You can check that this function is.
01:12:27.980 - 01:12:44.708, Speaker A: Convex with k equals zero. Notice that in order to stay. So, it is important that in the definition of k convexity, I said, for any couple of points, there exists one Judisic along which the function is convex.
01:12:44.756 - 01:12:47.540, Speaker B: Not along every Judisc, okay?
01:12:47.692 - 01:13:13.576, Speaker A: Because, you know, in the. In r two, with the. With the n infinity norm, you know, these are, given that the norm is partially flat, there are many cars that are juridict. So, for instance, this curve is adjudic. But of course, the function on this curve oscillates back and forth, and so it cannot be convex. But whenever I interpret with segments, these are the basics. And convexity is true because actually the functions are fine.
01:13:13.760 - 01:13:14.512, Speaker B: Okay.
01:13:14.648 - 01:13:15.040, Speaker C: Okay.
01:13:15.072 - 01:13:22.790, Speaker A: Now it is an exercise to check that the slope of this guy is identically one. What else can it be? This is just the first coordinate.
01:13:22.942 - 01:13:23.710, Speaker C: Okay?
01:13:23.862 - 01:13:28.794, Speaker A: And then, now let's wonder what the gradient flow trajectory starting. Let's say, from the origin.
01:13:29.534 - 01:13:29.894, Speaker C: And.
01:13:29.934 - 01:13:42.454, Speaker A: Okay. Gradient flow should be something that tries to decrease the function as fast as it can. So, a first conjecture about the gradient flow. I start from here, and I move to the left with speed one. And I can check that this car.
01:13:42.494 - 01:13:57.554, Speaker B: Satisfies the inequality that actually adjusts the raised. So this confirms the Georges theorem in two reals. Of course, that was not the point of the example.
01:13:57.594 - 01:14:00.974, Speaker A: The point is that. The point is that even this curve.
01:14:01.714 - 01:14:04.514, Speaker B: Or this curve, or, you know, anything.
01:14:04.554 - 01:14:07.834, Speaker A: That moves to the left with speed one and up and down with speed.
01:14:07.874 - 01:14:09.770, Speaker B: Less than one is still a gradient.
01:14:09.802 - 01:14:14.000, Speaker A: To trajectory because, you know, the value of the function at time t is.
01:14:14.032 - 01:14:17.176, Speaker B: Always, you know, minus t, and the.
01:14:17.200 - 01:14:23.884, Speaker A: Speed is always one because, you know, the vertical part of the speed does not matter as long as it's smaller than the horizontal part.
01:14:25.264 - 01:14:26.176, Speaker C: Okay?
01:14:26.360 - 01:14:31.128, Speaker B: So uniqueness dramatically fails if, you know, you don't have.
01:14:31.216 - 01:14:32.164, Speaker A: You don't have.
01:14:34.104 - 01:14:34.952, Speaker B: Okay.
01:14:35.128 - 01:14:36.408, Speaker A: Of course you can say, well, I.
01:14:36.416 - 01:14:39.804, Speaker B: Mean, this is not a strictly convex.
01:14:39.844 - 01:14:44.308, Speaker A: Function and the norm is not strictly convex. Maybe as soon as you have, you.
01:14:44.316 - 01:14:47.596, Speaker B: Know, theorem, you have a binary space.
01:14:47.780 - 01:14:59.156, Speaker A: Or actually rd with the norm strictly convex and a strictly convex function, then maybe gradient flows are unique. Nobody knows. Okay, so that's, you know, situation gets, you know, as hard as this.
01:14:59.220 - 01:14:59.476, Speaker B: Okay?
01:14:59.500 - 01:15:01.944, Speaker A: So there's no known uniqueness result.
01:15:04.684 - 01:15:05.452, Speaker C: Okay.
01:15:05.588 - 01:15:07.544, Speaker A: There is one particular case.
01:15:11.164 - 01:15:11.620, Speaker B: Which is.
01:15:11.652 - 01:15:13.332, Speaker A: Relevant and whose proof I would have.
01:15:13.348 - 01:15:14.660, Speaker B: Loved to give you, but I think.
01:15:14.692 - 01:15:15.836, Speaker A: I will not have time.
01:15:16.020 - 01:15:23.144, Speaker B: So we just state the theorem.
01:15:24.644 - 01:15:25.980, Speaker A: And this is a theorem of mine.
01:15:26.012 - 01:15:27.764, Speaker B: This is the, I guess, the first.
01:15:27.804 - 01:15:29.504, Speaker A: Result about heat flows on.
01:15:30.454 - 01:15:31.194, Speaker B: On.
01:15:33.894 - 01:15:45.954, Speaker A: Metric measure spaces. I mean, in the sense of lossroom be learning. Let's say anomalized.
01:15:52.134 - 01:15:56.984, Speaker B: Let mu be in d domain of the relative entropy.
01:15:59.684 - 01:16:06.384, Speaker A: Then the gradient flow. Well, there is at most one gradient.
01:16:07.004 - 01:16:16.584, Speaker B: Then there is at most one gradient flow trajectory starting.
01:16:18.604 - 01:16:19.544, Speaker A: From you.
01:16:24.184 - 01:16:28.144, Speaker B: Okay, in this case, regardless of any.
01:16:28.184 - 01:16:37.040, Speaker A: Kind of riemannianity assumption, uniqueness is still restored. This is the only result about uniqueness of gradient flows in a non Riemann like setting.
01:16:37.072 - 01:16:41.560, Speaker B: That is quite of a coincidence that this.
01:16:41.672 - 01:16:57.126, Speaker A: Okay, I don't have any deep explanation as to why this is true. Let me just give you one idea of the proof, because it is the first time and we'll see many instances of this phenomenon during this series of.
01:16:57.150 - 01:17:02.046, Speaker B: Lectures where the horizontal geometry given by.
01:17:02.070 - 01:17:10.754, Speaker A: The optimal transport problem interacts with the vertical geometry in some sense given by standard affine interpolation and the key lemma.
01:17:11.454 - 01:17:18.530, Speaker B: That we not prove. But the key lemma states that the.
01:17:18.562 - 01:17:22.010, Speaker A: Map that takes mu and returns the.
01:17:22.042 - 01:17:25.774, Speaker B: Slope of the entropy squared.
01:17:28.194 - 01:17:31.010, Speaker A: This map is well defined on the domain of the entropy.
01:17:31.202 - 01:17:34.094, Speaker B: This map is convex.
01:17:36.914 - 01:17:39.534, Speaker A: Convex with respect to a finite interpolation.
01:17:41.614 - 01:17:52.914, Speaker B: Not jurisdictionally convexity. The k convexity of the relative entropy.
01:17:54.134 - 01:18:00.394, Speaker A: Allows to conclude this convexity. K is lost here, just convexity of this guy.
01:18:02.694 - 01:18:10.004, Speaker B: Okay, if you have this, then the conclusion is easy, because if you have this.
01:18:12.224 - 01:18:13.924, Speaker A: And I will not prove that lemma.
01:18:15.984 - 01:18:46.468, Speaker B: But if you have that, one can conclude as follows. Say that. Say that mu t and nu t.
01:18:46.596 - 01:18:53.184, Speaker A: Are two gradient flow trajectories starting from the same mu zero.
01:18:55.804 - 01:19:12.324, Speaker B: And consider the curve sigma t. Just take the average. Okay, notice that. Notice that.
01:19:12.624 - 01:19:40.480, Speaker A: How much is it, the distance squared between say, sigma t and sigma s, the optimal transport problem. If you think so, if you think about this, you can pick, you can bound. So pick an optimal map, an optimal plan, sorry, from mu t to mu s. And another optimal plan, say gamma one, say gamma one is optimal, or gamma, say is optimal from gamma is a bad word. Alpha is optimal from mu t to.
01:19:40.512 - 01:19:43.880, Speaker B: Mu s, and beta is optimal from.
01:19:43.912 - 01:19:45.484, Speaker A: Mu t to nu s.
01:19:47.304 - 01:19:47.896, Speaker C: If you.
01:19:47.960 - 01:19:55.834, Speaker A: Take their average alpha plus beta divided by two, this at least is an admissible plan from sigma t to sigma s.
01:19:57.974 - 01:19:58.718, Speaker B: Right?
01:19:58.886 - 01:20:02.434, Speaker A: So I can use this plan to bounce from above this guy.
01:20:04.174 - 01:20:07.902, Speaker B: Okay, but then the integrating, the cost.
01:20:07.958 - 01:20:12.230, Speaker A: Integrated is linear at the level of plants. So this is less or equal than.
01:20:12.262 - 01:20:15.878, Speaker B: One half w two squared muti mu.
01:20:15.926 - 01:20:18.866, Speaker A: S plus w to squared new t.
01:20:18.890 - 01:20:24.186, Speaker B: New s. Just because you know, this.
01:20:24.210 - 01:20:35.834, Speaker A: Is maybe not optimal. Okay, out of these, you can deduce that even at mu t and u t were absolutely continuous also, sigma t is absolutely continuous with the, you know.
01:20:35.914 - 01:20:38.666, Speaker B: The derivative, the metric derivative being.
01:20:38.730 - 01:20:42.974, Speaker A: So if you want the metric derivative, the squared metric derivative is convex.
01:20:43.734 - 01:20:55.034, Speaker B: In the squared vast time derivative is almost everything. Okay.
01:20:57.694 - 01:20:59.966, Speaker A: Okay, but then, but then, you.
01:20:59.990 - 01:21:11.304, Speaker B: Know, we have, we have that the entropy of mu, zero is on one side. It is equal to the entropy of.
01:21:11.344 - 01:21:20.764, Speaker A: Mu, say capital t plus one half the interval from zero to capital p of mu, t dot squared plus this loop.
01:21:26.024 - 01:21:26.648, Speaker C: Right?
01:21:26.776 - 01:21:27.684, Speaker A: For any.
01:21:29.504 - 01:21:31.136, Speaker B: For any t, right?
01:21:31.200 - 01:21:35.412, Speaker A: And the same for new. This is the fact that nu is a gradient for project.
01:21:35.528 - 01:21:42.420, Speaker B: Right? But now what's the point? The point is that if these two are different, are not the same, then.
01:21:42.452 - 01:21:46.524, Speaker A: There will be some capital t such that nu, capital t is different from.
01:21:46.564 - 01:21:54.292, Speaker B: Nu capital t. Okay, pick that t. You write this identity for mu and.
01:21:54.308 - 01:21:55.624, Speaker A: This identity for mu.
01:21:56.884 - 01:22:00.684, Speaker B: And then you take the average, okay.
01:22:00.804 - 01:22:04.012, Speaker A: When you take the average, of course, the left hand side, you still have.
01:22:04.068 - 01:22:07.174, Speaker B: The entropy of mu, zero.
01:22:08.074 - 01:22:11.026, Speaker A: And here you have equality of entropy.
01:22:11.050 - 01:22:14.522, Speaker B: The average of the entropy is mu, capital T plus.
01:22:14.578 - 01:22:20.986, Speaker A: You see where this is going now, right? Divided by two. And then you have plus one half the average. I mean, one half of this term.
01:22:21.010 - 01:22:22.094, Speaker B: Of the one halfs.
01:22:22.754 - 01:22:36.206, Speaker A: But this QuaD slope is convex. By this lemma that I mentioned, the speed is convex by trivia reasons. So this is greater or equal. Then, you see, I can put here.
01:22:36.310 - 01:22:39.474, Speaker B: Speed squared plus slope squared.
01:22:43.454 - 01:22:56.914, Speaker A: And the contradiction comes from the fact that the entropy is strictly convex. Rollo is a strictly convex function with respect to a fine interpolation. So if these two guys are different, the entropy of the midpoint will be strictly smaller.
01:22:58.734 - 01:22:59.472, Speaker B: Right?
01:22:59.638 - 01:23:14.664, Speaker A: But this is impossible by, you know, by. So that will contradict the inequality that is always true for gradient, right? You cannot have that the energy at time zero is strictly bigger than the energy at the little time plus one half, this guy, because the entropy is complex.
01:23:15.324 - 01:23:18.468, Speaker B: Okay, so you have this interaction of.
01:23:18.556 - 01:23:23.144, Speaker A: You know, the Barstein geometry and these, you know, these affine interpolation that plays a role.
01:23:24.164 - 01:23:25.024, Speaker C: Okay.
01:23:26.904 - 01:23:31.552, Speaker B: All right, let me conclude with.
01:23:31.568 - 01:23:37.964, Speaker A: A couple of words about stability. By the way. So, by the way, if you combine the georges theorem with this uniqueness result.
01:23:38.664 - 01:23:46.584, Speaker B: Now at least if the base space XDM, if X is compact, then the.
01:23:46.624 - 01:24:05.344, Speaker A: Space of probability measures with this w two distance is also compact. So the Georgia ensures existence. These results in uniqueness. And if you believe to join the auto, we can. So we can say that this gradient flow that we now found is the heat flow on this CD kinetic spaces.
01:24:06.204 - 01:24:08.708, Speaker B: Okay, now of course you can be.
01:24:08.716 - 01:24:13.044, Speaker A: A little bit suspicious, because, you know, typically heat flow is something that, where you have something of this form, right?
01:24:13.084 - 01:24:16.116, Speaker B: So, I mean, you should at least.
01:24:16.180 - 01:24:35.310, Speaker A: Speak about the heat flow. And I promise we will have inflection, okay? As of now, if you have no experience with optimal transport, and jko, this is totally invisible, almost unbelievable. I mean, I wouldn't believe, but I promise that this. Okay, so the evolution of the densities in here will satisfy this equation. So we will actually have PD's in this metric measure context.
01:24:35.462 - 01:24:38.554, Speaker C: Okay, um.
01:24:42.054 - 01:24:42.886, Speaker A: Stability.
01:24:43.070 - 01:24:46.868, Speaker B: Now, uh, so, first remark.
01:24:46.916 - 01:24:55.144, Speaker A: So, of course, of course, gradient flows are, you know, the equation of gradient flow is a first order equation.
01:24:55.724 - 01:24:59.036, Speaker B: You cannot expect solution to this equation.
01:24:59.140 - 01:25:04.484, Speaker A: To be stable under a zero order convergence. So, for example, you can have a sequence of functions.
01:25:04.524 - 01:25:06.588, Speaker B: It's easy to build sequence of functions.
01:25:06.636 - 01:25:28.572, Speaker A: Say, on r, on r converging uniformly, a limit function, but for which gradient flows don't convert to the. For instance, just take the n is just like this with a little bump, but the bump is always, you know, smaller. And then the limit is just this guy. So you see that in the limit function, you know, if you start from.
01:25:28.588 - 01:25:32.468, Speaker B: The gradient flow, you move to the right and you decrease the energy.
01:25:32.636 - 01:25:37.588, Speaker A: But at any n, given that there is a local minima, you know, you cannot surpass the local minimum with the.
01:25:37.596 - 01:25:40.914, Speaker B: Gradient flow trajectory than there. Makes sense what I'm saying.
01:25:41.974 - 01:25:50.994, Speaker A: So the, the approximated level, all the gradient flow trajectories stop here because there was a bump, but the bump disappears in the limit. So in the limit, I just continue.
01:25:52.814 - 01:25:53.638, Speaker B: Makes sense.
01:25:53.766 - 01:26:00.194, Speaker A: So, so you cannot expect, for instance, uniform convergence to imply convergence of gradient loss.
01:26:01.254 - 01:26:01.854, Speaker B: Okay?
01:26:01.934 - 01:26:10.348, Speaker A: Of course, first order equation, you cannot. However, as I mentioned, we already mentioned before with the convexity business.
01:26:10.436 - 01:26:13.724, Speaker B: If, if the functions are uniformly convex.
01:26:13.764 - 01:26:19.984, Speaker A: Or uniformly semi convex, then I have a uniform second order bound, and then maybe, maybe I can pass the theme.
01:26:21.204 - 01:26:29.676, Speaker B: And the theorem is this here. This is also a theorem of mine.
01:26:29.700 - 01:26:35.274, Speaker A: On the same paper as before, but still. See also Sandy serfati.
01:26:41.294 - 01:26:49.954, Speaker B: That's one eight. The theorem does the following so say that en.
01:26:52.654 - 01:26:58.354, Speaker A: Are a sequence of functions on the same. Let me call, let me call it.
01:26:59.034 - 01:27:14.814, Speaker B: Y to zero plus infinity, b k convex and lower semicontinuous. And so that.
01:27:16.994 - 01:27:20.734, Speaker A: En gamma converges to infinity.
01:27:24.654 - 01:27:25.394, Speaker C: Let.
01:27:29.214 - 01:27:33.910, Speaker A: Say gamma nt be a gradient.
01:27:34.062 - 01:27:37.554, Speaker B: Flow trajectory for en.
01:27:42.494 - 01:27:48.878, Speaker A: With gamma n times zero, which is converging to a.
01:27:48.886 - 01:27:52.964, Speaker B: Limit point, uh, x infinity, I guess.
01:27:53.424 - 01:28:02.728, Speaker A: And, uh, and these are recovery sequence, meaning that the energy infinity of x infinity, if you want, is greater or.
01:28:02.736 - 01:28:06.000, Speaker B: Equal than the limit in n of.
01:28:06.032 - 01:28:13.824, Speaker A: The energy n of gamma n zero. The other inequality is always true by gamma convergence.
01:28:13.864 - 01:28:24.060, Speaker B: So this is, I'm asking that the energy is convergent and this is finite. Okay, then.
01:28:24.172 - 01:28:54.164, Speaker A: Well, actually then, so these are the hypotheses. I hope this is clear, does not confuse your brain. Then two things are true. First of all, there is compactness. Namely, you know, this family of curves is pretty compact in the, you know, in the topology of, in, with respect to local uniform convergence.
01:29:05.664 - 01:29:07.416, Speaker B: Any subsequence, as.
01:29:07.440 - 01:29:14.524, Speaker A: A further extraction, such that on any compact interval of zero plus infinity, it converges uniformly to a limit car.
01:29:15.464 - 01:29:35.524, Speaker B: To any limit curve is a gradient flow trajectory for infinity, starting from x and p.
01:29:39.164 - 01:30:04.744, Speaker A: Okay, the proof of this is easy. I will outline it in a second. But before going to the proof, let me point out why this is relevant for our purposes. So, if you believe me that the gradient flow given by this result and the Georges theorem is the heat flow, now, you can interpret this as saying, okay, perhaps under some compactness assumptions, at least the heat flow of CDK infinity space is stable.
01:30:07.094 - 01:30:07.954, Speaker B: Okay.
01:30:08.774 - 01:30:31.906, Speaker A: And the stability of the heat flow will be, you know, the cornerstone of, of all the stability results for differentiation operators in this CDK. Okay, if you believe that the heat flow. Yes. Is the, you know, solves DT Rho equal Laplacian of Rho. So in some sense, D Laplacian will also pass the limit. Please. Yeah, sorry.
01:30:32.010 - 01:30:33.374, Speaker C: Yes, thank you.
01:30:33.834 - 01:30:48.226, Speaker A: Yes, it is important for this statement. There are variants of these statements that do not require this, nor this. But in this case, the only concept of gradient flow that I have is a concept that works well if the initial point has finite energy.
01:30:48.330 - 01:30:49.814, Speaker B: So I have to enforce this.
01:30:50.754 - 01:31:00.964, Speaker A: So it is not sufficient that the initial points convert to the initial point is not even sufficient that the energy is converged. It's important that the energy remains uniformly bounded.
01:31:02.184 - 01:31:06.844, Speaker B: Okay, let's prove this.
01:31:07.144 - 01:31:09.684, Speaker A: Actually, the proof has basically.
01:31:12.224 - 01:31:12.800, Speaker C: Any proof.
01:31:12.832 - 01:31:14.616, Speaker A: That it was gamma conversion. The proof is easy.
01:31:14.640 - 01:31:15.324, Speaker B: Okay.
01:31:18.064 - 01:31:21.920, Speaker C: Here it is. Okay.
01:31:21.952 - 01:31:38.556, Speaker A: Compactness, component compactness is an exercise. It's as coliatezela, basically, because you know, what you have, you know, the definition of gradient flow trajectory ensures that the.
01:31:38.620 - 01:31:41.644, Speaker B: Energy, well, let me just say the.
01:31:41.684 - 01:31:48.500, Speaker A: Energy of, say, okay, gamma zero, this is greater or equal or equal, if you wish. The energy of gamma t plus one.
01:31:48.532 - 01:32:03.254, Speaker B: Half integral of gamma t dot squared s will be r plus the slope squared, right?
01:32:03.754 - 01:32:06.254, Speaker A: So in particular, it is greater equal of this.
01:32:08.354 - 01:32:09.426, Speaker B: Now, if you have a sequence of.
01:32:09.450 - 01:32:11.962, Speaker A: Curves for which these guys are uniformly.
01:32:11.978 - 01:32:15.830, Speaker B: Bounded from above, and they are, and.
01:32:15.862 - 01:32:37.326, Speaker A: These guys are uniformly bounded from below, and they are because the functions are non negative. Then what I have is a sequence of curves for which this integral is uniformly bounded. But a bound on the kinetic energy gives a bound on a quantitative information about holder continuity, because the distance between.
01:32:37.390 - 01:32:40.270, Speaker B: Gamma t and gamma s is less.
01:32:40.302 - 01:32:54.614, Speaker A: Or equal than the integral from t to s of the split, which is less or equal. I mean cautious parts s minus t square root of the intro, let's say.
01:32:55.034 - 01:33:00.294, Speaker B: From zero to capital t of gamma r squared.
01:33:02.834 - 01:33:03.466, Speaker C: See what I mean?
01:33:03.490 - 01:33:06.914, Speaker A: So if I know that on a big interval zero capital t, the energy.
01:33:06.954 - 01:33:09.820, Speaker B: Is finite on that interval, I have.
01:33:09.852 - 01:33:17.796, Speaker A: An explicit holder continuity estimate for my car. In other words, if I have a.
01:33:17.820 - 01:33:20.572, Speaker B: Sequence of cards for which I know.
01:33:20.588 - 01:33:39.704, Speaker A: That this quantity is uniformly bounded, this sequence of cars will be uniformly holder the value on a compact set askoy arcela end of the compact nest. What about an enemy carver is, uh, is a gradient flow trajectory.
01:33:40.284 - 01:33:41.756, Speaker B: Well, now let me, you know, let.
01:33:41.780 - 01:33:48.156, Speaker A: Me say that gamma nt is converging to some limit card, gamma infinity, say.
01:33:48.180 - 01:33:52.012, Speaker B: For every t, right? So what I want to do, I.
01:33:52.028 - 01:33:53.724, Speaker A: Want to pass the limit in the.
01:33:53.764 - 01:34:11.480, Speaker B: In, in this, right now, this quantity.
01:34:11.672 - 01:34:19.520, Speaker A: Passes to the limit by assumption. This guy converges to e infinity of gamma infinity zero, if you wish, by.
01:34:19.552 - 01:34:23.684, Speaker B: Assumption, this right, or if you wish.
01:34:24.144 - 01:34:26.284, Speaker A: The limb soup of this is bounded by this.
01:34:30.044 - 01:34:37.092, Speaker B: These points are converging. D function as are gamma converging by.
01:34:37.148 - 01:34:39.828, Speaker A: The gamma limit inequality.
01:34:39.996 - 01:34:42.092, Speaker B: I have that the, you know, the.
01:34:42.108 - 01:34:55.664, Speaker A: Lyme of en gamma nt is greater or equal than e infinity, gamma infinity, just because of this n gamma convergence.
01:34:58.354 - 01:34:59.094, Speaker C: Right?
01:35:01.554 - 01:35:08.134, Speaker A: So basically, to conclude, I want to prove that I have lower semi continuity both in here and in here.
01:35:10.234 - 01:35:10.974, Speaker B: Right.
01:35:13.474 - 01:35:14.214, Speaker C: Now.
01:35:14.954 - 01:35:16.534, Speaker A: And these are both easy.
01:35:18.354 - 01:35:18.954, Speaker B: And.
01:35:19.074 - 01:35:28.934, Speaker A: Sorry, it's taking a few more minutes, but it's almost done. So, we conclude this lecture on gradient flows, and then starting on Monday, we do sober functions.
01:35:35.434 - 01:35:37.458, Speaker B: So first of all.
01:35:37.546 - 01:35:41.498, Speaker A: So if you want, now, I have two sub lemmas, if you wish, or.
01:35:41.546 - 01:35:43.786, Speaker B: Observations, if I have a sequence of.
01:35:43.810 - 01:35:57.304, Speaker A: Cars, so gamma nt converging point wise to gamma infinity, t for every t. And I have that gamma nt prime.
01:35:59.164 - 01:36:01.324, Speaker B: As a function.
01:36:01.364 - 01:36:10.224, Speaker A: This is a function, and this converges, say, weakly in two to g, to say I don't know of some interval.
01:36:10.804 - 01:36:18.782, Speaker B: Then the limit curve, gamma infinity is.
01:36:18.838 - 01:36:22.434, Speaker A: Absolutely continuous, and its matrix speed.
01:36:24.614 - 01:36:24.974, Speaker C: Is.
01:36:25.014 - 01:36:27.714, Speaker A: Bounded from above by g of t for almost everything.
01:36:30.614 - 01:36:35.070, Speaker B: And the other lemma is that if.
01:36:35.142 - 01:36:57.394, Speaker A: En and here convexity plays a role, and this is the crucial point where in this theorem you need convexity. If en gamma converges to e infinity and uniformly k convex, then this I have a gamma limp inequality for the slopes.
01:37:08.694 - 01:37:09.434, Speaker B: Please.
01:37:12.074 - 01:37:16.586, Speaker A: All of them are k convex with the same k, not kn. I mean k convex if you want.
01:37:16.610 - 01:37:17.254, Speaker B: That's.
01:37:21.514 - 01:37:22.306, Speaker C: So.
01:37:22.490 - 01:37:26.314, Speaker B: So here k convexity plays a role.
01:37:26.354 - 01:37:48.064, Speaker A: Notice that this is exactly, this inequality is exactly the one that fails in this. In the example maybe for about bump, because if I have bumps and then converge it to something without bumps. Here at the approximately approximated sequence, I have point minima, so points local minima, so points where the slope is zero, but in the limit, the slope is identically one, so it's not less than equal to the remaining.
01:37:49.004 - 01:37:49.864, Speaker C: Okay.
01:37:54.484 - 01:38:08.208, Speaker A: I mean, these are really, I mean. Okay, let me, this is easy because this is, you know, remember the representation formula for the slope does the following the slope of e under k convexity, the slope of e is the soup, or may say zero convexity.
01:38:08.376 - 01:38:15.924, Speaker B: This is the soup of ex minus ey positive part divided by the distance x and y.
01:38:17.584 - 01:38:18.324, Speaker C: Right?
01:38:19.024 - 01:38:33.444, Speaker A: So I say that I want to prove this inequality, knowing that the functionals are, you know, gamma converging. What do I do? I pick an arbitrary point x. I pick. So I pick a point x and I pick an arbitrary test point y.
01:38:34.524 - 01:38:36.584, Speaker B: Okay, and.
01:38:38.484 - 01:38:50.044, Speaker A: Let me think 1 second. So I should pick the optimal sequence, which one I should pick the, this.
01:38:50.084 - 01:38:51.744, Speaker B: Should be less or equal.
01:38:52.284 - 01:38:57.892, Speaker A: I pick an optimal sequence. So fix x and y, x and.
01:38:57.908 - 01:39:01.640, Speaker B: Y and xn converging to x.
01:39:01.712 - 01:39:02.792, Speaker A: If I, if I want to put.
01:39:02.808 - 01:39:05.480, Speaker B: The limit gamma limit means that x.
01:39:05.512 - 01:39:26.794, Speaker A: N converge to x is given. And, and now, and now I find, you know, yn recovery sequence going to y recovery sequence. So where, where the functional converge. And then you see that this guy.
01:39:26.874 - 01:39:30.250, Speaker B: E of x minus e of y.
01:39:30.402 - 01:39:34.002, Speaker A: Divided distance between x and y. This is less or equal than the.
01:39:34.018 - 01:39:37.002, Speaker B: Limit in n of en.
01:39:37.178 - 01:39:41.122, Speaker A: Xn minus en yn divided by the.
01:39:41.138 - 01:39:49.066, Speaker B: Distance make sense because I have lower.
01:39:49.090 - 01:39:56.134, Speaker A: Semicontinity at this level and this guy converge. So this is true also if I take the positive part.
01:39:59.034 - 01:40:01.026, Speaker B: But of course, for every n, t.
01:40:01.050 - 01:40:04.814, Speaker A: Is bounded from above by the slope of en at xn.
01:40:05.954 - 01:40:07.098, Speaker B: So this is less trigger than the.
01:40:07.106 - 01:40:10.774, Speaker A: Limit of the slopes of en at xm.
01:40:14.074 - 01:40:14.814, Speaker C: Right?
01:40:15.274 - 01:40:16.778, Speaker A: But now I've done, this was true for any.
01:40:16.826 - 01:40:21.722, Speaker B: Yeah, so I proved that this slope of the limit guy is bounded from.
01:40:21.778 - 01:40:25.174, Speaker A: Above by the limit of the slopes for any approximated sequence.
01:40:27.794 - 01:40:29.134, Speaker B: So this is trivial.
01:40:30.034 - 01:40:42.658, Speaker A: And, and I leave this as an exercise. So I can conclude the direction. So I don't. So perhaps let me just say I.
01:40:42.666 - 01:40:48.594, Speaker B: Will not comment anymore about this statement of this result, except that in many.
01:40:48.634 - 01:40:50.054, Speaker A: Applications this.
01:40:57.954 - 01:41:01.106, Speaker B: Compact, you can mention this.
01:41:01.130 - 01:41:23.356, Speaker A: But I didn't say so this. So in order to get this compactness, I need the compactness of the space where all these guys belong. Nice result. And the one of well minus ultimately assumed compactness, some deceptively, probably something a little bit less like compactors of sub levels or something of this sort. I don't remember exactly, but I want to say is that many applications where you want to pass the limit on.
01:41:23.380 - 01:41:26.100, Speaker B: CD or RCD spaces, I mean you.
01:41:26.132 - 01:41:37.780, Speaker A: Might want, and typically, I mean, in many applications you really want not to have compactness. And the generation of this result from compact to non compact case is actually quite tricky and requires a deep understanding.
01:41:37.812 - 01:41:40.452, Speaker B: Of, well, I don't know the realization.
01:41:40.508 - 01:42:06.324, Speaker A: Of this result, but you know, I can apply this to the heat flow. So, to pass the limit on heat flow on convergence sequence of CDC infinity spaces. But if I don't assume compactness, more work is needed. And the generalization to the non compact case of the heat flow convergence has been done by myself together with Mondin and Savare. I wanted to mention this because on the other hand, that it's degeneration to non compact case of stability of the CDK infinity.
