00:00:00.320 - 00:00:45.310, Speaker A: This is a joint work with Maria Dostert, who is now in Katie Stockholm, and Fernando Oliveira, who is now in the Netherlands in Delft. Let me start with the formulation of the problem. So what's the average kissing number? Whenever we have a packing, a finite set of closed balls that are not necessarily congruent with disjoint interiors in the n dimensional space. We call such a set a packing. So you see, necessarily, this is a finite set. We do not consider infinite packings. They're all finite.
00:00:45.310 - 00:01:37.024, Speaker A: The contact graph of a packing has vertex sets. That is exactly the set of those closed balls. And there is an edge between two vertices whenever the corresponding balls are tangent to each other. So here it's written that the intersection is non empty. But because in the packing, as we said, all balls have disjoint interiors, it's only their boundaries that can intersect. Or in a simpler way of putting it, they are just tangentially. Here, the average kissing number will be just twice the number of edges in a contact graph divided by the number of vertices, of which we take the suprema.
00:01:37.024 - 00:02:16.754, Speaker A: So, we consider all possible packings. For each packing, we have twice the number of edges, which is the number of tangencies. Because tangencies are mutual. One thing touches another and so it touches us back. So we have twice the number of edges divided by the number of vertices. That is, the number of balls that are present in the packing. So this is basically just the average degree of the graph, and we take the supremum of average degree of all contact graphs.
00:02:16.754 - 00:03:24.934, Speaker A: So here, the average degree of a given graph is for sure a finite thing. But the fact that the supremum of the average degree of all possible content graphs is finite is not clear. So, before we get back to mathematics, let us say that the average kissing number is not only an interesting mathematical object, but it also has some applied meaning. So it is sometimes necessary to know this average kissing number, or have some estimates for it. In material science, it has some applications for spherical codes and designs, and it has to do with extreme problems in lattice theory, geometry in principle, in general. So being itself one of those problems. Okay, so let us see what happens when we try to climb the dimension ladder.
00:03:24.934 - 00:04:13.134, Speaker A: We start with dimension n equals two. And in a two dimensional world, the classical Kobe and grieve Thurston theorem tells that the kanted graphs of packing in the plane are simple planar graphs. And this implies that the average case number is no bigger than six. So, Koeber and Rief Thorsten theorem is a very classical one, definitely Thurston is not such a classical name, quite recent. And Rev was a little bit before, and Koebe is the classical mathematician. So all those three people reinvented and reinterpreted this theorem in different terms. And this is just one of its equivalent formulations.
00:04:13.134 - 00:05:31.584, Speaker A: So from this one, from the fact that taking graphs of planar ball packings are always simple planar graphs, it's not hard to deduce that the average kissing number is no bigger than six. Now, if we consider this hexagonal packing here, the honeycomb packing, then we see that the more honeycombs we take, the closer the average degree of the corresponding packing graph gets to six. And so this is the premium over such configurations, over such packings will be exactly six. So the average case and number in dimension two is six. And the upper bound comes from a quite classical theorem, and the lower bound comes by construction. Well, now the question is, what happens in any arbitrary dimension? Nice. So let us consider for a while the casing number for congruent balls.
00:05:31.584 - 00:06:54.034, Speaker A: Before, we said that the average kissing number has to do with packings where all the balls can have different radii. Now let us suppose that we have packings of congruent balls, and after rescaling, we can just suppose that the radius is one. In this situation, it is obvious that the kissing number, this capital k for congruent holes, is finite. What is not obvious is that the average kissing number is finite, because in principle, as you construct the spacing, you can take bigger balls and smaller balls and try to somehow stuff more smaller balls in the gaps between bigger ones. So maybe the degree, the average degree is going to grow. At least you can easily try to make the degree of any vertex in this packing graph as big as you want by adding smaller and smaller balls around a given fixed ball. However, Gupta, Berg and Schramm proved that the average casing number is never more than twice the casing number for Hungary in balls.
00:06:54.034 - 00:07:51.680, Speaker A: Actually, it really takes a short but clever proof. So the average caser number is bounded. I'm sorry. Okay, so what about the upper bound? So there is this bound by Cooper working Schramm, that we would like to improve, because obviously twice the usual kissing number is not such a good upper bound, even in dimension two. So it tells us that the average kissing number is bounded. But to bound the average kissing number, and we want to get a bound that is better than the classical Cooper Berg shrimp bound. So we can easily see that twice the kissing number for concrete balls is not a good bound.
00:07:51.680 - 00:08:51.814, Speaker A: Already in dimension two. But even in dimension three, it's not. So Cooper and SRAM improved basically this twice capital k bound in there original paper. And in 2017, Glazier came up with some better estimates for the average case number that are that beat this twice capital k bound. So how can we improve? And how much can we improve? Well, first let us talk about how much. So, in 2020, Dostart, Olivier and myself managed to improve the bounds in dimension from three to nine by using semi definite programming. And here is a link to our archive paper.
00:08:51.814 - 00:10:28.584, Speaker A: And I think this was the first improvement in indicator for higher dimensions. So what about lower bounds for the lower bounds? I think Epstein, Cooper, Berg and Ziegler in 2002 gave a very, very good lower bound for dimension three. Lower bounds in other dimensions are not as close to the upper bound that we know, but they can be obtained still. And many of them are contained. In this splag book is a famous book about lattices and groups, spherical codes by Conway and Sloane. So here is the table with the best bounds known so far, and with a comparison of the lower bounds, previously known upper bounds, and the new lower bounds. So here, in all dimensions that are higher than five, the upper bound, the upper bound is just twice the guessing number.
00:10:28.584 - 00:11:14.564, Speaker A: So here you see in the third column for all the dimensions 6789, just integers. So those integers are just doubled, are doubles of the corresponding kissing number for concrete balls. And in the fourth column, you can see the improved bounds. So in dimension six, seven, eight, the improvement is somewhat visible in dimension nine. Already, it's just a small improvement, but it still exists. And in higher dimensions, we also did some computations and we had no improvement whatsoever. So here it's not shown.
00:11:14.564 - 00:12:21.704, Speaker A: The main ingredient in our proof is glazier in upper bound. So in 2017, when Glazier proved his upper bounds for the average kissing number, already improving the Cooper Brook Schramm bound, he used the so called density function. So, let us see how this density function works. Well, you see here, first of all, we need the normalized area. So we take unit sphere and we take its rho dilate, and we take a radius r bool and see how much this bool intersects the rho dilate of the unit sphere divided by the area of this rho dilate. So this is just the normalized area of the intersection. So now we need to define the density function.
00:12:21.704 - 00:13:35.874, Speaker A: So, the density function is defined this way. So, whenever we have a configuration of balls with disjoint interiors that are tangent to the central unit ball, we take the rho dilate of this unit ball or the surface of this ball sphere of radius rho, and see how much normalized area those balls cut out on this rho dilate of the unit sphere. And so we just take the supremum of such a normalized area over all possible tangent configurations. So exactly what we have here in the picture. So we have the unit sphere, we have a configuration of those green balls that are tangent to the unit sphere in the center of this picture. Then we have a dilate of the unit sphere by some number rho, and we see how much this row intersects with the green ball. So we measure this normalized area, and then we take the supremum over all possible such configurations.
00:13:35.874 - 00:14:26.870, Speaker A: And if we go back to Glazier's theorem, we see that basically whenever we can get a good bound for the density function, we actually have a good bound for the average kissing number. So finally, we proved the following theorem. It cannot be formulated in a short way. So here we will have to go through several slides. As you see, it has several components. So first of all, n is a dimension that is three or bigger. The row is this factor by which we dilate the central sphere.
00:14:26.870 - 00:15:52.730, Speaker A: When we define there is a density function. Then this small r is an increase in bijection, and the function a is such that it satisfies certain properties related to the normalized area. Whenever we have such a collection of functions, we fix an integer d and take several kernels. We take d plus one kernel, which kernel is just a real valued square integrable function of two variables. And then we compose a small function f of t, u, and v, that is, a linear combination of Jacobi polynomials together as kernels. And now the theorem says that if the small f and the kernels that we choose, and we have basically total liberty to choose, the kernels as we wish, are such that first of all, every principle submatrix of this linear combination. So the 0th kernel minus the tensor product of this function a, and it's dual that we choose in the beginning.
00:15:52.730 - 00:17:03.254, Speaker A: We also have freedom to choose this a. This little a just has to satisfy certain conditions here with respect to the normalized area. So we can tweak our bound by means of all those parameters or functions that we have the freedom to choose. So this principles, this kernel has to be such that every principle submatrix is a positive semi definite. So every principal submatrix of any other kernel is principal is positive semi definite. Here, k varies from zero to d, so f zero has to have principal submatrices positive, semi definite, and satisfy even more stringent condition. And then this function, small f should be non positive whenever the first variable t satisfies a certain condition here.
00:17:03.254 - 00:18:33.774, Speaker A: So these are all technical conditions, so you can say, and indeed they are just formulated in such a way that our proof works. So all those restrictions are partially dictated by the way the SDP bound works, but actually is dictated just by some technical intricacies that are hidden in the proof. However, we still have enough freedom to choose the kernel functions, and so we may hope that with enough freedom we can make the right choice and get a very good upper bound. So whenever the conditions of our theorem are satisfied, we can bound the density and. Okay, let's see what we can do. So, first of all, this semi definite program is of similar type that has been already studied before. Well, however, the kernels that we use are different than in that case, and we can use either polynomial kernels or step functions as kernels in this semi definite program.
00:18:33.774 - 00:19:19.644, Speaker A: So whenever we make a good choice of kernels, we have a good bound. And sometimes a good choice means that we have to use polynomials, especially in lower dimensions. So in dimension and dimension three, for example, we need to use polynomial. So let's see what we have here. So in dimensions three, you see, we have a slight improvement over glycerin's bound. Here we have to use polynomial kernels and dimensions four and five, we also use polynomial kernels. So somehow with step function kernels, we didn't get such an improvement, although we tried.
00:19:19.644 - 00:20:18.364, Speaker A: But in dimensions six, seven, eight and nine, polynomial kernels do not give us much improvement. And if we use polynomial kernels of high degree, high here means twelve or 14, then the SDP becomes so huge we just cannot solve it. It literally takes a week to run on a computer, and then we have to verify the numerical results, and that becomes enormous amount of job. So sometimes just we cannot solve the SDP. So the result is inconclusive. The solver diverges. There is no convergence over several rounds of computations, several thousands rounds of computations.
00:20:18.364 - 00:21:50.774, Speaker A: So we decided to use step functions instead in order to reduce the size of the SDP and maybe get handled on its complexity. Step functions are still good enough, but they are less efficient than polynomial kernels in lower dimensions, but in higher dimensions. Just to overcome the overall complexity of the SDPs that we create, we used step functions and we got still some improvement, which improvement doesn't happen in dimension ten and beyond. So there we don't have any good bounds, and the SDP becomes just too huge to deal with it practically. So this solver we used was SDP, GMP. So we really need the GMP library in order to good with a precision. And well, after, after using a supercomputer cluster at the University of Neuchatel, we were able to push through all these computations and obtained, and obtained a better upper bounds for the average kissing number and probably the best known up to now.
00:21:50.774 - 00:22:44.634, Speaker A: And we will be definitely happy if anyone beats those bounds in higher dimensions or comes up with lower bounds here in this table, if anybody comes with lower bounds that are closer to the upper bounds, so not with those that are in the splag book by Conway and Slavonic. With some better ones, we will be also extremely happy. So these are two open problems for now to obtain better lower bounds and maybe improve on the new upper bounds. So with this, I would like to finish. Thank you for your attention. If you have any questions, please ask.
00:22:48.054 - 00:22:52.514, Speaker B: Thank you, Alexander. Does someone have a question?
00:22:53.374 - 00:22:56.514, Speaker C: Hi, Sascha, this is Karoy speaking.
00:22:56.814 - 00:22:57.486, Speaker A: Hi.
00:22:57.630 - 00:22:59.358, Speaker C: Very nice talk you had here.
00:22:59.446 - 00:23:00.118, Speaker A: Thank you.
00:23:00.246 - 00:23:29.734, Speaker C: I have just one question, and that's about the three dimensional case. You just showed us the chart with your new upper bounds. And there are some lower bounds there. Do you have any idea where the truth might lie in any of these cases? Do you have any suspicion that a particular arrangement would correspond, you know, to the extremal value?
00:23:30.554 - 00:24:22.674, Speaker A: No, no, I don't, I don't. I mean, with the lower boundaries. Well, let's see. So the lower bound is the one by Epstein, Cooper, Berg at Ziegler here. It's apparently very good lower bound, but it's still far from the known upper bound. So the upper bound is by one unit bigger. Well, you see, what the lower bound they had and the new upper bound we have, they differ, roughly speaking, by one, but also, although they obtained their lower bound from a concrete configuration.
00:24:22.674 - 00:25:42.474, Speaker A: So they employed if the 120 cells, the classical object in dimension four, that projected down to dimension three. And this configuration definitely gives all nice things and gives us very nice lower bound. But the upper bound that we get, it comes from this SDP, which doesn't have to correspond to any geometric configuration. So I cannot really say that this upper bound, it kind of indicates that, for example, the configuration that gave the lower bound is the one. I don't think so. I mean, the lower bound is not just one configuration. So, constructed a family of packings such that the limiting average degree of the tangency graph is this 666 over 53.
00:25:42.474 - 00:25:52.714, Speaker A: But by looking at the SDP solution. So far, I see no way to confirm that their configuration is the optimal one.
00:25:57.884 - 00:25:58.824, Speaker C: Thank you.
00:26:02.684 - 00:26:18.424, Speaker B: More questions? So, Alexander, can you prove nice properties about the SDPs that you solve, like, strong duality, this kind of thing?
00:26:20.384 - 00:26:24.364, Speaker A: We didn't try. We didn't try.
00:26:24.944 - 00:26:40.324, Speaker B: So, when you solve the problem and you're able to get an upper bound, what are the tolerance that you use in this case, you really have to have a feasible solution. Right. For the problem.
00:26:40.824 - 00:26:41.604, Speaker A: Yes.
00:26:42.544 - 00:26:51.764, Speaker B: So, how do you deal with the numerical arrows to be able to assure that you got an upper bound?
00:26:53.624 - 00:27:47.814, Speaker A: So, recently, Moustru, dostert, and Dilat wrote up a program that verifies SDP. So whenever you have an approximate solution, their program actually proves that this one is feasible, because it can solve. Well, it can kind of resolve the SDP over either irrationals or an algebraic number field and actually prove that the solution is exact, and all the inequalities or equalities that have to be satisfied are satisfied. Thank you. Yeah. So, we use this machinery on our solutions.
00:27:51.074 - 00:27:54.734, Speaker B: What are the sizes that you want to solve of svps?
00:27:55.154 - 00:28:21.634, Speaker A: I don't quite remember, but I. But probably. Probably about 10,000 variables and, well, I don't know, 20, 15,000. So about that.
00:28:26.054 - 00:28:31.434, Speaker B: Especially if it's dense, it's a problem. Right. Numerically?
00:28:32.994 - 00:29:01.814, Speaker A: Yeah. Numerically, that's hard. So, this is why we use SDPa, GMp. So we solve it really with high precision, and then we want to convert those solutions into, say, solutions over an algebraic number field and check feasibility by working for the number field. That's interesting.
00:29:01.854 - 00:29:03.394, Speaker B: I don't know about that.
00:29:04.654 - 00:29:26.054, Speaker A: Yeah. Well, I mean, this is what doster Delat and Philippe Moustro did recently. So they developed, in Julia, they developed this kind of a verification package for SDPs.
