00:00:15.600 - 00:01:38.274, Speaker A: There is one last interpretation for Cholesky's algorithm in the context of rths. Here is the question. H is given on the set omega, this is given, and x zero is a point in x. And we consider this subspace all the function in h such that f at point x zero is equal to zero. It's easy to see that it's a closed subspace of H. And if we denote the kernel in h by k, what is the kernel here? Or I mean, to distinguish kh, what is kh zero? In a sense, we answered this question before by saying that kh zero is the projection of kh onto this space. Ho true, that's the answer.
00:01:38.274 - 00:02:37.971, Speaker A: But give me an explicit formula. This is what we want to do, and it's not easy. It's not difficult indeed to obtain this result by this observation. If you have m subspace generated by just one element, I mean y, a general Hilbert space, and y is in H. So we consider all multiples of y. It's a subspace. What is the projection onto M from h to h? There's a simple formula for that.
00:02:37.971 - 00:03:36.062, Speaker A: The formula is this the projection of x one through n. Of course, it has to be a multiple of y. The projection to m and what is the coefficient? The coefficient is this x color y divided by y times y, or normal y squared. It is now that the formula is given, it is easy to verify that pm is equal to pm and pm squared is equal to pm. Therefore, it's an orthogonal projection, and the range is precisely m. So it's the projection we are looking for. If the formula is not given, you may ask what is pm but one? Now that the formula is given, the verification is easy.
00:03:36.062 - 00:04:57.732, Speaker A: So this part I leave it for you, and this part I use. Why did I say this? Because my m, when the projection onto M is given, the projection onto the orthogonal complement of m would be I identity minus pm. This is the projection into M. So in other words, p m curve of x is identity times x minus scholar times y. That's, that's the formula I need. What is my mistake? M in the context of now h is rkhs and x zero is a point in x. My m is the space one dimensional space created by kx zero.
00:04:57.732 - 00:05:56.594, Speaker A: And when I k, I mean in h. When I don't put the index, it means in the ambient space. So simply I write kx zero. In this case, what would be m. Curb amperb is this all f in h such that f is orthogonal to kx zero, which is the same as the space we are looking for, f orthogonal to. This is equivalent to say that f at point x zero is equal to zero. True.
00:05:56.594 - 00:07:08.004, Speaker A: And now projection into h zero is the same as projection to n. Perc is one minus projection into mix. And we have the formula here. Therefore, what would be the projection into h zero of ky? This is equal to identity ky minus the other projection. Our formula is there. But note that y is kx zero, so it's ky kx zero. Or if you wish, we can write it as ky minus.
00:07:08.004 - 00:07:47.944, Speaker A: This is k at point x zero x zero. This is k at point x zero, y k x zero. And this is precisely the thing we are looking for. I said at the beginning, kyle, but the kernel for the space h zero is precisely equal to the projection of ky onto that space. But we didn't know what's the formula. Now we know what's the formula. That's the formula we are looking for.
00:07:47.944 - 00:08:47.984, Speaker A: And if we write it in the form of capital k, capital k is zero, point x. Y is the same as little k, index y at point x. So we evaluate both sides at point x. So this is the same as this one, capital k, it's zero. Xy is equal to capital k xy, something you have seen before. Okay, x axis. But instead of putting this in front, we wrote it that way.
00:08:47.984 - 00:09:36.984, Speaker A: We wrote it as k. It's here. Xy is equal to k x y minus. We wrote this first. That's the kernel for the subspace of functions which vanish at point x zero. Does this ring a bell? Pij minus pik. Pkj divided by pkk.
00:09:36.984 - 00:10:25.892, Speaker A: We have seen this combination before, but in the finite dimensional case, our x zero is precisely the index k. Instead of x and y, we have I and j. So we obtain this by another method. This is a more direct method to say that the matrix that we obtained before is the kernel of the subspace of all vectors whose case component is equal to zero. X zero is equal to k. The k component is equal to zero in that setting. So that's another interpretation.
00:10:25.892 - 00:11:59.394, Speaker A: And this is a combination that we see quite often when we study the peak interpolation. So we should keep this in mind. Now, the remaining time, I talk a little bit about shor, or sometimes called Hadamard product or multiplication. It's a very natural question, and it's difficult to answer this question when somebody from high school asked you, because my daughter asked me this question, that when you have a and b, why don't you simply define a times b to b a I j b I j component by component. And why you multiply a row by a column, and then for all rows of a and all columns of b. This looks more natural, and indeed it's a good product, which is called Hadamard product or Schur product. And in order not to mix it with the regular one, we use another notation, put a object home b.
00:11:59.394 - 00:12:58.918, Speaker A: And it has many interesting properties, this Hadamato shore product. And contrary to the, to the regular one, it's commutative. A. Oh, b is equal to boa, and it has many, many more properties. It's already about four years, maybe five, and still it goes on. Tom Ransford and myself have worked on polynomial approximation in weighted diversity spaces. And our major tool, at least one of the major tools, but it's the most important one, is the formula for the Hadamard product of functions that we consider over there.
00:12:58.918 - 00:14:09.762, Speaker A: Again, the same phenomenon we have in function space theory. If we have fz one to infinity a n, zn and gz, well, we do f times g in regular product, or sometimes called Cauchy product. The first component is a zero, b zero, but the second is not a one, b one is a zero, b one plus a one, b zero, and so on. And when they go, we do the Cauchy product. And similar to the question above, a natural question is, why don't we define the product this way? If, ah, Ron is not good here, let's put star. Indeed, Star is used for the Hadamard product over there. It's not composition, but f Star G is defined this way.
00:14:09.762 - 00:15:10.204, Speaker A: And as I said, our major tool over there is to study f star G, the other one product, and obtain some estimation, which helped us to overcome the polynomial approximation question. It's just as common to that. This type of multiplication makes sense in other domains, too. And well, if somebody asks you, why don't we multiply matrices like that, or where, or vectors like that, component by component, you immediately can say, well, of course we can, but it's another type of product and it has many interesting properties. And one of them, yes, we have time to see the other still 13 minutes. I already explained about tensor products. Here is the short decomposition.
00:15:10.204 - 00:16:42.074, Speaker A: Pretty much we have seen this good part of it, but there is one thing at the end which needs more explanation. Short decomposition. We saw here as an outcome of cholexi. But there is a little bit more here. That's p b and n times n matrix. If p is positive, I write like instead of writing positive, then there, there exists vectors then vectors in the book is v one up to v one, v two, vn in cn such that p is equal to the sum k from one up to n v k star. This is the linear algebra notation.
00:16:42.074 - 00:17:39.312, Speaker A: If you want to use the operator theory notation, this is the same thing as k from one up to n v k tensor product vk is the same thing. In the first case, we have a matrix, a vector like this multiplied by its transpose. So the result is n by n matrix for each k. So this is vk, this is vk star, and, you know, n by n matrix. Add them all together, you obtain p, but you also, it is possible to interpret p as an operator on Cn. This is this version which is written here. And the action I defined before.
00:17:39.312 - 00:18:46.612, Speaker A: Just to repeat, the action of x tensor y acting on z is equal to inner product of z and y times the vector x. So here, x and y, both of them are equal to vk. So that's the general case. So two versions of this. We saw this before, indeed, we saw this decomposition. What is kind of new is that, moreover, any n by n matrix of the form p equal to the sum. And look here, I don't write up to n.
00:18:46.612 - 00:20:24.456, Speaker A: I write up to m. M could be smaller or bigger than n. It's not necessarily equal to wk, wk star, or if you wish, is positive. And the new part is here. Again, the fact that this is positive is immediate, because every component here is positive, is invertible if and only if the span of these vectors is the whole space. So we saw most of the theorem before. As a consequence of cholexi decomposition theorem, p is equal to ll, and ll can be written in this form with vk.
00:20:24.456 - 00:21:01.164, Speaker A: Playing is the case column. We saw this before. And each of these element here, wk, wk star is by itself positive. So the sum is positive. There's still nothing really new here. The only thing I need to explain a little bit here, why invertibility is equivalent to the fact that span of w one up to wm is the whole space cn. This is based on an interesting formula, is this one.
00:21:01.164 - 00:22:44.562, Speaker A: So I just explained the last part, proof. If z is in cn, what's pz? In scholars at inner product, it said, find a formula for this and the formula either either by this or by this, it's easy to get p of z. What happens in the last 20 minutes of my lectures, something bad goes on. Last time, when I stopped and came back again, it worked well. So I stopped and go back again. Yes, that's good. So the inner product is, for example, if I use, this is equal to the sum, okay, from one up to m wk tensor wk acting on z and scholarly z, this is equal to the sum k from one up to n.
00:22:44.562 - 00:23:49.066, Speaker A: By definition, the first component is by itself z inner product with k z inner product with this one times wk. Then again, in every set, I can take the first one out. Take this out and it's wknz, which is the conjugate of the first one. So k from one up to n, absolute value of z wk squared. If I use this formula, I use this one. If you use. If you want to use the other one, you still obtain the same thing.
00:23:49.066 - 00:24:59.230, Speaker A: But for the inner product of two vectors, the vectors say x and y two vectors. Note that this is equal to y star times x. So y star is like that, x like this, you should, we should use this. And still we obtain the same thing at the end. If, if we work with, with ww. I haven't done the calculation, but we can do a p z based on this, it would be z star times p z, which is z star sum k from one up to n w k w k. Then wdk star z.
00:24:59.230 - 00:25:54.314, Speaker A: Take z inside is zk z, and this one I can write z star wk star. So it's the same thing as z wk. And note that this is a complex number, so it's become zk. And finally, we obtain the same formula as before. This is what I obtained here too. Doesn't matter. Z or z.
00:25:54.314 - 00:27:16.076, Speaker A: Scholar, wk or wk score, z is the same because it's inside the absolute value. So at the end of the day, I have pz is equal to the sum absolute value of z wk ten squared. That's the formula. So another thing to consider is that the null space of pie, which is the point z such that p z is equal to zero, is also the same as the point z such that pz and z equal to zero. This is a good exercise for you. Of course, if p of z is equal to zero, this one is equal to zero. But y from pz equal to zero, we can conclude that p z is equal to zero.
00:27:16.076 - 00:28:35.584, Speaker A: It's a good exercise. I leave it for you to do it. And therefore, now, the green box formula helps us to see what is the null space of p. P z is zero if and only if every one of this is equal to zero. So it's the same as all z such that z one z two zw m equal to zero. Or put it this way, z equal to all the points z is orthogonal to w one z or span of w one up to w m. So that's another formula n of p.
00:28:35.584 - 00:30:41.394, Speaker A: Well, don't need to repeat it, it's there. What is, what is np? Peb in the general case is the range of p star. But p here is how to say, auto aij one self adjoint, itself adjoin. So it's r of p. And if we take the curve of both side, and considering the fact that in the finite dimensional case everything is closed, so r of p is equal to simply span and a vector p, an operator t from cn to cn is one of the basic and important facts in in linear algebra for which we can use this formula, or the formula for n of p. The fact is that p is invertible if and only if p is injective. This is the same as n of p is equal to zero if and only if p is rejective.
00:30:41.394 - 00:32:15.904, Speaker A: This range of p is equal to c. That's a standard but important fact in linear algebra. And therefore, by this green formula, or for the formula for the null space of p, we immediately see that therefore p is invertible, which in our case is equivalent to say that p is strictly bigger than zero, because it's already bigger than or equal, we want to exclude equality. And this is equivalent by the, by the green formula that the span of w one up to wm is equal to cm. That's a kind of nice theorem of shore. And again, another immediate consequence of that is this one. So here is the product.
00:32:15.904 - 00:33:30.248, Speaker A: Let p and q be n by n matrices. If both of them are positive, then the product is positive. If p bigger than or equal to zero q zero, then p rho q. The short product is positive, that's number one and number two. If both of them strictly positive, then the short product is strictly positive, which, if you apply the original definition, it's not that obvious. We call the original definition of p being strictly positive and q being strictly positive. This I will, I will erase.
00:33:30.248 - 00:34:21.024, Speaker A: But I repeat, this means that the sum alpha I bar alpha j p I j is bigger than zero. The other one is for q. And now from this we want to conclude that the sum for any eta of product is again positive. Absolutely non trivial. Absolutely. I invite you to sit and try to use this original definition to see if you arrive at this. And now we see the advantage of working with different representations.
00:34:21.024 - 00:35:34.284, Speaker A: Each representation has its own merit if it gives us some information we saw the short decomposition. Now let see an application for that proof. By previous theorem, we know that p is given by a sum. Say I. Let's use k k from one up to m. Okay, vk vk star, or if you wish, operator to the notation, and the same for q l from one up to another. I mean m prime wk wk star or wl wl star wl tensor wl.
00:35:34.284 - 00:36:45.926, Speaker A: And what would be p composed with q p home q a short product. It is distributed. It will be the sum over k, sum over l of, I mean, add up one of this, this notation. For example, I just write the first one to favor linear algebra. Vk pk. We are almost done if you pay attention to what this mean. We have a vector here.
00:36:45.926 - 00:37:56.504, Speaker A: We multiply by its transpose. Then component by component, we do the same with w. So this is v. This is wild. And then we do the short product component by component. If you write the formula, that's the simple, but the key observation to see that this is precisely equal to if you do vk, which means that you do the short product for v and w. That becomes a vector, then vk and then we are done.
00:37:56.504 - 00:39:31.674, Speaker A: Then we are done, because you have a vector here multiplied by its transpose, it's something positive. Sum of positive things is positive. So this immediately implies first part, because every term is positive. Just a simple observation. If you have the decomposition, then, and let me, let me also write with the second language, the language of operator theory, because sometimes we might use it the same p q is some k from one up to m. Vk, tensor product vk L from one up to m, prime the same thing for w. This is precisely what I wrote before, but with a different notation, it becomes sum over k, sum over l, v k tensor vk Denho wk tensor wk.
00:39:31.674 - 00:40:47.624, Speaker A: And this is as before, it's the same thing as vk, short product wk wl tensor the same thing vk. So we obtained the same thing as before, but it is slightly different language. And x tensor x is something positive, sum of positive thing is positive. So we obtain again number one with a second representation for the second, if both of them are positive, then why the short product is positive? We use this one. P is invertible if and only if the span of the vector is the whole space and our vector. Now we know what they are. All of this k goes from one to m, l goes from one to m prime.
00:40:47.624 - 00:41:41.278, Speaker A: So to answer two, we need to study the span of our vectors. Pk tensor wl. Then k is from one up to m and l is from one up to m prime. What is this span? Let me write it this way to. To be more clear, I have v one tensor w one, v one tensor w two two, and v one tends to vm prime. And the same thing for v two vm prime. And finally the same thing with vm.
00:41:41.278 - 00:43:30.474, Speaker A: These are my elements value m prime and span means that you multiply each of these by a constant and add them up. You can put the constant in front, you can put the constant with the first one, you can put the constant with the second one. And when you add, you immediately see that this is indeed equal. But at least you agree that it includes the when I multiply the first one by a constant and I put the constant with w and the second one by another constant, I put the constant with w two, and also the last one, I put it with wv prime and do the summation. It will be v one tensor span of w one up to wm, and I do the same with the second row, it would be v two tensor the same thing, and vm tensor the same thing. All of these are in my space. Everybody agrees up to here.
00:43:30.474 - 00:44:26.964, Speaker A: And now I use this assumption. Q positive, q strictly positive. A strictly positive means that the span is equal to cn, so it's one tensor cn. You understand what tensor c n means? Means. Means any vector in cn up to vm tensor cm. Anything I want I can create here, because q is positive. Anything in particular I can create either equal to one, one transpose.
00:44:26.964 - 00:45:04.338, Speaker A: I mean, I usually, let's keep our tradition. We usually write a vector like this. I can create a vector, all its element equal to one. So I choose my combination here to create eta the same. Here I create eta the same. Here I create eta. So it will be a Spanish what is v one tensor with eta? If you look carefully, either with.
00:45:04.338 - 00:45:43.024, Speaker A: Indeed, here is the case that I should have write it with this notation. This notation is better, but still it works. It is v one, v two nvm. And now I use the fact that p is strictly positive. This is equal to cm because p is strictly positive. And the conclusion is that poq is strictly positive. That's the end of.
00:45:43.024 - 00:47:04.158, Speaker A: So if both vk and wk span the whole space, either the tensor product or this notation, v k v k star wlw, all of these vectors create the whole space and we are done. Just check if I wrote everything correctly. Yes, everything is fine. Okay, so this shows that if both of them are strictly positive, then the short product is also strictly positive. And that finishes this, this section. Next week, we start to talk about tensor product, the explanation in the book, I mean, even they didn't go into detail. I've prepared a small text to explain what is the tensor product of Hilbert spaces and why we obtain an inner product in the tensor product.
00:47:04.158 - 00:47:49.768, Speaker A: And then we use this to study some spaces like, I mean, Bargain, not Bergman, Bergman space or Drury harvests on space. They are important in application. And indeed, in two weeks, no, in next week, indeed, we talk about non commutative. Yes, in two weeks and the week after, we talk about, the whole session is about three spaces. So it will be a good preparation for the week after. Michael Hartz will talk about such spaces. Okay.
00:47:49.768 - 00:47:55.824, Speaker A: Thank you very much. Thank you. Thank you.
