00:00:00.360 - 00:00:09.350, Speaker A: And it's my great pleasure to introduce our first speaker, Stefan Garcia, who will continue his mini course on model spaces.
00:00:09.502 - 00:00:55.114, Speaker B: Please. So, once again, I'd like to thank the organizers for having me, and I'd like to thank the participants for showing up, at least virtually to this second talk. So, I've been providing the slides and also the book chapter, and I believe that the link to that will be provided at the end of this talk. And for each lecture, I'm going to update the previous ones as various typos or mistakes or omissions turn up. So, even if you've downloaded the talk before or the book chapter, make sure you get the new version at the end of the talk, and the link will be provided in the chat. All right, so let's get started. Our next topic is density results in model spaces.
00:00:55.114 - 00:02:09.776, Speaker B: So the question is going to be, do the model spaces ku, or in other words, uh, two perp, where u is an inner function, do they contain convenient, dense subsets? Because having a dense subset of recognizable and easily handleable functions will allow us to get our grasp on these rather abstract spaces, which are phrased as ortho, complements of more traditionally defined explicit spaces. So we're gonna let this big v denote the closed linear span of the set that follows that notation. And we've got a little proposition here which says that let u be a non constant inner function, let lambda be a sequence of distinct points in the disk. And suppose that the sequence doesn't satisfy the Blaschke condition. The points of lambda don't head toward the boundary sufficiently rapidly. So if you don't satisfy the basket condition, then what happens is that the closed linear span of the reproducing kernels corresponding to the sequence lambda span, or the closed linear span of those kernels, is equal to the entire model space. And so, let me remind you that little case of lambda, that is the reproducing kernel for the model space.
00:02:09.776 - 00:02:49.140, Speaker B: And it takes this form right here. And so the proof is a straightforward one. Let's suppose that f is in the model space and it's orthogonal to the closed linear span of all of those kernels. Well, that's going to happen because, well, okay, you're orthogonal to k lambda if and only if you vanish at the point lambda. So you're orthogonal to this closed linear span if and only if you vanish on the entire sequence capital lambda. Capital lambda doesn't satisfy the Blaschke condition. And so lambda, let's recall, it's a zero sequence of a non constant h two function, if and only if you satisfy the blaschket condition.
00:02:49.140 - 00:03:38.962, Speaker B: Since lambda doesn't satisfy the Blaschka condition, if you vanish on capital lambda, you've got to be the zero function. So in other words, the only function in the model space orthogonal to this closed linear span is the zero function. So this closed linear span of those selected kernels is the entire model space. And this is an elementary result, but it's actually quite useful. So it tells us, for example, that the linear manifold not necessarily subspace, because subspaces need to be closed, but the linear manifold ku intersect h infinity, is dense in the model space. And the reason being? Well, for each fixed lambda, the reproducing kernel, k, sub lambda, is an h infinity function. And so we've seen that the span of selected kernels is dense in the model space.
00:03:38.962 - 00:04:27.096, Speaker B: So at least what we know is that the intersection of the model space with h infinity is dense in the model space. But we want to do a lot better than that. We want to have some sort of smoothness of, you know, we want to be a little bit more specific. So we're going to let script a denote the disk algebra. So that is going to be h infinity, intersect the algebra of functions that are continuous on the closed unit disk, but an analytic on the interior. So these are going to be the h infinity functions that admit continuous extensions to the closed unit disk. And there's a very deep and important result of Alexandrov, which says something very surprising, says that if u is any inner function, then the intersection of the model space ku with the disk algebra is dense.
00:04:27.096 - 00:05:22.734, Speaker B: And this is not obvious. Say if u is a singular inner function, or if u has a singular inner factor, it's really hard to identify a concrete function in the model space that is continuous up to the boundary. So this is a pretty non trivial result, and says that at least if we want continuity up to the boundary, we can get a dense set in the model space that is continuous up to the boundary. You can push this much further. The notes which will be provided to you will talk quite a bit more about this, but you can talk about criteria which guarantee additional smoothness, say, continuity of the first derivative, second derivative, etcetera. Now, it turns out to be a lot more complicated story, and those results tend to involve the behavior of the singular measure coming from the singular inner factor and how they behave on so called Berlin Carlson sets. But it's a little bit technical, so I'm going to push that to the notes or the book chapter, which you'll see in the chat later on.
00:05:22.734 - 00:05:52.704, Speaker B: So I want to move on to bases for model spaces. So this is sort of going in the same direction. We want to get a foothold on model spaces. We can do it by trying to find convenient dense subsets. We can also try to do it since we're living in Hilbert space. You know, the Hilbert spaces we can talk about are there convenient orthonormal bases, and that's what we're going to do now. So we're going to let U be a Blasca product here with zero's lambda n.
00:05:52.704 - 00:06:49.002, Speaker B: And Blaschka products are the easiest sort of inner functions to handle when you're dealing with model spaces, singular inner functions, a little bit trickier. So we'll focus on Blaschkit products for now. For a point w in the open unit disk, we're going to consider this disk automorphism, right? It's a Mobius transformation, maps d to d, and it maps the circle onto the circle. So we can use these disk automorphisms combined with the reproducing kernels corresponding to the zeros for the Blaschke product to construct a pretty convenient basis. So it's been discovered slash rediscovered by various people. We've been calling it the Takanaka malquist Walsh basis, sometimes goes by molquist Walsh basis, but it appears that Takanaka probably did it a little bit earlier. I won't go too much into the history of this, but the idea is essentially this.
00:06:49.002 - 00:07:15.374, Speaker B: Start with the reproducing kernel at the zero lambda one and normalize it, because we want an orthonormal basis. That's where the square root expression comes from. It's normalizing. So we've got a unit vector here. Now let us take a look at the kernels corresponding to the other zeros of my blaschki product, U. So that's why I have lambda K's here. Now, you might say these don't look like the reproducing kernels for the model space.
00:07:15.374 - 00:07:50.654, Speaker B: These look like the Koshisago kernels for h two. And in fact, they kind of do, because remember, for the model space you have a one minus phi of lambda 1 bar times, sorry, u of lambda 1 bar times u. But U vanishes at lambda one. So that's why the numerator is simpler. So this is why working with blaschker products is a little bit better, because if you take a look at the kernels corresponding to the zeros, the numerator basically disappears. And you essentially have one of the koshis eggo kernels. Now, of course, we have the square root expression that comes from normalization.
00:07:50.654 - 00:08:47.722, Speaker B: And then we have this finite blaschka product that sits out in front. Now, this finite Lashka product that sits out in front doesn't really harm us in the sense that it doesn't change the norm of my vectors. So all of these fks are unit vectors, and it turns out they form an orthonormal basis for the model space. And in the case where we have just simple zeros, essentially what we're doing is we're performing the Gram Schmidt process, the orthonormalization procedure that we teach our linear algebra students to the reproducing kernels, k, sub lambda n. So if you orthogonalize those kernels, you essentially get the Takanaka molquist Walsh basis. So you might want to think about why these turn out to be orthonormal. So they're going to turn out to be orthonormal, because when you take the inner product of two of these fks against each other, well, you'll have some finite Lashkate products in there.
00:08:47.722 - 00:09:34.110, Speaker B: And what you're going to do is basically cancel as much as you can, because finite Blaschky product is unimodular. And it's going to basically turn out that you're going to be able to eliminate most of these inside of an inner product, and then you're going to have a raw reproducing kernel on one side or the other of that resulting inner product. And when you use that reproducing kernel to evaluate the function in the other slot, well, it's going to have some, some of these disk automorphisms there that are chosen to vanish at exactly the right places. So that's why those inner products will end up being zero. Now, what I want to show you is a similar computation which also show you what happens in spirit. We're going to take a look at the compressed shift operator. So that is the operator where you multiply by z and project back into the model space.
00:09:34.110 - 00:10:00.174, Speaker B: Pu. Please recall, is the projection onto the model space Ku. So let's compute the IJ entry of the matrix representation of the compressed shift with respect to the Takanaka Malbquist Walsh basis. So we do it as follows. And I'm going to do this for I less than or equal to j because there's sort of two separate cases. But I want to highlight the global structure of the matrix. And this is going to be good enough.
00:10:00.174 - 00:10:31.754, Speaker B: So, to compute this matrix representation, well, we want to compute the compressed shift applied to Fj inner product with Fi. So this is the compressed shift applied to Fj. The projection on the model space is self adjoint, so I can move it to the other side. In the inner product, Fi is in the model space. The projection of Fi is just fi. So that's what I've done here. Now, I use the definition of the Takanaka Mulquist waltz basis here.
00:10:31.754 - 00:11:01.400, Speaker B: And so we get this gory mess. Now, those square roots you don't really have to worry about, because those are just normalization constants they pull out. So what we have here is this. Let's recall that c sub lambda, that is the Koshizego kernel for lambda. That's the usual reproducing kernel for the hardy space, not the model space. Because of course, what we have here is we don't have the inner function u appearing at the top. We just have one over one minus lambda bar jz.
00:11:01.400 - 00:11:33.454, Speaker B: So that's the regular Cauchy kernel for lambda j. Right? Here we have the regular Cauchy kernel for lambda I. And then we have a bunch of these disk automorphisms multiplied together. So we're going to use the hypothesis that I is less than or equal to j. In other words, this term over here has fewer factors than this finite blaschke product. They're the same factors, except this one has a little bit more. So what we can do, you compute this using the usual l two inner product.
00:11:33.454 - 00:12:23.634, Speaker B: Use the fact that these b's are unimodular, that b times b bar is equal to one, and you can cancel out these guys in the second factor. And now you have the product of these, the elf disk automorphism where l goes from I to j minus one. So this is where we're using the hypothesis that I is less than or equal to j. And again, we get this raw Cauchy sega kernel alone, by itself, on the right hand side of the inner product. And what that means is that this Cauchy kernel is going to evaluate what's in the other slot at the point lambda I. And when we do that, we get this, we evaluate what's going on here at lambda I, which is why we get a lambda I. There you evaluate this Cauchy kernel at lambda I, which is where you get this part from.
00:12:23.634 - 00:13:14.358, Speaker B: And then you evaluate these blaschke factors at lambda I, and you get this. So what this ends up being when all is said and done is, well, it's zero if j is greater than I. I guess maybe I made a little mistake here, but it's basically going to be a lower triangular matrix with the lambdas on the diagonal. So that recovers a special case of the Lipschitz Muller theorem from before, which says that the point spectrum of the compressed shift will be the zeros of the blaschka product or the Blaschka factor of the inner function. That's what we're saying here, at least in this particular case. What do we have? We've got a lower triangular matrix with lambdas on the diagonal. So the eigenvalues of the compression are the zeros repeated according to their multiplicity.
00:13:14.358 - 00:14:11.630, Speaker B: So this is a fairly typical type of calculation with the TMW basis. Now, another direction you could choose to go in would be to relax the notion of basis instead of insisting on orthonormal bases. What about something that's kind of like an orthonormal basis or a Reese basis? In other words? So a linearly independent sequence xn in an abstract Hilbert space, h is a respasis if it satisfies these criteria. Well, the closed linear span is better be the whole space. So that's what this is saying. And moreover, you act sort of like a orthonormal basis in an approximate parsival sense in the following terms for any finite sequence of coefficients. Here, when you compute the norm in the Hilbert space script h of this finite linear combination, you're bounded above and below by these ParSEval like expressions.
00:14:11.630 - 00:15:10.188, Speaker B: You've got the sum of the squares of the absolute values of the coefficients, but you don't have exact equality. You're bounded above and below by a constant times those square sums. And so it turns out there are several equivalent conditions to being a respaces. You could be a re space basis because you're the image of an orthonormal basis under a bounded, invertible linear operator. You might say, well, how does that happen? Well, the m one and the m two would be coming from the norm of the operator and the norm of its inverse. And I believe that there's actually a series of lectures later on on interpolation and sampling and all sorts of things, and I'm sure you'll hear lots about respaces in those lectures. Another equivalent condition being a respasis is that when you take a look at the gram matrix, the infinite matrix of inner products, the result you get is a bounded and invertible operator on the sequence space, little l two.
00:15:10.188 - 00:15:53.580, Speaker B: So those are equivalent ways of checking whether something is a re spaces or not. Usually the gram matrix criteria is sort of the easiest one to apply, although it's often still very technical to deal with. So we've relaxed the notion of bases to re spaces. Now, we don't care about orthonormality, and we need to figure out when do we get nice respaces of reproducing kernels in the model spaces. Well, okay, we're going to need this CArlson condition. It basically says, I got a sequence of distinct points, little lambda n, that's going to be uniformly separated. If this strange in femum is greater than zero, it's, it's kind of a technical looking thing.
00:15:53.580 - 00:16:40.084, Speaker B: But you can kind of see you got a Blaschka product sort of here where you've plugged in lambda I, but you've taken absolute values, but you want to take the infemum over all of these sort of things, over all I, and you want that to be greater than zero. It's a pretty strong condition. That's called the Carlson condition. What it's saying in sort of more, in a more intuitive and less technical sense, is that the sequence lambda is very, very sparse and spread out with respect to the hyperbolic metric on the. I suppose you could say the same thing about the pseudo hyperbolic metric on D. So, in other words, this is some sparsity condition, not in the usual geometry of d, but the geometry of d imposed by the hyperbolic metric. An important theorem, which I think is due to Carlos, and I'm sure somebody will correct me in the chat if I'm wrong.
00:16:40.084 - 00:17:20.608, Speaker B: If you, as a Blaschke product with simple zeros, lambda one, lambda two, et cetera, then the best of all possible things happen. You take a look at the reproducing kernels for these, you know, corresponding to the zeros. Well, you normalize them, of course. So you normalize them, and you get the square root in the top. But since these were zeros of the Blaschke product, you don't have the Blaschke product appearing in the numerator of these reproducing kernels. You just have a multiple of the standard Koshizego kernel. So what's going to happen? Those comprise a respaces for the model space if and only if the sequence of zeros is uniformly separated.
00:17:20.608 - 00:18:13.244, Speaker B: It's a pretty technical theorem. It's a difficult proof, and it's a nice result, at least for Blaschke products. We don't get automatic orthonormal bases of reproducing kernels, but we get something pretty close, as long as you satisfy the carlos of condition. So one way to create a uniformly separated sequence is to insist that the zeros tend to the boundary really, really fast. So here's one way in which you could accomplish this. So if there's a constant c that's strictly less than one, so that you have this exponentially fast convergence to the boundary, then you automatically get a uniformly separated sequence. And in fact, if your zeros are real and tending to one along the real axis, then this exponential separation condition is actually equivalent to uniform separation.
00:18:13.244 - 00:18:56.314, Speaker B: So there's a lot more that can be said here, but I'll stop on the re spaces and Carlson condition at this point because I want to pivot to, like, a weird little bit of trivia. So remember that the model spaces are backward shift invariant. S star is the backward shift. So, turns out the backward shifts of the inner function u belong to the model space. Ku now, these backward shifts of the inner function u don't actually have to be linearly independent at all. And in fact, they cannot be if u is a finite Blaschka product, because the model space corresponding to a finite Blaschke product is finite dimensional. And I've got infinitely many backward shifts.
00:18:56.314 - 00:19:22.622, Speaker B: But nevertheless, these backward shifts of the inner function u can actually act like an orthonormal basis. And so there's an interesting little proposition here. If u is inner and f is in the model space, well, the backward shifts of f are there as well. But what you have is this sort of strange thing. I believe I messed this up. I'm going to have to type this up. I'm going to have to redo this a little bit.
00:19:22.622 - 00:20:04.564, Speaker B: This should be s n of u, I believe. But the backward shifts of, uh, have this parseval like property. They form a tight, what's called a tight frame for the model space. And you have these things that look like orthonormal expansions with respect to the backward shifts of u, but they're not even linearly independent necessarily. And they're certainly not unit vectors, and they're not necessarily orthonormal in any sense. So that's yet another relaxation of the notion of bases. And this is related to frames, which you'll hear a lot about in some of the later lectures in this semester long program or I guess, year long program.
00:20:04.564 - 00:20:49.864, Speaker B: So I want to move on to continuability properties for functions in model spaces. So, a nice feature of model spaces is they have really good boundary behavior. Their elements have great boundary behavior. So, a function in h two we know has non tangential limits almost everywhere in the circle. It doesn't actually have to be analytically continuable across any arc of the circle. However, you can use gap series arguments, all sorts of techniques, very classical, to show that typical h two function doesn't have to be analytically continuable anywhere at all. But functions and model spaces are typically a lot more well behaved.
00:20:49.864 - 00:21:37.718, Speaker B: So, let's start with our inner function. Uh, equals b times s, where b is a blaschke product, zero sequence capital lambda s is a singular inner function with singular measure mu. Recall from the last lecture that the so called spectrum of u is the set well given by this sigma u is the closure of the zero set union, the support of the singular measure mu. So, recall that this came up back in the Lipschitz Muller theorem in the previous lecture. This is the spectrum of the compressed shift on the model space script ku. So that is why we call this the spectrum of the inner function. We'll let c hat denote the extended complex planes.
00:21:37.718 - 00:22:15.558, Speaker B: We're going to put infinity in there. So we're using the Riemann sphere model. We're going to let d sub e, be the exterior of the unit disk or the open exterior of the unit disk, including the point at infinity. So that's going to be the complement of the closed disk in c hat. So, one can show that the inner function u is analytically continuable to. Well, where, okay, the extended exterior disk, except one over z bar, where z is an element of sigma u. Now, you might say, well, why is that? Well, think about a Blaschke product.
00:22:15.558 - 00:23:12.744, Speaker B: Think about a Blaschke factor. Blaschka factors are totally nice functions, except when you look at that denominator, there's a pole in the exterior of the disc for each blaschka factor. Now, singular inner function is a little bit trickier to look at, but it turns out that what seems to be true is that if you stay away from the bad points that are dictated by your, your blaschka sequence, or the singular measure, the support of the singular measure mu, if you stay away from that, things will be great. Your inner function will be analytically continuable to the complement of that set of bad points. Now, it turns out that propagates to functions in the model spaces. So, functions and model spaces are analytically continuable to basically the same region that the inner function u was analytically continuable to. So that's much better than typical h two function, which isn't continuable anywhere.
00:23:12.744 - 00:24:06.848, Speaker B: Now, I want to tell you a little bit about sort of what happens on the boundary. We're going to need this notion of an Adc, an angular derivative in the sense of caratadori. So you have a point zeta on the boundary circle, and we're going to say that the inner function u has an angular derivative, in the sense of caratadori, of course, abbreviated adc. If the non tangential limits of u and mu prime exist at zeta, so the derivative also has to behave kind of well at zeta, and you have to have a unimodular value at one, not. So u of zeta has to have absolute value one. I'm not talking about the derivative, of course, here, the derivative just has to exist at that point in a non tangential limiting sense. So, there's a really nice theorem of ahearn and Clark which says this.
00:24:06.848 - 00:24:32.702, Speaker B: Now factor your inner function in the usual way, same notation as we've seen before. We're going to let zeta be a point on the unit circle. The following are equivalent. Well, every function has a non tan. Every function in the model space ku has a non tangential limit at zeta. So we know non tangential limits will exist almost everywhere. But almost everywhere isn't specific enough.
00:24:32.702 - 00:25:21.934, Speaker B: I want to know what happens at specific points. So, one of these equivalent conditions is that every function in the model space has a non tangential limit at that specific point zeta, equivalent to. For every function in the model space, f of lambda, remains bounded. As lambda approaches zeta non tangentially, that's equivalent to the inner function u, which defines the model space having an adc at zeta. So that's why these angular derivatives are important here. We can also phrase that in terms of reproducing kernels. So, the non tangential limit of u at zeta exists, and the reproducing kernel for the model space for the boundary point zeta lives in h two.
00:25:21.934 - 00:26:00.508, Speaker B: Now, since zeta is on the circle, there's some bad things that could happen down here. I could get a, you know, that doesn't look too good. One over one minus z, for example, does not belong to the Hardy space. So I could be having some trouble here. So, what this is saying is that, well, if your inner function u is nice enough that there's some sort of cancellation here, so that the resulting function actually lives in h two, this boundary kernel lives in h two, well, then you get these other results, right. So this containment in h two is not obvious or immediate. That's why it's one of these equivalent conditions in the Ahearn Clark theorem.
00:26:00.508 - 00:26:35.514, Speaker B: And this is equivalent to a Frostman like condition. You might have seen things similar, but with different exponents in various theorems. I think going back to Frostman, and probably, probably before. But, you know, with the exponent two here, what we have is a nice condition that is perhaps checkable if you have a reasonable enough, explicit enough Blaschky product and singular inner function here. If you know the singular measure, you know the Blaschka sequence, you could possibly check this numerically. Now, the functions that arose here in four are very important. Those are boundary kernels.
00:26:35.514 - 00:27:28.614, Speaker B: Those are reproducing kernels for the boundary points. And under the equivalent conditions of the Ahern Clark theorem, well, that boundary kernel actually belongs to the model space, and it actually does what you hope it might do. It reproduces the values of a model space function at a boundary point zeta on the unit circle. So they act like reproducing kernels for boundary points. And that is a really, really cool thing that doesn't happen in the Hardy space, because, of course, there are no reproducing kernels for boundary points for the Hardy space, because a typical hardy space function doesn't have to have a nice non tangential limit at a point on the circle that you picked, right? We all, you only got almost everywhere non tangential limits for hardy space functions. And that almost everywhere depends on which function you pick. So this is unusual and nice behavior.
00:27:28.614 - 00:27:53.470, Speaker B: Now, I'd like to move on to a new topic, which is pseudo continuation, which is a relaxation of the notion of analytic continuation. It turns out to be very important for model spaces. So we're going to let f and f tilde be meriamorphic functions. F is going to be meriamorphic on d. F tilde is gonna be meromorphic on the exterior of the closed unit disk. So it's gonna be on the outside. There's.
00:27:53.470 - 00:28:26.814, Speaker B: They live in opposite realms. So we're gonna say that f and f tilde are pseudo continuations of each other. If the non tangential, if the non tangential limits of f from, from inside d agree with the non tangential limits of f tilde outside almost everywhere on the circle. So you have a function on the inside and you have a function on the outside. Their values on the unit circle have to agree almost everywhere. If that occurs, then f and f tilde are pseudo continuations of each other. So here's some examples.
00:28:26.814 - 00:29:12.534, Speaker B: A rational function on d whose poles are in the exterior automatically pseudo continuable because it's actually analytically continuable to the complement of its pole. So there's no problem with the limits from the outside and inside tending to the same thing on the unit circle. Now something that isn't an honest analytic continuation might be something like this. Inner functions are pseudo continuable to the exterior of the disk via Schwarz reflection through the circle. Turns out the because inner functions are unimodular almost everywhere u times u bar is equal to one almost everywhere on the circle. You can check that. This funny thing, which is the Schwarz reflection through the circle, that that is a pseudo continuation to the exterior.
00:29:12.534 - 00:30:08.450, Speaker B: And you might say, well, didn't I just tell you about analytic continuation properties of u? Well, I did, but you could have a really pathological u. For example, these zeros might accumulate everywhere on the circle. The singular measure might have support everywhere on the circle. There could be some really bad things. So the bad set for the inner function might be the whole unit circle, in which case you don't have analytic continuation across any arc, but you do have a pseudo continuation that isn't arising from an analytic continuation across that arc, because that bad set for the inner function could be everything. Here's another example. The exponential function is not pseudo continuable because while it is analytically continuable to the exterior, the continuation is not meriamorphic and we have to have metamorphic continuations.
00:30:08.450 - 00:30:43.572, Speaker B: By this definition, you have an essential singularity at infinity. That means you're not pseudo continuable. It turns out analytic continuations and pseudo continuations are the same thing if you actually have an analytic continuation. So there's a technical detail here, but it says that if you're analytically continuable, then you're pseudo continuable to the exterior of the disk. But we've seen that there are examples where you're pseudo continuable to the exterior but not analytically continuable. So analytic continuation is stronger. Of course, here's an example.
00:30:43.572 - 00:31:17.914, Speaker B: Log of one minus z is not pseudo continuable to the exterior. Because you take a look at the, .1 you've got a winding singularity around one. And if you were pseudo continuable to the entire exterior disk, well, regardless of how you go around the, .1 whether you go from the left or from the right has to lead to the same thing but we know we actually have a winding singularity there. And the Riemann surface has infinitely many sheets for log of one minus one, log of one minus z. So the analytic continuation, the pseudo continuation, don't agree there.
00:31:17.914 - 00:31:57.834, Speaker B: And so you can't actually have a pseudo continuation that works for the entire exterior disk. So I'm setting up here a really important and powerful theorem of Douglas, Shapiro and Shields, which you see here at the bottom. For this, we're going to need one more definition, the Hardy space of the extended exterior disc. So h, two of the exterior here, it's basically f of one over z, where f was in the Hardy space. So it's pretty straightforward definition. You just plug one over z into a hardy space function. So the key result of Douglas, Shapiro and Shields is this membership in the model space ku is governed by pseudo continuation.
00:31:57.834 - 00:32:27.894, Speaker B: So an h two function is in the model space ku, if and only if f u has a pseudo continuation to the exterior of the disk that vanishes at infinity. So it's an if and only if. So it's a nice characterization. You can often say that can't be in the model space because that doesn't have such and such pseudo continuation. So, for example, log of one minus z can't be in any of the model spaces. So it's a nice criteria. It's a really beautiful result.
00:32:27.894 - 00:32:43.478, Speaker B: Here's the proof. I'm afraid for time I'll have to skip it. But you'll be able to see it in the slides if you download them at the end. The proof is quite nice. It's pretty slick. You can see there's nothing deep here. We're just screwing around on the boundary with boundary functions.
00:32:43.478 - 00:33:26.874, Speaker B: But again, for the sake of time, I'm going to skip the proof of this because we'll see some similar boundary manipulations later on. I want to point out a couple other results in this same direction. So, what about non cyclic vectors for the backward shift? Why the backward shift? Well, model spaces are backward shift invariant, so that's why we're interested in the backward shift here. So, function is non cyclic for s star. If, when you take a look at all the backward shifts of your function, take their closed linear span, that's what this big v here is. You don't get everything. So in other words, f is non cyclical if and only if it belongs to some proper invariant subspace for s.
00:33:26.874 - 00:34:14.918, Speaker B: In other words, you're non cyclic if and only if you belong to a model space. So you can take the Douglas Shapiro shield result, examine it a little bit further. And you can say, instead of belonging to some specific model space, which is what Douglas, Shapiro and Shields is concerned about, you say, do I belong to any model space? Well, that pseudo continuation criteria boils down to this. F is noncyclic for s star if and only if it's pseudo continuable of bounded type or pCBT. So by this we mean that f is pseudo continuable to the exterior of the disk, and the result is a quotient of bounded analytic functions on the exterior. Right. So bounded type you may have heard before.
00:34:14.918 - 00:34:51.954, Speaker B: So this is a really interesting result because it says you're in a model space if and only if. You let me rephrase this, you are in some model space if and only if you're analytic. If and only if you are pseudo continuable to the exterior in this very specific way, your pseudo continuation is of bounded type. So you can prove a couple of nice things like this that are not obvious at all via other means. The sum of noncyclic functions is noncyclic. The product of non cyclic functions is non cyclic as long as the product belongs to. Suppose that's another question.
00:34:51.954 - 00:35:25.904, Speaker B: Okay, conjugation on model spaces. This is where we're going to play around on the boundary. We're going to step back a little bit and we're going to look at abstract notion on Hilbert spaces. Again, a lot of the stuff that you see in this lecture will apply to HP as well. There are these script pu spaces where almost all of this stuff works out. I'm focusing on the Hilbert space setting because I think things are a little bit easier and more transparent there. So we're going to start with conjugations on Hilbert spaces.
00:35:25.904 - 00:36:25.596, Speaker B: So a conjugation on a complex Hilbert space script H. Well, is a conjugate linear operator. So it's linear, except when you pull a constant out, you have to put a bar on top of the constant. So conjugate linear involutive, meaning it squares the identity and isometric norm preserving. So an argument with the polarization identity and the conjugate linearity of a conjugation shows that being isometric, being norm preserving, is the same as being inner product preserving in this sense. Now, this isn't saying that c is an isometry in the normal sense, because the role of x and y here is reversed, right? Because C is not a linear operator, it is a conjugate linear operator. It's real linear, but as it's not complex linear, so the isometric property boils down to cx cy being not x comma Y, but y x.
00:36:25.596 - 00:37:42.814, Speaker B: So that's a peculiarity that comes from dealing with something that is a conjugate linear operator. So, Standard Zorn's lemma argument says that if you have a conjugation on a Hilbert space, you can find a nice orthonormal basis, that is c real, meaning you got an orthonormal basis, and each element of that basis is fixed by the conjugation. And that's a really cool thing, because that says that if I sort of try and think about the conjugation C and its action with respect to this c real orthonormal basis, I'm really just looking at conjugating the coefficient sequences. So with respect to this type of very, very special basis, C is essentially the standard conjugation on little l two. So any two conjugations on the same Hilbert space are unitarily equivalent. All conjugations on the same space are fundamentally the same, which makes the next theorem, which is a curious fact going back to the 1960s, an observation of Goethe and Lucenko really remarkable, because it turns out that every unitary operator is the product of two conjugations. Conjugations are conjugate linear, but when you put two of them together, you get something that's linear.
00:37:42.814 - 00:38:14.802, Speaker B: And what you get is essentially any unitary operator that you would like. So you might want to think about how to prove this. First thing is, you hit it with a spectral theorem. And, you know, it's a nice exercise, but it's a really cool result. Conjugations are basically all the same, but if you paste them together in different ways, you can get any unitary operator. Pretty nice result. What I want to look at is a specific conjugation one on the model space ku, and it's going to be defined in terms of boundary functions as follows.
00:38:14.802 - 00:39:03.582, Speaker B: It's going to map f to fz bar times u. You might say that looks illegal because we're going out of the hardy space. But remember, for model spaces, you can play around on the boundary, you can view model spaces as living in capital l two. And it just so happens that if f is in the model space, this thing, even though it doesn't look like the boundary values of an analytic function, actually happens to be the boundary values of an analytic function on d, it's a strange thing. And so, well, outer functions are determined by their modulus on the unit circle. So if I put absolute value bars here and take a look at what's going on? On the unit circle, f and cf have the same modulus on the unit circle. Therefore they share the same outer factor.
00:39:03.582 - 00:39:55.250, Speaker B: So what I'm claiming is that there's this nice map conjugate linear, but a nice map from the model space to itself that is preserving outer factors of functions. Now, what we have to show is that this actually maps the model space to itself. So, we're going to recall a proposition from the first lecture, which is that the model space can be represented in terms of boundary functions, as follows. It's h two intersect u times zh two bar. And this is again operating in l two, thinking only in terms of boundary functions. So, since u has modulus one almost everywhere, when you take a look at the definition here, we immediately see that c preserves outer factors. Assuming that this thing is the boundary values of some analytic function, it's also conjugate linear by definition.
00:39:55.250 - 00:40:20.824, Speaker B: It's isometric and involutive. You can check that those axioms hold just from this definition and the fact that u is unimodular. So at first, we only know that cf belongs to capital l two. It doesn't look like it belongs to h two, but it does. So let's do some checking. F is orthogonal to, uh, two, because f belongs to the model space. Ku.
00:40:20.824 - 00:41:03.802, Speaker B: So we check what happens with cf inner product, zh bar. Well, cf is fz bar. Uh, and the z bars cancel out. I can move the h to the left, I can move the f to the other side, and I get the inner product of u h with f, which is zero, and that holds for every little h in the Hardy space. So what is that telling me? It's telling me that cf is orthogonal to everything of this form, zh bar. So in other words, it's orthogonal to everything that only has negatively indexed form Fourier coefficients. So in other words, that tells me that cf, when I think of it as an l two function, only has non negative Fourier terms.
00:41:03.802 - 00:41:38.694, Speaker B: In other words, cf actually does belong to h two. It's a pretty remarkable thing, playing on the boundary, working within the ambient space, capital l, two nice things happen. Similarly, let's take a look at cf inner product. Uh, I write the definition of cf here, and I have u and u. Those can be canceled because u u bar is equal to one. And so I get this. Well, h is a hardy space function, so that's got to be orthogonal to every anti analytic function that vanishes at the origin, right? Same sort of argument.
00:41:38.694 - 00:42:04.674, Speaker B: And that means that while cf is orthogonal to everything of the form. Uh, so, in other words, cf belongs to the ortho complement of u capital h two. Or in other words, cf belongs to the model space. So, long story short, this funny looking thing actually does give you a conjugation on the model space. It mops the model space to itself. Remarkable. So let's do a few examples here.
00:42:04.674 - 00:42:39.950, Speaker B: You can work out on pencil and paper that if you've got a finite Bashkid product with zeros lambda one up to lambda n, you go through, on pencil and paper, work through what this conjugation is in terms of boundary functions. You get this result. The conjugation maps this general function in the model space. Ku. To this, you basically have taken the same polynomial enumerator, reversed the order, and conjugated the coefficients. So that is the conjugation on a finite dimensional model space. Now, we might want to see what does a conjugation do to reproducing kernels.
00:42:39.950 - 00:43:10.696, Speaker B: And this is a little computation. If I want to conjugate the reproducing kernel k lambda, well, I take the conjugate of k lambda times z bar times u. So let's distribute this conjugation there. So I distribute the conjugation. I get this expression, and remember, we're working on the unit circle. So z bar is okay, I can work on the unit circle. Z times z bar is equal to one.
00:43:10.696 - 00:43:41.904, Speaker B: When I distribute these things, u times u bar is equal to one on the unit circle. So when I distribute things, I get a difference quotient. And this is actually now the boundary function of an analytic function. I can actually talk about this as an analytic function. That is what the conjugation maps a reproducing kernel to. It maps a reproducing kernel for lambda to the difference quotient for lambda. This is actually analytic, because the zero in the denominator cancels with the zero in the numerator at lambda.
00:43:41.904 - 00:44:18.906, Speaker B: So the conjugation supposedly preserves outer factors. And in fact, that's true. If you take a look at the difference quotient and play around with it, do a bunch of algebra, that turns out to have the reproducing kernel k lambda as its outer factor. Now, the inner factor is this strange looking thing. It's a Blaschke product, or, sorry, it's a Blaschke factor corresponding to the use of lambda with u plugged into it divided by some other Blaschke factor. But this cancels out and works out to be some inner function. Really nice result.
00:44:18.906 - 00:45:04.916, Speaker B: Using frostman like theorems for almost every lambda, this will pretty much be a Blaschke product. So it's a kind of cool thing. So the reproducing kernel gets mapped to a difference quotient, but the difference quotient also has the reproducing kernel as its outer factor, and we know what the corresponding inner factor is. It's pretty cool. And this difference quotient is a reproducing kernel of some sort. It's the reproducing kernel for conjugate functions. In this sense, if I want to evaluate the conjugate function of f, not the regular complex conjugate, but the model space conjugate of f at lambda, it works out to be f in the second slot with the inner product against c k lambda.
00:45:04.916 - 00:45:32.074, Speaker B: That's the difference quotient in the first lot. So the difference quotient reproduces the values of the conjugate function. So it's kind of a cool thing. Now let's see what happens on the boundary. Some really cool stuff happens on the boundary. Let's go back and say, let's say that u has an angular derivative in the sense of carata dory at zeta. Then, as we saw in the Ahearn Clark theorem, the boundary kernel belongs to h two.
00:45:32.074 - 00:46:27.254, Speaker B: It turns out it's very compatible with the conjugation. The boundary kernel is actually almost self conjugate, because when you compute the conjugate function of the boundary kernel, it's this difference quotient. And because we're on the boundary, zeta is on the circle, u of zeta is on the circle, at least almost for almost every zeta. What happens here is I've just got a unimodular constant here that's a constant times the boundary kernel. So the conjugate of a boundary kernel is essentially the same boundary kernel times a constant. So if you do a suitable scalar multiple of the boundary kernel, you actually get a self conjugate function, which is a nice thing. We'll see later in the third lecture that this can be leveraged, along with the Alexandrov Clark theory to actually give us, in many instances, orthonormal bases of model spaces consisting of boundary kernels.
00:46:27.254 - 00:47:07.776, Speaker B: Moreover, those orthonormal bases can be adjusted so they're actually compatible with the conjugation so that you have a see real orthonormal basis. Here's an example you can work out by hand. I'm not saying it's trivial, but you can work out the details, because we're going to deal with a finite Blaschka product here. Let's suppose I've got a finite blaschka product, n zeros, and to simplify things, I'll let one of the zeros be at the origin, fix a point on the unit circle. Since I'm dealing with finite blascular products, I'm dealing with rational functions and polynomials. So one can show pretty quickly that the equation u of zeta equals alpha. Alpha is fixed here.
00:47:07.776 - 00:47:40.198, Speaker B: That has n distinct solutions on the unit circle, zeta one up to Zeta N. So the boundary kernels are almost self conjugate. As I said before, what turns out is a little bit nicer if I take the boundary kernels. And why is this alpha bar here? Well, remember, that's U of Zeta J. This is a boundary kernel here. This portion times some junk here. And this junk here is a normalization constant.
00:47:40.198 - 00:48:10.144, Speaker B: It's just a constant, right? And it works perfectly well because I'm dealing with a finite bioschki product. So there's no question about u working out here. And it turns out that these n vectors, they're basically normalized boundary kernels of a level set of the inner function. The kernels are coming from a level set. They turn out to be a c real orthonormal basis. They're an orthonormal basis for the model space. And each of these functions is fixed by the conjugation on the model space.
00:48:10.144 - 00:48:35.380, Speaker B: So it's a really, really, really nice result. Now, I have only a little bit of time left, so I'm afraid I might not be able to get to all of this. Let's see what I can get to. Let me. Well, let's see. I think I might have to punt on this because I have only a minute and 20 minutes left. Let me give you a quick idea of what's going on here.
00:48:35.380 - 00:49:14.894, Speaker B: If you play with boundary values enough, you realize that this conjugation links these pairs of functions in model spaces. A function and its conjugate has the same outer factor, right? Has the same outer factor. So the conjugation just jumbles up inner factors of functions. It keeps outer factors the same. And if you play with this a bit more, you can get to the following result, which I'll just state down here. I'm sorry that I have to rush through it, but I think I can at least give you the sort of main result that I was heading for. Remember, you can strip off outer factors of functions in a model space.
00:49:14.894 - 00:50:10.962, Speaker B: So let f capital f be the outer factor of some function in the model space. You can actually characterize everybody that lives in that model space that has outer factor capital f. It turns out there is this thing called the associated inner function of capital f. There's this inner function script I. It depends on you, and it depends on capital f, but you can identify what it is, and it turns out the set of all functions in the model space with your particular outer factor capital f just looks like this, some inner function that divides this magic inner function script I times f. So what you really get is a nice partial ordering on the set of all functions in that model space with the outer factor capital f. And the conjugation basically gives you a order reversing bijection from this set to itself.
00:50:10.962 - 00:50:21.174, Speaker B: So there's a lot of really nice things that happen with conjugation. I'm sorry that I had to very, very quickly sketch this. I wish I could have done this in more detail, but I think we'll have to end there.
00:50:23.914 - 00:50:42.074, Speaker A: Thank you. That's. Thanks, Stephanie. Questions comments? So you can shout them out or write in the chat.
00:50:42.694 - 00:50:44.474, Speaker B: Can I ask one thing?
00:50:45.534 - 00:50:46.198, Speaker A: Yes.
00:50:46.326 - 00:51:37.594, Speaker B: So any study of composition operators on this model spaces, are there notions of composition operators on them? I'm sure they've been looked at. I don't know too much off the top of my head. People have looked, certainly at maps between model spaces and their various sort of composition like transforms that map one model space to another. But I'm sure it's been looked at. I would suspect, aside from sort of relatively obvious composition operators involving Mobius transformations and things of that sort, it's probably not too rich mapping a model space to itself. But I may be wrong, although certainly you can map one model space to another, and that becomes quite interesting when you deal with composition operators. Thank you.
00:51:40.574 - 00:51:41.794, Speaker A: More questions?
00:51:44.294 - 00:52:10.824, Speaker B: Yeah, let me also say the slides for the previous talk and this talk have appeared in the chat, so I did correct some errors from the previous one. I'll probably do the same thing tomorrow. I'll probably send you an update on the second talk and so forth. And I believe if javad you could share the book chapter as well. For those of you following along, I did make a few updates to that to fix a few minor errors. Thank you.
00:52:12.644 - 00:52:30.464, Speaker A: Thank you very much for this, and it's downloadable. Okay, more questions comments? If not, let's thank Stefan again and the next talk will be in eight minutes.
