00:00:00.080 - 00:01:48.180, Speaker A: So last time we ended up with this amazing diagram, which is essentially showing the map between point configurations, endpoint configurations over the complex numbers or real numbers and the corresponding maps, the map to the real symmetric matrices that represent the pairwise distances between the points. I have complex c and choose two here, because one of the two maps, there are two maps which coincide over the rails. As we pointed out, one is the inner product map and the other one is the distance map. So one is shown in orange and the other one is shown in blue. And essentially they both coincide when the image is real symmetric matrices. But when the, sorry, the coincide when the domain is the real numbers and the image in both cases is the real symmetric matrices, or if you think of it as real distance vectors, and when the point configurations are complex, one of the maps could map potentially to complex distances, and so the matrices have complex entries and potentially complex eigenvalues. And whereas the other map always maps to the real symmetric matrices, except they could have negative eigenvalues.
00:01:48.180 - 00:03:58.444, Speaker A: So if we have real point configurations here, the image of the map is always real symmetric positive definite matrices, which means they have non negative eigenvalues. And if there are complex point configurations, then the inner product map could still maps only to the real symmetric matrices, but could have negative eigenvalues. One thing that I wanted to point out, and I think I did that last time briefly, but it's something to keep in mind when we talk about just having the positive semi definiteness condition already determining the rank. In those situations, we have to be careful if we're talking about complex or real point configurations. And the example that we had last time was when we had real matrices and they have non negative eigenvalues, and we want sort of a path in both the configurations, the configurations, domain of point configurations, and in the image of distance matrices that slowly moves out of the non negative eigenvalues to negative eigenvalues, it's forced to either drop rank or increase rank. So in other words, the one of the non negative eigenvalues will become zero before it becomes negative, or one of the zero eigenvalues becomes negative. So in other words, if you're on one of these strata of this cone, real symmetric matrices with non negative eigenvalues, that cone, in other words, distance point pairwise distances of points in real space.
00:03:58.444 - 00:04:47.088, Speaker A: If you're on one of the strata of this cone of dimension strictly less than n minus one, so n minus two. So let's say, and that means you have points, there's at least the points don't live in n minus one dimensions. They live in one dimension less. So there's one affine dependence. So what you could potentially do is ask yourself, well, I have n minus two non zero eigenvalues. In that case, remember, the number of eigenvalues is the dimension of the point configuration. And you could say that, well, I want to slowly move out so that I now have one negative eigenvalue.
00:04:47.088 - 00:05:34.994, Speaker A: So in other words, the point configuration goes to the complexes. Then, if you use the inner product map, you have to either drop rank or increase rank. So drop rank means all the one of the eigenvalues vanishes before it becomes negative, or one of the zero eigenvalues becomes negative. This is not the case if you use the distance map. So in the distance map, you can, you know, you're living in n minus two dimensions, and some affine new affine dependence happens. And just before the distance is, you know, no longer satisfied, one of the Kalimanger inequalities, let's say you can still maintain the same rank. And so in other words, the point configuration still lives in the same dimension.
00:05:34.994 - 00:06:48.778, Speaker A: And except that now the map is now going to map to, of course, you'll have complex point configuration. And in the case of the inner product map, it will, as I said, map to distance matrix of either one more rank or one less rank. But here, and correspondingly, the point configuration will either drop rank, drop dimension or increase in dimension, whereas in the distance map, the point configuration can remain in the same dimension. But if you use, you know, map forward, that quote unquote distance matrix will now have complex entries. So that's something to keep in mind when we talk about this idea of when the inequalities, namely the idea, the non negativity of the eigenvalues or the Caylee manger inequalities, either one of them, when they force the rank. When we say that, we have to be careful which of these two maps we actually use. So that's all I want to say.
00:06:48.778 - 00:07:16.614, Speaker A: We'll go deeper into this type of thing later. We're still in the tour stage. So the next topic, which I was already promised last time, was to talk about partial matrices. So we now don't have the entire, all the entries of the distance matrix, but only a subset of the entries given. So.
00:07:16.734 - 00:07:19.598, Speaker B: And can I ask a question?
00:07:19.726 - 00:07:20.646, Speaker A: Yeah, sure.
00:07:20.790 - 00:07:26.994, Speaker B: So just on that, just to remember what the map was, we have points p one through PN.
00:07:27.374 - 00:07:30.182, Speaker A: And here. Here. Yeah. Okay. Yeah.
00:07:30.318 - 00:07:31.194, Speaker B: For each.
00:07:31.574 - 00:07:33.954, Speaker A: Yeah, I think we had it in the previous slide. Yeah.
00:07:34.254 - 00:07:45.950, Speaker B: For each PI Pj. So each pair of them. Yeah, we take PI minus pj and compute its squared distance or its hermitian inner product with itself.
00:07:46.022 - 00:07:47.846, Speaker A: Exactly. Those are the two maps.
00:07:47.990 - 00:07:55.394, Speaker B: And that gives us one entry of an n by n matrix whose diagonal it consists of zeros.
00:07:56.214 - 00:08:34.128, Speaker A: Exactly. Okay, so that's, so you named both the two maps and the two maps coincide. When the point configurations are real. You'll always get positive symmetric def real symmetric matrices that are positive semi definite. But if the map, if the point configurations are complex, in one case you could even map the complex distances and complex entries in the matrix. And in the other case you'll still get only the reals but negative eigenvalues, potentially. Okay, and what I talked about, the rank is an important distinction.
00:08:34.128 - 00:09:06.764, Speaker A: What happens to the rank when you move out of the, from the positive to the negative eigenvalues? What happens to the rank differs depending on which of the two maps you use. Yeah. Okay, so, yeah, so now let's go to the graphs. So you have matrix. So you have a, I'm going to go for a second to, maybe I can just use annotation. Let's see. Let's try that.
00:09:06.764 - 00:09:42.124, Speaker A: So here we have, so essentially we have a partial matrix. So you have some of the entries given. So let's say I'm going to call the entries that are given. I'm going to mark them like this. These are of course zeros. You can always assume that you're trying to sort of figure out whether you can extend your partial set of entries. It's a symmetric matrix that's also known.
00:09:42.124 - 00:10:31.086, Speaker A: So I'm not going to do anything here. So some entries are given and other entries are not given. So if you look at the IJ values, now these you can think of, these are the vertices of a graph. And so essentially this IJ entry being present means that the corresponding graph has an edge between I and j. So the given entries represent a graph. And what you want to figure out is whether the remaining entries can be filled up in such a way that, in such a way that I don't know now whether I do, I now have to save immediately. I probably do.
00:10:31.086 - 00:11:00.720, Speaker A: Okay. Okay, so get out of there. Okay. And I have to do some kind of clear. There we go. Okay, so this, so that obviously didn't, it saved it somewhere. So I think I prefer to use the whiteboard.
00:11:00.720 - 00:12:12.680, Speaker A: I'll just use the whiteboard. So if you, the kind of questions you're interested in is whether or not you can extend it to a distance matrix, that means what we mean by that is existence of a real realization. So, you know, positive, semi definite matrix, whether it can be extended to a matrix of a given rank. So somebody gives you a rank and says, can you extend it to a positive, semi definite matrix of a given rank? And the questions of whether this can be done can again be split into three parts. Does there exist such a completion? How many completions are there? What is the dimension of the completion space, so to speak, and whether it's unique, whether the completion is unique. So these are the three general questions that you're interested in. And it so turns out, as I have pointed out before, you know, if we happen to be on the boundary, then so you can think of these, these subset of.
00:12:12.680 - 00:13:20.114, Speaker A: I think I had a different picture there, but the subset of entries, yeah, we drew it here. So the subset of entries is a kind of a projection of this cone on a subset of coordinates. And if you happen to lie on the boundary, then just having the positive semidefinite conditions could result in having, you know, only a particular rank for possible, for the, for the point configuration. So in other words, even without explicitly restricting the rank, so even without explicitly restricting the rank, and you simply say it's a distance matrix, there are situations when, which you could end up with finitely many solution or unique solution or whatever. And another way to interpret that is that the caylemanger inequalities force an equality. So the equalities then determine, I mean, bound the rank. And the inequalities are just saying that it has a real realization.
00:13:20.114 - 00:14:47.896, Speaker A: Okay, so another question that you're interested in, besides whether or not they exist, etc. Etcetera, is suppose you wanted to find the realization, what is the, or even determine the realization algorithmically. Determine the existence of a realization algorithmically. What is the computational complexity? So the overall, all of these questions, you know, they depend on the graph of the entries of the partial matrix. And to what extent do they depend and do we care about more than just the graph? Okay, so let's see how, let's see how the dependence on the graph sort of manifests itself. Okay, so let me just go to the whiteboard for a second. Okay, so there's a, since we already saw the one proof, the proof that we gave of the Caylee Menger conditions, used a kind of an algorithmic approach that you were finding the realization at the same time checking the Caylee Menger inequalities.
00:14:47.896 - 00:15:25.822, Speaker A: Or you can think of finding the realization as checking the gelimanger inequalities. And let's just think about how that proof actually worked. So the proof involved finding intersections of spheres. And let's assume for a moment it doesn't matter. Eventually it generalizes to every dimension. But let's assume for a moment that our points live in r two for a second. So there exists a realization in order to, if you remember that algorithm, the algorithm just finds the dimension of the realization at the same time.
00:15:25.822 - 00:16:31.516, Speaker A: In fact, it finds the minimum dimension of the realization as it tries to figure out, as it figures out whether or not any realization exists. Or in other words, whether the given matrix is an euclidean distance matrix. The only difference is when we gave that particular algorithm, we didn't think about whether all the entries were actually given in the matrix. What I'm going to say point out is that if in fact the dimension of the point configuration is bounded by some number d strictly less than n minus one, we are not going to use all the entries just to find a realization. Okay? So let's for a moment assume that D is actually equal to two. That that algorithm basically would place two points with the first distance between them. So the, there's a partial, I'm going to show you that it's only a partial set of entries is sufficient.
00:16:31.516 - 00:17:06.544, Speaker A: So we place the first two points without loss, put this at the origin, without loss, put this at the zero, comma sorry, or D 120. Right. And then we place the next point. And to place the next point, we just needed potentially these two distances. And so this is the third point. So this is D 13 and d, sorry, D 13. And this is D 23.
00:17:06.544 - 00:17:48.854, Speaker A: So we only needed these two distances. So, so far, yeah. And then when we place the next point, remember, at this point, we are not trying to verify whether a complete matrix is a euclidean matrix. We are just trying to find a realization. And the question is, how many of these entries do we actually need to find a realization? So let's say if there's the fourth point, we can place that fourth point using any pair of other distances. So you can choose whichever pair you want. So potentially you could have chosen this pair or you could have chosen that pair.
00:17:48.854 - 00:18:18.834, Speaker A: So essentially you place that point, finding these two distances. Okay, now the fifth point. Now notice that this distance between three and four is something that we did not use. Keep that in mind. So now to place the fifth point. Now remember that the fourth point could have been placed over here, or it could potentially have been placed over here. I'm going to use a different color.
00:18:18.834 - 00:18:21.390, Speaker A: Could have been placed over here.
00:18:21.422 - 00:18:22.470, Speaker B: Can I ask a question?
00:18:22.622 - 00:18:23.382, Speaker A: Yeah.
00:18:23.558 - 00:18:32.894, Speaker B: Just a reminder. When we were placing the third point, how did we solve the system of two quadratic equations in two variables?
00:18:34.594 - 00:18:52.814, Speaker A: It becomes just a single quadratic. I'm going to leave that as a homework. It just becomes one quadratic in one variable. Doesn't matter. You could solve two quadratics in two variables, but you can reduce it to one quadratic in one variable.
00:18:55.534 - 00:18:56.662, Speaker B: Okay, thanks.
00:18:56.798 - 00:19:36.640, Speaker A: Yeah. So the fourth point, again, it becomes one quadratic in one variable. If you just did this, there's a possible, we'll see it in the fifth point in a second. So it's still one quadratic in one variable and only depends on the original distance. D twelve over here. So now fourth point could have been placed over here, in which case the distance between the third point and the fourth point would be potentially different. And if you were interested in verifying whether this was a euclidean distance matrix, you would have to check whether that distance was the distance that was given.
00:19:36.640 - 00:20:06.492, Speaker A: But for the moment, we're just asking how many distances do we need in order to even find whether there exists a realization. Okay, so now let's look at the fifth point. So now the fifth point again, you can use any pair of distances. So let me take a slight difference. Change instead of place. I could have just placed, used these two distances. But suppose those two distances are not given.
00:20:06.492 - 00:20:47.446, Speaker A: Suppose d one five and d 25 are not given here. They're not present in this matrix. Suppose d three five and d four five were given. So you have these two. Okay, suppose those were the two numbers that were given in this matrix and the rest was not. So now notice that whether or not five can be placed depends on, could potentially depend on whether I put four over here or four over there. It could be that the distances between three five and five four do not satisfy the triangle inequality.
00:20:47.446 - 00:21:14.842, Speaker A: Had I placed four over here. So now we need to know. So now it tells me that offhand, in order to verify where to place five and not have to go back and change, change the position of four, we could do that. We could change the position of four. There are two possibilities for four. We could change the position of four in order to be able to place five. We can do that.
00:21:14.842 - 00:21:59.668, Speaker A: But notice that if I had been given the distance three four, then I would have right away in the beginning decided which of these two positions of four to choose. You can convince yourself that not both positions of four would, would. I don't want to use the word generically yet. But in general, not both positions of four could satisfy a given distance for three four. Okay, so three four has only two possible values. Once four has been placed, it's finite, fixed, finite number of possible values for three four. Once four has been placed.
00:21:59.668 - 00:22:34.990, Speaker A: Given that I've given, I've said that the dimension here is two. So in a way, this dotted line is look is dependent on the other distances. So once I have placed four, I've used all of these distances. These distance determine what the distance between three and four are. So then we go to the next. So if I knew three four, then I would have placed four in the right place. So the complexity of solving this would have been a lot simpler.
00:22:34.990 - 00:23:18.040, Speaker A: I would have known where to place five. Otherwise I would have to check both positions of four. You can still do it, except you can see that you might end up with an exponential increase in complexity as you place more and more points by the time you come to the nth point, every single point before it has two possible values. And so by the time you get to the nth point, you may have to check two to the n minus one possible positions for all of the other points to check whether the new point is there. On the other hand, if I'd given this distance, then I know right away where to place it. So this one extra distance at every given stage makes a big difference to the complexity. So five is given.
00:23:18.040 - 00:24:43.784, Speaker A: Again, five can be placed over there, or five can be placed over here. Right? And depending on where it's placed, if I had one more distance given to me, doesn't matter which one, right? Suppose I had been given the distance between one and five, or for example here, one and five. Knowing that distance, I can choose between the two placements of five. So, as you can see, there is a particular type of graph of distances had the particular IJ entries that were given to me. If there's these particular graphs, as you can see, every time I add a new point, it's connected to the old graph by two distances. And it doesn't matter where, which of the two, the new point is connected to any pair of two points before. And if I'm given a third distance, yet another distance, then in fact my complexity of solution also drastically drops from exponential to linear.
00:24:43.784 - 00:25:21.664, Speaker A: The actual graph makes a big difference as to whether I can do this if I weren't given this. So these are called, there are many words for this type of graph. They're called zero extension graphs, or Henneberg. Henneberg, zero. Henneberg. Zero extension graphs and the type of graph that I'm not drawn. And when I have a third thing there, they're called d lateration graphs.
00:25:21.664 - 00:25:26.810, Speaker A: So now, what do I mean by d? I've only been talking about d equals two.
00:25:26.932 - 00:25:28.594, Speaker B: And what's the third thing?
00:25:29.774 - 00:25:31.902, Speaker A: The third thing, what is the third thing?
00:25:32.078 - 00:25:35.714, Speaker B: That's what you adoration.
00:25:41.694 - 00:25:45.590, Speaker C: Alex, could you zero extension graphs?
00:25:45.782 - 00:25:51.834, Speaker B: Oh, sure, I was just asking because you said, and when I have the third thing, then we call it de lady.
00:25:52.414 - 00:26:12.500, Speaker A: Remember when we were adding each point, I had a third, if I had a third distance available, if the graph allowed me. So when I placed the third fourth point, I had not only d 14 and d 24, but also I had d three, four available here in my matrix or graph.
00:26:12.692 - 00:26:14.504, Speaker B: Oh, I see. Okay, thank you.
00:26:15.324 - 00:26:54.534, Speaker A: And what is d here? So I started out with d equals two. But notice that this can, from our original sphere intersection algorithm, this d can be anything. You know, it could be d could have been three. And in which case, you know, to place the first point, you would have intersected three spheres. So you would have three, you know, you would start out with three points. Once after you have three points, the fourth point will have three distances to place it in one of two possible positions. And you would need a fourth distance to decide which of those two positions to go.
00:26:54.534 - 00:28:00.984, Speaker A: So this idea of Henneberg zero extension graphs extends, or is a general definition for any d, except that as the dimension grows, the number of edges that are needed to place the new point increases by one. And correspondingly, the dlatoration graph has one more distance than is needed for the Henneberg zero extension graphs. So those Hanneberg zero extension graphs allow us, if the partial matrix had entries corresponding to these, either Henneberg zero or dlateration graphs. In the case of Henneberg zero extension graphs, you can find a realization in this simple way, although it may take exponential time. But if they were delaturation, then you actually have linear time. The same sphere intersection algorithm that we had now gives us a solution. Notice that there are tons of entries here.
00:28:02.084 - 00:28:08.788, Speaker D: Sorry, would this be a, wait, what d are we taking? Is this a zero lateration if we're in r two or.
00:28:08.876 - 00:28:24.734, Speaker A: Oh, okay. So let me just delete the word. So for d equals two, I think. Oh, sorry. This would be d plus one. So for d equals two, these are called trilateration graphs because we need three distances at any stage.
00:28:26.034 - 00:28:26.490, Speaker D: Okay.
00:28:26.522 - 00:28:27.266, Speaker A: Yeah. Okay.
00:28:27.290 - 00:28:29.054, Speaker D: If you're going to get through, then that's fine.
00:28:29.634 - 00:28:42.528, Speaker A: And in general, for d it'll be d plus one. Lateration graphs. But these are always called zero extensions for some reason. It doesn't matter. It's just called zero extension for dimension two or zero extension for dimension three. Whatever.
00:28:42.626 - 00:28:43.864, Speaker D: Okay, thank you.
00:28:44.684 - 00:29:21.010, Speaker A: Because the one extension means something else. We'll come to that later. So they're always called zero extension. You just specify the dimension whatever it happens to be. So these, so as we can see, the ability to realize or coming up with a realization and how quickly you can come up with the realization depends on what graphs are given here. So if I wasn't given such a graph, such a nice graph, then, you know, who knows how we can do this? How to do this? You certainly can't do it with this sphere intersection algorithm.
00:29:21.202 - 00:29:22.658, Speaker B: Can I ask another question?
00:29:22.786 - 00:29:23.494, Speaker A: Yeah.
00:29:23.954 - 00:29:31.254, Speaker B: For D equals three intersecting three spheres, can that also be reduced to one quadratic and one variable?
00:29:32.714 - 00:29:38.454, Speaker A: That's a very good question. I don't think so. It'll be two quadratics and two variables.
00:29:40.844 - 00:29:41.988, Speaker B: Okay, thank you.
00:29:42.116 - 00:30:19.834, Speaker A: Okay, but at any given time, you can do this. I have, I mean, I'm jumping the gun. I'll be all over the place if I get to that. It's related to what is called triangularizing a quadratic system. Okay, so, but that's several slides into the future, so I don't want to go there yet. Okay, so another thing I wanted to talk about is slight variation on these graphs. These are called Henneberg one extension.
00:30:19.834 - 00:30:50.984, Speaker A: Slight variation on these graphs are called tree decomposable graphs. So I just want to talk about three decomposable graphs for a second. So tree decomposable graphs are instead, just imagine for a second, as we just said, we can, whenever two distances are given. So here's a point. Here's a point. Whenever two distances are given for d equals two, you can place this point. Now, this distance doesn't necessarily have to be a given edge.
00:30:50.984 - 00:31:47.036, Speaker A: It could be that there is an entire set of submatrix of this distance matrix that allows you to find a graph. I mean, find a realization of a sub graph, which I am drawing in this way. And similarly, there could be another submatrix that allows you to find a realization of a subgraph in the way that we just talked about and three such subgraphs. And once we have those three such sub graphs, we have our three required distances, and then we can place this point. So having found this sub graph and this sub graph, we have these two distances. Now we can somehow find the common realization of the three subgraphs. This is sort of realized in its own coordinate system, let's say.
00:31:47.036 - 00:32:25.556, Speaker A: And this has been found in its own coordinate system. Now we have to sort of put them together to find a joint realization of everything together. And you can do this. So this kind of graph, which is recursively defined as being, as being. This is called a tree decomposable graph. It's recursively defined as being three tree decomposable graphs, each sharing, each sharing exactly one vertex. Okay, and the base case, a tree decomposable graph is just an edge.
00:32:25.556 - 00:32:45.044, Speaker A: So, so three decomposable graph, the base case is edge. And in general, three d. Three decomposable graphs, each pair while sharing a vertex.
00:32:48.664 - 00:32:52.684, Speaker D: Is that three for all the. Or is that d plus one?
00:32:53.664 - 00:32:57.564, Speaker A: Ah, yes. So this would be d plus one in general.
00:32:57.864 - 00:32:58.864, Speaker D: Okay, thank you.
00:32:58.944 - 00:33:32.718, Speaker A: Okay. Sharing a vertex. Okay, so, so this is called a three decomposable graph in general for any dimension. So that's a sort of a natural generalization. These Henneberg one and, sorry, Henneberg zero, and the corresponding tri. Later d la one. Laurie would now have one more distance that you can use at any given stage.
00:33:32.718 - 00:34:25.378, Speaker A: Okay, so I'll just define three decomposable for now. Okay, so these three decomposable graphs are very commonly. So, now that you brought up the question, Alex, I'm simply going to talk about this idea of solving the degree of these things. So, we'll come very quickly in a few slides to the idea of solving polynomial systems. And here we are solving a whole bunch of quadratics which correspond to the distances, the distance map that we talked about. These are not. You can alternatively think about this as solving, as we did in the original algorithm, where we thought about the sphere intersection as established as finding, taking a determinant and setting it to zero is exactly the same as solving for the sphere intersection.
00:34:25.378 - 00:35:36.926, Speaker A: One is taking place in the distance coordinates. These are the Cayley Menga matrices, where these are the distance coordinates. And the other one, you're solving for the distance coordinates directly in a way. And in the other case, where you writing all these equations down, PJ distances equals something, the given dij squared distances. So you can think of what we are doing, this peer intersection algorithm, as solving two types of polynomial equations, one that involves the distance map, the polynomial distance map, where the distances are given the partial, the partial entries of the matrices are the right hand side, and you're writing all these equations down. Another way to think about it is that you're directly solving for the unknown distances of the partial matrix by taking the Cayley Menger determinants and setting them to be equal to zero, where the known values are, you know, sort of given in here. And then the determinant is a polynomial in the unknown distances, or you're solving for those polynomials.
00:35:36.926 - 00:36:15.818, Speaker A: You can think of it as either way. This case, of course, quadratics, no matter what the dimension of the point realization, this is always quadratic. Whereas if you did it this cumbersome way, this cumbersome way, you know, obviously, depending on the dimension, the determinant degree is going to increase. So this is just a curiosity. It's a way to think about it. But this is how you want for checking is a different story, but for actually realizing, you want to use the quadratic system, this polynomial system, to solve. Okay, so Alex's question.
00:36:15.818 - 00:36:48.650, Speaker A: So you have a whole bunch. So these are obviously the number of variables that are the coordinates of these points. There are going to be d variables here and d variables. So these are, this is a multivariate quadratic system. And so what you're trying to do is solve this system. And there are many ways to solve, I mean, think about solving this system. And one of the most common ways is called grobna basis method.
00:36:48.650 - 00:37:23.294, Speaker A: And we'll come to that in much more detail in a few slides. And what the grobner basis is a kind of generalization, if you will, of gaussian elimination. It can be thought of as generalization of, you know, sort of GCD. It can be thought of a generalization of gaussian elimination. Right. And what does gaussian elimination do for linear systems? For linear systems, if you have a bunch linear, a bunch of equations. Linear equations.
00:37:23.294 - 00:37:43.056, Speaker A: Let's assume they're zero on the right hand side. For now, it doesn't matter. Could be something on the right hand side. You triangularize it. You triangularize the system. And with gaussian elimination, when you triangularize, nothing happens to the degree. The degree doesn't increase, they still remain linear.
00:37:43.056 - 00:38:27.504, Speaker A: The key thing is that each, you know, each time there's one variable here and there are two variables here and there, three variables in the system and so forth. So you can sort of back substitute. You can solve, you have one variable equals something, and then once you have that, you can put that here. And again you have just an equation in one variable, and again you have an equation in one variable and so forth. So in our situation here, when we were solving here, we basically solved for three. So using solving these two quadratics, we gave. And that's a univariate system.
00:38:27.504 - 00:39:52.580, Speaker A: You get a solution for the coordinates. So then when you're solving the next quadratic, it turns out to be, again, a univariate quadratic because you've already substituted for the values of this. So you have, when you're doing this, essentially, you have triangularized this polynomial system where each time you're solving with one fewer variables. And it turns out, because we have three decomposable graphs, we actually have a triangular system where the degree does not increase. So normally, when you do grobna basis or anything else, to actually triangularize a system, unlike the linear case, as you go down here, this polynomial will be a univariate polynomial, but it will have a huge degree. Typically, whatever this n is, it'll have some exponential degree in that. Okay? But the three decomposable systems are ones where you can triangularize and still maintain quadratics and still have only quadratics, still not increase the degree.
00:39:52.580 - 00:39:53.572, Speaker A: Okay.
00:39:53.708 - 00:39:54.516, Speaker B: Hey, Mira.
00:39:54.620 - 00:39:55.188, Speaker A: Yeah.
00:39:55.316 - 00:40:03.598, Speaker B: So the way to reduce the two quadratics and two unknowns to a single quadratic is by Grosvenor basis.
00:40:03.646 - 00:40:50.514, Speaker A: You're saying, no, no, no, you don't have to. I mean, it's much simpler than. You don't need Grosvenor basis for that. Right? All I'm saying is that if you did the very general Grobner basis method, you can still triangularize, but you'll end up with the equations having very high degree, but with the three decomposable graphs, what you have essentially done is triangularized the quadratic, multivariate quadratic system, but not increase the degree every time. You're always only solving for one variable more or some constant. I mean, I'm not going to be very particular right. Now maybe you're solving for some constantly many variables more, right, you, it has this triangular form, but we have not increased the degree.
00:40:50.514 - 00:41:48.134, Speaker A: Okay, so essentially having, and this is also, turns out that this also, because you don't increase the degree, this is what is galua. This is quadratically radical. The solutions are quadratic elements of the quadratic extension fields built one on top of the other. So this is also called quadratic radical, solvable radical solvable systems. Right? Because you don't increase the degree. So you, every time the square root, you know, you get a square root assuming, assuming it's real, and then you get an extension of the original coefficient field. Let's say the original coefficients of your polynomial were in the rationals or something, and you have a quadratic extension.
00:41:48.134 - 00:42:36.204, Speaker A: So you have some square roots in there. And then now the coefficients of your new polynomials will now have this. And then you have, so you sort of build a tower of quadratic extensions, you know, q and then square, and then you're adding on some square roots, and then, then you have q and the square roots, and you're getting square roots of those and so forth. But whatever it is, you only have quadratic. So this is also called quadratic radical solvable. So when you have a three decomposable graph as your partial matrix, you can actually say a lot of good things. First of all, you can realize it just using a triangularized quadratic system without an increasing degree, you have a quadratic radical solvable plus.
00:42:36.204 - 00:43:55.674, Speaker A: If you have that one extra distance at every stage, you have a d plus one lateration graph then, or the three decomposable version of the D plus one layeration graph, you now have in addition, a low complexity, just a linear complexity in solving for the system. So if you don't have that extra degree, you can actually show that solving, finding a solution, that exponential growth in complexity, you can actually formalize it as an NP completeness result. That's an old result by Sachs. So he showed that just, even with the tree decomposable graph, if you didn't have that extra distance at every stage, even for the just a Hanneberg zero extension graph, if you didn't have that extra distance at every stage, the fact that you have to check both possibilities every time can be formalized as an NP completeness complexity result. Okay, we will come to that eventually. Okay, so now I think I have pointed out the new share. No, not this one.
00:43:55.834 - 00:44:29.190, Speaker C: Mira, let me ask, when you have two quadratics, let's actually stick with the case of two circles that we're trying to intersect. So you can reduce that to a linear equation and the, and also the quadratic, one of the quadratics, you don't need both of the quadratics. Really?
00:44:29.262 - 00:44:41.990, Speaker A: You don't need both the quadratics, right? Yeah, but I don't want to really be. So I'm saying, you know, at every stage, you only have one more variable and still a quadratic at every step.
00:44:42.022 - 00:44:48.622, Speaker C: That's right. That's right. It's only one, one quadratic. There's a bunch of linears and then one quadratic. That's left, right?
00:44:48.718 - 00:44:49.354, Speaker A: Yes.
00:44:50.524 - 00:44:54.676, Speaker C: A single quadratic extension at each stage.
00:44:54.820 - 00:44:55.524, Speaker A: Exactly.
00:44:55.604 - 00:44:56.204, Speaker C: Okay, good.
00:44:56.244 - 00:45:01.356, Speaker A: And that's true, actually, for any d, by the way, you just have to work a little harder.
00:45:01.460 - 00:45:01.908, Speaker C: Yes.
00:45:01.996 - 00:45:02.252, Speaker D: Okay.
00:45:02.268 - 00:45:03.864, Speaker A: That makes sense. Yeah.
00:45:04.964 - 00:45:08.464, Speaker B: Can you allow us to chat with everyone? Is that possible?
00:45:09.044 - 00:45:18.746, Speaker A: Yeah, I don't know if I can do it when I'm in zoom. I thought that was the setting. I thought I had changed the setting to do that.
00:45:18.940 - 00:45:20.394, Speaker D: I can chat to everyone.
00:45:21.374 - 00:45:26.766, Speaker A: Can you do that by any chance, or am I able to do that within zoom?
00:45:26.830 - 00:45:32.914, Speaker B: Oh, I see. I meant I wanted to chat with Will because I wanted to ask him about this linear and quadratic thing.
00:45:33.894 - 00:45:37.154, Speaker A: Oh, yeah, that's what I meant. But how do I do that?
00:45:37.654 - 00:45:42.234, Speaker D: Maybe allow private chats between two members or something.
00:45:42.814 - 00:46:04.696, Speaker A: But isn't that something I need to do? I have to leave Zoom. If you go log out and then go and do something in the settings. No, we fixed it last time. If you hover over the chat symbol, there should be a little arrow above it. You click on that and then. Oh, yeah, everyone publicly everyone pub. Yeah.
00:46:04.696 - 00:46:07.924, Speaker A: I allow you. You can chat with everyone.
00:46:10.464 - 00:46:12.484, Speaker B: But also to individuals.
00:46:13.104 - 00:46:20.404, Speaker A: Oh, you want to do private chat? No, I don't want to allow that, Alex. You just, you can talk to will, but talk to him publicly.
00:46:21.384 - 00:46:22.640, Speaker B: Okay, no problem.
00:46:22.792 - 00:46:23.440, Speaker A: Okay.
00:46:23.552 - 00:46:32.120, Speaker D: Yeah, sorry, can I ask one last question that you probably answered already? So this matrix we got here.
00:46:32.312 - 00:46:33.088, Speaker A: Yeah.
00:46:33.256 - 00:46:39.884, Speaker D: That's the triangularization of the multivariate quadratic one we had on the last slide.
00:46:42.164 - 00:46:51.356, Speaker A: So you're right. I can see how things can be confusing. This matrix is totally different from the matrix we started out with.
00:46:51.540 - 00:46:52.244, Speaker D: Okay.
00:46:52.364 - 00:47:09.994, Speaker A: Yeah. This matrix is the distance matrix where some of the entries of the distances were given. And these distances are the right hand sides of our equations. And this matrix is these equations which have been triangularized.
00:47:10.894 - 00:47:24.434, Speaker D: Oh, okay. So the. So is. So the original matrix, each one we've got is an equation to form like PI minus pj squared equals dij squared.
00:47:25.414 - 00:47:49.444, Speaker A: Yes. So we started out with this partial, quote unquote matrix. You can think of it as a distance vector of which some of them have been specified, others have not been. Right. That's our graph. And each one of these entries, sorry, given entries, I'm going to make a bunch of quadratics like this with this, this number here on the right hand side.
00:47:49.744 - 00:47:50.464, Speaker D: Okay.
00:47:50.584 - 00:48:12.184, Speaker A: Okay. So I have this system of quadratics, multivariate quadratic system. Now that is what I'm figuratively or schematically representing now as this thing. Yeah. Except that I have now triangularized that system.
00:48:12.724 - 00:48:13.628, Speaker D: Got it.
00:48:13.796 - 00:48:53.288, Speaker A: Okay. By, you know, I haven't told you all the details, but I'm sort of. Because that if I really want to be able to do it for an arbitrary quadratic system. You know, I have to use more machinery. But with our example graphs that we had with our triangular, sorry, three decomposable graphs, you can see that we ended up with a quadratic system to be solved at each, every stage with one more variable every time, where if you solved the previous one, you can substitute it, and now you just have a single quadratic in one variable plus some linear, as Will says.
00:48:53.416 - 00:48:55.144, Speaker D: Okay, makes sense. Yeah.
00:48:57.884 - 00:49:47.956, Speaker A: Okay. So, right, so let's go back to. Okay, so I was ambitious to think that I could directly annotate this blank slide, but now I'll put the other stuff in here at some point. By the way, are you able to see the copy that I put on piazza? I'm making copies of these. And. Well, we can talk about that later at the end. Okay, so some of the things that we have seen here while we did this, in this simple example of trying three decomposable graphs and the corresponding triangularization of the polynomial system, we noticed a few things.
00:49:47.956 - 00:50:29.662, Speaker A: These extra distances for the d plus one lateration graph, as we pointed out, are sort of dependent on the other distances. So without that extra distance, you still have only finitely many solutions. At every stage, you only have finitely many solutions. So altogether, you have only finitely many solutions. These extra distances are dependent in the sense that once you have these other distances, this extra distance is determined. I mean, is going to be one of two things, one of two numbers. Because you have only finitely many solutions.
00:50:29.662 - 00:51:22.004, Speaker A: At two solutions at each stage, the extra distance can be only one of two things. So that's local dependence. Right? So these are things that we notice here. And so this idea of dependencies can actually be seen without having our underlying field. And, well, we talked about the reals and complex numbers. So there are numbers there, the distances are numbers and so forth. But we also talked about a graph, and we somehow said that these answers could be given the question of whether something exists or whatever could potentially be, and what the complexity is and so forth, could be just determined based on the graph.
00:51:22.004 - 00:52:10.894, Speaker A: So. So the question is, when is a graph sufficient to answer these, all of these questions, do the distances, coordinates, etcetera, actually matter? Do the actual values matter? Or to what extent? I mean, obviously they matter to some degree, but to what extent can we answer these questions just by looking at the graphs, just answering questions about the graph. So this is a. I guess Tony has already covered generic and all this when he talks. Tony, people who are attending Tony's class, he's already obviously talked about generic coordinates and generic frameworks and so forth, right?
00:52:13.874 - 00:52:14.694, Speaker C: Yes.
00:52:15.194 - 00:54:17.124, Speaker A: Okay, so you already have that idea in your mind that you can answer some of these questions at least because questions at least based on just what the graph looks like. Okay, so we'll go into more detail about what generic means and when can the question be completely answered with only the using the graph and when you need some other information and so forth as we proceed. But the idea of dependences, dependences between, in this case, edges of the graph, because this new edge, the third edge in a two lat, in the two dimensional zero extension, when the third edge was dependent, and this idea of dependence between various things in a graph. Or you can think of that particular polynomial, that distance being dependent on the other distances, or you can think of the polynomial that we had that correspond, that PJ squared that corresponded to that third distance being dependent on the previous polynomials, all of those three types of dependences that we have taught that I just mentioned, either a dependence in the graph sense, notion of dependence in the graph, dependence of the polynomials on the other polynomials, and depends on distance on the other distances. All of these are, you know, a common notion of dependence that can be captured by these, that have been captured by objects called matroids. Okay, so metroids are a combinatorial object. So in other words, the actual field underlying field, whether it was over the reals or complexes or rationals or maybe even finite fields, that has been removed somehow.
00:54:17.124 - 00:55:15.994, Speaker A: And we want to extract the idea of dependence and in a combinatorial way, to see to what extent it makes sense, purely combinatorial. So it was, you know, back in the thirties or something, they have, you know, thought about this idea. And so Vander Werden was interested in sort of the algebraic dependences. And previously, of course, with linear systems, you already have this idea of linear dependence that's a well known, everybody has seen it, say, in high school or in an early class. So, and we can now talk about dependences even in combinatorial objects. And that's what metroids are. So I'm going to just brief detour into matrix, because this is, we're still in the tour of the tool stage.
00:55:15.994 - 00:56:34.472, Speaker A: So we have talked about Keelemager determinants, Schoenberg's theorem and so on and so forth. And then we have come to polynomials, and now we have sort of extracting combinatorial properties of polynomial systems in the form of graphs. And now we're going to talk about this other important tool that we talk about, that we're going to use, which are metroids. And we'll think about all types of meteorites over graphs, over algebraic systems, polynomial systems over the Kalimanger determinants over these distances, and different types of metroids. I think Tony would have already talked about the rigidity meteoroid, which refers to linear, um, dependencies, really starts out not the generic rigidity metroid, but if you think of the rigidity metroid of a particular framework where you write the rigidity matrix down, then you think about the dependences between the rows of the rigidity matrix. These are ordinary linear dependents, and that's usually where, you know, the best place to start in thinking about what a matriarch is. So let's take the simple example of basic notion of linear independence in coming up with the definition.
00:56:34.472 - 00:57:24.474, Speaker A: So we'll see what is it about linear independence, sort of the basic axiomatic things about linear independence that we can extract to define a combinatorial object that that's going to be the matriarch. So let's just take this eight columns, and instead of rows of the rigidity matrix, now we're moving to columns. So of course they are linearly dependent. If there exists some, you know, we think of them as elements of some vector space. So you think of dependence on the vector space. So you have some elements of the field over which the vector space is built such that the sum over the vectors, the linear combination of the vectors with those coefficients, those field elements, is equal to zero. That's dependence, linear dependence.
00:57:24.474 - 00:58:24.274, Speaker A: So otherwise, you say that this is linearly independent. And in general, we'll deal with only finitely many such entities. So here, the entities here are the columns or the vectors. So we will, in general, remember, we're always trying to extract, what is it axiomatically, what do we mean by independence? When, what are the sort of key properties of independence or dependence that we can extract and then talk about combinatorially. So, one thing we can see in the case of vector space is that every subset of an independent set is independent. If two independent sets are given to you and one of them is strictly smaller than the other, then for some element of the larger set, you should be able to take it and add it to the smaller set to get an independent extension. I mean, a larger independent set from I one, that's another simple property of vector spaces.
00:58:24.274 - 00:59:05.098, Speaker A: And if you have a subset, then the maximal independent sets are always of equal size. Okay? So that's another thing. You know, you always think of basis, you can take different bases, um, different subsets of these, um, vector space, that set of vectors that happen to be independent. The maximal ones always have the same size. So in fact, you know, I two and I two prime are if and only if one. One is a consequence of the other. And this number, the size of the maximal independent set is the size of the basis.
00:59:05.098 - 00:59:48.714, Speaker A: It's also called the rank, which we know. So we know all of these things. I'm just extracting these properties so that we can now talk about, you know, talk about metroids in this way. So we can just take these as our axioms. Okay, so, so these are all just other ways of thinking about the same thing. There are multiple ways in which you can write the same axioms down, as we can see already for this case. So if we now extract those axioms and write this as axioms of a matroid, I mean axioms that define a combinatorial object that has these properties.
00:59:48.714 - 01:00:31.066, Speaker A: So you have a non empty subset e. It was called I in the other one, some kind of, it's called the ground set. In that case, it was the columns of that matrix. In general, it's a set of some sort, and you have subsets of it, which we're going to call bases or maximal independent sets. And with the property that no basis or base properly contains another base, no maximal independent set properly contains another maximal independent set. And if you have two that are basis and one is an element of, and, sorry, is any element of one of them, then you can get another element. Well, it's very similar to the other one.
01:00:31.066 - 01:01:05.070, Speaker A: You drop one of the elements from B one. So now B one is one smaller than the other base. So you should be able to find an element of the second base that you can add to it. So it's basically the same axiom that we had here. If you had two things that are one smaller than the other, you should be able to find an element here that you can add to it and get still an independent. That's exactly what this is. If you have two of the two bases, then you should be able to drop one from one basis.
01:01:05.070 - 01:01:42.950, Speaker A: So then it's of one smaller size, should be able to find something in the other basis that you can add to it. Also get a base. This is also called the exchange property. So these are the two basic axioms that define one possible set of basic axioms that define a metroid. There are multiple equivalent sets of axioms that define a matrix. So going back to our example, so you know, these are the columns, this will be our set, you know, the ground set. And you know, so there we had eight columns.
01:01:42.950 - 01:02:42.494, Speaker A: So any set of four columns form in that particular case the grounds maximal independent set. And I mean in general, because you had five rows, you may expect that there could be five, but in this particular case that turns out to be only four because the rest are dependent. And then you have a different basis here, different set of columns. And now if you remove the second vector in B one and replace it with the second vector in B two, you get another basis and you can simply verify that this is basis. So this is in that example, you can talk about this. Now that we have extracted the axioms, you can think about the same axioms. What do they mean if you don't have any underlying field or vector space or anything like that? So you just have a combinatorial object, a graph, let's say.
01:02:42.494 - 01:03:47.652, Speaker A: And it's another example. So you can think of the ground set instead of the columns of this, or the set of vectors which were the columns of this original matrix. In that example, you can think of the ground set as just the edges of a graph, and you can define independent sets as being acyclic. So now we can check whether the axioms are true. Every subset of an acyclic set is acyclic. And if you have two edge sets, sets that are both acyclic and one is bigger than the other, then you should be able to find an element of the second one, the bigger one, that you can add to the first one and to produce a slightly larger acyclic set. So if you think of the, why is this the case? Because if you have two acyclic sets and one of them is bigger than the other, then you know you can, it is possible that several of these, they are on the same set of vertices.
01:03:47.652 - 01:04:45.548, Speaker A: So it's possible that there are edges here in the larger one, that you add to the smaller one, creates a cycle. It's possible that that's the case. But then because this is a strictly bigger, you should be able to find one that does not create a cycle because you know there's one, it has to involve one more vertex and so you can add to it and you get an acyclic set. So this also gives you the idea of a rank in the set of edges, which is the size of the maximal spanning tree. It's typically called a maximal acyclic graph that actually spans the spanning tree of the graph. All the people in would have heard of a spanning tree even computer science people use spanning trees all the time. So a spanning tree would be such a maximal independent set in this metroid.
01:04:45.548 - 01:05:42.264, Speaker A: So this is a totally combinatorial generalization or extension or an analogy of the vector space, the linearly independent vectors of a vector space from which we extracted the axioms that define a matriarch. Okay, now there's, you can go from linear independence to algebraic independence. That's what Mandela was looking at. So an algebraic matriarch, by the way, all the references, as I said, you know, all the sources that I use, I usually put right in the beginning of the topic. Right? So there's wonderful lecture notes out there. So my job is to go and pick little bits from each one that sort of follow the flow of my lecture. So that way you can go and look up some more details that they might be giving.
01:05:42.264 - 01:06:27.302, Speaker A: So you can think of, you can define this notion of dependence or independence from going to algebraic independence. So this relates to this notion of genericity that we're going to talk about in a bit. So if you have a field extension. So in this particular example, we're going to take the field to be so, q and these are the polynomials over. So it's rationals, the polynomials with rational coefficients in two variables. And we consider the ground set given by the following polynomials. So the ground set of these polynomials.
01:06:27.302 - 01:07:12.612, Speaker A: Now we can think of the dependent and independent subsets in the. I have to decide which order to do these things, you know, so it's algebraically independent if there is no non zero polynomial such that p with alpha one, alpha two. So some other polynomial of these polynomials that is identically zero. Okay, so if that happens, then these are said to be algebraically dependent. If it doesn't happen. So we're gonna, this is the notion of ideal, which we're gonna talk about very quickly, but very soon in the next slide or something. But next two slides.
01:07:12.612 - 01:07:58.088, Speaker A: But this tells you what algebra. So you can look here and verify for yourself that the independent subsets of this set, this ground set are going to be, are given here. And as I said, there are many axiomatic, different types of axiomatic definitions of metroids. And so in this book, this is a book, book by Rosen, he has actually specified many examples where all the different types of bases, sorry, different types of axioms that are used to define metroids, the relevant objects have also been defined. Okay, doctor? Yes. So when you have that independent set, one, comma, two. Yeah.
01:07:58.088 - 01:08:26.668, Speaker A: Is that saying that there's no polynomial? No polynomial, yes. Whose coefficients are alpha one, alpha two. No, no. So you can think of a polynomial of these polynomials. Okay. Yeah. So these are algebraically independent.
01:08:26.668 - 01:08:45.084, Speaker A: If there's no polynomial of these, that is equal to zero. So p is in this extension where k is q. Right. So k would be rational coefficients with rational coefficients.
01:08:47.584 - 01:08:56.876, Speaker D: Okay. Is e here, k extended by a one through a four, or is it.
01:08:56.900 - 01:09:02.468, Speaker A: Just, there is four, a one, alpha one through alpha four.
01:09:02.556 - 01:09:06.732, Speaker D: Yeah, yeah, sorry. Alpha one, yes.
01:09:06.868 - 01:09:08.068, Speaker C: He is the finite set.
01:09:08.116 - 01:09:09.544, Speaker A: Yeah, yeah.
01:09:10.044 - 01:09:13.076, Speaker D: Okay. Yeah, thank you.
01:09:13.260 - 01:09:21.064, Speaker A: We're always every, I mean, we're never going to look at the infinite. I mean, whenever we are talking about matrix, our ground set is always finite.
01:09:21.364 - 01:09:22.454, Speaker D: Okay, thank you.
01:09:22.564 - 01:10:22.280, Speaker A: Yeah. So, so what types of polynomials? Okay, I think we asked this question two lectures ago. So we have certainly seen the distance polynomials, the quadratics, the simple quadratics. We have seen the Kalimanger polynomials, those, in our discussion so far, we have seen these two types of polynomials. One is quadratic, one is not quadratic, but so what? So one of them, and will already pointed out, you know, the Kalimangers are ccgs of the distance polynomials. So, so they're polynomials in the polynomials in this polynomial, so to speak. And we can, we have already talked sufficiently about the dependences between the Caylemanger relationship, and we now talked about dependences between the distance polynomials.
01:10:22.280 - 01:11:34.638, Speaker A: So we have seen, and, you know, correspondingly, there is a notion now because of metroids, there's a notion of a dependence of an edge of a graph in the ama, on the other edges of the graph, in the tree, decomposable graph. If you add that extra edge to give the d plus one lateration graphs, that extra edge is now correspondingly dependent. Okay? And so I haven't told you in what sense what the new meteoroid is over the graph by which you get that dependence. But you can imagine that that's the case. And I believe Tony might have talked about this in the sense of genetic rigidity metroids. Okay, so we'll come, when we come, by the time we come to the generic rigidity metroids, we would have already gone through some of the other things about polynomial systems and we would have finished this tour of techniques before we actually go into those. So again, for polynomial systems, what do we care about? Again, we care about the existence, whether it's zero dimensional, is the solution unique and under what assumptions on the polynomial systems.
01:11:34.638 - 01:12:32.184, Speaker A: So we think about what are called generic polynomial systems. Very briefly, this would mean that the coefficients of the polynomial systems are not the zeros of some polynomial. So the coefficients, which are just elements of the rationals or reals or whatever, are in our examples that we have seen so far, the coefficients are themselves just reals or rationals so that they don't form the zeros. They are not the zeros of some other polynomial. Okay, so that's what we mean by generic. And then you have non generic questions that you can ask about without having the genericity assumption that you have a specific polynomial system and you want to know about the solutions to that, whether they exist. They are zero dimensional, unique, etcetera.
01:12:32.184 - 01:13:36.684, Speaker A: So you could ask two types of questions, generic questions, and not general questions which are not necessarily under the genericity assumption. Okay, so now we, the, you know, to complete the tour of techniques, the last one before we start going into, deeper into the different topics is going to be solving polynomial systems. Okay? So the key sort of things that we're gonna visit, um, over the complex numbers and again over the real numbers are, you know, some things like this. Okay, so first is Bezoot's theorem. And this is taken directly from the Wikipedia page on Bezot's theorem. It's simply what you would expect, which you learn with when you talk about linear systems all the time. You look at the number of polynomials and how many number of equations and number of variables, and then you say something about the number of solutions.
01:13:36.684 - 01:14:51.466, Speaker A: So in the original form, the theorem basically states that in general, what does in general means generically, the number of zeros equals the product of the degrees of the polynomials. And so if you have n polynomials and n determinants in certain degrees, so the number of zero is equal to the products of the degrees of the polynomials. Okay, so, and that's generically speaking. So it's not always the case depending on the particular polynomial. So in the modern formulation, it's saying that if you have an projective hyper surfaces and there's an algebraically closed field, so we're talking about the complex numbers as far as we are concerned, and you define their homogeneous polynomials in n plus one indeterminates, then either you have infinitely many points. So in other words, these hyper surfaces meet on an infinite dimensional solution, or if it's finite, it's equal to the products of the degrees of the polynomials. And moreover, the meeting points of these are not higher dimensional.
01:14:51.466 - 01:15:59.814, Speaker A: In general, genetically it's always finite. Okay, so it's like saying that in general, if you take two linear, I mean, it's very rare that you end up with, it's measure zero that you end up with dependent equations and with the right number. And that's what this is saying. So, Bezut's theorem is fundamental in computer algebra because it shows, and it shows that most problems have computational complexity that's at least exponential in the number of variables. So essentially, it's saying that because you have, that sort of gives you a lower bound on what you can possibly expect. If you want to find all the solutions, you're going to end up with some exponentially many, since there are exponentially many possible solutions. So essentially, you will end up running your algorithm, whatever it is, in order to find all the solutions is going to be an exponential time.
01:15:59.814 - 01:16:53.320, Speaker A: So that sort of gives you what to expect. So when you're solving polynomial systems and you want to find all the solutions, you're going to end up with very algorithms that run for a long time. So two of the common, usual suspect objects that you have to think about when you're solving polynomial systems are varieties and ideals. There's a nice book by Cox, it's called ideals and varieties, and it has very computational treatment of these things. And it's. I would really recommend looking at that, but I'm going to take a bunch of notes and slides, actually, from Pablo Parillo, who is one of the organizers of this thematic program. This is actually from Wikipedia.
01:16:53.320 - 01:17:28.850, Speaker A: As you can see, an algebraic variety in the usual simple way of thinking about things, classical way of thinking about things, is that it's just a system of solutions, a set of solutions to a system of polynomial equations over the real or complex numbers. Usually they mean complex numbers. So we can, you know, sort of think about. This is the easiest way to think about varieties is simply the solutions. The set of solutions to system of polynomial system equation. So these are two examples here. I don't know if you see this or it's too small.
01:17:28.850 - 01:18:02.542, Speaker A: So on the left is defined by the equation x squared plus y squared minus one times three. X plus six. Y minus four equals zero. And you can see that taking the product of two polynomials and setting that to be equal to zero is the same as saying that both of. So at least one of them has to be zero. So at least one of the two polynomials has to be zero. And the first polynomial represents the circle, because it says circle of radius one.
01:18:02.542 - 01:18:44.480, Speaker A: And the second polynomial represents this line. So essentially, this union of these two is exactly the set of all xy values such that the product is equal to zero. So the product is equal to zero if either one of them is equal to zero. So that gives you the union and the one on the right is the quadric surface defined by one minus x squared minus y squared minus two. X squared plus z to the four equals zero. So they can look really weird. You can see that this is, you can factor this and see that, you know, convince yourself of that.
01:18:44.480 - 01:19:59.434, Speaker A: And you can, with Mathematica or maple or whatever, you can even Matlab, you can draw a lot of these that are solutions to simple polynomial systems. So formally, if you fine algebraic varieties, the zero set of a finite collection of polynomials, and this is defined as follows. So if you have polynomials in over the complex numbers, the variety is the set of all elements such that f of, you know, those, if you plug those in for the variables, this equal to zero for every one of these f's, that's called the variety defined by this set of polynomials. Okay, so this is related to the notion of ideals. And so essentially it's an ideal again, defined by a set of polynomials. A variety is defined by a set of polynomials. An ideal is also defined by a set of polynomials, saying that if an ideal in general is that if something belongs to the ideal, so zero is in the ideal, and if two elements belong in the ideal, then their sum is in the ideal.
01:19:59.434 - 01:21:00.044, Speaker A: And if an element is in the ideal and you have b over another polynomial, so this is a polynomial, this is another polynomial, then their product is also in the ideal. So you can take any polynomial combination of polynomials, and that will be in the ideal. So for our purposes, the important examples of polynomial ideals are the ones that vanish on a given set. So that's going to be all of the polynomials such that vanish on the zeros of a given set of polynomials, right? So that's going to be one of the key things that we're going to care about. This is called the vanishing ideal of s. S is a set, and typically that set is taken to be the roots of another set of polynomials, and the ideal generated by a finite set of polynomials, which is simply the sum, the polynomial combination, some polynomial combination of the given set of polynomials. That's what this is.
01:21:00.044 - 01:22:11.966, Speaker A: So there's definitions of principal ideals and finitely generated ideals, which you can look up, okay, so there's an important theorem called the Hilbert basis theorem that says basically that every polynomial ideal is in fact finitely generated, which means that you can, so the definition of finitely generated means that you can write this down as, as using a finite set of polynomials. So if you have an ideal, you can always write it down using a finite set of polynomials like this. So every polynomial ideal is in fact finitely generated. And we are going to use grobna basis, which is one form of the generators of an ideal and which makes it very convenient. As I said, it's related to triangularizing polynomial systems. I mean, it's the analogy in polynomial case of triangularizing linear system. So think of it as a gaussian elimination type procedure.
01:22:11.966 - 01:23:14.628, Speaker A: So that gives you a nice sort of triangular basis that generates the given ideal. And so now the question becomes, okay, given a polynomial p and a bunch of other polynomials that determine an ideal. So you're given p and you're given a bunch of f one to f, and you want to see if p can be written as a polynomial combination of f one through fm. So these are the two important questions and very difficult questions, and they're closely related to finding solutions of polynomial systems. Okay, so Hilbert null stellenzaerts actually connects ideals and varieties, at least to the extent that we care about them. So given a finite set of polynomials, we know how to generate the ideal, and we also know how to look at the corresponding variety. So variety is the set of places where they all manage.
01:23:14.628 - 01:24:01.194, Speaker A: The ideal is all the other polynomials which can be written as polynomial combinations of these. And one direction is very easy to see is that if f one through f and f one through fn minus. Well, okay, it's saying here. So how do these ideals relate to each other? If you look at the vanishing ideal of this, which is all the ones that vanish. Exactly when all of these polynomials vanish. And how is that related to the ideal that's generated as polynomial combinations of these polynomials? Turns out that they're exactly the same. So it's saying that if you look at all polynomials that are polynomial combinations of these polynomials, that's going to be exactly equal to the set of polynomials that vanish.
01:24:01.194 - 01:24:09.770, Speaker A: Exactly when all of these polynomials vanish. Okay, not exactly. So they vanish whenever all of these polynomials vanish.
01:24:09.962 - 01:24:12.414, Speaker C: Need some kind of radical assumption here.
01:24:13.874 - 01:24:14.974, Speaker A: Say that again.
01:24:15.314 - 01:24:49.084, Speaker C: I think we need that the ideal generated by f one through fs is radical if you want this. If you take an ideal f one through fs and look at a zero set, and then look at the set of all polynomials that vanish on that ideal. It's in general, not the original ideal. It's the radical of the original original ideal. So if a poly, if a power of a polynomial, for instance, vanished.
01:24:49.744 - 01:24:50.384, Speaker A: Okay. Yeah.
01:24:50.424 - 01:24:54.360, Speaker C: So I think this is just a tiny caveat.
01:24:54.432 - 01:24:55.844, Speaker A: Yeah. Yes, yes, yes.
01:24:57.944 - 01:25:03.736, Speaker E: So x square, for example, generates all polynomials that have a double root.
01:25:03.880 - 01:25:30.244, Speaker A: Yes, yes. I'm glossing over a few things here. Okay, so this is, this equality. One direction is very easy to see. We pretty much already talked about it. But the other direction is basically the theorem called Hilbert Nolstellen Zaarz. And I'm giving you a weak version of this from Wikipedia.
01:25:30.244 - 01:27:05.004, Speaker A: So you say then ideal contains the constant polynomial one if and only if the polynomials do not have common zeros, any common zeroes in this kn? So it's another way to formulate it is I is the proper ideal in k one to kn, then v of I cannot be empty. I is there exists a common zero for all the polynomials in the ideals in every algebraically closed extension. And we are important to note that this Hilbert's nolstelensatz, even the Weaknesstelenzaertz, requires this algebraic closeness of this field here. Okay, so, you know, a counterexample, if you're looking over the reals, you know, is this case. Okay, so one thing that we want to, for those of you here who are used to combinatorial, I mean, sorry, linear programming and optimization, I know there are several of you there. One of the key things that I want to show an analogy that runs through the whole, the whole next few slides is going to be this idea of duality. So in usually when you finding, you're trying to see if there exists solutions, and you say that the set of solutions is empty if and only if you can find lambda and v here.
01:27:05.004 - 01:28:18.004, Speaker A: What was that noise? I hope I'm not losing power. No, everything looks fine. So essentially, this is saying that if saying that there is no solution to a system of inequalities and equalities, let's say in linear programming duality, this is essentially saying that the dual program, so there is a lambda and v such that this is satisfied. Okay, so those of you who are used to linear programming, this is one of the most common things that you use all the time. So, showing no solution exists is equivalent to saying that some solution exists to another system. So the Hilbert Nordst can be thought of as at least the Weaknesstelenzatz, to the extent that we are going to use it, can be thought of as a similar sort of statement. It's saying that the set of solutions to polynomial system, complex solutions to a polynomial system, f going from one to m or whatever, of z, is equal to zero.
01:28:18.004 - 01:29:00.994, Speaker A: That set of solutions is empty if and only if the constant polynomial is in the ideal generated by those polynomials. Right? So one direction is, again, as we talked about, one direction. This is essentially a restatement of this statement here. Right. One direction is kind of straightforward. If, you know, if one is in the ideal, then you could not have written, since one cannot be written as if they vanish if they have a common zero, you could not write one as a polynomial combination of them. So the other direction is the hard part.
01:29:00.994 - 01:29:48.612, Speaker A: So this is also the same as saying, so one is in the ideal. And by the definition, I mean by the equality of the vanishing ideal. And the ideal is the just saying that there are some multiplying polynomials such that you can demonstrate basically that one is in the ideal by writing one as this polynomial combination of the original polynomials. So the existence, the non existence of a solution comes down to finding the existence of these multiplying polynomials such that this adds up to one. So that's that duality, that this same similar, sort of showing non existence of a solution comes down to finding this. And you have a similar situation here. Okay.
01:29:48.612 - 01:30:38.814, Speaker A: And so I'll talk about complexity and all that a little bit later. Okay. So in this context, you can think of this job of finding these qis, this multiplying polynomial, so to speak, such that this is equal to one. If you want to establish that this is, there is no solution. One of the ways to do that is to use these rhobases. In other words, you are going to take your set of polynomials and massage them into a form so that finding this becomes easy. Okay.
01:30:38.814 - 01:31:13.556, Speaker A: And this is a sort of an expository top article written by Bern Stonefalls in, I believe, American Mathematical Monthly, where he talks, and there are many others, by the way. In fact, I was going to recommend John Perry slides from University of Southern Mississippi. I didn't put the link here. The reason I didn't put the link is because if you click the link, it'll say it's a bad link, get out of here, and all that stuff. It's insecure and so forth. But I actually have that link here. I'm going to.
01:31:13.556 - 01:32:00.594, Speaker A: And he has a very nice set of slides that actually go through the whole situation. Gaussian elimination for linear polynomials. What exactly do you do? You know, you sort of find a type of greatest common divisor to do the triangularization, and he does the whole parallel development of the whole gaussian elimination and gcds for polynomial systems. And. And that sort of. I find that to be a very good source to get examples to do this. Okay.
01:32:00.594 - 01:33:18.030, Speaker A: So today I'm not going to do that. As I said, we're still in this tour, and today I hope to finish this tour so that we can start going into the details of the different topics as we. And so I don't want to go into the detail of a grobnobases example, which we will definitely do. But I want to quickly talk about what do we do with all of these things over the reals, because, remember, we started out with having real symmetric matrices that represent the distances, and we want real solutions to the polynomial that represent either the Kalimanger equalities or the polynomials, the distance polynomials. So all of this stuff that we have talked about over the last three or four slides, or maybe even six or seven slides, with the ideals and varieties and everything, is over the complex numbers, algebraically closed, Hilbert, Schultz was algebraically closed, and so forth. But we care about real solutions. We want to ask whether there is a real solution to a system of polynomial equations, not whether there's a complex solution.
01:33:18.030 - 01:34:42.194, Speaker A: So this weakness, tell and z that we had, you know, we need to get a form of it that we can actually use for solving real solving for real solutions. So, in other words, we have, in the linear case, and in the nonlinear case, we have, you know, we have LP duality. If you have linear systems, inequalities and so forth. And in the polynomial, the complex, we have no shell and z. And the question is, what can we do over the reals? If you have polynomial systems over there and we want real solutions, what do we do? Okay, so one of the things that, you know, you might expect when you're trying to find real solutions is you have discriminants, right? So if you just have a single quadratic in univariate quadratic to check whether it has a real solution, you have a discriminant polynomial in the coefficients of the original polynomial, and you're going to check whether that's non zero. So checking inequalities is sort of fundamental or integral and almost equivalent when you want to find real solutions. So you know whether you're finding real solutions or whether you're dealing with inequalities and equalities, polynomial equalities and inequalities, essentially the problems or the complexity is more or less the same.
01:34:42.194 - 01:35:43.154, Speaker A: So this basic problem, as if you're given a polynomial and you want to check whether it's non negative everywhere there's a very basic problem that's going to come up when we're trying to find real solutions, is how do you determine that? You know, you can do all kinds of quantifier elimination and all that, which takes forever, but we want to sort of see whether there's a simple way to do this. And there's a simple sufficient condition for f to be always non negative, is that it's what is called the sum of squares. This is a very key technique, and just be patient. It seems that I'm going all over the place, but this is going to make full circle and connect it to something that we've seen before. So that's why I want to go through this today. We're already at 1138. Okay? So we can, in order to check whether a polynomial is non negative everywhere, we're going to have to do things like this when we want to find real solutions.
01:35:43.154 - 01:36:23.304, Speaker A: One of the key ways you can do this is a sufficient condition. It's not often necessary. Check whether it can be written as a sum of squares. And it turns out that almost all cases, it's actually necessary. There are, of course, counterexamples. Okay, now can we compute this sum of squares decomposition, so to speak, efficiently? And we come to something very familiar that we have already seen, the positive semi definite cone. So the pleiadian distance cone, which we saw, which is the negative semi definite cone, but the corresponding gram matrix cone cone is exactly the positive semi definite cone.
01:36:23.304 - 01:37:12.576, Speaker A: So what we can, in order to determine, so just recap of what a semi definite program is. Semi definite program is simply that you want to find, given some symmetric matrices, AI, and you have the trace of aix to be equal to bi. And this is given that x has to be, you know, positive semi definite. And so the question is to find whether it's feasible. Feasible means you want to find whether there exists, whether this thing is non empty. Okay, so this is basically the intersection of an affine space and the self dual convex cone of positive semi definite matrices. So it's a broad generalization of linear programming.
01:37:12.576 - 01:37:58.644, Speaker A: And it has nice duality theory because things are convex. You can use all the tricks and trade tricks of the trade from linear programming, the duality that we talked about and all of these things. And essentially, since there is no rank condition, there's nothing that you're saying anything about in ranks of any matrices. It's essentially solvable in polynomial time. Okay, so because there's no rank. So, so it's like saying that, I am only saying that I only have the positive semi definitely condition. So in other words, I want to find a realization in some dimension, does there exist a realization of which the given matrix is a gram matrix in some dimension, right? So basic method, which is called the gram matrix method.
01:37:58.644 - 01:38:44.734, Speaker A: So given f, you can determine that. So z is a suitably chosen vector of monomials. In the dense case, um, all the monomials you're going to need. Remember, we're trying to get an so's or sum of squares, expansion of a given polynomial to determine whether it's non negative. So you just write, so you just, you can say that f is sum of squares if and only if f of s can be written in this fashion. And it can, you can solve it using a semi definite program. And where Cube, well, q can be factorized as L transpose L, because you have, you know, if it's written as a sum of squares, you're going to have non negative eigenvalues.
01:38:44.734 - 01:39:13.880, Speaker A: So you can, essentially, you can write q as L transpose L, just like, remember, we did the whole proof right in the beginning of Schoenberg's theorem. And all those ideas are here. Okay, so here's an example. F of x, f of x, y is in this way, you can write it in this. You want to check whether it's the sum of squares polynomial. You simply do the semi definite program. You just say that it can be fact.
01:39:13.880 - 01:39:56.814, Speaker A: You know, you're looking to see whether there's a PSD matrix here. So, Q has to be PSD. And so, in order to solve for it, you just solve the semi definite program that requires that this has non negative eigenvalues or singular values, if you will. And then you just find l using SDP. And then that gives you the actual expansion of f as a sum of squares polynomial. So, now, going back to the neuschtellenzatz, we want to have a question. We now are asking the question whether it has a system of, whether it's a system of inequalities and equalities polynomials, has real solutions.
01:39:56.814 - 01:41:09.174, Speaker A: And the answer is something called positive and Stellensatz. And it's a common generalization of Hilbert schneulensatz, or at least the weak version and LP duality. And this idea of infeasibility certificates is just what we talked about, which is that in order to show that something does not have a solution, you sort of show the existence of these multiplying polynomials. So these are called infeasibility certificates, multiplying polynomials so that one that demonstrate that one is in the ideal, in the case of Hilbert Schulz, that is a similar idea here. Okay, so, positive and Stalenz simply says that given you know this polynomial system, where you're looking to find whether there are real solutions to it, you define the cone of fi as the sum over si times the products of these guys. And the ideal of hi to be the sum over ti, where in particular, these multiplying polynomials are sums of squares. So these poly, they have to have that sum of square condition.
01:41:09.174 - 01:41:49.706, Speaker A: So, to show that this is zero or does not exist, or empty. Sorry, the set is empty. So essentially, we find f in this cone and h here, such that f plus h is negative one. So this is the analogous version of Hilbert Schultz. And that's right. So this allows us to get certificates. So that duality that we had that whole between in the case which we had for linear programming and which we had for complex or algebraically closed fields.
01:41:49.706 - 01:42:27.880, Speaker A: And answering this question, what can we do for polynomial, I mean, real cases, whether there exists a solution, whether we have a similar duality. And that's what we have here. So it says that there is a. We have real solutions to polynomial equations and inequalities, or do not have is empty if and only if you have these f and h, where f plus h is negative one. F is in the cone and h is in the idea. And you remember that in the definition of the cone, we had these Si's, which had to be sum of squares. And checking sum of squares is just an SDP problem.
01:42:27.880 - 01:43:17.574, Speaker A: So the set of emptiness proofs is convex. So the key thing is that if we had bounded degree, then we can find the certificates, which is these guys, by just solving semi definite programs. So I think that sort of completes our tour. We've done graphs, metroid polynomial systems, you know, over the reels, over the complexes, and the importance, I mean, Kelly, Menger, Schoenberg. The key things about the maps between point configurations and distances, and the things to watch out for. I think we have completed our tour. So what we're going to do next, starting from Thursday, is go into each one of these topics in detail.
01:43:17.574 - 01:43:34.774, Speaker A: Okay, I'll stop here and take questions. Sorry, I raced through the last bit, but I wanted to. I didn't want to go on forever with this tour.
01:43:37.484 - 01:43:47.956, Speaker C: Thank you, Mira, you. Excuse me, mira, you mentioned the woo writ method.
01:43:48.100 - 01:43:53.700, Speaker A: Yes, I have. We will do that. Yeah. We will do broader basis and woo writ in some detail. Yeah.
01:43:53.732 - 01:43:54.428, Speaker C: Okay, excellent.
01:43:54.476 - 01:43:55.012, Speaker A: Thanks.
01:43:55.148 - 01:43:57.304, Speaker C: Yeah, I'm curious about that.
01:44:01.024 - 01:44:17.444, Speaker A: Let me see what there are any questions for me on the chat. Good place to learn about the topics. One is some combination of the ideal certificate of infeasibility. Good.
01:44:23.264 - 01:44:35.576, Speaker E: Hi, Mira, I have a question. Yeah, so, a question about the projection of the. Of the Ed apps, right?
01:44:35.720 - 01:44:43.964, Speaker A: Okay. Yes, that's going to be probably the first topic I'm going to spend get into deeply.
01:44:45.184 - 01:44:45.728, Speaker E: Sorry.
01:44:45.816 - 01:44:50.884, Speaker A: Now that we've finished our tour. Okay, go ahead, ask me the question. Here's the projection picture.
01:44:52.564 - 01:45:10.284, Speaker E: Yes. So you project, suppose you have a graph, e, and this, and then you have a so. So, and then this graph has a dimension e minus k in the rigidity matrix.
01:45:10.404 - 01:45:11.060, Speaker A: Yeah.
01:45:11.212 - 01:45:12.364, Speaker E: In d dimensional.
01:45:12.524 - 01:45:13.264, Speaker A: Yeah.
01:45:13.884 - 01:45:18.890, Speaker E: So, so there are kids edges, which.
01:45:18.922 - 01:45:22.146, Speaker A: Are dependent on the remaining edges. Yeah.
01:45:22.250 - 01:45:26.762, Speaker E: Yeah. So the projection will, will be minus k dimension.
01:45:26.818 - 01:45:27.854, Speaker A: Right, right.
01:45:28.674 - 01:45:38.174, Speaker E: But my question is, are these, uh, is this set of a minus k dimension, uh, like affine space?
01:45:39.994 - 01:45:44.014, Speaker A: Um, that's the e minus.
01:45:44.344 - 01:45:49.804, Speaker E: So they are a subset of the linear or fan space.
01:45:52.184 - 01:46:29.644, Speaker A: No, it's not. Absolutely not. It's disconnected. And what, I mean, here's what it would look like. Maybe I should draw a picture. So first of all, I think you people, I probably have to explain to people what you're asking. Okay, so what he's asking is, okay, here's our euclidean distance cone.
01:46:29.644 - 01:47:09.796, Speaker A: This is the best way I can, I know how to draw it. And you know, as you know, the coordinates of this thing are the edge, I mean, are the pairwise distances of the point configuration of some point configuration. So if you look at these are all the pairwise distance vectors of all possible real point configurations. And so what he's saying is. So these coordinates represent, you know, pairs, really? So, or these are elements of your entries of your matrix. That's what, those are the coordinates here. And the entries are as, you know, edges of a graph.
01:47:09.796 - 01:48:02.300, Speaker A: So if you give me a graph, I have a sort of coordinate subspace. And what he's doing is he's taking the, and if you remember, the d stratum of this cone corresponds to all the point configurations that live in. So p one through pn, where each PI lives in rd. So the image of all of those under the distance map, that's our d stratum. And what he's saying is I'm projecting the d stratum on g. So if I take the projection of the d stratum on g in that previous picture in the slides, you know, I had drawn it. So he's.
01:48:02.300 - 01:49:04.024, Speaker A: Okay, so the dimension of this thing, this projection, the dimension of the projection on g of the d stratum is exactly. Whoops. Is exactly, I mean, can be shown, and we'll show it that this is the rank of g in the generic rigidity metroid. Okay, so rank of G in the d dimensional, generic rigidity metroid. So that's going to be the dimension of this projection. And if, and he called g, let e be the set of edges. And let's say, and you have seen in Tony's class, you know, essentially you have rows of them, some set of e.
01:49:04.024 - 01:49:44.384, Speaker A: The edges of e determine the rows of the rigidity matrix. And let's just say this is the rigidity, generic rigidity matrix. So these are indeterminates. So, and he's saying that, you know, the rank of g is going to be the number of independent rows. So if there was, let's say, k rows that were dependent, then the dimension of this projection is going to be whatever e is minus k. So e minus k is this rank, right. So now he's asking, what does it look like? What does this projection look like? Right.
01:49:44.384 - 01:50:12.320, Speaker A: So the projection is typically, I mean, it has the right dimension, it's got a certain dimension. And if it happens to be independent, so if k is zero, there are no dependent edges. If k is zero, then this will be full dimensional. So it will be size of e. So if there are e, so the coordinate subspace has e coordinates. And so this is full dimensional. So it's using up the full dimension of this coordinate subspace.
01:50:12.320 - 01:50:50.010, Speaker A: It, however, it's far from convex. Not convex. We'll have all kinds of gaps and stuff in it. And it's definitely not, you know, I don't know, it's definitely not a single affine subspace or anything like that. Okay, so this is a pretty ugly object. The only thing we know is that it's got little neighborhoods that have the full dimension if it had been independent. If it is not independent, then this won't have the full dimension.
01:50:50.010 - 01:50:51.618, Speaker A: It will only have dimension e minus.
01:50:51.666 - 01:50:56.722, Speaker B: K. Okay, mira, is it a semi algebraic set?
01:50:56.898 - 01:51:20.314, Speaker A: The projection, yes, because the projection is all the possible distances for edge lengths for the edges in b dimensions. Right? Yeah. So that's, you know, you can define that using your Klemanger equalities and inequalities and yes, it's a semi algebraic set.
01:51:21.254 - 01:51:24.734, Speaker B: Okay, thanks. This picture is really helpful for me too.
01:51:24.894 - 01:51:25.714, Speaker A: Okay.
01:51:27.054 - 01:51:37.710, Speaker E: Okay. Yeah. So basically you cannot find affine space of e minus k dimension that can turn this set, right? You can, you can't, you cannot, uh, that contains it.
01:51:37.742 - 01:51:40.878, Speaker A: Yes, of course. The entire coordinate subspace contains it.
01:51:41.046 - 01:51:42.990, Speaker E: Yeah. I mean a subspace of e minus.
01:51:43.022 - 01:51:52.982, Speaker A: K for that dimension. Um, no, offhand, no.
01:51:53.118 - 01:51:55.662, Speaker E: Yeah, but they are these.
01:51:55.758 - 01:52:01.130, Speaker A: You mean if k is not equal to zero, if k is equal to zero, then the entire coordinates are space contains it.
01:52:01.162 - 01:52:04.442, Speaker E: Yeah. If k is not zero, then you cannot have.
01:52:04.618 - 01:52:05.602, Speaker A: Okay, but.
01:52:05.618 - 01:52:08.014, Speaker E: But they are connected. But it's connected, right.
01:52:08.634 - 01:52:38.314, Speaker A: It's connected because it always contains zero. So that means you can make all the edges zero. So you can go from anywhere to anywhere through the origin. But of course, if you take. Typically what they look at is, you know, instead of looking at the entire cone, you sort of look at the slice of the cone given by the sum of the edges equals one or something like that, you know, then it won't be connected. Not necessarily connected.
01:52:40.054 - 01:52:42.354, Speaker E: Sorry, what?
01:52:45.654 - 01:53:05.684, Speaker A: Yeah, that's a good question, actually. So basically, often people, they don't look at the entire. If you look at the entire cone, it's connected. Right. So you can go always to the origin and back. Any d stratum is connected. Maybe I'm answering a question you didn't ask.
01:53:07.104 - 01:53:09.760, Speaker E: And projection is also connected, right?
01:53:09.792 - 01:53:11.804, Speaker A: Yes. Projection is also connected. Yeah.
01:53:12.944 - 01:53:17.124, Speaker E: That means if you move, like when you can, you can.
01:53:17.624 - 01:53:21.244, Speaker A: You can go from anywhere to anywhere in a continuous path. Yes.
01:53:21.584 - 01:53:23.204, Speaker B: But if you continue.
01:53:23.584 - 01:53:23.872, Speaker A: Yeah.
01:53:23.888 - 01:53:25.390, Speaker E: The minuscule parameters. Right.
01:53:25.512 - 01:53:26.954, Speaker A: You can always go to the origin.
01:53:27.034 - 01:53:28.114, Speaker E: Yeah, yeah.
01:53:28.274 - 01:53:34.134, Speaker B: But if you remove the origin, if you just remove the zero matrix, it's not connected.
01:53:34.634 - 01:53:40.694, Speaker A: Upstairs, it's still connected. Downstairs it may not be.
01:53:42.674 - 01:53:44.578, Speaker B: Oh, okay. That's interesting.
01:53:44.746 - 01:53:57.564, Speaker A: Yeah. So. Well, actually. Well, let's talk about that later. It's confusing to me. I have to think about it. Okay.
01:53:57.564 - 01:54:10.164, Speaker A: It's certainly not convex, that's for sure. Even with the origin.
01:54:11.744 - 01:54:15.084, Speaker B: Is there an easy way to see why it's not convex?
01:54:17.284 - 01:54:55.484, Speaker A: For some graphs it is, but. And those graphs are called the deflattenable graphs. But because for those graphs, what you can show. Here's what you can show. You can show that if there's a graph for which the projection of the d stratum is convex, then in fact, it's the projection of the whole thing, the entire cone. So the projection of the d stratum coincides with the projection of the entire cone. So what that means is it says that whenever there is a euclidean realization of this graph in any dimension, it always has a euclidean realization in d dimensions.
01:54:58.024 - 01:54:59.440, Speaker B: That's really interesting.
01:54:59.592 - 01:55:23.804, Speaker A: Yeah. Okay, so the first. This is probably the first topic we will do. I haven't decided how I want to order the different. I mean, we finished the tour of topics and I'm going to go. And we've given some of the basic theorems and proved some of the basic theorem. And now I want to go into a little more detail into each one of the topics separately.
01:55:23.804 - 01:56:18.944, Speaker A: That doesn't mean that they're going to be completely disconnected from the rest. I'm still going to sort of point out connections, but I have not decided what order to do this in, especially because I don't know has Tony. It'll be good for if you guys in the rigidity class have finished things about the d dimensional rigidity matriarch and all that stuff, so that I can talk about these projections and other things about that. So maybe what I first do is solving polynomial systems that arise from these distance equations and maybe generalize it a little bit to other constraints besides distance constraints. And we talk about that first before we go here and talk about projections of the cone and things like that. I haven't decided what order I'm going to do the details in.
