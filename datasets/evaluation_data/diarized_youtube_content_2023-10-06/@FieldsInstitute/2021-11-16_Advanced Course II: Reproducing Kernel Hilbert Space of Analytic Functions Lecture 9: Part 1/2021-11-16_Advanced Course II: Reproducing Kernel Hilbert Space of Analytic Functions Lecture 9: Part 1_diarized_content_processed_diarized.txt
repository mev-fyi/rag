00:00:00.120 - 00:00:45.480, Speaker A: No way. Today, close. Hello everybody. To continue, we need to talk a little bit about a construction which is called tensor product in ordinary linear algebra courses, in particular linear algebra one, which I mean, I gave a couple of times at Laval. We do not talk about tensor product. It comes a bit later in more advanced courses. And here in the book they just say tensor product and assume that we are familiar with its property, since it is not that obvious and we need to develop, I mean, the whole construction.
00:00:45.480 - 00:01:13.134, Speaker A: In order to understand, I decided to talk a bit about tensor product. So here is the setting. We have two inner products. Let me denote them by u and v. Sorry, two complex vector spaces. Inner products comes later, complex vector spaces. We all know what is u cartesian product of v.
00:01:13.134 - 00:02:07.680, Speaker A: This is okay. And what we want to study today is u tensor product with v. And here, here is the construction. Just let me give a bit of motivation. When we are with the cartesian product and we have one element, say x and y in this space in u times v, and we multiply by a scalar alpha times x and y, we know that scalar goes with both components. It's alpha x and alpha Y. But there is need a kind of construction such that, I mean to not mix with the pair x and y.
00:02:07.680 - 00:03:36.414, Speaker A: I mean, I write this, but I will explain later what it means. We need to define this object such that when we multiply by alpha, alpha goes to one of the components, say alpha x tends for y or x tends alpha Y. Not to both of them. We need something like this. And how can we obtain something like that? Here is the way we do it. First, we construct a very, very huge space. The space X is a complex vector space whose Hamel basis is this complex space whose Hamel basis is the set of all symbols, is symbols, a small u, small v, such that u runs over u and v runs over v.
00:03:36.414 - 00:04:52.472, Speaker A: In other words, every element in X, every x in X has a representation of this form. X is equal to a finite number of combinations, eu one v one eu two v two, up to eu and vn. These are elements of the Hamilton basis. So we multiply by scalars alpha one times this, alpha two times that, and alpha n times the last one. Absolute freedom for the choice of alphas, the coefficient, and also no restriction on u k and v k. I mean, the components can be any element of u, the pq, or any element of the bq. And note that this space x is really huge, even in the smallest possible case that we may consider, even in the case u equal to v equal to r the real one.
00:04:52.472 - 00:05:35.194, Speaker A: I mean we cannot go something less than that. And in this case u times v is the Cartesian, is the complex plane like this. And for any point here, any point in the plane is a different point of hammer basis. Be careful. E zero, zero is not zero is an element of the basis and the same as e eleven. And there is absolutely no relation between e eleven and e 22. E 22 is not twice e eleven, this is not two y eleven.
00:05:35.194 - 00:06:34.574, Speaker A: Each one of these points act as an element of the Hamel basis. So there are really many, many of them even in this case. And now to go and realize this goal, we need to somehow eliminate some of these elements. And the way to do this, how do we eliminate elements? In vector spaces we have x and somehow we find a subspace. And then we consider the quotient space xr. R is a subspace. When we do this, this is equivalent to say that we consider all of the elements of r to be as zero.
00:06:34.574 - 00:07:32.654, Speaker A: That's the meaning of elimination. Pictorial x is like that and I mean you can imagine r like this. And then we consider all the cosets and instead of considering elements like this, we consider each cell as one element. So one element now is a cell. So for an x given here, this cell is either denoted by bracket x or sometimes x plus r. In some books they write x r many different notations. But the whole idea is that we consider the cell of x, the coset created by x as one element, and depending on different r's we create different cell and different vector spaces.
00:07:32.654 - 00:09:29.836, Speaker A: Now what is a good r here to realize our goal, r is defined by this. Here is r is the subspace so r subspace spanned or generated by all elements of the following by all elements of the following two forms. First, one, look elements means element of x, any element of this one e with indices lambda one, u one plus lambda two, u two and v minus lambda one eu one, v minus lambda two e u 2 volts in x. This is a nonzero element. Indeed, it's an element created by three elements of the hammer bases, this element, this one and this one with coefficient one minus lambda one minus lambda two and. But now we consider this. Note that there is freedom on lambda one and lambda two for any lambda one, lambda two and c, and also freedom for the choice of u, one, u two and freedom for the choice of v.
00:09:29.836 - 00:11:20.534, Speaker A: So there are many, many elements that we consider here, and the second one is the same on the other component, e u lambda one v one plus lambda two, v two minus lambda one e u v one minus lambda two e u v two. And I mean the same for all lambda one, lambda two and c for any choice of u and for any choice of v one, v two and v. So again, a lot of freedom here. And so we have collected many elements, a lot of elements, and these elements create a subspace, which we called r. And then we make quotient with respect to that space, that subspace. What it means, naively speaking, means that we do it in a way that any of these elements, this and this in the new quotient space is equal to zero. Here is a notation for a little u and little little u in u and little v in v.
00:11:20.534 - 00:13:24.556, Speaker A: For the coset created by e index uv, that's an element of x. When I do this, this is a coset created by this, which, as I said, sometimes we may write it this plus r. The notation we use is u tensor phi, that's a eu notation. And x mod r is called the tensor product of u and v, and it's denoted by u tensor v tensorflow. It's a vector space, complex vector space. So, and note that any element of this is of the four main elements of u tensor v are of the form linear combination of elements like this. So generally speaking, we should write lambda j from one up to n u j tensor product vj, that's a coset like this.
00:13:24.556 - 00:15:02.754, Speaker A: And usually we multiply by scalar here lambda j. So that's an arbitrary element. And you'll see momentarily why I do not write lambda j here. I just consider uj tensor v j, where uj is in u v, j is in v, and n can be anything bigger than or equal to, you will see very shortly. And the reason is this lemma which says that our goal is achieved for any choice of lambdas and u's. We have lambda one u one plus lambda two u two tensor v is equal to lambda one u 110 should be plus lambda two u two tensor v. And similarly for the other component, u tensor lambda one v one plus lambda two v two is equal.
00:15:02.754 - 00:16:42.664, Speaker A: To note that there is no conjugate, it's just lambda one u tensor v one plus lambda two u tensor v two. In particular, I write in red in either in the first one or in the second one. If you take lambda two to be equal to zero, we see in particular lambda u tensor v is equal to. There is no need for is u tensor lambda v is equal to lambda times u tensor v. That's a special case which comes out of these two. If we take lambda two equal to zero and change, of course, lambda one to lambda. The proof is very easy that the proof is based on the fact that in any quotient space, if you have x in r, then the coset created by x, x plus r, or x mod r, like this, is the same as zero.
00:16:42.664 - 00:17:40.997, Speaker A: Or if you, you want to be very careful, the coset created by zero, which is the same as r. I mean, I write sometimes zero, sometimes r, but it's the same thing in the quotient space x mod r. We are talking in x. True. So to show that, for example, this is equal to this one, this is a coset that's another coset. It's enough to show that the difference is in r and that's the way it is done. So let's approve what is lambda one u one plus lambda two u two tensor v.
00:17:40.997 - 00:18:20.004, Speaker A: That's by definition. By definition, this is the concept created by this element, whose first component is first index is lambda one u one plus lambda u two u two. And the second one is v. True. That's, that's our original definition, which was somewhere above. Yeah. Symbol is like this.
00:18:20.004 - 00:18:53.344, Speaker A: And then here it is. Here. I mean, here it is. I give this definition. U tensor v is the coset created by the element euv. So I applied it here and I applied it two more times. Two more times.
00:18:53.344 - 00:20:09.234, Speaker A: U one tensor v is the coset created by element eu one v, and u two tensor v is the coset created by eu two v. Again, these three are definition, I mean, all of them. Now, I multiply this one by lambda one, and this time by lambda two, and add them up. So, lambda one u one tensor v plus lambda two u two. Tension v is the coset is lambda one times the coset e u one v plus lambda two e u two v. And we know the operation of the coset is linear. So this we can say it's the coset e u one v plus lambda two e u two v.
00:20:09.234 - 00:21:52.382, Speaker A: This element create a coset which is equal to. And now look at this one, look at this, a coset, and look at this, another coset. What can we say about their difference? Instead of writing the whole formula, I write left side minus right side is equal to by left and right. I mean left and right in this equation, in star is equal to the coset created by e lambda one u one plus lambda two u two, and v minus this one, you two. And we need to be very careful here, not to do the operation, which are not legitimate, but we want to do them. So we just follow the rules. And for two coset coset r minus coset b is the coset a minus b.
00:21:52.382 - 00:22:41.224, Speaker A: This is the coset e lambda one u one plus lambda two u two v minus lambda one minus lambda two u two. Just linear algebra. I just did the, the operation based on, on the, I mean, linear algebra rules and what I'm allowed to do. And now look at this element, the whole element, I mean, all of this, it's an x. But come here. What is my assumption? Your assumption? Assumption. Here it is.
00:22:41.224 - 00:23:18.294, Speaker A: Number one. R is created by the span of elements, all of the elements of the following forms, this form and this form. And this is precisely the form that we see below. So this is an element of r. So this belongs to rich and then belongs to r. We can say that this is equal to r. We can also say that this is equal to zero, because in the quotient, as I said, r plays the role of zero.
00:23:18.294 - 00:23:50.836, Speaker A: So left side minus right side is equal to zero. It means that left side equal to the right side. That's the identity we want to prove. And again, we obtain this because we treated r as zero. This identity is in the code quotient space. But I mean, after a while that we work with this, we forget about the quotient and treat them as usual. But in reality, this is something that happens in the quotient space.
00:23:50.836 - 00:24:51.340, Speaker A: The proof of the second was similar. Just do left side minus right side, do the operation, and you end up with something like this, such that inside the bracket belongs to r, and then you are done. So we obtain a vector space with the properties we wanted. And now let's study more. Some of these properties. One of the things that you need to be very careful about it, and this causes some problems. In proof, if we don't pay attention, is that an element x in the tensor product, of course, it has a representation like j from one up to n u, j times tensor vj, a different uj, energy.
00:24:51.340 - 00:25:26.450, Speaker A: But this is not unique. It has many, even we can say infinity, many representations like this. And so it's far, far, far away from being unique. This representation, we have one. But forget about unicity, and this causes certain problems. In some cases, we can have better properties of uj and bj. And here is one of them.
00:25:26.450 - 00:27:09.572, Speaker A: Here is another demo. Each x in the tensile product of u and v, x not equal to zero, has a representation sum j from one up to n u j tends to vj, in which the collection u one, u two up to u n is independent in u, and the other collection is independent in v, is independent in v. And again, attention. I don't say that any representation has this property. I say it has a representation with this property. And again, a not de. It means that it can have other representation still with an element of this form for which uj and vj are in independent.
00:27:09.572 - 00:27:47.368, Speaker A: Okay, so again, there is no uniqueness in this lemma. The only thing that you will see it's unique is this n the number of the basic elements. The proof is again easy. Before giving the proof, I forgot to say something and I. You can see it, why now, here I said that. Here it is. Here I said that an arbitrary element of utensil v is written in this way.
00:27:47.368 - 00:28:31.376, Speaker A: And I didn't put a scalar here. If you wish, you can put the reason I didn't put lambda j here, because you can put it inside either with uj or with vj, so you absorb it inside. And that is why we can say that it has a representation of the form uj product vj. No need to consider scalars. The proof of this here is the trick. It's an interesting trick. Again, from linear algebra, x has a representation like this.
00:28:31.376 - 00:29:42.878, Speaker A: Many of them need so with different n's. And now consider the smallest one. So, one representation for which n is the smallest possible. The way it's written here, the smallest possible n n among all possible representation of x. Consider the representation with n. Here is here being the smallest possible. And I claim that any representation with n the smallest possible, has this property.
00:29:42.878 - 00:30:45.404, Speaker A: U n u one up to u n are independent in u and v, one up to vn are independent in v. Then. Now to do this, assume this is not assume that, assume we log u one are dependent. And I found a contradiction. That's my assumption. So there, so there is alpha one up to alpha n, not all zero, such that alpha one u one plus alpha n u n equal to zero. One of them is non zero.
00:30:45.404 - 00:31:55.734, Speaker A: Again, without loss of generality. For example, assume that alpha n is not equal to zero. Then you can write un as linear combination of the others minus alpha one, alpha n one minus alpha n minus one, alpha n u n minus. And for simplicity, let me write lambda one u one plus lambda n minus one, u n minus one. So I can write un as linear combination of u one up to u n minus one. And then let's see what happens to to x. And un is like this and then what happens to xx is sum that j from one up to n u j tends or vj.
00:31:55.734 - 00:34:05.344, Speaker A: Well, I consider the first n minus one plus un. And instead of un, I can write this answer v. This is my un true. I just replaced un by the linear combination of u one up to un minus one. And now I try to absorb this term inside the previous terms by the rules that we know it holds by now that is not, that is not difficult, because we can write, we can write some j from one up to nature uj tends to vj. And this one we can expand and write some j from one up to n lambda j up to n minus one, uj tensor vn. And here it really doesn't matter if I put my parentheses like this, like lambda j uj tensor vn, or lambda j uj times they are the same.
00:34:05.344 - 00:35:06.316, Speaker A: And indeed, in the next step, I will take lambda j and even put it with vn. So this term is the same, but for the other term I write u j tensor lambda j vn and up to n minus one. N minus one is important. And now we can, we can mix the two sum and write some j from one up to n minus one. Both terms start with uj. So uj is good tensor, what is the second component? Vj plus lambda j vn. That's another representation for x.
00:35:06.316 - 00:36:22.884, Speaker A: But this representation has a property, n minus one. And this is a contradiction. This is a contradiction, because at the beginning we assume that n is the minimum, minimum, the smallest possible among all representation. And if they are dependent, we can go one step lower and obtain another representation with n minus one element. Therefore, u one up to un are independent by the same strategy, the same. Similarly, v one up to vm are also independent. That's the end of the proof.
00:36:22.884 - 00:37:08.354, Speaker A: To emphasize, please let me say one more time, that if n is the smallest, then x can be written as sum j from one to n u. J tends to be j. That's good, not uniquely. We can have, say, uj prime, tensor vj prime. We can have another representation for which these uj prime are also independent. Vj also independent. And video tensor product, we obtain x, and that's another.
00:37:08.354 - 00:38:10.094, Speaker A: So the representation is not unique, but the element we have the orientation that universal property. This is extremely important for tensor product. And even sometimes in book they say that forget about the definition of tensorflow, like even the thing I gave before. If you just use this property all the time, then you are okay and you can handle any problem. And this is called the universal property of a tensor product. That's why tensor products were introduced. Universal property.
00:38:10.094 - 00:40:10.424, Speaker A: And we can summarize it in this way. Universal property is a property that allows us to go from a bilinear form to a linear one. So it's a trip from bilinear to linear. And tensor product allows us to do so. If you have a map b defined on u cartesian product v into another vector space w, we say that it's bilinear if it is linear with respect to the first component and also with respect to the second component. In other words, b of lambda one, u one plus lambda two, u two, and nev small v is lambda one, p u one, v plus lambda two, and the same with the second component b of u, and again lambda one but not lambda 1 bar bilinear tool or two linear. It's linear with respect to the first component and also linear with respect to the second component.
00:40:10.424 - 00:42:45.004, Speaker A: And now we want to from this obtain a mapping. This mapping is given, we want to obtain a mapping out of this, which is linear, and that's the universal property, which I explained here is the trouble. So u v w are complex vector spaces, and b as above, from u cartesian product v to w is bilinear. Then there is a unique map, unique linear map. There is unique, got two things here, unique linear map, call it b tilde for the time being defined on u tensor v to r w such that b tilde of u tensor v is equal to b of u. Well, the proof you can also, I mean, draw commutative diagram like u tensor. Here it is b toward w and here is u tensor v.
00:42:45.004 - 00:43:44.764, Speaker A: Well, there is a mapping here. It's, this is the projection that we had before. We didn't give a name for this, but it maps u and v to u tensor v. And now there is a map here with tilde such that this diagram commutes. That's the, the meaning of this theorem, and it holds for me. The emphasize is on the fact that we go from bilinear b to b tilde, which is linear. And this trip, this voyage is unique, and uniqueness is clear.
00:43:44.764 - 00:44:51.434, Speaker A: Uniqueness trivial is somehow, is not somehow. It's precisely trivial since u tensor v. This spans the whole space, not this one. Put it this way, since u tensor v, u is in big u, v is in big v, this span the u tensor v. So this creates the whole space. So if a linear map is defined on this elements, then it is unique by linearity. The problem is that, I mean, you cannot start from this and say that, okay, here is my definition.
00:44:51.434 - 00:46:30.722, Speaker A: I define b tilde acting on u tensor v to be the value of b at the point uv. We cannot do that because we do not know if it's well defined or not. As I said, each x can have many, many combinations. So, I mean, I should have mentioned this before, but I can add it here. If x is defined by this representation, and also by another representation, say u, j prime, tensor vj prime, there are many, infinitely many such representations. And then what is b tilde of x? Well, b tilde of x. If we just say that, let's define b tilde by this formula is given by b of uj vj, the summation j from one up to n, and also by b of u, j prime, vj prime, or j or k doesn't matter j from one to m.
00:46:30.722 - 00:47:08.912, Speaker A: I mean, I just mean another representation. And there is absolutely no clue why this should hold. Why. And the goal of the proof, indeed, if you look carefully, is to show that this is true. Doesn't matter which representation you use. But when you apply the formula, even though at the beginning the appearance is different, but then you do the operation here and there, at the end you obtain the same value. That's the goal of the theorem.
00:47:08.912 - 00:48:25.064, Speaker A: And here is the way to do it. I do not start with this definition, I do it differently. Recall x. X was the huge, huge space whose Hamel base is uv with u in big u, v in big v. And on this space, on this huge space, I define definition a linear mapping l defined on x with a values in w. If a basis is given, then I'm free to define the value of l on any basis element. So if I define l of euv to be b at point uv, this is well defined for all u and v.
00:48:25.064 - 00:49:29.874, Speaker A: So at each basis element, I defined l, and for the rest, bilinearity. Note that we didn't use here. The fact that b has some properties. L of e, u and v can be anything. And I put be at the point u and v, no restriction, whatever needs to be. And now you see the role, the role of bilinearity. What is the kernel? Or we promised not to use kernel, the null space of n.
00:49:29.874 - 00:51:04.114, Speaker A: I want to show that r is a subspace of the null space of l. So that's the role of bilinearity. Bilinearity allows us to show this. Well, it's enough to show that the elements who create r in the null space, what was, what were those elements? The first element was of the form e lambda one u one plus lambda two u two, and v minus lambda one e u one, v plus minus lambda two e u 2 volts that was one of the element which formed, as I mean, spanning said for r. Let's find what is l of this. L of x is l of the whole thing. Here, linearity is l of the first element minus lambda one, l of the second element.
00:51:04.114 - 00:52:34.992, Speaker A: Let me write it. L of lambda one u one plus lambda u two, and v minus bilinearity is the definition. And then I use my original definition here. L at this base element is defined b at the point first component and second component the same for the other two. This is by definition, this is bilinearity and this is equal to zero. Because we assume that b is linear with respect to the first component, it's bilinear. So in particular, with respect to the first component, it's linear.
00:52:34.992 - 00:53:43.444, Speaker A: It becomes zero precisely with the same, so x is in the null space of precisely with the same method. If I call it y e u lambda one v one plus lambda two v two minus lambda one precisely. With the same technique, we can conclude that y is also is in null space of L. So the two categories of elements which create r are in the null space. And therefore we conclude that conclusion r is in the null space. So we have, resume what we have. We have l defined on x with values on w.
00:53:43.444 - 00:54:22.714, Speaker A: Generally when we do the quotient space, we consider the null space. Usually we consider l tilde from x mod the null space of L to w. This is okay, usually do this to obtain something which is one to one. But we do not need to consider the whole null space. I mean, you can imagine like this, that's the space x, that's the null space. And then we can see the cells. So this is n of n.
00:54:22.714 - 00:56:38.214, Speaker A: Sometimes we consider something which is a smaller, like this one. Then we have a smaller sets, smaller cells like this. So instead of doing the quotient with respect to nl, we can take the quotient with respect to any subspace of the null space, which is well defined. So we conclude that there is a mapping which I call b tilde. Now, from x mod r to w, and its elements are these cells, and these are precisely elements of u tensor v. So with this method, you see that v tilde, well defined from u tends to be toward w, and the value of b tilde at the point u tensor v, by our definition, is precisely l of this, which is b of, uh, true. Note that either here I mean the definition that we have here, we define l tilde at a coset to be equal to l of x, and the same here, either we call it b tilde and the same is here is the value of l at this point, or b of u.
00:56:38.214 - 00:57:22.990, Speaker A: So that is why we see at the end b tilde of u tensor v equal to b of u v is well defined. It was not clear at all at the very beginning. And knowing this, I mean, the proof is over. Knowing this, if I go back to the comment before the proof here, here is the comment. If you have any two representation for x, then you can calculate b tilde x by this formula or by this formula. And now you are assured by theorem that the identity holds. There is no problem with that.
00:57:22.990 - 00:58:02.778, Speaker A: This was not clear before. Now it is clear. Okay, so this finishes an important theorem. I devote a bit of more time to tensor product, because it has many interesting applications. In particular, when we study the multivariable version of this rkhs, which we will not do, we don't have time. It just remains two more weeks. But when you go into a multivariable situation, you will need this.
00:58:02.778 - 00:58:17.934, Speaker A: And that is why I go into detail to facilitate if you go in this direction in future. Good. So let's have a break. It's three. We come back after the break.
