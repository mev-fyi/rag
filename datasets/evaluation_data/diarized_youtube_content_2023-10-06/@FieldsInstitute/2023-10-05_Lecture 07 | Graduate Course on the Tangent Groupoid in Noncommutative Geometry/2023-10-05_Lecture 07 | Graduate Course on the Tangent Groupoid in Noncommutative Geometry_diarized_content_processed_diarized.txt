00:00:00.960 - 00:00:52.794, Speaker A: Today more, more crass commercialism. So let's keep the sponsors happy. This is a program on that we are part of, I am part of, I guess on operator algebras and what's it on? Operator algebras and applications. So we should, there should be an operator algebra somewhere in this class and it's going to happen several times but it will start today. Wow. Thank heavens you are here. Great to see you.
00:00:52.794 - 00:01:49.024, Speaker A: Okay, so what we're going to do is make some algebras of, well, scalable operators, secretly pseudo differential operators into sea star algebras by some process of normal completion and take a look at those and study them. Something very famous arises. We'll see. Get to at the end of the lecture in connection with this, the extension of pseudo differential operators, the sister algebra extension to which there is attached a long and very interesting history. Okay, so that's our goal today. By the time, assuming we get through what I want to talk about today, by the time we reach Thursday, it will be, it will be time finally to discuss the tangent groupoid. You're still smiling.
00:01:49.024 - 00:02:09.404, Speaker A: I thought I saw you beam when I said seastar algebra, but you're still smiling after I said tension grouping. Many interests. Okay. A man of great depth. These guys will be smiling on, on Thursday. I'm so curious. You put one more episode.
00:02:09.404 - 00:03:24.476, Speaker A: Um, yeah. Let's see how we did that. Okay, so um, let's, since I want to put something on this board, just, you know, it's supposed to be a commercial, right? So there needs to be something on this board. So here's the, excuse me. The thing which is going to emerge at the end, of course, we're just at the moment studying rn so the, the geometric space out of what, out of which we're going to build something seastar algebraic will be, will be rn this curly k. I don't need to tell seastar people what curly k is, but this is the sea star algebra made out of compact operators on the Hilbert space l two of rn compact operator is any operator which is a normal limit of finite rank operators. That's the same thing as taking the continuous, compactly supported integral kernels K of XY and building an operator for each kernel and then taking the norm completion of the algebra of integral operators that you get that way so that those integral operators are what got Hilbert started in the first place.
00:03:24.476 - 00:04:13.972, Speaker A: And so all we're doing is we're taking Hilbert's algebra of integral operators and we're completing it because that's the modern fashion to have, have sister altruist modern since 1946 or something. All right, so that's what we're going to do. And that's what this guy is. And then the next guy, I wrote this so big it's barely going to fit. So the next guy, let's call it something like this. That's the sister algebra you get by taking all compactly supported order zero scalable operators, pseudo differential operators that they form an algebra, and then you take the norm completion. Again, we don't yet know that order zero compactly supported operators give rise to bounded operators on Hilbert space.
00:04:13.972 - 00:05:13.040, Speaker A: And so that's a chore we have to accomplish before we can really talk about that. Finally, if you have at least an honest to goodness order zero pseudo differential operator, it has an order honest to goodness principal symbol function like we were talking about last time. And that's a smooth, compactly supported function on the n minus one sphere. It's one such function for each point on rn. So at each point on rn, there's some, there's a symbol family of operators s omega, and each s omega can be more or less completely characterized, as we discussed last time, by a homogeneous function. Well, if we're talking about order zero operators, a homogeneous function of order zero. So each pseudo differential operator, at least if it's an honest to goodness pseudo differential operator, is going to give rise to an honest to goodness symbol function.
00:05:13.040 - 00:05:46.434, Speaker A: The honest to goodness symbol functions will be smooth and compactly supportive. And what this zero means is the larger algebra of continuous functions on this locally compact space which vanish at infinity. That is, say, for every epsilon there is such a function is in here, if and only if. For every epsilon there's a complex set such that outside of that complex set, the function is less than epsilon. That's what vanishing at infinity means. Okay, and that's it then. Zero.
00:05:46.434 - 00:06:44.534, Speaker A: So that's probably a summary of all of the things that we want to get through today. We want to construct this so called extension of siesta algebras. It means that the compact operators sit inside the thing in the middle as an ideal. And the quotient siesta algebra, via some sort of generalization of the symbol map construction, is made isomorphic to this thing here. That's what we want to do. And, okay, we'll say, maybe say more about that in due course, but there's a bunch of things that we haven't talked about yet, which we need to talk about before we can properly mention any of the stuff above. Here's the first thing, which is something you might sort of overlooked because, you know, you just don't think about these things.
00:06:44.534 - 00:10:19.872, Speaker A: If you have a scalable operator, and this is not particularly special, not special at all to operators of order zero, any, every scalable operator of any order, any, maybe every. If this is something that we encountered recently in the past, it means the following, that if you apply fa to what you're allowed to apply, you understand compact. So we're using the usual pilfer spacing product, but so so far we're only talking about smooth, compactly supported functions. So the statement of the, which I haven't quite finished, but the statement so far of the theorem is that there is some operator called a star. And so you talk about the address, but I'm a luck knowing that we are going to be scared of. I suffered with this theorem, for there are many, many different ways to prove it, one way and then another way. Okay, so I'm going to tell you what the method is that I'm currently in love with.
00:10:19.872 - 00:11:24.204, Speaker A: I mentioned this incredible formula. Maybe I'll just do a little digression from a previous lecture, because it's such a beautiful formula. If you have a differential operator d, maybe this is not such a bad thing to mention. Here is what it looks like, a alpha of x and then some derivatives operator, which is an example of a scalable operator. Every differential operator is adjointable because you can just write down what the adjoint is, and the adjoint is closely related to what I'm about to write down, but it's not the same. Let's call this the sort of funny adjoint. Put a little diamond here or something.
00:11:24.204 - 00:12:56.374, Speaker A: Adjoints involve complex conjugates, inner products involve complex conjugates, so you expect a complex conjugate to be in the picture. Anyone who's at any time derivatives in the context of operator theory knows that the adjoint of Ddx is minus d dx. So there are some, and this is a good first attempt at forming the adjunct. But it's not the adjoint because the other thing that adjoints do is they reverse the order of multiplication. And in this formula here, I forgot to put the coefficient functions a alpha on the other side of the of the d d axis. So this diamond is not the adjoint, but it's pretty close to it. The reason you might be interested in D diamond, even though it's not the I joint, is that from the point of view of scalable operators or pseudo differential operators, it's easy to know what the symbol of this operator is? So this viewed as a, viewed as a classical pseudo differential operator, not a scalable operator, but in classical terms like you read about in Holmander, the symbol function here is this fellow here.
00:12:56.374 - 00:15:23.296, Speaker A: And this fellow has a simple function, which is very easy to understand. If the complex conjugate is the thing which will, if you were to write these fellows d and diamond, as you know, those tiny integral operators involving Fourier transform of f, as we'll see in a little while later, plus anyway, some function a of x psi, this is what, this is what the function a of x psi would be. And these two symbol functions, that's the good news. The bad news is that this I was which you can read about in vaguely sort of Euler vector field going on here. So I take the operator of commutate with BBX and the operator commutator with XK, that's an operator on here. And then you exponentiate it. What does that mean? You observe that each time you do this, you get the actual inspiring because you can imagine now how you could attack problem with the existence of adjointing in general.
00:15:23.296 - 00:16:21.084, Speaker A: Mainly you take a pseudo differential operator, scalable operator, but you could treat it as an honest goodness pseudo principle operator knows this was an integral operator with an a of Xi. You could take the a of X Psi and you could take its complex conjugate and that would give you a new operator, a diamond. And then you could argue that this Xi exponential Y for a diamond is the actual adjointed c. And if you could argue that you're in business because not only does this operation of iterated commutator lowers the degree of differential operators, it lowers the degree of differential operators. So it sort of makes sense that has them all up. When you see partly figure out exactly what you're supposed to be doing, because you don't yet know that v star exists. And so what am I doing here? You see, however, there's a strong potential through the existence of joint.
00:16:21.084 - 00:17:34.864, Speaker A: Write down what the eye joint is, and it's evidently a seated, for some reason we can't hear the audio. Here's what's actually this is exposed beautifully in Holmando's book. If you have a six student differential operator a, you can adjust it by a tiny amount by a smoothing operator to get a new operator that new operator b, you can apply this infinite series, and the infinite series actually converges. After you make the tiny adjustment to a, it makes sense to apply infinite series, even though we're talking now about pseudo differential operators. So the series doesn't terminate and the series really does define an operator and that operator really is adjunct to the operator you started with. So what you're doing is something much more powerful than asymptotic series. You're saying you can make one adjustment to a by a smoothing operator connecting operator b, and then this is a formula, the series converges and when applied to b and the series defines being.
00:17:34.864 - 00:19:59.674, Speaker A: Excuse me, sorry, here. Hello? Excuse me, I'm not sure if it's computer him or something, which we're not going to do because it's also quite difficult. Excuse me, I could not keep my professor. Hello? If you have an operator given, you know what the hilt of space is, the integral operator with kernel complex conjugate k. So that seems like a good place to start if you have an actual integral operator. We already know what kernel has to be and maybe that will help. So what we, let's talk about.
00:19:59.674 - 00:20:41.634, Speaker A: You can't hear me. Problematic. The audio is not clear. How problematic? How problematic was it? I mean, someone say something. I mean maybe, maybe we can't, maybe they were trying to say something. It's just no way of hearing. Hopefully someone text the chat here.
00:20:41.634 - 00:23:01.668, Speaker A: Yeah. Let's find our expertise not responsible for the technology. Okay, so I will continue, but maybe I'll try to write down as much as possible. Okay, hope you have a scalable family or less than minus n, maybe also with some minus p like this. Then what you have for these operators is that they're integral operators and the integral kernel is some little a c of x y. And we figured out a formula for it, which I can never remember, but I did copy it. Then what's the index? Like this, some long formula, a x lambda j minus one x plus, yeah, by all means, yeah.
00:23:01.668 - 00:24:19.444, Speaker A: No, you want the whole thing. Can you hear us? Testing. Yeah, yeah, they can hear. Oh, we can hear them or they claim they cannot hear us. Well, if the audio is not clear and certainly we cannot hear them, sounds like garbled, right? What is lambda? This is. Yeah, let me say more. Where first of all, lambda is any fixed number between zero and one.
00:24:19.444 - 00:25:12.284, Speaker A: You'd like that to make sure you have something reasonably convergent here. And as for these fellows here, they're defined by our famous equation r omega lambda. A omega t r omega lambda. Inverse is lambdas to the minus, well degree order. What are we calling it? M, let's see, plus k. Here we go, k x lambda. So these correspond? Yes, yes, for any lambda.
00:25:12.284 - 00:25:58.398, Speaker A: I mean, look, lambda lambda lambda lambda. Very complicated, but all of the dependence is somehow cancelling. I had a question. Yeah, I can hear you. Great. So when you're writing this, does kx lambda x comma t, I mean, resemble some kind of residue, some remainder, like in, as in tenor theorem? No, this formula, the formula which is on the board, the crazy formula, is an exact formula. Series converges geometrically, and it converges to the integral kernel of the operator, a big hd.
00:25:58.398 - 00:27:06.764, Speaker A: No, no remainder. This is exact if we're in a world where the degree is sufficiently negative. Okay, as usual, thank you. Okay, so, which I'll now explain. So the p is like, okay, everything that's written down makes sense no matter what p is, as long as it's non, as long as it's non negative. Just getting the degree below minus n is enough for this series to converge and define a continuous function of x and y. But if the degree is less than minus n minus speed, this thing doesn't just converge, it even converges after you differentiate turmoil minus j is going to come out and that's kind of bad news.
00:27:06.764 - 00:30:05.448, Speaker A: Differentiating this theory is not a good idea unless really geometrically decreasing powers of lambda. Increasing powers of lambda, I guess it will defeat the x relia minus days. The bigger p is, the more chance with the following is not just a continuous function, but it's continuously different x and y. Let's just say it's, it's different for an x. Yeah, in anyone but printed function. And what does this mean? Operator a seat is adjustable as a Hilbert space operator, because every operating virtual kernel has an adjoint with a Hilbert space operator. The adjoint, like I said before, as you said, and in this particular case, because the adjoint space are operators whose integral kernels are p times continuously differentiable, what we can say for sure is that these adjoint operators send continuous functions.
00:30:05.448 - 00:32:15.194, Speaker A: In fact, for that matter, l two functions differential. So it's not ideal is going in the right direction operator. Then it's more or less the outcomes operating operating time. Fortunately, the p depends on the operator, so we don't haven't defeated the problem yet. Travel less than construct a bunch of operators I call something like e on n. These operations were all scalable. Order M is positive.
00:32:15.194 - 00:39:14.714, Speaker A: Then we'll just, we can choose Em and one negative en's are just the parametrics of the given an operator family eight seat of any order minus e minus eight, we multiply by e plus and then we'll. And I should be doing this because some way of smoothing operators briefly, the end and now b has reached negative orders, whatever the degree it observations of that, and for that matter, the product of operators is again quasi enjoy get if you like to figure out what you do if you have a m, then moreover, if you split all of those, and if you think about it, obviously going to satisfy exactly the same scaling, this one here, where this moving operator still, if you have a scaling family of degree m, then the t wise adjoints exist, they constitute the smooth family, and that smooth family is again a scaling family of degree m. So every scalable operator which you get by setting t is equal to one is adjointable, and the address is the t one member of a scaling family of degree m. So if Apple operating of the same model, I think I mentioned before, but I'll there's something a little interesting going on with the idea of joint ability, because if an operator has an adjoint, then whether or not it was continuous to begin with, it's automatically continuous by the adjoint equation. That is to say, it maps by sequences which are convergent to sequences of test functions which are convergent in the usual notion of c infinity convergence. And if that's the case, then for both the operator and its adjoint, then both operators, think about it for a moment, give rise to operators on the topological joule spaces of smooth distribution. So a joint ability implies continuous plus extendability.
00:39:14.714 - 00:40:02.694, Speaker A: Extendability. So there's a sense in which joint ability is all about, I don't know, continuity, regularity, or something. And so it's not terribly surprising that a trick like this should come into play. This is all about solving equations. All right, so we have now addressed the overlooked view of adjointability. Every scalable operator is like you probably would assume from the very beginning, and the I joint is still a scalable operator like you probably would have assumed. Although there are some interesting surprises when you actually put a pencil to paper and write down the details.
00:40:02.694 - 00:41:03.734, Speaker A: Yes? Question. Yeah, so this fellow. Yep. Well, this is a formula for, at this moving operator, I look at the other side. So when you take stars, it's a formula for AP stars. You don't have to show that one AP star is equal to another one. I mentioned that there's only one ap.
00:41:03.734 - 00:41:41.414, Speaker A: Excuse me, moving depends. Yeah. Excuse me one moment, please. Sure, go ahead. So is eu n some kind of a shift operator? I could not pardon someone to be a reader. Did you fall asleep? No, I don't. I clung to you.
00:41:41.414 - 00:42:36.464, Speaker A: Maybe I just missed you one. Okay, but now it's on the right. Pardon? No, I was trying to figure out what is ear pen. What is ear pen? I cannot hear you. Can anyone hear me? He's asking what e of n is, but also the audio is almost completely shot on your end. Yeah. Okay.
00:42:36.464 - 00:42:57.174, Speaker A: I don't know. We could. Yeah. Sorry for the technical. It sounds happy, as far as I can tell. I don't know. Not complaining.
00:42:57.174 - 00:44:22.666, Speaker A: Visuals are still good, so I don't know. What about batteries? I think I'll just continue. Let's see if we can. And so I have a response to make the notes extra. Sorry. All right. For the people who appear in front of me, are we.
00:44:22.666 - 00:47:10.584, Speaker A: Can we just post that this issue has been settled? I lied about one thing, but if I fixed it now, you'll become less happy. Every scalable operator is a sense that the adjoint is another honest goodness scalable operator. The normal water zero is bounded or extend. I mean, practically speaking, what's going to make this very easy is we're going to use the Plancherelle formula, the fact that the Fourier transform is isometric in l two. So I know how to say why the proof is easier. But. But why this should be such a hard theorem for Lp, I don't know, but we're only going to do it for l two.
00:47:10.584 - 00:48:36.436, Speaker A: The result there. Oh, here's another theorem which in fact, I'm going to prove first. I skipped an entire. Every compactly supported scalable operator, as before, of negative order extends to a compact operator, a norm limit of finite rank operators, or a norm limit of integral operators on LSU. So that's the stronger statement, that the operators are compact, but it applies to fewer operators, just the ones of negative order. And the logic of this is that we'll want to prove the second theorem first. Now it kind of coming to show that exact sequence that we had.
00:48:36.436 - 00:49:03.644, Speaker A: It's beginning to look like it might actually come into being. Yeah. Question. Correct. There are no differential operators in the story of, I know, sea star algebras. Yeah. There are consequences, or lots of consequences for differential operators, which are the results of these theorems.
00:49:03.644 - 00:49:37.088, Speaker A: Yeah. Each. Yeah. So that tells you, for example, if you have a. Well, we're working on Rn, but let's just cheat and work on a manifold like a torus, for example. Deal with an operator on Rn acting on periodic functions. If you have an operator acting on periodic functions, something like little class operates in kind of a class into some matrix, then, yeah, it has parametric scriptures of compact operator.
00:49:37.088 - 00:50:56.074, Speaker A: And that will tell you that the operator is first of all, as they say, essentially self adjoint, susceptible to spectral theory, and secondly, that the spectrum is an isolated set of eigenvalues infinity. So those results are easily obtainable from what's written here, especially the bottom theorem. Yeah, you can still say intelligent things about differential operators using these theorems. Yeah, I haven't decided whether I'll do that. But it's fact that if you take an elliptic operator and you apply to it a reasonable function, the sort of functions that you see in pseudo differential operators, a function on, let's say it's a self adjoint operator, a function on the line which is bounded and differentiated, it goes to zero, faster and faster and faster. Symbol type function. F is a symbol type function and d is a self joint elliptic suited french operator.
00:50:56.074 - 00:52:27.974, Speaker A: Then you can make sense of f of activate differential operator might. Okay, so let's prove these in the order that God meant them to be proved, which is the order in which I stated them. First of all, as we observed, if the order is less than minus n, then a is an integral operator with continuous. So it's one of the operators that Hilbert started with. Definitely a compact operator. So a bar, by which I'll mean the continuous extension l. Two, this bar notation is just for today.
00:52:27.974 - 00:53:29.650, Speaker A: This is ugly. This thing certainly exists and is compatible. Now suppose you have an operator which is slightly less negatively ordered, so there is less than minus n over two. Okay, then we can form a star, because we just finished proving that adjoints exist. And now the order of a star times a is the sum of the orders of a star and the order of a. So this thing is less than minus. Nice.
00:53:29.650 - 00:55:01.854, Speaker A: So this thing has a continuous extension which is compact. Right, words we can ask ourselves, how big is a of f? Well, and there's an adjoint operator, so that's a like that. And this fellow here, a star a is a bounded operator. And so what you see is that, however, how big can a be when it's applied to f? It's not going to be at some constant more than the normal f squared. So this formula tells us. But the continuous extension of a itself at least exists. So all of these calculations here are done just to begin with, with a test function.
00:55:01.854 - 00:56:00.834, Speaker A: But they tell you that the norm of af cannot be too big, bounded by some multiple of the norm of f. So a extends to a continuous bounded operator. And as for this continuous extension, to go back to a star a, you can check for yourself that this is now it makes sense to write that. And it also makes sense to write this. On the left hand side is the adjoint that we were just talking about, obstacle operator. The right hand side is adjointed bounded Hilbert space operator. And they're the same thing.
00:56:00.834 - 00:56:48.024, Speaker A: These two formulas are checked just by applying both sides, everything to a test function. And then it's sort of obvious that these things happen. So what it all says is that this bounded operator, when you take this adjoint and you multiply it by itself, is a compact operator, because it's equal to this one here. This is equal to this and this equal to this, and compact. So this guy here is compact. So a bar is a bounded operator with property that a star a is compact. Now you can look in the textbooks and you can see that that implies that a itself is compared.
00:56:48.024 - 00:58:32.504, Speaker A: Probably one of the textbooks that you wrote is suitable record. Okay, now the rest of the proof is minus. I want these people, you know, they know it's going to, this is open. You're their only lifeline because they can't hear a word I'm saying. It's like the stakes have gone way up for you today. There's a chance, there's a chance for you to redeem yourself. Well, let's take a small advantage.
00:58:32.504 - 00:59:14.100, Speaker A: What I'm about to write down here is not terribly relevant, but it's a, it helps not to have to worry about asymptotic expansions and so on. It suffices to consider the case where a is op s. We saw that every order zero operator is one of these special operators, op s plus an operator of order minus one. I'm trying to show that every order zero operator compactly supported is bounded. I already know that the order minus one remained itself bounded. So as long as I can show that every class is bounded, I'm in business. So s.
00:59:14.100 - 01:02:45.024, Speaker A: Here is a symbol family zero, and it suffices. Theorem Y theorem two, because the extent to which a general a differs from this is one of these compact operations, is a negative order operator, and therefore compact and therefore bound. The reason I want to work with these special operators, and you don't really have to do what I just did to reduce to this case. But the reason is that it's easy to figure out for such an operator, a formula for the operator. They all look like this. Well, this a of X psi is what we were previously put in the last lecture attack. This works because we need, because we saw that the simple function in such situation is basically so there's some compact set of X's, so that if little x is outside of that compact set, it doesn't matter what psi is.
01:02:45.024 - 01:04:11.044, Speaker A: This thing is just plain zero. So that's the kind of function we're dealing with, and that's the operator that you're supposed to make out of it. And the trick to proving this theorem is Fourier transform. This will now what, how big is f is bounded operator? If I want to know how big f is as an l two function, of course, it suffices to ask the related question, what is the size of the Fourier transform of that function? Because the Fourier transform is isometric. What I need to do is take the Fourier transform of this horrible expression, and what's going to be involved then rn of another instrument. I'm just writing down what there was before. Afsi equal psi.
01:04:11.044 - 01:05:01.372, Speaker A: That's just the formula for a of f. Now I'm supposed to take e three minus PI x theta e x. That's the formula. So this is a function f, which is in here variables, by virtue of the compact supportedness of the a function in x, it's compactly supported in the x variables. So in the x variable, it decays so rapidly to zero, it just becomes zero after a certain time. And in the chi variable, it's decaying really, really fast. So it's an integral function in both variables together.
01:05:01.372 - 01:06:17.564, Speaker A: So you can do order of the integrals. So instead of integrating the ciphers, let's integrate the x first. And now I need to put everything. So what is this, the Fourier transform of this, a function in the x variable. So let's call that a hat. And we're evaluating the Fourier transform at, well, eta minus psi. This is no minus sign there.
01:06:17.564 - 01:08:10.394, Speaker A: That's it. And the advantage of having done this is that now there are no oscillatory factors. And so if this thing is going to define a bounded operator, we can just, you know, we have to see it just from the behavior of AHEC. It's not going to be any weird oscillatory cancellations anymore. The second variable is going to be not relevant to us, really, although I certainly could have put it in, because in this second variable, we're dealing with a bounded function, and it's the behavior of this function in the first variable that's important. What is that variable? Well, we're taking the Fourier flux point of contact, which is smooth, and in fact, but they're all smooth. They all have the same uniformly compact support, and they're all uniformly.
01:08:10.394 - 01:08:37.444, Speaker A: When you take the Fourier class form of a smooth, compactly supported function. You get a rapidly decreasing function, a Schwartz class function. In fact, it's the rapid decrease that's important here, independent of whatever. Xia, put another greek letter in here, theta. Whatever theta I put in the second slot. Okay? So this thing is going to be back, okay, depending on. Nice.
01:08:37.444 - 01:09:52.631, Speaker A: And then one plus sine minus a, because it's a short class function. Fourier transform, a short class variable uniformity. So if you don't like the of hat. If I'm only interested in this, which is all that I'm interested in, the answer is that it's no more than a constant, okay? Divided by two PI the n, because that's another constant. It's an integral psi minus eta to minus m times f hat psi. And now to think about this. There's no mysterious function a anymore.
01:09:52.631 - 01:10:42.408, Speaker A: There's no oscillatory factors. There's nothing. This is well enough, I guess, big n, bigger than little n. This is an l one function of this argument. And so this is an l one convolution. And if you convolve an l two function by an l one function, you get an l two function and the norm of the operator of convolution from l two to l two given by some single fixed l one function like the one we have here, is no more. Maybe it's exactly equal to the.
01:10:42.408 - 01:10:54.144, Speaker A: It's no more than one. Think about that. It's no more than the l one norm of that function. What is the l one norm of this function? I don't know. I don't care. It's just some finite number. So this is a bounded operator.
01:10:54.144 - 01:11:35.764, Speaker A: If you take. What have we done? This is a bounded operator of f. Hat. But hat is a bounded operator of f. In Fourier transform, if we take a and we pre compose it with maybe inputs of the Fourier expense, you get a bounded off it. The Planche rl says, if those Fourier factors are the analysts waking up here. Yes.
01:11:35.764 - 01:12:18.484, Speaker A: A. Yeah. So big a was assumed to be somewhere compactly supported with an operator. That implies that in little ways compact this portrait as a function of its first variable, x. It's also c infinity, because everything in this class so far is infinity. So we have some uniformly c infinity function a of x psi uniformly in the sense of c infinity and x. And you move psi around the same estimate.
01:12:18.484 - 01:13:23.966, Speaker A: Oh, yeah. Is there, what's the best way of improving this argument towards optimum? I don't know the answer to that. I'm not sure. It's the right question, because this argument is sparked from optimal there's nothing about the compactly supportiveness of a particularly relevant boundedness of a as we know in Hilbert space. If you have an operator on this part of the Hilbert space, direct summon operator on this part of the Hilbert space, which is orthogonal to the previous part, and then an operator on the third part, which is orthogonal to the first two, the norm of the three operators together is just the soup, the max of the norms of the individual operators. So you can easily prove bounded into operators now. So it's not really about compacting port.
01:13:23.966 - 01:14:14.774, Speaker A: It's also not really about smoothness of the function little f of x, not really. What enters into this picture, this argument is not smoothness, but it doesn't in doing so it's not a very efficient argument. It's easy, which is why I gave it, but it's not efficient. And so we're going to try to improve it right now to make a much better known mess. But we need for this argument to work, we need to start it somewhere. So we made a simple inefficient boundedness sphere, which gives you can work out what the constitutes are not very good. Now we're going to do something clever.
01:14:14.774 - 01:16:02.914, Speaker A: So once we theorem which we have, we're now entitled to talk about this. Generated by all compatible order zero operators, each one found it and closed on that collection of operators with an output closed under icon. So its norm closure is a sister output. The thing I was calling big size star of rn and it contains the compact operators as an ideal. And now we what say about it? And let me show you a nice part of the argument. So the story so far is that the world of C star algebra completions of order zero, compact exported pseudo differential operators certainly contains the world of compact operators, because you get all compact operators just by closing the smoothing operators with compact, and every smoothing operator with compact sport certainly is scalable of anybody. All of these fellows are in the story and they all stick inside here, sea star algebra closure, if you like, of all order zero actually.
01:16:02.914 - 01:17:27.602, Speaker A: And what I asserted at the beginning of the lecture is that there's a star homomorphism which extends this idea of symbol, which is fundamental to see star algebras and anything in this norm completion, even if it's not an honest scalable operator, it's just a norm limit of scalable operators. Anything still has some kind of symbol and it's a continuous function on this space and you get every continuous function. And moreover, the operators which have no symbol function, zero symbol function are precisely the compact operators. That's what we're trying to prove. All of the things that we just stated. And there are two things that need to be sorted out. Yeah, yeah.
01:17:27.602 - 01:18:06.662, Speaker A: All of these operators are. This algebra operator sits inside the sister algebra world, bounded operators in ilkaj. So we're borrowing the adjunct operations from the ambient synchronous and we're borrowing the norm from the ambient. So the famous CSPR identity automatically holds people algebra. And the only thing which doesn't hold until you impose it is the completeness, the closure under norm limits condition. So we just take the closure now it's complete everything. Yeah.
01:18:06.662 - 01:20:03.444, Speaker A: Each element of this istra algebra is an operator on Hilbert space for sure on l two of r, and the ones which are honest or scalable operators are acting in perhaps a honest way on smooth compact display function. And there are two things that need to be checked in order to be able to do what's done here. What needs to be done here? One is that if sigma is the degree zero principal symbol in the sense of the last lecture of a, then the norm of a tells you how big the symbol can be. And what do I mean by the norm of the symbol? I just need. And what's a? This is an actual, what is the origin? This says the symbol depends continuously in the sense of sias our algebra, on the operator. And so there's an extension by continuity of this symbol map from the set of all actual operating where we actually already find the symbol to all those limits, maybe the symbol. So you need to know that this is relatively unlikely.
01:20:03.444 - 01:21:53.554, Speaker A: Not so patient. Cesar Alpha is the quote. And in order to bring this matter to a conclusion, I will leave the it's just very elementary to check this thing, that it's a similar shape, that the operator is bigger. What does it mean for an operator to be big and little vectors to big vectors? So if the symbol is big, you have to construct vectors out of that symbol with the property that a applied to those vectors are big. Well, what is a vector? It's a function of x. Somehow set xi equals some fixed vector. And look at those functions.
01:21:53.554 - 01:23:32.914, Speaker A: Look at the family of functions that you get by fixing xi and letting x vary. That's a family of functions. Practice. What happens is in a decide that a not very interesting manifold and you look at all manifold like. I suppose you have a map. I guess maybe I'll use a different part as part of matter is still on just algebraically. Well then it was maximum.
01:23:32.914 - 01:24:47.404, Speaker A: And the reason that this thing is relevant problem up there is that we want to apply this to a being the quotient that appears just above. We already know that if we have a smooth, compactly supported function on rn times sn one, we can think of that as the principal symbol function of some symbol family. And then we can build an operator attached to that symbol family. And that operator is well defined functions, or rather operators in negative order, in other words, well defined workflow operators, which are compact. Theorem two from before. So that procedure gives you a map from symbol to elements when the quotients each are algebra. Take it honestly, goodness.
01:24:47.404 - 01:26:11.152, Speaker A: You build a corresponding symbol, the Fourier method like we discussed, and then you build the corresponding operator by that formula that we discussed a while ago. You observe the operator only depends on the original symbol, and therefore that's a recipe for building a star humid. What I'm saying is combine it with number one. On what I'm saying you're going to enter japanese. This is what it comes down to, just a little nice thing to the last couple of minutes, homomoxins between sister algebra are always contracted. The problem here is that this is not a sister algebra and you need something to make this work. For example, this could be expose this with, I don't know, polynomial functions on the unit interval.
01:26:11.152 - 01:27:09.434, Speaker A: You take the star algebra of polynomials functions on the unit interval instead of this particular cell algebra. The analogous result is not true. Why? Because you can canonically extend any polynomial function on the unit equal to a polynomial, and that suddenly is not normally preserving that extension. So it depends on what the south berives. And smooth functions have a key property that polynomial functions, which I'm now going to show you. And that's what makes this argument work. Well, first of all, it's convenient to take the algebras which are involved here and add an identity element, a multiplicative identity element algebra.
01:27:09.434 - 01:28:27.746, Speaker A: So on the left hand side, talking about functions on m, which outside of a compact set are actually constant. So I throw in the constant function on the left hand side. Well, if I represent a in such a way that a does not include the identity operator, then I just throw in the extra identity operator, generate an algebra that way, and there's a canonical extension of alpha to this fellow. So here's what we need to show. If you have a number c which is bigger than the norm of f, f being a a function either in this completed fellow or just in the original one, that would be fine. The norm of f is this max here, then c is also bigger than the way the argument works bigger than or equal to his best algebra inside the c star algebra a. So this is what we're going to show.
01:28:27.746 - 01:29:04.334, Speaker A: It doesn't mention the little pluses that I just added, but the pluses will appear in the next line. And here's the critical thing. Suppose you look at the function c minus f star f. That's a smooth function on m. Eventually. It's a constant function, and it can never be zero. It's bounded away from zero, because the biggest f squared can be, is the norm of f squared, and c is even bigger than that.
01:29:04.334 - 01:30:03.848, Speaker A: So this is a smooth function on m, which is bounded below. So one over that smooth function is another smooth function on m, eventually constant, no singularities, no nothing. Okay, well, that's true, but that's not what I meant to say. What I meant to say is something else, which is that this function, which is nowhere zero and always positive, is a function which has a square root which is another smooth function which is eventually constant. Let me use c star line with mod f squared c minus f star f is the positive function smooth positive function constant outside of point y. Square root is another smooth function which is constant outside of a compact set. Smooth positive function which constantly is constant out of complex set.
01:30:03.848 - 01:30:21.266, Speaker A: Great. The h exists. Okay then, so that's the critical thing about this algebra, that it has square roots, lots of square roots. This algebra cc infinity event. The algebra cc infinitive m plus which polynomials don't have. You can't do that. Polynomials.
01:30:21.266 - 01:32:20.274, Speaker A: Yeah, choose. Oh, thank you very much. So that's alpha. That's just another way of saying C squared. That's another way formulating the argument. Okay. And then by the sea star identity, that's just saying this.
01:32:20.274 - 01:33:13.462, Speaker A: So we didn't prove one. It's, to be honest, not very interesting. But I'll put it in the notes. We just proved two and two tells us we have this short exact sequence, so called extension of C star algebras back in the day, meaning, God forbid that I say this exactly 50 years ago, Brown, Douglas and Fillmore studied extensions of sister algebra exactly like this. And he showed that the, I don't know, very sister algebraic. They showed that the sister algebraic problem, apparently, of classifying these extensions is completely determined by k theoretical invariance. There's precisely an integer family of extensions like that one for each integer, which is realized as some sort of an index.
01:33:13.462 - 01:33:53.464, Speaker A: And this one here is the extension corresponding to the integer one or minus one, maybe, depending on your identifications. And that was the beginning of a very interesting story. All right, so we reached a sort of certain sister algebraically conclusion today. We've finished our preliminary discussion of scalable operators as of earlier today. And what we'll do next time is get rid of coordinates. Thank heavens this guy's breathing a sigh of relief. Coordinates.
01:33:53.464 - 01:34:13.884, Speaker A: We'll get rid of coordinates. I'll explain why it is that this theory is coordinate invariant. You don't need the x's and the xis. The whole theory is coordinate invariant, so it immediately extends to manifolds. Just because it's automatically, immediately coordinate and variant. Then we'll all be happy, sister. Algebra people today and manifold people tomorrow.
