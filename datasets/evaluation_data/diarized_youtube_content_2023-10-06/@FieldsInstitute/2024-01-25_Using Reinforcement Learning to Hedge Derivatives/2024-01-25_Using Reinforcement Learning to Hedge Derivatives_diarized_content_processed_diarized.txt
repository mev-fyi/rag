00:00:00.840 - 00:00:06.158, Speaker A: Okay. Well, good evening, everyone. Can you hear me here? Is it working?
00:00:06.206 - 00:00:06.794, Speaker B: Great.
00:00:07.534 - 00:00:53.002, Speaker A: So welcome to our monthly seminar series here at the Fields Institute in quantitative finance. It's a pleasure to have everyone back for the first seminar of the calendar year. Before introducing our speakers, I'd like to thank our sponsors, Waterfront International and Scotiabank through the center for Financial Industries, for their continued continual support of the seminar series here at the Fields Institute, as well as other activities in quantitative finance. So without further ado, I'll give a very brief intro to our very well esteemed colleague here who needs no introduction. Professor John Hull from University of Toronto at the Rotman School of Business. He's well known to probably all of us who've probably all went through his textbook. His textbooks, I should say.
00:00:53.002 - 00:01:37.494, Speaker A: I don't know what the count is right now and I don't dare guess. And ranging from major contributions all over quantitative finance, from stochastic interest rate models, stochastic volatility models, all the way into credit risk, and now venturing into the world. Or I shouldn't say venturing, really treading in the world of machine learning and reinforcement learning. And with Professor John hall, we have Doctor Zecis Poulus. He is a postdoctoral fellow at the Rutman School of Business and his interests also lie in machine learning and finance at the interface. And they will be doing a joint seminar here. So we're going to be tag teaming back and forth, and today we're going to learn all about unity in reinforcement learning to hedge derivatives.
00:01:37.494 - 00:01:39.714, Speaker A: And John, the floor is yours.
00:01:47.834 - 00:02:16.833, Speaker B: Well, thanks very much, Sebastian. It's a real pleasure to be here and thank you all for coming. Braving the elements out there. We weren't expecting as many people. Okay, so, yeah, what my friend Zsys and I are going to be talking about is using reinforcement learning to hedge derivatives. And there are actually three papers we have now in this area. They're all listed here.
00:02:16.833 - 00:03:08.094, Speaker B: The one which we'll be mostly referring to today is the top one, which we'll have more to say about later. These are all papers that come out of this research center that we have at Rotman, which is Finhub. And one of the interesting things about finhub is that it sort of combines academics and practitioners. So if we look at the authors of the current paper, there are six of them all together. Zsys and I are the only academics. The other four are actually all practitioners. As it happens, they either work for banks or they work for pension plans.
00:03:08.094 - 00:04:11.424, Speaker B: So let's get into this a little bit. We're talking about the hedging of derivatives. I've got a long history, of course, in working in the derivatives business. Traditionally, the way practitioners have learn about hedging derivatives, as you, most of you probably know, is to use what are called the greek letters. Greek letters delta, gamma, Vega and so on, they're all partial derivatives. For those of you who, like me, were forced to study ancient Greek at high school, let me just admit that Vega is not actually a greek letter, but everybody calls it a greek letter for this, in this context. So, but anyway, we're talking about the partial derivative of the value of the portfolio with respect to various things that define the environment, asset prices, volatilities and so on.
00:04:11.424 - 00:05:28.774, Speaker B: And periodically when you do the traditional approach, you set up these hedges every day to try and minimize, maybe even eliminate these greek letters. So what you're doing, in fact, is creating a hedge, an instantaneous hedge, over the next short period of time. Doesn't matter what, doesn't matter what happens to the particular variable when it's a volatility or an asset price or whatever. The value of your portfolio shouldn't change at all or shouldn't change very much. And so you're minimizing the change in the value of the portfolio of the next very short period of time. One thing that's always worried me about this is that it's not really what the hedger wants to do in that, you know, decreases in the value of the portfolio, of course, are a concern that increases, you're quite happy about. But what the traditional hedging approach does is to say increases in the value of the portfolio are just as bad as decreases.
00:05:28.774 - 00:06:27.244, Speaker B: We just want to keep the portfolio where it is. So it'd be nice to have an objective function which reflects what the hedger really wants to do. So this is where the reinforcement learning alternative comes in. We want, you know, whereas the traditional hedging approach takes a very short term look and says, you know, over the next very short period of time, we want to make sure there's not going to be a big change in the value of the portfolio. Reinforcement learning says over the next little while, maybe next ten days, next 20 days, we want to make sure we're well hedged. We want to make sure that nothing bad happens to this portfolio. And the potential advantages of using reinforcement learning, you have freedom to choose the objective function.
00:06:27.244 - 00:07:42.844, Speaker B: Obviously you're going to penalize losses and maybe even, you know, reward gains. And Sebastian's group has, has done work in that area. We also find that one of the big advantages is that transaction costs are lower when you take a longer term perspective. You don't do this thing of, you know, taking a certain position today and then unwinding the position tomorrow, which is quite, quite expensive in terms of transaction costs. And of course, it can be made consistent with the way in which the hedgers performance is assessed. You know, if the hedges performance is assessed at the end of each month, then it's been natural to have a reinforcement learning, reinforcement learning implementation which optimizes what happens during the month and has, in a sense, the end of the month as a time horizon. So I'm going to hand over to zsys to explain how we do the reinforcement learning approach in a moment.
00:07:42.844 - 00:08:39.560, Speaker B: But I'll give you the sort of simple two cent worth description of it here before I hand over. I mean, it's designed for, I mean, reinforcement learning as a machine learning tool is designed for multi period decision making in an uncertain environment. It's got lots of applications to that. And that's exactly what hedging is. Hedging is a multi period decision making task in an uncertain environment, an environment that's changing. You aim to find the best policy. So you're looking at all the states that could happen and thinking, you know, what's the best thing to do if this state happens after this amount of time, and so on and so on, what it does, and zsys will have more to say about this.
00:08:39.560 - 00:09:26.094, Speaker B: It starts with a random policy and iterates to improve the objective function, uses what's known as an actor critic approach. And in our basic application, you can think of maybe, you know, you're just hedging one or more derivatives dependent on a particular asset price. So the states are the asset price, the time to maturity of the, if you're looking at just one option, the time to maturity of that option, and the current holding that you have in whatever you're using to hedge that option with. Okay, so I think that's the point at which I hand over to you, ISIS.
00:09:29.834 - 00:10:20.216, Speaker C: Making sure the mic works. Yeah. All right, I'll just set up here. All right, thank you, John. Thanks everyone, for having us here. Yeah, so, as John said, well, it's actually several applications and quantitative finance that boil down to this concept of finding good sequences of actions in a partially observable MDB. And when we started this work dealing with delta hedging, let's say at scale two years ago in 2019, we looked into reinforcement learning as they go to framework for several reasons to accommodate or to facilitate a particular objective function we had in mind.
00:10:20.216 - 00:11:11.530, Speaker C: But I will try to explain how from that point in 2019, from the first paper until now, we moved to a different class of reinforcement learning algorithms, which are collectively referred to as risk aware reinforcement learning, particularly distributional reinforcement learning in our case. So I'll just try to motivate that before doing that. Well, I'm pretty sure many of you are familiar with the basic concepts here. I'll just go over this quite briefly and this partially observable MDP setup. We have the concepts of states, actions, rewards, and episodes. States in our context are essentially variables that summarize what we observe in the environment. That would be the market, and that in our case, could be prices, asset prices.
00:11:11.530 - 00:11:51.456, Speaker C: It could be the implied volatility, volatilities. It could be the composition of the current portfolio, and so on. Then we have actions which in our context would be hedging decisions, trading in and out of an underlying asset, or buying, selling a hedging instrument, such as not the money vanilla option. And then we have rewards with the random variables representing some sort of feedback to the agent, often the cash flow or. I'm counting p and l, right. The value of our portfolio yesterday versus the value of our portfolio today. And what usually happens is we have this concept of an episode.
00:11:51.456 - 00:12:13.362, Speaker C: The agent will start at some. There's no leader pointing here, right, there is. Oh, there is. Oh, nice. Something out of battery maybe? That's fine, that's okay. I'll be pointing. So the agent starts at some initial state s not right.
00:12:13.362 - 00:13:26.640, Speaker C: They'll just take an action, and then based on the environment's dynamics, in this particular case, the markets dynamics, plus, possibly because of the action they took at time zero, they'll just land into a new state, s, one they'll observe and receive a reward, one, and then take another action, move to the second state, up until some terminal state st, they will observe a final reward, reward, and then that's the horizon of that process. Now, of course, you'll probably be encountering multiple different states giving rise to multiple different episodes. So generally, the call in standard reinforcement learning is to maximize what we say, the expected return, which is essentially the discounted cumulative reward from point t or time t until the horizon. And that's great. Now, when it comes to risk management, when it comes to risk management, I think the battery actually died completely. Oh, yeah, of course I can do that. Unless there's another button that I press.
00:13:26.640 - 00:13:54.472, Speaker C: No. Going old school. Well, easier sent than done. Is this a PDF? Okay, it's moving. All right, so what was I. Yes. So, standard reinforcement learning, that's okay.
00:13:54.472 - 00:15:00.696, Speaker C: For example, in a, in a risk management scenario, if you can express your risk measure as an expectation, then you can work with the standard reinforcement learning setting. But, you know, if you're interested in a wide range of objective functions, risk measures, such as value at risk, or expected shortfall, mean, standard deviation, so on, it might be useful to have, you know, a mechanism or a framework that can facilitate easier optimization here, right. Or it can facilitate this wider range of objective functions. Right. So, at some point, we encountered a methodology and we used it, a methodology that estimates various moments of the return. So if your objective function or your risk measure can be expressed as a function of different moments of the return, then you can just fall back to standard reinforcement learning. Later on, we found it very convenient to work with another class of algorithms, which are called distributional, which we call distributional reinforcement learning.
00:15:00.696 - 00:16:37.554, Speaker C: And you'll see in a bit that this, the idea there is that you approximate the entire terminal distribution and where you can basically measure any attribute of the distribution to plug into your objective function, right? So here's what happens with, you know, this first class of algorithms. I think first time we saw this was work by Tamar in 2017, I believe, or 16, maybe. And this is what we used in the first paper that John had in the slides where we dealt with delta hedging in the presence of transaction costs. So you take a look at your objective function, and you find out that you're interested in mean standard deviation minimization, let's say, right? So your min, standard deviation objective can be written as this, okay? Like the expected cost, right, where c, sub t is the expected cost, let's say, from time t until the horizon, plus a constant times the standard deviation of the cost. And, you know, here it is a minimization problem. But of course, you can declare your rewards to be negative costs, and then you can have an equivalent maximization taking place. So Tamar, in 2016, in that paper called the learning the variance of the reward to go, proposed a solution where you have this well known q function representing the expected cumulative reward, given that you are in state s, and you take some action a.
00:16:37.554 - 00:17:21.714, Speaker C: In that work, they said, listen, take that q function and estimate the expected cost, and then use a secondary q function to estimate the expected cost squared. And then, since your objective function can be synthesized using these two point estimates, you can obtain a value, a point estimate for your objective function, and then you can further optimize. Right? Now, the problem with this method. It's a great method. It worked for us. It's computationally expensive, very intensive. I would say it actually, behind this queue, two Q functions, you have four deep neural networks, actually, that need to be coordinated in the learning process.
00:17:21.714 - 00:17:53.338, Speaker C: So it's quite computationally intensive, and also, it doesn't meet any arbitrary risk measure that you have in mind. Now, more recently, we've been working with this types of frameworks, the distributional reinforcement learning framework. So here's what's going on. You have time, I minus one, right? You somehow took an action there. You landed in state Si, received reward Ri. For the sake of simplicity, assume you only have two actions to choose from. In reality, action space is continuous, which is also the case in our experiments.
00:17:53.338 - 00:19:05.358, Speaker C: But, you know, assume that it's only two actions. You have to have a mechanism to discriminate between the two, right? And suppose now that you have a way, a mechanism to estimate, to estimate what would the have to go back? Not used to this at all, for some reason. So say you have a mechanism to estimate the distribution of the terminal return, given that you're in state s and you take action AI. And you can also do the same for action AI prime. Right? Now, at this point, since you have approximations of the terminal distributions, you can measure any attribute, including variance, expected, shortfall value at risk, and have a straightforward way to compare and figure out which one is the best action. Now, what happens under the hood is that the policy itself, which is the rule that we followed, given that state, we have to take that action, and the estimates as well. All those are approximated, or we represented as parameterized models using neural networks, deep neural networks.
00:19:05.358 - 00:20:02.330, Speaker C: Specifically, we can have actor critic architectures, or other architectures, as we'll see. Now, the first algorithm I would like to talk about. So I will present one algorithm very briefly, and then another one, and then I'll tell you that the one we used was a fusion of the two and why we did that, right? So, about 2018, I think Barth Maron came up with an algorithm called d four PG. You'll see why in a bit. What happens here is that the policy itself is represented with an actor network, takes in a state, and determines the action that should be taken in that state. And then there's a critic network that takes pairs of states actions and then estimates the terminal distribution from that, from that point onwards. Now, of course, you can parameterize your terminal distribution any way you want.
00:20:02.330 - 00:20:39.638, Speaker C: Maybe it's a gaussian. So you would have to estimate two parameters maybe it's a mixture of gaussian. So you would have to estimate three parameters per component of the mixture, right? The weight, the mean and the standard deviation. They empirically determine that a categorical representation is the most meaningful thing to do. So the way they did that is by determining a fixed amount of atoms, each atom corresponding to a value of the terminal, sorry, of the return. The terminal return. And then the critics job is to figure out the probabilities of these atoms.
00:20:39.638 - 00:21:21.928, Speaker C: And they showed that 51 fixed points were good enough for the benchmarks they used. That's why the algorithm is referred to as c 51 for that reason. Okay, so that was introduced by Bach Maron in 2018. It's called d four, because you have distributed, distributional, deep deterministic policy gradients in there, d four for short. Ours is something like d three and a half, but I'll explain later. Now, around the same time, I think we have another thread of work where another baseline algorithm, Dqn, was extended. So, Dqn uses q functions, traditionally using q networks, to estimate the q functions.
00:21:21.928 - 00:22:14.286, Speaker C: So there was an extension of this to facilitate, again, distributional representations of the return of the terminal return, but in a different fashion. So here we have a quantile representation of the return. What happens is, instead of starting with a fixed point, fixed set of atoms, you have a fixed number of quantiles, right? In the previous case, the atoms were equidistant in terms of values. Now the quantiles are equiprobable. So if you have n quantiles, you have probability one over n per quantile. And then the optimizations task is to figure out what the right positions are for this quantiles, to have this quantile function estimated sufficiently well for the return. So then the rest of the algorithm is similar to the traditional DQn.
00:22:14.286 - 00:23:10.510, Speaker C: So this is introduced by dummy in 2018. So around the same time. So what we did is we figured out that a combination of the two worked really well for hedging derivatives. What we did is we inherited several techniques from d four pg, which, remember, use a c 51 categorical distribution for the return, and we swapped it with quantile regression. So we now had all the good sample efficiency improvements from d four pg, but some, you know, probably a better representation for the return. And it did prove to be better, because we achieved better results when it comes to value at risk, expected shortfall on par results when it comes to mean standard deviation objectives. And actually very recently, this method was further improved.
00:23:10.510 - 00:23:53.224, Speaker C: That was primarily because of Parvin. She's somewhere here in the audience. Ms. Parvin, Malik Zadek and Professor poetagnotis is also here in the audience. We improved the quantile regression process in that method by introducing a generalized quantile, Huber loss, to handle outliers better and to also remove the need to play with hyper parameters in the classical Huber loss function, because especially in finance, you have other problems to worry about, such as model misspecification. So worrying about one less hyper parameter is generally desirable. And this is going to appear sometime in April in Ieee icasp now.
00:23:53.224 - 00:24:35.072, Speaker C: So this is the algorithm we use. We call it d four BTQR. It's the fusion of the Tu. When it comes to the formulation for the latest paper, the one that John is going to be focusing on, the state variables are quite simple. We only include the price of the underlying, the time to maturity, and the current holdings. Of course, when we deal with multi option portfolios in other papers where we do gamma or Vega hedging, we would include other state variables here to enrich the state space for that problem. The action is almost always the position we take in the underlying across different formulations and the reward, depending whether you have costs or not.
00:24:35.072 - 00:25:23.050, Speaker C: So in the general case, when you have costs, it would be something like the, you know, it would be the accounting p and l, essentially, where you pay a cost denoted as kappa and hi is essentially your hedging action. Vi would be, for example, the price of the option you're using to hedge. And of course, a component here that corresponds to the change in portfolio value from time I minus one to time I. If there's no cost, that will be effectively easier. Now, in what, professor, how will present. There's a bit of a twist, because we're dealing with barrier options. So in that case, we end up with episodes of variable length.
00:25:23.050 - 00:26:04.624, Speaker C: Why? Because our terminal conditions. There's two terminal conditions. One is we have a breach before expiry, and the other one is we mature without any breach in the barrier option. Now, that doesn't mean anything, but really what happens is once we have a breach, the option is now a vanilla. So we fall back to be assembled by Hetty. So we don't need to deploy an agent once there's a breach on the barrier. However, even if that's the case, the agent needs to be aware of cumulated rewards or cumulative p and l from that point until the expiry.
00:26:04.624 - 00:26:40.024, Speaker C: So we just keep simulating and accounting for those, and we summarize them in the reward prior to the bridge. So you might. So effectively, you end up with variable length episodes. But that doesn't mean that the reward prior to the terminal state is going to ignore what happens later, right after the bridge. So, I don't know, I think I'll just pass it to you, John, to give some, present some results on this particular methodology. And as I said, this is the simplest version. In other papers you can see that states are extended to include many other variables observed in the market.
00:26:40.644 - 00:26:41.892, Speaker B: Okay, thank you.
00:26:41.948 - 00:26:47.104, Speaker C: Okay, turn off the mic.
00:26:50.164 - 00:27:29.364, Speaker B: Okay. Yeah, I'll move up here then I can see everybody better. So. Okay, so now going to, shall we say, less technical material in a way. Well, what we're going to be doing is looking at some of the results of using the reinforcement learning algorithm as is described. This particular slide explains why the saving and transaction costs. I mean, it's fairly straightforward.
00:27:29.364 - 00:28:32.358, Speaker B: Imagine a situation where you're hedging a derivative and or portfolio of derivatives. The delta starts off at 50. If there's a particular movement in the underlying asset, delta could move up to 70. And then the next period, the movement, asset price might move down and delta could move back to 50 or it might move up to 90. So, you know, we could have, you know, this is not a sort of totally ridiculous potential set of movements as far as the thing you're trying to hedge, which is the delta of the underlying delta, is just the sensitivity to the price of the underlying asset. So one strategy is just to hedge delta in the usual way, and that's strategy one. And then the other strategy is wait until the end of the second period because, you know, at the end of the second period, you're either going to be back where you started or you're going to be up to delta equals 90.
00:28:32.358 - 00:29:27.274, Speaker B: And it just turns out that the second strategy, and you can easily verify this for yourself, the second strategy involves half as much hedging as the first strategy. This is a fairly extreme example. As I say, the saving in transaction costs from using reinforcement learning versus traditional hedging methods is something like 20% to 25%. In other words, there's 20% to 25% less transactions that are undertaken, which can be a big saving. Okay, so I'm now going to talk a bit about this last paper of ours, which is the barrier options paper. And, you know, the first two papers we looked at what are called plain vanilla options. And, you know, we first of all looked at a single plain vanilla option.
00:29:27.274 - 00:30:23.464, Speaker B: Then we looked at portfolios of plain vanilla options and so on. And we found that reinforcement learning works pretty well for plain vanilla options, particularly as far as the transaction cost savings are concerned, but also, you know, in, in terms of the objective function. But we thought it was interesting to sort of extend what we'd done to an exotic option, as they're called. And the natural one to choose is the barrier option because barrier options are very, are amongst the most common exotic options. And they have this interesting property that if you look at the delta of the option, again, delta is the sensitivity of the price of the option to the underlying asset price. If you look at Delta, it's actually got a discontinuity. In this diagram here, the barrier is at ten.
00:30:23.464 - 00:31:01.754, Speaker B: What that means, this is actually what's known as a down and in put option. So if you cross the barrier, it becomes a put option. But if you never cross the barrier, then it's never worth anything. So the question is, in this particular case, the barrier is at ten. So critical issue is whether the asset price actually crosses that ten point or not. The strike price is eleven. Of course, the strike price is only important if it actually becomes a put option.
00:31:01.754 - 00:32:21.172, Speaker B: Volatility is 30% and the maturity is ten days. So as I say, what you find is that you have this discontinuity in Delta, which means that if you just try and use regular delta hedging to, to hedge this option, it's not going to work well, because, you know, you'll be hedging. I mean, one of the problems is, of course, you, you hedge discretely, you don't hedge continuously. And so when you're getting close to that barrier, you could be thinking, well, one of two things is going to happen over the next day. Either we're going to jump over the barrier, in which case it's going to become a put option, or else, you know, we're going to jump the other way or nothing much is going to happen, in which case, you know, it'll, it'll, it'll stay the way it was before. So it's, you know, the lack of continuity creates problems as far as delta hedging is concerned. So we, and this is similar to what we did in our earlier papers, we compare two, three actual hedging strategies.
00:32:21.172 - 00:33:39.214, Speaker B: We compare regular delta hedging with myopic hedging, as we call it, which is each day the trader just looks one day ahead. And whatever the trader's objective function is, tries to optimize the objective function. But just looking at what the world might look like in one day's time and then reinforcement learning, hedging where, chooses a strategy involving daily rebalancing so we assume that you only rebalance once a day and it's optimal for the objective function over the life of the option. Okay, so we're just looking at one option just to make life simple in this situation. And we're choosing a strategy that is going to give you the best result, not between today and tomorrow, not over the next very short period of time, but give you the best result over the life of the option, as it turns out. So, assuming a time horizon, which is the life of the option, we used two different objective functions, quite simple ones, actually. Value at risk 95 bar 95, which is just the 95th percentile of the loss distribution.
00:33:39.214 - 00:34:32.204, Speaker B: So we want to minimize that. We want to make the 95th percent of the loss distribution as low as possible. And then c bar 95, which looks at the expected loss over the life of the option when the loss is larger than the 95th percentile. So you look at the tail of the loss distribution and look at the expected loss in that tail, and you want to minimize, minimize what? What that will be. And you might not like to know. We call it c bar 95, but it's also called expected shortfall or expected tail loss. So here's some results for the value of the objective function.
00:34:32.204 - 00:35:48.434, Speaker B: This is for just one particular case, obviously, we looked at quite a lot of other cases as well, but the price is 10.6 initially, the barriers at ten, the maturity is 20 days, the volatility is 30%, and the options on 100 units of assets of the asset. And we have daily rebalancing. So just running your eyes down this table, you can see that RL always does better than delta and myopic hedging, whether we're talking about value at risk or conditional value at risk. And that's not too surprising because reinforcement learning has got that particular objective function, you know, embedded in it, if you like. So, so, you know, if you, if you choose an objective function which is designed to do well as far as value at risk 95 or c bar 95 is concerned, it's not surprising that it does actually outperform other methods that are not designed to do that. But you can see this, you know, this, you know, a reasonable, you know, improvement as far as that's concerned.
00:35:48.434 - 00:36:50.844, Speaker B: And we thought it was interesting to sort of try and quantify where the improvements come from and near the barrier. We find that RL under hedges relative to Delta, because remember what that diagram showed? The diagram showed that actually if you hit the barrier, there was a jump up. As far as Delta is concerned, if you're just close to the barrier. You don't know whether you're going to hit the barrier or not. And if you're just hedging over the next instantaneously small period of time, then, you know, you keep Delta to be, in magnitude, fairly high. But if you're hedging once a day, you're hedging periodically. You recognize the fact that, you know, maybe there's going to be a big jump between today and tomorrow.
00:36:50.844 - 00:38:07.872, Speaker B: And reinforcement learning, hedging takes account of that and recognizes that Delta will decline if the barrier is breached. If you move away from the barrier, it turns out that it overhedges reinforcement learning slightly overheadgings because the cost is small in that case, because, you know, Delpha is small anyway if you're a long way away from the barrier. And so what we're talking about is a percentage increase in delta, which is fairly, you know, which actually is, in absolute terms, fairly small. We did, you know, quite a number of robustness tests, and this is one of them. Basically, what we said is in a reinforcement learning strategy, you set up your strategy based on certain assumptions. What happens if those assumptions don't turn out to be true? In particular, I mean, let's take the simple black scholars geometric brownian motion. Let's suppose you assume volatility is going to be 30%, but actually volatility turns out to be something else.
00:38:07.872 - 00:39:09.248, Speaker B: How well does the reinforcement learning strategy work? Well, this table shows you. I mean, if we look at the 30% row there, we see, of course, that reinforcement learning, uh, easily outperforms delta and myopic. It gives smaller numbers for bar 95 and c bar 95. And that's to be expected because you assumed a 30% volatility, and that was what you got. But if the as the volatility is different from what you assumed, you don't do quite as well, as you can see. But actually, it turns out, you know, for C VaR is remarkably robust as far as this is concerned. C VaR, you always do better than with reinforcement learning than you do with the Delta and myopic, even though your reinforcement learning strategy is based on the wrong, what proved to be the wrong volatility.
00:39:09.248 - 00:40:28.114, Speaker B: And I might add that what we assumed as far as Delta hedging was concerned here was that you were able to monitor volatility and you knew what the correct volatility is concerned. So we're assuming, we're comparing reinforcement learning where you don't have any control over, you don't change things as you find out new things about the volatility. We're comparing that with delta hedging, where, you know, how volatility changes. Here's some examples of the volume of trading, the average number of units of assets traded during the life of the option. For the examples considered, and, you know, fairly similar to what we get for hedging, plain vanilla options, let's say about it, 20% to 25% improvement. So we had a, as I said right at the beginning, one of the interesting things about Finhub is that we have these practitioners we work with. In fact, four out of six of the co authors on this paper were practitioners.
00:40:28.114 - 00:41:23.258, Speaker B: And they told us that actually, you know, if you're coming up with a trading strategy, what we do in practice is we look at particular scenarios. And one of the scenarios we look at is the scenario about March 2020, where the S and P 500, as you can see, sort of came down sharply and then moved up again. So they say, you know, there's this l shaped window, which is the first part of the scenario. And then there's the v shaped window, which is the second part of the scenario, overlaps the l shape window a bit, little bit. And one of the standard things we do is we check how well, whatever trading strategy is we're looking at would perform for those two things. And, yep, RL performs. It outperforms both myopic and delta.
00:41:23.258 - 00:42:24.494, Speaker B: For both the scenarios, losses were about. I mean, obviously, the scenarios are not good for your portfolio, and you do make losses most of the time, but losses were about 20% less than they were for delta and myopic. And again, we find that c bar outperforms var. We. This is similar to what we did in the second paper, you know, because, of course, delta hedging is not the only thing that you might want to replace if you're a trader. We have what is gamma and Vega hedging, where you hedge gamma hedging, you hedge the curvature as far as the relationship between the option price and the underlying is concerned. So in other words, it's the second partial derivative rather than the first.
00:42:24.494 - 00:43:12.230, Speaker B: It's the partial derivative of spectre delta. And in Vega, we look at sensitivity to volatility. So we. And we, you know, we found that, again, the saving and transaction costs were really important when we looked at plain vanilla options. When we looked at these barrier options, we found the same thing, that there was a big saving. Not only did we get better results with reinforcement learning, but there was a big saving in transaction costs. I mean, the key thing is that transaction costs of course are in order to hedge gamma and Vega, you have to bring options or other derivatives into your portfolio.
00:43:12.230 - 00:44:13.992, Speaker B: And you know, that's relatively expensive and there's relatively high transaction costs associated with those options. So any savings on transaction costs are more important because the bid offer spread is that much higher. Okay, there's a couple of things we're working on right now, which I thought I'd mention as we've described it. The reinforcement learning approach is something where you sit down one day, you run a model, you decide what your strategy is going to be for maybe the next 20 days, the next 30 days, the rest of the month, whatever it is, and then you just don't bother doing anything after that. You've got your computer program that tells you if this state is reached, then this is the action you take. If this state is reached, this is the action you take and so on. That's a bit unrealistic in fact.
00:44:13.992 - 00:45:28.104, Speaker B: And one thing we've been looking at is ways of making the strategy dynamic so that in fact the strategy adjusts as the environment changes. So for example, if you see volatility, you set your strategy for a volatility of 30%. But actually you observe the volatility is actually much higher than that. You probably do want to adjust your strategy or if the volatile is much lower than that, you probably want to adjust your strategy. So what we are proposing in this extension is the idea that actually you don't just set your reinforcement learning strategy once and leave it untouched, that actually you potentially revise the reinforcement learning strategy every day. So, you know, the software that dealers have these days is much, much better than our software, I might add. It's, but I mean, typically it only takes them a few minutes to run the reinforcement learning algorithm.
00:45:28.104 - 00:46:40.234, Speaker B: So running it every day, it's not unreasonable. In fact, you can use last days optimal strategy as the starting point for working out the optimal strategy for today. So, you know, one advantage of doing that is that you keep track of changes in the environment. But the other thing is of course that you might have more trades that you want to hedge and so you might want to update your hedging strategy just to reflect the fact that, you know, something unexpected happens, a customer came to you and wanted to do another trade with the same underlying. So that's one of the things we're looking at. The other thing which you know, could be separate or could be related to the first one is, you know, training the model using the observed behavior of the underlying asset rather than geometric brownian motion or some other model. And that's something that there's been a lot of research done recently.
00:46:40.234 - 00:47:54.328, Speaker B: And you can use generative AI to. I mean, you've got a historical data set for the underlying asset, which is not exactly going to correspond to geometric brownian motion or any other model you choose, but you have that data and you can use generative AI to expand that data set. So instead of basing your training on a particular model for the underlying asset, you can base it on, you know, historical behavior. So, and that's, we think we, you know, we think those are two, two important improvements as far as the application of reinforcement learning is concerned. So there are conclusions. You say we've loudly around ten minutes for questions, which is good. The advantages of using reinforcement learning, lower transaction costs, freedom to choose your objective function.
00:47:54.328 - 00:48:41.786, Speaker B: Our two objective functions, VaR and CVaR, were, you know, very simple objective functions. You could, without making it a lot more complicated, you could have a more complex objective function that rewards rewards gains as well as penalizes losses. We found it gets better results for some exotics than traditional approaches. Even when you take transaction costs out of it. You can choose a time horizon to match the way the trader's performance is assessed. And we find it's robust and gives good results during stressed periods. Disadvantage, of course, is that it's computationally more demanding than traditional approaches.
00:48:41.786 - 00:49:18.944, Speaker B: But as I say, this is not a real constraint these days with the sort of computer speeds that the dealers are dealing with. So JPMorgan has already moved a lot in the direction I'm talking about, and other dealers are looking at it as well. And, you know, I think this will be the way ahead. Big change coming as far as the way people actually hedge derivatives. Thank you. I'm sure you've got some questions.
