00:00:00.920 - 00:00:40.374, Speaker A: So we were discussing Dirac operators. We mostly would, mostly spent time, while muted, going off on a tangent about Dirac operators on odd dimensional manifolds. They all have index zeros. So to make an interesting problem, you have to do something else, which I'm not going to talk about. And let's get back to these guys here on even dimensional manifolds. And in order to construct the Dirac operator, the major part of the trouble is to figure out what the thing should act on. That's one of these spinner bundles.
00:00:40.374 - 00:02:14.864, Speaker A: As I discussed last time, when you build this operator, if you situate this operator within the context of Hilbert space theory, think of it as a so called unbounded operator. It becomes a symmetric operator, and that symmetry makes its index zero. So that's not the thing whose index you want to calculate, it's this thing whose index you want to calculate. It's part of the whole package of ideas of Clifford matrices and spinner bundles and so on, that the bundle STM actually breaks up into two pieces, a positive piece, direct sum and negative piece. And what the Dirac operator does is it exchanges them in the sense that it, the Dirac operator applied to a plus section, is always a minus section and vice versa. And so this thing is a Fredholm operator in the sense that it's kernel and co kernel of finite dimensional. And this is the index that we want to calculate, and more than one way to do this, but we're going to do this using k theory.
00:02:14.864 - 00:03:16.144, Speaker A: As I mentioned, there's potentially lots of Dirac operators out there. First of all, on a manifold m, there may be no Dirac operator, but if there is, there may be lots of genuinely inequivalent ones. So you have to specify which Dirac operator you're talking about. And the way you do that is you specify this particular choice of spinner bundle, this particular choice of bundle of a certain particular dimension two to the k. If m has dimension two k, and this bundle has to be equipped with a particular choice of so called Clifford multiplication, as we discussed last time, discussed in the notes. Okay? So you have to specify all of that. Before you can say what is the index, you have to ask, what is the operator? First? That means choosing or somehow identifying one of these stms, and then you can proceed.
00:03:16.144 - 00:04:04.188, Speaker A: Okay. And in order to describe the answer, we need to say a little bit about k theory. So this was invented by, of course, Satir and his Brook. So they invented an abelian for each locally compact, reasonable space, host of space. They invented k of x, a certain abelian group. Sir, I had something to ask. Yeah, so STM is the tangent model, or Em is the tangent bundle.
00:04:04.188 - 00:05:00.836, Speaker A: S of Tm is something that you associate to the tangent bundle. It's a complex permission z two graded bundle equipped with one of these clippered multiplications of the tangent bundle. So it's possible to think of each tangent vector over a point of m as being an operator on the corresponding fiber of STm over that point. So that's that extra structure that s of Tm has. Anyway, back to k theory. This is was what, as I've said more than once, what Kasparov reinvented. He thought about how to define this better, and we've discussed what the definition is, and last time we discussed a quite general and maybe a little bit complicated notion of cycle for this group.
00:05:00.836 - 00:06:34.842, Speaker A: So according to Kasparov, if you have families of. I'm not going to write down the whole definition that we discussed last time, but you're supposed to have, you can build a k theory class from suitable families of essentially self rejoined compact resolvent, which is like saying fret home. Maybe I should be a little bit careful here we have this funny notion of unbounded Hilbert space operators, which are only partially defined. These things, whatever they are, in detail, and that's what we discussed last time. Define classes in this group. And if the Hilbert spaces here are infinite dimensional, well, that's where Kastrov's theory achieves its full. But if the Hilbert spaces are infinite dimensional, you're, you're quite a bit away from what Tyr and Herzebrouck originally taught us about.
00:06:34.842 - 00:07:31.996, Speaker A: If X is a compact space, the way you build. Define this abelian group K of X is you look at all vector bundles over the compact space. You build the so called growth and deep group of all isomorphism classes and vector bundles. And that's what K of X is, an infinite dimensional things involving families of unbounded operators on infinite dimensional spaces. That's a little bit far from what a tiering and thinking about. But there's an interesting intermediate collection of examples which are in the world of Kasparov on the one hand, but they're also not far at all from the world of etira and Hasbrouck on the other hand. And that's what we talked about kind of briefly, what we talked about last time.
00:07:31.996 - 00:08:39.854, Speaker A: And I think I made a mistake last time. So one of the things I can do right now is correct that there is this famous bot element in the k theory of an even dimensional euclidean vector space, like at the top here. And for this we need a collection of essentially joint self adjoint operators mapping Hilbert spaces to Hilbert spaces, some family of Hilbert spaces to itself. And those Hilbert spaces should of course be parameterized by e. And it's kind of nice how it's done. You just take Clifford multiplication by e as a map on a spinner vector space just for the single vector space e. So s is not a bundle, it's just a single vector space, as we were discussing, on which, for example, every orthonormal basis of the vector space e gives rise to a family or finite family of Clifford matrices, as we discussed at the very beginning of the last lecture.
00:08:39.854 - 00:09:24.294, Speaker A: And I left a little bit of space here because according to our axioms, c of e is skewer joint, whereas according to Kasparov, well, Kasparov wants to consider sulfur joint elements. I mean, there's not a big difference between skewer joint and sulfur joint. You can always multiply by the square root of minus one. It's a little better to multiply by the grading operator like this. So gamma is the operator on s, which is equal to one on the positively graded part of s and minus one on the negatively graded part. And when you look at this fellow where there's a multiplication, some multiplication by minus one, somewhere, somewhere, this converts co v into a self adjoint operator, which is kind of nice. And that's it.
00:09:24.294 - 00:10:16.210, Speaker A: So here's a family of, well, essentially self adjoint compact resolvent operators. Are they unbounded? Well, not really, because each, this s is a finite dimensional Hilbert space and C of U is just a matrix. It's not densely defined. Any dense subspace of a finite dimensional space, of course, has to be the whole space. But somehow, by default, I mean, unbounded is supposed to mean not necessarily bounded. In that sense, CoV is an unbounded or gamma times C of E is an unbounded, essentially self adjoint operated. This family is unbounded in a second sense, which is that as e, the vector e, little e inside of big e as it wanders off to infinity, the norm of this operator, gamma times cv, the norm goes to infinity.
00:10:16.210 - 00:10:50.510, Speaker A: So the family is unbounded even if it's point wise bounded. And that means that Kasparov is doing a little bit of something or other forest when he builds this, when he describes this construction. Yes, yes. So, yeah, but I mean, these are linear operators. Yes. Which actually subspace. Yes.
00:10:50.510 - 00:11:19.170, Speaker A: Yeah, dense. These things should be defined on subspaces otherwise, it's hard to express what it means for the operator to be a linear operator. God forbid that we should consider non linear operators. Oh, my heavens. Anyway, so this is an example. And the point about this example, the reason I'm harping on about it, is that it's sort of halfway between what, you know, the full potential of Kasparov's theory, which allows you to deal with infinite dimensional fibers. Hilbert space is s here.
00:11:19.170 - 00:12:03.282, Speaker A: S is finite dimensional. It's in between that and what Etir and Hasselbroeck invented. And it's no big deal to take this element, which is written in Kasparov's speak and convert it into something which is written in tier Hersenbrook speech. So the topologists know exactly how to handle these elements, and they know how to calculate these using their chin characters and so on. This is perfectly within the realm of easy management by the algebraic topologists. In fact, when Etiran Husserbruck invented k theory, they invented it almost exactly to find a place to put these elements. That was before there was k theory.
00:12:03.282 - 00:12:42.470, Speaker A: There was this type of thing. And so this somehow predates k theory. It's the reason that k theory exists to find a place to put a construction like this. All right? And then there was a slight variation on this small generalization. So this guy here, we gave a name to, let me give it a name to it here as well. I think we called it beta of e s. It doesn't, I mean, there aren't many choices for s.
00:12:42.470 - 00:13:22.544, Speaker A: If e is just a single vector space up to ismophsem, there are two, and they only differ by the z mod two grating here. It's a little more complicated. If you have a vector bundle. How did I do this? A compact space like we have here, and a real euclidean vector bundle over it. You can ask for a bundle over m, which is equipped with one of these Clifford multiplications that we were talking about. And if that's the case, you can build this sort of thing again. Gamma of C of E.
00:13:22.544 - 00:13:57.788, Speaker A: Maybe I'll call it v for vector. Should be a map from s of PI of v. I mean the fiber at PI of v. Maybe I should call it something like this. Sv at PI of vm. What did I call it? Just so v. So this is a family again, of operators.
00:13:57.788 - 00:14:26.328, Speaker A: They're all essentially self adjoint. If I remember to put in the gamma, which I did, they're all essentially self adjoint. There are gradings on the fibers, which there have to be this compact resolving condition is satisfied, all of the technicalities are satisfied. And this gives another example. And this is so similar to the first one that it's also true for this, that this is well within the realm of the attir Hersenbrook theory. It's easy for the topologists. Even before there was k theory, it was easy for the topologists to understand what was going on here.
00:14:26.328 - 00:15:05.104, Speaker A: Actually, there was a small gap. The first paper that Etiran Herzeburg wrote, they had a conjecture about this element, but by the second paper they figured it out. All right, is this picture where you're looking at operating entity, is that. Yeah, that's exactly what's going on here. Like these are complexes of length two, or indexed, if you like, by z mod two instead of by z. Finitely supported complexes. Obviously, it's indexed by z mod two.
00:15:05.104 - 00:15:59.544, Speaker A: Ah, that's a good point. Maybe that's worth talking about. Let me put a little asterisk here. If you have a couple of. What shall I call these guys? I'm running out of good letters. V stands for a real vector bundle here, and s stands for spinner bundle. I don't know, t one minus t two each.
00:15:59.544 - 00:17:19.223, Speaker A: Of course, you have a couple of complex vector bundles. Each complex vector bundle determines a class in the k theory of x according to a tier in Herselberg, h stands for. And you can get this class from the Kasparov point of view, just by looking at the family of zero operators. From the fibers of h one to the fibers of h 20 is not much of an upper rate. Oh, hang on, that's not quite right, because we want this self adjoint thing. So let me fix that somewhere. What we have.
00:17:19.223 - 00:18:15.864, Speaker A: Sir, I had just one other question. When you say the equivalence class h two, so it means that all operators h I says that this. I mean, when subtract, what is the equivalence class? I mean, how are you asking about this top line that I'm pointing to? No, the line. Second line. This one? No, the line above you mentioned about an equivalence class. I just wanted to know how the equivalence class comes in. I'm not sure exactly what you're referring to, but these families that Kasparov builds, discussed over here on this board, what Kasparov says is that k theory is homotopy classes of families of the type that I'm discussing.
00:18:15.864 - 00:18:48.334, Speaker A: So the equivalence relation is homotopy, and a homotopy between two cycles over x is a cycle over x times closed unit into. Thank you. We'll see some homotopies today. All right, good. So it's very easy to go in one direction, not so easy to go in the other direction. And there are extra problems when x is not compact. But if X is compact, the relationship between, in one direction, between the tier Hersenberg theory and Kasparov's theory, is very easy.
00:18:48.334 - 00:20:27.674, Speaker A: All right, good. And one more thing, in the index formula, maybe I'll write it like this. So one of the things thatir and Singer already knew when they started out to prove the index theorem is they already knew what the answer was, at least for these Dirac operators, and because they had a lot of examples and they knew a lot of algebraic geometry, so they knew what the index of the Dirac operator was going to be, and they knew for sure that a construction related to this Tom element and related somehow to the way that you fit the normal bundle of the manifold into em, for example, using some tubular neighborhood construction, they knew that this was going to be relevant. And we have a nice way of understanding what that dotted arrow means, which is going to play a role here. Suppose you take the k theory of the deformation space associated with the embedding of m in e, the deformation to the normal cone. Let's not consider the whole thing. Let's, this manifold is equipped with a submersion down to the real line, and let's just consider the bit between zero and one.
00:20:27.674 - 00:21:43.438, Speaker A: Like that. P theory is a contravariant functor for reasonable maps of locally compact spaces. And so you can restrict t is equal to zero, and you can restrict that t is equal to one when t is equal to one. This deformation to the normal cone, this family of smooth manifolds parameterized by the real line, the fiber is just Ian, but t is equal to zero is special, and the fiber there is the normal bundle. And by virtue of what we were saying when we were listing the properties of k theory, this restriction is an isomorphism, because if you take the zero fiber and you subtract it out of this big space that we have at the top of the picture, what's left is a locally compact space which is contractible in the sense of locally compact spaces. It means that the one point compactification is contractible to a point through maps which preserve the base point. And so that means that there is the associated restriction map versus restriction at one, if you like, and his restriction at zero.
00:21:43.438 - 00:22:17.808, Speaker A: The restriction at zero map is an isomorphism. The restriction at one need not be. But anyway, from this diagram is now a unique map that you can put here, and that's what he and Singer knew was relevant. This particular map, they didn't build it using deformations to the normal cones, but they used. Built it using tubular neighborhood embeddings. But the deformation to the normal cone is a little bit better in a way, because it's completely canonical. You don't have to do.
00:22:17.808 - 00:22:54.324, Speaker A: If you have the normal bundle, it's some big thing which is straight in the fiber directions. The fibers are vector spaces. And to take that normal bundle and fit it as an open subset of e, you have to push down the fibers, you have to deform the normal bundle to make it squashed somehow like a disk bundle, and then you can sort of attempt to embed it in. So it's much more natural to do this. And we'll take advantage of that in a moment. Yeah. This space, the part of the normal bundle which lives between zero and one, after you remove the part which is exactly over zero.
00:22:54.324 - 00:23:21.824, Speaker A: So a half open interval. Yeah. And that's because. No, it has nothing to do with the geometry of e. Any half open interval times a locally compact space is contractible in the sense I was just talking about. So even if e was any manifold, you could still do this construction here. All right.
00:23:21.824 - 00:23:51.566, Speaker A: And we were calling this something like iota. It's an example of this so called wrong way functoriality of k theory. There's an inclusion map. Let's say there wants to be an inclusion map from the normal bundle into either given by some tubular neighborhood embedding. And k theory is supposed to be contravariantly functorial. So there wants to be a map from k of e back to k of nm. But we're not interested in that map, which doesn't actually exist.
00:23:51.566 - 00:24:24.986, Speaker A: Just wants to exist. But the other map, we. This wrong way map is what we're interested in. No, these are all topological spaces. I'm. Instead of writing this in this lecture, I'm k zero of x. I'm just c zero of X.
00:24:24.986 - 00:25:19.384, Speaker A: I'm just going to write K theory of X in homage to Etiran Herzberg. Where are they here? Because it's all about. We're trying to push this entire index problem into the realm of the algebraic topologists. We're going to get it so far into the realm of the algebraic topologists that we can just say to them, well, over to you now, our work is done like Superman, and we can zoom off, fly off into the sky, and the topologists can, on the ground, scurry around like pigeons with lots of crumbs and figure out what the index theorem actually says. Yeah, I think I figured open zero, close one kind of one point clarity. Yes. And so you just slide down the problem just to.
00:25:19.384 - 00:26:07.664, Speaker A: So if here is e and maybe e is a circle, then the space you'd get here would look like that. So the part of the DNC, in short, to repeat that you have, when you remove the special fiber, it looks. Maybe it's hard to think about it as a locally compact space, but when you one point compactify, which is built into the tier Herstenbruck theory, because it's a theory with compact support, you get a very simple picture. Okay. Yes. Oh, yeah. You had a question.
00:26:07.664 - 00:27:02.064, Speaker A: Yeah. So we had sketched this argument of Alencon using the tangent groupoid. And what we observed in that argument is that although Alain is trying, and successfully trying, he succeeds in converting the index problem to some problem, more or less formulated in the language that topologists can deal with. The conversion process is much more complicated, and so the conversion process needs further thought. The conversion process, as you say, involves some sort of harmonic analysis, some Fourier transformation. It's a little bit complicated. And what I'm trying to do is get to the essence of matters from the, at least from the point of view of the geometric people.
00:27:02.064 - 00:27:39.234, Speaker A: Okay. So I'm trying to resist saying in this lecture, seastar algebra, and certainly resist saying groupoid. What else should I resist saying? Not Hilbert space. We have Hilbert spaces. Not that there's anything wrong with sea star algebra. It is. Yes.
00:27:39.234 - 00:28:04.250, Speaker A: Yeah. Indeed there is a map from the one point compactification of this space to the one point compactification of this space. That's true. So, in that case, you're absolutely right. This is an instance where wrong way and right way functoriality happen to agree, because you're in the co dimension zero situation. Yeah. Yeah.
00:28:04.250 - 00:28:54.610, Speaker A: If you have a map from x to y, which is proper, which means that it extends to a map from the one point of x to the one point back, then what you get is a map of a pullback construction on Casper of cycles, which takes cycles for y back to cycles on for x. So it goes in the wrong way. But in this co dimension zero situation, where you're embedding nm, potentially, if you want to use tubular neighborhoods as an open subset of e, it just happens that wrong and right way functoriality agree. Just to add to the confusion. Okay, good. And what we're supposed to prove. So, although we have.
00:28:54.610 - 00:29:47.414, Speaker A: What I'm trying to say is that although there is this Kasparov stuff in the background. None of the Kasparov stuff, really, in a serious way, intrudes at least into the statement of this theorem. This statement, in other words, is intelligible to many more people than Kasparov. That's the claim. So it is, in that sense, an answer to the question, what is the index of d, which is an integer? And I'm speaking of this index down here, the second one, which is on the board, and it's a famous theorem of bot. But the bot element generates the k theory of e. This is a free abelian group with one generator, namely this thing.
00:29:47.414 - 00:30:35.504, Speaker A: What I've written down here is an element of the k theory of e. It's just index, some integer times the generator, and if you know what this element is, of course you know what the index is in principle, and we do know what this thing is. It's just this construction that we were just talking about evaluated on beta of m m, normal bundle, and then some spinner bundle for a normal bundle. And I'm not quite. So this is an identity. First of all, in the k theory of e, it makes sense. We have here apple equals apple, where two things are equal, which at least live in the same abelian group.
00:30:35.504 - 00:31:33.074, Speaker A: I was emphasizing that these spinner bundles are not unique. So you need to answer the question, which s are you talking about here? And that's answered by a little bit of additional hypotheses. If you take the trivial bundle over m with fiber s, this is a spinner bundle for the trivial bundle over m with fiber e. And if this thing is stm, hence the hat. So now I'm recalling some technical stuff that we were talking about last time, tensor s t, and not quite the math tells you you should put a bar over this. The bar of a vector bundle is complex. Vector bundle is the very same vector bundle, except you change complex multiplication.
00:31:33.074 - 00:33:37.164, Speaker A: So lambda times a vector in the new sense, like I said last time, is lambda bar times the vector in the old sense. So this is as spinner bundles for the trivial bundle e times m, which is the direct sum of t of m and the normal bundle, then that's the formula. So what's on the board? Apart from index of d, everything is completely finite, dimensional and geometric, and intelligible by people who study the works of Etir and Herzebrouck. That's supposed to be the point. And now what Kasparov does is, well, once you've absorbed the Kasparov theory in this funny little diagram here, it's obvious how you're supposed to prove this guy. So that's Kasparov's contribution. It's self evident what you're supposed to do, namely construct some class, like I said, in this big a theory group that we were writing down, so that when we apply this restriction to zero to it ending up here, we'll get the class m nms.
00:33:37.164 - 00:34:48.883, Speaker A: And on the other hand, when we apply res one, we'll get the bot generator times the index. Because if you can find this alpha, and if it does have these properties, then by definition of iota, star iota of this Tom element is the bot element times the index, and you're done. So you just have to make a particular construction. And what should the construction be? Well, it's supposed to be an element in the K theory of this deformation space, at least the part of the deformation space over zero one. So of course what we're going to do is build some family of essentially self adjoint operators parameterized by the points in this deformation space, and then we'll be done, if we can calculate what the restrictions are. And there aren't many alternatives really. So let's just see how it goes.
00:34:48.883 - 00:35:27.224, Speaker A: Good old Castro. And so I was beginning to tell you about the construction last time. Decided to fiddle with the signs a bit to make, well, possibly just to make the presentation correct, but also to make it more consistent with what we did a little bit earlier. So I have a couple of formulas here, which I worked out. They're very easy formulas. They're easy to check once you've written them down, but you have to think to write down the right formula. Anyway.
00:35:27.224 - 00:36:35.414, Speaker A: So I want to build a family of operators, like in the far whatever it is, left, no, right. Anyway, over there on that board, as far away from me as you can get, almost. I want to build a family in the sense of Caspar, of self adjoint, essentially self adjoint operators parameterized by the points of the deformation to the normal cone. And the way I'm going to do it is I'm going to build a submersion to the deformation to the normal cone, and the fibers will be certain manifolds, if this is a submersion, and then I can take the l two spaces of the manifolds, maybe I can throw in some vector bundles, spinners and so on. And that's what the family of Hilbert spaces is going to be. And then we just need to build the operators. And to make math work out nicely, in my opinion, you should do it like this.
00:36:35.414 - 00:37:36.994, Speaker A: This submersion is going to be a map of manifolds over the real line, if you like. So it's going to commute with the obvious projections down to r. So in the final coordinate, certainly want to put a t. And what I want to put in the intermediate coordinate is m minus te. The reason I want to do that is that such a map, as I've written down, automatically has a continuation to t is equal to zero, which looks like this. What I'll do is just take checking my notation proj m of e. So Proj M is just the projection from e to the tangent bond space at m of m.
00:37:36.994 - 00:38:11.414, Speaker A: Oh, I missed a minus sign, but I'll put it in so m times you have. Yeah. So does the map come from reality? Yeah, it's that map. But I. We're going to have to do a calculation. We're going to construct a bunch of operators and so it's much easier to, to know what they are. Yeah, sorry.
00:38:11.414 - 00:39:20.430, Speaker A: Yeah, I mean, that's comforting to know that there is this way of thinking about it which makes it familiar, seem familiar to certain people. So what do we know about this deformation to the normal cone? We know what the interesting smooth functions are on this space. There's a bunch of functions on this space which are smooth, and they're not very interesting. For example, the projection function from this space down to the real line is smooth. The interesting smooth functions look like this. You take a function on e, which vanishes on m, and then to build a smooth function on this space, if you have a function h on e which vanishes on m, you can look at this thing, h of m over, excuse me, h of e over t. So that's a function on the generic part of the, the deformation to the normal cone, the part away from the PI by t is equal to zero.
00:39:20.430 - 00:40:03.922, Speaker A: And it's a fact, it's by construction, by definition of the deformation to the normal cone that this smooth function, defined where t is not zero, extends to a smooth function when t is zero in the neighborhood of t is zero. That's what the deformation to the normal cone is all about. And what is the extension? Well, the extension is you just, a point in the fiber at t is equal to zero is some normal vector. And you just apply the normal vector to h. That's what the extension is. So if this map is to be a smooth map, it should have the property that when I compose with one of these special functions, like I was just describing, the composition should be smooth, and the composition is just going to be a function on m. Times z times r.
00:40:03.922 - 00:40:45.102, Speaker A: So we can easily check to see, see if it's smooth or not. What is the composition going to be? It's going to look like this, m e t goes to h of m minus te over t. And for sure, this thing extends smoothly when t is equal to zero. And when t is equal to zero, what you get is minus the derivative exactly as you want, minus, because of this minus. So I'm just indicating how you know that this is a nice smooth map. Yes. I didn't write it, but this is all correct.
00:40:45.102 - 00:41:46.084, Speaker A: If h vanishes on, if h is a smooth function on e, and it vanishes on m, then the thing in the top right hand corner, which is defined when t is not equal to zero, extends to a smooth function. And so whatever I write down for the formula for this map away from t is equal to zero, I want to check that it does extend to a smooth function, and it does by the criterion, by it does at least have to compose with these functions, h. All right, good. So this is the map we're going to study. And what are the fibers? Let me call them m sub et, like that. This is why I got the notes. Just get the signs right? Yeah.
00:41:46.084 - 00:42:59.864, Speaker A: The fiber over a normal vector ym. And when I say normal vector, I mean a vector which is perpendicular to the tangent space here. This you can easily calculate, this is just mxm minus y m, just because the minus sign is zero. So it's all really concrete. So we're taking m and we're taking advantage of the embedding to sort of stretch m, as you've seen. And in the limit of the stretching, what you get is a copy of the tangent space of m. Kind of nice.
00:42:59.864 - 00:44:29.954, Speaker A: And, yeah, where is my horizon? So we have a submission. We have a family of manifolds parameterized by the points of the deformation for the normal cone. And in order to do Kasparov theory, I want to put an operator on each manifold, and then I'm going to take the Kasparov element, the index element of that family, and that'll be my alpha, the one I'm talking about up here. So I'm going to build an operator. First of all, I need a vector bundle. Here's the spinner bundle on whose sections our Dirac operator acts. And let's just pull it back to m times e times r in the obvious way via the projection, just to m.
00:44:29.954 - 00:45:29.266, Speaker A: Now I want to build a bunch of operators on the fibers of our submersion. What's going on here is that w is a point in our deformation space. And what's bundle wise, what's going on is I'm taking the bundle STM, I'm pulling it back to this space, and then I'm just restricting it to a fiber. But I'm not going to write PI star of STM restricted to the fiber. It's too messy. So I'm just calling it STM. Okay, and what are these operators going to be? Well, there are two types of points in the deformation to the normal cone.
00:45:29.266 - 00:46:17.664, Speaker A: There are the generic points, and then there are the special ones. And for the generic ones, I'm just going to take t, extract the t coordinate and use that to multiply against the diracle. What about the Dirac operator? The Dirac operator acts on m, but what I'm doing is I'm identifying M with this fiber in the obvious way. What is the obvious way to identify this fiber met with m? Well, just projection onto the first coordinate. Under that projection, under that diffeomorphism, the bundle I've just built is obviously going to be just the original bundle stm that we started with. So it makes sense to say that on this fiber, the operator is going to be deep. Nothing complicated going on here.
00:46:17.664 - 00:47:27.296, Speaker A: And as for the special fibers over normal vectors, I'll just define those to be the symbol operators that we've seen again and again and again. The operators you get by choosing a local coordinate system, if you like, writing down the operator and local coordinates, freezing the coefficients at the point m and dropping the lower order terms. And that makes sense too, because if you take this bundle stm and you restrict it to the fibers at t is equal to zero. Here they are. Then, because we're pulling back the vector bundle stm over there by the projection onto m, you actually get a constant bundle over these zero fibers at t equal to zero. The bundle you get on which this operator, on whose sections of which this operator is acting, the bundle is the constant bundle, whose fiber is the fiber of Stm over the point little m. So it's all really simple there.
00:47:27.296 - 00:48:04.000, Speaker A: It's all constant coefficients at t is equal to zero. We're getting constant coefficient operators, which makes sense because the bundle on which these operators are acting are constant bundles. So easy, easy, easy. Yeah, sorry, can you, p goes from m times and r e the deformation to the normal cone. Oh, oh, I see. Sorry, there is a typo. Thank you very much.
00:48:04.000 - 00:48:40.644, Speaker A: M. Yeah, this projection, this is still correct. It's the orthogonal linear projection from the vector space e to this tangent space. Yeah, I'm thinking of these tangent spaces geometrically, like, you know, when you draw a sphere in three reals, you can see the tangent space. Thinking of that tangent space. All right, that was that. Yeah, we're not quite done.
00:48:40.644 - 00:49:41.114, Speaker A: This is not the family of operators that we want, but it's really close to the family of operators that we want. I need to fiddle with these operators little bit. And I'm going to fiddle with them by using, maybe I should call it cw two. Same thing. Let me put a c here, if you don't mind. Most of these fibers are compact, but not all fibers are compact. Let's just always have our operators acting on compactly supported functions.
00:49:41.114 - 00:50:29.014, Speaker A: Here is the s, which is the spin of vector space for e, in the formulation of the index theorem up there. So I'm speaking here of honest to goodness vector valued functions. There is no bundle except for the trivial bundle, maybe in this line. And this operator is going to be a super easy operator. I have a function, I guess I'll call it s anyway, and I want to evaluate it at a point of w. Maybe the point of w is some m e t, of course, point of mw. I mean, all points of mw have the form met because they lie inside the threefold product.
00:50:29.014 - 00:50:56.208, Speaker A: And all I'm going to do is I'm just going to multiply by Clifford multiplication by the e variable like this. Sorry. There we go. It's a multiplication operator. It's a matrix valued function on mw. And I'm multiplying by a matrix valued function. It's a slightly complicated matrix valued function.
00:50:56.208 - 00:52:04.234, Speaker A: If you restrict to one of these special tangent space fibers that we see here, it's a linear function. It's a linear matrix valued function. If you restrict to one of these fibers here, it's a complicated function which is a function of m because of this m that appears in this slot, it's Clifford multiplication by one over t times m minus c. So it's a complicated thing. So sumi, that you know just what it is. What is the gamma reference? Ah, this is the grading operator for the, for this, in this case, the spin of vector space s. And what we really want to study is the operator we just defined up there, plus the multiplication operator we just defined below.
00:52:04.234 - 00:53:10.512, Speaker A: And now this acts on the tensor product of st of m with, well, in the sense tensor product of vector bundles with the, just the constant bundle over m. Excuse me, mw with fiber s like that. That's it. These are the operators that we want to examine. So, yeah, that's what we want to examine and to get a sense. So it's a fact that these operators constitute a cycle for Kasparov's theory, and I'm not going to prove that. That's a matter of functionality analysis.
00:53:10.512 - 00:54:29.236, Speaker A: The essentially self adjoint condition isn't particularly obvious for any of these. Why, why are Dirac operators essentially self adjoint? Well, that's a theorem in analysis. What about when t is equal to zero? Now we're talking about non compact manifolds. Is it still true that the operators are essentially self adjoint? Well, yeah, it is true, even if you add this funny potential term, this one. But in order to make you feel better about this family, let's just try to calculate what's going on. And the most interesting case is when t is equal to zero. So for those operators, therefore, which lie over points in the deformation to the normal cone, which are actual normal vectors like this, what does this operator look like? The fiber on which it's acting is a vector space.
00:54:29.236 - 00:55:04.854, Speaker A: We identify that fiber somewhere, it's over here with a vector space. By projection onto this coordinate here, it just becomes the vector space t of m. So we're talking about operators on t of m. And the question is, what is this operator? Well, it has a part coming from Dw and the part coming from Cw, and the part coming from Dw is, as we discussed, what you get by taking the Dirac operator and freezing coefficients. Yeah, that's it. I'm dropping lower terms. I knew there was something else.
00:55:04.854 - 00:55:54.724, Speaker A: So it's going to look like this. So what I'm looking at here is the following. I'm choosing an orthonormal basis inside the tangent space. Once I've chosen an orthonormal basis, there's a corresponding system of coordinate functions, and I'm differentiating with respect to them. And this is the Dirac operator, after you freeze coefficients and drop lower order terms. Ah, tensorflow with one. And that's this part.
00:55:54.724 - 00:57:16.030, Speaker A: But now there's this part to consider. And what's that? Well, if we, if we're at some point in the tangent space, some x, of course we can write x as a linear combination of the basis vectors. And that's exactly what I'm going to do, what I'm going to get here, which is the right way of doing it, but we're not quite done, because there's this final y here that we have to take into account. And then we'll also get the same sort of thing, one tensor gamma multiplication by y. This one. Now we're done. We want to understand what's going on.
00:57:16.030 - 00:58:29.244, Speaker A: This is the operator. We're trying to understand this operator. I'm trying to make you feel happier about this operator. And the first thing I'm going to tell you is that the first two terms, well, anti commute, as they say, graded commuter with the third. So let's just consider the package consisting of the first two terms, and then trust that we can handle this little thing later. That's supposed to be a little x. And it's a scalar function, a coordinate function on the vector space tm tmm.
00:58:29.244 - 00:59:11.692, Speaker A: If you choose an orthomal basis, there's a corresponding system of local, not local, global coordinates on the vector space. Cj is typo c. Yeah, yeah, yeah. Sorry, my apologies. Got a little too excited. Well, here's a function here. There's a function, okay? The function is Clifford multiplication by e.
00:59:11.692 - 01:00:03.322, Speaker A: If e happens to be a linear combination of some vectors xj, and maybe a yj thrown in, then Clifford multiplication by that linear combination is a linear combination of Clifford multiplications. Because little c is a linear map. And that's all I'm doing. Yeah, I guess we're doing fine. So now let's use the following fact, which we haven't used yet, but it would appeared in the. Oh, it's still up there in the definition of the. In the formulation of the index theorem, which is the way in which these various spinner bundles are related to one another.
01:00:03.322 - 01:00:34.524, Speaker A: We have to use that, of course, because there are many different spinner bundles. And if you change the spinner bundle, you're likely to get a different answer. So we have to use this particular fact that you see up there. And I'm throwing in this complex conjugate. You'll see where it comes from in a moment. It's very modest role that it has to play, but it does play a role. And we're going to use this.
01:00:34.524 - 01:01:31.564, Speaker A: Yeah, yeah, yeah. Thank you very much. That was my, for what it's worth, notation. Oh, yeah. Anything else? So is this actually right? Is it projection to the issue? Excellent work, Jacob. Extra gold star for you. Yes, thank you.
01:01:31.564 - 01:02:23.344, Speaker A: Let's call that a typo. Good. So let's consider our operator, which I'm now trying to copy down better than I was writing things down before. So I'm just trying to transfer what you see up there. I think I need that. Oh, I see. There should be a.
01:02:23.344 - 01:03:15.452, Speaker A: So here's the first two terms of the operator, which I tried to write down correctly? I think I maybe did against all odds and all your expectations. There it is. Okay, on, it's acting on this vector space. But what exactly is it acting on? Well, we have to pull back this bundle STM's, like we were discussing up there to the tangent space. There it becomes the constant bundle with fiber s. I'm not quite sure how to do it, where to put the little m like that. What about these Clifford multiplications here? Well, the X's all act in the tangent direction, though.
01:03:15.452 - 01:04:30.734, Speaker A: They all are tangent vectors. So the Clifford operators are actually acting on this factor and not on this factor. They're acting as Clifford multiplication here, tensor one. So this whole operator that you see up there, not the whole operator, but the first two terms is acting just on the first two factors of s of tm, tensor s of tm, bar tensor s of nm. Another way of saying that which may be or may not be better is to say that the operator which lives up there, the first two terms of the operator which live up there correspond to this operator on this space tested with the identity on the vector space s n, sub little m m. So it's really this operator that we're trying to understand. So, and when you play around in index theory with Dirac operators long enough, you know that the right thing to do as soon as you have a Dirac operator, because kind of, it's how Dirac defined it, is to square this thing.
01:04:30.734 - 01:05:04.914, Speaker A: Do you have a name for this, this thing squared? I'm not going to write it all out again. So what happens is kind of interesting. It's the sum of a bunch of terms. If you just look at the first part, it's like Dirac's operator here are a bunch of Clifford matrices which anticommute with one another. So when you square just the first term, all of the cross terms go away. And what you'll get, because each of the Clifford matrices squares to the minus. The identity is you'll just get minus the Laplacian.
01:05:04.914 - 01:05:59.394, Speaker A: That's easy. All of these clippered matrices, because of the gamma square to plus the identity. So we'll get this fellow here, and then there are going to be cross terms which arise when you multiply something in here against something in here plus, and then you do it the other way. You multiply thing against thing in the other direction. What are those cross terms going to be? Well, they're going to involve, because the coefficient matrices anticommute, it's going to involve the commutator of D Dxj with xi. But that commutator is Delta Ij. And what's going to come out of this is just c of xj, tensor hat gamma or c of xj.
01:05:59.394 - 01:06:55.444, Speaker A: And the only little nuance to keep in mind is that this second c of xj is not exactly the same as the first. It's acting on the complex conjugate vector space right here. So that's the precise explicit formula for the square of this operator. And so now reasonably minded people will ask the question, what is this? Let's just call it h, so I don't have to keep writing it down more than one more time. Here we go. Yeah. Well, let's look at an example.
01:06:55.444 - 01:07:26.564, Speaker A: Dimension is two. It's actually a very interesting example. Lots of interesting indexes on surfaces. So there are two xjs. There's an x one and an x two. And so there are two Clifford matrices in play here, and they're unique up to unitary equivalence. So we might as well assume that one of them is this fellow.
01:07:26.564 - 01:08:17.048, Speaker A: As we discussed some time ago, the other one is the other matrix. These have to be matrices which are skew a joint like these two are. Each of them has to square to one, and they have to anti commute like they do. And then there's a grading operator, and the uniqueness theorem, which we began the last lecture with, disregarded the grading. So there are actually two copies, two possibilities for the grading, which differ by sign up to unitary equivalence. This is the complete story. And now you can calculate what this guy is.
01:08:17.048 - 01:08:45.420, Speaker A: I mean, just for fun, just like Isaac Newton did, you can just calculate. But much easier, you can just calculate up to unitary equivalence. This matrix looks like this. The eigenvalues are 20 and minus two. The zero eigenvalue is multiplicity two. We'll look at this in another way in a moment. But you got to just do it right.
01:08:45.420 - 01:09:47.823, Speaker A: You have to multiply the two by two matrices to feel good. And an interesting thing happens when you do this, which is that the two eigenvector is really, really special. Sorry, the minus two eigenvector. Now, I'm worrying that I made yet another typo, but I'm just going to, let me just point out that if when we defined this operator, we put the gamma on the other side, that would change everything by a signal. And I'm a little, I cannot think in my, with my head fast enough to know whether the signs I'm about to show you are right or they're wrong. So what I'm about to say may be off by a sign. And if that's the case, I have to go back in the notes and move all the gammas to the other side.
01:09:47.823 - 01:10:54.280, Speaker A: But let's assume I got it right. Here's what the two eigenvectors look like, or they look like this. Or I should say, this particular guy is a two eigenvector. And the interesting thing is that this works for any orthonormal basis of s s being stmm, I guess we called it. There's something canonical about the and coordinate free about the minus two eigenvector, which is not true about the other eigenvectors. They're not completely true about the other eigenvalues. And what I've written down is either true, or what I wrote down was a true statement about the two eigenvectors.
01:10:54.280 - 01:12:17.668, Speaker A: I really want it to be a true statement about the minus two eigenvector. I know that for sure. But to be honest, I'm having doubts, which I will resolve in the notes. This is the story when n is equal to two. Maybe I should just continue running out of time. But this operator is well known to the physicists, and it's well known that this guy here is always bigger than or equal to n to k. This is the famous quantum harmonic oscillator, or it's a sum of two k, many quantum harmonic oscillators, one on each coordinate line in this two k dimensional space.
01:12:17.668 - 01:13:12.348, Speaker A: And these operators. Yeah, yeah. What I mean is, if you call the operator for a moment q, what I mean is that q vb, I'll call it ss in the sense of operator theory, two k in this particular case. Yeah, thank you. If an operator is bounded below and it's self adjoint, then actually self joint, then it's invertible in the sense of unbounded operator theory. And that's exactly what's going on here. The operator is invertible.
01:13:12.348 - 01:13:58.216, Speaker A: And the only way you can restore some interesting index theory to the situation is by bringing down the spectrum of this operator from two k to zero, which means you have to look at the minus two k eigenspace of this operator which exists. It's not zero dimensional, it's precisely one dimensional, and that's the dimension. We did a calculation when k is equal to one. Here there's a very similar calculation you can do for any k. It's always the case that the spectrum consists of even integer or integers between k and minus k, going down by two k, k minus two k minus four, and so on. That's always the case. And because secretly there's some.
01:13:58.216 - 01:14:24.546, Speaker A: Anyway, that's always the case. And the minus two k vector eigenvector always has this form, except you don't stop the sum over after two, you sum all the way up to two k. And this thing is canonical. It's coordinate independent because of the bar here. If you form for any orthonormal basis, the sum of vj times the VJ bar in s tends to s bar. That's a unique canonical thing. It's independent.
01:14:24.546 - 01:15:05.148, Speaker A: The sum is independent of the basis. So it's sum. Yeah, independent basis. All right, so we have a good picture now of what the operators are when t is equal to zero. What are these operators when t is equal to zero? Well, if we, maybe I should go on a little bit. If we go back to our operator here, this one, which I'm not going to write down, I'll just continue by writing it like that. And you can put a square in what I'm about to write down, or you don't even need to put in the square.
01:15:05.148 - 01:15:40.884, Speaker A: You can say exactly what the kernel of this operator is. It's just the function e to the minus x squared over two times this canonical vector here. I don't know, vacuum vector. Maybe I should call it s vacuum. I don't really know why it's called vacuum of physicists. Tidy people. I don't know.
01:15:40.884 - 01:16:42.702, Speaker A: The Hilbert spaces on which these operators act break up into two pieces. The piece which is one dimensional, which is spanned by this gaussian function, and the other piece, which is the orthogonal complement to this piece, and on the piece where we have. So I didn't finish this statement. This is zero. And on the piece spanned by the gaussian vector, the operator is zero, whereas on the other piece, the operator is bounded below by one or not one, but two k. In fact, because the eigenvalues are all, like I said, so they're invertible. If you look at the operators at t is equal to zero, they act on a family of Hilbert spaces.
01:16:42.702 - 01:17:28.670, Speaker A: And this family of Hilbert spaces breaks up, which is parameterized by m, and it breaks up into a trivial one dimensional bundle, which is the one dimensional bundle spanned by these guys. And it's trivial because this is a canonical vector. It doesn't depend on any choice and basis or anything. So the field of Hilbert spaces over which our family of operators acts when t is equal to zero is always the direct sum of a trivial field. I'm talking just about the operator on this amount of this to begin with, that trivial field that directs on an invertible operator. And for the purposes of index theory, you can forget about the invertible operator. Vertical operators have zero index when you take t is equal.
01:17:28.670 - 01:18:25.554, Speaker A: And now let's go back and throw in the final term of the operator, which we conveniently forgot about up to now, restoring that piece into the story, here's what we find when we restrict to t is equal to zero. So when t is equal to zero, we have a family of operators parameterized by the points in the normal bundle, that family of operators. Now that we've restored this extra tensor factor, that family of operators acts on a certain family of Hilbert spaces. And what is that family of Hilbert spaces? Well, it breaks up into a direct sum of two other families of Hilbert spaces, one corresponding to the kernel of the harmonic oscillator or the shifted harmonic oscillator, and one corresponding one being the orthogonal complement. The orthogonal complementary part is just zero. This operator is bounded below. You don't even have to worry about it.
01:18:25.554 - 01:20:10.056, Speaker A: On the place where the operator is zero. The formula for this guy, this Kasparov operator, is much easier on the place where they, where this fellow and this fellow act as zero. The operator, of course, is just this thing, but this is the Tom element. So what I'm saying somewhere here, and we have the family that we're really interested in, which are these operators up here, and I'm only interested in those w's, which are now in the part of the definition, deformation to the normal cone over t is equal to zero. So this is a family index by vectors in the normal bundle. And we have a direct sum of an invertible family actually bounded below uniformly by two. Well, the square is bounded uniformly below by two.
01:20:10.056 - 01:21:14.074, Speaker A: So the operators are bounded by the square root of two, and the family which defines the bond class. We built a crazy family, inspired by Kasparov, also inspired by what we know about the harmonic oscillator, we built a crazy family. We try to study this family. What we find about this family is that when you restrict a t is equal to zero, it has an extremely simple form. And as far as K theory is concerned, which doesn't care about invertible families, the index of an invertible family is zero. As far as K theory is concerned, it's just the Tom element. Did I call it? Didn't call it anything.
01:21:14.074 - 01:22:08.294, Speaker A: But this is the Tom element. We built family Alpha. And we just discussed that the restriction to zero of alpha is the tom class. And it wouldn't quite be the tom class, at least not necessarily, if you didn't put the bar here. And the reason it wouldn't quite be the tom class, at least not necessarily, if you didn't put the bar here, is that this family of kernels spanned by these functions here, they'd still be one dimensional, but they might define a non trivial line bundle over M. So you'd be off just by a little line bundle. But since we had the foresight to put in the bar, we get this equation here on the nodes, rather than this guy here times some line bundle on mirror.
01:22:08.294 - 01:22:39.372, Speaker A: All right, good. And now it just remains to calculate this. This is much, much easier. I'll do it in the notes, but I'm not going to do it in front of you guys because we're out of time. There are a bunch of technicalities. What I really want you to absorb is just this sentence here. Because we're in this world of where everything is an example of one of Kasparov's constructions.
01:22:39.372 - 01:23:23.032, Speaker A: It's sort of obvious how to proceed. And of course, then you have to proceed, and then it gets a little technical. But this is the most important sentence in the lecture. It's. Yeah, I would say it's. I would say this. I mean, this is consistent, perfectly consistent with, with Kahn's approach that we were talking about a couple of lectures ago.
01:23:23.032 - 01:23:51.094, Speaker A: I would say it's not exactly consistent with the original etir singer approach. Their approach is much more brutal. They choose a tubular neighborhood embedding. They do some brutal calculation, which they, you know, they're proving a slightly more general theorem. They weren't just calculating the index of dirac. So I'd say this. I mean, this could be a model, but it's not a model that you notice or that you observe in nature as you go from one proof of the index theorem to the next.
01:23:51.094 - 01:24:39.594, Speaker A: Yeah, that's a very good question. You could hope, for example, that, you know, Eric van ErT speaks about the world's simplest index theorem in the world of Heisenberg, the Heisenberg calculus. Maybe in one or two really special cases like that, you can tweak this argument and make an explicit argument for some classes of, let's say, Heisenberg type operators. I'm not aware of any other examples. Maybe that would be more honest answer. Not aware of any. But that doesn't mean they don't exist.
01:24:39.594 - 01:25:08.474, Speaker A: If you don't use the Dirac, then you'll get something more complicated right here. And I wouldn't know how to analyze it. That doesn't mean it couldn't be analyzed. But I wouldn't know myself exactly what to do at this stage. Good. On Thursday, we'll talk about something completely different.
