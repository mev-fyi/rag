00:00:01.440 - 00:00:50.034, Speaker A: Yeah. So today I want to tell a few things without really getting too much into details about RCD Kn species. Because as of today, as of, I guess, Monday, we have only dealt with CD k infinity spaces and the corresponding RCD notion. And given that, I want to say, well, I mean, even the lower reach bounds are important, especially from the geometric perspective when they are coupled with an upper dimensional condition. You know, I have to discuss this topic, and before doing that, I want to make a comment about optimal transport and the concept of non branching. So, definition.
00:00:51.714 - 00:00:53.454, Speaker B: So a geodesic space.
00:00:56.354 - 00:01:16.374, Speaker A: This is just a metric space. Forget about CD or CD, whatever. Just a geodesic space is non branching. If this never happens, you know, you never have y shapes. This never.
00:01:17.414 - 00:01:20.714, Speaker B: These are Judysius never happens.
00:01:21.694 - 00:01:30.634, Speaker A: That is to say, if you know, given gamma one and gamma two geodesics.
00:01:33.414 - 00:01:37.558, Speaker B: If you know there extreme, say on.
00:01:37.606 - 00:01:39.354, Speaker A: Defined on the same interval.
00:01:41.094 - 00:01:41.542, Speaker B: As you.
00:01:41.558 - 00:02:03.258, Speaker A: Go, if the extraction to some non trivial interval, zero, a whatever, of gamma one is the same. Nicolas. Sorry. Yes, this blackboard is pretty dark. It's usually the one that you are writing on. It's usually better lighted. Okay, let me try playing with the, with the lighting.
00:02:03.306 - 00:02:24.534, Speaker B: Let's see. There's a light on. That's better. That's better. Yes. Yes. Yeah.
00:02:24.574 - 00:02:31.674, Speaker A: Sorry, I don't know. Well, this is, this is better than before. But can you read is better enough?
00:02:34.064 - 00:02:34.924, Speaker B: Yeah.
00:02:35.504 - 00:02:39.824, Speaker A: Yes. Yes, you can, or yes, it's better.
00:02:39.864 - 00:02:43.764, Speaker B: If I go and seek help from the fields?
00:02:44.584 - 00:02:52.680, Speaker A: No, I think I can read it. I'm not sure. Maybe. I also have the impression that these lights are getting, you know, lighter and lighter.
00:02:52.712 - 00:03:00.664, Speaker B: This one, it looks very good to me. I think there could be two issues. First, the actual, there's physical controls for the up.
00:03:00.744 - 00:03:04.924, Speaker A: Maybe it gives a little bit more. I'm doing my best.
00:03:06.624 - 00:03:09.352, Speaker B: That's good for me. I can, I can read it now.
00:03:09.528 - 00:03:38.324, Speaker A: Okay, great. Um, so, um, what I was saying. So, so I was speaking about the definition of being non branching and non branching means that if you have a, first of all, it's a definition that is given for juridic spaces. And this based non branching. If the following happens, whenever you have two judistics that agree on some non trivial interval a is bigger than zero, then the two geodesics are actually the same.
00:03:39.384 - 00:03:41.240, Speaker B: So this never happened.
00:03:41.352 - 00:03:45.344, Speaker A: You see that you have two jurisdictions that start the same and then they branch.
00:03:45.464 - 00:03:46.244, Speaker B: Okay.
00:03:48.024 - 00:03:55.934, Speaker A: Now if you do optimal transportometric spaces, this non branching is something you will tend to hear a lot because of the following fact.
00:03:58.274 - 00:03:59.066, Speaker B: That it is.
00:03:59.090 - 00:04:24.558, Speaker A: A theorem, but I have no time to prove it, so let me give it as an exercise. And anyway, I will not use it the same. So, by the way, so this is the same, you can read it the following way. So let me, you know, equivalently, equivalently, if the map, you know, you take.
00:04:24.606 - 00:04:27.834, Speaker B: This space, let's call Geox.
00:04:28.974 - 00:04:48.448, Speaker A: This is the space of geodesics on X, you know, this is a subspace of, you know, Carson, zero one. And you can consider on Gex, the map from Gx to say, x squared, that takes gamma and returns gamma at time zero and at time t, you.
00:04:48.456 - 00:04:54.064, Speaker B: Know, for, for any t in zero.
00:04:54.144 - 00:04:56.524, Speaker A: One, this map over here is injected.
00:04:59.704 - 00:05:02.084, Speaker B: It's another way of saying it.
00:05:03.104 - 00:05:05.004, Speaker A: Let me, let me elaborate on this.
00:05:05.624 - 00:05:09.112, Speaker B: Or equivalently, or if, or if an.
00:05:09.168 - 00:05:11.284, Speaker A: Equivalently, and equivalently.
00:05:13.144 - 00:05:27.984, Speaker B: If the map from geo x to the cars say on zero, t for, you know, yeah, if, yes, what?
00:05:35.084 - 00:05:50.544, Speaker A: T strictly between zero and one. Strictly between zero and one, I come, I, it's correct. I convinced you in a second. Let me just write this. It takes and returns simply its restriction to the interval zero.
00:05:50.584 - 00:05:56.444, Speaker B: T. That's, you know, is injected. Okay.
00:05:59.944 - 00:06:29.504, Speaker A: Injected. I mean, this is really a rewriting of what I said, you know, of the definition of non branching. The restriction map to some non trivial sub interval, to any non trivia subinterval, in fact, is injective. And this is the same because, you know, I don't really need to look at the entire sub interval. I just need to take two points, one initial point and one window point. Why say that you have two jurisdicts that agree on two points? So, I don't know, I have something like this. So this is a jurisic and this is another juridic.
00:06:29.504 - 00:06:37.784, Speaker A: Okay, well, but then the distance between this point and this point, you know, is, you know, this length is the same as this length. Basically.
00:06:38.404 - 00:06:43.676, Speaker B: Therefore, this guy is also judicial, you.
00:06:43.700 - 00:07:09.662, Speaker A: See, by triangle, in accordance. And therefore, and therefore, if we start from two judiciary that only have, you see, you agree? Of course, it's important that this is an intermediate point because as you mentioned, you can have even manifolds, and manifolds are certainly an example, I mean, the prototypical example of non branching space. Uh, uh, you can have judiciaries with the same initial and final point, but this is the matter of what happens, you know, inside.
00:07:09.838 - 00:07:15.114, Speaker B: Okay, all right, now the exercise is the following.
00:07:17.854 - 00:07:45.664, Speaker A: Now, you know that in optimal transport, you know, a typical issue, a typical problem is understand whether there is a optimum map or not, where there is a uniqueness of plan. And typically you need, you know, a lot of assumptions on those, you know, say on a d, you want to deal with a certain cost function. The initial measure should, you know, give zero mass to suitably small sets and etc, etc. But with non branching, you have the following beautiful and general result. So let x be nonbrenching.
00:07:53.484 - 00:08:05.464, Speaker B: And say mu t a w two geodesic, right?
00:08:06.044 - 00:08:11.464, Speaker A: And also, so this is a matrix space. So these are just measures. And let's pick t between zero and one.
00:08:14.164 - 00:08:19.244, Speaker B: Then for every s in, you know.
00:08:19.324 - 00:08:24.784, Speaker A: The closed interval, there exists a unique optimal plan.
00:08:28.144 - 00:08:30.924, Speaker B: From mu t to.
00:08:33.264 - 00:08:33.576, Speaker A: Mu.
00:08:33.600 - 00:08:47.324, Speaker B: S, and it is induced by a map, right.
00:08:48.944 - 00:09:20.642, Speaker A: The problem with this theorem is that it's useless, but it looks very cool. Okay, so it's useful because typically you never, I mean, you typically you want to describe as you basics from its starting point, not really from intermediate point, but, you know, it's incredibly powerful. And let me try to convince you that something like this could be true. The line of argument for this theorem is the following. You take an optimal plan and you prove that the optimal plan is induced by a map. If you do so, then the optimal.
00:09:20.658 - 00:09:22.234, Speaker B: Plan must be unique, right?
00:09:22.274 - 00:10:17.936, Speaker A: Because otherwise you could take the convex combination of the two plants induced by these two maps, and this would be a new optimal plan and not induced by a map if the map were really different. Okay, so how can this be true? Well, basically the following. So imagine you have this mu zero, mu t, mu one, and suppose that you have an optimal path, say, from your team, let me pick s equal one. I mean, if you can do the job for this, you can do for every. And suppose that there is some point which is being sent to two different points, a little bit here and a little bit here. But then this point should also be sent somewhere we can go into mu zero, say, just to one guy. And then it's a matter of checking the optimality condition to realize that this curve should be adjudic and this curve.
00:10:17.960 - 00:10:20.264, Speaker B: Should also be adjudicative, okay?
00:10:20.304 - 00:10:48.194, Speaker A: Which is exactly what is being excluded by the number of. Now, as I mentioned, the typical example, the typical example of non branching spaces remain a manifold. And in fact, this, I mean, if maybe you don't remember, but I've mentioned, I've been mentioning this when presenting, you know, the curvature, the curvature condition, the cdk infinity on a romanian manifold.
00:10:49.094 - 00:10:51.430, Speaker B: Actually, let me say, let me say.
00:10:51.462 - 00:10:54.646, Speaker A: Like this, on a corollary of the.
00:10:54.670 - 00:11:00.754, Speaker B: Exercise, if xd is non branching.
00:11:06.754 - 00:11:07.494, Speaker A: And.
00:11:07.954 - 00:11:18.374, Speaker B: And let's say x dm is a CD k infinity. Then.
00:11:21.074 - 00:11:24.454, Speaker A: And then, let me put this way.
00:11:28.414 - 00:11:34.390, Speaker B: For any, say, t maps to mu.
00:11:34.422 - 00:11:51.074, Speaker A: T, let's say curve of measures with absolutely, you know, continuous, I mean, absolutely continuous measures, geodesics w two and PI, lifting of it.
00:11:56.994 - 00:11:57.974, Speaker B: We have.
00:12:01.234 - 00:12:11.146, Speaker A: The following inequality. Log of rho t in gamma t is less or equal than one minus.
00:12:11.210 - 00:12:18.786, Speaker B: T. Log of rho zero and gamma zero plus t times log of rho.
00:12:18.850 - 00:12:43.784, Speaker A: One in gamma one minus k over two t one minus t, distance squared gamma zero, gamma one. This is all true. I should be careful with the quantifiers. So for PI almost every gamma, this is true for almost everything.
00:12:50.064 - 00:12:50.712, Speaker B: Or maybe.
00:12:50.768 - 00:12:52.920, Speaker A: For every t, for every t, dissolved.
00:12:52.952 - 00:12:56.884, Speaker B: For every t and dissolves for PI almost every gamma.
00:12:59.864 - 00:13:05.064, Speaker A: What it is, this inequality? This inequality is the one such that.
00:13:05.144 - 00:13:13.838, Speaker B: If you integrate it, you get the convexity of the convexity of the entropy, right?
00:13:14.006 - 00:13:15.726, Speaker A: Makes sense if I, if I integrate.
00:13:15.750 - 00:13:17.754, Speaker B: With respect to PI, right?
00:13:18.054 - 00:13:42.914, Speaker A: Because now the integral of log rho t gamma t d PI of gamma, right, this is here only gamma t appears. So this is the same as integrating, this is the same as integrating log by the, you know, definition of push forward log rho t in d e t push forward PI.
00:13:45.134 - 00:13:47.198, Speaker B: But this is mu t. So we.
00:13:47.206 - 00:14:07.354, Speaker A: Are integrating, you know, rho t log rho tdm, right, which is the entropy. This was for t equals zero, t equal one. And if we integrate distance squared gamma zero, gamma one, using the fact that PI is lifting a jurisic, what we get is really optimal, you know, the w two squared between mu zero mu one.
00:14:07.514 - 00:14:13.162, Speaker B: Okay? Now I hope you see that.
00:14:13.258 - 00:14:38.360, Speaker A: Now that this is way more powerful than just the convexity of the entropy because convexity on the entropy tells you that, you know, some convex, some integrated convexity inequality holds. This is telling you that you look, the integrands are, you know, are in a certain, you know, convex complex relation, okay? So this is just way more powerful, by the way. So how can we, you know, prove this result?
00:14:38.472 - 00:14:42.144, Speaker B: Well, there are, you know, there are.
00:14:42.184 - 00:15:10.574, Speaker A: First of all, there are a couple of observations that were to be made, perhaps related to this corollary. Well, first of all is that if d, if you have an unbrenching space which is 3d, well, then you have the convexity inequality along any w two jurisdict not just a long sum of this. How can this, how can they, how can this be the case? Well, take an arbitrary is a consequence of the, of the, of these, of these exercise above because take an arbitrary.
00:15:10.614 - 00:15:15.422, Speaker B: Jurisdict muti and, and now say, look.
00:15:15.518 - 00:15:25.634, Speaker A: Let me pick, you know, just a small t, but positive. And let me apply, let me notice that because of this exercise, there is only one juristic from mut to mu one.
00:15:26.134 - 00:15:28.190, Speaker B: There cannot be two, if you want.
00:15:28.222 - 00:15:31.950, Speaker A: Because this is the optimal transport analog.
00:15:31.982 - 00:15:36.094, Speaker B: Of this picture over here, okay? Or if you want, if the space.
00:15:36.134 - 00:15:50.434, Speaker A: Is non branching, the space of measures built on it is also non branching. So you have a unique Judia six from intermediate points. Certainly not, you know, you can have cut points like in many posts, but once you start going a certain direction, then, you know, you must follow it.
00:15:50.844 - 00:15:54.084, Speaker B: So if the judiciary is unique and.
00:15:54.124 - 00:16:20.312, Speaker A: Despite the CD along that judicial, you must have, you know, the curvature condition, right? But now you let t go to zero by some continuity or overseeing continuity statement. You get, you get what you want, okay? So first of all, you have, you have, you know, Judicial Max, this in place for, you know, along any, along any judicial here. This for every, for any pie lifting of the given judiciary is also cheating, because whenever you have a non branching.
00:16:20.368 - 00:16:23.484, Speaker B: Space, the lifting is unique, okay?
00:16:25.344 - 00:16:52.606, Speaker A: Again, it's a consequence of what we were saying about how you can have, how could you have multiple liftings if you have the same marginals? The same marginals. If you play a little bit with the definition, you see that this is the case. But now, basically you are done, because in order to deduce this, what it is that you do, I think I've mentioned some discussion this direction in the romanian setting.
00:16:52.750 - 00:16:58.294, Speaker B: If, because what it is that you do, you can say, look, pick your.
00:16:58.334 - 00:17:06.834, Speaker A: Pie lifting off your mute and notice that.
00:17:08.774 - 00:17:19.174, Speaker B: For any set of cars, c, zero, one, x, you know, Borrell, with.
00:17:19.294 - 00:17:21.126, Speaker A: You know, positive measure with respect to.
00:17:21.150 - 00:17:21.874, Speaker B: Gamma.
00:17:24.254 - 00:17:34.694, Speaker A: You know, I can take the restriction plan, which is just PI, restricted to gamma, normalized. These are, now, this is a probability measure.
00:17:34.854 - 00:17:35.554, Speaker B: Now.
00:17:37.334 - 00:17:47.408, Speaker A: By optimal, the support of this plan is included. Yeah, the support of this plan is including the support of this plan. Therefore, if that guy was a lifting of a Judisc, this also must be.
00:17:47.416 - 00:17:49.432, Speaker B: A lifting of juridesics by, you know.
00:17:49.448 - 00:18:13.236, Speaker A: The fact that if I, you know, if I pick a plan with support optimalities preserved and, and therefore, the map that takes t and returns et push forward PI gamma. This is a w two judicial, right? Now this is w two judicial. And therefore, the entropy must be convex along to judicial, because we just said.
00:18:13.260 - 00:18:16.220, Speaker B: That it must be convex along in this, okay?
00:18:16.412 - 00:18:52.234, Speaker A: And moreover, moreover, what I know, so, you know, so what I'm telling here, I want to say that this complexity, so, and therefore, and therefore, the entropy is k convex along it. And I want to say that this k convexity is the same as the convexity as this integral in some sense.
00:18:56.894 - 00:18:57.782, Speaker B: In d PI.
00:18:57.878 - 00:19:04.992, Speaker A: Okay, there is only one thing that I should pay attention to, which is, which is the following. If I want to say this, what.
00:19:05.008 - 00:19:09.416, Speaker B: I need to say is the following is peak.
00:19:09.480 - 00:19:12.244, Speaker A: Let's call this measure, I don't know, multigamma.
00:19:14.224 - 00:19:14.848, Speaker B: Okay?
00:19:14.936 - 00:19:21.104, Speaker A: And let's say that it has a density. I mean, it certainly has a density, which is, which is, let's say, roti gamma.
00:19:21.184 - 00:19:21.884, Speaker B: Okay.
00:19:24.224 - 00:20:01.442, Speaker A: And now, and now here is an important question. Is it true that rho t, I mean, and this is the density that appears in computer disappear? I mean, whereas there I was, that inequality is written with respect to the density of the original planet. Okay, so here there's a caveat. I should pay attention and wonder whether this is true, that Rhotigama at some point, you know, point gamma t along the jurisic is the same or not of rho t gamma t up to.
00:20:01.578 - 00:20:05.202, Speaker B: Maybe some normalization constant, up to, you.
00:20:05.218 - 00:20:15.866, Speaker A: Know, I guess up to this normalization constant of these four, you know, for a Judicial gamma that belongs to our.
00:20:15.930 - 00:20:18.314, Speaker B: Set of gamma, I should be a.
00:20:18.314 - 00:20:46.554, Speaker A: Little bit more careful with almost everywhere, etc. Etc. But, you know, and identity of this form should be true if I want to, you know, really say that the complexity of the entropy here is the same as the integrated inequality, okay? But this holds again by the non branching assumption. The non branching assumption is telling me, is telling me, this is telling me. Look, let me draw a picture.
00:20:47.294 - 00:20:48.034, Speaker B: Pick.
00:20:54.314 - 00:21:03.850, Speaker A: You have this plan PI that gives mass to some jurisdicts. And then we have the evaluation map et. And here, you know, I project down.
00:21:03.882 - 00:21:12.090, Speaker B: And add new t. And then I look at a small region over here and I wonder.
00:21:12.242 - 00:21:16.574, Speaker A: And I wonder, and. Sorry, no. And then I say the following.
00:21:17.114 - 00:21:20.170, Speaker B: I pick a sub plan, I pick.
00:21:20.202 - 00:21:26.254, Speaker A: Just say a few of these jurisdictions. I look at the image of this and I get a submeasure.
00:21:27.154 - 00:21:28.018, Speaker B: Okay?
00:21:28.186 - 00:22:06.796, Speaker A: And now asking whether this identity holds, basically, I'm asked to ask the following question. Pick a point if you want, or a small set here, you know where I am looking to the measure multigamma. And ask yourself, is it possible that some mass is being given to this point by judicials that are not green? Because in order for that to be true, I really wanted all the green mass, you know, all the mass of the original measure, of the white measure that is sitting in the green region should come from green jurisdictions.
00:22:06.940 - 00:22:07.824, Speaker B: Make sense.
00:22:08.904 - 00:22:19.484, Speaker A: But this is granted by the non branching assumption, once again, because the non branching assumption tells you that et is injected, you know, once injected, you have some injectivity at this level.
00:22:20.904 - 00:22:22.392, Speaker B: Okay, makes sense.
00:22:22.448 - 00:22:31.704, Speaker A: In fact, it is telling. It is telling if you want. Let me put it this way, by a map. Yes, and yes, and let me put this way.
00:22:31.744 - 00:22:34.724, Speaker B: And if PI is a lifting.
00:22:38.054 - 00:22:47.086, Speaker A: Then for every t stricted between zero one, you know, the map et, from the.
00:22:47.110 - 00:22:50.754, Speaker B: Support of PI to x is injected.
00:22:53.854 - 00:22:56.790, Speaker A: Once you retrieve dot ru implants, you don't even need to pick, to pick.
00:22:56.862 - 00:22:59.710, Speaker B: Your initial points, okay?
00:22:59.822 - 00:23:36.084, Speaker A: And this injectivity is what is telling you that here you don't have overlapping of mass any. In some sense, the mass that is even that is given to any given point comes from only one judicial. And because of this, you have this sort of identity. And therefore, once, once you have this k convexity, this k convexity, the same as this integrated inequality. But once you have this integrated inequality for any gamma, it is a matter of measure theory to instead conclude that it must be true at a pointless level. What's the set of gammas for which this inequality fails? Must have measured zero, because otherwise, integrating on it, we will find.
00:23:39.424 - 00:23:47.672, Speaker B: Okay, very well. Now, it is a theorem, deep theorem.
00:23:47.728 - 00:24:01.382, Speaker A: That solved, you know, a long conjecture that's been proved by Ching Deng here at Toronto, by the way. So, he was a student of Vitalikapovich. The finite dimensional RCD spaces, in fact are non branching.
00:24:01.478 - 00:24:04.598, Speaker B: Okay, I won't prove this theorem, this.
00:24:04.606 - 00:24:16.034, Speaker A: Is very hard, but I will rather mention a way easier result, but still deep. That has been, that has been proved by Raj Allen Strum.
00:24:19.374 - 00:24:24.354, Speaker B: That tells, okay, that basically tells that.
00:24:24.654 - 00:24:51.164, Speaker A: You know, from the perspective of CD and RCD business, maybe these RCD spaces are possibly branching, I don't know, but they are essentially non branching. So if you put the appropriate, almost everywhere, in some sense claim where needed, they act as the non branching space. Okay, in particular, the conclusion of this corollary is true even in the setting of RCD space basis.
00:24:54.704 - 00:24:55.444, Speaker B: And.
00:24:57.504 - 00:24:59.084, Speaker A: Let me give the definition.
00:25:03.944 - 00:25:12.404, Speaker B: Ah, I don't have it. Yes, definition.
00:25:14.784 - 00:25:20.044, Speaker A: And now this is a definition of, you know, that it's tailored to metric measure spaces.
00:25:35.384 - 00:25:39.964, Speaker B: If so, for every PI.
00:25:43.104 - 00:26:10.732, Speaker A: You know, this is a probability measure, as usual, on the split of cars with which is, which is with bounded compression, with so bounded et, PI should be less or equal than some constant times. The reference measure for every t in zero, one and some constant c and PI optimal jurisdiction plan and PI, you.
00:26:10.748 - 00:26:17.024, Speaker B: Know, optimal dualistic test plant optimal.
00:26:18.524 - 00:26:54.074, Speaker A: So this means that, you know, the total energy, if you want, coincides with the squared distance between the, you know, the initial and final marginals. Or if that is the same as saying that, you know, PI is a lifting of a Judi seq. We have that. We have that. Let me put this way, the map.
00:26:59.934 - 00:27:11.932, Speaker B: And for every t strictly between zero, one, the map, et, you know, is PI.
00:27:12.068 - 00:27:14.332, Speaker A: That goes from this space to the.
00:27:14.348 - 00:27:32.544, Speaker B: Space of curves to x, is PI essentially injected. Okay, what does it mean?
00:27:32.624 - 00:27:49.764, Speaker A: PI essential injective? PI essential injective means. Well, that you can, that is injective if you restrict it to a set, bored set of, you know, with complement, is PI negligible?
00:27:51.624 - 00:27:54.400, Speaker B: Okay, it's, you know, it's as injective.
00:27:54.432 - 00:27:57.812, Speaker A: As it can be from the perspective of, you know, PI.
00:27:57.948 - 00:27:58.704, Speaker B: Okay.
00:28:00.764 - 00:28:37.260, Speaker A: So it's, it's really an analog at least, of this conclusion over here, you know, except that we are not asking, first of all, we are not asking for actual injectivity, but just for essential injectivity. And we are not asking this to be true for any optimal plan, PI, but just basically for those with bounded compression. So here the measure, the reference measure m comes and plays a role. And plays a role. Sorry, question. Yes, I thought the definition was that this negligible set should be the same for all c that you can remove. I don't think, I don't think this is the case.
00:28:37.260 - 00:28:57.576, Speaker A: I'm not completely sure. I think this is not equivalent. I thought that that was the definition. Okay, I made the wrong. I'm being recorded, don't let me. So I think that this is the correct version. I think this should work.
00:28:57.576 - 00:29:08.400, Speaker A: So let me put this way. I'm more confident with this. Okay. Of course, if the set is the same for every t. Yeah. That's formally a stronger statement.
00:29:08.472 - 00:29:10.160, Speaker B: Yes, assumption.
00:29:10.312 - 00:29:15.232, Speaker A: Say again what I'm saying is formally stronger assumption than what you are saying.
00:29:15.288 - 00:29:16.608, Speaker B: Yeah, exactly.
00:29:16.656 - 00:29:32.844, Speaker A: Exactly, exactly, exactly, exactly. I think so. I guess the point is this. I think I able to prove this, and this is sufficient for all the proposals, for all the applications of this condition. So I may appear with this.
00:29:33.624 - 00:29:42.504, Speaker B: You're right, but I'm not fully confident about this. Can we.
00:29:43.084 - 00:30:04.464, Speaker A: That's fine, yes. Okay, thanks. Anyway, I will not give the proof of this theorem, so actually of the theorem that I still have to state. And here is. And so the theorem, which is a powerful result by Rajara and Sturm.
00:30:09.184 - 00:30:09.520, Speaker B: Does.
00:30:09.552 - 00:30:11.792, Speaker A: The following tells that if you know.
00:30:11.928 - 00:30:19.564, Speaker B: XDM is RCD k infinity, then.
00:30:22.784 - 00:30:24.884, Speaker A: Well, it is essentially on branch.
00:30:34.044 - 00:30:37.780, Speaker B: Right, which.
00:30:37.812 - 00:30:57.556, Speaker A: Is well, first of all, it's interesting, as I stated, because it uses an unbrenching assumption out of a curvature condition. While up to, you know, a couple of minutes ago, it was sort of more going in the other direction. Let me just quickly comment on how, you know, that can ever be possible without giving the details that are actually quite complicated.
00:30:57.700 - 00:31:01.946, Speaker B: And the idea, the rough idea for.
00:31:01.970 - 00:31:24.738, Speaker A: The proof is to start recalling that given that we have RCD condition, then we have the Evi property for the gradient flow. And therefore we have, from Evi, we have the entropy. Is convex, is k convex along any.
00:31:24.786 - 00:31:41.814, Speaker B: Gedi system, all w, two, z six. Okay? Now imagine that the space was not essentially number ranging.
00:31:42.634 - 00:32:04.520, Speaker A: Then you could have a situation that, qualitatively speaking. Now I'm being, you know, a bit rough. The actual proof requires a careful estimate. But the idea would be, imagine that you have many judicial debris also along an optimal plan. Then maybe you would have a situation like this. You have a vaccine, judic, that starts, say you have a major say on a ball or somewhere.
00:32:04.552 - 00:32:05.088, Speaker B: That's that.
00:32:05.176 - 00:32:10.168, Speaker A: It starts moving for a while along Judic, and then this judicial branch.
00:32:10.296 - 00:32:10.880, Speaker B: Okay?
00:32:10.992 - 00:32:30.236, Speaker A: Then, you know, some judiciary go in this direction, and some judiciary go in this direction. Okay, so far so good. Now, where. Where is it that the problem arise? Well, the problem arises when you start noticing that, okay, you could restrict now your given plan to, you know, the.
00:32:30.260 - 00:32:33.572, Speaker B: Green jurisdiction, and this would be, you.
00:32:33.588 - 00:32:38.068, Speaker A: Know, again, an optimal juridic plan. So you would have convexity of the entropy along.
00:32:38.116 - 00:32:41.140, Speaker B: Along this plan, but you could also.
00:32:41.172 - 00:33:00.776, Speaker A: Do the same for, you know, redesign, okay? And this would also be a convex. Convex. And now. And now. And now here is the problem. Now, when you combine these two, there is a crucial difference in what happens, you know, on the left part of the. And on the right part of the.
00:33:00.776 - 00:33:14.740, Speaker A: Because in the left part of the Judisc, when. When you add up the entropy, you know, coming from green and coming from red, you don't get the entropy of the full measure, because the entropy is not linear, it's super linear. You have an additional log of two.
00:33:14.772 - 00:33:16.544, Speaker B: That appears on the side.
00:33:16.964 - 00:33:28.144, Speaker A: And whereas here on the right, on the right hand side, on the right hand side, you have. You have something which is. Let me put this way. So you have. Imagine. Imagine you have this. You have two convex functions.
00:33:29.284 - 00:33:33.076, Speaker B: They say a green convex function and.
00:33:33.100 - 00:33:38.760, Speaker A: A red convex function. Now, their sum, of course, is still.
00:33:38.792 - 00:33:42.776, Speaker B: Convex, and their sum is, let me.
00:33:42.840 - 00:34:18.054, Speaker A: Put it, a yellow convex function. And on the. If this is one half, I'm one half after time, one half or after when the geodesic. So the graph that you are seeing in orange is exactly the graph of the entropy of the initial jurisdiction. But what is now the entropy, you know, before, before one half or before, you know, a little bit before one half. When all these things are together, you get the sum of the entropy plus an additional log of two that comes from, from the non linearity.
00:34:19.114 - 00:34:19.738, Speaker B: Right?
00:34:19.866 - 00:35:05.684, Speaker A: So now you have something that, you know, if you just look here, the left hand side, this is convex. On the right hand side this is convex. But here you have, you have a discrepancy of the order log of two. And now how can this thing be complex given that you can in such a shrink this interval as well as you wish, provided you pick a smaller set of jurisdicts so that the separation is going to occur in a shorter time. You basically have a jump of order log of two and that destroys any priority convexity that you might want it to, to have. Okay, now the actual proof is, you know, more complicated. But, you know, that's the basic graph idea.
00:35:05.684 - 00:35:06.564, Speaker A: Beautiful idea.
00:35:06.644 - 00:35:13.844, Speaker B: Okay, so this is essential non branching and a consequence of essential non branching.
00:35:13.884 - 00:35:31.134, Speaker A: I mean, if you have that and you follow the argument that I very vaguely pointed out before, you can conclude that on essentially non branching spaces, this curvature condition can be localized actually along the basics. So corollary.
00:35:34.034 - 00:35:37.730, Speaker B: You know, if x, if x.
00:35:37.882 - 00:35:41.970, Speaker A: Well, let x Dm be rcd to.
00:35:42.002 - 00:35:51.454, Speaker B: Infinity and PI be an optimal geodesic plan.
00:35:55.044 - 00:36:10.224, Speaker A: So the lifting of a Judiscal if you want an optimal plan, leaving on the state of this with say absolutely continuous marginals, then.
00:36:13.084 - 00:36:14.236, Speaker B: You know, for every.
00:36:14.300 - 00:36:17.672, Speaker A: T we have the new coil that I guess I erased the.
00:36:17.708 - 00:36:20.128, Speaker B: So log of rho t gamma t.
00:36:20.176 - 00:36:22.004, Speaker A: Less or equal to one minus t.
00:36:22.384 - 00:36:25.800, Speaker B: Log rho zero gamma zero plus t.
00:36:25.832 - 00:36:34.404, Speaker A: Times log rho one gamma one minus k over two t one minus t distance squared gamma zero gamma one.
00:36:38.064 - 00:36:43.788, Speaker B: This holds for PI almost everything gamma.
00:36:43.836 - 00:36:55.784, Speaker A: So we can localize along geodesics our curvature assumption, which is morally what you would have learned to do from the very beginning. But you cannot state the curvature condition in this form because otherwise you don't get stability.
00:36:57.004 - 00:37:00.184, Speaker B: Right. Okay.
00:37:04.164 - 00:37:27.754, Speaker A: So in some sense that's enough for what concerns, for what concerns rcd k infinity spaces. So what I want to do now, maybe after, after the break, is to introduce these finite dimensional versions. So in particular of the CD condition. So in particular the CD, CD and Cde, the entropic version.
00:37:31.054 - 00:37:31.526, Speaker B: Perhaps.
00:37:31.590 - 00:37:32.474, Speaker A: Let me just.
00:37:35.294 - 00:37:36.654, Speaker B: No, let's do the break now.
00:37:36.694 - 00:37:42.252, Speaker A: And then I discuss. Yes. The modified sine function later on. Uh, five minutes of break. Better.
00:37:42.308 - 00:37:43.180, Speaker B: Four. Okay.
00:37:43.252 - 00:37:46.524, Speaker A: And see you soon, please. Sorry, there was a question, please.
00:37:46.644 - 00:37:49.900, Speaker B: Sorry. It's on the break.
00:37:49.932 - 00:37:51.364, Speaker A: Yes, on the break, so we don't lose time.
00:37:51.404 - 00:37:51.628, Speaker B: Yes.
00:37:51.676 - 00:37:59.744, Speaker A: Yes. It's officially the break, so we assume in four minutes, 30 seconds, please. I have to do a lot of things. I want to give an Atlanta splitting before handing.
00:38:00.404 - 00:38:01.184, Speaker B: Yes.
00:38:03.804 - 00:38:13.440, Speaker A: Yes. Not so true. So you have an uncountable number of times, right?
00:38:13.592 - 00:38:22.724, Speaker B: Yeah, but they can disagree. Let me think.
00:38:35.804 - 00:39:12.448, Speaker A: So you are saying, if you are non branching in the sense that I presented, then you should also be non branching in the sense that Kapowicz was mentioning. That is, you can actually find one set of full PI measure on which this is. And the proof is you pick accountable or rational times, you pick the associated set of full measure, you take the intersection. And now you're telling that if you restrict et for irrational t to this set, this should still be injective.
00:39:12.576 - 00:39:13.884, Speaker B: And why is that the case?
00:39:15.584 - 00:39:32.156, Speaker A: It doesn't go on the wrong direction. Is not injective. Yes. Of gamma is the same of ET, of Gamma Tilde for t, which is irrational.
00:39:32.340 - 00:39:34.212, Speaker B: Yeah. Yes.
00:39:34.308 - 00:40:02.856, Speaker A: What is Q component? I mean, maybe. Maybe it could be different for. There is also some subtlety that I always, you know, I always have an addict when I. Whenever it's into this. So I'm sad. I'm happy with it, with this condition. That's something that could be, is that.
00:40:02.960 - 00:40:05.240, Speaker B: You know, you could have a set.
00:40:05.272 - 00:40:32.940, Speaker A: Of judiciary that in principle has, you know, positive PI measure. And what it happens is that, you know, some of these branch at time, you know, t some other time, t prime some other later time, and so on and so forth. So the whole set as positive measure. But whenever you look at any given time, t, those that branch at the time are at actually zero. And I'm a little bit confused about this sort of behavior. And. But, you know, the definition that the g.
00:40:32.940 - 00:40:43.116, Speaker A: I mean, this theorem is correct, and it is sufficient for the purposes of localizing, of getting this corollary, which.
00:40:43.140 - 00:40:47.124, Speaker B: Is where I want it to go. What.
00:40:53.624 - 00:41:01.244, Speaker A: Equivalent of these definitions that you do by using some so called useless theorem.
00:41:01.984 - 00:41:05.104, Speaker B: I think an adaptation of this theorem.
00:41:05.144 - 00:41:07.304, Speaker A: To when you assume not only non.
00:41:07.344 - 00:41:10.024, Speaker B: Brand, so when you.
00:41:10.064 - 00:41:33.574, Speaker A: When you assume something weaker, only essentially in the activity. So maybe it's not that useless. I could be. It could be. I'm not so sure. It could be. So, to be honest, I thought about this a couple of years ago.
00:41:33.574 - 00:41:56.914, Speaker A: I brought down some notes for myself, and now I'm referring to these notes, but not really. I don't, I don't have all the details in mind. I. I'm happy with this. It could be that, an equivalent, that in fact, it implies an a priori looking, stronger looking statement. But I don't want to, you know, make a clearer, you know, statement, because I'm not sure. I forgot there was one thing that I wanted to tell.
00:41:56.914 - 00:42:05.268, Speaker A: I forgot I should prove. Sorry. The commutativity of the heat. Well, I'll do this. I'll do this at the end of the lecture. At the end of the lecture. But I will, I will.
00:42:05.268 - 00:42:10.704, Speaker A: You know, there was a missing step last. Actually, let me give you last lecture.
00:42:11.484 - 00:42:11.988, Speaker B: I will.
00:42:12.036 - 00:42:29.104, Speaker A: I will do this later. All right, let me go to CD. CD star, CdE and then all the relatives. Now, when you do comparison geometry, you should be acquainted with the concept of, you know, generalize the sine function.
00:42:30.504 - 00:42:33.112, Speaker B: So in some sense, it's quite unusual.
00:42:33.168 - 00:42:41.560, Speaker A: That I'm presenting them to you today rather than in the first or second lecture. My excuse is that I'm an analyst, not geometry.
00:42:41.752 - 00:42:46.004, Speaker B: So, for k real.
00:42:47.904 - 00:42:52.192, Speaker A: Let'S define the generalized sinus function.
00:42:52.288 - 00:42:55.400, Speaker B: So, sk, this is a function that.
00:42:55.432 - 00:43:03.256, Speaker A: Goes from, uh, let's say from r to r. And this defines as the.
00:43:03.280 - 00:43:07.808, Speaker B: Only solution of second derivative of sk.
00:43:07.976 - 00:43:18.768, Speaker A: Plus k times sk equals zero, subject to the condition. So at very time zero, we are zero. And I guess the derivative at m, one is one.
00:43:18.936 - 00:43:28.060, Speaker B: Yes, right.
00:43:28.252 - 00:43:38.384, Speaker A: Of course, when k is equal to one, your function is sinus. When k is minus one is the hyperbolic sinus. And when k is zero, this is just the identity.
00:43:38.844 - 00:43:39.588, Speaker B: Okay?
00:43:39.716 - 00:43:57.454, Speaker A: And so these are convenient ways in terms of incorporating all of these into one. So perhaps when k is positive, you should think as sk. I mean, sk is really, is really, what is one over square root of k, sin square root of kt.
00:43:57.534 - 00:43:58.358, Speaker B: So that's.
00:43:58.486 - 00:44:02.194, Speaker A: And then the same analog with hyperbolic sine and cosine.
00:44:02.654 - 00:44:05.114, Speaker B: All right. Now.
00:44:07.894 - 00:44:20.244, Speaker A: This function is in some sense, the starting point for elaboration of our interpolation coefficients. And our interpolation coefficients are the following.
00:44:23.424 - 00:44:29.124, Speaker B: Now for. Now for parameters k in r n.
00:44:29.544 - 00:44:32.112, Speaker A: Between one and infinity. But now let me say, you know.
00:44:32.128 - 00:44:39.764, Speaker B: That n is finite and d is our distance function is non negative and.
00:44:39.804 - 00:44:44.812, Speaker A: Time, which is between zero and one. With all these parameters, we define, you.
00:44:44.828 - 00:44:52.420, Speaker B: Know, sigma, k nt of d. This.
00:44:52.452 - 00:45:06.424, Speaker A: Is sinus of, you know, coefficient, distance squared k over n at time t divided by sinus coefficient, distance squared, k.
00:45:06.464 - 00:45:09.648, Speaker B: Over n time one, at least.
00:45:09.696 - 00:45:39.020, Speaker A: You know, this is basically the definition. The only thing that I basically want to pay attention to is that when k is positive, when k is one, let's say I don't want to look at the sine function beyond the point PI. I want just looking, you know, where it's positive, and this as an equal in the definition of this coefficient. So I'm saying that this, I care about this. If d squared k over n is.
00:45:39.052 - 00:45:43.184, Speaker B: Less than PI squared, otherwise it's plus infinity.
00:45:47.164 - 00:45:53.944, Speaker A: You know, but for all intent, I propose is just pick this formula and forget about, you know, whatever things that can go wrong.
00:45:54.764 - 00:45:55.708, Speaker B: So what is this guy?
00:45:55.756 - 00:45:59.232, Speaker A: So why, why do we care about this guy? So this guy is the only solution.
00:45:59.328 - 00:46:06.044, Speaker B: Of second derivative plus distance squared plus distance squared.
00:46:06.584 - 00:46:23.650, Speaker A: K over n f equals zero, subject to the condition f zero equals zero and f one equal one. So it's just, you know, I'm picking that sinus and I'm just putting coefficients just in order to be sure that this is what happens.
00:46:23.752 - 00:46:26.598, Speaker B: Okay, very well.
00:46:26.686 - 00:46:50.322, Speaker A: But why should we ever care about anything of this form, especially if we are concerned with lower itchy bounds? Well, the point is that in the model space, you know, the sphere of the appropriate dimension are the, or the upper body space of the appropriate dimension. If you look at the jacobian determinant of the exponential map along a geodesic.
00:46:50.438 - 00:46:56.174, Speaker B: Of length d, that determinant satisfies exactly this.
00:46:58.634 - 00:47:13.458, Speaker A: Possibly with, okay, possibly with a different value at time one. But the ode satisfied with regard to this. Okay, and now the Ricci curvature. So the jacobian determinant is controlling how, in some sense, how the volume measure.
00:47:13.506 - 00:47:16.026, Speaker B: Changes along the basics, which is exactly.
00:47:16.050 - 00:47:19.414, Speaker A: The kind of thing that should be controlled by a lower bound on the rich curvature.
00:47:19.924 - 00:47:21.156, Speaker B: So in some sense, a lower bound.
00:47:21.180 - 00:47:34.564, Speaker A: On the Richey Cuba should tell us that the measure is, you know, more, you know, at least, well, let's say entropies are more convex than on modern spaces. So this autogen determinant come into play, very roughly speaking.
00:47:34.684 - 00:47:39.544, Speaker B: Okay, all right.
00:47:42.844 - 00:48:15.004, Speaker A: In some instances, we also need to use what CEO called the toe coefficients. And the toe coefficient is the big brother of the Sigma coefficient. And the toe coefficient is the following. It is given by the same parameters, and the toe coefficient at time t.
00:48:15.504 - 00:48:19.904, Speaker B: A n d is nothing but, is.
00:48:19.944 - 00:48:35.512, Speaker A: Nothing but the following, well, up to, you know, suitable exception when the dimension n capital n is exactly one or something goes wrong about this inequality. But basically, the top coefficient is. This is you take t and you.
00:48:35.528 - 00:48:37.696, Speaker B: Raise to the power one over nice.
00:48:37.750 - 00:48:43.224, Speaker A: And you multiply by this sigma coefficient with the same parameters.
00:48:43.764 - 00:48:48.956, Speaker B: The same, right? No, k minus one, sorry, d. And.
00:48:48.980 - 00:48:50.780, Speaker A: You raise it to the power n.
00:48:50.812 - 00:48:58.476, Speaker B: Minus one over n. Okay, all right.
00:48:58.500 - 00:49:11.468, Speaker A: So first of all, when k equals zero to n, sigma are the same, because when k equals zero, the sinus function is just the identity function. So also sigma is just d and then it's just t and then you.
00:49:11.516 - 00:49:12.384, Speaker B: Get the c.
00:49:15.564 - 00:50:14.828, Speaker A: In general, what this toe coefficient is doing is, you remember what we said about jacobian determinants in the model space where rather than looking. So if you're concerned about lower each one by k and upper dimension bound by n, well perhaps you should not look to the model space with dimension n and lower bound reach k. But you should look at the modest space with lower h one k by dimension n minus one. And notice that the, you know, you feel the rich equal. You know, you don't feel the rich equation direction of motion. Okay, this has to do with the fact that in romanian geometry, there is a trivial solution to the Jacobi, to the equation of Jacobi fields. And that is, you know, if your starting point, either your starting point or your starting derivative is the same direction of the speed of digitization you're moving to, then you just achieve a solution.
00:50:14.828 - 00:50:44.094, Speaker A: You don't see the full version in the direction. Okay, so, so when sometimes, sometimes it might be helpful, you know, to keep this in mind when speaking about lower HIV and say, look, when I see the curvature, I really see the curvature in my orthogonal directions. And if I add dimension n or dimension upper bound by n, then I really see the curvature in just in n minus one direction. And so that is what I should use to compare.
00:50:46.354 - 00:50:50.666, Speaker B: All right, that's the philosophy.
00:50:50.690 - 00:51:03.084, Speaker A: Then there are computations to be done, but that's the philosophy. So in one direction you don't see the curvature. And that's, you know, taking the, in the all the n minus one direction with the even scaling, you see the conversion bar.
00:51:03.464 - 00:51:22.664, Speaker B: Okay, all right, we are ready to give the definition of CD of cdk and space. I'm sorry, I have to erase this beautiful definition.
00:51:26.444 - 00:51:59.544, Speaker A: And these are definitions that have been given by sturm. Okay, this sigma and toko physiotherapy being produced by him, Lotte and wilhelmine's and sense only treated the case where either k is zero or n is infinity. And in that case, as I mean as you have seen already, infinity. But you know, the case k equals zero is easier, as I've already mentioned uh, you don't need these more complicated tools, but I mean that. Okay, okay, here is the finish CD.
00:52:01.964 - 00:52:12.944, Speaker B: Well, so X DSO, XDM is, you know, CDKN is so if for every.
00:52:13.304 - 00:52:21.416, Speaker A: Mu zero, mu one. Well, let me say absolutely continues with.
00:52:21.440 - 00:52:29.684, Speaker B: Respect to n with bounded support. Let me add bounded support.
00:52:32.104 - 00:52:55.654, Speaker A: There exists PI optimal jurisdiction, not as a plan connecting the two. So e zero push over PI is mu zero e one push over PI is mu one, mu one.
00:52:59.594 - 00:53:00.534, Speaker B: So that.
00:53:03.034 - 00:53:10.162, Speaker A: Writing et push over PI as rho t m plus some singular.
00:53:10.218 - 00:53:21.754, Speaker B: Part where, you know, sorry, mu t per this is mu t s, and mu t s stands for single.
00:53:21.794 - 00:53:23.254, Speaker A: This is perpendicular to m.
00:53:25.314 - 00:53:25.754, Speaker B: You know.
00:53:25.794 - 00:53:35.154, Speaker A: Writing this, we have that minus the integral of.
00:53:42.454 - 00:53:50.914, Speaker B: Rho t to the power one minus one over n prime d vol dm.
00:53:51.654 - 00:53:53.470, Speaker A: This is less or equal than minus.
00:53:53.542 - 00:54:35.534, Speaker B: The integral to one minus t one minus t a n prime distance gamma zero gamma one rho zero. The power one over n prime at gamma zero plus tau by time t same distance. So let me write the form and then I comment rho one one over n prime in gamma 1d PI of gamma.
00:54:35.914 - 00:54:37.654, Speaker A: And this should be true for every.
00:54:39.114 - 00:54:48.804, Speaker B: T t in zero one and for every n prime greater equal than nice.
00:54:51.574 - 00:54:54.590, Speaker A: All right, so what is this?
00:54:54.742 - 00:54:56.798, Speaker B: Well, first of all, let's have a.
00:54:56.806 - 00:55:09.846, Speaker A: Look to this sort of function over here on the left hand side. This is what is called the reni entropy function. You can see that if you correctly renormalize it and rescale when n prime goes to plus infinity, this goes to.
00:55:09.870 - 00:55:11.950, Speaker B: The entropy, to the entropy.
00:55:12.022 - 00:55:34.094, Speaker A: I mean, if I send n prime in this formula to infinity, the, you know, this just simply goes to one. There's nothing exciting, but if you do something like, you know, the intro of mine, what it is minus, I guess, what is x z? Let me say z to the power one minus one over n plus z multiplied by m. Something like this, this.
00:55:34.134 - 00:55:38.454, Speaker B: Should go to z log z. Okay.
00:55:38.614 - 00:56:05.308, Speaker A: And so here you could, I could add plus the inter of rho t, which is morally one. So I'm morally adding, so nothing. And then escalation. I could multiply this inequality by n by n prime if I want. So, to make this more evident, but we'll make the formula more complicated, and it is already complicated enough. You know, let's just keep this in mind, that the lenient entropy, so this sort of integral is a finite dimensional version of the Boltzmann channel.
00:56:05.356 - 00:56:07.904, Speaker B: And let's be happy with that. That's nothing.
00:56:09.864 - 00:56:12.080, Speaker A: The singular part does not play any.
00:56:12.112 - 00:56:16.416, Speaker B: Role, whereas for the Boltzmann entropy, right?
00:56:16.560 - 00:56:20.160, Speaker A: If the measure had a singular part, the entropy was plus infinity.
00:56:20.272 - 00:56:20.904, Speaker B: Right.
00:56:21.064 - 00:56:49.836, Speaker A: Why here are we just throwing it away? What does do with the shape? I mean, this is the correct thing to do from the perspective of calculus of variation. It has to do with the fact that the exponent here is smaller than one. Okay, imagine, imagine that you imagine that you approximate a direct mass with a uniform distribution over a small ball. And let's. And look at what happens at this rainy entropy or at the Boltzmann entropy, right? So let's. Let's have a look at this. So, if you.
00:56:49.836 - 00:56:55.108, Speaker A: If you don't have experience with functions of measures, this example should be quite illuminating.
00:56:55.236 - 00:56:55.900, Speaker B: So let's say.
00:56:55.932 - 00:57:06.804, Speaker A: Let's say that you have, you know, mu r is just the reference measure restricted to some ball of radius r centered at x normalized. So measure of the ball to the powers minus one.
00:57:07.184 - 00:57:08.776, Speaker B: Okay, let's compute.
00:57:08.840 - 00:57:13.120, Speaker A: Let's compute the Boltzmann entropy. So what it is the entropy, you know, that we have used as of.
00:57:13.152 - 00:57:15.524, Speaker B: Now, of these mu r?
00:57:16.304 - 00:57:17.816, Speaker A: Well, it should be, you know, the.
00:57:17.880 - 00:57:27.880, Speaker B: Integral of, you know, density, log log of the density. So the density is what, m of.
00:57:27.912 - 00:57:35.832, Speaker A: The ball, you know, to the power minus one log of the density. Uh, so which is what, there's a minus, and there's a log of, you.
00:57:35.848 - 00:57:38.632, Speaker B: Know, m of the ball or in the z.
00:57:38.728 - 00:57:40.864, Speaker A: And this should be integrated basically only.
00:57:40.904 - 00:57:42.952, Speaker B: You know, I mean, the density is.
00:57:42.968 - 00:57:43.736, Speaker A: Zero outside the wall.
00:57:43.760 - 00:57:47.888, Speaker B: So I just integrate in weight right now.
00:57:48.056 - 00:58:01.090, Speaker A: So, so integrating this is a constant. So, integrating over here, I get, you know, this gets simplified, and this is minus log of the measure of. Of the volume of radius r. So this goes to plus infinity as r.
00:58:01.122 - 00:58:02.666, Speaker B: Goes to zero, right?
00:58:02.770 - 00:58:31.916, Speaker A: Or as the measure goes to zero. And this is an indication, if you want. If you don't want to give too many things, but it's an indication that perhaps if the measure is a singular part, the correct value has to put to the entropy should be plus infinity, because whenever I approximate it with the most regular thing that I could possibly be, which is the uniform, with the measure with a uniform density, you know, in the final sum neighborhood of the support, what I get is plus infinity.
00:58:32.060 - 00:58:34.452, Speaker B: Okay, let's see what happens in the.
00:58:34.468 - 00:58:46.132, Speaker A: Same case where I have a something, you know, which is, which has an exponent less than one. So here, the crucial part is that the function z log z is a superlinear.
00:58:46.268 - 00:58:46.852, Speaker B: Okay?
00:58:46.948 - 00:58:56.898, Speaker A: And what remains after, you know, having calculated the density is just in some sense, the excess in linearity, if you want. So let's see what happens in the same case.
00:58:56.946 - 00:58:59.626, Speaker B: But if we integrate, you know, again.
00:58:59.690 - 00:59:03.962, Speaker A: Of course, over the same ball, the density to the power one minus one.
00:59:03.978 - 00:59:07.974, Speaker B: Over n. So I have the mass.
00:59:08.274 - 00:59:11.058, Speaker A: Of the ball to the power one.
00:59:11.106 - 00:59:15.174, Speaker B: Minus one over n. Right.
00:59:17.514 - 00:59:29.304, Speaker A: There's a minus in front. Wait a second. I mean, I'm a little bit, no, sorry, the dense, sorry, the density is this to the power minus one that.
00:59:29.344 - 00:59:36.280, Speaker B: We, so this is, you know, again.
00:59:36.352 - 00:59:45.216, Speaker A: This is, again minus one. We get simplified by the fact that set. So this remains minus the measure of the ball of radians, r to the.
00:59:45.240 - 00:59:49.838, Speaker B: Power one over n. Right?
00:59:49.926 - 01:00:29.074, Speaker A: So again, it's just the, you know, the difference from the linear part in some sense. But now, you know, if this goes to zero, this goes to, if this number goes to zero, I'm raising it to a positive power. This goes to zero. Okay, so in some sense, this justifies why in computer, this rainy entropy, we are not, we are not looking at the single part. Okay, justify, I mean, it's an informal justification. Of course. The proof, the proof it would be, I guess the correct statement is you define, first of all, you define iranian entropy, just, or Boltzmann entropy, just on measures that have no singular part, just as the internal density.
01:00:29.074 - 01:00:43.806, Speaker A: And then you extend that to the whole class of probability measures by taking the lower semi continuous relaxation with respect to weak conversions. In one case that you pay plus infinity whenever you have a single part. In the other case, you just, okay, okay. And that depends just on the behavior of the.
01:00:43.950 - 01:00:44.734, Speaker B: Okay.
01:00:44.894 - 01:01:06.982, Speaker A: Anyway, now what about here? Now the CDK kn condition is not, is not a condition about, strictly speaking, it's not a condition about convexity of the radiant entropy. You know, the CDK amphide is morally easy in some sense. No, it's just the entropy SC convex. I have jurisdictional. This is not like that. It's a bit more complicated. And although it looks like some sort.
01:01:06.998 - 01:01:09.160, Speaker B: Of complexity because now let's have a.
01:01:09.162 - 01:01:23.338, Speaker A: Look to this right hand side. It is the sum of two things that are, you know, really symmetric. One is, you know, time zero, the other one is time one. Let's have a look to the, to this quantity over here. And notice that if there was no.
01:01:23.386 - 01:01:26.994, Speaker B: Such guy over here, this integral integrated.
01:01:27.034 - 01:01:31.374, Speaker A: With respect to PI would be exactly the raynian entropy of mu zero.
01:01:33.154 - 01:01:34.014, Speaker B: Okay?
01:01:34.774 - 01:01:40.394, Speaker A: But here I'm paying a distortion coefficient. That depends on how long this jurisdict is traveling.
01:01:41.654 - 01:01:42.526, Speaker B: Okay?
01:01:42.710 - 01:01:49.310, Speaker A: And, and then there is the symmetric guide that there is a symmetrical type one. It's a bit more complicated, but, you know, the concept is.
01:01:49.462 - 01:01:51.834, Speaker B: Okay, um.
01:01:53.214 - 01:02:16.674, Speaker A: All right, that's the formula. One last comment. Why for every n prime greater or equal than n, why don't we just ask this problem? Well, there's a good reason. The reason is that a priority is not clear, given that we might have branching of jurisdiction, so on and so forth. It is not clear whether the validity of this for some n implies the validity for all the n primes bigger than that.
01:02:17.454 - 01:02:20.142, Speaker B: And, you know, among other things, you.
01:02:20.158 - 01:03:00.506, Speaker A: Would like this CDK and condition to have some natural, you know, inclusion. So if you have a space which is CDKN, you would like it to be at least, you know, CDK n plus one, for instance. Right? And so, and that's, on the other hand, it's also useful in terms of, you know, some inequality that you might want to derive to be sure that you have, you know, convexity, you know, for any guys. And, and of course, sorry. A theorem, theorem, Sturm arim manifold satisfies a CDK. And if and only if the rich curator is greater than k in the dimension of the silver DNA. The meaning, you know, that has to be there, otherwise we are speaking of nothing.
01:03:00.610 - 01:03:01.374, Speaker B: Okay.
01:03:12.074 - 01:03:16.694, Speaker A: Want to make evident how laser? Yeah. Let me now give the definition of CD star condition.
01:03:18.714 - 01:03:22.854, Speaker B: My space is CD star if.
01:03:22.934 - 01:03:48.830, Speaker A: Blah, blah, blah, blah, blah, sigma, sigma. Okay, that's the only difference between the two finishes. You just use the Sigma coefficients in place of the two coefficients. Theorem. Space is remaining many for the city star. If and only it is, it is, there's lower bound on the r by k and upper bound by n. In particular, remaining for CD star.
01:03:48.830 - 01:03:54.174, Speaker A: And CDKN are equivalent, because they are equivalent to this, you know, tensorial version of.
01:04:06.514 - 01:04:11.986, Speaker B: There's some easy relation or concept or.
01:04:12.010 - 01:04:30.152, Speaker A: Ease maybe then the computations are complicated. But, you know, it's just numerology. If you want, um, the following is true. So there are some basic relation between CD and CD star. Let me, let me point this out.
01:04:30.328 - 01:04:31.084, Speaker B: Yeah.
01:04:31.424 - 01:04:45.184, Speaker A: So, theorem. So, the CD star condition has been produced in a paper by Sturtmanbacher. And so, theorem. And if you want, this is a Sturm backer. Backer Sturm.
01:04:53.164 - 01:05:09.224, Speaker B: So the CDKN condition implies CD star kn condition. So if you are CD star for some positive k, then you are CD.
01:05:09.554 - 01:05:11.534, Speaker A: With a slightly worse case.
01:05:13.394 - 01:05:14.494, Speaker B: And the semen.
01:05:17.834 - 01:05:46.324, Speaker A: So in particular. But this is evident from the definition of sigma and Turco efficient. In particular, CD zero n is the same as CDkn. So the difference is only really felt sorry, the difference is already felt when the core value bound is not zero. And last but certainly not least, CD star.
01:05:46.824 - 01:05:50.084, Speaker B: Kn is equivalent.
01:05:56.344 - 01:06:10.332, Speaker A: Yes is equivalent. No implies, implies. There is some implies the CD lock. And we'll mention a second. What is cdlock? For every k prime strict is more.
01:06:10.348 - 01:06:17.060, Speaker B: Than k. What is CD lock? It means that every point has enabled.
01:06:17.172 - 01:06:21.460, Speaker A: Such that the CD inequality holds. If you're starting and ending measures have.
01:06:21.492 - 01:06:24.052, Speaker B: Support in that neighborhood, then maybe the.
01:06:24.068 - 01:06:31.552, Speaker A: Judiciaries go a little bit far from the neighborhood, just because the netbolt maybe is not convex, but certainly not too far, because there must be logistics.
01:06:31.608 - 01:06:36.672, Speaker B: So. Okay, all right then.
01:06:36.768 - 01:06:59.744, Speaker A: Okay, so here's the question. Why did Sturm border about, you know, defining ultra CD star one you already had the CDKN already had equivalents with, with the romanian case. And the reason, the reason is that it is easy to see. There's an easy, there's an easy also perhaps so on non branch, if x.
01:06:59.784 - 01:07:02.336, Speaker B: Is non branching, is non branching.
01:07:02.400 - 01:07:05.404, Speaker A: Let me add here, is non branching, I mean backwards too.
01:07:09.184 - 01:07:19.044, Speaker B: Then CD star lock, pn implies CD star here.
01:07:21.664 - 01:07:55.808, Speaker A: So the star version of the curvature admits a globalization theorem, at least under some, you know, non branching assumption that you can regard as a technical assumption. In fact, this has been extended also to essential branching spaces. I don't remember by whom, but there's a better theorem in a second, so I will. Okay, so, so, of course, I mean, when you define. So student made a few problems. When define defining, you know, these lower, these lower rich bonds. One was the stability, one was the, you know, the compatibility with the romanian case.
01:07:55.808 - 01:08:24.692, Speaker A: But then there are a number of questions that, you know, you would like, or a number of natural properties that you would like to expect any curvature bound to be in place. And one of these is locale, right? So, curvature is a local quantity in urbanian geometry. So you would like also your synthetic version of courage to be, you know, localizable. So, if I only look at small portions of the space, and I look that in each of these portions I have the lower bound on the rich equation upper one dimensional, can I conclude that my space is CD three?
01:08:24.868 - 01:08:25.684, Speaker B: Okay.
01:08:25.844 - 01:08:36.292, Speaker A: And this kind of statement is important because you cannot, you know, for stability purposes, CD star lock or CD lock.
01:08:36.308 - 01:08:37.396, Speaker B: They are not stable.
01:08:37.580 - 01:09:09.328, Speaker A: Okay, a priori. If you don't have a globalization, because, because you could have had sequence of spaces, each of which is siloc, you wonder. But in some sense, the locality is only true in smaller, smaller neighborhoods. If you are not able to integrate these inequalities then you get nowhere. In fact, I mean, the whole point of this convex inequality is to integrate a differential property, right? But if we just integrate small neighborhoods without being able to enlarge these neighbors, we are basically stuck almost where we were starting from.
01:09:09.416 - 01:09:11.648, Speaker B: Okay, all right.
01:09:11.696 - 01:09:27.832, Speaker A: Why this theorem is easy. So, frankly, this theorem is released. I mean, once you write down the, you know, the non branching, you understand that the, so if x is non branching, basically you can take out the integral from this inequality written for the.
01:09:27.848 - 01:09:28.644, Speaker B: Sigmas.
01:09:30.624 - 01:10:09.454, Speaker A: Much like we could take out the integral from. Okay, and once you take out the integrals, basically what you end up with is an inequality regarding the density of your interpol of the measures along the Judisc. And this density are convex in a sense that is related to the sigma coefficients. Why then, this convex inequality can easily, be quite easily generalized. And it's not the case this for the toe coefficients. Well, the reason is that the sigma coefficients, they do solve an OD. That's the basic, that's the basic point.
01:10:09.454 - 01:10:21.774, Speaker A: While they talk, of which they do not, there's no such a simple od, which is satisfied by the coefficient. So the sigma fingers of the d. So in some sense, this means that if you are, you know, if, if you are.
01:10:24.194 - 01:10:25.010, Speaker B: Let me put things.
01:10:25.042 - 01:11:11.518, Speaker A: So perhaps this is easier in the, in the, in the, you know, for convex function. So the fact that the convex function, you know, means hessian and non negative is, implies that you can recognize a convex function by looking at, you know, small intervals. Is this convex? You know, as far as the function is finite, you know, being a convex on, on a cover implies convexity on the global, on the global scale, okay? And this is kind of the same phenomenon because being sigma convex, being more convex than what is, you know, enforced by the sigma coefficient, is something that you can check locally because the sigma coefficients satisfy a certain audit. I'm very rough, but that's, you know, the bottom line at the bottom of the argument at this point.
01:11:11.606 - 01:11:15.246, Speaker B: Okay, all right, so are we, so.
01:11:15.270 - 01:12:00.326, Speaker A: Okay, but when we have CDCD star, there's some relation. But CD star is also, I mean, all these conditions are all stable. Basically, by the same argument that we're seeing for the, for the CDK infinity condition, there is no additional complication at that level. So we have condition, we have two conditions, both are stable, both are, you know, compatible with the remaining setting. One has the better property of being globalizable, at least under, you know, some non branching assumption. Why don't we just stay with CD star and forget about CD well, the point is that, and this has been a point, important point for a long time in geometric and functional inequalities, lignarovic spectra gap, you know, some, I think even bone and Myers, if I'm not wrong, and so on. CD gives the sharp constant.
01:12:00.470 - 01:12:02.470, Speaker B: CD star does not 3d start.
01:12:02.502 - 01:12:53.330, Speaker A: Sometimes you get other, rather than getting some, you know, coefficient k, you get k multiplied by n over n minus one, or vice versa. I mean, something that goes in the wrong direction and pays off the fact that you forgot one, you know, one, you know, you didn't use the, the fact that in the smooth case, it's true that you don't feel the rich evolution directional motion. So, constants are sometimes a bit off. So what do we do? This has been, you know, a huge problem in the theory for like ten years. No, maybe more, more than ten years. Until Kawaleti Millman. They have been finally able to prove that CD is equivalent to CD star, at least, at least under some theorem.
01:12:53.402 - 01:12:56.894, Speaker B: The musical theorem by Cavalecki.
01:12:59.954 - 01:13:02.894, Speaker A: This is the globalization of theorem. This is if you want.
01:13:04.714 - 01:13:08.762, Speaker B: If you want. It is the analogy of Toponogov's theorem.
01:13:08.818 - 01:13:21.226, Speaker A: For lower reach bound in the synthetic setting. So the statement is this. So if you have a space which is CD, I guess, well, CD star.
01:13:21.410 - 01:13:24.494, Speaker B: The n and essentially non branching.
01:13:29.794 - 01:13:30.202, Speaker A: Actually.
01:13:30.258 - 01:13:33.414, Speaker B: Essentially, essentially non branch.
01:13:37.954 - 01:13:39.974, Speaker A: And with finite mass.
01:13:42.154 - 01:13:47.694, Speaker B: Then, you know, it is CD theorem.
01:13:50.914 - 01:14:00.954, Speaker A: So you can view this as a globalization theorem, because CD star is basically equivalent to cDlock, more or less. You have CD lock, you get the glove.
01:14:01.114 - 01:14:01.974, Speaker B: Okay.
01:14:04.554 - 01:14:46.060, Speaker A: So there is this assumption of essential non branching. But as we learn, so at least if we care about the R CD version of this statement, thanks to Surmayara, this is not an issue. So this the, you know, among other things, I mean, if we are extremely, in some sense unpolite, we can see this result as a result that, you know, clarifies, clarifies the taxonomy of CD spaces. They're all equivalent. Okay, of course there's more than that. I mean, it's not a matter of nomenclature, it's a matter, there's a theorem about, you know, this rich curvature bound that can be globalized. Okay, this assumption is, you know, widely believed by anybody in the fields.
01:14:46.060 - 01:14:50.224, Speaker A: That is actually not necessary. But nobody has ever actually written down the proof, so.
01:14:54.244 - 01:14:55.104, Speaker B: Okay.
01:15:02.444 - 01:15:03.264, Speaker A: Fine.
01:15:09.364 - 01:15:10.704, Speaker B: Very well, great.
01:15:11.524 - 01:15:25.234, Speaker A: The technical reason why this is present is because this proof is strongly based on this integration techniques and this integration theorems. You know, it's easy to disintegrate. I mean, standard integration theorems is about probability measures.
01:15:26.014 - 01:15:29.382, Speaker B: So you know, if you want to.
01:15:29.398 - 01:15:39.486, Speaker A: Go to infinite measures, you have to be a bit more careful. But I mean if you, I'm glad that somebody is being, is writing down the full threshold.
01:15:39.670 - 01:15:42.542, Speaker B: All right, so why did I, you.
01:15:42.558 - 01:16:28.654, Speaker A: Know, mention all these and just, didn't just you, you know, speak about CDKN and end of the story? Well, for two reasons. First of all, because if you read papers or you go to conference about richer boundary, will for sure hear CD, CD stat and whatever, and I want, you know, this to be clear to everybody. And second, because in fact, the fact that, you know, these globalization theorem for CDKN, which is basically this theorem, is an important theorem. And I didn't want to overlook this result by, you know, not mentioning a little bit of the story. And okay, so that's one thing. And there's also another reason, because in fact now if I want to go to RCDKN, I need to introduce a third variant. And the third variant, and this has been introduced by Herbert Kuva and Sturm.
01:16:28.954 - 01:16:32.854, Speaker B: And the third variant goes like this is the CDE.
01:16:34.914 - 01:16:39.564, Speaker A: And the CDE condition is an interesting, in some senses, I mean.
01:16:41.664 - 01:16:42.112, Speaker B: It'S a.
01:16:42.128 - 01:17:08.284, Speaker A: Personal comment, it's a pt that it did not come out sooner because I think it's even simpler to describe. It does not involve raynian entropy function, it just sticks to relative entropy and well, starts, I think that. Let me start with the following definition. Say that you have a function from.
01:17:08.324 - 01:17:13.380, Speaker B: Zero, one to r, and I say.
01:17:13.412 - 01:17:41.970, Speaker A: That this is by definition is not k. So we have seen k convexity, right? So k convexity is bigger than k, is the same, is k and convex. If so, convexity means that in some weak sense at least, the hessian of f is greater even k, right? That's what k, what k complexity is. Now, k. And convexity means that the essential is bigger than this plus something.
01:17:42.042 - 01:17:44.866, Speaker B: Which is positive, which is the differential.
01:17:44.890 - 01:17:47.002, Speaker A: Of f tends or differential of f.
01:17:47.018 - 01:17:48.054, Speaker B: Divided by f.
01:17:50.714 - 01:17:53.850, Speaker A: Of course, when n is plus infinity, this reduces to the.
01:17:53.882 - 01:18:03.666, Speaker B: Classical complexity, but it is a stronger form of convexity. Um, now notice that this is, if.
01:18:03.690 - 01:18:27.754, Speaker A: You put, I mean, this is just again, numerology. If you replace f with what is minus. So let me define capital f as minus exponent of minus one over nf, then this is equivalent to the Hessian.
01:18:28.934 - 01:18:31.518, Speaker B: Well, let me call it fn, the.
01:18:31.566 - 01:18:43.874, Speaker A: Hessian of fn to be, you know, plus k over n, fn to be greater or equal than z.
01:18:48.734 - 01:18:55.364, Speaker B: Let me notice that this looks suspiciously close to this.
01:19:02.064 - 01:19:04.404, Speaker A: Okay, now now that we are this.
01:19:07.744 - 01:19:08.924, Speaker B: Let me raise this.
01:19:23.684 - 01:19:24.332, Speaker A: Now that we.
01:19:24.348 - 01:19:29.020, Speaker B: Have this, um, um, by the way.
01:19:29.052 - 01:19:31.060, Speaker A: So this is, and in fact, and.
01:19:31.092 - 01:19:40.556, Speaker B: In fact, this is also equivalent to the following inequality. Uh, fn of t is less or.
01:19:40.580 - 01:19:47.334, Speaker A: Equal than sigma one minus t. Actually, let me, sorry, me put this way.
01:19:47.714 - 01:20:00.414, Speaker B: So let me think.
01:20:05.794 - 01:20:08.018, Speaker A: Yeah, so, so if, let me put it.
01:20:08.066 - 01:20:14.682, Speaker B: So, if, if gamma, if, say, gamma.
01:20:14.778 - 01:20:31.254, Speaker A: From zero one to a matrix space, x is jurisic. And then the map t to fn of gamma t is, you know, satisfies.
01:20:31.754 - 01:20:37.634, Speaker B: Satisfies star, star.
01:20:41.334 - 01:20:51.430, Speaker A: If and only if. I mean, I can write this, this is a convex inequality, so I can integrate this inequality. And the integrated version of this inequality.
01:20:51.462 - 01:20:55.230, Speaker B: Is, this is fn gamma t is.
01:20:55.262 - 01:20:57.742, Speaker A: Less or equal than sigma one minus.
01:20:57.798 - 01:21:08.374, Speaker B: T a n fn gamma zero plus sigma one minus t. Sorry.
01:21:10.394 - 01:21:13.626, Speaker A: But sigma should take ultra distance.
01:21:13.690 - 01:21:17.266, Speaker B: The distance between gamma zero, gamma one.
01:21:17.450 - 01:21:28.664, Speaker A: F zero, gamma zero plus. The symmetric guy. Again, I don't understand that seeing all these coefficients is confusing.
01:21:28.964 - 01:21:31.540, Speaker B: And I want to, you know, assure.
01:21:31.572 - 01:22:09.938, Speaker A: You it remains confusing even after years of studies in this direction. So it's just what it is. So, but let me point out this. So, yes, so first of all, as I mentioned, this is differentiating quality that emits an easy integrated version. And if you integrate, if you integrate this version, in some sense, the, you know, it plays some role if you want this sort of the speed you're moving with. So if I'm anime space, so the distance between gamma zero and gamma one appears, and this is just, you know, an inequality.
01:22:10.026 - 01:22:20.034, Speaker B: Okay, all right. Now, definition. Definition.
01:22:20.374 - 01:22:22.074, Speaker A: Erba kuvada Sturm.
01:22:22.454 - 01:22:33.598, Speaker B: Erba kuvada Sturm. If I ever, xdm is so called.
01:22:33.646 - 01:22:39.094, Speaker A: The Cde kn space where e here stands for entropic dimension.
01:22:39.174 - 01:22:39.794, Speaker B: If.
01:22:41.574 - 01:22:44.150, Speaker A: The functional, let me put this way.
01:22:44.182 - 01:22:59.664, Speaker B: So the functional en of mu, which is, you know, minus x of minus, the entropy of mu divided by n.
01:22:59.964 - 01:23:01.424, Speaker A: Is k n convex.
01:23:02.964 - 01:23:06.104, Speaker B: Is kn convex.
01:23:09.924 - 01:23:28.600, Speaker A: No, in the, in the vast instance, in the vast network in w with respect to w. So again, what does this mean? Let me not, don't, let me write the full definition. This means that for any two measures.
01:23:28.672 - 01:23:31.536, Speaker B: With finite entropy, so we find where.
01:23:31.560 - 01:24:24.634, Speaker A: This is finite, because the domain of finiteness is the same. There exists a w, two geodesics, along which, you know, entropy as mu t is less or equal than sigma plus. Okay, so one of the advantages of these definition is that now, okay, let me actually, let me write. So you, what you get is an entropy of mu t less or equal to sigma one minus tanw two between the mu zero mu one entropy nu zero plus the other term. And now you see, this is a real inequality for the functional en, whereas, whereas here there is, it's not really a convexity of the entropy, because there is this mixture of tau and sigma that are, that are in the same.
01:24:28.374 - 01:24:29.234, Speaker B: All right.
01:24:30.454 - 01:25:00.124, Speaker A: Of course, one of the first thing that they prove is that for in my manifold, this is equivalent to curvature dimensional condition in the classical sense. And the interpretation is that whereas a lower reach bound can be interpreted as the hessian of the entropy bigger than k times the identity, a curvature dimensional condition can be read as, you know.
01:25:03.144 - 01:25:05.364, Speaker B: Formally formal work.
01:25:06.344 - 01:25:29.464, Speaker A: The regular version is, you know, okay, and why is this relevant for us? Well, this is relevant because of the following thing. Well, first of all, first of all, it's a, is a, is a, is a, is an easy theorem, is an easy theorem that if x is essentially nonbranching.
01:25:34.844 - 01:25:35.984, Speaker B: Non branching.
01:25:40.084 - 01:25:45.224, Speaker A: Then CD is the same as CDE.
01:25:47.204 - 01:25:48.064, Speaker B: Okay?
01:25:50.084 - 01:26:07.244, Speaker A: It's not easy that they are both CD, that's Kavaleti minimal. But in some sense, both these conditions that can be read through odes in some sense are equivalent. Okay, and why are they equivalent? Well, because in both cases they reduce the validity of this inequality without, without.
01:26:07.324 - 01:26:08.704, Speaker B: Instances, the integral.
01:26:10.524 - 01:26:29.104, Speaker A: Because under this non branching assumption, you can, you know, localize. And once you take out the integrals and look at what happens point wise, these two inequalities are really the same. For instance, CD star and CD are two different versions of integrating that inequality, okay? But they are equivalent once you can disintegrate them.
01:26:30.404 - 01:26:35.284, Speaker B: All right, now let me perhaps, now.
01:26:35.324 - 01:26:37.596, Speaker A: Here is the, and of course, once.
01:26:37.620 - 01:26:40.944, Speaker B: You have this, I mean, okay, now here is.
01:26:41.964 - 01:26:43.304, Speaker A: I need to raise something.
01:26:43.924 - 01:26:46.864, Speaker B: Let me raise this.
01:26:52.464 - 01:27:19.280, Speaker A: Now, of course, each of these definitions comes with the R version. We just add implication by hand. And if you want, you know, one of the things that Herbert Kwad and Stoom did was to centralize that the easy, it is easier to deal with RCDe. Okay, the thanks to this result is the same as RCD star.
01:27:19.472 - 01:27:21.832, Speaker B: If you want, the thanks to Cavaletti.
01:27:21.848 - 01:27:23.844, Speaker A: Millman is anyway the same as RCD.
01:27:24.704 - 01:27:34.124, Speaker B: Okay, the theorem that they proved.
01:27:39.144 - 01:27:55.748, Speaker A: Okay, there are various, okay, let me, let me state the most important, I think result. Then I will detail, give a couple of details. So the theorem is the following. So x is, you know, r c.
01:27:55.836 - 01:28:00.444, Speaker B: D e k n if and only.
01:28:00.564 - 01:28:09.076, Speaker A: Xdn if and only if the following four conditions are true. And this, as you will see, will be extremely diminished of what I wrote last time.
01:28:09.220 - 01:28:11.396, Speaker B: So, first of all, the measure of.
01:28:11.420 - 01:28:22.064, Speaker A: The ball of radius r centered x is bounded by some constant e to constant r squared, you know, whatever r positive given x no, given x.
01:28:25.404 - 01:28:26.124, Speaker B: Two.
01:28:26.284 - 01:28:48.960, Speaker A: The sublet to lips its conditionals. I wonder why it. So whenever you are sober function, we can pregame the boundary by one it has a one lipstick representative, three, it is infinite version, and four, the balkan inequality holds in the following version.
01:28:49.112 - 01:28:49.992, Speaker B: So for.
01:28:50.128 - 01:29:21.502, Speaker A: So for rcd k 3d, we have the following condition. So, for every, what was f in the domain of the Laplacian with Laplacian of fw twelve and g non negative g was in l two and in l infinity with Laplacian of g in l infinity. So, for rcd, the inequality was interval of Laplace energy grad or dx squared.
01:29:21.558 - 01:29:24.326, Speaker B: Over two dm greater or equal than.
01:29:24.350 - 01:29:31.734, Speaker A: The interval of g laplace f plus k times gradient squared.
01:29:32.114 - 01:29:34.234, Speaker B: This was, you know, if I close.
01:29:34.274 - 01:29:49.130, Speaker A: The parentheses, close the integral. This was, you know, the theorem that I presented last time with n equal infinity. What they basically proved is that here RCD EKN enforces this inequality with a.
01:29:49.162 - 01:29:50.974, Speaker B: Positive term in the right hand side.
01:29:54.054 - 01:30:18.954, Speaker A: Okay, and this is, you know, in the romanian setting, this is, you know, perhaps I should mention that in the romanian setting, a curvature dimension condition is, you know, characterized by the validity of this inequality. And this is just the, you know, the weak version, the way to write that in our setting.
01:30:19.334 - 01:30:22.734, Speaker B: Okay, how did they prove this?
01:30:23.034 - 01:30:32.842, Speaker A: Well, the scheme of the. Well, I did. I proved this or something. Yes, I did prove this. Well, at least I proved one implication in the n infinity case. And. And the scheme was the following.
01:30:32.842 - 01:30:43.226, Speaker A: Uh, you start in the RCDK infinity. If you are RCD infinity, then you have the heat flow. This heat flow satisfies the Evi. The evi. Let me actually, let me write it here.
01:30:43.330 - 01:30:50.926, Speaker B: So you had, you know, I mean.
01:30:50.950 - 01:31:00.606, Speaker A: Basically it's the same scheme. What I want to say is this scheme of the proof complicated by the fact that, you know, here, you know, formulas are more complicated and there are also technical issues to be dealt with. But the scheme of the proof of.
01:31:00.630 - 01:31:05.714, Speaker B: This, so, Rcd, you know, k infinity.
01:31:06.294 - 01:31:51.844, Speaker A: This implies, you know, if you want, if you want, this implies. Implies the Evi k condition for the gradient flow of the entropy. This implies, among other things, implies the contractivity, not the w two contractivity of the heat flow. And this implies by duality about rehab estimate. And this implies that, you know, by differentiating and zero bock and inequality.
01:31:55.024 - 01:31:55.576, Speaker B: Right?
01:31:55.680 - 01:31:57.684, Speaker A: So at least this proves you know.
01:31:59.504 - 01:32:01.696, Speaker B: Well, plus, of course there is.
01:32:01.760 - 01:32:29.986, Speaker A: Okay, this proves just four in some sense. Three is part of the definition and there was one and two somewhere else. But if you want to prove, define a dimensional version of this result, one, two and three, you already have. The point is just proving book. Okay. And one of the things that Erbert coalesce tool did was to realize, I mean, this definitely one of the reasons why this definition is useful is because it comes with an analog of these. Well, I don't write the form anyway.
01:32:29.986 - 01:33:08.840, Speaker A: It's too late. So Rcv kn implies an avi kn formulation for the gradient flow where you don't differentiate. DVi was a differentiate distance squared from a given point. And this satisfies some inequality. And what you can replace is your differential, not distance squared, but some trigonometric function depending on some sinus function of the distance. And, you know, one has to be careful with the parameters. And here you have not this, but you have, you know, a more elaborated contractivity estimates, which is better, actually.
01:33:08.840 - 01:33:19.592, Speaker A: Let me write, let me write at least, let me write at least the contractivity estimate for the heat load and the, and the finite dimensional battery. Yeah. Battery memory estimate. These are, these are good to know.
01:33:19.608 - 01:33:42.332, Speaker B: Anyway, so along the proof in some sense, they, they obtain this. Okay, okay.
01:33:42.348 - 01:33:49.244, Speaker A: The quota estimate is, in fact, it's a bit scary. Let me write it anyway. So it tells that I k over.
01:33:49.284 - 01:33:57.812, Speaker B: N of one half w two mu t mu t squared.
01:33:57.948 - 01:34:15.826, Speaker A: This is less than e to the sorry, mu t, mu s. In fact, one feature of the evikn is that the contractivity that gives works even for different times, whereas for our contractivity of the gradient flow, we only add, you know, the distance between what was new.
01:34:15.850 - 01:34:19.106, Speaker B: T and u t. No, not mu.
01:34:19.130 - 01:34:38.506, Speaker A: T and u s. You get nothing better than from triangle in a quad where here you get something better, which is minus k times s plus t s. K over s is our, you know, modified, you know, modified function. Modified sine function. Yeah, you have mu nu if you.
01:34:38.530 - 01:34:44.822, Speaker B: Want this to the power two, and then you have a plus and, sorry.
01:34:44.878 - 01:34:50.902, Speaker A: Plus n over k times one minus e to the minus k s plus.
01:34:50.958 - 01:34:51.554, Speaker B: T.
01:34:53.374 - 01:34:55.718, Speaker A: Okay, square root of t minus.
01:34:55.766 - 01:35:00.354, Speaker B: Square root of s square divided by two s plus t.
01:35:02.934 - 01:35:11.242, Speaker A: This might look ugly, but let me just notice that this implies the following a bit weaker statement, but still stronger than what you.
01:35:11.258 - 01:35:55.754, Speaker B: Would get from, from a to minus k times square t w two nu zero nu zero plus two n one minus exponential minus k two divided k times two square root of t minus square root of s squared over two. St is by definition just two over three, t plus square root of t's plus s. I guess.
01:35:58.914 - 01:36:13.962, Speaker A: This is not easy to read. It's certainly not easy to read. But in particular, if you take t equals s, you get exactly the, the input that you had before where n disappears. But, you know, but this is better.
01:36:14.058 - 01:36:14.866, Speaker B: Okay.
01:36:15.050 - 01:36:39.594, Speaker A: And perhaps it is more interesting, the battery memory contraction estimate, because that is really in terms of more differential inequality. And. And the background recontraction estimate is the following.
01:36:39.894 - 01:36:45.034, Speaker B: It tells that the differential of HDF scored.
01:36:45.374 - 01:37:00.378, Speaker A: So the, in the infinite dimensional version was this fine. The final dimensional version adds a positive term over here, and the positive term.
01:37:00.426 - 01:37:05.498, Speaker B: Is plus four k t squared over.
01:37:05.666 - 01:37:10.514, Speaker A: N times e to the two kt minus one.
01:37:10.674 - 01:37:17.234, Speaker B: And here I have a laplacian of ft of HTML square.
01:37:18.854 - 01:37:20.998, Speaker A: So get not only a point wise.
01:37:21.046 - 01:37:23.854, Speaker B: Control on the gradient, but also a.
01:37:23.894 - 01:37:27.194, Speaker A: Point wise control over the laplacian of.
01:37:27.614 - 01:37:30.550, Speaker B: My heat flow, depending on the dimension.
01:37:30.582 - 01:37:37.774, Speaker A: And of course, as n goes to plus infinity, this goes to zero, as you know, one would expect by reality.
01:37:37.894 - 01:37:41.882, Speaker B: Okay, so you know, yeah.
01:37:41.938 - 01:38:02.178, Speaker A: So you have to understand how to interpret the Evi through this kn convexity. Then you have to understand which sort of contractility property this implies. Then you apply duality's principle to get the bucky memory estimate. And from this you differentiate the time zero and you get.
01:38:02.226 - 01:38:05.242, Speaker B: And you get the Bochary quality, okay.
01:38:05.418 - 01:38:31.384, Speaker A: And the vice versa, where I didn't say say that much, not even in the case any infinity. But in sense, all these implications can be, you know, reverted with a non trigger work. Let me just conclude this lecture by pointing out that Ambrosio Mondino Savare.
01:38:34.424 - 01:38:35.364, Speaker B: Proved.
01:38:35.984 - 01:38:48.284, Speaker A: Basically the star version of this theorem. The rest is really the same, using not really. Not really.
01:38:49.904 - 01:38:53.684, Speaker B: So, Erbert coalescence term.
01:38:55.504 - 01:39:00.856, Speaker A: The kn convex function was not the entropy, but this modified entropy, x plus minus one over.
01:39:00.880 - 01:39:04.306, Speaker B: N. Now, if you, if the gradient.
01:39:04.330 - 01:39:27.642, Speaker A: Flow of that function is nothing but the heat flow, but, you know, flown with a different time parameter, you know, if you degrade, if you post compose a function, the gradient of PI composition f is nothing but PI prime times the gradient of f, right? So in some sense, in flowing, in flowing this modified entropy, you're still flowing the heat flow, but with a different, you know, time parameter. So the flow is still linear, among other things, because the heat flow is.
01:39:27.658 - 01:39:32.160, Speaker B: Linear, thanks to implementation what Ambrose model and Saudi did.
01:39:32.192 - 01:39:39.896, Speaker A: Instead, they studied the gradient flow of the renient entropy, okay, which is not.
01:39:39.920 - 01:39:42.296, Speaker B: Linear, and the Renai entropy is not.
01:39:42.320 - 01:40:23.622, Speaker A: Really convex, because the CD star condition is not really convex. There's this strange integral, but still this gradient flow has some sort of contractivity property that can be utilized to some sort of. Can be derived some sort of bucky memory estimate that differentiated gives the book inequality. Okay, so this came a couple of years later. Again, the backbone of the proof is the same. Technical details are very different because, you know, the functional to be used is different, but still the picture is the same. Some sort of evi condition.
01:40:23.622 - 01:40:44.410, Speaker A: Evi gives contractivity. Contractivity gives by duality, by cremate, and by differentiating both. So that's the end for today. What I will do on Monday is I will speak about distributional Laplacian. And we speak about, you know, Laplacian. We'll do geometric analysis. See this in finite dimension.
01:40:44.410 - 01:40:56.454, Speaker A: So we'll see that, you know, the distance on a CD or an RCD space, uh, hydroplassium or upper bound, which is sharp and is useful to prove the splitting theorem, among other things.
01:40:56.794 - 01:41:01.626, Speaker B: Um, yeah, that's it. So, end of the lecture. Yeah, sorry.
01:41:01.810 - 01:41:08.814, Speaker A: Uh, I thought you still wanted to prove that heat flow commutes with Laplacian.
01:41:09.154 - 01:41:09.626, Speaker B: Yes.
01:41:09.690 - 01:41:25.494, Speaker A: In fact, I was telling to. I was going to say this. So I. At the end of the lecture now, there is an addendum, which is. Which is the. You know, what I was not what I had to prove from the previous lecture. But, you know, if I didn't say this sentence, I would have felt guilty because my lecture should end at 1140.
01:41:25.494 - 01:41:29.774, Speaker A: Speaking, I guess, right. It's already 1155 or whatever.
01:41:32.314 - 01:41:33.854, Speaker B: So now, let me put this.
01:41:40.194 - 01:42:12.594, Speaker A: So there's. There's an easy remark and then a technical one. The easy remark is the following. We know that given f pl two, you know, the map that takes t and returns h t f is a gradient flow, by definition, is a gradient flow, you know, is a gradient flow of our trigger energy. And this means, among other things, that dt of Htf is equal to d Laplacian of HTf. And let's say that I don't. I know nothing about regularity of.
01:42:12.594 - 01:42:14.654, Speaker A: This, at least, is true for almost everything.
01:42:15.174 - 01:42:22.566, Speaker B: Okay. All right, now let's have a look.
01:42:22.630 - 01:42:26.390, Speaker A: Now, let me fix. Say, what should I fix?
01:42:26.542 - 01:42:29.494, Speaker B: Say s. And let me look at the map.
01:42:29.534 - 01:42:31.526, Speaker A: Let me put it. Let me look at the value that.
01:42:31.550 - 01:42:35.114, Speaker B: Takes t and returns h t plus s over there.
01:42:38.014 - 01:42:53.450, Speaker A: Well, you know, the heat flow is a semi group by uniqueness of gradient flow. This is both ht of hsf and hs of HTf. So this is also a gradient flow starting not from f, but from h.
01:42:53.482 - 01:42:55.666, Speaker B: S of f, but so still satisfies.
01:42:55.730 - 01:43:05.454, Speaker A: If you want, this satisfies dt of ht plus sf. This is equal to Laplacian of ht plus sf.
01:43:06.034 - 01:43:06.894, Speaker B: Okay.
01:43:07.454 - 01:43:12.550, Speaker A: As of now, it might seem that I heard nothing. But in fact, given that this is.
01:43:12.622 - 01:43:18.358, Speaker B: Also hs of HTF, and given that.
01:43:18.486 - 01:43:23.766, Speaker A: Hs as a map from l two to itself is linear and continuous, it commutes with derivatives.
01:43:23.830 - 01:43:30.766, Speaker B: So this is equal with hs of DtHf. Right?
01:43:30.870 - 01:43:42.904, Speaker A: So what I gain out of this is that for almost, so this is now equal by the previous point. This is equal to hs of delta of HDF.
01:43:44.404 - 01:43:45.980, Speaker B: So what I know, in a sense.
01:43:46.012 - 01:43:51.356, Speaker A: For free, without using any sort of regularity or beside this, is that for.
01:43:51.380 - 01:43:53.620, Speaker B: Nes, what I know is that for.
01:43:53.652 - 01:44:09.786, Speaker A: Almost every t, I mean, if I compute this and I compute this, I get the same result. Now, what I want to do, I want to send t equal t to zero under the assumption that f has a Laplacian in l two. Okay, and how can I do this?
01:44:09.890 - 01:44:15.146, Speaker B: Well, I think the, from the perspective.
01:44:15.170 - 01:44:23.474, Speaker A: There are various ways of doing this. You can, you can either rely on the theory of linear semi groups or you rely on the theory of gradient flows.
01:44:23.594 - 01:44:23.954, Speaker B: Okay?
01:44:23.994 - 01:44:36.110, Speaker A: Given that I, you know, I adopted the gradient flow, you know, approach in this course, let me, you know, start with this, emphasize the second point of view. And the remark is the following is.
01:44:36.142 - 01:44:40.062, Speaker B: That, okay, this is true for every.
01:44:40.238 - 01:45:06.364, Speaker A: Convex function, but convex magnitude function space. But this slope, you can compute. So the sheer energy is a function over there too. And therefore, you know, I can define the slope and the slope is actually equal, is actually equal to actually, let me put this way. So for any bra, this way. So for any convex and lower semicontinuous energy on an inverse space, the slope at any given point x.
01:45:07.024 - 01:45:11.016, Speaker B: This is the mean among v in.
01:45:11.040 - 01:45:14.160, Speaker A: The sub differential of e at x.
01:45:14.312 - 01:45:19.440, Speaker B: Of the normal v h. It's a generic part.
01:45:19.472 - 01:45:50.844, Speaker A: It's not hard to prove. Okay, just by definition, this means in particular that if this guy is empty, this is plus infinity according to the usual, you know, convention that wants that the mean or the inf of an empty set is plus infinity. Okay? So in our case, in our case, this means that the slope of the chigger energy, let's say that we are in the infinitesimal Burke setting, so that the Chig energy is quadratic. So in particular, there is only, if the sub differential is not empty, contains only one guy. This is the same as the l two norm of the laplacian ov.
01:45:52.704 - 01:45:53.564, Speaker B: Okay.
01:45:55.264 - 01:46:28.380, Speaker A: Why am I pointing this out? Because I have a priori estimates on the, on this loop, on this loop over convex, of a convex function. And so I have a priori estimates on Laplacian. So what I had, what I get is that the Laplacian, if you something that is constant time t squared. Okay, there was this slope of. Now let me interpret it this way. This guy squared, this is less or equal than I guess, I guess the.
01:46:28.412 - 01:46:34.084, Speaker B: Electron mobile for some constant c. That depends on nothing.
01:46:34.204 - 01:46:41.312, Speaker A: This is true. This is true for anything. Uh, humidity and space.
01:46:41.408 - 01:46:41.928, Speaker B: Right?
01:46:42.056 - 01:47:07.688, Speaker A: Where does this come from? So I had an equation of the form. There was something like numerical constant distance called xdy plus another numerical constant divided by t multiplied by t. Sorry, t times numerical constant times the energy of xt minus the energy of this point y plus another numerical constant p squared. Or anyway, something that goes like t.
01:47:07.736 - 01:47:13.302, Speaker B: Squared, slope of the function squared at xt.
01:47:13.438 - 01:47:18.034, Speaker A: This was bounded by the square distance between x naught and y.
01:47:19.974 - 01:47:29.594, Speaker B: You remember now, peak energy. Oh, sorry. That's embarrassing. Yeah.
01:47:30.014 - 01:47:31.194, Speaker A: Pick the energy.
01:47:33.914 - 01:47:34.734, Speaker B: Sorry.
01:47:38.434 - 01:47:59.146, Speaker A: Okay, um, pick the energy, the trigger energy. Pick y, the zero function. It has no energy. Um, so on the right hand side, you get the square norm of f. Forget about this. Forget about this. Here you have t squared, slope squared, but the slope squared is the normal d Laplacian.
01:47:59.170 - 01:48:03.150, Speaker B: You get this very well.
01:48:03.302 - 01:48:09.142, Speaker A: And, okay, so that's the first thing, that's an ingredient. Another ingredient is the closure of the.
01:48:09.158 - 01:48:13.274, Speaker B: Laplacian that I want to use.
01:48:23.774 - 01:48:46.140, Speaker A: So, closure of the Laplacian means this. Suppose that fn converges in l two to some function f. And suppose that these fn's have a Laplacian and this Laplacian converge in l two to some function g. Then f has a Laplacian and the Laplacian of f is exactly.
01:48:46.172 - 01:48:49.196, Speaker B: G. That's the definition of being, of.
01:48:49.220 - 01:48:53.602, Speaker A: Being a cross operator. Let me prove this. So first of all, let me prove.
01:48:53.618 - 01:48:54.934, Speaker B: That f is sobolev.
01:48:55.434 - 01:49:16.994, Speaker A: And how do I prove that f is sobof? That's basically, you know, the simplest instance of quadruple inequality. What's the energy of fn? The energy of fn is maple sine is fn Laplacian of fn. But this converges in a two. This converges in a two. So this guy, you know, remains bounded.
01:49:17.034 - 01:49:19.414, Speaker B: In a, and if this remains bounded.
01:49:19.454 - 01:49:33.030, Speaker A: Converges in l two by semi continuity of the energy. The energy of f is bounded. Okay, so at least f is in w twelve. Now let's prove that it has a Laplacian. And the Laplacian is exactly g. What does it mean, this property?
01:49:33.062 - 01:49:40.158, Speaker B: It means. So I want to prove that for every h in w twelve, I do.
01:49:40.206 - 01:49:46.016, Speaker A: Have that, you know, minus the inter grad f grad h is equal to.
01:49:46.040 - 01:49:54.324, Speaker B: The integral of g h, right?
01:49:55.704 - 01:50:12.716, Speaker A: Well, in fact, I don't need to prove this for any h in w twelve. It is sufficient to prove this for a dense subset in w twelve, because then, I mean, both sides of these identities are continuous with respect to w twelve. So I can just prove this for a, for a dense subset, which then subset I pick.
01:50:12.780 - 01:50:15.092, Speaker B: Well, I pick the domain of the.
01:50:15.108 - 01:50:28.708, Speaker A: Laplace, which I know to which which I know is dense, for instance, because of this inequality of these a priori estimates. Well, but if h is the domain of the Laplace, then I can integrate by parts linear.
01:50:28.796 - 01:50:36.294, Speaker B: And this is the integral of a, sorry, f Laplacian hook.
01:50:37.784 - 01:50:59.624, Speaker A: So this is what I want to prove. And what it is, I don't know. Well, for any h denominator Laplacian, I can run the same computation for fn. And what I know is that the integral of fn Laplacian h is the same as the integral of Laplacian fn h. In fact, I proved that the Laplacian was self adjoint. Exactly. But now these guys fn converges to f.
01:50:59.624 - 01:51:06.798, Speaker A: So this guy is converge to in l two, and the Laplacian of h is in a two and, and also h is in a two. So this guy converges to g h.
01:51:06.926 - 01:51:08.114, Speaker B: So end of the problem.
01:51:09.494 - 01:51:20.734, Speaker A: All right, so why, why do I care about this? Let me remember. I don't remember.
01:51:20.774 - 01:51:21.594, Speaker B: Let me see.
01:51:29.314 - 01:51:32.774, Speaker A: Matthias, help me. Why did I prove this? There was a reason.
01:51:34.514 - 01:51:36.934, Speaker B: Could you help me closing this one to.
01:51:37.474 - 01:51:41.134, Speaker A: There was a silly reason out of this. How to conclude? Once I proved the.
01:51:45.594 - 01:51:46.574, Speaker B: Left hand.
01:51:49.794 - 01:51:54.178, Speaker A: Yeah, so, but I want, so I want to send t go to zero here.
01:51:54.266 - 01:51:54.594, Speaker B: Okay.
01:51:54.634 - 01:52:01.534, Speaker A: Yeah, so, so these. So, yes, so if I send t go to zero here. Yeah, thanks. So, by the priori estimate.
01:52:01.614 - 01:52:02.182, Speaker B: Thanks.
01:52:02.318 - 01:52:02.942, Speaker A: That's the point.
01:52:02.998 - 01:52:04.354, Speaker B: Thank you. So.
01:52:06.254 - 01:52:19.638, Speaker A: S is strictly positive. Otherwise I don't have to prove that. I mean, so s is strictly positive. Therefore, ht plus s of f has a Laplacian for any t positive. And all these Laplacians are uniformly bound.
01:52:19.806 - 01:52:20.590, Speaker B: Right.
01:52:20.782 - 01:52:31.248, Speaker A: Now send peak tn going to zero. Take the laplacements of these guys, these up to. Subsequently, they must converge weakly somewhere.
01:52:31.296 - 01:52:34.136, Speaker B: Sorry, here is weak.
01:52:34.160 - 01:52:45.400, Speaker A: Convergence is sufficient. The operator is linear. So, you know, being strongly closed is the same as being weakly closed. Anyway, so now the Laplacian of h.
01:52:45.472 - 01:52:49.856, Speaker B: T n plus s f, they have.
01:52:50.000 - 01:52:52.584, Speaker A: The uniform bounded norm. So up to subsequent is they must.
01:52:52.624 - 01:52:56.816, Speaker B: Converge somewhere weakly in l two, but.
01:52:56.840 - 01:53:21.420, Speaker A: By the closure property of the Laplacian. What I deduce is that, is that in fact, hs of f has a Laplacian, and the Laplacian of h s of f is exactly equal to this guy g. This reconversion. So when I send the t and this and this now is independent on the sea, on the sequential system, because the limit is identified, right? So when I send t to zero.
01:53:21.492 - 01:53:24.944, Speaker B: Here, this converges to the Laplacian of hsr.
01:53:25.844 - 01:53:27.064, Speaker A: And when I said.
01:53:29.884 - 01:53:30.904, Speaker B: Wait a second.
01:53:31.564 - 01:53:44.864, Speaker A: I wanted to send t equals zero even here on the right hand side. Oh, yes, yes. Here I want to use. Here I use the factor, I use the fact that this slope on a longer gradient flow is decreasing.
01:53:45.324 - 01:53:46.164, Speaker B: Remember?
01:53:46.324 - 01:53:54.984, Speaker A: And if the slope is decreasing and our slope coincides with the norm of the Laplacian, the normal Laplacian is decreasing.
01:53:55.404 - 01:53:56.116, Speaker B: Okay?
01:53:56.220 - 01:53:59.156, Speaker A: So if I assume that half as.
01:53:59.180 - 01:54:02.276, Speaker B: A Laplacian, then that was the assumption.
01:54:02.300 - 01:54:20.174, Speaker A: That to exchange the Laplacian with the heat flow, if I assume that f has a Laplacian, it means that f has a finite slope. It means that for any positive s or positive t, htf has a slope smaller than the one of f. And so the normal Laplacian is smaller than.
01:54:20.254 - 01:54:24.750, Speaker B: The one of the normal one. So now these Laplacians, forget about s.
01:54:24.862 - 01:54:27.406, Speaker A: These Laplacians are uniformly bounded in l.
01:54:27.430 - 01:54:31.166, Speaker B: Two, and any weak limit by closure when.
01:54:31.230 - 01:54:33.744, Speaker A: When t goes to zero must coincide with the Laplacian over.
01:54:36.484 - 01:54:38.864, Speaker B: Right. Make sense, vitaly?
01:54:41.644 - 01:54:44.836, Speaker A: You know, by, you know, the intent, the average interval from zero to t.
01:54:44.860 - 01:54:50.428, Speaker B: Of Laplacian, of Hsf ds, right?
01:54:50.516 - 01:55:14.602, Speaker A: But now if the Laplacian is a closed operator by Ila, you see that you can bring it out, the integral by Hille theorem, not. You see that? And now this is converging. This hs f is converging to f. You have these uniform bounds on the norms of the Laplacian of Hsf. So this converges. So this guy is converging to the.
01:55:14.618 - 01:55:19.658, Speaker B: Laplacian of f as soon as f has a laplacian union two, right?
01:55:19.826 - 01:55:29.282, Speaker A: And one. So what I'm proving here is that if f has a laplacian union two, then this incremental ratio converges. Difference quotient converges with a Prussian of.
01:55:29.298 - 01:55:32.706, Speaker B: F. But now I'm done, because.
01:55:32.770 - 01:55:51.978, Speaker A: Because if I know this, if I know, this is really just a matter of writing. You know, hs plus tf minus HSF divided by t, you have on one side, depending on how you decouple these. On one side, this goes to the Laplacian of HSF. And, you know, on the other hand.
01:55:51.986 - 01:55:54.818, Speaker B: It goes to Hs Laplacian over according.
01:55:54.906 - 01:56:00.214, Speaker A: To whether you, in some sense, you take Hs out of the fraction or not.
01:56:00.954 - 01:56:06.894, Speaker B: Make sense, vitaly? No. So.
01:56:08.034 - 01:56:22.894, Speaker A: So this can be written, you know, this can also be written as hs of hdf minus f divided by t. And now this converges to this. St goes to zero.
01:56:23.054 - 01:56:30.334, Speaker B: All right, a nice weekend, and see you on Monday. Bye.
