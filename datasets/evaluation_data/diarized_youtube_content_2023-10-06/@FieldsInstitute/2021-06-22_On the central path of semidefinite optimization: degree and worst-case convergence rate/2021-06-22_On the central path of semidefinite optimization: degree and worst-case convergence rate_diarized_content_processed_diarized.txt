00:00:00.160 - 00:00:34.922, Speaker A: All I'm going to talk about here is the central path in semi depth in optimization, and it's worst case convergence rate. And this is a joint work with Professor Bossu at Purdue. So fortunately we had enough background on semi definite optimization. So I don't need to start over, but at least I can fix the notation here. So we have a pair of primal dual steel problems. I mean, just for short, I refer to it as dope or semi definitely optimization. And so we have a inner product here which is defined in terms of trace.
00:00:34.922 - 00:01:16.152, Speaker A: And we also have one assumption here. So this assumption is not needed for like solving them, but we need them later for deriving the bound. So we assume that all the coefficients are integers. All right, so before moving on, I can also start with an example here. So this is the Vernon STP relaxation of max code problem, just a simple case. If we optimize a linear objective function over this, this turns out to be a cp problem or co problem, which has a unique optimal solution. But the unique optimal solution is not that nice.
00:01:16.152 - 00:02:07.234, Speaker A: So as you see here, I actually put this on purpose because I'm going to come back to it later. The sum of, uh, the primal and dual solution is not positive definite. And as I, as I mentioned later, uh, this has some bad consequences. Uh, all right, so there are, I mean, we can just from scratch, we can have some conditions or assumption on the problem just to avoid trivialities. We don't want to have weakly infeasible or infeasible instances. So we assume that we have slater condition, or sometimes called interior point condition, which means there exists a feasible solution which belongs to the interior of the positive semi definite. And we also assume that all the constraint, all these matrices here are linearly independent.
00:02:07.234 - 00:02:55.524, Speaker A: And if we have these two conditions, it's quite well known in the literature that both primal and dual optimal values are attained. The optimal sets is non empty and compact, and every primal dual optimal solution can be represented by this polynomial system plus positive semi definiteness constraint. So everything is, I think, is working nicely. And of course, as you see from the equations, it's quite obvious that the optimal set is a basic semi algebraic set. So speaking of basic semi algebraic set, we have, I mean, two options to solve this problem. One obvious option is to solve it algebraically. We already have quantifier elimination.
00:02:55.524 - 00:03:35.094, Speaker A: So if you represent everything in terms of equations and, or in terms of a first order formula, that can be solved using quantifier elimination. But I mean, obviously we have exponential number of polynomials of degree n so that's doesn't seem to work. On the other hand, the other well known option is the interior point methods. And the interior point methods work on the basis of central path. So I come back to central path in the next section. But the idea is pretty simple. So we simply have the formation of the optimality condition, which I introduced before.
00:03:35.094 - 00:04:14.022, Speaker A: So we simply have a perturbation of the optimality conditions. And this is actually an equivalent form of the central pipe. So there are other forms of the central part here. I'm going to consider these four, and basically the interior point method. What the interior point method does here is to linearize this system while taking the parameter, the deformation parameter zero. So it turns out, I mean, it turns out that as mu goes to zero, we also have a solution. So so far, the best iteration complexity for an approximate solution, which of course they don't give the exact solution.
00:04:14.022 - 00:05:16.102, Speaker A: But if you want to get an approximate solution, the best iteration complexities for shorter step mutual methods, which is this. All right, if I come back to the central path now, I can, for this simple example, I can write down the central PI equation and the solution explicitly. So I simply have the primal central solution here, and the x twelve is actually a root of this polynomial. So if we basically solve for mu, or if we write t in terms of mu using its puso expansion, we can also have a bound or some sort of cemention on the convergence. And as you see here. So remember, we don't have a nice property at the optimal solution. And as you see, the central path here is a bit curvy here.
00:05:16.102 - 00:05:33.324, Speaker A: So it tangentially converge the optimal solution. But later we see an example where the central path is just a direct line. Okay, any questions so far before I move on to the central path and only talk about central path?
00:05:34.024 - 00:05:47.976, Speaker B: So ali, just a quick question. So, in the elliptope case, you said it's converging tangentially, but in the elliptop case you have everything is nice, everything is Slater, and the singularity degree is small, right?
00:05:48.160 - 00:05:59.948, Speaker A: Yeah, but not in terms of the, I mean, as I mentioned before. Yeah, everything looks nice, but the optimal solution is not as strictly. So later I define it, it's not as truly complementary. So.
00:06:00.036 - 00:06:01.024, Speaker B: Okay, I see.
00:06:06.044 - 00:06:07.224, Speaker A: Any other question?
00:06:07.524 - 00:06:21.278, Speaker C: Yeah, in your example, strict complementarity failed. Can you see that from this picture from, because your curve goes to the, to the vertex, right, right.
00:06:21.366 - 00:06:24.834, Speaker A: But this tree complementary already fails here at this solution.
00:06:25.174 - 00:06:33.334, Speaker C: So this is because, is there any relationship with the vertex because you've converged to a vertex.
00:06:33.454 - 00:06:44.004, Speaker A: Not necessarily. You might have, you might have a positive dimensional optimal set. And again, these three complementaries might fail if, if that's what you are asking.
00:06:44.304 - 00:06:45.216, Speaker C: Okay.
00:06:45.400 - 00:07:17.678, Speaker A: Okay. We can discuss it later. I mean, if you still have a question, because we have more to say about the failure of history. Complementary. All right, so let's talk about central path in terms of real algebraic geometry. So, we already know that the central path, the central solutions. Uh, well, in, in other words, central path is a smooth path, but it's also semi algebraic because we can describe the central path in terms of this equation.
00:07:17.678 - 00:08:07.700, Speaker A: So it's called the semi algebraic path. And what is already known about the central path here, that, uh, well, every center of solution is non singular. So if you take the Jacubian of the equation here, it's going to be non sing, uh, it's going to be non singular for every positive mu and, uh, well, for every mu. This solution is called a central solution. And the other nice property here that we have some kind of uniform bondedness, uniformly boundedness for central path. So if you just restrict the central path in an interval, the solution is, we can find above, we circumscribe all the central solutions here. And because of that property, we can rely on the result from real algebraic geometry that the central part always converges.
00:08:07.700 - 00:09:05.180, Speaker A: And that's because we have uniform bondedness, it's uniformly bonded. And just relying on the result from real algebraic geometry, which says that this path can be continuously extended to mu zero, which means the limit exists. So there is another proof in the literature based on the curve selection limit. But in this particular case, we don't need curve selection, just, just a simple property of real algebraic geometry. Okay? Those center of path always converges regardless of any regularity condition. But what happens if we have the first nice condition, which I mentioned at the beginning, there's three complementary if your optimal solution, or if the limit point of the central path is satisfying this property, which we call as three complementarity, then first of all, the limit point is proven to be the analytic center of the optimal set. It's always the analytic center of the optimal set.
00:09:05.180 - 00:10:06.254, Speaker A: And we have a lipschitzian bound on the distance from the limit point. So this is happening when the three complementary holes, and as a result, interior point methods which actually approximate the center of that, are also super linearly convergent under this condition. So if you look at this central pad with a different objective, which results in a solution in a limit point which is strictly complementary, as you see, the central pad is not in that case, it's not curvy. I mean, that's not always the case. You might have a little curve here, but at least it's simpler than the central pad in the previous case. And in fact, we have a lucidium bond here. So how about when the street complement fails? First of all, we don't know anything about the limit point because the central path doesn't necessarily converge to the analytics.
00:10:06.254 - 00:11:05.504, Speaker A: So that's first bad consequence. And as a result, interoperability methods are not going to be better than linear. So they are not, I mean, they are not super linearly convergent. And the Lipschitzian bound in previous case, which I mentioned in this theorem, which is Borkovich storm, is not going to be Lipschitz. So, coming back to the example at the very beginning, if we now use the Newton puse theorem, and if we expand the previous equation, we can actually verify that the distance of the central path from the limit point is the square root of is bounded by square root of mu rather than mu. So, instead of the Lipschitzian bound, we have the Holderian bound here. And this is just a nice case when there's three complementary belts, because we might have much worse results.
00:11:05.504 - 00:12:03.812, Speaker A: So, we already have a lower bound in the literature which says that, for instance, for this problem, which is, which depends on n, the distance from the limit point, from the limit point of the central part can be bounded below by mu two to the power minus n. So, on the other hand, we also know something about the distance of, of the central path or central solution from the optimal set, not from the limit point. And we already have this bound for the distance to the optimal set. So, the question is, what is the worst, what is the upper bound on the distance from the limit point? That's something which we don't know about, and we are going to discuss here in this topic. All right, before I move on, is there any question on, uh, minus three complementarity for the consequences?
00:12:03.988 - 00:12:18.772, Speaker B: So, oli, the, the, in the theorem, the distance x mu, sort of be that exponent the two to the. Actually, the next slide. Yeah, yeah, that's the one two to the one minus alpha, right?
00:12:18.948 - 00:12:31.014, Speaker A: Yes, basically, I mean, I should have written minus degree of singularity, but that's because singularity is bounded above by n minus one.
00:12:31.394 - 00:12:33.434, Speaker B: So that's the singularity degree, right?
00:12:33.594 - 00:12:34.010, Speaker A: Exactly.
00:12:34.042 - 00:12:35.010, Speaker B: Okay, got it?
00:12:35.202 - 00:12:35.618, Speaker A: Yes.
00:12:35.666 - 00:12:36.454, Speaker C: Thank you.
00:12:37.994 - 00:12:39.134, Speaker A: Any other question?
00:12:43.634 - 00:13:09.418, Speaker C: Okay, I'll ask just a quick one. Are you, are you going to always have your optimum at a vertex or we're restricting to the max cut problem or just general problems? General problem. Okay. Because for the max cut, having the optimum at a vertex is very important related to the strict commentary, right, right.
00:13:09.466 - 00:13:16.082, Speaker A: But yeah, we're not just talking about Maxwell, we are just talking about any problem, whether or not it's three complements or difference.
00:13:16.258 - 00:13:17.014, Speaker C: All right.
00:13:19.034 - 00:13:59.570, Speaker A: All right. So from this point on, my talk has a little real algebraic geometry flavor. So first of all, just for the sake of simplicity, I'm going to identify symmetric matrices by vectors. So that would be easier. And I'm going to set aside the positive definiteness constraint, because the central solution are always positive definite, so I don't need to enforce them in my analysis here. So I simply have a polynomial system consisting of the aux Alphine constraint from the primal dual and the complementarity. And this is, this operator is a symmetric Karnaki program.
00:13:59.570 - 00:14:44.074, Speaker A: So this is quite well known in the IPM literature. So we have, now we have a polynomial system. We don't have the positive definiteness constraint. So this system, the zero of this system, doesn't, doesn't just have a central path. It might have other solution which is not feasible with respect to the positive definiteness constraint. But the only thing I know here that the central solution is an isolated solution of this algebraic set. So if I have a way to describe the components of this algebraic set, I can also have a description of the central solution, and that's what I'm going to do here.
00:14:44.074 - 00:16:05.986, Speaker A: So the goal here is to describe the central solutions or every point, some points in this algebraic set. And by description I mean another notion from a real algebraic geometry, which is called real univariate representation, and in fact real univariate representation, it turns out that if we have a finite dimensional system, which means that this algebraic set, this is an arbitrary algebraic set, which means this algebraic set is finite, then every real solution of this system can be identified or described by some polynomials, k two polynomials, and assign condition. So in fact, every coordinate of the solution can be represented as a rational function in terms of the real root of a univariate polynomial. And the Tom encoding is again, is another notion. In real algebraic geometry, it's simply a sign condition on the f and the derivatives of derivatives of f. This kind of sign condition identifies every real root of f. So we basically have an identification of real roots of every real solution of this algebraic set.
00:16:05.986 - 00:17:08.524, Speaker A: So that's what I mean by description. So now, coming back to the central path, like I said, if I have a way to describe the components of the central path, the components of this algebraic set, then I also have description or real unified representation of the central solution. But so one way which I can think of is the critical point. So if we assume that this is a smooth manifold, and if you have a certain function, for instance the projection function, then in fact it turns out that the critical points of that function on this manifold are going to meet every semi algebraically connected component of the set. But the problem here is that this set is not necessarily bounded. When we get rid of the positive definiteness constraint, it might become unbounded and it's not necessarily smooth, it's not unsingular. So for the boundedness, I can have a trick here.
00:17:08.524 - 00:17:52.534, Speaker A: If you remember, the central pad is uniformly bounded, and all I care about here is the central solution. I don't care about the other solution of this set. So we actually have a ball here with a certain radius which circumscribe all the central solution in this interval. So that's all I want here. So above, which have all the central solutions in this interval, and then I can also add a dimension to my problem. So what exactly is happening here? We are basically constructing a cylinder based on this algebraic set. So we are going to a higher dimension and then we are intersecting the cylinder with a ball.
00:17:52.534 - 00:18:27.862, Speaker A: So now we have a bounded set. Okay, so this is a bounded set. If the set is also smooth, then you can again just rely on the previous theorem that every, all the critical points meet every semi algebraically connected components. And then you can do the projection, you can simply forget about the last coordinate here. So this is the coordinate I introduced. I can just do something about it, and then I can forget about this coordinate and I come back to my original problem. And that's exactly what this algorithm does here.
00:18:27.862 - 00:19:25.304, Speaker A: This is one of the algorithms developed by Professor Bossu, which basically its internal process is the following. So we already have a bounded set, it deforms the set, after that the set becomes nonsingular. So now we have a manifold, we are trying to find the critical points. The critical points sample every connected component of the set. We do the projection, and then we come back to the original set. So if we do that by applying the algorithm, we end up with a unit real universe representation for n, every solution in the previous set, including the central solution. So for every mu and every coordinate of central solution, we have a real unified representation in terms of the polynomials here.
00:19:25.304 - 00:20:27.964, Speaker A: So that's the first tools which we are going to use in finding the worst case convergence rate and in fact, we not only know about this representation, we also know about the degree of those polynomials. So we already know, we know based on the parameterized bounded algebraic sampling, that the degree of the polynomials in t is this. And in terms of mu, we have two to the power n plus n squared. And so this is not the main consequence of this talk. But at least as a byproduct, we can say something about the duty of the exposure of the central bank, which is bounded above by this number. So two to the m plus n squared. And well, if we, you know, if you want to restrict to generic STL problem, we also have a better bound by whose standard of, so my bound here, our bound here is for the case where we have worst case, we have the worst case result here.
00:20:27.964 - 00:21:21.370, Speaker A: The boundary is for problems where the data are coming from a RSAC or a generic SEO problem. All right, so let's move on, because we are not going to focus on the degree, we are going to focus on worst case commerce rate. So, is there any questions so far before I move on to the limit point of the central. All right, so far I have a representation for the central solution. The other missing part is also the limit point of the central pad. And now based on my previous derivation, I can also say something about the limit point. And just coming back to the optimality conditions here, and thinking of the central pad equation, I can think of the central pad as a deformation of the central path equations.
00:21:21.370 - 00:22:22.104, Speaker A: So if you deform the optimality condition, you end up with the central path equations. And if you want to have deformation, in fact you are doing a field extension. So from the field of real numbers, you are going to the field of algebraic to the cells overall. So under this deformation, if you think of the system as a deformation, then all the central solutions are defined over the field of algebraic crucial studies. And the good news here that the limit point of the central path is belonging to the limit point of those algebraic. So if you find the bounding points in this field and take the limit, then you end up one of the points here is the limit point of the central path. And just speaking of limit, the limit here is just a ring homomorphism from the field of algebraic prosasteries to the real numbers.
00:22:22.104 - 00:23:30.008, Speaker A: And the definition is just substituting every element of mu by zero in your expression here. So my next goal here is to find the limit, the limit of bounded points in this set. Under this field extension, is there any question on the limit point or other representations? All right, so as the last element of this representation, we have the limit point. And in fact the idea of the limit point, what we do here is pretty much similar to the l rule for the limits. So we simply look at the previous representation here. So we are putting mu here in this expression, but there is a risk that the denominator might become zero because we don't know anything about the limit. So just like the l'hopital rule, we are also looking at the multiplicity of the root of this polynomial here.
00:23:30.008 - 00:25:00.808, Speaker A: So the whole expression here simply means that just because you have coefficients in this polynomial in terms of mu, you can simply put replace those mu by zero here, and then you end up with f. So if you take, if you look at the roots of f and also the multiplicity of the root, you can basically come up with the new polynomials with this time describe the limit of the bounded points. And in fact, uh, using this operation, this procedure, we end up with a real univariate representation whose associated points contains a central solution in infinitesimal radius, which means that this actually describes the limit point. So, from our previous development, we also have a description for the limit point of the central path. So we can again describe, using almost the same degree for the polynomials, we can describe the limit point of the central pad, among other bounded points. And again, the good news is that I also have an upper bound on the degree of these polynomials. So if I just put them together based on the description of the central solution and the description of the central and the limit point, now I can estimate these two quantities, which I define as the convergence rate.
00:25:00.808 - 00:26:04.810, Speaker A: So how would I do that? First of all, I need to just remember that this is a distance to the limit point. So for every mu I have a unique, I have a unique solution, which is the distance. So if I somehow define, based on the previous polynomials, if I define these two new polynomials here, for every fixed t one, t two, which are actually the indeterminates from the previous representations, we can realize that the distance to the limit point is the root of these two polynomials. So the distance from x double star is the root of this, this polynomial, and the same thing for s. So, looks like we have a polynomial in mu d, t one and t two. So we can just wrap up everything in terms of first order formulas. So we have two quantified formulas, and so t one and t two are the quantifiers.
00:26:04.810 - 00:26:44.474, Speaker A: So by Tarski Steiner Brick theorem, we can get rid of, we can get rid of t one and t two, and again get an equivalent set. So the realization of these two formulas is equivalent to the realization of the quantifier free formulas. So now if we have the quantifier elimination algorithm, I can, at the end, I end up with the worst case convergence rate for the central bank. And I can describe here how we apply the quantifier elimination. So just here is the idea of the proof. First of all, the realization. So the realization here is basically the set of the solution which satisfy the formula.
00:26:44.474 - 00:27:30.752, Speaker A: So like I said, for every mu, we have a unique d. So the realization contains a unique d for every mu. So mu is the central path parameter. So if we do the quantifier elimination, we get a quantifier free formula which involves a polynomial equation. So involves a polynomial equation, and we also know the degree of that polynomial based on the complexity of quantifier elimination. And on the other hand, just like in the, in the example which I mentioned at the very beginning, this, for this polynomial, I can solve for d in terms of mu. So d is the distance, remember? And mu is the central parameter.
00:27:30.752 - 00:28:26.584, Speaker A: So I can try can solve for d using its puzzle expansion. And as a result, we have d belonging to the field of algebraic puzzle series with coefficient, with complex coefficients. So now you have a puso series, so you know how to find evaluation of the psoriasis. So all we need to do is just refer to the Newton polygon and look at the slopes of the Newton poly. So based on the degree based on the puzzle series, I know that the valuation must be positive because otherwise the central path doesn't converge. And we also know about a lower bound on the slope of the Newton polygons, which is one over the degree of r. And that's how we get this bound on the worst case convergence rate.
00:28:26.584 - 00:29:34.516, Speaker A: All right, so in summary, so again, we actually investigate the worst case converges on the central part in terms of real algebraic geometry. So what I did here is to compute real univariate representation of the central path and its limit point. And as you might notice, basically the degree of the polynomials plays a central role here. So, if you have a way to reduce the complexity of parameterized boundary algebraic sampling, you can have the reduced bound on the worst case convergence of resistance. But so far, this is the best pump on the worst case convergence rate. And so, as a result from the degree of the polynomials, we ended up with this one on the degree of the recipe closure of the central pack, and, and using the quantifier elimination and Newton Poser theorem, he also ended up with another one on the worst case convergence recurve syndrome. All right, so that was a summary of what I did in this work.
00:29:34.516 - 00:29:40.624, Speaker A: If you need more information, you can see our paper on the archive. Thank you very much for your attention.
00:29:44.244 - 00:29:49.344, Speaker D: Thank you. Let's thank the speaker. Are there any questions and comments?
00:30:00.284 - 00:30:17.404, Speaker C: So, ali, can you say anything at all about implementation, worst case? Like what if you had to actually include an algorithm when you're solving?
00:30:17.864 - 00:30:59.176, Speaker A: Well, perhaps I guess I can rephrase your question that way. So far, I mean, we had an upper bound on the Borsgate convergence rate, and we also have a lower bound. But probably one thing we can do here, which we didn't, is to find an example for which this bound is tight. So the best example we had so far for the worst case convergence rate or the best lower bound, is this example here. So, for this example, we have a bound of mu to the two to the power minus n. But the upper bound, which we got is mu to. So we have n squared.
00:30:59.176 - 00:31:12.584, Speaker A: So we are currently working on constructing examples which have borscht converted convergence rate close to that upper bar. Yeah.
00:31:18.844 - 00:31:20.784, Speaker D: Okay. Any further questions?
00:31:23.924 - 00:31:26.864, Speaker C: And did complementary slackness come in?
00:31:27.884 - 00:31:28.612, Speaker A: Sorry.
00:31:28.748 - 00:31:31.504, Speaker C: At the end. I sort of missed that.
00:31:31.964 - 00:31:33.580, Speaker A: Sorry, I didn't understand.
00:31:33.692 - 00:31:35.620, Speaker C: Sorry. District complementarity.
00:31:35.812 - 00:31:46.036, Speaker A: Yeah, we have a strict. I mean, in your. In our polynomial system. Yes, we have. I mean, we have centrality condition, because we are working with the center of the equations.
00:31:46.100 - 00:31:46.316, Speaker C: Yes.
00:31:46.340 - 00:31:48.424, Speaker A: So everything is written in terms of vectors.
00:31:50.364 - 00:31:54.504, Speaker C: So you didn't worry about strict complementarity tailoring?
00:31:56.324 - 00:32:09.760, Speaker A: Strict complementarity holds at the end, because we are taking the limits at that. Right. So, uh. So let. Let me just move forward. So when we take the limit here. So this is the.
00:32:09.760 - 00:32:14.016, Speaker A: This is a strict. This is. Oh, sorry. Did you ask about the complementary or.
00:32:14.080 - 00:32:15.944, Speaker C: I'm sorry, I meant strict commentary.
00:32:16.104 - 00:32:39.352, Speaker A: Well, we don't assume a strict complementary. So, I mean, this bound. This under general condition, whether or not a strict complementary holds. If a three complementary holds, then we have a better one. Right? So that's a lip sheet spawn are proved by stroke. But we didn't. We didn't have any bound in general, in particular, if a three complementary fit.
00:32:39.352 - 00:32:53.524, Speaker A: So for that case, we ended up with this bound for the general case. So the bound is mu one over gamma, where gamma is to the power of n plus n squared.
00:32:53.944 - 00:33:01.294, Speaker C: So does the degree of strict commentarity failing come in at all? I'm sorry, I missed that.
00:33:02.674 - 00:33:08.242, Speaker A: So, I mean, we did not. I mean, we did not assume a strict complementary in our derivations.
00:33:08.378 - 00:33:20.894, Speaker C: I know. But it's strict comment. If strict complementarity fails, it could fail, you know, with one eigenvalue, two or three. That didn't matter.
00:33:21.434 - 00:34:01.852, Speaker A: No, it doesn't matter here. So, I mean, we solely look at the algebraic equation defining the central path equation, right? So we don't care that much whether we have a strict complementary t or any other regularity condition, as long as the central path is well defined. So the only assumption we have here is the existence of the central path, which means all these algebraics. That's r non n for set, of course, for some mu. Yeah, does that, does that answer your question, or.
00:34:01.908 - 00:34:02.864, Speaker C: Yeah, thanks.
00:34:03.524 - 00:34:12.984, Speaker D: So, well, I guess you could ask if does your bound sort of degenerate to that known bound for the other case with the singularity degree in the exponent?
00:34:14.004 - 00:34:37.228, Speaker A: Well, like I said, this is the boundaries. Uh, in the worst case, under general conditions. So we don't, we don't assume a strict complementarity. So if you have a strict complementarity or other nice conditions. So we also have a nice bound here, as I mentioned here. So if you have a strict complementarity, or if the center of path equations are non singular, then you have a very nice bound, right?
00:34:37.396 - 00:34:52.363, Speaker D: You have that bound. But does your bound kind of degenerate to that? So in other words, if you add assumptions. Okay, your bound is very general, I understand, but what I'm saying is.
00:34:54.143 - 00:34:54.431, Speaker A: If.
00:34:54.447 - 00:34:59.295, Speaker D: You add these extra assumptions, does your bound then become these better bounds?
00:34:59.479 - 00:35:05.043, Speaker A: Sorry, what is the extra assumption? What extra assumption?
00:35:06.703 - 00:35:11.684, Speaker D: Such as the conditions such as strict complementarity holds and so forth.
00:35:12.024 - 00:35:32.444, Speaker A: So, no, it doesn't play anything in my derivation. So, yeah, so if, again, if we. I mean, there is a good bound in the literature. Vendors, three complementaries, but my derivation doesn't care about the strict complementary or anything like this, so. Yeah.
00:35:36.284 - 00:35:45.784, Speaker D: Okay, I guess that's the last talk of the session. Are there any further questions?
00:35:47.764 - 00:36:23.336, Speaker C: Just a comment. Your picture for the max cut where you converge to the vertex. I'm not sure you know this. This paper that, not mine. But to get a vertex, if you look at the normal cone at the vertex and take it object generically, take any objective from that normal cone, then strict complementarity fails generically. I don't know if you know this.
00:36:23.480 - 00:36:30.832, Speaker A: Theorem, not as specifically for. Is that just for the max cut or. Because.
00:36:30.928 - 00:36:35.644, Speaker C: Yeah, it's specifically a paper for the max cut. It's by.
00:36:37.304 - 00:36:38.484, Speaker A: Have the title?
00:36:38.864 - 00:36:47.936, Speaker C: Yeah, it's by Levent Tonsel and his student Marcel Silva, strict, competing in Max cut.
00:36:48.000 - 00:36:54.168, Speaker A: Stp oh, yeah, I guess I didn't sit up there. But thanks for.
00:36:54.336 - 00:37:16.524, Speaker C: Yeah, so that's why I sort of brought that up, because your picture was so reminiscent of that paper. It's sort of interesting that you end up converging to the vertex and then you talk about strict commentarity, but generically it will always fail for a vertex. Sort of interesting. Yeah.
00:37:17.624 - 00:37:18.684, Speaker A: Okay, thanks.
00:37:21.244 - 00:37:28.424, Speaker D: So what you call the real univariate representation, is that basically what they call the rational univariate representation?
00:37:29.564 - 00:37:49.278, Speaker A: I mean, if we, if we talk about real solutions, then we have real univariate representation. So when, I mean, when we have real univariate representation, we also need the towing code. So in the literature we have univariate representation and real univariate representation. So when you have real univariate representation, you also need the Tom encodings, which.
00:37:49.326 - 00:38:11.794, Speaker D: Actually encoding of the first real root, I guess. Okay, so usually that t of sigma or something is some kind of linear combination of the variables of the original system, right? And that is a solution to one univariate. And then you sort of plug in.
00:38:12.834 - 00:39:07.054, Speaker A: Well, I mean, you can, you can see the details in, in the well known BPR book, the book by Professor Bossu, Pollock and Roy. So there is, I mean, whole chapter on the derivation of the real universal representation for any finite dimension, any finite dimensional system. So yes, those are when we have the system. When, well, the system is finite dimensional, then the quotient of the ring of polynomials, module, the ideal is our vector space, right? So we have a finite basis for that vector space. So you, using the basis, you can define the characteristic polynomials and based on those characteristic polynomials, you can, and some other notation, you can define every coordinate as a rational function in terms of that.
