00:00:01.080 - 00:00:36.180, Speaker A: So that's the standard example, two dimensional grid. So you see it's periodic. If I just start shifting from here to go here, I will just take the same element. That's what we would think as an idea, but we will try to make it that rigorous in a bit. And of course here, there different type of periods. Over here I have z two, as I said, I have two dimensions. So let's start by saying something that has been told too many times.
00:00:36.180 - 00:01:52.304, Speaker A: This time, what's a framework in general? I have a graph, simply indirect it and I have a placement which realizations we set in RD. So I put the graph on in RD I'm stating that the vertices are countable set and that the degree of the vertices uniformly bounded. The rigidity matrix is indexed on the rows by the number of the edges and on the columns by the dimension times the number of the vertices. So since I'm interested in infinite frameworks, countable frameworks, it will be an infinite matrix. I'm not going to talk about any analysis today, so we'll just think of it algebraically and we see that given a vertex vw, we given an edge vw, I have everything zero apart from the coordinates at v and w. And there I'm putting a vertex vector p which is given by the distance shutter free coordinate of BMW. I guess there are no questions now.
00:01:52.304 - 00:04:05.224, Speaker A: And the infinitesimal flex phase, I will just give this notation, it's just infinitesimal flexes, which are pretty much the vector fields, the functions, if you want that they are on the other joints and takes values in Cd as well. So we have this property, since I have c, this one would be a bilinear form where this one would be complex number. The reason why I can take c here is that since this is real, this, then I can cut it into the real part, into the imaginary part, and from the real part I will take all the calculations that we want from real life, for example. So if I gather all these equations together, I will just want u r flex to be in the kernel of the rigidity matrix. So this was in general framework theory, I just want to go now to periodic framework. So I said periodic framework and I put an n in order to set up the period would be with respect to a isometric group about translations, which would have n n independent generators and I would have a finite set fe, which would be vertices, framework vertices and a finite set of framework edges fe. So I usually call this one, as Steve Power used to call them, motif vertices and motive edges and what is the property that these two sets have? If I take the motif of the vertices and I take all the translations and I take the union over them, I will take all the framework vertices and same for the edges, I take the motif, the small motif, the set of the finite set of the edges, I take all the translations and I take the union of them and I will take all the edge of the framework.
00:04:05.224 - 00:04:50.874, Speaker A: And the last thing is that I want all these unions to be disjoint. So if I take different case, the two sets that I will take will be disjoint. And as I said, this one was a framework. I want to say that this one is in rd in general. So n and d are not the same. Usually we care more about the case where n is equal to d and that's the case where we are talking about crystal frameworks. So any questions till now? Is that clear? We will have an example soon, but I just want to be sure that this is enough.
00:04:50.874 - 00:05:35.644, Speaker A: So for example, what we're going to see, and I want you to remember this one, is that fe, and fe is not a graph. So let's continue with some notation. As you can see, I can index all the vertices and all the edges of the framework with respect to the motif and the translations. So as we said, any vertex can be thought as a translation of a vertex that lies in fv. So this vertex can be thought as the k translation of picappa. So we'll name it as picappa k. Okay, and the same for the edges over here.
00:05:35.644 - 00:06:31.684, Speaker A: Okay, now if I take a vertex in the motif then the orbit of this vertex is just take all the vertices that appear from the translation of that vertex. And same for the edge. Okay, and once again we denote by v zero the set of vertex orbits and by zero the set of edge orbits. I don't think that we will need this one a lot. I just want to write it in order to make some connection with the other theory where we have the finite multigraph and the covering graph afterwards. So let's go to the grid over here. That's what we had.
00:06:31.684 - 00:07:33.134, Speaker A: A motif would be, for example to take fv as we have here, just this black vertex over here, this, this full, full dot which is p 10 zero. It's just the zero zero element of the plane if you want just one point and feed is this vertex e one and this vertex e two. Okay, now that I think about it, I usually write them the other way around, but it doesn't make any difference. So my question is. Can you see why fv fe is not a graph?
00:07:42.634 - 00:07:46.014, Speaker B: Yeah, I guess it's a gain graph or something.
00:07:46.834 - 00:07:55.146, Speaker A: Yeah, yeah. I will make the connection. For the moment it's not a graph because this is not inside the, the vertex set, right?
00:07:55.330 - 00:07:55.954, Speaker B: Yeah.
00:07:56.074 - 00:09:28.374, Speaker A: For this one, as you said, again, graph, what, what I would like to say, and as son said, if I think that p 100 is this one, I could think this one, this, this e one say as a loop that goes back to itself and it has the group element zero, one. And for this one, once again, another loop, this one is indexed by the group element ten, which is the gain graph, the multigraphs that we had before. And this here are the gains that Tony had defined on Monday. And that's the footed picture that we have over here. So you see this one, this point over here is the p 121 because from here I went two steps from that side and one step from that side. And now in order to make more clear how I define the gain we had in the definition that we had in the beginning, nobody said that I need to take such a fine motif as I have here. Maybe someone could take just one vertex here and one edge this one, and the other edge this one.
00:09:28.374 - 00:10:39.594, Speaker A: And still by all the translations, it will take, once again, the full grid over here. But once I have the infinite graph, I can then choose a gany motif and just assume that every element, every edge element that lies in the motif of the edges has one vertex in the motif of the vertices. And as you can see it here by the zero. Okay? So in my choice that I had before, you see, I have the one vertex here and every edge goes through that one vertex that is in the vertex motif. Okay? And this helps me a lot, as we will see with some calculations. But it's also easy now to find the gain. The gain is the k over here, okay? So it's the same as here, you see, I have this element ten, and as I said in the gain graph, this is the gain and 0101, I have the gains this way.
00:10:39.594 - 00:12:35.874, Speaker A: Now for its framework, I will just define p every time as I did before. In general frameworks, for every edge I will just take the distance on its coordinates between the two vertices and just trivial to check that if I'm inside the same orbit then p is always constant. Okay, so now with, with this change, what I wanted to do, and that's why I start with the rigidity and the placement as usual, is that I can think of the vector space, sorry, of the column space or the flex space of the rigidity matrix as the product of these euclidean spaces, right? I'm with the Euclidean metric so I'm thinking about basis or the normal basis. So I can just reorder as I want. And I can think of this one now as a product where k is just the number, runs along the numbers of the motif and kappa is in the group of translations. Let's put them, and similarly for the headspace here so I can think their digitimatrix this way. Okay? And I think they eligitimize this way.
00:12:35.874 - 00:13:53.620, Speaker A: Then infinitesimal flex lies in the domain so I can index it in the same way. And the first order condition will have this formula. Now yeah, I would like to ask if we are happy till now I'm not happy with the notation, but there is no other way to do it. Okay, so remember that kappa goes for the indexes, for the, for the indices inside the finite set of the motif. K and l over here are for the group of translations. Lambda is the same as kappa and so on. And now we can define the matrix valued function, which is a matrix with rows indexed by the finite set of motive edges and columns number of motive vertices times the dimension and the entries are not numbers.
00:13:53.620 - 00:15:10.934, Speaker A: Now are polynomials, are Loran polynomials, which means that as you see here, I can have negative powers. Usually we are interested in the domain to be tn over here. So I could write this one as this as well as a complex conjugate if you like it better. So what happens is that I have p and minus p as I had in the rigidity matrix, but I don't have the infinite amount of the sieves that I had before I did something like this in order to show how the sifts would work in the matrix. What I would have is that I would have just one here because I have zero over here. So this one would be z to the zero, and here I would have z to the minus k, okay? Where the k is actually the gain of the edge. Okay, this of course happens when we have the edge to have v and w to be different elements.
00:15:10.934 - 00:16:14.094, Speaker A: Recall that z could be, could be vcoma zero and v k to have the same, the two verts have the same motive vertex at first. So in this case I will just add these two things together. Okay, so I started without saying any connection. I just said do this. Okay, define it formally. And now once again formally defining finitesimal flex this way. So you have an element, a vector u, that lies in the space, in the column space of the, of the matrix valued function.
00:16:14.094 - 00:17:28.276, Speaker A: No, it's not a matrix value function is the matrix actually for a given value. And define now the vert, the vector in the whole domain of the rigidity matrix by this formula. So u tilde, kappa, comma, kappa. For a constant kappa, I will have the elements of the motif over here, the vectors that lie, that are applied to the motif. And now, depending on the kappa, I will just have an omega kappa power. Okay? And I need to say somewhere that omega over here lies two t to the. Nice.
00:17:28.276 - 00:19:10.186, Speaker A: So by saying t to the n, it means that every coordinate has modulus equal to one. Okay? So for example, as I'm saying here, if I take omega to be in every coordinate equal to one, this means that would be equal to u kappa, which means that it won't change as k changes. So this would be just strictly periodic because I will just move around the same vector along the different motifs. Okay? So in general, for any omega factor periodic flexes, I can take the set of all this infinitesimal flex with the same period, and I can define the space. This would be a finite dimensional space inside the infinitesimal flexes and inside, of course, the domain of the, of the rigidity matrix. Be careful. It's important here that I have defined the same omega, okay? Because, for example, one question that can come up is, what happens if, if I add two different omegas? The question is that this is a subspace, because I'm just adding two subspaces.
00:19:10.186 - 00:20:28.244, Speaker A: But is this a space of omega factor periodic flexes for some omega? Let's just leave it for now and let's continue. And if you are coming to the seminars, in order to make the connections, in general, for the rigidity matrix and the matrix valued function, or transfer function, as I call it there, it takes a lot of work to be done. But here there is a nice trick from power. That's not the first way that he did it. I think that first with o, and they used all the Fourier analysis machinery over there. But as I said, formally, you can just do this. So what we want to see is that if we take any omega factor, periodic flex, as we see here, and.
00:20:28.244 - 00:21:33.486, Speaker A: Sorry, sorry, I think that I ran in front. So let's say this now that I can see, actually the matrix valued function valued at some omega in the restriction of rg in that space. So if you take, if you take a vector, this does not need to be a flex, just a vector that has this formula, and I apply the rigidity matrix on that vector. Okay? So let's see what happens on the edge on the, on the row it's a k. So that vertex over there would be, this is k, this is kappa k. Pick up a comma k p lambda l plus lambda. So what happens over there? I would have the models that would be zero everywhere apart from these two elements.
00:21:33.486 - 00:23:15.794, Speaker A: Here would be p and here would be minus p. So this, this, you can think of it as a bilinear form, as a dot product, and I would have pulled, would be multiplied only with the respective elements of the vector u. And now by the formula over here, I'm just replacing, I will take the omega to the kappa u kappa and omega lambda plus kappa over here of u lambda. I will take out the common factor omega kappa, which is over here. And now if you look at this one, it's nothing else than u multiplied with the matrix valued function evaluated at the conjugate of omega. Okay, that's, that's an easy calculation, okay? Which means that if you say that lies in the kernel of that one, then this implies that this one is also zero, which means that util dies an infinitesimal flex and it's also an omega factor periodic infinitesimal flags. And of course I can do the same the opposite direction.
00:23:15.794 - 00:24:33.804, Speaker A: If this is zero, this is also zero. This cannot be zero. So this is true. So that's why, that's the reason why we define this rigid unit mode spectrum to be the omega that lies in the multidisc, where the kernel is not just a zero element. Okay? Because I know that if I have an element that lies in the null space in the kernel of the matrix function, evaluate that the conjugate of omega, then I can create an omega factor periodic flex of the framework. Any questions? Ok, we have some examples now to see. So the first one is the grid framework as before.
00:24:33.804 - 00:25:15.676, Speaker A: So let's, let's write it the way that I have done the calculations here. So this is my e one, this is my te two. This my fv has only one element and I'm in two dimensions. So this one is pretty much p one x pin one y. So this, this is the y direction, and this is for e one, this is for e two. So for e one, we see that p of epsilon one is one comma zero minus zero comma zero. This comes because this is just the point on two reals where this is placed.
00:25:15.676 - 00:25:50.258, Speaker A: Okay, this is the point where this is placed. This is the .0 comma one. So I'm just subtracting the coordinates. I have one comma zero, and since I have one comma zero, I expect that this would be something nonzero and this one would be the zero part. And here, since I know that all the vertices actually are coming from the first one, I will just have one minus z conjugate. Z conjugate, if, let me write it.
00:25:50.258 - 00:26:43.980, Speaker A: So I have a big z in two reals, that is z comma w. So when it says here z conjugate, it means that this one, for the first coordinate, it will be the z conjugate to the first power. Okay, it would be actually z to the 10 power, which is actually z. Okay. And for the second one, same job, but I have now z in the zero one power, which is actually w. And that's why I take the w conjugate there. So the kernel that we have here for just the strictly periodic flexes, you see, I have one one here, so if I take one one, it could be zero everywhere.
00:26:43.980 - 00:27:42.666, Speaker A: So the image could be just the zero subspace, or the kernel is the whole space. So the framework is okay, we knew already that would be periodically rigid because just one vertex. So whatever I put here, and if I applied periodically, it would come up through a translation. So periodically it means that periods, periodical flexes are trivial. Let's go now to the second example, which is once again the grid. But I changed the motif now. Okay, now I have fv to have two elements, p 10 zero, p 200.
00:27:42.666 - 00:28:25.018, Speaker A: So this one and this one, okay. And I have fe to be e one. Let me check also to see that I have correct. So e two, e three and e four. So e one, e two, e three and e four. And you see now for e one, I'm changing the vertex. So this one is p one, x p one y p two, x p two, y.
00:28:25.018 - 00:29:08.814, Speaker A: I am moving only in the vertical direction, so that's why these two are zeros. And now instead of having all the elements in one vertex, I will have one here and minus w over here, minus w conjugate. Okay, these two will be as before. And now we have the minus z and minus one. So what I can get from that one is that I have a kernel that has dimension three. And especially inside the kernel, I have elements that are of the form ten. Once again, recall that I'm talking about the strictly periodic case.
00:29:08.814 - 00:29:54.260, Speaker A: Okay, so I'm taking 10 zero. This lies in the kernel. This one should be m two. Sorry. And this actually means that I have the flex that moves that direction for p one, but stays the same for here. So here is the zero vector here is the 10 vector, which means that when I expand, this will be infinitesimal flex. But it's going to be periodic, but it's not going to be trivial.
00:29:54.260 - 00:30:28.894, Speaker A: So that's why it's periodically flexible. So as we see, the periodically flexible here depends on the choice of the motif. Okay. Before it was the same grid, I chose a different motif, and with that motif it was periodically rigid. And with a second motif it's not periodically rigid. Okay, I would like at some point to ask son if this is an idea of what's a flexible lats and non flexible lats. But yeah, we, let's continue for now.
00:30:28.894 - 00:31:38.154, Speaker A: Let's go to other examples. So now we're taking triangles, okay, someone could take this as a motif, but as I said, I prefer just in order to have easy to find the, the gains, to start with a vertex and put all the vertices, all the edges starting from that vertex. So instead of taking this vertex over here, I will take this vertex, this edge, and I will do my job. Okay, so here's the first time that the translations are not so trivial as before. We are not talking about the standard basis of translation here. So one translation is this one and charge one comma zero, and the other translation goes this direction and gives this vector. Okay, so if we think displacement, we know the values that we have in every point.
00:31:38.154 - 00:32:16.336, Speaker A: The edge set has three elements. The motif vertex set has once again, one. So I know that I would have two columns and three edges. E one, e two, e three. So e one, e two and e three, if you think about it, the same as before. So I'm taking exactly the same example. So what I'm saying is the same as before is, okay, forget about what was happening to the rest here.
00:32:16.336 - 00:33:20.140, Speaker A: Just consider this one and you take the same result here, consider this one and you take once again the same result. But now for this vertex, as you see, the difference between p 10 one and p 100 p, if you want, is a half comma square root of three over two. Again are the coordinates, the coefficients that we have before the Lorant polynomial in each side. So as you can see, I am moving one towards the second coordinate. That's why I'm taking the w conjugate here. And same story for this one. I think that here there is a.
00:33:20.140 - 00:33:49.462, Speaker A: Okay, no, it's not a type. Okay, good. So here p three is minus one two, square root of three over two. So that's why you see here, this is the other way around. It's not one minus w conjugate z. It's the other way around for the minus, and I have one gain for the minus one. So it's going to be z conjugate to the minus one, which is z, and one gain for w.
00:33:49.462 - 00:34:32.414, Speaker A: So that's why I would have a w conjugate as expected, since we have just one vertex. It's periodically rigid with this motif. But if you want, you can try, you can check that. If I take also large motif, it would still be periodically rigid. And let's go to an interesting, another example. I think that this one is more interesting. It has a quite rich vector vertex motif entire, so it has four vertices and nine edges.
00:34:32.414 - 00:35:20.244, Speaker A: And the important thing is that it has a triangle side here. Okay, the translations are again the same, but the thing is that here someone that is doing more combinatorics could see easily that here there's a local flex. I can just take a rotation from the center of this triangle, okay? In order to have this vertex to be orthogonal to that edge, and the same for the rest can happen just through symmetry. And then I can take this one to be zero, zero and zero. And that's a local flex. Everything else could be zero. By saying local means that it's finitely supported.
00:35:20.244 - 00:36:38.214, Speaker A: Okay? So I hope that there would be a way in order to see this one, because maybe here it's easier to see, maybe it's more difficult in other case. So let's see what happens. I have written down the matrix valued function. So because I had this space here, you see that I have p 200, p 300, p 400. So over here I'm not changing the gain, the gain is zero, if you want. So here I would have just constant numbers, okay? So if you check over here, these are the constant numbers that I have, and if I add these two, I will have a six by six matrix that could tell me details about our determinant. And if you check this submatrix over here, this one has zero determinant, which means that the big matrix has zero determinant every time, no matter what's the z and the w.
00:36:38.214 - 00:37:39.574, Speaker A: Right? So this means that the round spectrum for this framework is the whole two dimensional torus. Okay? And as we shall see that this framework has local flexes, we will prove it in a general way that when this happens, we'll have local flexes. Any questions? So let's continue with another example. This is the Kagomi framework. That's the picture that we get. Of course we can expand more. And the motif is given by three vertices and six edges, the translations are once again the same as before.
00:37:39.574 - 00:38:49.038, Speaker A: So we can calculate the kagomi matrix valued function, which is given by this formula. Okay, so this is zero on. So if I think the determinant this is zero when z is equal to one, or w is equal to one, or z is equal to w. So there are three curves where this can happen. So what I want to say is that the kagomi framework looks a lot, maybe not from the first view, but looks a lot with the, with a two dimensional grid. Instead of having this, it seems that there is a nice way to just take lines from here and then take lines from here. And this is also, can be seen in the results that we have from the polynomial.
00:38:49.038 - 00:40:17.924, Speaker A: Recall that for the first framework, for the triangular, for the two dimensional grid, it was just the two factors. And what I want to say is that this can be seen afterwards. I don't have enough time to show it here, but it has some nice theory as the grid, because I can have some band flexes that go only in one direction and everything else is zero. For example. So for this one, for example, take that, say that this is a and this is b. So if you take this one to be one, comma, minus one over square root of three, and this one to be the opposite, this is one, then you can prove that this one would give infinitesimal flex in that line and everything else would be zero. So we finished with that part.
00:40:17.924 - 00:41:03.804, Speaker A: And before we go, I just wanted to say some l two results. Without talking about l two. I have tried, as I said, to remove analysis as much as possible. So I need to start by talking about orthonormal basis. Okay, so take to orthonormal basis. So sigma over here is about the dimension cd, and kappa here is about the motif, and k here is about, let's say that's a crystal framework. So take the translations in zd and similarly for that one.
00:41:03.804 - 00:42:16.080, Speaker A: So I have orthonormal basis in the domain of the rigidity matrix and in the, in the code domain of the rigidity matrix. So if I take a crystal framework, then I, I can define these shift operators, which pretty much says do not change anything else in the northern one basis, but go from k to k plus I. So this is the one, the I. So if I had the grid, for example, and I was here in that point, if I was 10, then it would say go to that point. Okay, that's, that's the translations that we want over here. And similarly, I can define shifts in the code domain of the rigidity matrix. Okay, now you can do the calculations if you want.
00:42:16.080 - 00:43:45.962, Speaker A: I have done them in the seminar in the more general way, so I will just keep it. But I want you to believe me for the moment that the registry matrix is good from the point of view, that if I move because the framework is periodic, it will go once again to the same, it will have the same, it would be the same framework. It would be, the translation is an atom of the framework, right? So the rigidity matrix would just commute with the translations in the respective domain and co domain. Okay, so since it commutes with translations, that's the standard forget analysis argument, which says that whatever commutes with translations is unitarily equivalent with a multiplication function. That, that was first proved in the crystal framework setting by O power in operator algebra theories known for, for a long time. So this fe here is the Fourier transform, that is on the co domain space. So it's zero everywhere else and on the diagonal, equal to the Fourier transform, the scalar, the classical Fourier transform, and same here.
00:43:45.962 - 00:45:02.664, Speaker A: But instead of having e dimension for the square, we have dv dimension in order to be in the domain of RGB, okay? And we say that this one is a multiplication operator by a function phi, that is defined in order to make connection with the finite groups over here on the dual group of z to the d. So the dual group of this one with the standard discrete metric is just TD, the d dimensional torus. And of course, the matrix is this one. And the important thing is that this m phi over here is not something that we don't know. Okay? In case that everything works well over here, this m five is exactly the matrix valued function that we had before. Okay? We are talking about the same phi. So when I said in the beginning, I said without saying anything, I define generally a matrix valued function out of nowhere.
00:45:02.664 - 00:46:43.494, Speaker A: The reason why I did it is that there is some theory at first, and that was how an empower defined it. And then at some point said, okay, we just can play with the polynomials and work with that. Okay? So we have this theory, and now if I think of this one as, as an operator, as a multiplication operator, it's once again analysis. So I will just hide it under the carpet how this is proven. But we know that if a framework is square, summably rigid, then the column rank of the matrix valued function is always maximal, almost always maximum. If you want to say it in a different way, if there exists an f that lies in L two td over here, that is nonzero. Then this means that there exists a function where phi times f is zero, which means that if I take the inverse of the Fourier transfer that I have here, then this f would go to a square summable flex.
00:46:43.494 - 00:46:45.774, Speaker A: Yeah.
00:46:47.314 - 00:47:02.254, Speaker B: Lefty, I don't think the statement two is correct, because I don't think it's the. It's not the column rank, it's the. The kernel is zero. It has nullity zero for almost all Z.
00:47:04.994 - 00:47:05.814, Speaker A: Um.
00:47:09.554 - 00:47:12.570, Speaker B: And it should be an if and only if as well, at the very least.
00:47:12.602 - 00:47:14.642, Speaker A: Yeah, yeah. The if and only if. I agree.
00:47:14.778 - 00:47:15.426, Speaker B: Yeah.
00:47:15.570 - 00:47:19.694, Speaker A: So you say that the kernel of FZ is just.
00:47:20.794 - 00:47:24.334, Speaker B: Yeah. For almost all. Zed, you're not going to get it for all Z.
00:47:24.834 - 00:47:29.426, Speaker A: Almost everywhere. Yeah, yeah, yeah. I agree with you. I agree with you.
00:47:29.570 - 00:47:34.094, Speaker B: Yeah, yeah. I think you have to find, like.
00:47:35.234 - 00:47:38.334, Speaker A: Yeah, because we don't have anything else over there. Right?
00:47:38.754 - 00:47:43.174, Speaker B: Yeah, yeah. It's kind of like you define square summable independence. I think.
00:47:44.474 - 00:47:45.090, Speaker A: Yeah.
00:47:45.202 - 00:47:48.974, Speaker B: A thing. I guess I don't know much about it.
00:47:53.554 - 00:47:59.344, Speaker A: Somehow in my mind, I'm thinking these two things the same, but they are not. Yeah, you're correct.
00:48:13.804 - 00:48:18.144, Speaker B: So I think it might be true if it's got Maxwell counting condition.
00:48:18.564 - 00:48:21.668, Speaker A: Yeah, yeah. That's. That's what I'm thinking. That's what I'm thinking.
00:48:21.796 - 00:48:33.104, Speaker B: I think that's what they assume in the paper when they write it. They assume you've got Maxwell counting condition, but I don't. That's not required, as far as I can tell from the proof. There's, like, workarounds.
00:48:34.924 - 00:48:37.476, Speaker A: It makes the calculation easier to have this.
00:48:37.540 - 00:48:41.556, Speaker B: It does. The matrix is square, isn't it? So it just makes it a million.
00:48:41.620 - 00:48:43.404, Speaker A: Yeah. You can take determinants easily.
00:48:43.564 - 00:48:44.396, Speaker B: Yeah.
00:48:44.580 - 00:48:49.744, Speaker A: But I think that this can be, you know, can be fixed by taking a nice determinant somewhere.
00:48:50.094 - 00:49:14.274, Speaker B: So I think the version I thought was, you take, instead of looking at the determinant of Phi Z, you look at the determinant of Phi z transpose times Phi Z, and then you've got a square matrix, and that will have nullity zero if and only if Phi said, has nullity zero. I think that was the. I convinced myself that was true. The very least.
00:49:16.734 - 00:49:18.294, Speaker A: I'm not sure about that.
00:49:18.454 - 00:49:19.086, Speaker B: Yeah.
00:49:19.230 - 00:49:22.794, Speaker A: I think that you maybe drop some. Some dimensions this way.
00:49:25.014 - 00:49:26.554, Speaker B: So maybe it was something else.
00:49:27.374 - 00:49:28.094, Speaker A: Anyway.
00:49:28.214 - 00:49:29.714, Speaker B: Yeah. Anyway, sorry.
00:49:30.294 - 00:50:17.724, Speaker A: Let's continue now because we are close to the end. So I will show here. So, Maxwell counting equilibrium, as we were talking about, is that the number of the edges in the motif is equal to the number of the vertices of the motif times d. In order to have a square matrix for the Madrid valued function, and I don't believe that this is crucial, it just makes the calculations easier. And then these two are equivalent. If c has a non zero local infinitesimal flex, the same as it has a square summable infinitesimal flex. So from one to two, it's trivial.
00:50:17.724 - 00:50:52.324, Speaker A: It's obvious, actually. And we need to go from two to one. So start with f to b square sum aboline, the kernel of f. As we said before, this means, since phi now is, as we said, a square matrix, I can talk about determinant. So for every point that is in the support of f, we know that there is a vector that has zero. There is a point that is in the kernel. So the determinant should be zero over there.
00:50:52.324 - 00:52:29.614, Speaker A: But the determinant alone is trigonometric polynomial, that is zero in a positive measure space, which means that it's the zero polynomial. Okay, and now it comes some linear algebra magic that we used to do as undergraduates. I have not done the calculations the last year at least, but we can define phi tilde this way in order to take use of the cofactor matrices and get this product to be equal to the determinant of phi times the identity matrix, which is the zero matrix, since the determinant is zero, as we said. Okay, so we have phi times phi tilde. And the important thing here is that the entries are polynomials. Okay? And this means that if I tilde is nonzero, then I can take, there exists a nonzero column over there, say f, and I have phi of z times f of z is zero, where this one is a polynomial. Okay? So if I take the inverse Fourier function of the polynomial, this would give me a local flex.
00:52:29.614 - 00:53:21.224, Speaker A: Now, if I'm unlucky and phi tilde is zero, then I will just take advantage of the minimal polynomial. The minimal polynomial, as we did in linear algebra, it was defined as the smaller polynomials. As that q is equal to zero, we know that there exists one, because the characteristic polynomial gives zero. So since the determinant of phi is zero, this means that q of zero is zero. The reason is that if I take large powers, q will go to be a multiple of the characteristic polynomial. And we're working integral domain. So q of zero is zero.
00:53:21.224 - 00:54:12.094, Speaker A: So this means that q of f q can be separated in a product of two elements. The one would be phi and the other one would be q. One of I now q one of I is not zero, because if it was zero, then this would be smaller in degree from q. But q is the minimal polynomial with that property. So q one of I is different from zero. So instead of using the phi tilde, now, since I have this property, this property says that q, which is zero, is equal to phi times q one of phi. So I can think of q one of phi instead of the phi tilde over here in order to find my local flex.
00:54:12.094 - 00:54:27.834, Speaker A: And that's it. That's the last thing that I wanted to say. So, if you have any questions. Thanks.
00:54:27.874 - 00:54:58.552, Speaker C: Lef terrace. That was great. Yes. If anyone has any questions, please feel free. So, you had a question left, Harris, about fixed lattice versus variable lattice.
00:54:58.688 - 00:55:46.184, Speaker A: Yeah, yeah. I remember many times that I've tried to find somewhere the definition of fixed lattice and flexible lattice, and I have not seen it anywhere written. So, for me, what I'm thinking when I'm talking about fixed lattice and flexible lattice is that if I'm thinking about the grid, for example, with the fixed latch that comes from this motif, then this one is periodically rigid. But if I think this one has a flexible lattice, I cannot say that it's ultra rigid, let's say. So that's the idea that I have about flexible lattice myself through the flexes. And if, you know, I can define periodical rigidity, no matter the motif.
00:55:47.584 - 00:55:59.924, Speaker B: So I think you can, you can define stuff that's ultra rigid, but not flexible lattice rigid. I think that's doable.
00:56:04.344 - 00:56:07.744, Speaker A: First one, I have one question. Have you seen anywhere the definition written.
00:56:07.784 - 00:56:10.564, Speaker B: Down for flexible lattice?
00:56:11.304 - 00:56:12.124, Speaker A: Yeah.
00:56:15.194 - 00:56:30.974, Speaker B: I feel like Ileana did it at some point. Ileana and Cyprian, I think that they. That's what they usually deal with is flexible lattice rigidity.
00:56:33.794 - 00:56:57.838, Speaker A: I think when they say about something like flexible lattice, they say that you can take arbitrary, large finite graphs, I don't know how to say it, sub frameworks, and it would still remain periodically rigid. I think that's what they say.
00:56:57.966 - 00:57:02.230, Speaker B: No, don't know that. That would be. That's ultra rigid.
00:57:02.302 - 00:57:04.086, Speaker A: That's the ultra rigid part. Right.
00:57:04.270 - 00:57:13.982, Speaker B: So that is covered in. So I can't remember else who's done it. But Lewis has a paper on it.
00:57:14.118 - 00:57:19.630, Speaker A: I think that Lewis has a paper. And talking about the definition goes back to Cyprian and Ilyana about.
00:57:19.702 - 00:57:28.702, Speaker B: I think so, yes. But I can't remember where they put it. I just remember that they definitely wrote something about ultra rigid relaxing.
00:57:28.798 - 00:57:35.410, Speaker C: The ultra rigid is relaxing the size of the unit cell, which is different. Flexible lattice.
00:57:35.602 - 00:57:36.374, Speaker B: Yeah.
00:57:37.634 - 00:57:52.534, Speaker C: So in the fixed versus flexible lattice, you also have the same number of vertices in the orbit in the motif, as you call it, but just the, the flexors are allowed to move the vertices in a way that the lattice changes, changes its shape.
00:57:52.954 - 00:57:53.734, Speaker B: Yeah.
00:57:57.194 - 00:58:03.504, Speaker C: Whereas in the fixed lattice case, such a motion is ruled out in advance, sort of forced away.
00:58:03.844 - 00:58:10.664, Speaker A: So can we create example of a non flexible lattice, an unfixed lattice?
00:58:12.404 - 00:58:13.424, Speaker B: Let me think.
00:58:15.684 - 00:58:21.744, Speaker C: So I think any lattice that you've drawn could be, can be thought of as a flexible one.
00:58:34.824 - 00:58:41.284, Speaker A: You know, I have not seen down the definition clearly in order to say yes or no. I'm sorry.
00:58:48.744 - 00:59:16.744, Speaker C: So I don't remember seeing a formal definition either. What I remember is that in the infinitesimal rigidity, in the variable lattice, the flexible lattice case, you have additional columns in the rigidity matrix which allow additional infinitesimal motions that you don't allow in the fixed lattice case. And in my head, it takes like a square lattice and can change the shape of it as the vertices move around.
00:59:19.264 - 00:59:20.044, Speaker B: Yeah.
00:59:25.384 - 00:59:27.944, Speaker A: Anyway, we don't need to solve it now, but.
00:59:28.064 - 01:00:02.594, Speaker B: Okay, I've got one. It's a really dumb one, though. It's okay, if you take in two reals, you take the, uh. Okay, so this is, this isn't infinitesimal, this is going to be finite in some way. So you take a, like an infinite string, so you just have a vertex connected to the next vertex and it's all in a straight line, if you get what I mean. Yeah. So like that, to infinity.
01:00:02.594 - 01:00:16.154, Speaker B: So this isn't going to be infinitesimal rigidity, this is going to be just continuous motions. So this thing is going to be ultra rigid, not, not infinitesimally ultra rigid, just ultra rigid.
01:00:19.534 - 01:00:23.862, Speaker A: In which dimension are you? Now this is embedded in r1 or in two reals?
01:00:23.878 - 01:00:27.438, Speaker B: In two reals. So you embed it in two reals.
01:00:27.566 - 01:00:28.302, Speaker A: Okay.
01:00:28.438 - 01:01:07.654, Speaker B: So it's not going to be infinitesimally ultra rigid, because every, every part of the lattice that you take is just going to be a straight line with no give. But if you are allowed to flex the lattice, you can just kind of scrunch it up into like a zigzag. So this isn't infinitesimal rigidity. I couldn't think of anything that good off the bat. I think there should be infinitesimal ones as well, shouldn't there? I would have thought.
