00:00:00.280 - 00:01:04.156, Speaker A: Lecture, I gave an introduction to solver functions on metric measure spaces. Today I give another introduction to function metric measure spaces, totally different from the previous one and why I do so well, because in improving the two approaches are equivalent. We are going to learn a lot about analysis in metric measure spaces, and in some sense, each of the two definitions has its own advantages. So we were seeing that the definition of the relaxation of the asymmetry's constant gives basically for free, or almost for free, the gamma limp inequality for the shear energy, which is extremely useful in passing through the limit to first order differential in accordanceymmetric measure spaces. I mean these are stable by measure of vast convergence, whereas the theory that I present today is more, and is more related in some sense to Wasserstein geometry, in some sense relaxing. So on Monday we relaxed on with respect to l two convergence today. So we used the vertical instance interpolation.
00:01:04.156 - 00:01:29.272, Speaker A: Today we discuss about sublet functions in the sense of horizontal interpolation. And the starting observation is the following is the following trigger identity I guess say we are on rd, f is a small function. Well then, well then the modulus of.
00:01:29.288 - 00:01:40.284, Speaker B: The differential is the least continuous g that you can put in here.
00:01:47.764 - 00:01:49.028, Speaker A: This bound, right?
00:01:49.116 - 00:01:52.828, Speaker B: So it is clearly true that, I.
00:01:52.836 - 00:02:43.600, Speaker A: Mean, this inequality holds for any smooth curve, right? And for any g which is greater than modulus for the differential at every point. And it is quite obvious that modulus of the differential is the least one. If you pick in some point g street is more than this guy in some point, then by continuity should be smaller than enable root. And then if you pick gamma that goes in exactly direction of the gradient of f, you sort of contradict this inequity, right? Okay, now of course this works for, you know, smooth functions and at least t one functions. And as usual, if we want to pass from something that, okay, perhaps, let me mention, so this way we characterize the models of the differential without, in some sense, without really taking derivatives in a variational way. Okay, so we could say modules of the least continuous function such as this.
00:02:43.632 - 00:02:44.244, Speaker B: True.
00:02:45.304 - 00:03:55.424, Speaker A: Now, if we want to go from c one to sample f, I mean, this formulation per se does not work, for instance, because modules of the differential is something that lives only in l two, right, an l two function, you cannot really, you know, assign a value to this integral because typically you're watching the behavioral dysfunction on an equigible set. So there are various ways to in some sense handle this sort of technical issue if you want, but I mean conceptual one as well. So Samuel Galligan proposed one version based on the concept of modulus of a set of curves. I'm going to work with a different concept, which turns out to be equivalent to shamogalygon one which has been introduced by Ambrosio Savare myself. And it amounts at studying not this inequality, but in a sense, this inequality integrated with respect to certain probability measures on the set of continuous curves. So, let me give a definition. So you remember that, you remember that I think I already introduced this space, c zero, one x, right, this space of continuous curves with values in x.
00:03:55.424 - 00:04:04.934, Speaker A: And remember that as soon as x is completely separable, this is a complete and separable space. And remember that et for net et.
00:04:04.974 - 00:04:08.782, Speaker B: Is the evaluation map that takes a.
00:04:08.798 - 00:04:14.286, Speaker A: Curve gamma and returns gamma evaluated at mt. It is clearly a continuous map.
00:04:14.390 - 00:04:18.714, Speaker B: Okay, therefore, Borel, now let me give a definition.
00:04:19.934 - 00:04:28.454, Speaker A: A probability measure on the space of continuous curves is a test plan.
00:04:31.834 - 00:04:32.554, Speaker B: If.
00:04:32.714 - 00:04:43.138, Speaker A: Two things are true. First of all, there exists some constant, positive constants such that et push forward PI is less or equal than constant.
00:04:43.186 - 00:04:49.426, Speaker B: M for every t. And second, it.
00:04:49.450 - 00:04:51.174, Speaker A: Has finite kinetic energy.
00:04:58.814 - 00:05:02.030, Speaker B: Okay, so what does this mean?
00:05:02.062 - 00:05:15.390, Speaker A: Of course, first of all, as a matter of notation, anytime I write integral from, say, t to s of gamma dot squared, what I mean is that if a curve gamma is not absolutely continuous on that interval, that is plus.
00:05:15.422 - 00:05:18.302, Speaker B: Infinity, if it is absolutely continuous, that's.
00:05:18.318 - 00:05:34.494, Speaker A: The interval of the split. Much like when one writes the, you know, delish interval of a function, the object there, I mean, is meaningful whenever the function is sober. And so this condition means that in some sense, PI lives on absolutely continuous curves.
00:05:34.994 - 00:05:38.974, Speaker B: Among other things, it's concentrated onto the continuous curves.
00:05:40.274 - 00:05:59.904, Speaker A: This condition and this peaks only with the distance on our metric measure space. This condition speaks only with the measure. And it tells in some sense that the curves in this, where PI is concentrated, they do not overlap too much. You can read it in the following, in that way.
00:06:01.164 - 00:06:02.024, Speaker B: Okay.
00:06:03.964 - 00:06:45.594, Speaker A: So the concept of suble function will be given in duality with this sort of test plans. So, if you want to speak about, say, w one p functions, here you should speak about integral of speed to the power q. Okay, just I will work with w twelve function, but just for you to know, for p and q for pd friend. And to there you should put, you know, the holder, the holder conjugate. Now let me just mention that there are a couple of operations that they can perform over test plans. And I return with test plans something extremely basic. So if gamma is a set of.
00:06:49.334 - 00:07:06.194, Speaker B: Borel Borrell set of curves, and PI of gamma is positive and PI is test, then I can restrict PI to gamma and rescale.
00:07:06.664 - 00:07:10.964, Speaker A: So multiplied by PI of gamma to the minus one.
00:07:11.384 - 00:07:13.724, Speaker B: This is all surpass plan.
00:07:17.464 - 00:07:43.746, Speaker A: I just restricted PI to a certain set of curves. So why is this the case? Well, of course, rescaling will not affect the finiteness of the integral, of this integral. And if this integral was finite for PI certainly will be finite, you know, for this guy. And again, rescaling will only affect by the scaling factor, this, you know, compression property. And so that is, but if the.
00:07:43.770 - 00:07:47.734, Speaker B: List C, the list C, the list C.
00:07:50.354 - 00:08:01.776, Speaker A: Is called the compression constant of the plan constantly. And sometimes I might denote it by.
00:08:01.920 - 00:08:04.524, Speaker B: Something like comp PI, just.
00:08:06.864 - 00:08:26.452, Speaker A: So that's one thing that I can do. Another thing that I can do to produce new test plants from old ones is to say I pick two times in zero and one, and I consider a map that I will call restra for restriction. This is restor three s. This is.
00:08:26.468 - 00:08:30.980, Speaker B: A map that takes continuous curves and.
00:08:31.012 - 00:08:51.034, Speaker A: Returns continuous curves on zero, one. And the operation that it does takes a curve, gamma, it restricts it to the interval t s and then rescales. So it sends it to the curve, let's say r into gamma of one minus rt, plus rs.
00:08:52.734 - 00:08:59.046, Speaker B: Restricts rescales. Okay, it's trivia that this map is.
00:08:59.070 - 00:09:12.754, Speaker A: Continuous from, from this space to itself. This space is equipped, of course, with the subdistance. If you restrict, the sub distance certainly cannot increase. So this is in fact Walipz and, and therefore it makes sense.
00:09:14.614 - 00:09:15.230, Speaker B: For given.
00:09:15.302 - 00:09:18.346, Speaker A: T'S, it makes sense to consider the.
00:09:18.370 - 00:09:22.414, Speaker B: Push forward of a given press plan PI, right?
00:09:22.914 - 00:09:26.898, Speaker A: And this is test, if PI is.
00:09:26.986 - 00:09:30.094, Speaker B: For any tns, why?
00:09:30.434 - 00:09:33.410, Speaker A: Because, well, the marginals this time, marginals.
00:09:33.442 - 00:09:35.954, Speaker B: Of this plan will be sort of.
00:09:35.994 - 00:09:55.398, Speaker A: The rescaled instead, the margins of the original PI. So, will be bounded by some constant and the energy. So if you just restrict without rescaling, of course the energy decreases when you rescale. You pay, you pay a factor or, you know, the error factor, which is like s minus t or something like that.
00:09:55.526 - 00:10:02.654, Speaker B: Okay, so, so make sense, of course.
00:10:02.694 - 00:10:49.198, Speaker A: So typically, the first time, the first time once is the concept of test plan. It tends to be a little bit scared because one thinks, okay, where does this come from? How can I, you know, get my hands on this object? And I promise you that if not by the end of today, by the end of the next lecture, I will show you how to build many test plants. Okay, so at least you have an idea. But yeah, one way of producing those one way of producing those is an actually, you know, in some sense, a useful one is imagine. Imagine you're on Aldi. So what? How can you build test plans on Aldi? Well, imagine you have, you have a flow from zero one. The cross are different, or remaining for a few weeks, but let's just stick to nuclear.
00:10:49.198 - 00:10:53.078, Speaker A: So, this is a flow map. And suppose that, you know, for every.
00:10:53.126 - 00:10:56.174, Speaker B: X, say, the curve that takes t.
00:10:56.214 - 00:11:35.990, Speaker A: And returns ft of x, is, for instance, Lipschitz. I mean, I'm exaggerating, but. And let's assume that ft push forward, the measure is controlled by some constant times, the Rebeck measure, for everything in zero one. This, for instance, is decay, is if you are taking, if f is the flow of some Lipschitz vector field, you know, Cauchy Lipschitz theory. Lipschitz unbounded. So the curves will be elliptic because the vector fields are bounded. And you will have a bound on how much the mass is getting compressed because of diverge, because of bounds on the divergence of the vector fields.
00:11:35.990 - 00:11:38.354, Speaker A: Well, if this is decay. Nikola. Sorry.
00:11:38.394 - 00:11:39.614, Speaker B: Yes, please.
00:11:40.194 - 00:11:45.534, Speaker A: There is a, like, glare from the window, I think, in the middle of this blackboard.
00:11:46.594 - 00:11:47.374, Speaker B: Yes.
00:11:50.714 - 00:11:52.450, Speaker A: Can we do something? Is there.
00:11:52.602 - 00:11:53.162, Speaker B: I know.
00:11:53.258 - 00:11:57.094, Speaker A: Okay. Not really. Okay, just. I will do like this.
00:11:57.914 - 00:12:00.854, Speaker B: Okay, thanks. Thank you. Sure.
00:12:02.034 - 00:12:03.274, Speaker A: Thanks for pointing it out.
00:12:03.354 - 00:12:05.286, Speaker B: So, what was the thing? Okay.
00:12:05.310 - 00:12:19.118, Speaker A: Yeah. Now, if you take any measure, mu probability measure, on rd, let's say, smaller than some constant time laybeg, then you can define PI as, you know, f.
00:12:19.166 - 00:12:21.674, Speaker B: Dot, to show mu.
00:12:22.054 - 00:12:31.634, Speaker A: So f dot, this is the map that takes a point x and returns this curve that takes t and returns t of x. So this at least is a measure on the space of curves.
00:12:34.044 - 00:12:36.812, Speaker B: Okay, given that these.
00:12:36.868 - 00:12:58.844, Speaker A: Okay, perhaps, let me say uniformly. Now, if the curves of the form t center ft of x are uniform, Lipschitz, the kinetic energy, this kinetic energy integral the speed of any curve with the support of PI, will be bounded by ellipsis constantly. So the kinetic energy will be finite.
00:12:59.984 - 00:13:00.884, Speaker B: Make sense?
00:13:02.024 - 00:13:10.164, Speaker A: And what about the marginal condition? Well, who is et push over PI. Et push over PI is f t push over mu.
00:13:12.064 - 00:13:19.164, Speaker B: Makes sense, right?
00:13:19.864 - 00:13:29.340, Speaker A: But mu is smaller than some constant times the negative. So this is less trigger than constant. Ft push order back. But ft push for the bag was less so equal than some other constant.
00:13:29.372 - 00:13:33.264, Speaker B: So this is whatever, right?
00:13:34.164 - 00:13:51.720, Speaker A: So this is a way of producing test plans. You have an initial distribution and you, each atom of this distribution is sent along some line in such a way that these lines do not overlap too much. And then they have uniformly controlled energy.
00:13:51.872 - 00:13:52.672, Speaker B: Okay?
00:13:52.848 - 00:13:55.872, Speaker A: That's a way of producing this. This, it's not the only one, but.
00:13:55.928 - 00:13:59.416, Speaker B: This a way, right now, now that.
00:13:59.440 - 00:14:00.904, Speaker A: We have the concept of test plan.
00:14:00.984 - 00:14:12.400, Speaker B: By duality, we can build the subway functions. So let's so f. Let me write this way.
00:14:12.432 - 00:14:30.760, Speaker A: So in s zero m. So just a border function on x up to equal defined upper equality almost everywhere. Then we say that f is a sample function. And I will write, actually belongs to the Sobel f class. Let me read it with s two.
00:14:30.912 - 00:14:33.844, Speaker B: X, provided.
00:14:36.544 - 00:14:38.400, Speaker A: There exists a function g.
00:14:38.472 - 00:14:42.504, Speaker B: Non negative g inner two, such that.
00:14:43.644 - 00:14:48.444, Speaker A: The analog of that inequality that I brought above holds, but integrated with respect.
00:14:48.484 - 00:14:49.224, Speaker B: To PI.
00:15:04.284 - 00:15:16.512, Speaker A: Right, for every PI test. So we replaced inequality valid for every curve gamma to inequality integrated for every PI test.
00:15:16.608 - 00:15:17.324, Speaker B: Okay?
00:15:19.064 - 00:15:20.364, Speaker A: Any such g.
00:15:25.904 - 00:15:28.560, Speaker B: Is called weak upper.
00:15:28.592 - 00:16:04.752, Speaker A: Gradient for historical reasons. Okay? Actually, weak upper differentials should, should be in some sense a better notion because this guy acts in duality with speed of curves. So in some sense, it's more of the modulus of a cotangent guy rather than a modulus or a tangent guy. But you know, this nomenclature is, you know, as entering to use. There's no way of changing it. Yeah, that's it.
00:16:04.768 - 00:16:09.164, Speaker B: So I will keep it. Okay. Um.
00:16:17.144 - 00:16:24.136, Speaker A: Okay. Perhaps there is one important thing that.
00:16:24.160 - 00:16:27.904, Speaker B: Is worth to point out immediately, and.
00:16:28.024 - 00:16:29.044, Speaker A: That is that.
00:16:33.384 - 00:16:34.016, Speaker B: Let'S have a look.
00:16:34.040 - 00:16:43.144, Speaker A: To this right hand side. The important thing is that this right hand side is finite under the stated assumptions. Okay, let me put, let me notice.
00:16:43.184 - 00:16:48.528, Speaker B: That the integral of gamma t gamma.
00:16:48.576 - 00:17:06.996, Speaker A: T dot dt d PI. Let me use, let me use, if I use Cauchy's vaux, this is less or equal than the square root of the double integral gamma t dt d.
00:17:07.020 - 00:17:11.276, Speaker B: PI squared times the square root of.
00:17:11.300 - 00:17:15.612, Speaker A: The double integral gamma t dot squared dt d PI.
00:17:15.788 - 00:17:16.504, Speaker B: Right?
00:17:17.904 - 00:17:52.204, Speaker A: Now this is finite because PI has finite kinetic energy. Let's have a look to this guy. So this guy by Fubini can invert the order of integration. But now you see that this is by the properties of push forward this integral over here, this is equal to the integral of g squared d e t. Push over PI.
00:17:54.664 - 00:17:55.096, Speaker B: Right?
00:17:55.160 - 00:18:09.104, Speaker A: Because this is the integral of g squared in gamma t. So that's exactly what happened. But et push over PI is less or equal than a constant that does not depend on t times the integral.
00:18:09.144 - 00:18:11.600, Speaker B: G squared dm, you know, because et.
00:18:11.632 - 00:18:26.242, Speaker A: PI is less than constant and g squared is positive. So all of this is less or equal than, you know, I get the square root of this compression constant if you want. And then I have the l two.
00:18:26.258 - 00:18:31.374, Speaker B: Norm of g. Makes sense.
00:18:32.794 - 00:18:45.564, Speaker A: Okay, so let me state it the same thing in a different way. If I define PI hat to be just the measure PI, cross the back measure on zero one.
00:18:47.784 - 00:18:50.360, Speaker B: What I just said is that the.
00:18:50.392 - 00:18:53.016, Speaker A: Map that takes a function g in.
00:18:53.040 - 00:18:57.176, Speaker B: L two x and returns, let me.
00:18:57.200 - 00:19:00.304, Speaker A: Call it g hat, where g hat.
00:19:00.344 - 00:19:05.424, Speaker B: Of gamma comma t is g of.
00:19:05.464 - 00:19:11.254, Speaker A: Gamma t. This is, you know, in l two, PI hat, this is linear continuous.
00:19:17.354 - 00:19:18.294, Speaker B: Make sense?
00:19:18.834 - 00:19:25.010, Speaker A: I just proved that the ethanol of this guy is bounded by constant ethanol of z.
00:19:25.162 - 00:19:29.174, Speaker B: So that's. Okay.
00:19:30.754 - 00:19:45.362, Speaker A: Why am I pointing this out? Because now let's have a look to the. For a given function in the subwoofer class, let's have a look to the collection of weak upper gradients. It is clearly complex. If I take the convex combination of.
00:19:45.378 - 00:19:47.850, Speaker B: Two weak upper gradients, you know, there's.
00:19:47.922 - 00:19:51.814, Speaker A: Clearly a weak upper gradient. But this is also closed because of this remark.
00:19:54.234 - 00:19:54.906, Speaker B: Why?
00:19:55.050 - 00:20:06.182, Speaker A: Because if I take a sequence gn of weak upper grain that is converging inner two m to a limit g, and I look to the value of these integrals, well, these integrals for gn.
00:20:06.238 - 00:20:10.270, Speaker B: Do converge to the integral for g. Because you won't.
00:20:10.302 - 00:20:16.034, Speaker A: Because. Because the map that takes g and return, this guy is a linear operator on l two.
00:20:17.774 - 00:20:18.238, Speaker B: Okay?
00:20:18.286 - 00:20:19.238, Speaker A: Or if you wish.
00:20:19.366 - 00:20:20.062, Speaker B: Because.
00:20:20.238 - 00:20:33.094, Speaker A: Because this guy is nothing but this color product in L two, PI hat between g hat and the metric speed. And the metric speed is in L two. Because of this assumption.
00:20:34.914 - 00:20:35.570, Speaker B: Makes sense.
00:20:35.642 - 00:20:45.730, Speaker A: There is nothing deep. Okay? The only thing is that we are working on, you know, measures on path spaces, and you have never seen this. Maybe it's a little bit, you know, unusual, but there is nothing conceptually deep.
00:20:45.802 - 00:20:46.534, Speaker B: Okay?
00:20:48.114 - 00:20:52.944, Speaker A: Okay. So convex closed. There exists a unit guy of minimal norm.
00:20:53.114 - 00:21:00.196, Speaker B: Let's give it a name. So let's call it dfw to remember.
00:21:00.260 - 00:21:04.344, Speaker A: That'S the weak approach to minimum upper gradients.
00:21:05.404 - 00:21:06.224, Speaker B: Okay?
00:21:06.924 - 00:21:14.024, Speaker A: We have the r, we have a weak. Of course they will be the same. Okay, this guy is the minimal one, the one with this.
00:21:16.644 - 00:21:17.784, Speaker B: I don't know yet.
00:21:19.584 - 00:21:22.640, Speaker A: Whether. Whether this is minimal in the pointwise sense.
00:21:22.672 - 00:21:25.844, Speaker B: It is, but I have to prove it. Okay.
00:21:27.224 - 00:21:29.244, Speaker A: But there's a guy we know.
00:21:30.184 - 00:21:54.540, Speaker B: All right. Okay. There is problem. Let me do like this. Okay? Perhaps one comment that I can make, you might wonder.
00:21:54.692 - 00:22:36.320, Speaker A: So I've done something different here with respect to what I've done with the trigger energy. So here I really said that the function that I'm, whose sub checking I'm not imposing any integrability condition, just a zero. That's because basically what matters here is this difference is in l one with respect to PI some sense, but the function itself can be as big as I want. Giving the trigger energy approach. I was working with function in l two and was relaxing in l two. And why did I do so? Because I was a little bit lazy. Okay, the same approach that I've done, and I did not want to scare you too much, so I want to pass working the topology of l zero, whatever.
00:22:36.320 - 00:23:07.154, Speaker A: But if you pick what I've done with the two topology and you're working with zero topology for the shield energy, nothing changes. In some sense, it is criminal to that. But even on RD, it's natural to, there's no reason to use the same interior for the function for the gradient. The gradient is one thing, the function is very function. And this is even more true on metric measure spaces, where in general you don't have one career inequality. So you don't know where if the gradient is bounded in some norm, then the function is bound in some norm. So it's more natural to at least at the beginning to give the capital definition.
00:23:07.154 - 00:23:25.946, Speaker A: So in some sense, for the chicken energy approach, I fear that speaking about relaxation in l two could have been seen a little bit too exotic in some sense. So I was aware of it. But here there is no relaxation. Here there is really no point of imposing any integral bt on it. Who cares?
00:23:26.050 - 00:23:27.254, Speaker B: We never intervene.
00:23:31.154 - 00:24:01.092, Speaker A: Now becomes a technical proposition. Technical in some sense, in a, okay, I guess depends on the taste in some sense is technical in a boring sense, or at least in the sense that there is nothing about, nothing about analysis of metric, measure of space. This is really functional analysis. And what is important is not the proof of this statement, but the statement itself.
00:24:01.148 - 00:24:03.980, Speaker B: So I will focus on this and.
00:24:04.012 - 00:24:16.404, Speaker A: Give just some, you know, some detail on the proof, but without getting into. So the following are equivalent. So f is a sober function in the sense that we just described, and.
00:24:16.704 - 00:24:22.084, Speaker B: G is a weak of a grid, correct? Of course.
00:24:23.704 - 00:24:25.248, Speaker A: Now this is equivalent to the following.
00:24:25.296 - 00:24:28.144, Speaker B: Factor for any test plan.
00:24:28.184 - 00:24:56.100, Speaker A: PI. If we look at the curve that takes t and returns f evaluated at time t minus evaluated times zero, this takes value. In l one, PI is absolutely continuous, actually is in, you know, it's an absolutely continuous curve from zero one to.
00:24:56.132 - 00:25:01.494, Speaker B: L1 PI with derivative. And the derivative.
00:25:08.354 - 00:25:14.334, Speaker A: You know, exists, strong derivative. And actually, let me say what exists.
00:25:15.914 - 00:25:19.694, Speaker B: And for almost every t, and for.
00:25:20.194 - 00:25:42.154, Speaker A: Almost every t that exists, the derivative you know, the limit, if you want, as h goes to zero of. You know, the different quotient here would be f composition et plus h minus f composition et divided by h. There exists this limit in l one PI.
00:25:43.774 - 00:25:46.434, Speaker B: And, you know.
00:25:48.454 - 00:25:51.310, Speaker A: This limit in absolute value is bounded from above by gamma.
00:25:51.342 - 00:25:54.050, Speaker B: T gamma t dot PI.
00:25:54.082 - 00:25:57.802, Speaker A: Almost every gamma. We comment on this in a second.
00:25:57.978 - 00:26:10.174, Speaker B: Three. For every PI test we have that for PI, almost every curve gamma.
00:26:11.434 - 00:26:18.882, Speaker A: The function f composition gamma is in w eleven of the interval. This is now a function on the.
00:26:18.898 - 00:26:25.644, Speaker B: Interval, takes t and returns f at gamma t, which is a real number with.
00:26:27.984 - 00:26:33.352, Speaker A: The derivative. Of course, here in this case is the distributional derivative. This is less or even the same thing.
00:26:33.408 - 00:26:37.764, Speaker B: Gamma t almost.
00:26:41.064 - 00:27:01.404, Speaker A: Okay, so there are, there are, and there are, you know, three equivalent versions of this property of being sober. In some sense, one is the one that I gave and is the most integrated one. In two, I differentiated in time. In three, I also differentiated in space.
00:27:01.444 - 00:27:20.630, Speaker B: In some sense, it is easy to see that three implies two. Because if this is true, then you.
00:27:20.662 - 00:28:12.784, Speaker A: Know, the fact that you can bound the distance between this f composition et and f composition es by just basically integrating this and you get absolute continuity. And this bound on the derivative, of course, is related to this. And it is also easy to pass from two to one, because basically we integrate in time. If a function, I mean, function is, you know, in absolute, absolute continuous should be the interval is derivative. And once you integrate this bound on the derivative, you, you fall, you fall into the definition of sober functions. Okay, so the hard implication is the one that goes from one into three or differentiated ideally, how this is done? Well, the basic observation is, well, first of all. So let me see from one to.
00:28:12.824 - 00:28:15.084, Speaker B: Two, what you do is.
00:28:19.584 - 00:28:36.844, Speaker A: Well, so let me think 1 second. Okay, so the first one starts observing the following. Suppose that f is sub lets satisfy this and g is an upper grid.
00:28:37.264 - 00:28:41.136, Speaker B: Okay, now, if PI is test, also.
00:28:41.240 - 00:29:10.548, Speaker A: This restriction, you know, plan is test that I mentioned before. And if you plug a restriction t's before PI in this, in this definition, what you get here is that, you know, this minus this is bounded by the interval from t to s of this quantity. That's what you get if you start from a certain test plan PI and you apply the initial assembly function to the plan rest over PI.
00:29:10.676 - 00:29:16.344, Speaker B: Makes sense. Now, of course, I should change the call it arc. Okay.
00:29:18.204 - 00:29:29.208, Speaker A: So for every tms and for every test plan, a result. Okay, but now I can also say that actually these inequality, I can erase.
00:29:29.256 - 00:29:33.592, Speaker B: This, erase this, erase this, and claim.
00:29:33.648 - 00:29:43.600, Speaker A: That this inequality, not integrated by the integrand, should be true for PI, almost every card gamma. And why this is the case?
00:29:43.752 - 00:29:51.256, Speaker B: Because if not, suppose that this guy is strictly bigger for a certain set.
00:29:51.280 - 00:29:59.344, Speaker A: Of curves of positive measure. Well, now pick PI, restricted PI to that set of curves.
00:29:59.464 - 00:30:00.844, Speaker B: That is also a test plan.
00:30:01.304 - 00:30:05.244, Speaker A: So you integrate now this inequality, and you get the wrong inequality with the wrong sign.
00:30:06.144 - 00:30:07.084, Speaker B: Make sense?
00:30:08.984 - 00:30:13.084, Speaker A: What I said, what I said is that if I have a function.
00:30:14.904 - 00:30:15.504, Speaker B: F.
00:30:15.624 - 00:30:25.234, Speaker A: And I know that this integral of this function is less or equal than the integral of GDP for every gamma.
00:30:25.274 - 00:30:29.650, Speaker B: Borel, you know, Borrell, then f must.
00:30:29.682 - 00:30:32.014, Speaker A: Be less regular than G. PI almost everywhere.
00:30:32.394 - 00:30:35.214, Speaker B: I've just said this, okay?
00:30:35.914 - 00:30:46.594, Speaker A: And I can pass from the integration over the old space of curves to the gration just to a certain set. By this, you know, I take PI and I restrict and normalize it to 27.
00:30:47.294 - 00:30:49.982, Speaker B: Okay, well, now you.
00:30:50.038 - 00:31:13.922, Speaker A: So, so from now on, it's just a technical matter of, you know, understanding and unwrapping what Oda was said. So, basically, I just proved that if Ng is an upper gradient, then this inequality, so f in gamma s minus f in gamma t is less or equal than the integral from t to s of this function. For PI, almost every gamma, and for.
00:31:13.938 - 00:31:17.626, Speaker B: Any tns with the proper quantifiers from.
00:31:17.650 - 00:31:19.294, Speaker A: Here, is just technical work.
00:31:19.674 - 00:31:20.250, Speaker B: Okay?
00:31:20.322 - 00:31:23.042, Speaker A: But I hope it is clear that in such as, I'm going in the.
00:31:23.058 - 00:31:25.402, Speaker B: Direction of, you know, starting from an.
00:31:25.418 - 00:31:31.546, Speaker A: Integrated inequality and getting more pointless one. Okay, let me just warn you about.
00:31:31.610 - 00:31:35.242, Speaker B: One technical issue that is, could be.
00:31:35.258 - 00:31:44.860, Speaker A: Dangerous at the beginning. So here we have a curve with values on a Banach space, l, one.
00:31:45.052 - 00:31:47.580, Speaker B: Which is absolutely continuous in this sense.
00:31:47.612 - 00:31:59.744, Speaker A: In the metric sense, for instance. And I'm concluding, among other things, that this strong limit, you know, the limit of the difference quotients, exists in the binance space for almost everything.
00:32:00.964 - 00:32:01.900, Speaker B: Okay?
00:32:02.092 - 00:32:10.194, Speaker A: Now, of course, rather than having a Banach space with r, we knew that this is, you know, absolutely functions are differentiable almost everywhere.
00:32:10.574 - 00:32:13.234, Speaker B: For cars with Manuk space, this is not always true.
00:32:13.734 - 00:32:25.174, Speaker A: Banach spaces with the property that absolutely continuous curves are differentiable almost everywhere, are called the Banach spaces with the Radon ngodium property. A typical example of Banach space that.
00:32:25.214 - 00:32:30.750, Speaker B: Does not have this properties are one so and so.
00:32:30.822 - 00:32:52.802, Speaker A: The example, the example is the following. Take the curve that takes t and returns the characteristic function of zero t. This is in L1 of zero one. Of course, the distance in L1 between this function and the value at.
00:32:52.818 - 00:33:00.456, Speaker B: Time s is really just s. Minus t for trivial. So this is leeches, right?
00:33:00.650 - 00:33:09.036, Speaker A: And now, but of course, if you try to compute ft plus h minus ft divided by h, what happens is that model you are converting to the.
00:33:09.060 - 00:33:16.544, Speaker B: Delta t, you certainly don't have a strong limit in L1, you see?
00:33:18.204 - 00:33:33.386, Speaker A: So instead, absolute continuity per se is not sufficient to get differentiability in l one. What are allows these argument in this case to work is that not only so absolute continuity in some sense would.
00:33:33.410 - 00:33:37.738, Speaker B: Be knowing that f composition es minus.
00:33:37.786 - 00:33:41.734, Speaker A: F composition et integrated in d PI.
00:33:42.314 - 00:33:44.962, Speaker B: Is less or equal than some, you.
00:33:44.978 - 00:34:09.298, Speaker A: Know, inter for t two s of some l one function. That would be the definition of absolute continuity. Sorry, of the, yes, some of the one function in PI. But here we know something more by this argument that I very quickly outlined over here, we know not only we have a bound on the, on the l one distance, but really have a point wise bound. So these guys, let's trigger from the.
00:34:09.306 - 00:34:16.374, Speaker B: Iterative not really of gamma r gamma r dot br.
00:34:18.093 - 00:34:42.157, Speaker A: And this is true PI almost everywhere. So it's not an inequality between numbers, it's an inequality between functions defined almost everywhere. And this in some sense gives additional regularity and allows us to conclude. So a bound of this kind does not work in this case. If you want, the difference quotients are not uniformly integrable because they convert to the delta. Whereas if you are in a situation.
00:34:42.205 - 00:34:45.224, Speaker B: Like this, the different quotients are uniformly integrable, okay?
00:34:45.264 - 00:34:49.284, Speaker A: Because they convergence as they abounded from above morally by this guy.
00:34:49.584 - 00:34:52.392, Speaker B: Okay, so just a technical warning, the.
00:34:52.408 - 00:35:37.010, Speaker A: Full proof of this is containing the lecture notes that I've wrote with Erico Pascual on lectures on Nosmod differential geometry. I won't comment anymore on this, okay? Because the roof is just, you know, I would lose 2 hours or more just, just, you know, handling technical details that I don't want to do. So the. So I want to focus just on the statement. So if you have questions on the statement, you know, feel free. So, one good thing, one good thing of this, of this statement is that it reduces the verification of a sober property on metric measure spaces.
00:35:37.172 - 00:35:41.550, Speaker B: Verification of sombre properties on r w.
00:35:41.622 - 00:35:44.274, Speaker A: Eleven is a space that we know, right?
00:35:44.894 - 00:35:51.358, Speaker B: So in particular corollary, if g one.
00:35:51.406 - 00:36:22.530, Speaker A: And g two are weak upper gradients, then so is the minimum. Why this is a bit invisible if you start from the original integrated definition, because then you have to take test plan. You would like to look just a test plan where one of the two is more than the other, etcetera. Etcetera, etcetera. But once you have differentiated, you just.
00:36:22.562 - 00:36:25.306, Speaker B: Look at this and you say, well.
00:36:25.330 - 00:36:33.004, Speaker A: Let'S have, let's check whether mean of g one g two is, is a weak upper gradient. Well, I knew, because f is soble.
00:36:33.124 - 00:36:36.564, Speaker B: That this guy, and the derivative of.
00:36:36.604 - 00:36:46.612, Speaker A: This guy is bounded from bohr by g one gamma t gamma t dot. But I also know that this is less than g two gamma t gamma t dot. Therefore, it is less or equal than.
00:36:46.628 - 00:36:52.956, Speaker B: The mean end of the right for three variables.
00:36:52.980 - 00:37:06.524, Speaker A: So, once I've reduced to the real case, things, things are, you know, way, way better. So, in particular, this one is also minimal almost everywhere.
00:37:10.344 - 00:37:11.084, Speaker B: Right?
00:37:13.824 - 00:37:15.484, Speaker A: So, farther, corollary.
00:37:19.024 - 00:37:22.580, Speaker B: Effect is in w.
00:37:22.692 - 00:37:27.464, Speaker A: One, two r, as we defined last time.
00:37:27.884 - 00:37:32.620, Speaker B: Then f is soble in the sense.
00:37:32.652 - 00:37:44.224, Speaker A: That we discussed today with. We have a trivial inequality at the level of upper gradients, this guy is smaller than the relaxed guy.
00:37:45.324 - 00:37:46.544, Speaker B: Almost surely.
00:37:51.664 - 00:37:53.324, Speaker A: Let's see why this is the case.
00:37:54.184 - 00:38:17.724, Speaker B: The starting observation is that if f is Lipschitz, say we bound that support. Actually, let me prove it. Proof. So, if f is a lip sheets.
00:38:17.764 - 00:38:19.944, Speaker A: Function with bounded support.
00:38:22.044 - 00:38:22.784, Speaker B: Then.
00:38:27.244 - 00:38:30.516, Speaker A: A disobed. Let me see, let me write that again.
00:38:30.580 - 00:38:42.560, Speaker B: Leap af is a weak upper gradient. Okay, let's prove this.
00:38:42.752 - 00:38:48.008, Speaker A: So, first of all, lip sync support. So, this function is uniformly bounded by.
00:38:48.016 - 00:38:55.272, Speaker B: The ellipses constant of f, and it has bounded support, so, so bound that.
00:38:55.288 - 00:38:57.724, Speaker A: Sets a finite mass. This function is inert.
00:38:59.704 - 00:39:00.488, Speaker B: Just to start.
00:39:00.576 - 00:39:03.976, Speaker A: Okay, now, in fact, not only is a weaker per unit, but, but what.
00:39:04.000 - 00:39:06.504, Speaker B: Is true is that for everybody, for.
00:39:06.544 - 00:39:11.384, Speaker A: Every car gamma absolutely continuous, we have that, you know, f at gamma, one.
00:39:11.424 - 00:39:14.136, Speaker B: Minus f at gamma, naught is less.
00:39:14.160 - 00:39:17.124, Speaker A: Or equal than the integral from zero to one of lipa.
00:39:26.464 - 00:39:30.048, Speaker B: Why is this the case? Well, first of all, if gamma is.
00:39:30.056 - 00:39:48.384, Speaker A: Absolutely continuous, and I part compose an absolutely continuous function with elliptic one, I get something that is absolutely continuous. So the map that takes t and returns f gamma t is absolutely continuous and its derivative. So, so it's, you know, right.
00:39:50.284 - 00:39:59.050, Speaker B: The derivative, I mean, it's just chain rule, you see? So f f gamma t plus h.
00:39:59.162 - 00:40:20.546, Speaker A: Minus f gamma t divided by h. This is less or equal. Then let me exaggerate. Let me say that this is less or equal than the Lipschitz constant of f restricted to the ball centered in gamma t with radius. The distance between gamma t and gamma.
00:40:20.570 - 00:40:31.366, Speaker B: T plus h times the distance between gamma t plus h and gamma t.
00:40:31.430 - 00:40:42.554, Speaker A: Divided by h. This is for trigger reasons. I just replaced this difference by the ellipses constant. Of f, racist to a ball that contains both base points.
00:40:43.774 - 00:40:48.122, Speaker B: Okay, now you let h go to zero.
00:40:48.278 - 00:40:55.586, Speaker A: This guy goes to the matrix speed for almost every t. The radius of.
00:40:55.610 - 00:40:58.386, Speaker B: This ball goes to zero, right?
00:40:58.450 - 00:41:00.414, Speaker A: So this goes to the accidental vertical.
00:41:02.514 - 00:41:05.614, Speaker B: Right, makes sense.
00:41:08.394 - 00:41:14.684, Speaker A: So the derivative is bounded by this guy for almost every t, but the function is absolutely continuous.
00:41:14.754 - 00:41:20.040, Speaker B: So the bound integral, okay, so for.
00:41:20.152 - 00:41:31.152, Speaker A: F, which is lip synthesis support. That is true. And now, what do I do? Now say that f is subtle.
00:41:31.328 - 00:41:32.244, Speaker B: And let.
00:41:34.264 - 00:41:56.876, Speaker A: Fn be a sequence of lipid functions with bounded support in x converging fn is converging in l two to f. I don't remember if on Monday, when given the definition of chigger energy, I insisted in these functions having bounded support or not, I'm now imposing.
00:41:56.900 - 00:41:59.404, Speaker B: This bounded support because I want this.
00:41:59.444 - 00:42:13.320, Speaker A: Guy to be net two. So, if I forgot to say bounded support last time, there are two ways of handling this. Either you check that all of what I've done last time works with approximation.
00:42:13.352 - 00:42:16.456, Speaker B: With bounded support, or you neglect bounded.
00:42:16.480 - 00:42:47.254, Speaker A: Support here, and you think that the measure is finite, which anyway is something that we, any, every now and then assume just for simplicity, but it's not crucial. Okay, so it's just, just for brevity. So fn goes well, and lip afn goes steelinel two to the minima relaxed slope. Now, these exist because by the assumption that is in w twelve r and what we have proved the last time.
00:42:48.594 - 00:42:52.854, Speaker B: Right? Well, now what happens.
00:42:56.634 - 00:43:01.714, Speaker A: Let'S look at this. So what we know.
00:43:05.014 - 00:43:05.774, Speaker B: Picks a test.
00:43:05.814 - 00:43:20.834, Speaker A: Plan, and we know that this guy fn gamma, one minus gamma is less or equal than lip r.
00:43:22.574 - 00:43:24.674, Speaker B: Of fn, right?
00:43:25.994 - 00:43:36.946, Speaker A: The fact that the asymptote lip sync constant of fn is a weak upper gradient implies that this is true. Now let's send n to plus infinity.
00:43:37.010 - 00:43:38.294, Speaker B: And let's see what happens.
00:43:41.474 - 00:43:49.258, Speaker A: Let's handle the two sides differently. So, lipa of fn is converging in.
00:43:49.306 - 00:43:52.214, Speaker B: L two to the relaxed slope.
00:43:55.174 - 00:44:01.654, Speaker A: You remember that previously we've mentioned that the map that takes an l two.
00:44:01.694 - 00:44:11.914, Speaker B: Function in x and returns this integral is linear and continuous, right?
00:44:13.134 - 00:44:29.214, Speaker A: So if I'm converging in two, this integral, the t is a linear. So this converges to the, you know, the same guy with, with dfr, gamma t gamma t dot.
00:44:29.754 - 00:44:30.694, Speaker B: Make sense?
00:44:33.074 - 00:44:42.562, Speaker A: This is a linear operator acting, you know, on, on l two x. If these guys are converge, this guy is converge as easily.
00:44:42.738 - 00:44:49.232, Speaker B: Okay, now what happens here to the left hand side? Well, if this sequence of function is.
00:44:49.248 - 00:44:57.964, Speaker A: Converging in l two. I can also say, you know, that it is converging also up to subsequences, almost l, almost surely.
00:45:00.824 - 00:45:04.164, Speaker B: For almost every x, right?
00:45:04.824 - 00:45:24.144, Speaker A: Well, but then let's have a look to the measure PI of the set of curves gamma such that fn of gamma one does not converge to f of gamma one.
00:45:30.564 - 00:45:37.024, Speaker B: Well, this is PI, e one free.
00:45:37.064 - 00:45:40.864, Speaker A: Image of this set where fn does.
00:45:40.904 - 00:45:52.536, Speaker B: Not converge to f. Make sense, right? Okay, so this is equal to e.
00:45:52.600 - 00:46:06.860, Speaker A: One, push four PI of this set, but e one PI is absolutely continuous.
00:46:06.892 - 00:46:08.876, Speaker B: With respect to reference measure m. Or.
00:46:08.900 - 00:46:14.904, Speaker A: If you want, let me, so let me put, this is less or equal than constant compression, constant times m of this set.
00:46:19.564 - 00:46:23.412, Speaker B: So this is zero, right?
00:46:23.508 - 00:46:26.256, Speaker A: And for the same reason, if I can put zero here.
00:46:26.340 - 00:46:32.424, Speaker B: No, the same argument. Okay, so what it is that this proves?
00:46:32.544 - 00:46:34.688, Speaker A: This proves that for PI, almost every.
00:46:34.736 - 00:46:40.600, Speaker B: Gamma, this integrand is converging to f.
00:46:40.632 - 00:46:41.936, Speaker A: At gamma, one minus f of gamma.
00:46:41.960 - 00:46:42.604, Speaker B: Naught.
00:46:46.904 - 00:46:48.040, Speaker A: Is non negative.
00:46:48.192 - 00:46:52.564, Speaker B: By fatu, the limp of this makes sense.
00:46:52.644 - 00:46:54.276, Speaker A: So the integral I'm just using for.
00:46:54.300 - 00:47:00.060, Speaker B: Two now, the integral of f gamma one minus f gamma naught d PI.
00:47:00.172 - 00:47:02.064, Speaker A: This is less or equal than the limit.
00:47:12.364 - 00:47:13.104, Speaker B: Right?
00:47:16.684 - 00:47:19.278, Speaker A: So, so I proved that for any.
00:47:19.436 - 00:47:24.122, Speaker B: I test, this integral is less rigor.
00:47:24.138 - 00:47:25.334, Speaker A: Than this integral.
00:47:27.754 - 00:47:32.654, Speaker B: Which proves that the relaxed slope is a weak upper gradient.
00:47:34.434 - 00:47:43.202, Speaker A: Okay, which is the end of the proof, because the, the minimal weak upper gradient we just proved is minimal almost surely.
00:47:43.258 - 00:47:48.086, Speaker B: So here, this bound, okay, so, for.
00:47:48.110 - 00:48:13.634, Speaker A: Instance, for trivial reasons, given that asymptote is constants are upper gradients. And the definition via test plan is done in such a way that you have the natural lower semicontinuity of the energy in some sense implies that the weaker minimum, weaker gain is less or equal than the minima relapse.
00:48:15.004 - 00:48:15.864, Speaker B: Okay.
00:48:18.364 - 00:49:41.184, Speaker A: Let'S do four minutes break and then, and then we will continue. So, I want to list a couple of a few consequences of the things that we have understood concerning weak upper gradients. And basically the same calculus rules that I wrote for relaxed slopes are in place for a minimal weak upper gradient. So in particular, so if fn is convergent to f, and now I can also, you know, say in a zero if you want, or let me say m almost everywhere. And if DFNs are suble and the minimal weak upper gradients are converging even weakly, doesn't matter to some function g, then this implies that f is sublet and the g is a weaker gain. If you want, this is greater than this. You have, I guess, sub additive.
00:49:41.184 - 00:50:35.554, Speaker A: Let me just concentrate on the locality. So that's property one, property two, property three is the chain rule. So you see, I'm rewriting what I brought on Monday. But now for minimal weak upper gradient, and finally, the labyrinth that are sobolev unbounded.
00:50:37.174 - 00:50:37.942, Speaker B: Okay.
00:50:38.078 - 00:50:55.658, Speaker A: The product of sobole functions in general is not sobole, unless I assume so. So how do we prove this? Well, two, three and four, I proved by reducing to the analogous property on w eleven. W one, you know, and if you don't approve it, w one or not.
00:50:55.746 - 00:50:57.614, Speaker B: You know, that's real case.
00:50:58.274 - 00:51:00.374, Speaker A: Now, by this characterization, you get this.
00:51:00.714 - 00:51:02.614, Speaker B: Basically, okay.
00:51:05.354 - 00:51:14.458, Speaker A: This is basically what I just. Is the principle that I just used to pass through the limit from asynw constant to, you know, relax the slopes.
00:51:14.586 - 00:51:15.374, Speaker B: Okay.
00:51:17.494 - 00:51:21.274, Speaker A: Okay, so where these properties over there.
00:51:24.174 - 00:51:26.474, Speaker B: All right. Okay.
00:51:28.334 - 00:51:59.094, Speaker A: So at least, you know, the two theories, relaxed slope and minima, weaker pregnancy, they look very much the same. There is a natural inequality between them. So the weak upper gradients are less or equal than. Than the relaxed slopes. Now, I want to, you know, start the work for proving that they are actually equivalent. And in some sense, one good thing of this work is that all the intermediate steps are interesting on their own. So let me.
00:52:00.914 - 00:52:03.426, Speaker B: So, the first thing that I want.
00:52:03.450 - 00:52:20.154, Speaker A: To prove is that our first superposition principle. Sorry, question. Can you go back? Why is the one to.
00:52:20.574 - 00:52:24.074, Speaker B: Why one one? Yes.
00:52:25.374 - 00:52:52.534, Speaker A: Look, look at this blackboard. So, I know that this inequality is true for every n, you agree? And I want to pass for a fixed test plan. Fixed this plan. And I know that for every n this is true, this inequality, because fn is sober.
00:52:55.354 - 00:52:57.802, Speaker B: You agree? Yes. Okay.
00:52:57.818 - 00:52:59.694, Speaker A: Now I want to pass the limit in this.
00:53:02.134 - 00:53:02.954, Speaker B: All right.
00:53:03.574 - 00:53:30.990, Speaker A: Now, on the left hand side, I have convergence, you know, almost surely of the integral, because I have almost everywhere convergence of the functions. So by far two, the limit for this is greater or equal than the integral for the limit function. On the right hand side, you have the continuity. Now, this position principle, it says a very beautiful statement. I think it tells you, look, take.
00:53:31.062 - 00:53:34.154, Speaker B: A car t of measures.
00:53:35.214 - 00:54:00.404, Speaker A: This leaves in p two, x. And let's say that it is so w two. Absolutely continues. Absolutely continues with, you know, with respect to the distance w two, and actually, actually a bit more. So assume also that the matrix speed, in turn, that the computer with respect to this function is not only in l one, but let me assume that it is in l two of zero.
00:54:00.444 - 00:54:05.264, Speaker B: One, a little bit more of integrability. Then.
00:54:08.564 - 00:54:11.324, Speaker A: There exists a highly non unique.
00:54:11.404 - 00:54:19.616, Speaker B: In general PI probability measure. On zero one such that two things are true.
00:54:19.780 - 00:54:38.724, Speaker A: First of all, whenever I do et PI, I get mu t. And second, the integral of gamma t dot squared d PI of gamma is equal at nu t squared.
00:54:39.184 - 00:54:42.072, Speaker B: Sorry, this is for every t. And.
00:54:42.088 - 00:54:47.944, Speaker A: This is for almost every. Let me comment on this.
00:54:48.764 - 00:54:49.744, Speaker B: Let me comment.
00:54:50.684 - 00:55:15.330, Speaker A: This is nothing to do with oblique functions. Actually, it has also nothing to do with metric measure spaces. This is a metric statement. You have a metric space x completed, separable. Then this theorem is true. So, in this form, the theorem has been proved by Resini. In his first version on Rd, it was proved by myself, Ambrose and Savare in the.
00:55:15.330 - 00:55:23.346, Speaker A: This was an intermediate version proved by Lotte and Villany, I guess for geodesics, if I got to remember. But this statement is due, due to Licini.
00:55:23.450 - 00:55:24.258, Speaker B: Now let me comment.
00:55:24.346 - 00:55:36.174, Speaker A: So, say that you have an arbitrary probability measure on the cars on x. To such a measure, I can associate the curve of marginals.
00:55:40.654 - 00:55:44.526, Speaker B: This I can do, and certainly I.
00:55:44.550 - 00:55:47.574, Speaker A: Can bound the distance. Actually, let me say the distance squared.
00:55:47.614 - 00:55:50.454, Speaker B: Between mu s and mu t by.
00:55:50.494 - 00:55:56.394, Speaker A: You know, for trivial reasons, this is less trigger than the integral, the distance squared gamma s gamma td PI.
00:56:00.174 - 00:56:00.750, Speaker B: Right?
00:56:00.862 - 00:56:06.714, Speaker A: Makes sense if you want, because es comma et PI is admissible for this.
00:56:07.184 - 00:56:09.844, Speaker B: For this measure, you see?
00:56:10.904 - 00:56:15.864, Speaker A: Okay, but now this is less or equal than the integral of the integral.
00:56:15.904 - 00:56:23.284, Speaker B: Of gamma r dot, doctor, from t to s squared d PI.
00:56:25.064 - 00:56:32.084, Speaker A: Let's pretend that the curves PI is concentrated in absolutely continuous curves. This makes sense.
00:56:32.724 - 00:56:33.356, Speaker B: And now.
00:56:33.420 - 00:56:44.500, Speaker A: And now, you know, caution, this is less or equal than s minus t. The double integral from t to s of gamma r dot squared, doctor.
00:56:44.532 - 00:56:46.424, Speaker B: D PI, right?
00:56:48.644 - 00:57:04.934, Speaker A: So, so if you divide by s minus t squared, what you see is that this inequality in some sense is always true. You see, after you divide, and then you let s go to t, right?
00:57:04.974 - 00:57:08.502, Speaker B: This guy will convert the squared speed, right?
00:57:08.598 - 00:57:26.622, Speaker A: And this guy will convert. So once this drops, then you take the average integral, this will convert to the integral of the speed squared. So this inequality is always true in terms, providing, if you want, we interpret this left hand side to be plus infinity. If, if PI is not concentrated on.
00:57:26.638 - 00:57:32.578, Speaker B: Absolutely continuous curves, which is a safe, safe assumption.
00:57:32.626 - 00:57:35.666, Speaker A: Okay, so this reposition principle tells that.
00:57:35.690 - 00:57:38.490, Speaker B: You can do a smart choice and.
00:57:38.602 - 00:57:51.850, Speaker A: Be sure that in lifting a curve of measures from below to a probability measure on the space of curves, you don't increase the kinetic energy. Okay, what this is telling, what this is telling. Let me do a picture.
00:57:51.882 - 00:58:01.314, Speaker B: Maybe if mu t is the given curve in some sense, all you know.
00:58:01.814 - 00:58:06.554, Speaker A: All you have is a distribution of mass on your space that it is moving.
00:58:07.014 - 00:58:08.194, Speaker B: That's all you have.
00:58:08.734 - 00:58:24.244, Speaker A: Okay, this superposition principle tells, look, you can think the distribution of mass as, in fact, a set of, as a measure on the space of curve. You can identify every atom, or mu, zero, almost every atom if you want.
00:58:24.284 - 00:58:26.944, Speaker B: How it is moving, okay?
00:58:27.604 - 00:59:00.004, Speaker A: And in general, there are many ways of doing so. For instance, imagine that your distributional mass is always on the unidisc. Then you can imagine for every t. Then you can imagine, okay, this is neutrino mass of, you know, Constantin Constant curves. Or you can say, well, you know, every atom is, you know, moving around and doing cycles. In both cases, the marginal at any time, you know, is the back. But in the second case, you are wasting kinetic energy.
00:59:00.004 - 00:59:10.356, Speaker A: Why are you moving the atoms if in the end, the whole mass is not moving? So in some sense, going around would be an example of having this left.
00:59:10.380 - 00:59:14.012, Speaker B: Hand side strictly bigger while standing still.
00:59:14.068 - 00:59:19.404, Speaker A: Would be the example. One of the, you know, plan that could have been given by this level.
00:59:19.744 - 00:59:28.632, Speaker B: Make sense what I'm saying? Okay, let's put this number. The proof is beautifully easy, I would.
00:59:28.648 - 00:59:33.644, Speaker A: Say, because there is one thing that you can try and that works. So, you know, that's.
00:59:38.224 - 00:59:39.724, Speaker B: So, let me assume proof.
00:59:43.044 - 00:59:45.104, Speaker A: Let me assume I will cheat.
00:59:45.484 - 00:59:47.636, Speaker B: I assume I will not cheat.
00:59:47.660 - 01:00:08.916, Speaker A: But, you know, let me see for the moment that not only x is geodesic, and also, and also that there exists a map, let me call it G's geodesic selection that takes, you know, point a couple of points and returns.
01:00:08.940 - 01:00:09.624, Speaker B: Curves.
01:00:12.244 - 01:00:17.024, Speaker A: So that, so that this Borel actually measurable, let me say.
01:00:20.924 - 01:00:21.588, Speaker B: Uniform.
01:00:21.636 - 01:00:33.036, Speaker A: Okay, and, and, and GS, Xy is a judisc from x to.
01:00:33.060 - 01:00:37.048, Speaker B: Yeah, whatever.
01:00:37.096 - 01:00:37.672, Speaker A: X and y.
01:00:37.768 - 01:00:41.656, Speaker B: Okay. I'm just saying the space of jurisdiction.
01:00:41.680 - 01:00:53.016, Speaker A: And, you know, I have a way of assigning. I pick any couple of points. I pick a juridic. But I want to do this in such a way that when I do all the selections together, I get something that respects my sigma algebra.
01:00:53.120 - 01:00:55.564, Speaker B: Okay, let me assume this.
01:00:56.744 - 01:01:05.814, Speaker A: And now what it is that I want to do? Well, first of all, if you start creation to prove, you know, by the argument that I guess erased is sufficient to prove.
01:01:10.234 - 01:01:13.054, Speaker B: To find PI.
01:01:13.434 - 01:01:30.764, Speaker A: So that et PI is mu t for every t, and the total kinetic energy is less or equal than the kinetic energy, the car.
01:01:32.464 - 01:01:33.184, Speaker B: Makes sense because.
01:01:33.224 - 01:01:42.448, Speaker A: The other inequality, at a point wise level is true for almost every t. That's what, that's what I already mentioned, right? If I prove, why, if I find.
01:01:42.496 - 01:01:45.512, Speaker B: One for which the global energy is.
01:01:45.528 - 01:01:47.008, Speaker A: Less or equal than the global energy.
01:01:47.056 - 01:01:51.804, Speaker B: Lambda. Makes sense. Okay.
01:01:57.444 - 01:02:10.556, Speaker A: And I will find this PI by compactness, by approximation. Basically, I will do a polygonal approximation, piecewise linear, if you wish, or piecewise julistic interpolation.
01:02:10.700 - 01:02:11.384, Speaker B: So.
01:02:14.244 - 01:02:24.284, Speaker A: Let me start by defining. So, to define p one, let's pick, say, alpha one, pick an optimal plan between New Zealand, new one.
01:02:26.984 - 01:02:30.360, Speaker B: In the sense of optimal transport that exists.
01:02:30.552 - 01:02:36.000, Speaker A: Now, I can use this plan to find a judicial plan in some sense.
01:02:36.032 - 01:02:39.936, Speaker B: A plan PI, that will not satisfy.
01:02:39.960 - 01:02:42.560, Speaker A: The conclusion, but that will be a first approximation of the conclusion.
01:02:42.592 - 01:02:46.416, Speaker B: Let me define PI one to be.
01:02:46.440 - 01:02:48.986, Speaker A: Just this G's push forward.
01:02:49.090 - 01:02:50.094, Speaker B: Alpha one.
01:02:51.994 - 01:02:53.418, Speaker A: What it is about this plan?
01:02:53.546 - 01:02:59.482, Speaker B: So this plan, first of all, so G's is a measurable map from x.
01:02:59.538 - 01:03:06.494, Speaker A: Squared to the path space. So at least, so this is a measure on x squared. So at least this is a measure on the path space.
01:03:07.794 - 01:03:11.466, Speaker B: Okay, first of all, second of all.
01:03:11.570 - 01:03:26.470, Speaker A: What does this do? So I have, I have mu zero. I have mu one. I'm totally ignoring what the curve does in the middle, but I have an optimal plan from this guy. Maybe, you know, actually this point is sent to this point. This point is sent to this point.
01:03:26.502 - 01:03:31.038, Speaker B: Et cetera, et cetera. What they do is, well, look, every.
01:03:31.086 - 01:03:39.878, Speaker A: Time a certain point sent to a certain point, put a Judisic between, then this jurisic over here and, you know, consider the plan that you're paying.
01:03:39.966 - 01:03:43.286, Speaker B: But what are the properties of this planet?
01:03:43.310 - 01:03:47.318, Speaker A: Well, you can check, you can check that the energy, let me write, let.
01:03:47.326 - 01:03:50.006, Speaker B: Me write, let me write, let me.
01:03:50.030 - 01:04:14.134, Speaker A: Call this guy kinetic energy of the plant, just for brief. So the kinetic energy of this plant, PI one, you can compute, given that it is concentrated on Genesis, and the energy of a Jody sq. Coincides with the distance squared between the endpoints. This guy is equal to the integral distance squared, gamma zero. Gamma 1d PI one.
01:04:18.394 - 01:04:25.042, Speaker B: Okay, but this is, you know, by.
01:04:25.218 - 01:04:35.034, Speaker A: You know, you're looking at the zero and one marginal. So PI one, which is given by alpha one and alpha one is optimal. So this is equal to w two squared, mu zero, mu one.
01:04:38.934 - 01:04:39.794, Speaker B: All right.
01:04:41.094 - 01:05:05.874, Speaker A: That's the first thing that we know. Of course, this is not our plan, because this plan has the correct margin. That's only a times zero and one. By the way, this quantity, by the way, this quantity is less or equal of the kinetic energy. For the same argument we used before, the distance between two points is bounded by the interval of the speed. Cauchy.
01:05:05.994 - 01:05:07.174, Speaker B: So this is true.
01:05:09.354 - 01:05:17.618, Speaker A: Okay, but this plan has nothing in the middle times, has nothing to do with our current. So let's do something. But, so let's pick, you know, alpha.
01:05:17.666 - 01:05:21.054, Speaker B: I don't know how one two, actually.
01:05:21.474 - 01:05:33.364, Speaker A: Alpha two one and alpha two two. So alpha two one. This is an optimal plane between mu zero and mu one half. And this other guy is an optimal plan between mu one half and mu one.
01:05:36.184 - 01:05:37.760, Speaker B: Right? Okay.
01:05:37.792 - 01:05:57.606, Speaker A: Now by gluing, we can find that. We can find a plan, you know, glue, glue them. So we find a plan leaving on x to the power three. Let us, the first margin is mu one, sorry. The first two marginals is optimal between, say, mu zero and mu one half.
01:05:57.790 - 01:05:58.734, Speaker B: So it's optional here.
01:05:58.774 - 01:06:00.374, Speaker A: And then the second two marginals are.
01:06:00.454 - 01:06:01.714, Speaker B: Optimal in the other way.
01:06:03.734 - 01:06:17.942, Speaker A: Use this judistic selection map to build now a plan. PI two. PI two again, in the space of.
01:06:17.958 - 01:06:23.514, Speaker B: Probability measures, and I'm sorry, cars and.
01:06:23.554 - 01:06:41.934, Speaker A: PI two, how it is done by two. Let me describe by words, don't let me write foremost. So, PI two is concentrated on curves that are piecewise judicial. It is Judi six on zero one half. And this judi six on one half one. And, you know, whenever I restrict to the first half of the integer or the second half of the integer, I have something which is optimal.
01:06:43.594 - 01:06:50.194, Speaker B: Okay, you compute the kinetic energy of PI two.
01:06:50.894 - 01:07:30.946, Speaker A: And again, this is piecewise optima zero, piecewise jurisdiction. So you can compute the same sort of computation that is done above can be repeated. And this, and this gives them. And this gives, what does it give? It gives like something like twice w two squared mu zero mu one half plus w two squared mu one half mu one. Okay, which again is less or equal than the interval from zero to one of mu t dot square delta. This two is just for scaling reasons, because there is a square over here.
01:07:30.970 - 01:07:31.534, Speaker B: So.
01:07:35.234 - 01:08:00.654, Speaker A: Okay, now you see where I'm going, right? So now I continue this iteration, and what I do, I build a sequence of plants. So I build PI.
01:08:00.694 - 01:08:03.336, Speaker B: Nice.
01:08:03.350 - 01:08:06.824, Speaker A: On probability measures. On the continuous curves.
01:08:09.204 - 01:08:09.836, Speaker B: And this PI.
01:08:09.860 - 01:08:37.754, Speaker A: N, they all have the following properties. So the total energy is always bounded from above by the kinetic energy of the curve. And et push forward. PI n is equal to beauty. For every t of the form, what is I divided? Say two to the n I from zero to three, you know, the ic sort of fashion.
01:08:40.374 - 01:08:41.234, Speaker B: Okay.
01:08:43.134 - 01:08:46.246, Speaker A: Now suppose, suppose that, you know.
01:08:46.350 - 01:08:47.234, Speaker B: Say that.
01:08:50.494 - 01:09:14.484, Speaker A: PI n weakly converges to some PI with respect to the weak topology on the space of curves. I have no problems in English, right? Because I spent a couple of lectures at the beginning, discussing convergence and weak topology of measures on complete and separable spaces. And this is a complete and separable. So it's not locally compact, it doesn't matter.
01:09:16.104 - 01:09:24.600, Speaker B: Okay, super, say that, then we're done because the kinetic energy functional, because the map was.
01:09:24.632 - 01:09:35.810, Speaker A: Then notice that. So the map that takes gamma and.
01:09:35.842 - 01:09:39.610, Speaker B: Returns the kinetic energy, maybe.
01:09:39.642 - 01:09:46.090, Speaker A: I also gave an exercise concerning this. This map is lower Semicontino. This is intended plus infinity if gamma.
01:09:46.122 - 01:09:51.218, Speaker B: Is not absolutely continuous, and otherwise it's this integral.
01:09:51.346 - 01:09:53.374, Speaker A: Now, this map is lower Semicontinu.
01:09:57.814 - 01:10:02.262, Speaker B: Okay, now kinet, the kinetic energy of.
01:10:02.278 - 01:10:12.638, Speaker A: A plan is the integral of this one. But now we have seen many times that if I have one lower semicontinuous function, and the sequence of measures that.
01:10:12.646 - 01:10:16.646, Speaker B: Is weakly converging, then the integral with.
01:10:16.670 - 01:10:19.234, Speaker A: Respect to the limit measure is less frequent than the limit.
01:10:21.744 - 01:10:22.512, Speaker B: Right?
01:10:22.688 - 01:10:40.764, Speaker A: So we know. So the kinetic energy is lower semicontinuity, the kinetic energy. So as a consequence of the lower semicontinuity of the energy for one cup, it follows the lower semicontinity of the total energy of plants under weak convergence.
01:10:41.824 - 01:10:42.640, Speaker B: Right.
01:10:42.832 - 01:10:46.124, Speaker A: So this inequality passes to the limit.
01:10:47.064 - 01:10:51.400, Speaker B: Okay, and it remains to prove that.
01:10:51.432 - 01:10:53.204, Speaker A: PI has the correct marginals.
01:10:54.424 - 01:10:55.164, Speaker B: Right.
01:10:56.224 - 01:10:56.964, Speaker A: But.
01:10:58.744 - 01:11:01.384, Speaker B: What you have from here passing.
01:11:01.424 - 01:11:30.824, Speaker A: To limiting m, you see that et PI is equal to mu t for every dyadic number t. But moreover, given that the kinetic energy is finite, it also follows that demand that takes t and returns et PI. This is w two continuous. Because this is finite.
01:11:37.304 - 01:11:38.024, Speaker B: We can bound.
01:11:38.064 - 01:11:49.160, Speaker A: The same calculation that we've done before. Now, we can bound the distance between w two over marginal, over t margin, w two of the s marginal via the integral from t two s of the kinetic energy, right?
01:11:49.192 - 01:11:49.764, Speaker B: So.
01:11:53.744 - 01:12:01.364, Speaker A: It'S w two continuous. It agrees with our given carbon, all diadic rationale, then set. Then it must agree on every, on everything.
01:12:02.444 - 01:12:04.812, Speaker B: Okay, so that's basically the end of.
01:12:04.828 - 01:12:16.544, Speaker A: The proof, except that I have to do two things. First of all, I have to prove that there is some weak compactness for dyns, and I have to justify my initial assumption.
01:12:17.804 - 01:12:20.972, Speaker B: Okay, let's have a look to this first.
01:12:21.108 - 01:12:32.278, Speaker A: This is, I think, is a nice first example of a situation where one can pass to the limit or find weak compactness even on the space of.
01:12:32.326 - 01:12:36.230, Speaker B: Curves, over a space that is not locally compact.
01:12:36.422 - 01:12:50.914, Speaker A: This is typically something that might sound a little bit scary, because you say, where do I pick compactness from? And, but actually, you know, that's in fact not that hard. The basic observation is that, notice.
01:12:52.654 - 01:13:02.676, Speaker B: That this set of Et, push forward PI n with t in zero one, not necessarily diabetes.
01:13:02.780 - 01:13:03.904, Speaker A: And then integer.
01:13:04.484 - 01:13:05.492, Speaker B: This is tight.
01:13:05.628 - 01:13:16.412, Speaker A: I guess this is the basic observation. Okay, why is this tight? Well, because basically, if you only take.
01:13:16.508 - 01:13:19.196, Speaker B: A finite number of n's, this is.
01:13:19.220 - 01:13:29.134, Speaker A: Clearly complex, because I'm taking the. So if you want the map that takes t and returns this guy, et.
01:13:29.514 - 01:13:39.186, Speaker B: Push forward PI n. This is continuous, w two continuous for every n. So.
01:13:39.210 - 01:13:45.470, Speaker A: If you fix n and you look at the image of this map, this is w two, compact, because it's the image of an interval.
01:13:45.642 - 01:13:46.246, Speaker B: Okay?
01:13:46.350 - 01:14:11.954, Speaker A: So if you take a finite number, it's still compact. So how can this be not tight? Well, this could be not tight. If you pick, say, a sequence tk, it could be that e, t, k, PI, nk is not converging anywhere. With nk, that goes to plus infinity. With nk that goes to plus infinity and tk, that must go somewhere, because zero one is compact.
01:14:12.914 - 01:14:13.814, Speaker B: Okay?
01:14:14.634 - 01:14:42.254, Speaker A: But then the observation is that, in fact, this guy, as soon as nk goes to, basically the tk goes to, this guy goes to muti again, because, because thanks to the uniform bound on this, you get a uniform continuity estimate on this. So the w two distance between this guy and this guy is uniformly small.
01:14:43.874 - 01:14:44.994, Speaker B: And now you are done.
01:14:45.114 - 01:15:07.408, Speaker A: The marginals must converge, because you are replacing, basically, you are replacing an arbitrary curve with a piecewise geodesic. And it's, okay, you can check with triangle inequality. But perhaps should be intuitively clear that as you define this geodesic, these cars are approximate in uniformly the limit with what I'm saying.
01:15:07.576 - 01:15:11.944, Speaker B: Okay, now if this is tight, I'm done, right?
01:15:12.024 - 01:15:22.952, Speaker A: Because, okay, actually, then I'm done, meaning that I'm done, and I leave to you the rest of the, of the exercise. So, if this is tight, it means.
01:15:23.128 - 01:15:30.524, Speaker B: There exists psi from x into zero.
01:15:30.844 - 01:15:33.424, Speaker A: Plus infinity with compact sublevels.
01:15:40.324 - 01:15:43.744, Speaker B: Um, so that, um.
01:15:44.684 - 01:16:00.824, Speaker A: Uh, how was that? Uh, so that the soup over the mu in this set of the interlocutor, finite. And this was one of the characterization of tightness.
01:16:01.444 - 01:16:12.340, Speaker B: Right. And then the exercise, if you want, is the following. If prove that if psi from x.
01:16:12.372 - 01:16:30.684, Speaker A: To plus to zero plus infinity has compact sublevels, then the function psi hat.
01:16:31.384 - 01:16:34.764, Speaker B: It takes a cap gamma and returns.
01:16:35.584 - 01:16:40.824, Speaker A: The integral from zero to one of psi gamma, t dt plus.
01:16:40.984 - 01:16:45.204, Speaker B: The kinetic energy of gamma has compressed sub levels.
01:16:53.204 - 01:17:17.974, Speaker A: Now, of course, on the space, on the space c zero, one x. So the good news in some sense, is that it's easy to build functions with compulsory sublevels on this space of continuous curves. Once you have one such a function on x.
01:17:19.794 - 01:17:20.694, Speaker B: Okay.
01:17:25.594 - 01:17:34.774, Speaker A: And if you have this, now you conclude, noticing that the integral of psi hat with respect to PI n is uniformly bounded.
01:17:37.754 - 01:17:40.074, Speaker B: To conclude so psi.
01:17:40.114 - 01:17:48.820, Speaker A: Hat at compass levels, if the integrals are uniformly bounded, this means by, basically, this is an if and only if. This means the depends are tight.
01:17:48.972 - 01:17:49.740, Speaker B: Okay?
01:17:49.892 - 01:18:07.904, Speaker A: So that handles half of the program in some sense. Now, I should justify this existence of judicia, because there was no judicia in the assumption of my assumption. Well, there are, in fact, there are two ways of doing this.
01:18:08.234 - 01:18:12.578, Speaker B: Either you recognize that this part of.
01:18:12.626 - 01:18:51.072, Speaker A: Building this PI n by interpolating with piecewise juicy curve was only technical. I mean, a priori could pick the PI n, which is, you know, constant, for a little bit of time, the first, the first time, into the second, a little bit constant, etcetera, and produce a limit curve by approximation with piecewise constant carbon rather than piecewise. In such a CD six, this is doable, a little bit annoying, because you introduce that, I mean, you're approximating a continuous curve with the discontinuous ones. So that's a little bit tricky. You can do it, there's nothing wrong. But, you know, it's technically a bit more challenging. What you can do otherwise is to say, to observe it, use Kuratowski embedded.
01:18:51.128 - 01:18:55.484, Speaker B: So notice, notice, remark Kuratowski.
01:18:55.984 - 01:18:57.724, Speaker A: I mean, of course it's a theorem.
01:18:59.044 - 01:19:06.596, Speaker B: So any, if Xd is complete and.
01:19:06.620 - 01:19:35.524, Speaker A: Separable, actually just a parable, but separable embeds, then there exists an isometric bending on of x on a little elephant. In fact, Kuratowski tells you exactly who it is embedding, and it tells, you know, take x and n.
01:19:37.544 - 01:19:38.256, Speaker B: And now.
01:19:38.360 - 01:19:47.764, Speaker A: You send an arbitrary point x, you send to the sequence distance x xn.
01:19:48.224 - 01:20:01.094, Speaker B: Minus distance x naught, excel if you want.
01:20:01.134 - 01:20:14.074, Speaker A: If x is bounded, if x is bounded, another way of seeing this is the same principle is if x is bounded, send a point x you can send to the distance from x.
01:20:14.934 - 01:20:22.180, Speaker B: This is an L infinity function, you know, not on sequences, but on the.
01:20:22.212 - 01:20:27.024, Speaker A: Space those points, you know, sequences, if you want, index it by elements of x.
01:20:27.964 - 01:20:30.464, Speaker B: And this is an isometry with this image.
01:20:33.644 - 01:20:46.516, Speaker A: And now, but if the space is a parable, there's no need of picking all the points. You just can just pick accountable numbers. And if the space is not bounded, well, the image here is not, you know, could not be bounded. So in, as you normalize by subtracting.
01:20:46.540 - 01:20:53.580, Speaker B: The distance from one chosen point every time the same. And, okay, now what?
01:20:53.612 - 01:20:55.524, Speaker A: The advantage of doing this. Well, there are several.
01:20:55.644 - 01:20:58.588, Speaker B: First of all, you embed any separable.
01:20:58.636 - 01:21:01.724, Speaker A: Space on a Banach space, which typically is way better behavior.
01:21:01.804 - 01:21:03.140, Speaker B: Way better behavior.
01:21:03.332 - 01:21:30.634, Speaker A: So in some applications, this is useful. Of course, the embedding is extremely, you know, non to say, non canonic. You change, you change the sequence, you get a totally different embedding. But still, in some cases, could be useful, in particular, useful in our case, because linear infinity is a judicial space and certainly is a space where I do have this judistic selection map on little infinity. You can just define, you know, ds.
01:21:31.454 - 01:21:34.726, Speaker B: Xy at time t. Okay?
01:21:34.750 - 01:21:38.012, Speaker A: This, I mean, if these are points in l infinity, I can certainly interpolate.
01:21:38.118 - 01:21:39.816, Speaker B: You know, with a segment.
01:21:40.000 - 01:22:02.920, Speaker A: This is a juicy. It depends continuously on x and y. So empathy is border. So I'm almost there. There is this little issue that little infinity is not separable, which, you know, but who cares? So I start with x, I embed a little infinity, and then I take.
01:22:03.032 - 01:22:21.830, Speaker B: The closed convex r take, replace x with the closer complex of its image.
01:22:22.022 - 01:22:59.932, Speaker A: Convex, I really mean, just so not all digital infinity, but just in the sense of a fine spaces, not this sort of thing. And notice that if you start from a separable space, the closed convex cell is still separable, because how do you build the convex? Well, one way of doing this is I start from my points and I pick all the convex combinations of them with, say, rational coefficients. So if my original space was separable, then also this is separable. And then I just close and I don't destroy separately.
01:23:00.028 - 01:23:00.784, Speaker B: Okay?
01:23:02.324 - 01:23:15.084, Speaker A: And so that's. So, you know, so I now do. So I have my regional SpaceX. Well, I do all my computations in this guy, which now is the completely separable with geodesics and with such a.
01:23:15.124 - 01:23:19.156, Speaker B: Borel selection of jurisdictions, I produce a.
01:23:19.180 - 01:23:35.374, Speaker A: Plan PI that lives in this image, but then this image. So the plant priori lives in the, in the closed convex cell. But given that my car's new, we're really living in x and not in the convex side. A posterior PI believes on carcinx and they can pull it back.
01:23:36.274 - 01:23:41.858, Speaker B: You see what I mean? Perhaps. Let me, let me take the opportunity.
01:23:41.906 - 01:23:43.818, Speaker A: To specify one thing.
01:23:43.866 - 01:23:44.098, Speaker B: So.
01:23:44.146 - 01:24:26.366, Speaker A: So, in the node to optimal transport, the dimension of the beginning, I do an argument like this when proving that if the underlying space is geodesic, then the vastness space is jurisdiction. I say that there is always a bouret selection map, which is false, actually, or at least I don't know how to prove it. So, a Bourette selection, I can produce it on judicial space, at least if the space is compact. If it is not compact, I don't know how to prove it, actually. So I was loppy there. One should use a measurable selection. And I think, Luca, that you corrected me in a good way on max static exchange.
01:24:26.366 - 01:24:59.634, Speaker A: So Alman's selection theorem should be the one to be quoted. So, thank you. Actually, when preparing this lecture, at some point, I was thinking, okay, but how do I build this body selection? And I looked online and I find, you know, your comment. So, thanks. And. But then there is also, you know, if you don't want to go into measurable selection theories and sense this trick of embedding everything into little l infinity also does the job. Let me say one last thing.
01:24:59.634 - 01:25:02.686, Speaker A: Let me say one last thing before concluding this lecture.
01:25:02.750 - 01:25:03.874, Speaker B: Let me say one thing.
01:25:05.214 - 01:25:07.234, Speaker A: So at least I introduce.
01:25:07.654 - 01:25:17.770, Speaker B: I introduce one other actor, and the.
01:25:17.802 - 01:25:26.534, Speaker A: Actor is the gradient flow of the Chigger energy in the space l two. And what I need to know.
01:25:32.514 - 01:25:32.922, Speaker B: So.
01:25:32.978 - 01:25:38.094, Speaker A: Say that you have a e, which is a convex and lower semicontinuous.
01:25:39.924 - 01:25:40.904, Speaker B: Convex.
01:25:41.964 - 01:25:54.044, Speaker A: And the lower semicontinues. And then the theorem, the theorem. Then there's a deep theorem that goes.
01:25:54.084 - 01:25:58.156, Speaker B: Back to works of Brezis Pazzi, is.
01:25:58.180 - 01:26:37.074, Speaker A: That for every point in the closure of the essential domain of the effective domain, there exists a unicarf xd. So, which is continuous on the whole real line, zero one h. I mean, on the half line and absolutely continuous on the open half line with what solves the cauchy problem. In some sense, x at times zero is our given x. And the derivative belongs to the minus.
01:26:37.114 - 01:26:41.574, Speaker B: The sub differential of the x t. For almost everything.
01:26:45.074 - 01:27:18.658, Speaker A: I'm stating a deep result, or at least a non trivial result, concerns in concerning gradient flows of convex functions on Heber spaces. And what I'm pointing out, among other things, is that I can write the gradient flow Question. This is nothing to do with the Georges matrix theorem. So this, you know, you have, you have a convex function in liber space. You have, you have a concept of subdifferential. And you can use this concept of sub differentia to speak about the gradient flow equation that here is interpreted as differential inclusion. You would like to say the derivative.
01:27:18.706 - 01:27:24.210, Speaker B: Of x of the curve x is minus the gradient. Okay.
01:27:24.402 - 01:27:27.654, Speaker A: You can have multiple one, you just say belong to the minus of differential.
01:27:28.324 - 01:27:29.164, Speaker B: Okay.
01:27:29.324 - 01:27:56.254, Speaker A: And in fact, and for these curves, and for these curves, and actually, if I take the. Perhaps what is interesting is that the right derivative x t plus h minus x t divided by h, this converges as h goes to zero from above. So I'm taking the right derivative to the element of minimal norm.
01:27:57.954 - 01:27:59.574, Speaker B: So element.
01:28:01.554 - 01:28:08.914, Speaker A: Of minimal normal in this.
01:28:08.954 - 01:28:14.174, Speaker B: Guy, actually minus this guy for everything.
01:28:20.014 - 01:28:48.694, Speaker A: So by nature, this derivative. And so if you want, by the first part of the theorem, this derivative exists for almost every and belongs to a sub differential up to a sign. Now, a regularity property, if you wish, of this sort of flaws, is that for every t positive, I should say this time derivative, if you look just from the right, exists. So this limit is a strong limit in h. Okay? And it converges to the element of minimal.
01:28:51.074 - 01:28:58.498, Speaker B: Right, um, how you should think about this? Well, think to the case of a.
01:28:58.506 - 01:29:02.106, Speaker A: Convex function, which is that, say, the soup of two affine functions on rd.
01:29:02.170 - 01:29:03.986, Speaker B: Something like this, okay?
01:29:04.130 - 01:29:31.126, Speaker A: Now if you're taking the gradient flow of something like this, and you start, say, from, from this side, what happens is that you, you go, you know, as fast as you can without, you know, seeing the effect of this other thing. Now, at some point, you hit the line where they agree, or the hyperspace, and at that point instantaneously, you start flowing along the edges. So even if the curve is not c one, the right derivative exists at every point.
01:29:31.310 - 01:29:32.086, Speaker B: Okay?
01:29:32.230 - 01:29:39.894, Speaker A: And if you look at the point, you know, when you start, hit this hyperplane where, you know, these two affine.
01:29:39.934 - 01:29:42.526, Speaker B: Functions agree, what you're picking is really.
01:29:42.550 - 01:29:57.114, Speaker A: The element of minimum law. I mean, this, I mean, try to think about this example. Okay? Perhaps this is even more evident. If you take the gradient flow of absolute value, what happens? You start from a point, you just flow to the origin with constant speed. And the moment you read the origin, you stand still.
01:29:57.454 - 01:29:57.846, Speaker B: Okay?
01:29:57.870 - 01:29:59.070, Speaker A: In the origin, you have many sub.
01:29:59.102 - 01:30:01.798, Speaker B: Differentials, but what happens is that you.
01:30:01.846 - 01:30:07.358, Speaker A: Your derivative is the coincides with up to a sign with the one of Inman norm, which is, which is zero.
01:30:07.446 - 01:30:07.726, Speaker B: Okay?
01:30:07.750 - 01:30:14.790, Speaker A: And in fact, you don't move at all. Try to think a little bit about, about this and notice. So, so, I will not prove this result.
01:30:14.942 - 01:30:15.714, Speaker B: Okay?
01:30:16.494 - 01:30:36.530, Speaker A: By the way, the proof, even in this case, can be done by recursive minimization. So you minimize energy plus distance squared divided by two to the distance from the previous point. Let me just notice. And starting on Monday, we should work a couple of properties of this, of this flow.
01:30:36.642 - 01:30:39.866, Speaker B: In particular, I can apply this to.
01:30:39.890 - 01:30:45.454, Speaker A: The gradient flow of the trigger energy in particular. Corollary.
01:30:48.754 - 01:31:06.592, Speaker B: For any metric measure space, for any f inner two, there exists a uni curve ft. You know, when using f two, which is continuous on, you.
01:31:06.608 - 01:31:10.056, Speaker A: Know, even at zero absolutely contributes on.
01:31:10.080 - 01:31:13.004, Speaker B: The open off line with.
01:31:16.304 - 01:31:22.896, Speaker A: F zero equal f and d plus. If you want d plus dt over.
01:31:22.960 - 01:31:33.244, Speaker B: T equal Laplacian over t for everything. Right.
01:31:36.184 - 01:31:44.964, Speaker A: I applied that theorem to the chigger energy, and then notice that by definition, I defined the Laplacian as minus the element of invalid norm. So I'm really restating.
01:31:45.944 - 01:31:46.804, Speaker B: Okay.
01:31:47.824 - 01:31:52.126, Speaker A: And we wrote our first parabolic PDF measure space.
01:31:52.230 - 01:32:00.846, Speaker B: Okay. All right. So starting on Monday, actually, on Monday.
01:32:01.030 - 01:32:28.586, Speaker A: We'Ll see how the study of this heat flow will allows us and a combination, an incredible combination of the property of the heat flow with the vast distance. And this has to do with an argument, a beautiful argument due to Kuvada and to the study of the Hopflux semigroup in metric space. This is another ingredient that we will throw on the table. This will allow us to prove that these two concepts assemble a functions that.
01:32:28.610 - 01:32:31.138, Speaker B: You have just studied agree first thing.
01:32:31.266 - 01:32:33.234, Speaker A: And second thing, it will allow us.
01:32:33.274 - 01:32:35.882, Speaker B: To prove that this guy, or sweet.
01:32:35.898 - 01:33:02.420, Speaker A: Double f, this guy. This evolution coincides with the unique evolution of the. The gradient flow of the relative interval with respect to the vast angiomy on CDK infinity spaces. So these two flows degrading for the cheer energy and the grid for the relative entropy in these very two different geometries, they do actually coincide. And that is huge and will be the starting of the theory of RCD spaces.
01:33:02.572 - 01:33:05.020, Speaker B: Okay, that's all.
01:33:05.212 - 01:33:12.100, Speaker A: Have a nice weekend. So, vitaly, do you still have a couple of minutes? We discussed, we discussed the question.
