00:00:00.120 - 00:00:53.164, Speaker A: All right, thanks. Yeah. So this is ongoing work with Ohlone Cohen, Adam Smith, and Prashant Vasudevan. And I should say it's sort of adjacent to differential privacy, but it'll be a little bit of a different flavor. All right, so there's been several laws on the books now that give users a right to be forgotten. And our basic question is, like, what does it actually mean for a data controller to delete a person? And so the legal literature have identified two kind of facets to this. Right? So, one is confidentiality, which is that, like, erasure is mainly about keeping a user's data secret and then control, which is more about erasure is more about, like, a user's autonomy and, like, self determination.
00:00:53.164 - 00:01:36.378, Speaker A: So with this in mind, we kind of want to write down a formal definition of what this means. All right, so in our paper, we give a general framework for reasoning about erasure. This framework unifies existing approaches that only gave kind of partial answers to this question. And in writing this down, it also tells us, like, what insights. Sorry, what dimensions of erasure we haven't captured. Okay, so one specific area that's received a lot of attention recently is called machine unlearning. So here's the basic problem.
00:01:36.378 - 00:02:44.864, Speaker A: So say you train a machine learning model using some training data, and now someone from your model wants to be removed from the training data and from the model itself. All right? So ideally, we would just retrain the model from scratch, excluding that person's data, and we'd have an updated model that doesn't depend on their data at all. But this is expensive in general. So what we want instead is like an efficient unlearning algorithm that will update the model faster than retraining from scratch. And we want that the distribution over this unlearned model is close to the distribution over the retrained from scratch models. All right? So besides the legal scholarship I briefly touched on and machine and learning, there's been other sort of attempts at defining deletion generally. So this paper by Gar Gollum, Goldbasser and Vsudovan formalize deletion, kind of in a confidentiality notion.
00:02:44.864 - 00:03:32.624, Speaker A: It's a very strict definition in that, like many data controllers in the real world, don't satisfy it. And so Godin and Lamartine try to relax this formalism, but they relax it kind of too much. And so sometimes it doesn't give meaningful guarantees. And another kind of tool at our disposal is something called history independence that I'll talk about a little bit. And this kind of gets at a lot of what we want from deletion, and we think it's kind of the right way to think about it, but it doesn't give us everything. All right, so let me give you a little more context. All right, so we define this general definition of deletion that we think captures kind of a control notion of like, data autonomy.
00:03:32.624 - 00:04:13.324, Speaker A: And, um, this definition, oh, the colors got messed up. Okay. This definition captures, um, two existing notions. So one is adaptive history independence. Um, and this generalizes, um, definitions of machine and learning to, um, general data structures. And it also captures this confidentiality idea that was already, um, already published. All right, and, um, in doing this work, we also realize that there are some interesting differentially private controllers that aren't well captured by either of these, um, previous notions.
00:04:13.324 - 00:04:54.746, Speaker A: And so during this talk, for simplicity, I'll also, um, explore these definitions with like, very simple examples that separate and overlap between these definitions. All right, so let's first talk about confidentiality. All right, so here's the basic setup. So all of these definitions will have three parties. We have a data controller. There's a special data subject named Alice who will interact with the controller and then request that her account is deleted. Um, and then there's an environment that just represents all other users.
00:04:54.746 - 00:05:55.888, Speaker A: And the environment can also delete its accounts if it wants to. All right, so for deletion as confidentiality, we're going to use kind of a real ideal style definition, sort of similar to cryptography. Um, so in the real world, Alice interacts with the controller and then requests to be deleted. And we'll compare that to an ideal world in which Alice just never interacted at all. Okay, and what we'll require, or what they require, is that the environment's view and the controller state between these two are distributed identically. The environment can't tell whether Alice ever interacted at all with the controller after deletion. Right? So now let's look at how this works with a simple example.
00:05:55.888 - 00:07:08.284, Speaker A: So say we have a private cloud storage controller where users can upload and download their own files, so Alice can upload her own files and download them. And the environment can view its own files as well. So critically, notice that, like, the environments view is not affected by Alice at all. Okay, so this satisfies our definition as long as the internal state is stored in a history independent way, which I'll talk about a little bit more later. Okay, now let's consider something a little bit more complicated. So say we have something called a public bulletin board where Alice can make a post to the bulletin board, and then the environment can actually read Alice's post and make its own post in response. So in the real world, the environment can obviously tell whether Alice was made a post or not.
00:07:08.284 - 00:07:58.894, Speaker A: And so in the ideal world, when Alice doesn't participate, it's not clear what will happen, right. So the environment's view is completely altered and its behavior may be very different in this situation. And so I'm going to claim that like, no implementation of this public bulletin board will satisfy confidentiality. Um, but that's not great because a lot of like real data controllers have a public interface, right? Like the reason you might want to delete yourself is because the information is public. So we want to do something better that actually captures these kinds of data controllers. Okay, so this is where our next attempt comes in. Um, adaptive history independence.
00:07:58.894 - 00:09:14.970, Speaker A: Okay, so let's just start with the nonadaptive setting. Um, so history independence says that if I have two logically equivalent sequences, like consider the examples here where you're inserting five, inserting seven, and then deleting five, that's logically equivalent to just adding seven in the. And so for all such pairs of equivalent sequences, the internal state of the controller should be distributed similarly in these two cases. So just looking at the state, I can't tell whether five was inserted and deleted, for example. All right, so this is not a trivial property to satisfy a lot of data, sorry. A lot of data structure implementations that we actually use do reveal the order of insertions. For example, there are efficient implementations, but just for the purpose of this talk, let's just assume that, like dictionaries just store items in sorted order.
00:09:14.970 - 00:10:26.804, Speaker A: Okay? Okay, so there are more clever algorithmic ways of doing this so that you get better lookup times and such. But let's just assume this simple implementation is what we're using. All right, so now we generalize this to an adaptive setting. All right, so again, we'll consider kind of a real ideal situation. The real world will be exactly as before, and the ideal world will differ a little bit because we need the sequence of operations that the controller sees to be logically equivalent in these two cases. So we'll replace the environment with a dummy that just replays the real world queries in the ideal world. Okay, so what we'll require is that the internal state of the controller is distributed similarly in these two settings, even if you get to see the real view of the environment.
00:10:26.804 - 00:11:32.864, Speaker A: So the environment's real view doesn't help you distinguish between the states of the controller in these two situations, and I should say, like in our paper, we define this more generally to hold for any pair of equivalent sequences. All right, so let's just, for, if you're a little bit lost on this definition, let's just see how, how it works with the public bulletin board example. All right, so the public bulletin board does satisfy this definition. So Alice will make her post, the environment will read her post and then make its own post in response, and Alice will delete her account. So what will happen in the ideal world is that the dummy party will try to get Alice's post. The controller will be like, there's no such post. And then the dummy will anyway make the same response.
00:11:32.864 - 00:12:11.284, Speaker A: So this transcript may not actually be plausible, but it's logically equivalent, which is what we need. All right? And so assuming that the bulletin board is storing the posts, say, in like some predetermined sorted order in a history independent way, um, then like, the internal state of the controller will be identical in these two situations. Okay, so that's great. So that gets us more of what we want, right? We wanted to capture this public bulletin board example. Yeah.
00:12:14.584 - 00:12:25.244, Speaker B: Just a fast clarifying question. How can we tell on the right side that the environment, that the dummy didn't give back Alice's post? To my eyes, it looks the same.
00:12:26.584 - 00:12:29.256, Speaker A: What do you mean give back Alice's.
00:12:29.440 - 00:12:37.084, Speaker B: A red arrow from dummy to public bulletin board that says get Alice's post in both places.
00:12:37.704 - 00:12:41.040, Speaker A: Yeah. So it's just sending a request, like a get request or something.
00:12:41.152 - 00:12:42.440, Speaker B: Oh, the request is, okay.
00:12:42.512 - 00:13:05.024, Speaker A: Yeah. And then the response is, is just bottom or some error message or something. Yeah. Okay. Yeah. So this gets us a lot more of what we want to capture in terms of deletion, but not everything. So in particular, I'll show you in a second that differentially, private controllers don't satisfy this definition.
00:13:05.024 - 00:14:07.454, Speaker A: Okay, so, all right, so let's consider a controller where we're, um, just taking, we're publishing the sum of bits, or numbers, actually, bits is better. Um, publishing the, the sum of bits that users submit. And, um, to satisfy differential privacy, we'll add some laplace noise, um, both before and after, and publish that, um, and I'll say we'll collect these, um, submissions up until some pre specified deadline, like midnight, um, and use after users, um, sorry. If users try to add things after that, um, the state won't change at all. Um. All right, so if Alice only, um, contributes once, then both the internal state. So this noisy sum and the output is differentially private.
00:14:07.454 - 00:14:47.234, Speaker A: All right? And I would argue that, like, on getting a deletion request, you shouldn't really have to do anything to honor that request. And the reason is that, like, if Alice wants to delete her data was already not really contributing much to the sum. So you should get some kind of like, epsilon delta deletion guarantee. But, um, you don't get this. So this does not satisfy adaptive history independence. So this tells us that adaptive history independence is not enough. So I'll tell you why.
00:14:47.234 - 00:15:49.990, Speaker A: All right, so, like I said, adaptive history independence requires that for all logically equivalent sequences of queries, the, the distribution over the states are closed. Um, and so here are two equivalent sequences. So if everyone deletes from your data set, that should be the internal state should be equivalent to no one ever adding themselves to begin with. But of course, if the differentially private controller doesn't update its state at all, then it's going to have information that it wouldn't have if no one ever participated. All right, so history independence is not enough to get us to capture this idea of deletion as control. All right, so now I'll tell you about our definition. All right, so we have the same kind of basic setup as adaptive history independence.
00:15:49.990 - 00:16:59.964, Speaker A: So we again have a dummy controller that's replaying queries in the ideal world and will require that the state of the controller is identical with high probability in these two settings. So not just the distribution, but the actual state itself. And what this kind of gives us is that after Alice deletes, we can kind of explain the state without using her, uh, queries at all. Okay, and then there's, um, kind of a detail for randomized controllers, which is we'll allow randomized controllers to use some specially tailored randomness that's generated by a simulator. Okay, so this, um, alternate randomness will have to be plausible in that. Like, it's close in distribution to the real randomness. Um, but this can help to kind of mask or adjust for, um, slight changes, uh, due to Alice not being, uh, present.
00:16:59.964 - 00:17:44.014, Speaker A: So I'll explain that a little bit more later. Um, all right, so here's just a, a very simple example. So say the controller is storing the xor of lengthen bit strings that it gets from users to delete. It doesn't have to do anything. And I'll argue that with the following simulator. All right, so the simulator can parse the transcript to figure out Alice's contribution, and then it can reset the randomness. So it'll say that r prime this alternate randomness is actually r xor x sub Alice.
00:17:44.014 - 00:18:27.548, Speaker A: And in that way it's like completely removing kind of Alice's impact from the total sum, kind of denying that she was ever there. So hopefully it's clear that the states in these two situations are identical. And also the distribution over the randomness should be identical. Um, they're both uniform. All right. All right, so now I want to talk about this kind of interesting class, um, that we sort of started studying. Um, and this is where differential privacy comes in a little bit more.
00:18:27.548 - 00:19:44.124, Speaker A: So, um, you can. Yeah, all right, so um, right, so for this uh, sort of class of controllers that are kind of interesting, um, they satisfy property that we call adaptive, uh, pan privacy with continual release. So this is expanding on previous work on um, pan privacy and continual release and their combination, and also some recent work on adaptive continual release. All right, so in the continual release model, we're imagining that our mechanism gets some stream of inputs every day, let's say, and then at the end of the day has to produce some output based on this stream. All right, and, right, and so what we want to. So to protect to. Yeah, so we want to consider like two worlds, one in which Alice, Alice's data was included and one in which Alice's data was not.
00:19:44.124 - 00:20:58.720, Speaker A: And pan privacy sort of allows the adversary to also see the internal state of the controller once, or the mechanism. All right, so we want that Alice's presence or absence in the data is not detectable, even when you get to see all the public outputs as well as one, the internal state at one time. Okay, so I already showed you an example of a controller that satisfies this definition. So by adding two Laplace random variables, the output and the internal state are protected. Okay, so we want to compare this to our definition of deletion. We want to like see whether adaptive pen privacy with continual release is enough to satisfy deletion. But there's one problem, which is that, um, the way that we formulated it, uh, our adaptive pan private definition, um, is kind of a.
00:20:58.720 - 00:21:41.274, Speaker A: A, um, event level guarantee. So Alice only really gets to give one input. But our deletion guarantee is a user level one. So Alice could insert lots of things and then delete all of them. All right, so what we need to bridge this gap is a dictionary that will keep track of which users participated and only allow each user to participate at most once. And we'll assume that this dictionary is stored in a history independent way. So just assume that all the entries are stored in a sorted order.
00:21:41.274 - 00:23:25.246, Speaker A: All right, so then what we can show is that if our mechanism is adaptively pan private in the continual release model, then this compose controller that has a dictionary to impose the requirement that Alice only participates once, then that compose controller satisfies deletion as control. Okay, so here's kind of a general proof sketch. Um, so we'll fix an environment analys, and in the actual proof you have to do some reductions. But once we have that, we need to create a simulator for the deletion guarantee and show that the real state, one run on, on real randomness, is identical to the ideal state when run on our alternate randomness. And we need to show that the simulated randomness is close in distribution to the real randomness. Okay, so a general strategy that turns out it works in this case is to sample r prime from the same distribution as r, but conditioned on the good event that you get the same state in the end. All right? So, um, so if there is some resetting of the randomness that will give you the same state with, without Alice, um, we'll sample from the set of randomness that satisfies that.
00:23:25.246 - 00:25:13.784, Speaker A: Um, and if no such randomness exists, we'll just return a bottom. All right? And so one concern is like, do you return bottom too often? Right? So are these distributions close, and do you actually get the same state with high probability? And in our paper, we have this nifty coupling lemma that we prove that tells us that actually this, this works, so we're not going to return bottom too often, and so therefore the states will match with high probability. Just to give you a little bit more concrete sense for how this might work, in our specific example. So, in our example, the randomness is two Laplace random variables. And so our simulator is really simple. In this case, we can just, if Alice participated in the real world, we can just add one to one of the two variables to account for the fact that she's no longer part of the sum in the ideal world. Okay, so, right, so in the real estate, uh, we have this sum plus two Laplace random variables, and in the ideal state, Alice didn't participate, let's say, um, and so we'll increment one of the Laplace random variables by one, um, to counteract that, all right? And then by, you know, the properties of the Laplace random distribution, l plus one should be close to l.
00:25:13.784 - 00:26:31.904, Speaker A: All right, so just to wrap up, I showed you our general definition of deletion that captures these two existing kind of ways of approaching this problem, right? So, confidentiality, which was very sort of strict and gave a very strong guarantee, but didn't capture a lot of, like, real functionalities that we might want to reason about. Then I showed you adaptive history independence, which does capture this bulletin board example and many others, but doesn't sort of capture our intuition about differential privacy, giving us some kind of epsilon delta deletion guarantee. And then I talked about this sort of interesting class of data controllers that wasn't well captured by either of these. All right, so, yeah, just in conclusion, our paper gives this general framework for thinking about erasure. It identifies. Sorry. It unifies these kind of existing approaches that only gave us partial answers to this question.
00:26:31.904 - 00:27:03.050, Speaker A: And I'll tell you about what we haven't captured. All right, so, um. Right, so there's several things we haven't captured. This is not an extensive list. Um, but one is, um, we assumed that, like, just looking at the state of the controller, that you could easily tell whether Alice's data was present or not. That's not necessarily true in the real world. Um, so we haven't captured any kind of effort.
00:27:03.050 - 00:27:52.264, Speaker A: Right. So you could imagine that the. That it's like, not efficient to determine whether Alice is present or not. We also assume that the controller could, like, immediately update its state to delete Alice. But in the real world, you know, controllers may take days or weeks to actually perform the update required to delete someone. Um, and we have a kind of limited composition results in our paper, but we don't have any kind of general composition properties for deletion yet. All right, so in the future, um, you could think about connecting this to various, like, practical problems and databases, literature.
00:27:52.264 - 00:28:10.264, Speaker A: Um, or you could also think about, like, whether we can get better and faster machine and learning algorithms by using our less restrictive notion of what it means to delete someone. But that's all I have, and I'm happy to take questions, thanks.
