00:00:00.360 - 00:00:56.295, Speaker A: Topic advanced topics course. You're here because you want to learn Riemannian geometry, not because you want to, you know, cheat and get a good grade. Just, just work together. That's fine. So the course, these guys are going to control the cameras so that the audio virtual audience can see where I'm writing. The course is called the second course in Reminding Geometry. I know that some of you have had no first course in remaining geometry, and, and that's okay as long as you're totally comfortable with smooth manifolds, vector fields, lead derivatives, flows, tensors, differential forms, integration, non manifold fold orientations, the kind of stuff we teach in a smooth manifolds course, which is pure math, 465, 665 here at Waterloo, then you should be fine because I'm going to start with the definition of Riemannian metric, and I'm going to try to go through the sort of basics that one would see in the end of a course on.
00:00:56.295 - 00:01:27.175, Speaker A: Traditionally, you know, what happens is that people teach a course in this with manifolds, and in the last three weeks they try to jam as much reminding geometry as they can in there. So I'm basically going to start from there and I'm going to do it fairly quickly. I'm following this book by Del Carmo. I didn't bring a copy. Manfredo Docarmo, Romanian Geometry Chapter zero is just smooth manifold stuff that everyone should know. Chapters one, two and four I'm going to very, very quickly cover in two lectures, maybe three. And that's going to seem fast paced because I want to assume that you've all seen that stuff.
00:01:27.175 - 00:02:19.211, Speaker A: I know you haven't all seen that stuff, so you can refresh your memory by reading the textbook. But then it's going to go much more slowly because then I'm going to get into the proper syllabus of the course and we're going to do chapters three and as much from five to 13 as we can. So the main theme of the course is going to be to understand the relationship between geodesics and curvature. Okay, so I'm going to define geodesics, I'm going to define curvature, and we'll see that there's many ways in which they're related. So if you know something about the curvature or about the geodesics or both, you usually can conclude some very surprising and powerful results about the manifold, some global topological results. Okay, so on the course outline, which is on my website, and I think I posted it on canvas as well, you can see a tentative list of the topics it's basically the last 2/3 of Docarmo's book. Okay.
00:02:19.211 - 00:02:43.575, Speaker A: I'm not going to be following his notation because his notation is non standard and not so not so nice. And he has strange conventions for signs and normalizations that no one else uses. So I'm going to use those sort of standard conventions and normalizations. So just be aware of that. I'll mention it when it comes up. It won't come up today. Yeah, but other than that, are there any questions about the.
00:02:43.575 - 00:03:01.985, Speaker A: Yeah, they will be posted on the fields. The fields will have some kind of YouTube unlisted page and that'll be. But that doesn't mean you shouldn't come to class review. Yes. Okay. Yeah. All right, so let's get started.
00:03:01.985 - 00:04:04.749, Speaker A: So the good thing about a topics course is that I can get as far as I want, right? I want to get to chapter 13. If we only get to chapter 11, that's fine, right? No, it's no problem. So interrupt me and ask questions as often as you want. This is not. There's no race to get to some deadline by a certain date. Okay, so let's let MN be a smooth N manifold. What's a Riemannian metric? A Riemannian metric G on m is a smooth 02 tensor on m such that first of all, G of X Y equals G of Y X for all smooth vector fields X and y.
00:04:04.749 - 00:05:11.451, Speaker A: So this is the space of smooth vector fields. Standard notation. So what is a 02 tensor mean? It means that it eats two vector fields and gives you a smooth function in a tensorial way. Right? So this says that it's symmetric and moreover, G of xx is greater than or equal to zero with equality if and only if x equals zero. Okay, so really what it's saying is that is for all P and M, G at P, which is an element of TP star M tensor. TP star M is a positive definite symmetric bilinear form. Is there still nobody on the call? No.
00:05:11.451 - 00:06:15.505, Speaker A: I hope they're not locked out of it. Sorry, one second. One guy's looking for the zoom. This is not going to be happening after today and the others are not there. Okay, sorry about that. So this is really. This is the definition of Riemannometric.
00:06:15.505 - 00:06:59.655, Speaker A: So in other words, that is a smoothly varying family of inner products on the tangent spaces. So if we just have a smooth manifold, it doesn't come equipped with a Riemannian metric initially. Right? There's a. There's a choice you. You have to make. But every smooth manifold has lots of Riemannian metrics. So fact, every smooth manifold has lots of Riemannian metrics.
00:06:59.655 - 00:07:33.657, Speaker A: And the way you do it is using a. One way is using a partition of unity. I remember partition of unity allows you to reduce global constructions to local constructions. And locally any manifold is just an open set in Rn and we have the standard inner product on Rn. We can just restrict it to every. The tangent space at a point to an open set in RN is just RN canonically. So we just take the standard inner product that allows us to do this locally and then we patch it together.
00:07:33.657 - 00:08:10.669, Speaker A: And the reason why this works is because a partition of unity, they add up to one and they're all non negative. And if you take a convex combination of positive definite inner products, it's a positive definite inner product. So that's why it works. So there's going to be a lot of things that I'm going to tell you without proof because it's not the point of the course and you've maybe seen it, and if you haven't seen it, you can look it up. But you don't need to know how to prove this. To follow the rest of what we're going to do in this course. We're usually going to assume we have one manifold with a given Riemannian metric on it, and we might have another one with another Riemannian metric and we might compare the two in some way.
00:08:10.669 - 00:08:56.359, Speaker A: But we're not going to ask the question of, you know, what is the best Riemannian metric on any given manifold? That's an important question and a lot of people study that. Right? We will when we get to the definition of curvature on probably next and next week. Well, certainly next week because there's no class on Friday, it's Monday, Wednesday. I'll give some definitions of special kinds of metrics whose curvature has special properties. And that's a very interesting problem in math to study. When does a manifold admit certain special kinds of metrics? And we'll see some of that in this course because we'll prove results such as something called the Myers theorem, which says that if you had a complete Riemannian metric, you don't know what complete means yet, and the Ricci curvature was bounded below by a positive constant. You don't know what those words are yet.
00:08:56.359 - 00:10:22.987, Speaker A: Then it has to be compact, right? So therefore, if you had a non compact manifold, there's no way it's going to have a complete positive Ricci curvature metric on it. Strictly positive Ricci curvature metric. So we're going to see a taste of these kinds of restrictions on the topology of the manifold, if you want certain types of metrics on it. But mainly we're going to be assuming we have some given metric to begin with, and we're not going to be trying to find a particular type of metric. Okay, so the picture there's a point P tangent space to P at M, that's an N dimensional real vector space, and GP is an inner product on this vector space. So let's let E1 up to EN be a local frame for the tangent bundle over some open set U. What does this mean? This means that each EI is a smooth map from U to the tangent bundle such that EI at the point P is in TP of M.
00:10:22.987 - 00:11:23.275, Speaker A: So there's smooth vector fields over U on U. Right? An open subset of the manifold is a manifold in itself. And so these are just smooth manifolds on U such that for all p and u, e1 at p up to en at p is the basis of tpm. Okay? So it's a smoothly varying family of bases, and we can always get such a local frame. For example, if x1 up to xn are local coordinates, then d by dx1 up to d by dxn is a local frame. But not every but not every local frame needs to be a coordinate frame. But not every local frame is a coordinate frame.
00:11:23.275 - 00:12:29.485, Speaker A: And you can already see that because if you have two coordinate vector fields, the lie bracket is zero. So if you had a coordinate frame and the lie bracket of two elements is not zero, then there's no way that there's some coordinate chart in which that's a coordinate frame. Okay? So if we have a local frame, which we can always do, let's let E upper 1 up to E upper N be the dual coframe of the cotangent bundle. That means that each of these EIs is a one form, smooth one form on u, and EI applied to EJ is a smooth function, and it's this function Delta IJ. Okay? So at every point, these give you the dual basis of the dual space. For example, if EI, if the EIs are a coordinate frame, then the EJ's are the DXJs. Okay? But it doesn't have to be.
00:12:29.485 - 00:13:08.939, Speaker A: Now, if we have a coordinate frame. Sorry, if we have a local frame on. Well, let me not do that yet. Let me say that means we can write with respect to such a frame, the Riemannian metric is going to be gij EI tensor ej, There's a sum overall I and J here. I'm using the Einstein convention throughout. So what are these? Gij is just what you get when you apply G to ei ej. And this is a smooth function for fixed I and j.
00:13:08.939 - 00:14:13.095, Speaker A: This is a smooth function on U, real value function on u. And if you look at the matrix gij, this is a positive definite symmetric matrix N by n matrix of functions. Okay? Matrix of smooth functions. Okay, so suppose we had a. Suppose we had such a local frame, we can always run the Gram Schmidt orthonormalization process on it to get an orthonormal frame. So by running the Gram Schmidt process, we can always obtain a local orthonormal frame. I'll still call it E1 up to EN.
00:14:13.095 - 00:15:34.935, Speaker A: And this, what does it mean to be orthonormal? It means that gij, which is G of EIJ is just delta ij and therefore in such a frame our metric is just ek cancel ek. Okay, so you can always find local coordinate frames and you can always find local orthonormal frames if you have a metric. The coordinate frame doesn't even need a metric, right? That's just a smooth manifold object. But then you can ask the question, suppose I have a Riemannial metric, can I find a local coordinate frame which is also orthonormal? And you cannot always do that. And we're going to see next week that that's related to curvature. So let me just make a remark. It's not always possible in general to choose to find a local frame which is both coordinate frame and orthonormal.
00:15:34.935 - 00:16:16.789, Speaker A: This is related to curvature. In fact, you can do this precisely if and only if the curvature is zero. So we'll see this soon next week. Okay, so what can we do if we have a Riemannian metric? As I said, every manifold has lots of Riemannian metrics. Let's just fix one. If we have a Riemannian metric, what can we do? In each tangent space we have a positive definite product. That means we can talk about lengths of tangent vectors and angles between tangent vectors.
00:16:16.789 - 00:17:35.395, Speaker A: Anything we can do with an inner product. So let's let alpha be a smooth curve. So this is a closed bounded interval. What do I mean by smoothness on a closed bounded interval? I mean that there's some open interval containing this and a smooth map on that open interval into M such that when you restrict it to this closed bounded sub interval, you get this map. So this could, this is the curve alpha point P is Alpha at the initial parameter A, Q is alpha at the final parameter B. If I have a curve like this, well, we know that the velocity vector alpha prime T, this is in T alpha of T M, the tangent space. So if this is alpha at time T and that's the tangent space at alpha of T to M, then the velocity of the curve at that point is a tangent vector in this tangent space, alpha prime of T.
00:17:35.395 - 00:18:11.623, Speaker A: And because we now have an inner product on the tangent space, we can take the length of that vector. Okay, so alpha prime of T using the inner product, the metric G. What does this mean? This means the square root of G of alpha prime of t. Alpha prime of T. And if I'm being really pedantic, this is happening at the point alpha T. Okay, I'm usually going to use shorthand for the inner product because we're going to have a fixed metric G. I'm going to usually use angle brackets.
00:18:11.623 - 00:18:54.985, Speaker A: So you'll see me also write alpha prime of T. Alpha prime of T to the one half. Okay? This is called the speed of the curve alpha at the point alpha T. Okay, and we need, let me, let me emphasize, we need a metric to do this. If you have any curve on a smooth manifold, you can always define the velocity vector field. It's a tangent vector at every point along the curve. But to define the speed of the curve, which is the norm of the velocity, that depends on the metric.
00:18:54.985 - 00:19:55.245, Speaker A: And if you chose some other metric, the speed would be different. Okay, and now we define the length of this curve by integrating the speed over all values of the parameter. So the length of alpha is defined to be so length of alpha, it's the integral from A to B of the speed T. Okay, so certainly because alpha is smooth and G is smooth, this thing, this thing is even smooth everywhere except when the velocity vanishes. But it's continuous there at least. So we can certainly integrate the continuous function on the closed bounded interval. And this is greater than or equal to zero.
00:19:55.245 - 00:20:21.795, Speaker A: And in fact it's strictly positive. Unless alpha is a constant curve. Alpha of T is P for all T in a B. If that's true, then the velocity is zero. So this is integral of a non negative function. The only way you'll get zero is if that function is zero. That means the velocity is zero everywhere.
00:20:21.795 - 00:20:56.615, Speaker A: That means alpha's constant. Okay, so that's the definition of a length of a curve. Again, we need a metric to define the length. And if we chose some other metric, the length would be different. The first fact about the length that's important. And I'll leave this as an exercise for you. Let T be F of S, where F is a reparameterization, be a reparametrization.
00:20:56.615 - 00:21:38.215, Speaker A: What does this mean? This means that F is a smooth bijection from this closed bound interval to this one smooth bijection with smooth inverse. That's really the restriction of a diffeomorphism from some open interval containing this to some open interval containing this. Then there are two cases. If it's a bijection, this is going to have to be the derivative. Because it's a diffeomorphism, the derivative is going to be always non zero. And because this is connected, the derivative is going to be always positive or negative. So if F prime is positive everywhere, this is called orientation preserving.
00:21:38.215 - 00:22:24.255, Speaker A: And if F prime is negative everywhere, this is orientation reversing. And in this case it takes A to C and B to D. And in this case it swaps the initial and the final points. Okay, so suppose we have such a parameterization. Then the point is that the length is independent. So let alpha tilde of S be alpha of F of S. Okay, so alpha tilde is a smooth curve from CD to M, and the length of alpha tilde is equal to the length of alpha.
00:22:24.255 - 00:24:10.065, Speaker A: This is independent of reparametrization. Okay, so that's an exercise for you. It's this first year calculus chain rule substitution. And another fact is that because of this we can always pick a sort of canonical reparameterization preserving the orientation, which is what's called a unit speed parameterization. So fact number two, we can always reparameterize any curve alpha in an orientation preserving manner, preserving manner such that alpha we can assume that the closed bound interval starts at 0 and L is the length of alpha and the speed is one for all t. So of course, if the speed is one for all t, integrating one from zero to L gives us L, right? So L has to be the length of alpha if the speed is one, and we can always do this. So this is called an arc length parameterization or a unit speed parameterization.
00:24:10.065 - 00:25:15.157, Speaker A: And you can see why it's called arc length, because if you measure the length of it just from time 0 to time t, because the speed is 1, you're going to get T, right? So going from the parameter 0 to the parameter T, the length of it is exactly t. So that parameter is the length of the curve from the parameter T is giving you the length of the curve from time 0 to time t. So we're always going to assume, unless I say otherwise, that our curves are parameterized by arc length. Okay, maybe draw a picture again. You'll find that I have like three pictures that I keep drawing over and over again. But that's really all you need to do with geometry. So alpha of 0 is P, alpha of L is Q.
00:25:15.157 - 00:25:55.553, Speaker A: And this has length L. Okay, so one of the things we're going to do in our Imani manifold, now we know, we know how to compute the length of the curve. Okay, I'm giving you a preview of what's coming in the next three lectures or so. Since we can compute the length of a curve, let's assume our manifold is connected. Otherwise you can break it up into its connected components and they don't speak to each other, they're just completely independent. If it's connected, that means the same as path connected. So there's a continuous curve between any two points.
00:25:55.553 - 00:26:28.195, Speaker A: It's very easy to see by looking, by covering that image of that curve with charts, domains of charts, that you can smooth it out. And you can always join two points by a smooth curve. Okay, so given any two points in M, if the manifold is connected, there's always a smooth curve going from one to the other has a length. And now you can define the distance between those two points as being the infimum of the length over all smooth curves joining those two points. Okay. And we call it the distance because we'll show that it is a distance. It is a distance function in the sense of metric spaces.
00:26:28.195 - 00:26:55.945, Speaker A: Okay. So it gives you a topology because you have a metric space, and perhaps unsurprisingly, the topology you get is the same as the original topology you had on the manifold. That's part of the preliminary steps to getting to the Hoff Rhino theorem, which is coming in the next couple of weeks. Yep. Non constant. Thank you. If alpha is non constant.
00:26:55.945 - 00:27:40.393, Speaker A: Yes. Let me just say. That's right. That's right. Non constant. Okay, so the next thing is to talk about the Levy Civita connection, also known as the Levy Civita covariant derivative. Okay, so I'm probably never going to write Levy again.
00:27:40.393 - 00:28:36.739, Speaker A: I'll just put lc and always. So if you know about more general connections on more general vector bundles, this is a very special case, and you've probably even seen this one before. If you don't know what a connection is, it doesn't matter. You just have to know what this one is, and I'm going to tell you what it is. Okay, so given a Riemannian metric G on M, there exists a map nabla which takes two vector fields and gives you a vector field. And it's, it's bilinear over the reals. And instead of writing NABLA xy we write NABLA sub x Y.
00:28:36.739 - 00:29:13.065, Speaker A: So the first argument we put down here, the second argument we put over there. I'm always going to write this, I'm never going to write that again. Nabla is something that takes two vector fields and gives you a vector field. It's bilinear over r. That means nabla x1 plus x2y. And let me put a scalar here and it's, it's also linear in R. So if I do nabla x of lambda y1 plus lambda y2 then I get lambda nabla x y1 plus nabla xy2.
00:29:13.065 - 00:29:54.659, Speaker A: And so, so, so this is it. It takes two vector fields, gives you a vector field in a way that's bilinear over the reals. But we know there' we can do with smooth vector fields, we can multiply them by smooth functions. The space of smooth vector fields is not just some infinite dimensional real vector space. It's also a module over the ring of smooth function c infinity of M. So with, if I multiply X by a function, it pulls right out. But if I multiply Y by a function, it doesn't quite pull out.
00:29:54.659 - 00:30:34.669, Speaker A: There's a term where you just pull that out. But there's another term where you differentiate F in the direction of X and multiply by Y. So X applied to F, that's the directional derivative of F in the X direction, that's a smooth function. So I can multiply it by Y. That's a vector field. This is for all F in C infinity of N + two more properties which I'm going to write on the other board, okay? Because as you can see, so far I haven't used the metric. Right? I've just talked about something you could talk about on any smooth manifold.
00:30:34.669 - 00:31:31.063, Speaker A: Okay? This is called a connection or a covariant derivative on the tangent bundle. So it has to be R linear and it has to have this says that it's tensorial in X. And if we multiply X by smooth functions, they pull right out. And this is what's called a liveness rule, right? You want to think of this operation as differentiating Y in the direction of X. Okay? And it's a directional derivative. So you know that directional derivatives don't you know, if you take the directional derivative, all that matters is the point where you're taking the directional derivative and the direction that you're taking the directional derivative. So the only thing that matters, if you want to evaluate this vector field at a point P, the only thing that should matter is the value of X at P.
00:31:31.063 - 00:32:44.159, Speaker A: But if we're really differentiating Y, then it's not just the value of Y at P that should matter, but in an open neighborhood of P. I'll make that precise on the next board. But this is showing you that it is some kind of derivative, because if I multiply these two things together, the function and the vector field, I get a vector field. Some kind of derivative of this should involve a term where you differentiate one guy and not the other and vice versa. So I'm going to call this property 1 and 2. So I haven't written the other two properties yet. So our linearity in X and y and Properties 1 and 2 define a connection or covariant derivative on the tangent bundle.
00:32:44.159 - 00:33:10.355, Speaker A: And it's on the tangent bundle. Because the thing we're differentiating the Y is a, is the section of the tangent bundle. The thing that goes down here. If you, if you want to know what more general connections are, the thing that goes down here is always a vector field. You're always differentiating in the direction of some tangent vector to the manifold. But you could be differentiating more general things like sections of some other vector bundle. Yeah, yeah.
00:33:10.355 - 00:33:33.377, Speaker A: In my, in my, my eyes it was there. You're right. If I don't see your hand, just call out. Okay, so we haven't used the metric yet, but the Levi Civita connection. So by the way, these two words at this level are synonyms. Connection, covariant, derivative. They mean the same thing.
00:33:33.377 - 00:35:10.133, Speaker A: Connection is more general, because if you're talking about a fiber bundle that's not a vector bundle, you can still generalize the notion of connection in some way using what's called horizontal and vertical splittings. But then it's not really a covariant derivative anymore. It doesn't matter for this course. So if G is a metric on M, then there exists a unique covariant derivative Nabla, called the leviticus covariant derivative, satisfying the following two additional properties. So the first one, let me call it Levita A, says that if I take two vector fields, Y and Z, and I plug them into the metric, that's a smooth function. So I can take that smooth function and I can differentiate it with respect to another vector field X, and I should get a function and the one we get is this one. So again, let's think about what this means.
00:35:10.133 - 00:35:44.275, Speaker A: Okay? You should think of this as some kind of product of three objects locally. It really is. Right? If I write this in some local frame, yi ei and Z J ej, this will be gi j, yi Z J sum over I and J. So it's a sum of products of G's and Y's and Z's. So if I'm differentiating that function, there should be things involving derivatives of Y and not G or Z. Things involving derivatives of Z and not G or Y. And there should be another term where I don't differentiate Y or Z, but I do differentiate G.
00:35:44.275 - 00:36:25.805, Speaker A: And what I'm saying is that I don't want that term to be there. So this is saying Nabla G is 0 or Nabla is metric compatible. Okay? So if you've never seen connections before, just ignore this line. Right? Because I haven't told you what nabla g means. Okay, but if you have, then you know this is the same thing. So this, this definitely uses the metric. Right? And Levi CV says that if I take Nabla XY minus Nabla Y X, that's the same as the lie bracket of X and Y.
00:36:25.805 - 00:36:52.973, Speaker A: This is what's called the torsion free condition. Okay? So you can see this. I could have asked for this without a metric. This doesn't have a metric in the, in the defining property. This of course depends on the metric. What I'm saying is that if you had a metric, there is a connection that satisfies these two and there's only one. Okay.
00:36:52.973 - 00:37:44.557, Speaker A: And that's why this is sort of the preferred connection. So let me give you an idea of the proof. It's just a computational proof, by the way. This is called the fundamental theorem of Riemannian geometry. It's a little bit overhyped because it's trivial to prove. So it shouldn't really be called a fundamental theorem, but it is. So the idea of the proof, you can fill in the details if such enable exists, it must satisfy the so called Kozul formula, which is 2 times.
00:37:44.557 - 00:38:34.331, Speaker A: Remember I'm using angle brackets for G2 times. The inner product of Nabla x y with Z is equal to X applied to yz. That's a function I'm differentiating in the X direction plus Y applied to xz minus Z applied to XY minus the bracket of XZ with y minus the bracket of yz. Sorry, inner product x plus the bracket of XY inner product Z. So this Looks like a strange formula. Let's, let's see that it's actually true. Right? If I differentiate this using property A, there's going to be a nablaxy with a Z that's already there, ok? And a Y with a Nabla xz.
00:38:34.331 - 00:39:12.981, Speaker A: But the Y with a Nabla XZ is going to be killed off by one of these because this is Nabla XZ minus Nabla zx. So one of these, when you expand this bracket using part B, one of these will kill one of those. Same thing here and same thing here. And then when the dust settles, all the derivatives of X and Z are going to be gone and there's only going to be two of these left over. Okay? So if you expand the right hand side and use these two properties, you get that this is true. And the nice thing about the right hand side is it only involves the metric, it doesn't involve nabla. Right? So if you know the metric, then you know this right hand side for any X, Y and Z.
00:39:12.981 - 00:39:55.309, Speaker A: That means you know this for any Z and for any X, Y and Z. So since it's a positive definite inner product, if you know the inner product of two vectors, if you know, if you have a vector and you know it's inner product with every other vector, then you know what that vector is. So this, if you know this, for all Z, it determines Nabl X Y. Okay? So this determines, determines Nabla XY for all X and Y. And that's since the inner product is positive definite, all you really need here is non degenerate. Okay, but it's positive definitely. So it tells you that if such a connection were to exist, it would have to be unique.
00:39:55.309 - 00:40:47.195, Speaker A: There's no choice about what it would be. So then define Nabla by this formula. Show it gives a connection, in other words, the R linearity plus 1 and 2 and show LCA and LCB. Okay, and most textbooks, when they, when they argue about this proof, they'll, they'll just stop here and they'll say okay, so there, that's it. Right? But you really do have to do this. You have to show just because it's uniquely determined doesn't mean it exists. Okay, so the Levi civil connection always exists.
00:40:47.195 - 00:41:22.945, Speaker A: The main properties we'll be using is the properties I wrote down the R linearity, this property, this property, the metric compatibility and the torsion freeness. And you'll see we're going to be using all of them frequently. So as I said, if you've never seen connections before, it doesn't matter. All you need to take is this theorem, that this exists and it has these properties. Yes, I'm going to say. I'm going to say that now. Yes, so.
00:41:22.945 - 00:41:56.275, Speaker A: So here's what I wanted to say, actually, before I answer your question. I'll do it in a second. I wanted to say so remarkable. From 1 and 2, it's easy to show. It's not hard to show that Nabla X Y at P. This is, this is a vector field. So at P, this is in TP of M.
00:41:56.275 - 00:42:57.385, Speaker A: This depends only on X at P and on Y in a, an open neighborhood of P. Okay, so we say it's point wise in X or tensorial in X, and we say it's local in Y. Right. Its value at a point P only depends on the value of X at the point P and it only depends on the values of Y in some arbitrary open neighborhood of P. Doesn't matter what's happening far away. And the way this is done, this uses smooth bump functions. So see for example, Lee's book, Lee's book on Riemannian geometry.
00:42:57.385 - 00:43:42.015, Speaker A: It's probably also in docarmo, but Lee certainly does it in a way that's more transparent, more easy to understand. So these are, this is really what's saying that it's a. Every, every derivative, every directional derivative should only depend on the point. And the direction shouldn't depend on the fact that X was a vector field at all. We really just need a tangent vector at a point to pick a directional derivative. And if it is really differentiating something, it should only depend on the thing we're differentiating in an arbitrarily small open neighborhood at that point. Okay, so before I get to your question, there was one more thing I wanted to say a little bit more precisely, in fact.
00:43:42.015 - 00:44:35.295, Speaker A: So for this reason, for this reason, we often write NABLA XPY for NABLA XY at P, right? Because this is reminding us that all you need is xp. You never even needed X to be a vector field. Just take a tangent vector at P and this has to be defined in an open neighborhood of P. So it's a vector field defined on some open second, any P. Then we can evaluate this and it's a tangent vector at P. And even more precisely, even more precisely. Just want to make sure I say it the way I wanted to say it.
00:44:35.295 - 00:45:52.785, Speaker A: Yes. Suppose Y and Y tilde are vector fields on M or just on an open neighborhood of P. And let alpha be a smooth curve on M defined on some open interval containing the Origin such that alpha at 0 is P and alpha prime of 0 is XP. Then. Then nabla XP of Y is the same as Nabla XP of Y tilde. In other words, it's true that it doesn't just depend on Y at the point P, it depends on Y in a neighborhood of P. But all it really cares is the values of Y on an arbitrary curve passing through P with initial velocity xp.
00:45:52.785 - 00:46:53.225, Speaker A: So Nabla X P, Y only depends on the values of Y along an arbitrary curve passing through P with initial velocity xp. So again, you can see. Lee, for a proof of this, draw a picture and then I'll answer Daniel's question. Right. Daniel, I'm going to get everyone's name at some point. I didn't want to erase this. What's that? Alpha is the initial velocity Alpha prime of 0 is XP.
00:46:53.225 - 00:47:54.027, Speaker A: I'll draw the picture. No, I'm saying if you want to compute this guy right, take any curve with initial point P and initial velocity XP and look at Y only along that curve. And that's all that matters. So the picture. If this is the point P and this is XP sitting inside tpm. Okay, Then I can look at alpha passing through P and having initial velocity xp, I can also look at some other curve. Let's say Beta with also initial point P and initial velocity xp.
00:47:54.027 - 00:48:34.405, Speaker A: And to compute Nabla XP of Y, it depends only on the values of Y along alpha on an arbitrary open neighborhood of zero. Right. This. This passes through at time zero. So in other words, these are both curves with. Well, maybe I shouldn't have. Rather than draw.
00:48:34.405 - 00:49:20.235, Speaker A: Rather than draw two curves, suppose the vector field Y. Oh, wait. Sorry. Yes, sorry again. That's the case where I thought I had written it down, but I hadn't. Such that this with Y compose alpha equals Y tilde compose alpha one minus epsilon epsilon. So I, E, Y and Y tilde agree on alpha.
00:49:20.235 - 00:49:55.755, Speaker A: That's what I thought. I had written that down. Thank you. So in other words, if I have this red vector field which is defined not just along the yellow curve, but in an open neighborhood of this point P, and if I have some other vector field Y tilde which agrees with the. With the red one on the yellow curve, but they might disagree away. Right? So on the yellow. I've lost my pink chalk.
00:49:55.755 - 00:51:00.139, Speaker A: Right away from this curve, they might not agree these two vector fields. But on this curve, and in fact on an arbitrarily small neighborhood of the origin, this curve is defined in minus epsilon epsilon. As long as they agree on that curve, then these covariant derivatives would be the same. So in this sense we say it's a directional derivative. So this justifies calling Nabla XP Y the directional derivative of Y at P in the XP direction. Okay, so I'm going through all this quickly without really showing you the details, because as I said, it's supposed to be review for most people, and if it isn't, you'll. You'll fill in the gaps yourself.
00:51:00.139 - 00:52:15.975, Speaker A: But now let me answer Daniel's question. Why are these the right conditions for the Levi Civita connection? So, motivation Y is metric compatibility LCA and torsion free lcb. I'm not going to tell you why it's called torsion free. Why are these the right conditions? Well, what's the simplest Riemannian manifold? The simplest manifold is rn, and RN already has an inner product, natural inner product on it. And because every tangent space in RN at every point in rn, every tangent space is canonically isomorphic to RN itself. We can just put that same standard Euclidean inner product everywhere. Okay, so on rn, there exists a canonical metric G.
00:52:15.975 - 00:54:06.515, Speaker A: The Euclidean metric, which is G of x P Y P is equal to the sum From k equals 1 up to n X, k, y, k if x is xi ei, y is yj ej, where e 1 up to en is d by dx, 1 up to d by dxn is the standard global frame. So in RN we have global coordinates. Standard, the standard basis gives us global coordinates. And then we just say the inner product is defined by saying that this frame at every point is going to be declared to be orthonormal. That's the standard inner product. Okay, and if we do that, then if you compute what you get for the levy, that covariant derivative in this case, and we'll see it explicitly, I think, before the end of the lecture. So for this metric G on rn, the lebitvita covariant derivative is Nabla X Y, which is Nabla X of Yi ei.
00:54:06.515 - 00:55:06.949, Speaker A: Let me call them K is just X of YK ek. So this is an exercise and this is exactly what we want directional derivative to be. In rn, we know where the directional derivative is of a real scalar valued function from calculus. And if we have an RN valued function, then we just think of it as an N tuple, because we have this global frame, we just think of it as an N tuple of scalar valued functions, and we're just taking the directional derivative of each component function. Okay, so this is the sensible notion of directional differentiation in rn. And the reason we have a sensible notion is because we have a global frame in general, on most manifolds, you don't have a global frame, okay? And if you do have a global frame, you can, you can choose a metric by declaring that to be orthonormal. So I don't know if that answers your question, but I could have gone the other way around.
00:55:06.949 - 00:55:49.119, Speaker A: I could have said it makes sense to define directional differentiation in RN by this formula. And then if you compute, if I defined it by this formula, it's metric compatible and it's torsion free. I went the other way. I said, suppose you took the definition we gave of levitic connection and apply it to this metric. This is what you'll get. And this makes sense, right? But we could have said this is what we want directional differentiation to be in rn, and you would have found out that it's metric compatible and torsion free. So you would say, those look like nice conditions, right? And then the nice thing about those conditions is that if you impose them both, you get existence and uniqueness, right? If you don't impose both of those, you might, you might not get uniqueness.
00:55:49.119 - 00:56:40.535, Speaker A: And if you try to impose some other things, you might not even get existence. Is that good? Okay, how are we doing on time? Okay, the clock is right. All right, so right as. I'm just, it's worth repeating like, I'm going through things quickly here because this is not really what I want to be giving a course about, right? But I need to state it all to make sure we're all on the same page. I fix notation to remind you what these objects are. But probably after the second lecture, when I'm not going to be assuming anyone had seen anything, I'm going to be going. I'm going to be, you know, crossing all the T's and dotting all the I's.
00:56:40.535 - 00:58:02.585, Speaker A: Okay? So because covariant differentiation is local, we can characterize it, or let's say describe it completely in terms of local frame. So what do I mean by that? I E. Suppose E1 up to EN is a local frame for the tangent bundle over some open set U. Then I can take EJ and covariantly differentiate in the direction of ei. So what does this mean? I told you, these have to be vector fields on all of them. But if I want to evaluate this at some point, P in U, all of this should matter is how, what the value of this is at P, and the value of this on an open neighborhood of P. So I can take U.
00:58:02.585 - 00:58:42.451, Speaker A: U is an open neighborhood of P. Then this at P is well defined. So in other words, I mean, it's one of these facts I wrote down without proof. If you, if you're happy, if it makes you happier, you can extend these to all of them in any way you want. Using smooth cutoff functions, extend them by zero, like choose some smaller open set contained in U, which is a neighborhood of P that has closure contained in U, and then cut it off to zero outside and extend it by zero. And then you can define this with global vector fields. Okay, but the point is we don't need to do that because connections are local.
00:58:42.451 - 00:59:38.459, Speaker A: So Nabla EI EJ is some smooth vector field. Well defined smooth vector field on U. Because I showed you that for any point in U we can, we can make sense of this. And therefore, since it's a vector field on U, I can expand it in terms of the local frame. So these gamma kij are smooth functions on U. These are called the Christoffel symbols of Nabla with respect to this local frame. Okay? So whenever you have a connection, this doesn't even need to be the Leviticus connection, but we're only going to work with the Leviticus connection.
00:59:38.459 - 01:00:15.863, Speaker A: For every choice of local frame, you get these Christoffel symbols. So these are smooth functions. They have one superscript and two subscripts. And so you might think that it's a 1, 2 tensor, but it's not. So note, these are not the components of a 1, 2 tensor. And the reason they can't be is because, remember we had this formula is good. This tells you that it is tensorial in X.
01:00:15.863 - 01:01:23.467, Speaker A: But this formula says that we have a term where we differentiate the Y. So because of this it's not tensorial in Y. And that's why the local frame description of the connection is not a tensor. Joe, question or you just. Okay, and so as a remark for the Euclidean metric on RN with respect to the standard global orthonormal frame, EI is D by DXI. The standard coordinate chart we have gamma Kij is 0. OK? And you can see that here it says that if you want to know how this vector field is changing in the X direction, it's only if the coefficient functions are changing.
01:01:23.467 - 01:02:18.549, Speaker A: These guys are not changing. In other words, this is another way to describe what is this Levy, the connection of the Euclidean metric. It's exactly the connection for which the standard global orthonormal frame doesn't change. Okay, but even if you're looking at the Euclidean metric, but you took some other local frame which is not this global orthonormal frame. The Christoffel symbols might be different. So even for the Euclidean metric, the gamma kijs will in general be non zero for other local frames. And if you've never done this exercise, a good way to really understand what these things are.
01:02:18.549 - 01:02:49.777, Speaker A: Take the standard euclidean metric on R2 and write it down in polar coordinates and the D by Dr. D by Dr. And D by D theta, that's a frame that's defined everywhere except the origin. Right? Because R doesn't make sense there. So if you take R2 minus the origin and you look at the Euclidean metric there, you have this frame. In fact, if you put an R here, then this is orthonormal with the Euclidean metric. But it's not.
01:02:49.777 - 01:03:18.566, Speaker A: You can see it moves. Right? If I'm over here, D by Dr. Is there and RD by D theta is there. So this is not. Somehow this frame is moving, is changing as we move at different points on the manifold. Meanwhile, the E1, E2 frame is the same everywhere. Okay, so if you work out what the Christoffel symbols are for the euclidean metric on R2 in polar coordinates, you'll see they're not zero.
01:03:18.566 - 01:04:35.855, Speaker A: Okay, about 10 minutes. Okay, so this is what. Where am I? Okay, so the gamma is. Well, the gammas tell us how the frame E1 up to EN is changing in the directions of the E1 up to EN. And that's enough to encode the whole, the whole description of the connection in that frame. Right, so now we can see. So if you know the Christopher symbols, if I take any vector field, I can write it as Y YJ ej.
01:04:35.855 - 01:05:07.905, Speaker A: And if I take Nabla X of Y, that's going to be Nabla X of YJ EJ by. There's a sum over j here. But this is linear R linear. So I can move. I'm not even writing the sum symbol. I can pull it out, right? And by the, by the Leibniz rule, I'm going to have a term where I differentiate this function and do nothing to this vector field. And then a term where I do nothing to that function and I differentiate this vector field.
01:05:07.905 - 01:06:41.683, Speaker A: And if I write X as XI ei, then I can write this as X applied to YJ EJ plus XI YJ gamma kij ek. And let me change my dummy index of summation here. So this tells me that Nabla xy, if I expand it in this frame, it's going to have some kth component times dk, where this kth component is just going to be obtained by taking the directional derivative in the x direction of the kth component of y + this other term gamma kij xi yj, which is encoding the fact that the frame is also changing as we move along. Ok, so that's the description of the covariant differentiation in this local frame. All right, let me give you one more formula. So I'm going to recall the Kozul formula, which I can't recall, so I have to look it again. There you go.
01:06:41.683 - 01:07:59.981, Speaker A: It said that two nabla xyz is x y z plus y XZ minus z XY minus XZ y minus x plus xyz. Okay? So that's the formula that the Leviticus connection has to satisfy. And I argued that that uniquely determines it. Suppose E1 up to EN is a coordinate frame. Then this formula actually tells us exactly what the Christoffel symbols are. Because if I take let X be which d by dxi, Y be d by dxj and z be d by dxk. Then I've got nabla d by dxi applied to d by dxj that's equal to gamma ij.
01:07:59.981 - 01:08:49.085, Speaker A: Let's say M d by dx m sum over m. So if I put that in here, the first thing to note is that all the lie brackets will go away because the lie bracket of coordinate vector field is zero. That's the advantage of a coordinate frame. And what's this? If y is d by dx j and this is D by dxk, this is just gik. So we get. What do we get? We get two nabla I j m g mk on the left. And here we get d by dxi of gjk plus d by dxj of gik minus d by dxk of gij.
01:08:49.085 - 01:09:44.587, Speaker A: Okay? And from here you can solve for the gammas, because this is an invertible matrix. It's a positive definite matrix. So you end up getting the Gamma Kiji is 1/2 gkl dgil dxj + dg jl dxi - dgij dxl where gij upper ij is the inverse matrix of little gij. In other words, g upper I k g lower kj is delta I j and we know it's invertible because it's a positive definite matrix symmetric matrix. And the inverse is also positive definite symmetric. So there's a method to calling it this, because if you know about fiber metrics on Vector bundles. This is the local coordinate.
01:09:44.587 - 01:10:25.045, Speaker A: This is the components of a positive definite fiber metric on the cotangent bundle in the dual frame. But we're not going to be talking about that metric on the cotangent bundle, so don't worry about it. So don't, you know, don't memorize this formula. There's no reason to. But it shows you that the Christoffel symbols in local coordinates are obtained by taking some kind of derivatives of the metric coefficients and then multiplying by the inverse of G. And you know that the inverse of a matrix. There's a formula from, what's it called, Kramer's Rule, that this thing is the adjugate, the classical adjoint divided by the determinant.
01:10:25.045 - 01:11:05.803, Speaker A: So each entry here is a homogeneous polynomial of degree n minus 1 in the entries of G divided by the determinant, which is a homogeneous polynomial of degree n in the entries of G. So it's linear in first derivatives, but it's very nonlinear in zero derivatives. And so this is very nonlinear in the entries of G. It doesn't look that bad. It looks like it's quadratic, but that's an inverse. So that's a horrible thing. And when we define curvature next week we'll see that curvature involves taking a derivative of this plus a square plus adding a square of this.
01:11:05.803 - 01:11:42.145, Speaker A: So it's even more nonlinear. And that's why Riemannian geometry is difficult, right? Because the expressions you have are fully nonlinear, partial differential expressions. Oh, okay. Probably does not make sense for me to make the next definition because I got four minutes. So maybe what I'll do is I'll take two minutes to just briefly tell you what's coming next. I got halfway through what I thought I would get through, which I knew was going to happen. That's fine.
01:11:42.145 - 01:12:51.931, Speaker A: Okay, so let me just say I was hoping I could do two. Two lectures to get through the sort of background, but it'll be. It'll be three and a half, probably, so next time. So vector fields along the curve, covariant differentiation along a curve. And this will be crucial for everything we do in the rest of the term parallel transport and then geodesics and basic properties. So let me just say in words, from now on, we're going to have a fixed Romanian metric on our manifold, and that determines this levitic covariant derivative connection. We're only going to be using that sometimes.
01:12:51.931 - 01:13:29.165, Speaker A: We'll have two Romanian manifolds, each with their own metric, each with their own covariant derivative but we're not going to be changing metrics on the same manifold. And so given that, there's going to be this way of taking given a curve and given a tangent vector at the initial point of that curve, of transporting it along the curve to get a tangent vector at the final point of that curve in a way which is somehow constant. It's not. It's only changing because it sort of has to change on a curved manifold, but no more than that. That's called parallel transport. And that's going to be a first order linear ode. Now we're going to talk about a geodesic.
01:13:29.165 - 01:14:08.837, Speaker A: So here the curve is fixed and then for every initial, any tangent vector at the initial point, we get a tangent vector. The final point here we're going to fix an initial point and initial velocity, a tangent vector at that point. And we're going to look for a curve which has the property that it starts at that point and its initial velocity is that initial tangent vector. And it's always parallel along its own along itself. Its velocity is parallel along itself. That means the velocity vector field is only changing by the amount it's sort of forced to change by the curvature of the space and no more. That's called a geodesic.
01:14:08.837 - 01:14:44.519, Speaker A: And that's going to be a second order ode, but a nonlinear ode. So that's what we're going to talk about on Monday. I think that's easily going to fill up the 80 minutes. And then on Wednesday we'll talk about curvature, Riemann curvature, sectional curvature, rigid curvature, scalar curvature. So I think in those three lectures, and possibly another 20, 25 minutes of the fourth lecture, we'll go over all this material. So this is chapters one, two and four of Del Carmel. That's the stuff I'm going through quickly.
01:14:44.519 - 01:15:06.815, Speaker A: And then when we do 3 and 5 to 13, we're going to go slow. Okay, that's the plan. And the first assignment probably won't show up until the third week because I'm not going to be giving questions on this sort of basic stuff. So there'll be five assignments about every two weeks starting in week three. Any questions?
