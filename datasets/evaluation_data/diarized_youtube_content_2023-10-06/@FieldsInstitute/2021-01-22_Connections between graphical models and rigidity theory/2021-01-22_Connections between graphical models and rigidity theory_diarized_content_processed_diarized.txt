00:00:00.720 - 00:00:28.324, Speaker A: So at the end of Caroline's talk yesterday, I think will Travis asked like, so what's the connection to rigidity theory? And this is the answer to that question. And most of what I'm going to present can be found in more detail in a paper of gross and Sullivan. Oh, are you writing, Daniel? Yeah. Oh, it's white on black. Sorry, I just got confused. Okay. Yeah.
00:00:28.324 - 00:01:07.024, Speaker A: And the paper is called the maximum likelihood threshold of a graph. So. And I should say at any point, please feel free to stop me. I don't know. I know there's like a ton of different backgrounds here and. Yeah. So please feel free to stop me if anything's confusing.
00:01:07.024 - 00:04:15.914, Speaker A: But anyway, the setting and some of this is going to be review from Caroline's talks, but the setting is in that of gaussian graphical models. And these are going to be multivariate normal distributions will have mean, which I'll denote by mu, and covariance matrix denote by Sigma. And given a graph g, the corresponding graphical model Mg is the set of multivariate normal distributions mu sigma, such that sigma inverse ij is equal to zero if and only if ij is not an edge of g. So for example, there, if g is this four cycle. An Mg is the set of multivariate normals with covariance of the form x one one, x one 20, x one four, x 22, x two 30, x three three, x three four and x 44. And. Okay, these covariance matrices are, of course, symmetric.
00:04:15.914 - 00:04:29.134, Speaker A: I'm sorry. I'm sorry. With not covariance matrix with inverse covariance matrix.
00:04:33.114 - 00:04:40.534, Speaker B: And Daniel. Yeah, so we can just ignore mu because we can take it to be zero, comma zero, comma zero, comma zero.
00:04:42.434 - 00:04:58.318, Speaker A: Yeah, yeah. Uh, you could ignore it. Yeah, I guess, you know, the, we're, we're going to actually think about like possibly having data. So technically, you know, it might not be zero, but, but you could, uh.
00:04:58.366 - 00:05:01.102, Speaker B: Re, you could recenter at all without.
00:05:01.198 - 00:05:05.142, Speaker A: Yeah, yeah, it doesn't, it doesn't really come into, it doesn't come into the picture, really.
00:05:05.318 - 00:05:06.486, Speaker B: Okay, thanks.
00:05:06.670 - 00:06:35.124, Speaker A: Yeah, no problem. And so, and I think Caroline mentioned this, but just so we're all on the same page, the significance of a graphical model is that if you have random variables, x one through xn, that are distributed according to some distribution in this graphical model. So I am abusing the hell out of this notation. Mg is a bunch of probability distributions, not just one, but if x one through xn satisfy probability distribution from this graphical model, then xi is conditionally independent of xj given the set of all other random variables. So that's the sort of intuitive meaning of these constraints in a graphical model.
00:06:36.424 - 00:06:38.016, Speaker B: Can I ask another question?
00:06:38.120 - 00:06:38.856, Speaker A: Yeah.
00:06:39.040 - 00:06:47.404, Speaker B: So this is taking the positive semidefinite cone and slicing it with a linear space.
00:06:48.264 - 00:07:15.684, Speaker A: Yeah, so, yeah, yeah, yeah. So the inverse covariance, yeah, the model you can think about taking. Yeah, the positive definite cone. Positive semi definite cone. Well, positive definite positive definite cone, and then slicing it with linear constraints of the form certain entry zero. And this gives you all inverse covariance matrices in the model.
00:07:18.664 - 00:07:26.050, Speaker C: Could I ask a question there to follow up? Why is it positive semi definite? I see, it's symmetrical.
00:07:26.072 - 00:07:48.794, Speaker A: But so covariance matrices in general have to be positive semi definite. And the reason is just that, I mean, if you just look at how they are defined, that just sort of falls out from the definition. So, like.
00:07:51.594 - 00:07:58.994, Speaker C: Um, because it's, it's, it's this variance business. It's, it's x times x transpose or something like that.
00:07:59.114 - 00:07:59.698, Speaker A: Uh, yeah.
00:07:59.746 - 00:08:00.010, Speaker C: Okay.
00:08:00.042 - 00:08:00.322, Speaker A: Yeah.
00:08:00.378 - 00:08:04.294, Speaker C: Okay. So that the, it comes for the covariance, not from the structure.
00:08:04.994 - 00:08:11.094, Speaker A: Yes, yes. Yeah. It just comes, yeah, yeah, yeah. Any other questions?
00:08:11.514 - 00:08:20.714, Speaker D: So this is a silly question because my eyesight's not great. What does it say in the little, uh, curly brackets? Set notation.
00:08:21.374 - 00:08:28.874, Speaker A: Oh, that's not your eyesight, that's my handwriting. Xk such that Xi and Xj are not equal to xK.
00:08:29.174 - 00:08:37.354, Speaker D: So the multiplication of the two are not equal to, or consider those two things. So neither are equal to xn.
00:08:41.054 - 00:08:56.614, Speaker A: So what this is saying is that if. Oh, sorry, I'm missing something whenever I j is not an edge of the graph.
00:08:58.154 - 00:08:59.946, Speaker D: Okay, sorry.
00:09:00.010 - 00:09:11.814, Speaker A: Yeah, yeah. Xi and xj not being an edge, is equivalent to the two corresponding random variables being conditionally independent given all the other variables.
00:09:12.744 - 00:09:15.084, Speaker D: Okay. All right, thank you. Cheers.
00:09:16.064 - 00:09:19.144, Speaker A: Alexander asked. These are inverses of covariance matrices.
00:09:19.184 - 00:09:21.324, Speaker B: So should they be negative semi definite?
00:09:22.064 - 00:09:27.176, Speaker A: No, no, they're still. So the inverse of a positive definite matrix is still positive definite.
00:09:27.240 - 00:09:27.704, Speaker E: Oh yeah, yeah.
00:09:27.744 - 00:09:28.964, Speaker A: Okay, never mind. Sure.
00:09:29.264 - 00:09:30.044, Speaker C: Thanks.
00:09:36.024 - 00:11:54.864, Speaker A: Any other questions? Okay, so this is the general thing that statisticians in some areas care about. Here's the problem. Given data x one through xk in rn, supposedly generated according to some graphical model mg, find the maximum likelihood estimator for the covariance of this distribution. So just unpack this a little and scroll to this example. So the setting here is you suppose you already know what the graph is and you want to find the actual values of the covariance matrix that best explain your data. So if you're, you know, graph is this four cycle, you have some data, you know, say 20 vectors in r four, and you want to find the covariance matrix of this particular form that is most likely to have generated those 20 vectors in r four. So, so now there are two regimes here.
00:11:54.864 - 00:14:06.888, Speaker A: The first regime is when k is greater than or equal to n, and this is easy, or at least always possible. So just for example, if the graph is just the complete graph, so you're just finding the best fit multivariate normal, no model other than the fact that you're normally, then the MLE is the following, it's just the sample covariance matrix. And yeah, this is where mu hat is just equal to your sample mean. So this is not the regime that I want to think about. I want to think about regime two where k is going to be less than n, and in applications where this comes up generally much less than n. So the example to keep in mind here is one in say, genetics or genomics, where you're trying to fit a graphical model to some network of genes in an entire genome, say, well, this means you have on the order of tens of thousands of random variables, but you're not collecting that many samples. So this is sort of the idea to keep in mind.
00:14:06.888 - 00:15:06.564, Speaker A: And then here the MLE may not exist. So a question that arises for a fixed g, what is the minimum k such that the mle in this graphical model exists for any set of generic data points, x one through xk. So does the, does the question make sense?
00:15:12.424 - 00:15:14.472, Speaker C: I have several questions, actually.
00:15:14.648 - 00:15:16.770, Speaker A: Yeah, yeah, please ask away.
00:15:16.872 - 00:15:25.006, Speaker C: Um, first of all, the maximum likelihood estimator is, is solving some kind of optimization problem, right?
00:15:25.150 - 00:15:25.614, Speaker A: Yes.
00:15:25.694 - 00:15:29.994, Speaker C: What exactly are we, are we optimizing again?
00:15:31.494 - 00:17:07.800, Speaker A: Yes. So in general, if you have, you have some model m and you have. So, so by model, I'm just thinking of a set of probability distributions and you have some data vector that's generated according to some probability distribution in this model you want to find, okay, you're going to look at the probability that you observed x given that x is distributed, or, sorry, the probability that you observed x one, given that x one is distributed according to some distribution d times the probability that x two that you observe x two, given that x two is distributed according to that same d, dot, dot, dot times the probability that you observe x k. Um, k is distributed according to the probability d. So here, I guess sort of implicit is, I'm assuming that my data is generated. Um, is I, is independent and identically distributed according to some distribution d that sits in the model. I don't know what it is yet.
00:17:07.800 - 00:17:30.863, Speaker A: So what I'm going to do is I'm just going to maximize this function over the set of all probability distributions in the model. So d. Yeah, so I guess maybe just to write this out more formally, the optimization problem is to maximize this subject to the constraint that d is in the model.
00:17:32.323 - 00:17:57.634, Speaker C: Okay, that makes sense. And now I have a follow on question, which is that this is obtaining a single d that is going to maximize this quantity. But I've always kind of wondered whether what that means for other distributions that are close to D. For instance.
00:17:59.694 - 00:18:00.030, Speaker A: The.
00:18:00.062 - 00:18:59.324, Speaker C: Data that we got was just a sample, and it's likely that if we sample again, we're going to get a different collection of data and get a different MLE. I'm curious about whether the mles that we get from re samples are in some sense close to each other or expected to be close to each other. I can imagine situations where the MLE where m is such a family that there's a big high spike for probability at one particular d. But if you move to a very far away other collection of D's, there's a whole bunch of D's that get just a little bit below that, but they're all close together, and somehow that would make more like, that would be a better predictor somehow.
00:18:59.744 - 00:19:04.924, Speaker A: So you're saying basically, what if this optimization problem is not convex?
00:19:05.384 - 00:19:08.004, Speaker C: Yes. Is this a convex problem?
00:19:08.544 - 00:19:12.004, Speaker A: Yes. Excellent. Yeah, yeah.
00:19:13.024 - 00:19:13.844, Speaker C: I see.
00:19:14.304 - 00:19:32.284, Speaker A: Not in general. Not in general. There are two things that are key here. One, we're dealing with normal distributions, and two, the restricted space of normal distributions is a linear is made by taking linear slices of the inverse covariance matrices.
00:19:34.544 - 00:19:41.720, Speaker C: I see. So we're using the structure of mg strongly here. Great.
00:19:41.832 - 00:19:43.044, Speaker A: Yeah, yeah.
00:19:43.744 - 00:19:47.986, Speaker C: Thank you. That really answers an old question of mine. Thank you.
00:19:48.080 - 00:20:05.234, Speaker B: Okay, can I, can I make an additional comment? This is Alex. That question just made me think of, I mean, if we perturb our data a little bit, the MLE will in general also move a little bit, correct?
00:20:05.534 - 00:20:05.982, Speaker A: Yes.
00:20:06.038 - 00:21:03.804, Speaker B: But an interesting question is sometimes you perturb your data and the MLE won't move at all. And so, like. Okay. Because one thing I've studied is something that we called logarithmic voronoi cells, and these are just the preimage of this MLE estimator. So, like, given a specific point on your model, given an inverse covariance matrix with zeros in the right spots, what are all the data vectors that would be mapped to that matrix? And so the dimension of that set of data vectors would somehow tell you how resistant to noise your situation is. Anyway, this is just what Will's question made me think of, so interesting.
00:21:04.824 - 00:21:18.940, Speaker A: Cool. Yeah. So I'm about to distill this into just completely mathematical questions. So if there are any more questions on the statistics side of things, let me know.
00:21:19.132 - 00:21:49.824, Speaker B: Okay, I did have one more question. I'm sorry, I'm speaking a lot. I was wondering if you knew of an example where you can prove the MLE does not exist. I assume that you need some sequence that goes off to the boundary, but I don't really know how this would look. I was wondering if you knew an example k much less than n, and the MLE does not exist. And we can be confident that it doesn't exist because something.
00:21:51.524 - 00:21:58.544, Speaker A: I don't have one ready to go, but that would not be difficult to find. I can get back to you on that.
00:21:58.884 - 00:22:00.448, Speaker B: Okay, great, thanks.
00:22:00.636 - 00:22:01.364, Speaker A: Yeah.
00:22:02.384 - 00:22:14.484, Speaker C: One more question about regime one. Yeah, yeah, your example has the complete graph. If the graph isn't complete, what happens?
00:22:16.744 - 00:22:47.264, Speaker A: So I don't actually know what the specific thing you would do in that case is, but there, I mean, yeah, this, this, there's, I'll have to look a little to find the exact thing, but there is a thing, and it's not, it's, it's not particularly like difficult or surprising, I guess, but I don't remember it off the top of my head.
00:22:48.924 - 00:22:49.984, Speaker C: Okay, thanks.
00:22:54.344 - 00:25:22.758, Speaker A: Any other questions? All right, so yeah, we're in regime two, and we just want to know, for a given graph, what is the smallest k such that the MLE exists? And spoiler, you can use the smallest d such that your graph is rigid in the d dimensional rigidity matriid to give you an upper bound on this number. Um, so, and yeah, that, that in a nutshell is the connection. And now I'll get to this more explicitly. Um, just, but first, so an equivalent question to the one that I posed here that now gets rid of all the statistics. Um, oh, and, and the fact that this is equivalent is due to, um, Dempster in 1972. And it says, for a graph g on n vertices, find the minimum k such that for almost all PSt matrices a of rank k, there exists a positive definite so full rank matrix b such that a equals bij for all, for all edges ij of G. So now we're asking the question.
00:25:22.758 - 00:26:18.306, Speaker A: Given a graph g on n vertices, find the minimum k such that for almost all positive semi deferent matrices of rank k, there exists a positive definite matrix of b such that it agrees with this positive semi definite matrix on the entries corresponding to G. But then you're free to just change the other entries. So this becomes a matrix completion problem where it's sort of like the opposite of what you usually think about in matrix completion, where now we have a low rank PSD matrix, some of the entries are fixed, namely the ones corresponding to entries of G. Sorry. Oh, and the diagonals, sorry. We have a low rank matrix. We fix, lowering PSD matrix, we fix the diagonals, we fix all the entries corresponding to g, and then we let the others vary.
00:26:18.306 - 00:27:52.534, Speaker A: And we want to complete this matrix so that it's full rank and still positive. Definitely. And so here's a definition. The minimum k from the above problem is called the maximum likelihood threshold of G, and we'll denote it mlt of G. And now here's the theorem just giving this connection between rigidity and graphical models. Let G be a graph. Then G is independent in the d dimensional rigidity matrix.
00:27:52.534 - 00:28:43.018, Speaker A: I'm sorry. Then, sorry. There's an if there. Then if G is independent in the d dimensional rigidity matriid, then the maximum likelihood threshold of G is less than or equal to D plus one. So in particular, Laman graphs correspond to graphical models where you need, where three data points are sufficient. So this is only an upper bound. So you could perhaps require fewer.
00:28:43.018 - 00:28:55.774, Speaker A: But if you're trying to fit a graphical model where the graph is a Le mans graph, you only need three observations to guarantee that your maximum likelihood estimate exists.
00:28:56.954 - 00:29:08.254, Speaker B: Daniel Kaisk again, this would be a graph on, what was it, 10,000, 10 million nodes for the gene problem.
00:29:08.754 - 00:29:10.002, Speaker A: Yeah, could be.
00:29:10.098 - 00:29:17.734, Speaker B: Okay. And you'd only need three samples to fit your model.
00:29:18.354 - 00:29:27.514, Speaker A: If the gene network happens to be a lemon graph. Okay, which might be unlikely, but it's still pretty cool.
00:29:27.554 - 00:29:28.624, Speaker B: You only need three.
00:29:28.794 - 00:29:31.224, Speaker A: Yeah, yeah, yeah it is.
00:29:33.124 - 00:29:43.436, Speaker C: So, remind me again that the d dimensional rigidity matriid, when is, what does it mean again, for g to be.
00:29:43.460 - 00:30:12.856, Speaker A: Independent in that matrix, it means that you can, if you think about your edges in it as like distance constraints, it means you can basically arbitrarily set the distances between those points indicated by the edges in the graph, and there will be a way to fill in the remaining, or there will be a, this actually corresponds to like a point configuration. I see.
00:30:12.880 - 00:30:23.980, Speaker C: So if I realize the graph in d dimensional space with just a generic realization, then it will be rigid no, no, no.
00:30:24.052 - 00:31:19.730, Speaker A: It means actually, okay, let's think about it from like a matrix completion perspective. That's a little bit more natural. So think about a euclidean distance matrix where you are only observing the entries that correspond to the edges of a graph that's independent in the rigidity metroid. Well, then if. Yeah, if this graph is independent in a d dimensional rigidity matriidal, then if you plug in generic entries to this like partial euclidean distance matrix, then you can complete it to an honest euclidean distance matrix. Actually, honest is maybe not the right word because this is over the complex numbers. But does that make sense, Dan?
00:31:19.762 - 00:31:26.014, Speaker D: It might be easy to relate it back to stresses on the framework or something like that.
00:31:28.314 - 00:31:32.106, Speaker A: Do you want to do that, Sean? This, I think, is a language you're more comfortable in.
00:31:32.250 - 00:31:55.874, Speaker D: So you would say that it's going to be independent in the d dimensional rigidity matriid. Basically, if you embed it generically into d dimensions, every bar is effectively doing something. So you have no redundant bars. So the removal of any bar will affect how it flexes. Say.
00:31:59.334 - 00:32:01.314, Speaker C: Are you saying that it's minimally.
00:32:02.534 - 00:32:18.054, Speaker D: It doesn't have to be minimally rigid. It could be even less than that. So say if you have a tree, you could remove a bar and you're going to have all the tree flexes you had previously, but now you can separate it into two sections and move it around so you got more motions than you had previously.
00:32:18.434 - 00:32:23.114, Speaker C: I see, I see. Okay. That makes good sense. Thanks.
00:32:23.274 - 00:32:24.374, Speaker D: Yeah, sure.
00:32:29.834 - 00:33:06.642, Speaker A: So, yeah, and then I guess the rest of what I have planned is basically just to sort of sketch, sketch the proof of this. Yes. Are there any other questions about the theorem itself? Okay, so just real quick, I'm going to introduce some. Dan, could I ask a question? It's Bill. Yeah. Hey, Bill, do you mean really main independent, because the antigraft would be independent or a graph with one edge would be independent in the theorem. Sorry, sorry.
00:33:06.642 - 00:33:25.454, Speaker A: Oh my God. Yeah, yeah, sorry. Yeah. Um. Yeah, yeah, yes, I do. I do mean independent. Um, so the bound is bad.
00:33:26.774 - 00:33:39.234, Speaker F: It says that you can get a good bound if the graph is actually minimally rigid. For the case, for the cases that Bill mentions, that just says the bound still holds, but it's not that good.
00:33:40.014 - 00:33:45.634, Speaker B: Yeah, so, like, edge would just have two variables, right?
00:33:45.974 - 00:35:09.574, Speaker A: Yeah, but, okay, so like for. So, okay, this means that like, if your graph is just, is just like an independent set of vertices, a set of isolated vertices, then you only need one observation to complete it. But that maybe isn't so. Unintuitive, because the corresponding covariance matrices are just diagonal matrices. And then I guess here, I don't know, does that, does that, does that make sense, Bill? Not really, but I'll think about it then. Yeah, maybe another way to think about it is the fewer edges you have, the more constraints you have on your covariance matrix. So being independent in like, a really small rigidity matriarch dimension means that you have a lot of constraints, so you need fewer observations to be able to really shrink this space that you're solving this maximum likelihood problem over.
00:35:13.454 - 00:35:27.634, Speaker E: Can I make a point as well, Dan? I mean, in terms of the rank completion problem, if you had the empty set, that means that you could just start with some positive, semi definite matrix and you can just change all the entries to get a full rank matrix, right?
00:35:29.454 - 00:35:31.194, Speaker A: Yes. Yeah.
00:35:32.054 - 00:35:45.572, Speaker E: So the empty set, if you started. So the empty set would say, okay, you know, according to your other characterization, I can just change any entries. I want to get a full rank matrix. So you can just choose any matrix you want then.
00:35:45.748 - 00:35:50.904, Speaker A: Yeah, yeah, yeah, yeah. That's a good way to think about that. Yeah.
00:35:54.884 - 00:36:00.020, Speaker B: Dan, I typed some things in the chat. I'm just wondering if I'm understanding it correctly.
00:36:00.172 - 00:36:01.500, Speaker A: Okay, yeah, let me.
00:36:01.652 - 00:36:13.794, Speaker B: I said graph equals one edge. Then you'd have two by two PSD matrices. And so the MLE will exist when you have three samples, is the theorem.
00:36:15.294 - 00:36:17.574, Speaker A: No, the MLE will exist when you have two samples.
00:36:17.694 - 00:36:28.870, Speaker B: Okay, two samples. And then if your graph is the empty graph on n nodes, you have n by n PSD matrices whose every off diagonal entry is a zero.
00:36:29.062 - 00:36:29.862, Speaker A: Yes.
00:36:30.038 - 00:36:34.234, Speaker B: Okay. So again, the MLE will exist given a couple samples.
00:36:37.014 - 00:36:40.606, Speaker A: Given one sample.
00:36:40.790 - 00:36:46.634, Speaker B: Okay, just take. Yeah. What are the diagonal entries of the inverse matrix?
00:36:49.734 - 00:36:59.994, Speaker A: I mean, in this particular case, they would be one over the variances of each variable. They're all going to be independent now because you have zero covariance.
00:37:00.634 - 00:37:01.330, Speaker B: Okay.
00:37:01.442 - 00:37:02.250, Speaker G: Yeah.
00:37:02.442 - 00:37:04.534, Speaker B: All right, thanks. That was helpful.
00:37:08.274 - 00:37:38.754, Speaker A: But, yeah. So now just to briefly go through the broad ideas and the proof, just let me. May I ask one more question about this statement? So is there any example? The inequality is strict. Yes. And this is from a later paper. So. Yeah, I can dig that up for you if you want.
00:37:38.754 - 00:38:09.706, Speaker A: Yeah. So when Elizabeth and Seth published this paper, they did not have an example of where the MLE and this minimum thing were different. But, yeah, later, later that was found. Okay, thank you. Yeah, yeah. And by the way, if there are other questions sort of related to this. Yes.
00:38:09.706 - 00:39:37.354, Speaker A: If anyone here is, like, interested in real algebraic geometry, you know, the MLE is hard to understand, but I don't know. There's a lot that could be done there. But yeah. Anyway, any other questions? Okay, so just want to briefly introduce some notation that I'm going to use. So, given a polynomial map f from rk to rd, or, sorry, rn, let's use rn. Define m of f to be the matroid of linear independence on the Jacobian of f at a generic point. And there are two examples that are relevant here.
00:39:37.354 - 00:41:20.574, Speaker A: One, which is probably not surprising, I'm going to let fd n be the map from our dn to rn. Choose two, sending a configuration x one through xn to the set of their squared pairwise distances. Okay. Then of course, in this case, m of d. N is the d dimensional rigidity metroid. And then the second example is, I'm going to let g of rn. So here, g for gram is going to be the map from r to the rn to r to the n plus one, choose two.
00:41:20.574 - 00:42:45.114, Speaker A: And here, here I'm thinking about this space just as the, this space r to the n plus one, choose two. This is going to be the set of symmetric matrices where a set of points x one through xn now is going to map to just the inner product of each pair. Okay? And then in this case, m of grn is the matrix completion matriid for rank r symmetric matrices. Okay, and then finally, what is the.
00:42:45.274 - 00:42:56.194, Speaker C: Can you just help me? What's the second exponent on the r? What's the dimension of the image space here? Yeah, I just can't read it. Yeah.
00:42:56.274 - 00:44:20.004, Speaker A: Oh, n plus one, choose two. Ah, great. Yes. Yeah. So, so with this notation in mind, I can tell you what the, the main proof strategy here is. The first step is that if g union the set of all diagonal entries is independent in this matriid gr, then the maximum likelihood threshold of g is less than or equal to r. Now, I'm going to explain this in a minute, but then once you have that, then the second step is just to show that the matroid from grn mod the diagonal entries, or contract the diagonal entries, is isomorphic to this rigidity matriidal.
00:44:20.004 - 00:44:47.334, Speaker A: I'm sorry, I'm sorry, I have to get my indices right here. This should be d plus one, comma n, and then mf dn. Sorry, I'll write this a little bit neater. Okay.
00:44:49.234 - 00:44:51.094, Speaker C: What does contraction mean?
00:44:52.114 - 00:45:33.966, Speaker A: That means the set, your independent sets in this contracted matriid are all the independent sets that contain a basis of the set of things you're contracting by, and then you just remove anything in that set. And actually this is an independent set, so it's even simpler. So when you contract by an independent set, it's just you look at all the independent sets that contain this thing you're contracting out by, and then you just remove them.
00:45:34.150 - 00:45:35.514, Speaker C: I see. Okay.
00:45:38.914 - 00:46:33.832, Speaker A: It corresponds to edge contraction in a graph if you're looking at graphic matriids. So yeah, once you have these two things here, well then together they just give you the original theorem. Right. If you're independent in this D dimensional rigidity matriidal, well then that set union the diagonals is independent in the d plus one symmetric matrix completion matriid. And then in that case, the first part tells you that the maximum likelihood threshold is less than or equal to d plus one. So for two, I'll just refer you to the original paper. The proof is not complicated.
00:46:33.832 - 00:46:45.564, Speaker A: It's just linear algebra, basically. But it's one of those things where I think it's just easier to read on your own than it is for me to walk through it. And then for one.
00:46:48.384 - 00:46:50.044, Speaker B: What'S the original paper?
00:46:51.384 - 00:46:55.884, Speaker A: The maximum likelihood threshold of a graph by Gross and Sullivan.
00:46:56.604 - 00:46:57.704, Speaker B: Okay, thanks.
00:46:58.084 - 00:49:19.404, Speaker A: Yeah, I can, yeah, if you want me, I can, I can send a bunch of references at the end here. But then for one, I'll just briefly go through a proof here. Assume g union, the diagonals is independent in M of GRn. So this implies that the dimension, the projection of PSD and rank r cone oops onto these coordinates is full dimensional. So this almost completely just follows from the definition of what it means to be independent in this matriid. I can go into that and maybe a little more detailed people are interested, but I think that's, I can, I can also just wave my hands at that. Um, then, if some partial rank r PSD matrix and the interior of this projected cone can't be completed.
00:49:21.824 - 00:49:22.392, Speaker C: To full.
00:49:22.448 - 00:51:30.724, Speaker A: Rank, full rank, positive definite, then complete to PSD of rank R, perturb slightly to get a positive definite matrix, then reproject, and the result is arbitrarily close to the original partial matrix. And, you know, if you prefer to think about this in a picture. So this is the PSD cone. And this is, this is the PSD matrices of rank R. Then if you look at the image of the projection, you'll get this full dimensional set, which is the projection of the rank R part.
00:51:33.584 - 00:51:35.856, Speaker B: Projecting onto what coordinates?
00:51:36.040 - 00:51:47.172, Speaker A: Sorry, your independent set plus the diagonal entries. So your graph G plus all the.
00:51:47.188 - 00:51:55.548, Speaker B: Diagonal entries okay, so these are matrix entries, the coordinates of the entries of the matrices. Okay.
00:51:55.636 - 00:52:56.960, Speaker A: Yes, yes, exactly. Yeah. And the idea is, well, if you have something in here that's, that un projects to just this lower dimensional part of rank R matrices, well, then you can wiggle it slightly off of the PSD cone or slightly off of your rank R part, and then you'll get something close to your original point that now by construction has an inverse image in the positive definite part of the. Yeah. And actually this part of this is actually due to Caroline in 2012. It's cited in Seth and Elizabeth's paper. Yeah.
00:52:56.960 - 00:53:20.512, Speaker A: And she has a better version of this picture. That's all I had planned to get through. Yeah, I mean, I'm happy to take any more questions and also just send all the references people asked for. Thanks, Daniel.
00:53:20.648 - 00:53:36.414, Speaker C: Oh yeah. Thank you for this talk. This answers that question that I asked in Caroline's talk, the question about connections between rigidity and some of the statistics that we were looking at there. So this is cool. Thank you very much.
00:53:36.574 - 00:53:37.614, Speaker G: Yeah, no problem.
00:53:37.654 - 00:53:41.022, Speaker A: I'm glad. And I can go ahead and just.
00:53:41.078 - 00:54:59.284, Speaker F: Send, I guess the reason the bound is not strict is because you have a situation where the, the projection is not full dimensional, but still the fibers contain points in the interior. So you could have, so trying to answer the question as to why this is not a strict bound. So in other words, if I took, there could be projections that are not full dimensional and, but the fibers may still contain points in the interior. So in other words, you have matrices that you wouldn't expect using this analysis to have PSD matrices which have completions, but they, to have interior completions. Interior means positive definite. That's what I guess you mean, right? Yeah.
00:55:00.264 - 00:55:52.684, Speaker A: Let's see. So I think, right, I think it might be the other way around. Right. Like you, you can have, if I understood what you were saying correctly, which I might not, it is possible that every rank are, every partial rank R PSD matrix can be completed to a full rank PSD matrix. But simultaneously, it is not the case that the image of the rank R PSD cone under this projection map is full dimensional.
00:56:11.774 - 00:56:14.834, Speaker F: So what is the every there. Repeat that statement.
00:56:16.494 - 00:56:59.760, Speaker A: So you can have a situation, you can have a graph where when you take, where any corresponding partial rank R PSD matrix can be completed to a positive definite matrix. But the prediction actually, here it is. Here it is. You can have a situation where every partial rank r symmetric PSD matrix can be completed to a full rank positive definite matrix, but you still cannot arbitrarily specify the entries of this partial matrix and always be able to complete to.
00:56:59.792 - 00:57:05.320, Speaker F: Rank R. Well, isn't that the same as saying that the projection is not full dimension?
00:57:05.392 - 00:57:06.336, Speaker A: Yeah, yeah, yeah.
00:57:06.440 - 00:57:31.760, Speaker F: So I'm just trying to say that the unusual cases where the bound doesn't hold, well, I don't know whether they're unusual or not. The bound is bad would be the cases where the projection onto their coordinates, onto the edges of the graph. Projection of the PSD cone on the edges of the graph is not full dimensional, but still you happen to have fibers that pierce the interior.
00:57:31.912 - 00:57:33.644, Speaker A: Yes, yes, yes.
00:57:34.224 - 00:57:36.008, Speaker F: So what are those examples?
00:57:36.176 - 00:57:38.724, Speaker A: Yeah, so I'm digging for the paper right now.
00:57:39.784 - 00:57:46.012, Speaker F: It was very over constrained graphs for that dimension.
00:57:46.168 - 00:58:45.724, Speaker A: I think it's just like complete bipartite graphs of a certain size. And like basically what the authors wind up, it's Greg Beckerman and Reiner Zinn. But what they wind up showing is that if you look at the asymptotics, like they have a family of graphs, or just like some subset of complete bipartite graphs, the maximum likelihood threshold or something grows linearly and then this, like other quantity grows quadratically. So yeah, I think more can be done to like get a better concrete handle on this. I think as far as I know, the state of knowledge on this is like, okay, we know they're not equal, but like, and there are counterexamples, but I don't know how. Well, yeah, I don't know how well understood the phenomenon leading to the counter examples are.
00:58:48.704 - 00:59:14.916, Speaker G: Daniel, can I ask a very, maybe, it's sort of really silly question. I'm sorry to have asked it so late. Maybe I don't understand what you mean by a partial PSD matrix here. You mean a biology by partial that, that you have a principal sub matrix if you, after a permutation, but.
00:59:14.940 - 00:59:27.820, Speaker A: Oh, but yeah. Oh no, I'm sorry. No, by partial I just mean like there was some PSD matrix of rank r and then you forgot a bunch of the entries. That's what I mean by partial PSD.
00:59:27.852 - 00:59:30.932, Speaker F: Of rank R. Partially filled.
00:59:31.108 - 00:59:31.884, Speaker A: Yeah, partially filled.
