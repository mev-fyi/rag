00:00:01.680 - 00:00:58.794, Speaker A: So the last topic to be discussed here is actually related to the question of how common chordal graphs are in applications. So in our applications, chordal graphs arise as the result of a graph algorithm or graph elimination or as a choral extension. So typically as the conclusion will be that chordal graphs arise as the sparsity patterns of sparse cholesky factorizations after. So the original pattern plus the fill in. But graph elimination is sort of defined like this. So we start with an ordered undirected graph, not necessarily choral. And then so we have an undirected graph like this one which is a cycle of length six, so certainly not choral.
00:00:58.794 - 00:01:40.514, Speaker A: We have chosen an ordering and then we process the vertices in the order levels chosen. And at each step of the elimination process we take the next vertex, look at the vertices in the higher neighborhood so the adjacent vertices that follow it in the ordering. And if they're not already adjacent we add an edge between those vertices. So for example here we start with vertex one. Four and six are not adjacent. So we add a vertex in that position or a zero, non zero in the pattern. Then we go to the next one.
00:01:40.514 - 00:02:29.194, Speaker A: Two we look at three and five are not adjacent. So we add an edge. In the third step we look at the adjacent vertices to vertex c that follow it in the ordering. So that's four and five and not for example four and two because two precedes three in the ordering. And then after this step actually continue with four, five and six. There's no extra edges added anymore because the higher neighborhoods already complete. So this process results in a graph that's chordal by construction and that has this ordering that we had chosen as a perfect elimination ordering by construction.
00:02:29.194 - 00:03:42.178, Speaker A: And it describes sort of many types of algorithms that can be viewed as elimination methods in the case, or this is a larger example. First, where we have this undirected graph, we chose this ordering and then, so the filled circles would be the original edges and the original graph which is not chordal. And then this process of elimination will add the open circles in the graph and the result is a chordal graph that has this as a perfect elimination ordering. So that's known as a triangulation of the original graph or a chordal extension. The added edges are the filled or fill in edges. And for our applications it's actually exactly what happens if you apply a factorization algorithm symbolically and determine the sparsity pattern of the Schlesky factor. It also describes actually other types of elimination algorithms.
00:03:42.178 - 00:05:01.516, Speaker A: So the one that we're interested in is sort of Swelewski factorization or Gauss elimination. There's other algorithms that involve elimination of variables, for example Fourier Motskin algorithm for linear inequalities. Dynamic programming over graphs where you optimize over one variable at a time. Probabilistic networks graphical models so one difference between Cholesky factorization and some of these other applications is that in many of these applications, for example probabilistic networks with discrete distributions, the complexity of the algorithm will be exponential in the largest click size of the eliminated graph. Schlesky factorization is much simpler because the complexity depends on the filling, but it's still polynomial in the clique of the Schlesky pattern, the clique sizes of the Schlesky pattern. So there's different trade offs in choosing and ordering depending on these applications. So we'll be interested in the application to cholesky factorization.
00:05:01.516 - 00:06:16.244, Speaker A: So we'll always use a Cholesky factorization of this form LDL transpose with l unit lower triangular and deposited diagonal. And to see that the pattern of the Cholesky factor is actually given by this graph elimination algorithm, we can just look at one step in the Cholesky factorization, for example in the outer product description of the Schlesky factorization. So we start with the matrix a that's positive definite. In the first step, we factorize it as a product of three LDL transpose, where the zeros in D below the first column in D have been removed. And then in the recursive algorithm, we continue with this Schur complement block and then factorize it again step by step until we arrive at a complete factorization. But if you look at what happens to the sparsity pattern in this first step, and look at the sparsity pattern of actually the sum of these three matrices. So l one, then this symmetric matrix and l transpose.
00:06:16.244 - 00:07:12.382, Speaker A: Then we see that in this block the sparsity pattern will be the sparsity pattern of C. So if an ij element will be non zero if it was nonzero in the original matrix, or if bi and bj are both nonzero, which means their I and j are adjacent to the first 1st vertex that we eliminated. So that's exactly what happened during this graph elimination. So if we illustrate it with the same figure here. So we start with a matrix with this sparsity pattern. In the first step of the Schlesky factorization, we take the first column and don't change. It's that's the first column of l, but then, in this sure complement block, we introduce a non zero in position four, six.
00:07:12.382 - 00:07:46.364, Speaker A: Here, because four and six were adjacent in the, both adjacent to the first node, we eliminated. And then we continue with the factorization of the smaller matrix by looking at the first column. In a smaller matrix, three and five are adjacent to that vertex. They're not adjacent in the original pattern. So we'll introduce a non zero in that pattern. And then continuing like this, we get the filled pattern of the chileski factor. And by construction, that's a chordal pattern.
00:07:46.364 - 00:09:01.763, Speaker A: And the numerical order that we used is the perfect elimination order. So, in practice, many algorithms exist for choosing an elimination ordering. There are some complexity results that are very famous. So one says that, Bianna Kauki, is that finding the minimum ordering that gives you the least amount of filling in the pattern is np complete. In also finding the one that results in the smallest maximal cliques is np complete. There exists minimal orderings, minimal in the sense of inclusion, that have the property that the ordering you compute by these algorithms gives a filled set of edges that doesn't include the filled, the edge set of any other ordering. So it doesn't necessarily mean that the fill in is small, but it means there's no other ordering that gives a subset of the filled edges.
00:09:01.763 - 00:10:07.074, Speaker A: And those are, that can be done in polynomial time. And those are useful also because a minimal ordering algorithm applied to a chordal graph will have to give you a perfect elimination ordering, because by definition, a chordal graph we've seen has a perfect elimination ordering. So in that case, for if you choose for sigma prime perfect elimination ordering, then this is the empty set. So if your ordering is minimal in this sense, then it has to produce perfect elimination ordering. So in practice, we often use non minimal heuristics that don't have this guarantee that are faster than a minimal ordering, that work well in practice, can work well in practice. But this also means that if you apply them to a chordal graph, they might actually introduce some filling. But if the affiliate is small, it's not an issue.
00:10:07.074 - 00:11:59.984, Speaker A: So the common ordering methods that we use often are the approximate minimum degree ordering in Matlab, for example. And then a final comment is, it's sort of interesting, I think, in the sparse matrix literature that people have developed or studied very efficient methods for analyzing the filled graph. So the eliminated graph before constructing it, before actually computing all the filled edges. So you can compute most of the statistics you might be interested in ahead of time, with a complexity that is actually linear in the size of the original graph, before applying the elimination, and that includes constructing the elimination tree, calculating the fill in, calculating all the degrees, higher and lower degrees of every vertex in the fill graph, finding the number of cliques the size of each clique, setting up a supernodal elimination tree, and so on. So that's interesting also. So these are developed mostly for sparse Leski factorization methods, but they're also interested in useful, I think, in our applications and semi definite programming, for example, because the application that will apply the ordering is more complicated than just a Cholesky factorization. So you can actually afford more time in comparing different orderings, getting their properties, and then trying to find a good ordering before actually constructing it.
00:11:59.984 - 00:13:07.424, Speaker A: So that was actually the end of my first part. So most of these slides are taken from a survey paper that we wrote, and it's based on other books and survey papers. So these are some well known references and books. So in part two of the lectures we apply this to sparse matrix symmetric matrix completion problems. This will simplify a lot of the notation that we had for general graphs before. So we'll just look at the notation we'll use first and then look at different applications to questions that are relevant in semi definite programming and Euclidean distance matrix completion problems. So again, the graphs arise as a sparsity graph or a symmetric matrix.
00:13:07.424 - 00:14:24.272, Speaker A: So the vertices are just the indices of the rows and the columns. This is a notation we use for a symmetric matrix of order n with a sparsity pattern e. And again, the precise information we get from the graph is that if there's no edge between two vertices, then that entry in the matrix must be zero diagonal entries, and the entries corresponding to the adjacent edges may or may not be zero. And that also means that a matrix can have different sparsity patterns, because we can always take one of the zeros in the matrix and treat it as a numerical non zero. So unless the matrix is already completely dense, we can always extend the sparsity pattern by adding an edge and get another phallet sparsity pattern for the same matrix. So that's already, then we're using an extension of the original graph. So we'll use again the choral sparsity patterns then will arise as the sparsity pattern of a Schlesky factor.
00:14:24.272 - 00:15:19.116, Speaker A: So we have a sparse matrix here with the, for example, the filled circles are the original pattern, a symbolic if you choose a numerical order for the Schlesky factors for the factorization, then we introduce filling in these positions. And that's a practical way of computing a choral extension of the original matrix. In many cases. For many sparsity patterns, this can be done quite efficiently with a small amount of filling, because that's the reason why Sparschuleski factorization is such a useful technique. But of course there are also many sparsity patterns where the filling is too large to be to use this. So that also gives us some simple patterns that we can look at. So the pattern is choral.
00:15:19.116 - 00:16:06.656, Speaker A: If it has a cholesky factorization, it's zero filling. So that's, this one on the left is maybe the simplest pattern that has this property, because if you have a matrix with two dense overlapping diagonal blocks, the Schlesky factor will have the same pattern with no filling. So this is maybe the simplest non complete chordal pattern. And it's one that we often use to illustrate the key idea of some algorithms. And then using the all the machinery of the click trees, we can extend it to a general coding pattern. But often the algorithm is followed by considering this simple pattern. The band patterns have that property.
00:16:06.656 - 00:17:05.524, Speaker A: So that's a choral pattern, an arrow pattern, or a block arrow pattern. We can also simplify some of the notation we introduced before. So we assume that we'll always assume from now on that the numerical order is a perfect elimination ordering. So we assume that we did all the pre processing that I discussed yesterday on the original pattern. So the numerical order is a perfect elimination ordering. So we have a click tree, an elimination tree, and then we can use just a standard elimination tree in algorithms that iterate over the columns of the matrix one by one. Or we can use an supernodal elimination tree by grouping these columns in block columns with a similar structure.
00:17:05.524 - 00:18:09.764, Speaker A: And the supernodal elimination tree is actually represented by the same illumination tree. But here the square nodes are always the first vertex or the first column that starts one of these block columns. Those are the, what we call the representative vertices of the supernodes, for example column five here. Then the nodes in the elimination tree are partitioned in supernodes and the partitions is indicated by these heavy lines. And the nodes in the same sets, so that's called a super node, are arranged here. So there are consecutive columns in one of these block columns. And then I have this supernodal structure where a supernode is corresponds to a dense diagonal block of different dimensions.
00:18:09.764 - 00:18:56.992, Speaker A: So those are called the supernodes. So one two is a supernode 567 and so on. And then below each diagonal dense diagonal block, I have columns with the same structure. So here I assume that all this preprocessing has been done. So the matrix has been arranged in this nice form where the standard algorithm where we compute over one column at a time, or the supernova version where we take block columns, are almost identical, except that in the supernodal version use some more dense matrix computations. And it's related to a click tree. Again.
00:18:56.992 - 00:20:07.184, Speaker A: So this supernodal, this elimination tree doesn't give you the full information about the structure because it doesn't give us, for example, for 567 we have this, you know that this is a dense block, but doesn't tell us exactly where the non zeros are below that block. So that's the difference with a full click tree where we add to every node and the elimination tree, the remainder of the non zeros in that book. So here is maybe just the example, or to illustrate this in a simple pattern. So we have two overlapping diagonal blocks. So that's a very simple click tree which has two clicks. The first entries here, n minus Q entries, would be the first supernode. Then we have the intersection with the other clique, and then we have the top of the root of the simple tree.
00:20:07.184 - 00:21:09.124, Speaker A: In the case of a band pattern, we would have a chain. Each supernode is actually just one as one element, and these are the clicks. If you just move along the diagonal, we get clicks of size w plus two, w plus one, w plus one. Block arrow pattern would have this click three. So for each vertex on the diagonal plus this column, this row, and then this dense box here form a click. And then we have the last one would be the root and the elimination tree, or in the click tree, and then the rest are just arranged like this. So often in notation we'll have, we'll have to select sub vectors of vectors.
00:21:09.124 - 00:22:19.668, Speaker A: And so we introduced this notation with selection matrices. But it's easier to explain just an example. So if you have a five dimensional problem, so vectors of length five or five by five matrices and an index set two, four, five, then we can define this selection or selector matrix that just has this interpretation right, that if you apply to a vector x, you get that sub vector, if you apply it symmetrically to a matrix, you get that sublock of two, four, five, the principal sub block with elements two, four, five. So this, we could just write equivalently as a bit indices. But the adjoint of these operations will also arise. So if you apply the transpose of this selection matrix to a vector, we take a three vector and copy its elements to positions two, four, five of a vector that's otherwise zero. And if you apply it to the adjoint of the matrix operation, then we pre and post multiply with the transpose and with the matrix.
00:22:19.668 - 00:23:39.204, Speaker A: So that means we take a three by three matrix and copy it in those positions in a zero matrix. So these two operations, these adjoint operations are the reason why we introduced these selection matrices, some other notation that simplifies compared to what we did yesterday. So yesterday we used this notation for the higher neighborhood of a vertex. So here, to simplify notation for every column, you'll just write the positions of the non zeros by gamma. So gamma one are the positions of the non zeros in column one, gamma two in column two, and so on. And then we'll try to introduce or use some notation that makes it easy to extend the basic algorithm that uses a recursion over an elimination tree to a supernodal version that uses the supernodal elimination tree. So in the basic algorithm, we'll iterate over all the columns of a matrix or the nodes in an elimination tree.
00:23:39.204 - 00:25:14.064, Speaker A: And then the for each column we define by alpha I the non zeros in that column below the diagonal. So if we have an algorithm that's formulated as a recursion of the over the elimination tree, either in topological order, that means starting at the leaves and pre processing every note only after all the children are processed. But in our conventions, actually where we have a numerical order, this is a topological order because we arranged or chose to that as a perfect elimination order. Or we use an inverse topological order where we visit, we start at the root and then visit every node in the elimination tree only after the parent has been processed. So in an elimination tree algorithm where we use a recursion in one of these two orders, the new five, for example, new will be just the node that we visit. So mu five is just node five, comma five is as I already defined. So all the entries non zero positions in that column, and alpha five is just the non zeros below the diagonal.
00:25:14.064 - 00:26:47.374, Speaker A: And the important property for the perfect elimination order and the corresponding elimination tree is that the entries of this subset of the entries are all ancestors of the node five on the end tree. So six 7810 are somewhere on the path between five and the root. And that's the important structure that allows us to solve certain problems by a simple recursion in topological or inverse topological order. So this would be the basic version where we process the columns consecutively, starting at the first one or starting at the last one. To extend this to a supernodal version, we could take the same pattern that divided in supernodes, we have the smaller supernodal elimination tree, because every node in this supernodal tree is a supernode that actually collects several nodes in the original elimination tree. So the only difference here is that new will now be the supernodes will have more than one element, for example 567. The five subscript is the first column index, so that's called the representative vertex of the supernode.
00:26:47.374 - 00:28:04.174, Speaker A: Gamma five has the same meaning, it's just total all the non zeros in the first column and alpha five then gives us the non zeros in the block column below this diagonal block. So with a small difference in notation in the elimination tree, we have only nodal sets here. So a single node, this is the notation you use. The only difference here is that the nodes are replaced by supernodes, and then the algorithms will be very similar. So we look at three important classes of matrices, positive semidefinite sparse matrices, positive semi definite completable matrices. So those are important in algorithms for sparse semi definite programming and many other applications, and then probably tomorrow euclidean distance matrix completion. So the everything I sort of discussed so far relates to the cone of positive semi definite matrices exactly because of the connection with the sparse Cholesky factorization.
00:28:04.174 - 00:30:07.724, Speaker A: And a sparse key factorization will be if here we allow reordering, we use this notation for a sparsial Sq factorization, we reorder the matrix, or we assume it has been already reordered, so that the numerical order is the elimination order we use. And then we use LDL transpose, where l is unit lower triangle triangular and ds positive diagonal. And then the sparsity pattern of that Schlesky factorization would be actually the Schlesky, the sparsity pattern of L plus L transpose. And this is sort of what I explained already, except here I add explicitly the possibility of a symmetric reordering. So there is a famous result from the early papers on choral patterns that made a connection with Chileski factorization due to rows. And it says that if a matrix or a sparsity pattern has a perfect elimination ordering, then the it's possible to find an cholesky factorization that doesn't add filling that was already discussed before in a more general notation for graphs, but applied to matrices, it means we can choose the reordering to have zero filling, and it's actually necessary and sufficient if in this sense that if a sparsity pattern is not coral, and of course it's always possible for a specific matrix a in that pattern that there exists a zero filling factorization. But if a sparsity pattern is not coral, you can find that positive definite matrices in that set of matrices with the pattern e, for which no perfect elimination ordering exists.
00:30:07.724 - 00:30:56.834, Speaker A: So for every given ordering, you can always find a matrix with that pattern that's positive definite, that introduces filling. So if you see it as a property of the actually set, the sparsity pattern. So the set of positive definite matrices with that sparsity pattern, it's Nester insufficient. So in semi definite programming, this is important because it's one of the two cones in a primal and a dual sparse semi definite program. So the first cone I will consider in more details is the set of positive semi definite matrices with a given sparsity pattern. So the notation we often use is this. So this is the intersection of the dense positive semidefinite cone with the symmetric matrices with sparsity pattern e.
00:30:56.834 - 00:32:10.456, Speaker A: Or if you write in a full, it's just sparse positive set symmetric matrices with that pattern that are positive semi definite. So even without assuming that the pattern is chordal, just for the general sparsity pattern, we can say the following about this set. It's a close convex cone because it's the intersection of a positive semidefinite cone, which is convex, and a subspace. It has non anti interior if you relative to the sparse matrices, because we assume that all the diagonal entries are considered as non zeros. So an identity matrix is actually part of this cone. And it's also pointed because if x and minus x are positive semi definite and x must be zero, right? So these are, so it's a regular cone or proper cone, and this holds without assumption on the sparsity pattern. For choral sparsity patterns, we have an important decomposition property that's been used in many algorithms, and that says the following.
00:32:10.456 - 00:33:16.664, Speaker A: So it's easiest to illustrate with a simple example here. This is a simple chordal pattern with three overlapping cliques or principal dense sub blocks. So this theorem says that a matrix with this sparsity pattern is positive semi definite if and only if it can be written as a sum of positive semidefinite matrices with these very simple sparsity patterns. In each case we take one of the cliques in the original pattern and have just one positive, one dense block with that in that position. So this is from some classical papers from the eighties. If we were to use this, write it in some formula, it would look like this with our selector matrix notation. So the h I's are these dense small blocks of the size of each of the cliques.
00:33:16.664 - 00:34:06.564, Speaker A: And then we use this selector matrix operation to copy it to the right positions in the matrix. So we have to pre and post multiply with this selector matrix and the sum we iterate over the sets that indicates or is actually the set of representative vertices. So the first vertex of each clique. So in this case there are three elements in this set. The first one would be one. So first vertex of the first column, then j would be the first element of this clique and k, so this would be a sum over three blocks. And so there's a very useful decomposition theorem.
00:34:06.564 - 00:35:11.924, Speaker A: It's obvious that for a general pattern, choral, even if it's not choral, it's always a sufficient condition to prove that something is positive, semi definite. But the important fact is that if the pattern is chordal, it's mystery insufficient. And so this is not too difficult to see, at least for simple patterns. So if you look at just as the simplest non code, the simplest chordal pattern with two clicks, then this decomposition will actually follow from a sparse Schleski factorization. Because we know that if we factorize this matrix, it has a Schlesky factor with zero fill. So L and L transpose will have this form. And then if we group look at this as an outer product, and we collect the columns or the terms in these two submatrices, then we see that the first columns here will give us a matrix of this form, the second column will give us a matrix of this form.
00:35:11.924 - 00:36:26.544, Speaker A: So you can actually extract or compute this decomposition also quite efficiently using a sparsulesci factorization. And at least for the simple pattern, this shows why. Right, so the general proof is sort of, we can skip, so it's written here, but it relies on these properties of elimination trees, and so it can be proved quite easily from those properties. So the, one of the standard algorithms in sparse cholesky factorization is the multifrontal algorithm that was proposed by Duff and breed in 1983. And I'll explain it here in a few, in a little more detail, because so it is standard methods in sparse linear algebra. But the, the algorithms that we'll discuss later are all inspired by the multifrontal Schlesky factorization. So we try to come up with multifrontal type methods for other things than cholesky factorization.
00:36:26.544 - 00:37:41.614, Speaker A: So the details already matter. But it relies heavily on the elimination tree. We can assume the sparsity pattern is coral, or if the original matrix was actually not coral, we kind of implicitly use the coral extension that's the defined by the ordering that we used for the Schoisky factorization. And so the important property is that the multifrontal factorization is a recursion of the elimination tree in topological order. So, starting at the leaves in the tree and then working to the finishing with the root in the tree. So we can skip the derivation that follows from the properties we had, but this will be the one iteration in the multifrontal factorization. So with each vertex, each node in the elimination tree, we associate a small dense matrix that's known as the, the update matrix.
00:37:41.614 - 00:39:09.118, Speaker A: And then one step in the factorization goes like this. When we visit a node I in the elimination tree, we take that column and the non zeros in that column, and that row we add to it contributions from all the children in the elimination tree. That's called the frontal matrix. And then we take the frontal matrix and compute the diagonal elements in that position for that entry, its column, the corresponding column, or the non zeros in the column in l whichleski factor, and a new update matrix for that node in the elimination tree. And they follow from this equation and they can be computed like this. And then at that time we can discard the update matrices for the children in that of the node that we processed, because they're not needed later. So for just in elimination, in the simple example when we visit index nine, for example in the elimination tree, then this is the frontal matrix.
00:39:09.118 - 00:39:53.520, Speaker A: We take the 9th column and the 9th row, at least the non zeros in that column and row, we add update matrices from each child of that vertex. So for two, eight and three, those are small dense matrices. So for example, for eight, we get a three by three dense matrix, because there are three elements below the diagonal. That's a small dense matrix. We have to copy it to the right positions in f, and we do the same for each of the other children of node nine. That's the frontal matrix. Then we do these computations on the frontal matrix and we get the update matrix of node nine.
00:39:53.520 - 00:41:05.936, Speaker A: And at that time we can actually discard the update matrices for two, three and eight and continue in the next step of the factorization. And the motivation for this is that we try to use these update matrices as auxiliary variables that are dense and that are just created and deleted during the algorithm. But they allow us to use some dense matrix computations to during this sparse Scholeski factorization. So all these algorithms also have a supernova version, and it's exactly similar, or what you would expect. You compute a block diagonal block Schlesky factorization, where instead of a diagonal d, D will be block diagonal with diagonal blocks given by the supernodes, and then the rest is actually identical. L is units, block, lower triangular, and so on. And then the supernodal version will be exactly similar, except that at each step we visit a supernode in the elimination supernodal tree, and then compute this entire block column or the factorization.
00:41:05.936 - 00:42:15.836, Speaker A: So the diagonal block and the block column of l using very similar steps. Right. So we have a frontal matrix, and the only difference is here, for example, if you get to node a, that we take two by two block initially and otherwise the algorithm is very similar. So using a supernodal version allows us to have fewer steps in elimination tree because they're compressed into supernodes, and also use more dense computations because we have block matrices instead of dense sub matrices instead of just columns. So this is sort of the type of algorithm that we would like to use because they're sort of very efficient and use these elimination trees in a very nice way. So the first new or next problem is sort of something that's very common in many applications. It's a question that arises in many sparse matrix problems.
00:42:15.836 - 00:43:20.334, Speaker A: So we're given a sparse matrix a here, opposed a definite matrix a. The inverse, of course, is a dense matrix in general. But we would like to compute the selection of the entries in the inverse without actually computing the entire dense inverse, which would be very expensive. So here we look at the question where the selection of entries that we are interested in are the entries that are actually non zero in the matrix a. Are we not interested in all the other entries in the universe? So here. So that's actually the projection on the sparsity pattern of the inverse of a, right, where the projection just means we set the entries outside e to zero. And we're interested in computing again this projected or selected inverse without computing the entire inverse, or computing as few entries from outside the projection as possible.
00:43:20.334 - 00:44:24.434, Speaker A: So for an choral pattern, turns out that you can do this exactly. So you can compute the entries in this projection without computing any other entries in the inverse by a very similar recursion as the Cholesky factorization. And it actually follows from sort of a careful inspection of this equation. So suppose we computed the Scholesky factorization of a. A is a positive definite matrix with a chordal pattern, and then we write this equation for the inverse, and then we look at one column, one dense block in that matrix. So jj is the diagonal entry in the inverse. So b here is the projected inverse.
00:44:24.434 - 00:45:02.640, Speaker A: So bjj is the diagonal element in the inverse. This column is actually all the positions in the inverse in that column that we want to compute, because that's sort of the subset of the entries that are non zero in the pattern of E. And then we have this block here. So L will have this non zero. These are the non zeros in column j of L. It's one on the diagonal, and then the rest are in positions alpha j. L inverse is upper triangular, l inverse.
00:45:02.640 - 00:46:41.584, Speaker A: The transpose is the upper triangular with one on the diagonal. So after multiplying with d, we get this in those positions. So if you look at this carefully and use some of these properties that we've seen, you can see that if you know L and D, because we first compute the cholesky factorization, then you can actually efficiently compute this block, all the entries in this block, if you already know this two by two, this two two block, and maybe it's clear in an example, this is a pattern with nine by nine matrix. The filled circles are the non zeros in the pattern. This would be the entire inverse, so, which would be probably completely dense, including the all the other entries. But if you look at the sub block of the matrix with index five and then the non zeros in column five, so we get this three by three block that here we denote by b and consists entirely of entries that we're interested in computing because of the coral structure and because these entries here in column five will also be non zero. Give us non zeros in these positions by the property of the perfect elimination ordering.
00:46:41.584 - 00:47:44.474, Speaker A: So if you look at this equation of a inverse times a column of l times l minus t times the inverse of d, then this is the j column or column five of L. So it's one and has two non zeros in these positions. L minus t is upper triangular. So this is the diagonal element, and the other nonzeros are here. So from this you see that if you're interested in computing n three five five in the inverse and this rest of that column, then you can do that very easily if you already know the values of this block. Because in this matrix, if you know that this two two block is given and you know l and d, then you can compute this vector from the second equation that gives us a transpose of this row vector. And then from that you can compute the diagonal.
00:47:44.474 - 00:48:51.380, Speaker A: And that's exactly what we get. If we go through the elimination three in inverse topological order, because then by the time we arrive at node number five, we have computed, we finished everything, the part of the matrix that the columns seven, eight and nine, because they precede index five in the inverse ordering. So at that time we'll actually have computed this two two block. So that's the idea. And we can arrange it in sort of a multi frontal, that's our term for this, but because it's sort of very similar to the multi frontal Chileski method, and you get a very similar algorithm that can be computed to solve this projected inverse problem. And as usual, there's a supernova version. And this maybe gives us the conclusion.
00:48:51.380 - 00:49:52.930, Speaker A: So here we tested this algorithm for this projected inverse question on a large set of patterns from the sweet sparse matrix collection. So there sparse matrices that range from very small to about 130,000 nodes and columns. I think this gives us some of the statistics, the order of the matrix and the number of non zeros. So here we compare the time needed to compute a cholesky factorization, or to compute this projected inverse, only the entries and inverse in the positions of the non zeros. And the conclusion is it's, it's very similar. So you can compute this projected inverse at a cost that's not much higher than the Schlesky factorization for the same sparsity. So this projected inverse question comes up in many applications.
00:49:52.930 - 00:50:55.762, Speaker A: But in our applications, this came up when we looked at barrier methods and interior point methods for the sparse positive semi definite cone. So if you have a cone of sparse positive semi definite matrices, then we would use or define a logarithmic barrier function in the usual way as minus log depth of the positive definite matrix. And in algorithms, then it's important to be able to efficiently compute this barrier function and its gradient and its Hessian. So the value of the barrier function would be computed, can be computed very efficiently from the Schlesky factorization. So you compute the Schlesky factorization, and then this is the barrier function. The gradients with respect to the set of symmetric matrices would be exactly the projected inverse with a minus sign. And the Hessian is given.
00:50:55.762 - 00:52:16.704, Speaker A: It's easier to explain the Hessian in terms of the directional derivative of the gradient. So if you take the gradients in a certain direction y, and then linearize it in that direction, then you get this expression. So, to compute the Hessian and apply it to a direction, we need to form this product s inverse, y s inverse, and then project the dense matrix on the sparsity pattern and these expressions are all valid for any sparsity pattern, coral or non coral. But you see that it's important to be able to compute the gradients efficiently. So to compute this, projecting projected inverse efficiently without actually computing the entire inverse. And here also it's important to compute this projection efficiently without actually computing s inverse, which is dense, and then forming this dense product with all these operations can be implemented efficiently if the pattern is chordal or we use a chordal extension. So we already seen that the Cholesky factorization here, the first step we would use the multifrontal factorization that's in recursion in topological order.
00:52:16.704 - 00:52:58.744, Speaker A: The projected inverse follows by this projected inverse algorithm is a recursion in inverse topological order. And that actually suggests already how we can compute this hessian, because we have these two steps. If you linearize this in a certain direction, then we get an algorithm for the directional derivative of the gradient, right? So the algorithms for the evaluating this directional derivative will consist of two recursions, one in topological, one in inverse topological order, that correspond to these two steps, but linearized in that direction.
