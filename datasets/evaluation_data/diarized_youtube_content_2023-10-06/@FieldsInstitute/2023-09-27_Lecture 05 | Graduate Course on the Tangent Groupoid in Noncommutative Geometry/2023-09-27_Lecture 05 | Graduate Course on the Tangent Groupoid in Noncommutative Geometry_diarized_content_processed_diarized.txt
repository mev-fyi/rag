00:00:00.720 - 00:01:19.086, Speaker A: All right, we had a little private discussion for fun around this exercise. And I'll add this exercise to the homework, because eventually, not today, but eventually, we'll actually use this little fact or something related to this little fact. I spent months really worrying about a certain issue which we're unavoidably going to collide with on Thursday. And it all comes down to understanding about 20 lines in one of Kormander's books. And it's taken me months to go through 20 lines and understand really what he was talking about. It's like reading a haiku or something, you know, after years, you finally figure out what show really meant. Okay, we were discussing back in the class, we were looking at the way in which you can take a symbol and build an operator out of it.
00:01:19.086 - 00:02:45.314, Speaker A: Op s, which I think in my notation last time, stood for one of these scaling families. Let me call it what it is, the scaling family. And if you were to evaluate this just at a particular t, just pick out one operator, of course you'd get corresponding scalable operator. And I explained last time that if you have a scalable operator, any scalable operator at all, in fact, let's do it for families. You can write it as the family associated to its symbol, which maybe I wrote with some index like this. Maybe a was an operator of order m, in which case he would have a symbol s of degree m. And now I'm making the m nice and prominent there so you can see it.
00:02:45.314 - 00:03:58.674, Speaker A: And then from the symbol, you could build an operator, or indeed a whole scaling family of operators. And what you get is not what you started with necessarily, but it's something that agrees with what you started with at the symbol level. That is to say, the symbol of s is just s. And so the difference between a and op of the symbol of a, the difference is effectively in a family of operators of one lower order. It's t times a family of operators of one lower order. And that family of one lower order you can apply the same procedure to, and write it as plus t squared times another lower order term, and so on. And what we'll spend a little bit of time on right now is understanding what and so on really means, what the three dots really mean.
00:03:58.674 - 00:04:34.144, Speaker A: But we discussed how you can obtain such. It looks like some sort of asymptotic expansion last time. And now we want to get real, get serious, to make it into something that actually converges in some crazy sense. All right, that's what we're going to do. Any questions? The Internet still out there? Yes. Okay, good. So tempting to look at the big thing, but it's the little, just the little dot of the camera there that I should be looking at, I guess.
00:04:34.144 - 00:06:05.068, Speaker A: Okay, so here's the result. This, this is sort of an optional result, but it sure is conceptually very clarifying to have it available. And maybe I'll state it in the language of operators rather than scaling families just to cut down on the t's and so on. So suppose we have a whole bunch of scaling of symbols like we will have as soon as we apply this iterative procedure to one a to begin with. And suppose the orders or degrees go down, so m one is bigger than m two is bigger than m three and so on. I don't think we need to worry about what these dots mean. I just mean and so on.
00:06:05.068 - 00:07:16.110, Speaker A: And of course they're going to minus infinity because they're integers. They just can't avoid going to minus infinity. Now there's a small technicality which is involved in constructing this op of s, which is that some kind of support condition has to be respected. So let's assume that all of these families are compactly supported. And because life is too short to do otherwise, let's assume they have they, meaning the families have a common compact support. So there's some, these are families we're talking about here. For each little omega, which is a point in rn, there's an operator called s little omega, s sub omega or s upper m lower omega.
00:07:16.110 - 00:07:45.734, Speaker A: And what I'm saying is that there's one compact set out there, c. So that if omega is not in c, all of the sm of omegas are zero. Just because that makes things the simplest. Yeah, yeah, yeah, yeah yeah. This just, you know, makes it easier. So the, the conclusion is that you can actually sum these up in the following sense. There is a scalable operator, one scalable operator.
00:07:45.734 - 00:08:06.114, Speaker A: I'm not sure if scalable is a real word. My spell checker tells me it isn't a real word. But you know. You know what I mean, I guess. Well, I don't know. Not in France. You know, in France they get very picky about this sort of thing.
00:08:06.114 - 00:08:39.494, Speaker A: But yeah. Anyway, in English you're free. Yeah, I always, yeah. When I'm teaching my sister algebra class, I always dwell on unitalization. You know, something as a unit, so it's unital and then you can make a sister algebra unital, so you unitalize it and then the process of doing so is unitalization. You just go on forever, you know. Yeah.
00:08:39.494 - 00:09:46.934, Speaker A: There is a scalable operator of order, the biggest one m one, which is the sum in the following sense. Let's continue with statement on this subsequent board. Here's the statement. If you pick any cutoff point p, so p is some integer. And you could imagine taking a and subtracting the sum of all of these up of SmJ's, the member of the family at t equals one like that, where the MJ's are bigger than p. There's only a finite number of MJ's bigger than P because the MJ's are strictly decreasing. So that's just a finite sum.
00:09:46.934 - 00:10:21.154, Speaker A: No harm in writing that down. And difference is a legitimate thing. And it's certainly a scalable operator. It's just a combination of scalable operators. Ah, scalable operator. And the order of it is pull or less order on scalable operators is a filtration. Anything that's order p has order p plus one and order p plus two.
00:10:21.154 - 00:11:06.356, Speaker A: So, but anyway, it has order no more than p. So in this sense, a is like the sum of all of them. If you think of this notion of order, this filtration is telling you what convergence is about, then the series in that sense converges. It converges in some addict topology to a, the infinite series converges, some sort of adic topology to a. The limit is not unique, is it? Because if you add to a, any smoothing operator, then smoothing operators of order minus p, or order p for every p. So this will be just as true for a plus k as it was true for a. On the other hand, that's it.
00:11:06.356 - 00:11:34.400, Speaker A: If you have two a's, both of which have this property a and b, let's call them, then a minus B clearly has to be an operator of order p for every p, and therefore it's a smoothing operator. So this nails down what a has to be up to. Smoothing operators. Okay. And they all are infinite series in the sense here. Okay, makes sense. Comments? Questions?
00:11:34.552 - 00:11:36.364, Speaker B: So this will be the last part.
00:11:36.904 - 00:12:17.074, Speaker A: Yeah, it's, it's completeness in this addict topology, which is not a great topology, not a housekeep topology. But anyway, no, they worry about this. They again and again, they want an operator. They don't. Yeah, they don't use language of general topology to describe this. All right, so this is what we want to spend a little bit of time thinking about before we go on. Maybe before we do that, let's just look at corollary to this.
00:12:17.074 - 00:13:14.430, Speaker A: If a one, a two, and so on, is any sequence of uniformly in the sense. We were just discussing compactly supported. I'll change where I put the parentheses, I think uniformly compactly supported. After you've proved the version with uniform compact support, you can sort of fiddle with it a little bit and prove it without. But let's just put in the support condition. Scalable operators of strictly decreasing orders, like these operators op SMJ one. But now I'm just talking about anything at all.
00:13:14.430 - 00:14:26.948, Speaker A: Then there is an operator a, so that if you do the following thing, you look at the order of the scalable operator, which is a minus the the sum from, I don't know, one to n of these big n. Let's say of these a little n's. This order goes to minus infinity as n goes to infinity. So in the same sense, there's convergence here. It's sort of exactly the same theorem, except the a's are now just general a's, not these ups. But it really is the same result, isn't it? Because each of these a's, as we've just been discussing here, can be expanded in a series. And if you look at all of the op of SMS that come from all of the a's, you get a gigantic bucket full of operators op of SM.
00:14:26.948 - 00:15:45.204, Speaker A: But for any given degree or any given order, there's only going to be a finite number of all of the up of SMS coming from all of the ajs, which have order bigger than that cutoff that you just introduced. So by expressing each an as a series like this, and then applying this result to the sum of the series, you obtain the corollary from this theorem. And this is very convenient, because you don't always want to be expressing everything in terms of this particular construction, especially because, as Dan was extremely troubled by the other day, this thing is not canonical on a manifold. You know, this, this requires some extra coordinates in order to say exactly what this up of SM is. All right, so let's just see how this is proved. Basically, the, the proof is calculus, or analysis, if you like. And so there's no avoiding it in this argument.
00:15:45.204 - 00:16:26.464, Speaker A: You really do need some epsilons. It's, you know, so sorry to the geometers. But that's the kind of argument this is. Let's see what's actually going to be involved in this. Well, first of all, or, but finitely, many of these MJ's are going to be less than minus n. So we can certainly, by throwing away the finite number, we don't have any difficulty adding up a finite number of things. This is all about adding up an infinite number of things.
00:16:26.464 - 00:17:14.414, Speaker A: We can certainly assume that all of these mJ's are less than minus n for all n for all j. Excuse me. So each of these operators which appears here, of course, these are all translation invariant operators. They're all given by convoluting by something. But what you're convolving by is in general some kind of distribution, little messy to think about. However, if the degree is less than minus n, these are actually convolutions by continuous functions, as we proved earlier. Each of these is an example of a scalable operator of order minus n, if that's what I say.
00:17:14.414 - 00:18:11.166, Speaker A: So it's given by a continuous integral kernel k of xy. But the k of x y is translation invariant. So I can really write k of x y as k of x y is equal to some function little s of x minus y. And that's what I'm going to do here. Each of this guy here is a convolution by some continuous function s of mj depend on omega, x minus one of compact support. How compact is this compact support? Well, it's uniform. Some one compact set in which everything is contained.
00:18:11.166 - 00:19:42.860, Speaker A: No matter what m is, j is or omega is, they're all sitting inside of some compact set. And the smaller the order gets, the better behaved. This function is, as we also observed. So, in fact, if this order, you can improve the size of it by making it smaller, maybe mj minus is less than n minus k, like this. Then smj omega x minus y, is a ck function uniformly so in omega, that is to say, the map which sends omega to s sub omega, upper mj, that's a function of x minus y. That map is a map into the Banach space of ck functions, moreover, into the subspace of the Banach space of ck functions, which are all uniformly compacted, compactly supported in some, I don't know, closed ball of radius 70. Okay, and now what do we need to do? We want to basically add up all of these ops, but each of these ops is built in a certain explicit way, as we discussed, out of these big S's, and you just fiddle with the index at the bottom a little bit.
00:19:42.860 - 00:20:21.724, Speaker A: And so what the problem really is, is to be able to add up all of the little, all of the big S's as MJ varies. And that really means that we need to add up somehow all of the little S's as MJ varies. And it's a problem because all of these S's, as things stand, may not be addable up able. For example, it's perfectly possible that after you've been given all of these smjs, you discover that these functions are all equal to one identically, at x minus y equal to zero. They could all just be 11111. So you can't just add them up just like that. You've got to fiddle with them first.
00:20:21.724 - 00:22:03.402, Speaker A: Now we'll discuss the fiddling. So these functions, they satisfy the hypotheses of what I'm about to write down. So we interrupt the proof to give a lemma, excuse me, which is just a sort of a calculus alamma. Suppose we have just a function. What am I calling these functions? S, right? Yeah. So suppose I have a function with a bunch of properties. First of all, it's supported maybe just in some particular fixed ball, which I will call ball.
00:22:03.402 - 00:22:33.058, Speaker A: So ball refers to some particular ball, radius 20, and then in the other variable. For the moment, I don't care. Let's. I think this is fine. I have in mind that this s should be compactly supported. But I didn't particularly specify what the support is. I may or may not want to insert compactly supported as time goes on here, like I did in the notes.
00:22:33.058 - 00:23:45.198, Speaker A: Anyway, first of all, in one variable, it's compactly supported. If I restrict this function away from the diagonal, in the second variable, away from zero, that is, in the second variable, then what I get is a smooth function, c infinity. We know that each of these integral kernels of all of our operators big s, are given by, well, continuous functions if we're in order less than minus n. But those continuous functions can only have a singularity at zero away from zero. They are c infinity functions, because we proved that all of these operators are pseudo local. And that's what pseudo local amounts to if you're talking about convolution operators, for an integral operator given by a kernel k of x y p, pseudo local means it's a c infinity function away from the diagonal x equals y. In this convolutory situation, pseudo local means smooth function away from zero.
00:23:45.198 - 00:24:27.366, Speaker A: So as soon as you remove zero, you're talking about c infinity functions. And it was a smooth family. So in fact, these guys are smooth in both variables like this. Okay, we're imagining that this function, what did I say? Pk was ck to begin with. So maybe I'll just stick that up there. Okay. Correct.
00:24:27.366 - 00:24:49.542, Speaker A: Correct. Yeah, all variables. In fact, it's much better in these variables. It's the infinity in those variables. Yeah. Let me just say, let me just add here when I say, satisfy the hypotheses of. Maybe I'll say it like this.
00:24:49.542 - 00:25:07.934, Speaker A: They. So the s function I'm imagining is s, m, j, omega of x. Like that.
00:25:14.034 - 00:25:16.894, Speaker B: We might have k greater than zero.
00:25:19.564 - 00:26:24.944, Speaker A: Yes. Yeah. But there's something else you can say, which is, if you restrict to this subset, which is a copy of rna, then this also is c infinity. So the actual values in the rn direction are c infinity. That's because it was a smooth family. It's smooth in the omega direction, and we're running out of room here. Unfortunately, the same is true if you take any partial derivative, as long as alpha is not too big.
00:26:24.944 - 00:27:11.824, Speaker A: After all, it's only k times continuously differentiable. So you shouldn't attempt to differentiate more than k times. So we have functions which have these properties, and now we want to fiddle with these functions in order to make them smaller. We have a whole bunch of functions like this one for each j or each mj, and our ambition is to add them all up. But we can't add them all up because it might be that s of omega zero is equal to one. Like I just said, for every mj and every omega in this famous compact set. Yeah.
00:27:11.824 - 00:27:36.632, Speaker A: Correct. These are the x's are coordinates in the second variable. Yeah. So we're differentiating in the second variable and then we're restricting in the second variable to x is equal to zero. Okay. And what you get, I'll leave you to think about it, is a smooth function of x, smooth in this transverse direction. Okay.
00:27:36.632 - 00:27:52.460, Speaker A: Now, so let me just go up here. Just leave that. We interrupt, interrupt the proof temporarily, to prove, or at least consider, this lemma. And I didn't tell you what the conclusion, it's not a lemma yet. Right. It's just. It's like a dream.
00:27:52.460 - 00:28:50.476, Speaker A: We're just supposing, you know, then for every epsilon, so sorry, there's an epsilon. Where did it go? I can see Epsilon appearing two times, maybe three times in the class. That's certainly enough for me by now. But anyway, here's one of the appearances. Yeah. Yeah. At any moment, Jamie, everyone in this class is entitled to a full refund of everything that they paid me.
00:28:50.660 - 00:28:51.384, Speaker B: So.
00:28:53.364 - 00:29:57.332, Speaker A: For every epsilon bigger than zero, there is some other function. There's an intermediate function, s, which is going to occur. So let me call this one s double prime. So, first of all, it's only a smoothing operator. If you like difference away from the original s, the difference is c infinity. And if we wanted to, we could make all of these guys supported in now, ball times ball, we could fiddle with these functions to make them all compactly supported in the second variable. That's kind of clear, because away from zero, these functions are c infinity.
00:29:57.332 - 00:31:03.434, Speaker A: So I could easily chop them off by some bump function. That's not difficult at all. And actually nothing is difficult. But the business partner of the lemma is the third point, which is that, how to say it? For all omega and all alpha partial indices, multi indices, let's say less than or equal to p. If you look at how big are the derivatives of the s prime family with respect to the x variables, again, we're just differentiating in the x direction and we're only interested in what happens at zero. Zero is the tricky place. All of these fellows here less than epsilon.
00:31:03.434 - 00:31:24.794, Speaker A: All alphas or omegas. Yeah, there we go. Oh, I did not write this right, so I'll correct this in a moment. P is less than or is equal to k. That's correct. I had in mind the proof here. I want to do this everywhere.
00:31:24.794 - 00:32:03.976, Speaker A: These functions are just plain small. And actually, if I want that, maybe I should put here k minus one or alpha equal to k. No, k is good enough. Let's write the first time so we can make them small. Here's what we're going to do. We're going to take each of the family's big sm. We're going to take that family and represent it by a little sm.
00:32:03.976 - 00:32:53.352, Speaker A: As we were discussing on the board that's behind this board, we're going to take that little sm and modify it in exactly this way. And after we've produced the modified little s m, we'll get a corresponding modified operator big, whose integral kernel, whose family of integral kernels is s double prime. Okay, let's drop the primes. Can't always be talking about s double prime. So we take the family of operators, and each family we adjust by some c infinity function, some smooth family of smoothing operators corresponding to this difference. And after we've done so, we get a whole bunch of operators represented by integral kernels, which are really small and they're really small even after you differentiate them this number of times. So now you can just add them up.
00:32:53.352 - 00:33:20.750, Speaker A: And after you've added them up, you get, you get the desired operator a wherever it was. Yeah. A this one here. So this is the business part of the proposition. How do you get a. Well, you use the, these formulas for the new s's, and you just observe that for the new s's, the infinite sum makes sense. And that's going to be the a.
00:33:20.750 - 00:33:28.590, Speaker A: It's the actual legitimate in the sense of calculus, infinite sum of the modified s's, where each s is modified in.
00:33:28.622 - 00:33:35.594, Speaker B: This way, pushing stuff further out in the expansion. You have to do this one.
00:33:37.374 - 00:33:37.686, Speaker A: Push.
00:33:37.710 - 00:33:44.226, Speaker B: It a little further, and then that is you make s two property so you can get something near zero that adds up.
00:33:44.410 - 00:33:58.014, Speaker A: Yeah, well, you can do them all at once. I mean, each SM has to be subjected to this treatment r. And it didn't tell you what epsilon is. There should be a bunch of epsilons which are so small that they all add up like a two to the minus j or something. Yeah.
00:34:00.954 - 00:34:05.766, Speaker B: So when you change the s to s double prime, doesn't that change?
00:34:05.810 - 00:34:17.074, Speaker A: Change, no, because you're only changing each of these operators by a smoothing operator which has order minus infinity.
00:34:20.574 - 00:34:22.914, Speaker B: It all accumulates in minus infinity.
00:34:24.054 - 00:34:35.458, Speaker A: There's a whole bunch of little order minus infinity terms which, using, which you've done various modifications. Yeah, it was a question, is a.
00:34:35.466 - 00:34:51.774, Speaker B: Question just like how you, like, have these s's. Add something, build the capital s operator, take off of that thing.
00:34:52.994 - 00:35:09.758, Speaker A: Yeah, well, not off of it. Oh, off of, you take, let me say this again. You take up of modified SMJ. You observe that the series consisting of the sum of op of modified SMJ actually converges.
00:35:09.886 - 00:35:18.734, Speaker B: This is what I'm saying. Like, can I, okay, can I just sum the functions and then do it like. So you're saying pick the sum.
00:35:18.774 - 00:36:10.904, Speaker A: I'm saying if you sum the functions, you get a function, or correspondingly an operator, which is not a symbol family, because you're adding together symbol families of different degrees, which is not. So you have to build the operators first and then add them up. Or you have to extend the op construction to some families of operators which are not symbol families. So yeah, you apply. What are we actually doing when we say summing them up? We're saying that for each smooth function f, and for each point little x, a sum like this. Let's just put it in a little box. And I realize I missed up a small thing here, so I'll add it to the discussion.
00:36:10.904 - 00:37:19.548, Speaker A: So we have these modified guys, smj, omega. We want to apply them to a function and we want to add them up over all j. And the first thing we want is that we'll have arranged that this thing here converges. So that tells you what the operator is as a map from functions, let's say smooth, compactly supported functions to just functions. Now you have to ask, is this function defined by this convergence series is it smooth? Does it vary smoothly with omega? Can it be extended to a scaling family so that it really is a scale Abel operator? There's probably other things, too that I haven't thought about. All of those things you have to check. But the basic thing to get off the ground is to say the operator a is going to be defined by this formula a applied to a function f evaluated at a point x will be this sum, which will converge.
00:37:19.548 - 00:37:47.150, Speaker A: I mean, what? It will converge, definitely, if you believe this recipe. And if the epsilons have been chosen one at a time for j equals 12345 and so on, in such a way that the epsilons converge. So, for example, choose epsilon to be two to the minus j. We're interested in just this convergence to begin with. Okay, so that defines a bunch of operators that we call. Let's. Yeah, you're right.
00:37:47.150 - 00:38:09.154, Speaker A: I screwed something up there. We call these, I don't know, let's call them super big s sub omega of f. Right. Now, to define the operator a, I lied. That is to say, I made a mistake. It wasn't, I mean, it was a lie, but it was an unintentional lie. That's called a mistake.
00:38:09.154 - 00:38:53.152, Speaker A: To define the operator, we define a of f at x to be super big, s sub x applied to f and evaluated at x in the usual way. So that's the actual formula for a in terms of super big, the family super big, omega. And everything has to be smooth in all possible variables. And that's what you have to manage. And that's what makes this a little bit painful, which means I'm not going to do it in front of you. I'll do some of it. I will show you a little bit about how this is proved.
00:38:53.152 - 00:38:54.124, Speaker A: But. Yeah, go ahead.
00:38:59.584 - 00:39:05.780, Speaker B: But we're allowed to subtract some little bump function around zero, which is going to be our double prime.
00:39:05.852 - 00:39:06.388, Speaker A: Yep.
00:39:06.516 - 00:39:08.548, Speaker B: Or s double prime. Maybe the difference.
00:39:08.676 - 00:39:16.012, Speaker A: Yep. No, s double prime. Well, yeah, sorry. S double prime is what you get after subtracting the bump function. Yes. Okay.
00:39:16.068 - 00:39:23.932, Speaker B: That's right. And then, because we're allowed to add smoothing things. But aren't we adding an infinite number of smoothing?
00:39:23.988 - 00:39:26.864, Speaker A: Yep. So you have to be cautious.
00:39:27.964 - 00:39:31.384, Speaker B: And aren't we just picking hand down the road back in the world?
00:39:31.684 - 00:40:12.824, Speaker A: Well, you got to be cautious about that. You're allowed to throw that term away. This difference, which is a c infinity function, is not something you're ever going to study again. You're not, for example, going to try and add up all of these differences that would be like taking this sum for the original S's which isn't going to converge, and somehow comparing it to the new sum for the new s's which does converge, that the whole point is to get rid of the old s's which don't converge. And we've done it by making these modifications. So the size of this is of no relevance. So these could all be identically equal to one at x equals zero.
00:40:12.824 - 00:41:03.112, Speaker A: And that would be no problem because we're never going to attempt to add those up. Yeah, yeah, sorry. I can just for the sake of the historical record, just adding a few words then. Yeah. So first you figure out how to add up the s's. Then you define the a's in terms of the s's the way you always did. And the a sub t's for that matter, in terms of the s is the way you always did.
00:41:03.112 - 00:41:44.426, Speaker A: If you're talking about families. Speaking of which, and there are a billion things to check, you know, is it smooth in this variable? Is it smooth in that variable? Does it satisfy the scaling relation? You have to check that too, you know, on and on and on. That's, see I'm the professor and you guys are the students, so that's what you get to do. Yeah, that's right. Well, okay. Okay. I think, I think this counts as just one epsilon all together.
00:41:44.426 - 00:42:45.274, Speaker A: It just counts as one episode. Okay. I propose not to say more about the mechanics of the overall argument, the checking of these hundred things. Does it really converge? Does it really define this series in the box? Does it really define a smooth function of x? Does that smooth function of x really depends smoothly on omega? Is the resulting operator really continuous as an operator from cc infinity of rn to cc infinity of rn, which it has to be to fit into our theory. Does it really satisfy the scaling relation? That is to say, can it be extended to a family, the obvious family? And then does that family satisfy the scaling relation? Answer, in a word, yes, but lots of things to check. Okay, no one ever writes this. You know how it is.
00:42:45.274 - 00:43:12.102, Speaker A: Uh, no, no. It's certainly, if you look in the paper of Bob and Eric, they, they do this much less justice than even. I'm uh, doing it right now. And it's traditional, yeah. To, to just kind of bully your way through this. And I'm doing, yeah, I'm, I'm helping you guys out much more than your typical pseudo differential operator professors. So just, just be grateful for that.
00:43:12.278 - 00:43:21.474, Speaker B: Sounds like the theory is right for some way. You can just build this so that we don't have to keep checking it over and over again.
00:43:22.734 - 00:43:53.808, Speaker A: Yeah. Oh, it's not backcomp. No, let's not exaggerate. No, it's, it's not that complicated. No, no, no, it's, you can do this in five pages. It's not that bad, but it just requires a steady hand to get all the quantifiers in the right order. And you could alternatively just ignore this issue.
00:43:53.808 - 00:44:37.852, Speaker A: What we're going to try to do in that was the original subject of this lecture. Hopefully we'll get to it in good time is invert certain operators, modular smoothing operators, by means of some iterative technique. We have an operator like the Laplace operator. It's a fact that the Laplace operator, which is a scalable operator of order two, is invertible modular smoothing operators, and the inverse is a scalable operator of order minus two. And that's relevant to many things. It proves, as we saw much earlier in the class, it proves that the Laplace operator is hypo elliptic. Any solution of delta U equals v for v, smooth has to be a smooth function u.
00:44:37.852 - 00:45:18.880, Speaker A: That's a theorem which will follow from all of this. And we'll be able to do that not just for the Laplacian on flat space, but for any Laplacian built out of any bunch of gijs with any metric whatever you like, any elliptic operator at all. So that's why we're doing this. You could say, well, why don't you just prove it's invertible modulo not smoothing operators, but CK integral operators for every k, wouldn't that be just as good? And the answer is probably, yeah, for all. I mean, that also implies hypoelipticity, if that's true for every k. So take your pick. If you choose to ignore what I'm now telling you, that's fine.
00:45:18.880 - 00:46:28.800, Speaker A: And it just means that somewhere else there'll be some extra quantifiers. There's an inverse module, or CK operators for every k. So you can take your medicine now because it's good for you and just, you know, not complain, or you can forever be adding extra quantifiers down the road. What will it be? Okay, good for you, good student. All right, let's take a look at very quickly how this thing is proved, and then we'll be done somewhere, I don't know, back over here as I'm giving these lectures, because I'm writing up notes and I'm saying to myself, you know, I ought to have done this a different way. And so what's going to happen is that, as previously advertised to, at a certain point, we're going to do this whole thing again. I'm going to tell you what is the tangent groupoid, and you're going to say to yourself, thank God he finally told us what is the tangent groupoid, because now everything looks simple and beautiful.
00:46:28.800 - 00:47:25.774, Speaker A: And when I do tell you what is the tangent groupoid? And when we do all of this again in the context of the tangent groupoid, there'll be a chance for me to tidy up some of the things that I've done in the past, change the definitions a little bit to define something equivalent, but to define something in a different way, the same thing in a different way. So that's kind of what I'm planning on doing. I'm more or less following what Eric and Bob are doing in telling you all of this stuff, but I feel one could adjust things a little bit and make the whole story just a little bit simpler. Okay. And let's now go back to that lemma somewhere, maybe here. So I need a little bump function. I don't know, sigma.
00:47:25.774 - 00:48:27.564, Speaker A: So a little fellow that's smooth and compactly supported in this ball that appears over there. I'm just going to multiply everything by sigma and then we'll be done. It'll be compactly supported. And I wanted, I wanted to introduce some intermediate thing. Yeah, let's do it this way. Let's take a look at this fellow. So I'm just going to multiply by this cutoff function in the sigma variables.
00:48:27.564 - 00:48:46.744, Speaker A: Just taking a look and seeing whether I like this. Yes, I can see what I'm going to do. Okay, good. Here's the old one. And suppose I were to stop right here. Just close that parenthesis. Let's just examine what would happen if I were just to close that parenthesis.
00:48:46.744 - 00:49:44.442, Speaker A: Would the left hand side be equal to the right hand side? Well, no, because I multiplied by this cutoff function. But the difference between the left hand side and the right hand side would be a smooth function in all variables. If I said something a little bit more about this sigma at the moment, I could have chosen a zero function, sigma. So I want a function which is identically one in some neighborhood of zero. That would be a better choice. Now the difference between the left hand side, excuse me, this guy. The difference between this guy and s of omega x would be a smooth function because the difference would be a function which is identically zero in a neighborhood of x equals zero, which is where all the problems were.
00:49:44.442 - 00:50:28.594, Speaker A: So where this function is not a c infinity function. So, by multiplying by sigma of x, I've done no real harm. That s prime is going to differ from s by a c infinity function. However, I haven't done anything for this yet. So let's get to work on this thing by subtracting off a Taylor series. So what shall we do? We'll just take all of the derivatives of the original s and we'll just evaluate them at zero. Oh, and if it's a Taylor series, you're supposed to do two other things.
00:50:28.594 - 00:51:26.814, Speaker A: One is you're supposed to divide by some factorial, and the other is you're supposed to put some power axis like that. All right, so I just replaced s as a function of x by its Taylor series up to degree k in the x variables, and we didn't do anything to the omega variables. So far, so good. Well, what happens when you do this? Well, you obtain a difference here, forget about the cutoff, that's not immediately relevant. You obtain a difference which vanishes when x equals zero and its first derivative vanishes in any direction. Vanishes when x is equal to zero. All of the derivatives of this difference at x equals zero vanish on the nose.
00:51:26.814 - 00:52:01.422, Speaker A: That's what Taylor series are for. This is just for the compact support that we wanted here. So this guy and all of its derivatives are really, really small when x is equal to zero. That doesn't quite tell us this, but it's getting to be in, looks like we're going in the right direction. Why? If all of the derivatives are small when x is equal to zero, then this function itself is really small in a small neighborhood of x equals zero. So if I do more cutting off, so I cut off this function. So that's supported really, really close to x, equal to zero.
00:52:01.422 - 00:53:12.706, Speaker A: It's going to be there really, really small. That's what we're going to do. So this is just s prime. And now define s double prime of x. Well, I'll just cut off again, but much more close to zero with some suitable delta, much, much, much smaller than the epsilon that's given to us on the previous board. So what we can say about this function s prime is that it's little low of x to the p. It is bounded, not just bounded by a constant times the norm of x to the p as x goes to zero, x to the k.
00:53:12.706 - 00:53:46.510, Speaker A: Excuse me, I keep wanting to call that p. It's actually bounded by a function of the norm times x to the k. And that function is continuous, and its value at zero is zero. Okay, so this function is really, really small. Write it down correctly. And after you differentiate it p minus one times or p times even, it's still going to be really small. After you differentiate it p times, you'll get out a whole bunch of one over deltas.
00:53:46.510 - 00:54:16.046, Speaker A: Not good. That'll make it really, really big. But you're dealing with a function which is so small that even after you multiply by one over delta p times, you'll still have a function which goes to zero as x goes to zero. That's how it works. So let's just summarize. Is it still here? Well, half of the theorem is here. This is what we were trying to prove.
00:54:16.046 - 00:54:42.054, Speaker A: We were trying to prove that you can add these up. There is an operator a which is equal to the sum of all of these op of s's in the technical asymptotic sense, adic convergence that we were talking about before. And because I'm in charge and I'm old and lazy, I'm going to declare this theorem proved. You've got a problem with that?
00:54:44.154 - 00:54:47.294, Speaker B: And that's essentially over zero.
00:54:48.034 - 00:55:20.886, Speaker A: Yeah, it's proved by means of this argument in the box. The idea is extremely simple conceptually. There's nothing going on here at all. But I have to admit, there's a large number of things to check. Smooth in omega, smooth in x, scaling relation, et cetera, et cetera. Well, good day's work there. Let's move on.
00:55:20.886 - 00:56:37.364, Speaker A: Let's use this thing somewhere, maybe over here. Got to keep the cinematographer busy. All right, so the reason this whole subject was invented was the following definition, which I'm going to give in a sort of a odd and clumsy way, just for technical convenience. We'll have a discussion after the definition. This definition that I'm about to give is equivalent to at least two other definitions which are more elegant than this one. But this is a good balance between definitions, which are extremely easy to check, and definitions which are extremely easy to work with. So let u be an open set.
00:56:37.364 - 00:57:05.300, Speaker A: I guess, implicitly. There are some epsilons in the background there, hidden epsilons there. We're just back to just one family of operators now. And it'll be of degree m of any order. It doesn't matter. Secretly, we're thinking of m equals two. That's a sort of classical example.
00:57:05.300 - 00:58:17.684, Speaker A: But m could be anything. If I have a symbol family s, let's say degree two m. We'll say that this thing is elliptic over u, that's why u comes in if. And now we have to decide exactly which flavor of definition we would like. And so let's relativize this from you to a slightly smaller open set v which is compact within you. So the closure of v sits in u and it's compact just because it helps. So for each open set u which sits inside, excuse me, v which sits inside you and its closure sits inside you as a compact subset of you.
00:58:17.684 - 00:59:50.278, Speaker A: So I'm going to allow myself a little bit of wiggle room here by specifying a compact piece of you and then saying that something should exist, so to speak, over that compact piece of you, rather than just over all of you. In fact, you can build this thing that I'm about to tell you over all of you all at once. But it's easier just to work with this. There is some other symbol family, let's call it t of the complementary degree degree minus m. And what we're just going to ask is that if you suppose you compose s with t omega y, so s omega with t omega, then you'll get a family of symbol, a symbol family of degrees zero, because m plus minus m is zero. And if you subtract the family of identity operators, that's also going to be a symbol family of degree zero. And what we're asking is, does this actually be zero or at least a smoothing family? So this thing here, I want to be a smoothing family, except things get a little hairy near the edges of u.
00:59:50.278 - 01:00:55.214, Speaker A: And so let's just stick in a cutoff function here, and let's do it the other way around, too. So these should be smooth families of smoothing operators. Can you say again? Yeah, so you choose the v first, then you have to choose t, depending only on v the way I've written it. And then for every phi, we haven't told you what the condition is, whose sports is in v. Now you want this to be true. Suppose you make a mistake when you're writing down this definition in any one of the conceivable ways, by mixing up the quantifiers, no bad thing will happen. You'll get another definition which is equivalent to this definition.
01:00:55.214 - 01:01:43.514, Speaker A: At the one extreme, you could ask that each s omega, each individual s omega should be invertible by some individual t omega. So that s omega times t omega minus the identity is a smoothing operator, and the same the other way around. And the only thing you need to add to that is that the t omega should have homogeneity degree minus two. So minus m, you have to, that's important. Okay, so if point wise, a condition like this holds, then you're in business. In fact, this condition holds, which is a condition spread over any open subset v whose closure is a compact subset of u. And if this condition holds, then in fact you can build a family of operators parameterized by the points of view.
01:01:43.514 - 01:02:25.284, Speaker A: Now we didn't, I guess we did talk about that, a family of operators parameterized by the points of view, which is a scaling family of order minus m. We didn't technically say what such a thing is. All of our scaling, excuse me, symbol families were parameterized by rn, but you could just relativize to an open subset of rn and you could talk about a symbol family over u. So the other extreme s is elliptic. If there's a symbol family of degree minus m over u such that these compositions are smooth families of smoothing operators over u. You could just build one family which doesn't depend on v at all. That's the one extreme.
01:02:25.284 - 01:03:05.604, Speaker A: The other extreme is you just check this thing point wise, where the closure of v is a single point, if you like, and that's not quite possible for an open set, but you get the idea. Okay, so all of these things turn out to be the same. And the reason I wrote this one down is that it's just sort of relatively easy to work with. If you go all the way up to the edges of u, you need another sort of auxiliary argument to deal with what happens as you go all the way to the edges of you. And if you make this condition, if you make the definition of point wise condition, you need some other auxiliary arguments to spread from a single point to a little ball around that point. This seems like the happy medium, but they're all the same. So knock yourself out.
01:03:05.604 - 01:03:59.404, Speaker A: You can define ellipticity however you like. It will always be the same thing, as long as this is part of your definition. This is crucial for the whole enterprise. If you remove that or you change the degree, you get a definition of something else. Okay. If you just say it's invertible modular smoothing operators by some other operator that, then you're talking about something else which is not ellipticity. Yeah, people talk about all manner of variations on hypoelipticity and they, they put something else in the box quite often in order to study this or that class of hypoleptic operators.
01:03:59.404 - 01:05:00.476, Speaker A: So hypo elliptic means that, as we said before, that solutions to equations with a smooth inhomogeneous term are smooth. So if Au equals v and v is smooth, then u is smooth. That's what hypoalliptic, that's the official definition of hypoallptic. And the way in which this definition tends to get modified is you get rid of this homogeneity condition and you say something a little more crazy, like this operator t should be built of operators whose degrees sort of range around m. Maybe you don't use exactly a degree M inverse, but you use a degree m inverse plus a little bit of degree m plus one, plus a little bit of degree m minus one. I don't know, something like that. Then you could, you could knock yourself out again and try and build some, some new definition.
01:05:00.476 - 01:05:03.384, Speaker A: Yeah, so we've sort of seen that.
01:05:04.924 - 01:05:10.304, Speaker B: If you have an operator that has a parent parametric, then it's.
01:05:10.844 - 01:05:11.292, Speaker A: Yeah.
01:05:11.348 - 01:05:14.784, Speaker B: And is this sort of like putting an additional condition on?
01:05:16.424 - 01:06:07.234, Speaker A: Yeah. So what we're going to do is argue state in the very next theorem which will appear in that board, I hope up there that if you have this condition, and this condition is supposed to be easy to check, because although I didn't write it that way, it's actually a condition you can check point wise. If this condition holds, which is supposed to be something that's easy to check, then there will be a parametric. And that parametrics will lie in this world of scalable operations generators. Let's write that down. What I just said when I was waffling about variations on the definition of ellipticity was not exactly accurate. But since it was just waffling, I won't go back and fix it.
01:06:07.234 - 01:06:55.114, Speaker A: Okay, so what do I want to say? And then again, it's, it's a matter of getting. So there are a billion versions of this theorem. I chose one that I thought might be a reasonable. Yeah, well, we'll see about this. Yeah, okay, maybe I'll write down a theorem using the same sort of language that's up there. So assume that some symbol family s degree. You know what I should do is start with a single operator.
01:06:55.114 - 01:08:57.613, Speaker A: Suppose I have an operator which is scalable of order m as a symbol, which is scalable elliptic over some u. Sorry for the elliptic way of speaking there, or writing. And so let me continue with this funny business. Each time you have an open subset whose closure is a compact subset of you. Just to make my life simple, you can find scalable operator b, I guess, of order minus m such that it inverts a on either side. Well, if it were to invert a, it would mean that these operators are zero and that wouldn't give me any place to tell you anything about u or v or anything like that. So I need to stick in some phis here so that these guys here are smoothing operators for every by smooth, compactly supported function whose support is n b.
01:08:57.613 - 01:10:32.324, Speaker A: And you could ask this b that's constructed, how much does it really depend on v? And the answer is that if you do the math properly, you can choose one b which works for all versus at once. So you didn't actually need to put the v in the picture. There is a b which you can choose before you choose the v, and then you don't have to mention v at all in this theorem. But then it's just a little harder to prove the theorem. Okay, so let's look at an example somewhere, maybe here. So suppose you have an operator which is a differential operator, which is made out of a whole bunch of functions, gij plus a whole bunch of lower order terms. What's another letter? H h k plus h I k.
01:10:32.324 - 01:11:49.384, Speaker A: So there's a general second order differential linear differential operator, except I called the leading order terms gi coefficients, gij, and the traditional, the traditional hypotheses that you put on these functions is that if you make them into a matrix and you put a minus sign in front, then you get a positive definite matrix for all x. All x, well, at least all x in u. Yeah, if you have the, the Laplace beltrami operator in a coordinate patch, then it has this form for some complicated h's, which I don't remember what they are. And. Yeah, so that example fits into this picture here. So suppose we have this. Any one of these guys is elliptic over you.
01:11:49.384 - 01:13:17.144, Speaker A: Let's just see why. So this is a scalable, as we know, operator of order two. We know how to make a scale ing family out of this single scale Abel operator. Namely, you just multiply it by t squared, and then you get a family of operators which extends, in the complicated sense that we were discussing, to t is equal to zero in an interesting way, not as the zero operator, but in an interesting way after this rescaling, and the symbol operators that you get from that procedure, we know what they are. Namely, you take the formula for d, you drop all of the lower order terms and you just plug omega into the coefficients. That's just this thing, this constant coefficient operating. And now we want to know, is this thing a scalable, excuse me, is this a family of symbols? Is this elliptic in the sense of this, wherever it is, definition? Yeah, up here over u.
01:13:17.144 - 01:14:23.050, Speaker A: So we have to look at these fellows and point wise and a little bit more over v. We have to somehow invert them. And now we'll cheat in order to do so. Not really cheating, but we'll remember the Fourier transform, which we've been studiously avoiding up to now. But it's pretty hard to do what I'm about to do without the Fourier transform. If you apply this operator to a function, f, and then you take the Fourier transform, then we all know what you get. Well, you get a function of the Bayes variable, psi, psi, one up to psi n, and we all know what you get.
01:14:23.050 - 01:15:05.870, Speaker A: I guess I didn't put the minus sign in, so now it'll come here. You get minus the sum of g, I, j omega. These are just numbers, just constants. But these derivatives just become psi, I, psi, j, like that, and then f. Hat that's what the Fourier transform does. That's why God put it on this earth, to convert differentiation into multiplication. And this is positive definite matrix.
01:15:05.870 - 01:15:51.014, Speaker A: So unless all sides are zero, this thing here is definitely non zero, and I can take one over it to concoct some inverse operator. In fact, it's not just non zero. It's a positive number, as we know, not including the minus sign, which makes it a negative number. Okay, so. So we would like to define t omega by just inverting this thing. So, t omega f hat psi should be. Well, I'm just going to invert this fellow.
01:15:51.014 - 01:16:53.696, Speaker A: I guess I should remember the minus sign. Still make myself just a little bit more room. But there's an obvious problem with this formula, which is that the whole thing blows up in our face at xi equals zero. No problem. Let's just put in some little bump function like this. So that's a sort of definition, but it's not quite yet a perfect definition. It's a sort of definition, because we don't and can't easily say what the.
01:16:53.696 - 01:17:18.076, Speaker A: Well, we can say, but it's a little bit messy. What the Fourier transforms of smooth, compactly supported functions are. They're not smooth, compactly supported functions, they're smooth, but they're not compactly supported. So this formula here doesn't necessarily map smooth, compactly supported functions to smooth, compactly supported functions. That's the problem. Maps Schwarz functions to Schwarz functions. And it does a pretty good job in doing it.
01:17:18.076 - 01:17:52.546, Speaker A: If you compose s with t in either direction, well, they commute with one another. What you get is, well, not the identity function. You get the function multiplication by one minus psi, which differs from the identity function, multiplication one by one by multiplication by the compactly supported function xi. But back in non Fourier transform land, that means it differs by multiplication by not multiplication by operation. By a smoothing operator which has a Schwarz class integral kernel. It's not quite properly supported, so it's a little bit out of our world. Otherwise it's fine.
01:17:52.546 - 01:18:36.110, Speaker A: If we were dealing with Schwarz functions instead of cc infinity functions, this would now be done. But because we're dealing with cc infinity functions, we have to fiddle just a little bit. Yeah, that's a very good question. Not really. It's a little bit of a trial to properly incorporate Schwarz functions into the whole theory. For example, to talk about Schwarz functions on the tangent groupoid, which is where we're eventually going. Some of the things which was self evident when we were earlier working on scalable operators become a little bit more complicated now.
01:18:36.110 - 01:19:09.174, Speaker A: For example, it was evident that every scalable operator was pseudo local because there was some series expressing the integral kernel away from the diagonal. And that series actually was a finite series of smooth functions. In the case of compact support or proper support, if we move into the world of Schwarz functions, then that series is a genuinely infinite series. So you have to worry about adding it up. It does add up and everything is fine, but it's just a little bit more complicated. Complicated. I haven't decided.
01:19:09.174 - 01:19:51.602, Speaker A: This is one of the things I have to worry about in the next week when we get to the tangent group. Or should we just, you know, bite the bullet and define Schwartz functions and then this problem that we're now discussing will disappear? Or is this problem small enough that we shouldn't worry about it? I haven't decided on that. Okay, so we would like to define this. And we're not going to define this because this fellow here is not quite a compactly supported function. It's not quite a properly supported operator. This operator is not quite a properly supported operator. So it doesn't quite fit into our world in all other respects.
01:19:51.602 - 01:20:57.358, Speaker A: It's fine. It really does have scaling degree minus two. So the crucial thing in this definition is satisfied. If you mean that the dilation applied to t minus lambda squared times t is a smoothing operator with a Schwarz class kernel. So apart from this issue of compact supports, everything is fine. And we just have to fiddle with things a little bit to make compact support. If we want to obtain an actual family of teas, we just have to modify this slightly.
01:20:57.358 - 01:22:12.574, Speaker A: We just have to adjust the supportive t. And what that means in Fourier transform land is we want to make this function here into a function whose Fourier transform is compactly supported. And maybe I will leave that to you to think about. So all I need to do is adjust one minus sigma xi, this thing that we built. We just have to adjust it to have compactly supported Fourier transform.
01:22:20.834 - 01:22:30.450, Speaker B: I was just. How do you do that? So sorry, you're saying you want to adjust that thing such that after I take the Fourier transform, it is compact.
01:22:30.602 - 01:23:25.374, Speaker A: Yeah. So after I take the Fourier transform, I want this thing to look like what it does. Now times, let's say, a smooth, compactly supported function of psi, which is equal to one in a neighborhood of zero, which is where all of the interest is. If after I take the Fourier transform, I want to adjust the operator by a multiplication operator, then before the Fourier transform, I need to adjust this by a convolution operator. So I need to take a bump function sigma on the rn, and I need to take its inverse Fourier transform, and I need to convolve with that. That's what you do. Well, okay, fine.
01:23:25.374 - 01:24:08.022, Speaker A: No problem. Yeah, the thing, I mean, it's a little hard to say maybe what the operator is. If you're allowed to talk about Schwarz kernels, then no problem. You just take the Schwarz kernel of this thing, which is a distribution on rn times rn, and you just multiply by some sigma of x minus y, and you're done. That's a new Schwarz kernel whose corresponding operator is the one that we're interested in. Okay, so all we really needed to know was that these Gijs formed an invertible matrix. And, well, we needed to know they formed a smooth family of invertible matrices.
01:24:08.022 - 01:24:47.214, Speaker A: Actually, we needed to know that if you like, the inverses formed an appropriately smooth family. But all of those things are automatic. If you have a matrix which is invertible, then any nearby matrix is also invertible. If you have a matrix which depends smoothly on some parameter omega, then its inverse also depends smoothly on that parameter omega. So all of these auxiliary bits and bobs which are discussed in this definition, you get for free. If you can solve the problem of inverting t omega point wise, excuse me, s omega point wise modular smoothing operators, then you can solve it globally. In fact, on all of you, you don't even have to worry about the voice.
01:24:47.214 - 01:25:04.884, Speaker A: But this is a reason. The way I've written it is a reasonable compromise. Oh, shoot. Between doing it point wise and doing it all on you. Sorry. It's not positivity which is at stake here. It's invertibility, which is at stake here.
01:25:04.884 - 01:25:47.382, Speaker A: Yeah, I mean, that's an easy way of saying the operator is invertible. Yeah. I could have just said it's invertible. Well, I waffled around and said, next to nothing. Let's just wrap this up in 30 seconds. If you have an operator which is reasonable to look at, then the chances are you can check the ellipticity condition reasonably quickly using the Fourier transform, and then you can conclude, using the theory that the operator is automatically, while, among other things, hypolyptic, has an inverse modular compact operator. That's where we are, thanks to this business.
01:25:47.382 - 01:26:11.174, Speaker A: Well, I haven't quite proved the theorem, but you'll see the proof of this is a doddle. This is simple algebra based on what we've done up to this point. There's nothing to this whatsoever. But once we've done that, next time, you'll see that checking invertibility and constructing parametrics becomes, thanks to this machinery, a doddle. Really easy. Just algebra. It's kind of amazing.
01:26:11.174 - 01:26:17.674, Speaker A: Okay, so sorry for going over by a couple of minutes. We are done. We'll continue on Thursday.
