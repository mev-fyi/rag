00:00:02.320 - 00:00:47.778, Speaker A: Today I'm going to be talking about reservoir computing with superconducting circuits. This is Graham Rollins, and I would like to thank all of my colleagues at Raytheon BBM Technologies, as well as our collaborators at the Ohio State University. Our motivation in this work is to create a reservoir computer that is capable of keeping up with high speed communications and that can scale to even higher bandwidth tasks. The outline of today's talk is as first, I'll give a primer on neural networks. Second, I'll go over reservoir computing and more specifically, how hardware implementations of reservoir computing work. Then I'll discuss our implementation and superconducting systems, and then the performance and the actual hardware realization of that system. But first, a preview of what's to come.
00:00:47.778 - 00:01:40.484, Speaker A: We show our superconducting reservoir on the top left. We'll go into more details about this later. The dynamics in these junctions are shown on the bottom left. These oscillatory dynamics form the basis of our reservoir computing architecture, and we use this architecture to perform tasks which are common to reservoirs in the literature. On the bottom right, we show the channel equalization performance of our reservoir recovering symbols over a nonlinear, noisy channel. What we find is that our reservoirs perform essentially at the ideal limit, assuming knowledge of the perfect inverse of of the channel transformation function, and they compare favorably to adaptive LMS schemes and certainly favorably compared to no equalization. The superconducting reservoir is capable of doing channel equalization at speeds around 100 gigabits per second, with just a few Josephson junctions, or a few tens of Josephson junctions, which are easily fabricatable.
00:01:40.484 - 00:02:42.252, Speaker A: This sort of performance can also be seen in other tasks, such as parity calculations, speech recognition, and more. Next, I'd like to talk for a minute about neural networks in order that we can see how reservoir computing is qualitatively different. A schematic of an artificial neural network, one of the feedforward variety, is shown on the top left of the slide. Beta is fed into this network in parallel and passes through a number of different layers, an input layer in green, one or more hidden layers in blue, and an output layer in orange, which produces the results, whether that's a scalar, a vector, or some sort of a categorical variable. In this diagram, each circle represents a particular node, and the dotted lines represent the connectivity between these nodes, each one having its particular weight. The update rule for any particular node is shown on the bottom of the slide. It says that the output of a node is an output function, or, sorry, an activation function, acting essentially on all of the inputs weighted by the weights of all these individual connections.
00:02:42.252 - 00:03:28.974, Speaker A: There's also an optional additional bias b. This activation function f is quite important in terms of its functional form. What we show is one example on the bottom of the slide, which is a nice differentiable function which makes training actually quite a bit easier. You can think about the inference, meaning reaching a decision in the network as occurring in a layer wise operation. Show in the first equation on the right side of the slide, the response of an individual layer, now shown as the bold vector y, is the response of the activation function to all the inputs coming into that layer, and some larger weight matrix for each layer. It's going to subsequently act on the outputs of the previous layer. So what we get is basically a bunch of function composition shown in the second equation on the right.
00:03:28.974 - 00:04:36.102, Speaker A: Training is the prescription of finding out what all of these weights are going to be, and that minimizes the error with respect to the desired output of the system. Training an artificial neural network can actually be quite difficult and time consuming, and it's done by a process known as backpropagation. The general idea is to find the weights that minimize the error of y, which is the actual output of the system. With the target output y tilde, the errors are propagated backwards through the network, one layer at a time, and the weights are updated to improve the guess. Typically one uses a gradient descent method, although there are alternatives where the weights are subject to some sort of an update rule, which is shown on this equation here, where the weight change delta w is given by some descent rate times the derivative of an error function with respect to the individual weight. This error function which we show on the bottom left, is generally some sort of a distance or some sort of a norm between the desired and the actual output of the system. With the derivative of the cost function, you can calculate all of the weight changes, and this information is propagated layer by layer back through the system.
00:04:36.102 - 00:05:37.394, Speaker A: And this is iterated upon until the system converges on a set of weights, which are ideal for solving the problem at hand. The problems are that this is quite slow, it's prone to finding local minima, it's very sensitive to initial weight values and dependent on activation functions. In deep learning, in particular, when there's many different layers, this process can actually get stuck or even explode, depending on the cost landscape. A question one might pose is whether we can think of any natural physical systems that have artificial neural network like behavior apart from the brain. Of course, in terms of condensed matter or other physical systems, it's really quite hard to think of something. And therefore all of the hardware which has been used to accelerate neural network inference essentially implements the same prescription that we saw in previous slides. Whether it's a tensor processing unit, GPU application specific circuit, or something else, they basically solve it the same way as the software networks, and therefore they're ultimately bottlenecked by memory latency and routing costs in the system.
00:05:37.394 - 00:06:37.384, Speaker A: Next, I'd like to talk about reservoir computing, and particularly how that's implemented in hardware systems, and we'll see that this is really quite a bit different than what's done for artificial neural networks, at least when we move to the hardware. Reservoir computing really evolved out of some interesting observations around the training of recurrent neural networks. Recurrent neural networks, which are shown on the left side of this slide, have temporal dynamics that make them very well suited to time domain problems, but they're notoriously difficult to train. If you look at the schematic, you can see why this might be the case. Instead of the feedforward sort of networks we looked at earlier, we see that information can flow both forwards and backwards through the network. Even in these recurrent loops shown, training these is quite difficult, and standard backpropagation fails. Typically, one must resort to backpropagation through time, which is essentially an unrolling of the network, or using global optimization techniques such as genetic algorithms or others.
00:06:37.384 - 00:07:33.074, Speaker A: Reservoir computing really emerged out of the observation that one had a fair bit of leeway in controlling the usefulness of the system for a particular task only by training the output weights in the system. The reservoir in this case essentially becomes a black box whose connectivity is not particularly important and can be initialized to random values. Training the system becomes extremely straightforward when there's only a single output layer. The idea is that you simply run data through the system and perform an explicit matrix inversion to obtain the output weights. So, to summarize the difference between these two approaches, on the left side of the slide, we show the artificial neural network approach. The general you run the system, obtain the outputs, you propagate the errors backwards through the system, trying to converge on weights effectively. You repeat one and two as many times as you need to get the accuracy that you desire.
00:07:33.074 - 00:08:06.326, Speaker A: And finally, once you're all done, you can run the system with the final trained weights, of which there may be many. The reservoir approach is quite different. You run the system to obtain the signals x e. You calculate the explicit output weights wout, and you run the system and simply multiply by these weights thereafter. How you actually achieve these weights is shown on the bottom right. You can use something as simple as the Moore Penrose pseudo inverse, which is really just a matrix inverse. You can also use ridge regression, which is just a way of keeping the weights from becoming too large.
00:08:06.326 - 00:09:12.704, Speaker A: Subsequently, as we see in step three, the guess for what you want is just the weight matrix multiplied by the output of the reservoir. The differences in the training modalities between neural networks and reservoirs extend into the regimes of their applicability. Neural networks are very extensible and can be modified to suit a number of different problems. This can be quite expensive, however, because any changes in the network topology require retraining, and as we know, back propagation is a very time and energy intensive process. To put some numbers to those claims, if we look at the training overhead for a modern network, something such as the transformer network, which is used for machine translation, we know that approximately 200 million free parameters need to be discovered in order to perform the tasks that are required. This can take in excess of a million dollars of cloud computing costs, a megawatt hour of electricity, and a million pounds of CO2 emissions, generating a gigabyte of weight data which is not insubstantial. Advances such as spiking neural networks might go a long way in reducing these sorts of costs, but that has yet to be realized at scale.
00:09:12.704 - 00:09:59.452, Speaker A: As we mentioned before, hardware acceleration in these systems basically does the same thing that software networks do, and therefore they're ultimately going to be bottlenecked by memory access and routing delays. Reservoirs, on the other hand, are much more general purpose. A single reservoir might be generally larger than a neural network that solves a similar task, but the same reservoir might be used to solve many tasks. Very little redesign is needed, and in fact, almost no redesign is needed if the reservoir is sufficiently complex. Training is fast and easy. As we know, the reservoir is effectively operated in the forward or the inference mode, and the weights are simply solved for this gives very minimal overhead. Hardware acceleration is really where reservoirs excel.
00:09:59.452 - 00:11:08.402, Speaker A: Since a reservoir is effectively a black box, really any sort of a system can be used to replace its functionality. Now, what can a reservoir actually do? Apart from a number of things that neural networks can do, they're much more useful at time domain tasks solving time domain problems. They also have very interesting applications to nonlinear forecasting, analog simulation, nonlinear control, and these sorts of things. To show one capability of a actually software based reservoir called an echo state network, we show how the Echo state network here is trained to emulate a chaotic delay differential equation known as the Mackey glass system. The reservoir is trained for some interval on the output of the Mackey glass equations shown here, and at some point the output of the reservoir is fed back into the input and allowed to free run. So the time domain trace of this is shown on the bottom of the slide with the target Mackey glass behavior shown in black, and the free running echo state network response shown in red. The actual attractors from this data are shown on the left and the right side of the slide for the actual Mackie glass attractor and the reservoir on the right side.
00:11:08.402 - 00:12:02.214, Speaker A: We see that the reservoir is actually able to emulate the full attractor of the system in the chaotic regime, which is quite impressive. So now that we've seen a little bit at least, of what a reservoir can do, can we think of a natural physical system that behaves like a reservoir? And the answer, somewhat conveniently, might be almost all of them. As we mentioned earlier, a reservoir is effectively a black box which maps inputs to outputs. We can replace this black box with a physical system in which nature does the computation for us. The evolution of the physical system provides the reservoir functionality. No weight memory is required, and basically natural variation in physical system parameters are sufficient to provide the richness in the system that we require to perform computation. Most importantly, the computation can take place at the natural speed of a physical system.
00:12:02.214 - 00:12:56.960, Speaker A: The only requirements are that the response must be nonlinear, repeatable and complex. It must also exhibit the fading memory property, which is something we'll go into detail a little later. A number of different physical systems have actually been used to implement reservoirs, MEMS, memristors, FPGA's, pintronics, optical systems, and many others. One of the canonical examples is shown in the bottom. Effectively, a bucket of water was used to perform spoken digit recognition with a clever input and output scheme where the ripples in the water were read out in response to input data, which was translated into motion of servo motors, which were plinking motors in and out of the water. Now I'd like to talk about our actual architecture. Our superconducting reservoirs are composed of a large number of superconducting oscillators arranged as shown on the bottom right side of the slide.
00:12:56.960 - 00:13:35.704, Speaker A: We'll go into more details about that later. These sorts of oscillators are very good candidates for hardware reservoir computing for a few reasons. First of all, they're highly nonlinear, and second of all, they're very easy to couple, whether that's resistively, inductively or capacitively. This easy coupling eliminates the need for using virtual nodes which can drastically degrade system throughput. Virtual nodes are really only required in oscillator systems with a single oscillator, which need to have virtual nodes in order to have rich enough dynamics to support reservoir computing. In our sorts of systems, this is not required. This makes scaling to very large numbers of nodes and very high throughputs quite feasible.
00:13:35.704 - 00:14:17.604, Speaker A: Next, our oscillators can be operated at extremely high speeds, easily in excess of 100 GHz. This means that they can be used for applications such as high speed channel equalization for 5g networking, or that they could be used to accelerate lower rate tasks by many orders of magnitude. On the top right of this slide, we plot what we expect to be the throughputs of a variety of different reservoir computing systems. Finally, these reservoirs can be interfaced directly to superconducting logic schemes, which provides components such as ADC's, counters, integrators, etcetera that can all operate at 50 GHz or beyond, really eliminating the sort of bottlenecks that would be required for room temperature control and readout.
00:14:18.784 - 00:15:14.558, Speaker B: Before going into specifics about our architecture, we give a brief overview of Josephson Junction based oscillators. A Josephson junction shown on the top right of this slide consists of two superconducting regions coupled by a weak link. We speak about the phase phi, which is the difference of the superconducting phases on either side of the weak link. The current and the voltage of the Josephson junction are given by these two relationships shown by the first two equations on this slide. With the addition of an external current I, the effective potential becomes a classic washboard, which is shown on the bottom right of the slide. As one increases the current being applied to the Josephson junction, eventually one induces oscillations into the voltage state, which are shown on the right side of the slide. Increasing currents I cause increasingly larger oscillations at increasingly higher rates, as is shown here, including the capacitive and resistive loading of the junctions.
00:15:14.558 - 00:16:09.428, Speaker B: The dynamics are analogous to a pendulum where the current is a direct torque. The oscillations are highly non sinusoidal and they correspond to a pendulum which is going over the top rather than undergoing small oscillations at the bottom. The frequencies of these oscillations, omega C, can be quite high. They can be easily 100 to 350 GHz or above. If you arrange a number of Josephson junctions in a line in this parallel geometry shown on the right side of the slide, you can obtain a Josephson transmission line. The operation mode which is shown here is that all of these Josephson junctions are biased close to their critical current. If any one junction is induced to go over its critical current, it causes a single flux quantum to be emitted from that junction, which will travel down the line and cause a cascading effect, where subsequently all the other junctions will emit a single flux quantum as well.
00:16:09.428 - 00:17:02.034, Speaker B: In this way, information can travel down the line. As it turns out, these equations obey a slightly modified discretized version of the sine Gordon equation, which is shown on the left. This supports rich soliton dynamics, including annihilation, pair creation and pass through, all of which have been observed in experiment. It's worth noting that, as with most systems, there is a mechanical analogy which extends this analog of the pendulum to a whole line of pendulums coupled by springs. Our reservoir architecture is essentially a slightly modified version of the Josephson transmission line. We use a parallel input scheme which has been used in mechanical oscillator reservoirs shown in the paper. At the bottom of this slide, at the top right, you see that a bias signal is distributed to all of the junctions and puts them all into an auto oscillatory state.
00:17:02.034 - 00:17:47.440, Speaker B: On top of this, we have an input signal is, which is added on top of the bias. This creates complex wavelike dynamics and propagation of these dynamics throughout the line. In this case, you can see the blue trace on the right side of the slide is what is input into the system. The response is shown at the bottom of the slide. This wave like dynamics, the time average values of which are shown for a few junctions in these orange, green and red traces in the center of the slide. Some heterogeneity is actually required to get useful reservoir computing capabilities in this reservoir. Either this can be achieved by driving a subset of the junctions, as we do here, as shown by the blue input signal only being distributed to a fraction of the junctions in the line.
00:17:47.440 - 00:18:35.594, Speaker B: Alternatively, some spread in the critical current or another circuit parameter can be used in order that we do not excite a uniform mode of the system. As one can see, the sample and hold time can be quite short in the system as low as 15 picoseconds. Depending on the choice of the junction parameters. This time scale can be extended in order to make it more convenient to access the architecture from room temperature. Next we ask, how are we going to read out the system? The reservoir is read out by looking at the average oscillation rates of some subset of the junctions in the reservoir. This is actually more in line with the liquid state machine approach. In simulations, we treat two cases with similar performance first, flow rate reservoirs, where we assume onboard readout logic and sample hold times as short as 15 picoseconds.
00:18:35.594 - 00:19:41.024, Speaker B: We also treat a slower reservoir with additional capacitance and a realistic microwave chain, which is meant for facilitating readout and control from room temperature. In this sort of a system, we can use sample and hold times as long as 250 picoseconds. In order to achieve the slur reservoir, we use a minimal superconducting logic chain, which is shown on the top right of this slide. This decimates the data and compresses the output information into the 12 analog bandwidth we expect coming out of our fridge. The specific operation of this output chain is shown on the right side of the slide passing through an output junction, a buffer, a toggle flip flop, and then an SFQ DC, which effectively gives a non return to zero output with the time encoding of information. Ultimately, we achieve this blue trace, this reconstructed low pass filter of the output oscillation rates at the junctions, and we compare to the ideal case low passing the junction oscillations directly, we see that they compare favorably. Next, we'll discuss the performance of these superconducting oscillator reservoirs on a few tasks.
00:19:42.284 - 00:20:37.722, Speaker A: First, we look at the application of our reservoir to the task of high rate channel equalization, which is shown pictorially on the left side of the slide. What we want to do is to recover the original symbols for a modulation scheme in a nonlinear channel, in this case subject to multipath interference and additive white gaussian noise. The performance of the reservoir is shown on the right side of the slide. In the red dots. We see that our reservoir performs quite favorably when we compare to an adaptive least mean squared channel equalization technique, which is a traditional technique for equalization. This also compares very favorably to the channel equalization with knowledge of the perfect channel inverse function, which is shown by the blue dotted line. In terms of performance, we find that a Josephson transmission line reservoir with 40 junctions can equalize at a rate of 100 gigabits per second while still outperforming the least mean squares algorithm, which has been trained on a number more points.
00:20:37.722 - 00:21:31.924, Speaker A: In the case of the adaptive LMS, that's 10 million training points, whereas the reservoir was only trained with 10,000 points. So we've learned that our reservoir effectively implements a perfect inverse filter, which is able to implement the exact inverse of the channel transformation function. In fact, all of the reservoirs we've implemented superconducting and otherwise saturated this performance, implying that the accuracy among any different number of reservoirs is likely to be quite similar. One might as well then choose a reservoir which has the highest efficiency speed or some other sort of desirable quantity. The next task we examine is higher order parity calculations. The task here is effectively to calculate the product shown by the equation on the left side of the slide for a bitstream which is played serially through the reservoir. This is a somewhat contrived example, but it is known to be a difficult problem in machine learning.
00:21:31.924 - 00:22:27.906, Speaker A: The accuracy of the reservoir in calculating higher order parities from p one through p eight are shown on the plot on the left side of the slide. You can extract important information about the memory capacity and the mutual information in the reservoir by examining this data. What we find, as shown on the plot on the right side of the slide, is that we're able to achieve comparable memory capacities to other reservoirs which are seen in the literature. What is different in this case is that we're able to do this at 50 gigabits per second, which is quite a bit higher than other applications we've seen. The final application we'll discuss today is to speech recognition, specifically to spoken digit classification using a freely available dataset, the link to which is shown on the slide. We use 16 female and male speakers training on ten utterances and testing on ten utterances each. The input pre processing that we use is very minimal.
00:22:27.906 - 00:23:11.204, Speaker A: Raw audio data is low pass filtered with a cutoff of 240 rectified before being sent into system. This could of course just be done using an analog circuit. The audio is then played through the reservoir at about a billion times its natural timescale. The overall performance we obtain is about 70% for the identification of these digits. These are somewhat preliminary results, and we believe we could do better with more optimization. The performance for every digit is shown in the confusion matrix on the right side of the slide, which shows the guessed digit in correspondence to the actual true digit that was spoken. These results are actually obtained for a slightly different reservoir, which consists of only five junctions in a series.
00:23:11.204 - 00:24:00.084, Speaker A: Globally coupled geometry. These results are also basically the same as can be obtained with the JTL reservoir with 40 or 50 junctions. More details on the former sort of reservoir soon we emphasize the usefulness of our reservoir, being able to process raw data. Speech processing machine learning almost always starts with the feature extraction phase, with spectrographic, or even more numerically expensive cochlear transformations of the input data. Even linear classifiers can obtain very good results with its extensive preprocessing, but it takes a lot of time and limits the overall throughput in this sort of procedure. The fact that our reservoir can process raw data allows us to sift through huge amounts of speech data. We envision this being very useful, sifting through a large volume of data, looking for keywords which could trigger a more thorough analysis.
00:24:00.084 - 00:24:57.304, Speaker A: Being a reservoir, the actual keywords being used could easily be substituted simply by changing the output weights of the system. In this last section, I'd like to go over the actual hardware realization of the reservoirs we've been discussing in this presentation. Our chip designs are shown on the left side of this slide. The JTL is shown in the center, which is quite large in this case because we use these shunt capacitors to slow down the pulse propagation times from four picoseconds to 25 picoseconds. This allows us to use sample and hold times on the order of 250 or 300 picoseconds. The highly irregular RF input scheme is shown in the top, and this is used to introduce heterogeneity into the system. As we've discussed earlier, the output stages are only used on a smaller subset of the junctions, eight in this particular case, and this is using this NRZ decimation sort of stage that I showed earlier.
00:24:57.304 - 00:26:02.414, Speaker A: This overall system is compatible with our usual 40 pin high speed fridge layout. We recently received these chips back from fabrication and are very excited to begin measurements. Some photographs of the same architecture I showed on the previous slide are shown here with highlights of the reservoir itself, the Josephson transmission line on the top right of the slide, and the output stage, which is used to step the data down to something that can be measured from room temperature. That's shown on the bottom right of the slide. In conclusion, today we've talked about reservoir computing, which is an effective paradigm for tackling a lot of different time domain problems and can conveniently be implemented a number of different physical systems. What I hope we've discussed is that superconducting reservoirs offer an extremely fast implementation in a low noise system for reservoir computing that is quite compatible with a full digital logic scheme possessing very high rate signal processing capabilities. We hope that by using superconducting reservoirs, we enable application space in the very high frequency and the very high throughput regime.
00:26:02.414 - 00:26:03.734, Speaker A: Thank you very much for listening.
