00:00:00.560 - 00:01:04.944, Speaker A: So today I will talk about theorem of alternative and gale matrix and stress matrix. So I will show you how can we use the theorem alternative to prove some gig matrix to prove some theorem about the stress matrix. And the strengths matrix is related to the EDM completion problem. So yeah, I'm basically following the book about Abdul Faki. So what's the book? Let me show the book. The book. This is the book Euclidean.
00:01:04.944 - 00:02:23.434, Speaker A: So the book is euclidean distance matrices and their application in JT theory. So, so I will, I will go through, I will just introduce the important theories in, in chapter eight. And then the chapter eight also is connected to chapter three and chapter one, chapter two. So use some theorem from previous chapter. Okay, so, so until they. So I talked about this how the infeasibility certificate, right? So, so basically we have a theorem like that. So for linear equality, we have, if you want to certify the feasibility of a linear equality linear system, then we have another problem which is feasible.
00:02:23.434 - 00:03:15.614, Speaker A: So there are two statements. So exactly, if one of the problem is infeasible, it means another problem is feasible. No, and vice versa. So we have a polynomial over c. So this Hilbert is non stennicides. And we also have this linear inequality, which is the sarcasm, right, where we have exactly two statements and we have two statements and exactly one of the statements is true. So all these theorems are similar.
00:03:15.614 - 00:04:22.864, Speaker A: So basically, if one of the problem is feasible is if only another problem is feasible. So now we can, there is actually a theorem of alternative in the same depth programming. So until we have these theorem alternatives for linear equalities for polynomials, today we introduce another theorem alternative in the semi definite program. And this is semi network programming. This theorem usually is the, is the key to prove a theorem about this specs matrix. So the main theorem I want to prove is this theorem. So let GP be an r dimensional bar framework on end nodes.
00:04:22.864 - 00:05:10.112, Speaker A: So r is less, r is less or equal to n minus two. This two. So then, then there exists n minus one dimensional bar framework Gq. So, which is equivalent to Gp if and only if there does not exist, then there are positive semi definite stress matrix omega of Gp. So this, this n minus one dimensional framework is the maximum dimension we can lift because there are only endpoints. So maximum dimension is n minus one. And this.
00:05:10.112 - 00:06:49.514, Speaker A: So this framework Gp is liftable to maximum dimension if and only if there does not exist a non zero positive semi definite stress matrix omega. So this theorem is basically, it's just a EDM version of the theorem alternative. Okay, so yeah, before I so I want to introduce some basic notation and first, so, so, yeah, so we define, define this matrix. G j is equal to I minus one over n e transpose. This matrix is the, uh, it's just the matrix that we can, uh, so called the so called center central matrix, right? So if you have some coordinates of points and then you multiply, uh, this, uh, multiplied by j, then all these coordinates are centered. So, uh, yeah, so this, uh, so, so j squared is equal to j and the rank of j is n minus one. So and the j defends orthogonal projection from s c in.
00:06:49.514 - 00:07:53.474, Speaker A: So here scn is the symmetrics matrix and which are centered. So, so it means that if it is in the orthogonal space of e, right, so this j, different orthogonal projection from the center space to the n minus one symmetric matrix space. And yeah, and we have j times b times j is b if b is already centered. And now we define v. B is the matrix. We can factorize j. So we factor j equals to v, g equals v times v transpose and v is n times n minus one matrix.
00:07:53.474 - 00:08:27.846, Speaker A: Okay, so we had, we had four, four column rank. Yeah. And then you are using, this way we can defend the mapping. Or the linear operator fv is equal to fv. Dot is one half times v transpose times v. This is mapping from the, the hollow space. The hollow space just means so s, sub h n.
00:08:27.846 - 00:08:59.178, Speaker A: This, this space, this means all the matrix. It's just the set of the n by nsymmetric hollow matrix. I. E. The diagonal in phase are zeros. And here this mapping from sh into sn minus one. So because this matrix is hollow, right? So diagonal is zero.
00:08:59.178 - 00:10:01.758, Speaker A: So, so it has the same sh and has the same dimension as s as the n minus one times n minus one. The space of the n minus one times n minus one symmetric matrix. So, so basically our EDM, EDM matrix is a, is a hollow matrix because the diagonal EDM is zero zero. And the PsD matrix are in sn one. So, so the inverse mating from sn minus one to shn, we define it as like. So kv is the inverse mapping. So which is, which is k, v times v transpose where k is just this, k is the diagonal x.
00:10:01.758 - 00:10:46.834, Speaker A: E transpose plus the e times diagonal x transpose plus minus two x. So, so basically k is kx, ij is just xii plus x, jj minus two x xij. Yeah. Okay, so, so now we have two mappings. So fv is the map, that is a linear Alphabet maps the EDM matrix to Psd. Matrix. And the inverse mapping kv maps the Pst matrix to EDM matrix.
00:10:46.834 - 00:12:05.904, Speaker A: So kv and fv defend one to one correspondence between the hollow matrix h of other n and the symmetric matrix of n minus one. And also we defined one r one correspondence between the EDm of matrix of embedded dimension r of order n. So under PSD matrix of order n minus one. So by using this, these maps, we can, when we study the EDM, we can actually study it in the PSD cone, right? So that's how we, that's the idea. So we use this one to one mapping between EDM matrix to study this, the stress matrix. Okay, yeah, so far, are there any questions about this stuff?
00:12:08.244 - 00:12:21.854, Speaker B: This is, okay. Now the one part that I'm not so sure about is when you define the theorem of alternatives, did you have, did you state it in the beginning? I mean, why is that different from far crash lemma.
00:12:23.914 - 00:12:24.974, Speaker A: Which theorem?
00:12:26.514 - 00:12:28.414, Speaker B: Yeah, this theorem of alternative.
00:12:31.194 - 00:12:33.962, Speaker A: Yeah, this is, yeah, yeah, exactly.
00:12:34.018 - 00:12:35.306, Speaker B: For Kash lemma.
00:12:35.490 - 00:13:33.224, Speaker A: Yeah, this is exactly, yeah, sorry. Yeah. Actually, let me talk about the proof, this theorem alternative. Okay, yeah, so this theorem says, uh, uh, yeah, let's a zero, a one, a n be given matrix in sn. So unbind symmetric matrix and assume a zero is positive semi definite and then exactly 102 similarly true. So there exists x in rm subset a zero plus x one, a one plus delta plus x m em is strictly positive definite, right? This is strictly positive definite. Or, or there is this, the y being nonzero matrix and the positive semi definite such that the trace of y AI is zero and the trace of y zero is zero.
00:13:33.224 - 00:15:19.606, Speaker A: Y is, is zero. So this theorem is basically the hyperbole separation theorem, right? Uh, so, so, uh, so, uh, yeah, so first it's obvious that both of this, uh, so, so these two statements can, cannot be both true because if one and two are both true, and then place, uh, and then you have a trace, trace of y and ax, right? It's strictly positive. It's positive like ax, this is x, this is uh, x one, a one plus plus x m em. So if one is true, then this ax is, uh, is, uh, is three to the power seven definite. Therefore, uh, uh, and if, if two, if y is positive semidefinite, then trace of y axis is positive. But this is impossible because tracer, this is just equals trace of y is zero plus the sum of trace of y x I AI. And then this is zero and other than zero.
00:15:19.606 - 00:15:58.492, Speaker A: So there's a, it's a contradiction. So there's a positive on the left hand here is positive. Here is a zero. So there's contradiction. So it can be both true. Well now to prove the other direction, yeah, so let's assume, assume the, the first statement is not true. So, so there does not exist x such that this a zero.
00:15:58.492 - 00:17:26.624, Speaker A: This affine space is this affine combination is strictly positive. So that means uh, this cone and this there's a, so this, this cone of a positive. So that is a PSD cone intersecting ex, right, is just, only has zero, right? Uh, yeah, uh no no. Yeah, no this uh, this is wrong. So this uh uh, intersecting this uh s plus plus this, this matrix, this is set our matrix that strictly part of semi definitely. This is the matrix of PSD matrix of a positive definite. So the intercepting of these two sets are amplitude.
00:17:26.624 - 00:18:50.514, Speaker A: So so you can draw a picture. So here this is the set of the strictly part of semi definite, strictly part of the definite matrix. And this ax is light on this. So like this, right? So this is a, this is the, this is a, this is the, this ex. So so of course ex is contained in this ex is uh, is alpha. Uh, in this um, is this affine subspace, uh, of course ex is containing this matrix, right? Ex is contained in, in this space is contained in a linear subspace. So let's say ex bar which is x zero e zero plus x one e one plus plus xml.
00:18:50.514 - 00:20:42.052, Speaker A: So yeah, so, so ax is ax is contained in a bar x and a bar x is a is a linear subspace. So so therefore you can find a, uh, so given, given two convex so you have given a convex set as plus plus n and the linear and the linear subspace if they only intersect if their intersection is amputated or, or the intersection of the intersection of a bar x with the interior of the, of the PSp cone. It's empty, right. Then you can find a hyperplane that separates this, these two sets, yeah, there's a hyperplane that separates these two sets under, yeah yeah. Under this hyperplane, you know, it contains this, this, this linear subspace a bar x. So therefore if if y is not true then, then this a bar x is then there. That means you can find such a y.
00:20:42.052 - 00:21:51.586, Speaker A: That subject defines a hyperplane and then it contains this, I find subspace this one. Uh, yeah, and uh, so yeah, then I want to show, I have to show that if uh, so if if there does not exist so if there does not exist x such that this, this is true, then, then I claim that a bar x is also there. That does not exist. X such that has a bar x is positive definite. Why? Because it's a zero. It's positive. It's positive, definitely.
00:21:51.586 - 00:22:33.270, Speaker A: If you have a x x zero, x zero, x x m subset a x zero, a one, a zero plus, this is PsD. Then. And then if x zero, if x zero is positive, is positive, right, then you can just scale it to make x zero one. Then you have a contradiction with this. With this. Okay. If x zero is negative, then again, I can add.
00:22:33.270 - 00:23:17.084, Speaker A: Because this, because this, because a zero is positive definite, I can simply add, just add this, add some x zero bar a bar a zero, x zero. I can just add this, add this stuff to make it. I can just simplify. The x zero is zero. X zero plus x zero bar is one. And. Yeah, and the x zero bar a zero is plus d is.
00:23:17.084 - 00:24:27.152, Speaker A: And then I have x zero plus x zero bar times a zero times. This guy being positive with the definite. So. So again, that's a contradiction. And if x zero is zero, well, under I have x one, you know, I have x one, I have x one, a one plus xm em being positive, being positive definite. Well, then I can, again, I can add a very small, I can add a very small, like epsilon is zero. And this will.
00:24:27.152 - 00:24:52.572, Speaker A: Because this is very small. And it. So this, this thing will be to be positive, definitely. And then I scale it, so this becomes one. And then I got a contradiction. So that means. Yeah, this says if one is true, then I can, I can say if, if one does not true, then I can.
00:24:52.572 - 00:25:29.444, Speaker A: I can see if even one, if there does not exist x such that e zero plus x one, a one plus xm plus em being part of. Definitely. I can say it's equivalent to say to saying there does not exist x x zero, x one, x m such that x zero is zero plus x one. A one plus delta plus xm em being, being positive definite. So this is why, this is why equivalent.
00:25:35.024 - 00:25:36.144, Speaker B: Okay, thank you.
00:25:36.264 - 00:25:45.150, Speaker A: Yeah, yeah. And then we have a near subspace. And then we can apply the hyperplane separation theorem.
00:25:45.312 - 00:25:46.018, Speaker B: Okay.
00:25:46.146 - 00:26:00.454, Speaker A: To prove it. Yeah. Okay. Yeah, so this theorem is just hyperbole. It's just hyperbole. Separate theorem of Pakistan. Okay.
00:26:00.454 - 00:27:08.660, Speaker A: And yeah, we define this map and one to one correspondence between the EDM of embedding dimension r order n and the p's matrix order n minus one. And now let's review the stress matrix. Right, so what is stress matrix? So a stress of apart framework is a real value function omega on the edge set of graph g such that this is true as we have seen many times. So, so here the summation is the sum of the edges and omega I j times PI. Pj is zero. So PI, PI is PI is the, is the, is the coordinates of the points. So and then what is the stress matrix? A stress matrix is the following.
00:27:08.660 - 00:28:04.544, Speaker A: So it's unbiased strength matrix such that omega I j is minus omega ij for, for ij on the edge set. And it's zero if I, g is not on the edge set. Right? So here, this, this e bar, g bar is the complement map of g. Yeah, so, so it's the set of null edges. And of course, the diagonal of the stress matrix has to be the summation of other elements that's not the diagonal. So it's easy to say that omega is in the orthogonal subspace of e. All right? So, so omega times e is zero.
00:28:04.544 - 00:28:57.704, Speaker A: So, so that, so therefore, the maximum rank, so the max rank is the follow is just n minus one. The maximum of the omega, the stress matrix is n minus one minus r. So r is from, the r is from, because omega has to certify from here. So from here, omega times p is zero. So p is the coordinates, is the point, is the set of the points. With the translator, each row of the piece is a, is a coordinates of a point. So this one, reduce the rank by r.
00:28:57.704 - 00:29:46.180, Speaker A: This p reduced rank by r, and, uh, under this, uh, this one, right. This, uh, e. So omega has to be, uh, has to be orthogonal to e reduced emission by another one. So therefore, the maximum omega is the stress matrix is n minus one minus r. Well, here I assume that, assume ppa is centered. So p is, p is in e, orthogonal. So therefore, I make sure that e is not in, in the range of p.
00:29:46.180 - 00:30:52.584, Speaker A: So you can reduce the dimension, the rank by one. So here we have this theorem, which is very simple. So if p is the configuration matrix of the r dimensional node framework and omega is the symmetric matrix, then omega is the stress stress matrix of this framework. If and only if omega, omega times e zero. Omega p times p zero. And omega I, j is zero for the ij in the, for all the ij on the narrator set. Right? So, so this is the characterization of the stress matrix.
00:30:52.584 - 00:31:17.794, Speaker A: Yeah, of course. Here I have to assume that p times e is zero. So p is centered. So the points are centered. Uh, yeah, actually, I think it's a p transpose. P transpose times. Uh, yeah, p is, uh, exists, right? So p one.
00:31:17.794 - 00:31:40.554, Speaker A: Trans. P two. Yeah. P, yeah. Uh, okay. So now let's move to the Gale matrix. Gale matrix.
00:31:40.554 - 00:32:26.154, Speaker A: So what is the Gale matrix? So, first, let's define the gale space. So again, by any diametric d of embedded dimension, r. So the Gale space is denoted by gale d. So gi d. This is defined as gale d is the null space of the configuration matrix. P transpose here is the e transpose. E is the matrix of the vector of all ones, right? Yeah.
00:32:26.154 - 00:33:31.292, Speaker A: So, yeah, ea is. And this null space is, of course, equal to. Because I assume that p transpose times e zero, then this nonspace is just the nonspace of p transpose intersecting the nonspace of e transpose. So the nonspace of e transpose, they are. Yeah, so, yeah, yeah, so, so there's a null space of e transpose, does not contain the numb space of a p transpose. Yeah, because the p transpose times e zero. So the numb space of e transpose contains the.
00:33:31.292 - 00:34:12.134, Speaker A: Contains the range of p. Therefore, it does not contain the null space of p transpose. So therefore, this, this one. This one is. This definition makes sense. So the dimension of Kod is the r bar is just n minus one, minus r, right? So minus r is from the non space of p transpose. And minus one is from the nonspace of v transpose.
00:34:12.134 - 00:35:09.094, Speaker A: So this is the Gale space. So now what is the Gale matrix? So the gale matrix is an n times r bar, right? So it's a that forms the basis of the gale space. Okay, so so the gale matrix is just simply a matrix that forms the basis of the gale space. So, so, so, so it has a full column rank. Yeah. So it said this is n times r bar. So the r bar is the dimension of the gale space.
00:35:09.094 - 00:36:20.146, Speaker A: So, Gail, matrix is a full column, four column matrix. It's a matrix of four column block. Yeah, and this gill matrix has this property such that there's z transpose times p is zero and z transpose times e is zero. Right? Because it's from the property of the gale, Gale space. Now, there's a connection between the gale space gale matrix and the stress matrix. So let's let's. Let's see, uh, be the gale matrix of the bar framework, gp of embedding, dimension r of unknowns, and then omega if is a, is a stress matrix of gp of the framework, if and only if omega can be.
00:36:20.146 - 00:37:02.622, Speaker A: Can be expressed in terms of the Gauss matrix. So, so omega is equal to z times phi times z transpose. So d is the gal matrix, and phi is the. Is the. It's a it's a symmetric matrix of, of n minus r minus one. And of course, we have to, we require that the omega I j. Then the interest corresponding to the nan igs are zero.
00:37:02.622 - 00:37:27.294, Speaker A: Zero. So that's it. That's the, that's the characterization of the stress matrix within the, is the gill matrix. So, so, yeah, so the Gill matrix, the stress matrix. The range of stress matrix is just the range of the gale matrix. Right. From this, this, this formula.
00:37:27.294 - 00:37:29.134, Speaker A: So the range of the.
00:37:30.634 - 00:37:46.730, Speaker B: So what is the advantage of using the Gail matrix? I mean, why not just use the stress matrix? I mean, the definition, in one case, you don't have the transpose. Just.
00:37:46.802 - 00:38:16.054, Speaker A: Yeah, this is. Yeah, this is useful to prove things, I think. Okay, I think the one here is to use. To. Oh, to prove, to prove things. And, and also you, you can use this. Now, you have a formula for the stress matrix, right? So, so, so given, you know, given a bar framework, you could, you can explicitly compute the geometrics, right?
00:38:17.514 - 00:38:31.454, Speaker B: You can do in the characterization of the stress matrix that you had earlier. Just a little bit. Scroll up. Yeah, yeah. So, I mean, you can do the same here, right?
00:38:33.494 - 00:39:00.954, Speaker A: But here you have a small dimension, right? So Phi has a small dimension than the stress matrix. Omega. Right? So omega is n by n matrix. But here can be. The gil matrix can be explicitly computed, and only Phi is unknown. So Phi is much smaller. It's n minus r minus one instead of.
00:39:00.954 - 00:39:15.206, Speaker A: So you want to. So therefore you want. If you want to find a stress matrix, you can find the gill matrix. You are restricting yourself to a much smaller space.
00:39:15.390 - 00:39:16.274, Speaker B: I see.
00:39:16.734 - 00:39:33.734, Speaker A: Yeah. Then there's an example I'll talk about. Maybe I can talk this example. Yeah, maybe I show the example now. I plan to show this example later, but since you asked, let me show you this example.
00:39:34.354 - 00:39:55.074, Speaker B: Well, I mean, instead of using the stress matrix, you could have just used the left null space of the rigidity matrix. That's also the similar dimension as what you have. It's not. I mean, the dimension is similar, right? N minus r minus. Oh, wait, hang on.
00:40:00.814 - 00:40:07.914, Speaker A: The last kernel of the digits is r times n, right?
00:40:09.494 - 00:40:15.022, Speaker B: It's n minus r times. Nice.
00:40:15.108 - 00:40:18.178, Speaker A: Yeah. But here is a. Phi is nice.
00:40:18.226 - 00:40:19.934, Speaker B: This is smaller. Yeah.
00:40:21.434 - 00:40:26.574, Speaker A: So actually, you can use this, actually to compute the stress matrix.
00:40:26.914 - 00:40:45.346, Speaker B: Well, it's not n minus r. Sorry. It's even. What is it? Yeah, it's bigger. It's maybe. Yeah, it's something like n minus r.
00:40:45.370 - 00:40:50.734, Speaker A: Times n. Yeah, because the dimension of the rigidity matrix is.
00:40:58.394 - 00:41:24.184, Speaker B: Cannot be more than something like db minus something, d times n minus something, right? So you can think of that as the maximum dimension. So the left null space would be something like dn minus r or something like that. Dn minus r, yeah, yeah. Or something. So this is a little bit smaller. Okay.
00:41:24.884 - 00:41:28.636, Speaker A: Yeah, yeah. This does not involve d, right?
00:41:28.740 - 00:41:30.544, Speaker B: Yeah, it doesn't involve d. Yeah.
00:41:32.504 - 00:42:41.514, Speaker A: Okay. And then later I will show you example how we compute this. How can we compute the stress matrix within the gale matrix, right? Given a bi framework? So, so yeah, this is the proof, is the proof of the theorem, yeah. So for one, so suppose we, so from the definition of the geo matrix, so we have this z transpose times p, zero, z transpose e zero. And from definite stress matrix, omega, p is zero, omega, e is zero. So you compare, yeah, and you compare. So compare these two.
00:42:41.514 - 00:43:44.724, Speaker A: So omega is just a times a transpose times z because they have the same z is just transpose is the null space of p and omega is the null space of p. So there's a linear transformation that samsung, omega is a transpose z, yeah. And they are both of a maximum rank, and also omega is equal, is transmetric. So omega is equal to omega, transport equals z times a. Therefore there are, therefore omega. Just, it's g times phi times z transpose. And phi is symmetrical.
00:43:44.724 - 00:44:49.368, Speaker A: Yeah. Yeah. So this is just some simple linear algebra from, from this tool property, right? So, so from this tool and these two, these are these properties. And of course you want to require that the stress matrix only has zero, non zero metric elements for the non edge set. So we have this condition as well. Okay? So now let's see how accurately we prove this terminal, how we connect this to the theorem of alternative. So let's see, let's suppose Gp is equivalent to dq.
00:44:49.368 - 00:45:46.032, Speaker A: So Gp is r dimensional bar framework on endnote. And suppose there exactly is minus one dimensional framework, bi framework gq, that's equal to Gp. So that means, uh, uh, well, so let's, let's say. So d is the, is the edm of g, right? G is the d is the edm of Gp and d bar. So here, this d bar. So d bar is the Edmund gq. And then of course gp is equal to gq, if and only if fv, d bar is equal to fv.
00:45:46.032 - 00:46:29.424, Speaker A: D plus the summation of y, I, j, f v times of eig. This is because if D, if GP and GQr, you couldn't, then DP is just B minus some, plus some Y, IJ and Eij, right? So eij, all the other distances in the, in the, in the, in the graph, it's the same. So it's zero. So only, only difference is the ij that's in the null edge set, the graph g. So this is the, this is the, this is very easy to see, right?
00:46:30.044 - 00:46:30.824, Speaker B: Yeah.
00:46:31.404 - 00:46:34.304, Speaker A: And then because it's fv, it's. Hang on.
00:46:35.004 - 00:46:39.744, Speaker B: Since we have just ten minutes left, I'm just wondering if you could.
00:46:41.644 - 00:46:42.292, Speaker A: Just.
00:46:42.428 - 00:46:49.824, Speaker B: Scroll up to the theorem for a second. Just scroll up to the statement you wrote among. Just above.
00:46:50.244 - 00:46:52.412, Speaker A: Yeah, yeah.
00:46:52.508 - 00:47:22.444, Speaker B: So for the universal rigidity statement, what you want to say is instead of saying then there exists an n minus one dimensional bar framework, what you want to say is you want to write the negation of it, right? You want to say there does not exist an r plus one dimensional bar framework if and only if there exists a non zero positive, semi definite stress off a certain rank.
00:47:23.304 - 00:47:24.440, Speaker A: Yeah, yeah.
00:47:24.512 - 00:47:26.280, Speaker B: Minus one, minus one, I guess.
00:47:26.432 - 00:47:28.504, Speaker A: Yeah, yeah. Right.
00:47:28.664 - 00:47:37.644, Speaker B: So I mean, you can directly prove that now with this. I mean, you don't have to prove this weaker version here, right?
00:47:37.984 - 00:47:41.266, Speaker A: Yeah, yeah, yeah.
00:47:41.440 - 00:47:51.954, Speaker B: Okay. Just because we don't have time and we want to. Well, hang on. You're talking next week. I mean, not next week, the week after that.
00:47:56.134 - 00:47:58.474, Speaker A: Yeah, yeah. If.
00:47:59.934 - 00:48:22.434, Speaker B: Right. I think in the current schedule we have. Sorry to interrupt. I just want to make sure, because we are running out of time here. So I think we had lefty only talking the week after that. I mean, he can start on Thursday. So you actually have some more time if you want.
00:48:22.734 - 00:48:23.438, Speaker A: Yeah, okay.
00:48:23.486 - 00:48:28.954, Speaker B: Yeah, I can talk about after the workshop. Yeah, yeah, go ahead. Sorry, continue.
00:48:29.934 - 00:49:11.464, Speaker A: Yeah, yeah. And then this foe is one to one mapping between the EDM cone and the PSD matrix. And then we have this, this, this relation just applying fv. And this is, this will be the. So this, this FVD is, this is, this is a PSD matrix, right. And f, v prime is also a FPS matrix. So we can use this theorem alternative.
00:49:11.464 - 00:49:45.804, Speaker A: So there exists such a, this is y I j. And if that, if, if there exists such a yig, then, then you can see this, you can say that, saying that there does not exist. Yeah. Nonzero. Such that trace of fv d y zero and trace of fvigy zero and trace of one half. And this one is just, this is just this. And fv energy is just this.
00:49:45.804 - 00:50:48.416, Speaker A: And from the trace of one half v transpose d d v v y is zero. Then I can. Because this is a. Dispose of psd. I can, I could call y is equal to u, find u transpose where u is nonspace of kvd and it was just nonspace of this matrix. So, so, okay, and this, and from the second property equation, this is, so this is equally saying that the trace of these guys, I plug in y. And what I got, I got these, this equation, this equality.
00:50:48.416 - 00:51:51.904, Speaker A: And my omega would just be let my omega be this, right, it just this one, this is my omega, this will be my stress matrix. So, so yeah, omega, of course. Then from here I have trace of omega e I j zero for the nine edge set. So this is one of the properties met. And now I just want you to show that z times v times u is the gale space. Uh, so how do we actually check that this is the giggle matrix? So I check first to check z times transpose e is zero. That means this is because z transpose u transpose v transpose e is zero.
00:51:51.904 - 00:52:21.944, Speaker A: And because j is equals v, we transpose and the j is zero. Right? So, so, uh, so g is factorized as v times v transpose. So, so therefore weight times e is zero. So this property is met. Now I need to, I need to check z transpose p, zero. So this is, this is zero. So, so how do I.
00:52:21.944 - 00:53:10.344, Speaker A: So z transpose p is u transpose b times p. And now I have a range of kvd is range of v transpose dv is range of, well it's range of this, range of this. That's because v transpose v. V transpose is equal to v transpose. Yeah, because simply because this is just a j. So therefore, therefore, v transpose this. Therefore v this is equal to this.
00:53:10.344 - 00:54:08.822, Speaker A: And this is, again, this will be equal to this. And this will give you the PSD matrix or the gram matrix. And, and hence, hence the range of KVD is the range of v transpose p. V transpose p times p transpose times v. So now the null space is also the same, right? The same. Therefore u is in the null space of Kvd, which I here I, from here, u is not space of Kvd. Oh, that means, that means u is the null space of v transpose p times p transpose v.
00:54:08.822 - 00:54:31.274, Speaker A: So therefore u transpose v transpose times p is zero. So pro, this is a geometrics. So therefore, therefore omega is a stress matrix. So let's finish the proof. Yeah, I guess that's it. I don't have time to talk about applications.
