00:00:00.840 - 00:01:39.370, Speaker A: We're still sort of in the introductory tour of the various techniques that we're going to be. Then later on, going deeper into it may, this introductory tour may last a little bit longer, maybe another week, and then we'll start going into the individual topics of the class in more detail. Already. I wanted to, during this introductory tour, I wanted to give you sort of exciting hints at the connections between these topics and sort of just wet your appetite a little bit, especially since several of the problems that we're working on in the research discussions at Fields Institute have a lot to do with some of these connections. Okay, so last time we stopped at the Cayley Menga theorem, where I sort of gave a rough proof, but the last lecture was somewhat disorganized. So I'm just going to go a little more into that so that we can then illuminate the connections between Schoenberg theorem and Cayley Menger theorem, how the different things relate to each other. Okay, so this is taken from the, in the slides, by the way, I do my best to sort of indicate where I got the material from, and either the link is actually present somewhere in the slides, or sometimes I take from other people's lecture notes, and they're not that clear in the notes itself what the source of the notes is.
00:01:39.370 - 00:02:20.582, Speaker A: But the link will make it clear what the source of the notes is. So either the link or the page itself, once you go to it, will make it clear what the source is. Okay, so the Caylee Menger theorem, we're going to state it a particular way and try to prove it. I just want to recall again what we call the Cayley Menger determinant. So essentially, if you remember, these deltas are the entries of the distance matrix. And if I choose a particular submatrix of it, where I think of, there are endpoints. I mean, sorry, there are, these distances are supposedly, we don't quite know yet when we are just about to check the Cayley Menger determinants.
00:02:20.582 - 00:03:42.232, Speaker A: But think of this as n by n matrix, which essentially has entries which are purportedly or allegedly the distances, pairwise distances between n points in our real euclidean space of some dimension. Nobody has told you what the dimension is, by the way, with endpoints in rn, you can always embed them in euclidean space. You can always embed them in n minus one dimensions, realizing the given set of distances. If the distances are at all realizable, why is this? Let me repeat the question. If you have n, choose two distances given to you and you need to find points, endpoints such that realize these pairwise distances. You understand what I mean by realize, which means I want to put coordinates for the points such that the pair by its distances between the points is exactly the given distance in that particular entry of the matrix. And then my claim is that you can always do this if you allow n minus one dimensions, if at all, it is possible.
00:03:42.232 - 00:04:20.584, Speaker A: So in other words, if it is possible that there is some dimension, some dimension in which euclidean space in which you can realize these distances, then it's always possible in n minus one dimensions. Why is that the case? Okay, think about that. We're going to see this.
00:04:20.884 - 00:04:23.304, Speaker B: Just draw spheres around.
00:04:23.764 - 00:04:51.696, Speaker A: Exactly. And you're going to see this. We are going to see this exact intuition appearing as we proceed. Okay, so if you have you just inductively, you start with two points at a certain distance. You set the distance, whatever supposedly between those two points. You put the two points there. It's a real symmetric matrix with the distances that are positive, let's say non negative, so you can actually put them there.
00:04:51.696 - 00:05:58.454, Speaker A: And then the third point can be placed using the distances from the first point to the third point and the second point to the third point. By just drawing circles, you can decide which of the two sides you want to put the point in. You can do it either way and then proceed in this fashion with intersecting spheres of one dimension larger. There are some complications that happen when some point ends up lying in the affine span of the previous points. You have to be a little careful. So with this introduction, we can continue talking about, we can sort of assume that we can always embed in n minus one dimensions. So anyway, so here we have the definition of the Cayley Menger determinant, which takes a submatrix of the given distance matrix given by a particular subset of the row or column entries.
00:05:58.454 - 00:06:45.548, Speaker A: And you add on this extra ones and zeros, as we pointed out before, and take the determinant of that, and these are squared distances. And by Heron's formulas, just a generalization of Heron's formula. You'll find it everywhere on the web. You can prove that this determinant is just the volume, simplicial volume of any set of points that realize those distances. Okay, now, so essentially what this theorem is saying. So that's all the definitions here. So the theorem is saying that a real symmetric matrix with zero, diagonal and positive entries is a euclidean distance matrix.
00:06:45.548 - 00:07:46.974, Speaker A: What is the definition of that? It just means that it's realizable by a set of points in d dimensions, if and only if. This determinant for every such subset of indices is non negative, and the determinant is equal to zero for any subset that is of size d plus two. So, if you're wanting to embed in d dimensions, you want to look at any d plus two, any d plus two sub size submatrix or higher sub matrix. Once a d plus two submatrix vanishes, then all the higher ones will vanish. So I could just say exactly d plus two over here. Okay? So what with some conditions, we'll talk about these. Okay, so now, how do you prove this theorem? We're going to prove it exactly, you using sphere intersections, which we just talked about a moment ago.
00:07:46.974 - 00:08:17.894, Speaker A: So, first thing we want to keep in mind is a couple of facts. So the intersection of these spheres with centers in RD. So I'm thinking of the. So I want. Usually, when they talk about the circle, you say s one. But I'm not using that notation. I'm simply saying, if, for example, you'd have two points in two dimensions, then I'm actually intersecting circles.
00:08:17.894 - 00:08:52.836, Speaker A: And if points, when I'm looking at points living in three dimensions, then I'm intersecting spheres, the usual spheres and so forth. So the intersection of these spheres in RD. And then here it talks about how many solutions and whether the solutions exist and so forth. So the solution exists in RD if and only if the. When I say RD, I mean, I'm not interested in complex solutions. There could be complex solutions. I'm just talking about real solutions.
00:08:52.836 - 00:09:55.734, Speaker A: It exists if and only if the d simplex formed by the centers and the intersection point exists. I mean, seems like a very obvious statement. If and only if the Cayley Mengar determinant of pairwise distances is non negative. So the Caylee Manger determinant is an entity that. That you can compute regardless whether the points actually exist or not, right? So it's just a number that you can compute. So what this is saying is that you come, you take the Cayley Manger determinants of the distances between the centers and the radii of the spheres, and you compute the Cayley Menger determinant, and that's non negative if and only if that d simplex exists, if and only if the point that's the intersection of those spheres actually exists in RD. Okay? So, and then if this intersection, this intersection problem has two solutions.
00:09:55.734 - 00:10:28.164, Speaker A: If the determinant is strictly positive. Okay? And exactly, you know, these are real solutions. When I say solutions, I mean still real solutions. And then it is unique if the determinant is zero. Okay, so one of the distances is dependent and has infinitely many solutions if the Caylee mango determinant of only the centers is zero.
00:10:29.704 - 00:10:32.964, Speaker C: Hey, mira, what's a two simplex for you?
00:10:34.704 - 00:10:37.084, Speaker A: Okay, d simplex has d plus one points.
00:10:39.944 - 00:10:56.600, Speaker C: So the intersection of two spheres with centers in r two exists in r two. If and only if the two simplex formed by the centers exists. Uh, that seems like a line segment.
00:10:56.752 - 00:11:02.576, Speaker A: No, no, two simplex. When I say two simplex, I mean a triangle. It has d plus one points, but.
00:11:02.600 - 00:11:07.964, Speaker C: There'S only two spheres. If d equals two, there are two spheres.
00:11:09.064 - 00:11:14.896, Speaker A: Yeah, and I'm talking about the intersection of them. So if I have two, but a.
00:11:14.920 - 00:11:18.084, Speaker C: D complex formed by the centers, there's only two centers.
00:11:19.554 - 00:11:22.226, Speaker A: The centers and the intersection point.
00:11:22.330 - 00:11:26.986, Speaker C: Oh, and the intersection point. Okay, thank you. Thank you.
00:11:27.090 - 00:11:46.972, Speaker A: Yeah, so it has d, basically d plus one point. So in the example that you have, there are the two centers plus the point of intersection. Yeah. And then if you had three, then you intersect three spheres, in which case, their point of intersection and the three points together. Pharmaceutics.
00:11:47.108 - 00:11:55.420, Speaker C: Oh, I still don't understand. Because if there are two intersection points, won't we have a polygon that has.
00:11:55.452 - 00:12:13.854, Speaker A: Four sides and each intersect? I should say. I didn't say all intersection points. I'm saying each. I mean the intersection exists. And take any one of the intersection points and take the simplex each. Okay.
00:12:14.354 - 00:12:15.162, Speaker C: Okay.
00:12:15.298 - 00:12:16.146, Speaker A: Okay.
00:12:16.330 - 00:12:17.098, Speaker C: Thank you.
00:12:17.226 - 00:12:55.308, Speaker A: Okay, so, and then the second one is saying, as you say, that there could be two solutions, right? And this happens when that determinant is strictly positive. That means that area, the heron's formula area, is strictly positive. And when I say two solutions, you know, you see what I mean? There are two points, right? If the two points coincide at the same place, I'm not thinking of them as two solutions. Okay, and then is unique, too distinct, maybe. Yeah. Okay, and then. And it's unique if the determinant is zero, etcetera.
00:12:55.308 - 00:13:36.134, Speaker A: So this, if you convince yourself of this fact, then we can look at this algorithm. By the way, I put the reference to the algorithm over here, which essentially does exactly what we talked about a moment ago, which is by inductively increasing dimension. So it's easiest to understand this algorithm if you take one path along the flowchart. Is it too small? Yeah, I could try to increase the size a little bit. Is it too small for most people?
00:13:36.794 - 00:13:38.650, Speaker C: Yeah, this one is a bit small.
00:13:38.842 - 00:14:02.054, Speaker A: Okay, I will try to see what I can do. Put it on full screen may help. Does this help? A little bit. Or I could try one other possibility, which is you could just zoom in the slide section. There should be a zoom slider somewhere. Yeah, yeah, something like that. Okay, view.
00:14:02.054 - 00:14:08.214, Speaker A: Zoom in that better?
00:14:10.554 - 00:14:11.298, Speaker B: Better?
00:14:11.466 - 00:14:21.034, Speaker A: Yes, better, but still not great. Okay, zoom in some more. Okay, that's good, right?
00:14:21.154 - 00:14:22.010, Speaker B: Much better.
00:14:22.162 - 00:14:52.728, Speaker A: Okay, so, this is an algorithm that actually came as a paper published as a paper in 2015. So it's solving a slightly different problem, but we can use it for our purposes. Okay. What it's doing is it's assuming that what is given the input is a input distance matrix. Remember the theorem we're trying to. Okay, how are we proving the theorem? One direction of the theorem is easy. Right.
00:14:52.728 - 00:15:24.782, Speaker A: So if all of these conditions. No, sorry. If the matrix is equilibrium distance matrix, which means that the point configuration exists, then these conditions are true. That's I'm proving the opposite direction, which is that if these conditions exist, then such a realization exists. Sorry. If these conditions are true, then such a realization exists that the matrix is actually in distance matrix. So, improving that direction, and they are proving slightly, something slightly different with this algorithm.
00:15:24.782 - 00:16:32.420, Speaker A: Doing something slightly different with this algorithm. They are starting with in a matrix that they are assuming is a euclidean distance matrix, and they are finding the realization. Finding a realization. So what we are doing is using that algorithm to show that if the Klemenga conditions hold, prove the only if direction of this, of the if direction of this theorem, namely that if the Caylee mengar conditions hold, then the realization exists. Right? So, so, if you look at the algorithm, it starts out, it's got this major one for loop. So, which essentially says that the whole algorithm, essentially, at least the inner loop, is only running order n times. So it starts out with two points, and then it sets them at a distance, you know, so we use the word delta squared, and they're taking square root of d, which is just because their capital d is already in squared distance.
00:16:32.420 - 00:17:12.463, Speaker A: So they are setting two points at that. You know, they're setting them at these places on, on the line. And there's this one, I have to point out that there's this one command here called expand, which all it does is it adds a dimension to the coordinates of the point. So here, the coordinates right now are in one dimension. So each point just has one dimension. But then when you do this thing, it will put them in two dimensions, so it will actually add a zero to each one, each one here. So that's what this expand is.
00:17:12.463 - 00:17:48.660, Speaker A: Other than that, it's self explanatory. Pretty much. The algorithm. The algorithm, so it starts out with putting two points on a line at a certain distance between each other. And then there's this for loop, which looks at all the other points one at a time and at any given. So initially there is a case set as one, and along one possible path through this loop, the dimension increases. During another different path through the loop, the dimension may not increase.
00:17:48.660 - 00:18:45.104, Speaker A: So at any given time, the, so the for loops, you know, variable isn't going in lock step with the dimension. Okay, so in other words, what we're saying is that at certain times when we add a point, we may increase the dimension, and at certain points when we add a point, we may not increase the dimension. Okay, so now if you look at this, you take the intersection of k spheres, whatever k happens to be the dimension at that particular time. And there are three possibilities. One possibility is that that intersection, and remember, the s sub super k is basically the notation that we had earlier. The sphere centered at a point in rk plus one with radius r is denoted as kpr. Okay, so initially we start out with, so you're thinking of the points even though they're in, you know, one dimension.
00:18:45.104 - 00:19:28.134, Speaker A: Think of them, k is equal to one, you think of them in two dimensions, and you're putting the radius of two circles, right, so you're taking the intersection of two circles. If that so intersection is empty, then you simply say, well, it can't be embedded. That's it. So the set of points can't be embedded. So in other words, you've given me two points and there's a third point, I equals three, which is at a distance between, such that the distance between it and the remaining two points is maybe smaller or bigger than the length, the distance between them. Then the triangle inequality is not met, basically. So that means you can never embed those points in real.
00:19:28.134 - 00:20:09.762, Speaker A: So this is essentially saying that this is, you can't embed, that's what return infinity means. Otherwise, if you get one point, so there are case statements, there's 1.2 points or infinitely many points. Okay, you shouldn't really get, I think that, you know, except for the very first step, I don't think you will ever get this, but we can double check that. So in the very first step, it could be that d twelve is zero. So these two points coincide with each other. You put two circles, could be that the, you know, both circles coincide, and then you just have infinitely many points.
00:20:09.762 - 00:20:42.074, Speaker A: Right. So in that case you're simply saying that the two points are coincident. Yeah, but, okay, let's go through this. So if you have, let's say if you have three points and then you, and you have not and k is still one, so then you're intersecting three circles at three points. Centered at three points. Typically the solution is either empty or one, one solution. And that's this case.
00:20:42.074 - 00:21:23.098, Speaker A: But if you intersect two, then you would have two solutions. So that would be this case. So if there are two solutions, it means that you have, you would need an extra dimension to put the third point right, the next point. So that's what is happening. You add one more dimension and you expand the vectors, which means you're adding one more coordinate to each of them. And you also say, okay, for the future intersections, I now need to intersect a sphere around one more point. Okay, think of it differently.
00:21:23.098 - 00:22:42.108, Speaker A: If you had a unique solution, you just ignore that point. From this point on, you don't worry about it because that point is in the affine span of the ones before it. So any future positioning, there is a problem with this because there is a problem here because they are assuming that you can ignore that point because what is given is, is an, is there is a promise that it is an euclidean distance matrix, right? So, and you, we can't ignore it. We are trying to check whether it is a euclidean distance matrix. I mean we, so we need to actually check that, right? So, or at least make sure that it has something to do with the Cayley mengar conditions. Okay, so, so then, so basically if you wanted to check if along this thing, if you wanted to change this algorithm to check whether something is an euclidean distance matrix, it actually finds the realizations and then the remaining distances that you did not inspect, never used. For example, if a point is in the affine span of another set of points, the distances between that point and any future point will never be checked by this algorithm.
00:22:42.108 - 00:23:34.944, Speaker A: So that would have to be checked if you really wanted to check if it is a euclidean distance matrix. So some questions. Where can, instead of doing all this stuff and suppose I just wanted to check if it is equilibrium distance matrix, where can I use, we are not yet quite there in terms of doing the proof. I'm still talking about taking this algorithm and changing it into one that checks whether it's a clean and distance matrix. Then we'll go you. It'll be very clear how the proof of the, if direction goes, namely if the Kalimanga conditions are satisfied then you automatically have the solution. Okay, so, so where can we suppose we're trying to now decide what the matrix is EDM.
00:23:34.944 - 00:24:10.844, Speaker A: And you don't want to do this. Intersections. Where can we use. So based on the fact that I had in the previous slide, what can you do instead? Where can we use the Cayley Menger determinant checks? So where can we use non negativity of Klemenger determinants? And where can we use zero determinants? And here, you know, where you would normally have to check all the unused distances. How to do this without solving fei? You're saying something.
00:24:12.944 - 00:24:13.472, Speaker C: Step.
00:24:13.568 - 00:24:19.004, Speaker A: Step eight, you can use zero calorie manger. Determine it. Okay.
00:24:20.464 - 00:24:26.756, Speaker C: Step ten, you can use non negativity of kick determinate.
00:24:26.820 - 00:24:46.220, Speaker A: Step ten. Yes. Essentially finding the intersection is non empty already here. Step six, right? Yeah. Okay, so this is non empty. The intersection of the spheres is non empty. Then you can use non negativity of the k l manga determinant.
00:24:46.220 - 00:25:22.234, Speaker A: And for it to be unique, you can use zero k luminger determinant instead. Yeah. And then right at the end where you're going to check all the unused distances, how can you do so? In other words, let me try to draw this and explain the situation in which I'm, situation that I'm talking about. What I mean by unused distances. Okay, new share whiteboard. I'm going to not look at you guys. Okay, share.
00:25:22.234 - 00:26:12.714, Speaker A: Oh, I have already a different slide here. Okay, so what I'm saying is if you have happened to have, let's say three points and the fourth point happened to lie on the plane. So in which case when you intersected the spheres around these three points, or alternatively looked at the Cayley Manger determinant of these four points. Or you looked at the Cayley manger determinant of these four points. Oh, what happened? The kalimanger determinant turned out to be zero. I'm going to give them names. So this one is p one.
00:26:12.714 - 00:26:48.930, Speaker A: What happened? Okay, so this guy is p one. This is p two, p three and p four, the corresponding all the six pairwise distances. I put them into a klemanga matrix and that turned out to be zero. Alternatively, you can say the intersection of the spheres around points p. Let's say points p three, p four, p one, p two, p three turned out to be p four. Okay, but it's unique, uniquely p four. So what we can now look at.
00:26:48.930 - 00:28:01.084, Speaker A: So suppose that happened and so you would ignore p four for the future steps, you know, I, this, I would not get included into that capital I in that, in that step. And so you would ignore p four altogether. Then say you found, if you found a p five, and then you would simply look at p one, p two, p three again, and now again intersect spheres and then say, let's say you found a point p five and p five also, it turns out that the six distances of the k le manga determinant between p one, p two, p three and p five also vanish. So cm of, I would say cm of one through four vanishes and cm. I mean, this is a common notation, which is a real, to me, the way, I mean, I think of this notation as a little bit putting the cart before the horse, because it's sort of assuming these points exist. Because really all you're doing is taking the distances so the points don't exist in, they axiomatically exist in some metric space. And you're taking these purported distances between them and you're looking at this thing.
00:28:01.084 - 00:28:36.254, Speaker A: So suppose we know these two things, one, two, three and five. This is one through four. They're both zero. And then the next time you proceed, you would ignore p four and p five altogether. And you would just look at p six as a separate, another intersection of three spheres between these, neither p five nor p four. If you look at the algorithm, we'll go back to it in a second. Neither one of them actually includes, is included in this capital I, which is the set of points that are in, used to find sphere intersections from this point on.
00:28:36.254 - 00:29:09.170, Speaker A: So what happens? Therefore, you have looked at all of these distances. You have inspected all of these distances because you wanted to look at the simplicial volume. This simplicial volume. Similarly, you also looked at these distances. Let's see. So you also looked at all the distances here and this one. So it's also worked.
00:29:09.170 - 00:29:57.874, Speaker A: But the distance that you have not looked at will never look at, at least the way this algorithm proceeds. The way this algorithm proceeds, the distance you will never look at is this one, for example. I mean, this is the only one, really, with this many points, this distance will never be inspected. So, because this algorithm is only. So, let me go back and try to. So, if I was good enough, I would draw on the same slide. But somehow my ability to control the pen on this thing is very low.
00:29:57.874 - 00:30:28.182, Speaker A: So I have to use separate slides. So, for drawing. Separate whiteboard for drawing. So now you see this capital I here is not, will not include any points that will happen to lie in the same. See because whenever this is a single unique solution, that point is never included. And this corresponds to the zero k manga determinant. And so it's in the affine span, so that never gets included in the capital I.
00:30:28.182 - 00:31:46.642, Speaker A: So if I have p four and p five, which both turned out to be unique solutions here, then the distance between p four p five is never inspected. They just assume I'm given an EDM. All I'm trying to do is find a realization so I don't have to inspect it. But so you need to check all of these unused distances at the end. So now the question is, how do you check all of these? And if, sorry, if we are changing this algorithm to one that checks whether something is a kalimanga. Sorry, whether something is equilibrium, distance matrix has a realization, then how do you do this without actually solving, for solving these sphere intersections? I mean, we decided that in order to check whether it's empty or unique or two, then you can do all of them by just checking non negativity without actually doing it, without actually finding the intersection. So this last thing where you had to check the distance between, in that example, p four and p five, how would you do that without solving the sphere intersections? So if you solve the sphere intersections and have the coordinates, then you can simply compute the distance and check whether that distance was the distance that was given to you for p four, p five.
00:31:46.642 - 00:32:02.434, Speaker A: If not, how would you do that? Any ideas? Is the question clear? Question not clear.
00:32:07.094 - 00:32:08.354, Speaker C: Will you repeat it?
00:32:08.774 - 00:33:51.860, Speaker A: Okay, so remember, we are trying to adapt this algorithm or modify this algorithm so that it now checks whether a given input matrix is equilibrium distance matrix, or in other words, that it has euclideans, I mean, a set of points that realize those given input distances. To check this, I mean, the current algorithm assumes that it is a euclidean distance matrix and simply outputs a realization, or in other words, a set of points that realize those distances. We are adapting it to check whether the input is a euclidean distance matrix. So what I pointed out is that if you ran this algorithm through and you go to this situation where the algorithm would notice that, I mean, would place p one, p two, p three correctly on a triangle, then it would notice that p four is also happens to be on the same plane, and would notice that p five is also happens to be on the same plane. So from this point on in the algorithm, when it's checking p six, for example, will simply ignore p four, p five, the places p four and p five, it finds the coordinates, and then after that, it simply ignores them. So in other words, the distance between p four, p five that's given us input is never checked by the algorithm as being the actual distance between p four and p five. P four was unique and p five was unique unique real solution and unique real point and unique real point.
00:33:51.860 - 00:34:30.908, Speaker A: And that distance is fixed. And that, and so whatever was input was given should be this distance. But it never checked that because it was not in, it was not intended to check whether the input was a euclidean distance matrix. But we, to, in order to determine whether it's an EDM, should check this distance. However, we are trying to avoid actually realizing the points. We are just trying to check all of these conditions, that an intersection exists and so on and so forth, by just using Kaylee manga conditions. By just using Kaylee manga conditions.
00:34:30.908 - 00:35:10.404, Speaker A: So for example, instead of computing this intersection to see whether such an intersection is non empty, we simply check the non negativity of this in order to check whether it's unique. We simply check the zero k le manga of this. You know, and we want to do something similar here. Instead of actually checking those distances, we want to, I mean, in order to check the distance, we would have to realize the, find the coordinates of the point and then check the distance. We don't want to do that. We just want to compute another Klemanga determinant and check something about it. So what would that be? Okay, so.
00:35:13.744 - 00:35:25.884, Speaker B: Mira, it seems like maybe we could choose a Kaylee Manger distance like in our example. One, two, okay, I'll go back.
00:35:30.424 - 00:35:59.996, Speaker A: One, two, four and five. Yes, that's an option. In fact, there are many equivalent, there are many equivalent characterizations. We'll come to that in a second. Many equivalent subsets, proper subsets of Caylee manga conditions that are sufficient for the realization to exist. And we are looking than what is given in the theorem. And we are trying to get to that, make that thing.
00:35:59.996 - 00:37:37.814, Speaker A: So what the standard, at least menger's original proof, instead checks the simplicial volume of all five. Okay, so it says, if we do know that these guys are on a plane, and these guys are on a plane, so we know the simplicial volume, if it is non negative, is going to be zero, right? So instead of saying it's non negative, you simply check whether it's zero. So that menga's original proof in, in this axiomatic way, where he assumes these are in some metric space and just checking whether it can be, there's a congruent one in users congruence, rather than isometrically embedded in euclidean space, then you just check that the entire five simplicial volume is also zero. So, okay, so before we go to that question, you know, I just wanted to put here what the people, authors of that paper, wrote, the paper in which this algorithm appeared. They say that the k spheres in RK, we assume their centers are in general position, so they span a k minus one affine space. At the point where you're finding those intersections in the capital I, the set of vectors, I mean, set of points that actually end up in capital I. Then we have at most two points in the intersection of these spheres.
00:37:37.814 - 00:38:30.872, Speaker A: We have no point in the intersection. No point of the intersection is empty. One point of the intersection lies in the k minus one dimensional affine space generated by the others, and two points if there are no points of the intersection in the k minus one affine dimensional space generated by the centers. Okay, and this, what they call trilateration, is really a d lay, which is what we're, we take a simplex of a given dimension and add one point to it at any given time. The algorithm's complexity basically takes time proportional to order k squared, because you have a strangular linear system to solve for the sphere intersections. And then that leads to a total time of order n cubed in the worst case. Okay? So remember that.
00:38:30.872 - 00:39:18.928, Speaker A: So this thing, if you actually end up finding all the sphere intersections, will take order nq. So now here's the question. So this takes order n cube, because there are order nice Kalimanger inequalities that you actually, you know, you actually verify. If you look through this, you know, going through this loop, there's only one pretty much in every run through the loop, one Klemenga condition. The verification of the condition takes time, but the number of conditions that you check is order n. And we already discussed which ones and why. We can think a little more about whether we can have alternative ways of doing it.
00:39:18.928 - 00:40:15.522, Speaker A: Okay, but I also want you to notice that we are asking about which Klemenger conditions are sufficient to decide if something isn't, is realizable as a whether the realization exists, set of points where the realization exists. I mean, set of distances for which a realization exists. But this is a little bit different from asking whether some sets of distances imply the others. We're saying that some sets of Caylee Menger equalities and inequalities imply the others. So you don't have to check those others because you've checked a few of them, then all the others are automatically satisfied. It's a little different from asking which sets of distances which are, in a sense, the, the variables of these scaly manga polynomials. Which of those distances are dependent, are dependent on others.
00:40:15.522 - 00:41:12.504, Speaker A: So it's sort of like in the linear setting, this is like saying which variables are dependent on each other versus which equations, or in other words, which rows of a matrix versus which columns. So that's slightly different. So I want you to pay attention to that. So it's clear. Clear now, though, it should be clear now, though, how the proof of the if direction of the Caylemanger theorem goes is essentially all of these are these inequalities. Once these inequalities are satisfied, and it's a much smaller subset than that's in the right hand side of this theorem. Okay, so this right hand side theorem says, look at every possible subset, and there's like exponentially many in this in n, a number of kle mengar determinants that need to be checked in order to check whether the matrix really is equilibrium matrix, or in other words, has a realization.
00:41:12.504 - 00:41:44.936, Speaker A: We are saying we don't need it. We, with this many checks, we can actually find that we know there's a realization. This algorithm finds the realization. Okay, so, so this last thing I haven't yet, I haven't yet proved, but that was, like I say, it's relatively easy to prove, and that's Menger's thing. And I'll come to that. I'm going to make, at least make the theorem statement. Okay, so here's the theorem before we.
00:41:45.080 - 00:42:15.890, Speaker B: Yeah, at some point, anyway, I'd like to go back to the sphere intersection questions. Yeah, maybe just the slide before this. I'm kind of confused about what this says. So I see two things that seem to seem to say some determinant vanishes at the bottom, and I'm trying to distinguish what's the difference between the two.
00:42:15.922 - 00:42:27.994, Speaker A: Oh, yeah. This determinant that I'm talking about here is a determinant that involves all the pairwise distances between both the centers of the spheres and the point of intersection.
00:42:30.454 - 00:42:31.194, Speaker B: Right.
00:42:31.494 - 00:42:34.274, Speaker A: Okay. And this one is only the centers of this.
00:42:34.574 - 00:42:37.022, Speaker B: I see. Determinant of the centers only. I see.
00:42:37.078 - 00:42:38.822, Speaker A: Yeah, got it. Yeah. Maybe I should have said.
00:42:38.878 - 00:42:43.606, Speaker B: That's right. No, this makes sense. This was clear in your example, but now, now I understand.
00:42:43.750 - 00:42:58.122, Speaker A: Yeah. Okay. Um. Okay, so now we're a little bit out of order. I'm saying things. I'm putting the cart before the horse a little bit. But I have come to a slightly different theorem here.
00:42:58.122 - 00:43:43.828, Speaker A: But be patient, they're all very highly related to each other. Okay, so this one is saying, is making a pretty strong claim, even stronger than what we did here. Okay, so here it said, okay, you check these particular set of Caylee Menger inequalities. And this part of it is about order n of them. And, well, this may be to check all the unused distances. You may potentially be checking a whole bunch of pairs and pairs of points. So actually this might actually add more distances, something like order n squared, because you're looking at whole ton of pairs, like in that other slide that I had.
00:43:43.828 - 00:44:45.336, Speaker A: We'll come to that theorem a little bit later. Okay, before we go there, I'm making this claim. I'm saying a matrix is EDM if and only if the delta s is greater than or equal to zero for the submatrix determinant. For k s, one through k k going from one through n. This is a little bit different from what we might end up doing in a course of the algorithm if you happen to have a finely independent, finely dependent points. But I am saying this is an alternate set of Kalimanger conditions that also is equivalent to the matrix being an EDM. And how do you prove this? So this is saying, so if you look here, this is saying that, you know, in a way, you can sort of simply ignore that you have all these affinely independent points and then doing different things for those and all that.
00:44:45.336 - 00:45:10.488, Speaker A: You simply, regardless of whether, you know, you end up with an affinely independent point at some point or not, you simply include it into this. I. Okay. And then you just keep checking the Cayley manger determinants being zero. For as you keep adding one point or another, they'll be zero, but certainly non negative. So you just keep checking the non negativity of these. Just ignore it.
00:45:10.488 - 00:45:34.572, Speaker A: Ignore whether you have only a unique point or two points. If it's two points, pick an arbitrary one of them. It really doesn't matter which one. If it's only one point, take the point, put it into I, and just come. Keep computing these Kaylee manger determinants, one larger than each stage. It gets larger. You order the points initially in any way you want.
00:45:34.572 - 00:46:16.500, Speaker A: So the correspond, or in other words, the rows and columns of the given matrix, you order them any way you want. You take sub matrices, one larger at each time. So you have n such submatrices, and then you compute all of them and there are only order n of these and which is smaller than, like I said, in this case, when you actually check, you have to potentially check a bunch of pairs. So for each pair, you're going to have another simplex. So in that example that I had in the other slide, p four, p five was one such pair. So you'll have many such if they were all on the plane. Let's say you'll have p six, p seven, p eight, and so forth.
00:46:16.500 - 00:47:06.172, Speaker A: So for every such pair, you're going to have to check a five simplex to be volume zero. So that can add a bunch of pairs. But I'm claiming, no, you can actually just stick with these. And it works if you just want to check whether it's an EDM and don't care whether, which dimension it falls into. So, the proof of this is simply to say that, think of asking whether the realization that actually spans n minus one dimension exists. So in other words, something in the interior, so to speak. So we want to check whether the EDM has a realization that includes, that has all the dimensions.
00:47:06.172 - 00:47:57.576, Speaker A: So the realization uses all those n minus one dimensions. So in, if this has, if, let's consider the case. So if, if it has those n minus one dimensions, then certainly all of these submatrix determinants are going to be greater than or equal to zero. That's the easy direction of the Caylee Manger theorem. You know, even subset of the easy direction of the Caylee manger theorem that the converse. If all of these, sorry. If all of these exist, are strictly greater than zero, if they're all strictly greater than zero, then this algorithm would have proceeded in one of those pads where.
00:47:57.576 - 00:48:31.378, Speaker A: So where's the algorithm? So, if each of these is strictly greater than zero, this number would all have, I mean, we would have always had to pick one of two solutions. We would have always increased the dimension and we would have always added one. When you do sphere intersections, you look one dimension higher for the spheres. And you would have proceeded in this fashion. So you would only have checked order n of these determinants. You would never have used to do any unused distances. You would never have any unused distances.
00:48:31.378 - 00:50:01.594, Speaker A: All the distances will be used. So essentially, this is saying that if the algorithm proceeded throughout checking only all of these as strictly non negative, I mean, strictly positive, then you actually have an n minus one dimensional realization. That's if and only. So if all of these were strictly positive, then these are realization that actually uses all the n minus one dimensions. So that means that you have, that's an open set, and you now simply take the closure of that set, which means that includes all the realizations that span any dimensions smaller than or equal to n minus one and corresponding. The closure here will be the, instead of staking strictly greater than zero, you'd say greater than or equal to zero, just the closure of that. So this statement is simply the closure on both sides of the statement that a matrix is EDM realizable in the full realizable in a manner that the points are in general position, so to speak, in n minus one dimensions, if and only if the determinant of delta s is strictly greater than zero.
00:50:01.594 - 00:50:51.634, Speaker A: So take that, I just proved that statement, and then I simply took the closure of that statement. Okay, so this is basically saying that in this algorithm that we gave, we could have just, you know, not even done this at the end, we could have ignored this. These different paths, these different paths are needed in order to find which dimension is the minimum dimension in which you embed. But if all you're doing is checking whether the matrix is EDM, you couldn't, you don't need all these different paths through this. You just go right through and check and do these checks only. I mean, do this k plus one, add the k plus one and I every time and do only these checks. And that should already take care of it.
00:50:51.634 - 00:51:55.002, Speaker A: Okay, so now, so this is saying somehow that these guys are the bounding sets. So the bounding, whatever, these are the boundary, this gives, these are the boundary of the set of edms. And so let's look at the original characterization of edms that we had with Schoenberg's theorem. Recall, I can try to make you dizzy and go all the way to the back. Yep. Okay, so this is basically saying, you can say it in one of two ways, is that the matrix is negative semi definite. The distance matrix is negative semi definite on the subspace of all vectors, automated the one vector, people usually just drop this, but, and just say negative semi definite.
00:51:55.002 - 00:52:45.624, Speaker A: Or you can say that the thing that you form in this way from the distance matrix, which turns out to be positive semi definite, because, you know, you can show that this guy is just the gram matrix of that set of points. That would be the realization of the EDM. Okay, so the rank condition is of course telling you what the dimension of the realization is. So if you look at this condition, it sort of tells you that the set of all edms, you can think of them as edms, or just as I said, think of them as points in points in rn, choose two space. So think of the distance tuple as a vector. So I will go to new share and go back here. Share.
00:52:45.624 - 00:53:14.352, Speaker A: And we can go back to the previous slide where I actually drew, try to draw a cone. I was testing whether I capable of drawing a cone on this weird thing here. So this is in rn. Choose two. So I'm thinking of this delta ij, Delta IJ squared, because I thought I had just used delta IJ for the squared distances, but apparently not. Okay, so delta ij squares, um, that's in r n. Choose two.
00:53:14.352 - 00:54:00.564, Speaker A: So you can think of it as a, either as a matrix, if you want, or you can think of it as, you know, as a tuple of, you know, delta squared, I j distances. And, you know, and you're putting each one of these points is one of these guys. And we, the coordinates are, of course, one distance. Each is a pairwise distance, corresponds to a pairwise distance, let's say. And the claim is that edMs, or euclidean distance matrices, those that happen to have realizations in some euclidean dimension, form a convex cone. So what is the definition of a convex cone? Somebody.
00:54:08.024 - 00:54:10.680, Speaker C: You can scale it by a real number.
00:54:10.832 - 00:54:11.496, Speaker A: Yeah.
00:54:11.640 - 00:54:19.084, Speaker C: Inside the convex cone. And any line segment between two points in the cone will be contained in the cone.
00:54:19.424 - 00:54:56.284, Speaker A: Good. So those are, that's so basically saying that if x is in the cone, x is a vector. I'll just call it some x or something. Okay. If x is in the cone, is in the set, you know, set. If x is in the set, then for every alpha greater than or equal to zero, alpha times x is in the set. And in fact, for any pair x and y in the set, think of them as these tuples or distance vectors.
00:54:56.284 - 00:55:56.692, Speaker A: So you take any two vectors like this. So x and y in the set. Then for every alpha beta greater than or equal to zero, alpha x plus beta y, given a set. Okay, so now, going back, I mean, just looking at the Schoenberg theorem characterization, it's not clear from the Caylee manga characterization that this is a cone. I have to switch back. So, so, from the Cayley Menger characterization, I don't want to make you dizzy and go there again, but from the Cayley Menga characterization, you have all these determinants vanishing and determinants greater than or equal to zero. Even the most optimal or efficient way of characterizing the set of EDMs with those order n k le manger conditions that I put at the very last, as the last slide, it's not, I mean, these are all high degree polynomials.
00:55:56.692 - 00:57:10.726, Speaker A: Determinant is a huge high degree polynomial, especially that particular character characterization, uses even n by n determinants. So that's a very high degree polynomial. You're saying these polynomials are non negative, that those are the bounding things and manifolds or whatever varieties and those bounding varieties, it's not at all clear that they would, what's inside that they're bounding? The interior of that thing, that object that's bounded by those closure even, is convex at all. But from this restriction, this definition, which is saying that these are exactly corresponding. So this negative semi definite, or for the moment, let's just look at the gram matrices, that those are positive semi definite. The fact that they're all positive semi definite, which means they have non negative eigenvalues, makes it clear that it is a cone, because they satisfy this condition that I just talked about. So, homework, you can, those of you who have, don't do this every day, just take two matrices.
00:57:10.726 - 00:57:51.154, Speaker A: So x here is a matrix now, or is a distance tuple. It really doesn't matter. I don't have, you have to be able to think of it as a matrix because you're going to compute its, its eigenvalues, so, or singular values, it doesn't matter. So basically you are taking the two of these matrices. So x and y are both matrices. So I'll just make them big. So that, so this, these are matrices, or distance tuples, whatever, and you take any, these are scalars alpha and beta.
00:57:51.154 - 00:59:13.484, Speaker A: Alpha and beta are scalars. So you just non negative scalar. So when you, whenever you take this, this type of a positive combination, if these were positive semi definite, then these, this sum would also be positive semi definite. So that makes it clear that it is a cone. So now it magically tells us that these really ugly looking, where does this not stop? Okay, new share so this ugly looking object bounded by these guys, or alternatively these other guys that we had, that involved all those pairs as well, either one or maybe the one that will point it out, maybe that's two, I haven't thought about it, but that's various different subsets of these Caylee manger inequalities. So those, the fact that they bound something that's convex would not be obvious from this way of thinking about it. But once you see that it's the same, once you have that other Schoenberg characterization that it's a convex cone becomes clear.
00:59:13.484 - 00:59:25.764, Speaker A: Okay, so, so we drew the picture. Now let's consider the boundary. Should we take a break, guys? What do you think? Or go right through.
00:59:28.584 - 00:59:29.248, Speaker B: Either way.
00:59:29.296 - 00:59:47.194, Speaker A: Yeah. Any preferences? Maybe we can chat a little bit. I feel very lonely in this. When I'm on, I'm the only person talking. Yeah. Maybe a break. Yeah, let's take a break and chat a little bit.
00:59:47.494 - 00:59:48.262, Speaker B: Sure.
00:59:48.438 - 00:59:52.702, Speaker A: Okay. A few minutes. Like five minutes or something.
00:59:52.878 - 01:00:06.470, Speaker B: I find this really interesting because this cone has very nonlinear boundary, very non.
01:00:06.502 - 01:00:23.438, Speaker A: Linear boundary, very different from the metric zone that we talked about in the very first lecture or something. You remember, the metric zone for just metric space is a nice. You know, it has only the number of linear equations bounding. It is exactly the number of facets, right?
01:00:23.566 - 01:00:24.318, Speaker B: That's right.
01:00:24.406 - 01:00:40.854, Speaker A: Right. These ones are so non linear that they sort of. Each one sort of potentially has many pieces to it, you know, as a facet, as a boundary. Facet of the cone. Yeah.
01:00:41.014 - 01:01:23.444, Speaker B: I'd be tempted to try to understand this cone in terms of maybe sub cones that have linear facets and kind of just try to grow it somehow up until, you know, like, presumably we could get the bulk of the cone inside some subcone that has linear facets and that might be easier to understand. And then slowly start, like, working on understanding parts of the cone that don't. That don't satisfy those linear constraints.
01:01:24.744 - 01:01:56.144, Speaker A: I have to say, you know, there are whole communities of people whose entire lives are spent on understanding this cone. Okay. The facet structure of the cone, its projections, its fibers. You might even argue that a good chunk of combinatorial rigidity is about that. Okay. I don't know. Is there anybody who would contest what I'm saying? Maybe Faye or.
01:01:56.144 - 01:02:02.024, Speaker A: I was wondering if Henry Osir. Henry isn't here today, but.
01:02:05.284 - 01:02:09.104, Speaker C: So the boundaries can't define when these conditions are zero.
01:02:10.124 - 01:02:13.904, Speaker A: Yes. Exactly where we had here. Yeah.
01:02:14.484 - 01:02:17.624, Speaker B: Yeah. This is a super cool object.
01:02:18.124 - 01:02:33.304, Speaker A: Yeah. And as I said, there are alternative characterizations of this as well. I see some new people here. There's Michael Milgram. Okay. Hi, Michael.
01:02:40.864 - 01:02:43.484, Speaker B: I just drop in every once in a while.
01:02:43.904 - 01:03:01.564, Speaker A: Oh, okay. Don't expect to say anything. Okay, that's fine. I'm just. I created a piazza site with all the participants, but thank you. Okay, so. So if we.
01:03:01.564 - 01:03:32.008, Speaker A: The Kaylee manga conditions, all the versions of them, subsets of them, they all basically define the same cone. And so now we can sort of ask ourselves, you know, that it. You know, right in the beginning, I was talking about. Maybe that slide is still here somewhere. No, that slide kind of disappeared. No, maybe here it is. Okay.
01:03:32.008 - 01:04:30.132, Speaker A: So I was talking about the two theorems and how they sort of. This is sort of, from an analysis perspective, this theorem, and this is more from an algebraic, if you will, or break geometric perspective. I guess the original mengar proof was very classical, somehow axiomatic. So classical geometric perspective. So essentially, how do you relate these things? I mean, how, how do you reconcile these conditions? So the non negativity of the, I know, the fact that it's negative, semi definite, I won't repeat that long thing there, and is in a way the same as these scaly Menger inequalities. And the fact that it has a particular nank. In other words, the thing has a certain realization in a certain dimension, will come, comes down to certain set of equalities.
01:04:30.132 - 01:05:40.564, Speaker A: And in the smaller subsets of these kilimanga conditions that we talked about, you have a similar dichotomy of the inequalities and the equalities. So, and so the question is how, I mean, to what extent are, can you increase or, you know, sort of illuminate these correspondences even further? Okay, so let's consider, so what happens. So some of the questions we can ask. So is what happens when some of these are zero? And what happens the, when we know that, what happens when some of these are zero? As I said, I put the cart before the horse and already started talking about dimension. But we know, we know what happens when some of these are zero. Essentially, if sufficiently many of them are zero, then we know that sufficiently many of the k le manga conditions are zero. Then we know that the whole realization sort of falls in a lower dimensional subspace, or all the points are finally dependent.
01:05:40.564 - 01:07:04.368, Speaker A: And if they are negative, what does that mean? What happens with the Klemanga conditions are negative, or you have negative eigenvalues? Either way, think whichever way is convenient for you to think about it. It can't be realized, realized in this dimension in automation? No, it can be realized. Let me ask you, can it be realized somewhere? If some argument is negative, yes, let's say some Klemanger conditions are negative, or you have some negative eigenvalues. Can it be realized somewhere? No. Well, I don't know what realized means. Higher dimensions? No. Any, any eclidean thing requires, you know, if you want to realize, by realize, I mean put it in real space, real euclidean space, then you're going to have to have non negative conditions.
01:07:04.368 - 01:07:12.084, Speaker A: It doesn't matter which dimension. What about complex numbers?
01:07:12.944 - 01:07:16.364, Speaker C: Yeah, complex can be realizing complex.
01:07:16.944 - 01:07:44.344, Speaker A: Yes. So I don't know what people say mean when they say realized. I don't necessarily mean realized. The word real appears there. But when I say realize I don't necessarily mean that it has to be in the reals, right? So, but they can be, you can get complex. So if you remember. So let's go to the, oh no, this, we did all this already.
01:07:44.344 - 01:08:19.924, Speaker A: Maybe I have something better to say here. So, okay, let's look at some, maybe I go a few slides. I think I just went out of order. I started saying things before, and certain things that I should have said I have not said. Okay, so here's a theorem which I already talked to you about, you know, sort of informally, but here it actually states it now that we for a moment. So keep that in mind. In other words, let's keep this in mind about possible complex realizations.
01:08:19.924 - 01:09:31.066, Speaker A: We'll come to that in a second. Okay, so maybe I can just start drawing that picture and we can talk about that in a second. Okay, so this corresponds to non negative eigenvalues. It also corresponds to all of these klimanga conditions being non negative. And essentially, if you have a real distance still, let's assume it's real, symmetric, and so on and so forth. If you have it here somewhere, you have a distance tuple that's outside of this cone, then you could have a complex realization of it. I could ask question differently, you know what if some Kaylee menga condition that not necessarily a full subset of them, one single k le manga condition becomes strictly negative, then what happens? Okay, so these are two slightly different questions.
01:09:31.066 - 01:10:27.714, Speaker A: One of them you can say is to get out of here, you can say these guys are all saying negative, positive, non negative eigenvalues. So you sort of walk little by little, let's say out of here, and you get to a negative eigenvalue. Let's say this is exactly the non negative eigenvalues of the off real symmetric matrices. And let's say I'm still staying with the set of real symmetric matrices. I'm in the positive quadrant. So they're all positive, non negative, the entries in this matrix and the entry, and it's hollow, diagonal and everything. Suppose I'm just staying in that, in that subset of this real space, and I walk down here and I start from a positive eigenvalue and I end up in a negative eigenvalue, right? A particular eigenvalue of my matrix, let's say.
01:10:27.714 - 01:11:11.114, Speaker A: Okay, so my particular singular value eigenvalue doesn't matter which one you think, whichever you're convenient with. So basically you have these guys, some, some number of them, and the rest are all zero. If oops what happened? Okay, so some number of these are non negative and the rest are all zeros. Somehow it doesn't like this color, something like that. So rest are all zeros. I give up. Okay, undo.
01:11:11.114 - 01:11:56.386, Speaker A: Okay, so these guys are zeros. Okay, so here's a positive one. Let's say, let's say positive one that you slowly walk out and it becomes negative. If you're, if you're doing this, it's a continuous path. Some point, it's going to be zero, which means essentially you have dropped dimensions. Sufficiently many dimensions. I mean, sufficiently many of these simplicities have collapsed, right? So essentially all the points which used to now live, live in some higher dimension.
01:11:56.386 - 01:12:45.436, Speaker A: Sufficiently many of these simplices, or Cayley Manger determinants became zero, so that this entire, the whole thing collapse, all the points collapse to some smaller dimension. That's what it meant when you get here. But you could ask the question, what if only one of these Caylemanger determinants inside here, they're all non negative? What if only one of them became zero? So in other words, one example would be maybe you're somewhere on the boundary, which means that you are, let's say, a smaller dimension. You're not using all the n minus one dimensions for the endpoints. Let's say you're in two dimensions. All the points are living in two dimensions. Let's say this is a different picture.
01:12:45.436 - 01:13:07.092, Speaker A: So all of these are not unrelated to that picture. So let's say all the points are in two dimensions. And, uh, you know, you, you're changing the configuration as you change the configuration, or you're changing the distance vectors. You. All the distance vectors are here. I mean, distance distances are here. It's a distance tuple.
01:13:07.092 - 01:13:22.580, Speaker A: I've just drawn it this way. We'll come to that. There are different interpretations. You can think of it as a point configuration, or you can think of it as the set of pairwise distances. We'll come to that in a little bit. So, and I'm drawing both representations here. So essentially you can change this.
01:13:22.580 - 01:14:01.666, Speaker A: Keep changing this. Let's say move this point so that it lies here, so that one of the Cayley manga conditions that used to be non neglected positive now becomes zero. So this point moves here. But then let's say, now the triangle inequality no longer holds. So let's say this distance, I changed the distance matrix in such a way that this distance is smaller than the sum of these two distances. So at that point, at that exact point, that particular Cayley Menger. So this is the same point p.
01:14:01.666 - 01:14:57.588, Speaker A: I'm moving it that Klemanger determinant of these three distances, or these three points, that submatrix will become negative, but the whole thing is still in the same dimension. This is still living in using up two reals. It's using, the affine span is still two reals. So that means you are on some kind of a boundary thing here that corresponds to two reals. And one of these scaly Menger determinants vanishes, but it's still in r two, which is a little bit different from this situation where when you change, go from a positive eigenvalue to a negative eigenvalue, the eigenvalue vanishes, which means the entire point configuration drops one dimension. So the equivalent thing would be all of these points end up on a line. That's what this one would be.
01:14:57.588 - 01:15:51.056, Speaker A: Okay, so I could have drawn a similar picture with, you know, already on the boundary. We're sitting on the boundary, and certain numbers of eigenvalues were positive, and a whole bunch of them were zero. And you were moving out, and you're slowly changing this. So now how do you reconcile this? You understand what I'm saying? So I'm trying to reconcile non negativity of eigenvalues with Cayley manga determinants being non negative. And I'm giving you a situation where continuous path that sort of forces an eigenvalue to go from positive to negative will have to go through zero. So that path will have to pass through zero. And that putting that eigenvalue as zero means that all the points drop a dimension.
01:15:51.056 - 01:16:37.114, Speaker A: But here, thinking about in Klimanger terms, I can very well change one Klimanger determinant from zero, or a bunch of others which are affected by that Calimanga determinant. They can all go from positive to zero to negative while not having the points drop one dimension. So it's not quite clear what exactly is going on. Anybody has an idea what the correspondence between the eigenvalues and the Cayley manger determinants is in this, in this particular context. Does it even make sense what I'm saying? What is bothering me or what should be bothering you?
01:16:46.414 - 01:17:23.508, Speaker B: I guess I'm a little unclear about the way you get this path. So I'm imagining that you have a fixed matrix delta, and that corresponds to a realization that happens to sit inside the cone. Yeah. And then you've got some kind of parameter that you're changing. And as you change that parameter, all of the entries of delta change slightly. And hang on, one process to move along this path.
01:17:23.636 - 01:17:37.732, Speaker A: Well, just one thing that you said, I'm not, I don't completely agree with. It doesn't have to be inside this cone, the starting point. It could already be on the boundary of this cone. Yeah, fair enough. Yeah. Some of these. These are already zero.
01:17:37.732 - 01:17:39.852, Speaker A: Yeah. Yeah.
01:17:39.908 - 01:18:14.686, Speaker B: At some point along this path, you get to the. Eventually you get. Get to the place where. Where you've got a negative eigenvalue and your points have completely collapsed. But before you get there, you may hit the boundary. In fact, you have to hit the boundary. And when we're on that boundary, we may only have one of these Kalimanger conditions becoming negative.
01:18:14.686 - 01:18:22.834, Speaker B: And the question is, what is that? What is that? What's the interpretation of that single Caylee Manger condition becoming negative?
01:18:24.254 - 01:18:51.698, Speaker A: Yes. Because that single Klemanger condition becoming negative, it doesn't have to be a single one. It could be a few of them. But whatever it is, them becoming negative doesn't force this set of points to drop dimension by one. Doesn't seem to force the positive points to drop dimensions by one. I mean, anymore, though, they don't exist in real, real space. Yes.
01:18:51.746 - 01:18:54.554, Speaker B: There's just a complex realization at this point.
01:18:54.674 - 01:19:26.142, Speaker A: There's a complex realization and doesn't seem to drop dimension by one. So here, on the other hand, when this becomes, in order to follow any kind of continuous path here, to go from having a positive eigenvalue to a negative eigenvalue, you would have to go through zero eigenvalue. And that actually forces all the conditions, all the daily manga conditions to. I mean. Sorry. All the points to actually lie on one dimension fewer. Right.
01:19:26.142 - 01:20:15.204, Speaker A: So what all I'm trying to point out is that if you want to reconcile these two views of what this cone looks like, you need to be able to answer some of these questions. And that's supposed to motivate my next slide. Wait. Okay, so, so now we. Okay, so now we're just going back to talking about particular dimensions. Until now, you know, we sort of incidentally mentioned it. But, for example, this theorem didn't care about dimension at all.
01:20:15.204 - 01:20:38.504, Speaker A: You know, it just said, am I in the. In the cone or not? In the. Is it an EDM or not? Okay, so I need to change. Stop. Share for a second so that. I think that's my phone. Oh, come on.
01:20:38.504 - 01:21:02.488, Speaker A: There we go. Okay. Okay. Can go back. Share screen. Okay. Yeah.
01:21:02.488 - 01:21:30.984, Speaker A: I was trying to avoid showing you my email, but okay. It doesn't matter. There's nothing that important in it. So not private. So, yeah. So now we're going to start talking about particular dimensions. When can we say what? Kyle mega conditions must be checked to determine if a realization is in dimension D.
01:21:30.984 - 01:22:28.168, Speaker A: So I kind of mentioned this with this p four and p five before, but this is, as I said, how Menger initially proved it. And this theorem holds, essentially says that if it's realizable dimension d, and not in rd minus one. So it's actually a pretty strong statement. It's saying the affine span does not drop in dimensions realizable in D if and only if there is a set of indices. And the algorithm actually essentially finds a way to locate that set very efficiently. There's a set of indices of one less than or equal to k less than d plus one, such that the Klemenga submatrix determinants of those indices are strictly positive. And so, remember that if it's d plus one, there are only k.
01:22:28.168 - 01:23:18.238, Speaker A: I mean, if there are only d plus one of these determinants, each one larger than the other, of one dimension larger than the other, that we're checking positivity, and then we have these pairs. So beyond d plus one, there are a whole bunch of points whose indices are greater than d one. I mean, this is not the original ordering. This is some ordering that exists. And so you can check these two determinant simplices, and then the simplex of one dimension larger, as we talked about. So this would have been, in that example, p four and p five, or, sorry, p five and p six, maybe I forget. So p four and p five, and this would have been p one to p three along with p four.
01:23:18.238 - 01:24:38.114, Speaker A: P one to p three along with p five. And this is the one with p one to p three along with p four and p five, in that example that we had when we started up. And this, and it turns out that this set of conditions is sufficient to actually determine the particular dimension in which you have a realization. Okay? And so this theorem should be contrasted from this theorem, which doesn't care about the dimension, just says, does there exist one euclidean realization of some dimension? And there we only have these n determinants that we may have to check. But in this case, you may have to check more determinants. As far as I know, these are the minimum numbers that I know of in terms of complexity, number of determinants that you would need to check. I'm not paying attention to the size of the determinant obviously, there's some complexity involved if you have large determinants, but so there are many equivalent characterization with other such complete sets of Klemenga conditions, both for just whether there is euclidean realization and for the case of whether there's euclidean realization of a particular dimension.
01:24:38.114 - 01:25:50.612, Speaker A: Okay, so as a result of what I just said, the original algorithm, as a result of this theorem, this original algorithm guarantees that you find exact minimum dimension in which something is realizable. Okay, so, um, so this is the. Yeah, so, so you just have to add this, this fact here that that given set of d plus one points and any two other points, d and k, lie in there. I find span if and only if the k lemanga determinants of this one, this one, and the subset of zero. So this fact, the theorem that we had in the previous page just follows from this factor. And checking all unused distances, instead of actually computing the coordinates and then finding the distances, you can do it this way. Another thing I want to point out is that this notion of dependence versus strict dependence of distances here, if you have, in this case, let's say j was the point, p 4k was the point p five, and this is the simplex five.
01:25:50.612 - 01:27:00.934, Speaker A: This will be the four simplex of one, two, three and four. Four simplex of one, two, three and five. And the five simplex of one, two, three and four and five. If you look here, the, you know, maybe I should go back to that slide here. So, okay, so in this slide, so if you have, if you look here, let's for the moment, just pay attention to one, two, three and four. So in one, two, three and four, essentially, this is saying that once you have, because they lie in two dimensions, they happen to lie in two dimensions. In other words, this Cayley Manger condition of 1234 being equal to zero, and all the others were positive, the smaller ones were positive.
01:27:00.934 - 01:28:28.256, Speaker A: You actually have that this distance is, is, all the other distances are independent. And let's say p two, p three, as a distance is dependent because of that Cayley Manger condition, this being equal to zero, one of them and all the others not being zero, one of them is dependent on the others, and the rest are independent. In a very loose way of talking about dependence on independence, you may, you might say that, well, not quite. I mean, because you could have had, in the previous dimension, p two could have been on the other side. So, you know, there is another possible distance between p two and p three, right? So, and similarly, if you look at these four, you could say that five of these distances would imply the, sorry, five of these implies, distances imply the 6th one, except for this possibility that it could have two possible values, but at least a finite number of values. So, however, if you now look at all five of them, once you know, we know that this Klemenga determinant is zero, and we know that this k. Lemanga determinant is zero.
01:28:28.256 - 01:30:03.104, Speaker A: So in other words, p one, p two, p three and p four, p one, p two, p three and p five. Once you know those two are zero, then the zeroness, if you will, of all of them, the determinant of p one, p two, p three, p four, p five, that condition of the full force simplex being zero, volume being zero, actually completely determines this distance. Okay, so there's this small difference between the extent to which this distance is determined by the remaining distances, or this distance by the remaining distances, versus this distance being determined by all of these, by that last Klimanga condition, given the first two kilimanger conditions. Okay, so, and this, I think in graph theory theoretic terms, this actually seems to make a fair bit of difference. And we'll talk about it later. Okay, so, okay, so now let's for a moment think about this. Realize the relationship between the eigenvalues and the Cayley manga determinants.
01:30:03.104 - 01:31:08.456, Speaker A: For that purpose, I just wanted to talk about how one computes the realization from using Schoenberg. Okay, so using Schoenberg, you know, if you remember, briefly recall the proof went roughly like this, has a clean and realization if and only if there is the off that, or go in the other direction, if you will. You know, the realizations psd gamma that has been defined in this way, starting from Delta, which we did in the course of the proof, um, we just massaged delta into a gamma like matrix, if you remember. So, um, so the gamma that you obtained from delta turns out to be the gram matrix of that realization, uh, if one ex, whichever that realization is. And then, you know, the rest of it is just because of this formula, it follows that delta is negative, semi definite over that subspace. I've just left that out. So that's how the proof went.
01:31:08.456 - 01:31:48.332, Speaker A: And once we have this, we can basically extract the gamma from the delta in this way. And then, you know, gamma must be of this form. So you can, you know, simply factor it. So you do singular value decomposition, you will get it. Gammas, you know, to be of this form, us v, in this case, u and v are the same. And, sorry, v and u transpose are the same and they look like this, where the number of non negative singular values will be the number of nonzero entries. Here, the d here represents the dimension in which the realization takes place.
01:31:48.332 - 01:32:19.360, Speaker A: And then, see the d coordinates of the points. And the rest of the coordinates are zero. So these rows are basically the coordinates, the d coordinates of the points of the realization. And s is a diagonal matrix with d non non zero, I mean positive eigenvalues and singular values and zeros on the diagonal. Okay, so, and if you remember, you know, the actual factor factorization. Oh, sorry. This is v and u.
01:32:19.360 - 01:32:58.906, Speaker A: Now the actual points will be points. Where did I have that? Yeah, so it's p transpose, p, where gamma is now of this form, usu transpose. So basically this p transpose will be this thing times the square root of these eigenvalues. I'm just repeating what we already did. So these are positive. Then we have a real realization. If these are potentially negative, then we have some complex, this times these complex numbers get you complex realizations.
01:32:58.906 - 01:34:47.310, Speaker A: Okay, so basically what we're saying is that, so we can try to draw an interpretation of these point configurations, which could be complex or real. These are modulo, the euclidean isometries and the equivalent way of looking at them as distance matrix, the bijection between these and if in all cases they are symmetric, because you're taking distances, you have to be a little careful once you start talking about complex points, what we mean by distances, when this is where the divergence takes place between the KaYLeE menga, I mean, sort of the algebraic way of looking at it and the Schoenberg way of looking, I mean the analysis way of looking at it, where the distance in this case would be the complex distance, which is based on the complex inner product, where, which is always a real number. But when you talk about the distances here and the Cayley Menger, these are polynomials, distance polynomials, which are the, what we think about, which is if you have two points, it's ex two points, it's Xi minus. In two dimensions, it's xi minus x j squared plus y I minus y j squared. So which could be potentially a complex number too. So there's a little, at that point, you have to be careful once you have complex realizations, what we talk about. So here, when you talk in analysis terms, where was I? So when you do this, for example, you usually take the complex conjugate, sorry.
01:34:47.310 - 01:35:25.926, Speaker A: When you do the inner product, you take the gram matrix. Where did I have the gram matrix? Yeah. When you take the gram matrix, you would actually take the complex conjugate here. So as a result of which, what you mean in terms of when you factor it and you get the set of points, what those points are, they're still complex if these are negative. But what those points are will be different. So you have to be careful at that point. There's a divergence between these two theorems in terms of what we mean by the realization for that, or in other words, what we mean by the distances.
01:35:25.926 - 01:36:07.854, Speaker A: So given a set of distances, because those distances are defined differently in the two cases, it's still complex. I mean, for real cases, to show that the realization is real, once you're in real domain, they all mean the same thing, but once you're in complex, they mean something else. So you have to be careful about that. I think we are nearing the end here, but let me draw this picture so that we can. Okay, so we have something to do on Thursday. That's all it means. But let me at least draw this picture here so that we can leave with that picture.
01:36:07.854 - 01:36:42.194, Speaker A: Okay, so what we're saying here is that there are two views of things. So here we have, you know, point configurations. Whoops. Okay. Definitely did something bad. That's. I have my pen has a button that if you somehow manage to press that button by mistake, that's the end of that.
01:36:42.194 - 01:37:18.854, Speaker A: Okay, so hopefully now I'm fine. So if I have a point, config a set of points, let's say endpoints, and for the moment, let's say they are in d space. That means this is to put them all together as in that matrix form, for example. So you have. They had rows like this. But I'll just put these as columns for now. So you have p one, p one one, p 1d, whatever.
01:37:18.854 - 01:38:05.904, Speaker A: And then you have n of these. Okay, so that's a point configuration. I'm thinking about this as a point configuration. Of course, this is mod, you know, the pleiadian isometries, right? So orientation preserving ones, I guess. And so now you have that on this side, and then you have the distance vectors. So you have the delta, I think we call them Delta IJ, living in r, and choose two on this side. So you have, there's a bijection between these guys.
01:38:05.904 - 01:38:41.624, Speaker A: Oh, sorry. Off rank. So these guys living in twos too. And the matrix, delta IJ. If I say here, these are living in D dimensions. So this, these guys correspond to off rank d. Okay, and in general, as I said, you know, you can always, you know, without loss of generality, we can just say d is less than or equal to minus one.
01:38:41.624 - 01:39:34.694, Speaker A: Okay? So there's this bijection between these guys. And we can, sometimes it's convenient to think in these terms. Sometimes it's convenient to think in terms of the distance matrices. Now, and this is, I'm thinking of, these are all in the reals, everything is hunky dory. These, these things are PSD, no, negative, semi definite. And sometimes we know, and these are in bijection with the gram matrix entries, gamma, IJ, I mean, gram matrices. And these are PSD, right? So essentially we have this cone that we had before of where every point in here is one of these guys, or one of these guys.
01:39:34.694 - 01:40:27.906, Speaker A: We, you know, it really doesn't matter. There's a linear map between these, I mean, or at least from the squares, there's a linear map. Okay? And then, and then you could have, potentially, these points could be complex. If they are complex, then we have to be a little careful what we mean by this distance here. In one case, in the real case, it really doesn't matter. Everything that I've drawn is correct. When you have complex numbers, then what we mean by here, if we use the usual distance, using the inner product, then you have, you know, the realization here in complexes would be what I just said a moment ago.
01:40:27.906 - 01:41:30.500, Speaker A: We take the gram matrix here and think of it as, you know, the complex conjugate there and then do the realization. But the other way. If you wanted to use the Cayley Mengers and say what happens when a Cayley Menger is non negative? You would then be using this usual distance, I mean the algebraic distance, so to speak, which if you think of, you know, sum over I x between two points, x and y would be sum over. So two points, let's say maybe p one and p two, or p and q maybe instead of x and y, because those look too much like coordinates. Okay, so p q. So the ith coordinate, I just did this. So that's the sort of the one that's used for the Klemangers.
01:41:30.500 - 01:42:40.302, Speaker A: And then here you just use the inner products, complex inner products to get. So you would use p minus q, comma, p minus q star, whatever. And I mean this automatically when you do the complex in a product, you have the. So essentially you have to be a little careful, but you can still try to do this correspondence that I was talking about. So next time what we'll do is we'll talk a little more about the structure of the EDM cone what the boundaries look like, what they correspond to in terms of lower dimensional realizations, what we mean by that, what the dimensions of those boundary facets are, and what the intermediate dimensions. So you'll see that dimensions of the boundary facets will jump. And so we need to know what the intermediate dimensions are.
01:42:40.302 - 01:43:58.394, Speaker A: And we'll also consider this question that we just talked about. What happens if some Kalimanga conditions vanish? But the, but, you know, the dimension doesn't drop. So in other words, what, what exactly is the consequence? What, what does that mean in terms of the eigenvalues? So what that to be, to sort of a spoiler alert. So in that situation where we saw the complex, where one of those triangles in that picture that we had, where one of the complex, maybe I lost that picture. No, here. So where one of the, one of these guys becomes negative, let's say it turns out that in that situation, the complex realization actually jumps in dimension. So what happens is you do change, but what happens is these eigenvalues, some small negative numbers appear here.
01:43:58.394 - 01:44:33.284, Speaker A: Okay? So essentially you're still going from one of these eigenvalues, non negative eigenvalues, as you, instead of in your path. What happens is some of these eigenvalues become small negative numbers appear. So in other words, you end up with a complex realization in higher dimensions for this. So, yeah, so that's the, that's the, to just at least give you some kind of end to that story.
01:44:36.344 - 01:44:37.256, Speaker C: Hey, Mira?
01:44:37.360 - 01:44:38.044, Speaker A: Yeah?
01:44:38.504 - 01:44:44.894, Speaker C: Can we, can we maybe look at it in the case n equals three and then n equals four?
01:44:45.394 - 01:44:55.754, Speaker A: Yeah, let's do that next time. Okay, I will, I am going to do that. I have to show there's no other way for me to explain some of these things about the dimensional strata of the boundary.
01:44:55.914 - 01:45:06.786, Speaker C: Okay, cool. And then maybe, because at that, in those cases, we could maybe write down explicitly all the equations that define the different parts of the boundaries.
01:45:06.970 - 01:45:20.394, Speaker A: We can try, but I have to tell you, the dimensions just blow up like crazy right away. You're going to have to take a few things on. Oh, just, just by proving it without actually working out examples.
01:45:21.294 - 01:45:22.062, Speaker C: Okay.
01:45:22.198 - 01:45:25.634, Speaker A: Yeah, because they just grow that it's n. Choose two dimensions.
01:45:26.574 - 01:45:30.486, Speaker C: So it becomes, when n equals three, it'll be in three.
01:45:30.630 - 01:45:57.996, Speaker A: We can try then you. But yes, you can. Equals three. Jump. Unfortunately, there's no jump in dimension. I'll tell you what I mean by jump. So essentially what happens is the boundary of the boundary of any object, you expect to be one dimension lower than the interior, the relative dimension should be one dimension lower than the interior, but the interior is n.
01:45:57.996 - 01:46:39.504, Speaker A: Choose two. We know that the boundary would expect to be n. Choose two minus one. But if you think about it, in this bijection between that I had here a moment ago between the points and the point configurations, and this, if you think about it in these terms, the point configuration, modulo the special Eulidean group for n minus one dimension. I'm sorry, these were already in n minus one. Sorry. So for n minus two dimensions, which would be, you would expect to be the boundary would be, according to this calculation, d, which is n minus two.
01:46:39.504 - 01:47:04.144, Speaker A: N minus two times n, right, minus the minus the, whatever this dimension is, which is d plus one. Choose two. So in this case, it would be n minus two. Choose two. Now, it so turns out, if you do the calculation, that that number turns out to be exactly n minus one. Sorry, n. Choose two minus one.
01:47:04.144 - 01:48:20.972, Speaker A: So the bijection, so the two ways of calculating it, you can reconcile the dimension of the boundary. But if you go one dimension lower than that, the points in, say, n minus three dimensions, you do the same calculation based on this bijection, and then you ask yourself, okay, is that one dimension lower than that? Are those the one lower dimensional facets of the boundary? And the answer is no. And then you can ask, okay, one dimension lower facets of the boundary, what are they then? They are not the point configurations of one dimension lower, but one dimension lower facets of the boundary. What are they then those turn out to be all those different special positions that you can have of different affine combinations of the points turning out to be zero. And these just rapidly multiply. And you can't get that level of complexity when you're only working with three points. This is my long story to tell you why working on an example of such a small example, um, doesn't illustrate these, these things like jumps in dimension and all that.
01:48:20.972 - 01:48:22.344, Speaker A: You see what I'm saying?
01:48:23.244 - 01:48:29.916, Speaker C: Yeah, that makes sense. But maybe for n equals four, if we slice it with a certain plane or something.
01:48:30.060 - 01:49:01.352, Speaker A: Yeah, we can try. We can try. Okay. In fact, I was talking to will, um, that, you know, I would be interested in. This is a very unusual course, in a way, because it's kind of drawing connections between things that people usually don't take the trouble to draw very close connections to between. So. And I think, I don't know to what extent it helps, but I think having different ways of looking at the same object is always useful.
01:49:01.352 - 01:49:22.624, Speaker A: Right. So I was hoping that will and I could get together and sort of make a more. We'll think about how this could be made into a nice, more studied course with lots of examples and exercises and things like that. And if you want to join that, that would be great.
