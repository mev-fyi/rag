00:00:34.794 - 00:01:01.768, Speaker A: Okay. Yeah, I guess it's time. It's time to start. So what you have seen. So, first of all, an organizational matter, next week, there will be no lectures. There's going to be the workshop next door, and the 10 October is thanksgiving here in Canada. So there will be no lecture because of this.
00:01:01.768 - 00:01:05.272, Speaker A: So the next lecture will be in two weeks, okay? Exactly two weeks from now.
00:01:05.328 - 00:01:06.032, Speaker B: Okay?
00:01:06.208 - 00:01:20.152, Speaker A: Now we have seen, we have seen during our, you know, what was Monday, the definitions of major grammar, vascular convergence on one side and of gamma convergence of the other.
00:01:20.288 - 00:01:20.920, Speaker B: Okay?
00:01:21.032 - 00:01:46.692, Speaker A: Now here is the, you know, the basic theorem that links them, which is the following. And our first goal for today is to understand the statement of this theorem, and then they will prove it. And the statement is that basically, you know, let me say normalized spaces, you know, for finite and even infinite. So measure gravas convergence is equivalent to Moscow convergence of the relative entropy.
00:01:46.808 - 00:01:48.692, Speaker B: Okay, I want to, you know, to.
00:01:48.708 - 00:01:53.932, Speaker A: Explain what this means. And so if you remember one of.
00:01:53.948 - 00:01:56.828, Speaker B: The things that I said, so that.
00:01:56.876 - 00:02:15.352, Speaker A: You know, I leave it there, but, you know, we comment later on. So one of the things I've mentioned last time was that the effective domain of the energies typically can, can, you know, can change along gamma convergence. So you remember that. So if I define the domain of.
00:02:15.368 - 00:02:19.296, Speaker B: A function as the set of points.
00:02:19.400 - 00:02:24.160, Speaker A: Where, you know, where the functional is finite, not plus infinity.
00:02:24.272 - 00:02:24.904, Speaker B: Okay?
00:02:25.024 - 00:02:46.134, Speaker A: Then typically, if I have en that gamma converges or Moscow converges to infinity, well, then what we have is that the domain of the limit function is included in the, you know, Kuratowski limit of the domains of the.
00:02:47.074 - 00:02:48.642, Speaker B: All right, remember this?
00:02:48.738 - 00:02:49.738, Speaker A: Am I speaking nonsense?
00:02:49.826 - 00:02:50.242, Speaker B: Right?
00:02:50.338 - 00:02:54.134, Speaker A: And typically, typically, this can be disjoint from all of these.
00:02:55.034 - 00:02:55.934, Speaker B: Okay?
00:02:56.794 - 00:03:05.434, Speaker A: So the typical example is, I mean, the silly example is en is just finite. Say zero at xn plus infinity everywhere else.
00:03:05.594 - 00:03:06.146, Speaker B: And then.
00:03:06.210 - 00:03:33.038, Speaker A: And then xn is convergent to x infinity. And, you know, the limit first of the limit function will be the infinity would be, you know, the function making zero just in x infinity and plus infinity everywhere else. And that is a perfect example where, you know, the domain of finiteness are disjoint, but they are converging. Okay, similarly, similarly, if we have mn, which is weakly converging to a measurement infinity on it, on a given common.
00:03:33.086 - 00:03:38.562, Speaker B: Space, then the support of m infinity.
00:03:38.658 - 00:03:40.894, Speaker A: Is included in the Kuratovsky lemming.
00:03:43.994 - 00:03:48.778, Speaker B: Of the support of m. It's a little.
00:03:48.826 - 00:03:56.698, Speaker A: Bit of a, you know, first little analogy, very little vague analogy between gamma convergence and, you know, weak convergence.
00:03:56.786 - 00:03:57.010, Speaker B: And.
00:03:57.042 - 00:04:20.080, Speaker A: But we can sort of build on top of this a notion of convergence, of functionals defined on a sequence of varying spaces. So say that we have, say that we have x n metric measure spaces measured. Gamma was converging to x in p. Okay, then by definition, by definition, this.
00:04:20.112 - 00:04:22.764, Speaker B: Means that there is, y.
00:04:24.584 - 00:04:41.304, Speaker A: Completes a parable and isometric bendings, yotta n isometries, I mean, isometric and bending.
00:04:44.244 - 00:04:47.764, Speaker B: Savings such.
00:04:47.804 - 00:04:55.560, Speaker A: That, such that, but such that yota n push forward m n weakly converges.
00:04:55.632 - 00:05:02.408, Speaker B: To infinity by definition. Right.
00:05:02.576 - 00:05:03.284, Speaker A: Now.
00:05:05.144 - 00:05:05.936, Speaker B: Stay.
00:05:06.120 - 00:05:23.456, Speaker A: So let's call, let's call this, you know, the collection of the space y and the embeddings. Realization of the convergence. So, once you have a sequence which is converging in the measure grammar sense, and you fix a realization, once you fix the, you can say you can.
00:05:23.480 - 00:05:27.284, Speaker B: Give a meaning to x n points.
00:05:27.784 - 00:05:33.124, Speaker A: In, you know, the capital space x n converging through some point x infinity in the limit point.
00:05:34.264 - 00:05:35.404, Speaker B: By definition.
00:05:38.304 - 00:05:44.284, Speaker A: This means that, you know, once you see these points inside the given big space, this convergence.
00:05:47.604 - 00:05:52.996, Speaker B: Right. Of course, the definition depends on the.
00:05:53.020 - 00:06:24.216, Speaker A: Chosen, you know, their relation, but that takes it. But the dependence is not that big in some sense. Okay, then I'll comment in a second. Now, if you have, say, if you have now a sequence of functionals, let's say en defined from xn to r r whatever. Now, once we have this notion of convergence of point, I can say, I can say what it means that, say I can define the gamma limit, let's.
00:06:24.240 - 00:06:26.444, Speaker B: Say, of the ends.
00:06:26.904 - 00:06:30.472, Speaker A: This would be in the gamma limit. So, I mean, these would be functional.
00:06:30.648 - 00:06:32.404, Speaker B: From x infinite to our bar.
00:06:32.824 - 00:06:34.224, Speaker A: And how are they defined?
00:06:34.304 - 00:06:48.764, Speaker B: What the gamma limit of en at the point x will be the inf, you know, of the limit of en xn among all the secret, the inf.
00:06:48.804 - 00:06:54.304, Speaker A: Is taken among all the sequences x n. They're convergent to x and with x n belonging, you know.
00:06:57.764 - 00:06:58.704, Speaker B: Make sense.
00:07:00.804 - 00:07:27.188, Speaker A: I mean, once, remember, no, once, I mean, basically, to define gamma convergence, I only need the notion of convergence of once. I know what it means, convergence to point. This is the same, this is the same as saying, as thinking this en same. So you define en if you want.
00:07:27.236 - 00:07:32.400, Speaker B: Tilde from y to r barrel by.
00:07:32.432 - 00:08:00.320, Speaker A: Putting, you know, e n tilde of y is equal to en of x, if, you know, y is equal to yota n of x and plus infinity. Otherwise. So basically you are thinking of those functions defined on the space y plus infinity, where they were not previously defined and as the original function. Okay, and then, and then this is true. So then the gamma limb mean for.
00:08:00.352 - 00:08:03.616, Speaker B: Gamma lean soup of these FuNctionals, you.
00:08:03.640 - 00:08:20.432, Speaker A: Know, we went one after you, you know, would be. So the gap. So, so the gamma limb of the en hat is equal to the gamma.
00:08:20.488 - 00:08:23.484, Speaker B: Limb of the en.
00:08:24.764 - 00:08:35.424, Speaker A: I guess, I guess this composition, this composed I infinity is equal to this.
00:08:37.084 - 00:08:38.344, Speaker B: On x infinity.
00:08:42.324 - 00:08:43.316, Speaker A: And the same with the gamma.
00:08:43.340 - 00:08:46.084, Speaker B: And so I'm saying that the limit.
00:08:46.124 - 00:08:50.834, Speaker A: Functional, basically, if you evaluate at points on the image of x infinity, that's.
00:08:50.884 - 00:08:51.554, Speaker B: You know.
00:08:53.694 - 00:09:09.070, Speaker A: Okay, and I do this with function at some points, but I can do the Same with Function, let's say, on probability measures. So I can say for very, very, the same way I can say that.
00:09:09.142 - 00:09:13.702, Speaker B: Mu n probability measure on xn is.
00:09:13.798 - 00:09:18.394, Speaker A: Weakly converging to mu infinity, the probability measure on x infinity.
00:09:18.894 - 00:09:19.634, Speaker B: If.
00:09:24.454 - 00:09:35.774, Speaker A: Yotta n push forward mun is weakly converging, push forward mu infinity, you know, in, you know, weak convergence as probability measures.
00:09:35.814 - 00:09:36.874, Speaker B: Now on Y.
00:09:41.214 - 00:09:44.494, Speaker A: I'm putting all these pieces all together in this big space.
00:09:44.534 - 00:09:46.900, Speaker B: There's nothing monitoring, okay?
00:09:47.052 - 00:10:01.388, Speaker A: And similarly, I can say that nu n is w two convergence to infinity. If you know, w two of, you know, this measure, new n comma, the.
00:10:01.396 - 00:10:10.052, Speaker B: Other infinity is going to zero, right?
00:10:10.108 - 00:10:25.256, Speaker A: And I, you know, I can, and once I have these notions of convergence, I can speak about gamma convergence functional on the space of probability measures. And, you know, gamma convergence should have an underlying topology. So I could speak gamma convergence with.
00:10:25.280 - 00:10:27.728, Speaker B: Respect to weak convergence or with respect.
00:10:27.776 - 00:10:33.496, Speaker A: To w two convergence or with respect to both, in which case I'm speaking about Moscow convergence.
00:10:33.680 - 00:10:35.040, Speaker B: Okay, this is stronger than this.
00:10:35.072 - 00:10:38.564, Speaker A: So instead, this is my strong convergence, weak convergence. When I think about.
00:10:41.344 - 00:10:42.284, Speaker B: Make sense.
00:10:43.844 - 00:10:45.812, Speaker A: Now a word, a word.
00:10:45.908 - 00:10:47.984, Speaker B: So a word about.
00:10:51.924 - 00:11:30.098, Speaker A: The dependence on the underlation. Because of course, all this depends on the chosen realization. If you come with a different realization, you might say, okay, my gamma. So my sequences will converge to different points, my measure will converge to different points, so I will gamma converge to different parts. Well, yes and no. I mean yes, but basically this occurs only when there is some auto isometry of the limit structure. So up to isomorphism of metric measure structures and perhaps, you know, functional preserving isometries, this does not really change.
00:11:30.098 - 00:11:36.626, Speaker A: So we can agree that, you know, let's fix one realization and do computational over there and things won't change that.
00:11:36.650 - 00:11:46.100, Speaker B: Much if we just, okay, okay, so now the statement is this, right?
00:11:46.172 - 00:11:48.500, Speaker A: It means I fix the validation and.
00:11:48.532 - 00:12:00.844, Speaker B: After we fix the function of the gamma component, almost code questions, proof.
00:12:01.004 - 00:12:08.786, Speaker A: Okay, and I must say MSc. So in here you will see one thing. So, as it often occurs, okay, the meta statement.
00:12:08.850 - 00:12:11.562, Speaker B: So as it often occurs when either.
00:12:11.618 - 00:12:17.370, Speaker A: Grammar was or for measure of convergence is involved, or gamma convergence is involved, proof are easy.
00:12:17.442 - 00:12:17.754, Speaker B: Okay?
00:12:17.794 - 00:12:28.854, Speaker A: So that's one of the things, one of the power of this concept is that, you know, they're extremely general, extremely good. But in proving the things gamma converge or molecule typically is easy, I mean, not always. I mean, there are difficult theorems, but difficult.
00:12:33.824 - 00:12:38.072, Speaker B: So this implication is basically a joke.
00:12:38.248 - 00:12:45.040, Speaker A: Okay, I've noticed this in preparing these lectures. I don't think these ever appear before I leave to use an exercise.
00:12:45.192 - 00:12:48.280, Speaker B: Okay, it's a joke in the meaning.
00:12:48.312 - 00:13:06.676, Speaker A: That, I mean, it's true, but it's totally useless. Okay, I don't think any use will ever come out of this remark. I thought, I thought presented these ways, and if and only it was sort of, you know, cool from the mathematical perspective, from an aesthetic point of view, but it's totally relevant. So the important thing, the important thing.
00:13:06.700 - 00:13:09.396, Speaker B: Is, that's the crucial thing.
00:13:09.540 - 00:13:12.184, Speaker A: And this is basically what lost two mabillani.
00:13:13.964 - 00:13:15.628, Speaker B: Well, of Milanese last two, I.
00:13:15.636 - 00:14:05.144, Speaker A: Mean, proved in their paper. So this implication is the key to the proof of the stability of the kurvas two dimensional condition. So everything in the end will boil down to this sort of stability. Okay, we will see some statement concerning sobola function that is independent on this, but, you know, for constraints, lower reach bound. This is your. Now, actually, in their papers, they never really mention gamma convergence, not even in the book optima transport or the new baby lenitis mentioned. But, you know, if you, if you want to know gamma conversion, and you read the papers, it's clear that, especially from the paper of Lottembelene, it is clear that this is what they proved on the weber of stone.
00:14:05.144 - 00:14:08.168, Speaker A: This is a bit more sort of a hidden thing, but I know it's there.
00:14:08.256 - 00:14:10.296, Speaker B: Okay, now let's prove this.
00:14:10.440 - 00:14:46.684, Speaker A: So I have to prove two things, right? I have to prove gamma limit and gamma limit. Let's put the gamma limit. So, first of all, gamma proving that if you think for a moment that if I have two sequences, let's say.
00:14:46.724 - 00:14:52.576, Speaker B: Mu n, mn inside, let me identify.
00:14:52.760 - 00:15:22.130, Speaker A: How these measures, I mean, on xn with the measures on y and such, that mu n weakly converges to mu and mn weakly converges to m, then, you know, all these probability measures as well. Then the entropy of mu with respect to m is less or equal than the limit of the entropy of mn.
00:15:22.282 - 00:15:23.946, Speaker B: Of mu n with respect to.
00:15:24.130 - 00:15:25.882, Speaker A: So what is sometimes called the joint.
00:15:25.938 - 00:15:28.834, Speaker B: Lower semi continuity of the entropy, typically.
00:15:28.954 - 00:15:39.934, Speaker A: When one first learns about the relative entropy, knows that it is lower semiconture and as a function on the other. But in practice, lower semiconterior.
00:15:40.774 - 00:15:42.582, Speaker B: Okay, so if I prove this, I'm.
00:15:42.598 - 00:15:57.206, Speaker A: Done with gamma limit. So, let's prove this. And this, in fact, is a consequence of the following beautiful duality formula. And by the way, this joint lower semicontinuity has nothing to do with the particular shape of the Boltzmann meaning interval.
00:15:57.230 - 00:15:59.894, Speaker B: Of, you know, Rollo growth.
00:16:00.014 - 00:16:12.330, Speaker A: Whenever you integrate a convex function of the density, basically, basically, you always end up with this sort of, so this kind of statement generalized to those rainy entropy that also occur in the study of pulver dimensional condition.
00:16:12.402 - 00:16:17.890, Speaker B: Okay, and the form actually duality formula is this.
00:16:17.962 - 00:16:29.994, Speaker A: So there's a beautiful duality formula that reads this. So let's define u of z as z log z. And so this is a convex function. It admits dual.
00:16:30.154 - 00:16:37.376, Speaker B: So u star of w, this is the soup over z of z dot.
00:16:37.440 - 00:16:39.644, Speaker A: W minus u of z.
00:16:41.304 - 00:16:42.144, Speaker B: Okay?
00:16:42.304 - 00:17:01.956, Speaker A: I mean, you can compute this, this is like e to the w minus one, but doesn't really matter. So, so take a convex function over here, you take this other, you know, the convex. And now the interesting thing is that the entropy admits the following dual formulation. This is the same as this is.
00:17:02.020 - 00:17:07.104, Speaker B: The soup over phi in say, cb.
00:17:09.884 - 00:17:22.492, Speaker A: Of the integral of phi dm minus the integral of u star composed phi.
00:17:22.628 - 00:17:28.774, Speaker B: D. Yeah, I think it should be this way. Yes. Okay.
00:17:29.754 - 00:17:38.454, Speaker A: And this soup is also, I could also take this among or on pie Borel unbounded.
00:17:38.754 - 00:17:53.206, Speaker B: I mentioned this for later, Borrell. So let me comment on this.
00:17:53.390 - 00:18:00.382, Speaker A: First of all, if I prove this duality formula for PI continuous, this lower semicontinuity statement is done, right?
00:18:00.438 - 00:18:02.874, Speaker B: Because for fixed phi.
00:18:04.694 - 00:18:31.042, Speaker A: So this, you know that. So here, there is a coupling within the measures. Here, you know, I'm just the sum or difference of two functions, right? And each of these, you know, for given phi continuous and bounded, this is weakly continuous. In m u star composed, phi is a continuous function. So this is weaker continuous in mu. So the difference of two continuous functions is continuous. I take the soup is jointly reversed.
00:18:31.058 - 00:18:32.778, Speaker B: In equal to right.
00:18:32.906 - 00:18:34.294, Speaker A: So if I prove this and that.
00:18:34.634 - 00:18:36.586, Speaker B: Okay, and how do I prove this?
00:18:36.730 - 00:18:37.414, Speaker A: Well.
00:18:39.154 - 00:18:40.094, Speaker B: Not hard.
00:18:42.654 - 00:19:32.808, Speaker A: Okay, the full proof requires some approximation procedure, but I can quickly convince you that this beautiful formula is in place. And this has to do with the duality of the properties of the legend transform. So, given that the function u is convex and lower semicontinuous, okay, should extend, I should say that u is plus infinity if z is negative, so that this two makes sense among all the real z. So not only u star obeys that formula by definition, but also u of z is actually, you know, the dual of u star. I think I gave you an exercise about this some time ago for functions on rna.
00:19:32.846 - 00:19:35.988, Speaker B: Okay, this is on r, the dual of the dual.
00:19:36.036 - 00:19:59.868, Speaker A: If you start from a converse function, the dual, the dual is the function itself. Well, but now let's have a look. You know, let me, okay, I will skip some technicalities, but the basic idea is this. So the entropy. So let's say that nu is equal to rho m. I should discuss also the case where this is, you know, the measure is not absolutely continuous, but, you know, let me skip that. So then the entropy of mu with respect to m.
00:19:59.868 - 00:20:03.284, Speaker A: This is the integral of u of rho, the m.
00:20:05.824 - 00:20:06.616, Speaker B: Okay.
00:20:06.760 - 00:20:12.684, Speaker A: Actually, let me put this way, is u. Let me exaggerate with the notation u of rho of x d m of x.
00:20:14.904 - 00:20:15.644, Speaker B: Right.
00:20:16.784 - 00:20:30.244, Speaker A: Now pick your preferred, your preferred function, phi from y to r. Let's say bounded born, just to be sure, but just to be sure that things remain integrable.
00:20:32.024 - 00:20:39.404, Speaker B: And use phi of x as w in here.
00:20:41.824 - 00:20:47.568, Speaker A: This is greater or equal than the integral of rho of x. Phi of.
00:20:47.616 - 00:20:54.124, Speaker B: X minus u star of phi of x dm of x.
00:20:57.734 - 00:21:19.606, Speaker A: With equality. If I pick, if, okay, when is there equality? There's equality. If, you know, use, it should mention this is equal to, is actually equal to z dot w minus u star w. If what it is w is.
00:21:19.630 - 00:21:21.974, Speaker B: Equal to, um.
00:21:28.074 - 00:21:40.498, Speaker A: I don't remember. Anyway, so, so the, so the superlies, what I want to say with equality for, were chosen five forward chosen, chosen.
00:21:40.626 - 00:21:44.814, Speaker B: You know, PI of x. Okay.
00:21:45.434 - 00:21:48.684, Speaker A: Um, I should remember when this is told, I should.
00:21:48.724 - 00:21:52.984, Speaker B: So I think that when.
00:21:55.204 - 00:22:06.944, Speaker A: I guess that when, when e to the w minus one is equal z, I mean, we can compute, you know, we can make a derivative. So.
00:22:08.964 - 00:22:10.020, Speaker B: So this means two things.
00:22:10.052 - 00:22:11.860, Speaker A: So first of all, let's have a look at what is this expression.
00:22:11.892 - 00:22:12.464, Speaker B: Now.
00:22:14.174 - 00:22:28.606, Speaker A: Now everything, let me say, let me say we, row bounded. Let me just, row bounded. Just for, you know, let me exaggerate with the assumptions. If row is bound, row is bounded. Actually, I don't need it. Rho was in l one. Phi was bounded.
00:22:28.606 - 00:22:45.244, Speaker A: So this is in l one. Okay, so Phi was bounded. The u star of phi remains bounded by this because I know what it is. So this is really the difference of the integrals. Now, the interval of phi rho Dm. This is the same as the interval of phi d mu.
00:22:47.224 - 00:22:55.152, Speaker B: So, okay, and then, and then I.
00:22:55.168 - 00:22:59.964, Speaker A: Remain minus the interval of u star composition phi the m.
00:23:03.764 - 00:23:05.652, Speaker B: Okay, that's, I.
00:23:05.668 - 00:23:21.384, Speaker A: Mean, more or less the end of the proof, right? So, so I proved that. So I proved that this inequality. But I also know that for some well chosen phi, that in principle is just a, you know, is a Borrell function, I can get equality.
00:23:21.684 - 00:23:22.932, Speaker B: Okay, okay.
00:23:22.948 - 00:23:43.166, Speaker A: And then there's an approximation argument to say that, you know, sort of dominant, some dominated convergence and stuff like that, to be sure that I can actually, you know, I'm not claiming that this is a maximum, just claim that this is the soup. So you can approximate maybe the optimal, measurable phi with a well chosen sequence of continuous functions. Make sense? Yeah, that's basically all the idea.
00:23:43.310 - 00:23:43.994, Speaker B: Right.
00:23:45.214 - 00:23:51.754, Speaker A: Okay, so joint lower semi continuity. What about, what about the gamma range?
00:23:56.094 - 00:23:58.414, Speaker B: So I need to prove, need to prove.
00:24:05.474 - 00:24:37.034, Speaker A: For any new in, let me say p two x infinity that I'm thinking as a subset of p two y, there exists a sequence mu n in, let me say p two y such that mun is now w two converging to mu. Because I'm speaking multiple convergence, right? So in some sense, the gamma limb soup should be in the stronger topology.
00:24:39.854 - 00:24:40.594, Speaker B: With.
00:24:42.414 - 00:24:50.966, Speaker A: You know, the end limb soup of the entropy of mu n with respect to the measures mn less or.
00:24:50.990 - 00:24:54.274, Speaker B: Equal to the entropy of m infinity.
00:24:54.654 - 00:25:12.594, Speaker A: Mu with respect to. That's what I need to prove right now as a first remark. And it is valid whenever one wants to prove a gamma limb super argument. I don't really need to do this for every new thing. It is sufficient.
00:25:15.374 - 00:25:20.074, Speaker B: Sufficient to do so to approximate.
00:25:22.014 - 00:25:52.194, Speaker A: For mu infinity ranging in a set density. This is a concept that in tackles operations appears quite often. What does it mean? Density energy means that not only and set, let's say d. That's the energy. This means that for every mu in.
00:25:52.274 - 00:25:57.586, Speaker B: P two, the x infinity, there exists a sequence.
00:25:57.690 - 00:26:07.874, Speaker A: Okay, new, now let me call differently new n inside this set d, such that on one hand I have, you know, the density.
00:26:08.034 - 00:26:10.650, Speaker B: So this is the end.
00:26:10.842 - 00:26:23.762, Speaker A: And the, in this case, the entropy of nu n is converging to the entropy of nu. Here we have one fixed functional, which.
00:26:23.778 - 00:26:27.170, Speaker B: Is the entropy with respect to the limit measure, right?
00:26:27.362 - 00:26:44.414, Speaker A: And I'm thinking density energy means, of course, with respect to that function, I mean the diverset, which is at the same time dense, but where, where, you know, I can, while approximating my stuff with measures on, in this denser set, I can also approximate the energy.
00:26:45.394 - 00:26:49.674, Speaker B: So for instance deltas, or singular measures.
00:26:49.794 - 00:26:52.082, Speaker A: Are weakly dense, are also dense, w.
00:26:52.138 - 00:26:54.586, Speaker B: Two dense, but they are not dense.
00:26:54.610 - 00:27:02.512, Speaker A: In energy, because the entropy of any singular measure is always plus infinity. So I cannot, you know, approximate it in entropy.
00:27:02.578 - 00:27:07.188, Speaker B: Right. All the measures, of course, in general.
00:27:07.356 - 00:27:15.384, Speaker A: Whenever you have convergence, you have a lower semi continuity statement, right? So that's, in some sense, he's asking that there is no loss, no drop of energy.
00:27:15.684 - 00:27:17.304, Speaker B: Does it make sense what I'm saying?
00:27:18.124 - 00:27:43.914, Speaker A: Now, it's quite clear by a diagonalization argument that if you can prove the existence of these approximating sequences just for mu infinity belonging to some set density, then you're done. Okay, what's the general argument? Well, I pick a generic mu infinity. I first of all approximate this in energy. Then for every n, I pick new.
00:27:43.954 - 00:27:52.186, Speaker B: N. Now, for this new n, I can build the approximating sequence, you know, realizing the gamma limb soup, and then.
00:27:52.210 - 00:27:53.734, Speaker A: I act by diagonalization.
00:27:56.254 - 00:27:57.114, Speaker B: Okay.
00:28:00.014 - 00:28:03.434, Speaker A: Now, what is the set density that I pick.
00:28:07.454 - 00:28:19.794, Speaker B: D to be the set of measures mu equal rho m for rho, which is rho m infinity.
00:28:21.334 - 00:28:24.356, Speaker A: Continuous and bounded infinity makes no sense.
00:28:24.380 - 00:28:33.020, Speaker B: Of course, online, it's not hard to.
00:28:33.052 - 00:28:42.092, Speaker A: Check that measures with density, which is continuous and bounded are density. You know, the density with respect to.
00:28:42.108 - 00:28:45.784, Speaker B: The w two distance is quite trivial, I would say.
00:28:47.084 - 00:28:57.194, Speaker A: And whenever I approach. So, okay, perhaps, let me, let me point out this. So let me, let me, let me, let me try to convince you that this set is dense in energy. So.
00:28:59.014 - 00:29:00.646, Speaker B: If I pick a mu, which.
00:29:00.670 - 00:29:21.812, Speaker A: Is singular, then density in energy, I mean, I just need to, I mean, this condition is trivially satisfied. So I just, I just need to, now, because by the semi continuity, I always have a line, but given that this is plus infinity, the limb soup is automatically satisfied.
00:29:21.878 - 00:29:26.632, Speaker B: So this is, I don't understand, what is the sound?
00:29:26.688 - 00:29:27.936, Speaker A: This looks like the.
00:29:28.040 - 00:29:35.840, Speaker B: I thought it was the projector, but. Okay, anyway, so for mu, which is.
00:29:35.872 - 00:30:08.976, Speaker A: Singular, I just need to approximate my mu with, I mean, w two with respect to measures in that class. But the fact that measures with continuous bounded density are weakly dense, I mean, that's, that's, you know, that's not attached. Okay, okay, weak. And then w two exercise. Okay, that's not that much of a difference. What about approximating? Also the entropy indicates in the case a measure, I mean, is not a singular. Well, if mu, let's say, I mean, one can do like this, say that.
00:30:09.000 - 00:30:11.080, Speaker B: Mu is equal to rho m for.
00:30:11.112 - 00:30:30.354, Speaker A: Some rho that now is in l one. Okay, one thing that we can start trying is, okay, first of all, we truncate the density rho, and then, you know, put a normalization constant. And then it is clear that the entropy of these truncated measures at this, given a, these truncated measures are converging Wasserstein and b, that the entropies are converging.
00:30:30.394 - 00:30:33.242, Speaker B: Just a computation, okay, because after, if.
00:30:33.258 - 00:30:46.162, Speaker A: You truncate, basically you are decreasing the entropy, okay. Then you are multiplying by the normalization constant, but the normalization constant is going to one, so that will really not destroy. So now we are reduced to check the case when, when rho is an.
00:30:46.178 - 00:30:49.466, Speaker B: Infinity, okay, but, oh, l infinity is.
00:30:49.530 - 00:31:20.722, Speaker A: Almost like the sum. So this is almost like the sum finite sum of, say, alpha I, I can approximate in l infinity with alpha I characteristic function of Ei. For some, you know, it's almost a simple function, I can approximate anything. But if you approximate in l infinity, then, then it's obvious by dominating convergence that the entropies are converging. Okay, okay, now these EI are borrell by interior regularization, I can say that these, I can by far approximation, this.
00:31:20.738 - 00:31:24.682, Speaker B: EI can take them compact and disjoint, okay?
00:31:24.778 - 00:31:41.354, Speaker A: And once you have a finite number of compact disjoint sets and they're kind of function, you can, you know, interpolate between them in a continuous way and without, without increasing too much the entropy, because the deity is a uniform about it. That's the scheme of the proof, okay, of the approximation arbitrary.
00:31:42.534 - 00:31:48.590, Speaker B: So if you accept this, well, now we are done, because now, now I.
00:31:48.622 - 00:32:52.776, Speaker A: Need, I need to prove this fact, not for an arbitrary limit measure mu, but for a limit for a measure mu which has a continuous density with respect to, and you know, the difficulty in this gamma in soup is to exhibit the approximating sequence. So I should pick not a random approximating sequence, because for a random approximating sequence, typically I will have the infinite quality here. I really don't want to lose anything in passive, okay, so I have a limit measure, and I want to find approximate measures whose entropy is actually convergence to that of the limit. But if my limit measures continuous density, this gives me a way of finding approximating measures that sort of look like, because if rho is bounded and continuous in y, of course, of course, I can assume up to, you know, I take that I can assume the draw is non negative. For sure it was non negative, you know, in the support of m infinity, it was negative somewhere else, just, you know, take the positive path and we continue to be true.
00:32:52.880 - 00:32:54.328, Speaker B: So I can assume this.
00:32:54.496 - 00:33:19.472, Speaker A: Well, now, what can I pick as mun. Well, look, I want to pick the measure with the same density, but with respect to, you know, the difference. And you can tell me. Wait a second. I'm not sure, I mean, I'm sure that this is a non negative measure, but maybe, maybe it is not a probability measure and you would be right. So I should put here a constant c n, which is an amalization constant where cm is, you know, the integral.
00:33:19.528 - 00:33:25.844, Speaker B: Of rho d m n minus one. Okay.
00:33:27.124 - 00:33:36.304, Speaker A: Now notice that since rho is bounded and continuous and mn is weak convergent to m infinity, this constant goes to one.
00:33:36.764 - 00:33:40.324, Speaker B: In m in particular, for all n.
00:33:40.364 - 00:33:43.544, Speaker A: Sufficiently big is not zero. Okay, so the d measure is well defined.
00:33:44.444 - 00:33:45.304, Speaker B: Okay.
00:33:45.844 - 00:33:47.664, Speaker A: And how much is the entropy?
00:33:48.484 - 00:33:51.524, Speaker B: Okay, so the fact that this constant.
00:33:51.564 - 00:33:59.534, Speaker A: Is going to one should make it clear that mun is w two convergent to mu infinity.
00:34:01.154 - 00:34:03.954, Speaker B: Okay. Okay, I should make one.
00:34:03.994 - 00:34:27.774, Speaker A: Okay, wait a Second. So I should be a little bit more precise. It is clear, given that Mn is converging weakly to m infinity, that mun weakly converges to mu infinity. I should work a little bit harder to prove w to convergence. I will comment on this in a Second. Let me just check the, the entropy first. What is the entropy of mu n with respect to mn? Well, I know what's the density, right.
00:34:27.934 - 00:34:42.954, Speaker B: That's the integral of c n rho log c n rho d m n. Right. And how much is this?
00:34:43.774 - 00:35:06.294, Speaker A: Well, this is equal to, I mean, a log of the product is sum of the logs. So this is equal to c n log of c n integral of rho dmn plus, I guess, c n integral of rho log rho d m n.
00:35:09.714 - 00:35:11.786, Speaker B: Right. Now let's look.
00:35:11.810 - 00:35:23.622, Speaker A: So what happens here? So, so roll of row is banded continuous. So roll of row is also banner continuous. I mean, there's non negative. So, so this goes to the, you.
00:35:23.638 - 00:35:26.966, Speaker B: Know, the interrogal growth dm infinity, which.
00:35:26.990 - 00:35:38.354, Speaker A: Is precisely the entropy of our original measurement. That's, you know, the good news. We are approximately, the entropy makes sense. This goes to one.
00:35:41.994 - 00:35:43.294, Speaker B: What about this term?
00:35:43.754 - 00:35:57.334, Speaker A: Well, actually, c n rho n c n. So this integral is one. And this goes to zero. So the entropy do the correct thing.
00:35:57.714 - 00:35:58.574, Speaker B: Okay.
00:36:00.834 - 00:36:11.044, Speaker A: So I am done. If I actually, I am able to check the mun converges w two to the limit measurement infinity.
00:36:11.984 - 00:36:12.832, Speaker B: Make sense.
00:36:12.968 - 00:36:57.780, Speaker A: This would be obvious if I had made as much a choice at the beginning and not limit myself in taking the set of measures with bounded continuous density, but with bounded continuous density with bounded support. If I'm living on measures with bounded support. Then, you know, with uniform bounded support, then w to convergence is the same as we convergence, right. How can I enforce also this condition on d when you know, approximately density energy? Well, I mean, I've already said at some point I was approximating a generic density with finite sum of, I mean, linear combination of characteristic functions of compact.
00:36:57.812 - 00:37:01.072, Speaker B: Sets, a finite number of those, a.
00:37:01.088 - 00:37:21.896, Speaker A: Final number of, you know, compass says the union is still bounded. So now, when I'm interpreted within them, there's no reason of, you know, getting a measure function with, you know, unbounded support. I can just, you know, leave on some big bowl make sense. So that's, you know, end of the idea. I mean, of course there are two technical details here and there for what concerns the actual approximation argument.
00:37:21.920 - 00:37:38.452, Speaker B: But, you know, the idea questions on this, okay, Corolla, these are first actually.
00:37:38.508 - 00:37:53.844, Speaker A: Definition and then, so definition.
00:37:57.264 - 00:37:58.004, Speaker B: So.
00:38:03.664 - 00:38:08.944, Speaker A: Let'S say here m, I admit the possibility that m is not, not.
00:38:08.984 - 00:38:14.624, Speaker B: Finite, okay, is CDK infinity.
00:38:14.664 - 00:38:16.324, Speaker A: Of course, this definition, loss probability.
00:38:19.764 - 00:38:22.544, Speaker B: If for any.
00:38:24.404 - 00:38:29.212, Speaker A: Mu zero, mu one in the domain of the entropy, actually, let.
00:38:29.228 - 00:38:31.332, Speaker B: Me see, for any, let me be.
00:38:31.348 - 00:39:06.954, Speaker A: More precise, for any new zero, new one, let me say equal, you know, rho zero rho one m with bounded support. And, and, and the integral of rho I log rho idm is finite. And, and yeah, there is.
00:39:11.474 - 00:39:12.274, Speaker B: Mu t.
00:39:12.394 - 00:39:19.014, Speaker A: At w to Judysi from mu zero to mu one.
00:39:22.914 - 00:39:27.254, Speaker B: Such that the entropy.
00:39:29.834 - 00:39:50.824, Speaker A: Of mu t is less or equal than one minus t. Mu zero plus t times the entropy of mu one minus, what is k over two t one minus t w two third mu zero mu one.
00:39:52.924 - 00:39:53.784, Speaker B: Okay.
00:39:56.844 - 00:39:59.864, Speaker A: Few comments on this definition. First of all.
00:40:01.784 - 00:40:02.472, Speaker B: I did not want.
00:40:02.488 - 00:40:25.736, Speaker A: To write in the domain of the entropy, because I did not define actually the entropy functional as a function on the full space of probability measures if the m is not, is not finite. So I took sort of the safe approach and I say, look, I just consider measure with bounded support such that the density log of the density is integrable. And for this, and for this, I'm.
00:40:25.760 - 00:40:29.562, Speaker B: Asking the existence of some, okay, so.
00:40:29.578 - 00:40:53.434, Speaker A: In particular, if I pick, say, some singular measures, I'm not asking anything about the existence of jurisdictions, okay, this has to do with the, with the stability, because if I impose that condition, then I would not be able to conclude the stability, as you will see in a second, okay, there are compact transitions. So, considering the statement, you know what w two is, that perhaps you don't know what a strategic is. I mean, I've never told you.
00:40:53.594 - 00:40:53.914, Speaker B: So.
00:40:53.954 - 00:41:03.054, Speaker A: Let me tell you that a geodesic kinematic space, a soda kinematic space.
00:41:08.634 - 00:41:09.210, Speaker B: X.
00:41:09.322 - 00:41:53.764, Speaker A: Is a curve gamma. I mean, for me, unless otherwise steady, the curves are always defining zero one, okay? Such that the distance between gamma s and gamma t is always equal to the distance between gamma zero and gamma one multiplied by the symmetric. So this is the say so. I mean, in a moment we speak about speed of curves and length of curves. And this is the same as asking that the, this curve has length exactly equal to the distance between the endpoints and it has constant speed.
00:41:54.424 - 00:41:55.048, Speaker B: Okay?
00:41:55.136 - 00:42:30.354, Speaker A: But if you don't know what length is and what speed is, you know, still this makes sense. And it's, you know, notice that it would be sufficient to ask this inequality for the triangle inequality. Because, because, I mean, if you have zero one t s and you have, you know, the distance between these two guys, the distance between these two guys and distance between these two guys, by the triangle inequality, the sum of these three quantities must be at least the distance between zero and one. So if you have an inequality of this kind every time, you basically are enforcing equality.
00:42:30.894 - 00:42:39.318, Speaker B: Okay, okay. So the theorem that I will just.
00:42:39.366 - 00:42:41.126, Speaker A: Prove in the case of normalized spaces.
00:42:41.150 - 00:42:48.450, Speaker B: But of course does not, does not require that is that this condition is.
00:42:48.482 - 00:42:51.414, Speaker A: Stable under measure gravox convergence.
00:42:51.714 - 00:42:56.090, Speaker B: And the underlying principle is that convexity.
00:42:56.242 - 00:42:58.854, Speaker A: Is an ocean that is stable under gamma convergence.
00:42:59.834 - 00:43:06.890, Speaker B: Here again, lot, Tony, let you know.
00:43:06.962 - 00:43:11.436, Speaker A: X n where mix it converging major.
00:43:11.500 - 00:43:27.068, Speaker B: Gamma razor to x infinity normalized and xn is cdk infinity.
00:43:27.156 - 00:43:28.620, Speaker A: And here, of course, it's important, the.
00:43:28.652 - 00:43:56.080, Speaker B: K is the same for n. For every n in n, then x infinity is CDP infinity. Okay, uh, proof. Well, let's pick.
00:43:56.272 - 00:43:58.152, Speaker A: I need to check whether the cognome.
00:43:58.208 - 00:44:05.420, Speaker B: So pick, you know, mu infinity zero, mu infinity one.
00:44:05.452 - 00:44:07.932, Speaker A: Okay, embed all this space in a.
00:44:07.948 - 00:44:12.556, Speaker B: Common y, a realization of the convergence. Okay?
00:44:12.700 - 00:44:31.684, Speaker A: And think all these places just as, you know, inside, inside this big space y. And, you know, pick, you know, this s in the definition, you know, bounded support, etcetera.
00:44:32.504 - 00:44:33.364, Speaker B: Okay.
00:44:43.584 - 00:44:55.914, Speaker A: Perhaps I'm thinking whether I should make one before a comment. Okay, perhaps let me first make a, make a remark. Let me notice that if.
00:45:01.534 - 00:45:02.634, Speaker B: Let me make this.
00:45:04.374 - 00:45:05.114, Speaker A: If.
00:45:10.694 - 00:45:23.232, Speaker B: M of x is finite, then dw to, you know, so if m.
00:45:23.248 - 00:45:39.848, Speaker A: Is fine, let's say m is one, then I know what derivative entropy is for any, you know, probability measure. I've defined it. And, and I can wonder now whether this w two jurisdiction exists for any couple of measures in the domain of.
00:45:39.856 - 00:45:43.430, Speaker B: The entropy, then make sense what I'm saying.
00:45:43.512 - 00:45:45.334, Speaker A: So not just we bound the support.
00:45:45.714 - 00:45:47.294, Speaker B: Okay, then.
00:45:49.434 - 00:45:56.094, Speaker A: Let me say, and x CD infinity, infinity. Okay, then.
00:45:57.794 - 00:46:00.018, Speaker B: For any mu zero, mu.
00:46:00.066 - 00:46:07.734, Speaker A: One, in the domain of the entropy, there exists mu t as in definition.
00:46:11.494 - 00:46:12.854, Speaker B: So I don't have to force myself.
00:46:12.894 - 00:46:15.582, Speaker A: To work with bounded measures, measures with bounded support.
00:46:15.758 - 00:46:18.114, Speaker B: Okay, and why is that the case?
00:46:18.534 - 00:47:10.754, Speaker A: Because peak two measures in the domain of the entropy and let's approximate them, you know, this as before, this pace of measures with bounded support is dense in energy, right? So we can find, for given two measures, we can find mu zero n w two converging to mu zero and mu one, n w two convergence to mu one with, you know, also the entropies convergence zero and the same for me, one. This we can do, I mean we basically have done this before.
00:47:10.914 - 00:47:13.874, Speaker B: Okay, now for EAch of these for.
00:47:13.914 - 00:47:16.314, Speaker A: Fixed, then I can apply the definitioN.
00:47:16.394 - 00:47:19.546, Speaker B: And therefore there is, there is a.
00:47:19.730 - 00:47:32.454, Speaker A: Sequence of Judiscs, mu tn, w from, you know, mu zero n to nu.
00:47:32.534 - 00:47:36.154, Speaker B: One n make sense, satisfying the assumptions.
00:47:37.214 - 00:47:41.654, Speaker A: So if I'm ABle, so now if I'm able to pass to the limit.
00:47:41.774 - 00:47:45.486, Speaker B: In this, in the, in this, I'm done, right?
00:47:45.550 - 00:48:18.776, Speaker A: So if I, if I prove that these jurisdicts converge, perhaps up to passing to a subsequence, to a limit, to a limit judicial between museum mu one, by the weak lower semi continuity of the entropy, I'm done, right, because, is it clear what I'm saying? I have this INEQualitY I have with the nth, basically t n mu zero n mu one n w zero n w one. Now when I let n go to infinity, by construction of this approximation, this right hand side converges to the correct.
00:48:18.800 - 00:48:21.768, Speaker B: Thing and the left hand side is lower semicontinuous.
00:48:21.816 - 00:48:28.864, Speaker A: So I'm done. So, all is left to prove is that this admits, you know, to subsidize that this form a compact set.
00:48:29.484 - 00:48:31.932, Speaker B: Okay, now for instance, if the space.
00:48:32.028 - 00:48:46.904, Speaker A: Effects were compact itself, this would require, you know, no brainer, because, because the space of probability measures would be weakly compact this. So also we can compact with respect to w two.
00:48:47.644 - 00:48:51.160, Speaker B: Now we have uniformly continuous Fermi of.
00:48:51.192 - 00:48:56.416, Speaker A: Cars with values in the same compact set as colleague tell me that there's.
00:48:56.440 - 00:49:02.784, Speaker B: Some sequence with conversion. Okay, so I really, basically, basically, really.
00:49:02.824 - 00:49:10.924, Speaker A: Only need to find some compactness, you know, statement concerning this. And the compactness statement is here.
00:49:13.384 - 00:49:21.038, Speaker B: And compactness comes from this bound, because.
00:49:21.086 - 00:49:22.514, Speaker A: The point is that.
00:49:24.974 - 00:49:31.678, Speaker B: The observation is that the soup in n and t.
00:49:31.766 - 00:49:37.070, Speaker A: Of the entropy of these measures nu.
00:49:37.102 - 00:49:40.154, Speaker B: T n is finite.
00:49:42.314 - 00:50:12.156, Speaker A: Okay, perhaps here I need, I need to discard a few ends, a few initial ends. But let me say, I mean, I can assume, there's no harm in assuming that not only these entropies converge, but they are uniformly bound. If the first elements of the sequence were better, just, you know, change them as a. Okay, and then, but then you see that this followed by, by this convexity inequality, because this quantity is uniformly bounded.
00:50:12.180 - 00:50:15.500, Speaker B: In n. So if you interpolate with.
00:50:15.532 - 00:50:24.344, Speaker A: It, they remain uniformly bounded in n. And in t, I have w to convergence. So certainly this quantity is also uniformly bounded in n, in t. And therefore, that is true.
00:50:27.244 - 00:50:27.984, Speaker B: Right?
00:50:31.044 - 00:50:52.282, Speaker A: Well, but then, but then, but then what do I do? Well then, basically, now the observation is that, and this is a nice, this fixed probability measure, the set of n fix c. So for every, you know, m, let me say in py and c positive, the set of measures mu.
00:50:52.378 - 00:50:59.734, Speaker B: Such that the entropy with respect to m is less than c is tight.
00:51:02.154 - 00:51:04.984, Speaker A: And therefore, you know, relatively complex, weakly relative.
00:51:07.444 - 00:51:09.504, Speaker B: I leave you this as an exercise.
00:51:10.964 - 00:51:47.144, Speaker A: What's the idea? The idea is that when is it that the entropy, you know, becomes plus infinity? Becomes plus infinity? When the density of mu is too big. I mean, I mean, when the density blows up. Now, so m is a fixed measure. So m, as a measure, is tight, right? So for every epsilon, you know, there is a delta, there is a compact of it. Outside the compact, I have little mass. Okay, now, so outside a certain compact.
00:51:47.184 - 00:51:52.944, Speaker B: Set, m has little mass. Now take a reference measure mu with.
00:51:52.984 - 00:52:01.514, Speaker A: Some bound on the entropy. How much mass that measure mu can ever give to this small complement of the, of the composite?
00:52:02.054 - 00:52:02.710, Speaker B: Not too much.
00:52:02.742 - 00:52:03.134, Speaker A: Otherwise.
00:52:03.174 - 00:52:07.634, Speaker B: Otherwise, it has too high density and makes sense.
00:52:09.134 - 00:52:14.342, Speaker A: So basically, you combine this. So with this, you get the relative.
00:52:14.398 - 00:52:20.774, Speaker B: Compactness in the, for the weak topology of these guys, right?
00:52:20.894 - 00:52:38.698, Speaker A: And so now you are basically done because you extract, you know, a subsequent where these guys convert to a limit measure pro t, rational, let's say. But now w two is lower semi continuous with respect to weak convergence. So, you know, basically you are building, and this is sufficient to build you an interpolating measure.
00:52:38.826 - 00:52:41.694, Speaker B: Okay, make sense?
00:52:42.314 - 00:52:48.918, Speaker A: Okay, now if you know this, then we are done. And then we take a little break.
00:52:49.066 - 00:52:51.354, Speaker B: Um, epic union zero.
00:52:52.014 - 00:53:00.990, Speaker A: So I need to prove, now I have a converging. Well, perhaps, perhaps MLA. So let's do five minutes break now, and we, I prove this statement after the break.
00:53:01.062 - 00:53:04.246, Speaker B: Okay, let's make a small, I don't.
00:53:04.270 - 00:53:05.314, Speaker A: Want to rush it.
00:53:07.894 - 00:53:09.434, Speaker B: Five minutes, not more.
00:54:15.014 - 00:54:36.888, Speaker A: The curve dimensional condition is stable under a measured gravitational convergence. And I will prove this, I mean, just for normalized spaces. But as you can imagine, this normalization has nothing to do just because, you know, I only introduced convergence for this class of spaces. But this statement is, can be generalized in natural ways to, you know, without.
00:54:36.936 - 00:54:39.968, Speaker B: Assuming the reference measure to be finite.
00:54:40.016 - 00:54:50.720, Speaker A: And of course, there are analog statements for whatever k and whatever n you put here, okay? And the idea of the proof is the same and is to basically use the gamma convergence of the underlying entropy function.
00:54:50.792 - 00:54:54.672, Speaker B: So let's prove this statement.
00:54:54.808 - 00:54:59.224, Speaker A: So what is this? I know that for this page, like.
00:54:59.264 - 00:55:02.296, Speaker B: Xenophobia, I always have this, you know.
00:55:02.320 - 00:55:21.844, Speaker A: Vast and judicial connecting measures along with the entropy is convex. And I want to prove that the same is true for the limit. So what do I do? I pick two measures in the limit space, okay. And in fact, you know, given that this remark, I can pick them directly in the domain of the limit entropy.
00:55:23.424 - 00:55:27.252, Speaker B: Okay, and let me say with, you.
00:55:27.268 - 00:55:28.864, Speaker A: Know, with.
00:55:31.444 - 00:55:34.276, Speaker B: Finite second moment, okay, of.
00:55:34.300 - 00:55:53.032, Speaker A: Course this is important because when you entropy, meaning, okay, should I said with finite second moments, because otherwise I cannot, I cannot approximate in varsity time. Okay, so I pick two measures with finite second moment. It is the measure the entropy. And I wonder whether is this judiciary connecting them along which the entropy is convex.
00:55:53.108 - 00:55:54.992, Speaker B: Okay, and what do I do?
00:55:55.088 - 00:56:13.064, Speaker A: Well, basically I do the same thing that I've done here. In the, in the case of a fixed space, I first of all, I find, and I can find by the previous result, I can find the sequences mu n zero, say mu and one. These are converging in w two to.
00:56:13.104 - 00:56:17.004, Speaker B: Mu infinity zero and mu infinity one.
00:56:18.144 - 00:56:20.336, Speaker A: And with the entropy that satisfies the.
00:56:20.360 - 00:56:30.234, Speaker B: Gamma limb supine poly, the limb soup of the entropy of mu n zero.
00:56:30.734 - 00:56:33.006, Speaker A: M n is less or equal than.
00:56:33.030 - 00:56:37.334, Speaker B: The entropy of mu infinity zero with.
00:56:37.334 - 00:56:39.994, Speaker A: Respect to mnp and the same for the output.
00:56:41.054 - 00:56:44.514, Speaker B: Okay, makes sense.
00:56:44.894 - 00:56:50.900, Speaker A: Now, in particular, given that this is finite and the same with the entropy of mu one, for n sufficiently big.
00:56:50.932 - 00:56:55.508, Speaker B: This will have finite entropy, okay, finite.
00:56:55.556 - 00:57:17.224, Speaker A: Entropy in p two of xn. Therefore, by, you know, the definition of okay, I'm erasing this. I will need this then, as I come. But anyway, so by the definition of cdk infinity plus this remark, I know that there exists, you know, mu n.
00:57:17.264 - 00:57:20.248, Speaker B: T, which is a w two judisic.
00:57:20.296 - 00:57:21.768, Speaker A: I mean, n is fixed, is a.
00:57:21.776 - 00:57:27.000, Speaker B: W two julie sigm t from mu.
00:57:27.152 - 00:57:31.560, Speaker A: N zero to mu n one such.
00:57:31.592 - 00:57:34.324, Speaker B: That, well, such that. Let me write it.
00:57:35.544 - 00:57:40.164, Speaker A: Um, but I don't want to write I'm too lazy. So let me add it so mn.
00:57:42.404 - 00:57:47.972, Speaker B: I can use columns mn unt m.
00:57:48.108 - 00:57:55.620, Speaker A: N mu n zero, mu n one entropy with respect to the measure mn.
00:57:55.772 - 00:58:02.796, Speaker B: And that's okay. This is true for every n and every t. Okay.
00:58:02.820 - 00:58:04.504, Speaker A: For every n sufficient big and every.
00:58:05.484 - 00:58:13.504, Speaker B: Okay. Now, as before, when I let n go to infinity, this quantity, you know.
00:58:13.544 - 00:58:36.018, Speaker A: Satisfies the correct inequality. So the limit of this quantity is less or equal than the limit entropy. This also satisfies the correct inequality. This converges because I have w two convergence of the entropies of the measures. W two converges of the measures. So the only thing that is left to do is to prove that perhaps up to possibly pass by subsequence, these.
00:58:36.066 - 00:58:38.306, Speaker B: Curves carve those measures.
00:58:38.450 - 00:59:09.314, Speaker A: They do converge to a limit card, which is adjudic between limit between the limit measures. If I do this, if I prove that there is some weak convergence, then by the gamma line inequality, and that because I would have semi continuity on this left hand side. Okay, this is really a prototypical of gamma convergence. The right hand side converges because I've chosen the, you know, the smart, the recovery sequence and the left hand side. I mean, I talk to a pastor, maybe the sequence is not optimal, but I passed it.
00:59:10.054 - 00:59:10.914, Speaker B: Okay.
00:59:13.574 - 00:59:23.838, Speaker A: Well, first of all, again, as before, I mean, the argument is really the same that we have just done before, but on a sequence of line spaces, if by any chance this space y on which I am embedding anything.
00:59:23.886 - 00:59:27.522, Speaker B: Is compact, I don't sense.
00:59:27.618 - 00:59:30.058, Speaker A: For the compact case, that's all you need to do.
00:59:30.146 - 00:59:30.854, Speaker B: All right.
00:59:32.194 - 00:59:36.294, Speaker A: For the non compact case, I need to prove, need to prove.
00:59:40.714 - 00:59:41.250, Speaker B: Need to.
00:59:41.282 - 00:59:54.004, Speaker A: Prove that, that, you know, the, these, you know, the set new nt for n in n n t in zero one is tight.
00:59:59.144 - 01:00:02.872, Speaker B: Okay, let me prove that this is the case.
01:00:02.928 - 01:00:05.016, Speaker A: Actually not really proof, but, you know.
01:00:05.040 - 01:00:13.496, Speaker B: I use, use for this as before, use the fact that this soup in.
01:00:13.560 - 01:00:46.812, Speaker A: N and t of the entropy with respect to mn nu nt is finite. Previously, it was the entropy with respect to a fixed measure, and I argued that that was sufficient to get the tightness of this. Now, the measure is varying. How can I still get tightness? Well, this measure is varying, but these measures are weakly converged into a limit measure. So this is still a tight friendly. So the argument that I sort of.
01:00:46.828 - 01:00:52.020, Speaker B: Sketched before still apply. You give me epsilon.
01:00:52.052 - 01:00:55.748, Speaker A: I can still find a compact set such that all these measures give mass.
01:00:55.796 - 01:01:00.156, Speaker B: Less than epsilon to the complement order of the component. So if the previous sort of line.
01:01:00.180 - 01:01:03.984, Speaker A: Of thought, you know, convinced you then this should also.
01:01:04.684 - 01:01:08.148, Speaker B: Okay, and how do I conclude out of this?
01:01:08.236 - 01:01:42.264, Speaker A: Well, I conclude because, because now, because now if this is tight, there are various ways of doing that. Let me, let me argue this way. So for every t rational, I mean, that I guess, is the first idea that one can get. There exists, you know, a sequence nk such that, such that mu and KT wiki converges to some mu t. And with the diagonalization, I can find a sequence such that this is true for every t rational, just because these are accountable choices.
01:01:43.524 - 01:01:46.396, Speaker B: Okay, now what it is the, so.
01:01:46.420 - 01:02:06.984, Speaker A: I would like this to be sort of the restriction to rational numbers of a Jud sq. Between, you know, my original measure mu zero and measure mu one. So let's compute, how much is it, the distance between mu t and u s? Well, this is mu and w two is lower semi continuous. With respect to weak convergence, this is.
01:02:07.024 - 01:02:13.764, Speaker B: Less or equal than the limit of w two mu nt new ns.
01:02:16.464 - 01:02:36.232, Speaker A: Nk, I should say. But these are points along a Judisic between dimension. So I know by the definition of Judisc how much this quantity is. This is s minus t times the distance between mu n zero and mu.
01:02:36.288 - 01:02:39.524, Speaker B: N one and nk.
01:02:41.864 - 01:02:48.564, Speaker A: But now when, when k goes to plus infinity, this converges to the distance between mu zero, mu one.
01:02:49.184 - 01:02:55.364, Speaker B: Okay, all right, end of the proof.
01:02:56.424 - 01:03:18.300, Speaker A: See, I mean, now, sorry, I don't know. This is, now, this is really a restriction, you know, to rational soviets. In particular, the map that takes t rational and returns mu t is lipstick. So I can extend by this inequality, so I can extend it to, you know, ellipses above the final rule, zero one. And that is must be a subject, right? And now it's clear that, you know.
01:03:18.332 - 01:03:25.064, Speaker B: The lower semi continuity is there on the edge, right? So end of the.
01:03:27.164 - 01:03:57.854, Speaker A: There'S nothing conceptual, you really just in such as need to put in line, put in line the definitions. I want to make some comments before passing to the next, to the next stage of the series of lectures. So which ones? Okay, so the first of all, first of all, first comment about this convex inequality. This is not asking. So if you look, let's say, let's.
01:03:57.894 - 01:03:59.790, Speaker B: Draw a graph of what it is.
01:03:59.822 - 01:04:10.806, Speaker A: The entropy of what could be the entropy of mu t along a Jody sq. As given by the definition, let's say that we have, this is the entropy of mu zero is the entropy of mu one.
01:04:10.910 - 01:04:11.246, Speaker B: Right.
01:04:11.310 - 01:04:21.446, Speaker A: And what is that that input is telling? Well, the inquiry is telling that basically you should be, you know, the entropy of mutant should be below this line. But that's only saying so it could.
01:04:21.470 - 01:04:22.554, Speaker B: Be something like this.
01:04:23.934 - 01:04:31.034, Speaker A: So in principle I'm not telling that the map that takes t and the return entropy of mu t is convex.
01:04:31.334 - 01:04:33.110, Speaker B: This is completely acceptable.
01:04:33.302 - 01:05:00.374, Speaker A: Then of course you could say, wait a second, but here is, okay, of course I'm drawing the picture for k equals zero. For simplicity, k equals zero. Now this entropy is rising, then it's not convex. I mean, does not respect the inequality. If I instead pick this guy and this guy. And of course the answer is that, okay, maybe there is another w two z connecting these measures. And this measure along with the entropy is bounded from above by the current.
01:05:00.374 - 01:05:12.082, Speaker A: And then one can counter argument. Okay, but why do you put, why do you give that definition rather than asking for actual convexity? And, and that's because if I ask rascal convexity, I don't know whether, whether.
01:05:12.138 - 01:05:14.074, Speaker B: It is stable or not, okay.
01:05:14.934 - 01:05:33.390, Speaker A: Or perhaps proving stability would be hard because you really need, you see, you really need here what it is that we use to prove stability. We use that on the right hand side, we could pass the limit. On the left hand side we are by compactness and we get the lower semicont. And in doing this, it was useful that, you know, we have a fixed.
01:05:33.462 - 01:05:36.934, Speaker B: Right hand side and then, and then.
01:05:36.974 - 01:05:50.896, Speaker A: And then we construct our approximation. If instead you ask for convict inequality to hold even for transport intermediate times, then you are in trouble because you need to have convergence of the entropy even in the intermediate times, which is.
01:05:50.920 - 01:05:53.512, Speaker B: Tricky and typically false.
01:05:53.608 - 01:06:31.254, Speaker A: So things are more complicated. So that's 1st 2nd thing. It is perhaps a good time to mention the following lemma. I want to make some comments about the definition of entropy in general Cdk infinity spaces. And I need, first of all, I need the following lemma. This has been, basically, this has been proved by Sturm in the first of these two acta papers. So say that x is CD infinity.
01:06:31.254 - 01:06:39.634, Speaker A: Then there exists a constant, and say.
01:06:39.674 - 01:06:47.930, Speaker B: Okay, pick a point in x, okay. Then there is a constant c, possibly.
01:06:47.962 - 01:07:18.462, Speaker A: Huge but finite, such that the measure of the ball of radius r centered in x is bounded by constant e to constant r squared for area. So you get a control on how big the measure grows out of the Cdk infinity condition. The control is terrible, is e to the r squared.
01:07:18.598 - 01:07:20.958, Speaker B: Okay, maybe you are you, if you.
01:07:20.966 - 01:07:38.868, Speaker A: Are used to polynomial growth of, you know, the volume of the ball, perhaps you should keep in mind that on the hyperbolic space, like, you know, really the disc, the measure of the ball grows exponentially, not with r square, okay, but with the factor in front that blows up with the dimension.
01:07:38.956 - 01:07:41.772, Speaker B: So it's unsettling, not too strange to this.
01:07:41.948 - 01:07:57.864, Speaker A: And this is optimal because if you take, if you take, you know, r with the euclidean distance and the measure which is like e to the x squared over two dx, this like is CD one infinity.
01:08:00.264 - 01:08:04.884, Speaker B: So I'm going to test this here. Okay, how do you prove this?
01:08:05.184 - 01:08:30.365, Speaker A: Well, you prove this by the same idea. And this idea is the same that leads on the finite, you know, finite end to the bishop gram of inequality. And the idea is this, you pick, I need to, so first of all, I don't, I only need to prove.
01:08:30.389 - 01:08:33.733, Speaker B: This for r bigger than one, because.
01:08:33.773 - 01:08:47.793, Speaker A: For r between zero and one, I mean this is at least one, if I pick this constant sufficiently big, this is always bigger than the ball over is one, right? So I can restimize. So now, now let me pick, what did I do?
01:08:49.573 - 01:08:53.014, Speaker B: Let me pick as mu zero.
01:08:53.834 - 01:09:18.914, Speaker A: I just consider the measure restricted to the ball of radius one half centered in my point, normalized as mu one. I pick the, I pick the measure restricted to the ball of radius r centered in my point.
01:09:21.254 - 01:09:24.822, Speaker B: Okay, now these are two measures with.
01:09:24.838 - 01:09:58.412, Speaker A: Bounded support, clearly finite entropy. Okay, this measure, notice that, okay, perhaps I should say I can always assume that x is in the support of the measure. I mean, otherwise, you know, things are only easier. So these are positive and finite, because I'm always working with measures that are finite on bounded sets. So these are finite number. And now I get the existence, so let me just, okay, now let me work with j. I get the existence of muti such that this convex inequality holds.
01:09:58.412 - 01:10:07.064, Speaker A: Okay, now notice the following. So let, so, so there is, so if mu t. So let you know.
01:10:07.964 - 01:10:15.568, Speaker B: Mu tilde be a w 2d sic. And let me draw a picture and.
01:10:15.576 - 01:10:51.782, Speaker A: Then I make a computation. So my starting measure has support in the ball of radius one half. Let me draw the ball of radius one. And then I have a big ball, you know, with this r, okay, and what does that, does this w, this measure, this graph mu t is interpolating between this normalized measure and this other normalized measure, okay, and what I claim, what I claim is that, I mean, the measure is being, the mass is being basically transported. So what I want to say is that there is some time for which this mu t remains under, in the.
01:10:51.798 - 01:10:56.062, Speaker B: Ball of ridge, in the bowl of Ridges one, okay?
01:10:56.198 - 01:11:25.644, Speaker A: So the claim, and I actually, I can compute this time what it is. So they, so the biggest distance that could possibly ever been between a point in here and a point in here is what is one half plus r that's an upper bound of the distance in the point of the small ball and the point of the big ball. So a measure that is. So if I have a car Judisic.
01:11:27.104 - 01:11:31.232, Speaker B: You know, from a point, you know.
01:11:31.368 - 01:11:42.676, Speaker A: We want to have to a point in the r after time t it will have traveled, you know, the distance between gamma t and gamma naught will be at most t times one half.
01:11:42.700 - 01:11:43.584, Speaker B: Plus r.
01:11:46.164 - 01:12:08.886, Speaker A: So if t is sufficiently small so I want this distance to be bounded by one half so that gamma t remains in the BOlivia one. If we start it from the smallest one and this is true if the smaller than one or this one half divided, you know, one half plus half, it makes sense what I'm saying, no?
01:12:08.950 - 01:12:11.422, Speaker B: So for this t, you know, a.
01:12:11.438 - 01:12:27.254, Speaker A: Judiciary that starts from the small ball and goes in the big ball will remain the ball in the ball of muscle. Okay, now you should know, to continue you should know something about w two that I did not discussed yet.
01:12:27.714 - 01:12:37.266, Speaker B: But perhaps you can believe me if I beg you strongly enough that a.
01:12:37.330 - 01:12:55.480, Speaker A: Judicial dysfunction is made by atoms that are traveling along geodesics on the base space. So what I want to say is that what I want to conclude I will do this with more detail later on for the moment perhaps, believe me.
01:12:55.592 - 01:12:59.304, Speaker B: That because of that the support of.
01:12:59.344 - 01:13:09.484, Speaker A: Mu t will be included in the bold of raise on centered next bar for t less than one half divided one half plus.
01:13:12.824 - 01:13:13.724, Speaker B: Okay.
01:13:16.904 - 01:13:19.028, Speaker A: Does it? At least he can make sense, you.
01:13:19.036 - 01:13:21.412, Speaker B: Know I'm moving the mass, okay, the.
01:13:21.428 - 01:13:40.784, Speaker A: Mass moves along Judesix. That's where I'm asking you to believe me. And therefore, and therefore by this simple computation. Now what's the point here? Well, first of all, I want to notice that I do know how much is the entropy of mu zero because I know what's the density, right?
01:13:41.324 - 01:13:44.724, Speaker B: So the entropy is the integral of.
01:13:44.884 - 01:13:46.724, Speaker A: Log of the density in this case.
01:13:46.804 - 01:13:52.358, Speaker B: Log of that normalizing constant c zero d mu zero.
01:13:52.406 - 01:14:02.070, Speaker A: Basically write d density of mu zero dm. But that's. And so this is a number. So, so this is log, this is.
01:14:02.102 - 01:14:04.158, Speaker B: Log of c zero.
01:14:04.206 - 01:14:18.210, Speaker A: But of course I know who is this here, right? C zero is one over this measure. So this is equal to minus minus log of the measure of b one.
01:14:18.242 - 01:14:18.814, Speaker B: R.
01:14:21.514 - 01:14:23.298, Speaker A: And for the same computation, the.
01:14:23.346 - 01:14:28.234, Speaker B: Entropy of mu one is equal to.
01:14:28.274 - 01:14:50.058, Speaker A: Minus log of the measure of the ball. Okay, so what I conclude, so of course by the CD condition, the entropy of muti is bounded from above by. Okay, I should say one minus t that plus t. This. Let me just actually, well, one minus.
01:14:50.106 - 01:14:55.854, Speaker B: T log m. I mean, this is a number.
01:14:56.314 - 01:15:09.354, Speaker A: I call it alpha whatever. And then I have plus t times, actually minus t times log of m of Br.
01:15:11.294 - 01:15:13.222, Speaker B: And then I have minus k over.
01:15:13.278 - 01:15:19.954, Speaker A: Two t, one minus. And what is the distance between these two measures?
01:15:20.294 - 01:15:27.534, Speaker B: Well, let me. So, so each atom of br must.
01:15:27.574 - 01:15:43.228, Speaker A: Be sent into this ball of radius, one r. So the distance it travels is at most, let's say two r. So, so w two distance squared is like, you know, at most four times r squared. Something that goes with a square.
01:15:43.276 - 01:15:43.916, Speaker B: Right.
01:15:44.100 - 01:15:47.636, Speaker A: The constant is for sure not the best one that you can put, but let's put it there.
01:15:47.780 - 01:15:53.584, Speaker B: Make sense. Okay, now, now I want to make use of this.
01:15:55.524 - 01:16:13.596, Speaker A: And for these, what my argument is that the entropy. So if I call, I don't know, new, the measure restricted to b, one normalized c, whatever, then the entropy, because.
01:16:13.660 - 01:16:18.996, Speaker B: Of this, the entropy of nu t.
01:16:19.180 - 01:16:28.384, Speaker A: You know, will be greater or equal than the entropy of nu if t is smaller than one half divided.
01:16:30.324 - 01:16:34.132, Speaker B: Basically what I'm saying is that this.
01:16:34.148 - 01:16:37.744, Speaker A: Is an instance of a more general statement. If I have a measure which is.
01:16:38.124 - 01:16:42.228, Speaker B: Concentrated on a certain set, its entropy.
01:16:42.396 - 01:16:57.536, Speaker A: Is bigger or equal than the entropy of the reference measure normalized to that set. And this is basically Jensen's in a pool. Okay, because, so let me, let me prove this.
01:16:57.640 - 01:17:09.604, Speaker B: Let's say that mu t is rho m. Rho m restricted to be one, okay?
01:17:09.904 - 01:17:16.152, Speaker A: Then the entropy of mu t is.
01:17:16.248 - 01:17:20.504, Speaker B: The integral of mutual of rho d.
01:17:20.884 - 01:17:46.744, Speaker A: M versus v one. And this is the same as, as integrating with respect to, you know, the. Let me normalize the measure. This is basically new. And then I put one denormalization constant. Why am I doing this? Because now this is a probability measure. U is convex.
01:17:46.744 - 01:17:47.696, Speaker A: U is deadlock z.
01:17:47.720 - 01:17:49.128, Speaker B: Of course, u is convex.
01:17:49.176 - 01:17:56.896, Speaker A: So by Jensen's inequalities is greater or equal. Then I have this factor in front, and then I have u of the.
01:17:56.960 - 01:18:07.324, Speaker B: Integral of rho d tilde. Make sense.
01:18:09.864 - 01:18:17.724, Speaker A: But I know exactly how much this is because c tilde can take it out. The interim is one.
01:18:18.064 - 01:18:18.944, Speaker B: That's the mass.
01:18:19.064 - 01:18:55.080, Speaker A: So that's U of C tilde, right? So I have, I have. So this is equal. Let me make the copy. So this is one over C tilde, and then u of C tilde, U of c is c tilde, log C tilde. So these cancels are. And with, but, so, so, and that's, and that's, you know, and that's by the same, that's the end of the problem. So this guy, this guy is a greater or equal, let me write it here, is greater or equal than the entropy of nu.
01:18:55.080 - 01:19:04.364, Speaker A: Mu is the normalized measure on the unit ball. So the same computation as before tells me that this is minus log of m of b one.
01:19:08.054 - 01:19:16.030, Speaker B: Okay, and I'm done, because now this is fixed, this is fixed, and here I have basically.
01:19:16.182 - 01:19:19.246, Speaker A: An r squared, and now you see, you see, this is the guy that.
01:19:19.270 - 01:19:22.974, Speaker B: I want to control from above, I control mine.
01:19:23.014 - 01:19:25.062, Speaker A: So, minus the log is on the.
01:19:25.078 - 01:19:30.574, Speaker B: Grid, so bring it out and do it, okay.
01:19:34.594 - 01:19:54.066, Speaker A: So this sort of, so the basic idea of the trick is to use CD, so what is that they've done? I use the cdk infinity condition to interpolate between measures of two different boats, concentric, I use the optimal transport argument to understand how the measure behaves and where the support should be, and then Jensen's inequality, and it gives me the.
01:19:54.090 - 01:19:56.994, Speaker B: Conclusion, okay, now.
01:19:59.294 - 01:20:55.172, Speaker A: If I could pick, now I had to pick mu zero, why did I pick mu zero dimension, the ball of raise one after then the delta zero? Because that zero is typically infinite entropy. So it's probably from this point of view, but in the CDK and case, when Renaissance are involved, taking deltas is in some sense allowed. And these, and this allows you to make, you know, a sharper estimate, because when you interpret between mu r, sorry, the ball of radius r and delta, you know that, you know, say after time one half, you are inside the ball of radius one half without, you know, having to add, you know, little extra terms and so on and so forth. So this is a sharp control on the ratio of the volume of a ball of radius r, and the volume of radius r prime with r smaller, and if, you know, I will not give you any detail on this, but basically, if you take the definition of CDKN, apparently sort of trick to contract the balls, what you get is the.
01:20:55.188 - 01:20:58.660, Speaker B: Bishop grammar inequality in the sharp property, okay?
01:20:58.732 - 01:21:27.794, Speaker A: That tells you that the ratio of the volumes is governed by the ratio of the volume in the model space. Of course, they didn't tell you what is CDKN, so you can, you know, but basically it's just a computation where just means that the difficult part is finding the CDK course, but once you get that, the conclusion is easy. Why did I bother you with all this? Because this is interesting for the following purpose, for the following, you know, reason if this occurs.
01:21:30.334 - 01:21:33.782, Speaker B: This implies that if.
01:21:33.798 - 01:22:00.570, Speaker A: I pick, you know, say, let me say c prime, just c plus one. Then the integral of e to the minus c prime distance squared from x bar DM is finite. Okay, up to, you know, basically the measure a priori on a CD can free the space can certainly be plus.
01:22:00.602 - 01:22:03.442, Speaker B: Infinity, but it grows in a controlled way.
01:22:03.498 - 01:22:17.682, Speaker A: It's the most exponential squared. And because of that, if you basically multiply by a gaussian like it becomes a finite measure. And it is interesting for the point of view of entropy, because, because of.
01:22:17.698 - 01:22:34.318, Speaker B: The following remarks, let me first observe.
01:22:34.366 - 01:22:43.862, Speaker A: A change, sort of a change of reference measure formula. Say that you have two measures, m tilde and absolute respect to m. The.
01:22:43.918 - 01:22:47.766, Speaker B: M tilde equal eta m. And let's.
01:22:47.790 - 01:22:57.306, Speaker A: Say that you have a measure mu. Let's say that is also absolutely continuous with respect to m tilde. So in particular, it's also absolutely continuous respect to m. Let's say mu is.
01:22:57.330 - 01:23:00.042, Speaker B: Equal to rho m. And let's say.
01:23:00.058 - 01:23:14.134, Speaker A: I want to compare the entropy of mu with respect to m and with respect to m tilde. Let me just do algebra. So, so the entropy with respect to m Tilde of mu.
01:23:15.114 - 01:23:18.094, Speaker B: Well, this is the integral of what.
01:23:18.594 - 01:23:55.816, Speaker A: Of, I should write, you know, the density of mu with respect to m tilde, log of this density Dm tilde, right. Then of course, let me simplify and let's just compute the density of mu with respect to m tilde. And this is equal to what? This is equal, you know, this mu respect to dm, dm with respect to dm Tilde. So this is equal to the integral of log of. So d mu respect to dm is.
01:23:55.920 - 01:23:56.564, Speaker B: Rho.
01:23:58.744 - 01:24:14.878, Speaker A: Dm and dm t. This is, this, right? Okay, so this is equal, let me continue. This is equal to the integral of log rho d nu minus the integral.
01:24:15.056 - 01:24:20.614, Speaker B: Of log eta eta d mu.
01:24:21.874 - 01:24:24.974, Speaker A: And of course this is the entropy.
01:24:27.074 - 01:24:28.934, Speaker B: With respect to n of mu.
01:24:31.034 - 01:24:42.394, Speaker A: So basically the difference between these two entropies is the intra, with respect to the measure mu, whose entropy we are calculating of log of log of the density of, you know, dimension one measure with respect to.
01:24:43.294 - 01:24:46.302, Speaker B: Okay, why is that interesting?
01:24:46.398 - 01:25:14.906, Speaker A: Well, because, now let me just a technical point, but I see people getting confused about this. So for once I want to make this computation. And so what I, what we just.
01:25:14.970 - 01:25:19.746, Speaker B: Learned from here is that if I.
01:25:19.770 - 01:25:29.930, Speaker A: Define m tilde, I mean, if I, yeah, if I define m tilde to be a normalizing constant e to the minus this c prime distance squared x.
01:25:29.962 - 01:25:34.094, Speaker B: Bar m, then for, I mean, for.
01:25:34.134 - 01:25:41.430, Speaker A: C prime big enough and c normalizing, this is a probability measure now, I.
01:25:41.462 - 01:25:45.790, Speaker B: Apply this formula over here in some.
01:25:45.822 - 01:25:48.446, Speaker A: Sense in reverse terms, and I deduce.
01:25:48.510 - 01:25:51.614, Speaker B: That if everything goes well, in some.
01:25:51.654 - 01:26:05.964, Speaker A: Sense, if all the computations are justified, the entropy mean with respect to the initial measure, which is possibly, you know, infinite mass is the same as the entropy of mu with respect to m tilde.
01:26:06.744 - 01:26:11.280, Speaker B: Okay? And then there is plus the integral.
01:26:11.352 - 01:26:21.960, Speaker A: Of the log of this with respect to mu. And what is the log of this? Well, the log of this is. Well, first of all, I have a.
01:26:21.992 - 01:26:25.292, Speaker B: Log of circuit, which is constant, and.
01:26:25.308 - 01:26:31.284, Speaker A: Then I have minus c prime, the integral of distance squared upon x bar.
01:26:31.324 - 01:26:35.744, Speaker B: D. Okay.
01:26:38.004 - 01:26:54.604, Speaker A: So what it is that we can, so anytime, anytime we have a space whose reference measure m is positive plus infinity. But after normalizing with the gaussian kind of factor, becomes a finite finite mass. I can define.
01:26:57.264 - 01:26:58.720, Speaker B: The entropy like this.
01:26:58.792 - 01:27:37.020, Speaker A: For every mu in p two, x. You remember, I guess, either for the first or the second lecture, I mentioned that when the reference measure is plus infinity, you need to be careful about defining the entropy because you could have this, you know, minus infinity going on. So it could be, in principle, not well defined. And there is a, there is a. So basically, you don't want the, your measure mu to be too much distributed with respect to how much mass the reference measure is giving to, you know, big sets, basically. Now, what I'm telling here is that if the mass of the lesson measure, you know, does not grow more than.
01:27:37.052 - 01:27:42.052, Speaker B: This exponentially squared away, okay, then it.
01:27:42.068 - 01:27:47.822, Speaker A: Is sufficient for the reference measure mu to be in p two. So a finite second moment to have a well defined entropy. You see?
01:27:47.838 - 01:27:49.910, Speaker B: Now this is what defined if nu.
01:27:49.942 - 01:27:51.766, Speaker A: Is in p two, this guy is.
01:27:51.790 - 01:27:54.502, Speaker B: Well defined because until there's probability, this.
01:27:54.518 - 01:27:56.154, Speaker A: Is a number and this is finite.
01:27:57.174 - 01:27:58.074, Speaker B: Okay?
01:27:58.494 - 01:28:28.894, Speaker A: So these, I mean, I can take this as definition of entropy in, you know, whenever this is a finite measure, and, and it is clearly w two lower semiconductor, not weakly over semiconductor. I don't know whether it's weakly or semi, because there is a minus in front of here, and I have the sort of the wrong, the wrong over semiconductor on this side. But if I'm weakly converging, plus my second moment to converge, this guy, which has the wrong sign, is converging, and this guy is lower symmetry.
01:28:31.474 - 01:28:32.374, Speaker B: Okay.
01:28:36.474 - 01:28:43.914, Speaker A: Is there anything else that I want to mention? Okay, just a couple, a couple of more things. And then for today, for today, I stop.
01:28:51.094 - 01:28:54.438, Speaker B: First of all, maybe, I don't know.
01:28:54.446 - 01:29:00.158, Speaker A: If it is clear from the sketch of the proof that I gave, but.
01:29:00.286 - 01:29:07.242, Speaker B: The importance of insisting on having measures.
01:29:07.298 - 01:29:42.344, Speaker A: With finite entropy, when asking for the existence of the geodesic interpolating this is to get this stability of the cdk infinity condition. Because the fantasies of the entropy enters into play into getting the compactness of the approximating jurisdiction. If you don't have this, you don't have compactness. And I don't know how to actually, at some point you have to, if you want to stability, you have to exhibit existence of limits. This is such as true. And if you have to exhibit something, you know, compactness typically, you know, should be there.
01:29:42.424 - 01:29:44.968, Speaker B: Okay, as heuristic.
01:29:45.136 - 01:29:47.604, Speaker A: So, so that's one thing.
01:29:48.664 - 01:29:49.404, Speaker B: And.
01:29:51.104 - 01:29:58.324, Speaker A: Okay, let me perhaps just present, just present where we go in two weeks from now.
01:30:01.704 - 01:30:09.784, Speaker B: So what we approved today is, well, this theory, ck infinity, zk infinity is.
01:30:09.824 - 01:30:22.884, Speaker A: Stable, which is a consequence, as I mentioned on the, can be seen as a consequence of the stability of convexity, or k convexity with respect to gamma convexity.
01:30:23.204 - 01:30:27.644, Speaker B: Okay, now here is an interesting point about convex functions.
01:30:27.684 - 01:30:30.540, Speaker A: Let's think, let's think. Let's think about, you know, on a.
01:30:30.572 - 01:30:32.052, Speaker B: D, if you have a sequence of.
01:30:32.068 - 01:30:49.364, Speaker A: Convex function that's converging, that is converging gamma point wise, uniformly, whatever you want, to limit convex function, to limit function. But then, first of all, the limit function is complex. And second thing, the, the sub differentials, if you know what that is, of this problem, of this approximately complex functions.
01:30:49.404 - 01:30:52.380, Speaker B: Do converge at the limits of differential, okay?
01:30:52.412 - 01:30:59.620, Speaker A: So in some sense, once you have convexity and, you know, the stability of convexity for free, you get the stability.
01:30:59.692 - 01:31:04.164, Speaker B: Of a first order condition associated with convexity, okay?
01:31:04.324 - 01:31:18.084, Speaker A: And that will be, you know, our, you know, our screwdriver to enter and make actual analysis on these CD community spaces. And the guiding principle is the following. I will write a sentence.
01:31:19.864 - 01:31:25.324, Speaker B: So the heat flow is the.
01:31:27.304 - 01:31:29.124, Speaker A: W two gradient flow.
01:31:35.544 - 01:31:37.164, Speaker B: Of the relative entropy.
01:31:40.104 - 01:31:58.154, Speaker A: Well, the gradient flow of the relative entropy with respect to the versatile w to distance. This is, you know, important and very.
01:31:58.194 - 01:32:02.574, Speaker B: Important statement by Jordan in their letter.
01:32:06.794 - 01:32:10.294, Speaker A: Otto, they made this observation on RD.
01:32:11.064 - 01:32:11.884, Speaker B: Um.
01:32:14.024 - 01:32:40.242, Speaker A: And of course, if you. So, so maybe you don't know what the gradient flow is, but as of now, you know what is the heat flow. And you should know by now that this is an important concept. You know what the entropy is and this is an important concept. And this guy, you know what this is. And this is an important concept. So perhaps it's good idea to understand what this is because it provides a.
01:32:40.258 - 01:32:42.890, Speaker B: Link between these three concepts, okay?
01:32:43.042 - 01:33:02.374, Speaker A: And even if Jordan, JKO, as they called, usually idea were born in RD. They provide us an angle to maybe, possibly, if you understand what this is, maybe we can define the heat flow as a gradient flow of the angle.
01:33:03.744 - 01:33:07.072, Speaker B: You know, Dostoevilani gave us, you know.
01:33:07.088 - 01:33:23.368, Speaker A: This CDC condition tells you, you know, if you combine entropy and distance in such a way that things are convex, then you are in an interesting class of space. Giorda Quine tell us. Look, if you take the gradient flow of this function with respect to this distance, then you obtain an interesting flow, which is the heat flow.
01:33:23.496 - 01:33:26.584, Speaker B: Well then perhaps why not start studying.
01:33:26.624 - 01:34:07.874, Speaker A: The heat flow from this matrix perspective as gradient flow of the entropy. And, you know, and as we will see, the fact that a, this heat flow is well defined, this class of CD captivity space and b, this table is, you know, the cornerstone of the stability theory for differential operators in this CD cathedral. Okay, so in two weeks from now. So the next lecture is in two weeks because next week there is a conference and the 10 October is holiday Monday. So on Friday in two weeks, we will start discussing what are already a gradient flow in a matrix set and we try to make rigorous this.
01:34:09.014 - 01:34:13.234, Speaker B: Okay, that's it. Sure. Please.
01:34:21.894 - 01:34:40.244, Speaker A: Sober to the, the other RCD for sure. I think auto CD actually we can maybe, I mean, they don't know what this is. Maybe we can discuss this in a second after, after the lecture. I think. I think CD can feel sufficient and I think I have an idea of the proof. Let's comment on this in a second.
01:34:41.584 - 01:34:44.216, Speaker B: Okay? Okay.
01:34:44.240 - 01:34:44.624, Speaker A: Goodbye to.
