00:00:00.480 - 00:01:03.418, Speaker A: We were essentially at this place when we stopped. We. In fact, I was going to go over the Wikipedia thing, which we'll do at the end of the lecture today. So I was trying, going to give the proof of Schoenberg's theorem. So, essentially, I had sort of introduced you to metric cones, and then I had talked about how the universal metric, which is the l infinity, can take any finite metric space and embed into l infinity. In fact, I forgot to mention that, you know, there's a simple embedding of l one into l infinity, just as a warm up, but I put the slide here so you can have a look at it. Um, so, so now what we are going to do is try to prove Schoenberg's theorem, which is relatively easy.
00:01:03.418 - 00:01:53.664, Speaker A: It's not that hard to prove. And I just want to point out that this is a good place to mention this reference. There is this handbook of geometric constraint systems principle, which is on the website that which is linked from the website that Christophe put on the chat. So it's not inexpensive. So what I will try to do is extract chapters from it and put it maybe in the piazza so that people can access them. So the theorem is actually in the these Schoenberg's theorem, etcetera. Can you see Schoenberg's theorem here? I can try to make it a little bit bigger.
00:01:53.664 - 00:02:45.914, Speaker A: So, Schoenberg's theorem, et cetera, are discussed in the very first chapter of this book. The proof is not given, but here's the theorem. I also have linked in that in this site, notes that have been prepared by my graduate students when I lectured in my class at various times. So we will actually use some of that as well. So I will put all the references there. So anyway, this is Schoenberg's theorem. A real symmetric matrix with zero, diagonal and positive entries is euclidean distance matrix if and only if it's negative, semi definite on the subspace of all vectors orthogonal to the zero one vector.
00:02:45.914 - 00:03:23.480, Speaker A: One thing I have to mention is that we keep forgetting to say that when we say, at least in this, often people go back and forth, but often when the euclidean distance matrix is mentioned, the distances are squared. Distances, right? So distance squared. And people sometimes use distances, sometimes square distances. Sometimes it doesn't matter, for certain things it doesn't matter. But, you know, you should be able to tell generally from the context. But if you're not, just ask. Okay, so this is the theorem.
00:03:23.480 - 00:03:49.746, Speaker A: And the theorem can be stated slightly differently and is stated often slightly differently. And so I just want to give you the other way in, which is stated. This, by the way, is from Liberty's book. It's called not book, I think it's like an article or something. It's open in the Wiley open library. It's called something like six gems of distance geometry, or something like that. Right.
00:03:49.746 - 00:04:42.444, Speaker A: So where was I here. So this formulation is actually, um, often used. So you can see that there's slight, slight difference. And let's like take this formulation. It basically says that a real symmetric matrix is the euclidean distance matrix of n points, which means it's the pairwise distances between point n and point j is given as dij. And if and only if, the matrix that is given in this way. So now the ijth entry is given by just taking the d one I th entry of this original matrix, and d one jth entry squares and subtract the dij squared entry.
00:04:42.444 - 00:05:30.744, Speaker A: And that matrix, which is called g, is, happens to be positive, semi definite of rank r. Okay, so this is if and only if condition. And what the proof does is usually very simple. It just massages the euclidean distance matrix into the form of the inner product matrix of a set of points. So that's all the proof is. So the inner product matrix is also called the gram matrix. So essentially you take two sets of point, I mean a set of, if you've given a set of points, basically you can take, think of each point as a vector, and then basically take the inner product of the point I with the point j.
00:05:30.744 - 00:05:57.288, Speaker A: And that will be the I jth entry of this gram matrix. It's a well known matrix. So basically, and you can show very quickly that every gram matrix, sorry, every PSD matrix, every positive semi definite matrix in matrix with non negative eigenvalues, that symmetric is also happens to be gram. And that proof is very simple. And I have it here.
00:05:57.416 - 00:06:00.448, Speaker B: So if you ask a question before we move past that slide.
00:06:00.576 - 00:06:01.284, Speaker A: Yeah.
00:06:01.784 - 00:06:04.152, Speaker B: So if we go back to the slide with the two theorems.
00:06:04.208 - 00:06:05.644, Speaker A: Yeah, this one. Yeah.
00:06:06.744 - 00:06:23.234, Speaker B: So in the top theorem. Yeah, like let's say D equals two, yet the rank is, let's say one. What does that mean? If the rank is less than, strictly less than D plus one, say it's a rank one matrix and D equals two.
00:06:25.414 - 00:06:31.674, Speaker A: If it's a rank one matrix and D equals two just means that the points all lie on a line.
00:06:34.014 - 00:06:35.834, Speaker B: Okay, and how do I see that?
00:06:36.294 - 00:06:38.364, Speaker A: I haven't proved theorem yet.
00:06:39.184 - 00:06:48.404, Speaker B: Okay. Will there be an interpretation for when the rank is strictly less than D plus one?
00:06:50.384 - 00:07:06.844, Speaker A: Yes. So this basically just says that whichever dimension, think of it as the affine span of the set of points. Yeah. The dimension of the affine span of the set of points is exactly the rank of the matrix.
00:07:08.044 - 00:07:08.904, Speaker B: Okay.
00:07:09.724 - 00:07:11.580, Speaker A: Okay. Or plus one, whatever.
00:07:11.652 - 00:07:13.164, Speaker B: Yep, that makes sense. Thanks.
00:07:13.284 - 00:08:27.684, Speaker A: Yeah. So, so, okay, so we are going to go to the proof, and before I go to the proof, I just wanted to say this minor thing, which is sort of assumed whenever this matrix, this theorem is stated, is that every PSD matrix is automatically a gram matrix. A gram matrix means this inner product matrix. And so essentially the entire proof, just simply, as I said, massages the inner product matrix of the set of points into the distance matrix of that set of points. So let me quickly go to that sort of simple known fact that every PSD matrix is such a gram matrix. Very quickly. So if you took matrix to have eigenvalues along the diagonal and zero everywhere else, and take the matrix having the eigenvector corresponding to that particular eigenvalue as the jth column, so then basically you can take the matrix that's given this way.
00:08:27.684 - 00:09:26.914, Speaker A: Yeah, gamma y transpose. And so now, because gamma is all positive, all non negative. If gamma, if it so turns out that the eigenvalues in the matrix are non negative, the matrix is positive, semi definite, then this is a non negative. This lambda here is non negative. So you can basically take the square root and this factors into Y square root of gamma, square root of gamma Y transpose. And now you can call Y Times square root of gamma as x. And so that gives you your factorization of the original matrix whose eigenvalues were non negative, that you started out with the PSD matrix as the inner product matrix of two of a set of points x one through x.
00:09:26.914 - 00:10:09.854, Speaker A: So that's basically just, and it also, by the way, this proof also tells you if you're given a PSD matrix, how to get the corresponding set of points whose gram matrix the given PSD matrix is. So this actually gives you the realization. So x is the set of points whose inner product matrix is the given PSD gram matrix. PSD matrix, inner product matrix and Gram matrix are names of the same thing. They're used to say the same thing. So this is saying basically that any PSD matrix happens to be a gram matrix. So with that in mind, we can go to the proof here.
00:10:09.854 - 00:10:58.924, Speaker A: And I'm going to just, I'm sort of introducing you to some of the common things that I will be using as references. One of these is these notes that, as I said, my students wrote up with significant revisions from me from previous lecturing. So essentially, the proof is simple. Um, it's just, as I said, the idea of the proof that you're massaging a euclidean distance matrix. I mean, you're. You're massaging a gram matrix into a euclidean distance matrix. So we start, if euclidean realization exists.
00:10:58.924 - 00:11:23.628, Speaker A: Which means that you have some set of points p one through pn. We call them x one through xn before. But you have to get used to this, um, changing of notation every once in a while. So. So p one through pn for some d. And then, without loss of generality, we can assume initially, you know, d is less than or equal to n. Because you can always embed in euclidean space.
00:11:23.628 - 00:11:51.844, Speaker A: You can always embed n points in n dimensions. So because the affine span of n points cannot be of dimension higher than n. So then take the points, take p to be just this. And then the gram matrix. Then its gram matrix would just be p transpose p. And now what we're going to do is. And here we're making the statement here, which I just proved a moment ago.
00:11:51.844 - 00:12:11.084, Speaker A: The gram matrix is n by n matrix. And it. So one direction is easy. You know, if you take p transpose p. Then it's positive, semi definite. And the converse direction I just proved a moment ago. So the gram matrix is an n by n matrix and exists.
00:12:11.084 - 00:12:33.996, Speaker A: P exists. Which means for a given matrix, it can be factored in this way as p transpose p. If and only if gamma is positive, semi definite. One direction is easy. The other direction I just did. So the next, um, we will show the relationship between this, uh. The euclidean distance matrix.
00:12:33.996 - 00:12:45.796, Speaker A: And the gram matrix of this set of points. That's it. That the entire proof is just the. The relationship between. Setting up the relationship between the gram matrix. And the, uh. And.
00:12:45.796 - 00:13:13.594, Speaker A: Sorry. This gram matrix and the euclidean distance matrix. So you take the set of points and you assume that the centroid of the points is the origin. Which you can always do. So we are just going to define this thing. P transpose the inner products as. Because we know that the centroid is the origin.
00:13:13.594 - 00:13:31.074, Speaker A: We know that the sum of all the inner products. Basically, um, is going to be zero. The sum over one of the. For any. So sum over I of Gi is going to be zero. And sum over j of g I j is also going to be zero. G I j is the inner product between the point I and the point j.
00:13:31.074 - 00:14:47.694, Speaker A: So and now we. Because we had the capital delta, the distances are small, delta so if starting out with delta ij, denoted as PI minus pj squared, which is now the entries of the distance matrix, these are the entries of the gram matrix. Gi is just the inner product between PI and pj. So basically you have the distance matrix can be written, I mean, sorry, the distance delta ij, which is the squared, can simply expand it out to this, which is in fact just Gi II plus G, JJ minus two gij. And now you can just manipulate. So if you take the sum over I of these guys, you know, you notice we haven't yet come to this yet using this yet, but you simply get this expression, and then you take over the j's, you take over the I's, you take over the j's, you get the same expression once over the I's and once over the j's. And then now you can think of, and you can also take the sum over I and j, and you'll get this expression.
00:14:47.694 - 00:16:10.894, Speaker A: And so now you can write the grayman entry, the Gram matrix entry I, j in terms of the distance matrix entries. And this just turns out to be, if you wanted to write it out as main in matrix form, this just turns out to be gamma capital gamma, which has the entries gij, turns out to be this, which is identity matrix, all ones matrix. And so then we basically get that delta. So now delta, which we know is the Gram matrix and which we know is positive, semi definite, is now therefore tells us that gamma is. Well, if you just wanted to prove the second version of the theorem, we are already done. Where was the slides? So for the second version of the theorem we have, all it meant is to say that all it wanted, all we needed to do is say that this matrix, which we just turns out to be the gram matrix because its gram is PSD of rank r. So the proof basically showed that the gram matrix was equal to this thing.
00:16:10.894 - 00:17:07.566, Speaker A: If you wanted to show this thing, all you have to do is show that this item is negative semidefinite on this subspace, provided g is positive semi definite, and that's all there is. So that essentially proves Schoenberg's theorem. So the general structure is that, you know, you say that delta is the, is a euclidean distance matrix if and only if. Well, we haven't mentioned the rank yet, but we have just done the PSD part of it. So it has a euclidean realization if and only if the corresponding, the grammat. This gram matrix is symmetric PSD. No, the gamma that was produced in this way is symmetric PSD.
00:17:07.566 - 00:18:12.972, Speaker A: And this is exactly the same as saying that delta itself is negative semi definite, okay. Of rank b plus one, I mean, on that subspace which we mentioned in the, in the theorem. So that's essentially the proof of Schoenberg's theorem. As you can see, it's not difficult. And as I said, the main idea is to massage the Gramion into the euclidean distance matrix. Okay? So the, so if you go back to this slide. So we talked about having sort of a correspondence between two theorems, one of which is Schoenberg's theorem, which we just showed, where there's the negative semi definiteness part, or in other words, the positive semi definiteness of the Gram corresponding Gram matrix, which we wrote this in terms of.
00:18:12.972 - 00:19:20.414, Speaker A: And the corresponding thing here is a statement of determinants of certain sub matrices of this k le manga matrix, which is obtained from the distance matrix. Now, and the rank condition was supposed to then become equivalent to setting some of these determinants to be actually equal to zero. This is non negativity and this is equality to zero. So in fact, we can forget this. The reason I didn't emphasize the rank when we were talking about the proof, the rank just follows the matrix as we go. So if you start out with points in a certain dimension, the rank of the gram matrix is exactly the same as that. So if all the points lie on a certain subspace of a certain dimension, then the gram matrix will have exactly the same dimension, because it's simply the inner product matrix.
00:19:20.414 - 00:20:12.754, Speaker A: And we also talked about why you can see that from the proof that I gave as well, the number of non zero eigenvalues. Sorry, just go here for a second. So in gamma, the num, I mean, sorry. In this thing, what is this thing? I don't know, the thing in the middle, lambda lambda. So lambda has some number of non negative eigenvalues. And so y and y transpose will have basically essentially rank equal to the number of non negative eigenvalues, which is also the dimension of the set of points whose granian it is. Okay, so, okay, so that's how the dimension comes in.
00:20:12.754 - 00:21:18.064, Speaker A: But if you go here, I wanted to say that we can ignore this part, this whole rank part and this altogether, and we actually get any equivalence between the two statements. So essentially this is the same as saying delta is an EDM if and only if it is negative semidefinite, blah blah blah. Or the expression that we got in terms of this, which is the gram matrix, is positive semidefinite, if and only if all of these determinants are non negative. So you can forget the rank business for now and simply show this, this equivalence. So let's see how we might do such a thing. So. So that would be in a way that would, that would show, that's another way of just showing the Caylee manga, the first part of the Caylee manga theorem.
00:21:18.064 - 00:22:02.794, Speaker A: So let me go to the whiteboard. Okay, so one way to think so, because if something is an idiom, we just showed that. Hang on one sec. I have to be able to move this so that I don't. There we go. Okay, so we have a set of points whose realization. So we have Delta is an EDM, is an EDM.
00:22:02.794 - 00:22:55.964, Speaker A: And let's just say dij, one less than or equal to I, less than j, less than or equal to n is an EDM. And we have this negative semi definite. But we know that this is the same as saying, I mean, by definition that there are these points that exist p one through pn, such that PJ, as I said, we sometimes do square. So I'll just call this delta ij or not. So these guys, and I'm ignoring what dimension they're living in. They're living in some euclidean space and without loss, this is, that dimension is less than or equal to n. So we can just say that.
00:22:55.964 - 00:23:46.274, Speaker A: So if this is true, then this is true. Then these, you know, the real, you know, those points exist. So those points exist. So essentially that means that, you know, let's think of these points. Okay? So if I could, I can basically take any simplex. So I'm just going to call this simplex whatever, any simplex here. And it's volume.
00:23:46.274 - 00:24:32.184, Speaker A: So, because this realization exists. So there are set of points which are embedded with these distances. So these are the pisces. Okay? So these are the PIs. So the simplex, the volume of the simplex, which is now given by that Cayley Menger determinant, I'll just call it. So if this is a k simplex, then it's given by the CMK volume absolute value of this. Because these are points.
00:24:32.184 - 00:25:59.022, Speaker A: This simplex volume is some volume, and volumes are non negative. Okay? Now you may ask me. So. So that gives you one direction to go the other direction to say that if there is a set of points such that all of the simplex volumes are non negative, then in fact, sorry, if I have a matrix such that all of these determinants are non negative, then in fact, there exists such a set of points. You can do that. You can actually find that realization by just the non negativity of these, by just starting out with three points, let's say, and you can, you know, the corresponding Kali Menger volume for three points, the triangle area being non negative, just corresponds to the triangle inequality. And so that means that the triangle for three points, the triangle can, you can find three points and place them with those corresponding distances in a triangle, because that determinant is non negative.
00:25:59.022 - 00:26:40.044, Speaker A: Hence, that's the equivalent of saying the triangle inequality holds. And so you can place the points, then you can add another point and use the triangle inequality again because that's non negative. Then now you know that. So this is, I'm basically giving Kaylee Menger's proof very quickly. So basically, you know that again, for these three points. So maybe I use a slightly different color. So you placed this with no problem.
00:26:40.044 - 00:27:13.372, Speaker A: So let's call these guys p one, p two, p three, p four. And because of the triangle inequality between these three guys and this, this. So you know there is a. So if this is the distance, the only problem is you're not quite sure that. So you know, there's the triangle that you can place between p one, p three and p two. Again, because that is not negative. And you know that, okay, this is the same distance.
00:27:13.372 - 00:28:24.904, Speaker A: So you know there is another triangle here. The only problem is you don't quite know at this point whether, if you, once you have placed these triangles, whether this point and this point will actually coincide with your p one and p three, you know there is such a triangle because the triangle inequality holds. And you know there is such a triangle, but you just don't know that these will coincide here. Now, remember that, no, I mean, we are not restricting any dimension whatsoever. So you can place these points, p one, you can increase the dimension by one and place these points wherever you want in the third dimension and do this. Now, the reason why this is, in fact, you know, there exists such a simplex in three dimensions is because the CMK volume for the four points also is non negative. So you know that there is such a simplex in three dimensions that will, you can achieve this as a simplex in three dimensions.
00:28:24.904 - 00:29:00.124, Speaker A: So, and you proceed this. So this is basically by induction, you can prove that one. You can add basically one point at a time. And you know that all the volumes, the necessary volume, you'll add dimension also one, by increasing the dimension by one each time. And looking at all the volumes being non negative means that there is such a realization of the set of points. If all these simplex simplex volumes are non negative. That's a very rough sketch of the proof.
00:29:00.124 - 00:30:17.966, Speaker A: You can actually, if you go to the gems, the six, let's see this one here. No, that's not it. So if you go to this, the gems and distance geometry, he actually has a complete axiomatic proof of this. Um, of, um. Where is it? Yeah, I think it's here. Yeah. So, um, axiomatic proof, I mean, Menger did this with just the metric axioms, and, um, he has the proof of this, but essentially the, uh, I, I basically gave you, it's a pretty, it's not easy to read because it's done in an axiomatic way.
00:30:17.966 - 00:31:28.688, Speaker A: But the idea is what I tried to give just now, why is it not sharing? For some reason, it doesn't want to show my whiteboard. What could be going on. Do you see my whiteboard? Yes. So did you see, did you see the other thing I showed you a moment ago? No, just the whiteboard all the time. Oh, you've only been seeing the whiteboard throughout? Yes. Oh, okay, let's see, what did I try to do? So now you see this thing? Yes, yes. Okay, so this is from six gems and distance geometry, or whatever that liberty open Wiley publication.
00:31:28.688 - 00:32:42.706, Speaker A: And it's an. And he gives a complete sort of axiomatic proof of what I just said with the angles and so on and so forth. And the idea is clear. Is the idea clear to you, what I just talked about, with how you can show the converse direction, how you can use the non negativity of each one of these matrices, which correspond to these simplex volumes, and by induction, build a realization of these euclidean realization of these points. Is that clear? Any questions? Am I not hearing what people are saying? Or nobody's talking or can you hear me?
00:32:42.850 - 00:33:04.468, Speaker C: I can hear you, Mira. Okay, I have to say, I'm not totally following the connection between the Caylee Menger determinants and the reason why you can put these points in position. But maybe I just need to think about that.
00:33:04.636 - 00:34:10.204, Speaker A: Okay. Yeah, it's, if you actually follow Menger's proof, it's kind of, it's not easy to read, although the basic ideas, idea is very, very simple. So, first of all, you need to. The first, the basic thing you need to know is that if a determinant is non negative, let's say CMK is non negative, then you can actually. So basically, all we're trying to show is that if this is non negative, you can find such a simplex, and for three points, you can see that this, this being non negative essentially translates to just those three distances satisfying the triangle inequality. That's all. So it's basically a discriminant.
00:34:13.184 - 00:34:21.068, Speaker C: Okay, I'll think about that. Are the D's, there's two D's that you have, you have, you have the delta and the D. Are those the same?
00:34:21.196 - 00:34:22.604, Speaker A: They are the same, yeah.
00:34:22.764 - 00:34:24.148, Speaker C: Aha. Okay.
00:34:24.276 - 00:34:49.450, Speaker A: Sorry about that. So this is, that came about because of the. Let's see. So we started out with D, but then when I started using those notes, the notes were using Delta. And in a way, Delta is better because we're calling this matrix capital Delta anyway, that we've been doing throughout.
00:34:49.642 - 00:34:51.106, Speaker C: Right. Okay, thanks.
00:34:51.250 - 00:36:10.580, Speaker A: Yeah. So this is basically the relationship between the two ways of thinking about distance matrices. Okay, so what we can do now, I mean, I can spend a little more time on it, maybe next time, because I think we need to be very clear about this for what we're going to do next. So I can spend a little more time on this, maybe next week. So for now. Okay, so how about Schoenberg's proof? I mean, proof of Schoenberg's theorem, was that. Okay, just manipulation, really? Yes.
00:36:10.580 - 00:36:38.242, Speaker A: Okay, so this one connection to Kaylee Minger. I can just do better next time. Okay, so before we go, I just wanted to, just a suggestion, maybe we have to, like my colleague said, we have to think about it a bit. So go over the proof again and you know. Yeah, yeah. We can mention it maybe next time if we need further explanation or. Okay, sounds good.
00:36:38.242 - 00:38:10.326, Speaker A: I'll ask you whether you want me to go over it one more time. So I guess what I will do now is move to the next topic, which is how many of these Cayley menga conditions? So let's go to the next page. So we have a bunch of Klemanger conditions, and we are, we can talk about the equalities for a second. Okay, so when we talk about the simplex volume, the Klemanger conditions, the simplex volume being equal to zero. So, for example, if I say that the Kalimanga condition for four points, this is equal to zero, essentially means that you're saying that the volume of the simplex given by these four points is zero, which means that they lie on an affine span. The affine span of the points is of dimension less than or equal to two, because if it had been three, then you wouldn't, the volume wouldn't be zero. So the affine span is less than or equal to two.
00:38:10.326 - 00:39:52.824, Speaker A: Now. So basically what this is saying is that if you add a bunch of, add these kliminga conditions for k less than or equal to d. So if you, the simplician volume being non negative, simply says that, you know, those points actually exist in real space, and this thing being equal to zero is actually putting a restriction on the dimension of their affine span. So this condition, essentially, if you put this condition, it restricts the dimension of the realization of the set of points. So now my question is, how many of these conditions are required? So another way to say it is, if I assert a certain number of these scaly manga conditions, equalities. When I say conditions, I generally mean the equality and not the inequality. So if I assert a certain number of these, then does a real, by asserting a certain number of these, let's say of k less than or equal to d, does that automatically imply a realization in d dimensions? So, or do I have to assert all of them? So you might remember the very first question that I asked about the number of these metric inequalities that we actually need to assert.
00:39:52.824 - 00:41:31.936, Speaker A: These are not inequalities. These are equalities. But my question is, given that a certain subset of CMK equal to zero conditions hold, okay, for k less than or equal to d, is it sufficient, is this, given that they hold, are these sufficient for existence of a realization in RD? So realization existence in RD. Okay, so if I. So, so that's the question. What do you think the answer to that is? Proper subset. So in general, the way this theorem is stated, you know, all of these conditions, every one of the submatrices of a certain size of k less than or equal to d has to be equal to zero for this realization to exist in a certain dimension.
00:41:31.936 - 00:42:12.134, Speaker A: But my question now is, is it sufficient to have a proper subset of them be satisfied so that we are guaranteed that just from that proper subset being satisfied, you can find a realization with just those. Now those, a proper subset may only involve a subset of the distances, in which case, I'm simply asking whether you can assign distances to the other ones which are not involved in these proper subset, such that the realization is in RD. So that's the question. What do you think?
00:42:13.074 - 00:42:18.620, Speaker D: Yes. Need to be rigid. Subsets need to be rigid.
00:42:18.802 - 00:42:39.528, Speaker A: I see. Uh huh. Okay, good. So what you're saying is that if you, the set of distances involved in the subset is a set of pairs, because every distance is between a pair, and you're saying that that collection of pairs forms a rigid graph. Is that what you're saying?
00:42:39.696 - 00:42:40.444, Speaker D: Yes.
00:42:40.784 - 00:42:58.204, Speaker A: Okay. Okay. So that's. So let me just go to another slide. So this is Faye, right?
00:42:58.284 - 00:43:00.304, Speaker D: Yes. Yeah.
00:43:01.884 - 00:43:20.202, Speaker A: Good. I'm starting to recognize people's voices. That's good. Okay, so Faye is saying, I don't know why doing this, but. Okay. Phase. Saying that if the pair.
00:43:20.202 - 00:44:09.374, Speaker A: The set of pairs. Distances, pairs, whatever, involved, or, I don't know, involve is not a night word that occur. That occur in the subset of Caylee minger conditions. Forms. Rigid graph. How do you prove this?
00:44:12.474 - 00:44:15.294, Speaker D: So, in RD, right?
00:44:15.914 - 00:44:30.994, Speaker A: Yeah, rigid graph. In ard. So we had the Kaylee manga conditions. Let's see, the Klemanga conditions were d dimensional ones. Yeah.
00:44:31.414 - 00:44:38.974, Speaker C: All right, and do you mean generically, Richard? Like, you're not talking about. You have an assigned.
00:44:39.014 - 00:44:47.870, Speaker A: No, this is not generic litigation. You have a particular set of distances, right? Yeah. Yeah.
00:44:47.902 - 00:44:49.630, Speaker C: Okay, that's true. Okay.
00:44:49.822 - 00:45:16.254, Speaker A: Yeah. So you take that particular set of distances, and you take the graph without those distances. He says that's rigid. Is that a reasonable statement? Am I interpreting your statement correctly, Faye?
00:45:16.634 - 00:45:17.414, Speaker D: Yes.
00:45:17.934 - 00:46:05.084, Speaker A: Okay. So I think what he's saying is that this set of distances forms, you can say a graph together with distances on the edges, right, of the graph, the given set of distances that occur. And that thing has a locally unique realization or something. So this thing has locally unique realization. Okay, why? Why do you think this is true? Maybe you can give an example as a start.
00:46:08.184 - 00:46:12.188, Speaker D: Well, you can think about, like, four points with two triangles, right?
00:46:12.336 - 00:46:13.064, Speaker A: Yeah.
00:46:14.084 - 00:46:21.144, Speaker D: So the two triangle shares one, one edge. So. So there might be two realization, but.
00:46:21.564 - 00:46:22.304, Speaker A: But.
00:46:22.684 - 00:46:27.824, Speaker D: But both of them are in r two instead of three reals.
00:46:28.644 - 00:46:29.004, Speaker B: So.
00:46:29.044 - 00:46:35.384, Speaker D: So the CMK three is zero for three. 4k equals three.
00:46:36.164 - 00:46:36.740, Speaker A: Okay.
00:46:36.812 - 00:46:38.244, Speaker D: But. Yeah.
00:46:39.584 - 00:47:17.128, Speaker A: Okay. So let's start with something a little bit simpler. Let's build up to this. So, let's consider the set of Klimanga conditions that correspond to the following. The graph, which is what kind of familiar to you, which is also called zero extension. So if you have, for example. So we're looking at Klimanga conditions of less than or equal to D.
00:47:17.128 - 00:48:03.920, Speaker A: So CMK less than or equal to D. And these are the ones that have been set to zero, and we have a subset of them, right? So. And I'm going to just for the moment, because I can't draw. So let's say d is two. Okay? So we have to be a little careful here, whether it's d or d plus one. Okay? So if we have the Klemenga condition for four points to be set to zero, then we know that that simplex volume has disappeared. So essentially, we know is zero.
00:48:03.920 - 00:49:00.384, Speaker A: So we basically know that these four points lie on a, in two reals. So the only thing that you know, so this distance is a very special distance. That's what it means in order for this to actually lie on the plane. So now for the next point, essentially, in order to. So basically, if you know that this simplex, any one of these simplices, it really doesn't matter. So take three of these points and this one. If that Kleymanger condition is zero, then we know that this point can also be placed in the same affine span of these four points.
00:49:00.384 - 00:50:29.474, Speaker A: And similarly, you can take any one of these triangles, and the next point, any one of these triangles, you could have started with the original triangle. It doesn't matter. And you can place this point here. And because we know that these guys all fall in the same two space, their affine span is of dimension two. And this simplex that I have just drawn is also of zero volume, because I have asserted it, then we, which means that this also falls in the affine span. So basically, every point, as long as we keep that particular subset CMK, which corresponds to these, for these collections of four points, as long as all of them vanish, then essentially we know that this lies in. So basically, instead of having n choose four, if there are n points, then picking n choose four, there are n choose four of these Klemenger determinants for k less than or equal to two for d.
00:50:29.474 - 00:51:32.624, Speaker A: Basically, remember, we add these extra columns and stuff like that. D is the dimension. It's easier to always think about the dimension rather than the actual size of this matrix. So as long as we have these, instead of these n, choose four, we only have order n of these Kalimanger conditions that we have asserted to be equal to zero, which still guarantees that this realization exists. Now, by which we mean, we know there exists a way to fill out the matrix by all these other distances, such that the resulting matrix is still of the appropriate rank, or corresponds to realization that lies in a certain dimension. So we only needed this many of them, even though there are so many, and choose four of these conditions. And it also is talking about which distances we have specified.
00:51:32.624 - 00:52:29.128, Speaker A: This kind of a graph is called a delatoration graph, and it's what would be zero extension in one dimension larger, one dimension higher than the dimension d that you're thinking of. For those of you who know what zero extensions are, what that just means is that, remember that. Also notice that I could have added a point here. For example, I could have added a point that isn't necessarily, necessarily adjacent to three points that are connected by a triangle. I mean, it could be any three points here. I mean, I so happen to draw them in that way. But this could have been any three points.
00:52:29.128 - 00:53:24.588, Speaker A: Say, let's say that point and that point and that point. You know, there is a simplex there. We have already shown that all of these other points, the previous set of n minus one points, lie on a d space. That means that there is some distance here. And, you know, and so essentially there is a simplex here and that simplex volume for that corresponding CMD. If we make that equal to zero, then essentially that says that this is true. So in any case, you have only order of n of these assertions that you need to make in order to ensure that there is some extension by extra distances that still gives you a realization in two dimensions.
00:53:24.588 - 00:54:16.982, Speaker A: So essentially we have a, and these graphs are called dlateration graphs, and people may know them as zero extensions in one dimension higher, and they happen to be rigid in one dimension higher. Right? So these are rigid, minimally rigid. Minimally rigid. And we, I think Tony has already talked about this. So these are minimally rigid. So if here it's d, d two, they're minimally rigid in three dimensions. So that's one example.
00:54:16.982 - 00:55:23.584, Speaker A: So now we see the how graphs come into this. So that's one example of how you can take a matrix where only some distances are given, and these distances correspond to the distances that occur in the subset of, subset of Kalimanga conditions that we talked about. And then we can fill the rest, which is essentially comes from finding a realization. Once you find a realization, you get all the other distances. And because the whole realization lies in a certain dimension, that means that you have filled it out in such a way that the rank does not increase. The rank is still the same. So whatever, I mean, the rank is whatever Klemanga conditions, the size of the Kelly manga conditions subset that you had.
00:55:23.584 - 00:56:39.616, Speaker A: So this, you can see, you can see hints of where the metroid, metroid that corresponds to having these distances somehow being dependent on these distances once these klimanga conditions are asserted. So that comes in as well. I think we have crossed our time limit here. I'm just going to quickly go back to the slides for a second and just go back here to this slide. So we, you know, we started out with distance geometry. We had some results on metrics and norms. We now have shown how graphs sort of come into everything and graphs and hint of matroids, although that was just a bare hint.
00:56:39.616 - 00:57:39.854, Speaker A: We of course, talked quite a bit on and off about the complexity, which is the number of checks we need to make. It's not order n to the fourth, but say order nice. Right in the beginning, we had this lower bound on complexity, where for metric cones, we actually needed to make order n cube checks. So some of that. And then now we'll talk next time about what we're doing here, which is the realization here of given a distance matrix to find a realization, or a partial distance matrix to find a realization in a given dimension. Also can be thought about as solving a system of polynomial equations, because obviously the distances are polynomials. And how that leads to some questions, whether common solution exists to a set of polynomials.
00:57:39.854 - 00:58:34.494, Speaker A: That's saying that the one polynomial is in the ideal generated by them. If you're looking over the complexes and how you can change that, I mean, modify that question to a question about whether there's a real solution. And that brings the idea of Hilbert's nol Stellenzatz and positive and Stellenza. So, hopefully that's what we'll do next week, so that we'll be able to at least walk through all of these little bubbles that we have here in the context of euclidean distance matrices. Realizations. Realization of euclidean distance matrices. Okay, so before we go, I just very briefly, I think my student William Sims, actually shared this with you.
00:58:34.494 - 00:59:11.930, Speaker A: This is the Wikipedia page drafts. They are in Wikipedia drafts. The colored pages are the ones that hopefully we will prepare in this class. These gray pages already exist. And I mean, they can, of course, be modified and improved and so on and so forth. So I'm hoping that as we proceed, each of you will sort of take on some of these pages could be more than one person per page, because I don't think we have as many pages, people who are in the class right now at least, and improve it. So the drafts are already there.
00:59:11.930 - 00:59:45.184, Speaker A: They were already worked on by some students from my previous time that I gave the class. And William will help you if you find it difficult, if you haven't accessed Wikipedia before, et cetera, you can just contact William and see whether it should be very, very easy. It's meant for, you know, crowd crowdsourcing. So it should be very easy for you to make changes, etcetera. But. So William is here. He's on the chat, so you can, he's one of the participants, and you can talk to him.
00:59:45.184 - 00:59:54.684, Speaker A: I will open up the piazza forum today, and hopefully everything will be a lot smoother starting from next week. Okay, thank you.
