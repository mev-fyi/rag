00:00:00.680 - 00:00:07.154, Speaker A: Thank you, Mateus. Let me share my screen and we can go ahead and get started. Is this visible to everyone?
00:00:07.894 - 00:00:08.702, Speaker B: Yes.
00:00:08.878 - 00:00:47.204, Speaker A: Okay, great. So thank you, Mateus, for the introduction. Thank you, the OECD group, for their great presentation. It's my great pleasure to share with you the work we did with the bank of Canada project on business closures in the time of COVID Also, a big thank you to Sohail, Thibault, and Tom for their help and guidance throughout this project. Certainly, they made their impact and helped us out a lot. So I'd like to start with just a brief outline of what we plan to cover. I want to start with a little bit of motivation into why we care about measuring business entry and exit and how this leads into the research objectives that we had for these three weeks.
00:00:47.204 - 00:01:29.134, Speaker A: Then Artur and Philip will tell you about the methodology we used and the actual results we were able to find over the course of this project. And Graham will close with some implications for policy and sort of where we plan to go with this project in the future. And, of course, we'll have time for questions at the end. So how can we measure business entries and exits? And why is this important? Well, so here you can see a couple of different figures. The first is this black line, which measures GDP growth in Canada relative to December of 2019. And there's a very sharp drop around March and April of 2020, of course, when the COVID crisis hit. And that drop is mirrored by the proposals, which is sort of measure of business entries.
00:01:29.134 - 00:01:37.362, Speaker A: But what's particularly interesting is that the number of bankruptcies over this period also dropped. And it's not immediately clear why this.
00:01:37.378 - 00:01:38.394, Speaker C: Would be the case.
00:01:38.554 - 00:02:34.216, Speaker A: So, for example, you could imagine that businesses are voluntarily exiting when the COVID crisis occurs, sort of as an indication of them perceiving, being unable to participate or become solvent in the future. But also, there's this idea of zombie business. You can see this little, cute little figure we have at the bottom, and it's the idea that these businesses are solvent, but they're not actually participating in the economy to the fullest extent. So clearly, this way of thinking about business entry and exits through proposals and bankruptcies isn't really sufficient for what we want to capture. Now, there's a more existing data that tries to solve this problem, and these are proposed by Statcan. So these are a couple of measures of the number of opening businesses, closing businesses in Canada, as well as just a constant index of the number of active businesses and continuing. Now, these measures sort of try to do exactly what we sought to do in this project.
00:02:34.216 - 00:03:26.744, Speaker A: But there's two main issues with them. The first is that they're infrequent because they're based off of administrative tax data. And finally, these measures can actually be very delayed on the order of months. So when we think about trying to understand, for example, how a lockdown in eastern Ontario for the past week has affected the rate of business closures or the fraction of business that are temporarily closed as a result of this lockdown, it's actually very difficult to conclude anything from this data. Our idea in this project is not really new. It's been common in recent years to try to use online web scraping methods for real time economic measurement. The OECD group mentioned this one project, which is the weekly tracker in which they use the Google search data on different frequencies of term and terminologies or terms to determine how active the or to determine sort of a real time measure of GDP in a particular country.
00:03:26.744 - 00:04:22.446, Speaker A: And this works very well. So this gives you an idea of how GDP changes between official announcements. There's a paper, for instance, that uses images from Google Street View and machine learning methods to try and actually estimate the socio political characteristics of a particular region in the United States. And finally, there's this billion prices project, which uses online retailers to scrapes billions of billions of prices from online retailers to compute real time inflation indices. So there's really no reason to believe that we can't do the same for business entry and exit. The challenge then becomes what is the data we use, how do we get it, and how do we design a system in a way that allows us to get this real time index? So the first question is, what is the actual data that we use? We looked at a couple of different options. The first and the one we'll sort of be discussing for most of the remainder of the presentation is Google places.
00:04:22.446 - 00:05:20.724, Speaker A: So they have this wonderful claim on their landing page, which is that we can count on accurate real time location information, which at the end of the day is exactly what we want in this project. But moreover, there's a list of many, many places they have active users, and this is commonly used by applications in search of doing similar projects. We also briefly looked at Yelp, which provides a similar service and more of an extension, but still something we considered is actually looking at news articles. So for instance, in the Ottawa Citizen, perhaps there's an article that comes up when one or some business is open in a particular area. So if we could detect that, this would give us an idea of entries. Our broad research objectives are to mimic these real time macroeconomic measurements through online data. So we want to create a system that can scrape a list of businesses that are active in one particular snapshot in time.
00:05:20.724 - 00:06:13.574, Speaker A: And if we were to look at several snapshots, well, we could have this panel data set in which we understand how businesses are sort of participating in the economy over time. And the idea is that, of course, this could be very useful for policymakers. So, for example, in eastern Ontario, I believe Monday of this week, they announced a lockdown. So we have a snapshot of all of the businesses, 43 different types, which my coworkers will tell you a little bit about later in eastern Ontario, from about a week ago. So if we were to take another snapshot today, then we'd be able to see in real time what happened to. To these businesses as a result of the lockdown. And this allows you to think about, like, regression discontinuity designs, and moreover, sort of actually think about how this lockdown is affecting economic activity in a particular area.
00:06:13.574 - 00:07:11.384, Speaker A: Another interesting implication of these results would be to, for example, we can devise indexes of business entry and exit, not only for all of Canada or for a particular region, but also by sector. So you could think about how different policies have affected the retail sector, or the restaurant sector, or the entertainment sector heterogeneously. So this, of course, would be very nice to have to give a little outline about what will follow. We believe our work can be used to create these real time automated data sets that capture business activity across region and across different sectors. And this can be useful for the assessment of different policy interventions, sort of akin to what the OECD group has mentioned. And finally, no one likes zombies. So we hope that our methodology can be used to identify not only where are the zombie firms, but sort of how can we think about the emergence of zombie firms as a result of policy actions.
00:07:11.384 - 00:07:14.704, Speaker A: Great. So I'd like to hand it over to Artur next.
00:07:16.084 - 00:08:24.114, Speaker C: Cool, thanks. Daniel. Can I have the next slide, please? Okay, so let me start with diving deep into what actual data set is available for us in the Google API places, and hopefully convince you that this is the place to start the research with, as it is quite rich data set that has quite a lot of interesting information that we can use for the project. And beside the very basic information, such as the location of the business, its name, it provides us what we kind of want, which is the business status, whether it's open, permanently closed, or temporarily closed. On top of that, it adds further information, such as the opening hours, which we can use as an indication how well the business is currently performing, for example, during COVID restrictions. So whether they have restricted the amount of operating hours and hence impacting the economic output. And on top of that, we have further information, such as the user ratings.
00:08:24.114 - 00:09:06.434, Speaker C: So this is where people like me and you can put the ratings on the actual place in real time. And we do have access to not only the ratings, but also the number of the ratings. So we can track this over time as well, which can be quite a nice way of indicating how well the business is doing in terms of the interaction with the customers. And we can see specific trends, whether the interaction is going down or up. And this can be actually an indicator whether the business can potentially be failing or shutting down. So these are all quite interesting information. And let me now go into the types of the business in the next slide.
00:09:06.434 - 00:10:03.720, Speaker C: So what's interesting is that Google is actually looking at classification of each business in their own way. So they do have their own way to classify the type of the business as seen in the list. So there are some usual stuff, such as bars, parks or gyms that were quite relevant to us. But because this list has been created by Google for Google purposes, you will find that it's not complete. So, for example, if we want to find manufacturing businesses, this might be much harder using this data set. But what's also interesting is that each place doesn't have to necessarily have just one type of business attributed to it. So, for example, places that are nightclubs are often also seen as bars.
00:10:03.720 - 00:11:30.074, Speaker C: And I think already looking at this provides quite interesting insight on the correlation between closures of different businesses. Since if we are seeing a place that is both nightclub and a bar closing down, then we'll see that not only a nightclub has closed down, but also a related business of a bar. Okay, let me go to the next slide then. So, to really scrape the data and get the data from Google, we need to provide a location so it's pointing on the map, and then a radius around this point. And then natural way for us to actually define how we do this is through looking at the postal codes, as this quite nicely incorporates the information about the demographic data. So the population density, as well as enables us to do quite detailed analysis between different places or regions that are under particular Covid restrictions, and see the impact of such restrictions as compared to other regions that are not following the same restrictions. So for us, in this methodology, the main difficulty was to actually define how do we perform the searches to get the data.
00:11:30.074 - 00:13:09.134, Speaker C: So because we need to cover the entire Canada or the places that we are interested in, in circles that are defining the scope of our Google query, we are limited in terms of on the upper hand, Google is returning only 60 places per query, so our circle has to be quite small. We cannot just say, give me the results for entire Canada and download the big dataset. So we need to design them small enough to actually cover less than 60 businesses. And on the other hand, we want to have them as big as possible, since each circle that we are creating generates costs, as this is a query that we need to make to Google. Okay, let's go next. So we are using this approach as a initial place for our data analysis pipeline where we are taking the FSA data, which is the postcode data from census, and additional information about demographic, and we are enriching it with the initial search that we are getting from the Google API. So we have designed an adaptive algorithm where we are trying to create those circles in an adaptive way, trying to learn what is the correct the size of a circle for each region or for each place, so that we can minimize the number of queries and hence the cost.
00:13:09.134 - 00:14:28.394, Speaker C: And then this comes into the entire pipeline that can run iteratively and periodically where we are trying to create a real time series, real data time series. And the underlying part of it is the unique business id. So this is how we are identifying each of the blades and then we can actually perform checks whether the place is still open, whether has closed, or whether there are new business ids that are popping up, which indicates the entry of the place of new business. And business updates can be done. Sorry, can we go back for a second? Business updates can be done in multiple ways, so there are more costly full searches where we are getting quite detailed information about the place, or they can be possibly done in a much cheaper, shorter queries where we are just checking for the status of the business. And finally, once we have gathered this data set over this time series, we can perform further analysis to actually try to answer the main questions of this project. Okay, next slide please.
00:14:28.394 - 00:15:33.918, Speaker C: So, as a proof of cancer that, you know, this is not only a theory, but we are able to detect businesses that are opening or closing. And we have looked at the East Ontario area, which is categorized by the Cape coastal code. And we did two scrapes of the data on April 12 and April 15, where we particularly focus on cafes and nightclubs as an example of the businesses. So we are seeing that the number of places for the cafes has actually increased by three between the two periods, from 947 to 950, whereas for the nine clubs, it has decreased by two. However, this is not a full picture. As if you are looking at the aggregate number. This is composed by six new entries of the cafes and three disappearances, whereas for the nightclubs, there were no new nightclubs opening in this time period, whereas to have been closed down.
00:15:33.918 - 00:16:37.934, Speaker C: So now, this is the important part where we are validating whether this data is trustworthy and whether it makes sense. And we've seen that actually we are quite good in terms of capturing the exits of the businesses as the ones that have been marked as permanently closed. We were able to verify that this is indeed the case. However, in terms of the new business entries, this is a bit more difficult, as when we called the new businesses, they actually have been operational already in the past. So this wasn't the date of the entry of the business. So this is really a place where we can enrich and do certain validation, whether our conclusions or whether the data that's being provided to us by Google makes sense. And this is where we are taking other data sources, such as Yelp, to see whether the data agrees with each other.
00:16:37.934 - 00:17:27.602, Speaker C: But we also can do more typical machine learning, scraping of the social pages, such as Facebook or Instagram, with the name of the places, and to see if indeed those places can be active or have been posting information prior to the dates to kind of enhance and verify the results from. From our data set. Okay, let me go next to the maps to just show you how this actually looks like. So this is an example of a map that we are getting. And you can see that there's a lot of businesses that are aggregated in one specific region. And the distribution of those platos, platos, platos, Platos. Plates, Plato.
00:17:27.602 - 00:17:28.414, Speaker C: Splatos.
00:17:34.034 - 00:17:41.690, Speaker D: I think Google is trying to stop your presentation, too. Yeah, there you go.
00:17:41.882 - 00:18:32.474, Speaker C: Sorry about that. Yes, indeed, I am criticizing Google too much. So I think currently we do not have enough data, actually, in terms of closures, in terms of how heterogeneous these things are. But what's nice about using the postal codes is that actually in Canada, the second character is able to divide different regions in urban areas. So this kind of helps us already to see what are different impacts of the policies or the closures on. On different areas. Okay, so let me quickly just zoom in into the most populated region in terms of the cafes.
00:18:32.474 - 00:19:43.010, Speaker C: And again, as you can see, there is quite a lot of heterogeneity across different regions where there is quite a lot of businesses in a single little location, this could be a shopping mall and so on. So we are expecting there will be a lot of correlation between the business types and locations and closures or openings. Okay, so let me now jump into the heat maps. So what we are actually seeing from the data set. So here we are looking at the eastern Ontario as an example and this has actually entered Covid restrictions during recent time, I think last week. So during the workshop, so we could see real time the effect of such measures. And you can see that the amount of businesses, fractional businesses that are closed in that region is quite high as compared to the next region.
00:19:43.010 - 00:21:27.088, Speaker C: If I can show the next month of British Columbia, which is much lower number of closures in the same time period. And this is because they have not been affected by strict Covid restrictions. Okay, so let me quickly talk about cost because this is the main consideration for us as the data sets that Google provides is quite granular and enables us to do quite a lot of analysis, but at the same time to extract it we need to do a lot of queries and each query actually has an associated cost. So I think 1000 queries is in terms of like $2 or something like this. So if we are looking at almost 3 million queries to scrape the entire Canada and 2 million of it being the non real areas, you can see that the cost of such scraping is becoming non trivial and especially if we have to repeat this analysis quite frequently over time to get the time series. This is something that we need to tackle in a careful way and consider in a sensible way. So let me go to the next slide and say that there are ways on how we can tackle this and one way is for example the free version of the Google places API query where we can actually find just the closure of the place so we can track whether the business is still open.
00:21:27.088 - 00:21:53.614, Speaker C: We cannot retrieve any other information about it, but we can see whether the business is permanently closed and has been removed from the database. So this is quite quick and cheap way of doing so for us. Another thing is we can incorporate further data sets such as Yelp or social media scrapings from Twitter or Facebook.
00:21:55.994 - 00:21:56.330, Speaker A: That.
00:21:56.362 - 00:23:03.144, Speaker C: Can also be quite helpful in terms of enriching the database. There are this usual stuff such as just random sampling and trying to estimate. We do not need to have the precise number to the dot on the businesses, but we can do some more aggregation and some general trends searching as well as we can look into further partnership with Google in terms of providing better terms and conditions, in terms of how to use their service. Okay, so let's go next. So let me just summarize what we have been able to do during the workshop. So we were able to do to collect 150,000 businesses, and we were focusing on the two regions, eastern Ontario and British Columbia, which covers around 300 fsas. And we have then translated to try to unify the.
00:23:03.144 - 00:23:32.644, Speaker C: The data that we are getting from Google into more standard ways. So, for example, we have classified each business type by the NAICS code, as well as we have prepared this automated and adaptive framework for gathering the data and preparing the time series. And this has enabled us to gather quite rich data sets that my colleagues will now further analyze.
00:23:36.084 - 00:23:36.684, Speaker E: Great.
00:23:36.804 - 00:25:21.564, Speaker F: Thanks, Arthur. So, as Arthur was saying, we've sort of shown that we've kind of got proof of concept and that we've got a methodology that appears to work. We can scrape lots of data. We have a sort of reasonable indication of business entry and exit, albeit with some additional queuing or. But we're also interested in seeing what else can be done with the data now that we have it. So not just our index of entry and exit, but also what other kind of analysis can we glean or what kind of interesting stories can be told with this data? So, a couple of things that we had in mind were things like, you know, how can we combine some of this data with economic data, such as from the census? Can we see differences in characteristics by sector, or by rural versus urban? Or can we look at how things like some of the more interesting aspects from the Google data, such as user ratings, correlates with any of the economic or business open share of businesses that are open data. But before we do all that, the key thing that we wanted to, to make sure and key thing that we had to do first was to sort of do a bit of QA about quality assurance on the data? So, in the next slide, the first thing that we wanted to check was when we were actually looking at businesses by sector, how is it that they're actually being flagged by Google? What this graph shows is that there was actually quite a strong correlation in terms of businesses that were self identifying as nightclubs, were also self identifying as being other businesses.
00:25:21.564 - 00:26:03.060, Speaker F: So around 90% of businesses that said they were nightclubs also said they were bars. Just under 75% also said they were restaurants. And then you can see that there's also some weaker correlations with various other sectors. Meal takeaway, cafes, stores, even a gym and a spa. There's another example in the next slides where this looked at a similar thing for restaurants. And again, the correlation wasn't quite as extreme here, but you can see that there's still some sizable correlation between businesses who are restaurants self identifying in other various banners. So something that we have to be aware of when we're doing any analysis on this.
00:26:03.060 - 00:26:57.184, Speaker F: It doesn't mean that the data is useless, but we have to make sure that if we're ever doing any sectoral analysis or we're coming to any conclusions, that we are fairly confident that we are talking about a specific sector or a specific type of industry. One easy way to do that is to, on the next slide, simply look at businesses that only have one flag. So, for example, a significant amount of business is only self identified as just one thing, and therefore you have a reasonable degree of accuracy in what you actually think it is. But there were still quite a lot of businesses who maybe had more than one flag. Sometimes you could dig in and it was a bit more evident. So, for example, there was a businesses that identified as a zoo and a cafe. We're pretty sure that is a zoo and it has a cafe, but obviously there are other examples where that's not as clear.
00:26:57.184 - 00:28:26.032, Speaker F: So again, nothing that invalidates what we have, but just something that we have to be aware of when we're doing any analysis on it. The other thing that we thought was a good idea, and which we've got on the next slide as well, was to check again the sort of bias of our data. We did hypothesize from the start that because we are scraping data from Google Maps and the data relies on businesses putting the data up there themselves, obviously we're going to get a sort of bias towards businesses who have, you know, customers, customer footfall. For them, having their business on Google Maps is quite important, whereas, so, for example, restaurants, cafes, et cetera, whereas, you know, accountants aren't necessarily as incentivized, perhaps. So one way that we wanted to sort of check this bias was, as Artur said, trying to take good goals classifications, reclassify them under the north american system naics, and then compare our distribution of sectors compared to more official data. So what the graph at the top shows is for British Columbia. And this was the orange bars are the distribution by sector that we scraped from Google places compared to the blue bars, which is the distribution of sector from a more official business register.
00:28:26.032 - 00:29:21.874, Speaker F: So from the provincial government to have a business register and a distribution. And as you can see, there's quite a lot of difference between our distributions. Rather unsurprisingly, the Google maps data picks up a lot of wholesale trade, retail trades, transportation, also arts and entertainment, accommodation and food sectors. So obviously a lot of businesses who rely on customers and footfall who are probably more likely to broadcast themselves on Google places, and we are. We're really not picking up the full distribution of other types of industries. So again, this doesn't completely invalidate the usefulness of this data. It is still real time data on certain sectors of the economy, but we just have to be really careful not making any grandiose claims or just being aware of the limitations of the caveats to any analysis we do on this kind of data.
00:29:21.874 - 00:30:22.264, Speaker F: However, having said that, we did want to see what kind of interesting stories it did tell. So the next slide simply just shows all the postcodes across the different KNV postal areas that we scraped. The y axis shows the percentage of businesses that were closed at the time of scraping. And the broad story here is that whilst there were not insignificant amount of places that didn't have any business closures, the vast majority of postcodes did have a share of businesses that were closed. The vast majority of them were between zero and 10%. And you had a bit of a right hand tail where you had some more extreme examples with the highest, in our case reaching almost 30%. One thing I would caveat though with is that some of those more extreme shares were in areas that had relatively low numbers of businesses and hence it didn't take many closures for those shares to become quite high.
00:30:22.264 - 00:31:10.454, Speaker F: But generally it's relatively stable. It's not too wild, which was slightly encouraging on the next slide as well. We did actually because we have been scraping by postal code. It was quite useful because it allowed us to match up our data set with published economic data. A really good one for us was the census from 2016. So obviously slightly lagged, but some of the key messages and the distribution of things like income and unemployment and census in theory, hopefully shouldn't have changed that much across postcodes in a couple of years. So we were quite interested to see how things like the share of businesses closed in a postcode correlated or not, with different economic indicators.
00:31:10.454 - 00:32:10.774, Speaker F: One of the ones we looked at was unemployment, and although it doesn't look like a very strong relationship, there is actually a very weak positive relationship between unemployment and share of business closed. It is statistically significant, albeit, as I say, the magnitude wasn't super strong, but that was quite interesting. There's maybe some evidence that would support some of the theories that those areas in the economy who historically have higher rates of unemployment and thence maybe, you know, slightly worse off or, you know, higher levels of deprivation may have been marginally more exposed or more, or hit hardest by Covid. There might be some evidence there, albeit, you know, low sample size, etc. I don't want to make any too strong conclusions at the moment. We also looked at income as well. And again, the relationship wasn't super strong, but it was, again, positive.
00:32:10.774 - 00:33:26.678, Speaker F: And again, it was statistically significant. And this one is quite interesting because this would imply that areas who had higher levels of median income in 2015 also have a higher share of businesses closed. And when you dig into this bit a wee bit more, it does appear to be more of an urban rural dividend. And again, I think this plays into some of the biases that we've had with our data, in the sense that some of those areas who have slightly higher shares of businesses closed have actually been quite heavily urban areas, been downtown areas, possibly areas, particularly things like cafes and restaurants, who maybe have struggled more as more people have worked from home, less commuters, et cetera. So it's quite an interesting story. Again, can't jump to any strong conclusions on this, but, you know, it's quite interesting to see that there's this relationship and what it may or may not be explaining. And then a final thing that we thought was quite interesting was the relationship between user ratings on Google Maps and the share of businesses closed, or whether a business was flagging as being temporarily closed, or whether it was still operating there.
00:33:26.678 - 00:34:36.484, Speaker F: When you look at all industries, but also by specific sector, there is some statistically significant evidence that, on average, businesses that were flagging themselves as temporarily closed had worse user ratings than those who were still open. And that's obviously quite interesting. And that would maybe support the idea that some of these firms who did close were maybe not exactly providing the best service to start with. And one of the more interesting implications for this, and it goes back to that zombie firm research question we had. If we are worried that because of the big fiscal policy supports that have happened, because of the big monetary policy support that's happened, that may be some firms who in normal times would have closed, but because of these huge waves of fiscal support or monetary support, have actually stayed open. If. If that's true, and eventually those zombie firms do eventually close, we wouldn't necessarily expect this relationship and this graph to change over time.
00:34:36.484 - 00:35:04.304, Speaker F: If it does, then that might be some weak evidence that something has changed, and it might help us with some evidence towards this sort of zombie firm question. It's not perfect, but we thought that was quite interesting. So that's just a brief example of some of the sort of analysis we've done with the data. I'm going to pass on to my colleague Phil, who's going to talk a little bit about another source of data. We looked at Yelp and then do some concluding summary remarks.
00:35:06.444 - 00:35:49.364, Speaker E: Thank you, Graham. Yes. So we also looked at an alternative dataset, a Yelp data set, which I guess we all know Yelp as a source of providing user ratings. Yelp actually had two sources of data that we looked into. They have a general dataset which was created as a result of a machine learning challenge approximately a year ago, and they also have a real time API. What, it's not a complete data set. It mostly considers metropolitan areas.
00:35:49.364 - 00:37:06.816, Speaker E: There's obviously more businesses in Canada. But what's really useful about this data set is that you can similarly look at it in terms of postcodes. That's something you'll see on the next slide where you can see that there is a certain number of businesses in most postcodes, and we're able to compare this against the businesses that we get when we look into the Google API. That's at the next slide where you can see that, in a sense, Yelp provides usually more businesses than Google maps businesses. But we actually checked whether these businesses. So in random samples, for instance, for this postcode, we checked whether the businesses in Yelp actually still all existed. And what we found is that usually when we queried Google businesses, they existed.
00:37:06.816 - 00:38:54.364, Speaker E: Yelp, both in the API and in the machine learning data set, usually provided more businesses in the area and around, but the ones that it provided extra usually didn't exist, which shows that in a sense, this data set is not kept clean in the same way. Okay, so overall, the Google data set seems to be much more useful for our purposes. So let me conclude our talk so that we're ready in time. So what we have done really with this project is we have provided proof of concept and that we have the technical abilities to really create a very nice dataset which can provide real time information about business exit and entries. We would have no problem scraping such a dataset on a weekly frequency from Google, and this would be a data set that actually has really rich information between user ratings, price levels, and in a sense, the ability to see how business to consumer establishments pop up or end. Leave the map with what it seems relatively high reliability, even for one. Obviously, one going forward should compare that to other less high frequency data sets like business census.
00:38:54.364 - 00:39:51.592, Speaker E: But we are confident that these are actually, that the number of establishments would probably do relatively well there. Good. And unfortunately, we have encountered, as could be expected in such a project, also some challenges. Obviously, there is, in a sense, a certain bias in the data, especially towards b, two c establishments. Google Maps mostly has things that people are actually interested in finding. So you might have trouble finding a factory or it might be categorized as something odd in Google places. There is also obviously the problem that we can't retrospectively go into Google Maps and scrape things in the past.
00:39:51.592 - 00:41:38.314, Speaker E: So any time series that we create from this data set would be in the future, which is in principle fine. Our biggest obstacle in the sense is that there's some practical limitations towards the cost. But we propose. But for this, there is actually a solution, which is that we would suggest that the bank of Canada or the canadian government is joining into some kind of a partnership with Google to allow this queries, to allow this scraping in such a way that it is, in a sense, a cost effective pricing scheme, which also would, as we have shown, technically be no problem for us to create a data set. Once this is, in a sense, possible, we can possibly find some alternative ways with the free data to create districts. But if we want kind of to have the full view, we would need some kind of a partnership, and I think it would be something that is worth doing. So, all in all, myself and the rest of the team, thank you for listening to our presentation and we look forward to your comments.
00:41:40.454 - 00:42:18.784, Speaker D: Thank you very much, team. And this last point is very important. As we heard yesterday from the OECD Google trains presentation, what it took for establishing the relationship between Google and the OECD was a phone call between the chief economist and Google. So maybe that's something that the bank of Canada needs to do. So, talking about the bank of Canada, I'd like to call on Thibault to give some feedback as the representative who proposed the problem. Thibault, you opened it up at the beginning of the workshop by saying that you didn't like zombies. Do you feel better prepared and safe against zombies now?
00:42:19.624 - 00:43:16.858, Speaker G: Well, now that I know that I can just pick up my phone and make one single phone call and solve the zombie problem, I certainly feel better now. Yes. So thank you, guys. That was a great challenge. And I have to say that's kind of reflecting two trends in central banking and very much at the bank of Canada, that we are trying to invest more and more in data science and rely on real time data. I mean, when I think back at our poor economist colleagues who had to make forecasts last April, last May and say, okay, where was the economy going? And they really struggled because they essentially scraped the whole gigantic macro aggregates models they had because that was just not usable, not worth doing anything. And they started from a white page again.
00:43:16.858 - 00:44:09.490, Speaker G: And that's when all those real time indices came in the picture as being much more powerful in those cases. So they started developing 1015 20 of those indices, and that was really life saving back then. So it shows the real value of investing in data science, real time data. And we are, I guess, here on this workshop all thinking of doing it ourselves. So that's one big push for us. The other one is digging more and more into micro data, like what's the behavior, how to explain the behavior of individual firms, individual banks, individual households. And that kind of project where you really look at individual businesses entries and exits really goes after this richness of the data.
00:44:09.490 - 00:45:11.084, Speaker G: And that's something very new for many of us economists, central bankers, we're really used to modeling the aggregates, but really there's a lot more that can be squeezed and learned from understanding individual behavior and then maybe aggregating it up. So really those two trends are probably not just trends at the bank of Canada, but across central bank. So those skills that you guys developed there, and that's just certainly very well in line with what we are trying to achieve overall. Now, the great thing with that kind of project is that it really forces us to think out of the box and solve technical challenges. And that's what I've seen you do through these three weeks, and I'm very much grateful for that. And as it's outlined in the results you have, there's a lot already that can be squeezed out of this proof of concept. So now I guess the next step might be just one phone call away.
00:45:11.084 - 00:46:11.598, Speaker G: But indeed, I agree that there's probably two operationalities. The cost aspect has to be taken care of. So either with a partnership with Google, I don't know, maybe they think that bank of Canada has a, as a brand name is valuable to them too. Otherwise, I can also see an alternative route that would be focusing on a few cities and downtown areas where there's the highest density of businesses, and that is most likely to be having an impact on overall canadian lives and welfare. And then we could just have a smaller scale version of it, focusing on downtown Ottawa, downtown Vancouver, downtown Toronto. And that in itself, I mean, without covering all of Canada, because Canada is a lot of free space. That in itself might actually be very valuable to just keep tracking it every week or every month forward.
00:46:11.598 - 00:46:26.738, Speaker G: And that might be also within reasonable cost and operationalization constraints. All in all, that's a great thing that you've done here and up to us to now, see how that can be levered. Thanks, guys.
00:46:26.866 - 00:46:28.894, Speaker D: Thank you, Thibaut. Alan?
00:46:29.594 - 00:46:59.582, Speaker B: Yeah. Just. Am I muted? No. Just one remark that struck me after listening to you again, and also having participated a little bit with you and Tom, is that we're all obsessed with trying to measure actual numbers of deaths from COVID Okay, so how do we do that? Well, we do it by looking at excess deaths. Right. That's a sort of classic method. And so we could have done one, could have done the same thing with businesses.
00:46:59.582 - 00:47:35.990, Speaker B: You know, why not look at excess deaths of businesses? Because I think, you know, I suspect that quite a lot of these little places spring up and spring down again. And it would have been interesting just to know how many in over the period would have actually appeared and disappeared. Maybe none. Maybe I'm wrong just because you're taking snapshots, you wouldn't even observe that. But it's just a thought that, you know, there is a flow in and out of these places, and some of them are pretty short lived. So that was one remark that I had. Second question is, how quickly do Google upgrade? And I just don't know that.
00:47:35.990 - 00:48:19.484, Speaker B: But I do know that when I googled my home at one point with my address, I got a completely wrong place on Google Maps. And after protesting, it took five years before Google re put the house in the right place. And it was difficult when you're renting it out, you know? And last remark is that this business about seeing stuff in real time, it reminds me of nowcasting. Now, casting became very popular simply because of the time it was taking to get data coming out. And so nowcasting was sort of jump in and give you a very quick shot at what was going on right now. So in some sense, you're moving with a tide there, I think. Okay, thanks.
00:48:21.224 - 00:48:29.994, Speaker D: Thank you, Alan. I think so. Hil has an answer to your question about access, exits and entries. Do you want to come in?
00:48:31.214 - 00:48:53.964, Speaker H: Sure, yeah, just. Great question. Obviously, that's something we want to look at. The issue is that we don't get historicals from Google places API, so we can't query past dates. So going forward, once we have enough data to basically form a long time series, then we can look at changes versus the background due to a shock.
00:48:55.184 - 00:48:55.964, Speaker B: Thanks.
00:48:57.744 - 00:49:01.804, Speaker D: Are there other questions for the. Yeah, Tom, go ahead.
00:49:02.664 - 00:50:04.034, Speaker I: To Alan's second point about, you know, the idiosyncrasies of Google. So as a complete newcomer to the idea of, you know, web scraping, I mean, it's fascinating for me. I mean I had no experience with this kind of thing. But the Google, working with Google and honestly, as we were forced to in the timescale, Google wasn't aware of our project, of course, and so we had no insider information. It's extremely challenging to make these things operational. The terms and conditions that you sign on with are very obscure and they do not reveal any sort of mechanisms that go into the structure of the underlying query mechanism. So this is why I think another fundamental reason why you actually need to be in partnership with them.
00:50:04.034 - 00:50:30.764, Speaker I: You definitely cannot do this sort of anonymously, just trying to figure out what they're doing. You need to have a partner there within Google who's actually interested in the project and is willing to explain what it is that Google does because it definitely changes the interpretation of the results you're getting.
00:50:34.744 - 00:51:19.534, Speaker B: Just to add one thing to that, Tom, which is that there's a new developing now with Apple, for example, where they are sort of promoting their data privacy, you know, so you don't want your stuff to be communicated to everybody. So we're going to keep stuff that we're getting sort of secret from other people. And of course what's happened is that a whole load of people who advertised by using that stuff can no longer use get to these consumers to advertise because Apple won't let them have the data anymore. And I think that sort of thing may start to happen with Google too. So they can protect themselves and keep sort of other people out of this game. And I think that's going to make life much more difficult. So as you said, you have to have an ally inside.
00:51:24.914 - 00:51:36.714, Speaker D: Yes, that's all true. Other comments or feedback or general remarks for the group. So if not.
