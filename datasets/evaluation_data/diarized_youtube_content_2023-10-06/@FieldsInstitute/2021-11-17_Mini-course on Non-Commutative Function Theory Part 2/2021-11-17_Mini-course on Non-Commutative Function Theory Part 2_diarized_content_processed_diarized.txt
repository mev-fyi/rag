00:00:00.240 - 00:00:41.928, Speaker A: Depending on where you are. It's my great pleasure to introduce our first speaker of the day, Mike jury from University of Florida, who will continue his tour of non commutative functions. Theorem, Mike, please. Thank you, Ilya. Okay, so, last time I tried to give something of the basics. So what is a non commutative function? And what I want to do today is dive deeper into one particular class of examples, namely things given by non commutative power series with an eye towards it. Again, this is supposed to be a focused program on function spaces.
00:00:41.928 - 00:01:24.102, Speaker A: And so I want to talk about specifically the parts of non commutative function theory that deal with function spaces. And so it turns out a natural thing to look at. Well, I mean, if you think about function spaces in general, the sort of starter function space is, of course, the Hardy space disk, the space of square summable power series. And so if you're willing to invent power sort of non commutative power series, or power series and non committed variables, it would be reasonable enough to look at square summable ones and ask what they do. And so that's what I'm going to start doing today. Before I do that, I want to tie up a couple of Loose ends. There was a little bit at the end of yesterday's talk that I ran out of time for, so I want to do want to mention those things.
00:01:24.102 - 00:02:08.714, Speaker A: So yesterday I introduced these NC sets. So again, these are going to be, I'm not trying to recall all the notation, but again, we had a set inside this d variable matrix universe. It's a graded set. So I have, at each size of matrix n, I have a subset of D tuples of n by n matrices. It respects direct sums, and usually we go a little bit farther and insist on some kind of openness. And so I just said it's going to be open at each level, but at its most basic, just close under direct sums. So one way to do topology on these sets is what I said is, just assume it's Open at each level, and that's what we call the fine topology here.
00:02:08.714 - 00:02:55.144, Speaker A: And that's adequate for many purposes. Although a thing that you might dislike about the fine topology is that, well, if you're trying to work in a sort of NC realm, then it would be natural to suppose that in your topology, your open sets should themselves be NC sets. They should be, in particular, closed under direct sums, or respect direct sums, which these fine topology don't, because I have a single open set at a single level, and that would be an open set for my topology. But of course that's not an NC set. So you could go looking among, look for reasonable topologies where the open sets are in C sets. And there's several things you could do. There's at least two of them here that are frequently used.
00:02:55.144 - 00:03:34.166, Speaker A: One is called the free topology. And so this is the topology generated by the polyhedral domains that I mentioned last time. So just some matrix of polynomials and insist that that be contractive. So that includes row ball and column ball and so forth. But the point is, you let delta run over all matrix polynomials, and then this gives you a sort of let that generate a topology that gives you a base of NCO sets. Or there's what's called the fat or the sometimes called the uniform topology, which is just generated by these sort of NC polydists. And again, I'm not going to use detailed things about these topologies as I go on.
00:03:34.166 - 00:04:22.350, Speaker A: I'll mostly sort of ignore them or mostly work at the fine topology. But I do want to mention these things. And in particular, what I want to again mention without going into detail is now that we've got these sets and we've got functions and importantly we've got NC derivatives, you can start asking, or wanting to do more of calculus. And you could ask, I mean, in a very elementary way, just take basic theorems of calculus and ask, do they have NC versions? And this often gets interesting. So I've made a sort of list of some of the more interesting ones here. So there is an NC version of the inverse function theorem, in fact more than one version, because it turns out that you have to be very careful about the topology. So there's a version for the fine topology, a version for the uniform topology.
00:04:22.350 - 00:05:22.666, Speaker A: The hypotheses are slightly different, the conclusions are slightly different. So I put in the footnote, the topology matters here when you want to do this. Likewise, there's a version of the implicit function theorem. And again, the same thing is you have to work in the correct topology and be very careful, because if you think about what the implicit function theorem is going to say, it's going to assert the existence of this function, which is going to be continuous and differentiable and so forth. In the NC world, you'd want, again, you have to be careful about, when you talk about locally bounded or continuous or whatever you need to be careful about, you can do local polynomial approximations. So again, if you believe the point of view I tried to put forward last time, that NC functions should be sort of generalized, should generalize polynomials in sort of the same way that polymorphic functions generalize ordinary polynomials. Sort of an elementary fact about holomorphic functions, of course, is that in their domain, they can be locally uniform, uniformly approximated by polynomials.
00:05:22.666 - 00:05:54.144, Speaker A: So you could ask for the same thing here, but as soon as you say locally uniformly again, you're imposing a topology. And so you have to ask about what topology you're working in. So, local polynomial approximation does work. This is a theorem of Bankler and McCarthy, but this works well in the free topology. But, for example, the implicit function theorem does not work well in the free topology. And so a thing that happens then is it turns out there's sort of no one topology. That's the correct one to use all the time.
00:05:54.144 - 00:06:31.584, Speaker A: And this is, well, it's a fact of life. But the point is, and I think in their book, what they say is there's no Goldilocks topology. You can do polynomial approximation or you can do an implicit function theorem, but you can't do both beyond just sort of ordinary calculus. You can get the sort of more complex analysis things. And then, very recently, James Pascoe has proved rather striking version of the monodromy theorem in the free setting. And I think he's going to be talking about that in his lecture tomorrow. So I won't say more, and I'll just advertise that lecture.
00:06:31.584 - 00:07:35.402, Speaker A: Besides this, you can think of sort of deeper theorems and complex analysis, and more towards the kind of function theory that people like us like to think about. Like, so I say, realization of monotone and convex functions. Here I'm thinking of the Lervener theorem about matrix monotonicity, if you know what that is, and if you don't know what that is, I will refer you to Ryan Doyle's talk in a couple of hours, and he will look at that. And again, this is really a case where this sort of functional calculus point of view is ascendant. The whole point is, if I just ask for monotone functions on an interval, well, those can be almost anything. They have to be measurable, but, I mean, they can be discontinuous and so forth. Whereas if you insist that your function is monotone, not just at the scalar level, but matrix monotone, that is monotone also, for the self adjoint functional calculus, well, the famous theorem of Lerdner tells you that it actually extends to the analytic in the upper half plane and has this Netherland representation and so forth.
00:07:35.402 - 00:08:19.688, Speaker A: So, asking for this fully matrical sort of hypothesis, this functional calculus hypothesis allows you to often obtain a much stronger conclusion, and that's a sort of through going theme of the whole theory. So again, I won't say more about that, but I'll refer you to Ryan's talk this afternoon. But okay. So, moving on from that, like I said, what I really want to talk about today are things that, again, look like function spaces and especially square summable power series. So we're going to imagine, like, you know, we go back to the, the very beginning of function spaces in the disk. You take a square symbol power series and then work from there. So, okay, polynomials are NC functions.
00:08:19.688 - 00:08:38.220, Speaker A: We want a larger supply of them. So I said I'm going to look at power series. So to set up some, just the notation and the basics. So again, X will be a point in the matrix universe. So d tuple of matrices of some fixed size. And I want to look at power series. I want to take monomials.
00:08:38.220 - 00:09:04.468, Speaker A: So since things aren't commuting, I can take products of the individual x's, x one times x two times x three, and so forth. And I get these monomials. So for the notation, I'll have a word in my d letters and I'll call that alpha. If the word has n letters in it, I call that the length of alpha. That's that notation. Absolute value bars is the length of alpha. And then this is like a monomial.
00:09:04.468 - 00:09:47.762, Speaker A: X to the alpha just means write a product of the x's. If I have the word I one, I two, I three, I write the corresponding X's in the same order. I also have an empty word, which I take to have length zero. And as you would expect, by convention, x to the empty word would just be the identity matrix of the same size as x. Okay, so those are our sort of monomials, our multi usual sort of multi index notation. And so then I can form these into power series. Well, formal power series coefficients in front, and then ask, of course, when will this converge? I'm not going to try to do a whole general theory of power series here.
00:09:47.762 - 00:10:59.804, Speaker A: I just want to, again, highlight the square summable case, because I want to get to function spaces. Okay, so if I have such a formal power series, let's look at square summable ones and so, and ask, what can we say about the convergence? So again, what we have in mind is in the one variable case, of course, if I take a square summable power series z to the n, then we know this converges in the unit disk by a very simple Cauchy Schwarz estimate. And so that's why when we talk about the hardy space, of course we talk about the function theory in the disk. I mean, the point is, it's not that we pick the disk first. We choose first to work with square summable series, and disk is forced on us because that's the domain where they naturally converge. So what I'd like to do, hopefully, is look at square summable power series in this NC setting and ask, is there some kind of natural domain on which these series converge? And then that's the domain on which I should be doing this theory of Nc function spaces. Well, at least for this square summable case.
00:10:59.804 - 00:11:37.344, Speaker A: So I want to analyze the convergence of this series, and we can do this very concretely. And I'll prove a simple estimate that will be sort of satisfying when you see it. And the way to do it, the first thing we'll do, which is the sort of standard thing you would do even in the commuting multivariable situation, is it's convenient to organize your series into homogeneous degrees. So for each length n, I group together all the monomials of degree exactly n. So all the words at the length exactly n, and I'll sort of grade the series. Okay, whoops, this is a typo. This should be big n there.
00:11:37.344 - 00:12:19.252, Speaker A: So I sort of grade this series by homogeneous degree. This inner sum is of course a finite sum. If I have d letters, then there are d to the n terms here, just all the words and d letters. But what I'll do is I'll estimate those homogeneous terms individually and then sum the estimates to estimate the whole series. So how should I estimate this homogeneous sum? Well, so here's a simple way to do it. What I'm going to do is I'm going to write, okay, so c alpha, x to the alpha. What I'll do is I'm going to write this as a row of x's times a column of scalars.
00:12:19.252 - 00:13:30.894, Speaker A: So what I can do is I can list all the monomials in some order, I mean, lexicographic or it doesn't matter, but just list all the monomials in some order, and then take the x's and write them in the rows of x alpha, x beta and so forth. So I list all those monomials in the rows, I make one long matrix out of each monomial, and then I can write the c alpha times the identity of the correct size and c beta times the identity of the correct size, and so on. And so I can, so this big matrix I can factor as a long row times a tall column. And then what I'll do is to take norms, well, I'll just say the norm of this is going to be less than the norm of this row of x's times. Well, the norm of this column of scalar matrices, but I mean this, we know what the norm of this column of scalar matrices precisely because they're scalar. Here I'm just going to get the l two sum of these scalers over all the words of length n. Okay, so I have in the l two some of the coefficients there, and that's why I did it this way, because again, our assumption is that the whole series is l two coefficients.
00:13:30.894 - 00:14:12.274, Speaker A: But now I need to find an intelligent way to estimate this row of all the words. So how can we do that? Well, rather than do it in the general case, let me just show you a small example, and then you'll see how it goes. So let me just take two letters and degree two. And so, rather than write x one x two, I'm going to write x and y for my variables. And so my row would look like xxx yx yy. I want to estimate the norm of that. Well, rather than estimate the norm, let me do the square norm so I can take adjoints.
00:14:12.274 - 00:15:14.378, Speaker A: Okay, so now if I multiply that out, what I get is a sum, and so I'll get x x. In the next pair I get xy y x star. And notice that I can pull out an x on both sides. So I get x x plus y y x star. And then in the second two terms I'll have a similar thing with a Y on the outside. Why start? And now if I stare at this, what I see here is this expression in the middle is exactly what I'd be looking at if I was looking at the row norm. So this thing will be less than the row norm of xy squared times the identity and the same thing there.
00:15:14.378 - 00:16:00.914, Speaker A: So this whole thing as a positive matrix is going to be less than the row norm of xy times xx star plus yy. But then that's the row norm again. So I get the rho norm xy. Now to the fourth power times the identity. So what that tells me is the square norm of this thing is going to be less than the rho norm of just the original x to the fourth power. And the point is four is two times the degree. So if you iterate that argument, what I get is that if I look at in degree n and make the row of all monomials of degree n.
00:16:00.914 - 00:16:26.234, Speaker A: Like that, I just iterate that argument. N times the square norm of that is less than the row norm of x to the two n power. So what I do is go back to the estimate I did before. Okay, so I have this row of X's times the column of C's. The c's are estimated by this. The norm of the c's is the euclidean norm. I have that row of x's, which now I just estimated as the nth power of the row norm of x.
00:16:26.234 - 00:17:28.344, Speaker A: And this is good, because now what I see is as long as the row norm of x is going to be less than one, I can control this by, well, I have the square sum of the c's, which is going to be finite times this geometric thing. And so ultimately, I'll control this, this whole series by a geometric series, and I win. So if I put that all together, if I sum up, say, the first k of them, I apply my estimates in each homogeneous term I get. So, in each homogeneous term, I get the kth power of the rho norm times the sum of the squares of the c's in that degree. And then if I apply Cauchy Schwarz to this, I get again, a geometric sum there and the square sum of the c's. And what you end up with is the following proposition. So what you can say is that if the series is square summable and the form is nc power series, it will converge in the row ball as long as the rho norm of x is strictly less than one.
00:17:28.344 - 00:18:15.204, Speaker A: And if I go back up for a second, this geometric estimate, because again, I have this geometric series here. So this geometric estimate tells me that for each individual x, the norm of the matrix fx, again, this is just going to be a single n by n matrix is less than. Well, okay, so it's with the square roots and everything, one over square root, one minus rhoma x squared times the l two sum of the coefficients. And I said that this, we were going to end up with what I call a satisfying estimate. Because again, if you think about how this estimate works in just the Hardy space in one variable, the crude estimate on the point wise value of a hardy space function is exactly this. It's the l two norm of f over square root, one minus mod z squared. But really the same estimate we just did just in one variable.
00:18:15.204 - 00:18:51.348, Speaker A: It's simpler. But we get the convergence in the row ball with this good norm estimate. All right. So what this suggests is then, well, this square summable power series will converge in the well doesn't suggest it does say that this square summable power series will converge in the Rho ball. So we should, like in hardy space we worked in the disk. Now to analyze these series, we should work in the row ball. But if I go back and look at that estimate I did, I mean, I rigged the argument so that we land on the row ball in the end.
00:18:51.348 - 00:19:38.870, Speaker A: But I could have done it differently, because when I began, of course, what I said I was going to do was I was going to write this some C alpha, X Alpha as I wrote it as a row of X's times a column of C's. But of course I could have in exactly the same way written it as a row of scalar matrices times a column of these monomials. These two things are equal because the identities will commit with the x's. So I could have done all my estimating, starting with this column of x's like that. And if you run through the same argument, well, it's no surprise that what you land on then is the column norm of X instead of the row norm. Indeed you get. Exactly.
00:19:38.870 - 00:20:12.594, Speaker A: I guess I gave myself some more room, but let me write it and I'll take the next slide. So, by the same reasoning we get that. Well, I think I just wrote it here. Yeah. So anyway, if you do the same argument, just if I had factored that thing in the opposite way, I would get that same square summable power series. Well, it converges in the row ball, but it also converges in the column ball. And we know that I mentioned last time, while those two domains look the same at level one, they're different to pirate bubbles.
00:20:12.594 - 00:21:01.108, Speaker A: So it converges in the row ball and the column ball simultaneously. So then, which one of those domains should I work in? Well, now it's sort of, maybe the right answer is, well, maybe neither of them. Maybe I should look at this more closely and figure out what sort of, could I have done this calculation in a more organized way? So, because I factored it one way I got the row, one way I got the column, but presumably there's lots of other different, more elaborate kind of estimates I could have done there that they still could have got something out of. So let's think about how we estimate those homogeneous terms in a more organized way. Let me write down some mess of an expression and then talk our way through it. So here's a quantity I'm going to define. So let's read this from the inside out.
00:21:01.108 - 00:21:49.274, Speaker A: So in the inside of this norm here I have these homogeneous terms that I'm trying to estimate, the thing that I've highlighted there. So the goal was to get the norm of that thing. So ultimately what we wanted was to control that by the l two norm of the coefficients times, basically something geometric. So I'm hoping to get l two norm of c times, like an r to the n there. So if I wanted to get an r to the n out of it with the l two norm of the c's, well, to get the l two norm of the c's, what I can do is I can normalize the c's to have sup norm, sorry, not to have normalize the c's to be have square norm, euclidean normal one. And then what I do is, I take this expression, if the c's have euclidean norm one, I want this norm to look like r to the n. To get the r out, I should take the nth root.
00:21:49.274 - 00:22:34.390, Speaker A: I don't need it to literally look like r to the n. I only need it to look like r to the n in the limit. So, asymptotically, so take the limit for at least one soup, and I wanted it to be less than one. So I have a geometric estimate. So what I can do then is define this quantity sigma x to be this, you know, rather complicated expression. And if I do that, then as long as this thing is less than one, I can run the same argument that I did and get a geometric estimate at least on the tail. I can control at least the, since I'm only saying the limb superior, I'll control at least the tail by a geometric series, and I'll get the series to converge whenever this quantity is less than one.
00:22:34.390 - 00:23:31.214, Speaker A: So my square semoles will converge whenever this monstrosity is less than one. And the estimates that we did showed that. Well, what we saw was that the estimates we did show that sigma x is less than the rho norm of x, and this sigma of X is less than also the column norm of x. In the literature, it's more common to use the greek letter rho instead of sigma, but I'm using the english word rho to say rho. And I can't pronounce the two things differently. That's just the convention for the talk. But okay, so whenever, so really, beyond either the row ball or the column ball, I have this more complicated domain, namely the set of all x's where sigma of X is less than one.
00:23:31.214 - 00:24:13.760, Speaker A: And it will converge whenever that happens. And if you unpack the definition of this thing, what you can see is that this is actually an NC domain. It's an NC set, because this quantity sigma will respect direct sums. You can also see, and in fact, the thing that's sort of better than either the row or the column norm is that this quantity is a similarity invariant. Because if I conjugate s by a similar x by a similarity, I can pull it out and I'll get this sort of condition number of s, but that will wash away in the limit. So really my square summable power series will converge on this larger domain, whatever it is. But we have this sort of formidable quantity sigma X.
00:24:13.760 - 00:24:50.574, Speaker A: But let's claim that we can actually understand this in a good way. So, all right, so I've defined this mess of a thing. So one way to kind of catch your breath here and figure out what's going on is to ask, what would this look like if we had done it in one variable? If I'd done this in one variable, well, what would be my words? My words would just, I'd only have one letter. So my monoid, my words would just be, you know, positive integers. So in my homogeneous polynomials, I'd only have one of them. I'd have x to the nth power. I could put coefficients in front of that, but I'm looking by normalizing my coefficients to have norm one.
00:24:50.574 - 00:25:26.866, Speaker A: So I'm just putting unimodular coefficients in front so I could ignore that. So I'm just looking at the norm of x to the n, and then I'm taking the nth root. There's no soup here, because again, it's just, and then I'm taking the limit and asking that to be less than one. Well, okay, so now we have something familiar, because of course this is nothing but the Gelfont formula for the spectral radius of x. And in fact, we know from the Gelfand formula, I don't need to say lim soup, I can actually just, the limit actually exists. And it turns out that for this quantity, likewise, the same is true. In fact, the limit exists.
00:25:26.866 - 00:26:18.192, Speaker A: And I don't need to say limb soup there, which is immediately obvious, but that's okay. But the important point, of course, is that we have a, now a natural interpretation for this quantity of sigma X. It's some kind of a spectral radius, and in the literature it's known variously as either the joint spectral radius or the outer spectral radius. And it's quite an interesting quantity in its own right. And I'm going to only say a couple more things about it and then leave it behind, but it will figure into some of the things I want to say in the last lecture. But it will help me sort of resolve this question about the convergence of these series, because again, we know that the series will converge whenever this quantity is less than one. But.
00:26:18.192 - 00:27:01.402, Speaker A: Okay, so this is some notion of a joint spectral radius. So actually, I wanted to say something. Okay, well, let me just say it. Okay, so here is a theorem which is due originally to Pepescu and was revisited in a subsequent paper by Solomon Shaliazzymovich and also in a survey by Pasco, which has a number of nice results. This Pasco survey has a number of interesting results about this joint spectral radius. But this theorem that I've written here, I say x is similar to. So what I mean is x is similar to some tuple y with rho norm less than one.
00:27:01.402 - 00:28:15.540, Speaker A: So in other words, x is similar to a row contraction to a strict row contraction. X is similar to a strict row contraction if and only if its outer spectral radius is strictly less than one. And again, if you think about what this says in one variable, again, the spectral radius is just the ordinary spectral radius. This is the fairly well known rota strang theorem, which just says that for any matrix, or indeed any bounded operator in Hilbert space, you are similar to a strict contraction if and only if the spectral radius is less than one. So we have a very nice multivariate NC analog of this fact. A corollary of that then, is that, well, if you think about convergence, I said, well, actually these series are going to converge everywhere in this sort of spectral radius ball. But a corollary of this rhodostranc type result is that, well, everything in everything in the spectral radius ball is have all has spectral radius less than one, and therefore is similar to something in the row ball.
00:28:15.540 - 00:29:22.992, Speaker A: So, in fact, if I wanted to, I could get away with just looking at my series in the row ball, even though they all converge in some strictly larger domain. The values in that larger domain are determined by the values in the row ball, because, remember, our NC functions respect similarities. I guess I should have mentioned that this is, this is important, but it's clear now. Remember, our f was this convergent power series, and the point is, once this power series converges and we have that norm estimate, once this power series converges, of course the function is going to respect direct sums and similarities, because it's a limit of polynomials and polynomials respect the rec sums and simulation. So I do have the option of just working in the row ball if I wanted to because then the values of f will be on the whole similarity. Sorry, the whole spectral radius ball would be determined by their values in the row ball. But nonetheless, at this point I don't think I've made a convincing case for why I should prefer the row ball to either the column ball or the spectral radius ball, or any of the other domains.
00:29:22.992 - 00:29:51.840, Speaker A: However, I claim there is a reason that when you're dealing with these square summable power series, there is a reason to prefer the row ball as a domain domain, which I won't tell you today. I'll save that for the third lecture. We talk about multipliers, but ultimately I do want to it does sort of make sense. The row ball is in some sense the natural domain for these things, at least from one point of view. And I'll get to that later. Maybe while I'm here, let me mention one other fact. It's not.
00:29:51.840 - 00:30:35.688, Speaker A: It's sort of tangential, but useful. The one way you can prove a bunch of things about this outer spectral radius or this joint spectral radius is to show that in fact this complicated quantity that I wrote down is in fact genuinely the spectral radius of an ordinary matrix. In fact, what I claim is true. You can find a nice proof of this in the Pasco paper. This joint spectral radius of x is in fact obtained. If I look at the ordinary spectral radius of the following matrix, I look at the sum XJ tensor, so the tensor here is the chronicler tensor product XJ bar. So bar here is the complex conjugate, so not the adjoint.
00:30:35.688 - 00:31:45.418, Speaker A: There's no transpose here, just the complex conjugate of the entries. If I take that, this is now just going to be an n squared by n squared matrix, because I have the chronicler tensor product. If I throw in a square root there to get it scaled correctly, this outer spectral radius is just the ordinary spectral radius square root of that matrix. And this is also connected to something called the quantum parenthrabinius theorem, which would take me too far afield. But this is a very interesting quantity, and it's connected to a bunch of other things. And I will revisit it, like I said in the last lecture, because we will have use for this again. But anyway, what I want to do now is get back to the power series themselves and okay, so where are we? We're looking at NC power series with square summable coefficients, even though we could look at it in the spectral radius ball, I am for the moment just going to restrict myself to the row ball, partly because in the row ball.
00:31:45.418 - 00:32:36.488, Speaker A: I don't just have that it converges. I mean, I have that it converges with this norm estimate. And if I look at this norm estimate, again, this is supposed to be familiar looking, because it reminds us of exactly the sort of cheap norm estimate we get in the hardy space in the disk. The first thing you do with this, once you've got this estimate in the Hardy space in the disk, is you say, well, if I look at points in the disk and I look at the map that sends f to f of z, well, this is telling you exactly that. This is a bounded point evaluation. If f is in the Hardy space. And once I have bounded point evaluations, I have reproducing kernels, and we can get into reproducing Colonel Hilbert space theory and so forth.
00:32:36.488 - 00:33:07.172, Speaker A: But now it's clear that that happens here, at least if I work in the row ball. So again, if I worked in the spectral radius ball, I'd have to think about this a little bit further. And you can say something, but you have convergence, but you don't quite have an estimate like this. So I won't worry about it right now. But anyway, if I work in the row ball, I have this nice norm estimate. So what that means is if I work in the row ball and I take a point in w in the row ball level n, I can look at the map that sends f to f of W. So this f, now it's a square summable power series.
00:33:07.172 - 00:34:42.681, Speaker A: And I guess I should have said, of course I can make this set of square summable power series into a Hilbert space, right? If I want, I can call it sort of the nch two space. And of course the inner product is just going to be the l two inner product on the coefficients C alpha, let's say D alpha bar, like the coefficients for G. So just with the l two inner product, it's a Hilbert space, and we want to make it into an NC Hilbert function space. And the point is that what this estimate shows you is that at each level n, the map that sends an f in the Hilbert space to this n by n matrix is going to be bounded. So I give this nch two space, I give it the h two norm, I give the n by n matrices the usual norm, and this tells me I'm going to get a bounded well, okay, so it's not a linear functional anymore, because this f of w, of course, is matrix valued rather than scalar valued. So one thing that I could do if I wanted was to say, well, I'm going to have not a single reproducing kernel that evaluates a function of the point, but rather some kind of graded kernel where I have these matrix valued point evaluations at every level. It could do that, although a drawback to doing that is really what you want to do with a reproducing kernel is the point is you're evaluating at a single point, you get a scalar, and so the point is a bounded point evaluation, then by the least representation theorem is given by inner product against the vector.
00:34:42.681 - 00:35:36.844, Speaker A: So I really want to do that. If I'm going to get an inner product against a vector, then I really need to have a scalar valued evaluation. But that's easy enough to do because all I need to do is say well, I'm going to take my matrix w and then pick any pair of vectors x and u in cn of the same size as w, and then just hit f of w with this bilinear form, take f of w, apply to u, and then take the inner product with x. So all that's really happening there is I'm taking f, I'm evaluating it w, I get this matrix f of w. Now I write this matrix f of w in some arbitrary basis and then read off an entry of that matrix. So basically I get a scalar value mapping just by writing down the matrix, write it in some basis and read off one entry. So reading off one entry of this matrix, that's a scalar value bounded point evaluation.
00:35:36.844 - 00:36:31.578, Speaker A: And therefore by the Reese representation theorem, it must be given by inner product, sorry, inner product against some element of the Hilbert space, which will depend now on these three pieces of data. It will depend on the matrix and this pair of vectors that I chose. However, I mean, there's a little bit of non uniqueness here because I can choose a different triple w u x. I can choose another triple, say w prime, u prime, x prime. There will be many different triples that actually give me the same vector here. Right? And the way to see that is to remember that f is similarity invariant. And so if I, if I replace w by a similarity and similarly move the u and the x by the similarity, I can find different triples that actually evaluate to the same thing.
00:36:31.578 - 00:37:30.894, Speaker A: So if I know this kernel function here, it doesn't uniquely determine the triple, but in the end it turns out that's okay, it's not a thing we need to worry about. But the point is for each such triple we get this sort of point evaluation. And then the next thing you would do in the classical setting is organize these into a theory of reproducing kernels. So I'll say a little bit about that, not in great detail, but at least enough to get us where we want to go. So we have these point evaluations which are, maybe I should, I should put this, so this, again, this is the inner product of f with this kernel function kwx u. And if you were doing this in the hardy space, the first thing you would do is, well, you do this nice calculation that gives you an explicit expression for the reproducing kernel, which is given by this rational function and so forth. And I'll say something about that towards the end, but let's actually look at what this calculation does.
00:37:30.894 - 00:38:10.838, Speaker A: Okay, so if I write down again my f, I should have written it, there is c alpha z to the alpha. So if I write down f of w and then take this inner product, well, by continuity, I can pass the inner product with the u and the x inside the sum. So this inner product is given by this sum. But remember, the inner product is just the l two inner product on the coefficient. So this is just the coefficients of the f times the complex conjugates of the coefficients of k. So that means I can stare at this sum and read off the coefficients of k. So the alpha coefficient of k, because remember, k is in the Hilbert space.
00:38:10.838 - 00:38:51.040, Speaker A: So k is given by a square syllable power series. The coefficients of k are just given by that expression. There's a couple of different ways I can handle it. I'll handle it one way now, and I'll handle it the other way in a few minutes. But one thing I can do is if I want the complex conjugate of this thing, well, again, this is really just the inner product. This expression, if I wrote it out, would just be inner product w alpha ux in cn. And so if I, like, I can just switch, I can complex conjugate it by switching the inner order of the inner product and then moving the w over and taking an adjoint.
00:38:51.040 - 00:39:34.324, Speaker A: And that leads me with this expression. So by taking complex conjugates, I get that these expressions of the w alpha stars now are the coefficients of k. The reason to do this, of course, is that in the classical theory of kernels, then what you do is you take the inner product of one kernel against the other kernel, and you get this sort of bivariate function, this kzw, and then you study properties of that bivariate function. So here it's slightly more complicated, but in the end, it sort of works out in a reasonable way. So if I do this calculation now for kwx u and another point, k, z, yv. I've skipped the steps, but you write it all out. If you write it all out, what happens is the x sits on the outside there, the y sits on the outside there.
00:39:34.324 - 00:40:23.146, Speaker A: In the middle I have the z to the alphas, the adjoints of the w alphas, and then this thing in the middle, this vu, remember v here is a column and u is a row. So this thing in the middle is really a rank one matrix. And notice too that the z and the w don't have to be the same size. Here z is of size n and w is of size m. Then my vu is a rank one matrix of size n by m. So this whole product is then a matrix of size n by m. It doesn't have to be square, it's square times rectangular times square times like that.
00:40:23.146 - 00:41:11.416, Speaker A: So those are bad shapes. But anyway, so what it means is I can interpret the kernel this way. I can view this kernel as a map that takes this inner matrix of size m by n, puts, squeezes it in between the z and w, and sum and outputs another matrix of size n by m. Really what I have here is, well, the w and the x and the y on the outside are reading off an entry of this matrix. But what I've highlighted in blue is really the way I want to think of this is I'm going to take this matrix in the middle, apply this kzw. So what kzw, this bivariate function is really going to be, is a linear map. So when z is of size n and w is of size m, it's going to be a linear map on the space of rectangular m by n matrices.
00:41:11.416 - 00:42:11.910, Speaker A: And so I think on the next slide I cleaned this all up and so I can interpret it like this. So this is of size m by n and size m by n. If z is of size m and w is of size n. Okay, so the way to, one way to interpret this kernel then as a bivariate function is if you notice these are rank one things. But by taking linear combinations I can take linear combinations of rank one and get but a coherent way to think about this bivariate function. This kernel is a bivariate function and is as a mapping on rectangular matrices like this again of all sizes. And if I wanted to, I could go further and I could talk about, well, since the original functions f had respected direct sums and similarities, there is a sense in which you know, this bivariate function, k, z, w, will respect direct sums and the zs and W's, it will respect similarities in the z's and W's by formulas, which I could write down, but I won't.
00:42:11.910 - 00:43:03.624, Speaker A: But if you wanted to puzzle it out, I mean, it's easy enough to work out from what we said, but it just ends up being a soup of letters, which is sort of not very appealing to read on the slide, and it's bad enough as it is. But anyway, the point is, the way to think about the bivariate function is then it's this mapping of all these different sizes. And the important thing is reproducing kernels should be positive in some sense. And let me maybe not say that yet, but anyway, what happens is, once you've got started this way, it's possible to build up a general NC theory of reproducing kernels along these lines. And rather than try to say all of it, I'm just going to. I've hopefully convinced you that it should sort of work. And this paper, ball marks and Vinikoff 2016, works out in great detail this whole theory in very great generality.
00:43:03.624 - 00:44:06.594, Speaker A: But this sort of executive summary for what we want to say is the things that you hope to be true are going to be true as long as you work out the correct statements. If I have a space of NC functions on some NC domain, and the point evaluations are bounded in the sense that I described, then you do get one of these, what are called CP NC kernels were completely positive. So what that's going to mean is the way that the kernel is positive is if I look at z and z and I evaluate that p, that's going to be positive whenever p is positive. And in fact stronger than that, the map that sends Kz p to k zp, this is all square now, is a completely positive map from mn to mn. So that's, that's the correct sensor in which the kernel is positive, positive, definite, positive, semi definite. It's completely positive like that. So Hilbert function spaces where the point evaluations are bounded give you kernels.
00:44:06.594 - 00:44:49.160, Speaker A: Conversely, there's an Erin shine Moore type theory that says if I have a kernel that has all the right properties, it's positive in the appropriate sense, in the CP sense, and respects direct sounds and similarities and so on, it will be the kernel. There will be a space of NC functions on this domain where that has a. So there's much, much more that can be said beyond this. I mean, and I haven't given the precise statements here. But anyway, it's a long paper and they have, and there's many different variations. You can do versions where you don't worry about similarities, you just do grading and direct sums and then you don't worry about local boundedness. There's a lot of variations of this, but I'll just, hopefully you'll believe me if I tell you in the broad strokes it all works.
00:44:49.160 - 00:45:37.184, Speaker A: You just have to figure out the correct statement. So that's sort of a detour into general statements about kernels. But let's go back to our specific kernel, and again, we have all these sort of complicated calculations with the W's and the X's and the U's, and it can get kind of wearying. So let's pause for breath again and again. If you get tired and you just want to figure out what's going on, a good thing to do as a sanity check is always figure out what happens at level one, because at level one you're dealing with ordinary scalar functions and they're holomorphic and you can see what happens. So let's do a calculation of what our kernel for this. Again, this is our NC hardy space or spaces square summable NC power series.
00:45:37.184 - 00:46:22.540, Speaker A: It has this sort of complicated CP NC kernel. But what does that kernel look like at level one? The point is at level one we can calculate very explicitly. So level one, these are one by one matrices. So I just have a row of scalars there, a row of w's there again in the row ball, what this means is the euclidean norm is less than one. So these Z's and w's belong to the ordinary open euclidean ball bd in the variables. And since I mean since these are one by one matrices, this business of evaluating the vectors in the bilinear forms superfluous. So for these one by one vectors where I evaluate everything, I'll just set all those to one.
00:46:22.540 - 00:47:08.416, Speaker A: Okay? So then if I wash out, if I go back and look at that expression, the sort of complicated expression that I wrote again, the z's and the w's in here, if I look in this bit, the zs and the w's are all just scalars and the z to the alpha is then just as usual multi index notation. But note that the alphas are non commuting words z to the alpha, and I'm still writing them different things in different orders. I'm not sort of grouping the terms. I mean I could, but I won't. This vu in the middle is gone, that's just one. And the x isn't the y, and the outsides are gone, those are just one. So really all that I get is a sum of z to the alpha, w bar to the alpha over again all words in all these non commuting words in the d letters.
00:47:08.416 - 00:47:52.408, Speaker A: So, to work out what this says, again, we'll do the same trick we did before, which was group it into homogeneous terms. If I group it into homogeneous terms of degree n. Well, now look at what this sum is. It's a sum z to the alpha, w to the alpha over all words of length n. And if you do a little bit of combinatorics, it's almost trivial. But if I write out the sum z, one w 1 bar up to zdwd bar and raise that to the nth power and multiply it out, well, if I don't group any terms, what I'll get is simply all words of length n in the w in the z's times the corresponding word of length n and w. Now, if I remember that these z's and stuff commute, I can sort of group those terms.
00:47:52.408 - 00:48:36.032, Speaker A: But it's better if I don't in some sense, because now I can just write this as this sum zjwj bar to the n. But this sum now is of course, just the usual euclidean inner product in CD. And now that's just the geometric series. And so what I thought, what falls out is an extremely simple rational function of two variables. Of these two, well, not two variables, I mean, bivariate and the z and the W. The z and W are themselves in CD, which looks, if I did it in one variable, of course, would be the Zago kernel for the Hardy space. So what that tells me is that if I restrict everything to level one, I'm working with some reproducing kernel Hilbert space of holomorphic functions in the euclidean ball, the space with this kernel.
00:48:36.032 - 00:49:18.544, Speaker A: This space has a name. It was already mentioned in Raphael's talk and in Jolie's talk yesterday. Many of us know what it is, the drury harvest in space. And so, one way in which the dreary harvest in space arises naturally. If you allow that non commutative things are natural, then one way that arises naturally is simply what you get if you take this sort of NC reproducing Colonel Hilbert space of square summable series and restrict it to level one. So, as has already been noted, the Drury Arborson space will be the subject of Michael Hartz's lectures next week. But I want to, and I'll say more about this connection in the last lecture tomorrow as well.
00:49:18.544 - 00:50:46.936, Speaker A: But the point is, we have this connection now is if I take a function in this NC hardy space and just restrict it to level one, that gives me a map. This h 2d is the usual notation for the Drury Arbus in space. So what I get is, and because the kernels line up this map is a partial isometry. But conversely, again, which you can see by a kernel argument or otherwise, given any function f in h 2d, there exists a unique, let me call it, say, f tilde in the NC hardy space so that the norms are the same, and f just is this restriction of f tilde to level one. So given any function in the, in the Drury Arvisson space, it has a unique norm preserving free lift, sort of. There's a unique way to extend it to all the higher levels to give me a so that it's represented by a square summable NC power series with the same Hilbert space norm. Okay? And this fact, together with a companion fact about the multipliers of these spaces that I'll talk about in the last lecture, is what opens up the possibility of proving things about this sort of space of ordinary holomorphic functions via non commutative methods.
00:50:46.936 - 00:51:28.060, Speaker A: And I'll give one example of a theorem of that sort next time. And so I'm out of time, but maybe I'll just throw up this last slide and it will be a segue into tomorrow. If I did that same calculation I tried to do for the landed on the Drury artisan colonel. For a general point zw, I could still do that same algebra, and the point is the same algebra. I would get these z alphas and w alpha bars. Again, this w bar is the entry wise complex conjugate, not the adjoint, the general form of the kernel. I'll say this carefully at the beginning of the next lecture, but the general form of the kernel still has this sort of rational kind of aspect to it.
00:51:28.060 - 00:52:01.566, Speaker A: It looks like it's basically going to be one minus w bar tensor z inverse with some vectors outside. And it turns out this is what's called a descriptor realization of an Nc rational function. So in general, this sort of NC zego kernel is a rational function in the NC sense. And I'll say more about that next time and talk about multipliers and shift operators and invariant subspaces and Berling's theorem and those kinds of things for this NC space. So I'll stop there, and then I'll wrap this up tomorrow. Thank you. Thank you very much, Mike, for a wonderful talk.
00:52:01.566 - 00:52:18.054, Speaker A: Questions, comments, suggestions? Well, if there are no questions, let's thank Mike again.
