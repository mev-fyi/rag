00:00:11.880 - 00:00:48.346, Speaker A: Okay, let's continue and finish this chapter. There's just one more proposition to do. And the chapter is over. We go to chapter three. Samitra, did you start recording? Yes. Yeah, really something here. I think you can click on participants.
00:00:48.346 - 00:01:22.828, Speaker A: And there, I think you can see that. Okay, well, I used to hear that. I don't hear anymore. Anyhow, let's finish this section. We saw that g, the gramion is always positive. So it gives us a kernel function, so we can produce an Rkhs out of this. And now we want to explore more and see what is this h of G? We know it exists in what is.
00:01:22.828 - 00:01:44.398, Speaker A: And here is the proposition. And when you sit and do this little by little, it's not something difficult. It's. It's. It becomes a triviality at the end. But it's still. It's good, because with this, it's almost linearity.
00:01:44.398 - 00:02:38.044, Speaker A: We'll see. It's conjugate linear at the end. Any Hilbert space can be looked as an archaeologist. So, to not to mix h with the Hilbert space that we are starting with, let's call it l. So, l is a Hilbert space with inner product that I will denote by l and define k by define k on l. Note that l here plays the role of x. In other words, the point of this space, or l element of the Hilbert space.
00:02:38.044 - 00:04:03.344, Speaker A: See, by the formula k of x and y, two element of l defined to be the inner product of x and y in l. Then we know that it's a kernel function. Then this part we know function. Of course, it's better to mention on x equal to l. Emphasized our point set is l and h of k. Or as I wrote about h of g, because the gramion is positive. But h of k is a better notation is the precisely the vector space of bounded linear functional l.
00:04:03.344 - 00:06:14.734, Speaker A: In other words, it's the dual and the norm of a functional as an element of this space in h of k is the same as the norm as a bounded linear functional on link. You see? So we start with k. And if we fix some points x one up to x n. Well, it's. Sometimes they write h. Sometimes they write, I want to show that k is positive, k is positive is equal, because it depends on it's defined on L. It means that for any h one up to hn on L, the matrix k hi hj is positive.
00:06:14.734 - 00:07:04.984, Speaker A: That's the meaning. And for any n. And what is this? This is equal to for any h, one up to h n in L. And for any n, the definition of k, inner k of H one. And definition, k of x and y is the inner product of x and y. So this is the inner product of hi and hj in L. And this is precisely the granule.
00:07:04.984 - 00:07:47.394, Speaker A: And we saw this in the previous proposition. The gramion is positive, therefore k is positive. And we have a kernel. So we have an rkhs. H can be called h of k or h of g, if you want. I mean, any of this notation. We have a reproducing kernel Hilbert space whose kernel is, is k.
00:07:47.394 - 00:09:15.254, Speaker A: But note that h is defined or element of h is an rkhs on L, its element or functions on L. If f is in H, then f is defined as a function on L. We want to distinguish these functions. What are these? To start with, as in the Mahr theorem, we see, we try to see what are elements of W. So what is ky kyro at point x? Note that these are elements of L, by definition is k, x and y. This is the definition of k, the relation between little k and capital k. And the definition of capital k is the inner product of x and y in literally.
00:09:15.254 - 00:10:27.044, Speaker A: And this is precisely the inner function, the linear function created by y at point x. So k of y is equal to fy, which belongs to. We can, we can denote it by L star for any y. And if you do any linear combination, sigma alpha j k j, to obtain what, what are the elements of w? It's j from one to n alpha j f y. And you need to be careful here. I write it in red. Be careful.
00:10:27.044 - 00:11:59.634, Speaker A: This is sum j from one to n f of alpha j bar yj, because yj is on the second component when the inner product is on the second one, and when you took it inside, it becomes bar, and then it's f of the sum j from one up to alpha j bar y j. So at the end of the day, any combination, if sum f y with y in L. And this shows that w is precisely over l star is the family of all bonded linear. And indeed, without explicitly saying that, I use ris theorem, because ris theorem says that any element of l star is of this form. For any element of l star, there is a y such that the linear functional is of the form f y. So in rather implicitly, this theorem is used here. But at the end, we see that w is equal to l star.
00:11:59.634 - 00:13:29.414, Speaker A: And recall the discussion we had with sheldon before the break. Why can I go from here and say w bar is also equal to l star? And this is in the finite dimensional case, the same justification work, everything is complete. So we have this in the infinite dimensional case. Here is the way to do it. What is the inner product of f w one and f w two in k? What is this? Well, this is equal to by what we have here, the kernel function at point w one. Kernel function at point w two in k. And this is k of w one at point w two, which we know is capital k at point w two w one.
00:13:29.414 - 00:15:12.678, Speaker A: Again, I write it in red to be careful here. And that by the original definition of k is w two and w one inner product in l. So the golden formula is that f w one, f w two, their inner product in k is equal to the inner product of w two and w one in l. That's that principal identity. And now from this you have the isometry we are looking for. What is the sum of alpha I f w I I from one to n, its norm in k? Well, you square it, and I have done this before, I do not repeat this again, but I have done this before. You square it is if the norm with its inner product with itself, one sum with I, the other sum with j, you have this, then you replace it by this and do the reverse operation in L and then sum them up.
00:15:12.678 - 00:16:10.712, Speaker A: At the end you find it's the sum I from one up to n alpha I bar bar. Because of the change of the order here, wi in L squared for any collection of w I's and Alpha I's. So this has some interesting consequences. One of them is what we wanted to prove here, that w bar is equal to ls two. Because if you have any Cauchy sequence here, it gives a Cauchy sequence there, that Cauchy sequence is convergent. And then you go back to the beginning in the original one is also convergent. So this implies that w bar is equal to l star.
00:16:10.712 - 00:17:17.254, Speaker A: It's the whole dual. And also we have an isometry, I mean the isometry that see from l star, which is our h of k to l, it's enough just to say f of w maps to w. Because as you saw about you can, you can put alpha I here inside it becomes alpha I bar and then sum and obtain the same thing. It's not indeed something new. I have done this before. So we have this c and c is an isometry, but it's conjugate, I mean, because of the bar which is there. So this ends the proof what I wanted to prove.
00:17:17.254 - 00:18:32.204, Speaker A: So it shows that because of this rejective isometry, it, it covers all w because of this rejective isometry. Instead of looking at L, which is an arbitrary Hilbert space, you can look at L star as an Rkhs. So in other words, L is almost L star is almost a copy of l, with a better property that it's an Rkhs. Why? I say almost because it's not linear, it's conjugate linear. But still it's a good way to do it. So, in the light of this proposition, even though we saw before that l equal to l two of zero, one is a Hilbert space, but not an Rkhs, which is true. But if we look at L, it's almost equal to L, and this one is an Rkhs.
00:18:32.204 - 00:19:10.064, Speaker A: That's the content of this section. Well, this long journey, this finishes over chapter two. And now we can start chapter three. We have about 30 minutes. We can start the very beginning indeed. Chapter three. Interpolation and approximation it's mainly an interpolation.
00:19:10.064 - 00:20:58.464, Speaker A: That's one of the earlier reasons kernel become became important. They have applications in interpolation and sampling and approximation. And the basics are covered in this chapter. Let's start with a definition to see what interpolation means. Definition here, x and Y are sets x one up to x n are elements in X, and it's very important to keep in mind all the time that they are distinct and y up to y n or element of Y not necessarily distinct even they can be repeated. A function g from x to y such that g of x I is equal to yi is a function that we are looking for. And the technical term that we use, we say that g interpolate these points.
00:20:58.464 - 00:22:38.144, Speaker A: Well, I say a function g from x to y, which does the job, but most of the time we have a function space. In our case is a reproducing kernel Hilbert space h, and what we want on g, not any function g to be in h and interpolates both. This makes the life a bit a bit more difficult. And that's the topic of this section. Another notation which I will frequently use. If I denote this set by f x one up to xn in x, and I have a reproducing kernel h on x, the space generated by kernel function kx one kx two up to kxn. This will be denoted by h index f.
00:22:38.144 - 00:24:39.854, Speaker A: Ah, I forgot the span. Put here span the linear span of kx one kx two up to kxn is a finite dimensional space and we denote it by h of f. Clearly, the dimension of h of f is less than or equal to n. And now I seek some equivalent condition to see under what conditions this dimension is equal to n, or equivalently is strictly less than n. This is important to arrive at our main theorem, which will be, I believe, next week, because we just have 30 minutes. But we proceed to see how we can do that. Well, first, dimension hf is strictly less than n if and only if if and only what? There is alpha one alpha n, not all equal to zero, such that alpha one k x one plus alpha two k x two plus alpha kxn equal to zero.
00:24:39.854 - 00:26:12.264, Speaker A: So that's the meaning of being dependent. And this happens if and only if. This happens if and only if there is alpha one, alpha n, not all equal to zero, such that for all f in h f inner product with this equal to zero. In other words, I replaced this identity being equal to zero by being orthogonal to all elements. You know, in simple fact, in the Hilbert space, something is zero if and only if it is orthogonal to everything. But now use the fact that we have a reproducing kernel. So there is alpha one, alpha n, not all equals zero, such that for all f in h sum j from one to n, alpha j bar f at point x j equal to zero.
00:26:12.264 - 00:27:37.846, Speaker A: So you don't see the kernel anymore, somehow hidden. And this means that there is a relation, linear relation, non trivial one, which works for all elements of, of this space. So what's written here? Well, it's just a formula, but it means that there is, I mean, non trivial, trivial linear dependence between the elements. Another way to write the same thing is that there is alpha one up to alpha n, not all equal to zero, such that for every f in h, the vector f of x one up to f of xm is orthogonal to the vector alpha 112. True. And now we obtain another characterization. I mean, all of these are equivalent.
00:27:37.846 - 00:28:46.152, Speaker A: The first one, I wrote it in red. Let me add this one is red two if and only if. If I define t index capital f from h to cn by image of f is its trace f of x n. This relation says that the range or the trace of this is orthogonal to an element which is not zero. This is equivalent to say that this is not surjective subject. That's another characterization which I will use. Okay, this is good.
00:28:46.152 - 00:29:35.136, Speaker A: This is good. Well, there are several of them. I just highlighted two of them. Dimension less than one. And the fact that this is not subjective, but sometimes the one in the middle. I will also, I will also use that there is a linear dependence between the elements of hook so in these cases, if this is the case, we see that we cannot interpolate. So here we see that if this is the case, the point there is no function f such that f of x one is equal to alpha one, f of x two equal to alpha two, and f of x n equal to alpha.
00:29:35.136 - 00:30:44.686, Speaker A: We cannot interpolate this point alpha one up to alpha n as our y one up to one j. So this is, if we look at at the very beginning dimension, less than or equal to n up to here t t f is not subjective. We see that this is also equivalent to the fact that we cannot interpolate x one x two xn with all of cn. Another comment is that I have mentioned this comment before. Stuff like this might happen either in this form or in this form. As we saw in examples one and four. At the very beginning, k zero was equal to zero, k one equal to zero in one example and in another one k zero minus k one was equal to zero.
00:30:44.686 - 00:32:33.074, Speaker A: So a relation like this can happen. Another important observation is that h of f, the finite dimensional space is in H is automatically closed. So we can talk about the projection on F. Instead of writing projection on Hof, it's simpler to to write Pof. But I mean more complete is p on Hof is the projection, the orthogonal projection onto Hf. I will use this. What is the properties of this g? Assume that g is in h, g is orthogonal to f, g is in Hf curb or g is orthogonal to hf if and only because we know is some elements which expand this space f if and only if g is orthogonal to the set kx one up to kx because the span of this is equal to hf.
00:32:33.074 - 00:33:28.604, Speaker A: And this means that g inner product with kx one g inner product with kxn, all of them equal to zero. And more concisely, g at point x one is equal to g at point x two is equal to g at point x. N equal to zero. I write this again because this is even though simple, but it's very important. G is in Hf. Herp if and only if gx one equal to zero. Let's see immediately one consequence of this.
00:33:28.604 - 00:34:51.854, Speaker A: Suppose that f is an arbitrary element of h, no restriction. So f at point x one is something, point x two is another complex number, and so on up to f two x. And now look at the projection of f still is in H. What is in Hf? What can we say about this projection? What is pf at point x one and pf at point x one can we say something about this? And here is the point. The meaning of pf is important. Pf is the orthogonal projection. So pf is orthogonal projection onto Hf.
00:34:51.854 - 00:36:23.584, Speaker A: Therefore, for any f f minus, its projection is orthogonal to this space. When you orthogonal into a subspace, the difference is the difference between f and its projection is orthogonal to that subspace. And now we use this box formula. If something is orthogonal to h of f, then at all points x one up to x n is equal to zero. So f minus the projection at x one, at x two, and at x all of them are equal to zero. So in other words, pf at point x one is equal to f at point x one, and pf at point x m is equal to f at point x. What is the message of this? This is something very important.
00:36:23.584 - 00:38:05.334, Speaker A: Message is that if you want to interpolate it really with point x one up to x n, it really doesn't matter if you stay outside and look at all functions in this space, or go into the smaller finite dimensional space of hf and study the same problem. If, of course, if the answer here is yes, general question is yes too. And moreover, if here the answer is yes, it means that if you have inter, if you can interpolate some points, then you don't need to stay outside, you go inside. And still there is a function which does the job. And that function is better by this proposition. If you want to go for minimum energy or minimum norm, let x one up to x n, be a set of distinct b distinct points in x, and let y one up to yn. Or yeah, in CBR three.
00:38:05.334 - 00:40:27.514, Speaker A: Assume that's important. Assume we can interpolate. Assume there is g in h that interpolates these points. Then the projection of g is the unique function of minimum norm that interpolates this point. Proof. First, some part is already shown, the part that interpolates, because we already saw that pfg at point x one is equal to g at point x one, and this is y one up to xm. So we have interpolation, pfg interpolates.
00:40:27.514 - 00:41:43.710, Speaker A: And why is it unique? In a sense, it can be described of the minimum for any other edge. It's written here. Any different? Well, suppose g one and g two interpolate. Suppose are there. What can we say? Then consider h equal to g one minus g two. So h of x one is equal to g of x one minus g two of x one. It's one y one minus y one equal to zero, and the same h of xn is equal to zero, h of x one up to h of xn equal to zero.
00:41:43.710 - 00:42:50.374, Speaker A: We saw that this is equivalent to, to say that h is orthogonal to h. This was mentioned somewhere above. Here it is. If a function is zero at those points, it's equivalent to say that it's in the orthogonal space. So in other words, g one can be written as g two plus h, where h is in Hf curve. This is also somehow reversible. So it means that if you start with assume that h is in Hf curve, and you define g to be g two plus h, then clearly g two interpolates.
00:42:50.374 - 00:44:48.384, Speaker A: G interpolates these points like in the linear differential equation, some other concept we can, we can present all the functions which interpolates. So the set of all these functions are this form any, any, does any interpolating function is of the form one function which I choose to be pf of g plus another one which is orthogonal to this space. I choose pf of g because it's in this space. And all the functions have this form, a complete characterization of them. And you see that the norm now, because of this orthogonality, is the norm of p f of g squared plus the norm of h squared. So this is minimal if and only if h is equal to zero, which means that f is equal to pf of g. That's the only function in this space which gives you the minimum one.
00:44:48.384 - 00:45:54.814, Speaker A: There is another indirect message also in the proof. I mean, it is mentioned, but I want to clarify it further. If you have g one and g two, which both interpolate, both interpolate, then uniqueness tells us that the projection of g one is the same as the projection of g two. At the end of the day, you end up with the same function. And this function is in the span of kx one up to k x n. Please let me repeat. This proposition does not say that we have all the time an interpolation.
00:45:54.814 - 00:46:39.118, Speaker A: It says that, look, there was an assumption. It says that if the interpolation is possible, it's part of assumption. Then there is a unique one of the minimal norm, which is a linear combination of kx one up to kxn. True. So therefore, that that is good. We know that if there is any interpolation, we can go down and obtain something which is optimal of smallest norm, smallest energy. But on the other hand, still we do not know when is this possible.
00:46:39.118 - 00:47:06.754, Speaker A: And that's the topic of over. Next week, to characterize cases in which interpolation is possible and mixing with this, we will have existence and uniqueness theorem up to here is just the uniqueness. Existence is for the next week. Thank you very much. I think it's enough for this week.
