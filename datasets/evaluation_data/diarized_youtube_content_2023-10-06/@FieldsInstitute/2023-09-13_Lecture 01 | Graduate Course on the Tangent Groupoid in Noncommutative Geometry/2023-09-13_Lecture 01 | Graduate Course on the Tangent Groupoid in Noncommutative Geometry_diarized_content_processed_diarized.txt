00:00:00.960 - 00:00:22.634, Speaker A: I think we can start. I think, you know, the door is closed. My name is Nigel Higson. There you go. Let's see, what are the most important issues to cover in this class? There will be no exam. There will be no midterms. There will be no required homework.
00:00:22.634 - 00:00:44.134, Speaker A: There will be no, no, nothing really. Just me talking. Actually, I made a bunch of. I'm making some notes. My notes may survive for three lectures and then peter out. I don't know. But the notes do include homework, but the homework is optional.
00:00:44.134 - 00:01:26.994, Speaker A: Just things that I found interesting but couldn't be bothered to write up. I turned into homework problems. Okay, so here we are, allegedly. I'm going to talk about the tangent groupoid over the course of the semester. We'll see how that goes. The tangent groupoid is something from geometry, and if you are a geometry, then it's no big deal to learn about what is the tangent groupoid. It's really just five minutes, so it wouldn't even last half of one lecture.
00:01:26.994 - 00:02:22.354, Speaker A: But if you're an analyst, the tangent groupoid seems a little bit over the top. And, you know, first of all, it's called a groupoid. It's one of those terrible things, and then it's some weird geometric construction. So because most of the people in this program are analysts who, of one sort or another, even the non commutative geometers are really just analysts, I'm going to approach this tangent groupoid indirectly through operators. And so we'll only get the tangent groupoid at a point where it seems inevitable and natural. This is by plan, that by the time the tangent groupoid is defined, we'll all say to ourselves, well, why wasn't this around hundreds of years ago? It'll be such a natural thing by then. We shall start, however, somewhere else.
00:02:22.354 - 00:02:54.532, Speaker A: I'm feeling kind of groggy because I just got into town last night and I just got into North America the night before, so I'm a little disoriented, not really sure where I am. But let's begin with 430 in the afternoon, right? That's right. Yeah, it's. I should be at Burpee. Well, yeah, it was a tough journey. I got back from France, no problem at all. But then I had to get from Pennsylvania to Toronto.
00:02:54.532 - 00:03:34.754, Speaker A: And so I sat for 2 hours on the tarmac in Philadelphia airport, going exactly nowhere. And then. Yeah, anyway, here is the Laplace operator, the. The Laplacian. One of the things I did, not exactly in preparation for this class, but in preparation for some other lecture, a while ago is I decided to look in the works of Laplace to see if Laplace really invented the Laplace operator. And he did. I'll stick in a minus sign, because secretly, I mean, really, I'm an analyst.
00:03:34.754 - 00:04:44.148, Speaker A: The analysts put in negative signs, and this is the operator that he built. I'll put it in the notes. You can find it's only in one place in his entire, whatever it is, 13 volume collected works. But in one place, the operator is written down in some strange cylindrical coordinates. But there it is. And the reason that Laplace introduced this operator was a little bit interesting, and it's sort of relevant to what we want to do. Let me just keep the three reals there for a while, because, well, for reasons that will become clear in a moment, what Laplace observed is that if you apply the Laplace operator to the gravitational potential, which is basically the function one over r, then what you get is zero will come to what happens at r equals zero in a moment.
00:04:44.148 - 00:05:36.524, Speaker A: But as long as you stay away from this troublesome singular point, then the Laplacian of the gravitational potential one over r, which we learn about from Newton, is just zero. And the reason he was interested in this is kind of a fascinating story. I didn't know anything about Laplace, and then I found myself having to give a lecture to a bunch of astronomers, and I thought, what am I going to tell a bunch of astronomers? I didn't realize this was going to happen, but now I was in front of a bunch of astronomers. I had a couple of days notice. I thought, well, I'll just read a bit about Laplace, and that'll be some bond will have some common, common subject to talk about, me and the astronomers. So what Laplace did is he saved the universe two times. It's kind of incredible what he did.
00:05:36.524 - 00:06:37.028, Speaker A: He was just a titan who kept the heavens stable and in place and kept us safe from all sorts of disasters. So there was something back in the day after Newton's principia. But still, in early days, in the first 100 years after the principia, no one really believed that Newton got it right. I mean, they all understood that Newton could predict elliptic orbits. But as for anything beyond that, no one really understood that Newton got it right. And in particular, there were some troublesome discrepancies between theory, Newton's theory, and observation, something called the great inequality of Jupiter and Saturn sounds like some kind of diversity issue. And so what observations indicated is that as time was going on, Saturn was orbiting the sun slower and slower and slower.
00:06:37.028 - 00:07:33.172, Speaker A: And Jupiter was orbiting the sun faster and faster and faster. And this would mean that at some point, Saturn would just wander away, according to Kepler's law, wander away into the heavens and just disappear, rings and all from us, whereas Jupiter would spiral in towards the sun, and, of course, between Jupiter and the sun is us. And that was a troublesome, troublesome thing in the day. And so there were prizes proposed by the french academy to resolve this discrepancy between Newton's theory and observation. And I think Euler won all of these prizes, but he didn't actually solve the problem, but Laplace did. Laplace eventually figured out that the motion of Jupiter and Saturn, the way they mutually interfere with each other's beautiful elliptic orbit, is periodic, and the period is. I wrote it down.
00:07:33.172 - 00:08:16.184, Speaker A: I'm not sure I put it in these notes, something like 949 years. So it's not obvious that we are saved, but every 949 years, whatever was happening ceases to happen, and then the opposite happens. I guess every 450 years, that happens, it's periodic of period, 949 years. Okay. But as for the Laplace operator here, this was Laplace's second heroic act. There's a similar problem with the moon, which appears to be orbiting around us on the earth faster and faster and faster. And again, there's an obvious problem with that from the point of view of continuation of civilization.
00:08:16.184 - 00:09:04.960, Speaker A: And this is another problem that's much harder that Laplace also resolved. And so there again, it's a periodic motion, not a so called inequality, means a deviation from perfect elliptical orbit, stable elliptic orbits. And so Laplace figured out that the behavior of the moon as it orbits the earth, again, is periodic. Now, the period is millions and millions of years, and you didn't figure out the period. I'm not sure if anyone's figured out the exact period. The second thing, this resolution of the problem with the moon's orbit was published exactly 100 years after the principia. So 1780 something.
00:09:04.960 - 00:09:41.174, Speaker A: Seven. Okay, thank you very much. So, after 100 years, Laplace saved not only us on the earth, but also Newton, and Newton's reputation, and the rest is history. The rest is well known to us. Okay, so one of the things that Laplace was worried about was the fact that the moon is not a perfectly spherical object. Neither is the earth, for that matter. And he wondered whether the slight non sphericity of the earth and the moon might contribute to this inequality.
00:09:41.174 - 00:10:44.658, Speaker A: And I don't know if I can remember the exact formula. And so he began to investigate this. He wanted to know, what is the gravitational field of a not perfectly spherically symmetric body? And for that purpose, he expressed something like this. He expressed the Laplacian in spherical coordinates, and the formula is something like this fellow here. That's just a calculation. But then the next thing he did was he completely determined what the Laplace operator on the two sphere does. He found a complete system of eigenfunctions, which are these notorious spherical harmonical functions.
00:10:44.658 - 00:11:30.626, Speaker A: I think he called them ylms, just like everyone, he does still to this day. So l is 0123, and so on. Those are the eigenvalues, and m is another parameter going between minus l and l. And if you now attempt to solve this equation, Laplacian of a potential equal to zero. This is true for any gravitational potential, including the gravitational potential of a non perfectly spherically symmetric body. It's very tempting to separate variables and express any gravitational field as an infinite series involving, among other things, these ylms. And that's exactly what he did.
00:11:30.626 - 00:12:31.140, Speaker A: And he took the first, I don't know, dozen or so terms, and he just calculated by brute force to understand, compute the gravitational field of a non perfectly spherically symmetric body like us, like the, like the, like the planet we live on. So that's what he did. And after all of that work, he discovered that it has nothing to do with this, this problem of the moon spiraling into the earth. But nevertheless, he bequeathed to us the Laplace operator and some very significant, as it turned out, objects for future development. This idea of spectral theory for the Laplace operator on a geometric space, and this idea here of harmonic functions. So this is what Laplace did, what I learned, because I wanted to bond with the astronomers. So I told them all of this, and it turns out they don't know anything about this.
00:12:31.140 - 00:13:18.654, Speaker A: They've moved way past Newton's theory, and all they want to know about is, is black holes these days. And so they don't, they don't know any of this any more than we do. Okay, all right, maybe I'll not write it as a definition. Continue speaking informally. My plan is just to be casual today. This week, we're all just getting adjusted. We'll just try to introduce the topic that I want to follow over the next several weeks, and so we'll get a little more serious with definitions and lemmas and so on.
00:13:18.654 - 00:14:01.354, Speaker A: Proving things next week. This week is just for, just for fun. Let me go back to this business of the Laplace operator acting on one over r. If you slightly adjust one over r, multiply it by one over four PI. Then you can make a better version of Laplace's equation. Second equation on the top here. Namely, when you apply the Laplace operator to this normalized version of the gravitational potential, what you get is the Dirac delta function at the origin.
00:14:01.354 - 00:14:50.064, Speaker A: Is there a minus sign? Well, I think it's okay the way it is, because I made my operator, I built a minus sign into my operator. I think it's okay. So this function here, and it is a honest to goodness function in three variables. It's a locally integrable function. Nothing fancy going on here, this thing. But of course, it's not differentiable at the origin. This thing here is what's called a fundamental solution.
00:14:50.064 - 00:16:05.974, Speaker A: And so that is more or less thanks to, thanks to Laplace, an additional, small additional calculation is needed here. And if we define. Yeah, yeah, yeah. Four PI is the surface area of the sphere, and that's not, not a coincidence. So let's just introduce some modern notation here. Here's an operator that you could build on functions, let's say, with compact support. And when you apply this operator, you'll get a function which is not of compact support.
00:16:05.974 - 00:17:43.406, Speaker A: What is the operator? It's just convolutional. We define an operator in this way, then something rather interesting happens, namely, you invert not d, but the Laplace operator. Well, to the full extent that you can, with what we have available to us on these boards so far, namely, as an operator from compactly supported functions. I guess we're in three reals. Sorry for all the typos, the non, not necessarily compactly supported functions. There's a small blemish here, which is that this identity is not exactly the identity operator. It stands for the inclusion of smooth, compactly supported functions into compactly supported functions.
00:17:43.406 - 00:18:50.544, Speaker A: But apart from that, you can check on the basis of this formula here, because convolution with the delta function is the identity operator, you can check that you invert the Laplace operator in this way. And so you can solve Laplace's equation by applying the operator q. That's written down here. So, not only did Laplace make good use of this fact, the second line on the board there, in his quest to save civilization from the moon, he also, incidentally, at the same time, found an inverse to the Laplace operator, which he just invented, also. Okay, but it's a little fishy, isn't it, that the Laplace operator should have an inverse? Because, after all, there are lots of harmonic functions which the Laplace operator actually annihilates. So it doesn't quite make sense, does it, to say that the Laplace operator is invertible exactly the way I've written it. Well, what's the problem here is that the I doesn't mean the identity.
00:18:50.544 - 00:19:44.914, Speaker A: It means inclusion and all of the functions that you would, which are harmonic functions on three reals. Of course, they're not compactly supported, so there's no contradiction. But these equations here aren't as good as you might like them to be. Get something that's killed by the plasma. Well, you can't. That would be true if there was such a thing. Well, everything you said is true, but the only harmonic function which is of compact support is the zero function, so it's not possible to.
00:19:44.914 - 00:20:42.704, Speaker A: If it wasn't of compact support. Yeah, let's just. Maybe I'll write it down over here. So let's call this function one over r. Let's just call it lambda, just to give it a name. Of course, what's going on here is that when I write convolution, I mean this formula here, and so the integral is not. You have to struggle to attach a meaning to the integral if f is not a compactly supported function.
00:20:42.704 - 00:22:27.074, Speaker A: Yeah, well, that's one option, but we're going to explore a, going to explore a slightly different approach. If you really want to learn about harmonic functions, then it's really not adequate to follow Laplace's line of attack here and do nothing else, because you can't apply this convolution operator, you can't apply Laplace's discovery to investigate the nature of harmonic functions. And there are, of course, many, many ways around this. But what I want to do is follow one possible approach. So let's just consider, instead of convolution by this function, one over four PI r. Oh, I wanted to say, let me not just back up a little bit here. What about harmonic functions? We'll see soon.
00:22:27.074 - 00:23:07.324, Speaker A: I got a little bit ahead of myself. So before getting to what I was about to write down, let me just say something else when you look at this. So this, it's not perfect, the discovery of Laplace. It's not adequate by itself in order to study harmonic functions. On the other hand, it's rather beautiful that when you want to solve the Laplace equation, what's involved here is just a very simple algebraic function. And you might be optimistic that the whole theory of the Laplace equation is something which is completely algebraic. But that's not exactly the way things work.
00:23:07.324 - 00:24:34.224, Speaker A: So we can't I don't know, get too happy about the fact that the function which appears here is so beautiful and algebraic, for the following reason, that if you slightly, it's just a coincidence, or it's a happy circumstance that this particular simple algebraic function appears so centrally in the work of Laplace. Suppose you consider this equation here instead, which is this operator here, or the solutions of this operator. The null space of this operator consists of solutions, I guess, of some Poisson equation. And suppose you want to find a fundamental solution for this operator here. Well, what's the answer? The answer is this thing. It's a little disappointing in a way. The function that is involved is one over four PI r here, and then on the top, e to the minus k r.
00:24:34.224 - 00:25:01.574, Speaker A: So it's a little odd. What happens. We had something very, very algebraic over here, and we make a small algebraic change to the operator over here. We just add a multiple of the identity to it. But the way that's reflected in the fundamental solution is a little bit strange all of a sudden. Now you have to deal with this exponential function. And it's not just exponential functions which enter the picture.
00:25:01.574 - 00:25:54.710, Speaker A: We could write down a whole list. In fact, if you go to Wikipedia, you can find a whole list of fundamental solutions. But here's another one that's very familiar to people, mostly from complex analysis. If you drop down a dimension to two dimensions, then the relevant function is no longer one over r. It's the logarithm function. Now it's a little blemished, isn't it? You have something beautiful and algebraic over here, and then you start fiddling with the problems, start carrying the same idea somewhere else, and look what happens. You get exponential functions and logarithm functions, and you, you can get these things combined in all sorts of crazy ways.
00:25:54.710 - 00:26:40.040, Speaker A: And it's just, it's a bit offensive, isn't it, to have all of these functions? You have x and log x at the same time, or x and e to the x at the same time. That's log is the integral of the previous log is the integral of previous thing. Yeah, that's, that's true. Okay. And so, roughly speaking, the topic of the first few lectures is to try and understand what these various fundamental solutions have in common with one another. What these operators, what positive features these operators have. Okay, any questions? Looking over.
00:26:40.040 - 00:27:21.392, Speaker A: Of course, the zoom people are just black boxes as usual. Any questions from the living? I guess we can go there. It's actually quite satisfying to check these things somehow. You have to actually do it. To really believe that the relevant fundamental solution here is one over r. And over here it's the, the logarithm function. I mean, why does the dimension really matter? It's a little.
00:27:21.392 - 00:27:45.154, Speaker A: Until you actually do the calculation, maybe you won't even believe it. If it works in three dimensions. What's special about three? Three is nothing special about three should work in any dimension. No, it's not the way it works. What happens in four? Of course the answer is known, but I don't. Okay. I don't know the answer.
00:27:45.154 - 00:28:12.472, Speaker A: Say again? Yeah. It would not be one over r squared because one over two minus one over r squared and r four. Are you sure? So that has, does that have the right two minus. Okay, fine. That has the right homogeneity. Yeah. There is a little check you can mental check you can run.
00:28:12.472 - 00:29:06.394, Speaker A: Okay. All right, very good. So I want to address some issues which are kind of offensive to the modern mind, one of which is these operators, you know, don't quite map the space of good functions, the test functions, into themselves. And the first thing we're going to do is get around that problem by modifying these fundamental solutions. And then we'll see what we can do with those modifications. Suppose I have an operator. What should we call it? A linear operator.
00:29:06.394 - 00:30:17.700, Speaker A: And no manifolds for now, just rn. So what I want to do is to begin to construct a more, from the modern point of view, adequate framework in which to study these equations. So we would like operators, for example, that we can compose with, with, with other operators. They should map the same space to themselves and several definitions. I think there are three or four definitions I'm going to give you are directed towards that end. We'll say that an operator is properly supported if it has the following property. Each time you pick a smooth, compactly supported function, you can find another smooth, compactly supported function such that.
00:30:17.700 - 00:30:58.064, Speaker A: What? Well, suppose you apply a only to functions which live, so to speak, underneath phi. So you study this operator here. Phi here means not just the function phi, but also the operator of point wise multiplication by phi on smooth functions. And of course, it sends all smooth functions down into smooth, compactly supported functions. You might ask, what kind of functions do you get after you apply a phi to a function f? Well, on the face of it, you could get arbitrary functions. But what we're going to say is that, in fact, that's not what happens. You only get functions which live under psi like this.
00:30:58.064 - 00:32:14.760, Speaker A: And for the sake of symmetry, we'll do it on the other side, too, so that says, in particular, if you apply a to all functions which are supported in some particular compact set, like this one here. Did I do this right? Yeah, we did it right. Like this compact set here, then all of the functions you get by applying a's to functions in this compact set will all be supported in some other particular fixed compact set over here, namely the supportive pSi. So what you're supposed to imagine is something like this, that the function phi is supported in some region like this, and then the function psi is just supported in a slightly larger region like this, and the height here is supposed to be one. That's funny. Yes. For every phi, there exists a sign.
00:32:14.760 - 00:33:09.876, Speaker A: That's the same thing as saying that PI. You don't have to specify until you're already told. It is true that you can speak of the compact support of an operator, of an operator being compactly supported. And indeed, I'm saying that a phi is compacted. Yes, yes, indeed. Yeah. My convention, which I'm sure I'll violate, I already violated it in the notes, is that when we apply the operator to an element of the vector space of functions on which it's defined, that element will be called something like f or u.
00:33:09.876 - 00:33:32.824, Speaker A: Some roman letter greek letters correspond to functions which are going to be used more as operators. Multiplication operators. Are we happy with this now? Happy? Heath, he's gone. It's not even there. Oh, is there? Yeah. Okay, good. Hi again.
00:33:32.824 - 00:34:31.264, Speaker A: All right, so these convolution operators are not compactly supported. That's a, that's a minus. Okay, so these three definitions that I'm giving over here somehow come as a little package. This is a familiar one to everyone who's in this program at the fields institute. An operator, as before, a, as above, is a smoothing operator. It doesn't have to be properly supported, just any operator. A mapping compact supported functions to functions.
00:34:31.264 - 00:35:28.474, Speaker A: If there exists a smooth integral kernel, the usual thing. Just have to write it out. So the operator is given by integration like this. And if I say that, if I insist that f be compactly supported, then there's no problem with the integral. The integral just makes sense. And it's a smooth function. And maybe this part here.
00:35:28.474 - 00:36:41.644, Speaker A: Yes, because f is a Roman letter, I could write M sub five for the operator of multiplication by Phi, but I don't know, I'm sort of prejudiced against that. SORRY. Ah, you can write whatever you like. Of CouRSe, you can write m SUb Phi, and then we can all be happy. Yes, there are going to be four definitions. Same thing as before, same kind of operator. Oh damn, this looks like a Volterra operator.
00:36:41.644 - 00:37:20.734, Speaker A: I mean Volterra operators. So there are different volterra operators out there. And I'm addressing this question Here in This LITTLE box, the operator which tries to be inverse to differentiation. The operator of definite integration is an example of an integral operator like this. But the integral kernel is, if you work it out and think about it, not going to be a c infinity function, it's going to have some actual discontinuities. Jump discontinuities. I guess we'll see that in a moment.
00:37:20.734 - 00:38:30.414, Speaker A: Okay, so if that's what you think Volterra operator is, then it's not. A VOltERrA operator is not quite a smoothing operator. More definitions will say that an operator is pseudo local if it has the following property, and you can vary this definition in a number of ways. But let's discuss here and get the same definition. In the end, let's discuss arbitrary smooth functions, not necessarily compactly supported, but let's suppose those ports are disjointed. Pseudo local means that the composition of these three operators is a smoothing operator. This is a property that Volterra operators do have.
00:38:30.414 - 00:39:36.994, Speaker A: Maybe I'll give an example, because it's a little more fancy. Suppose we go back to this example over here. Suppose you have a function which you can integrate, at least over compact sets, reasonable, measurable function which is locally integrable. Call it lambda, like we did over there. And this thing is an example of one of these pseudo local operators, if not only if this function lambda is a smooth function, except at one place, which is zero. Maybe I should put it like this. I restrict this function to the complement of zero.
00:39:36.994 - 00:40:38.274, Speaker A: Then I get a c 50 function. This is a property that one over r has. Of course, log r also has this same property. If you're a PDE person, then you're probably accustomed to thinking of pseudo locality in a slightly different way. But it's equivalent to what's written here in the context of operators like the ones that I'm discussing, which map smooth functions to smooth functions. Okay, and now let's try and put some of these things together. One more definition.
00:40:38.274 - 00:41:55.334, Speaker A: Oh, maybe we'll start with a differential operator, some kind of linear partial differential operator. What does that mean? It means there's a formula for d, just the usual sort of formula, only with a finite number of terms. So there would be some coefficient functions, and then some partial derivatives. Suppose I have an operator a like all of the ones above. I mean, it just maps smooth compactly supported functions to smooth functions. Like all of the operators we've been discussing. Yeah, I forgot to say so.
00:41:55.334 - 00:42:39.144, Speaker A: But all of these convolution operators have the property, which is not totally obvious. It's just obvious that these operators map smooth functions to smooth functions. We have to think a second why that's true, that these operators do map cc infinity, on which they're defined into c infinity. Anyway, so these are all examples. These convolution operators are what we're talking about here. Let's call this thing here left parametrics. So, I just invented this term this morning, and I'm not sure if anyone else has ever called it this before, but let's call this guy a left parametrics for D.
00:42:39.144 - 00:43:21.244, Speaker A: Maybe I should, instead of my usual incredibly sloppy handwriting, maybe I should, because it's a weird word, make it clear what the spelling is. When I was five years old, I won a competition in my village for handwriting. My village had 100 people, so it wasn't a terribly difficult competition anyway. But things have gone downhill since then, as you can tell. Yeah, I think I was competing against my peers. Yeah. I also won an art competition as well.
00:43:21.244 - 00:44:04.790, Speaker A: The first and only time I won an art. Okay, so a is left parametrics for d if. Well, left means I'm going to compose a on the left of d if the operator. So let's just pay attention to what's going on here. Like we discussed before, this makes sense, let's say, as an operator, from smooth, compactly supported functions into smooth functions. So the identity here just means inclusion. Not exactly the identity.
00:44:04.790 - 00:45:33.526, Speaker A: This thing here is a smoothing operator, and let's call it a parametrics, if. Which I won't bother to write out clearly anymore, because it's the same spelling as this is a parametric if the same thing is true, the other way around, one minus da, is two. So many definitions. How about an example? Well, here's the sort of thing that we have in mind, which is parametrics, which also has this proper support condition, which I sort of neglected over there. Let's work on our two. Been thinking a lot about the logarithm function I spent forever on. Yeah.
00:45:33.526 - 00:45:56.464, Speaker A: Well, I was stuck in Philadelphia airport. When you're stuck on the plane, they turn off the engines, and then the air conditioning turns off. There you are sitting on the plane. So I thought, well, I'll compute the Fourier transform of the logarithm function while I'm waiting. It's tough, man, to compute the Fourier transform of the logarithm function anyway. But I did it. If you take this operator here.
00:45:56.464 - 00:46:30.816, Speaker A: Oh, what was the normalization? Something like two PI. Oh, I think I missed off. Small sign somewhere. It's too late now. But the correct sign here is minus the logarithm. So that's what the fundamental solution is. So this guy here is indeed parametrics.
00:46:30.816 - 00:47:20.602, Speaker A: In fact, both of these operators here are just plain zero, not just smoothing operators. But if you multiply this function, the logarithm, by something, which cuts the logarithm off to make it compactly supported. So this is going to be some c infinity function. Compactly supported. I have in mind to solve this problem of a not mapping smooth, compactly supported functions into smooth, compactly supported functions by applying a cutoff like this. On the other hand, I don't want to multiply by zero. That would be doing too much violence to the problem.
00:47:20.602 - 00:48:21.794, Speaker A: So let's insist that this function should be identically one in a neighborhood of zero. Thank you. You're going to be like this the whole class. No, I'm very glad for your assistance. Anyway, so here's a function which does not suffer the same demerit as the logarithm function by itself. Namely, it is now a compact supported function. So the convolution with this function does map smooth, compactly supported functions to smooth, compactly supported functions, just like d does.
00:48:21.794 - 00:49:25.360, Speaker A: And it's parametrics. Now, if you multiply or compose d with a on either side, you do get an operators, let's say from smooth, compactly supported functions to smooth, compactly supported functions. And if you believe what I wrote on an earlier board that jacob was referring to the fact that the logarithm function is a fundamental solution, then you have to believe that this guy here is a parametric. It's really just a repackaging of that calculation. So, this is the parametrics. The philosophy here is that smoothing operators are small, and to get identities, modulo smoothing operators is an acceptable trade off in order to work in some realm of reasonable operators, mapping compactly supported functions into themselves. And just as a little reward for digesting all of these definitions.
00:49:25.360 - 00:50:07.584, Speaker A: Ah, one more definition and then we're done. Yeah. Good. Oh, japanese technology. I'm the proud owner of a japanese blackboard, eraser, vacuum cleaner. It looks like a toaster. You've seen these things? Yeah, yeah.
00:50:07.584 - 00:50:34.516, Speaker A: I went to, the first time I went to Japan, I was in a faculty office, which is much bigger, about two times the size of this classroom, as is the standard in Japan. And know how to treat professors in Japan. Damn it. It was such a big office that it even had a toaster in it. I thought, this is incredible. But then on closer inspection, I saw that it wasn't quite a toaster. The slot is oriented in the wrong direction for a toaster anyway, but it cleans the blackboard eraser.
00:50:34.516 - 00:51:14.444, Speaker A: It's a great tool. Right? What was that? Yes. Yeah. Now we can, we're finally in a position to say something intelligent about harmonic functions through this string of definitions. And just one more. So, an operator d, like the one above, is said to be hypo elliptic. It's currently very fashionable to study hypoelapticity in a variety of contexts actually related to geometry.
00:51:14.444 - 00:51:57.424, Speaker A: And here's the basic definition. It's a local thing. Each time you have an open subset of rn, and each time you have a smooth function on that open subset. I don't have to say smooth, compactly supported function, because I'm dealing here with a differential operator. I can certainly apply d to f. And the condition is that whenever df is a c infinity function. Oh, hang on, I didn't.
00:51:57.424 - 00:52:47.316, Speaker A: That's what I said is true, but it's not what I wanted to say. Every. Well, distribution generalized function, maybe it's just, maybe D is an operator of order two, and f is just two times differential. So at least we can make sense of df, so we don't have to worry about distributions. The conclusion is supposed to be that, in fact f is a smooth function. This is a well known property of harmonic functions, sorry, of the Laplace operator. And it's reflected in a well known property of harmonic functions that are automatically smooth.
00:52:47.316 - 00:53:37.636, Speaker A: In fact, they're automatically real analytic. And finally, a little bit of a payoff. D is still a differential operator. D for differential, yeah, not necessarily that one, but this one. Yeah. And finally, a little bit of a payoff if, if D has a parametrics. And in order to make the argument work, you need just a little bit more.
00:53:37.636 - 00:54:49.022, Speaker A: Namely, you need this pseudo locality up here, which you have, for example, if you're dealing with a convolution operator under reasonable hypothesis. If D has a left parametrics, that is, and you don't actually need proper support, this stage, that is pseudo local. So the idea is that the parametrics is an inverse. And from the equation df equals g, you can figure out what f is in terms of g. And there's enough information there to recover the conclusion of hypo ellipticity that indeed, if df, which is g is c infinity, that f has to be c infinity two. Of course, the parametrics is not exactly an inverse, it's an inverse modulo smoothing operators. But every smoothing operator has the feature that it maps distributions to honest to goodness smooth functions.
00:54:49.022 - 00:55:21.544, Speaker A: So that's another way of thinking of smoothing operators. And you're basically done, except for the fact that you're supposed to check this definition not just on rn, but on every open subset of rn. And pseudo locality allows you to do that by interposing some cutoff functions to take a function on an open subset and make it into a function on the whole manifold. Very simple. It's a simple exercise. In fact, it is an exercise in my current version of the models. So there you go.
00:55:21.544 - 00:56:04.280, Speaker A: So for example, the Laplacian on three reals and the Laplacian on two reals. Two examples we were looking at, and a bunch of other things. These guys are hypo elliptic. And in fact you can find a parametric according to what's written. Write it down anyway, up here. In fact, you can find a parametric which is a properly supported operator. I forgot to say something about properly supported operators.
00:56:04.280 - 00:57:12.504, Speaker A: So let me just go back, I gave you this definition, and then we had a long argument about notation, and then I forgot what I was doing. So let me come back and say something about, about that. So here's a remark that should have come after the definition of proper support. If you happen to have an operator which is properly supported. Yes, before I get to that operator a, as above, you in particular mean every operator is properly supported. No, I was too lazy to write this line down. So what I meant was here, right here, very soon, every single operator that will be discussed in this class will be properly supported, I think.
00:57:12.504 - 00:58:20.940, Speaker A: But so far it's not necessary. I just meant a map. Smooth, compact supported functions to smooth functions ah, yeah, that's an excellent, I'll add it to the list, I'll give you those ones to grade. Okay, if a is properly supported, then we get the following two facts. First of all, a actually maps, as I did say, but I didn't write down smooth, compactly supported functions to smooth, compactly supported functions. But it also, if you're interested, maps smooth functions smooth functions if you have a smooth function which is not compactly supported, then you don't immediately know what a applied to that function is, because a is only defined on smooth, compactly supported functions. But you can apply to that function some cutoff function like you see here.
00:58:20.940 - 00:58:58.134, Speaker A: You can multiply your f by some phi and apply a to phi f instead of just f. And now it makes sense. At least you can make the application. And then you want to know what you get. Well, you want to know what is the value of that function at some particular point in rn. And it follows from the definition that the value of that particular function, a, of phi, of phi times f at this particular point doesn't really depend, does not really depend on the cutoff function, as long as the cutoff function is sufficiently broadly supported. Like in this picture, it looks more like psi than phi.
00:58:58.134 - 00:59:45.716, Speaker A: And so you can extend in a natural way a from here to here, if you like. Okay, so these are the good operators. So if you're a linear algebraic, proper support is like saying you have an infinite by infinite matrix which is supported in some kind of band around the diagonal. And of course, you can easily build a matrix which violates that condition. You can take a matrix which just lives in one row, it's not going to have proper support. Or you can have a matrix which lives in one column, and it's not going to have proper support. If you like, you're violating these two conditions one at a time.
00:59:45.716 - 01:00:31.154, Speaker A: So you can easily build examples like that. If you do Eckhart's exercise, then Eckhart's exercise will tell you how to interpret proper support in terms of k, namely the support of k, which is the function. I'm going to do the exercise right now on Rn times Rn. That support, which is a closed subset of Rn times Rn, should have the property that if you project from that support down to either coordinate copy of Rn, that projection map is a proper map in the sense of general topology. So, yeah, you know, I could choose, for example, just to answer your question more bluntly, and specifically, I could choose k to be the constant function one. Now you're in trouble. That's not going to be a properly supported operator, but it is an operator.
01:00:31.154 - 01:01:58.594, Speaker A: I mean, it gives rise to an operator through this formula. How you doing? Still got stamina for another half hour. Oh, a theorem, of course, in all of these examples, when I'm ready to just mention this back over here, take my fancy razor with me. It's not bad at all. We could probably agree only to study compactly supported operators corresponding to k's, if you like, which are honestly compactly supported functions. And we probably do just fine most of the time, but not exactly all of the time. We will be wanting to look at operators on cc infinity of Rn, which are actually translation invariant.
01:01:58.594 - 01:02:46.798, Speaker A: That's going to build an important role. And translation invariance is obviously incompatible with compact support. Yeah. Okay. Beyond just preserving the support, what are the kind of things? Yeah. So the operators that we're going to be studying are differential operators, like this one here, and the operators we obtain from differential operators by trying to solve the equation df equals g. In other words, these parametrics, which is the best you can do when it comes to solving these differential operators, the equation df equals g in general.
01:02:46.798 - 01:03:17.054, Speaker A: So those are the operators that we're going to be studying as auxiliaries. In our study, we'll be studying geometric operators on Rn, namely translation operators and also dilation operators. They play a big role, too. Those are not operators that are going to be the immediate focus of our, the direct focus of our attention. But we'll be studying parametrics through dilations and translations. So they'll be part of the story. And there'll be examples of operators.
01:03:17.054 - 01:04:35.070, Speaker A: For example, none of those geometric operators have this pseudo locality property, so they're a little bit outside of our main objective, whereas all of the operators will be studying in the algebras that we'll construct in a little while will be pseudo local. So the main operators are the operators like a, for example, this one here, which is very interesting operator. This guy here is the main operator we want to study, or this and its friends are the main operators we want to study, but we should be playing around with, for example, translations and dilations to understand them better. And the reason, the ostensible reason, I guess, that we want to study these guys is that we want to obtain variations of this little theorem here, little lemma. I guess it is all right. The operators that Laplace, the operator that Laplace gave us, and the variations that I've shown you so far, for example, the Laplace operator in other dimensions. And what else did we look at? We looked at this Poisson operator, which is just the Laplace operator plus three, or something like that.
01:04:35.070 - 01:05:29.824, Speaker A: These are all examples of translation invariant operators. And for all of these, everything that I said so far somehow becomes a lot clearer if you just introduce the Fourier transform, which didn't quite exist in Laplace's time. Fourier comes after Laplace, which is kind of interesting, because Laplace was perfectly comfortable expanding functions in a sort of generalized Fourier series, using the Pieter Weil theorem, if you like, and expanding functions on the sphere terms of spherical harmonics. So he was happy to do that, even though he didn't have Fourier to lean on. So that actually comes first before the Fourier series there is the Peter Weil theorem. There you go. Anyway, the whole story becomes, if you're dealing with translation invariant operators, it becomes a lot easier if you have at your disposal the Fourier transform.
01:05:29.824 - 01:06:40.734, Speaker A: And I'm just going to write this down once, mainly to put the twos and the PiS and so on in the right place. So let's agree that the Fourier transform is this particular operator, like that. So this makes sense. As long as f is smooth, compactly supported function, the integral makes sense, and the result is a smooth function, not necessarily compactly supported. When you're playing around with the Fourier transform, you come to realize after a very short time that it's much better to deal with Schwarz functions than smooth, compactly supported functions, because the Fourier transform of a Schwarz function is just another Schwarz function. So there's a certain parity to the whole thing. Anyway, if you make this your definition of the Fourier transform, then the way the inversion formula works is you get a one over two PI n coming out of the front like this.
01:06:40.734 - 01:08:16.214, Speaker A: Let's work just with Schwarz functions, functions on Rn, which are not compactly supported but have the appropriate rapid decay. Of course, as everyone knows, if you want to study, for example, the Laplace operator, he would be on RN. Then the whole story becomes a lot more transparent, so to speak, under Fourier transform. At this conference in Paris, Michel Verne, she began the whole conference by denouncing commutative diagrams. So everyone had to hurriedly change their slides, you know, eliminate all of the commutative diagrams. Anyway, here's a commutative diagram, and what you're supposed to put down here is just multiplication by the function psi squared. For our choice of sines in the Laplace operator, the function sine.
01:08:16.214 - 01:09:21.208, Speaker A: So if you're trying to find an inverse to this operator, or as close to an inverse as you can get, then clearly, under Fourier transform, you're trying to find an operator which is as close to an inverse as you can get to the operator of multiplication by psi squared. So all you're supposed to do is multiply by one over Psi squared. And now you see the reason for this whole business of smoothing operators and error terms and so on, parametrics, which is that the psi squared function has a singularity. Excuse me, the one over psi squared function has a singularity, xi equals zero. So you have to get rid of that by multiplying by a little bump function or one minus a little bump function. So when you look at everything from this Fourier transformation point of view, it all becomes rather transparent. The operator, a which is a parametric for delta, should simply be the operator whose, which corresponds under Fourier transform to multiplication by one over psi squared.
01:09:21.208 - 01:10:09.524, Speaker A: Or maybe something like one minus sigma of psi, like up here times one over one minus sigma. Psi times psi squared. Yeah. So maybe here we'll say we want to find a parametric. Then what we should put down here is something like one minus sigma. I'm just. And yeah, so this somehow explains, rather conceptually, why you need to deal with error terms, these smoothing operators, why you're talking about parametrics and you're not talking about inverses of operators.
01:10:09.524 - 01:11:55.426, Speaker A: This works out so well because the operator delta, the Laplace operator, is translation invariant. So it has to do with the group structure on Rn. It's compatible with the group structure on Rn in a very special way. Fourier transform, as we know through later years of education, is somehow an expression in function theory of the fact that Rn is not just a space, but it's a group. If you have an operator which are not a translation invariant, for example, a differential operator with non constant coefficients, like we had a little while ago, things are not quite so simple. Namely, there's no very simple way of understanding the compatibility with the operator up here with some other operator down here under Fourier transform, there's going to be something you can put down here. Obviously, if you put any reasonable linear partial differential operator up here.
01:11:55.426 - 01:13:27.044, Speaker A: But as to what it is, that can be quite a complicated problem. And roughly speaking, that's the problem that we're interested in, except that we're not going to do what I'm now about to write down, more pseudo stuff. So, one takes inspiration from the Fourier transform in this rather simple story over here. But if you want to apply it to non translation invariant operators, then you need to begin to consider the following operators, the ones I'm about to write down, which is some sort of weird hybrid between what happens up here in configuration space and what happens down here in momentum space. So here's one over two PI n, which is signal, with somehow taking some inspiration from the Plancherelle formula. And I'm adding here f hat of psi, like you see in the Plancherelle formula, and then the Fourier kernel, like you see in the Planche Charles formula. But we'll stick in here a function.
01:13:27.044 - 01:14:24.338, Speaker A: And if a is a reasonable function, which I'll tell you what it means in just a moment, then this thing here is an example of a pseudo differential operator. So the a which appears here is called a complete symbol. Little a is called a complete symbol of big a. And it's easy to explain what this function should be. It's kind of a miracle that once you write down the definition I'm about to write down, such a beautiful theory emerges. Here's the function a. What's an acceptable a to get a reasonable theory of operators? Well, the following is an acceptable condition.
01:14:24.338 - 01:15:00.994, Speaker A: Suppose you take this function and you differentiate it a certain number of times. Alpha plus beta, and that's alpha derivatives in the x directions and beta derivatives in the psi directions. Like that. Do I have enough room to do this? The reasonable condition? It took a while for people to figure this out. So this is the distillation of homeander of what the reasonable operators are. Here. We'll take a supremum and it's good to.
01:15:00.994 - 01:15:34.674, Speaker A: Well, let's just do it this way. This is maybe the simplest collection of operators. I'll put a condition both on x and xi. Excuse me? I won't put any condition on X and Psi. They're just anything. And the condition is that this is no more than some constant which is allowed to depend on alpha and beta times japanese brackets of psi, which, I'll remind you what this is in just a moment. And then m minus beta.
01:15:34.674 - 01:16:18.350, Speaker A: So, first of all, this angle bracket thing is just a way of talking about the norm of xi, basically. But when Psi is zero, the norm is zero, and that's a nuisance. So you study this thing here. Excuse me. Got a supreme of the line? Yeah. Thank you. I should divide or just get rid of that? Yeah.
01:16:18.350 - 01:16:39.128, Speaker A: Thank you very much. Thank heavens for Jacob. Where would we be without Jacob? I don't know. Yeah. So what we're defining here. So that's. I had realized I hadn't done yet.
01:16:39.128 - 01:16:58.384, Speaker A: So this defines a symbol function of order, M. And you can think of M as being an integer. One, two, three, minus one, minus two, minus three. That kind of thing. Zero. Actually, zero is most important for us. There you go.
01:16:58.384 - 01:17:54.034, Speaker A: What does this mean? Well, first of all, maybe I should show an example so you get a sense of what's going on. We're not going to study this definition carefully. We just need to be aware that it exists. Because we're going to make some circuitous route towards defining exactly the same class of operators in a different way. But we shouldn't ignore many decades of history. Here's an example. If you have a partial differential operator, the sort we were discussing before, a whole bunch of coefficient functions.
01:17:54.034 - 01:19:14.700, Speaker A: This is a pseudo differential operator under mild conditions because I took a supremum here over all of rn, these functions here should be uniformly bounded, and all of their derivatives should be uniformly bounded. And what's interesting is what is the a function? And it's just about the only thing it could be. Namely, it's just the sum of all of the a alpha of x's times. Like in Fourier theory, I xi the alpha like this. So this definition is designed so that differential operators automatically fit into the context of pseudo differential operator theory. What's miraculous about the whole? And this type of function, which is a polynomial function in the xi variable, certainly has this property. The more you differentiate with respect to xi, the smaller, lower growth the function is, after all.
01:19:14.700 - 01:19:54.364, Speaker A: And if you differentiate m times with respect to xi, of course, that's it. The polynomial becomes zero, or maybe m plus one. Then the m plus first derivative is zero, and after that, there's nothing more to say. It's kind of a mirror. So these functions here are prototypes for this definition. And it's a miracle that the one property of differential polynomials that we've captured here, namely that when you differentiate them, they get better behaved, they don't become less quickly increasing, less increasing at infinity. It's a miracle that that alone is as codified in this definition, is enough to create a reasonable theory.
01:19:54.364 - 01:21:11.914, Speaker A: Yeah, you can do this for arbitrary alpha. So the condition is that for every alpha and beta, these are multi indices. Of course, there should exist a constant such that this also, okay, yeah, this definition took those a long period. Well, they probably, I can't remember, talked about a small subclass, which are the so called classical operators, in which this a has a sort, has an asymptotic expansion. I can't remember whether they include that in, in their papers or not. But yeah, this definition, as it's written here, is straight out of textbook volume three. And it's, it provides either exactly the same operators that you see in ethereum singer, or a slightly more, slightly bigger class, which we will also narrow down in due course.
01:21:11.914 - 01:23:00.224, Speaker A: Let me see, what do I want to say? Now I can tell you what the class is about. It seems like it's about a sort of quixotic or idiotic quest, which is to find or analyze or let's say, use the word fine, develop the theory. What's the better way of saying it? Of course, the goal of the class is to talk about the tangent groupoid. This is the sub goal that we'll achieve, we hope first with that. So we want to construct the entire theory of pseudo differential operators while suspended upside down, immersed in gigantic barrel of water. Without using the Fourier transform, why would we want to do this? I mean, there's nothing wrong with this theory. The definition here is easy to motivate because it comes from the theory of differential operators.
01:23:00.224 - 01:23:50.434, Speaker A: And after you've made the definition, you have to ask yourself, can I do what needs to be done? Can I understand, for example, what happens when you compose two of these operators? Is it, again an operator in this class? And if so, what is its symbol function? You need to answer questions like that. But all of these questions have beautiful, concise, and really not that difficult answers. So the whole story is quite easy to tell with the Fourier transform. And what we're going to do is repeat the whole story without mentioning the Fourier transform. So what's the deal? Do we have something against Fourier? I mean, you know, I like Laplace, but maybe, maybe I didn't tell you yet about Fourier. What I think about him, the deal is that there are contexts. Well, first of all, it's an interesting intellectual exercise.
01:23:50.434 - 01:24:40.200, Speaker A: There are two ways of studying operators. There's the way down here and the way up here. There's the way which involves integral kernels, k of x y, maybe now for k of xy's, which have some singularities, like we saw in this example of the logarithm function, has a singularity at zero. You can study operators, all of the operators that you should like to study directly by studying their integral kernels. Or you can use this Fourier transformers we sort of done here, and study them downstairs, mostly using Homelander's formula, formula of Holmander and Cohen and Nahrenberg and so on. So it's interesting to, you'd like to know the full story upstairs and downstairs. What happens, I guess, upstairs is that the characterization is surprisingly different.
01:24:40.200 - 01:25:43.076, Speaker A: We won't characterize just operators by anything remotely resembling a formula like this. We won't say that the collection of integral kernels of these operators, Schwarz kernels, if you know the lingo, is a collection of distributions satisfying, blah, blah, blah, just like this thing here, will characterize it in a completely different way. So it's interesting that some completely different story gets told. You begin to feel a little more sympathetically about, or you see in a different way, all of these interventions by smoothing operators and so on and cut off functions, they appear in a different way in this theory, which is rather interesting. And finally, there are many contexts in which this Fourier transform is just not available. There are many operators which don't particularly obey the symmetries of the group Rn, the additive symmetries of the group Rn, but they do obey the additive symmetries of, not the additive symmetries, the symmetries of some other lie group. And for those, it's not appropriate to use the usual Fourier transform.
01:25:43.076 - 01:26:20.290, Speaker A: And it turns out not to be possible to use the versions of the Fourier transform that the other versions which are out there. Quant Charles formula, which exists for many, many lead groups. And so for those operators, it turns out to be very beneficial to have a perspective on pseudo differential operators which doesn't involve the Fourier transform. So let me try and summarize. In the remaining two minutes. We'll be studying operators first of all, and for the next couple of weeks, probably just operators in RN. But obviously we have ambitions to work on manifolds.
01:26:20.290 - 01:27:17.824, Speaker A: As is fashionable, we want to study operators on RN. We're studying operators which help us understand the properties of differential operators. For example, as illustrated by this lemma here, we want to do it in a way which doesn't mention the Fourier transform, not because we're prejudiced, because against the Fourier transform, but because, first of all, it's an interesting intellectual exercise. The answer as to what these operators are is arguably simpler than what you see up here, although the more I look at this, the happier I am with it. It's really not that complicated at all. But also the approach to operators, the very same operators that we're going to develop, has the advantage that it applies in a wide variety, a wider variety of context. I didn't give you the magic ingredient, but I'll just write down one more thing and then we'll stop.
01:27:17.824 - 01:28:07.560, Speaker A: But the magic ingredient is something like this. Suppose you take the logarithm function and you rescale it in this way by some positive number lambda. Then what you get back is the logarithm function. Well, pretty much. And so what puts the logarithm function into this story is the fact that it's basically scale invariant in this way here. It's not exactly scale invariant, a little bit off, like this guy here. So, for example, if you take the logarithm function of lambda x.
01:28:07.560 - 01:29:20.004, Speaker A: Now let's study this thing up here. I'm going to forget about the sigma. So we have some sort of cutoff function like this, and you rescale this function, then what you get is just the function you started with, plus some nice smoothing function. So it's not exactly scale invariant, but it's scale invariant modulo smooth, compactly supported functions, which, if you think of them as operators, are smoothing operators under convolution are smoothing operators. And so we shall take this guy here as a starting point to try to develop in a surprising way, not due to me. So that's wonderful things about it. Try to develop in a surprising way this theory of these operators, so as to prove theorems like this theorem here, without ever mentioning the wicked name of Fourier again.
01:29:20.004 - 01:29:38.376, Speaker A: All right, we are done. Gosh, I talked for an hour and a half. It's over for today. We'll meet on Thursday if you're interested. Any questions from. I keep forgetting it's the same question, which I can't make go away, but. Any questions?
01:29:38.520 - 01:29:39.764, Speaker B: Sir, I had a question.
01:29:40.064 - 01:29:40.984, Speaker A: Yeah, please.
01:29:41.144 - 01:29:54.984, Speaker B: The thing is that we are taking the course for credit. So in that case is. So it's optional. It's still optional, right. The work, the homework and other stuff. Other thing.
01:29:55.604 - 01:30:36.530, Speaker A: So if you're taking this class for credit, then you need to negotiate with whoever's going to give you the credit as to what you have to do for the class. So you need to make contact with officialdom. And presumably officialdom will then make contact with me and will come to some agreement about what you have to do. The way I like to teach is no homework because I hate more than anything else in the entire world, grading homework. So I will minimize the work for me. That's my goal in life. And if official them.
01:30:36.530 - 01:30:41.034, Speaker A: If your officials say you don't have to do anything, then that's fine with me. You don't have to do anything. 90 seconds.
01:30:42.254 - 01:30:55.434, Speaker B: I have already informed the university, so I think they won't would be fine with just sending a final grade without. I mean, I have just informed the university. They'll let you know.
01:30:56.534 - 01:31:18.120, Speaker A: Yeah. I recommend, if you are interested in taking this class for credit, first of all, you contact me and then you ask your graduate chair to contact me as well. Just so we all agree what's happening. But as for actually doing homework, this is up to you. The more you want to learn, the more homework you should do, as is always the case.
01:31:18.232 - 01:31:21.684, Speaker B: But it's still just. I mean, it still remains optional.
01:31:25.144 - 01:31:43.520, Speaker A: As far as I'm concerned. Yeah, it's optional. If you want to sit in the class and do no work at all, okay, that's fine. I mean, it just means you won't learn the material, but I don't see why you can't get a grade for it. That's, you know, we're all grown ups, right?
01:31:43.592 - 01:31:49.016, Speaker B: Thank you. So it's fine. It's fine if I sit in the class and just understand? Yes.
01:31:49.120 - 01:32:21.612, Speaker A: Yeah. Yeah, and I'll see about erasing the last two or three minutes of the video. It's out there on the Internet. Yes. Okay, anything else? Any questions? Comments? You have any references? References? Yes. So what we're doing is what we are going to do is follow the paper of Bob and Eric on I can't remember what it's called. The groupoid approach to pseudo differential calculi.
01:32:21.612 - 01:33:02.514, Speaker A: So that's the paper we'll be studying first without the groupoids. I do have some notes and so I'll be supplying notes, but in particular in my crude, incomplete notes I'll put the references to keep looking at the wrong thing. I'll put the references to Eric and Bob's paper and some other relevant papers. Got a reasonable answer? Yes. Thank you. Okay, anything else? I don't see how. Yes.
01:33:02.514 - 01:33:42.024, Speaker A: So my hope is that by the time we've gone through this theory without mentioning groupoids you'll see that there's a burning need to put a groupoid into the picture because it's going to make everything so much simpler. And it will not involve paths except in some formal sense of paths specified just by a beginning point and an endpoint. So by the time we get to groupoids you'll be irritated with me and asking why didn't I start with groupoids? Because it'll seem so natural and inevitable that we should have been talking about groupoids from the very beginning. That's the hope.
