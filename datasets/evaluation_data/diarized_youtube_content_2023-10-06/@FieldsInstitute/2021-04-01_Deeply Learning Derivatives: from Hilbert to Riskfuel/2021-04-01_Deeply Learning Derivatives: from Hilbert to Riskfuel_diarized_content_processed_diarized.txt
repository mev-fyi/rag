00:00:00.120 - 00:00:39.994, Speaker A: This little image here is just a teaser for things to come, but let's just dive right in. There we go. So I want to kind of focus our discussion around a recurring theme for today. And this is a really old and human theme. I think the idea is we always have families of problems that we want to solve, and typically we have a slow method to compute individual solutions. And the run on question here is, can we represent the solutions operator as a composition of simple function? And the offshore of doing this kind of thing is that you get to bifurcate time and accuracy. So we're going to be visiting this theme at various periods in time and culminating in how risk fuel is actually just the most modern incarnation of this idea.
00:00:39.994 - 00:01:13.070, Speaker A: So let's go way back in time. So I'm going to start at about 2000 bc, and around then, probably, the cutting edge technology is looking at things like quadratic equations. So over here, I have this quadratic equation, and I've written it in a little bit of a funny way. And the reason I've written like that is because I really want you to think about this as a solutions operator, from a family of problem to a space of solutions. So I've written it kind of like a map. Basically, any quadratic equation with its three parameters spits out the two solutions. So here I'm thinking over the complex numbers.
00:01:13.070 - 00:01:32.212, Speaker A: Or in general, you could restrict to the reals. And of course, there's multiplicities. So the idea is we don't even need a quadratic formula. Basically, all of these equations, they have their solutions. So you can just think about the map that goes from the equation to the solutions in one step. But, of course, we all know that we don't really do it that way. So here's a more concrete version of this operator.
00:01:32.212 - 00:02:27.294, Speaker A: So we have the, if I write z squared plus q equals zero, then you all immediately think, well, of course, in that case, I know what the two solutions are. So I can really think about this as spitting out the positive and negative square root of minus q in this case. And this idea really started with the Babylonians. And so they were the ones who realized, hey, I can realize this general operator upstairs for the full quadratics as a tower of simpler functions. So you can really think about this as a composition of simple binary operations or unary operations, where you're just adding, subtracting, multiplying, dividing, or taking a square root. And of course, you get to bifurcate time and accuracy, because you could have, say, a table of square roots and then using this decomposed technique as a tower of simple function, you can very easily solve quadratics on the fly. So this is the first version of this late motif which is represent a solutions operator as a composition of simple functions and use that to bifurcate time and accuracy.
00:02:27.294 - 00:03:10.310, Speaker A: So let's go way almost 2000 years ahead and it's still the same idea. So there's this kind of forgotten art of mammography. And the idea was to make this idea of precomputing hard binary operations into something you could actually do on the fly, on the go. So I mean, stone tablets are kind of heavy to carry around, but if you have a piece of paper, you can carry on these gadgets called nomograms and they allow you to do fast inference. So here I've got this funny looking graph which kind of looks like a quadratic but not really. It looks upside down and kind of wrong and it's not quite what you think. So here the x axis is really p, which is the parameter of the x variable in the equation and q is the y axis.
00:03:10.310 - 00:03:54.554, Speaker A: And the idea is, if you're trying to solve this quadratic where you've kind of made it manic in a sense, you just go on your p axis and you find your p dot. On your q axis, you find your q dot and you just draw a line and the two oops, and the two intersection points are the solution. So if you were to zoom in, you can't really see here it's quit, but there's a little two there and a minus something. And so the idea is that by realizing solutions as collinearities, you can actually build these little graphic solvers for equations. Now you could take this and chain it together so you could have a piece of paper where you have a bunch of nomograms one after the other. So you have, say, I don't know, four different binary operations that are very hard to solve. And you could have this little thing that you carry on in your pocket that could actually save your life.
00:03:54.554 - 00:04:44.754, Speaker A: So in this case, actually this is a nomogram to kind of compute the intensity and speed of spread of a forest fire. So presumably before a pocket calculators were a thing, this is what you'd have with you on the field when you're trying to decide if you need to abandon ship and run or keep fighting the fire. And once again, it's still the same idea over and over again where you represent the solutions operator as a composition of simple function to bifurcate time and accuracy. Now, I thought I was going to be really cool and kind of do this for something in finance to show you guys, something that hadn't been done before, but turns out that Elroy dimson beat me to it. So this funny graph you have here is a nomogram for black scholes. So we have kind of four different quadrants, and one of the first starting point is the axis of maturity. So let's take a concrete example and just kind of walk through it because it's kind of cool.
00:04:44.754 - 00:05:32.360, Speaker A: Let's pretend you had maturity eight months, interest rate 10%, fall 60%, and normalized spot 130%. So if you look in this diagram, I've kind of circled the relevant quantities, and the idea is you rebuild black shoals as a tower of simple operations. You start with the maturity and then you draw lines until you meet the curvy ones that represent both interest rate and volume. Then you will cross to your normalized spot, and then once you're not there, you find the join of those two lines and voila, you figured out the answer 34. You don't need to know any mathematics. You can literally just use this on the trade floor pre calculator, say, and figure out your answers. But it's actually a little bit cooler than that even, because you get easy sensitivities and easy implied fall.
00:05:32.360 - 00:06:15.668, Speaker A: So what do I mean by that? Well, you have this graphic solver, and you can easily just wiggle the lines and find a different solution. So here you can find the delta pretty easily, and you can also find the Vega pretty easily just on the fly. And so, of course, nobody actually uses this in practice, but I think this is a good illustration. Once again, this principle that if you can decompose things into simple things that can be pre computed, you can get a lot of mileage on the fly. So this was just a very, I guess, long segue to Hilbert's 13th problem. And so Hilbert, you know, then as now, he also wanted to bifurcate time and accuracy. And we know him as a very pure mathematician, but actually he was concerned with conducting ambitious engineering projects.
00:06:15.668 - 00:07:12.404, Speaker A: So little known fact about the 13th problem, but the way he sold it was actually that this was the critical technology behind things like the french railway railroad system, where they needed to do a lot of cut and fill operations that were very difficult to do. So I like to call this, actually Hilbert's 13th nseric application, because he was already then very in tune with the idea that you need to justify your existence to the world by concrete applications at every step and so what he actually observed was, there's a very nice class of functions for which you can build these nomograms. I had, basically, the rules of the game are, if you can break something up into a bunch of operations, one after the other, where each of them only uses the most two inputs, you can build nomograms. And of course, he wasn't thinking of computers. And so this was basically the state of the art of the things that are tractable by humans. And so this class of maps he was concerned with, where the algebraic maps of, at most two parameters. So, I mean, that's kind of fancy lingo, but basically you don't square root, that's an algebraic map of one parameter, namely the square root of.
00:07:12.404 - 00:07:47.194, Speaker A: What are you taking? And going back to the earlier equations I had, you can take a cubic and just let one of the things vary. And that's a thing that spits out three numbers. That's a map of one parameter. So here you can think about this blue line as being the parameter q varying. And for each of those queues, you can spit out the three roots. And sometimes they collide similarly with quadrics and four roots and so on. So basically, his challenge was, are there any limitations to this approach? Namely, are there problems that we could never, never solve using these kinds of techniques? Which was the best he knew at the time.
00:07:47.194 - 00:08:30.708, Speaker A: So he actually said it in a very concrete way, in terms of some family of polynomials. But for us, we'll take it a little bit more broadly. So this is perhaps a hand wavy take on Hilbert's 13th. But I think it actually really captures the spirit of his question and the way he meant it to be understood. So there's these two guys, Kolmogorov and Arnold, and they kind of, they tongue in cheek misinterpreted his question to say, oh, can you do this for continuous functions? And they prove this kind of counterintuitive statement that says that there's no such thing as a truly multivariate continuous function. So when you write it like that, it kind of sounds absurd the first time you see it. So let's do just two concrete examples to think about.
00:08:30.708 - 00:09:00.634, Speaker A: You might say, hey, multiplication, that's definitely a multivariate function. I mean, I've got two things and I'm multiplying them together. Or even here you could take xyz goes to x to the y over z. That definitely does not look like a single variable function. And their observation is those are really secretly nested compositions of single variable functions. And in fact, it's a bit of a white lie, but there's only one multivariate function, which is addition. So if you think about those two, this is how you do it.
00:09:00.634 - 00:09:18.240, Speaker A: If you wanted to just do it in practice, you could use exponentials and logs. And so let's just work through the first one in detail. You have log x. Well, that's just a function of a single variable, namely x, log y, single variable y. We're adding the two things together. Fine, that's a two variable function, but it's the only one. And then exp of the whole thing.
00:09:18.240 - 00:10:08.634, Speaker A: So somehow you've managed to sneak your way out of having to actually multiply two numbers by using only addition and single variable functions. And for this other one, you can kind of do it also. And the crazy thing they proved is that they're able to do this for all continuous functions at once. So the formula here is not all that important. But basically, I want you to think about this as a two step process where the inside the pink box is actually this universal thing that does not care at all about the specific function you're looking at. And then you're kind of summing up over some other continuous function, capital phi. And so this is kind of a statement about the fact that there's really not that much complexity in terms of decomposing continuous functions, or conversely, that continuous functions are so wild that you can capture arbitrarily complex things using them.
00:10:08.634 - 00:11:00.622, Speaker A: So this is kind of the way I think about it. You know, basically any function, you can split it up into some universal change of coordinates and some tailored piece that actually depends on the specifics of the function. And I really want us to rethink about this statement, as every function that's continuous can be decomposed as a touch of nonlinearity and a good dose of linear algebra. So, to clarify this a little bit, what I mean by just a touch of nonlinearity, well, there's only this little fee, which is fixed once and for all for all functions in the entire universe, and a single other continuous function, capital phi. And the rest is just adding, scaling and shifting, which are basically the operations of linear algebra. So this is kind of a meta statement that says that you should be able to have at it with any function using only those kinds of tools. But now, of course, there's a lot of people like to poke at this theorem, and, you know, in theory, there's no difference between theory and practice.
00:11:00.622 - 00:11:59.774, Speaker A: So in theory, we know we can do all functions like that, but, you know, in practice, there is. So this is the prelude to, you know, we have Hilbert's 13th. Well, what's riskfuel's first problem? So what are we doing? We're kind of reapplying the same pattern, but in a more modern perspective. So we're going to be thinking about things like a family of partial differential equations. And I really mean family here in the sense that we're not fixing the coefficients. So, I mean, if you had a flat volume, so you have a single sigma, single interest rate, no structure there, you would allow all possible values of the volume itself, and you want to solve all those at once, kind of like the quadratic, where we let the parameters vary as we wish, and then the other thing we want is, you know, we usually have our method, in this case, typically slow and noisy, to compute individual solutions. And more than that, you can easily augment this to say, well, I'm going to let these volatilities depend on time and space.
00:11:59.774 - 00:12:39.004, Speaker A: You could have a nice interest rate structure. You can augment the dimensionality in many different ways. And at the end of the day, all we care about is we need a family of problems. Maybe it's some conditional expectation, maybe it's a PDE with exotic boundary conditions. We don't care. And we want some form of this operator o which takes in coefficients of some sorts that could be functions, perhaps like in this case, and spits out individual solutions. So, given that our goal is to find a mesh free representation of this solutions operator, and inspired by things like Kolmogorov, Arnold, we're going to want to use something like a simple nonlinearity and a good dose of linear algebra, and we'll get a little bit more into why you might want to do that in a bit.
00:12:39.004 - 00:12:59.508, Speaker A: And of course, spoiler alert, yes, we can do it. And it results in lightning fast inference, and you get lots of free sensitivities on the fly. So maybe this is a good point to pause in case there's a question. If not, I'll just resay my late motif, which is where?
00:12:59.556 - 00:13:01.144, Speaker B: Don't see any in the chat as yet.
00:13:01.924 - 00:13:10.624, Speaker A: That's great. So, can we do this? And it's the same idea over and over again. We're decomposing functions into simple compositions.
00:13:11.724 - 00:13:20.600, Speaker B: Actually, one just appeared, which I think it might be useful to address this now. So the question is, what do little Phi and big Phi look like on the previous slide?
00:13:20.632 - 00:14:01.878, Speaker A: I guess the answer is they look insane. And maybe to give you a bit of the flavor you can think about, how do you represent a two dimensional array in a computer. Really what you're doing is you're taking each row and you're storing them one after the other by some given offset. And the idea is you can take two dimension, imagine a discrete two dimensional function, and then you start playing this game, trying to represent, encode it as offset values in some single one dimensional line. Of course, you say that doesn't quite work. And the idea is, well, you know, when you hit that hard enough and you start going a little bit fractally, it does work. So it's not, I mean, people have made that construction explicit.
00:14:01.878 - 00:14:45.902, Speaker A: But I really don't think the takeaway here is that you should be trying to construct this little phi and this big Phi. You should just take this as inspiration to say that there is a way to get equality on the nose. And what we'll see in a second is, in fact, you should be willing to give up this equality to make this constructive. So the answer is, they look very fractally and crazy, but they exist. So the approach that we actually end up using is deep neural nets. So I guess I'll, I don't know if anybody's going to be learning for the first time what they are, but the way I think about them is a deep learnet is basically this touch of nonlinearity and a good dose of linear algebra. So the linear algebra comes about in terms of affine transformations.
00:14:45.902 - 00:15:11.394, Speaker A: So an affine transformation, I mean, that's just a linear transformation. So a matrix. And then you allow yourself to shift by a translation, that's affine transformations. And then you apply point wise, your favorite non polynomial polynomial function. So that's the nonlinearity in this case. So here's a nice example of the most archetypal non polynomial function, which is the rail you. So basically it's two straight lines, so it's zero until zero, and then it shoots up as y equals x.
00:15:11.394 - 00:15:42.694, Speaker A: So nothing fancy here. And the idea is you just compose these together over and over again to get some map. And the game is going to be to try and see if we can get something close to any original function by playing this game. So I've written it in composition, function, compositional notation. But you can also just, if you prefer maps with arrows. Basically, the idea is you start with rn, you do affine, nonlinear, affine, nonlinear, and so on, all the way up to your output. And I'm restricting to a single variable function.
00:15:42.694 - 00:16:17.384, Speaker A: But I'll let you use your imagination to see how this generalizes easily. And this is kind of a lot to look at. So I tend to think about them just like this. So here's the most basic way to imagine a neural net. You have your dimensional of your dimensionality of your inputs, and then these black arrows here basically do this linear transformation and apply that nonlinearity and pass to the different size vector space and so on. So that's all neural net deep neural nets are. And you might be hoping, given something like Kolmogorov Arnold, that you can get close enough to some function by using those kinds of gadgets.
00:16:17.384 - 00:17:12.716, Speaker A: So, here's a meme I really like. But basically, if you look at the previous slide, you see, well, it's just multiplying matrices together. And so here you have one astronaut who was realizing after looking back at the earth from above, that all these long term, short term memory cells, any kind of crazy neural net lingo you've heard, it's all just matrix multiplication. And of course, once he's realized that, he can't be allowed to live on. So it's a bit of a joke, but basically, any neural net you've ever heard about can be written in this way, so you don't actually need to know anything beyond that. Okay, so the real nets, you're just mapping together basically matrices and some non polynomial function, and you might be saying, well, why are you doing this? So let's see what the upshot is. So basically, if you're trying to represent some function, the cool thing here is that you actually have a kind of algorithm to try and find that best possible representative.
00:17:12.716 - 00:18:08.444, Speaker A: So the idea is you had that slow and noisy method to compute individual values of this function, and then you can use gradient descent to nudge the weights and biases of the affine transformation. Or you can think the entries of the matrix that represent them to find the combination of all those maps that will give you the lowest error. So you're moving around here in the affine transformation space, and you're trying to find your best possible representative so you can do some stochastic gradient descent there. The other upshot really is that the simple structure here, namely that it's just matrix multiplication, means that you can do things very, very efficiently on GPU's. GPU's were initially built for gaming, and so they're very good at, I guess, shading things when you're looking for some kind of animation. And it turns out that those very same operations are extremely good at doing this kind of game. They're rigged to do well on GPU's, so much so that actually nowadays, they build GPU's specifically tailored towards this application because that's a big part of their business.
00:18:08.444 - 00:18:44.940, Speaker A: So if it works, this is what tells you that this is actually a pretty good idea of a thing to do. All right, so I've given you the construction we're going to use. Namely, we're going to be combining those affine maps and those nonlinearities. We're going to have at it forever, and we're going to hopefully find a good representative of our function. Let's talk a little bit about theory. You know, is it even possible to do in the first place? So, here I have my deep neural nets in theory, with an exclamation interrogation on the deep. Why? Because I'm talking about a single hidden layer of unbounded dimension.
00:18:44.940 - 00:19:28.396, Speaker A: So here's a simple, simple neural net. You just go from rn to rm, back down to r, so pictorially inputs very big middle layer, and then you're going straight to your outputs. And the kind of insane result here is that you can approximate an arbitrary continuous function uniformly on compact sets by using this kind of get. So you might say, hey, that sounds kind of too easy. And of course, the catch is that the size of this hidden layer grows without bound. And in practice, you can't really quite use that, but you can use this as a guide, kind of like Kolmogorov Arnold, in terms of how you're going to want to think about those neural nets. And there's this other nice result that I like, is that you can control the error in terms of the number of neurons.
00:19:28.396 - 00:20:06.304, Speaker A: So the size of this hidden layer and the number of known function values. So kind of, you have two parameters that you have at your disposition, and you can choose to evaluate the function in more points or build a bigger net. And using those two gadgets, you can kind of get some control over the error of the approximation of your function by this neural net. So here's just a. I guess you can look up in the video if you really want the sources, but. So we got Sibenko Hornic, Baron Hornic, white pincus and a bunch of other people. And basically this result, I'm just saying, is kind of a compilation of those various papers.
00:20:06.304 - 00:20:39.360, Speaker A: So that's kind of, and I call this deep neuralism theory because, you know, a lot of people will say this is a cute result, but it's kind of useless in practice because it doesn't necessarily tell you how to do it for the original problem we care about. So let's, let's, let's look at another version of the theory behind deep neural nets. So we had a very, very shallow neural net with a very, very wide layer. Let's play the opposite game. We can look at neural nets with a fixed width. So I'm not allowing the internal layers to grow at all. In fact, I'm only going to let them go maybe one or two dimensions bigger than the input.
00:20:39.360 - 00:21:18.962, Speaker A: And then once again, you have a companion result which says that you can approximate an arbitrary continuous function on compact sets using those. So you fix the width and you let the layers grow without bound. So what's the game we want to play? Well, I mean, I'm allowed to do whatever I want in terms of width and depth. And we have those kind of two results that say that you can go one route or the other and you kind of want to interplay between the two. So shallow networks, they need exponentially more neurons than deep ones. And that kind of gives you a hint that you want to go maybe wide and somewhat deep. So just to drive this home, there's actually some cool examples where if you try to use just two hidden layers, your net is going to look something like this.
00:21:18.962 - 00:22:07.274, Speaker A: And with three, you can do something like this. So I'll give you about 10 seconds to try and think about what kind of function you would end up with this kind of crazy dichotomy between the two. And maybe I'll let you give me your guess later. But basically there are some functions you can construct where you can show that you're going to be stuck like this. If you use only two layers and using three, you're going to have something more like this. And here are various papers that compile together, give the results on the previous page. So all of that to say that we have this theoretical foundation which gives us not exactly the results we want on the nose, but it gives us the hints and the tools that we want to use to tackle the real problem.
00:22:07.274 - 00:22:27.714, Speaker A: So in practice, all those results, you don't know what the function is, you don't get to peak. So they're very mathematical in nature. You're given some abstract function, you don't know anything about it, and you're trying to build a neural net that's going to have to work for it. In practice. That's not how it works. In practice. You know exactly what function you're after, you know what it looks like, you know how it behaves.
00:22:27.714 - 00:23:07.282, Speaker A: And so you should use this to your advantage. And from this point of view, there's really three main things I want to highlight that matter a lot. The first is the architecture. So that's things like how deep and how wide you want to go, or even fancier things like restricting the shape of the matrices to have certain block forms. The training data matters. So how many points are you going to generate if you have a slow and noisy method? Are you going to dial up the accuracy way up and have less points, or are you going to get some really, really bad points and lots of them? And the third one is the training method matters. Here I have three pictures of some low dimensional functions that are meant to try and give you an idea of what could be happening in high dimensional space.
00:23:07.282 - 00:24:01.622, Speaker A: And basically, if you're trying to do some kind of gradient descent to minimize a function here, here or here, you're going to have to do very different flavored things not to get stuck. So, recapping architecture we have with depth, special matrix forms, the training data, you know, what's the quality, what's the quantity, and where are we going to put those points? And then the training method matters. Stochastic graded descent, that's really just the beginning. And there's a whole industry of ORC in order to fine tune that so you can get better results. And what we do at risk fuel is really kind of play around with the nonlinear interaction between those factors and a lot more to make this work. So let me maybe go a little bit deeper on each of those points. So here's a slide I like a lot, and a result I like a lot, actually, because in our kind of applications, actually, we do have control over the data, so we can generate lots of it.
00:24:01.622 - 00:24:47.674, Speaker A: But in some of the applications you may have heard about in the real world images, for instance, the number of training points or the number of known function values is much smaller than the number of free parameters in your neural net. Classical statisticians would look at that and say, well, of course you're going to overfit like crazy, but in practice they don't. And this paper here called frequency principle Fourier analysis sheds light on deep neural networks, was about trying to explain why that happens. And the phenomenon that shows up is that deep neural nets, they love bass. They fit functions from low to high frequencies. And so if you have a very noisy signal, it's going to see through the noise and average it out. Why? Because it's not going to be offset by the variations in the noise.
00:24:47.674 - 00:25:15.564, Speaker A: Here, maybe I should tell you what you're looking at. This blue line here is some kind of a scaled sign curve, and that's the truth. That's the true signal that we used. Now we're going to pretend we don't know the truth. And those little blue dots are going to be some training points that we're going to generate using a noisy algorithm. Basically, we're going to start from the known value and we're going to bump it by some noise. Then the red line you have here is actually the result of a very simple neural net trying to fit based on that data.
00:25:15.564 - 00:25:54.120, Speaker A: You see, it's not doing so great here, but it's doing better than you might expect based on how far off all those data points are. As you add more noisy data, it starts to get a better fit. And in the end, you've added a ton of really, really bad data, but you're still getting a fit on the nose. This is a hint that noisy signals, they're not a problem for neural nets. This leads me into the second thing, that quantity beats quality. Since you know that the neural net is not going to be affected by noisy signals, you might use that to your advantage. So here I have three different shots at learning.
00:25:54.120 - 00:26:32.954, Speaker A: In the first two, I'm using very, very, very accurate points, so there's just a teeny little bit of error, but less of them. And in the last one, I'm really turning down the accuracy all the way down and getting lots of them. So the compute cost to generate the training data here is the same in all models, but we see that somehow the worst data points, where you got more of them, that actually works out better for you. So that's something to keep in mind. And the third one is you want to be smart about data placement. So especially in finance, we tend to have these jump discontinuities. And so on the left hand side here, we're placing some training points kind of at random, and we see there's no point really up at the edge of the jumps.
00:26:32.954 - 00:27:11.496, Speaker A: And so the neural net, which is in red, is struggling to fit the true signal. On the other hand, if you have points that are well spaced and go right up to the edges and nooks and crannies, then nailing the fit is no problem. This is where you really want to use domain knowledge to your advantage. If you're an expert in quantitative finance, you can actually use that expertise to help you train better neural nets. All of that was the background, so I thought I'd show you guys some results. Let's see how this behaves when you try and do things like exotic options. So let's do the classic archetypal exotic option.
00:27:11.496 - 00:27:47.204, Speaker A: So we have the european option, so you can exercise the expiry. Only American allows you to exercise anytime. So those are two early exercise feature and somewhere in between we have Bermuda and we get to exercise at any predetermined time. So we're going to have a set of dates between inception and expiry where you're allowed to call whatever you had. More specifically, one of the problems we looked at is the Berms bermudan swaption. The idea here is it's a Bermuda style option where you have a series of contingent options on different underlying swaps. You get to call at different points in time.
00:27:47.204 - 00:28:33.566, Speaker A: The interesting subtlety is that depending on the point in time at which you call, you're actually entering into a fixed versus float swap. That asset is actually different than those different locations in time. So it adds an extra dose of complexity. And these are interesting because it's actually probably one of the only exotics that has a actually somewhat liquid market because they're used by mortgage issuers to hedge prepayment risk. But of course, this kind of discrete exercise structure into different underlyings makes it extremely difficult to actually price this in practice. You're going to have to run some slow Monte Carlo and you can't really be doing this on the fly. What we actually did is that we took that slow and noisy pricer and we built a prototype of a bermudian swaption and it worked.
00:28:33.566 - 00:29:00.764, Speaker A: So we built one with 157 dimension of inputs. So this is a full volume surface, we have some interest rate curve, and we have a bunch of trait specifics like the strike, the non call period term. And then we use Quantlib because it's open source so people can go and benchmark. And we basically built a pricer for this asset in Quantlib. And we use that to generate some training data. And then we built a deep neural net that would replicate it. And what you're seeing on the left here is basically.
00:29:00.764 - 00:29:14.920, Speaker A: So you see here that the risk fuel, that's the deep neural net. We find a value like 252.2 for a specific trade. Quantlib will be a little bit off. 251.9. But the key thing is, you know, this took 0.2 seconds, whereas the Quantlib model is taking 7.7
00:29:14.920 - 00:29:38.146, Speaker A: seconds in this case. And I should add that while Quantlib was chugging along for 7.7 seconds, we actually computed all the Greeks. So here you have the bucket deltas and the bucket Vegas, which is something that you would never really have on the fly in a real trading environment. So that's kind of cool. And actually, maybe at the end, I'll show you guys a live demo of how to play with that. So this is a price that we have up on Riskfield.
00:29:38.146 - 00:30:02.574, Speaker A: So you can really play in and tweak all the inputs and see how it works in practice. And the second one I wanted to, I guess I lost my track of time. The second one I want to talk about is an FX double knockout partial barrier option. So that's another one that you're going to have a lot of trouble. You know, there's no analytic solution. You're going to have to do some slow Monte Carlo, perhaps even, you know, a couple rounds of Monte Carlo, a couple rounds of semi analytic. You've got your crank Nicholson going all over the place.
00:30:02.574 - 00:30:28.182, Speaker A: You got to really worry about that. So what's the, what's the underlying. Well, we have a, let's think of a call, say, and you start here, the blue dot, and then you've got these two barriers, but they don't start right away. So you've got the partial barrier come there. So, you know, maybe we have something like this. And we see here, you know, you poked right above the barrier, but you didn't get knocked out because it was before the observation period started. And then maybe we went on later and when we got knocked out there.
00:30:28.182 - 00:31:08.534, Speaker A: So that's what the double knockout partial barrier option is. And so we built one with 81 input dimensions, full volume surface interest rate curves, and some trade specifics. Once again, the cool thing is, when we train, we're able to really take our time and go and fill out the whole domain space. Typically in applications, people are very concerned with getting things to run fast using their very slow methods. What that means is that they have to cut a lot of corners, and they really need to tailor the model so that it cuts the right corners to get the answer. Maybe overnight, say. But we're doing all of that training upfront, so we get to fill out the whole training domain.
00:31:08.534 - 00:31:39.100, Speaker A: The models that we trade, say, before COVID the parameter ranges that we allowed in our family of problems was so wide that even Covid doesn't knock them out. You don't need to retrain. That is cool. I will show you guys a video to illustrate that on the fly in a second. And maybe my last slide before I actually show you guys some live demos. We think about deep learning and finance as having three main pillars at risk, fuel. So that the first pillar was, let's build those fast pricers.
00:31:39.100 - 00:32:20.304, Speaker A: Let's use those slow pricers that people have to build deep neural nets that can be used for lightning fast inference, and Greeks and everything else you might want. In some sense, this is a liberation of the quants, because there's always the model you want to have and the model you're forced to have, because the model you want is too slow. Maybe you want a full local stochastic volatility model in production. That's not going to work. But if you can run that slow model slowly over a big domain ahead of time, and then train a deep neural net to actually run that inference for you live, then you can have that so you can be as creative as you want in ways that you never even thought possible before. So that's the fast pricers. The second pillar is unsupervised learning.
00:32:20.304 - 00:32:52.524, Speaker A: So we have this paper called, I guess it's called, I forget what's called, hands off variational auto encoders. And the idea is, we're trying to figure out how to understand the shape of the space of volatility surfaces. To really get at this idea of the domain. A lot of people, I put up a vault surface, and they say, that doesn't look like a realistic wall surface. And my answer to that is, who cares? I've got a pricer that can price everything and anything under the sun. It doesn't care that the volume surface looks like nothing you've ever seen before. At this point, it's just mathematics.
00:32:52.524 - 00:33:27.244, Speaker A: The neat thing there, too, is you get to build these resilient models. We had this. We had this trial run where we trained on data that was from 2012 till 2020, just before COVID And that model learned to calibrate, fit all types of volume surfaces, but it had never seen a real financial crisis. And then when we dropped that model into Covid, it actually does pretty damn well. And I think that's a testament to the strength of these methods. And then we're progressing linearly. But the third pillar that we're going to be venturing into is now that we have those fast models, we have ways to really explore the space of possibilities.
00:33:27.244 - 00:33:47.884, Speaker A: So we're no longer looking back, we're looking forward. We want to enter to reinforcement learning. So deep hedging and so on. So that's that. Now, maybe I'll start with the pricer. So hopefully you guys can see my screen. So this is what you see if you go to pricer dot riskfuel.com.
00:33:47.884 - 00:34:17.400, Speaker A: and so you'll have. So here you see all the kind of inputs you have, the strike, the non call period swap term, and then we have some sliders to help you play around with. Some realistic or unrealistic interest rate curves, doesn't really matter. And a full volume surface, once again, with some sliders that you can play around with. And if sliders are not your cup of tea, we can actually, you can go in and just point by point, specify the value you want for your trade. And so, you know, here we have this cool button. You can just send a random batch.
00:34:17.400 - 00:34:27.472, Speaker A: Let's just evaluate a bunch of trades. You see here, risk fuel is done very quickly. So 0.2 seconds. Quantlib for 4.33. Still computing here. This one's still computing.
00:34:27.472 - 00:35:02.724, Speaker A: Okay, 5 seconds. And for every one of the risk field ones, we get the full grics. So we got the bucket deltas, the bucket Vegas. And I guess you can also view the trade inputs on the left hand side to see what random parameters it chose. And so I encourage you to go on there and just play around and see what's actually possible when you have those types of models at your disposal. And perhaps the last thing I want to show you, this is going back from the berm into the FX double knockout barrier options. So, here's the kind of thing you can do if you have a deep neural net and you're sitting there on the trade floor.
00:35:02.724 - 00:35:41.364, Speaker A: So what I'm showing you here, so this is going to be your p and l for some portfolio of a couple thousand double barrier options. On the right hand side, I've got euro USD, GPY, AUd, a couple other currencies. This is all just random data. So I'm simulating what could be happening to those FX pairs live in the market. And what you're seeing is really live pricing of that entire portfolio of a thousand of double knockout FX barriers. And you can see your full p and l based on each of the assets, each of the underlyings. You have, all your deltas, all your vegas live, including your full volume sensitivities.
00:35:41.364 - 00:36:20.064, Speaker A: And then PNL is kind of cool, because you can see. See where you're losing money, where you're making money. But the Greeks and the deltas are the more important one, because it's one thing to know that you're losing money, but if you're losing money and you see that you're getting unhedged, you want to know what you need to do in order to get back in the line during the live trading session of that day. I think this tool is something that most people don't really have access to right now. And I think that probably fast forward a couple of years, this is going to be the standard in the industry. So I guess while I let this play on, I'm happy to start taking questions at this point.
00:36:22.764 - 00:37:16.280, Speaker B: Okay, well, thanks a lot, Maxine, for very interesting talk. There has been a flurry of questions that came in just a short while ago, but perhaps I'll rewind back and start with. So, BYD bide, would you like to ask your question? Okay, I'm not sure if he's around or has access to a mic, but I will read it out for you. Not exactly sure which slide this is pertaining to, but the question says, can we see it as an approximation of a manifold in a high dimensional space? So he's referring to the universal approximation theorem or something like this.
00:37:16.392 - 00:37:51.308, Speaker A: Yeah, I mean, so there's this, you know, data in nature is not distributed uniformly at random. And so perhaps where this question is going is, when we're training and we're trying to place our data, we actually assume the worst. And so when you're using those things in the real world, yes. The data that you're seeing is actually sitting on a much lower dimensional manifold than what appears. So a full volume surface. So some of the results we found in our auto encoders, and I think this also ties into rough volatility work. You don't need 60 parameters to really understand how the volume is evolving.
00:37:51.308 - 00:38:03.194, Speaker A: And so secretly, a lot of those things are much lower dimensional. And so part of the reason why these things work so well is that. So maybe that's the best I can answer that question without talking to the actual questionee.
00:38:03.614 - 00:38:10.234, Speaker B: Okay, how about Paolo Maccado? Machado, would you like to unmute and turn on the video and ask a question?
00:38:15.854 - 00:38:16.246, Speaker A: Okay.
00:38:16.270 - 00:38:26.370, Speaker B: If not, I'll read it out. So, hello? Does that depend on the regularization parameter of the neural net? Oh, yes, we can hear you.
00:38:26.522 - 00:38:27.458, Speaker A: Okay, sorry.
00:38:27.546 - 00:39:00.080, Speaker C: Yeah, maybe that was, like, about the paper that you were referring to about the fourier coefficients, but on a related question, maybe more relevant to what you've shown afterwards. Yeah, it was about that, but more relevant to what you've shown later on. My question was, what happens to your method? Like, when you're, when your inputs are outside the range that the neural network has seen and has been trained on? Yeah, so the extrapolation.
00:39:00.232 - 00:39:42.244, Speaker A: So we don't have to extrapolate, right? Because we get to, you know, you might say, okay, I'm trading some asset volume is typically between 200% and zero. We'll happily train up to 400% so that, you know, when the day comes and the volume is shooting at 400%, you may decide, okay, now it's time to retrain. But the point is, you get to go way out of bounds ahead of time, so you don't actually get caught. And I think this is kind of this mind shift where most, most current implementations, people are worried about efficiency. And so you tailor your approach so that it works with the current market conditions today, when you're building a neural net, you can tailor it so that it works for everything today. And five standard deviations in every direction. That's not a problem.
00:39:42.244 - 00:39:58.604, Speaker A: Now, of course, if you go ten standard deviation, then you might have your fallback. So, of course, you still have your slow pricer. So if you're going ten standard deviations out, you just toggle. Okay, I'm switching from my neural net back to my monte Carlo. I'll have to wait till tomorrow for the answer, as usual. So it's kind of. You get the best of both worlds, really.
00:40:00.104 - 00:40:07.640, Speaker B: Okay. Up next, we have Kirim Ogurlu. Would you like to unmute yourself and ask the question?
00:40:07.832 - 00:40:25.726, Speaker D: Yeah, ma. Hello. Thank you for the talk. My question was very, is actually this light? I mean, while you were talking it, I just was curious whether you put, I mean, for the noise, any assumption, like a probabilistic distribution, like a uniform distribution.
00:40:25.830 - 00:40:33.114, Speaker A: Yeah. This is just a gaussian noise, but basically any noise is fine as long as it's unbiased. So if your noise is biased, then you want to account for that.
00:40:34.174 - 00:40:38.374, Speaker D: Okay. But the gaussian means that it is unbounded. So you don't make like.
00:40:38.454 - 00:40:39.430, Speaker A: No, you don't care.
00:40:39.622 - 00:40:40.862, Speaker D: Okay. All right. Thank you.
00:40:40.878 - 00:40:45.598, Speaker A: But, I mean, of course, in practice, you do have bound, let's say, if you're doing Monte Carlo, you know how bad your noise is, right?
00:40:45.726 - 00:40:48.742, Speaker D: Yeah. So that's why I was asking. Thank you.
00:40:48.838 - 00:40:49.674, Speaker A: Thank you.
00:40:50.294 - 00:40:57.074, Speaker B: Okay, uh, Thomas Obitz, would you like to unmute and ask your question? You can turn on your video as well.
00:40:57.534 - 00:41:35.754, Speaker E: Well, you mentioned that for Bermuda training network in 157 dimensions. Now, in a one dimensional problem, it's all fairly straightforward to construct an approximation, but typically, where you are running into issues with all of these quick based methods is in higher dimensions, because you need a lot of quick points and takes whatever you generate, a price industry. So how many price training points are you actually? How long does that take to.
00:41:36.094 - 00:42:04.004, Speaker A: So, in all those cases, we tend to think about grids, but we don't think of grids. This is mesh free. So the idea is you want to figure out where you want to expand your compute budget. And for the Bermuda, I think you're going to want quite a bit of points. But of course, if it takes a year to generate the data, it's not useless. So when we first put those kinds of things out, some people are like, this will take you 64 years to generate the data. And it's like, well, clearly it didn't because we have it right here.
00:42:04.004 - 00:42:17.084, Speaker A: So I would say a reasonable upper bound is if it's taking you more than a week to train your data for your neural net that you're going to be using for the next year, then maybe that's not a good trade off. But so far we haven't met any kind of problem that we can't tackle with that kind of approach.
00:42:20.304 - 00:42:27.974, Speaker B: Okay. Up next, Harriet Tapsat. Would you like to turn on your video and or voice and ask questions yourself?
00:42:30.714 - 00:42:49.490, Speaker F: Hi. Great talk. Maxim, really loved it. So I had question regarding training, and I think to some extent it was answered in the previous question, but just curious about the fact that how do you take care of all the corner cases? Because when we generally price any derivative, I mean, there are a lot of corner cases out of the money. In the money.
00:42:49.602 - 00:42:52.014, Speaker A: Yeah. So move your corners further.
00:42:52.994 - 00:42:57.574, Speaker F: So will that lead to like quite a lot of data to train?
00:42:57.994 - 00:43:09.254, Speaker A: Yeah. So, I mean, you can generate as much training data as you want. The point I was trying to make with those kinds of slides is, you know, you're a quant, you know what's going on, put the data where it needs to go.
00:43:10.874 - 00:43:18.016, Speaker F: Yeah, I get your point. So you are generating the data from some model. It's not coming from the market directly, right?
00:43:18.130 - 00:43:38.852, Speaker A: No, no. So maybe that's a good point. So we do not use historical data. So a lot of people tend to think that, oh, yeah, you know, you don't have enough historical data and the market's changing on you. So that's going to be useless data. And the idea is, no, no, we don't take any position on what the market is going to do. So we will train across the entire domain and we're going to put the data where it needs to go.
00:43:38.852 - 00:44:00.982, Speaker A: And that way you're going to get a net that's going to be resilient to crazy market moves. Because from our perspective, it's not a crazy market move, it's just another point in the domain that we trained on. So, no, no. Historical. So the art and maybe this ties into this last slide where I say, you know, like, some of the research we're doing on unsupervised learning is, how can we get better and better at knowing what's out there that has never been seen before.
00:44:01.158 - 00:44:01.662, Speaker B: Right.
00:44:01.758 - 00:44:10.462, Speaker A: We don't care if we're, like, accounting for some situations that will never happen. We just like that our nets are able to handle them even if they never exist. Got it.
00:44:10.478 - 00:44:21.652, Speaker F: And one follow up question would be that, let's say, if I want to use this model for pricing and code the prices in the market, can I trust it for all the corner cases? Just for, let's say.
00:44:21.788 - 00:44:46.328, Speaker A: I mean, let's say you're getting nervous. What do you do? Well, you just switch back to your old method, and then you'll see it gives the same answer, and then you can toggle back. So that that's the, you know, it's not like, oh, you go, deep neural net, and then you can never use your old machine. It's basically like a turbo button. So you're sitting there, you've got your slow Monte Carlo, you can go around business as usual. All of a sudden, the market's going crazy, and you're like, gee, I really wish I knew how much this was worth right now. Well, press the turbo button, get that answer.
00:44:46.328 - 00:44:54.684, Speaker A: You can even start the slow competition there, and you'll get the confirmation once that competition finishes that the answer was accurate. You'll get to make that decision on the fly when it matters.
00:44:55.384 - 00:45:14.066, Speaker F: Right. Makes sense. And just one last one. So, generally, when we do the pricing, we have this risk neutral approach, where we take the most recent data from the market and then price it. So, does it also incorporate this neutral approach in a vague manner, or it takes, like, the historical data into consideration?
00:45:14.210 - 00:45:38.032, Speaker A: It takes. So, basically, if you had your pricer, what I would do is just use your pricer to build a neural net. So I leave the choice of how you're going to handle inputs to you. So if you want to use a risk neutral approach, the deep neural net will have a risk neutral approach. If you have some fancy thing where you're not going to be using that, that the deep neural net will not be using that. So we're not reinventing models. We're taking existing models.
00:45:38.032 - 00:45:42.004, Speaker A: So you pick your favorite, and we accelerate that exact model.
00:45:44.104 - 00:45:46.284, Speaker F: Makes sense. That's it for me. Thanks.
00:45:47.864 - 00:45:56.844, Speaker B: Okay. Cameron Wissen, twitch. Would you like to unmute, turn on your camera and ask the question.
00:45:57.924 - 00:46:26.646, Speaker G: Right. Hi, Maxine. Thanks for your talk. So, yeah, the question really is, like, you know, given all the work that you've done, do you have a sense of, you know, when you're trying to price, say, like the black scholes? That's your, that's your target, like the call option, more layers or more neurons, that improves the accuracy. So, for instance, like, you have one layer with 10 million nodes, just to be kind of absurdly extreme. Yeah, you have like, 10 million layers with one.
00:46:26.710 - 00:46:55.284, Speaker A: With one note, I would go 10 million with one. No, well, one node is not enough. Then you're basically looking at a one dimensional function. But I do see that a lot. Some people will write a paper and say, oh, we have this fancy approach, and it beats deep neural nets. And then I look at what they did, and they went to tensorflow, they downloaded tensorflow, and they have this one single hidden layer network, and they cranked up the neurons to infinity, basically, and said, hey, it doesn't work better than such and such. And it's like, no, you want to.
00:46:55.284 - 00:47:05.832, Speaker A: You want to play that. That push pull between depth and width is really probably the most fundamental lever you have at your disposition, and you should use it wisely, and I guess.
00:47:05.888 - 00:47:07.644, Speaker E: Is that a question?
00:47:09.664 - 00:47:10.444, Speaker B: Sorry.
00:47:12.704 - 00:47:21.644, Speaker G: So, like, when you. When you think about the hyper parameters, like the number of layers versus the number of neurons per layer, is that even something that you consider?
00:47:22.024 - 00:47:53.710, Speaker A: Absolutely. So you got to use your domain knowledge. So it's not as easy as just generate a random architecture and press play. There's a lot of trial and error, and it is definitely an interesting mix of theoretical science and very empirical science. So it's an engineering problem and a theoretical problem at the same time. And you build up a lot of know how over time on what works best in what kind of situations. But, you know, if you, you know black scholes, you can implement that yourself very easily.
00:47:53.710 - 00:48:09.114, Speaker A: I think we even have one where we posted publicly that one. Basically, whatever you do, it'll learn it because it's so simple. But you can play around and take a. Take those extremes, like, you know, build your 10,000,001 hidden layer, see how that does compare to a ten by ten, say, and then you might learn something interesting.
00:48:11.494 - 00:48:15.366, Speaker B: Okay, next. We still have quite a few here.
00:48:15.390 - 00:48:16.174, Speaker A: But I don't know if we're going.
00:48:16.174 - 00:48:23.616, Speaker B: To get through all of them before the end of the question period. We'll make our way through. Leonid shart, sir.
00:48:23.800 - 00:48:28.684, Speaker H: Yes, hello. Yes, thank you for. Nice.
00:48:30.664 - 00:48:32.712, Speaker B: Leonid. There's a lot of distortions.
00:48:32.848 - 00:48:34.324, Speaker A: It's hard to hear what you're saying.
00:48:34.904 - 00:48:39.324, Speaker B: Perhaps it's too close to you. You need to put it further away.
00:48:39.744 - 00:48:41.084, Speaker H: Can you hear me now? Well.
00:48:43.544 - 00:48:43.928, Speaker A: There.
00:48:43.976 - 00:48:47.624, Speaker B: But we can make out what you're saying. So try, try, continue.
00:48:48.044 - 00:48:56.812, Speaker H: Yes, sorry, I have a few questions. Question about those mammograms or what you called.
00:48:56.988 - 00:48:57.864, Speaker A: Yeah, yeah.
00:48:58.604 - 00:49:01.748, Speaker H: They have a reference for the black shoes one. Where did you get it from?
00:49:01.836 - 00:49:18.294, Speaker A: I got it from here. Here, let me. Can I do this? Oh, no, I have the view only, not edit, but I don't know if you can make that out. It's by Elroy Dimson. E l r o Y D I m S o n. It's really cute. It's like a three page paper.
00:49:22.274 - 00:49:25.734, Speaker B: I'm sure if you just Google search, option valuation, nomogram.
00:49:26.514 - 00:49:29.014, Speaker H: First time I see it and I've never seen this.
00:49:29.674 - 00:49:30.802, Speaker A: It's really cool.
00:49:30.978 - 00:49:43.854, Speaker H: Yes, indeed. I have another question specifically for. You were mentioning bermudian swaptions. Do you have an assumption on number of recall dates?
00:49:44.554 - 00:49:53.694, Speaker A: Yeah, well, let's look. I mean, I don't remember the. I think it's every six months. So basically, if you have.
00:49:55.634 - 00:50:01.234, Speaker H: So you model it as a continuous parameter, basically the frequencies, you cannot specify specific dates.
00:50:01.354 - 00:50:36.306, Speaker A: No, no, no. So maybe that's another confusing point. People tend to think that we have to rig everything and specify specific dates, but it's really coming back to this idea that we're working at the. I keep losing my slides. But now we want to learn the operator that goes from the space of all possible parameters to the space of solutions. And so your dates and all that jazz, that's part that's kind of baked in, into your PD or conditional expectation as some exotic boundary condition of sorts. And that's considered that part of the parameters that go into it, and we view them on the same footing as the parameter in front of a quadratic.
00:50:36.330 - 00:50:43.906, Speaker H: Equation, but then means that you need to learn them as part of what you simulate. You simulate different dates.
00:50:44.050 - 00:50:45.594, Speaker A: Exactly. Exactly.
00:50:45.754 - 00:50:48.414, Speaker H: So if you have one parameter, it's easy because.
00:50:49.834 - 00:51:32.110, Speaker A: Yeah, I mean, just think about a vanilla black scholar. I mean, here I have a slightly souped up black Scholes, but vanilla black Scholes, you have a parameter like sigma or the interest rates, and you could say, oh, I can learn it for a specific value of sigma, or I can learn it for ranges of values of volatility and ranges of values of interest rates. And I think every input to any such problem, even if they're discrete or not discrete, you can think about it as having a range of possible values, and you can explore that space. Now, of course, if you have this insane problem that has zillions of parameters, you might want to break it up. In theory, you could price basically every single PDE using a single humongous neural net. But then you end up in this GPT-3 style stuff, and that's hard. It's easy.
00:51:32.110 - 00:52:02.542, Speaker A: You want to decide where you're going to want to chop up the different assets into different kind of neural nets. And we find that we can go fairly general on a given neural net. So maybe, to give you a concrete example, fairly general. I talked about FX double knockout pairs. There's only one model for all the FX pairs. It's not like we built one for each different FX pair. And the reason that works is because you kind of, like, with that black scholes nomogram, you encode things as normalized values.
00:52:02.542 - 00:52:31.436, Speaker A: So you kind of build these coordinate, I mean, I want to say currency free coordinates. And then you just explore the full possible space of possibilities. So when you're training, like, I don't know, mexican peso, you. Or, I mean, when you're looking at mexican peso or the british pound, you haven't tailored the model to look at the interest rate or to look at the effects curves of that specific currency pair. You've trained it to look at all possible effects curves of all possible currency pairs. And it's like the model doesn't know. It's like pricing mexican peso.
00:52:31.436 - 00:52:36.104, Speaker A: It just knows it's pricing some kind of trade with some kind of parameter.
00:52:38.444 - 00:52:53.824, Speaker B: Okay, maybe we should go to the next question. Let's see. William Zhao, would you like to ask your question? I think it's somewhat related to Sethi's question, but yes.
00:52:55.124 - 00:52:58.652, Speaker A: So thank you so much for the talk. I have a simple question.
00:52:58.708 - 00:53:02.204, Speaker D: Just, you mentioned you have, like, 153.
00:53:02.324 - 00:53:06.540, Speaker A: Input feet input dimension. So can you, like, tell us more.
00:53:06.572 - 00:53:08.744, Speaker I: About which, like, features you're using?
00:53:09.044 - 00:53:33.590, Speaker A: Yeah, for the berm, do you have a full wall surface? And then you have. So ball surface, you have interest rate curves. You have the term. So the exercise dates the strike. There's a non call period. So at the beginning, there's a part where you're not allowed to dive in. And so on, the biggest input of the ball surface, you can probably count.
00:53:33.590 - 00:53:55.242, Speaker A: I can't remember, it's one to 30 times the other direction. And if you go on the website, I mean, it's there. So basically, go to pricer Dot riskfield.com, and then you'll see right there, those are the exact inputs. So everything that's an input to the model. You have the ability to talk, to play around with yourself and see how well it does against Quantlib. Okay, great.
00:53:55.378 - 00:54:03.214, Speaker B: I think that's a similar question to motion Sefi's question about elaborating the inputs of the NN. So I'll go to the next questioner.
00:54:03.554 - 00:54:08.578, Speaker I: Do you actually mind if I, if I like it?
00:54:08.586 - 00:54:10.362, Speaker B: Would you want to clarify the question? Go ahead.
00:54:10.458 - 00:54:11.032, Speaker A: Yeah.
00:54:11.178 - 00:54:37.966, Speaker I: So, summer and longer talk. I was lost about your methodology. So you were saying that, say we're having something like black Scholes model with a set of coefficients, and you're learning a mapping from the set of coefficients to the solution space. I wonder if that's what your neural net does. It's like, is it learning a map from these set of possible coefficients to the, of solutions?
00:54:38.110 - 00:54:38.766, Speaker A: Yes.
00:54:38.910 - 00:55:06.258, Speaker I: And if so, as far as I know, when the geometry of the domain, like when, when we are learning functions, if the domain of the function is like irregular. If the geometry is irregular, yep. Computationally, we would face difficulties. And that's the case when like the coefficients are collinear or things like that. I wonder if neural nets can handle that without any difficulty.
00:55:06.376 - 00:55:39.744, Speaker A: I mean, they can't handle it without difficulties, but with enough work on your part, they can handle it just fine. So that's part of the, maybe you remember that I had that slide where, you know, like data placement matters. You know, you know, the function, the solution operator. You know, I think what you're describing is that, you know, in some places it's not that wild, but there are parts where it's going to be doing crazy things like think of a barrier option. You know, when you're, say you're deep in the money and really close to being knocked out, then you're either going to make a lot of money or make nothing. So of course, you're going to have to have this non Lipschitz jump in the function in that region of space. And so you're going to have to worry about that.
00:55:39.744 - 00:55:51.904, Speaker A: So, yeah, there is a bit of an art to this thing, but when you've trained a neural net to handle those kinds of situations and you're using it on the fly, it's not a problem. Okay, thank you.
00:55:53.364 - 00:56:29.632, Speaker B: Okay, maybe this would be the, actually, there's two more questions, but we're really close to six. So, Yanis Silly Gakas, would you like to unmute and turn on your video to ask a question, or shall I read it? Okay, I'll read it out. So isn't it correct? Incorrect. Sorry. Isn't it incorrect to train your nn with older data which were generated by different traders with different risks, preferences, utilities, question mark how do you choose your training data?
00:56:29.808 - 00:57:15.394, Speaker A: So we don't use, we don't, we don't choose historical data. So I think that the point here you're making is, yeah, it's stupid. If I was training on historical data my net would only be good for things that have happened in the past and so we do not do that at all. We in fact generate completely, we generate data that covers everything and anything under the sun in terms of the space of coefficients. So to give you a very simplified examples, let's take a very simple black Scholes. What I would do for the black Scholes here, I've got the sigma, I've got the r and I've got the possible spot interest rate and time and I would cover every single possibility in that domain as much as possible and I would not look at what has happened in the past. I would just say, you know what, I'm going to be agnostic to the market and I'm going to make sure that mineral net can handle anything regardless of what has happened in the past.
00:57:15.394 - 00:57:17.494, Speaker A: And so we actually don't suffer from that problem.
00:57:19.714 - 00:57:30.094, Speaker B: Okay, maybe this will be the last question in the formal part of the seminar. So Thomas Obitz, would you like to unmute and turn on your video to ask a question?
00:57:32.034 - 00:57:54.574, Speaker E: So obviously neural networks are not the only way of doing approximation and multiple dimensions and therefore a standard flow from functional analysis would be radio basic functions. Did you compare neural networks against other methods, mechanisms and what were the results?
00:57:55.354 - 00:58:38.734, Speaker A: So all those tools work really well in low dimension when you start going into the hundreds. And lately we've been doing some insurance stuff in the thousands of dimensions there isn't even any competition left. So those, I mean there's also the Chebychev interpolation. Those work great for a handful of parameters, but as far as I know, the only technique I know that doesn't rely on cutting very gross corners and that generalizes up to the thousands of dimensions is neural nets. And so why, why focus your attention on a specialized tool that's going to stop working when you hit 20, 30, 50 or 100 when you have this one that works great and that will not hit that block.
