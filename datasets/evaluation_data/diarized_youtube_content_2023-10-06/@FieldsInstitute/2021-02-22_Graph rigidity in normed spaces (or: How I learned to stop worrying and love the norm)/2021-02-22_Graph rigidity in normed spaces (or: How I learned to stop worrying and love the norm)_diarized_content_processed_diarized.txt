00:00:00.200 - 00:00:53.794, Speaker A: In this talk, my plan is to kind of go through some of the basics of this sort of area of rigidity theory with different norms. And as the title says, I'm hoping that you don't worry too much. There's a lot of problems that are involved in this area, but you'll learn to love the change of norms because you get very strange and wonderful things happening. So that's the idea of the talk. So, basically, when we do a lot of this rigidity theory, a lot of constraint system stuff, we often use the Euclidean norm. It's a good starting point, but sometimes we want to. What will we do if we change norm? So I've got the euclidean distance here, but another possibility that we could use would be the LQ null.
00:00:53.794 - 00:01:27.384, Speaker A: So all we've done here is we've switched it from a square root, which is brackets, to the power half. We change that to one over q, and we've changed it to instead of x squared, so on and so forth. We've got the absolute value of x one to the power q. Now, with Euclidean norm, it doesn't actually make a difference because the absolute value squared is the same as just squaring it. But we have to be a bit careful here because you could end up with, if q is like 1.5, you could end up with some issues. So we need to do these absolute values just to be safe.
00:01:27.384 - 00:02:21.564, Speaker A: Okay, so I've generalized, I want to generalize from these sort of problems with euclidean norms to these other things. But why have I chose q to be between one and infinity? Why haven't I chose q to be lower? Why isn't Q like a lower number? Well, if q is like a lower number, we don't get all the properties we want. So if q is like zero or less, then we're definitely in trouble, because then we're going to have any coordinate that zero is going to call an issue. So we can't have that. But why don't we, why can't we have q, maybe strictly between zero and one? Well, there's a few things we want to keep when we generalize these problems. So there's three properties we want to keep. The first is when we generalize to this new map, we want that it can identify two different points.
00:02:21.564 - 00:02:49.610, Speaker A: So I've got this as it's positive, we want it to be positive. That makes things nicer. But we want it to only be zero if and only if it's the zero vector. And later on, when we identify distance, this allows us to say that two points are two different points. If we don't allow this, then you're getting into non house door spaces and all sorts of kind of problems crop up. The next one is this idea of absolute homogeneity. This just makes things easier.
00:02:49.610 - 00:03:28.360, Speaker A: If you know where something's pointing, if you know where a vector is pointing and you know what its length is, then if you want to get something that's twice as long, you just double the length. So this makes things very nice. Now this is third, property is not kept if Q is between zero and one, and that's sub additivity. Now this one doesn't seem as kind of as important to the other two, but it is very crucial later on, especially when it comes to differentiation. Having this sub additivity property makes things very nice if we have all three of these properties with whatever map we've chose. So we're generalized from the euclidean norm to something else. We just call it a norm.
00:03:28.360 - 00:03:57.574, Speaker A: So this is what a norm is. I'm only going to be dealing with real norms, real finite dimensional norms and norm spaces. I'm not going to be doing anything complex. That's in part because complex norms aren't differentiable. So we're just going to stick with nice real norms, nothing going crazy. Okay, so what's the setup? We're going to take a framework. So it's going to be a simple graph, a finite simple graph not doing anything infinite today.
00:03:57.574 - 00:04:31.976, Speaker A: And this map pull from the vertices to the real space. I eliza, sometimes called the placement or the realization doesn't really matter. Okay, we're going to be thinking about this, what we call the rigidity map. Now the rigidity map is now in with respect to the graph and the norm. Usually when we're doing euclidean stuff, we can forget about what norma is because we're obviously stuck with Euclidean one. But this map basically allows us to take a framework and tell us what its edge lamps are. Okay, we're not squaring the edge lamps.
00:04:31.976 - 00:05:02.254, Speaker A: That's usually done to make things like quadratic, but because for the most part we've got no chance of getting something algebraic, we're not bothering to square anything. So we're just keeping it as it takes a framework, takes placement, gives out the edge lengths. If two things have the same edge lengths, we're just going to call their frameworks equivalent. I guess this is, I should probably put equivalent with respect to the norm. But yeah, we've got two frameworks that live inside this num space. They have the same edge lengths they're equivalent. So nothing's really exchanged there.
00:05:02.254 - 00:05:30.544, Speaker A: We've also got this idea of a rigidity matrix. This is just going to be the derivative of the rigidity map. This is similar to what it is in the Euclidean case. It's just a derivative of the rigidity map. You maybe want to multiply through rows by some constants just to make things a bit nicer, but that's roughly what it is. Okay, I put the jacobian matrix, you can think of that, or the total derivative. There's a few different names for it.
00:05:30.544 - 00:06:32.336, Speaker A: Importantly here I've assumed that fg is differentiable at P, which I'll cover a bit more later on. Okay, so we got this rigidity map and rigidity matrix. Now what sort of problems are we looking at? Because there's a lot of different ideas we can look at, but we're going to be focusing on kind of the local rigidity, the, that type of rigidity, nothing too complicated. So this is if you have two equivalent frameworks and they're sufficiently close so the vertices don't differ by too much. When are they essentially the same? Okay, when is it basically just the same framework? When has nothing changed? Now I'll get into this line essentially the same later on because there is, this is open to interpretation, but for the moment, just that's the sort of problems we're looking at. We're looking at these local rigidity sort of problems. So what's some motivation for doing this? We've got a few, a few reasons for doing this.
00:06:32.336 - 00:07:20.130, Speaker A: It's not just completely abstract. One idea is understanding how rigidity works in this more general setting will help us understand how rigidity works in the more concrete setting of euclidean spaces. Basically, techniques that we build in this tend to have to be more careful, more precise in some ways, and we want to use that in euclidean spaces at some point. There's also the idea of if you can show something happens, for maybe you have a property you want to prove in the euclidean space, but it's very hard to prove that. But maybe it's very easy to show in a polyhedral norm space. So this is a norm space where the unit ball is a polyhedron. Okay, maybe it's very easy to prove there.
00:07:20.130 - 00:08:00.678, Speaker A: And you can also show that this property, when you take a sequence of norm spaces there is with the, maybe the Bannock Missour compactum distance, there's a few different ones you can pick. Maybe you can show this as a closed property. Well, if you can do these two things, then you can automatically prove it for the euclidean space. And you've not had to touch anything in the euclidean space while doing so. This may be a bit optimistic, but this is one idea that you could use it for. Another idea which I'll hopefully have time to cover at the end is packings. So like circle packings, but in this case we'll be doing the one level up generalization.
00:08:00.678 - 00:08:37.300, Speaker A: So there's homothetic packings of convex bodies. Homothetic just means you can translate it and you can scale it, but you can't rotate it. And that's the next level up of generalization from the circle packing. We'll hopefully have time to discuss that more. Another possibility, formation control. So if you have drones, you want them to follow a certain formation. You want to describe the distances between the drones for this problem? Well, maybe the euclidean distance isn't best for what you're doing, so maybe a different norm.
00:08:37.300 - 00:09:15.288, Speaker A: So the cylinder norm like I've got here, maybe this is important, maybe you want to keep the z axis distance slightly different to what the x and y distances is then non euclidean norm space stuff. This is where you want to be. I've got an example here, maybe subnautical drones, they're all flat going in formation underwater. Well then you want to count depth separate. You want to count this as something slightly different because going up and down will change the pressures and stuff like this. So maybe you want to keep this separate or XY is not so much of a problem. Okay.
00:09:15.288 - 00:10:13.024, Speaker A: And then the last reason, which is more the motivation why I do it, which is just, it's a very interesting topic and you get very strange examples and counterexamples that are very non intuitive. And this is why I enjoy it. It's a very strange topic. Okay, now I mentioned previously on the last slide about two things being the same. So our first hurdle when setting up these problems is what do we consider the same if we have two frameworks, when are they counted as the same thing? Now this isn't so obvious for Euclidean, it's fairly obvious, but I'm going to give you two possibilities, and in euclidean spaces, they're the same. So we've got a weak idea of the same, which I'm calling complete equivalence. So it just means both frameworks have the same edge lamps, so between any two vertices, the distances are exactly the same and we can call this a complete equivalent.
00:10:13.024 - 00:10:56.404, Speaker A: We also have a stronger one, so the stronger one will imply the weaker one, which we call congruence. So if you have two frameworks, there's an isometry of the norm space that maps from one framework to the other. Okay? Now, in euclidean spaces, these two things are exactly the same thing. So it's always true that congruence will always imply complete equivalence, but in euclidean spaces, complete equivalence implies congruence. Now, our first hurdle with normal spaces is this is never true or this is never always true, to be more accurate. So here is an example of a framework. It has complete equivalents because it's just two points with an edge connecting them.
00:10:56.404 - 00:11:31.780, Speaker A: We've got it in the l infinity norm space. So this is the norm space in 2d that has the square as its unit ball. In this space, this framework is completely equivalent. But what you'll notice is this vertex here, I can actually slide it along here. In fact, I can just slide it the whole way around and the distance won't have changed between these two points because it's the l infinity norm. This one's at zero, this one's at one stay and it's just sliding around. This hasn't changed.
00:11:31.780 - 00:12:02.214, Speaker A: So this means that if I take this vertex to be over here, so if I take this vertex to be over here and this one to stay where it is, so I have where the hand is, then I've got another equivalent. I've got another, the same framework. It's equivalent and it's also completely equivalent. But these two frameworks aren't congruent. There is no isometry that maps zero to zero. And this point to this point that doesn't exist. Maybe I should have, maybe put it there so it doesn't look like a reflection.
00:12:02.214 - 00:12:28.824, Speaker A: So there's no isometry that maps it like that. Okay, so for this space, they aren't the same thing. Yes. Or a question from William. Yes, I do mean that you do need two frameworks. So I'm saying that this framework here and the other framework is where the hand is and this vertex is. So all I've done is I've slid this vertex over here.
00:12:28.824 - 00:12:41.486, Speaker A: Now these two frameworks won't have complete equivalence. That's more clear. Oh, sorry. Those two frameworks will have complete equivalents. They won't have congruence. Sorry. That's what I mean.
00:12:41.486 - 00:13:08.464, Speaker A: Is that more clear, I hope? Yeah. Okay. Right. Which of the two should we go for? Because there's upsides to both. Complete equivalence is obviously a lot easier because we can just check if we have a complete framework. It's always going to be, any other framework is going to be completely equivalent. You just check edge distances, a lot easier.
00:13:08.464 - 00:13:44.560, Speaker A: To do congruence is a lot more difficult, but we are still going to go with congruence. The strong version. We want the strong one. And basically the main reason is this will tell us more useful information about the system of points. So what we can have with these spaces is we can have two frameworks. I don't want to get into it just yet, because it's in a few slides, but we could have one that's kind of rigid and another one that's also rigid with this complete equivalence. But one has some sort of continuous motion and the other one doesn't.
00:13:44.560 - 00:13:53.948, Speaker A: So the saying they're both rigid has not really given us much information. Sorry, I've just got a question.
00:13:54.116 - 00:13:58.664, Speaker B: Yeah, there's a question. So Mira asks if isometry is have to change when you change the metric.
00:13:59.564 - 00:14:24.894, Speaker A: Yes. So isometry, the definition of isometry, will change when I change the metric. We'll get into this later on. The dimension of the isometry group is important, is that's the important part. And that definitely changes just for now. The Euclidean norm has the most isometries you can get, basically. So whenever we change norm, we're effectively losing isometries.
00:14:24.894 - 00:15:12.914, Speaker A: But I will get into that later on. Okay, what's the second reason for picking this equivalence? It allows us cleaner combinatorics. So when we do stuff later on with Maxwell count, this will give us a cleaner idea what's going on. It will give us better counting conditions. Okay, second hurdle, and I'm just going to briefly go through this, is infinitesimal, infinitesimals and differentiation. So we often want to talk about infinitesimal motions and stuff like this when we deal with rigidity, because it's easier to look at the infinitesimals and linearize the problem than it is to deal with the actual motions of a framework. Where's the problem? Come in.
00:15:12.914 - 00:15:51.218, Speaker A: Well, then the norm might not be differentiable. So then how do we talk about infinitesimals? Because we're talking about an infinitesimal change of the norm of like the distance between two points. You're kind of looking at this sort of thing, and if the norm isn't differentiable, then you automatically got problems. So maybe there's just no hope. Maybe the norm is just we can find norm spaces that just never differentiable, then that functions like this exists. So why can't norms? Unfortunately, norms like this can't exist exactly, because they're convex. And this is one of the properties we wanted to keep.
00:15:51.218 - 00:16:22.734, Speaker A: Now, if we didn't have this sub additive condition, there'd be no hope. But because we've got this sub additive condition and because of that we've got a convex function, there is some hope on this. And what it means is that if you have a norm, then it will be differentiable almost everywhere. So the set of points where it's not differentiable can't be too many. And most, most norms we look at, it'll actually just be a very small subset. It won't be. It'll be an easily identifiable subset and it won't be so bad.
00:16:22.734 - 00:16:25.134, Speaker A: So this is the good news.
00:16:25.514 - 00:16:28.814, Speaker B: Is the set of differentiable points always residual?
00:16:31.614 - 00:16:33.814, Speaker A: I don't know what residual means in this context.
00:16:33.894 - 00:16:35.942, Speaker B: It's always complement of a meager set.
00:16:36.078 - 00:16:49.942, Speaker A: Yes. Yeah. So the. Yeah, yeah, it's exactly that. Yeah. Can we say more? So the set points where it's non differentiable will be a measure zero set in the labay measure. So it's.
00:16:49.942 - 00:17:20.414, Speaker A: I think that's slightly stronger. Might be wrong, but yeah. Yeah. Because of this, we're going to talk about well positioned frameworks. So if I say well positioned, I just mean that the framework we've got in terms of the rigidity map is a differentiable point. And it means that we can do this infinitesimal work and nothing's going to go haywire. Basically just to kind of give you an idea of what one of these ones looks like that's not well positioned is something like this.
00:17:20.414 - 00:17:50.982, Speaker A: So we got the same framework as we had before, but we're put it. So one vertex is here at this corner. Now the parts that of the norm that are differentiable is basically any point in the space which is in this sort of region and doesn't lie on one of these lines. Okay. Because these lines have corners and corners on differentiable. That's basically the idea. I don't want to go into it too much, but that's, that's roughly, you can think of corners as non differentiable points and this one is not.
00:17:50.982 - 00:18:10.914, Speaker A: And the problem is it kind of, it thinks it doesn't have any infinitesimal motions even though it should, because you can move that point around. So you would logically want it to have infinitesimal motions. And you have to kind of change the theory to make this work. And it's a bit. Yeah. So you have to do on a case by case basis. We're just going to avoid these problems.
00:18:10.914 - 00:18:41.494, Speaker A: Okay. Rigidity. A lot of these definitions will now look very familiar because they have not changed at all. Just to say quickly, a rigid motion is just kind of a motion of your framework where every point is remaining congruent to the original point. Okay. So nothing too crazy going on here. Okay, so the first of our three types of rigidity we're going to be looking at is continuous rigidity.
00:18:41.494 - 00:19:14.238, Speaker A: This is the type of rigidity I always think of as rigidity. It's rigidity. Rigidity. Can you move the vertices and change the distance without changing the distance between adjacent vertices? So it's going to be, we're going to say it's continuously rigid if every continuous motion of the framework that preserves distances between adjacent vertices. That is a rigid motion and it's continuously flexible if that's not true. So if it has a continuous motion, but that preserves distance, but is not a rigid motion. Okay.
00:19:14.238 - 00:19:30.394, Speaker A: They're the two different types. So on our previous slide, the previous example would have been continuously flexible. Okay. Because we can just move that vertex along like this. And that doesn't correspond to a rigid motion. Right. Local rigidity.
00:19:30.394 - 00:20:09.410, Speaker A: So you might have seen this as well. This is what Shinichi mentioned in the morning. So this is if every equivalent framework that is sufficiently close I put within epsilon. I just mean if you pick another framework and you perturb each vertex by at most epsilon in the norm, it doesn't matter what norm you pick is a topological property. So if you just perturb it slightly and there's such an epsilon exists where perturbing it slightly will still mean you have to be congruent, then you're locally rigid and you're locally flexible. Otherwise I won't cover this in too much detail. But yeah, the last one, this is one we're going to be focusing on maybe later on.
00:20:09.410 - 00:20:34.424, Speaker A: A lot more is infinitesimal rigidity because this one gives you a lot more what you can say about the combinatorics of the graph, basically. So we're going to say it's infinitesimally rigid if every element in the kernel of the jacobian, of the rigidity map. So in the kernel of the rigidity matrix, that's going to be a tangent to a rigid motion. Okay.
00:20:34.764 - 00:20:36.340, Speaker B: Can I ask you a question now?
00:20:36.492 - 00:20:37.224, Speaker A: Yep.
00:20:38.364 - 00:20:41.664, Speaker B: So your definition of rigid motion didn't put much smoothness.
00:20:42.084 - 00:20:43.384, Speaker A: It does not, no.
00:20:44.604 - 00:20:47.824, Speaker B: So are you guaranteed that it has tangents?
00:20:48.724 - 00:21:08.164, Speaker A: I'm guaranteed by assuming the framework is well positioned, basically because it just has to be tangent to a rigid motion. So you can actually guarantee this? There's a lot hidden behind this simple definition. There is some work to be done, but it is still true.
00:21:09.584 - 00:21:11.684, Speaker B: Okay, sure. Thanks.
00:21:13.064 - 00:21:36.844, Speaker A: Okay, so we have a result here from me. So that's. In any norm space, local rigidity will always imply continuous rigidity. The proof is basically trivial. It follows exactly the same proof pattern that you would use to prove it. In the euclidean case, it's very topological proof. Effectively doesn't take too much work.
00:21:36.844 - 00:22:27.734, Speaker A: What we can also say is that if the rigidity matrix has constant rank on a neighborhood, so the rank doesn't change of your rigidity matrix if you perturb the points a bit. Notice this is assuming that your framework is well positioned on an open neighborhood of itself. This is important because you can't talk about the rigidity matrix unless you talk about, well, position placements. So if we perturb slightly, the rank stays the same. Well, if that's true, then we can do a lot of like manifold sort of differential geometry stuff, and then we can say that infinitesimal rigidity, local rigidity and continuous rigidity are all equivalent. Okay, this is kind of like an AsiMo Roth result. Um, now where does this break down? Now, you'll notice I haven't put that local, uh, uh, continuous rigidity implies local rigidity.
00:22:27.734 - 00:23:00.100, Speaker A: So Asimo and Ruff proved that this is true in euclidean spaces. So in any euclidean space, if something's continuously rigid, it's locally rigid. Now they proved this via a lot of like, complicated algebraic geometry. Um, well, maybe not complicated, but it used algebraic geometry. Now, we're not going to have algebraic geometry in every normal space. I just described a norm that's made out of a square that cannot be described by an algebraic function. Okay, so we can't be using all these algebraic geometry tools.
00:23:00.100 - 00:23:26.604, Speaker A: But maybe, maybe that's not a problem. Maybe there's other tools we can use. So is this result true? If you have something that's continuously rigid, does it imply local rigidity? And the answer is no. There are counterexamples. So I'll quickly go through a counterexample. It's going to take a while to get your head around this idea. So what we've got on the right is the unit ball of a 2d norm space.
00:23:26.604 - 00:23:55.682, Speaker A: Now, you might think you want to start with functions, and then you get the unit ball of the norm space. From that, it's usually better to think of the unit ball. And as long as it's got some convex set properties, you can then get the, the norm back again. Okay, so we got this unit ball and we got this framework. Now the unit ball. All I've done is I've taken a circle and I've just shaved pieces off. So these black points, they all originally lying on the circle.
00:23:55.682 - 00:25:09.914, Speaker A: And all I've done is at these black points I just can have a line connecting one to the next, like so. Okay, now, now next, I've taken a framework and I've purposely made it very long, so I've made it very, very elongated. And the reason for doing this is that every one of these black edges, when we plot it on the unit ball, so when we translate this edge, so that, say v one sits here and v three sits wherever it is, if you scale down v three as well, it will sit here on the circle, say, okay, we can do that to all the edges, all the black edges. Now what's going to happen? Well, even with the red edge included, all these edges are going to think they lie on the Euclidean, in the Euclidean, in a euclidean 2d norm space because the distance constraints between the points will be exactly the same as the normal standard euclidean distances between the points. Okay, how are they supposed to tell otherwise? Because the distance, if you're pointing in this direction, the distance is exactly the same in both norms. So it thinks it's in the euclidean norm. Okay, so we've kind of tricked the framework.
00:25:09.914 - 00:25:42.290, Speaker A: What we're going to notice though is that if we delete this red bar and we have the framework like so on there, when we rotate it, it will look like a normal rotation. So we can just flex it to move one way. But when I say flex it, it'll still just look like a euclidean rotation. It just looks like we're picking up the framework and doing this. Now, this isn't an isometry in our norm space. If it was an isometry, then the unit ball would kind of. Well, it would have to be a circle.
00:25:42.290 - 00:26:22.274, Speaker A: So it's not an isometry, but it's some sort of continuous motion that just looks like a rotation. Now what happens while we're doing this is the distance between v one and v two will vary. So when, if we say v one is put at the center and v two is put here, as we rotate this thing slightly, v two, we know just from how it's plotted, will follow the path of a circle round. But the unit ball is slightly smaller than the circle of places. So the distance is going to alter. So the distance is going to keep getting in. Our norm is going to keep getting bigger, then one again, then bigger, then one again, then bigger, then one again.
00:26:22.274 - 00:27:03.414, Speaker A: That's going to keep happening. Now because we have this limit point, like so we have an infinite amount of frameworks that are very close to our framework, like this, where the distance between v one and v two is one, but there's no continuous path that follows them along. Because during any continuous path, the distance between v one and v two will want to increase and then contract. Though it won't actually look like this for us when we watch it, it will just look steady. But in the norm distance that we've chose, it will look like it shrinks and changes. This is a very kind of weird counterexample. So you have to kind of think about it for a bit.
00:27:03.414 - 00:27:46.224, Speaker A: But yeah, this one it doesn't work for. Next question. So we said continuous rigidity doesn't always imply local rigidity. Now I already said on the previous slide about infinitesimal rigidity implying all three, but you notice I did this little caveat that it had to have constant rank on a little neighborhood of it. So what if we don't have that? So if we just, we know that the framework is infinitesimally rigid, but we don't really know anything else about the framework. Can we say that it's also locally rigid? Now new cleared in space, this is obviously true. Basically you've got a differentiable function.
00:27:46.224 - 00:28:06.114, Speaker A: Differentiable functions with Max on one rank will be maximal rank on an open neighborhood. So a little perturbations will keep it. So we automatically have this constant property with non Euclidean. I mean, I've just shown you a framework which is actually infinitesimally rigid. Sorry. Infinitesimally flexible. Sorry.
00:28:06.114 - 00:28:42.514, Speaker A: But it is well positioned. But it doesn't have a neighborhood of well positioned placements around it. So we can't use this argument. Exactly. So is this true for all norm spaces? If I have something that's infinitesimally rigid and it doesn't matter if it's infinitesimally rigid on an open neighborhood, is it automatically locally rigid? And the answer is we don't know. We think it probably is true, but there needs to be more work done in this, sort of like we're doing differential geometry without being able to differentiate. So some problems are creeping in.
00:28:42.514 - 00:29:15.534, Speaker A: It's known, fortunately, very fortunately, to be true for all nice spaces. So as long as the norm is differentiable on an open set of points, this will be true and we have no issues there. So that's fine. And if you can think of a norm, it probably has an open set of differentiable points. The one previously does not, but I had to very carefully create it, so it had problems. Most of the norms don't have this. So any norm you can think of probably going to be fine.
00:29:15.534 - 00:30:09.698, Speaker A: The last one is the weirdest of all. Does there even exist in infinitesimally rigid framework? And the answer is, for most norm spaces, we still don't know. So if you have a norm space, we conjecture that there exists some infinitesimally rigid framework. We don't know what it is, but we assume there is one. But we have no idea of how to prove this in some ways. So if we can show one is infinite, estimate rigid, there is a method that we can use basically by adding a vertex and connecting it to the rest in a kind of clever way that we can prove that there has to be an infinite amount of infinitesimally rigid frameworks. We do know it's true for certain classes of norm space, so we know it's true for all normed planes, so all 2d norm spaces.
00:30:09.698 - 00:30:46.190, Speaker A: If you give me a 2d norm space, I can tell you an infinitesimally rigid, or I know there's an infinitesimally rigid framework, I might not know where it is, but I know there exists one. And we can actually say way better than that. But the moment we're just going to say, we can say that, at the very least we can say there is an infinitesimally rigid framework. So rigidity theory makes sense in the infinitesimal world. It's also true for all LQ norm spaces. As long as there's a smooth LQ norm space, you can show that k 2D, you can embed this somehow in your space. So that is infinitesimally rigid.
00:30:46.190 - 00:31:21.100, Speaker A: That's very recent result. It's also known for cylindrical norm spaces. So the 3d cylindrical norm space and the 4D hyper cylindrical norm space. So the cylindrical norm space is the norm space, where the unit ball is a cylinder. The 4D is the 4D variation of that, we know it's true for them and this is it. These are the spaces, or including, or I guess it's already covered, the Euclidean case, this is covered by the LQ. We know it's true in all those three cases, and that's it.
00:31:21.100 - 00:31:29.212, Speaker A: We don't know about any other norm spaces. We think it is always true, but we don't have any evidence actually.
00:31:29.268 - 00:31:30.412, Speaker B: Can I ask a question?
00:31:30.588 - 00:31:31.572, Speaker A: Yeah, sure.
00:31:31.748 - 00:31:33.584, Speaker B: Is this for every number of vertices?
00:31:35.974 - 00:31:39.166, Speaker A: So if you go for every number of vertices, it's not true in any.
00:31:39.190 - 00:31:42.462, Speaker B: Norm space, any sufficiently big number of.
00:31:42.478 - 00:31:50.554, Speaker A: Vertices, this is what we want to know. Sufficiently big, is there an infinitesimally rigid framework on this number of vertices, as long as it's sufficiently big and we still don't know.
00:31:51.974 - 00:31:56.702, Speaker B: Okay, so for each norm space there exists a number of vertices with an.
00:31:56.718 - 00:32:01.880, Speaker A: Infinitesimally rigid framework we think at the moment we don't have.
00:32:01.992 - 00:32:04.208, Speaker B: Okay, but so in your list, maybe.
00:32:04.376 - 00:32:06.800, Speaker A: My list, yes, in my list that's true, yes.
00:32:06.952 - 00:32:22.624, Speaker C: Okay, wait, you're saying for any norm space there exists a number of vertices such that for some larger number of vertices, that is, for every number of vertices there's an infinitely digit framework, which.
00:32:22.664 - 00:32:36.384, Speaker A: Do you mean conjecture to be true? So we know it's true for the bullet pointed point points, but we don't know. This is an open problem that needs fixing really, to be completely honest.
00:32:38.484 - 00:32:45.436, Speaker C: Oh, I see, so you're saying that if you can find it for one number, then you can use the zero extension or something like that and get it for all the other numbers.
00:32:45.540 - 00:32:52.724, Speaker A: It is exactly the zero extension. Yes. You can throw in the other edges if you want, but we just want to show that there's an infinite list of graphs that are infinite.
00:32:52.764 - 00:32:53.424, Speaker C: I see.
00:32:57.234 - 00:33:29.624, Speaker A: Okay, so some other general weirdness. I'm going to cover Lewis's point in a second. So euclidean norm space is just going to be something in the chat. So potentially every sufficiently large continuously rigid framework could be infinitesimally flexible. That could be true. We have difficulty even finding continuously rigid frameworks that are sufficiently large. So we already have problems there.
00:33:29.624 - 00:34:05.494, Speaker A: There are sort of ways of doing it, but it's difficult. But that's talk for another day. We're going to focus on infinite atomic rigidity from now on, just to keep things simple. Okay, so some other general weirdness, I guess, caveat for infinitesimal rigidity, we're first going to talk about there's euclidean norm spaces. So this is just the norm comes from an inner product. And there's non euclidean norm spaces where this isn't true. Most norm spaces are non euclidean.
00:34:05.494 - 00:34:40.066, Speaker A: There is effectively only one euclidean space up to isometry, up to isometric isometry, and you already know it all the rest. If you find another one, it's effectively you've just taken a linear transform of the unit ball of a sphere. That's all they are. So they're all just ellipsoids and stuff like this. Okay, right. So we're going to take a framework in a non euclidean space, or we're just going to take a non euclidean space actually. So for every k less than or equal to d plus one, every k simplex.
00:34:40.066 - 00:35:11.610, Speaker A: So just a framework on k vertices. So you can just think of it as the complete graph on k vertices that will always be infinitesimally flexible. So this answers Lewis's question for every set of vertices. If you give me a set of vertices, is it always true there's an infinitesimally rigid framework for small enough sets of vertices? No, that is not true. You basically don't have enough constraints. You can show that there's independent placements of this. Even that's not particularly too easy.
00:35:11.610 - 00:36:01.128, Speaker A: But you can show it, which I won't go into, but yeah, you can show this, but you can't show because they're independent, they don't have enough constraints. Basically. I'll get into this a bit later, but yeah, the set of points with maximal rank rigidity matrix doesn't need to be open like I covered before, because you could have points that are not well positioned. So that causes the problems. Now the set of maximal rank rigidity matrix frameworks will be an open set in the set of well positioned frameworks, but it might not be an open set in your, in the set of just placements in set of frameworks. It's like a slight detail. So this why, this is why we think that infinitesimal rigidity still implies local rigidity.
00:36:01.128 - 00:36:30.384, Speaker A: It might be some way of fixing this. The next one. The set of frameworks that have maximal rank rigidity matrix doesn't even need to be dense. It doesn't even need to be a dense set. Now this isn't fixable. There's a lot of norm spaces where you can have infinitesimally rigid framework and an open set of infinitesimally rigid frameworks. You can also have an open set of infinitesimally flexible frameworks, so density is never going to be achieved.
00:36:30.384 - 00:37:06.424, Speaker A: Polyhedral norms have this. It's a very strange property. So it's kind of quite interesting in this way. You have to be very careful about the placements sometimes depending on the norm space. The last point is kind of this. We like to think of rigidity theory in terms of generic frameworks. Well, this is a really bad idea sometimes, sometimes we can get away with it, but in a lot of norm spaces we can't be doing this, because a generic framework is something where you imagine that the, the kind of the rigidity matriarch that you want to think about has been realized by a well positioned frameworks, rigidity matrix.
00:37:06.424 - 00:38:00.024, Speaker A: Well that might not be true. You might get like, I know what set of graphs I've got that are rigid in whatever definition we want, but there's no graph where there's no placement of a framework where if I say, oh, it's got a subframework, the graph of the subframework is a rigid graph. That doesn't guarantee that that sub framework is rigid. The way to think about it is with these polyhedral norm spaces you can get an induced coloring of the graph and cycles and colorings give you dependencies. And then what you can do is just use Ramsey's theory to say that if I have sufficiently large framework, I'm going to have a sub framework which is monochromatic and this will have to be dependent. So this is never going to be achieved in the polyhedral norm space. So we have to be very careful when we're talking about generic and stuff like this.
00:38:00.024 - 00:38:33.596, Speaker A: Okay, I mentioned before about combinatorics. We're going to get onto this now, which is Maxwell's counting condition. We're going to do a generalization of it. So I've got this induced number here. So this is the number of edges of the induced subgraph on a vertex x. And we got this DK sparse and DK type conditions that you probably have seen before in a similar sort of sense. So I'm just picking k to between zero and d plus one.
00:38:33.596 - 00:39:12.324, Speaker A: Choose two. I'm going to say the graph is DK sparse if every induced set of edges is less than this, so long as the set of vertices we're looking at is sufficiently large. And we're going to say that a graph is decay tight if this is true. Plus we have this top count condition as well. Okay, you've probably seen it with two three tight or three six sparse and stuff like this. You've probably seen it written like this, but you can generalize it to different numbers. Think of it as D is going to be the dimension of the space we're in and k is going to be kind of the degrees of freedom of our framework.
00:39:12.324 - 00:42:41.578, Speaker A: So we got this result from Derek and Berndt that if you have something that's infinitesimally rigid and it's got enough vertices, this is just because when we have too little amount of vertices, it just gets bit complicated. The graph containing the, the graph has to contain a decay tight spanning subgraph where k, like I said before, is the degrees of freedom. It's technically the dimension of the isometry group of the norm space. Okay, we can give, well, yeah, we can give better numbers to what we said about k, because I've just said k is the degrees of freedom. Okay, we can say that k has to be at least d, so you always have translations so translations will always be an isometry so the degrees of freedom of your framework will always have it always have at least d degrees of freedom because you can always just translate the framework but we can actually say it's never going to have the Euclidean count unless, well, I'll get to the second but it can't have more than Euclidean count because basically euclidean spaces have as many isometries as you can possibly have and if you think about it, it's just because the unit ball, you can rotate to every other point on the, the surface of the unit ball which you, if you can do that with a norm space, it automatically makes it Euclidean this is the next point if we have k, if k is equal to d plus one, choose two then the space has to be euclidean norm space and that's an if and only if so the only case we're going to get with like three six sparse if we've got d is equal to three there's only one norm space which will be three, which will have the three six count everything else will have to have 33435 has to be one of those three now we can't have three five in fact because if k is not equal to six in this case, if it's not the euclidean count, then there's actually another bound it says it's a lot lower, it can't be, it has to be a significant drop so this means we can't, we can have three six sorry, we have 33435 is out we can't have three five so then the next one is the euclidean count of three six so if you have a non euclidean norm space we know it's either a three three or a three four count now for people you know about sparsity and tightness, you'll notice that both of those numbers are actually generate matrix, which is quite important okay, I'm going to quickly go over the euclidean stuff I've just put a remark saying all euclidean norm spaces of the same dimension are isometrically isomorphic so if you have a euclidean norm space, there's an isometry from it to any other euclidean norm space so this is a result of Polachek and Geringer and Leman they did not write it like this I've written it a bit weirdly um, so I'm saying that if I have a 2d normed plane, a euclidean norm plane, then there is going to exist an infinitesimally rigid framework GP in the norm plane, if and only if the graph contains a two three tight spanning subgraph. Now you can actually go further. You can say there's a lot, there's like an open dense set, open dense residual set.
00:42:41.578 - 00:42:59.724, Speaker A: In fact, it's actually a lot stronger. Okay, it doesn't. We. So we can say more, but this is kind of the basic sort of result I want you to think about. So we got this result here. So this is the maximal count. We've said that the max we already know from the Maxwell count, infinitesimal rigidity implies this DK tightness.
00:42:59.724 - 00:43:29.684, Speaker A: And we've now got a result that says DK tightness implies existence of an infinite estimated framework. And as you probably know from Shinichi's talk, the reverse is not true. If D is greater than or equal to three. Very famously so. And here is the famous counterexample, the double banana graph. What we have here is. So I'm saying that the problem here is three six sparsity doesn't generate matriid.
00:43:29.684 - 00:44:09.644, Speaker A: And if you want to think about it, both k five graphs, if we have a three six sparse matriid, if it existed, if we join them together and move in an edge, that should still be a circuit. But in the three six sparse matriid, it's not, because it's the double banana graph, and that's actually got a three six count. So something funny has gone on. This is still a circuit in the rigidity matriid, by the way, it's just in the three six sparse matriid, if it existed, which it doesn't. So this is where things start breaking down. Okay, let's move on to 2d Non Euclidean. Well, we can say a similar result here.
00:44:09.644 - 00:44:51.004, Speaker A: So sorry. We can say that if we have a 2d, non euclidean normed plane, then there's going to exist an infinitesimally rigid framework in the normed plane, if and only if the graph contains a two two tight spanning subgraph. So three has gone down to two. This is kind of the degrees of freedom has gone down from three to two. Okay. This framework gives a good image of what's going on here, because we've got a k four, which is rigid, it has an infinitely rigid placement, another k four, and we join them at our vertex. Now, normally this would be flexible because we have the isometries that just rotate one round the other.
00:44:51.004 - 00:45:17.860, Speaker A: But in a non euclidean space, we're not going to have this extra isometry. If we had a continuous motion of isometries like this, we'd have to be a euclidean space, it would increase our isometry groups count to three. But we know that if you have three degrees of freedom, that means you have to be a euclidean space. So we don't have this extra degree. We have like a finite amount. But it's like it'll go from here to a 90 degree turn maybe, or something like this. We don't have a continuous motion.
00:45:17.860 - 00:45:59.178, Speaker A: So this framework is actually rigid and it would be rigid in any non euclidean normal plane. Okay? And the reason of this is this result was proof for multiple different norm spaces. It was proof first for LQ norm spaces, then it was proof for polyhedral norm spaces. And then it was just proved for all norm spaces. But yeah, we have this count here. Okay, one more thing. So the two too tight count actually is equivalent to having two edge disjoint spanning trees and no extra edges.
00:45:59.178 - 00:46:35.102, Speaker A: Than that is another thing you might want to keep in mind. So we did it for two d, three d. Things start getting difficult because we don't even know if there exists infinitesimally rigid frameworks. But we can say some stuff in LQ spaces. And LQ in some ways are the perfect balance of euclidean and non euclidean because they have from Euclidean, they have this idea of genericness. Genericness exists so you can find placements where for all intents and purposes, you can think of it as a generic placement. So I am going to now say the graph is rigid in this space.
00:46:35.102 - 00:47:01.892, Speaker A: We can say this and it's not too ambiguous what we mean. Note that I'm not doing Q equal to one or infinity. Them two are not smooth, so we're just doing the ones in between. We also have that infinitesimal rigidity. As long as Q isn't equal to two, well, that gives us D degrees of freedom. So we're going to have the DD tight spanning subgraph. That's the condition.
00:47:01.892 - 00:47:54.930, Speaker A: So if you invert rigid, you have a DD tight spanning subgraph. Now we conjecture that a graph is going to be rigid in any sort of smooth LQ space if and only if it contains a DD tight spanning subgraph. So why do we believe this is true, but it's not true in the Euclidean case? Well, most importantly, DD sparsity does form a matriid, which is a very useful result. So this actually is a matrix. And because of this, t sums. So that's where you take two graphs with t cliques and you glue, well, not cliques, sorry, t vertices, you glue them at the vertices and you remove one edge from the intersection. That will always if you have rigid circuits, that will always produce rigid circuits.
00:47:54.930 - 00:48:26.374, Speaker A: So we can't have these problems that we get in the Euclidean case, like the double banana, we're taking two k five s which have rigid circuits glowing at an edge, removing the edge, and we've got something that's flexible. This can't happen. And in fact we can kind of create this move called the banana handle, which I won't go into, but it effectively involves gluing a k five minus an edge anywhere on your framework and it will still be rigid. You can do stuff like this. So double banana like bad graphs don't exist. And this is why we think it is true. In LQ norm spaces.
00:48:26.374 - 00:49:06.384, Speaker A: We don't have too much evidence. It works in 2d case, but does in the Euclidean case. We also have that we can say it's true for a sufficiently large set of graphs. So if your graph is formed from K 2D by a sequence of d dimensional zero extensions, one extensions, vertex splitting operations, then three are the same as they will be in the Euclidean case. The only one that's different is the vertex decay 2d extension. You replace a vertex with a copy of K 2D, which is a very non euclidean move. And then you just share the neighborhood edges around however you want.
00:49:06.384 - 00:49:32.954, Speaker A: If you do any of these operations, the new graph will be rigid. Okay. And this is as much as we can say. We can say it for certain special cases, but at the moment it's still an open question. But we still. We think it is true. Okay, I'm going to skip this stuff because of time constraints, but just say that we can.
00:49:32.954 - 00:50:18.398, Speaker A: There is a results on disk packings and we can extend disk packings to other types of packings, actually. So I'll quickly show the image. So here we have a circle packing and we can change it to some other types of convex bodies because convex bodies and norms are kind of a one to one correspondence. I'll leave that for another day. Some of the topics I didn't have time to cover graph flattenability in various norm spaces. So this is, you have a framework and you want to say, okay, it's in n dimension saying I want to flatten it down into d dimensions. Is this always true? This is very different study.
00:50:18.398 - 00:50:44.876, Speaker A: It uses very different ideas. In fact, the euclidean and non euclidean results tend to just be the same, except in the L infinity case. So it's very different sort of thing. There's a reference here for that. I will mention something about this, but I didn't want to go into too much detail. But you can do continuous motions. I should really put continuous flexes sort of thing, some continuous flexibility of frameworks and norm planes.
00:50:44.876 - 00:51:06.384, Speaker A: This has also been covered. You have to do it a lot more on a case by case basis. But in some ways this isn't a surprise. You have to do this with euclidean things as well, such as like the k 33 graph. You would have to think about what you're doing very carefully. But yeah, there is some work on this. When it can happen, if it can happen, it often implies that the space is euclidean.
00:51:06.384 - 00:51:48.680, Speaker A: Interestingly, and the last one is, I've generalized things with norm spaces, but you can generalize in other ways. So these people went with, so Stacey and Mahoney, they went from, we have distance constraints which are euclidean. To imagine it's okay, it's all on a manifold, the constraints are some sort of like smooth constraints between things, and then we have an isometry which is invariant to them and we can do it that way. It's very abstract way of thinking about it. There is some overlap with what I've done, but there's a lot of not overlap because I do a lot of non differentiable stuff going on and they always assume smoothness. So it is very different. And they mostly just get some asimo and rough style results.
00:51:48.680 - 00:52:06.024, Speaker A: But it is his own thing that is involved with formation control and stuff like this. Okay. And here is a list of references, and I think that is my time done. So thank you for that.
00:52:09.644 - 00:53:01.892, Speaker B: So thank you, Sean, for a really interesting and sort of quite broad overview of a nice topic that's really developing recently. So I'm going to throw things open to the audience for some questions. So if you for some reason prefer my voice to yours, I don't know why that would be the case, but should you. Sorry, actually, then you can also put it in the chat and I will read it out. I know that this is a session chair service, so if not, I actually have a question. So it's about packings. So, so I guess you get the two two count for your packings instead of two three.
00:53:02.068 - 00:53:33.170, Speaker A: Yep. I'll just show you the result now. So that's your result. This is a bit of a dumbed down version of your result, by the way, so I do apologize, but I was trying to make a point about this. So the version we get is that if you have a regular symmetric body that is not an elliptic disc, then for all choices of positive scalars, for almost all choices of positive scalars, then if you have those radii and a homothetic packing and you put it together, then you're going to get a two, two sparse graph. So you can think of it.
00:53:33.202 - 00:53:37.654, Speaker B: So are you able to actually realize all the two, two sparse graphs with some body?
00:53:38.474 - 00:54:23.704, Speaker A: No, but the next result is the next best thing. So we can say that this is almost always true. So for almost any choice of convex body, this will be true. Now, obviously this is not all of them because I'm saying too, too sparse and too, too tight can't be recognized with Euclidean. But the hope is that some way we can make some restrictions on getting rid of the two too tight stuff and those ones that cause problems. Focus on the two, three sparse ones and use this sort of idea to prove it would be the idea. It seems likely to be true always, as long as the unit ball is not sufficiently bad, like a square or something.
00:54:23.704 - 00:54:37.664, Speaker A: So squares can cause problems because you can't kind of randomly get a k four in there without going to non degenerates.
00:54:38.604 - 00:54:43.424, Speaker B: I guess I'm imagining just controlling the edge directions by changing the body slightly or something.
00:54:43.894 - 00:54:50.594, Speaker A: Yeah, basically. Yeah. It's a lot of technicalities, but that's the general idea.
00:54:52.014 - 00:54:52.406, Speaker B: Okay.
00:54:52.430 - 00:54:52.590, Speaker A: Right.
00:54:52.622 - 00:55:00.314, Speaker B: So yeah, if you give yourself some body. Right. I guess I'm imagining if you give yourself some shape freedom, you can control edge directions and this will give you two. Two.
00:55:00.814 - 00:55:24.904, Speaker A: This is exactly the idea. It's you, you change the body slightly, but you make it so that the points don't shift too much when you do it. You have to be very careful when you do it. You also have to show density. So you have to show that there's a nice set of points of ones where it does work. And I basically went for kind of the analytically described ones. You can show that a dense subset of them will have it.
00:55:26.604 - 00:55:35.864, Speaker B: That's really nice. Cool. So if there are no other questions.
00:55:36.624 - 00:56:24.464, Speaker A: Yep. So do you know anything about whether bulk or would work for LCP spaces? And if that's too hard, at least is there a. What's the sensible analogue of KD plus two, comma D plus two? And have you tried to calculate the rank in 4D? Just to see if you get a, some specific P norm and just to see if there's a nice flexible circuit or something of that form? Okay, so bulk or Roth. So you're going to get slightly different results because say, K 33 is always flexible in LP spaces if P is not, or LQ spaces, as I was calling them, Q nine. It's independent though? Yes, it's actually independent. So that's a good thing. So it's not completely hopeless.
00:56:24.464 - 00:57:13.404, Speaker A: I think you would probably, if you want to bulk a rough style result when you say it's on a certain type of surface, LQ spaces where Q is an even integer, it seems like a good place to start. Things are going to get a bit messy though, because your stresses that you're thinking about. So your set of equilibrium stresses will still be a linear space, but the set of things that describe is no longer like a linear thing. And this is, this kind of ties into, I didn't want to go into it too much, but when you do the global rigidity results, stress matrices and all this just go out the window completely. It's a complete nightmare. You don't have the stress matrix at all. So a lot of nice tools that you get from it, you don't have, you have to do everything the hard way basically, which is very difficult.
00:57:13.404 - 00:57:33.694, Speaker A: But yeah, it'd be interesting to have a look at it though, to be honest, just even to do like four dimensions, whatever the analog of KD plus 2D plus two is, just to test the rank for some specific, for the four norm, say. Yeah, it'd be interesting to check out. I've never, I've never computed it. No, I never looked into it.
00:57:37.474 - 00:57:45.854, Speaker B: Okay, so there is a question in the chat. So Mira asks for the packing result. Do the obstacles of the Euclidean case remain for D equals three?
00:57:47.834 - 00:58:14.406, Speaker A: Yes, in a way, yes. So let me think. So the obstacles are basically this packing dime. Oh, hang on, I think the answer is still yes, because three six tight is as low as you're going to get. Yeah. So I think the obstacles are still there. I don't know there's much way to get around it.
00:58:14.406 - 00:58:37.124, Speaker A: In some ways there's also extra obstacles. So yeah, I won't go into it, but there's extra obstacles you can obtain. Things can get even worse in media, other norm spaces, this is usually the thing, things even become very, very nice and simple, they just get a lot worse.
