00:00:00.240 - 00:01:10.654, Speaker A: So what are we going to talk about in, in this week's class? Here's an outline. So, I'm going to start by talking about incidence results and their connection to invariant theory. And that will lead us to two algebraic objects, the bracket ring and the Grassman Cayley algebra. And I'll talk about their relationship and how they come up and arise in rigidity theory. The main kind of part of the talk today will be on pure conditions of planar frameworks that admit motions when they're in special position. So I'll say a lot more about that, and then we'll talk about how pure conditions behave under projections in zero Henneberg moves. And I'm hoping that we get to that point by the end of today, but it's possible that we won't.
00:01:10.654 - 00:02:09.594, Speaker A: But in any case, in the continuation, we'll talk about pure conditions of spatial frameworks. So by that I mean frameworks in three dimensions, where I'll do some examples. And then we'll also talk about Wu's method to prove geometric statements. And there I'll draw a connection between a theorem due to Walter Whiteley and Baron Sternfels and how that connects up with Wu's method. Okay, so let me start by talking a little bit about incidence geometry. And please, if you have a question or any comment, please just speak up. I'm not so great at following the chat right now, so.
00:02:09.594 - 00:03:30.562, Speaker A: Or if you put something in the chat, maybe Mira can interrupt me and tell me to look there. All right, so let me say something about incidence geometry and just connect it so, with what we've been talking about. We've been discussing which distances between vertices of a graph are known given distances on the edges, right? That's one of the things that, that we talked about, how to given distance constraints on the edges of a graph. What other information can you infer from the fact that your graph is embedded in some metric space? For instance, I'd like to shift the conversation slightly to focus on replacing distances by incidents. So by that, I'm going to focus on which incidences are determined by other known incidences. And here's an example, kind of the prototypical example of what I mean, shown here in the bottom right. Let's take two blue lines and three points on each of the two blue lines and join those points up in this way.
00:03:30.562 - 00:04:01.552, Speaker A: In fact, these points are joined up in a kind of hexagon formation. If you trace here, can you see my little hand icon? Go from one to six to two to four to three to five back to one. Yeah, we can see that. Excellent. Then you trace out a hexagon. Okay. It's true that the hexagon, six sided figure, crosses itself somehow degenerate in a certain way, but we trace out that hexagon.
00:04:01.552 - 00:05:25.168, Speaker A: And if you identify opposite edges of the hexagon, meaning edges that, like edges one and four and two and five and three and six. Right. If you intersect those edges, sorry, not identify them, but intersect them, you get three new points of intersection, which I've called seven, eight and nine here. And what's true about those three new points is that they always are collinear. So in this sense, this sequence of eight collinearities, the two blue collinearities and the six black collinearities imply this 9th red collinearity. This is known as Pappas's theorem. It was known to the egyptian mathematician Pappus in 340, but it was probably known much earlier than that because his book, the synagogue, is really a collection of interesting mathematical facts that were known.
00:05:25.168 - 00:06:50.104, Speaker A: It was kind of like an encyclopedia of geometry. And it's one of the ways that we know some of the things that were done in greek geometry prior to that. The synagogue is a phrase, of course, refers in some ways to a jewish temple, but in other kind of more abstract setting, it refers to a collection, either a collection of individuals in the temple setting or in this case, a collection of mathematical facts. And if you'd like to get a very nice proof of Pappus's theorem, there's in fact, I think, nine proofs of Pappas's theorem in the first chapter of Jurgen Richter Gebert's beautiful book projectives on projective geometry, which I highly recommend. Right? Or if you don't want to read Juergens book, but you do want an overview of what's in it, you could look in the math Monthly, where I wrote about a five page book review of it. All right? So you might think incidence geometry only has to do with lines and points. And in some sense, that's kind of right.
00:06:50.104 - 00:08:00.764, Speaker A: But those lines and points considerations actually can tell you quite a lot. It can tell you also about curved objects. So, for instance, Blaise Pascal, when he was 16 years old, he proved a generalization of Pappas's theorem, where he replaced the two blue lines in the last picture with a single conic, with a single degree two curve, right, at an ellipse, maybe a hyperbola. And the two blue lines, course are a degenerate conic, reducible conic. And so he noticed that the same theorem holds if the two blue lines are replaced by an irreducible conic. And that's a nice observation in this direction of the result. He was showing that if the six point points lie in a conic, then these three auxiliary points must be collinear.
00:08:00.764 - 00:08:56.772, Speaker A: And this result has been rediscovered many, many times. There's a famous theorem in algebraic geometry called the eight implies nine theorem, which says that if you have two cubics that intersect in, of course they have to intersect in nine points. Then if you have a third cubic that goes through eight of the nine points, then it has to go through the 9th point of intersection as well. And here the two cubics are the set sum of the sets of lines. If you take the lines one, which lines do I want? Every other line. One, three and five and two, four and six. Three lines make up a cubic.
00:08:56.772 - 00:09:53.288, Speaker A: So you get one cubic from that and a second cubic for the other black lines, and they intersect in nine points. And you can see all nine points here in the picture. And where's the third, third cubic? The one that's supposed to go through eight of them will take the union of the blue line, the blue conic, and the red line. And that red line, say, going through seven and eight, it goes through two, and the blue line goes through six. So this degree three curve goes through eight of the points, so it has to go through the 9th point of intersection as well. And that's what this, that's an illustration of the eight implies nine theorem in this instance. So Pascal proved this direction.
00:09:53.288 - 00:11:24.642, Speaker A: And then Breckenridge and Maclaurin to mathematicians in the United Kingdom, they proved the converse direction about 100 years later, that if these three points lie on a line, then the other six points have to lie in a conic. Right? So the point here is that you get statements about conics directly from statements about incidence of lines. And so incidences of lines are really kind of an important way to get access to geometric facts sometimes. Pascal's theorem is called the mystic hexagon theorem, and I wanted just to point out why it's called mystic like, what's magical or mystical about it? I thought about this for a while. I don't know the actual reason why some people called it that. But here's my guess. If the six points lie on a circle and they form the vertices of a regular hexagon, then when you look at the edge of the six points, they're obviously on a conic.
00:11:24.642 - 00:12:51.504, Speaker A: So the opposite sides have to intersect in points, and they have to lie in a line, right? And so here's two opposite sides and they have to intersect somewhere, but they're parallel lines, so they maybe intersect off at infinity and these other two lines are also parallel so they have to intersect off an infinity. And finally, the third set, so what we find is that, is that the three points that we're supposed to obtain have to lie at infinity. And so that means that there's a line at infinity going through all those three points, right? Which maybe isn't so obvious when you're just starting to think about compactifications of the projective plane or compactifications of the regular plane. It's not so obvious that you have a whole line at infinity or that there's some kind of linear structure to those infinite points. And I think that's maybe why it's called mystic. It's because it's giving you information about something that normally lies outside of the euclidean plane that we look at. But for most of the rest of the talk, I want to be talking about the projective plane.
00:12:51.504 - 00:13:48.310, Speaker A: And so let me say a few words about that as well. First of all, the projective plane, I'll remind you, is compact. And because of that we have a theorem that's credited to french mathematician, but actually in the form that I'm going to state it is already in Newton's principia. So Bezou noticed that any two curves without common components defined by the vanishing of polynomials of two degrees, say d one and d two, then they have to meet in d one times d two points in p two. But you have to interpret those points suitably. So I'll try to explain a little bit about that. We've already seen this a little bit.
00:13:48.310 - 00:14:16.274, Speaker A: I mentioned that two cubics meet in nine points. Two degree three curves should meet in three times three points. Here's a simpler example. Maybe it's got a degree two curve and ellipse and it's got a bunch of degree one curves. When we look at this curve, y equals zero, we see that it hits the degree two curve in two points. And that's what we expect from Bezou's theorem. Right.
00:14:16.274 - 00:15:04.034, Speaker A: Degree two and degree one should meet in two times one points. But as we slide this line upwards, the two points of intersection move and eventually we slide the line up to y equals two, where the two points of intersection come together. And we'd still like to call this two points of intersection. So we should count the points here with multiplicity in some sense. And I'm not going to go into details about how you count multiplicity. In fact, that can be a kind of complicated thing. But, but the idea is that one should count points with multiplicity when required.
00:15:04.034 - 00:15:54.234, Speaker A: There's a bit more gymnastics that are required. When you slide the line even further up, it looks like this line y equals three doesn't meet the ellipse at all. That would violate besieu's theorem. So what's going on? Well, algebraically, when you plug Y equals three into this equation and solve, you do get two solutions. It's just that the X coordinates are complex, so you get two distinct complex solutions. So in our interpretation we should be counting complex solutions. And then finally, as we saw in the previous slide, we should also be willing to count solutions at infinity because sometimes that's required as well.
00:15:54.234 - 00:17:09.284, Speaker A: All right, so we'll sometimes potentially use Bazoo's theorem. But it's important for us that the projective plane is compact and has some nice properties like this. So let me just try to remind you about the projected plane and some of it, the ways we talk about it. So I'm going to think of the projective plane as a Grassmannian. And since the Grassmannian may or may not be something that everybody's familiar with, let me just try to give you a brief introduction to Grossmanians. So we're going to define the space GKN to be a parameter space so that every point in the parameter space designates a k dimensional subspace of n dimensional, of an n dimensional vector space. Okay? So you can think of gkn, if you wish, as just a set of k dimensional subspaces of n dimensional space.
00:17:09.284 - 00:17:58.960, Speaker A: And you might say, well, I know an example of such a set. When I look at the one dimensional subspaces of a three dimensional space that's giving me lines through the origin in three dimensional space. And each line through the origin is a point in projective space, right? A point in the projective plane. So g 13 is another name for g 13. Then it's just a projected plane, p two. And in p two, the german mathematician Moebius introduced these homogeneous coordinates. And they're quite helpful, of course, for us.
00:17:58.960 - 00:18:50.440, Speaker A: So they say that given a line, given, sorry, a point in p two, it can be thought of as a line through the origin in three dimensional space. Of course, that line goes through lots of other points. If it goes through the point x, y, z, then you can use x, y and z as the homogeneous coordinates for the line. And you might say, well, wait a sec, that line went through a lot of points, right? It didn't just go through one point, went through lots. And that's true. So this single coordinate is not really well defined as a coordinate for the line. Instead, we get a whole equivalence class of coordinates here.
00:18:50.440 - 00:20:15.222, Speaker A: And we make use of the fact that, that all the points on the line are scalar multiples of one another, right? That like a second point on this line is, say, three times x, three times y and three times z. So what we do is when we look at homogeneous coordinates like this, we identify any two points with the same homogeneous coordinates or even identify points whose homogeneous coordinates only differ by scalar multiple, non zero, scalar multiple. And because of this identification, I and many other authors like to put these colons between the points instead of commas. That reminds us that it's really the ratio of x to y and x to z and y to z that are the important things to determine the point as opposed to x, y and z themselves. Because of course you could scale the points. All right, here's a little picture of projective space again. And remember, point and projective space is something like one of these red lines.
00:20:15.222 - 00:21:00.994, Speaker A: It's aligned through the origin in three space. And one thing that we often do is we identify a portion of projective space and say that it's the kind of finite part of the plane, a projected plane. And to do that, what we could do is we can intersect with this plane. Z equals one. And we can see that almost every line through the origin hits this plane. Z equals one at a unique point with z coordinate, of course, equal to one. And if x y one is that point, then the coordinates, homogeneous coordinates for this line have to form x y one.
00:21:00.994 - 00:21:49.318, Speaker A: And in particular, the last coordinate is not zero. So we can kind of, as long as the last coordinate of a point in projected space is not zero, we could scale it so that the last coordinate is one. And we're talking about a point, a finite point. We can identify these lines with the points in the plane. Okay? So we identify the point x y one, say, with the point x comma y. So we sometimes think of this big subset of projective space as being the finite part of the projected plane, which is kind of the usual euclidean plane. And then what's left over? Well, the lines that don't hit the plane, those lines are parallel to this plane.
00:21:49.318 - 00:22:20.554, Speaker A: And so they must be lines in the xy plane themselves. So they're lines whose homogeneous coordinates look like this. They've got an x and a Y coordinate, but their z coordinate is zero. And we think of those points as points at infinity. There's in fact a projective line of such points as Pascal pointed out to us. Right. And it's in fact the line z equals zero.
00:22:20.554 - 00:23:08.328, Speaker A: Why do we say that those points are at infinity? Well, let's just do a little calculation here is I'm tracing out some points here on this finite projective part of the projective space. I've got the .0 y one. So let's see. It's got a zero x coordinate, but it's got y coordinate that's increasing. So the points are kind of going along here as y gets larger and larger. And notice that if we scale this point by dividing by y, we get the same .011
00:23:08.328 - 00:23:52.996, Speaker A: y. But as y goes to infinity, one over y goes to zero. And so you can see that in the limit you reach this .010 excuse me. And this is one of the points at infinity. And this kind of calculation kind of justifies maybe or explains why we call these points points at infinity, because they're the points that you reach by tracking a line off to infinity. Okay, so that was just a reminder of some of the notation around the projective plane.
00:23:52.996 - 00:24:50.224, Speaker A: We're going to actually be talking a fair bit about another Grossmanian. It's the Grossmanian G 24. So this consists of two dimensional subspaces in four dimensional space. So one dimensional subspaces in four dimensional space would just be three p three just be three dimensional space. And if you've got a two dimensional subspace in c four, then that corresponds to a line in three space. So we're talking about lines in three space being parameterized by the Grassmannian G 24. And I'd like to try to explain a little bit about how we think about these lines and what homogeneous coordinates would look like for them.
00:24:50.224 - 00:25:47.114, Speaker A: So if you've got a two dimensional subspace in four dimensional space, you could pick a basis for it. And here I picked two basis vectors and I've written them as rows of a two by four matrix. And you can say, well, those two basis vectors kind of represent the, represent the two dimensional subspace that we're talking about. And notice that if we picked two vectors that were a basis, then this matrix m has to have full rank. Right. And remember when we looked at points in p two, we said there are lines through the origin and we picked a non zero point on the line and used it as our homogeneous coordinates. Right.
00:25:47.114 - 00:26:21.750, Speaker A: That's very similar. We were picking to what we're doing here before we were picking a basis vector. Really x y z. Right. For the line and now we're picking two basis vectors, right? Or two vectors in a basis. And before we insisted that the basis vector that we pick couldn't have degenerate rank. If you looked at the one by three matrix given by that basis vector, it had to have full rank as well.
00:26:21.750 - 00:27:50.164, Speaker A: This is very similar to what we did when we were constructing the projective plane. We're just constructing now a similar kind of thing up in three space. Okay, but there's many different bases for a given two dimensional subspace we could have picked a different basis, right. And so how do the matrices relate to each other if they both represent the same subspace? Well, it means that if they both represent the same subspace, you can do a change of coordinates from one to the other. And the change of coordinates can be realized by a, by a row by row operations. And so there's a two by two invertible matrix so that a times this m gives you, gives you the other matrix that you're looking at, right. So I guess what I'm trying to say is that there's really an equivalence class of matrices that correspond to a given subspace, given two dimensional subspace and they all differ by multiplication on the left by an invertible matrix.
00:27:50.164 - 00:28:53.044, Speaker A: Okay, so now that we've got these matrices, what could we do with them? Well, we could take their two by two minors. We could take all the two by two submatricies of m and take their determinants. And here there's four columns, so there's four. Choose two or six, six two by two minors. And if we take their two by two minors, the six of them, we could use those minors as the coordinates of a map from our g 24, from our set of lines in p three to six dimensional space. So the first coordinate would be x one times y two minus x two times y one. And you would have similar coordinates for each of the other for all six.
00:28:53.044 - 00:30:07.484, Speaker A: This is a map, but it's not a well defined map because of course if you change m, change your matrix representing the line in p three, these, these numbers change, right? Because what, what happens? Well, you're taking the determinant. If you change m by multiplying on the left by an invertible matrix, you're going to, you're not going to change the actual subspace, but you're going to change these numbers. In fact, how do these numbers change? They change, each of them gets multiplied by the determinant of a, which is a non zero number because, because a is invertible. So in fact, the six numbers that you're mapping to, they're not well defined, but they're well defined up to scale or multiple. Right? So if we want to make this map well defined, we can, as long as we pass to a projective five space. So that's how we make the map well defined. And so we have this map given by the six miners, two by two minors, sending g 24 into p five.
00:30:07.484 - 00:31:07.986, Speaker A: And these six coordinates are called the pleucor coordinates of our line. So let me just say a few more words about G 24. It's a subset of p five, once you've embedded it in this way. But it's not all of p five. In fact, it's a four dimensional subset. And I've written down one matrix here to show you why, kind of intuitively why it should be four dimensional. So why should it be four dimensional? Well, you could use your row and column operations to produce ones and zeros, and then you would have four kind of remaining values that wouldn't be determined by the row operations.
00:31:07.986 - 00:31:42.016, Speaker A: You could essentially get any value here. And so this suggests that there's at least a four dimensional piece of G 24. And of course, it's possible that you can't put it into this form format, but there's. So there's other pieces of G 24 as well. But it turns out that those other pieces have lower dimension as well. Here's another way to think of G 24 as being four dimensional. Well, G 24 is a set of lines in three space.
00:31:42.016 - 00:32:24.410, Speaker A: And I'm here in one of my bedrooms in my house, and I can imagine those lines in three space. And most of those lines hit the ceiling of my room and also hit the wall over here. Right? Okay, so they hit one of the walls and they hit the ceiling. And on the ceiling they've got two coordinates to say where they hit the ceiling. And on one of the walls, they have two coordinates to say where they hit that wall. And, and so there's four kind of parameters to describe the line. So again, it suggests that G 24 is a four dimensional space sitting inside a five dimensional space.
00:32:24.410 - 00:32:54.534, Speaker A: So it should be a hypersurface, should be cut out by a single equation. That's what we would expect. And I want to explain what that equation is. So it turns out that the miners of M. Sorry, let's go back. We've got our matrix M, a two x four matrix, and we're looking at all these two by two minors of M. You might think, okay, those are six numbers and they're independent but in fact, they're not independent.
00:32:54.534 - 00:33:42.730, Speaker A: There's relations among them, among the minors. And I'd like to explain that to start. So we're going to view the columns of M as four vectors. So this is a bit strange, because before we looked at the rows of M as being the two basis vectors of our two dimensional space, right? But now I'd like to switch perspectives and think of the columns of M. Okay, you could do such a thing. And what I'm going to assume is that v one and v two span two space, right? If they don't, you could just relabel. But let's assume that they span two space.
00:33:42.730 - 00:34:40.508, Speaker A: And so then v three, for instance, is a linear combination of those two vectors. And to find out which linear combination, we could write down this little system of equations here. But v one and v two as the two columns here of this matrix, and x and y are my coefficients of my linear combination, and v three is the vector I'm trying to write down as a linear combination of v one and v two. And we'll use Cromer's rule. And Cromer's rule says that if you want to solve this equation, you can do so very easily. You can take v three and substitute it for v one. You get a new matrix with v three as one column and v two as the other column.
00:34:40.508 - 00:35:25.934, Speaker A: You can take that two by two determinant and divide it by the two by two determinant of v one and v two here. And then what follows, I'm going to use brackets for, for determinants. So this says the bracket three two means take the determinant of the matrix given by 0.3 and 0.2, or vector v three and v two. And so that's x and y is similar, but instead you replace v two with v three. If you write v three, then you can write it as this x times v one plus this y times v two.
00:35:25.934 - 00:36:19.038, Speaker A: And if you clear denominators, you get this linear combination. Right. Okay. And then what we're going to do is we're going to, we've got these two vectors, they're equal, but two of them, and I'm going to plug them into this matrix as the first column and have the fourth vector or v four be the second column and we'll take the two by two, determine it. Right. And of course, if you plug the same vectors in to here, you're going to get the same number coming out. But when you plug in this vector using multilinearity of the determinant, you can kind of pull these, this constant out, and you just get the determinant v one v four.
00:36:19.038 - 00:37:12.974, Speaker A: And similarly, here you get one three times two four. And then when you plug v three in here, you get the three four. So we get this equation. And if you move some terms to, I guess, the right side and maybe adjust the determinants so that they look a bit more natural, like three two is really minus two three. Then you get this equation here. And this is actually the defining equation for g 24. Remember, each of these two by two determinants is one of our coordinates on p five, right? So this is saying, among the six coordinates for p five, this is the relation that they satisfy.
00:37:12.974 - 00:38:02.220, Speaker A: They satisfy a quadratic relation given in this way. Okay? So G 24 is a degree two dimension four hypersurface in p five. Okay, it's a kind of complicated statement, but I think we kind of got to it using some basic linear algebra. And that allows you actually to do all sorts of fun stuff. And I don't want to detour too much, but I think this is nice. So I was very impressed by this when I first saw this. You could ask lots of enumerative problems, could state lots of enumerative problems.
00:38:02.220 - 00:39:16.634, Speaker A: And here's one, probably the most famous of the basic enumerative problems. You could ask, how many lines are there that meet four given lines in three space? So, first of all, we might say to ourselves, like, well, why should there be any lines that meet four given lines in three space? And the answer is, well, remember that the lines in three space are G 24, and we think that they've got the dimension of G 24 is four dimensional. And to meet a line in three space for one line to meet a fixed line is a co dimension one condition. Let me try to explain why that's the case. But if I can explain why it's a co dimension one condition, placing four co dimension one conditions should cut us down to a zero dimensional space. So just a finite collection of elements of G 24. And in that case, it's just a finite collection of lines in three space that meet all the other four lines.
00:39:16.634 - 00:40:31.834, Speaker A: So we're expecting a finite number from this, as long as we understand why meeting a line is a co dimension one condition. And here's actually the kind of expression, suppose I've got my line that I'm trying to figure out whether it hits, whether it hits the second line. So I've got the top two rows, let's say, are my, are my variable line. And the bottom two rows are the two basis vectors for the line in three space that I fixed that I'd like to meet. And so we get this matrix. And if you do Laplace expansion not along one row, but along two rows, so it turns out you can do Laplace expansion across more than just one row. And to do that, you take, say, this two by two determinant and multiply by this two by two determinant.
00:40:31.834 - 00:41:26.454, Speaker A: You have to do this for every two by two determinant in the top row, top two rows. So you get like column one and column, let's say three here. And that gets, that two by two determinant gets multiplied by column two and column four here. And so you can see those determinants here. I've written down the Laplace expansion, and the two lines meet if this determinant is zero, because they, because they only span a three dimensional space. Right, their point of intersection. And then those two kind of rays sticking out the point of intersection gives you one dimension, like along the origin up to this point.
00:41:26.454 - 00:42:10.094, Speaker A: And then there's these two directions as well. So they define that they, when they meet, they define a three dimensional space. And so this four by four determinants should vanish, right? Great. So this expression vanishes when the two lines meet. And by this expression, I mean that by these little ones and twos, I mean this is the three four determinant for the second line. In that case, I think it's the line that we're fixing. So these are fixed numbers with the twos, and the sub one brackets are variables.
00:42:10.094 - 00:42:55.638, Speaker A: So we just get a linear equation. So that's what I was saying before. I was saying that there's a co dimension, one constraint to meet a line. And in fact, we know something more now, we know that it's a linear constraint. What we're doing is we're trying to intersect g 24 and these four linear constraints. So what's happening? Well, Bezou's theorem tells us in p five, you would expect the number of solutions to the system of equations to be the product of the degrees. And so we get degree two for g 24 and degree one for all these four lines.
00:42:55.638 - 00:43:34.606, Speaker A: So we should expect two solutions. And that's the answer. There are two lines that meet four lines in general position in the, in three space. And there's a kind of nice way to see this. If you deform those two lines, those four lines, so the two of them meet and, and deform the other two, so they meet, then the line that goes through those two points of intersection, meets all four lines. Right? That's. That's obvious.
00:43:34.606 - 00:44:07.768, Speaker A: That's one of the two lines. Where's the other two lines? Where's the other one? Well, those two lines they're meeting, they span a plane. And these two lines that meet, they span a plane. So these two planes meet in a line, and that line meets all four lines as well. So those are the two. Those are the two lines that hit all four lines in that special case. And a lot of enumerative geometry actually kind of made arguments like this.
00:44:07.768 - 00:45:08.234, Speaker A: They would move the lines to special position and then see the answer. And then they would try to argue that the answer doesn't change when you move further. But eventually, people got pretty worried about that kind of argument, and they were kind of unsure about a lot of things. Like, well, why? Why did the numbers not change when you move? And why exactly? Can you just count the numbers? What about multiplicity? Like, are you missing something? And then Hilbert asked, as the 15th culvert problem, to make this calculus of enumeration rigorous. And essentially, it was achieved using what's called Schubert calculus. So it's one of the successful stories from the Hilbert problems. Okay.
00:45:08.234 - 00:45:57.284, Speaker A: Okay. We did moved a little bit to enumerative things, but we're going to use G 24 a lot. So I think it's helpful to play with it a little bit now. Okay, maybe this is a little overkill, but it suggests something. So, here's a nice theorem due to Neil White. And when you know that there's four, that every set of four lines is hit by two lines, you might ask, well, what sets of five lines are hit by a line, right? Like, normally you'd say, no, five lines don't get hit by any line. But maybe if the five lines were in special enough position, they would get hit by five by another line.
00:45:57.284 - 00:46:54.360, Speaker A: When one line hits a bunch of others, you call it a transversal. So Neil asked for which sets of five lines does there exist a transversal? And what's clear is that this is independent of the choice of coordinates that you put on three space. It doesn't really matter what your choice of coordinates is. So this turns out to be a problem in invariant theory. And his answer was, the five lines have a transversal if this determinant vanishes, and this determinant is a determinant with brackets as the entries, and each of these brackets is a four by four determinant. Each of these points, a one, b one, etcetera, are in three space, three space, projective three space. They're vectors with four entries.
00:46:54.360 - 00:47:48.200, Speaker A: And so these are four by four matrices that we're taking the determinants of. So it's a determinant of determinants. And in the 19th century there were many mathematicians who were intimately familiar with all of the calculus of determinants of determination. Determinants. They do amazing things in their papers. Folks like Kaylee and Salman and all these impressive geometers do some just phenomenal calculations with these kinds of things. But a lot of that kind of fell out of fashion when Hilbert introduced abstract algebra and some non constructive techniques that were very powerful.
00:47:48.200 - 00:48:51.794, Speaker A: And it's only in the last, I don't know, 40 or 50 years that we've kind of come back to these constructive techniques. And now, of course, we can use computers to help us do these calculations. And that means that sometimes these constructive techniques are very useful. So anyway, this is an example of some of the kinds of things we're going to be talking about. All right, so that was a bit of a kind of segue to invariant theory. Invariant theory got a big boost, although it had been going strong for many years prior to this. But in 1872, Felix Klein introduced a kind of program to axiomatize and understand geometries.
00:48:51.794 - 00:50:16.334, Speaker A: And it's called the Erlangen program, I think, in honor of where he spoke when he, when he was talking about this. But he wanted to characterize geometries by which properties are invariant under a group of transformations. So for instance, euclidean geometry, the things that are invariant are statements. They're quantities that are not changed by distance preserving transformation. So if you've got a distance preserving transformation like a rotation or a translation, then you can talk about you've got a property, sorry, that's not changed by such things. And you can say that it's invariant, right? And similar to that, if a property is not changed under a change of basis, then you could say that it's a projective property. So change of basis has the same kind of role in projective geometry that our distance preserving transformations like rotation and translation have in euclidean geometry.
00:50:16.334 - 00:51:07.884, Speaker A: So often we'll be talking about projective transformations, but sometimes we'll also want to talk about Euclidean. So Klein really wanted to say that these geometries were distinct. And at least I think he overstretched the analogy a little bit at the beginning. And at the beginning he really wanted to say, like in projective geometry there's no notion of distance. And I've heard this said many times, but in fact it's not true, you can measure distances in projective geometry appropriately done. And if you look in Richter Gebert's book, it's done there. And this was known.
00:51:07.884 - 00:52:05.852, Speaker A: In fact, Laguerre knew it long before Klein, and Klein knew it too. So I think he was just stretching a little bit, the analogy. And in fact, when he came to write up his Erlangen program, if you look up the reference for Klein's program, it's usually given as this memorial address that he gave to the American Math Society in the early 19 hundreds. It was 1904. And at that point already he kind of backed off some of the statements that he'd made earlier, and he acknowledged that, yeah, of course you can measure distance in projective geometry. It's just not as important, which is probably true. Okay, so we're going to spend a lot of time working with brackets.
00:52:05.852 - 00:53:26.274, Speaker A: And I want to use a very similar argument to what we worked on before to describe why brackets are invariant. So we can use a change of basis, an invertible three by three matrix, and act on p two. Using that change of basis, you could take a three vector and multiply it on the left, and you get a new three vector. And we'll view that as a change in basis in this case. And notice that the three points in projector space are collinear if and only if they satisfy a linear dependence. And that happens if and only if the three by three matrix that you get by making these, the columns of the matrix has determined at zero. So here's, here we're showing that if you have, if you want to measure collinearity, you can do it using one of our determinants, one of our brackets.
00:53:26.274 - 00:54:57.924, Speaker A: And I'd like to remark that vanishing of the bracket is an invariant. It doesn't depend on our choice of basis. And to see that, notice that if you change our basis, so we multiply p one by a on the left and multiply p two by a and multiply p three by a. And if we then evaluate the determinant on the transformed bracket, what do we get, of course, well, you can pull the determinant out and you get the determinant of a, which is just a non zero number, times the bracket one, two, three that we had before. These two determinants, since they differ by non zero scalar, they either, they both vanish at exactly the same time and don't vanish at exactly the same time. So it doesn't depend, the vanishing of this bracket doesn't depend on the basis that you chose, which maybe is kind of obvious, because after all collinearity shouldn't depend on the basis either. Okay, so these brackets, these three by three determinants, they are invariants of projective space and invariants under change of basis.
00:54:57.924 - 00:56:02.510, Speaker A: And we'll see in the next couple slides that brackets on points in p two determine the values of any invariant. So let me just say those things and then maybe we'll take a short break. So. All right. So what are invariants of points in p two? Well, if you had an invariant, something that doesn't depend on some statement that doesn't depend, and on your choice of basis, you'd say that that property is an algebraic property if it's determined by the vanishing of a polynomial. And the property is well defined if every variable appears to the same power in all terms. Otherwise, when you scale your points, the polynomial's values shift.
00:56:02.510 - 00:56:55.574, Speaker A: So that, that's, that's not good. Or at least they shift not in a, not by, by a scaler. They, they don't, they don't, they don't span a one dimensional subspace. They span something higher. And that, that's a problem. But if you, but if you're looking at algebraic well defined properties, um, uh, what we, what we find is uh, is this theorem, the first fundamental theorem of invariant theory, which says that every such algebraic well defined invariant can be written as a polynomial in the brackets. Okay, so if you want, we saw it before that the brackets themselves were, uh, algebraic and well defined.
00:56:55.574 - 00:57:45.918, Speaker A: But now we're saying that the only things that are invariants are things that can be built as polynomials, well defined polynomials using these brackets. But the brackets themselves are far from being independent as we've already seen. So you can use Cromer's rule again in three space. So I won't spend much time on this slide, but you do exactly the same thing that we did just a moment ago. You put your three, if you've got, say, four points in p two, then you just put three of the points down as columns. And let's imagine that those three points span all of three space. And then you write the fourth point as a linear combination of those.
00:57:45.918 - 00:58:35.944, Speaker A: And you can use Cromer's rule to find explicitly what the coefficients of this linear combination are. And then you can substitute the two sides of this equation, their vectors, into this bracket as the first column. And the second and third columns are a fourth point and a fifth point in projective space. And after doing a little bit of rearranging, you get this equation. Notice that when you plug this point in to this bracket you get a determinant that's a four four five. And since the two columns are repeated, you get zero. On the right hand side we get zero, and on the left hand side, we get precisely this.
00:58:35.944 - 00:59:19.858, Speaker A: For instance, you get one four five coming from this p one substituted in. You get the other terms, too. And again, you get this kind of quadratic relation. And these quadratic relations are super important. You can kind of build them up in lots of different ways. But these relations, like the one that we just defined, are called Grassmann plucker relations. And all these relations that come from Cromer's rule generate, in fact, the relations among all the brackets.
00:59:19.858 - 01:00:21.362, Speaker A: And that's the statement of the second fundamental theorem of invariant theory that describes all the relations on the brackets. And of course, if you've got a first and second theorem, you've got to ask like, well, what's the third theorem of invariant theory? And that's much less known. But essentially, you would expect that to be relations on the relations, right? So you'd start looking at resolutions, homological resolutions, of a ring of invariants. We're not going to go there. It's kind of amusing, and it's important. So this ring of brackets, ring of these brackets representing determinants, is an algebra. You can multiply brackets, you can add them and things like that.
01:00:21.362 - 01:00:59.488, Speaker A: You can break down polynomials in them. But of course, we have these relations among the brackets. So it's not a polynomial ring in these brackets. It's got algebra with relations generators and relations. And we call that algebra, the bracket algebra. And it models two different things that are both important to us in some sense. It models the coordinate ring for the Grassmannian of k spaces and n space.
01:00:59.488 - 01:02:18.396, Speaker A: But it's also represents the invariance of endpoints in pk minus one. And we saw that here when we took, when we took k equals three, and we had each point, each column being thought of as a point in p two. So when you kind of think of this bracket algebra in terms of the columns, you're thinking of it as a ring of invariance, of collection of points. And when you're thinking about it in terms of rows, you're thinking about it in terms of the Grassmannian defining equations for the Grassmannian. So there's these kind of two dual notions, and that is both incredibly rich and also sometimes slightly frustrating, because sometimes you can be thinking about things in just the wrong direction. Okay, that's what I want to stop on for now. We've been going for 65 minutes let's just take a five minute break.
01:02:18.396 - 01:03:30.414, Speaker A: That's okay. And I'll take any questions. You can relax for a moment, and we'll come back at 1110 and start again. Mira and I were talking before the start of the seminar as well, that one of the difficulties in this class is that we're all at our home institutions as well, or even just at home, and we all have other responsibilities and other things that keep interfering with our ability to pay attention to these lectures and follow them. And that's kind of, it kind of makes it unfortunate in some ways. I actually really value the interchange with seminar participants, and it's kind of unfortunate that the online format and more importantly, the kind of distance that everybody has makes it tricky to pay attention to things.
01:03:36.514 - 01:04:22.012, Speaker B: Hi, I was wondering, I don't know if this is the second half of your talk, but if you could say anything on the bracket algebra, like anything more on the bracket algebra, because there's some interplay between it and the Grassman Cayley algebra, but I don't fully understand the difference between the two. And. Yeah, and perhaps just outline, perhaps a distinction between them. Something like that.
01:04:22.148 - 01:05:17.574, Speaker A: Yeah, definitely. We'll definitely talk about this. It's a bracket algebra is, get this right here. It's, it's a bigger algebra than the Grassman Cayley algebra. So in some sense, the Grassman. So the bracket algebra has inside of it a somewhat smaller algebra called the Grassman Cayley algebra. And that algebra, the Grassman Kaylee algebra, kind of expresses geometry of linear spaces very nicely.
01:05:17.574 - 01:05:40.550, Speaker A: So how, if you want to describe a line meeting, a plane or something like that, and a point, and then the Grassmann Cayley algebra allows you to kind of write down expressions that somehow encode that. Precisely.
01:05:40.742 - 01:05:44.430, Speaker B: And this is like joins and meets. Joins and meets.
01:05:44.582 - 01:06:47.224, Speaker A: We're going to introduce that, too. Yeah, so that's the, that's a kind of, it turns out that's a subalgebra of the bracket algebra, every expression in the Grassman Cayley algebra of a certain step. So I have to be a bit careful. But roughly speaking, every invariant expression that you're interested in can be written out as a bracket, can be kind of expanded into a bracket polynomial. But we'll see later, maybe on Thursday, actually, that not every bracket polynomial can be expanded as a grass michele expression. And yet the two things are very closely related, essentially. I think they have the same dimension.
01:06:47.224 - 01:06:51.904, Speaker A: So one's slightly bigger, but not much bigger.
01:06:52.244 - 01:07:18.624, Speaker B: Okay, that makes sense. That's kind of weird. Having the same dimension, but. Yeah. So, like, I always get the wrong way around in my head, but, like, a line being contained in a plane would be a bracket equal to zero, right?
01:07:18.744 - 01:07:19.684, Speaker A: That's right.
01:07:19.864 - 01:07:27.716, Speaker B: And that would be a join in the Grassman Kaylee. No, the. Yeah, the crossman Kalia. Or a meet.
01:07:27.780 - 01:07:50.674, Speaker A: You could do that as a join. Yeah, you could say. Because you would expect a line and a plane, usually, to span a bigger space. Right. And when they fail to span such a bigger space, their join would be zero. So you could kind of detect that. That failure by.
01:07:50.674 - 01:07:58.830, Speaker A: And in particular, you detect the geometric condition that the line is contained in the plane by. By seeing that their join is zero.
01:07:59.022 - 01:08:37.942, Speaker B: Yeah. And so, like, perhaps this is the way I sort of thought of it before, and you can maybe correct me. Correct me if I'm wrong. Like a join in the Grotton KD, algebra is a sort of like a bracket. And then there are these some. This is referring to the, I think, Neil White's chapter in a handbook, which you probably read. He writes it as precisely that one.
01:08:37.942 - 01:08:48.625, Speaker B: Yeah, he. He writes it as, I think, a sum of, like, brackets with permuted elements.
01:08:48.809 - 01:08:50.673, Speaker A: Yes. We'll come to this as well.
01:08:50.833 - 01:08:51.449, Speaker B: Okay.
01:08:51.561 - 01:08:52.057, Speaker A: Yeah.
01:08:52.145 - 01:08:53.065, Speaker B: Yeah. Okay.
01:08:53.129 - 01:09:02.641, Speaker A: So hold on to this question. But it's a great one. It's. It's the one that kind of leads us into the things we want to talk about. For sure.
01:09:02.817 - 01:09:04.529, Speaker B: Okay, cool.
01:09:04.721 - 01:09:05.233, Speaker A: All right.
01:09:05.313 - 01:09:07.217, Speaker B: Just one quick last question.
01:09:07.345 - 01:09:08.125, Speaker A: Yeah, sure.
01:09:08.249 - 01:09:14.182, Speaker B: Uh, by invariant, do we mean like, uh, invariant under a projective transformation or.
01:09:14.238 - 01:09:14.454, Speaker A: Or.
01:09:14.494 - 01:09:15.394, Speaker B: Or what?
01:09:15.814 - 01:09:25.086, Speaker A: Yeah, invariant under a projective transformation. Um, or more generally, just a change of coordinates.
01:09:25.230 - 01:09:37.893, Speaker B: Oh, okay. So that would be the change of basis matrix when you're talking about brackets. Okay, I just think I. I think I missed that bit of your definition.
01:09:38.553 - 01:09:41.453, Speaker A: You definitely did it, but didn't write it.
01:09:41.793 - 01:09:42.533, Speaker B: Yeah.
01:09:44.953 - 01:10:13.128, Speaker A: If you've got a property that doesn't depend on your choice of basis, then that's an invariant. For instance, if you had six points in the plane and you wanted to know, do they all lie on a conic or not? It doesn't matter what. What basis you choose. It's not like that's. That answer is going to change if you were in a certain basis than in another one. Okay. The conic itself, the equation of the conic would change, but the.
01:10:13.128 - 01:10:21.444, Speaker A: But the property of lying on a conic doesn't matter. And so that's an invariant property, and it should be expressed as a bracket polynomial.
01:10:21.944 - 01:10:23.168, Speaker B: Okay, thank you.
01:10:23.296 - 01:10:31.584, Speaker A: And, in fact, we're going to see one of those soon, in fact. Possibly right now. Right now, in fact. Okay, so, cool.
01:10:31.624 - 01:10:32.264, Speaker B: Thank you.
01:10:32.384 - 01:11:03.524, Speaker A: Cool. No, thanks for the questions. So, we're actually going to look at exactly this question that we were just chatting about when the six points lie in iconic. And, I mean, we know an answer. Essentially, Pascal gave an answer, right? He said, okay, when these three other residual points are collinear. But I want to come back to this using a Grassman puker relation. So here's the Grassman P.
01:11:03.524 - 01:12:05.216, Speaker A: Luker relation that I'm looking at. But if some of these points are collinear, let's imagine which ones did I say? Three, four, five. If points three, four, and five are collinear, then this bracket vanishes, right? It's zero. So this term goes away, and you just get an equality of these two bracket monomials. These two bracket monomials are equal whenever some third term is zero. So what could we do with this? Well, together with Pascal's theorem, we can say what has to happen, bracket wise, when we have a bunch of collinearities. So, for instance, what happens when all the collinearities, all the incidences of Pascal's theorem, are satisfied? Well, then the .78
01:12:05.216 - 01:12:34.882, Speaker A: and nine are collinear. That was the conclusion of Pascal's theorem. So that turns out to imply this equality of bracket monomials. And a bunch of other coincidences tell you equality of a bunch more bracket monomials. Sometimes they have a sign in front of them, depending on which thing you're canceling. But roughly speaking, you get these. And now here's the great thing.
01:12:34.882 - 01:13:12.470, Speaker A: Awesome. We're going to multiply all of these brackets on the left and all of the brackets on the right to get two equal bracket polynomials. Right. And then we're going to cancel all of the black brackets. All of the black brackets have a counterpart on both sides of the equation. For instance, 597 must appear here somewhere, and it appears here. So these two brackets could cancel, and all that's left are these four red brackets on the left side and these four red brackets on the right side.
01:13:12.470 - 01:13:58.668, Speaker A: If you check the signs, they multiply to plus. And so what we find is that if you satisfy the conditions for Pascal's theorem, that is, if you lie on a conic, then this equality must hold among the brackets. So this bracket equation. Or you could make it a bracket polynomial by moving this over to the other side. This bracket equation tells you when six points lie in a conic. And that's an example of a bracket polynomial telling you something about an invariant property. It's a cool trick.
01:13:58.668 - 01:14:41.414, Speaker A: I call this a binomial proof. And there's a lot of very beautiful things that can be proved with binomial proofs. All right, let's talk a little bit more about what we were just mentioning. The Grassman Cayley algebra. So the Grassman Cayley algebra is a sub algebra of the bracket algebra, but it looks very different. So let me try to explain. So the Grassman Cayley algebra, which I'll abbreviate, GC algebra, it's essentially the exterior algebra on an n plus one dimensional vector space.
01:14:41.414 - 01:15:46.668, Speaker A: But an exterior algebra, you'll remember on a vector space you're allowed to take the wedge product of a bunch of vectors and those wedge products determine differential forms essentially, and it's an alternating product, et cetera. But we're going to introduce two new operations on the, on the exterior algebra and those two operations are going to be called the join and the meet. And so let me say just a few more words about the exterior algebra. How? I'm thinking about it. I'm thinking that the exterior algebra is made of a bunch of pieces, graded pieces. It's the direct sum of graded pieces. And the dth graded piece is, is the vector space that's spanned by the wedge product of d vectors in our vector space.
01:15:46.668 - 01:16:33.584, Speaker A: So the first graded piece is just the vector space v itself. It's just spanned by one by copies of one vector. And the second graded piece has v one wedge v two. And where you're allowed to have any vectors v one and v two picked from v. Right. So you get the second wedge space of your vector space and etc. Goes on all the way up to the dimension of your vector space where, where you'll get a wedge product of all the vectors.
01:16:33.584 - 01:17:15.564, Speaker A: That doesn't make sense. A wedge product of the, of the b, of all of a bunch of basis vectors would be, would be a, would be a representative for that, for that graded piece. It's a one dimensional graded piece. And Giancarlo roda introduced some notation for the Grassman Cayley algebra and popularized its use tremendously. He did something unfortunate, I think, but kind of. I'll try to explain why. I think it's helpful.
01:17:15.564 - 01:17:35.962, Speaker A: He replaced the usual wedge symbol with the upside down wedge symbol with the v symbol. Okay. And he called that the join. And I guess when I think of the join, I think of it almost like a union. A cup. Right. In latex.
01:17:35.962 - 01:18:22.698, Speaker A: And so the join here is given a similar symbol to the cup. So what is the join the join is just, in some sense, it's just the exterior product. You're just using the wedge, but we're just calling it by a different name. But let me try to explain why we call it the join. First, I should mention what the support of an extensor is. First. What's an extensor? An extensor is just a wedge product of vectors in your vector space v.
01:18:22.698 - 01:19:38.064, Speaker A: So you might say, well, that's everything, right? Everything in e is a wedge product of vectors. And that's not quite true. The things in e are sums like linear combinations of wedge products, right? So it's kind of like having a single kind of component rather than having a sum of different components. So an extensor you should think of as a single component, and an extensor like this, which is the join of a bunch of vectors, is the span of those vectors. So the support of an extensor is the span of the vectors that you're joining together. And so in some sense, if you think of span as being like a union where you're kind of building something bigger, then calling these things the join, I think, is kind of helpful in that sense. And we often, because these things are so confusing, we often drop the v's and we'll just write the vectors side by side.
01:19:38.064 - 01:21:13.704, Speaker A: And so this is supposed to represent the join of vectors v one through d. And this extensor has span given by the vectors v one through v. And if you take the span of n plus one vectors, or you take the join of n plus one vectors, what you're doing is you're taking the alternating product of n plus one vectors and an alternating product of vectors in an n plus one dimensional vector space with n plus one vectors. You can think of that as the determinant of an n plus one by n plus one square matrix, right? And so that determinant is a bracket. So you can see that in some sense, when you've got something in the n plus one th piece graded piece here of Eric, you're getting sums of such things, and so you're getting actual brackets. So before I said that the, the grass McKeley algebra is a sub algebra of the bracket algebra, and now I should take that back, they're kind of not quite right. That's not quite true.
01:21:13.704 - 01:22:16.124, Speaker A: It's that the n plus one dimensional pull piece of the Grassman Kaylee algebra as a subalgebra, and that's the one that we actually often care about. But, but I guess the Grassman Kele algebra somehow has more things, right. It's got more graded pieces and those aren't subalgebras of the, of the bracket algebra. So I'd say that I misspoke there earlier. Okay, well, what do we do when we've got two extensors? Like if you've got an extensor like this one join of vectors X one through XK, you've got another extensor, the join of vectors y one through Yl. Then how do we take their union? Well, you just take their union by, by taking this wedge product and wedging it with this wedge product. So that, very easy.
01:22:16.124 - 01:23:26.354, Speaker A: And so that's what we call the support of their join. So that's the join of these two extensors is the wedge product, and the support of their join is the span of their supports. But like we were just talking about before, if you had one of the extensors representing the join of say two points, which is a line, right? And another extensor representing the join of three points, which is a plane, and if the line lies in the plane, then when you take the wedge product, when you take the join and you're supposed to wedge all these vectors together, you get zero because the vectors are, are dependent, right. There's a dependency among them. And so the support of the join of two extensors is the span of their supports. If the vectors are independent. If they're not independent, then you get zero.
01:23:26.354 - 01:24:11.284, Speaker A: So that's a more complicated thing. But you can, but you essentially detect dependence using the join is one way to say it. Okay, so I mentioned that we have the join and that we're also going to define the meat. Up till now I've just been talking about a somewhat more complicated way of writing the wedge product. So we haven't actually done anything new, but we've given it something, a new name. Let's talk about the meet. So if you've got again, extensors X one through XK, so the join of K points, the join of l points.
01:24:11.284 - 01:25:31.372, Speaker A: And if K plus L is already at least the size of your ambient vector space, then we can define what's called the meet. And the meet is a somewhat complicated object, how to say this? So you take Y one through yl and you put some of the points in x, you put them at the front of Y one through yl in a, you pick enough of them in order to make a bracket. Okay, so I guess you have to have N plus one of them here and then you leave the rest as just join and then you take a sum, signed sum over all such brackets that you could make. But the permutation that you're allowed to use is a little bit complicated. You're only allowed to, to use what's called a shuffle permutation. So you're allowed to pick some of these points X one through XK. But what we want is to preserve the order of the points.
01:25:31.372 - 01:27:26.552, Speaker A: So if I pick one and three, I want these guys to be one and three here. I don't want them to be three and one, although normally a permutation would be allowed to flip the order like that here. We want to kind of maintain the order amongst these things that we've picked to put into here, and we want to maintain the order over here as well. So, and the sign is just going to tell us, if you, if you have these x's and put them next to these x's and you want to reorder them to give X one through XK, what's the sign of the permutation plus or minus one that you have to use? So we'll see this in a bunch of examples and it'll become a bit more familiar, but it's a somewhat complicated notion of, like, what's the meat? And several years ago, Liz Duran and Jessica Sidman and I were sitting around talking, and this, this topic came up. We said, like, why is the meat defined the way it is? Like, why should it be defined this crazy way? Right? And eventually we decided that the reason is that the meat is supposed to be dual to the join operation, that there's a duality that you can define. On the exterior algebra, which flips graded pieces, the top graded piece goes to the bottom, etcetera. And that duality should somehow turn joins into neats.
01:27:26.552 - 01:28:17.314, Speaker A: And if you write that down carefully, this is actually what comes out, this formula. So in some sense, that's what it's encoding. And this ensures the support of the meat of two extent, of two extensors is the intersection of the supports of X and Y. So it's not even clear that if you have an extensor here and an extensor here and you take their meat, you're going to get an extensor. But it's true that it is. And the thing that you get is the extensor that represents the supports of X and Y. The support is the intersection of their supports.
01:28:17.314 - 01:29:00.100, Speaker A: So the meat is kind of like trying to intersect two subspaces in the same sense as the join is like taking the space of two subspaces and of course, once you've got the meat and the join, you can make a whole lattice. And there's also a poset, wonderful mathematics that involves lattices. And Rhoda, of course, you know, is a combinatorialist with really broad interests, was interested in that poset and he wrote a lot about it. There's really nice papers on that. But I want us just to get a bit of practice with the meat. So let's, let's give it some. Let's try.
01:29:00.100 - 01:29:31.924, Speaker A: Okay. So let's practice the meat in the join and it'll become clearer. Okay. So let's consider two lines, AB and CD. So by Ab I mean take a point a and a point b and join them with a line and CD similarly. And I'm going to work in, I've said in P two, of course, I really want to work in the ambient vector space, three dimensional space. So I'm in a three dimensional space, a is a three dimensional vector, b is a three dimensional vector.
01:29:31.924 - 01:30:04.164, Speaker A: And I've got their join, which is ab, which is a join b, which is a line. Okay. And I guess it's a line in projective space, but it's really a two dimensional space in our three space, right. Two dimensional space in our ambient three space. So I've got these two dimensional spaces in our ambient three space and I'd like to try to understand what's their meat. Okay. Well, of course their meat should be this, this point, right? That this point x.
01:30:04.164 - 01:30:45.174, Speaker A: So what I want to do is I want to express x this intersection point. I want to express it as a vector on the two dimensional space spanned by c and d. So I'm going to express x as lambda, C plus mu d. So lambda and u are numbers and c and d are vectors, right? I'm trying to express x as this union. Sorry. It's this linear combination of these guys. And then what I'm going to do is I'm going to join AB to x and on the left side I get ABX.
01:30:45.174 - 01:31:31.324, Speaker A: And ABX is the join of three vectors in three space is going to be a bracket. It's going to be the bracket ABX. And ABX is zero because X lies on the line AB as well. So there's that dependency, so that's zero. So there's the bracket ABX and it's zero. And on the other hand, when I join AB to this linear combination, I get lambda times ABC and that join of three vectors in three spaces, a bracket ABC and plus mu times Abd. So what we get is that lambda times ABC plus mu times Abd has to be zero.
01:31:31.324 - 01:32:26.922, Speaker A: And I can find lambda and mu just by observation that forces that. Right. If lambda is Abd and mu is minus ABC, then I get zero. In fact, any scaling of that pair will also give zero. But here's a pair that expresses that's appropriate, is Abd lambda coefficient times C plus the mu coefficient, which is minus ABC times D. So X is given as this linear combination of C and D. But do the same argument with A, B and C D switched, and you get that X is also a linear combination of.
01:32:26.922 - 01:33:20.924, Speaker A: Absolutely. And b. Right. And it's got, turns out it's got these particular coefficients. And so that tells us that if you move everything to one side, you get this nice expression here, and not sure why I put this in red, except that what I'd like to do now is join this, both vectors here, the vector complicated vector on the left hand side, and the vector zero on the right hand side. With ae, what do we get? Well, I get AEC from here, get Aed from here. Here I get aae.
01:33:20.924 - 01:33:51.284, Speaker A: And aae is zero, because I've got a repeated twice. Right. And if you want to think a wedge, a is zero already. So maybe this was red in order to remind me that this thing drops out. And then I get AEB as the bracket here. And so we get this expression, and this expression we've seen before. This is one of the Grassman pluger relations.
01:33:51.284 - 01:34:27.100, Speaker A: The Grassman Cayley algebra forces the definitions of the meet and join kind of encode the Grassman plucker relations as well, using this kind of approach. All right, here, mostly we use the join. Let's see if we can use the meat as well. Not sure that I do. Well, we'll see. So here's another example. I've got three concurrent coincident lines.
01:34:27.100 - 01:35:06.482, Speaker A: This will be useful to us later. So let's do this one. So I've got the line ab, I've got the line cd, and I've got the line ef, and they all meet in one point at x. And I'd like to try to understand, what's the bracket condition that characterizes when I've got three lines like this that are coincident. Well, okay, we can write x as before as a linear combination of c and d using the same trick. And then we could join X with ef. Right? When we join X with ef, you get, get this expression, ef with x.
01:35:06.482 - 01:35:53.374, Speaker A: And then ef with c gives us this. And ef with d gives us this, and e of X is zero. So you get that abd times EFc is equal to ABc times e of D. And this expression is going to characterize when these three lines are coincident. These three lines are dependent, so to speak, in p two. Okay, let's go back and try to see what we can understand about conics. So suppose we had six points that lay on a conic.
01:35:53.374 - 01:36:43.480, Speaker A: Then the conic that we're looking at, it's a member of what's called a pencil. So what do I mean by that? Well, first, I should explain what I mean by this symbol. One two, x. So one and two are the homogeneous coordinates for points one and two of our six points. And x is the homogeneous coordinates for a general point, x one, x two, x three, or xyz, if you wish. Okay. And of course, I could plug, if I plug that into a bracket like this, when I expand out the bracket, of course, I've got a determinant here, and it becomes a linear function in the coordinates of x.
01:36:43.480 - 01:37:16.654, Speaker A: Right? These other two columns are fixed. They're numbers. So this is a linear function. And I've got a different linear function here. And of course, if I multiply two linear functions, I get two lines, right, or at least they're vanishing, defines two lines, and they define these two red lines. The red line that goes through points one and two, which is this guy, and the red line that goes through points three and four. So it's a particular clonic that goes through all four points, one, two, three and four.
01:37:16.654 - 01:37:52.464, Speaker A: And I've drawn it a separate set of lines, but somehow I've mislabeled them. Sorry about the picture. The picture should go from go with one and three lines between one and three and two and four. Sorry about that. But I've got a different set of lines here. Yeah. And it turns out that every conic through four, through these four points, one through four, is a linear combination of these two conics.
01:37:52.464 - 01:38:54.924, Speaker A: And so that's what I mean when I say that it's a member of a pencil. A pencil consists of the linear combinations of two things, in this case, two conics. And so we could try to find these lambda and mu. And because the conic that we're looking for, the one through that also goes through five and six, because it goes through x equals five, you can plug five into here and get zero. And that's going to tell you what Lambda and Mu are just like before. So now we know what Lambda and Mu are, and they turn out to be these two other brackets evaluated at five. So for instance, if I evaluate this at five, I get 135245 and it's the coefficient lambda and the coefficient mu is this evaluated at the red bracket evaluated at five, the negative sign, 125345, the negative sign.
01:38:54.924 - 01:39:31.094, Speaker A: Okay. And then we want our conic to also go through six. So you plug x equals six in there and you get that this polynomial has to vanish. And that's exactly the polynomial that we had here. Right? So you can recover a bunch of nice things using the meat and the joy. Although I think I've really only used the join here at this point, I don't use the meat very much. I might have to wait.
01:39:31.094 - 01:40:27.114, Speaker A: Okay, so I'm going to speed up a little bit because I think some of you have already seen frameworks. So let me just set some notation. So g is going to be a graph, it's going to vertices and edges and p is going to represent a realization into n dimensional space. And we're going to be interested in whether our graph admits nontrivial infinitesimal motions that preserve the edge lengths. So for each edge we get this squared edge length and it's given to us from our realization. And I'd like to preserve this edge length, right. So I'd like to find velocities, call them versus velocities, of all the vertices such that this length is preserved.
01:40:27.114 - 01:41:31.794, Speaker A: So one way to do that is to, is to take a linear motion here of our vertex, a linear motion of our vertex b. And what I'd like is that this distance between a and b should be the same, should continue and so its derivative should be zero. And when you take the derivative and set and set time equal to zero, we want this condition to be true. So this is the vector difference between the positions of a and b. And these are the velocities of a and b and they should be perpendicular, which is the key requirement. So infinitesimal motions are solutions to these equations. And it turns out that these equations define for us a linear system.
01:41:31.794 - 01:42:15.026, Speaker A: And so the v's that we're looking at, the velocities that we're looking for are solutions to particular linear system. A matrix times v is equal to zero. And the matrix depends of course on our graph and our positions of the vertices that we started with. And the matrix is built in this way. I think we've seen this rigidity matrix many times before. It's got one row for each edge and it's got a column for each coordinate in a vertex. So there's n coordinates for the vertex a, there's n coordinates for the position of vertex b, et cetera.
01:42:15.026 - 01:43:33.776, Speaker A: And so I get nv columns, and row ab has entry pa minus pb. So this is really a vector, right? It's n dimensional vector. And so I've written it as one item, but it's really nice components. And then pb minus pa in the pb in the b columns, again, n entries here, and then zeros everywhere else. Okay? And this matrix, we're interested in its kernel, right? We're interested in its solutions to mv equals zero. And it turns out that the trivial or rigid motions or rotations and translations are always a solution to this. So this matrix always has a kernel, and the framework is going to be said to have a non trivial infinitesimal motion when the matrix has more than this in its kernel, that is, when its rank is less than the number of edges minus n plus one over choose two, and that rank has to be of course bigger than zero.
01:43:33.776 - 01:44:27.614, Speaker A: Otherwise you don't really have anything in there. So I'm probably being a bit loose with questions about the framework. I'm assuming that it's a full framework, that it's vertices span n dimensional space and things like that, but maybe I can skip those details. So we say that framework GP is infinitesimally rigid if its rank is equal to this quantity, and if g is at least n vertices in general position. So that's the fullness that I was talking about. And for n equals two infinitesimal rigidity. So infinitesimal rigidity in the plane implies rigidity.
01:44:27.614 - 01:45:57.954, Speaker A: There's no non rigid motions of g that preserve edge lengths in that situation. But for n bigger than or equal to three, infinitesimal rigidity is strictly stronger than rigidity. And all of you, I think, probably know that Lamann and earlier Paula Jack Geringer showed that for n equals two, a generic realization of a graph is infinitesimally rigid if the graph contains a two three tight spanning subgraph, that is, if the number of edges is twice v minus three for the graph, and if for every induced subgraph we have the number of edges on that induced subgraph is two times the number of vertices minus three or less. And we'll say that a graph is isostatic if its generic realization is infinitesimally rigid, and also if the rows of m are independent. So what we're requiring here is that the graph be generically infinitesimally rigid. And our interest in this week's talk is mainly in special positions of isostatic graphs. So which isostatic graphs, which graphs that are generically rigid actually have motions when the vertices are in very special position.
01:45:57.954 - 01:47:07.646, Speaker A: And those special positions are, they're very special, but they're not dependent on the coordinates that you use for your vector space. So the existence of such special positions is an invariant, and we're going to try to measure that. And the condition that says that your framework is in a special position where it has a motion when generically it doesn't have a motion, is called the pure condition. It's called the pure condition. So whether an isostatic framework admits a non trivial infinitesimal motion doesn't depend on a choice of coordinates, as I was just saying. So this is an invariant property and there's a polynomial in the brackets, then that's bit built using the locations of our vertices. That vanishes when there's an infinitesimal motion.
01:47:07.646 - 01:48:21.040, Speaker A: And so we'd like to be able to write down this polynomial and say ah, if this polynomial vanishes, then you have an infinitesimal motion, non trivial infinitesimal motion, and that polynomial is called the pure condition. And I'm not totally sure why it's called the pure condition, but maybe only, maybe it's called pure because it only involves the positions of the vertices. Let's have a look at that. Okay, so in order to compute the pure condition, we need to talk about what a tie down is, and I think we've seen this already in Mira's talks when we talked about doctor plans. We have to sometimes tie down some of the vertices so that they don't move. So to actually compute a pure condition, we need to characterize when that rigidity matrix m has less than full rank. And we're assuming that our framework is isostatic, so that generically it's rigid.
01:48:21.040 - 01:49:26.462, Speaker A: And we're assuming that e is nv minus n plus one over two, so we don't have enough rows to make the matrix square, we're missing n plus one. Choose two rows. So when n is three in the plane we're missing three rows, and when we're working in three space we're missing six rows. Okay, and what we're going to do is we're going to just add three or six rows to our matrix in order to make it a square matrix. We're going to do that in such a way that we don't change the rank of the matrix itself. Okay, so it turns out that we can add rows to the matrix to make it square without affecting the linear independence of its rows. And we do this by adding fictitious edges, a times ax where a is a vertex and x is like a new vertex that we're kind of tying down a to.
01:49:26.462 - 01:50:54.430, Speaker A: And as we call it a tie down. And if we choose the position of x to be the position of a minus ei, the standard basis vector, then it turns out that what we're adding is an identity matrix in the bottom left corner and followed by a bunch of zeros. And we're not going to bother writing down columns for the tie down vertices in the rigidity matrix because we're going to assume that the tie down vertices, the axes, but they don't move at all. Right, and this has the effect of removing all the rigid motions from our framework. For instance, if we're in the plane we're tying down one vertex with two, forcing it to connect up to two vertices. And that has the effect of essentially forcing that vertex to be zero zero and then another vertex, the third tie down, we apply it to another vertex and that has the effect of forcing it to not be able to rotate. You can kind of move it, slide it in a one dimensional direction along the line that forms with zero zero.
01:50:54.430 - 01:51:30.040, Speaker A: So we could kind of force it to be, say on the x axis. And that's what we did earlier, right? We've done these kinds of things before. Okay, so we apply these tie downs and now our matrix is square. A rigidity matrix is square and now we can compute its determinant. And its determinant should be, should vanish precisely when the columns, original columns in the rigidity matrix weren't independent. That is that we had an infinite non trivial infinitesimal motion. So that's great.
01:51:30.040 - 01:52:17.874, Speaker A: So the augmented matrix m is square and when it's determinant vanishes we get the pure condition holding. What we'd like to do is write the pure condition, write this determinant of the square matrix as in terms of the brackets, and it can be written in terms of brackets. We just have to find out how to do so. Okay, and so let me show you an important trick. I think this is really beautiful trick. It comes up in my calculus three class. But somehow I never really understood this trick to apply to rigidity until now.
01:52:17.874 - 01:53:21.884, Speaker A: But suppose you've got a vertex a and it's tied down to vertex x and also tied down to vertex tied down vertex y. So then this determinant, this is, I'm working in two reals, so this is really a vector with two components in it and another vector in two components. So I could take the two by two determinant of this matrix and by this I mean really the determinant of the difference not of vertices but of their positions. Right. And if you write their positions as position of a is a one, a two and then one for, because we're thinking of two reals as, as point in projective space as a point in the finite projective plane. Will. Yes, can I interrupt for a second? Sorry about that.
01:53:21.884 - 01:54:15.580, Speaker A: Is there a natural stopping point soon so that we can, and I'm kind of aiming for it now but, and so I think we should finish in the next two or three minutes. Okay, sounds great. Thanks. Yeah, yeah, thank you for the prompt. Okay, so it turns out that this determinant is that this two by two determinant is also a three by three has the same value as a three by three determinant and this three by three determinant is the determinant. But we called a bracket before a, x and y. And so this trick comes up again when we compute pure conditions.
01:54:15.580 - 01:55:36.028, Speaker A: So let me just say, I like to say this and then do the next example and then that's it for today. In a very nice paper in Asiam Journal, White and Whiteley proved that after applying these tie downs and you have to do some tie downs that are so called admissible, then the square determinant that we have, we've got the square rigidity matrix that determinant equals a polynomial in brackets that involves the tie down vertices and another polynomial factor that doesn't involve the tie down vertices. And it turns out that this polynomial factor cg, not only doesn't it involve the tie down vertices but it's independent of which choices of the tie down vertices you chose. And so it's, it's, it's an invariant of your graph and, and this polynomial is precisely the pure condition that we were looking for. So it's very nice fact. Let's see it in practice in this one example. So this is the simplest possible example.
01:55:36.028 - 01:56:25.696, Speaker A: We're still working in R S two and we're looking at the triangle ABC as our framework. Okay. And, and so we're going to tie down a to two spots and tie down b to a single spot. Right. And here's our augmented rigidity matrix. It's got two columns for a, two columns for b and two columns for C and we've got six edges here and I've got the first edge here, ax tie down and then ay the tie down edge and then I've got the edge b, and then the tie down edge, bz, and then et cetera. You can see that this matrix is a very nice form, maybe too nice to be true in general.
01:56:25.696 - 01:57:12.834, Speaker A: But the way we've ordered things, this matrix is upper triangular block. Upper triangular. And so it's determinant is this two by two determinant, which, using our trick, is exactly axy, times this two by two determinant, which is a bz, and then times this determinant two by two determinant, which is ABC. You can see these are the extraneous factors that involve the tie downs. And this red bracket is the pure condition. So the pure condition is the bracket ABC. And so what we're saying is that if you have a triangle ABC, and if this bracket vanishes, then you get a non trivial infinitesimal motion.
01:57:12.834 - 01:57:46.800, Speaker A: And what does it mean that this bracket vanishes? It means that A, B, and C are collinear. So you're in this kind of situation where your triangle is degenerate, and then your infinitesimal motion moves c, but leaves a and b fixed. Right. And that's kind of the classic first example of a pure condition. And this is a good place to stop. Next time I'll talk about more examples and, and we'll get into three dimensions. But this is, this is great.
01:57:46.800 - 01:57:48.704, Speaker A: We got, we got far enough today.
