00:00:00.400 - 00:01:33.304, Speaker A: So being a workshop on low rank, I thought I would speak about maybe the most classical of all low rank problems, which I think we all know, which is simply given a cloud of points that are drawn from a low dimensional subspace, how do we fit that subspace to the data? And this is the classical principal component analysis problem that dates back to even the work of Beltrami and Jordan. And of course we're all very familiar with the results from hoteling and Eckhart, householder and young in 1936. Now, in practice, the issue is that the data is never clean, it's always contaminated by noise and outliers. And much of this talk is going to focus on robust PCA particularly. The question is how do we develop algorithms, theory and analysis for the case where there is also a number of outliers just for the sake of notation? Capital N here is the number of inliers and x is going to be the matrix of inliers, O is going to be the matrix of outliers, and capital M is the number of outliers and x Tilde is going to denote the given matrix which has both inliers and outliers, and I don't know which ones are inliers and outliers. So there is always an unknown permutation. And it's well known that the SBD based solution for finding the subspace is very sensitive to archives.
00:01:33.304 - 00:02:23.700, Speaker A: So what are the classical results? Or a few? A few. The literature in robust BCA is very, very rich. But let me highlight four recent papers and the main message is that all state of the art methods require the dimension of the subspace to be sufficiently low and the number of outliers to be sufficiently small in order to be guaranteed to operate correctly. In particular, one of the most well used methods in computer vision is called ranSac, which stands for random sampling and consensus. And it's an incredibly simple method. All it does is to say let me sample a number of data points at random. Let's say I'm feeding a plane as illustrated here.
00:02:23.700 - 00:03:25.186, Speaker A: I'll sample two points. Those two points define a plane. If those two points are blue points, I'm going to be lucky and that's going to be a good subspace and many points are going to be well fit by it, and then I stop. But generally speaking, I'll sample both blue and red points and I'm going to get a bad subspace and that's not going to be well supported in the sense that not too many points are going to pass through it. You just do this multiple random times until you sample two blue points and you're done. Again, this method works well if the dimension of the substrate is small and if the number of outliers is sufficiently small, more familiar to this community. There was all of the period of research, Mariame and Pablo have both worked on it, and also Emmanuel Candice on low rank approximation, where the idea is that you simply take the data matrix and you decompose it as the sum of a low rank component.
00:03:25.186 - 00:04:12.614, Speaker A: That's going to give you the subspace and a matrix errors that is supposed to be columns sparse, and the number of nonzero columns is expected to be the outliers. Again, this requires, and I'm going to introduce a slightly different notion here, that ratio of the dimension of the subspace, little d by the ambient dimension, capital d. Well, I'm going to call that relative dimension, and usually the results require this relative dimension to be sufficiently small. And the second quantity is going to be the percentage of outliers, m, divided by the total number of points. So results will require these two numbers to be small. And in addition, you need incoherence of the lower rank part with respect to the column sparse part. This is work by Emmanuel Canves.
00:04:12.614 - 00:05:25.984, Speaker A: There are extensions that rely on l one minimization as opposed to nuclear norm minimization. But I'm not going to get into the details, but the same flavor again of results, low relative dimension is needed. But this new notion that if inliers and outliers are well distributed, whatever that means, and I'll become more precise in a moment, then methods perform better. And so this question that you need to also impose or make assumptions on the distribution of the data also arose in this paper. And last but not least, there's been great work by the group of Lehrman, where essentially what it's done is to project the data onto the subspace and then minimize the sum of l, two norms to get a robust measure. So this is very much in the spirit of classical PCA, this objective, except that in classical PCA you would use the provenance norm, and then you do a convex relaxation of this projection matrix p. And you can solve this using semi definite programming, or there are many other techniques to solve this problem, but again, you need the relative dimension to be small and the number of outliers to be small.
00:05:25.984 - 00:06:04.516, Speaker A: Let me actually show that with numbers. If you actually run all of these algorithms, x axis is the outlier ratio from zero to one. The y axis is the relative dimension that I was talking about. Before. So the dimension of the subspace relative to the ambient dimension, and many random experiments are done here. And if you succeed at perfectly determining the inliers versus outliers, or in this case perfectly estimating the subspace as well, then you have success, and otherwise you have failure. And you see sort of a phase transition here for ransac.
00:06:04.516 - 00:07:07.414, Speaker A: And clearly it illustrates that you need the outlier ratio to be sufficiently small and the relative dimension to be sufficiently small. And this is true for the other methods that I mentioned. The nuclear norm style methods are a little bit better, but not a lot better. And similarly, the other l one method, as well as the reaper method from Lehrman. What's important to note is that all these methods fail in exactly the opposite regime for this workshop, namely the high dimensional regime. So if the subspace has high dimension, then no method works, or maybe it works, but you can tolerate a very small fraction of outliers, that in this case is about 10%. So what I'm going to be talking about today is work that began with Manolit seqiris in 2015 of this technique called dual principle component pursuit, where it is particularly designed for this high dimensional regime.
00:07:07.414 - 00:08:13.522, Speaker A: And as you can see from this plot, it works much better than all other methods in that regime, and it also works in the low dimensional regime as well. And it therefore gives the best trade off of performance, allowing you to achieve higher relative dimensions as well as higher outliers. And the method is actually incredibly simple. I'm going to get into the details in a moment, but it's simply an l one minimization problem on the sphere. So here the vector b that we are optimizing over, you can think of it like the normal to the hyperplane, and so you're trying to find a hyperplane that best fits the data in a certain sense. I'll provide all the details in a moment. And so the interesting thing is that unlike all previous objectives, this is a non convex problem, and it's non convex simply because of the unit norm constraint on the normal vectors, but it's also non smooth because of the l one penalty.
00:08:13.522 - 00:09:28.314, Speaker A: So therefore, if you would like to have guarantees both on the optimization side, like can I find the global minimum to this problem, as well as in terms of the geometry, like do I find the global minimum that gives me the true normal to the substance, then you need to do non convex and non smooth analysis to get these results. And in particular, therefore, this motivates what this talk is going to be about. The key questions that I'm going to be trying to understand are the following. First, what are the global minima to this non smooth, non convex problem? And in particular, are they orthogonal to the subspace, which is the main goal of the work? And I'll be providing both geometric and probabilistic analysis. The geometric analysis will tell us essentially that if the inliers and outliers are well distributed, as I'll define in a moment, then any global minimum to this non convex and non smooth problem will be guaranteed to be orthogonal to the true subspace. And the second one is a probabilistic analysis. So let's say inliers and layers are drawn at random according to some distribution.
00:09:28.314 - 00:10:32.974, Speaker A: Then with overwhelming probability, the global minimum will be orthogonal to the true subspace, and the overwhelming probability will require, and will be able to tolerate a fairly large number of outliers. Here, as you see, the results will allow us to tolerate a number of outliers that can grow with the square of a number of inliers. And that's what would allow us to go well beyond the 10% that I was talking about earlier. The second part of the talk, although I'll just touch on this very briefly, so it'll just be a few minutes, will cover algorithms, and I'll focus on one of the simplest ones, which is a projected subgradient method, and we'll have theory for it, and we'll be able to show that it converges, is guaranteed to convert to the global minimum. And last but not least, I also hope to be able to present a few experiments on point clouds and being able to use this method to estimate planes on point cloud data.
00:10:33.964 - 00:11:15.664, Speaker B: Rene, sorry, just one question. So, in the comparison that you were doing, if you're solving somehow your problem with the local method, that's a little bit an apples and oranges comparison, right? Because you're comparing against convex relaxations, you could also solve, like those problems locally, right? If I understand correctly, like all these non convex formulations for completion of things like this, or even for compressed sensing and stuff, where you solve non convex penalty, like these LP norms, for instance, right?
00:11:16.444 - 00:12:10.724, Speaker A: So for all cases, well, RadsAc is special, but I guess at least for this method, you could use non convex formulations if you like. For the bottom, you could use non convex formulations if you like, as well. But I don't think there is been any theoretical results or guaranteeing the a geometric characterization of the global optimum and how it relates to the true subspace or algorithms that would have guaranteed convergence to the global minimum for these cases in the high dimensional regime, which is the one I'm talking about, where, for example, the case of hyperplanes, where the co dimension is equal to one. I'm not aware of those results. And if you know them, I'd love to hear.
00:12:13.504 - 00:12:14.764, Speaker B: Okay, thank you.
00:12:23.364 - 00:13:05.304, Speaker A: All right, so let me begin with the geometric analysis. So, the goal of DPCP is to be able to find a normal vector. Let me begin with a simple phase. Imagine it's just a hybrid plane, so you want to find a normal vector that is orthogonal to as many points as possible. And so one way to do that is to simply compute the distance from every point to a hypothetical hyperplane. And in this particular case, the distance is just the dot product between the data point and the normal. And then you count how many points have a distance that is bigger than zero.
00:13:05.304 - 00:14:04.794, Speaker A: That count is exactly the number of outliers, if b was the true normal. And so what this is really trying to do is to minimize the number of outliers that are fit by a hyperplane with normal b. And one way to write that in matrix notation is simply the zero norm of this x transpose b vector, which is just the collection of all distances from points to the hyperbole. And so we are going to do, actually a relaxation to Pablo's question, where we are going to substitute the l zero norm by the l one norm. So, in that sense, I hope the comparison that Pablo was referring to is a little bit more fair in the sense that we also relax l zero to l one. The reason the problem remains non convex is really simply because of the spherical constraint. But other than that, the objective is a convex objective.
00:14:04.794 - 00:15:08.786, Speaker A: Okay, so, to build intuition for the theory, I wanted to begin with a connection to the continuous case that we developed back in 2015. And the theory also makes some assumptions. So I'll begin by stating those assumptions. We're going to assume first that the data is projected onto the unit sphere. So what that means is that the outliers, which are going to be the blue points in here, are in the unit ambient sphere, while the inliers, which are going to be the yellow points, are going to be at the intersection of the subspace with the unit sphere. And so it's really like a great circle is going to be the inliers and the sphere is going to be the outliers. And so in the classical formulation, you just have points that have been drawn from both the subspace and the sphere, and you're trying to solve the problem that I stated about which is to minimize the above objective.
00:15:08.786 - 00:16:20.872, Speaker A: Now, in the continuous, if you were to do a continuous formulation, then you could substitute the sum of the absolute values of the distances by an integral, where the inlier term would be an integral over this great circle and the outlier term will be an integral over the ambient sphere. And if you assume that the distribution is uniform, it's interesting that it's just calculus three almost. You can actually compute these integrals on the sphere analytically and you get something like this. So the second term, which should be the most familiar, is really going to be equal to the number of outliers, in this case times a quantity that is called the average height of the unit hemisphere and depends on capital d, which is the dimension of the ambient space. And so the second term is a constant. That's, that's the first point. And I was marveled at this trivial calculation at the moment, because what this means actually is that in the continuum, the more outliers the better.
00:16:20.872 - 00:17:18.002, Speaker A: In a sense. If I have a continuum of outliers and they're all symmetric and perfectly well distributed, then the objective function is actually not a function of the outliers, is that this is a constant. All right, so c, sub d is just going to be a constant, depends on the ambient dimension and the total number of outhires. So the only term that depends on the, on b is going to be the first term, and the integral is nearly identical. But because you're integrating only on the subspace, on the great circle, you're going to get here, the cosine of theta, maybe I should have taken the absolute value here, but the theta is the angle between b and the true normal to the subspace. And so you can immediately say that the c, that's a trivial calculation. To see that the global minimum, the only global minimum in this case needs to be orthogonal to or the subspace.
00:17:18.002 - 00:18:21.184, Speaker A: So all the global minima needs to be orthogonal to the subspace. So in the continuum case you would get something trivial. So that motivates to say, okay, what am I going to do in the discrete case? Well, if the number of inliers and number of outliers was sufficiently large so that I would have a situation like this here and sufficiently well distributed, probably the objective would not deviate too much from these idealized continuous objective, and probably the global minimizers would still be orthogonal to the subspace. So that's precisely sort of the motivation for the rest of the talk, is what are measures on the inliers and what are measures on the outliers. That would guarantee you to be sort of close to this continuous situation and therefore still be able to get results pertaining the global optimality. And so here are going to be, now going back to the discrete problem. Here are going to be some of the quantities.
00:18:21.184 - 00:19:06.904, Speaker A: We're going to be looking at the first one. And this first quantity was actually in the paper of Lerman in 2015, is called inlier permanence. And all it is, is really just look at the inlier term. So some of the distances to the hyperplane in absolute value and find the best hyperplane that fits the inliers if you wish. But see, the b is restricted now to be a vector inside the subspace, and then you divide by n. So if we were not doing l one norms, if this was really the classical l two norm squared, you can immediately see this. This would be something like the smallest singular value of the matrix of a lives.
00:19:06.904 - 00:19:57.064, Speaker A: So in other words, think about this cx mean quantity as the smallest singular value of the data inside the subspace, okay? So that's what the inlier permanence is. And a situation like this one would mean that this cx mean quantity is zero. So when the data inside the subspace is the generate or not full rank, then you would have the Cx is going to be zero, and that's going to be bad. And the good situation is going to be when the Cx mean is sufficiently large. In other words, the smallest singular value is sufficiently large. So that's going to be the first measure on the inliers, which, you know, inliers being well distributed will really mean that this Cx mean needs to be sufficiently large. And how large, I'll become more clear in a moment.
00:19:57.064 - 00:20:34.574, Speaker A: The second measure is going to be a measure on the outliers. This quantity, by the way, is exactly the Riemannians of gradient of the outlier objective. So this is the sign of the distance vectors for the outliers. Therefore, this is a weighted average over the outliers weighted by those signs. So that's what the vector o sub b is sort of the average outliers in a sine weighted manner. And then you project. So that's exactly the subgradient and then projected.
00:20:34.574 - 00:21:35.580, Speaker A: And so the largest value of the norm of this subgradient is what this eta o quantity is going to be. And again, you can convince yourself that if the outliers happen to be well concentrated in one region of the sphere, you're going to get a very large value for this EtA o quantity. At the other end, if the outliers are well distributed, you are going to get a quantity that is sufficiently small. In particular, actually, the intuition is what does the average outlier look like? And so when the data is concentrated, the average outlier might be orthogonal to the b vector. So that's why you can get a very large ar, while when they're uniformly distributed, you can prove, actually this is a result. That is true in the continuum case, that the OB, when you have a continuous distribution and that is uniform, the OB is actually equal to b. And so that's why you're going to get a very small.
00:21:35.580 - 00:22:36.306, Speaker A: So Eta O is actually theoretically zero in the continuous case that I was talking about earlier. So what does it mean then for the layers to be well distributed, it means that this Eta o quantity is sufficiently small. We're going to need another quantity measuring the distribution of the outliers, which is what is the maximum value of the outlier term versus what is the minimum value of the outlier term. You might remember from my introduction that in the continuous case, the outlier term was constant, and therefore the maximum and the minimum are the same. So this kappa o would be zero in the ideal case, where the data is uniformly distributed on the scale, but there's going to be a larger gap. And so when the data is badly distributed, there is a gap. And so therefore this means that the outliers have a lot more effect, because the second term has a lot more variability when the outliers are badly distributed.
00:22:36.306 - 00:23:23.874, Speaker A: Okay? So again, we're going to want this kappa to be sufficiently small in order for the outliers to be well distributed. Okay? So with that in mind, let me immediately have a characterization of the landscape, particularly when it pertains critical points. So what this lemma says is that the critical points of the objective are either orthogonal to the subspace, that's what the first part says, or they are very far from the subspace. So at a high level, that's what it says, and more precisely, very far from the subspace means that the angle needs to be, sorry, very far from the orthogonal complement to the subspace. So it means that the angle needs to be larger than this. Theta is sharp.
00:23:24.294 - 00:23:24.734, Speaker B: Okay?
00:23:24.774 - 00:24:42.726, Speaker A: So in the picture here at the bottom right. Therefore, what it means is that there will be that the critical point is either at the top of the sphere, which is going to be the global minimum, or is going to be near the equator, and the angle or the distance from the equator is this theta sharp. But in between there is no critical point of the objective. So that already tells you that if my goal is to guarantee that all critical points are global minima, then I need to make the theta sharp as big as possible. Okay? And interestingly, there is an analytical characterization of the theta sharp in terms of the number of outliers and then m and the number of inliers n, and of course, the quantities eta and c that measure the distribution of outliers and the distribution of inliers. So, for example, you can immediately see that if there is no outlier, if m is going to zero, or if m is small relative to n, then this will be the r cosine of zero here. That's going to give you theta sharp being PI over two, meaning that the critical points, the theta sharp, is sufficiently large.
00:24:42.726 - 00:25:40.628, Speaker A: So if theta sharp is PI over two, then this means that there is no global minimum. There is no critical point other than the global minimum. Again, as you remember from the quantities that I was talking about earlier, eta o is the distribution of the outliers and is small when the outliers are well distributed. In the continued case, eta o also goes to zero and cx becomes large when the inliers are well distributed. So in other words, this angle is really very geometrically interpretable in the sense that things are better when you have less outliers or when their layers are well distributed, or when the inliers are welding distributed. So, in particular, this is what I already said. But the table here just illustrates in words that the better distributed data are, the larger the Cx mean, the smaller the eta o and the larger this theta sharp.
00:25:40.628 - 00:26:12.604, Speaker A: And it will be getting closer and closer to PI over two. And in the continuous case, it's exactly priority. Next. So that was critical points. And as I said, the main point was critical points are either the global minimum, which is at the top here, or they are far. So this is a theorem. Now, it's a sufficient condition that is a little bit stronger than what we had before, guaranteeing that there is no global minimum here near the equator.
00:26:12.604 - 00:27:08.878, Speaker A: And the condition, if we go back, let's go back to the theta. The theta was this ratio m over n and eta over c. So now it's exactly the same, m over n and eta over c squared plus the same quantity, but with the kappa. The kappa, if you remember, was the gap between the minimum and maximum for the outlier term. And so one squared plus the other squared less than one. Then any global solution to the DPC problem must lie in the orthogonal complement to the subspace and as I said, when does this condition hold? In practice, you can have various situations where it holds, but naturally, when the number of outliers is small. But you can actually even have situations where the number of outliers and the number of inliers both go to infinity.
00:27:08.878 - 00:27:46.014, Speaker A: Let's say that you've got a, they converge to having a constant ratio, for example. Then the eta o term will be going to zero and that's enough. And the kappa o will be going to zero as well. So that's enough to satisfy this condition. So in other words, when the outliers are sufficiently well distributed and the inliers are sufficiently well distributed, eventually this condition will hold. And that's why you get, and we see that in practice, in fact, the more data you have, the better results you get. Okay, so let me move to the probabilistic analysis.
00:27:46.014 - 00:28:48.664, Speaker A: Everything I said so far, assume that you give me a data set and I can compute geometric quantities from the data. I mean, they're not verifiable because you're going to give me a corrupted data set. But there are quantities, as I said, like the smallest singular value of the inliers, and they're not verifiable because I don't know which ones are inliers or not. So what about a probabilistic analysis where I say, okay, the data is drawn somehow according to some distribution. Can I guarantee that maybe this one, this inequality, what is the probability that what's on the left is less than one when I randomly draw the data from some distribution? So that's exactly what we're going to do. So if the inliers are drawn uniformly at random from the intersection of the subspace and the unit sphere, this cx mean quantity will be close to one over the square root of the subspace dimension with high probability. That's what this first statement says.
00:28:48.664 - 00:29:52.942, Speaker A: And the other two measures, the geometric measures that we had for the outliers, will be roughly like one over square root. The number of outliers and these quantities can all be derived from concentration inequalities on the sphere, except that you need to consider spherical caps in order to get them. And so if you put all of these together with the condition, actually maybe. Let me first tell you what's happening. So the cx mean quantity, which is in blue, generally increases and the larger the better. And it's upper bounded by the average height of the sphere c, sub d, which is roughly like one over square root of d, while the other two quantities, the outlier, quantities get to be smaller and smaller, as in this case, the x axis is both the number of inliers and the number of outliers growing and going to infinity. And remember, in the condition, I've got sort of ratios of these quantities.
00:29:52.942 - 00:31:02.444, Speaker A: It's like the eta o over cx minus and kappa o over cx means. So the ratio is just becoming smaller and smaller because I'm essentially dividing the red and green quantities divided by the blue quantity in order to verify this condition. So the condition gets better and better as I increase the number of data points. And so this is now the main theorem is that any global solution to the DPCP problem lies, is orthogonal to the subspace high probability. And provided that the number of outliers m is bounded above by the number of outliers squared. And this is a little experiment where the x axis is the number of inliers and the y axis is the number of outliers, and wide means success, namely that you recover the true normal vector to the subspace and black means failure. And roughly there is this transition, there is this boundary here, and that boundary is approximately on the order of the square, which is what is predicted by the theorem.
00:31:02.444 - 00:31:52.774, Speaker A: So how does this compare to some of the previous algorithms that I mentioned at the beginning? I think the important observation is that all previous results would tolerate a number of outliers that would grow linearly with the inliers. So the fact that we've been able to go from o of n to o of n squared in the number of outliers you can tolerate, it's a big deal. Now, this doesn't mean that our result is necessarily better. We need to understand it a little bit more. But things that are worse is our dependency on the ambient dimension. For example, we have the ambient dimension here in the denominator, while other methods have a better dependency on the ambient dimension. So that's, there is a little trade off in there that motivates additional research.
00:31:55.234 - 00:32:21.994, Speaker B: Sorry, like a general question. So this thing relies very, very strongly on the distribution about layers, right? If that would, do you have a sense of how do you compare the robustness of your formulation versus some of these ones? I mean, if the isotropic assumption is no longer satisfied, I mean, how quickly does that degrade versus some of these other methods?
00:32:22.534 - 00:32:48.584, Speaker A: Well, I think this theorem is exactly addressing that question, because the theorems are all under the same theoretical assumptions, namely all have the same generative model, that the data is drawn uniformly from the sphere, and it gives you a comparison of, for the theoretical conditions.
00:32:50.964 - 00:33:04.744, Speaker B: I meant empirically. Right. Like, I mean, it is possible that something works really well, you know, when you have a distribution, but as soon as you move away from it, kind of performance decays very quickly, right?
00:33:07.444 - 00:33:50.994, Speaker A: Yes. So we have not done an analysis that would, you know, try to look at all distributions. One particular case that I want to study is cases where the outliers are adversarial. So there are many cases of deviations that you could consider. But in the, we've certainly studied the uniform case in more detail, and it's very clear that none of that, the, the experiments that I've shown already, already give you a hint of what to expect. But we've tried other distributions and real data sets, and ultimately the only competing method is ransack. None of the other convex base methods are compete.
00:33:50.994 - 00:34:09.634, Speaker A: That's been our experience with various real datasets and various geometric problems in computer vision. We have tried this on, but we've not done an exhausted study like controlling specifically what the distributions are to see whether there might be one regime, what is preferable for one method versus another.
00:34:09.794 - 00:34:23.066, Speaker B: Yeah, yeah. I guess the question was just local sensitivity, right. Whether somehow you're this. I mean, is it intrinsic? Does it have to be intrinsically more sensitive because you have this better dependence on them.
00:34:23.210 - 00:34:23.894, Speaker A: Right.
00:34:25.294 - 00:34:34.754, Speaker B: But why trade off that? Well, because again, I think somehow one is using very strongly the averaging properties.
00:34:35.494 - 00:35:31.064, Speaker A: But I would say that. So first of all, if you're suggesting that there might be a result that is not data dependent, I would doubt that that would be the case, because how the data is distributed has to play a role on, on whether the global minimum is what you want or not. Like, even for classical PCA, when you look at algorithms, you need to look at lipsys constants, and those will be a function of the data for these problems. Like if you want to set the step size of gradient descent, and that's one over the leach is constant, then. And so things do depend on, say, singular values of the data matrix. Always. Yeah, something data independent.
00:35:32.524 - 00:35:53.100, Speaker B: My point is, sometimes some methods are very sensitive to the assumptions and sometimes they're not. Right? I think that is my point. Some algorithms, you may have two algorithms that are equivalent in theoretical performance, but one is extremely sensitive to the assumptions and the other one is not.
00:35:53.172 - 00:36:14.824, Speaker A: Yes. So I cannot answer the question from the perspective, have we done any analysis that is exhaustive to lead to something conclusive? No. Have we tried the methods in sufficiently many data sets where these assumptions are likely violated and demonstrated that the method performs well, the answer would be yes.
00:36:15.564 - 00:36:17.144, Speaker B: Great, thank you.
00:36:18.424 - 00:37:38.564, Speaker A: Okay, so let me briefly, I'm running out of time, so let me just briefly describe the optimization method, although it's relatively trivial, so just use spectral initialization, compute the subgradient of the objective and then project onto the sphere so it's projected subgradient, nothing fancy. And the only maybe interesting thing is that we have a particular way to choose the step size that is geometrically diminishing. And we've been able to prove that if the initial condition, if the initial angle from the b zero with respect to the theta sharp, and that theta sharp was that threshold for the critical points that I spoke a little bit at the beginning, then the tangent of the angle of the kth iterate with the true subspace with a true normal vector is bounded above by beta. Beta is the rate at which you choose your schedule for the learning rate to the k. Essentially the rest, forget about it. But essentially is to say that the tangent of the angle decreases geometrically at a rate beta. And so this sort of shows the convergence of the method to the global minimum.
00:37:38.564 - 00:38:27.898, Speaker A: Let me just briefly, I'll flash this. There is no time, but obviously we have extended these two subspaces of higher co dimension and we have all of the theory for that. We have results that have extended these to the noisy case and showing that the angle is not zero, but bounded above by the noise level. We have extended these to also to a noisy case and have this statistical analysis as a function of the signal to noise ratio. I'll skip all of those results and the same is true for the algorithm. We also have results on the convergence up to a factor that depends on the signal to noise ratio, which is what's represented by sigma. Very briefly to the experiments.
00:38:27.898 - 00:39:06.560, Speaker A: We've tested these on many cases and I only have time to present one of them. This is point cloud data from the kiddie data set. Roughly speaking, you've got a bunch of videos taken by a moving car and you can see the data. Blue points are the ground plane and red points. Everything that is not in the ground plane is an outlier, and typically at least 50% of the point cloud points are outliers. On the top left you see one image on the bottom left. Here you see the ground truth manually annotated as to what is true ground versus not.
00:39:06.560 - 00:39:33.256, Speaker A: And here you see the results of our method that sort of gets it perfectly. This is over large data sets and many frames, so this is typically the f one measure or our area under the curve that ideally should be one. So it's 0.93 for our method, and without getting into the details, but you can see if you just look at the far right, this would be what the nuclear norm style method will give you. So it really fails. It's 0.2 versus 0.9,
00:39:33.256 - 00:40:03.064, Speaker A: and the only competing method that does well released is ransac. And in order for ransac to do well, we need to give it a lot of time, so let it sample a lot. So ten times ransack means that we give ten times the time. DPCP, let's say, runs in 1 second. Just to give one example, we'd let ransac run for ten times that, and it doesn't work. If we let ransack run 100 times, then it works. Just to give you a little bit of a comparison there, this is just giving you more numbers.
00:40:03.064 - 00:40:59.306, Speaker A: But I think the picture already summarizes that. So in conclusion, we've revisited a very, very old problem, robust PCA, in a new domain, which is the case of high relative dimension hyperplanes being one particular case. And this very simple method based on l one minimization on the sphere, which is non smooth and non convex, works very well both in the low dimensional and high dimensional regimes. We've proven that the global minimum is orthogonal to the true subspace, and this is true in both a geometric and a statistical sense. And in the statistical sense, we can tolerate as many outliers as the square of the number of inliers. And we also proved that the riemannian subgrading method projected converges at a piecewise linear rate to the object. Thank you very much for your attention.
00:40:59.306 - 00:41:05.274, Speaker A: Sorry that I went over time a little bit. Great.
00:41:05.354 - 00:41:15.254, Speaker B: Thank you so much. And I think I have the. It's my fault that you run over time. I think I was asking you many things before. So if we have any other questions.
00:41:17.554 - 00:42:01.944, Speaker C: Yeah, I have some curiosity. Do we have really to distinguish between inliers and outliers? And I'll support this assertion by taking you to the sphere that you started with. And I can look at it as block sphere, where superposition is discussed in quantum computing, where they also look, they eye all the time a situation to improve optimization problems.
00:42:08.524 - 00:42:12.144, Speaker A: I apologize, I'm not so sure that I understood the question.
00:42:12.974 - 00:43:50.034, Speaker C: Just a curiosity, intuitive question. It's not criticism at all. I admire your beautiful work and results. I'm just wondering if the possibility to characterize every point on the sphere which happens on the something called block blocked sphere, is this allowing me to break in a way the distinction between inliers and outlier points, because all of them, in your case, sit or live on the sphere. So I can now do the so called convex combination of p times outlier plus one minus p inlier, namely a point can be with probability, have two lives, can be in and can be out with certain probability p, and can be out with one minus p. And that may allow for some may, I'm not sure. It may be interesting to check if the optimization that we all deal with actually can be done more smoothly.
00:43:50.574 - 00:45:07.114, Speaker A: Yes. So I apologize. I don't think I'll be able to answer the question, partly because I'm not familiar with the setting, but at least from where I'm coming from, a formulation where first observation that I want to just reiterate, in case it was not clear, the inliers are expected to concentrate on a low dimensional structure. So it isn't necessarily the case that both inliers and outliers are in the sphere and they have the same distribution. They are expected to have very distinct distributions. The case that you're referring to where I would just draw a point in the sphere with p probability p and one minus p and reverse the roles I don't think would be applicable at least to all of the applications or the problems that we typically work on, where there is clearly a true subspace that exists and that's where the data is expected to lie. And all of that layers are generated due to errors in sensing, or as I illustrated in the real world problem is that everything that is not on the plane that you're interested in.
00:45:07.114 - 00:45:17.246, Speaker A: But I think maybe we need to take it offline because I think it would require me to understand a little bit of background to where you're coming from to be able to better answer your question.
00:45:17.430 - 00:45:18.794, Speaker C: Okay, thank you.
00:45:21.154 - 00:45:25.578, Speaker B: So we have one more question from Laura. Laura, you want to ask it directly?
00:45:25.706 - 00:46:11.968, Speaker D: Yeah, sure. Hi Renee. Thank you so much for the very nice talk. Very clear. I always like hearing you talk about it, your work, because then I understand it 100 times better. So my curiosity is about knowing the rank of the true subspace from the statistics that you show in your theory, like inlier permeance. It seems to me that if you're, let's say, underestimating the true rank, the permiance may even get better in some lower dimensional subspace, maybe where the data of the inliers are concentrating, but maybe the statistics for outliers could get worse for your theory.
00:46:11.968 - 00:46:19.674, Speaker D: It wasn't clear to me. So if I didn't know the rank of the subspecies space, you know, what would the theory be telling me?
00:46:20.334 - 00:48:00.870, Speaker A: Thanks. Great question. And I think if I give this talk in six months, I'll be able to answer it, because that's exactly what we're working right now, and we have a number of partial results in that regard. So, first, just for the sake of clarification, everything we have done in the past assumes that, you know, the true co dimension of the substrate. And so when it comes to the case of unknown co dimension, and that's what we're doing right now, we have explored, first of all, what is the right formula? First of all, there are issues when the co dimension is unknown, because if you want to do something like it's written here in this equation where you enforce orthogonality, then if you over underestimate the co dimension, you're going to have some issues. And so the other question is, the other approach, which is the approach that is on the top, is that you estimate one normal at a time, but then you have the question of where do I stop and what happens? So, without revealing too much about the results we already have, we've actually been able to show that if you forget about the orthogonality constraint and simply enforce the unit norm constraint, and you simply run your algorithm with whatever number of normal vectors we can prove that it converges to. So let's say that you overestimate c, so you have a c prime larger than c, then it would convert to c linearly independent vectors.
00:48:00.870 - 00:48:52.272, Speaker A: And you just need to have different random initializations like akin a little bit to the power method that if your initializations are not on the orthogonal to the true subspace, and then you'll be fine. And so my conjecture to trying to. So we're in the middle of doing the theory so we don't have precise connections with the permians, but that you were mentioning. But my sense is that there is theoretical results would ideally be the same because the pyramids would still be measured in with regard to the true subspace, not with regard to the one you're trying to estimate. But that's still a conjecture. So in six months, hopefully I'll tell you.
00:48:52.408 - 00:48:53.884, Speaker D: Great, thank you.
00:48:54.944 - 00:49:00.284, Speaker B: So maybe if Renee is generous, we can answer the very last question from Akshay.
00:49:00.634 - 00:49:02.450, Speaker A: Actually, there are two questions.
00:49:02.562 - 00:49:08.974, Speaker B: Oh, okay. Somebody Akshay.
00:49:09.994 - 00:50:07.406, Speaker A: Oh, hi. Thanks, Renee, for the great talk. I was wondering if the, if the outliers are also, they have some structure, say they also come from the same substance, would you be able to apply this iteratively to learn both subspaces, or does that mean the outliers are not very well distributed? Great, great question. So this is a problem that also Laura has worked on. So what if now I have two subspaces or many subspaces, and the goal is to do clustering as well as identify outliers. So in that case, actually, let's imagine for the time being that you had a union of subspaces, then you want to estimate one of them and all of the remaining ones become outliers. So it turns out that you can do that.
00:50:07.406 - 00:50:53.054, Speaker A: And some of the papers, I didn't talk about them, but we've already done this. So we have proven that you estimate what is known as the most dominant substance, that the algorithm, the same algorithm, same theory, same optimization problem, but just run on data with different structure, is guaranteed to convert to the most dominant hyperplane. Most dominant is defining the paper in terms of geometric quantities, such as similar to the ones that I mentioned today. But if you wanted the simplest version, it may be the one with the most, the one with the most points. So anyhow, the answer is yes. And the Adao quantities that characterize the outliers are a little bit more sophisticated to capture the extra structure.
