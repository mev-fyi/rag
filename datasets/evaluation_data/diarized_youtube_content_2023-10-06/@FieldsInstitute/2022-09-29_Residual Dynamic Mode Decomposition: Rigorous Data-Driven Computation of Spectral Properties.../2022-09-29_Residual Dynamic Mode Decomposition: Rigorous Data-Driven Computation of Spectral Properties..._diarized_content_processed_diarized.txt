00:00:00.760 - 00:01:08.678, Speaker A: Okay. Hello, welcome back. Now we're going to have a talk about the numerical analysis of Koopman operators. So I think Igor gave us a really nice introduction to this talk, telling us how important spectral properties are of Koopman operators. And Koopman operators are typically infinite dimensional. And when you have finite amount of trajectory data, how do you go about computing those spectral properties in a rigorous way? For example, if the Koopman operator has continuous spectra, the very act of truncating it to a finite matrix destroys the presence of continuous spectra. So how might you be able to compute something about it when your main go to is truncating? You also have problems of spectral pollution, which Igor also mentioned, where you compute eigenvalues that have nothing to do with the underlying continuous dynamics and more about the how you've discretized it.
00:01:08.678 - 00:01:53.738, Speaker A: You can also have problems of too much, too little. And we're going to be answering those kind of questions in a numerical analysis way. So, Matt and I are card carrying numerical analysis. We're not from dynamical systems, but we were reading the book of Steven Brunton and Nathan Coutts. And in his book, in their books, they describe how difficult it is to compute spectra. And we thought maybe we can translate our tools that we've been using to compute spectra in computational physics to dynamical systems and the Koopman operator. So just to give us the setting, we have a phase space omega, and we're pretending that we do not know the underlying dynamics.
00:01:53.738 - 00:02:56.952, Speaker A: So we do not know this function f. We have a discrete time dynamical system governed by that unknown function f. And we get to collect trajectory data from this dynamical system, and we want to recover properties of that underlying dynamics. So we have initial conditions. These are the xms in blue, and we have the YMS, which are the trajectories collected after one time step. Now, for an ergodic system, of course, you could have a very long trajectory that you cut up and you splice it up to form trajectory data of this form, or you could just collect one time step data from your dynamics. Okay, so the classic way of viewing a dynamical system, as you know, is the geometric poncare point of view, which is great if you want to understand fixed points attracting Tor, attracting Torrey and manifold slow and fast.
00:02:56.952 - 00:03:37.074, Speaker A: And this is really good for understanding local properties, let's say, of the dynamics. But for nonlinear dynamics, it doesn't give you a global picture. And that's how Koopman can come in. It can give you a linear operator from which to do some global analysis on. So the way I like to think about this, as I try to introduce it to the numerical analysis community, is to tell them that there's this function g, which can probe the dynamics. Maybe this g has physical meaning, like the velocity of the state or its momentum, or maybe it doesn't. It's just some function that you're probing with.
00:03:37.074 - 00:04:29.262, Speaker A: And what you do with your favorite function. Measurement function g, is at every time step, you just ask, you just apply your g to the state, and you get to read off what your measurement is. Now, after you've been doing this for a while, you might start to think of a look ahead measurement function, which is g composed with f. This would be the measurement you get back if you, or at x, you wait one time step, and then you measure. So g compose with f, is that look ahead measurement function. And once you've got bored with that g, you will start trying different measurement functions, g. And as soon as you start doing that, you start to motivate the use of the Kuman operator, because the Kumen operator, you plug g into the Kuman operator, and it returns to you the look ahead measurement function g composed with f.
00:04:29.262 - 00:05:03.514, Speaker A: Now, because composition operators g composed with f, are linear. This Koopman operator is linear. But what you've done is you've traded, obviously, nonlinear dynamics that you had with the poncare point of view, with f, into typically infinite dimensional dynamics. But it's at least it's linear in the Kubernetes operator. And for this talk, we're going to work in l two in our two space. That's where you get to choose your measurement functions from. Okay, I imagine most in the room know about this.
00:05:03.514 - 00:06:28.824, Speaker A: And why do we really like it? Because if the dynamics happen to be linear, let's say, as a simple toy case, then spectral information would be very useful for long time dynamics. So here I'm just showing you that if you had, if your dynamics was linear and you had the eigenvalues and eigenvectors of a, then you would be able to do the long time dynamics of this dynamical system just from looking at eigenvalues and eigenvectors. I mean, the idea of Koopman, though, that gives you the access to this kind of simple analysis with eigenvalues and eigenvectors to more complicated problems, as, you know, nonlinear dynamics, even chaotic ones. And we really like the Koopman mode decomposition. And so, if your Koopman operator does not have singularly continuous spectra, then you can write down Koopman mode decomposition in the way we've done on this slide. So you take your measurement function g, and let's suppose that, that you can decompose g into a sum, a linear combination of point spectra and eigenfunctions of the Kubman operator. That's kind of analogous to what you would do for a finite matrix.
00:06:28.824 - 00:07:35.594, Speaker A: But then, because the Koopman operator is an infinite dimensional operator, you have other aspects that come in to its spectra, in particular the continuous spectra. And here we've got a continuous parameterization of eigenfunctions there, as we've written it, which is dealing with that infinite dimension part, the continuous spectra part. However, if you can do this Kumar mode decomposition and compute it, then you also have access to things like geometric features. As ignorant was mentioning, the invariant measures can maybe come in. You've got long time behavior, as well as trying to understand some coherent structures. And this is because once you have those broken it up into the eigenvector functions and those generalized eigenfunctions, the action of the Kuman operator in each of those modes is easy to understand. And our task as numerical analysis is to try to compute, uh, aspects of the spectral properties of this kubernetes operator from that trajectory data.
00:07:35.594 - 00:08:20.236, Speaker A: So that, that's our task here, that's our goal. And to try to do that in a rigorous way where we can ensure that we do not compute too much or too little. We deal with continuous spectra correctly and we answer the question at the end, is it correct? And so that's what we do here. So that, of course, the spectra of the Koopman operator is an operator. We can just define that as a set for which the resolvent is unbounded, or that the shifted version of the operator is not invertible. We're going to have to, at some stage, form some finite dimensional things so we can actually compute with them. And when we do that, we have to make sure that we are avoiding spectral pollution.
00:08:20.236 - 00:08:51.044, Speaker A: So avoiding that the truncation produces eigenvalues that have nothing to do with the continuous dynamics. We want to make sure that we don't miss any of the spectra. How do you deal with continuous spectra when truncation destroys it? And we also want to know as we do this, is it correct? Okay, so Matt is now going to tell us how to avoid too much and too little with a thing called resDMD.
00:08:52.944 - 00:09:15.644, Speaker B: Thanks, Alex. Okay, so the first step is to build a matrix approximation of the Kubernetes operator. And for this, we're going to use something called dynamic mode decomposition. So that was introduced in the previous talk. So there's some excellent references for this method down here. But what I'm going to try and show you on this slide is that DMD is really a galerkin. Approximately.
00:09:15.644 - 00:09:52.774, Speaker B: Okay, so we start off with a dictionary of functions, psi one up to psi n on the state space. And we're going to try and approximate these two types of inner products. Okay, so the first is just the inner products amongst the basis functions themselves. The second involves the Kubernetes operator acting on one of these basis functions. Now remember, all we have access to is data like this. So x, blue, y in green, one time step forward. So we're going to treat those x points as quadrature points for this integral with weights w one up to wm.
00:09:52.774 - 00:10:08.740, Speaker B: Okay, so you approximate this in a product via this quadrature rule. In matrix notation, it looks like this. So I've color coded everything. So this is just the matrix corresponding to the dictionary or the basis functions evaluated at the x point.
00:10:08.922 - 00:10:10.524, Speaker C: So psi x.
00:10:10.904 - 00:10:24.084, Speaker B: And then similarly, when you look at this inner product here, all you have to do is trade one of these x's for the y. That just shifts this blue matrix to this green matrix down here where you're evaluating at the y points.
00:10:24.664 - 00:10:25.072, Speaker C: Okay?
00:10:25.088 - 00:11:08.400, Speaker B: So once you've got these two inner products, you can approximate your Kubemann operator on the linear span of these basis functions, or dictionary functions, via this matrix here. Okay, so you take the inverse of this ground matrix and apply it to this matrix here. For those of you who've played around with DMD, this is actually the transpose of what people usually compute. Okay? But, okay, so we've got this finite matrix approximation. Recall the problems of too much, okay, so that's spectral pollution, too little. How do we know we've got all of the spectrum? That's important for capturing all of the dynamics. How do we deal with continuous spectra? And suppose we, for example, compute eigenvalues of this finite matrix.
00:11:08.400 - 00:11:39.400, Speaker B: How do we know which of those eigenvalues are reliable? And by reliable I mean they're close to the spectrum in some sense of the true sort of kubernetes operator in infinite dimensions. Okay, so to tackle this, a bit of audience participation, we've got three matrices really here we've got blue, blue, blue, green. And if you take the adjoint of this, you get green, blue. If we're being democratic, can anyone guess what a missing matrix? If you're just going by the color.
00:11:39.432 - 00:11:44.000, Speaker C: Scheme for the next slide is blue.
00:11:44.032 - 00:12:11.184, Speaker B: Blue blue green green blue green green green green. Yes. Yeah. So let's look at green green and let's see what that corresponds to. Okay, so here are the first two matrices we had on the previous slide. So I've labeled them g for the gram, k one for Koopman operator over here. Green green corresponds to a quadrature approximation of this inner product here.
00:12:11.564 - 00:12:11.932, Speaker C: Okay?
00:12:11.948 - 00:12:43.572, Speaker B: Where I've got a kubernetes operator on both sides here. So I'm going to call this k two. Now, you can think of this as a galerkin approximation of k star k if you take k across. But the important thing for us is that this allows us to approximate residuals in infinite dimensions. So that's why we call our method residual DMD. For example, suppose you have a candidate eigen function g expanded in your, your dictionary with a candidate eigenvalue lambda.
00:12:43.748 - 00:12:44.116, Speaker C: Okay?
00:12:44.140 - 00:13:03.684, Speaker B: Then you can express this squared residual in terms of these three types of inner products, which correspond to this nonlinear matrix pencil involving these three matrices g k one, k two. Okay, now you've got access to this infinite dimensional residual.
00:13:03.844 - 00:13:04.500, Speaker C: You're in business.
00:13:04.572 - 00:13:15.384, Speaker B: Okay, so for those of you with a background in sort of computing things in infinite spectral properties in infinite dimensions, you'll get excited, like I did when I, when I saw this connection. You can now start to do stuff.
00:13:15.764 - 00:13:19.196, Speaker C: Okay, for example, let's look at solving.
00:13:19.260 - 00:13:57.146, Speaker B: The problem of too much. How do we avoid spectral pollution? Or in other words, suppose you come with a candidate eigenvalue. How do I tell whether that eigenvalue is reliable? So here I've written down the residual we had on the previous slide, squared residual, and I've normalized by the approximation of the norm squared of g. So remember, boldface, g is the vector here that represents my function. Okay, so the first algorithm is very simple. It's a very simple addition to dynamic mode decomposition. We first of all compute these three matrices g, k one and k two.
00:13:57.146 - 00:14:29.454, Speaker B: So k two is the additional matrix. We then compute this eigen decomposition. So eigenvectors here, v eigenvalues lambda, and then the additional step is a cleanup procedure. So for each eigen pair, we compute the residual. Okay, so the square root of this thing here, and then we discard those eigen pairs where the residual is larger than epsilon for some input tolerance, epsilon. Okay, so this is sort of an epsilon cleanup procedure. You can make this into a theorem.
00:14:29.454 - 00:15:01.444, Speaker B: So, suppose that the quadrature rule converges. So it's the quadrature rule here as you collect more and more data. So capital m, remember, is the number of data points we have now, snapshots. So suppose that quadrature rule converges, then in the large data limits anything. The output of this algorithm is guaranteed to be inside the epsilon pseudo spectrum. So that means that if you look at the reciprocal of the resolve norm, it's bounded by epsilon. So I'll define the pseudo spectrum on the next slide.
00:15:01.444 - 00:15:07.464, Speaker B: But you should just think of this as a way of measuring how close lambda is to the spectrum.
00:15:08.684 - 00:15:09.464, Speaker C: Yeah.
00:15:12.324 - 00:15:45.524, Speaker B: Quadrature rule converges. So this is quite a general framework. So for example, in the ergodic case, you can have a single long trajectory, and then this would converge by the ergodic theorem. You could have a whole bunch. You could pick these x's randomly, for example, and do one time step forward, and that would again give you a convergence by the strong law of large numbers. If you were free to choose these, you could maybe try and actually build up a quadrature rule with these w's. So as long as you've got that sort of data collection step that, so that these inner products converge.
00:15:45.524 - 00:15:49.358, Speaker B: Yeah, exactly.
00:15:49.496 - 00:15:52.454, Speaker C: Really a complement. Yeah. And that you're saying.
00:15:55.274 - 00:16:00.418, Speaker B: We'll see later on. Yes, we can deal with continuous spectra assuming this quadrature rule converges.
00:16:00.466 - 00:16:03.214, Speaker C: Yeah, yeah, yep.
00:16:13.634 - 00:16:35.596, Speaker B: So that, yeah. No, no, so you've got this reciprocal. Yeah, it's kind of a weird way of writing it, but it's. So, for example, this would vanish. So the resolve would blow up, be infinite, right on the spectrum. Take the reciprocal zero. So this thing here, you can extend it to being zero on the spectrum and it gives you a function that rises off up the spectrum.
00:16:35.596 - 00:17:07.464, Speaker B: So if this function is small, you're close to zero, which is where the spectrum is. There is something in disguise here. If you look at the adjoint, you also have to look at the adjoint of that resolve as well to avoid that kind of thing. But that can be dealt with in the same way.
00:17:07.504 - 00:17:08.124, Speaker C: Yeah.
00:17:15.164 - 00:17:31.904, Speaker B: Ah, yes. So if you, for example, can bound the error in these quadrature approximations or even get a rate, there's a way of passing that through and getting an extra term on the right hand side that corresponds exactly. Great questions.
00:17:32.804 - 00:17:33.204, Speaker C: Yes.
00:17:33.244 - 00:17:38.914, Speaker B: Okay, so that avoids spectra. Oh, there's another question. Yeah.
00:17:46.374 - 00:17:46.822, Speaker C: It'S what?
00:17:46.838 - 00:18:17.482, Speaker B: Sorry, I don't quite. Oh, I see. No, no, so there's nothing. There's no. Yeah, yeah, so there's no connection there. But we will actually use kernel methods in a different setting later on to select the basis functions. But there's the, there's nothing sort of kernel related here.
00:18:17.482 - 00:18:18.294, Speaker B: This is just.
00:18:20.074 - 00:18:20.722, Speaker C: You can think of.
00:18:20.738 - 00:18:22.570, Speaker B: It as just a Galerkin approximation of.
00:18:22.602 - 00:18:25.054, Speaker C: K star k. Yeah.
00:18:25.554 - 00:18:27.694, Speaker B: You get this extra thing for when you square this.
00:18:27.754 - 00:18:28.614, Speaker C: This is a problem.
00:18:28.774 - 00:19:07.994, Speaker B: Okay, any more questions? Great. Okay, so you've got a way of dealing with spectral pollution, but you're not guaranteed to approximate all of the spectrum. Okay? And that's because the eigenvalues up here in this decomposition need not approximate all of the spectrum of the Koopman operator. Okay, so it's very easy to construct examples shift operators where this would fail. So we still have the problem of too little. How do we actually capture all of the spectrum? We're going to have to go beyond just taking eigenvalues of this DMD approximation. So that's where pseudo spectra come in.
00:19:07.994 - 00:19:34.082, Speaker B: Okay, so the epsilon pseudo spectrum is the union of spectra of perturbed operators where the perturbation is at most epsilon. This limit here can be made precise if you put a certain topology on closed subsets of the complex plane. But the point is that as you take epsilon to zero, this pseudo spectrum will converge down to the spectrum. So the spectrum is always guaranteed to be inside the pseudo spectrum and that.
00:19:34.098 - 00:19:36.322, Speaker C: It will shrink down to the spectrum.
00:19:36.498 - 00:19:47.358, Speaker B: Okay, so how do we approximate pseudo spectra? Well, we compute these three matrices as before, and then over a computational grid. So think of these as candidate points.
00:19:47.546 - 00:19:49.006, Speaker C: In the pseudo spectrum.
00:19:49.150 - 00:20:34.170, Speaker B: We compute this minimum residual. So here I'm taking the spectral parameter and I'm minimizing this residual over expand expansions in my linear subspace. Okay, this is just, you can write it out. It becomes a generalized SVD, very easy. And then you output those points over the grid where this minimum residual is at most epsilon and also the corresponding eigenvalue, sorry, right, singular vectors which correspond to the approximate eigenfunctions of the Kubman operator. Okay, so you can prove a theorem again, suppose that the quadrature rule converges. Then you get error control, so the output of this set in the large data limit.
00:20:34.170 - 00:21:19.414, Speaker B: And again, if you don't have, if you don't pass to infinity, there's an extra term here that you can control in some cases, and the output is always guaranteed to be inside the pseudo spectrum, right? So you know the output's reliable, but you also get all of the pseudo spectrum. Okay, so you can prove that it will locally converge, locally uniformly to the pseudo spectrum as you increase the number of functions in your dictionary, assuming you get suitable density properties of those functions. Okay, so we can avoid the problem of too little. So we'll give some numerical examples later on. So you've avoided too much, too little. And now I'm going to hand over back to Alex, who's going to talk about continuous spectra in the next part.
00:21:21.194 - 00:21:25.414, Speaker A: Okay, thanks, Matt. So, to talk about continuous spectra.
00:21:29.354 - 00:21:29.762, Speaker C: To.
00:21:29.818 - 00:22:18.352, Speaker A: Talk about continuous spectra, we first have to talk about exactly what we mean by computing something with continuous spectra. And we don't exactly go for the continuous spectra, we go for some properties related to it. So this is a little bit like white light. We all know that white light is made up of continuous spectra of a rainbow of colors. And in the same way that kind of the Koopman mode decomposition can separate terms up in a similar way to how light is going into a rainbow. But what we compute is more related to this thing down here. So it's not necessarily the wavelengths that can appear, but it's more the intensity in each wavelength is what we're computing.
00:22:18.352 - 00:23:00.938, Speaker A: So here, this is just an example where you can tell from the intensity of wavelengths what's in the earth's atmosphere. And so in this setting, it's that intensity, the energy per wavelength, that is of physical relevance. So, to make this precise, we're now going to. So up to now, we've assumed almost nothing about the Kubernetes operator, I guess just closed and densely defined. So the spectra isn't the whole space. And now we're going to make sure the dynamical system is measure preserving. And in that case, then that means that the Kubman operator is an isometry and the spectrum is contained in the unit circle.
00:23:00.938 - 00:23:46.810, Speaker A: And if there is, well, when there is spectral measure, it's supported on the boundary. So this is a very important assumption for us. This is an isometry. But to make connections to the spectral theorem, we have Wald's decomposition, which allows us to extend the isometry to a unitary operator. So that's Wald's theorem is telling us that you can take a isometry here and write it as a unitary operator with some unilateral shifts, which can be extended to bilateral shifts. So just to make the connection of exactly what we're computing. So we all know for a normal matrix, there's an orthonormal basis of eigen functions that can decompose the complex cn.
00:23:46.810 - 00:24:29.912, Speaker A: So if I take a vector v, then I can write that out as a sum of projections, n of them onto each eigenspace. They're orthogonal projections on each eigenspace. And the action of a on each eigenspace is easy. It's just stretching or shrinking in that eigenspace. And so what we're closer to what we're computing than just the spectrum for the continuous setting is the energy. So if you give me your favorite vector v, then I will return to you the, this kind of energy that the vector V has in each eigenspace, this quantity here, v transpose vj squared. So it's a little bit like this.
00:24:29.912 - 00:25:28.604, Speaker A: If you take eigenvalues that you can plot them discrete, let's say they're real and you've plot them discrete. Well, what I'd return to you is closer to this, what we call spectral measure, which is impulses, the energy that the vector v has in each of the eigenspaces corresponding to each eigenvalue. And then this has a continuous analog, because let's imagine there's a family of matrices a here, and they can get larger and larger. Then as I do this, I will fill in some more spikes corresponding to more eigenspaces that I'm computing. And eventually you could imagine in the limit, there's a function here that envelopes these spikes, which is very close to what we're computing related to the continuous spectra when there's an interval of spectra. And just to make that precise, in terms of operators, if the Kumen operator is unitary, remember that also includes isometries. By unitary extension, there's a projection valued measure.
00:25:28.604 - 00:26:24.204, Speaker A: So sums go to integrals n. Projection operators from the finite case goes to a projection valued measure, now parameterized by a continuous variable y, and the action of the Koopman operator in each one in terms of this projection value measure is easy to describe because it's just stretching or shrinking. Now by this parameter, this continuous parameter y. So this is the analog of the spec of a. It's an analog of a spectral theorem for unitary operators. And in particular, the analog of the spectral measure is kind of the energy that g has as you look on it, over a brown measurable interval of the continuous spectra. So that's, this quantity here is analogous to this quantity here, but this is holding for sets now instead of just individual discrete eigenvalues.
00:26:24.204 - 00:27:03.114, Speaker A: Okay, so we're going to try to get access to this spectral measure as a way of probing the meaning of the continuous spectra. And I don't know, for the audience maybe, can you have two dynamical systems with Koopman operators that have the same spectra in terms of spectral measure? And does that correspond to two topologically invariant dynamical systems? I don't know, but that's maybe an extension of what Igor was asking during his talk. Even for spectral measures addition to continuous spectra.
00:27:06.414 - 00:27:07.194, Speaker C: Yeah.
00:27:15.534 - 00:27:26.354, Speaker A: Okay. I was wondering if that is still true, if I'm allowed to probe it with these g's and find out their energy. Okay.
00:27:29.044 - 00:27:59.360, Speaker C: My personal perspective. Okay. Okay, great. Okay. Yeah. Okay.
00:27:59.392 - 00:28:42.136, Speaker A: So, that was me trying to jump into dynamical systems, and I'll jump back to numerical analysis. And so, how do we actually evaluate this spectral measure? Well, it's not really even point. It cannot even be point wise defined, so we have to be a little careful. And there's a trick that's very common in computational physics, and I've seen it also in the Koopman literature, where you first smooth the spectral measure using a convolutional type kernel. And so, the simplest one you might pick is this kernel, known as the Poisson kernel. And what this is really doing, if you look at it, is actually kind of trying to approach the. The spectral measure lives on the unit circle here.
00:28:42.136 - 00:29:23.350, Speaker A: And you, when you took this epsilon here, it's convolving. And as you squeeze Epsilon down to zero, this is what the Poisson kernel is looking like. So it's eventually tending to a delta function. So, for Epsilon greater than zero, what you're doing is smoothing the spectral measure and then evaluating that smooth version of it point wise. And we really like this Poisson kernel because it can actually be computational. This is relatively well known in the physics literature. But in this case, you can write down what this smoothed version is, not as an integral, but in terms of cauchy transforms and eventually collapse it.
00:29:23.350 - 00:30:01.174, Speaker A: To just evaluating the action of this resolve an operator and computing inner products, which we can do robustly with error control, using what Matt was describing with res DMD. So, that's how we do this. And we do similar things in computational physics, like applications in the continuous setting. So, if the Kumen operator happened to turn out to be a self adjoint PDE, then we can tackle that as well, using slightly analogous ideas from the PDE setting. So, Matt is now going to walk us through an example.
00:30:07.894 - 00:30:40.274, Speaker B: Okay, so, we're going to take an example. So, this is an infinite unitary matrix. The exact numbers don't really matter. I've just picked them because I know what the spectral measure should look like. But this is sort of a generalization of a shift, right, which is a typical building block of many diagonal systems. So, the type of behavior I'm going to show you over the next few slides is quite generic. Okay, so remember we've got this smoothing with the Poisson kernel, right? And we've got this evaluation of the resolvent.
00:30:40.274 - 00:31:15.474, Speaker B: We've been careful to only evaluate the resolve outside the unit disk here. Okay, so this picture here on the left will be important in what files? So, the first thing you might try is fixing the truncation size or the number of dictionary functions that you're considering and varying the smoothing parameters. So I'm going to have the poles of the resolvent here as I decrease epsilon, the smoothing parameter. These will approach the circle, right. There's a zoomed in section here, so you can see the approach. This black curve is the spectral measure or the density of the spectral measure. It's absolutely continuous in this case.
00:31:15.474 - 00:31:29.068, Speaker B: So that's the thing I want to approximate. And the red curve shows the approximation of the convolution with the Poisson curve. So this red curve is my approximation.
00:31:29.196 - 00:31:30.332, Speaker C: Of the spectrum measure.
00:31:30.388 - 00:31:59.094, Speaker B: So, let's see what happens. So we're okay for a little bit, and then stuff starts to blow up as we get very, very close to the spectrum. So I'll play it once more. Okay, so as we get close here, it becomes unstable. And that's simply because we fixed the truncation size. Right. So we're approximating the spectral measure of the Galerkin approximation, which is a finite matrix that has a series of diracs here.
00:31:59.094 - 00:32:31.214, Speaker B: So that's why you're seeing this blow up. So that is not going to give us the sort of infinite dimensional spectral measure. What about if we vary the smoothing parameter? Sorry, fix the smoothing parameter, but vary the truncation size. Okay, so now the poles will be at these fixed locations, and I'm going to increase the truncation size n here. Oh, I should say the truncation size here. It just corresponds to a finite square truncation of this matrix. Okay, so let's see what happens.
00:32:31.214 - 00:33:23.414, Speaker B: So now we converge, but we converge to something that's too smooth, right? Which makes sense. We just fixed Epstein. So this is converging to the convolution of the spectral measure with the Poisson kernel. So, in order to actually get this black curve, what we have to do somehow is take epsilon smaller. So we avoid this over smoothing, but also increase the truncation parameter so we don't blow up. And that's where, if I go back a couple of slides here, the res DMD stuff really comes into play, because you can, by dealing with residuals, figure out whether your truncation size or your dictionary is rich enough to be able to approximate these resolvents, and then you can adaptively choose your parameters to get convergence. So, for example, so watch the truncation size and the smoothing parameter change in union.
00:33:23.414 - 00:34:08.414, Speaker B: I now converge to the spectral measure. Okay, so you might think, oh, we're done. But if we were, then it would be a very short talk. The convergence is very slow. So as I take epsilon, the smoothing parameter, to zero, the error is typically order one epsilon log, one over epsilon. So here I've got the, the maximum pointwise error of the approximation of that black curve, or the spectral measure, the density of the spectral measure, the smoothing parameter here, and you get this slow convergence. Now why is that an issue? Because we're adaptively choosing our dictionary size or our truncation size, which shoots off to affinity as epsilon gets smaller.
00:34:08.414 - 00:34:48.074, Speaker B: So to show this, I've got the error against the truncation size for different smoothing parameters. So you can see, as I decrease epsilon, this slope gets shallower and shallower. And of course, this is a huge problem in data driven computations because typically you're dealing with a small truncation. Nice, right? You've got a finite amount of data, and that means you can't just keep on adding more and more functions into your dictionary and make your matrix larger and larger. Okay, so can we improve this convergence rate? It turns out we can. So there's a way of generalizing the Poisson kernel using smoothing kernels that look a little bit like this. So there's m terms here.
00:34:48.074 - 00:35:22.956, Speaker B: Okay, so m equals one is the Poisson. So remember this is supported on the periodic interval minus PI to PI m two, m three, and so on. Visualizing this, you can think of it as replacing that one dot with m dot. So this corresponds to the third order kernel. And again, there's a way of writing out that convolution in terms of these Cauchy transforms, right? So you've got sort of like an m four version of that plumage lemma or pinching. Okay, so now you can do that. So now here I'm using it with a 6th order kernel.
00:35:22.956 - 00:35:36.156, Speaker B: So there's six resolvent points approaching each point on the spectrum. Notice that I will never approach the spectrum. I'll never get too close to the spectrum to get convergence here. So it's much, much faster.
00:35:36.340 - 00:35:38.104, Speaker C: Okay, so like that.
00:35:40.644 - 00:36:04.030, Speaker B: And that means you can take a much smaller truncation parameter. Okay, so let's actually look at the convergence rates. They're over here. Smoothing parameter. This is again the maximum error for the density function. You see the improved convergence rate. So, for example, if you wanted five digits, and we're using a 6th order kernel, you wouldn't have to take epsilon smaller than 0.1.
00:36:04.030 - 00:37:03.506, Speaker B: So we can make this all completely rigorous. We have convergence theorems that describe the automatic selection of the truncation size with order m convergence rates and error bounds for the density of the continuous spectrum if it's separated from the rest of the spectrum in a point wise and lp sense, integration against test functions. Here you don't need any assumptions on the spectral measure. So here I'm integrating the smooth approximation against a test function, for example, forecasting or something like that, and I get again this order m convergence. You can also use this to recover the discrete spectrum, the spectral decomposition and things like that. Okay, so before I hand over to Alex, who's going to talk about verification, I just want to sort of tell you why verification of these outcomes is very important. Okay, so this is a paper that I worked on where we computed spectra with error control.
00:37:03.506 - 00:38:01.826, Speaker B: So we were able to tell after a computation which part of the spectrum of an infinite dimensional operator we could trust. So we looked at applications in physics. So this is a quasicrystal, and because we were able to verify the output, okay, we had certainty in the computed spectral properties, and then, you know, we could collaborate with physicists and actually discover interesting new physical phenomena. So we're hoping that, you know, verified computations of Koopman operators can perhaps lead to a similar type of happening in dynamical systems. Okay, so we've dealt with too much, too little in the first part of the talk. We've just talked about continuous spectra. I'm now going to hand over to Alex, who's going to talk about verification and answering the question, is it right? So, actually it arises from a certain class of orthogonal polynomials on the unit circle.
00:38:01.826 - 00:38:10.974, Speaker B: So I don't think it's a rotation of that example, but we can talk after. But I can write down explicitly what it corresponds to.
00:38:11.354 - 00:38:16.242, Speaker C: Do you have an explicit formula? Yeah, you do? And it's absolutely.
00:38:16.338 - 00:38:16.570, Speaker B: Yeah.
00:38:16.602 - 00:38:17.174, Speaker C: Yeah.
00:38:20.874 - 00:39:15.064, Speaker A: Okay. So as numerical analysis at the end, we would like to give error estimates as well as try to understand if what we've computed is accurate. And this gets us into the final bit of our talk of can we actually verify some of our computations? And so, just as the simplest toy example, we're going to take a discrete time version of the nonlinear pendulum. Here we chose delta t is like 0.5, and we have x one, which is the angle, and we have x two, which is the momentum. And what we're going to show on the right hand side here is, it's essentially a pseudo spectral plot, which we use a lot in numerical analysis to understand whether computed eigenvalues are accurate and if they're meaningful. And so this gray region here is this pseudo spectra.
00:39:15.064 - 00:40:02.564, Speaker A: As I play the video, you'll see it changing. And the magenta dots here are the eigenvalues computed from the truncated Koopman operator. Now for this nonlinear pendulum, the infinite dimensional Koopman operator has a continuous spectra supported on the circle. So what you would hope is that this pseudo spectra engulfs the entire unit circle once I increase the discretization to be large enough. Also, any of these magenta dots that fall outside this gray region when I play the video should not be trusted because they do not correspond to a place where the resolvent operator is large. So they're spectral pollution. So let me play the video or maybe I'll play this twice.
00:40:02.564 - 00:40:38.200, Speaker A: So here's what's happening. As we're increasing the NK, the truncation size, you're getting a lot of spectral pollution in the middle, but you're also getting some genuinely good approximate eigenfunctions or eigenvalues that correspond to eigenfunctions. And so when we have the nk to be larger, of course, this is starting to engulf the entire unit circle, as it should. And, okay, this is just for epsilon equals 0.25 to show you the band. So it's very visible. But of course you could go away and decrease this epsilon if you wanted a tighter tolerance.
00:40:38.200 - 00:40:44.616, Speaker A: And as you do that, this pseudo spectra will pinch or squeeze up to the unit circle.
00:40:44.800 - 00:40:48.624, Speaker C: Yeah, yeah.
00:40:49.804 - 00:41:20.766, Speaker A: Well this, this is for, yeah, so this is for first, it's discrete time. So we discretize in time. Uh, delta t is 0.5. But we also have to discretize, well, when we, we have to decide how we're going to collect trajectory data and then eventually compute those inner products with quadrature rule. And so here we are taking, uh, minus PI to PI. That's the circle. So we take a trapezoidal rule, which is in numerical, NASA is a Gauss quadrature rule for the unit circle.
00:41:20.766 - 00:41:34.114, Speaker A: And for the real line, you might, you could take many different things, but what we do is we take a, we chop the real line off and then do a trapezoidal rule on the truncated domain.
00:41:38.174 - 00:41:41.802, Speaker C: Yeah. Yes.
00:41:41.898 - 00:42:08.254, Speaker A: So, exactly. So we have not used this simplistic integrator here. But you could. Yeah, we haven't done that here, but you could do that. Typically in my mode of operating is just to rack down the accuracy, to just preserve quantities to 16 digits and call that good. But of course, you can do things with simple integrators as well. Maybe you want me to play this again, just so you can see.
00:42:08.254 - 00:42:32.214, Speaker A: Maybe you should notice that the pseudo spectra is not always just one continuous region and how this is computed. There are many different ways to compute the pseudo spectra, but this one is computed in a naive way, which is you try, you look at the resolvent, how big it is on every grid point. But you can do cleverer things in numerical analysis to compute these regions that are faster.
00:42:33.034 - 00:42:37.294, Speaker C: Okay. Yep.
00:42:40.174 - 00:43:17.632, Speaker A: Yes. Oh, okay. Maybe I should say then, because. Yeah, that's actually genuinely what's happened. Because you might wonder, why does it start here on the right and then squeeze around going left? But if you actually look at the approximate eigenfunctions that are getting calculated corresponding to those magenta dots, then they get more oscillatory as you increase the angle. And therefore they're harder to capture when NK is small, and you start capturing them when NK is large. So that's explaining how it's converging to the entire unit circle.
00:43:17.632 - 00:44:06.164, Speaker A: Does that kind of answer your question? Yeah, I don't know why that is. So anyway, here I've just plotted some of the approximate eigenfunctions that are calculated, like, let's say, the low frequency ones. These are known as phase portraits. Maybe you can see the eye that you usually get with the nonlinear pendulum inside there, closely related to what ignore was saying. With those limit cycles being produced here, we should trust these because we computed these with a residual of 0.05. So we're actually trying to compute them. And maybe you'll notice just that the oscillations are increasing as the argument of the, of the lambda increases, which means that as we go in this direction, they're getting harder to approximate from a numerical analysis point of view.
00:44:06.164 - 00:44:42.644, Speaker A: Okay, so as we keep saying, yeah, we are, we are getting the eigenvalue at one, but we did not plot the approximate eigenfunction associated to it because it was. Yeah, yes, right, right, exactly. It's too simple. So we didn't plot it. Yeah, but we are capturing that. If I go back, you'll see.
00:44:42.684 - 00:44:42.884, Speaker C: But.
00:44:42.924 - 00:45:50.384, Speaker A: Okay, so anyway, in the, at the end, of course, we always have to compute inner products, like the action of the resolvent, as well as inner products and we need this for us, corresponds to cleverly collecting trajectory data, if you are able to. So for low dimensional problems, we like to collect them so that the corresponding quadrature rule is very accurate. So for us, in numerical analysis, there's been a lot of work on good quadrature rules. And so if we can select the initial conditions of the trajectory data to be nodes in a quadrature rule, that can greatly speed up the convergence between our approximation, which is the sum, the weighted sum, and the true continuous inner product that we're trying to approximate. Of course, other things that are more common, as Matt was saying, is like ergodic sampling and Monte Carlo integration, which corresponds to randomly sampling. However, we can also do very high dimensional problems as well. So this, this is a problem which has ambient dimension around 300,000.
00:45:50.384 - 00:46:58.794, Speaker A: This is a fluid problem. We're looking at airfoils that are periodically stacked. There's an inlet on the left and there's an outlet on the right. And here we're showing you the Koopman modes, or three Koopman modes. And so if you show this to a fluid dynamicist, they will look at these and say some things are physically relevant. Like they might try to say there's a turbulent boundary layer shedding off one of the airfoils over there, or there's an acoustic source, because in this problem, they're often caring about the frequencies that you hear as wind goes, as the fluid goes over the airfoil. But we asked the question, can you trust any of these DMD modes, these DMDs, this DMD calculation, which one of these can you trust and which one you cannot? Of course, we can do the same, a very similar computation, but now with the extra matrix, which gives us res DMD and compute, okay, slightly different modes for the same frequencies.
00:46:58.794 - 00:47:30.844, Speaker A: But along with that calculation comes the error estimate that we can tag on with it. So we can now go to a fluid dynamicist, which we have, and showed these pictures, and give them, in addition, some errors that we think. So they know which ones to trust and which ones they shouldn't trust. And this actually changes some of the fluid dynamics for them. Like there's a little bit more acoustic vibration that's coming off. There's a bit more of the turbulent fluctuations coming off this. And the acoustic source on that third one seems to be dampened.
00:47:30.844 - 00:47:47.764, Speaker A: But in these high dimensional problems, how are we going to do the approximation of those inner products? Because in 3000 dimensions, this is extremely large for anybody in numerical analysis. And so Matt is now going to tell us about exactly what we do in the high dimensional regime.
00:47:51.264 - 00:48:21.836, Speaker B: Great. Okay, so when d, the ambient dimension of your dynamical system is very large, then of course you're going to run into the cursive dimensionality. You can't just write down a suitable dictionary. So it's very popular to learn a dictionary. So some methods, such as DMD with a truncated SVD, do this with a linear choice of dictionary. We're going to use kernel methods or kernel EDMD. So that's where you approximate these inner products using the kernel method.
00:48:21.836 - 00:48:51.480, Speaker B: There is a slight connection with the question earlier I should correct myself. So in high dimensions you can use kernel methods to learn the dictionary. You could also use neural networks and things like that. Okay, but a question you might ask, you've got this learning here. Can you trust the output of the learning method? Right? It's very difficult to have any sort of convergence guarantees or theorems telling you that the dictionaries you produce are actually correct. For example, how do you avoid overfitting.
00:48:51.512 - 00:48:52.706, Speaker C: Et cetera, things like that?
00:48:52.890 - 00:49:31.754, Speaker B: So the above algorithms have some nice features. For example, when we're computing pseudo spectra, we do so with error control or verification. When we're computing spectral measures via those cavity transforms and the smooth measures, we have an adaptive procedure to figure out whether this dictionary is rich enough so it's possible to verify the learnt dictionary after you've done the computation. Okay, so you might come to me and say, hey, I've computed this dictionary. Does it work? I'll go away and say, ah, you know, the spectra, I'm very good, maybe try something else. You go away, you try it again, you come back and then it works. So you've got a way of checking afterwards.
00:49:31.754 - 00:49:55.020, Speaker B: Okay, so we're just going to end with a couple of examples of this in action. The first is from real experimental data. So here I have a jet, okay, here I have a wall. I've collected the velocity field in a small window here. The Reynolds number is approximately six times ten to the four. It's highly turbulent. The ambient dimension is 100,000.
00:49:55.020 - 00:50:37.474, Speaker B: Okay, so that corresponds to the positions where I measure the velocity field. And I've got here two spectral plots with two different dictionaries. So on the left hand side we've got what happens when you use a DMD approach. Okay, so these dots are the eigenvalues, the color is the residual. So there's a bunch of stuff near one which looks pretty accurate, but a lot of spectral pollution. And in fact, if you have really good vision, I should perhaps put a zoomed in section, but you can see that a lot of this spectral content is shifted to the left hand side. So if you plot the energy, for example, you'll see that the linear dictionary doesn't preserve the energy of the system.
00:50:37.474 - 00:50:57.830, Speaker B: In contrast, if you use a non near dictionary, so, this is a kernel method, you get a much sharper spectra here in this region with smaller residuals. These modes here actually correspond to transient modes. So this black curve that I plotted is just successive powers of a base base eigenvalue.
00:50:57.942 - 00:50:58.222, Speaker C: Okay?
00:50:58.238 - 00:51:29.948, Speaker B: So you can start to compute these Kubrick modes. So this is a longer lasting mode. This is sort of a. A more transient mode here, and you get these errors as before. If you're interested in the physical meaning of these computations and further examples, you can see our paper, which will hopefully soon appear in JFM. Okay, so that's verifying the dictionary using this technique. The second example that I want to show you is that you can compute spectral measures in high dimensions.
00:51:29.948 - 00:52:18.216, Speaker B: Okay? So here I've got a protein molecule, and I'm looking at the atoms and the positions and momentum of all of the atoms in this molecule. That gives me my dynamical system with an ambient dimension of roughly 20,000. In this protein, there's two key parts, which I've labeled lid and NMP. And they give the shape of the protein, which determines how it reacts to they phosphates in the cell. And then there are two key parts here where you have dihedral angles that give you that shape. Okay, so these are the observables that chemists care about, right? So they want a fingerprint of the dynamics to look at the dynamics of the shape of this thing. And to do that, you pick these dihedral angles as your g for the spectral measures.
00:52:18.216 - 00:52:42.720, Speaker B: So that's what these plots here are showing. Okay, so this is a 6th order kernel with a spectral resolution of roughly ten to the minus six. Okay, so you're able to compute these spectral measures in crazy large dimensions as well. I should say. You can do this. You know, you don't have to have a God exampling to be able to get this algorithm to work in high dimensions. As long as you have the convergent quadrantry rule, it works.
00:52:42.912 - 00:52:43.416, Speaker C: Okay?
00:52:43.480 - 00:53:15.894, Speaker B: And the final example is trustworthy. Kubem mode decomposition. So, this is another experiment for this paper here. So I should say, actually, Lorna Rayton's at Cambridge and Matzo, because of Virginia Tech, Matt was the guy who did all these cool experiments and gave us the data. So he had a laser cannon where he was blasting some air, and this would cause a supersonic shockwave. And the goal was to predict the pressure field or the shape of this wave, which is highly non linear. So that's what I've shown here in blue.
00:53:15.894 - 00:54:36.658, Speaker B: To get a dynamical system, I'm going to use delay embedding up to this initial time here in this dash line, and I'm going to compute the Kubman mode decomposition. So I'm going to compute that based on a few realizations of this experiment, and then test its predictive power on a few more realizations so that the Kubernetes mode decomposition hasn't seen the shockwave. What I'm going to do is I'm going to take the eigenvalues in the Kubernetes decomposition and I'm going to consider two ways of ranking them or ordering them. So the traditional way of doing this is you look at the modulus of the eigenvalues, right? So you know, how much energy do they capture? And if you do that and you just keep the, the fourth largest eigenvalues, 40th largest eigenvalues, you get these black dots here, so these red dots, okay? So you get the initial peak, but it's not quite there, and then it struggles with the second peak and so on. Now that you have residuals. So, for example, you know, this type of picture here, you can start ranking the eigenvalues by their residual. So in other words, you have a way of measuring how close they are to representing the sort of the true underlying dynamics with the infinite dimensional Koopman operator.
00:54:36.658 - 00:55:22.874, Speaker B: So if you do that and you keep the 40th eigenvalues with the smallest, which is residuals, you get these black dots here. So it's a much better approximation of this pressure wave. Okay, so let's actually look at the number of modes that we keep in the kutma mode decomposition versus the relative mean squared error. Okay, so the black is this new residual ordering. Okay, so you get a much better compression. The red is the sort of the way you do it if you didn't have a way of verifying or measuring the error of these eigenvalues. Okay, so to wrap up, this is actually part of a wider program on infinite dimensional computational analysis and related things.
00:55:22.874 - 00:55:32.674, Speaker B: So this allows you, for example, to compute spectral properties rigorously. So there's a bunch of papers down here that you can read for that as well.
00:55:33.974 - 00:55:48.914, Speaker A: So we also have things about continuous linear algebra, which is about avoiding discretization problems. When you discretize PDE's and trying to delay discretization as long as possible. So you can compute quantities with rigorously.
00:55:49.694 - 00:56:06.914, Speaker B: You can classify these types of computational problems in something called the solvability complexity index hierarchy. And that allows you to precisely measure the difficulty of these problems, particularly in infinite dimensions, and then also prove that your algorithms are sharp or optimal, realizing those bounds.
00:56:06.954 - 00:56:20.894, Speaker A: Okay, so this also extends to things like computer assisted proofs and PDE learning. And from us as card carrying numerical analysis. We're hoping that some of these tools can be useful to this community.
00:56:23.074 - 00:56:36.094, Speaker B: Okay, so, summary rigorous data driven Kubernism. So we tackled the problems of too much or too little. The idea there was a new matrix for the residual that led to Res DMD for computing spectra.
00:56:36.254 - 00:56:48.554, Speaker A: And when we had continuous spectra, we used spectral measures. We convolved that with a rational kernel in general, and computed them with the resolve operator and those inner products with Res DMD.
00:56:48.974 - 00:57:21.964, Speaker B: And finally, we looked at the question, is it right both of these problems here, we had algorithms that could verify the output. Right? So you're able to use resDMd to verify computations. For example, we use this to verify learned dictionaries when the dimension is very large. If you want code for these methods for ResDMD, you can also visit this GitHub repository. This has been of interest. And please do chat to Alex and myself throughout the conference. We're very much looking forward to learning more about dynamical systems.
00:57:23.224 - 00:57:23.624, Speaker C: Thank you.
