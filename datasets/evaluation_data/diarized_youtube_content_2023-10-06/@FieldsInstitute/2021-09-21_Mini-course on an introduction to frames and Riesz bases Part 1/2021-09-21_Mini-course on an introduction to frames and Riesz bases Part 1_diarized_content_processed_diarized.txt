00:00:02.320 - 00:00:27.134, Speaker A: Welcome to the week six of our focus program. We continue with three lectures by Professor Ole Christensen from Technical University of Denmark and his lectures on frames and operators, basic properties and open problems. I mean, we'll talk about risk spaces and some applications in signal processing, too. Professor Olep, please.
00:00:27.794 - 00:01:19.702, Speaker B: Thank you very much. And thank you, Jared, for organizing this very interesting program. There are so many wonderful talks here, so I really enjoy being part of this. As Jared already said, the topic would be frames. So let me just remind you about the key feature of frames on the very first slide. The reason that we want to look at frames is that we get at the composition of the underlying Hilbert space, meaning that if you take arbitrary elements in the Hilbert space, they can be represented using a superposition of the elements FK in the frame with some coefficients that are giving as inner products between the given elements, and some dual elements that we will define later. And when you look at this decomposition, you can see right away that this is very similar to what you have for autonomous bases.
00:01:19.702 - 00:02:08.304, Speaker B: But the idea behind frames is that it gives us so much more flexibility than for autonomous bases. And this is one of the aspects that we will discuss, and we will actually attack the entire topic from the operator point of view. What we will do is to consider frames in general Hilbert spaces in the first lecture, and then in the next lecture, we go to specific frames in the Hilbert space, l two far. And in the third lecture, we will consider more open problems. And what you will feel in the entire flow of the lectures is that operator theory is really the driving force. So you can think about frames as elements in the Hilbert space, but you can also think about the entire thing as simply operated. So now I cannot move to the next slide.
00:02:08.304 - 00:02:42.916, Speaker B: Can there be any technical explanation for that? Now it works. Okay, good. So here you just see the overview of the three lectures. This is exactly what I explained. But let us go one step back and say, before we even introduce frames, the goal is to consider a particular Hilbert space and then consider expansions of elements in the Hilbert space. An expansion simply means what, what we could call generalized linear combinations. So linear combinations are of course, by definition a finite linear combination.
00:02:42.916 - 00:04:01.620, Speaker B: But here we just have possibly infinite sum of some coefficient ck times the element in this sequence, fk, that we call building blocks. So they are not framed yet. But the question is, what do we want from such expansions? And a natural thing would be to say, we want this to be a convenient expansion. And what does it mean to be a convenient expansion? That would mean that when we want to look at the expansion of a given, if it should be, for example, easy to find the coefficients ck, that could be one requirement. Another requirement could be that when we look at a particular class of signals, then only few of the coefficients ck are large, meaning that there are a lot of small coefficients that can be thrown away such that we get a truncation, we get a approximation of our signal that is useful for whatever application we are looking at. And those of you who are familiar with wavelets will know that this is exactly what happened there. Because if you look at a wavelength system, and that means an autonomous basis for l two of r, and we use it in the context of image processing, then we can take our images and we can expand them as exactly representation.
00:04:01.620 - 00:04:42.214, Speaker B: The sum of some coefficients times the elements in the wavelength system. And in exactly that case, we know that images, they typically have some structure. And this is reflected in the coefficients, meaning that many coefficients are small. And then what we typically do for the wavelengths is that we insert this thresholding where we throw away the small coefficients, and then we get a very good and very efficient approximation of our signal that can be applied in practice. So this is the same idea that we are looking at here. Whatever condition we put on this expansion, this really depend on the, on the concrete context. So I just gave you an example of it.
00:04:42.214 - 00:05:29.944, Speaker B: Now, what kind of cases do we have where we have expansions of that type? There are different types of bases, and I just want to define it just for completeness. So the first type of base you could look at would be what some people call just a basic and some people call shallow basis. So what this means is we are looking at a collection of elements in the Hilbert space. It could actually also be a boundary space for that part. And then we say that it forms a basis or a shallow basis, if whenever we are looking at arbitrary elements in the space, they have unique representations as infinite sums of some coefficient ck depending on the f times the given elements. Ek. And the very important special case is the one where we have an orthonorm basis.
00:05:29.944 - 00:06:17.098, Speaker B: And an orthonorm basis is simply a set of vectors that have length one and they are perpendicular to each other. That means they are what we call an autonomous system and they should at the same time be a basis. And the reason that we take the step from general basis to basis is that we have a wonderful theorem that actually characterize orthonorm basis. So what it is saying is that if you take an orthonorm system, all the six conditions that I have marked here are equivalent. And the first one is that Ek is an autonomous basis. I will not go through all of them because I guess you know them, but that Ek is an ozone basis is actually equivalent with all other conditions. And if you look at condition two, you can see that this is exactly the type of expansion we are interested in.
00:06:17.098 - 00:06:58.428, Speaker B: So to be an orthonormal basis is equivalent. If it is already an autonomous system, then it is equivalent to having the expansion of arbitrary elements f as the sum of these coefficients f in a product with Ek times Ek. And this is what I would call a convenient expansion in most cases, because you have direct access to the coefficients in the infinite sum. For shadow bases, you don't have this access to the coefficients. There are some ways to find them, but this is usually more involved. Here you get them directly in terms of the elements in your horizontal basis. The important thing to notice here is that it's also equivalent to the condition four, which is the so called parsevel equality.
00:06:58.428 - 00:07:49.674, Speaker B: And this is something that will play a role for the generalization that we do later. Those of you who know frames will already know what I'm saying here. So we come back to exactly the condition four later. This is one characterization of orthonormal bases, but there's actually another one that is also very important in many cases. And what it says is that if you start with one orthonorm basis, then we can easily characterize all orthonorm bases, because all of them appear by taking this given orthonorm basis and then act with a unitary operator, then when we let you run through all our unitarian operators, then they set you acting on ek runs through all autonom bases. This is a very convenient representation, and we come back to that soon. Also, I guess you're also familiar with the standard example of orthonorm basis.
00:07:49.674 - 00:08:37.386, Speaker B: So if you look at the Hilbert space l two on the interval zero one, and you make Fourier series in that space, so you look at the complex exponentials e to the two PI I kx, and now you let k run through all integers, then this is actually an orthonormal basis for that space. And the Fourier expansion that we have is precisely the expansion in that autonomous basis. So this is a very nice example of an autonomous basis. So why do we want to continue? Why do we have to listen to 3 hours of lecture on frames? Because autonomous bases are actually very nice. But the problem is that the conditions are very strong for elements to be an autonomous basis. Think about it, that you have to have vectors that are exactly perpendicular to each other. This is a very strong condition.
00:08:37.386 - 00:09:10.104, Speaker B: And especially when we go to a Hilbert space like l two of r, this is definitely not something you're getting for free. And they need to span the space. So there are actually a lot of restrictions on orthonorm basis. So even though we like them very much, the matter of the fact is just that. For example, if you look again at the wavelength theory, there are many cases where you can very well construct autonomous bases. But you cannot construct autonomous bases with exactly satisfying extra conditions that you would like in a particular application. And that's the reason that we want to take some extra steps and generalize autonomases.
00:09:10.104 - 00:10:00.504, Speaker B: And the first natural step for doing this would be to look at what we call re spaces. And before we define re spaces, let's look at the sequences. And they are just collections of elements in the Hilbert space such that the equation you can see displayed in the third line is satisfied. So we are looking at a finite arbitrary collection of finite coefficients ck and then the linear combination ck times ek ck times fk here. And we take the norm of that, and we want that this is equivalent with the l two norm of the sequence ck. This does not by itself mean that the sequence will span the vectors, I mean span the entire Hilbert space. So you could for example, have an alternate basis and you kick out ten elements.
00:10:00.504 - 00:10:41.070, Speaker B: Then you would still have this property, but you would not have the completeness anymore. And that's the reason that we take the next step and define respaces. So a respace is a resequence which at the same time is complete in the space. So the closed linear span of the elements is equal to the heat of space. Sometimes you prefer to work with a definition that I stated here. Sometimes you work with another definition, and I'll not prove that they are equivalent. But an equivalent definition is actually similar to what we saw for autonomous basis, that you can start with an arbitrary orthonormal basis and then you act with elements, operators of a certain class of operators.
00:10:41.070 - 00:11:32.800, Speaker B: And in this case it is the class of bounded projective operators. So if you take an arbitrary ozone basis and you act with a class of bounded bijective operators, then what comes out is exactly the class of free spaces. And that characterization is very convenient for several proofs. For example, the proof of the statement that I will give here, I will not give all details of the proof, but if you look at the first line, you can see that if you are looking at an orthonorm basis fk. And now you write it as u acting on ek for a certain operator u and, and a fixed choice of the autonomous basis e k. Then you actually get that the sum of the absolute value squared of the inner product between arbitrary elements f and f k actually is again equivalent to the norm. Now the norm of f in the hebrew space a.
00:11:32.800 - 00:12:25.970, Speaker B: So this, there's a, there's a lower bound one divided by the norm of u to the minus one to the power two times norm of f to the power two, and there's a corresponding upper bound. And again, those of you who know frames will see that we are clearly on the way to frames here. But what I wanted to say here is just that if you had the case of an orthonormal basis, then you had equality of the term in the middle, and then the norm of f to the power two. But here we have these two values, two bounds that are coming in, which actually is related to applications. I'm not the type of guy who will speak a lot about applications, but if you make algorithms corresponding to reach basis, then you would actually like this, that these two bounds are close to each other, because this is typically something that the speed up algorithms. So you don't want the two bounds to be far away from each other. The other important thing is the property, you see, that is called two.
00:12:25.970 - 00:13:20.428, Speaker B: It says that if you make a particular choice of a family decay, namely we take this operator u that we are looking at, we take its inverse and we take it adjoint, and we act on the same orthonormal basis decay. Then we get a family of elements decay that actually satisfies similar to what we had before. We get that f can be written as an infinite linear combination of the elements fk, but now with coefficients that are giving us inner product between f and this new family tk. And that's actually a very elegant and nice proof of this. And this is what you see in the very last line of the slides. So what you do is you simply start with an outer element f, and then you act on it with the identity operator, which we write as u composed with u to the minus one, because one of the assumptions that we have is that u to the minus one actually exists. Because we were saying that u is a bijective operator.
00:13:20.428 - 00:13:56.264, Speaker B: And then you take this element u to the minus one f, and you put it into the decomposition in terms of the autonom basis e k. Then you get the second equality sign. And then from here you can take u to the minus one in the inner product, and you can move it to the other side, side of the inner product. Then you get that joint, and the u that is outside the infinite sum, we can move it inside and it can act on the elements ek. And then we simply get the given elements fk. So this is the way to prove that. So you see, it follows really from the properties that we have of the operator U.
00:13:56.264 - 00:14:43.102, Speaker B: So there's especially family decay that comes up here again, if you refer to line three, the DK equal to u to the minus one adjoint acting on DK. This has a special name. So this is called the dual of the FK. This is actually a reach basis by itself, because we saw that the reach basis are just the systems that we get by starting with an orthonorm basis. And then we act with a bounded bijective operator, and by the assumptions that we have also u to the minus one star is bounded by the active operator. So that means the decay by itself is actually also a re space basis. And that means we can run the same procedure and then ask, what is the dual of the family decay? And this is what I calculated in this blade equation.
00:14:43.102 - 00:15:19.016, Speaker B: This is just to invert the operator one more time and adjoin it one more time. So then we come back to exactly the operator we started with. That means the dual of the family decay is actually the FK. Again. So we can say it's not just that Dk is the dual of Fk, but these two systems are simply duals of each other. And that's the reason that we call them a pair of reach basis. And what it means is that if you look at this decomposition that we got, where we were looking at a linear combination of the elements f k, but with coefficients given as inner product between f and dk, then we can actually interchange the role of Fk and DK.
00:15:19.016 - 00:16:06.214, Speaker B: It does not matter which one is inside and which one is outside. This is a very nice property of the race basis. Another thing that we will actually need, especially when we come to the discussion of open problem in the last lecture, then you need the concept of biotronized systems. So you already know what it means to be autogonnet, an autonomous system. But these two elements, these two elements appearing in the pair of dual breach bases, they are actually what is called Biatronet. And that means if you take the inner product between an element FGA and an element decay, then you also always get Kronegart's Delta KJ. And I'll not go through the proof but you can see it's really something that you can do in just two lines, so there's nothing deep about it.
00:16:06.214 - 00:16:53.124, Speaker B: So all what we have been speaking about here could be called decomposition in terms of basis. And I remember very well, as I was a PhD student and my advisor came to me and said he's seen a very nice paper by David Walnut and Chris Heil. And that was the one from Sim review. I guess some of you know it, a paper from 89. And the idea in that paper was to look at over complete systems, so systems that you could use to decompose vectors in your space, but the coefficients were not unique anymore. And I was very surprised about that idea, because in linear algebra we always look at cases where we have a basis, so why should we have this or completeness? That was very surprising for me. Then, of course, it became my main occupation very soon.
00:16:53.124 - 00:17:39.424, Speaker B: But I would like to give you an example showing that they actually come up very naturally. So let's go back to the case where we are looking at the Fourier series in the space l two of the interval zero one. Let's look at a sub interval. An open sub interval will measure smaller than one of this interval from zero one. So you can think about just an example. You can take the interval from zero to one two, then l two on this sub interval, let's say l two on the sub interval zero to one two. We can identify that with a subspace of l two on the interval, because we can simply take the functions in l two on the sub interval, and then we can extend them by putting them equal to zero outside the interval from zero to one two.
00:17:39.424 - 00:18:28.106, Speaker B: Now, let's take any function in l two on this interval zero to one two, then, because it's now a subset of l two on the intel one, then we have our fluid decomposition. So we can write it as what you see in the last line as f equal to the sum of the standard coefficients times ek. And this is a decomposition that holds in l two on zero one, because this is the underlying Hilbert space. Now, but actually we also get that f is equal to exactly the same series. When you are looking at l two and intel I, if you think about how the l two norm is working, you take the integral or your interval of the function to the power two. So if you have convergence on the interval zero one, you certainly also have convergence on an interval, on a sub interval. And this is what I write in the first line.
00:18:28.106 - 00:19:16.664, Speaker B: So, the conclusion is simply that this Fourier composition that we have, this is also valid in l two on the sub intravel. But the issue is that when you're looking at the decomposition in the Hilbert space L two on I, then suddenly the decomposition is not unique anymore. And the reason is that the function f that we started with, we just extended it by putting it equal to zero outside the inside I. But you can extend it many other ways. So what you see here in line five six is that you can also make an extension that we call f tilde, where you simply extend it by putting it equal to one outside the intel instead of putting it equal to zero. Then again, this function f tilde is in l two on the interval zero one. And we have our decomposition that you see in line seven eight.
00:19:16.664 - 00:19:47.656, Speaker B: But again, you can do the same trick. You can restrict that expansion to the interval I and then you also get convergence in l two on the interval I. But on I the function f is equal to f tilde. And that means you are decomposing the same function as before. You still decomposing the function f. But now the expansion that comes out will be with coefficients that are the inner product with f Tilde and ek instead of f and e k. And that means suddenly you have two expansions.
00:19:47.656 - 00:20:40.120, Speaker B: What you see in line two here you have two expansions, one of them with a coefficients product between f and e k and the other one with coefficients inner product between f Tilde and ek. And these coefficients, they are certainly not the same because if they were the same, then the two functions f and f tilde would simply be the same. Or the integral where we take this inner product, the inner product is still taking over the interval from zero to one. But on zero to one, the two functions are not the same because they have been extended from the inside I to the entire interval in two different ways. So the set of coefficients are different. And that means when you're looking at line two, you see that you are looking at a function or signal if, but it has two different expansions in terms of your family decay. That means this idea of having non uniqueness, it actually comes up automatically.
00:20:40.120 - 00:21:23.174, Speaker B: We didn't do anything except for restricting our functions to a subspace, but suddenly we lose the basis property. And the conclusion that comes out here is that bases actually come up very naturally. And as I say here in the few last lines stated in blue, is that I actually think it's very natural to consider sequences that are not basis but still have the property that we can expand them. And this is what will motivate the next step. The step from non redundant systems to redundant systems. And in order to do that, we need a few steps first. We need to consider the so called basel sequences first.
00:21:23.174 - 00:22:23.424, Speaker B: And I guess most of you are familiar with the basal sequences. So what we say is that a given sequence of elements in the inverse space form a basal sequence if they exist, a constant such that whenever we take the inner product between fk and arbitrary elements in the space, and you take the absolute value, put it to the power two, and make the sum or all elements in the sequence, then we get something that is bounded by a certain universal constant b. So the constant b should be the same for all elements f and then times norm of f to the power two. This is definitely something that is satisfied for our reach basis, because this was one of the inequalities that we stated before. So re spaces are a special case of this. The nice thing about Bessel sequences is that we can actually characterize them in terms of operators. So let's remember that this is actually one of our goals, that we will take everything and phrase in terms of elements in the helical space, but also rephrase them in terms of operators.
00:22:23.424 - 00:23:14.684, Speaker B: And this is what we see here. So to be a basal sequence is actually equivalent to have a linear operator t that maps all sequences from little l two into the Hilbert space. That this operator is well defined and bounded. It's actually not a complicated thing to prove. I will not discuss the details of the proof, but I think you have seen it before. So the thing to observe here is that the issue to check, when we say that we start with a basal sequence and we want the operator to be a bounded operator, is mainly to check that this infinite series of ck times fk factory actually is a convergent series. And if you check that carefully, then you also see that the operator that is defined that way actually becomes bounded.
00:23:14.684 - 00:23:55.324, Speaker B: So if you just check carefully that it is well defined, then you get the boundedness by exactly the same argument. This operator is one of the key elements in the frame zoo. So we have a special name for it, and it is called the at the beginning it was actually called a prep frame operator, but then it got taken over by the name synthesis operator. So I think nobody today used the name prep frame operator anymore. So we will also call it the synthesis operator for the rest of the talk. So you can see it is stated formally in line four. So this is simply the operator going from little into the Hilbert space where we put in the sequence, and it returns the corresponding infinite linear combination of our elements fk.
00:23:55.324 - 00:24:53.290, Speaker B: So because this is a bounded operator, we can calculate, it's a joint operator. And you can see already here t is something that will give us some interesting coefficients back, because t acting on f is actually a sequence of coefficients, these inner products between f and f k. So you can see this sounds like an interesting operator in the business that we're doing there. And what we can also do without any frame condition, actually, just based on the Bezel condition, we can look at these two operators and we can compose them so we can look at what we will call the frame operator, which is just tt. And then you see that what comes out here is simply the sum of inner product between f and f k times f k. And this is clear also from the beginning that this is an interesting operator, because a special case of a baseline sequence would be that we are looking at an orthonormal basis. And for an orthonormal basis this operator s is simply equal to the identity.
00:24:53.290 - 00:25:29.474, Speaker B: So somehow, looking at the setup here, you can see that we are on the way to generalize what we have done so far. The way this will be done is to give the formal definition of a frame. And a frame is simply a Bezel sequence where we have a corresponding lower bound, as you see here in line three. So I do it a little bit fast because I think you have seen the definition before. And there's a special case where the frame is tight. This is the case where we can choose a and b to be the same number. An important thing about this definition is that you can see the way that we have extended things.
00:25:29.474 - 00:26:10.894, Speaker B: We started with a class of orthonorm basis, then we extended it to a class of reach basis. And now we have extended the class of re spaces to the class of frames because the condition that we are looking at here for frames, this is something that we have seen already, is satisfied reach basis. So we have really extended things step by step. The interesting thing about the frame definition is that it opens a lot of new flexibility here. For example, something that cannot happen for basis, namely or completeness. And you can see it at the last bullet that I look at two frames that are not basis. So in both cases we start with an autonom basis ek but for the first frame we simply repeat the element e one.
00:26:10.894 - 00:26:52.014, Speaker B: Then we don't have an orthonorm basis anymore. And we definitely also don't have a rebasis anymore. But we have a frame because this extra element will not destroy the frame condition. The second element is similar, but there is a slight difference here. If you look at it, you can see that we are still looking at the orthonorm basis, but we are adding another element, namely an infinite linear combination of the elements. Ek. So that means when we speak about linear independency, even though there is some extra element in the second family I'm looking at here, it is actually still linearly independent, because by definition, linearly independence is something that relates to finite subsets of the sequence.
00:26:52.014 - 00:27:37.564, Speaker B: And if you make a finite linear combination of the elements, then certainly you still have the probability that all coefficients have to be equal to zero. So both families are, both families are or complete, but the first one is actually linearly dependent, but the second one is or complete, but still linearly independent. So this is a very crucial difference between these two families. And this is something that again come up when we discuss open problems in the third lecture. But I mean, there's a reason that I mention exactly these two examples. If you look at arbitrary frames, then the frame operator, and you see the definition again up in the very first line, actually has some very nice properties. So it's a bounded operator, it is invertible, and it is self adjoint and positive.
00:27:37.564 - 00:28:38.524, Speaker B: And these properties are exactly what we need in order to be able to generalize these decomposition that we looked at before. So the important statement, and for me, this is really the classical and most important statement at all in frame theory. This is telling us that giving a frame, you actually get the desired decomposition of the space. We get that every element can be written as a sum of infinite inner product between the given f, and then the inverse frame operator acting on fk times the elements fk, and you get a corresponding decomposition where the s two and minus one is just moving to the fk outside the inner product. And the proof is actually exactly the same as what we saw in the case of re spaces. If you look at the third last line, you can see that you simply start with the element fix, and then we act with the identity operator, which is now written as s put together with s to the minus one. And then again we take s to the minus one f, and we simply put into the operator s.
00:28:38.524 - 00:29:15.444, Speaker B: And if you do that and then use the fact that s is self adjoint, and therefore also s to the minus one is self adjoint. Therefore we can put the operator s to the minus one to the other side of the inner plot, and then you get exactly the decomposition that is stated. The second one is done exactly the same way. You just write the identity, the opposite way you write it is to the minus one acting one is. So that means it is actually not heavy to get this. As long as you have these properties that we were stating here for the frame operator, then the frame decomposition actually comes by itself. You can also see here that we actually only need what is stated in the first bullet.
00:29:15.444 - 00:29:46.984, Speaker B: The second bullet is just some extra information that you might need in other contexts. And this family that appears in the inner product, namely s two minus one fk, is also a very important part of frame theory. So it has a special name. It is called the canonical dual frame. And when you use the name canonical, it's because we later see that typically they are actually other choices. But it's canonical in the sentence that is the one that comes up automatically when you make this proof. So it is a very natural one to consider.
00:29:46.984 - 00:30:26.892, Speaker B: So as I was a PhD student, as I saw this first time, I was thinking, but then we don't need to think about bases anymore because this is wonderful, we can just apply frames instead. But there's a slight problem here, because we know that the operator s two one exists. But one thing is to know its existence. Another thing is to be able to calculate it in practice. If you think about how this looks like in the case of finite dimensional spaces, if you're looking at a Hilbert space that is two dimensional, then s is an operator mapping a two dimensional space into a two dimensional space. So this is what we usually call a two by two matrix. And this is quite easy to find, the inverse.
00:30:26.892 - 00:31:12.994, Speaker B: And if you switch on MaTLab or maple or things like that, you might also be able to do it for 100 times hundred matrices. But if you look at a ten to the power of 16, Simon's ten to the power 16 matrix, maybe it's not that fun to calculate the inverse anymore. And what we're doing here is even worse because we are looking at an infinite dimensional Hilbert space. So it is absolutely a problem that we are having this s two minus one. When you want to do applications, if you want just two theory, you might not be annoyed by this operator, but definitely for applications you need to do something with it. And this is exactly the reason that we very often take a step back and only look at what is called the tight frames. Because if you have tight frames, then we have equality in the frame condition, what you see in the first line.
00:31:12.994 - 00:32:19.088, Speaker B: And if you do the calculation that you see down in the 6th line, then you'll see that this actually implies that if you look at the frame operator acting on an element. If and you take inner product with f, then this is actually exactly the same as just taking inner product between the number a times f acting on f. And what it actually means is that because the operator is self adjoint, then you can actually conclude that the frame operator is simply equal to a times the identity. This means that the frame operator simplifies drastically, and this means that you are getting exactly what is stated in the corollary in line four, that instead of having this s to the minus one acting on f, k s to the minus one is just acting then by division with the number a. So that means s to a minus one acting on fk, what we had before, this is simply equal to fk and then divided with the number a. And that means you get this very nice decomposition that we have in the forward line. So what you see here is that the way this is stated is almost the same as what you have for an orthonorm basis.
00:32:19.088 - 00:32:55.080, Speaker B: The only thing is this factor one divided by a that you have in front, and that one you can get rid of if it annoys you, because you can simply use a different normalization of your vectors, then you also get rid of the factor one divided by a. And that means you're having exactly what we have for orthonormal basis in that case. So this is very nice. And there are other reasons that this is very nice. So often you want to know something about the coefficients that appear. So let me push the button one more time so you get the next line. Also, if you look at the standard decomposition using a, using the canonical dual frame.
00:32:55.080 - 00:33:38.692, Speaker B: If you look at this in the last line, then it is actually very difficult to say something about the inner products, because there's this s two hanging there. So, for example, let's think about that. We are doing something in the Hilbert space l two, and you want to do something that involves functions with compact support. Then if you start with a frame where the functions fk have compact support, maybe you also want these functions that appear in the inner products to have compact support. For example, if you want to calculate these coefficients, then it is definitely an advantage that the functions have compact support. But now it is not fk. That is stated that it is s two f k, and this operator s two minus one might be completely chaotic.
00:33:38.692 - 00:34:23.264, Speaker B: It might very well be that if k has compact support, but then when we act with s two minus one, we lose the compact support, and there could be other properties that are lost. It could be that we want to look at functions f k that are I don't know, infinitely often differentiable, for example. And then it could also be that f two is destroying that property and all this kind of problem, they completely disappear for tight frames. Because if you look at the decomposition for tight frames that you see in line one, then there's no s to a minus one. Actually there is an s to a minus one, but it is just multiplying with a factor one divided by a. So this means it is not destroying any of the nice properties for FK. So this is clearly an advances for tight frames.
00:34:23.264 - 00:35:05.856, Speaker B: And for this reason you might like to use what I'm writing here. Namely, there's also a very classical statement saying that if you take any frame, you can actually construct a tight frame, because you take the operator s to the minus one, and then you use the fact that this is a self adjoint person operator. So you can construct a unique square root of that operator, and that we call s to the minus one two. And then the statement says that s to the minus one two acting on FK. This is actually a tight frame also, and the frame bound is equal to one. So even this number a disappear. And then you are getting the decomposition that you can see in line six, in the last line here.
00:35:05.856 - 00:35:49.222, Speaker B: But this is somehow theoretical, what is going on here, because exactly the same issue as before comes up. You might have some FK with very nice provinces, but now you are acting with over a to a minus one two. So that means you might again destroy the nice properties. So this is really a theoretical statement here. It's something that tells you that you can construct a tight frame, but it's not something that tells you that you can construct a tight frame with nice properties, because the procedure that is involved in it actually might destroy the nice properties. So we should be a little bit careful here. Also, let's go to the next step that could be characterization of frames.
00:35:49.222 - 00:37:02.372, Speaker B: And this actually made me not like this talk very much, because it's about a result that I proved in 92, and I already feel very old by myself. But by seeing this, I guess you might think that I'm coming from the Stone Age or being a neanderthaler or something like that. But it is a matter of fact that I actually proved it in my PhD thesis, and it's quite a few years ago. Now, what it says is that a frame, we can actually characterize that in terms of something similar as what we did for the Bessel sequences. We had that a sequence was a Bessel sequence, even if and only if the operator that we stated here was a well defined mapping from l two into the Hilbert space. And now what it means to have a frame instead of a Bessel sequence, we simply change this word into the Hilbert space with the word onto the Hebrew space. And we actually know it that this operator has to be onto, because the sum that we are looking at, the sum of coefficients c k f k, we know that for a special choice of the coefficients, namely these coefficients, we looked at before the inner product between f and f to minus one f k, then we know that what comes out from this infinite sum, this is exactly equal to equal to the element f.
00:37:02.372 - 00:37:51.756, Speaker B: So we know that part of the frame, naturally it means that this operator has to be on top. So what it says here is, it is a frame even only if it is mapping l two onto the Hilbert space. And what we saw in the past is that it's a Bezel sequence, even only if it is just mapping into the Hilbert space age. So this is the crucial difference between Bessel sequences and frames. But we can also characterize frames in another way, namely using operators acting on orthonormal bases like we saw in the past also. So if you take any autonomous basis for our hybrid space, then we can characterize the frames as the families again of the form U acting on EK for a certain class of operators. And here the class of operators is simply the bounded and subjective operators.
00:37:51.756 - 00:38:27.174, Speaker B: And we can compare that to what we did in the past. So if you look at the automobile basis, we got them by starting with an orthonormal basis and then act with the unitary operators. And then we generalized that and we looked at re spaces, and then the condition of you being unitary got relaxed, and instead we should just have bounded and bijective operators. Now the step from responding frames means we still have boundedness, but now we don't have bijectivity anymore, we just have that. It's your objective. And if you want, you can take the next step also and look at patient sequences where the search activity is gone. And we just ask for the operator being bounded.
00:38:27.174 - 00:39:21.606, Speaker B: So here you can see really in terms of the operators how we are relaxing the conditions in each step. And then you can of course ask yourself, shouldn't there be a fifth bullet? Also? What about this boundedness? Couldn't we do something about that? And the truth is that you can actually run kind of a frame theory for unbounded operators. But this is more involved partly because these coefficients that appear in the frame decomposition, they are related to the pseudo inverse operator. And when we are trying to generalize what is going on here. We very fast run into some unbounded operators there. And it's not that nice to do it, but there are a couple of pales about it in the literature. But it's not something that we do in what we could call mainstream frame theory, of course, because mainstream frame theory is mainly related to what we could also do in practice and use for applications.
00:39:21.606 - 00:40:27.808, Speaker B: So for that point of view, I would say that the four statements that we have here, that we have here, are the most important ones. Another thing we can do is to relate frames with our re spaces. And we have already seen that re spaces actually are frames. But then you could ask yourself, what about the other thing? What does it take for a frame to be a re space? And the condition is exactly what you see here, that if there's a linear infinite linear combination of the elements f that happens to be zero, then it should imply that the coefficients ck are equal to zero for all elements k. This is a characterization of the frames that form re spaces. What this also says is that if you're looking at a frame which is not a read spaces, and this is exactly what we call or complete or redundant systems, then this means that there exists some coefficients that are not equal to zero, but still such that the infinite linear combination of ck fk is equal to zero. And what this means is that if you're looking at a frame that is not a reach basis, then we actually get non uniqueness of our coefficients right away.
00:40:27.808 - 00:41:40.400, Speaker B: Because if we look at the canonical coefficients that we have, namely the inner product between f and f to the minus one f k, then to the frame decomposition, we can always add a zero. And we can write this zero as the sum of the coefficient ck times fk for these coefficients ck that make the sum be equal to zero. And this means that we can see it here right away, that we are getting several choices for our coefficients, because we can use the coefficients that we got before, but we can also add these coefficients ck that appear in this infinite linear combinations, and then we can actually take even an extra step. And again, it was something that was surprising me the first time I saw it. So here we are just saying that these inner products, we can add some coefficient ck to them, and then we are getting new coefficients that can be used in the expansion, but we can do much more than that. So what it says is that these elements f to the minus one f k, we can actually replace the elements by some other elements fk, that can then be put into the inner product. And then we get new coefficients that will recover our if.
00:41:40.400 - 00:42:35.944, Speaker B: So, what this statement says is that they exist, some decays that are not equal to these coefficients, these vectors coming from the inverse frame operator, but still having the probability that any given if can be written as the inner product of f times dk, inner product between f and tk times the fk. This was somehow surprising me a lot, that even inside the inner product you can put in other elements. But this is what the theorem says and it is not that difficult to prove it. Actually, again, I will not do the proof, but it was just surprising me that they always exist like this. And again, this is a point where operator theory comes in very strongly. These decays, arbitrary elements, decay that we can put in here. They actually called a dual frame compared to this canonical dual frame we had before.
00:42:35.944 - 00:43:17.910, Speaker B: And we can actually characterize them in terms of operator theory. Because if you look at what our decomposition actually says. So the decomposition is stated in line six here. If you look at this, you can actually see that this is composed of some of these operators that we looked at before. Because if you look at the sequence f k first, then we had the so called synthesis operator. The synthesis operator is the one that you see in line two, this operator t, taking coefficients ck and mapping them onto the infinite linear combination ck times fk. So it's clear from line six that we actually have the synthesis operator for f k.
00:43:17.910 - 00:44:18.894, Speaker B: But now it's not acting on arbitrary coefficient ck. It is acting on exactly these coefficients that are the inner product between f and dk. But the sequence of coefficients in a product between f and f k, they are precisely the coefficients that appear in the adjoint of the synthesis operator for the system decayed. So if we call the synthesis operator for the system decay for u, then the decomposition that we are aiming at, namely the one in line six. This simply means that if you take t and compose the U star, so we compose these two, since this is operator, but we start in one of them, then you simply get the identity. And if you take that joint of both sides, then this means that u composed with t is equal to the identity. So what this is telling us is that if you take the fk as the starting point, you fix fk.
00:44:18.894 - 00:45:04.546, Speaker B: That means you also fix the synthesis operator. That means you also fix the joint of the. Since this is operator, that means we know now the operator t. What it is telling us is that to find the dual frames means exactly to find a sequence decay such that if it takes its synthesis operator, then it is what we call a left inverse of the operator C star. This is what you see in the last line that we have to find the operator u such that it is a left inverse of t. So what this is telling us is that there's actually hope that we can characterize all our dual frames simply by characterizing all left inverse of the t. This is what Zhidong Li did in 91.
00:45:04.546 - 00:45:59.142, Speaker B: You don't need to look exactly at what is going on here, but you can see from the statement that he's actually characterizing all the left inverse in terms of the second space equation in the third last line. And if you take this equation and then you bring it back to what it means for the underlying sequence decay, then you actually get a characterization of all dual frames, which was this was the aim of Shirong Li, and he did it in the same paper. So what he is proving is that the dual of this given frame is simply giving as this display equation where you take the canonical dual s two one if k. But then you have to add. It's not really a perturbation term, but intuitively I like to think about it as a perturbation. So you start with s two minus one f k, and then you add these terms that are stated there. So that means now we have a characterization of all dual frames, and this means suddenly you can start a lot of applied questions.
00:45:59.142 - 00:46:52.334, Speaker B: You can say, what is a dual that has the best property? And the best property is really something you define. You can say, what is the one that has the smallest support? What is the one that has convenient expression, the one you can calculate easily? And all these kind of questions, they suddenly make sense when you have an expression of this style. And if you have time tomorrow, I will actually attack this kind of questions in the setting of Gaba frames and jewels of Gaba frames. I'll not use the statement of student leak, but I'll use some other tools to attack the question of how to find nice jewels. But what you can see here is that as soon as you have access to the class of jewels, it makes sense to ask these questions. However, there's also something that is not nice here, namely that we somehow do this because this dual frame is to minus one if k is difficult to find. So if s two minus one if k was easy to find.
00:46:52.334 - 00:47:31.914, Speaker B: There's no reason to look for other dual frames in general, but we somehow want to avoid that one. But what you can see is that this is the first term in this expression that we had for DK. So it is still sitting there. We actually did not get rid of it. And the methods that I present for Gauss system, they actually get rid of this. Now, I don't know how applied you are, but you might ask yourself, are there some concrete cases where this is useful? And I actually want to end the first lecture just by giving you such an example. There's a very nice series of paper by Lammers, Powell and Ilmartz from 2009.
00:47:31.914 - 00:48:15.776, Speaker B: And what they are doing is to look at what is called sigma delta quantization. And what it means is that you are considering a frame now in a finite dimensional space because you want to do things in practice. So you look at a frame for RD. That means you are looking at a collection of vectors, if k k is going from one up to some value capital n. And those of you who are familiar with frames in finite dimensional spaces will know that to be a frame in a finite dimensional space simply means that you have a collection of vectors that span the space. So that means for sure the n that we are looking at, the capital n, is something that is bigger than or equal to d. And what we are doing here in Sigma delta quantization is actually surrogate case, where n is growing.
00:48:15.776 - 00:49:02.508, Speaker B: We throw in more and more elements in our space, even though we keep the space RD. So we keep the dimension, but we throw in more and more vectors. Now, if you take any dual frames, you have the frame decomposition as we have been looking at before. And theoretically, you might be very happy about that. But how about applications? If you, for example, want to transmit information about the signal f from Denmark to Toronto, you might want to do it the way that we agree on how to use a certain frame FK. And what I said to you is simply these coefficients, the inner product between f and dk. If I do that, then you can recover the element f simply by putting it into the frame composition and apply this frame that we agree on.
00:49:02.508 - 00:49:48.630, Speaker B: So, in principle, this is a way to transmit signals. But the problem is that I cannot really transmit a number like PI divided with four times logarithm of two. You have to agree on a certain, what we call Alphabet of coefficients to send. For example, the rational numbers are there needs to be a certain discrete set of coefficients that we can send. And this means you cannot just transmit these coefficients in a product between f and e k. You have to transmit some coefficients that we could call DK from our Alphabet that are just a reasonably good approximation to the real coefficients. And that means what you get in Toronto is what you see in the third last line, you get an approximation of the element f, which is just a sum of these new coefficients decay f k.
00:49:48.630 - 00:50:50.510, Speaker B: And then of course, we want to do it in a way such that we get a reasonable approximation of the given element f. And then if you look at it in a statistical way, you can actually prove that when the more redundancy you have, the more likely it is that you get a good approximation, because somehow if you have 1 million vectors in a ten dimensional space, then all these small errors that you're making statistically, they actually more or less erase each other. So there are some analysis about this showing that the more redundancy you have, the better this actually works in practice. Now, what is going on in Sigma delta quantization is that there's a certain parameter r that you can choose, and then there's a procedure called r order sigma delta quantization. You can do that for all values of r. And all of these procedures actually give you some coefficients decay that you might use. The only question is, how good are these coefficients? And if you run this procedure with a canonical dual frame, then it turns out that you cannot get a very good approximation order.
00:50:50.510 - 00:51:54.848, Speaker B: You cannot get approximation order into the power minus r, but you can do it with other dual frames. So the approximation order into the power minus r, which is somehow telling you how good the procedure is. You cannot do that with the canonical dual, but you can do it with other duals. And for me, this is a strong argument saying that we actually need the general dual frames, because this is a very applied context, and you can see here they simply perform better. And I think if you should contact, if you want to know more about this, it is really a good idea to speak with the three authors, because I just mention it here because I like the example, but as you know, I'm more theoretical guy, so I'm not the one who go into these more signal processing oriented things. But I think it's a very nice example of why other dual frames actually are very important. I think there's another speaker starting in a few minutes, so maybe it is a good idea that I stop here.
00:51:54.848 - 00:52:02.616, Speaker B: I had a few more slides, but I can speak about them tomorrow. So I think it's best that we stop here. Don't you agree?
00:52:02.680 - 00:52:04.766, Speaker A: Yeah, but thank you.
00:52:04.830 - 00:52:05.118, Speaker B: Ole.
00:52:05.166 - 00:52:07.174, Speaker A: Let's thank the speaker.
00:52:07.334 - 00:52:10.142, Speaker B: Thank you. Yeah.
00:52:10.158 - 00:52:51.522, Speaker A: We have a few minutes for any question or comments. Is there any question or comments for Ola? It did Ola. I have one on page either 26 or 27 here. Yes. When you define this the 27, please. The next, the next one here. Do you, do you assume that ck as a sequence is in l two? When you define this?
00:52:51.698 - 00:53:03.434, Speaker B: Not necessarily. You just say that for any linear combination that makes it converge, then the coefficient actually has to be zero. Okay, so it's just any sequence, it should happen for all sequences that makes it converge.
00:53:03.554 - 00:53:13.054, Speaker A: But when you say sum k from one to infinity ck f k is equal to zero. You can see the ordering is important. You do not consider unconditional.
00:53:13.874 - 00:53:33.154, Speaker B: But you're starting with a frame and the frame always keeps unconditional convergence. So. Yes, so if you don't put in coefficient ck that belong to your right, that is very true. But you could equal. Well here say that you just assume that it's zero for all elements in l two, that would also be okay.
00:53:33.574 - 00:53:42.358, Speaker A: Okay. Because I was worried about the other way around, not assuming it's frame. I want to check it and I go back. Okay. Yes, thank you.
00:53:42.446 - 00:53:43.474, Speaker B: That's good. Comment.
00:53:45.694 - 00:53:48.954, Speaker A: Any further comment or question for Ola.
00:53:52.594 - 00:54:18.208, Speaker B: So I have, of course, the full presentation there. My plan is to shorten. I have a full file of 150 pages or something like that, and I don't go through everything in these 150 pages. And I plan to remove what I don't use in the file that I show you. But if some of you are interested in this, you can always write to me and then I can send the complete file with all 150 pages that will contain much more than what I actually say.
00:54:18.386 - 00:54:23.544, Speaker A: Please. Thank you very much indeed. Okay, let's thank all again.
00:54:23.964 - 00:54:24.364, Speaker B: Thank you.
