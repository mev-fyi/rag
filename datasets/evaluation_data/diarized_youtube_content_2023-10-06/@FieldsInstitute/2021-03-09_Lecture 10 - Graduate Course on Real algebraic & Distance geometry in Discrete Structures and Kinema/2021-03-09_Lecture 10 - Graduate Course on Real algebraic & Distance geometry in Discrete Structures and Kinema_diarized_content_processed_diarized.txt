00:00:00.120 - 00:01:17.284, Speaker A: So we were in the kind of begun the proof of this important theorem for finding optimal doctor plans. And it basically says that if you know that the graph is independent in the rigidity metroid, so essentially we can assume that there is an abstract underlying abstract rigidity, meteoroid, and independence is, is defined in that metroid. And so we're just looking at this graph and rigidity and so forth are all defined in this matriid. And you know, when we talk about rigid and so forth, we mean rigid, vertex, maximal, proper subgraphs, we mean that. And so in that case, if it's independent, then we have certain just count conditions that we can rely on. So, and we can even assume that there is an oracle that will sort of tell us whether something is rigid. Or we can use this if there is a sparsity metroid that is associated with it, there are network flow pabil game type algorithms that will decide whether something is rigid.
00:01:17.284 - 00:02:35.384, Speaker A: The optimality here is equated to this optimality of the doctor plan is equated to the doctor plan being canonical. So that if we can then as an algorithm, if you're able to find this canonical doctor plan, then we are more or less done, right? So, and then we, when the algorithm, when we talk about the algorithm, which we already did briefly, but we can go a little more this week, the algorithm uses actually a variant of this canonical doctor plan called the pseudo sequential doctor plan. And we'll come to that later. Anyway, right now, what we're trying to show is that this canonical doctor plan is the optimal doctor plan, this observation and this lemma we went through last week. So essentially it's just saying that certain simple conditions can be assumed because we happen to know that the overall graph is independent. Okay, so, and then we're going to rely on these two. I proved the first one, so we'll skip that proof.
00:02:35.384 - 00:03:44.614, Speaker A: And now we are sort of in the proof of the theorem itself. So we simply construct the canonical doctor plan, and later on, and in fact we did this proof also, this is simply saying that there is a canonical doctor plan, because the definition of the canonical doctor plan has two conditions, and it's not clear that there is always a doctor plan that satisfies those two conditions. We just said, let's start with the complete doctor plan, which is known to exist. Complete means children at every stage are all the rigid vertex maximal proper subgraphs. This is of course a huge thing and has no chance of being optimal, because at any node you have lots and lots of rigid vertex maximal proper subglass so the goal is to trim it down. And then of course, that leads to the first property that if, sorry, second property that if their intersection is large enough, if a pairwise intersection of any two of these rumps is large enough, then it's sufficient to retain just exactly two of them. And because their union is the whole thing.
00:03:44.614 - 00:04:15.564, Speaker A: And that was one of the lemmas. That was the lemma before. And if otherwise, you retain all of the children. So this, we can show, satisfies all the properties of the canonical doctor plan. And so we're done. And so essentially this uses the lemma to prove this, the fact that this can be constructed. Okay, now the, now we need to show that the doctor plan is optimal.
00:04:15.564 - 00:05:24.392, Speaker A: I don't know if I can make it a little bit bigger without. Yeah, I guess we could go from here, right, so essentially the property two, you know, the property two is this rigid vertex maximal subgraphs. So we also observed this last time, is that we can always modify the property, modify the doctor plan to satisfy that property without increasing the maximum plan in. So essentially our, our measure that we're trying to optimize is the fan in. And so increasing the depth of the doctor plan we don't care about. So we can always increase the depth of the doctor plan by just adding some nodes along the path to the root. And as long as it ensures this rigid vertex maximal property, we can always, um, achieve this.
00:05:24.392 - 00:06:14.824, Speaker A: Okay, so the proof, um, okay, so the proof of the optimality is done by induction on the height of the doctor plan. So the base case is essentially height zero, which is, which means that all the nodes are single edges. The induction hypothesis is that the canonical Dr. Plans of high t are optimal for the, for whatever graph happens to be at the root node. Okay, so the root node has a graph, I mean, without, because the original graph that we started out with, this independent whatever, this graph is also independent. So all the properties that we proved with lemma and on the observation are all still true. So it's just saying that the doctor plants of height t are optimal for the root node.
00:06:14.824 - 00:07:12.564, Speaker A: So for whatever the graph happens to be at the root node. And then for the induction step, we take a canonical doctor plan of t plus one rooted at a node. And we can assume that the doctor plants that are the sub trees, so to speak, of the children, are themselves canonical for them, because that's how they've been constructed. And so from the induction hypothesis, we happen to know that r of CI, which means the canonical doctor plan for the children, each one of the children of the node c is in fact optimal for its CI. So because we are just saying that the canonical doctor plan is optimal. So we know that the doctor plan sitting under each one of the children is itself optimal for that node. I mean, that graph which is sitting at CI.
00:07:12.564 - 00:08:39.454, Speaker A: So what we're going to do is demonstrate that there is a set of nodes that must be present in any doctor plan r for c. So just to, you know, I keep talking about the two properties, and we saw the two properties a moment ago, but let me just remind you of the two properties. Where are the two properties here? So, yeah, children are the rigid vertex maximal subgraphs of the parent. And then if so, essentially if you either they all intersect trivially or they have exactly two that intersect non trivial. And I think when, here, when we're referring to property two, it's the. Yeah, so property two is basically just the, I think here property two is the property one. That's because this was done in two different, taken from two different sources.
00:08:39.454 - 00:09:15.524, Speaker A: Yes. So this is that it's the children are rigid vertex maximal proper subgraphs. That's, that's what this property two is. Okay, so, so it's sufficient to demonstrate a set of nodes that must be present in any doctor plan that satisfies property two, which means that the children are rigid vertex maximal proper subgraphs. Probably one is that you only pick two of them if their intersection is non trivial, or pick all of them. Their intersection, pairwise intersection is trivial. And we know that these are the two clear cases.
00:09:15.524 - 00:10:22.604, Speaker A: You can't have the situation where some pair is non trivial and some pair is trivial because of the rigid vertex maximum property. And we proved that in one of the lemmas, I mean in the lemma or the observation. So, yeah, so it's just, so we are going to show that any doctor plan that satisfies property two must have a particular set of nodes s. And furthermore, for any such doctor plan, either there are two claims, s must be the set of children of circulation, or for all the ancestors of S. R has the minimum possible fan in of two. So either S is already the set of children of C, or if you actually have something in between C and S. So in other words, some intermediate ancestors of S, then you can simply say that the root, you don't have to worry about it because the root pass the minimum possible fan in of two.
00:10:22.604 - 00:11:13.154, Speaker A: So we're trying to show that it's optimal. And we know that we can never get lower than two. So if we can show that it has the minimum possible fan of two, then we're done. So these are the two possible cases, right? So S has to be present. We know that any doctor plan for C that satisfies property two must have that property, and so either, and then we show one of these two things. So basically that's the strategy of the proof. So the first claim is that for a node C whose clusters have trivial pairwise intersections, any doctor plan of C that satisfies property two must also satisfy property three at C I E, the set of children SFC consists of all clusters of C.
00:11:13.154 - 00:12:01.350, Speaker A: Because this is the only choice. It's the minimum fan in at C for any doctor plan C with property two, including a known optimal one. So property two, remember, is that it's children are the rigid vertex maximal proper sub graphs. So, and property three is that essentially they have trivial intersections. So either, sorry, if they have trivial intersections, then all of them are there. So what we're trying to show is that if sees children are all of these nodes that have to be present, then it must be the case that they actually have trivial intersections. And in that case we don't really have a choice.
00:12:01.350 - 00:13:42.966, Speaker A: We have to have them as the children. Okay? The second claim shows that the case of node C whose rigid vertex maximal proper subgraphs have non trivial pairwise intersections, every canonical doctor plan of C that uses any possible choice of two such subgraphs results in the maximal possible fan and two, sorry, minimum possible finite two in the ancestor nodes a, leading to the same maximal anti chain of descendants of c. So what, essentially what this is saying is that in the other case, every, so if s, if the elements of s are not directly the children of c, then sort of the paths, so to speak, from this s to the descendants of c are all nodes with fan in exactly two. Okay, so there are only two choices. So overall we're trying to show that s is a set of clusters that must be present in any doctor plan that satisfies the property, and that because s must be present, the two options for s, neither one of them either, it's unavoidable, it will be present in every doctor plan. So the every doctor plan is going to have that size. Or we ensure that all the intermediate nodes have fanin exactly of two.
00:13:42.966 - 00:15:09.680, Speaker A: So that doesn't really change the optimality. Okay, so the antigen is maximal in the partial order of rigid subgraphs of c under containment. That is, s satisfies the property that every proper vertex maximal rigid subgraph of C is a superset of some d in s. Okay, so because it's a maximal anti chain, no element of s is contained in the union of other elements of square, and the union of elements of s is in fact c, right? So property of doctor plans has to, I mean, you cannot have a basic doctor plan, canonical or not, has the property that their union, the union of the children, is the parent. And so because of that, you know that we're going to use that to say that these children s have to be present, these sorry nodes s have to be present in any doctor plan that has that property. So we're using that here. So thus, any doctor plan that satisfies property two and hence contains two or more of the rigid vertex maximal proper subgraphs of c, the children must also contain every element of s.
00:15:09.680 - 00:16:12.064, Speaker A: So together, the two claims complete the proof that every canonical doctor plan is optimal. So essentially what this page is saying is that once we prove the two claims, then it would then imply that the canonical doctor plan is optimal. I don't know. I'll stop here for it's kind of, as I mentioned last time, it's an interesting way of proving that a structural property, namely this canonical doctor plan, ensures that that structural property ensures optimality among all doctor plans which don't necessarily have to be canonical. So the only property of doctor plans that we care about here are that their union of the children is the parent, and they're all rigid. Okay. Okay, so now we can go ahead and start proving the two claims.
00:16:12.064 - 00:16:59.464, Speaker A: So the first claim that says that the set of clusters whose pairwise intersection is trivial in. Sorry. Whose pairwise intersection and trivial beam must be children of C in an optimal doctor plan. So what we're saying is that in an optimal doctor plan, if the children of. I mean, sorry, if the c has a bunch of rigid vertex maximal proper sub drafts whose pairwise intersection is trivial, then they must be children. So to show that the union of no subset of the children can be c, thereby requiring all of them to be included as children. Okay, so this is.
00:16:59.464 - 00:17:48.344, Speaker A: So remember that for any doctor plan, the union of the children has to be the parent. Has to cover the parent. Yeah. Has to be the parent. So if we can show that for any pair of these, if their intersection is trivial, then or any proper subset of them, their union cannot be c, then it must be that they must all be included. Okay, so we proved this by contradiction. So once we prove this, then we are basically saying that in the canonical doctor plan, the case where the parent has rigid vertex maximal subgraphs were all have pairwise trivial intersections.
00:17:48.344 - 00:18:36.248, Speaker A: There's nothing much you can do in terms of decreasing the fan in over there, because they all have to be children in any doctor plan, canonical or not. Okay, so. So to prove this by contradiction, you assume that a strict subset is minimally rigid. I mean by definition, I mean, by definition of doctor plan. And because the whole thing is independent, you know, the nodes are being rigid implies they're actually minimally rigid. So if this u is not c, then we found a larger proper subgraph contradicting vertex maximality of Cr. So it must be that u is equal to c.
00:18:36.248 - 00:19:32.286, Speaker A: By the way, we already said that we can, without loss, assume vertex maximality, if you remember, because we can always introduce intermediate nodes in the doctor plan that ensure vertex maximality without increasing the fan in. So here we are assuming everything is vertex maximum. So without loss, we can assume that the only doctor plans we're considering are the ones that have this rigid vertex maximality property. So if u is not equal to c, then we found a larger proper subgraph contradicting vertex maximality. So it must be that u equals c. So, in fact, I have to point out that in the first, if you read the original papers with the doctor plan, we didn't use this vertex maximality property. What we used was something that we called cluster minimality, which is exactly what this is.
00:19:32.286 - 00:20:26.814, Speaker A: It's saying that, you know, if this extra property that we have here, so that if you took a, any node of the doctor plan, no proper subset of its children would have been rigid. So in other words, in any node of the doctor plan, if you look at the set of children, it's sort of like a minimal subset whose union is rigid. So if you took any proper subset of them, it would not have been rigid. So that's, so that was a different, slightly different condition than this vertex maximality. Sorry, rigid vertex maximal. Proper subgraphs. But it's here we are using that property here.
00:20:26.814 - 00:21:18.338, Speaker A: So we're saying that this means that we would have found a larger proper sub graph contradicting vertex maximality. So it must be that u equals c. However, since the intersection is trivial, so we know that u intersects. So one of the items of the lemma that we used in the beginning is that if their intersection is trivial, then their union, sorry, if their union is isostatic, then for every KCI union, Ck is isostatic. And if the intersection is trivial, then for every k, the intersection between any two of them is trivial. So if you go here, sorry, claim one. Claim one.
00:21:18.338 - 00:22:35.084, Speaker A: Okay, so this would mean that so u intersect ck must be one or more trivial, that is, disconnected vertices. Okay, so by definition of doctor plan, ck, which is c intersects ck and you know, so because it's contained in the child isn't contained in the parent. So we'll know that u equals c. So ck is actually u intersect ck. So ck is a collection of disconnected vertices and an isostatic subgraph of c, which is not possible, right? So since c is isostatic, it means that the union of no proper subset of c one through cn is isostatic, nor is it equal to c, thereby proving claim one. Okay, so claim one, essentially saying that in once you have a set of clusters whose pairwise intersection is trivial, they must all be, they're all contained in c. Then they must all be children of c in any optimal doctor plan with no canonical conditions, except for the fact that their union must include the parenting, must cover the parent.
00:22:35.084 - 00:24:07.922, Speaker A: Furthermore, since canonical DIa plants has nodes with proper rigid vertex maximal subgraphs as children, if as in this case, their pairwise intersection is trivial, it follows that any node has at most as many children as a doctor plan without this restriction, because the union of the children must contain all edges of the parent. So that's the union covering thing. So therefore a canonical doctor plan is the optimal choice in the case of trivial intersections. Okay, so now we have to consider the case of non trivial intersections, okay? And that's essentially what two is talking about. So if some pair of the child clusters has a non trivial intersection, then choosing any two as children will result in the same maximal anti chain of descendants of c. So this is basically saying that throughout to get to c one through, sorry, to get to all the intermediate nodes that we would have, would all have a fan enough two, so that you're not really doing any worse than the optimal diagram. Okay, so to prove claim to note that if CI intersects Cj is isostatic, then by one of the, the first observation that we showed CI union Cj is also isostatic, this all based on the fact that the overall graph is independent.
00:24:07.922 - 00:25:13.032, Speaker A: This means that by isostatic, remember is minimally rigid. This means that the union of any two children by the lemma, the union of any two children of c is c itself. Thus, any two children can be chosen to make a canonical dear plan that is the minimum possible fanin node for that doctor plan. However, to guarantee that any two are the optimal choice, it must ensure that the minimum fan in over all descendants leading up to a common maximal anti chain s of subgraphs. Okay, so this s that has to be present in every diagram, every optimal er plan. Okay, so you're making a choice here, right? So the problem is, in the previous case there was no choice. So essentially we said if the children, sorry, if nodes, max children are, have non trivial intersections, then every doctor plan is forced to have all of those nodes as children because no subset of them is rigid.
00:25:13.032 - 00:26:15.196, Speaker A: So it really, and since they together have to cover the parent, in any doctor plan, they all have to be children. So that particular fan, and was unavoidable in this case, we're saying if their intersection is non trivial, then you make a choice. But then the fact that you're making a choice here doesn't hurt anything. So first of all, you can make a choice in any pair of them is going to cover the parent, that's fine. Secondly, we're also going to say that which pair of them you choose, it really doesn't matter, because in any doctor plan, there's got to be a particular subset s of graphs and the path, so to speak, the intermediate nodes between s and c in this canonical doctor plan will all have fan in two nodes. So it's not going to make any difference. So to prove that this holds, I mean, it's not going to change the optimality property.
00:26:15.196 - 00:27:01.174, Speaker A: So to prove this, let's take the set. So denote the intersection of all the elements in this set of the cluster ck to be I, and let's take our kick to be c difference this ck. So if you took CI and CJ, where I naught equal to j are the children, let's say that those are the children. So we can assume that all subgraphs are induced sub graphs of C. That's how we pick them. Rigid vertex maximum proper subgraphs are all induced. We know that c equals I union this remainder, so to speak.
00:27:01.174 - 00:28:12.524, Speaker A: All of these remainders, by the way we've defined them, and CI is the intersection union these remainders. So think of this as this is the common intersection of all of them, and this is all the remainder stuff, which is not an intersection. And see is this union of the remainders of all, and CI is the union of all except one, the ith one. So the isostatic vertex maximal subgraphs of CI are in fact I union. This you're taking out the ith one and the first one, I one and the I minus eight one and the second one, and so forth and so forth. You continue like this, all of whose pairwise intersections are isostatic subgraphs. So any two of these are viable children for CI this continues for n minus one levels, always with fan of two, at which point every descendant of C is some I union rk for this, with every k appearing at least once.
00:28:12.524 - 00:29:33.518, Speaker A: So at the last level there are exactly two rigid vertex maximum proper subgraphs and hence a unique choice. So essentially this is just showing that how you chose the choices, I mean the pair at every stage, regardless of the sequence of choices of CI and Cj and of their descendants at each level. The doctor plan has the optimal fine of two for every node for n levels, and the collection of last level nodes contain the same maximal anti chain of subgraphs for all choices. Okay, so essentially just saying that there are two cases, the trivial intersection case is unavoidable and the other case you make a choice and the choice ensures optimality because the fan in is always two and the last level nodes at this bottom have to be present in every doctor plan. So you can't really avoid that either. So that's, so that's the overall structure of the proof to show that the canonical doctor plan is in fact optimal. So now the algorithm, I'm just quickly going to go over the algorithm that we talked about before.
00:29:33.518 - 00:30:39.888, Speaker A: It uses this idea of a branch, which I think is another new idea in this paper to, to find the optimal. So we change the canonical doctor plan slightly into something called a pseudo sequential doctor plan that makes the algorithm much faster, I mean much easier to analyze and so forth. So we have this canonical doctor plans as we know, this is pairwise intersection, rigid vertex maximum per subs graph with the pairwise intersection being non trivial in this case. And then, you know, you can think of it this way. So because the intersection is non trivial, there's a common intersection and these are the remainders. And, and you continue in this fashion, right? So we want to prove that this, so we define this new class of Dr. Plants called pseudo sequential Dr.
00:30:39.888 - 00:31:48.514, Speaker A: Plans, and we show that the fan in is no larger than the canonical doctor plan and then we show how to build it. So there's a second sort of layer of the proof. The first layer was just to show that this canonical doctor plan is optimal. The second is more based on. The second proof is sort of needed by the particular algorithm that we have is that we use this other particular class of canonical doctor plants called pseudo sequential doctor plants, and we essentially show that they are like canonical in the sense that their fanin is no larger than the canonical one, although they disobey some of the properties. Okay, so a doctor plan where if all pairs of nodes intersect trivially, then all of them are children, non trivially then exactly two that integer. But then the pseudo sequential doctor plan, you don't take c one and c two, but you take c one and the pseudo sequential doctor plan of c two different c one.
00:31:48.514 - 00:33:19.190, Speaker A: Okay, so you know, instead of doing this here, they have this common intersection. You basically take this, the doctor plan of this and the pseudo sequential, and the pseudo sequential doctor plan of whatever remains, so to speak. So the remainder, so the example is here. So this is the canonical one and the pseudo sequential one keeps, this part keeps this one of the children, c one, let's say. And then it takes the remainder. So in other words, if you threw out the intersection, the intersection in this case is the straight prism here, the vertical prism, what is left over? If you see, because this triangle belongs to the vertical prism, the only parts that are left over are these three edges that connect the one triangle to the other triangle in the prism and the triangle itself. Okay, so, and you keep doing this, right? So the first, I mean, I'm not going to do that proof, but you can prove that this modification of the canonical doctor plan is not going to change the fan in, optimal fan in.
00:33:19.190 - 00:34:00.084, Speaker A: It may create more nodes with the same fan in, but the optimal fan at the fan in the overall, you know, we're minimizing the maximum fan in. That doesn't change. And you can kind of see it here. So you've got this one node here which eventually at the end is going to have all of those elements. So here this one will have, you know, fan in a five and you have a fan in a five, some, some node with a fan of five at the end. Anyway. So what we can show is that this modification of the canonical doctor plan to pseudo sequential doctor plan will not change the max planning.
00:34:00.084 - 00:34:27.360, Speaker A: It may create more nodes with that fan in, but it won't change the max plan. So I'm not doing that proof, that's a separate proof. It's very similar. It uses this common anti chain idea and that we talked about before, and in proving that canonical is optimal. So it's exactly the same. It's very similar. Right.
00:34:27.360 - 00:35:15.444, Speaker A: So now what we do is we actually produce this, the algorithm produces this pseudo sequential Dr. Plan using this idea called a branch, which I think is important in a way. Um, I've never seen it before in uh, in, in you know, sort of tree traversal type algorithms. So branch is of a tree, t is every node on the path from a to b and their children. So essentially what's shown here is the branch. The leaves of the branch is exactly the set of rigid vertex maximal subgraphs of a difference b. So in other words, if you have a tree with a root here and this is b, I take a difference b, then these guys, these black guys, are exactly the rumps of a difference b.
00:35:15.444 - 00:35:55.524, Speaker A: And we can find the ramps of a difference b in order, v squared, we can show that. And so then that's how the algorithm proceeds. So essentially you can think of a front kind of moving in this direction. Okay, so to, by computing the ramps of. So, so the algorithm starts with a, tries to find a pseudo sequential Dr. Plan by computing the ramps of g difference e for every e. Okay? So it doesn't even need to do it for all e, but at most it will have to do it for all e and doing a quadratic amount of work for each leaf.
00:35:55.524 - 00:36:33.518, Speaker A: So we started a basis step. We compute g as the single node in the doctor plan. Recursive step, you compute a branch for each leaf in the doctor plan. So how do you do that? You arbitrarily choose leaf l and an edge f in l. And then you compute the rumps of l difference f. And then for each of the rumps, choose an arbitrary edge g and compute the ramps of l difference g. And then compute the branch from l to f using a linear number of intersections to position each leaf.
00:36:33.518 - 00:37:58.524, Speaker A: So there is a little bit of a work done. We can compute this rumps easily because we have taken gene intersection e, but then to sort of figure out where they have to be positioned takes a little bit of work. And that's, that, that's the overall complex, I mean, analysis of the algorithm to find the pseudo sequential doctor plan. And there's another proof that I'm skipping here, which is to show that the doctor plan has, that the pseudo sequential Dr. Plan has the same maximum plan in, as the canonical one. Okay? So what I want to do next is go over for the remaining, you know, I don't know, maybe 50 minutes, go over more carefully what we briefly discussed for how do you, after having found the doctor plan, how do you optimize the algebraic complexity of solving this parent system given the solutions of the child systems of the doctor plan? So in this case, think of this as one node of the doctor plan. And, and that node has all of these potential children.
00:37:58.524 - 00:38:41.674, Speaker A: So as you can see, they all have trivial intersections. So in a canonical or pseudo sequential canonical diaplan, they would all be children. What is shown here, which is the, you know, the pink circles which are triangles. There's a green circle which is also a triangle. And then there are these, these black circles which are tetrahedral, I mean, k force. And they all form the whole thing and they, oh, I forgot to say, this is, of course we're doing this in three dimensions. And so the trivial intersection here, say between this cluster and the pink cluster, I mean this black cluster here and the pink cluster here is an edge cg.
00:38:41.674 - 00:39:16.034, Speaker A: So, so this would be in, in a, in a canonical doctor plan this would be a situation where you have c which is the whole thing. And then the CI's all have trivial intersections and their union is the whole thing. You know, union of them contains all the edges of the original c. So these are all the trivial intersection pair. Trivial pairwise intersection. Children of sea. So this would have been the case of claim one.
00:39:16.034 - 00:40:07.380, Speaker A: Okay, and now we want to, so this is good. So optimal doctor plan is, will, will contain this in a way. And so what we want to do is figure out, now any optimal di plan is going to give you this. Now you're going to further optimize to figure out how to solve the system. And we sketched how we were going to solve the system, sort of, you know, the obvious way of doing it, which is, you know, pick a minimal arbitrary minimal minimal covering set of child clusters. So covering set in this case is it contains all the vertices. So we picked in this case c one, c two, c three, three black guys.
00:40:07.380 - 00:40:43.270, Speaker A: And then we said, so this is the vertices. Remember, when we say covering set, we mean it, the vertices, not the edges. Whereas the requirement of the doctor plan is that the union of the children contains all the vertices and edges of the parent. Right. So this is just the vertices. So we take these three guys and then you set this coordinate system, take a minimal covering set. So why take more clusters than are needed? And in fact take the minimum one and this turns out to be the minimum one, c one, c two, c three.
00:40:43.270 - 00:41:14.074, Speaker A: And then take the coordinate system c to be one of the child clusters. So this is the obvious way to do it. So I have unoptimized, so to speak. So set the coordinate system to be one of the child clusters and call it say c one. And now you say, oh, c two is sort of connected to c one at this point a. C three is connected to c one at the point b. And then those two, I mean, so, and these two have an additional constraint between them.
00:41:14.074 - 00:41:59.986, Speaker A: This is the di, these two have an additional constraint which is the ef and then the cluster c two and c three, in addition to having a distance constraint, also have an incidence constraint at c. And so you solve this system, you know the solutions to these three. So you only solve those three children. And then you, um, add these in order to get the system for c, you add these three distance constraints plus two incidents. I mean one, two, three incidence constraints. Okay. And then to do, and you do the obvious sort of local coordinate systems being transformed using rotation and translations.
00:41:59.986 - 00:43:04.672, Speaker A: And that gives you a bunch of variables which are connected by trigonometric relations. And that will give you in the end, something like how many equations and unknowns? I think some. Okay, so this will degree at most three in the six variables, blah, blah, blah, blah, blah, related to the three trig relation equations of this form. But we can do better than that by, again, there are two possibilities. So the, yeah, so this is the original way of doing it. So where this is the home cluster, you know, you have the rotations about these two. And then you could say, in addition, we have no, this is one possible optimized way of doing it.
00:43:04.672 - 00:43:30.756, Speaker A: But let's not worry about it. This is the way you would do it. In the end, to really optimize things is to take the cluster in the middle. So this is not a minimal covering set. It's not a minimal covering set. You basically take the cluster in the middle as your home cluster. And then you sort of use, you know, the fact that you don't solve explicit incidences.
00:43:30.756 - 00:44:07.946, Speaker A: You simply say, well, they have the same ac. So essentially I can parameterize c two using one single rotation about ac. And I can parameterize c three's position using a single rotation about this and a single rotation about this. So those are the three rotations. And then after that you simply have these, how shall I say, distance constraints left. There are no incidences at all. So you just have this three distance constraints.
00:44:07.946 - 00:44:42.146, Speaker A: And if you do that, basically. So in other words, you're picking a set that's not the minimal covering set. And you choose the middle one as the home cluster. And then you simply do what I just said and you'll get something quite a bit better. Basically you just have three equations and plus of course, this one. And so essentially you have six distance constraints and the six variables compare with eight. And then all constraints are quadratic.
00:44:42.146 - 00:45:29.274, Speaker A: Okay, so even that eight already in the first option, as I said, what I initially said was without using the quaternions. But then after we, you know, it's already optimized to get the eight in the first place. But we can do a lot better by just using this non minimal covering set and this idea. Okay, so, and you know, the distance is remain quadratic because there's, you know, you parameterize this using this one rotation. So the coordinates of this are not going to increase in degrees. So basically you just have a quadratic here. Okay, so for this distance, right.
00:45:29.274 - 00:46:40.624, Speaker A: So essentially what we're doing here is that we are using these rational quaternion based parameterizations. In that class of parameterizations. We want to minimize the degree slash number of equations. So that's the optimization problem. So this paper that we worked on, and this was published in 2010 in the Journal of Symbolic Computation, essentially under this class of parameterizations tells you an combinatorial problem that can be a combinatorial optimization problem that then optimizes the algebraic complexity. I didn't have time, unfortunately, to make slides out of this, but I will do this before I post it on the copy on the piazza. So this is starting with a slightly different problem example.
00:46:40.624 - 00:46:58.178, Speaker A: It's three, it's very similar. It's not, maybe it is the same example. Oh, it is the same example. Sorry. It is the same example. So it's got the triangle in between, it's got the three tetrahedra. So this would have been c one.
00:46:58.178 - 00:47:39.730, Speaker A: The triangle in the middle would have been one cluster and c two, c three, c one, c two, c three are the three tetrahedra. And then there is an additional distance constraint that's shown here to solve this, some of this that we have already seen. So we've seen this. This should be familiar. So to do the pairwise parameterizations, okay, so the notation, okay, ta is the translation that maps a to the origin. T inverse a maps origin to a. It's a point.
00:47:39.730 - 00:48:24.732, Speaker A: A rab is the rotation that maps b minus a to the x axis. Okay, so these are these quaternion, I mean these, these rational quaternion based parameterizations. MBC is the matrix whose columns span three reals. So basically it's b c b cross c, and then t is the undetermined translation that we're trying to find. R is the undetermined rotation about the x axis, one degree of freedom. So essentially these ones. So if you go here, you know, we're trying to find the rotation about this central triangle in these three guys, that's an unknown rotation.
00:48:24.732 - 00:49:15.096, Speaker A: So that's r and q is the undetermined unit quaternion, which is in the case where you, the incidence is at a single point you have three degrees of freedom. And that's what we showed right in the beginning when I said, so if we did it this way, you have a three degrees of freedom rotation here, as opposed to this one degree of freedom here. So that's this q. So r looks typically like this, q looks typically like this. And we're using these simple stereographic projection. So we're going to think of c as this number. I mean, all the cosines like this, and s like this, q looks like this.
00:49:15.096 - 00:50:26.058, Speaker A: And so essentially, we apply the quaternion transformations to c two and c three. We still retain three by three incidence equations and three distance equations, reducing the complexity to this. And this was the case that I talked about in the sort of intermediate way of doing it with Saturnians, but you can actually make it better by choosing c one, c two, c three, c four as the covering set, instead of the minimal covering set, which we picked here as just three of them. So when we started out and said, let's pick three of them, c one, c two, c three is the unoptimized one. We now pick c one, c two, c three, c four. So the middle one and the three four three around it pick the home coordinate system for c four. Since c four and each of the CI share two points, the position and orientation of CI are now fixed, except the rotation of the axis through these two points.
00:50:26.058 - 00:51:25.964, Speaker A: These rotations can be explicitly parameterized by these r's that we had here, undetermined rotation about the x axis. And so the complexity of the parameterized system then becomes three polynomial equations in the variables, the maximum degree of four after clearing the denominator and reduces to two due to the identical choice of c one, c two and c three. So essentially we end up with just two degree two and three equations. So, okay, so the interpretation. So now we want to say, okay, what is the, what is this optimization that we just did that allowed us to reduce the complexity among this, I mean, optimize the complexity among this class of rational parameterization. So what we did is, okay, so here's the picture. So we have the c four that was in the middle and all of these, you know, complete maximal, I mean, there's the canonical doctor plan.
00:51:25.964 - 00:52:06.094, Speaker A: All the children have pairwise trivial intersections. They're all present. This is one node of the doctor. So now we are, by the way, the other case where the two side that are exactly two children and they have a non trivial intersection. That's extremely easy because you have solved the two children and then they have a non trivial intersection, which means in 3d they would have at least three points. So then you just completely, you know, it determines the rotations and translations of one of the clusters because those three points have to be coincident. So you know, there's no, almost nothing to be solved there.
00:52:06.094 - 00:52:57.000, Speaker A: So it's the case where you have trivial intersections between the child clusters. That is the difficult case. Okay, so now we are, so when thinking about the way we do the rational parameterization, we think about what is it, what happens to the number of variables and what happens to the degree when each time we do one of these ways of parameterizing. So if we do three a point. So essentially if you think of the number, you know, number of, so if we're identifying a point, essentially, so, or identifying three, sorry, sorry. This is identifying three points. This is the case of the non trivial intersection.
00:52:57.000 - 00:53:53.018, Speaker A: You basically have nothing, almost nothing to do. Okay, so if you, if you're identifying two points, then you have this, you know, rotation about a single axis and then you have one variable which represents the rotation about that axis and then you have degree two. By this rational parameterization we talked about, if you in fact identify a single point, then you have degree three. I mean, then essentially you have three degrees of freedom and you have, so you would have three parameters in the parameterization and the degree will be four. By this rational parameterization, if you have no intersections, then there's nothing. So you basically have six degrees of freedom because there's no intersection. So nothing is fixed.
00:53:53.018 - 00:54:35.940, Speaker A: And so then you have degree four. So this is talking about the number of vertices that are in the intersection between two children. If there's a non trivial intersection, there's almost no work to be done. The intersection is two, just one of the trivial cases. Then we have to, we have good, pretty good case. So the number of degrees of freedom or the number of new variables that are added in the system is one number of variables. If the intersection is single vertex number of extra, I mean, degrees of freedom is three because, you know, one vertex is pinned and the rest have to, has three degrees to move around and the degree of the equation is four and so forth.
00:54:35.940 - 00:55:39.544, Speaker A: Okay, so that tells us what we want to optimize. So these are the things we want to optimize, obviously. And so, and with an emphasis on the number of variables because it's well constrained. So the number of that would be the number of equations as well, and with the secondary trying to optimize the degree. Okay, so first optimize this, then optimize the degree. Okay, so to do this, we can now ask the question how well does the system of minimal algebraic complexity from the class of these parameterizations compare with parameterizations of s that are not in this class? Okay, so that's basically the question that we're going to solve. So, so these are all properties which we already knew in the canonical doctor plan.
00:55:39.544 - 00:56:23.844, Speaker A: So the key idea, key object here is something called an overlap graph. So if you have your standard collection of rigid bodies, by which in this case without loss, we're never going to consider the non trivial intersection case because it's so simple. So it's only trivial intersection. So you have the entire complete list of proper maximal rumps. Okay, so that's the standard collection that we are looking at. So what we, an overlap graph is a weighted undirected graph whose vertices are the rigid bodies, by which I mean this children of a given node which has already been. So there, they have already been solved, so to speak, their systems have already been solved.
00:56:23.844 - 00:57:11.634, Speaker A: And if an edge between a pair of Cicj represents k incidences in that table, the weight weight wk in that table that we just saw is assigned to that edge. So we're making a graph. Each vertex is a cluster is one of these guys. So each vertex is going to be one of these guys. An edge between them will be weight depending on how much is in common. So for example, if you look at c four and c six, they have in common a single vertex. So the k there is three wk in that, sorry, wk in that case is three, because, you know, once you fix this vertex, this guy can still move around, has three degrees of freedom.
00:57:11.634 - 00:57:49.522, Speaker A: And the degree, you know, is something that we also consider. Okay, so let's go back here. Okay, so there's a graph that's drawn for this. This is the overlap graph. And you see that c four is in the middle and it's connected to all of these guys. See three c four and all of the other ones have put there for a moment. We have split this up into c two one and c two two, just to show that at the next level, you know, potentially you might have to solve these guys.
00:57:49.522 - 00:58:26.214, Speaker A: But don't worry about that. Okay, so both of these together would be this cluster c two. And then you, you find, so you find a minimal covering set. I mean, you don't find, you don't need a minimal covering set. What we need is a covering set that, such that you can find a spanning tree with the minimum total weight. Okay, so here's the spanning tree tea. Here's the covering set that we choose, and here's the overall collection.
00:58:26.214 - 00:59:24.986, Speaker A: So you start out with a collection of children from which we want to pick a covering set in such a way that it has a spanning tree with minimal weight. Okay, so is the incidence constraints of the covering set, and incidence tree in SC is a spanning tree of the subgraph of the overlap graph induced by this covering set. It represents the set of constraints that includes one, a subset of the incidence constraints, constraints that rigidify the bodies in sea. And we will eliminate the constraints in Eft using the parameterizations that were in that table for the weighted value. Right. So the remaining constraints that have not been eliminated together, the further distance constraints that are needed to rigidify the bodies that are not covering set form the system. So in this case, for example, if we picked c four and c four, c one, c two, c three.
00:59:24.986 - 01:00:45.274, Speaker A: So these two together are c two. So if we pick c four, c one, c two and c three, then well, c four, c one, c three, and then it has picked c six and c five. Okay, so if you pick these, then what happens is you end up with the spanning tree having weight one. So essentially it picks c one, c two, c three, c five and c six, c five and c six, c five and c six. So you can see that we have c one, c two, c three, c five and c six, right? And that's a spanning tree. And you can see that there's one degree of freedom here, one degree of freedom here, one degree of freedom here, one degree of freedom here. The reason we did that instead of just picking c one, c two, c three, c four is that now you don't have this distance constraint in the final system, right? So essentially you've lost this distance constraint.
01:00:45.274 - 01:01:51.798, Speaker A: And so you just solve this kind of c one, c two, c three, c five and c six. Okay, so this one is attached to this. This one is attached to this. This is, so you still have one distance constraint, this distance constraint here between c one and c three, and you, one more here between these, okay, so it turns out that that is the optimal, in fact, what we had here with c one, c two, c three, c four is not the optimal one either. Okay, so once you do this, we'll get the actual set of constraint. I mean, so here, these are two possibilities. This problem, this here's the overlap graph, the actual overlap graph, the sub graph of the weighted overlap graph induced by one covering set.
01:01:51.798 - 01:02:26.770, Speaker A: So you pick one covering set, covering set, remember, is all the vertices covers all the vertices of the parents. This is another covering set. And it, no, it's the same covering set. In both cases, you pick the same ones. Here's the spanning tree, and then there's also the choice of the root. Okay, so of all the covering sets of. The algorithm consists of two parts.
01:02:26.770 - 01:03:05.724, Speaker A: Find the optimal incidence tree. The optimal incidence trees of all the covering sets. Determine the set of spanning trees over all the choice of trees and roots. Determine a rooted tree that minimizes the sum of the depths of all the nodes. The depths of all the nodes gives you the degree, right? So the minimum spanning tree minimizes the number of variables. And choosing the root in such a way that the depth is minimized minimizes the degree. So essentially, and then the part two, after you do this, of course, there's the part two, which you actually solve the system.
01:03:05.724 - 01:03:45.184, Speaker A: Okay? So that's how this one was solved. And this is all the possible solutions. So this shows, is another example, bigger, slightly bigger example with an overlap graph. So here, another standard collection of rigid bodies. If you can see it's a, it's got like several tetrahedra. You can see that there's a tetrahedron. So I think there's a triangle here and a triangle here, but then the other four are tetrahedra.
01:03:45.184 - 01:04:37.540, Speaker A: That's another, turns out to be another prop, you know, bunch of clusters which have trivial intersections. And then this turns out to be the overlap graph. And then you can pick multiple possible spanning trees of different covering sets. And then we pick one of them. And then we do the, turns out that you can do all of them in like this one chain with a single distance constraint and a single incidence constraint here. So three incidences plus one distance, and then that turns out to be the optimal one, and you get these solutions. There's yet another example here, which is a tetrahedron in the middle.
01:04:37.540 - 01:05:00.546, Speaker A: As you can see, there are a whole bunch of rigid bodies that you can see here. It's a whole bunch of triangles. There's a tetrahedron in the middle. Triangle, triangle, triangle, triangle, triangle, triangle, triangle, triangle. Right? So a whole bunch of them. Here's the overlap graph, and that's another collection with pairwise trivial intersections. So here's the overlap graph, and here are possible spanning trees.
01:05:00.546 - 01:05:47.080, Speaker A: And, you know, choice of spanning, choice of covering set, choice of spanning tree, choice of the root. The root allows you to minimize the degree. So those are the optimizations. And then basically the complexity of resolving in general, because these systems are small, these problems are easy, but the hard part is picking the covering set that minimizes the spanning tree. That's you can reduce the Steiner tree problem to it. So that's an NP complete problem. So in general, if this gets very big, this simultaneous optimization of covering set in order to get the minimum spanning tree is a hard problem.
01:05:47.080 - 01:06:17.204, Speaker A: The minimum spanning tree by itself, once you pick the covering set, is an easy problem. But you're picking a covering set to minimize the spanning tree. That's a hard problem. And then you have a second optimization, which you're separated. You can't optimize two things at the same time. So you first optimize the number of variables, in other words, the covering set and the spanning tree, and then minimize the degree by choosing the root appropriately. So that's the paper here.
01:06:17.204 - 01:06:58.864, Speaker A: As you can see, we have done several examples and turn out quite nicely, compared it with some other subdivision. I mean, in terms of actually solving the system, we were pretty agnostic. It was a canned solver. We used this numerical solver by Gauco. Turned out to be really fast, but we found all the solutions. And so this was just a classification of different solutions for one of the problems here. These are all based on some kind of platforms, you know, Stewart platform type problems.
01:06:58.864 - 01:07:39.864, Speaker A: If you look at this, you can think of it as having like a rigid body on top, like a plate, if you will, and there's a plate at the bottom. Think of this as a kind of a plate. And then you have, you're connecting, you know, all of these to all of those. This, this is also a type of a platform. We have five on top and five on the bottom. But these are all tetrahedral here. They're all rigid, they're all, you know, they're all isostatic collections.
01:07:39.864 - 01:07:48.452, Speaker A: So. Okay, so that's one paper then. Yeah.
01:07:48.628 - 01:08:06.764, Speaker B: I have a quick question. Spoke about how the Steiner tree problem was NP complete and how that was for NP hard and how that was constraining some of the complexity.
01:08:06.924 - 01:08:07.624, Speaker A: But.
01:08:10.324 - 01:08:45.746, Speaker B: In practice, when one's got these NP hard problems, we often use some kind of non optimal but highly efficient techniques, right? Like we use some kind of randomized approach to solving and we get something that we can prove maybe is within, you know, 90% of the optimal or something like that. For the Steiner problem that you mentioned, do you know of kind of practical random approaches that would be useful?
01:08:45.930 - 01:09:21.994, Speaker A: Um, I'm thinking, by the way, I'm thinking of the combinatorial Steiner tree problem, right? So it's not the geometric Steiner tree. So, um. So yeah, I don't know offhand, I think it is one of those very difficult ones. So the approximation algorithms, you know, once you go to np complete problems, they're all NP complete. So in other words, if any one of them had a polynomial time algorithm, p would be equal to np. But between them there's a nice gradient. There's a huge gradation of how good the.
01:09:21.994 - 01:10:19.238, Speaker A: I can stop sharing here while we talk. There's a big gradation of how difficult the approximation algorithm is. So for example, the traveling salesperson problem, you can show that if it has a constant factor approximation, then p would be equal to np. So in other words, there's no constant factor approximation algorithm for the traveling salesperson problem if you believe p is not equal to np. Similarly, it can be shown that the click problem and similar ones close to it, independent set and so forth, don't have a, even something like, let's say there's a clique of size square root of n. You can't even get within, say, n to the one third or something like that. Not just a constant factor, even n to the epsilon factor, unless p equals np.
01:10:19.238 - 01:11:36.416, Speaker A: So there's, on the other hand, you have all these other very nice problems which have constant factor approximations. So you have bin packing set coverage, a whole bunch of very easy ones, which can be solved either using linear programming relaxations or semi definite programming relaxations. There's a whole class of them that can be solved using semi definite programming relaxations. So you have a whole bunch of them like that, and then, you know, so, so you have whole gradation of problems that have constant factor approximation algorithms. Some have even, they're even better. They're not just constant factor approximation, they're constant factor approximation schemes, which means that you give me a constant factor and I can solve. You solve it for you in that constant factor if you allow me to run polynomially long in that factor, right? So like Euclidean traveling salesperson problem, it's NP complete, but has that property, right? So, so you have fallen fully polynomial time approximation schemes.
01:11:36.416 - 01:12:26.804, Speaker A: Then you have constant factor polynomial time approximations. Then at the other end you have all these problems which, you know, if they even had something like an n to the epsilon approximation, AP would be equal to NP. And then you have some things in the middle like traveling salesperson, which don't have constant factor approximations unless p was NP. So you have this whole gradation of NP complete problems. Now, I believe Steiner Tree is one of the harder ones, both the geometric version and the combinatorial version. But what we are hoping is that, see, since we've already gone through the optimal doctor plant, we have a good doctor plan. So essentially, a child, a parent, and then the number of children is about as small as you can get.
01:12:26.804 - 01:13:17.484, Speaker A: So now for this problem, the recombination problem, that's where we're starting. The number of children is at the size of our overall problem. And let's hope that that's small enough that our Steiner tree problem is not so big. So in this example that we did, you know, the platform examples. No, I mean, that's not. You can see the sizes of those trees, right? They're small, you know, but that portion of the complexity, as you know, that complexity as a, in perspective, in, as a sense of proportion, of the overall algebraic complexity, is very, very small. So.
01:13:17.484 - 01:13:49.280, Speaker A: But we had to be honest and say, if you really wanted to do this, it would be the Steiner tree problem. You know, that's why we put it there. Okay, so I see a lot of comments on chat. I'm just seeing everybody's happy with when they're presenting. Alex is talking about monodromy solver. When did you ask that question, Alex?
01:13:49.392 - 01:13:51.152, Speaker B: Sorry. Yeah, I was just sort of.
01:13:51.288 - 01:14:09.558, Speaker A: I don't think I really understood Will's question, but based on the answers and stuff and discussion of complexity. But it was, I was just thinking, like, it's very difficult to solve, to find all solutions, but often you can use monodromy to find some solutions of a polynomial system.
01:14:09.606 - 01:14:11.514, Speaker B: And it's a probabilistic thing.
01:14:11.974 - 01:14:29.752, Speaker A: Yeah. Oh, I see. So what you were talking about RP? I think he was talking about randomized polynomial time algorithms, which most NP complete problems don't have. Right. So RP, if. Sorry. People don't believe RP and NP are the same.
01:14:29.752 - 01:15:10.374, Speaker A: People believe that RP is most likely polynomial time. Okay. Yeah. So they believe that if you can do something in randomized polynomial time, like, for example, primality testing is a classical example, right. In the eighties, or maybe very late eighties, it was shown that testing whether a number is composite is, has a randomized polynomial time algorithm. Soon after that, it was shown that, in fact, testing with its prime also has a randomized polynomial time algorithm. So in other words, it's in the class RP, intersect corp.
01:15:10.374 - 01:15:29.370, Speaker A: And that class has a name, by the way. It's called a Las Vegas algorithm, which is always correct, almost always fast. No, p. Yes. P equals. Faye is asking whether p equals BPP is a conjecture. Yeah.
01:15:29.370 - 01:16:12.776, Speaker A: People believe for sure that I'm seeing this. If this is same to what you said. The randomized polynomial time algorithms are one sided error, so they're a little bit different from BPP. But the conjecture is that p equals Rp. People believe, I mean, I wouldn't say it's as strong as a conjecture, but they certainly do not believe that randomized polynomial time and NP are the same thing. Neither do they believe that NP and BPP are the same thing, but generally believe that randomization is just a kind of an artifact. We can always de randomize it.
01:16:12.776 - 01:18:03.810, Speaker A: And then you would have a polynomial time algorithm. Right? So, Wiggerson wrote a paper that said either p equals bp, p, or exponential time algorithms have p space, and p space and exponential time are the same, or something like that. So, essentially pointing out that if we don't believe that these two other classes are equal, which we don't, then it should be the case that p equals bpt, right? So I think there's a paper by impagliazio on impagliazo and witness, okay, say, indicate that if we don't believe that I think something like exponential, something like p space and exponential time or something, that two other classes are unequal, then, okay, so, you know, we are, we went all over the place. So anyway, let's see, what other questions were there? Monotromy solver, etcetera, etcetera. But that's a whole different course. I mean, if I love teaching complexity, I've been teaching complexity for a long time, that'll be a whole different course. Let's proceed to another aspect of these incidences.
01:18:03.810 - 01:18:54.884, Speaker A: I mean, trivial intersections of systems that are the children of a parent in the doctor plan. Okay, share screen. Okay, so first I will go. Okay, there's another issue that comes up, which has to be considered, which so far we have not considered. Okay, I'm not quite sure what is going on at this point. Maybe I stop. Share for a second.
01:18:54.884 - 01:19:41.584, Speaker A: 2nd, give me a moment. No, somehow my acrobat reader is acting up suddenly in the middle of this. There it is. Okay, came back. So go back to zoom. Share screen. There we go.
01:19:41.584 - 01:20:21.174, Speaker A: Can you see my. What did it do now? Stop. Share. There we go. Okay, so I think I'm just not going to increase. The problem was, I tried to take it to full screen and it just gave up. Okay, so here we go.
01:20:21.174 - 01:21:17.358, Speaker A: Okay, can you see my acrobat reader paper? Yeah. So there's a problem that we have not considered. When we did all of these trivial intersections and treated them as incidences, there is a, there's a problem that we did not consider. And that's an important problem that gives rise to another underlying natroid. Okay. And so we basically wrote three papers about this, the one that I just showed you, and then the second one, which is here, which discusses this particular problem, and then a third paper that sort of connects the combinatorial problem that we solved just now with the overlap graph and the minimum spanning tree and so forth. In the minimum spanning tree, there's the spanning tree.
01:21:17.358 - 01:22:22.274, Speaker A: I mean, there's the usual graphic metroid underneath there and the metroid that I will talk about just now, which relates to incidences. And then we wrote a third paper that sort of puts those two metroids together. The graphic metroid with the spanning tree and this one, which I call the C matriid, puts those two together and talks about a gen common optimization problem. Yeah, it actually shows that if you solve the C matroid, I mean, you find an independent set in the C matriid, then you can just independently do the optimization that we talked about a moment ago with the spanning tree. And that's good enough. In other words, it produces the optimal solution. In either case, once you first solve this problem that I'm talking about right now, find an independent set in this metroid, and then you apply the second optimization.
01:22:22.274 - 01:23:03.734, Speaker A: It works out fine. That's what the third paper did. So what are we really doing here? So what we're saying is that if we had, let's say, three rigid bodies, c one, c two, c three, which correspond to the children of a parent C, let's say, in a minimally rigid system, c one, c two, c three themselves are minimally rigid subsystems. I simply call them rigid bodies. And they have this pattern of incidences. So in other words, they, all three of them share a vertex and two. Every pair of them shares two vertices.
01:23:03.734 - 01:24:08.938, Speaker A: What this is showing is the different ways in which you can think about the incidences. So you could say, for example, okay, so the important thing to notice is that because this distance between v three and v four is actually the same distance in c one and c two, it better be the same. If it is not the same, then you cannot put them together. Could be ill, I mean, would not have a solution. So, because there is this extra constraint, this incidence, this incidence between c one and c two along this, that shares this pair that has a same distance between these two, means that if you are established, if you're writing down the incidence, you do not have six equations. So in other words, you don't have three. I mean, this is in three dimensions.
01:24:08.938 - 01:24:52.374, Speaker A: So there's x, y, z here and xyz here. You would say, oh, the coordinates of this point in this cluster is equal to the coordinates of this point in this cluster. That would be three incidences and three incidences here. But because this distance is in fact the same between them, there are only three plus two independent such incidences that you can establish. Otherwise it would be an over constrained system. Similarly, you can do the same thing here between these two. And then once you have done that, in fact, there are only two other incidences that you can establish without making it an over constrained system.
01:24:52.374 - 01:25:58.666, Speaker A: Here's another possibility, is that you put three here, two here, just any pair. Instead of x, y, z, you put x and y, and then here again this. And then you put here a different one, the z coordinate, same between these two. And then you put one incidence here, pick whatever you want, x, y or z, and then you put three incidences here. Or you have this. So essentially, this is saying that they're all, I mean, which do you pick? Which of them is independent? Are they dependent? Maybe some of these incidence systems. So essentially, just trying to pick a set of incidences, keeping in mind the only sort of dependence is caused by the fact that this distance between these guys, say this distance or this distance is the same, is already fixed between these.
01:25:58.666 - 01:26:44.266, Speaker A: Okay, so. And it turns out that having a well constrained system, so in other words, an independent system is important because you actually do it. You actually practically see the problem. If you have an over constrained system, it just turns out to be badly conditioned and you end up with no solution, no one. Give it an over constrained system. For example, I started out and I just gave three, three, three, and this distance is fixed. You might expect that if this distance is the same in these two, there would be a way to reconcile the three coordinates of the point on this side, three coordinates of the point on this side, and so forth.
01:26:44.266 - 01:27:23.448, Speaker A: But there's small errors in the solution. When we found the coordinates of these points when we solved c one, and similarly here. So the exact distance between them in this cluster versus this cluster turns out to be slightly different. So that now when you try to equate these three and equate these three, it will come back and say, no solution. Okay, so this is, it's not some cooked up kind of problem here. In fact, the problem came about because we faced this issue that it would come back and say no solution. So we had to pick an independent system and so independent system of incidences.
01:27:23.448 - 01:28:13.456, Speaker A: And that's where this came from. So somehow it's extremely slow. So these are just three ways, the different ways that are being chosen. So we want to avoid these introduced incidence over constraints. Okay, close this guy. Close this or dependences while retaining any inherent over constraint. So in other words, there may be some inherent over constraints which we don't want to throw out.
01:28:13.456 - 01:29:06.054, Speaker A: Okay. It's just that we don't want to introduce these guys. You can actually ignore this inherent over constraints because in the problem that we are thinking about, we're starting out with something that's independent system is independent. So there are no over constraints that are already there. There are no dependent, existing dependences. So we want to attempt, we want to detect only these in these dependences that we are introducing when we are doing the recombination where we're treating the solution for, I mean, sorry, the system for solving c given the solutions of cis, the children cis as a system of, because they have trivial intersections as a system of incidences. Okay, so the second attempt at the above example, there are only six incidence constraints at point v four.
01:29:06.054 - 01:29:38.874, Speaker A: V four is the one that's in the middle that are independent. Three of them between c one, c two and three of them between c two, c three. Because once you say c one is identified with c two and c two is identified with c three, you have automatically identified c one with c three at the point v four. Right? So there's, you say these two are equal. These two are equal at the point v four. Then all three are equal at the. So the other three complete a cycle of incidences and hence give a locally detectable dependence.
01:29:38.874 - 01:30:18.774, Speaker A: Okay, so discarding these three reduces the number of incidences to 15, but they still clearly form a dependent system. Okay, so here, this is this example. I don't know. So hard for me to scroll somehow it takes a long time. Just the acrobat reader is reacting very slowly. Okay, so just saying that this, this is the one that we just discussed. So we've got three here, three here.
01:30:18.774 - 01:31:11.194, Speaker A: Once we've said that the v four position here is equal to this, then we don't put these. And then we put the two, two, two here. So this is independent. Okay, now the third attempt. Okay, third attempt. Okay, so we did this already. Three, we didn't see 23.
01:31:11.194 - 01:31:32.972, Speaker A: Okay. Okay, 15. Okay, third attempt. Note that the shared distance constrained between each pair of clusters permits a total of only five independent incidents. Constraints. The shared distance is the one between v four and v three or v four, v two and v four. V one.
01:31:32.972 - 01:32:25.394, Speaker A: And so to account, choose three incidence constraints at the points v one, v two and v three, totaling nine and only four independent incidences. Let's say two incidents for the x and y coordinates between this and two. This reduces the number of incidents to 13, but they're still clearly dependent. Okay, fourth attempt, avoiding both the above types of local dependencies. Taking care to choose only twelve incidences still does not guarantee an independent system where in the right of figure one, choose three incidences each at points v two and v three, one at v three, two incidents for xy, etcetera. But we show that the last incidence is dependent on the ten incidences between c one and c three and between this. Okay, so just illustrating that, figuring out whether it's independent or not is not an easy thing.
01:32:25.394 - 01:33:20.794, Speaker A: So for a small example, you may be able to do it, but in general, you know, you need to analyze the underlying matrix. So, so this is, so these are all the ones that were actually in the figure. The first three attempts that I read about were not actually in the figure. Okay, complete maximal decomposition, which we have heard about, which is this. Okay, so here's an example. So this is another example which illustrates the problem. Just illustrating the problem.
01:33:20.794 - 01:34:29.314, Speaker A: Okay, so I want to get to the picture where the C matroid is shown, but it's very hard to do. Okay, here, um, another problem is being illustrated about what happens if you chose incident. I mean, if you choose dependent constraints. Okay, so we have the complete maximal decomposition, which we know about, which we're going to assume canonical decomposition. And then, so we're using a simple count, we're assuming everything is independent and so on and so forth. We're using a simple count of the degrees of freedom. And now we're going to define, under this assumption, we're going to define a well formed system of incidences, okay? So it has no local cycle of incidences.
01:34:29.314 - 01:35:30.984, Speaker A: So for any vl and k greater than or equal to three. So there's this recombination system is what we call I of the set of incidences that we have here. Between the system is what we call I and no local cycle. So if you essentially have v followed by Cic two l and then v CI two CI three l, et cetera, blah, blah, blah, blah, blah. So that clearly creates a local cycle of incidences. Then id if, sorry, if all of these are there, then we don't want to have the incidence, incidence between the first one and the last one. Okay, so that's one.
01:35:30.984 - 01:36:37.064, Speaker A: Then we've got so v and l are the two vertices on which c one. I mean, each of these two pair of clusters is incident, incidence, incident. Then if you have a subset of clusters, then those incidents, then we want to ensure that the sort of the obvious incidence degree of freedom count has to be less than or equal to what you expect for that subset of clusters. Right? So, and then the overall number has to be equal to the, for a well constrained system, what do you expect for a well constrained system? So we're starting out with the whole thing being minimally rigid. All subsets are minimally rigid, everything like that. Because we have this, we're starting out with something like a canonical doctor plan, which we call the complete maximal doctor plan in this case. Okay, so all of those basic assumptions you, you take.
01:36:37.064 - 01:37:23.804, Speaker A: Okay, so having said that, we have this seam graph that we can define from which you can get an underlying matriid. I'm going to really go quickly here. In fact, it's probably easier for me to just describe the matroid with a picture, so, especially because I'm having such trouble moving this acrobat in a document. Okay, so here's the, these are the different seam. This, this is what we call a seam graph. This is the graph on which the matriarch is defined. And so these, this is the same path, this is same cycle and.
01:37:23.804 - 01:37:36.124, Speaker A: Oh no, I give up. I think I'm going to have to do this on Thursday. I think we'll have time on Thursday. I'll stop here.
