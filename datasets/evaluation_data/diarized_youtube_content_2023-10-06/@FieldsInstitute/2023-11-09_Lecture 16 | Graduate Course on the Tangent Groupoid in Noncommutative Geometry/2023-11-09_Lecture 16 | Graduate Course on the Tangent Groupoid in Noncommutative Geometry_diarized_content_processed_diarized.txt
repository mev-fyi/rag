00:00:00.680 - 00:00:54.990, Speaker A: We're going to look again at the index theorem today, and my goal is to strip out as much as possible of all of the stuff we've been learning about groupoids and groupoid, sea star algebras. I was once teaching a class with John Rowe on tangent groupoids and the index theorem, not, not like this class, because these scalable operators hadn't even been invented then. But anyway, the tangent groupoid was there, and the index theorem was there. The main objective was to prove the index theorem. And it was going really badly because there's just so much to talk about. And there were a bunch of mathematicians in the room, mathematics students, and they were all completely confused. And there were a bunch of physics students who weren't confused at all.
00:00:54.990 - 00:01:42.802, Speaker A: But somehow that worried me even more because I had no idea what they were learning. And so I thought, oh, geez. So it was too late to do anything in the class. But dwelling on this disaster of a class, I decided to try and boil down this tangent groupoid proof of the index theorem business to its essential gooey residue. Like, you know, when you make vegemite or bovril or something? Bovril is an english version of vegemite, which is made out of beef. And you take an entire cow and you put it in a big pot and you boil it in some water for a few months, and after you've boiled it for so long, all that's left in the big pot is just a little sticky ball of black goo at the bottom. And that's what Bovril is.
00:01:42.802 - 00:02:54.594, Speaker A: So I wanted to do the same thing with the index theorem to sort of boil it down to its disgusting sticky essence so it could be spread on a piece of bread and consumed. That's what we're going to do today. Okay? We have not in this class talked about the Dirac operator, and so I need to say just a few words about that. Well, you can try this experiment at home. So let's just do a quick review of the rack operators in some small number of minutes, see how long it takes. We're going to cut lots of corners, of course, because the time is so short. First of all, the following you can always do.
00:02:54.594 - 00:04:33.434, Speaker A: If you have a kick, then you can always find a bunch of matrices, two k of them, and they're of size, two to the k by two to the k. And this collection is unique in the obvious sense, up to unitary equivalence, and the matrices satisfy the following algebra. First of all, you can fiddle with this a little bit, but let's make our matrices skewer joint, skew a joint. The critical conditions are these, that when you square these matrices, you get well, you can make your choice plus one or minus one. We're going to make minus one. And anticomute spy is not Jack. So you can do this, and you can only do it in one way, just one way up to unitary equivalence.
00:04:33.434 - 00:05:35.138, Speaker A: And the base case is this. Here are two matrices which fit the Bill. K is one, two to the k is two. So there should be two by two matrices. And these fellows do the job, or the other way around, whichever you like, you still do the job, and they're unique up to unitary equivalents. So if I did list them in the other way, there'd be some way of doing some miraculous conjugation which would reverse the order back to its initial state. Okay, and now we can build the Dirac operator on rn.
00:05:35.138 - 00:06:50.664, Speaker A: It's a linear partial differential operator of order one, and it acts on vector valued functions. And the vectors are this big two to the k by two to the k long, and it's just this sum of cj dx to the jack. Of course, we have a drive to do everything in a coordinate free way. So let's try and decoordinatize this. A coordinate free version of both the theorem and the Dirac operator would be something like this. Let's start with an even dimensional maybe, I'll say, keep this notation. Two k dimensional euclidean vector space, a real vector space with an inner product.
00:06:50.664 - 00:08:20.716, Speaker A: The theorem above can be expressed in the following way about the uniqueness of a certain real linear map from e into the complex endomorphisms of a complex in a product space of dimension two to the k. And this algebra up here you can write in kind of an easy way. It's the algebra which says that c of e star is minus c of e. And if you're talking about this a linear map, the you can economize a little bit. The equation is that c of any e squared should be minus the norm of e times the identity. The idea is that if you have a bunch of matrices c one up to c two k, like in the theorem, you can build a map from r two k into matrices in the obvious way. Namely, you take a vector in r two k.
00:08:20.716 - 00:09:12.843, Speaker A: It's just a list of two k numbers, and you form a linear combination of these c's. And that gives a map and it has the right properties. This algebra here condenses into this algebra here. Okay, now you can define the Dirac operator. Let's just squeeze it in here and it would look like this. Now you can just sum over any also normal basis of e. And what do I mean by Nabla, ej? I just mean directional differentiation in the direction ej.
00:09:12.843 - 00:09:43.844, Speaker A: Okay, so that decoordinatizes the Dirac operator. But if the euclidean vector space is r two k, and if this map c is built so that the jth basis vector goes to this matrix up here, cj, then you get exactly the same Dirac operator. Yes, not yet. I'm going to talk about orientations next. Yeah, let's do that. Next. I think I want to do it next.
00:09:43.844 - 00:09:45.364, Speaker A: Yeah.
00:10:04.474 - 00:10:06.018, Speaker B: This acts on function on e.
00:10:06.026 - 00:10:53.304, Speaker A: Of the values of s. Good point. Yeah, this acts on vector valued functions. S, vector valued functions, this particular differential operator in the small box at the bottom. So continuing with this coordinate free version, if you have a bunch of vectors for e which form an orthonormal basis, like we already used in the definition of the Dirac operator, then you can form the following thing. Oops. And the notation should be a little different.
00:10:53.304 - 00:11:35.948, Speaker A: So this also is an endomorphism of s. That's the complex vector space s. Now you can calculate what happens if you take the adjoint of this element. Well, if you take the adjoint of this element, all of the order gets reversed. And each cej picks up a minus sign because they're all skewer joint. And so what. And, but these Eis and ejs, c of Eis and c of j's, they more or less commute or anti commute with one another.
00:11:35.948 - 00:12:08.696, Speaker A: So you can mess around with what happens when you take the adjoint. And what you'll find is you get back to what you started with. And you can also imagine squaring e. Well, if you believe what I just said about the adjoint, you could imagine writing gamma in two different ways. One, the sort of reversed version of this way. And you'll see that when you square gamma, there'll be a lot of cancellations, you'll get a lot of Cej times Cej, and that's just not one, but minus one. And now you get a bunch of minus ones.
00:12:08.696 - 00:13:37.924, Speaker A: And you had a bunch of square roots of minus ones. And you, you wonder, how does it all work out? The answer is you get the identity, which you can check in this little example in your head. And finally, this gamut depends only on the orientation class of the basis. It's not quite independent of the choice of orthonormal basis, but it almost is. If you vary the orthonormal basis in a continuous way, then you'll get one and the same gamma. But if you jump from one, if the, if the change of basis matrix from one orthonormal basis to another has determinant minus one, then you won't get gamma, you'll get minus gamma. So an orientation of e determines what's called a grading or z two grading of s minus ligament, namely an orthogonal direct sum decomposition of s into two subspaces.
00:13:37.924 - 00:14:56.144, Speaker A: These are the plus minus one eigenspaces. And if you have a z two grading of s with a certain additional property, namely that each of these Clifford operators should send s plus plus minus and s minus twice plus, then that grading will, going backwards will determine an orientation. And all of this stuff extends to, all these ideas extend, at least locally, to vector bundles. Suppose we have now v sum, let's say smooth, smooth, but not really relevant, but that's where we're going. Real euclidean vector bundle over some manifold m. I'm thinking of the tangent bundle for v of m, but I'm also prepared to accept the normal bundle for some embedding as being a suitable v. So I sort why I'm just calling it v.
00:14:56.144 - 00:16:49.440, Speaker A: Let's say that the rank of this bundle, what's relevant here, it's not the dimension of m, but the rank of this bundle. And so let's assume it's even dimensional. The fibers are even dimensional of dimension two k. So we can ask does there exist a complex permission, in other words, with an inner product vector bundle s of rank two to the kid. And now we could ask to make this construction here fiber wise. So in each fiber we have a real vector space and we can ask is there a corresponding s fiber? So we'll be mapping now into the endomorphisms bundle of s with, well, the same conditions, namely each fellow, each, the image of each vector should be a skewer joint operator on the corresponding fiber. And yeah, the square should be the norm minus the times, the identity.
00:16:49.440 - 00:17:54.804, Speaker A: I missed off a little square. This is not scaled properly, but if I put a square there, it will scale properly. And the answer is not necessarily, but we're only going to be interested in situations where this thing does exist. And this fellow, if it exists, we'll call s spinner bundle for v, Fliford bundle for v. If v is a trivial vector bundle, then for sure you can do this, because that's just like doing it for a single vector space. So locally you can always do this. So it's a problem in topology to decide whether you can solve this problem globally and construct a certain vector bundle.
00:17:54.804 - 00:19:03.714, Speaker A: Here's an example. Suppose v was the underlying bundle of a complex emission bundle. It's like saying suppose my even dimensional euclidean space is just ck. Yeah, yeah. Underlying means the real vector bundle underlying the complex vector bundle. Forgetting about multiplication by I, you can write down an explicit solution. Namely, you take the exterior algebra of the as a complex vector bundle, so that has the right dimension.
00:19:03.714 - 00:20:06.026, Speaker A: If you did this exterior algebra as a real vector bundle, it would have the wrong dimension, but v of course, would have dimension. K is a real as a complex vector model. So the exterior algebra would have dimension two to the k, rank two to the k. And now you can define these Clifford operators just to be some interesting combination of exterior multiplication and contraction. Interior multiplication. When you contract elements of an exterior algebra, you're supposed to use vectors in the dual, not in V itself. But we're supposing there's an inner product floating around complex inner product.
00:20:06.026 - 00:21:12.044, Speaker A: And that complex inner product identifies the dual space of v with, with the complex conjugate of v. So what comes out of this is exactly the sort of matter that we want. And indeed this is not. Even though everything is complex, this is only a real linear morphism bundles or a bunch of real bundles. So if V is a real bundle which carries a complex structure, then you can solve this problem. And another place where you can solve this problem. Trying to find the longest piece of chalk.
00:21:12.044 - 00:22:42.956, Speaker A: Nothing exceeds 4 cm anymore. It's getting. By the end of the semester, it'll all be. That's when the class will end, when each piece of chalk gets down to 1. Then, forgetting another way you can solve this problem is if you already have a couple of solutions for a couple of factor bundles, v one and v two, like we're discussing over here, then this fellow is a solution. That is, say spinner bundle for v one plus. What's this funny tensor product with a hat? Well, it's just the ordinary tensor product, but the hat means that the way in which the vectors in v one and v two act on this is a little bit difference.
00:22:42.956 - 00:23:28.134, Speaker A: So c of a vector in v two, let's do it this way. But with v one on something in here, s one and s two should just be c of v one s one s two. Pass it like that. On the other hand, c of v two acting on an s one, tensor s two. I'm not allowed to write s one tensor c of v two, s two. Because then there would be commutation of c of v one and c of v two instead of anti commutation. So you just plug in sine and I guess I need to.
00:23:28.134 - 00:24:34.714, Speaker A: Let me write this, figure out how much I forgot to tell you earlier. What's missing in this discussion is that I only want to consider, and I didn't say it here, although we just talked about it up there, I only want to consider complex emission gradient. How do I say Zeta graded is graded. Just saying graded grading. So this bundle should break up with a direct sum of two orthogonal sub bundles like that. And it should be the case that c of v exchanges these two fellows. How does it say c of V applied to something in s plus? Is something in s minus and vice versa? Sorry, I forgot to say that.
00:24:34.714 - 00:25:21.232, Speaker A: And so when I say spinner bundle, I'm including that which I didn't, but now I did, so all is good again. And that means that we can make a formula for the way that c of v two acts on a tensor product, which is contingent on whether this s one section lies within s plus or s minus. And if it lies in s minus, you have to stick in a minus sign, should be in degree s one like that. And then it all works out nicely. So, this business of constructing spinner bundles has a certain, I don't know, additivity, multiplicativity. It's like the exponential function converts sums to products. Did you have a question?
00:25:21.288 - 00:25:37.844, Speaker B: Just to make sure that something here we built grading. If it is the case and changes the parts, then it will always be the grading induced by this part. Globally, yes.
00:25:38.424 - 00:26:32.836, Speaker A: If there is a grading, then that grading determines an orientation on V. Because what you can do is you can build this gamma operator for any local frame or in any fiber, and you can say that a basis is oriented if and only if the gamma you build that way agrees with the operator on this direct sum, which is one here and minus one here. And that divides all possible orthonormal bases into the, the good ones and the ugly ones. And so now you have an orientation. And it's a fact, almost by the definition of the way I did it, that this is, that the grading must come from that. Ganon? Yes. Yeah.
00:26:32.836 - 00:26:56.634, Speaker A: Okay. Yeah. All right. This is an extreme distillation. But fortunately, Eckhart wrote an entire book on this. And so for more details, you can ask him or read his book. It's right there.
00:26:56.634 - 00:27:24.514, Speaker A: You might as well just ask him. Okay, now let's get to the main issue for us, this is effectively linear algebra that we're doing. This is linear algebra. There is up to unitary equivalency in each one of these. I'm saying that's the generalization of this theorem here. This theorem deserves to be proved, and I mean, it must be proved in your book. Right? I mean, yeah, it's.
00:27:24.514 - 00:27:25.234, Speaker A: There you go.
00:27:25.354 - 00:27:31.154, Speaker B: But it was from you that this thing is equivalent to having a species spectrum.
00:27:31.314 - 00:27:54.530, Speaker A: Yes, I'm going to mention that in just a moment. Yeah. At least in the context of the tangent of the structure. Yeah, yeah, it's. Yeah, we'll maybe make a comment on that in just a moment. Yeah, we're heading in the direction of spin c. Right, good.
00:27:54.530 - 00:28:40.330, Speaker A: I don't think I have anything more to say about that. Now, let's. Okay. Wasn't five minutes. Well, it depends. I guess I did mention the Dirac operator in the first five minutes. Now, from now on, e will be euclidean vex space of even dimension like we had at the beginning, if you like.
00:28:40.330 - 00:29:58.834, Speaker A: I switched from e to v here, and that was extremely fortunate because this v is not to be confused with this e. All right, now I suppose we have a closed submanifold. Suppose I have a spin of vector space, or e, like this, e, to some even dimensional euclidean vector space, blah, blah, blah, blah. I need some, let me call it like this. It's not completely consistent with the notation that's going to come, but by sov I'll just mean the constant, the trivial bundle over m over e. Excuse me, with fiber s of e is promoted, s is promoted from being a vector space to a vector bundle.
00:30:00.894 - 00:30:03.574, Speaker B: Do you want fiber e or fiber s?
00:30:03.694 - 00:30:51.794, Speaker A: Thank you very much. I want fiber size. Right. So, yeah, let's imagine taking this bundle, which is a bundle over e and restricting it to m. Now, it's a bundle over m, and every tangent vector for m can be thought of as just a vector in e. And so it acts by clippered multiplication on this space. And the same thing for normal vectors.
00:30:51.794 - 00:32:05.074, Speaker A: For the next little part of this lecture, when I say a normal vector, I mean one like this that points in the normal direction, something which is orthogonal to tangent vectors, not a vector which is in the quotient of all vectors, modular tangent vectors. So normal vectors to the embedding of m into e are in particular tangent vectors to e at certain points. And so they can be thought of as vectors in e. And so they act on this thing. And it's meaningful to ask whether this thing can be decomposed as some bundle, which is a spinner bundle in the sense we've just been discussing for the tangent bundle, hence a hat, some bundle, which is a spinner bundle. For the normal bundle, lm means the normal bundle, the embedding, which it's best to think of here in this geometric sense, as vectors perpendicular to tangent vectors. Okay, that's a question.
00:32:05.074 - 00:33:43.746, Speaker A: And the answer is not necessarily. After all, if you could do it, then m at least would have to be orientable, because each one of these spinner bundles, now that I fixed the definition and put in the grading, determines an orientation. But it does happen from time to time. For instance, if, for instance, the tangent bundle carries a complex structure, I mean, in this sense over here, it's the underlying bundle of some complex vector bundle. I don't mean that m is a complex manifold. I just mean you can fit some complex vector bundle structure on e, excuse me, tm, because then we can take, and here's just a little more linear algebra. Well, we might as well define this fellow to be exactly what we were discussing on the right hand board, this exterior algebra bundle, which does carry a Clifford structure or spin or structure.
00:33:43.746 - 00:34:30.819, Speaker A: What about the normal fellow? Well, you could do the following thing. You could look at STM. So the operators or the vectors in Tm act on the fibers of STM as linear transformations via this Clifford multiplication map, this one here. And they also act on this thing, like I was just saying. And so you could look at maps, morphisms of vector bundles from STM to se, which are compatible with this Clifford multiplication structure. Let me call that hom sub clip Tm. I didn't tell you what the Clifford algebra is by this.
00:34:30.819 - 00:35:26.674, Speaker A: I just mean that Clifford multiplication on this fellow and on this fellow commutes with the morphism. And now you can check, I mean, there's an obvious map now from the tensor product of these two things into so v, namely, just evaluation. If you have something in s of tm and a morphism from s of tm into s of e, of course, you can evaluate the morphism on the something. And that'll give you something in s of e. And that evaluation map is an isomorphism of vector bundles, indeed of Clifford, of spinner bundles, if you fix up the signs, right, which I'm not going to get into. So sometimes this happens, and then we can define a Dirac operator. It'll depend on this choice of solution to this splitting problem.
00:35:26.674 - 00:36:14.574, Speaker A: And locally, it'll be given just by the usual expression. So these xi's here should be a local orthonormal frame, the tangent button. What you get here by linear algebra doesn't depend on the choice of frame. So this is some well defined global operator. What's this thing? This is differentiation in the direction of xi. It's a suitable connection in the sense of geometry. I'm not going to dwell on this too much.
00:36:14.574 - 00:37:59.924, Speaker A: On the bundle s tier, the word suitable has an actual definition, and if there's time, I'll mention it at the end. Some connections are better than others for some of the small technical purposes that are going to come up, but it's not a big deal. And that's an operator on sections C infinity sections of this bundle s. And it's a good operator from the point of view of this class and the point of view of index theory, because it's elliptic. It is, as they say, formally self adjoint. So that means if you're studying l two inner products, then you can move the D across. And the key thing here is elliptic.
00:37:59.924 - 00:39:16.900, Speaker A: It's a formally self adjoint elliptic to differential operator, and it has grading order one, which means the following thing. This space of sections decomposes into the even sections, direct some of the odd sections, the plus sections, I guess I was calling them direct some of the minus sections. And D evaluated on a plus section is a minus section and vice versa. That's what odd grading order means. This is enough to guarantee that there is an index. And now the question is, what is the index? So I shall tell you the answer, and I shall attempt to indicate why the answer is correct for these particular operators. I should say that the solution here, when it exists, it's not necessarily unique if there's one solution.
00:39:16.900 - 00:39:51.296, Speaker A: If you think about it, if you take one solution, STM, and you just tensor it with a complex line bundle on M, that's another solution, because you can tensor the SNM vector bundle by the dual complex line bundle, and this will still be a true equation. So as soon as you have one solution, you can move it around using line bundles. So it's almost always the case that this thing is not unique. But that's okay. Neither here though, nor there for us, for that matter. This connection, the suitable connection, is also not unique. So there's a whole bunch of operators.
00:39:51.296 - 00:40:41.874, Speaker A: But the choice of a suitable connection is not really relevant for index theory. All suitable connections will give rise to the same index. But solving this pRoblem, figuring out what the bundle is, does make a difference. The index theorem will notice, index theory will notice, if you change s of Tm to something new, I should say, may notice, and a choice of s of Tm, as Eckhard was saying, is what's called a choice of spin c structure on the manifold m. So, first of all, the manifold M may not have a spin C structure. And if it does have a spin C structure, one of these choices, it may not be and usually isn't unique. Okay? But for example, almost complex manifolds, these guys here have a canonical choice or natural choice of spin c structure coming from this choice of size.
00:40:41.874 - 00:41:53.024, Speaker A: That's correct. Any two spin c structures can be moved one to the other by a complex line bundle, a thing which looks very much like this. Yeah, exactly. Now, let's return to Kasparov for a moment and his definition of a theory. Of course, Kasparov famously defined something called KK, where you shouldn't assume that either of those K's stands for Kasparov. That's what it's called. But as I explained, there's a nice definition of K theory, which is built using families of fredholm operators.
00:41:53.024 - 00:43:26.554, Speaker A: And now I want to extend Kasparov's notion of cycle by talking about essentially self adjoint, compact, resolvent operators. You could go much further than this, but this is as far as we want to go. And there'll be some grading floating around from now on, and the operators will be interesting and interested in. We'll all have grading over one. But that's a lesser, a lesser issue. Suppose I have an operator D, which is defined not on the entire, not on an entire Hilbert space H, but on some subspace called Dom D, the domain, and it maps into H. And let's suppose this subspace is a dense subspace, dense linear subspace, not a Hilbert space in its own right, just a dense subspace that's called an unbounded operator in the world of operator theory.
00:43:26.554 - 00:44:36.248, Speaker A: And the only ones we're interested in are the ones which satisfy the very same formal adjointness condition that I wrote down before. And this only makes sense if s one and s two are both in the domain. This is the Hilbert space in a product for H. These are the operators we're going to be discussing. I'm going to be imagining that both this domain and h break into a direct sum of two parts, an orthogonal direct sum of two parts, just like everything else is broken into two parts in this lecture. But we don't need to discuss that just yet. What von Neumann discovered back in 100 years ago almost, is that operators which look like the self adjoint by virtue of satisfying this equality here are not good enough to do any kind of functional analysis with, and you need a stronger condition.
00:44:36.248 - 00:45:37.584, Speaker A: And one way of satisfying that stronger condition is to speak of what's called essential self adjointness. There is, I guess, r bounded operators, and essential self adjointness has to do with taking d and studying these so called resolvent operators on a finite dimensional space. If you have a self adjoint operator on a finite dimensional space, so it satisfies exactly this equality here, but you're in finite dimensions, then you can check very easily that the operator is d plus minus I times the identity. Any purely imaginary number or any non real number times the identity. Those guys are invertible. So you can talk about the inverses in general. That's not the case, and so you have to assume that they do exist and they ought to be the inverses in the following sense.
00:45:37.584 - 00:46:43.314, Speaker A: So an operator, it's easy to see that the operators d plus minus I times the identity are one to one. And because they're one to one, this operator d plus or minus I times the identity is uniquely defined on the image of d plus or minus I times the identity. And the question is, is there an extension, oh, I missed off a word which I'll have to add in a moment. Is there an extension of that operator to a bounded operator going in the other direction? I'm speaking here of arbitrary self adjoint extensions, but I want to add the word unique here. There are a unique couple of operators which satisfy these equations. An operator is self adjoint. If there are unique operators which I'm going to write this way, which satisfy this equation, they're uniquely determined on the range of d plus minus I times the identity.
00:46:43.314 - 00:47:48.974, Speaker A: But if that range wasn't dense, then they wouldn't be uniquely determined. So the uniqueness is really tantamount to knowing that the ranges of these d plus minus I identity operators are bounded, are dense, excuse me? And these guys are examples. It's a little more tricky to prove that, but it follows from the functional analysis we've been doing using pseudo differential operators. As long as you're dealing with a suitable connection so that you have this equation here, this thing is not only formally self adjoint, it's essentially selfie joint. And here's the rest of the verbiage from this title.
00:47:51.034 - 00:48:07.184, Speaker B: Yeah, can I ask, so if D was actually run on all of. Yep, it was actually celebratory. Then we know that it's spectrum is yep. And so in particular, you know that 15 plus one side is.
00:48:09.364 - 00:49:03.304, Speaker A: Yes. So if you have a bounded self adjoint operator, then it's automatically essentially self rejoined. In fact, it's essentially self adjoint means almost very nearly self adjoint. But a self adjoint operator is indeed in particular almost and very nearly self adjoint. And this extra term compact resolvent means which I didn't quite fit down. There is a term you can only apply to essentially self adjoint operators because it refers to these bounded operators. These are compact operators.
00:49:03.304 - 00:49:47.974, Speaker A: Actually, if one of them is compact, then so is the other. So take your pick. And it's a fact which also follows from, well, the theorem we proved that negative order pseudo differential operators are compact operators. But this fellow here is not just essentially self adjointed is or has compact resolve. Has the compact resolvement properly? I'm not quite sure. Now, what does Kasparov have to say about essentially self adjoint? This is not Kasparov, this is von Neumann. And I don't know, these theorems are due to the Swedes, Laos, goding, or someone.
00:49:47.974 - 00:50:54.966, Speaker A: That's not what Kasparov's business is. What Kasparov does is he explains how to build k theory elements from families of operators like this. Suppose I have now a family of compact resolvent, essentially self adjoint. I'm not going to keep doing that. Operators, the operators we've just been talking about on the fibers of a continuous field of Hilbert spaces. What do I mean by this? Well, I don't really mean that D is defined on all of Hx. I'm just following convention here.
00:50:54.966 - 00:52:08.674, Speaker A: What I mean is that for each DX there is some domain of Dx on which the operator is defined, and all of these lovely properties hold. As for these domains, as you move from point to point, I don't really care how these domains change. I just, and I don't really care how the Hilbert spaces change, except that I do care that in the background there is the structure of continuous field of Hilbert spaces like we discussed before. I need the following thing, which, which is quite the constraint. Now let's study these resolvents. So these are honest to goodness operators, and they're honest to goodness individually point wise compact operators. And what I need is this is a continuous field of continuous section, I'll just say a continuous field of compact operators.
00:52:08.674 - 00:53:06.464, Speaker A: And if the space X is not compact, I want this to be vanishing at infinities. What I mean by a continuous field of compact operators is that it's a section of the associated continuous field of compact operator algebras, as we discussed earlier. So they're not just individually compact, these resolvent operators. As you vary x and move from fiber to fiber, these resolvent operators have to vary continuously. What on earth does that mean? They're operators on different Hilbert spaces. What does it mean for them to vary continuously? Well, we discussed that if you have a continuous field of Hilbert spaces, there is attached to it an associated continuous field of sea star algebras, which, whose fibers are the compact operators on Hx. And those are the types of, that's the type of continuous variation that Kasparov settled upon, which was a great thing.
00:53:06.464 - 00:54:53.724, Speaker A: He had exactly the right idea. Oh, and just for good measure, I'd like this to be a section which vanishes at infinity. So the norm, point wise norm of these operators, which we know is a continuous function of x, according to the definition of continuous field, that function is supposed to vanish infinity. That's part of the data. And if it's the case that h breaks up as a direct sum of two subfields, an orthogonal direct sum of an h field and an h minus field, and if it's the case that the same thing is true of these domains, of these operators dx, and if it's the case that that dx maps the plus sections in the domain to the minus part of hx and vice versa, then Kasparov builds an index of this family in the K theory of X, which will be today's notation for the K theory of c zero of x, because we're only going to be talking about K theory of commutative C star algebras today. And the way he does it is not very mysterious. I'm not going to write down the details, but he converts the family of operators DX into a bounded family of breath home operators of exactly the sort that he built.
00:54:53.724 - 00:55:34.764, Speaker A: K theory outer this, if you're a c star person this, or an operator person, this is what he does is he applies the usual point wise so called bounded transform. So any family like this, with all of this stuff, yada, yada, yada, gives rise to an index class. And I'm only interested in telling you about the index of that one Dirac operator over there, up here. But as you'll see, it's going to be very important in order to express the answer and proof that the answer is correct. To talk about families like up there.
00:55:36.704 - 00:55:56.540, Speaker B: I'm unfamiliar with Shadow, I've heard of it, but I don't know, like intuitively can I kind of think that like you have for each x this d, which is, I guess somehow you're going to bound them right for each x. So in particular you get like a kernel sub x. But these assumptions will tell you that these together to give actual vector models.
00:55:56.572 - 00:56:39.884, Speaker A: Over X very close. It's very close to that. If, if you have a family like this and if the base space is compact and then the following is true, you can take that family and you can just nudge it a little bit within this type of family that we're talking about here. And after you've done the nudging, it will be the case that the individual kernel vector space is glued together to be a smooth finite dimensional vector bundle. So that vector bundle is also z two graded. The kernel breaks up into a plus part and a minus part. Then the index class is the plus part vector bundle kernel vector bundle minus the minus part kernel vector bundle you can do.
00:56:39.884 - 00:57:05.274, Speaker A: That's not how Kasparov does it. He uses the bounded transform, but that if you want to think about this concretely, that's what's going on. The index class is just the family of kernels in the plus part of D minus. The family of kernels in the minus part of D. That's a theorem, not the definition. In the special case, after you've done the nudging so that the kernels really do form a vector bundle.
00:57:05.434 - 00:57:12.466, Speaker B: And the banishing and affinity is that sort of like it's like it's somehow translated to the multiple factification effects.
00:57:12.530 - 00:57:45.604, Speaker A: Yeah, that's the usual curse that we're talking always about. K theory with compact supports, always in cohomology, you have to figure out what kind of support condition you're dealing with for cycles. Co cycles, I guess I should say. And here we're dealing with compact support. So if you like the one point compactification, and that's the reason that that's there. And you can get away with that if you want to consider some other types of K theory. But we don't get away from that.
00:57:45.604 - 00:58:52.132, Speaker A: Okay, here's an example, the notorious bot element. So it's going to be an example of one of these Kasparov cycles, and the base space is just going to be e, an even dimensional euclidean space like it always is. For the rest of this talk, what's this field of Hilbert spaces? Well, the Hilbert space is, which is supposed to exist, is supposed to be a family of Hilbert spaces, one over each point of v. And the Hilbert spaces will all be the same, just s so this bundle of Hilbert spaces is distinguished. First of all, every fiber is finite dimensional. Secondly, the fibers move continuously. Thirdly, they don't actually move at all.
00:58:52.132 - 00:59:49.174, Speaker A: It's just a constant bundle, but it's a continuous field of Hilbert spaces. What is the operator beam in this family that we're talking about up here? Well, it's just Clifford multiplication by e from s to s, unless e is equal to zero. Clifford multiplication by e is invertible because it's skewer joint and its square is minus the norm squared of e. And in fact, clearly we're in finite dimensions. These operators vary continuously with e. They're certainly a self, essentially self adjoint because we're in finite dimensions. And, and when you look at these inverses, the things up there, the resolvents, because d squared becomes really, really big as e goes to infinity.
00:59:49.174 - 01:00:24.904, Speaker A: These resolvents have to become really small as x goes to infinity. So these form a continuous, in fact, c zero section of the compact endomorphisms of this finite dimensional bundle of finite dimensional Hilbert spaces. Here, compact doesn't mean anything. It's finite dimensional. So that's an example. And so there is a class index of d, which I'll write as beta, and it depends on e, and it also depends on s. And it lies in the k theory of the topological space e.
01:00:24.904 - 01:01:29.474, Speaker A: And according to the famous theorem of bot, that K theory group is the integers, and it's freely generated by this particular class, constructed well in this particular way by Kasparov, but of course, construction directed by bot in a very similar but different way. And now here's another example. I'm going to see if I can squeeze it right on the bottom here. So, these elements are called tom elements, but you'll see they're just really the bot element. So now suppose s is a spinner bundle or a vector bundle v over m. And I'll suppose here that m is a closed manifold, a compact manifold. In order to define a k theory class in Kasparov's way, I need a continuous field of Hilbert spaces and I need a of operators.
01:01:29.474 - 01:02:02.542, Speaker A: So what's the continuous field of Hilbert spaces? Now, I want to build a class in the k theory of the total space of v, the space total space of the manifold of the vector bundle v. So I need a Hilbert space for each vector. And this I'll just define to be the spinner space associated to PI of v. PI is the projection from the vector bundle v down to the base mix. So little PI of v is some point m in the manifold. And there's a fiber of this spindle model over that point. That's what I'm talking about.
01:02:02.542 - 01:02:22.134, Speaker A: And now the rest you can guess. Dv will just be Clifford multiplication by v from s by v. S PI v. That's part of the data of one of these spinner bundles. As we had before. We don't have it anymore. And that's it.
01:02:22.134 - 01:03:11.294, Speaker A: And we get a class from Kasparov's construction. And now I'll call it m e. What do we call it? V. And then we have an s here, and it's in the K theory. And it's not completely disjoint from the previous example, because putting these two things together, the class beta of E. You know what? Let me call this class here, this bundle sv, just to emphasize a point that it's a bundle related to v. And then this should be s and v, of course, and this should be Sv.
01:03:11.294 - 01:03:56.504, Speaker A: And that should be sv, just to make it look bigger. S is a bundle, so it deserves two letters. If it has just one letter, it's just a vector space like this. Okay. Anyway, beta, this is what you get by including zero as a sub manipulative e and using the bundle se. Okay, so there you go. And now I can tell you what the index theorem says precisely, not with that eraser.
01:03:56.504 - 01:04:51.980, Speaker A: And then we can attempt to prove it. The major problem is to figure out what the heck the index theorem says. Usually when you see the index theorem, there's some Todd class in the. What the heck is the Todd class? You know, how could you possibly prove the index theorem when you don't know what the Todd class is? And that's one of the fundamental mysteries of the universe. You know, what is the Todd class? And what is it doing in the index formula? But here we can get around that. Once we have a statement which is made out of planar ingredients, then it becomes almost evident how you should prove the index theorem. So let's just remember what situation we're in.
01:04:51.980 - 01:05:59.534, Speaker A: We're assuming that m, a manifold, is sitting inside of either finite dimensional, even dimensional vector space. We're assuming that e is equipped with a spin of vector space s, and we're assuming that s the bundle associated to e. This two letter thing, when you restrict it to m as a Clifford bundle, as a spinner bundle for the trivial bundle with fiber e restricted to m, this thing is s. T. M decomposes as a tensor product like that, except there's a little subtlety, and the decomposition I really want, just for technical reasons, involves not this bundle, but the complex conjugate bundle. If you have a spinner bundle s and you look at the complex conjugate bundle, it just means it's the same bundle, except you change the definition of scalar multiplication. So the multiplication of lambda times a vector in the new sense is the multiplication of lambda bar times the vector in the old sense.
01:05:59.534 - 01:06:42.108, Speaker A: Well, when you do that, whatever Clifford structure, whatever spinner structure the old bundle had, it's still a spinner structure on this thing. So s bar tm is as much a spinner bundle as s of tm. And what I want to imagine is that we have some spinner bundle s of tm for the manifold m that we constructed somehow, for example, using differential forms like we discussed earlier. And then we have a decomposition where there's a bar like this. And if you don't like having a bar here, you could put a bar here and a bar here. Maybe I'll just do it. Maybe you're thinking of this spinner bundle s and tm as primary.
01:06:42.108 - 01:07:27.552, Speaker A: So you don't want to monkey with it, fine, you just put bars everywhere else. So we want to choose s of NM in this sort of slightly odd way in which some complex conjugate appears. It's a very, very minor thing, but that's the way the math works out. You need this thing. Okay? And now we can state the index theorem. We want to know what is the index of the Dirac operator that you build out of s of tm? And what I'm going to do is tell you what it is. Of course it's an integer, but I'm not going to calculate it as an integer.
01:07:27.552 - 01:08:24.000, Speaker A: I'm going to tell you what it is. When you do a little bit of a construction and put yourself in the K theory of E. The K theory of E is a free abilian group with one generator. So basically the integers. But we're going to slightly change things, and we'll change things by multiplying by this, but element this guy up here, and what's going to appear, there's a little space you have to leave. What's going to appear is the Tom element, which I think I also just called beta for m normal bundle of n. In this particular choice of spinner bundle, the one which solves this equation here, the thing I just wrote down doesn't lie in the K theory of E.
01:08:24.000 - 01:10:20.324, Speaker A: It lies in the k theory of the total space of the normal bundle of m. And the normal bundle of M is not e. And so you have to do one extra thing, fixing up, which is to put a iota here, which I'll tell you what it is now. So, Iota is a map in case theory, and it's the map which is associated to the space which is the deformation cone to the normal cone for the embedding of m into e. Once you have this space, we obtain from this, which is, I suppose, an example, the groupoid with no morphisms. We obtained a whole bunch of c star algebras, namely the c zero functions on the fiber of this space over t, like that. And as we discussed earlier, this is an example of a continuous field of sister algebras.
01:10:20.324 - 01:11:42.528, Speaker A: What the deformation to the normal cone looks like is it's a big manifold, and it has a submersion down to the real line. And the fibers over the points of the real line are all copies of the e, the big space, except for the exceptional fiber at zero, which is a copy of the normal bundle. So this is a continuous field of siesta algebras with zero fiber, just c zero of Nm, and all of the other fibers plus c zero of either. It's one of these continuous fields of siesta algebras with only one exceptional fiber, and that produces, as we discussed, a map from the special fiber to the genetic or the other fiber. This is something which lives purely in the world of topological spaces, commutative c star algebras. You can ask a topologist what this map is, and they'll tell you. In fact, I already did tell you the answer.
01:11:42.528 - 01:12:35.886, Speaker A: The answer is, you take a tubular neighborhood embedding of Nm as an open subset of e, and that induces a map from the k theory of Nm to the k theory of e. And that's what I'm talking about. If you choose a tubular neighborhood embedding, you can concretely realize iota in a way that a topologist would be happy with. And what you see in this formula, therefore, is everything that you see in this formula, a topologist would be happy with, except for this index, because this is the bot element. Okay, we described it with the help of Kasparov, but it's the usual bot element and the index, and it's made out of finite dimensional vector bundles. Although we have the possibility in Kasparov theory to use infinite dimensional Hilbert spaces and so on, we didn't when we built the direct, the direct, the bot element. Same thing for the Tom element.
01:12:35.886 - 01:13:06.946, Speaker A: Everything in this formula is intelligible by a topologist. They can tell you, so to speak, what, in quotation marks, everything is in this formula except for the index term. And therefore you can solve by the index form, divide by the Bart element, and you'll get a concrete formula. So it's. It's a good formula, is what I'm saying. As long as you have the assistance of a topologist, you can actually get a number out of this and a big computer. You can actually get a number out of this? Yes, that's literally true.
01:13:06.946 - 01:14:05.704, Speaker A: If you had a big computer and a complete description of the embedding of m into e in local coordinates and with all of the functions, something you could tell mathematically, the topologists could feed the recipe for what's going on here into the computer. These are, in effect, integers, or this is, in effect, an integer. It's an element of this abelian group and the topologist could calculate what it is. All right, now, how are we going to prove this thing? I'm trying to say that the virtue of this formulation of the index theorem is that there's nothing which is mysterious to a topologist in it. There's maybe plenty that's mysterious to you in it, because, you know, this first time you saw the Tom element. So it looks. Obviously it looks mysterious, but it won't look mysterious to a topologist and they'll be able to tell you exactly what's going on here.
01:14:05.704 - 01:14:46.264, Speaker A: Yes, exactly. Yeah. What actually. Yeah, I guess that's. The objects which have Tod classes are complex vector bundles. And we slightly revered from the world of complex vector models. But if it happened to be the case that this SNM was built using the exterior algebra construction, then this thing would be completely expressible in terms of the Todd class, of the.
01:14:46.264 - 01:16:07.382, Speaker A: Of the complex vector bundle Nm. Otherwise just a little bit more complicated. And up to this point in the lecture, nothing in this lecture resembles anything in any of the previous lectures. These two. There's a division line between what happened before 1030 in the morning today and what happened after. But now there'll be some connection because this particular map that I'm about to write down, we looked at two days ago, I switched the order of r and e, and now I decided to switch it. Yeah, I switched it.
01:16:07.382 - 01:17:06.700, Speaker A: Anyway, what is this map? M e t. I'm imagining that m is actually sitting in, inside of e. So I'm not going to mention the map h. And this map is going to be exactly, almost exactly the map we had before, minus some of the minus signs. If t is zero, then what you do is you take this vector and you think of this vector as a normal vector over the point m. That is to say, you look at the orthogonal complement, the complementary orthogonal part of the vector e in the decomposition of e into the tangent space at the point m directs from the normal space. That's what this square bracket thing means.
01:17:06.700 - 01:17:36.378, Speaker A: It's a vector in the normal bundle which sits inside of here. This is a point in the normal bundle, comma zero. It's a point in this space. And what you do here is you build a certain element of E. And what it is, is something that we've more or less seen before. This particular formula you could use, you could throw in some minus signs if you want to, but the WAY things are working out here, the minus signs aren't necessary at this point. They do appear somewhere else.
01:17:36.378 - 01:18:34.134, Speaker A: So up to some small issue with minus signs. This is exactly the formula that we saw before. And this is a submersion. LEt's look at what the fibers over this mapper over a point et zero point in this normal space, which lies just in one of the typical fibers. Well, it consists of all triples, m f t, such that f is equal to, excuse me, e is equal to tf plus m. And now you can solve, what does that mean? It means that this guy here is t inverse e minus m. M e.
01:18:34.134 - 01:19:17.278, Speaker A: I think I wrote it down. Hope I wrote it down. E minus m. So t and e are fixed if we're talking about this particular fiber. But m is allowed to vary freely throughout the manifold m. And this thing, of course, this fiber is isomorphic in an obvious way to just m, namely, project onto the m component. And these are the only vectors, excuse me, only points in this threefold product, which under this map gets sent to point e, comma m.
01:19:17.278 - 01:20:05.074, Speaker A: I should have used a different letter, too many e's, but you get the idea. And what happens over zero? Just this. So now we're dealing with a normal vector at the point m, zero, whatever the normal vector is over that point. The only things which are allowed are, I missed off the factor here should be something in this space, M. And I also missed off the m is there. Good. So the fiber here is isomorphic in the obvious way for the tangent space of M.
01:20:05.074 - 01:22:13.378, Speaker A: So we have a smooth family of manifolds, and this smooth family of manifolds, mostly as t and e vary, it's mostly a copy of m, except the manifolds degenerate in some smooth way. The tangent spaces at the, at the point where t is equal to zero. And now to prove the index theorem, here's what's involved? What's involved is you need to construct a class, I don't know, alpha, where in the k theory of this space, not this entire space, but let's just look at this. The part of this space which lies over t's between zero and one. If I take the continuous functions on this space, c zero functions on this space. These are exactly the, this is exactly the sister algebra of the sections of the continuous field you get by restricting this continuous field to the interval between zero and one. I'm going to construct a class, or not as the case may be, but all I need for this class is that when I restrict to t equals zero.
01:22:13.378 - 01:23:08.894, Speaker A: I'll get this, Tom class, just trying to remember my notation. So here's a certain space that I'm writing down. It's just a locally compact space and inside of it there's the closed subspace, which is the fiber over zero. So there's a map like this NME fiber over zero, but the fiber over zero is just the normal bundle where it's somewhere written here. Yeah, right there. And the same thing, some similar thing here. So now I just want to get index of d times.
01:23:08.894 - 01:24:11.820, Speaker A: But class, now instead of restricting t equals zero, I'll just restrict to t equals one. Sorry for squeezing this in microscopically there. That will do the job, because the way this map iota works is the following. You take a k theory class in the fiber at zero, you realize it as the image of a K theory class. Upstairs there's a unique fellow which at the level of K theory alpha, which maps to any given element in the K theory of Nm. And once you have that alpha, you just restrict it to t equals one. And that gives you the image of iota.
01:24:11.820 - 01:24:55.324, Speaker A: That's how iota is defined. Iota is defined by saying that for every element I don't know x of the k theory of nm, I can find some element y such that the restriction of y is x. And then I define iota by iota of x as the restriction to one of Y. And that's exactly what I've done here. So if you can find this alpha, you're done. I mean, you know, this thing exists, you just want to construct it in a concrete way. And how is it going to be constructed? Well, we have only one way of building elements in the k theory which can possibly be of relevance to this discussion, and that's the way that Kasparov is describing here.
01:24:55.324 - 01:25:27.332, Speaker A: So what I need to build, which I'll have to either squeeze into the notes or the beginning of the next lecture is a family of essentially self adjoint, compact resolvent operators in exactly this Kasparov sense up here, which restricts to a family at t is equal to zero, which I can identify with this fellow. What is this? This is the index of some other family. So as long as I can show that my family restricts to. It's gone. Now the bot. Oh no, here it is. The Tom family.
01:25:27.332 - 01:26:06.318, Speaker A: I'll be in business or restricts to something which is manifestly homotopic to the Tom family. I'll be in business. And then the same thing when I restricted t equals one. It's really concrete. And if you spend a few months internalizing this subject, you won't need my assistance to figure out what the alpha is, because there's really only one possibility for how you could possibly build it. If you're going to build it this way. There's only one family of operators, D sub x, you could possibly consider, parameterized by the points of the deformation to the normal cone, that you could possibly consider.
01:26:06.318 - 01:26:51.490, Speaker A: Ah, given that I've told you that there is this interesting submersion, clearly we should build a family of operators on the fibers of this submersion. That's what should be involved here. And with that extra clue there's, there's a unique solution and we'll have a race and you have until Tuesday to construct it yourself. Otherwise I'll just mention the answer next time. The most important part of the argument is to, like I said before, understand the statement of the index theorem in a way which says that apple equals Apple Times index and which is also an apple. Apple equals apple times apple. You want to understand everything is kind of the same as being given by kind of the same construction.
01:26:51.490 - 01:27:02.390, Speaker A: And that's what we're on the verge of doing. We better stop. But I will answer your question. Oh no, you just. No question. Okay, just. Okay.
01:27:02.390 - 01:27:19.614, Speaker A: Just. I see. Just to hear better. Okay, great. I already have big ears. I don't need to do that. Any other questions? Any questions from the, you know, the ESA? Good.
