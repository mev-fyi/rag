00:00:01.160 - 00:01:24.510, Speaker A: Good morning and thank you for your interest in this talk. This is about the machine learning of turbulent and geophysical flows and how we can improve their performance using scale separation. It is a joint work with some colleagues that come from the climate science background and other colleagues that work in artificial intelligence at the CAE secler. So, machine learning in science is usually used when we are dealing with a complex system that features multiple special and time scales, and for which we have a large availability of training data. But the most important feature of the system we usually analyze with machine learning techniques is that they miss an equation of state, such as the examples that I'm showing in these slides, for example, neuroscience, genomics, traffic and robotics. We don't know how to deal with this system with equations, or there are very simple models for this system, and then we use machine learning techniques to infer patterns and the way these patterns dynamically interact. And this provides some information on the complex dynamics of this system.
00:01:24.510 - 00:02:36.280, Speaker A: When we are dealing with geophysical and turbulent flow, however, we have the equation of states, the Navier Stokes equations, and therefore we could in principle represent our system with models and equations. And this is what is done with climate models. For example, they try to reproduce all the range of phenomena that you can see in the inset here, that show phenomena that goes from the range of few meters and few minutes, such as dust devil and tornadoes. Two phenomena that occur on very long time scales here to thousands of years. And they are on the scale of the planet. So the most important things to understand here is that even with the computing power that we have at our disposal now, we cannot represent all this range of spatial and temporal scales. And so we have to cut the resolution of climate model.
00:02:36.280 - 00:04:12.524, Speaker A: Now the cut is around here, ten to 50 km, so that we cannot represent several phenomena that are on smaller scales and also for phenomena that are on very long timescales. We struggle because we need to integrate our climate models for a very long time. So here machine learning can help us because we can focus on some range of these large spatial temporal spectrum and learn the behavior of some specific phenomena occurring at target, special and timescale. So in this talk we focus on the problem of the weather forecast as represented by the sea level pressure fields. And here you can see an evolution in 6 hours time frames for the global sea level pressure fields extracted from the NSEP reanalysis data set. And in this evolution you can see how cyclones and anticyclones progress towards the east at mid latitude. For example, you can see the presence of cyclones between the British Isle and scandinavian peninsula and the presence of anticyclones over the eastern United States.
00:04:12.524 - 00:05:48.218, Speaker A: And the future of these dynamics is that it is somehow well defined because these cyclones and anticyclones have a specific site that is order of thousands of kilometers and a specific time of evolution, that is order of days. So, if we focus on these problems, we can ask ourselves whether the machine learning techniques can learn this kind of dynamics just by feeding them with the data of the sea level pressure field. Which means that we don't need to resolve the full set of the Navier Stokes equation, the three dimensional atmospheric grid, but we can just use the two dimensional sea level pressure fields and learn their behavior using some machine learning techniques and then also make some forecasts of these fields. Okay, so this will be the focus of our talk. And to obtain such forecasts, we have to decide which techniques to choose among all the possible algorithms of machine learning. And here we will use reservoir computing via ecostate network as suggested by Patak et al. In the physical Review Letter paper of 2018.
00:05:48.218 - 00:07:03.366, Speaker A: And why we use this because patak and collaborators have shown that ecostate network for chaotic system can provide forecasts beyond the lapun of time. Here I am showing our reproduction of the results obtained by Pata Ketal in the Lawrence 1963 system. And in particular I'm showing the behavior of the x variable evolving from equations on a blue line, and the machine learning forecasts on the orange line. And this forecast, after learning the behavior of the system for a while, be initialized at time zero. And as you can see, they stick together up to five liapon of times and then they separate. So which means that the machine learning via reservoir computing and ego state networks can afford very good performance in this simple chaotic system system. So now we will try to describe this technique and then apply to the grammat data I shown you in the previous slide.
00:07:03.366 - 00:07:55.350, Speaker A: So, the ego state network take as input the variables of the system at the specific time t. So if we have the Lorentz 1963 equations, we have three variable x, y z. So x of t. The input serial data is l dimensional vectors. And remember that variables need to be standardized by removing their mean and dividing by standard deviation for example. Then these variables are fed to an input layer. And here in the input layer we have l times and random numbers, random weights which we choose uniform in the interval -0.5
00:07:55.350 - 00:09:11.664, Speaker A: plus 0.5. And why we take such an interval? The reason is that and such uniform variable? The reason is that we don't want to add additional parameters to our ego state network. But we want the most simple and the simplest possible network to perform parametric scan of our parameter space and understand something about how such ego state network represents the evolution of some geophysical flow. We are aware that we could use sparse matrix. It should improve performance. But again, I want to stress that here the goal is not to obtain the best possible forecast of the sea level pressure fields, but just to show, to point out some way to improve the performance using the physics of the underlying systems. So after the input serial data are weighted by the random input layer, we are going to the reservoir, that is an n times n matrix.
00:09:11.664 - 00:09:58.336, Speaker A: And again, each line consists of random weights uniform in -0.5 plus 0.5. And the activation function is an hyperbolic tangent. Once the reservoir aft, we have the output layer, that is a matrix n times l. And here this is starting from an output layer with random weights again. -0.50.5 but then this is what is optimized during the training via a ridge regression, so that the output matches xt plus dt.
00:09:58.336 - 00:11:26.710, Speaker A: So the subsequent instance of our dynamical system, and here is the magic. And we can forecast using this ego state network in a recurrent way, the behavior of our chaotic system. So here is the full equations. It is very simple and as you can understand, of course, it just transforms the output xt in the output xt plus dt in using this hyperbolic tangent and activation function. Okay, so if we just try this technique without any, any optimization or thinking to what is the physics of the system on our data, and we train the network using 200 neurons and learning times of ten years, and we perform a forecast length of ten years. What we can see is that the target, after a very long time, is still an ensemble of cyclones and anticyclones progressing towards the east. But the forecast seems to be stacked in some sort of fixed point of our system, so it doesn't look good at all.
00:11:26.710 - 00:12:37.446, Speaker A: Similar results have been found by other colleagues. And so this means that with respect to the Lorentz 1963 system, the forecast of the dynamics of these spatiotemporal fields, the, is the sea level pressure. It's not straightforward. And what we do is to take one step back to assess what is wrong in this forecast and whether we can obtain a better forecast of our system. So, to do this, we use again some test systems from low dimensional dynamical system. And we go back again to the Lorentz 1963 equations that, as you know, are representing a model of atmospheric convection. And we also use the Pomona bil intermittent map that is very interesting, because it can reproduce some intermittent behavior of our that is also encountered in geophysical flows.
00:12:37.446 - 00:13:57.298, Speaker A: For example, in our example of the sea level pressure fields, sometimes cyclones and anticyclones do not evolve anymore towards the east, but they are stuck in one position, causing the phenomenon of atmospheric blocking that could be understood here in the bohemian intermittent map. By looking at these intervals of times where the dynamics doesn't really evolve, but it still near an unstable fixed point. So this also happens in the atmospheric dynamics phenomenon we are representing. The analogy is far from being trivial, as we have also studied in our previous papers. For example, you can look at Ferrand et al. 2015 in climate dynamics, where we make a clear analogy between the behavior of such pomon manville intermittent maps and the dynamics of blocking. So these two systems since, are the way to study some issues that can occur in geophysical flows, especially Lawrence.
00:13:57.298 - 00:15:37.414, Speaker A: The first things that we want to explore in the Lawrence, 1963 is the learning, the dependence on the learning times. Okay, so if we look at the previous plot that I shown before in times in linear scale for the x variable, we can see that it is true that the equations and the machine learnings yield similar performance is up to liability times five. But when we go to the logarithmic scale, as shown in the bottom plot, you can see that the equations for an ensemble of simulation performed much better than the machine learning, and that we have an initial jump that we do not see in a linear scale. And it is due to the learning time and to the fact that in this specific simulation we didn't learn long enough the dynamics of the system. Okay, so, to study this dependent learning time, we can show how the performance improve when we train the model for just 1000 iterations. And this is the green line for 10,000 iterations, this is the blue line, or for 100,000 iteration, and this is the red line. When we increase the learning time, then we can get two performance that are comparable to those obtained using the equations.
00:15:37.414 - 00:16:43.690, Speaker A: So this is very important. First message is that if we don't learn the dynamics of the system long enough, so we don't observe all the possible behavior, all the possible patterns, chaotic patterns, patterns of the system, we will have a forecast that is not as good as the equations. The second danger is the presence of small scales, dynamics and intermittencies. To understand this, we will add additive noise to the Lorentz 90 63 equations and do Pumon and ville intermittent dynamics. And the equation for the addition of noise is represented in the equation that you can see on the slide. And as you can see, the noise is again represented by xi of t. So, a random variable uniform in -0.5
00:16:43.690 - 00:18:04.202, Speaker A: plus 0.5. What I'm showing in the figure is the percentage of failure in reproducing the underlying attractor for Lorenz on the left hand side and pomomanbill on the right hand side for Lorenz. You can see that we are varying both the network sides on the y axis and intensity of the noise on the x axis. And what we can read on this plot is that for low intensity of the noise and for good resolutions, we have almost always a good representation of the Lorentz 9063 attractor. So, the problem starts when the noise is very large and network size is small. And we have that if the dynamics is noisy, even just ten to the minus two is enough, the failure rate of our machine learning reservoir computing for the Lorentz attractor starts to become very high. For the Bowman wheel, the noise is sum up to the presence of intermittency.
00:18:04.202 - 00:19:02.562, Speaker A: And as you can see, the eco state network struggle very much when we have intermittent system and it cannot reproduce the behavior the Pomona wheel maps, even for very small noise sites. So in the limits of the logarithm of epsilon order to ten to the minus seven. And this is due definitely to the presence of intermittency. So we have that indeed noise. So the presence of small scale dynamics and intermittency can represent danger for the use of ecosate network in the complex system. So now the possible solution is to use a scale separation for our dynamics, and therefore we can filter the noise filter. Here we call the noise.
00:19:02.562 - 00:20:33.388, Speaker A: But we know that for the atmospheric data, what we call the noise in the example of the sea level pressure fields are indeed phenomena occurring to smaller scales, such as for example, those occurring by convection or interaction with the geography. So here, to filter the small scales, we use the simple possible way, that is the moving average filter with window size smaller, much smaller than the Liabunov time. Then we apply the ecosystem network to the filter system only and add back the residual to the forecast to see, to preserve somehow some dynamics of the small scale. When we do this for the 1963 and the pomon bill map with the right window sights, that is what I've shown before. It must be smaller than the liability of time, but large enough to filter some of these small scale dynamics. We know, we see that the moving average improve the performance for the Lorentz and the bomovan wheel. For the Lorentz, we improved the performance even with small network size and for the Bowman wheel.
00:20:33.388 - 00:22:00.074, Speaker A: What is interesting is that we can get some sort of intermittent behavior for very small noise. In fact, if we look at one specific trajectory for the Pomoman wheeler system, we can see here that the original trajectory is represented in blue and the obtained trajectory with the machine learning is represented in red. There is no correspondence perfect match between the two. But one thing that we can observe just in a qualitative way in the moment, Bill map, is that we are able to obtain some intermittency just by using moving average filters of window size equal three in the ecosystem network algorithm. So this is of course not good, but is not even that bad, because this spiky behavior of intermittency is what is observed, for example, when we have turbulent or atmospheric variables. So there is some hope that filtering techniques can help even with intermittent. And then we now test on the sea level pressure fields.
00:22:00.074 - 00:23:22.730, Speaker A: And we have first here a visual inspection, very qualitative inspection, first for short time weather forecast. So here I've learned the dynamics of the sea level pressure fields for ten years using time frames, global sea level pressure maps, each 6 hours. Then I've used 200 neurons, and then I perform a forecast length of ten years, starting immediately after the ten years of learning. Okay? And here I'm showing just the first hours after the learning time. Okay? So when I start the forecast, and as you can see, there is no much difference between the no filter and the moving average, although it seems already the moving average with the 12 hours window. There is some more active dynamics with respect to the no filter dynamics. But the real improvement is that when we look at forecast length, but forecast time, long term behavior, we see again that the non filter simulation is stuck, so it doesn't evolve anymore.
00:23:22.730 - 00:24:27.320, Speaker A: And the moving average simulation instead is still evolving and producing some dynamics that of course doesn't look like the target, because we are well beyond the liabun of time, but is evolving in a similar way at those obtaining in the target. So now how to make more quantitative this statement? Use other indicators. For example, we can look, we can look at the spacetime statistics for the NSEP data. For example, the distribution of the target data is shown in blue here, and no filter. With machine learning, forecast is shown in red, a moving average of 12 hours in yellow. And as you can see, the yellow curve matches better. The blue curve, although the tail of the distributions are still not that good, which means that the representation of extreme cyclones and anti cyclones is biased.
00:24:27.320 - 00:25:04.674, Speaker A: And other things that we can look at is the wavelet spectra. So in the wavelet spectra we have that the longest time scales, it is here, is in the bottom down of these plots. And 0.25 corresponds to the scale of one day. So 0.06 corresponds to the scale of weeks, and so on and so forth. And we can see that no filter has no signal at all on the scale of up to months.
00:25:04.674 - 00:26:34.860, Speaker A: So the dynamic is completely stacked on this scale. But the moving average, it has some signal also on smaller time scales, although this signal is small, but is already a first improvement. So now we can also have again a more quantitative assessment if we measure the distance, the key square distance with respect to the distribution of the real data as a function of the Windows sites and for different network sites. We can see on the a plot that without moving average filter, we have a very large distance for small network sites. And then there is also divergence for large network sites and for for window size of 1224, 48 and 72 hours, we obtain smaller distance from the data, but divergence already with small network sites and this distance from the NCP data must be also complete with the predictability horizon. It means how long we can use the forecast that we perform theory. This is measured as a threshold on the root mean square error between the data and the machine learning forecast.
00:26:34.860 - 00:28:18.294, Speaker A: And as you can see, the best performance is obtained for Windows sites of 12 hours and network sites of 200 neurons. When I say the best performance here, I'm using both the indicator of the distance from the NSEP data in the a plot, the predictability horizon. So if we combine the two, we obtain that these 200 neurons and these moving average filter is the best options that we can have to represent easily with the ecostate network our data. And this is why a posteriori I shown you at the very beginning of the of the talk these combination of parameters. Okay, you can also see that when we have a very long window sites, then we are approaching the liability of time, that for the cyclones and anticyclones should be order of four or five days. And so we reduce the predictability horizon. So, to conclude this talk, we can say that it is not straightforward to apply machine learning techniques to geophysical flows because turbulence shows small scale dynamics and intermittency worsen the performance and partial predictability can be recovered by using filtering techniques from small scale dynamics, for example using moving average principal component or wavelets.
00:28:18.294 - 00:29:25.122, Speaker A: And of course, you can see that when we move to this domain of the machine learning via ecosystem networks, we are just using stochastic dynamical system. So it will be very beneficial to interact with the mathematicians in this community working of stochastic dynamical system to improve also the theoretical understanding of this system. And as a reference, of course, I cite this patak et al. First paper in physical review letters, but now they are also working on weather forecasts and they published some new interesting papers, so you should check them out. I also cite this paper by Sebastian Scherr, Gabriele Mesori. That is a very nice paper on how to do climate and weather forecasting. And what are the problems using these machine learning, reservoir computing and other machine learning techniques.
00:29:25.122 - 00:29:45.014, Speaker A: And then there is our paper that is in review, in physical review fluid now, and you can find it on archive and on Hal. Thank you very much for your attention and for following this webinar.
