00:00:00.200 - 00:00:57.514, Speaker A: Tony for inviting me to give this talk and thank you all for being here. So I'm going to talk about the rank function of the rigidity matriid, and I'll go through some more generalities about matroids in order to do this. And sort of the kind of like guiding results for this talk come from a paper of Lobosch and Yemeni in 1981. So I would still like recommend actually going through the paper because I'm not really going to get through a lot of the proofs or really any of the proofs here. I'm going to send a link to the paper in the chat. I guess the point in this talk is to give you the background around that paper in a very general metroid theoretic sense. So to begin, I just want to start with rank functions of matriids.
00:00:57.514 - 00:03:13.354, Speaker A: So let m be a matriid and define the function r. That's going to go from the subsets of your ground set e to the integers. And we're going to define this by the rank of some set s is equal to the maximum over independent sets that are contained in s, and we'll take its cardinality. So yeah, and I don't know what. Yes, I apologize in advance if my notation is maybe not consistent with what Tony's been doing, but I'll use this notation mei for a matroid where m is the matroid, e is the ground set, and I is a set of independent sets. But anyway, the rank function, it just takes a subset of your ground set and spits out the size of the maximum or the largest independent set within it. So for example, if m is the matroid of linear independence of a matrix with column set e, then the rank of some subset of columns s is the rank of the column submatrix of a on column set s.
00:03:13.354 - 00:03:23.502, Speaker A: Oh, by the way, is this recording? Did, has record been hit?
00:03:23.678 - 00:03:24.750, Speaker B: Yes. Okay.
00:03:24.782 - 00:04:29.226, Speaker A: Okay. Just making sure. All right, cool. And then another example that's gonna be relevant for here comes from graphic natroids. So let's say recall that given a graph g with vertex set v and edge set e, the graphic matriid m of g has ground set. The edges of g and independent sets are cycle free sets of edges. The rank function is given as follows.
00:04:29.226 - 00:05:47.060, Speaker A: The rank of a subset s is equal to the number the number of vertices in the entire graph minus the number of connected components of the graph v with edge set s. Okay, so just since this is going to be important later, I'll give an example of this example. If g is, say the complete graph on four vertices. And we'll say s is this. Then the rank of s is number of vertices minus the number of connected components, which is one, which is three. And if, say, s prime is just two edges, then rank of s prime is four minus the number of connected components, which is two, which is too.
00:05:47.212 - 00:05:47.984, Speaker C: Okay.
00:05:50.644 - 00:06:09.264, Speaker A: All right, so, yeah, and by the way, feel free to stop me at any point with questions. So anyway, it turns out there's a very nice characterization of which functions can be rank functions of metroids. So here's a proposition.
00:06:10.364 - 00:06:11.236, Speaker D: Hey, Daniel?
00:06:11.340 - 00:06:12.008, Speaker A: Yes?
00:06:12.156 - 00:06:16.576, Speaker D: Um, can I ask about this, the previous example?
00:06:16.640 - 00:06:17.360, Speaker A: Yeah, please.
00:06:17.472 - 00:06:26.840, Speaker D: So each connected component will give a vector in the kernel of the incidence matrix of the graph, right?
00:06:26.952 - 00:06:27.644, Speaker B: Yes.
00:06:28.064 - 00:06:32.484, Speaker A: Or, um, not quite so over.
00:06:34.904 - 00:06:35.208, Speaker B: Sort.
00:06:35.216 - 00:06:41.544, Speaker D: Of the delta vector on that connected component will be a linear independent vector in the kernel of the incidence matrix.
00:06:42.044 - 00:06:58.564, Speaker A: Yeah, so, well, not quite. Okay, so if your incidence matrix is over the field with two elements, then yes, but if not, you have to put an orientation on the edges, any orientation, and then take the directed incidence matrix and then that will be true.
00:06:58.684 - 00:07:05.584, Speaker D: Right, okay, that's the one I was thinking of. Then can you help me? Is this easy to see? Then immediately.
00:07:07.644 - 00:07:10.264, Speaker A: That the rank function. So let's see.
00:07:14.084 - 00:07:19.464, Speaker D: Because the cycles are the vectors in the kernel of the transpose.
00:07:30.084 - 00:07:45.554, Speaker A: Okay, the way to think, think about this, I would say is each connected component has a spanning tree, okay? And if you just take a tree and look at that incidence matrix, there's not going to be any linear dependencies.
00:07:45.974 - 00:07:46.754, Speaker D: Yeah.
00:07:47.414 - 00:07:50.594, Speaker A: And as soon as you have a cycle, that gives you a linear dependence.
00:07:52.734 - 00:07:57.908, Speaker D: So, okay, that was, that was better. That's a better way. Okay, thank you.
00:07:58.006 - 00:08:19.768, Speaker A: Yeah, no problem. Um, any other questions? Okay, so rank functions of matriids can be characterized. Um, and in fact, you can define a matriid in terms of its rank function instead of in terms of its independent sets. And, and here's how. So, um.
00:08:19.856 - 00:08:39.104, Speaker D: Oops, okay. Hey, Daniel, sorry, can you finish? I think I cut you off because the rank function is what I wanted to see if you can see that easily. If we scroll up a little bit. The rank function is number of vertices minus connected components and can you finish explaining?
00:08:39.764 - 00:09:15.324, Speaker A: Right, right. Okay, so, so each connected component of the graph is going to contribute. Okay. Okay, so, so, so imagine your graph is just a forest, right? Now the forests are independent sets. And in particular, each connected component of the forest is an independent set. So the rank of each tree in a forest is just the number of edges and the number of edges is just the number of vertices in that connected component minus one, because it's a tree.
00:09:16.304 - 00:09:20.388, Speaker D: Oh, that's okay. That's good.
00:09:20.556 - 00:09:31.580, Speaker A: Yeah. So now do this over all connected components. What you're doing is you're adding up the number of vertices in each connected component, which could be an isolated vertex, and then subtracting one for each connected component.
00:09:31.732 - 00:09:33.132, Speaker D: Okay, thank you.
00:09:33.308 - 00:09:44.462, Speaker A: Yeah, and that's it for forests. And then for grass that aren't forests, we'll just restrict to a spanning forest, and that's the largest independent subset that you can make.
00:09:44.628 - 00:09:46.482, Speaker D: Okay, perfect, thank you.
00:09:46.578 - 00:10:55.044, Speaker A: Yeah, no problem. All right, so here's what a rank function of a matrix is. So let e be a set. Then a function r from the subsets of e to the integers is the rank function of a matroid if and only if it satisfies the following three axioms. R1, which is just that, f. Oh, sorry. R of s is always greater than or equal to zero for all subsets r two, which is that the rank of some set is less than or equal to the rank of some set union, a new element which is less than or equal to the rank of s plus one for all subsets s of e and elements x and e.
00:10:55.044 - 00:11:35.434, Speaker A: And then finally, so with any sort of axiomization of matroids, you usually have a handful of kind of like easy axioms. And then one little more subtle one. This is the more subtle one. And it says that r is submodular, which means the following. It means that the rank of s, whoops. Union t plus the rank of s intersect t is less than or equal to the rank of s plus the rank of t. And of course, this is for all subsets s and t of b.
00:11:35.434 - 00:12:56.764, Speaker A: Okay, and now just two quick notes. If r is a set function, is the rank function of a matrix m, then independent sets are sets such that their cardinality is equal to their rank. Yeah, that's like the first note. And then the second note is that rank functions are monotone. In other words, the rank of a is less than or equal to the rank of b when, whenever a is a subset of b.
00:13:01.024 - 00:13:01.944, Speaker D: Hey, Daniel?
00:13:02.064 - 00:13:02.736, Speaker A: Yes?
00:13:02.880 - 00:13:09.404, Speaker D: Is there sort of like an illustrative example for the submodular condition?
00:13:19.084 - 00:14:06.534, Speaker A: So define your matroid, your rank function to just be the cardinality of a set. So then you get an equality here in this sub modular inequality, right? The cardinality of s and t. Yeah. So s union t plus the cardinality of the intersection is equal to the cardinality of s plus the cardinality of t this is just inclusion exclusion, right? Yep. So now the rank function is telling you that. Well, when you take. So, yeah, I'm gonna, I'm gonna, I'm gonna phrase that just maybe it's a little bit more illustrative to see this as follows.
00:14:06.534 - 00:14:47.724, Speaker A: So I'm not, I'm not exactly answering your question. I'm just sort of. Well, okay, this is helpful already. Okay, good, good. So what the submodular function is saying is that, well, you know, by taking the union of two things, you're not going to increase the rank any more than you would just by, like, adding things in. Like, in other words, like, you know, the whole is not more than the sum of the parts. That's kind of the, that's sort of how I think about it intuitively.
00:14:49.024 - 00:14:55.000, Speaker D: But there can be surprising inequality where it drops.
00:14:55.112 - 00:15:59.042, Speaker A: Yeah, you would have redundancy, right. So for example, now let's say you have a matrix. Let's do a really dumb example. This one, two, three. So the rank of the set just consisting of one is equal to one, and the rank of the set two and three is equal to two. And now the rank of the union of these two is strictly less than the rank of one plus the rank of two and three plus the rank of the empty set, which is zero. And basically what this means on an intuitive level, or like in this example, is that you're not going to increase the rank or the dimension of the linear space spanned by these vectors by throwing in this redundant thing.
00:15:59.042 - 00:16:06.434, Speaker A: So the sub modular inequality basically says that as you add more stuff, you can get redundancy, but you can't get synergy.
00:16:08.094 - 00:16:13.486, Speaker D: This is great. Okay, now this third one makes complete sense. Very natural. Thank you.
00:16:13.590 - 00:17:12.314, Speaker A: Great, great, great. I'm glad to hear that. Yeah. Any other questions, by the way? All right, so now what I want to do is I want to weaken these axioms. So suppose f is a different set function, satisfying these three axioms. So I'm going to call the first one r prime of one, which is that f of s is greater than or equal to zero for all non empty s, r prime two is going to be that f is monotone. So in other words, f of a is less than or equal to f of b.
00:17:12.314 - 00:17:22.304, Speaker A: If a is a subset of b, and then r three, no prime there. This is just f is submodular.
00:17:30.084 - 00:17:32.996, Speaker D: How did these change? What were the unprimed ones?
00:17:33.180 - 00:18:05.000, Speaker A: Yeah, so the unprimed said that it's non negative always. Now I'm allowing f to be negative at the empty set. The second one used to be this, which said that your rank can increase by at most one. Sorry, this. Your rank can increase by at most one when you add a new element. Now, all I'm saying is it just has to be a monotone function. And then the third axiom is the same.
00:18:05.000 - 00:19:25.218, Speaker A: It's just submodular. So we're going to consider these. I'm going to get to y real quick. But just the natural example to keep in mind is let e be a set of linear subspaces of f to the n for some field, and then define f, you know, as the set function by f of s to be equal to the dimension of the span of the union of all the elements in square. Then I claim that f actually satisfies these three axioms. But it's not the rank function of a matroid. Because in particular, if, say, you have something in e that's like a two dimensional subspace, well, then f of that can be two, and the rank of a single element in a matriid is never going to be more than one.
00:19:25.218 - 00:19:41.364, Speaker A: And that follows from the original unprimed r two axiom. And sort of the example that I actually motivates considering this at all is.
00:19:41.944 - 00:19:47.244, Speaker E: Wait, I'm sorry, Daniel, could you just say that example again? The two.
00:19:48.304 - 00:20:00.164, Speaker A: Yeah. So e consists of subspaces of a ambient vector space.
00:20:00.864 - 00:20:04.964, Speaker E: Oh, it's subject. So it could be a two dimensional space itself.
00:20:05.984 - 00:20:06.764, Speaker A: Yes.
00:20:08.024 - 00:20:08.864, Speaker E: Okay.
00:20:09.024 - 00:20:09.504, Speaker A: Yes.
00:20:09.584 - 00:20:12.524, Speaker E: Okay, thank you. That makes sense.
00:20:14.424 - 00:20:28.030, Speaker A: I'm sorry. Could you also. I think I missed what he said when he defined us. Is that the span of the union of all elements of s. Yes, thanks. Yeah. And so.
00:20:28.030 - 00:21:14.914, Speaker A: And just real quick to sort of like bring this back down to earth a little bit more. When all of these have. When all these subspaces have dimension one, this is just a metroid. Yeah. But then, then another. So, yeah, another example that is going to be more relevant for rigidity is I'm going to get to this in a second. Let r1 and two reals be rank functions of metroids on the same ground set.
00:21:14.914 - 00:21:56.474, Speaker A: Then f, which is equal to r1 plus two reals minus one, satisfies the above. And in particular. So this is why I actually need the flexibility to be negative on the empty set. Because note that, well, with this example, f of the empty set is going to be negative one. Um. And then finally just another, you know, if you, um. If you've studied matriarch theory before.
00:21:56.474 - 00:22:58.684, Speaker A: Oh, sorry, sorry. Actually, no, no, forget that. Sorry. We're going to come back to that. Yeah, so, so, okay, these are, these are the class of functions that I want to, I want to consider, and the reason why I'm introducing them is the following theorem of Edmonds and Rhoda in 1966, which says that if f is a set function satisfying these three axioms. R prime one, r prime two, and r three. Then let I be the subsets of e satisfying that the cardinality of.
00:22:58.684 - 00:23:34.704, Speaker A: I'm sorry, sorry. Substance of e satisfying. I need to name these something. Sorry. The subsets I of e satisfying for all subsets I prime of I. The cardinality of I prime is less than or equal to f of I prime. Then mix, which I'm going to define to be the matrix on ground set e with independent sets.
00:23:34.704 - 00:24:19.704, Speaker A: I is a matroid, and I'm going to denote it by m of f. Now there's more, some more. Over. The rank function of m of f is the following. It's the minimum over.
00:24:21.184 - 00:24:41.804, Speaker E: Yeah, I'm sorry, I don't understand. This definition is I is script I the maximal collection of subsets, or something like that. I mean, this doesn't define a unique, doesn't look to me like it defines a unique object.
00:24:42.264 - 00:25:00.374, Speaker A: So, okay, so I guess, here, I'll make this. There's two eyes here. There's a script I and then block I. Script I is a set of sets, and block I is in script I if and only if it satisfies this property.
00:25:11.274 - 00:25:24.366, Speaker E: Okay. No, I don't understand this at all. Can you read that property to me? I'm having trouble even reading it. It's block I prime is a subset.
00:25:24.390 - 00:25:34.114, Speaker A: Of I for all subsets I prime of I. The cardinality of I prime is less than or equal to f of I prime.
00:25:38.814 - 00:25:43.034, Speaker D: So that's the test, whether block I is in script I.
00:25:43.154 - 00:25:43.618, Speaker A: Yes.
00:25:43.706 - 00:25:47.914, Speaker E: Okay, now I understand. Yes, this is, this makes sense. Okay, thank you.
00:25:48.074 - 00:26:00.254, Speaker A: And actually, I'm glad you asked that because I realized I forgot to mention I prime, not empty. This might not be true for the empty set, but that's fine.
00:26:02.314 - 00:26:14.804, Speaker D: And then, Daniel, can I ask, is there an example where f of I prime is bigger, strictly bigger than the number? Is that going to be the same subspace example?
00:26:15.864 - 00:26:42.414, Speaker A: Yeah, yeah, actually. So, yeah, take, yeah, take the subspace example with. Let e be a set with one element that is a subspace of dimension 50. Say. Then you define a matriid on that set with one element, where your independent sets are any subset of this one element set whose cardinality is less than f of that value. So now f of the empty set. Empty set is always independent whatever.
00:26:42.414 - 00:26:56.734, Speaker A: Take your unique non empty set consisting of that one element. The cardinality of that is one, and f of that 50 dimensional subspace is 51, is less than 50. So that's an independent set.
00:26:58.314 - 00:27:13.434, Speaker D: So can I think if before I thought matriids were columns of a matrix, aka one dimensional subspaces of a vector space. Now I can think that matriids also have subspaces.
00:27:15.494 - 00:27:57.274, Speaker A: I think, I think, yeah, I guess, I mean like flat. So yeah, if you have a matriarch, you can, yeah, you can group together, like you can partition the elements of it. You have to be a little bit careful and that might take us a little bit too far down a rabbit hole to be precise about what I mean there. But if you take some set of flats of. Actually, I have to think about this a little more. But yes, there is a way to go from, there is a way to combinatorially abstract this example of subspaces.
00:27:58.494 - 00:28:00.478, Speaker D: Okay. And then can I ask another question?
00:28:00.606 - 00:28:01.230, Speaker A: Sure.
00:28:01.382 - 00:28:14.118, Speaker D: Okay, so if we take e to be some set of subspaces and we define f as you did above, with like the dimension of the span of the union.
00:28:14.246 - 00:28:14.954, Speaker A: Yeah.
00:28:15.414 - 00:28:27.194, Speaker D: Then it's. Then by this we get a matridge. But can you give another example of a set e and a function f where this works?
00:28:27.494 - 00:28:31.874, Speaker A: Yeah, and we're getting there. That's actually the 2d rigidity metroid.
00:28:33.654 - 00:28:36.754, Speaker D: What's the set e and what's the function f?
00:28:37.774 - 00:28:44.994, Speaker A: We'll get there. Okay, stay tuned. Yeah, yeah. So.
00:28:47.494 - 00:28:50.514, Speaker D: Is there a third example that we won't get to?
00:28:51.414 - 00:29:18.704, Speaker A: There's plenty of examples because this is something that appears in combinatorial optimization textbooks. But I only care about the rigidity theory example. No, there are other examples in symmetry, force rigidity. I have a paper on this. Yeah, yeah, we can talk about that later. But yes, there are other examples. There are other examples.
00:29:19.084 - 00:29:19.532, Speaker D: Okay.
00:29:19.588 - 00:29:21.532, Speaker A: There's even motivation outside of rigidity theory.
00:29:21.628 - 00:29:24.300, Speaker D: It's like one phrase of the function.
00:29:24.412 - 00:29:27.156, Speaker A: In that case, one phrase of the.
00:29:27.180 - 00:29:33.024, Speaker D: Function, one phrase to describe this function, even though you can't obviously talk about it.
00:29:33.684 - 00:29:44.164, Speaker A: So we'll get there. Let's talk about that later. There's too many things to define to do that without just being confusing.
00:29:44.584 - 00:29:45.804, Speaker D: Okay, fair enough.
00:29:46.464 - 00:30:27.972, Speaker A: I will give you a concrete the 2d rigidity matriarch is an example of this and the function is not that difficult. We'll get there. But before the rank function of matriids of this form I'm going to write it down. Don't. So here, I'm going to write it down and I'm going to explain it. And it's complicated for humans, but polynomial time for computers. So the rank function is this, where the minimum f is.
00:30:27.972 - 00:31:12.044, Speaker A: Sorry. The minimum is taken over partitions. F equals f zero, comma, dot, dot comma, f k of your ground set. So I don't want to dwell too much on this exact, like what exactly? This, too much on this rank function. The point is that this can be computed in polynomial time.
00:31:13.824 - 00:31:20.144, Speaker D: But the rank function should take in a subset of e and spit out a number. What's the subset?
00:31:20.184 - 00:31:31.814, Speaker A: Sorry, sorry, sorry, sorry, sorry. Subset of. Yeah, good point. Yeah. So here the rank of s is equal to. Yeah, sorry. That, that's a good point.
00:31:34.034 - 00:31:35.294, Speaker D: Okay, thank you.
00:31:36.514 - 00:33:43.264, Speaker A: Yeah. This can be computed in polynomial time, and this is due to a result of Lobosch and Shriver 1981. And it actually uses like, linear programming in the ellipsoid algorithm. So it's polynomial time, but slow. But anyway, okay, now here's where I get to the punchline, and the question that you asked, alex is, here's an example, and this is due to Lobos and Yemeni 1982, which says that let rubber, let e be the edge set of the complete graph on n vertices, kn, and then let r from two to the integers be the rank function of the graphic matriid of kn. Then the 2d rigidity matriid, in other words, the matriid whose bases are these two three sparse graphs, or two three tight graphs. Then the 2d rigidity matriarch is m of two r minus one.
00:33:43.264 - 00:33:56.144, Speaker A: So in other words. Well, I'll give you a minute to just process that and then I'll do an example.
00:34:04.864 - 00:34:11.712, Speaker D: So the function f is equal to the function twice of the function r minus one.
00:34:11.808 - 00:34:12.344, Speaker A: Yes.
00:34:12.464 - 00:34:33.873, Speaker D: Okay. And then in the definition above, there was a sum, and it just wasn't clear to me that this is like well defined. What singles out f naught. That we're taking the cardinality of f naught, but then we're applying f to the f sub I's.
00:34:36.053 - 00:34:58.640, Speaker A: So, I mean, this is, you're taking a minimum here. So curly f. This consists of a choice of f zero, and then a choice of all the others, f naught. You're taking the cardinality of the rest of them, you're taking f of, and you're adding them up. There are many, many ways to do this. And the minimum number that you get out is the rank of s. I see.
00:34:58.672 - 00:35:03.688, Speaker D: Okay, so which one is f naught can also be there. We just take the minimum.
00:35:03.856 - 00:35:04.752, Speaker A: Yeah, yeah.
00:35:04.848 - 00:35:07.776, Speaker D: Just one of them. You have to actually count. That's all.
00:35:07.920 - 00:35:34.184, Speaker A: Yeah, yeah. Okay. And just one. I mean, and this is something you can compute with linear programming, which can be solved in, you know, polynomial time with the ellipsoid method. And. Yeah, and actually, another thing that's maybe another computational consequence is that this gives you a polynomial time certificate that your rank is less than or equal to some number. Right.
00:35:34.184 - 00:36:26.100, Speaker A: Because if you exhibit some partition such that, you know, if you forgetting about the minimum, this thing evaluates to like twelve on it, then you know that the rank of your matrix, your set is at most twelve. That's, that's the other sort of like consequence of this. But anyway, um. Okay, yeah. Anyway, the whole punchline is now that the 2d rigidity matrix satisfies this, or it can be constructed this way. And a consequence is that you have this formula for the rank function that can be computed in polynomial time. So, yeah, I get, I guess the corollary here is that the rank function of the 2d matriid is given by 2d rigidity.
00:36:26.100 - 00:37:06.424, Speaker A: Metroid is given by the rank of some graph. G is equal to the minimum over all ways of partitioning it of the edges f zero plus the sum of twice the number of vertices in f I minus three.
00:37:14.604 - 00:37:19.068, Speaker D: Wait, but we have a two, r minus one. Now we have a two, something minus three.
00:37:19.236 - 00:37:37.044, Speaker A: So what's the rank function of the complete graph? Right, so r. Sorry, I'm gonna, I'm gonna. R is. R is overloaded here, let's make this r hat. Right. Remember, r is the rank function of the graphic matriid. And we already established that r of g is just equal to.
00:37:37.044 - 00:38:11.834, Speaker A: Oh, I'm sorry, I'm sorry, three shouldn't be here. This should be number of connected components of fi. But it turns out that if you just restrict these partitions to be interconnected graphs, that's. Okay. So that, that's why I put a three there. But yeah, just remember that the rank function of g is equal to the number of vertices of g minus the number of connected components of g. So this should actually be.
00:38:11.834 - 00:38:23.514, Speaker A: Sorry that. I'm sorry, this is, this should be twice this minus. Sorry, twice this minus one.
00:38:30.894 - 00:38:32.742, Speaker C: Hi Daniel. I have a question.
00:38:32.878 - 00:38:33.594, Speaker A: Yes.
00:38:34.534 - 00:38:46.942, Speaker C: So when you say this rank function can be computed by the ellipsoid method. So this ellipsoid is a polynomial in.
00:38:46.958 - 00:38:52.536, Speaker A: Terms of data length l. Sorry, sorry, what was that? Polynomial in terms of what?
00:38:52.720 - 00:39:04.124, Speaker C: So the episode. Ellipsoid method is bounded by a polynomial. And this polynomial is in terms of the data length l, right.
00:39:04.904 - 00:39:05.768, Speaker A: In terms of the what?
00:39:05.816 - 00:39:07.976, Speaker C: L, the data length.
00:39:08.120 - 00:39:08.936, Speaker A: Data length, okay.
00:39:08.960 - 00:39:17.834, Speaker C: Yeah, the size of your data. Right. So what's this, the data lens corresponding to your graph or.
00:39:22.774 - 00:39:39.674, Speaker A: I don't know, off the top of my head, but yeah, I mean, if you're interested, you can check the paper of these three authors, but yeah, it's polynomial. The length of the data is polynomial in the number of, say, vertices. So.
00:39:42.354 - 00:39:55.934, Speaker D: So somehow you, you take this computation over all partitions and you turn it into a linear program in some dimension, and that dimension is the length of the data.
00:39:56.714 - 00:40:08.180, Speaker C: No, the length, the lens of data is the entry. The length of the entry. So, yeah, like for example, you have one, one, one. This data lens is, it's four. Right.
00:40:08.372 - 00:40:36.638, Speaker A: This is, yeah. I don't want to go into implementation details, just since I don't really know much about that. And honestly, if you were going to do this, practically, you wouldn't even use the ellipsoid method. Right, because simplex method is faster. Right. Or whatever people who work in optimization do is faster than the ellipsoid method. So the point is you can do, the point is that you can do this.
00:40:36.638 - 00:40:44.594, Speaker A: How you would practically implement that is maybe for another course. But, yeah, but anyway.
00:40:47.494 - 00:40:48.274, Speaker C: Thanks.
00:40:49.334 - 00:41:52.424, Speaker A: Okay. But yeah, this is the whole punchline. Yeah. And just to sort of see this in an example. Oh yeah. And I guess, by the way, so I mentioned rank functions of matroids, why you should care. So the rank function of a rigidity matriid tells you the minimum number of edges to add to a graph in order to make it rigid.
00:41:52.424 - 00:42:21.974, Speaker A: And in particular, this number for n vertices is two, n minus three minus the rank of your graph. And so, for example.
00:42:22.594 - 00:42:25.410, Speaker E: So you're not saying that it's the rank itself.
00:42:25.602 - 00:42:26.818, Speaker A: It's not the rank itself.
00:42:26.986 - 00:42:27.346, Speaker E: Okay.
00:42:27.370 - 00:42:28.610, Speaker A: Yeah, the co rank.
00:42:28.642 - 00:42:32.146, Speaker E: I guess rank function helps you determine.
00:42:32.290 - 00:42:57.864, Speaker A: Yeah, yeah. And I guess if you want to think about it intuitively, like itself, more directly, it's like the dimension of your linear space of infinitesimal flexes or. Sorry. No, no, no, that's, um. No no no, it's the co. No, no, that, that doesn't solve the problem. It's like the co dimension of that.
00:42:57.864 - 00:43:23.262, Speaker A: Wait, no. Um. Yeah, sorry, sorry. Yeah, this number is the. Yeah, this co rank is the dimension of the linear space of infinitesimal flexes. Hi, Daniel, is this statement true for other dimensions or is this only 2d? It's only 2d. Well, obviously with different values, but.
00:43:23.262 - 00:43:26.714, Speaker A: Yeah, even with different values, it's only true.
00:43:27.014 - 00:43:28.674, Speaker B: Two d. Two d.
00:43:32.454 - 00:43:52.466, Speaker A: Yeah. And there's a geometric reason for this, actually, which I can. Okay, I'll briefly say something about that, actually. So, like. And I'm probably. I hope I don't lose people, but it's possible that I will. If I do lose you just ask me to slow down.
00:43:52.466 - 00:44:52.426, Speaker A: But the geometric reason for this, why it doesn't generalize, is that. So the rigidity matriid is the linear matriid of the tangent space to the manifold, parameterized as. Sorry. The manifold in, let's say, r to the n. Choose two parameterized as, like, dij is equal to xi minus xj squared plus y. I minus yj squared, which is just, you know, the distance between xi and xj squared. Okay.
00:44:52.426 - 00:45:02.194, Speaker A: And, you know, I'll just say, like, forget non smooth stuff. I'm being. I'm being a little hand wavy here.
00:45:02.774 - 00:45:17.754, Speaker D: And in your. On your right hand side, you should maybe say v or something. Right hand side, the norm squared of Xi minus XJ that you've written.
00:45:18.414 - 00:45:20.314, Speaker E: Vi minus VJ.
00:45:20.614 - 00:45:49.294, Speaker A: Oh, yeah, yeah, yeah. Thanks. Yeah. Okay. And now. So there's two claims here. So, one, I can expand and work over C.
00:45:49.294 - 00:46:43.034, Speaker A: So I just let x, y, and, you know, their values be complex and just keep the same polynomial two. In this case, this manifold is a Hadamard product, which I haven't defined, of two linear spaces, whose bases actually. Two linear spaces.
00:46:44.494 - 00:46:44.806, Speaker D: Sorry.
00:46:44.830 - 00:47:22.894, Speaker A: I'm going to say such that if a is a basis, the matroid of a transpose is the graphic matriid of kn. And then three, if a manifold.
00:47:25.994 - 00:47:27.994, Speaker D: Daniel, you said to stop you, so.
00:47:28.114 - 00:47:29.122, Speaker A: Yeah, yeah, yeah. Please, please.
00:47:29.178 - 00:47:29.482, Speaker B: Yeah.
00:47:29.538 - 00:47:29.890, Speaker A: Of course.
00:47:29.922 - 00:47:36.594, Speaker D: I haven't been shy before. Okay, so a is a basis of one of the linear spaces.
00:47:40.654 - 00:47:49.634, Speaker A: Yes. Okay, so. Okay, yeah, yeah, they're. Okay. They're two linear spaces. They each have. They're different.
00:47:49.634 - 00:48:02.624, Speaker A: But if you take a basis of either of them and then transpose it, and then look at the matroid, in both cases, you get the same metroid, which is the graphic matriidal of the complete graph.
00:48:03.524 - 00:48:05.332, Speaker D: Okay. And then another question.
00:48:05.468 - 00:48:05.828, Speaker A: Yeah.
00:48:05.876 - 00:48:09.100, Speaker D: This is a manifold. It doesn't have any singular points.
00:48:09.252 - 00:48:21.224, Speaker A: I'm just. I'm waving my hands here. Forget about the singular points. It's not. But it's. Yeah, no, it's not, but, yeah. See? Yeah, forget none.
00:48:21.224 - 00:48:25.748, Speaker A: Here's my. Here's my little parenthetical. Forget non.
00:48:25.916 - 00:48:28.544, Speaker D: Forget. I thought it said to get.
00:48:28.894 - 00:48:30.270, Speaker A: Oh, no, no, sorry.
00:48:30.382 - 00:48:32.846, Speaker D: Doesn't make sense. So we're getting on good stuff.
00:48:32.870 - 00:49:51.610, Speaker A: Yeah, you know, it's a variety. It's not a manifold, but anyway. Yeah, and then. And then sort of like the general big guns here, which is. Which is. Actually goes back to your original question, Alex, of how do you get more stuff defined like this? If a manifold is a Hadamard product of linear spaces, let's say with bases, let's say a and b, then the matriid of the tangent space is actually given by this matriid that you get from summing the rank function of a of the matriid of a transpose plus the rank function of the matrite of b transpose minus one. And now here's where it breaks down in three dimensions.
00:49:51.610 - 00:50:03.574, Speaker A: In three dimensions, you're not a Hadamard. The relevant manifold here is not a Hadamard product of two linear spaces.
00:50:06.554 - 00:50:11.282, Speaker D: Where. Can we read more about this story that you were just outlining?
00:50:11.378 - 00:50:49.104, Speaker A: Yeah, this story is a paper of mine, actually. I'll put that up. I'll pull up an archive link. Oh, and the proof uses tropical geometry, actually. So you would like that, Alex? Yeah, yeah. I hope I didn't lose too many people there. I know the details are like.
00:50:49.104 - 00:51:45.252, Speaker A: I really waved my hands of the details there, but hopefully that gives a little bit of clarity as to why such a thing is like, you wouldn't expect such a thing to work for three dimensions. Yeah. And then. And then finally. So, okay. I think to most people in rigidity theory, the point of the paper is actually the following, which I did not get to, which says that. So this is a corollary of lobos, and corollary is maybe not the right word, but a consequence of Lobos and Yamini's characterization here, which they go through in their paper, is that if g is twelve connected, then not twelve connected.
00:51:45.252 - 00:52:10.640, Speaker A: Sorry, sorry, that's the wrong number. Six connected. Sorry. If g is six connected, then it's rigid in two d. And they use this characterization of the rigidity matriid, you know, in terms of these rank function. Sorry, these non ranks of modular functions. In order to get that.
00:52:10.640 - 00:52:32.020, Speaker A: You can read about that in the paper. It's not a very long proof, and there's a conjecture that comes out of that. That is, if g is twelve connected, then it's rigid in three.
00:52:32.052 - 00:52:47.984, Speaker E: Dan, just remind me what six connected is. Does that mean that if you remove any six vertices, you're still connected?
00:52:48.404 - 00:52:50.984, Speaker A: It means that if you remove any five vertices, you're still connected.
00:52:51.524 - 00:52:54.772, Speaker E: You need to remove six in order to disconnect it.
00:52:54.868 - 00:53:25.514, Speaker A: Yes, exactly. You need to remove at least six. So I actually was able to get through everything I wanted. So, yeah. Are there any other questions in the last five minutes?
00:53:32.894 - 00:53:53.902, Speaker C: Yes, so, yeah, so this. Back to my question about the rank function. So this rank function can be computed into in polynomial time by, in terms of matrix and the number of vertices. Is this true?
00:53:54.038 - 00:53:54.714, Speaker A: Yes.
00:53:57.574 - 00:54:11.852, Speaker C: Okay, so, so given the. Yeah, so given the graph, then this polynomial is totally determined by the number of edges and the number of vertices of this graph. The complexity.
00:54:11.948 - 00:54:15.596, Speaker A: Right. Yeah, yeah. Okay.
00:54:15.620 - 00:54:16.824, Speaker C: Yeah, thank you.
00:54:19.764 - 00:54:47.394, Speaker E: This conjecture in three dimensions seemed very odd to me. Is there, is there any computational evidence for that? Or, like. It's a very odd statement? I would have expected that you can construct examples with arbitrarily high connectivity.
00:54:54.334 - 00:55:00.274, Speaker A: Actually, Tony, do you want to take this one? This is probably more. Yeah, sure.
00:55:01.174 - 00:55:33.430, Speaker B: So, Lovas Nemni, construct an example that is eleven connected but not twelve connected, that is not rigid. But you can go the other way. So you could replace the number twelve with whatever your favorite big number is. And it's still a conjecture rather than something we can do. So you seem to have the opposite intuition to me, Will. My intuition is that it should. If you make the twelve a really big number, it should be easy to prove it's rigid because the 100 connected says minimum degree is 100.
00:55:33.430 - 00:56:17.122, Speaker B: And a minimally rigid graph in 3D will have average degree six. So you've got so many extra edges compared to at the minimally rigid subgraph, it should be easy to show the rank is high with all those extra bits. But somehow it's not. So, Tony, is it true that twelve connected is rigid in the abstract maximum metroid? Yes. So Bill Jackson, Katie Clinch and Sinny Putanigauer's recent papers prove that. Yes, in the cofactor matrix or the maximal three dimensional rigidity matriarch. This conjecture is true.
00:56:17.258 - 00:56:26.218, Speaker A: Yeah, with the conjecture, what's the current stumbling block on proving it?
00:56:26.266 - 00:56:26.938, Speaker B: Or is it.
00:56:27.026 - 00:56:28.694, Speaker A: We don't know how to tackle it?
00:56:30.874 - 00:57:06.314, Speaker B: Yeah, I just don't know what technique to use. So you could, as I say, you could increase the number twelve as high as you want. You need to have some reason why the rank of the rigidity matrix should be full. And I guess we just don't. The reason it's true in 2D, as Daniel said, is because we know combinatorially what the rank function is in 2D, but in three D. I think you need some geometric insight to move from this twelve connected to rank of the rigidity matrix. I don't think that's a good answer to your question, Joe, but I don't have a good answer.
00:57:07.214 - 00:57:26.354, Speaker A: Yeah, it sort of answers that we don't know how to go about it. So. Yeah. Is it just then that twelve is the lowest number that we haven't constructed a counterexample for? I didn't quite follow the whole thing. So you might have said, and I just missed it.
00:57:29.814 - 00:58:06.394, Speaker B: Yes. So Lovas and Yemeni, in their paper where they proved six connected, they constructed five connected but not rigid graphs in two D. And then they showed also eleven connected but not twelve connected graphs that are flexible in three D. I think if you look in that paper that Daniel linked, it's sort of small complete graphs in a ring and they're just pasted together in a clever way. So twelve is definitely best possible if it's true. And I guess 40 odd years later, if twelve was not going to be the right number, that and there was a small counterexample, it would have been found. So any counterexample must somehow be quite big.
00:58:08.054 - 00:58:38.454, Speaker A: Okay, so it doesn't follow like, or it's not suggested by what Daniel was saying beforehand that like in the way, in the same way that G being six connected means it's rigid in 2D as a consequence of what you've just said up there, g being the conjecture that g being twelve connected doesn't seem to follow him from that argument.
00:58:39.394 - 00:58:53.134, Speaker B: No. So we don't have the rank function for the 3d rigidity matriid and the rank function that Daniel described. It was, I guess the main reason why six connected is provable.
00:58:54.934 - 00:58:57.194, Speaker A: Oh yeah, I didn't think of that. Thank you.
00:58:59.254 - 00:59:26.634, Speaker B: Is there any conjectures for higher dimensions? So six and twelve is d times d plus one. So you can make the natural conjecture. I have no memory of reading anyone showing counter examples or showing that was tight in all dimensions. But maybe this is in the literature somewhere. I don't know. Also, is it known for stuff like body bar frameworks? It's true. For 3D body bar frameworks it is true.
00:59:26.634 - 00:59:34.594, Speaker B: I guess there is probably an analogy for d dimensions in that context. Yeah, that may exist somewhere.
00:59:34.754 - 00:59:43.894, Speaker A: Okay, is there something for six connected equivalent on surfaces?
00:59:45.294 - 01:00:18.724, Speaker B: Yes. So on, say on the cylinder. So if you fix a cylinder in three dimensional space and you ask about rigidity with the points constrained to the surface, then the analog of the two v minus three count is two v minus two. But two v minus two is really nice in combinatorics. It's two edge disjoint spanning trees. And so if your graph is four edge connected, I think it always contains two edge disjoint spanning trees. So you could replace six connected with four edge connected.
01:00:18.724 - 01:00:46.180, Speaker B: So four connected ores implies four edge connected. So that would be an easy consequence of the rigidity characterizations in that context. So somehow there, I think the rigidity characterizations are harder. But this sort of connectivity corollary is easier. You can move to a different surface and probably you can play a similar game, but not off the top of my head. I can't remember what happens first. Say a 2 volts minus one, or a two v two.
01:00:46.180 - 01:00:47.664, Speaker B: V minus zero count.
01:00:51.684 - 01:01:00.388, Speaker E: For, for d equals one. The statement is. What's the statement? If g is one connected, then it's rigid.
01:01:00.436 - 01:01:06.396, Speaker B: And a graph is rigid if and only if it's connected in one dimension. And yes, one connected means connected.
01:01:06.500 - 01:01:11.228, Speaker E: One connected means connected. But then the number breaks, the patterns breaking down.
01:01:11.276 - 01:01:11.916, Speaker A: Right.
01:01:12.100 - 01:01:26.748, Speaker B: That doesn't fit the DD plus one, but I think DD plus one is where the twelve is natural in the proof technique of low vas Yemeni. So I mean, the six connected actually gives you a lot more than just rigid, it also gives you globally rigid.
01:01:26.876 - 01:01:27.516, Speaker A: Yes.
01:01:27.660 - 01:01:38.212, Speaker B: And two connected is the one for globally rigid in. So it doesn't exactly break down, it just.
01:01:38.268 - 01:01:39.464, Speaker E: Ah, interesting.
01:01:39.884 - 01:02:04.584, Speaker B: It's proving something slightly different. Yeah. And you can even go even further. So Bill Jackson and Chiefbord Jordan showed that six connected implies globally rigid, but they could even delete a few edges so that six connected compared to five connected is tight. But six connected is not tight in the sense you can delete a few edges away and still have the, the nice rigidity properties. Is it like two or three edges or something?
01:02:04.624 - 01:02:04.880, Speaker A: You can.
01:02:04.912 - 01:02:05.424, Speaker B: Yeah, something small.
