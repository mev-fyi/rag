00:00:00.640 - 00:01:08.914, Speaker A: Okay, thank you very much. So, like last time, as many of you are more in c star algebras, I'm going to be giving a bit of background, and I will hopefully talk about recent results as well. But of course, getting the main idea of the framework is essential to get done first. So, in free probability, as I mentioned last time, there are these analogs of entropy, right? So, classically, the classical entropy of a probability distribution with some density is like. So if this mu is given by a density with respect to a vague measure, then you define its entropy in this way. Now, in the free probabilistic setting, when you have more than one variable, there's not such an easy way to write down entropy. But what Voikilescu did.
00:01:08.914 - 00:02:47.114, Speaker A: Voikulescu asked, what would be the analog of this hook, or analogs, our page for multivariate non commutative loss. Now, I think since everyone here was here on Tuesday, maybe you still remember a bit what is, what non commutative laws are. It's this description of the joint moments of several non commutative random variables. And so Wojcielescu is trying to come up with an analog of this entropy, and you can't write it on an integral formula, but what you can do is, I guess, use the idea of, hopefully I spell this right, Boltzmann. And this is coming from physics and especially statistical mechanics, that entropy, the entropy of some state, entropy of some macroscopic state is like the log of the number of microstates that have that macroscopic state. And so woe Gillescu interpreted this or applied this framework in an interesting way. So the macroscopic state is going to be the non commutative law.
00:02:47.114 - 00:03:24.374, Speaker A: So that's some non commutative law move, which I'll recall, this is a mapping from polynomial algebra. You to see, that's, you know, it's a, in fact, a facial state. And you, you might assume some, some val as well. And then the microstate here will correspond to n by n matrix approximations.
00:03:31.294 - 00:03:54.130, Speaker B: You know, I may just interject. I always think this is a somehow overly complicated approach to entropy. It seems to me that entropy is just where you take it for granted that something has exponential growth and the entropy is just the time constant. I mean, of course, that's what you're saying.
00:03:54.302 - 00:03:54.658, Speaker A: Yeah.
00:03:54.706 - 00:04:10.178, Speaker B: Somehow the word exponential is part of the language where no one ever uses the word logarithm. It's just somehow question of how established it is in the federal language. Okay, forget about it.
00:04:10.346 - 00:04:20.614, Speaker A: Yeah. Yeah. So, I mean, it is true that. Yeah, it's fine to think about this from the viewpoint of exponential, and I can, I can write down that viewpoint as I'm going.
00:04:22.744 - 00:04:29.324, Speaker B: Mathematical content. That's just the question of, I seem to be getting on to linguistics quite often.
00:04:29.864 - 00:05:01.864, Speaker A: Okay. Anyway, so I will comment on that later. Yeah. Okay. So what do we mean by matrix approximations? Well, in statistical mechanics, in various forms of entropy, right, there's always, like, you're always taking some sort of limits, and you consider better and better approximations, you know, and there's also limits based on n. So here we're going to consider. So again, let me mention here, Sigma doctor, the space of non commutative laws of d tuples.
00:05:01.864 - 00:06:44.766, Speaker A: And then, as I mentioned also, there's a weak star topology here. And so then when we talk about matrix approximations, what would that mean? Well, let's say consider, consider some open neighborhood. Let's say we're trying to approximate a specific long loop, right? So take, consider some open neighborhood that contains moo, and then we can say, and here I think I'm going to use a lowercase n, because otherwise I'll forget. So for this open neighborhood, we consider the set of matrix tuples. So let's say I'll just write superscript n xn in sulfa joint matrices to the d such that the non commutative probability distribution of x with a non commutative law is in this neighborhood. Right? So this is again the non commutative law of the matrix xn with respect to the normalized trace on the matrix algebra. All right, so these will be our matrix approximations.
00:06:44.766 - 00:07:04.834, Speaker A: And then in Voikalescu's framework, it was, you would consider the log of the volume of this thing and then take some limits. So let me write this down.
00:07:10.214 - 00:07:13.834, Speaker B: Are you calculating that?
00:07:14.454 - 00:08:03.314, Speaker A: Yes, by computing the volume of that set with the big measure? Yeah. Well, right, so, I mean, this is a definition, not a computation, I think. So you take, you consider the log, there's a certain normalization. Okay, so this, this here is just the normalization. I'm not going to explain it now, because I'm going to talk about a different version that doesn't use phlebag measure anyway. Okay? But there's some normalization. And you take the limb soup as n goes to infinity of this, and then you take the interim over the neighborhoods o.
00:08:03.314 - 00:08:51.238, Speaker A: And so the idea here is basically that the volume of the space of microstates is approximately this, like some normalizing constant exponential of this. So this, I'm sorry, my board work here was not very good. But you define the entropy of the law, which is denoted by the greek letter Chi. Chi of mu is this expression. And so this is saying that the volume of the space of matrix approximation should be approximately exponential of n squared times this entropy.
00:08:51.406 - 00:08:54.714, Speaker B: How come entropy is something you define rather than talking?
00:08:57.494 - 00:09:39.754, Speaker A: Well, you also have to calculate it, but there needs to be a definition first. Otherwise, what are you calculating? Like I said, you can't write down something like integral of rho log rho because there is no density. Yeah. This non cumulative law is not a measure, it's a state on this cser algebra. And so you can't express it in density. You don't have any, anything to get your hands on to write down a formula for entropy. So you have to do more work to even define what you're talking about.
00:09:39.754 - 00:09:46.114, Speaker A: Yeah. Okay.
00:09:48.374 - 00:10:00.478, Speaker B: People have, it's one way of measuring it. Maybe there's more than one way of measuring it. Okay, so there's the idea of definition, but you just look at angry.
00:10:00.606 - 00:10:01.006, Speaker A: Then what?
00:10:01.030 - 00:10:19.904, Speaker B: This is relatively, they draw their relative word frequency on a lot of graph paper and they get a straight line. And that's the entropy. The cope is the entropy. Yeah, that's probably good. Probably pretty good for deciding whether someone actually wrote something.
00:10:20.564 - 00:10:35.386, Speaker A: Well, yeah, yeah, yeah. Right. But this is an entropy in a different context. Right. Where you don't, it's not, it's not like a. So, first of all, it's not the discrete Shannon entropy, but then also it's not even the continuous differential entropy. Right.
00:10:35.386 - 00:11:41.984, Speaker A: It's an adaptation of that for this free probabilistic setting. And this also has heuristics from random matrix theory. But again, I don't want to go into that, because what I want to talk about today is applications to von Neumann algebras. So, yeah, free group runoff and algebra specifically. So maybe first I should, I should mention. Yeah, so, so if you take, if you take x one, or let me say, I guess, mu, right, mu, the distribution of, I say s one through SD, which are freely independent semicirculars. This is, like I mentioned before, an analog of Gaussians and free probability.
00:11:41.984 - 00:12:33.770, Speaker A: The spectral distribution is a semicircular shape from minus two to two, and they're freely independent. And so this is like the most canonical example. And you can consider the von Neumann algebra generated by these things, and that will just be this reduced free product of the von Neumann algebra of the individual ones, which would just be an l infinity space. And so this will be isomorphic to the von Neumann algebra of the free group on degenerates. And so this free probability theory has given us a lot of information about the von Neumann algebra free group. Yeah. Okay.
00:12:33.770 - 00:13:12.414, Speaker A: And in case you're wondering about this, why is this the same as this? Well, if you take a commutative von Neumann algebra, right? If you're taking l infinity of the interval from minus two to two as a measure space, that's equivalent to l infinity of the circle, and l infinity of the circle would give you har unitaries. And har unitaries would be like, you know, an element with infinite order in a. In a group, funnily, min algebra, that would be a hard unitary. Right. And if you take d freely independent hard unitaries, that gives you free. I mean, you can also, you know, of course, you can say freely independent are unitaries. And get this as well.
00:13:12.414 - 00:13:34.934, Speaker A: Yep. So. Right. Yeah. And the free entropy of this distribution of the semicirculars here, maybe you'll write sigma here, the distribution of the semicirculars of the free entropy, just equal to. That's right. I think it's just equal to d.
00:13:34.934 - 00:14:30.554, Speaker A: And therefore, you know, and in particular, right, this is finite. So here's the result of Oklahescu. So suppose that. Yes. So suppose that you have x one through xd, which are coming from m sulfur joint. So let. Yeah, I guess here you have a Rachel von Neumann algebra.
00:14:30.554 - 00:15:41.260, Speaker A: Von Neumann algebra with tracial state. And let's assume that m is generated by them. Then if the free entropy of the distribution of the x's is bigger than minus infinity, so this entropy, like the differential entropy in probability distributions, it can take positive or negative values. So, for instance, if you rescale your measure, then this will add something to the entropy, so you can actually achieve. You can achieve any real value. And so the condition that tells you that it has a large entropy, qualitatively, is that it's bigger than minus infinity. So if this is true, then you can deduce various things about the app.
00:15:41.260 - 00:15:46.484, Speaker A: So then m has.
00:15:49.224 - 00:15:50.124, Speaker B: Infinitive.
00:15:50.944 - 00:15:54.120, Speaker C: No, no, but I don't understand, like, the opposite, like, what?
00:15:54.232 - 00:15:56.552, Speaker A: It could be equal to minus infinity.
00:15:56.728 - 00:15:57.432, Speaker B: So it's just.
00:15:57.488 - 00:15:58.872, Speaker A: So if you wrote not equal to.
00:15:58.888 - 00:16:02.444, Speaker C: Minus infinity, so it's just you're basically convert to being finite.
00:16:02.944 - 00:16:11.004, Speaker A: Yes. Yes. I'm not saying there's anything less than minus infinity.
00:16:12.034 - 00:16:19.226, Speaker B: It doesn't have to be negative. Is the entropy in this setting? Is the entropy always negative?
00:16:19.330 - 00:16:21.546, Speaker A: It can take any real value.
00:16:21.730 - 00:16:23.294, Speaker B: What about plus infinity?
00:16:25.874 - 00:17:14.664, Speaker A: No, it can't take plus infinity. So if you have a finite, you have finitely many variables here, because it is going to be. So here. Okay, I'm going to write, I'm going to abuse notation and write the entropy of the tuple itself. It's like less than, I don't remember, but it's like less than or equal to some constant plus a constant times the l two norm of x squared. So it can't be arbitrarily. So if you have a certain x, then it has a certain l two norm, and the entropy can't be any bigger than that.
00:17:14.664 - 00:17:49.904, Speaker A: But, you know, depending on what x is, it could be arbitrary. You know, for some value of x, it can be arbitrarily large. It can take any real value. So then this is just the conditions saying that it's not trivial, right? That it's has some amount of entropy. So if this is true, then you have consequences for the von Neumann algebra. It does not have any clair algebra, right. And also m does not have property gamma.
00:17:49.904 - 00:18:21.422, Speaker A: Now, if you were Benoit, algebraic, then I would write this down and you, and you would say, aha, I know what those things are, and I care about those things, but I don't know. Do you guys know about. And what is cartoons of algebra? Of benoit algebras? Yes. Okay. Property gamma. Well, I'm sure that you know it. I'm asking everyone in the room, you.
00:18:21.438 - 00:18:23.914, Speaker B: Know, once I was talking with your imagine.
00:18:24.934 - 00:18:30.554, Speaker A: Yeah, yeah. But does anyone in the audience? Everyone in the audience, you know what property gamma is?
00:18:34.334 - 00:18:43.566, Speaker C: If I'm told that it has no comprehensive algebra and has no property gamma, I'm unexcited. That means I'm not gonna, I'm not gonna care about this algebra because I can't handle it.
00:18:43.590 - 00:18:44.798, Speaker B: I don't know what to do if.
00:18:44.806 - 00:19:37.494, Speaker A: I don't have a recovery. You can't handle it, right? I mean, this is why Wojcielewscu is here, to study free group on oumen algebra, because you cannot handle free group algebra, right? So the free group one algebra is kind of, it's, it's hard to decompose into any of these things, right? So cartoons of algebra means basically, okay, so cartoons of algebra means an abelian subalgebra whose normalizer generates the whole thing, right? And so basically having kurtans of algebra here, let me write this down. This means that, you know, this, this arises somehow from. Well, I don't know. I don't know if I would say explicitly a group measure space construction, because maybe it's not a group, but, you know, some some sort of thing like this. Yeah. Okay.
00:19:37.494 - 00:19:47.694, Speaker A: Okay. Right. So maybe it'll say something like group measure space construction.
00:19:49.914 - 00:19:56.174, Speaker B: If entropy is equal to minus infinity, can you conclude that there must be.
00:19:56.954 - 00:20:11.014, Speaker A: No, no, this is definitely not equivalent. Right. Okay. And then property gamma means that it has sort of approximately, but it has some non trivial approximate centered.
00:20:15.834 - 00:20:23.294, Speaker B: But in terms of knowing the history, I just like to mention that Uri Manning said when he was a student, he read the Mary van.
00:20:24.074 - 00:20:41.314, Speaker A: Okay. Okay, cool. Yeah. So pregroup phenomenologists are kind of hard to decompose in this. In this way. Right. And that's the reason they can be very mysterious.
00:20:41.314 - 00:20:48.314, Speaker A: But by using this entropy and quantities like this, we can actually get some kind of idea about their properties.
00:20:50.974 - 00:20:52.222, Speaker C: As far as we know, maybe all.
00:20:52.238 - 00:20:55.086, Speaker B: The rules are in one control that.
00:20:55.110 - 00:20:56.094, Speaker C: Hasn'T been solved yet.
00:20:56.174 - 00:21:23.144, Speaker A: Sure. Yeah, yeah. And in fact, I mean, this was one of the motivations for coming up with something like this, because one of Woikolescu's ideas. Well. Is. Well, maybe by looking at your space of matrix approximations and somehow assigning a dimension to it, you could detect whether free group algebras for different number of generators are isomorphic or not. Now, this hasn't been done, meaning this problem is still open.
00:21:23.144 - 00:21:32.898, Speaker A: And I don't really know if this is the right approach to solve it, but it certainly has given us a lot of information about the properties of these free groups.
00:21:32.986 - 00:21:35.054, Speaker B: This is not obviously.
00:21:37.234 - 00:21:59.672, Speaker A: I mean, so this free entropy itself is definitely not an invariant of the phenomenon algebra. There is a free entropy dimension, and that's something which, as far as we know, could be an invariant. But also, there doesn't seem to be any clear way to prove that it's an invariant for the von Neumann algebra.
00:21:59.768 - 00:22:01.724, Speaker B: If you take the specific generators.
00:22:10.664 - 00:22:28.884, Speaker A: Well, yeah, yeah, yeah. If you take your semicircular generators, we know what that is. Exactly. Right. So there's. So there's a whole. I mean, this is not what I was going to talk about in this talk, but there's a whole theory about transforming measures by some smooth functions and how the entropy changes in that case.
00:22:28.884 - 00:23:11.634, Speaker A: And so that can give us a handle on what the entropy is and what we can. How to compute it. But what I want to talk about today is another version of this entropy, where you actually do get something which is a von Neumann algebra invariant, and it doesn't distinguish between the different free group von Neumann algebras, because actually it will be plus infinity for all of this. But it is very well adapted to deducing a lot of these properties. So let me mention here. So Ken Lee Zheng wrote many papers about free entropy dimension. And there's a certain one where he talks, defines a strongly one bounded bone marrow algebra.
00:23:11.634 - 00:23:35.796, Speaker A: And you might ask me, why is called strongly one bounded? It doesn't matter. It's like the answer to that. Why it is called that is technical. And it's, and I think the approach that I'm explaining today explains, explains these properties. It doesn't explain the name, but you shouldn't care why it's called that. It's, it doesn't matter. Okay.
00:23:35.796 - 00:24:38.704, Speaker A: And so strongly one bounded, this defined a certain property of. So he defined this in terms of, well, there's several ways that you can define it. One way is in terms of the free entropy of gaussian perturbation of your x's. But another way is defined in terms of covering numbers of these microstate spaces. And so, and Zhang showed that strongly being strongly one bounded is a von Neumann algebra invariant, meaning that you can compute it for one set of generators and another set of generators, and it always will give the same answer. Yeah. So this here, it defines algebra invariant.
00:24:38.704 - 00:25:37.004, Speaker A: I'll write w algebra for short, because it's equivalent to von Neumann algebra. And so then Ben Hayes in 2018, and this was, I forgot when this was, but it was like five to ten years earlier than this. And anyway, so he is to find a define some h, some quantity, which is like a version of free entropy, but it's using these covering numbers. So this is calls this one donded entropy, which again is a terrible name because you see this name and you think, oh, well, it must be less than or equal to one because it's one bounded. But that's not what it means. Okay? It's not less than or equal to one there. It's just some quantity, right? So h of x one through xd.
00:25:37.004 - 00:26:38.064, Speaker A: And this thing here, this thing is an invariant of von Neumann algebra. And technically also the trace because, yeah. If you have an infinite direct sum, then you have several choices of traces, and this can actually change the value, but it's an invariant of the von Neumann algebra with its specified trace. And h being less than plus infinity is equivalent to saying that the von Neumann algebra is strongly one bounded in the sense of jung, basically. And so being this is somehow like repackaging stuff from here, but in perhaps a more streamlined way. And now you can reason about this invariant here. And so ended h.
00:26:38.064 - 00:26:42.824, Speaker A: I'll get to the definition soon.
00:26:45.464 - 00:26:47.124, Speaker B: Different definitions or.
00:26:48.624 - 00:27:28.044, Speaker A: Well, so I think. So Jung didn't explicitly write the definition of the h. So he defined this notion of being strongly one bounded, and in the proof introduced such a quantity like this. And then Hayes explicitly wrote the definition of this quantity and proved other properties of it, you know, additional properties that had more applications. So let me give you a sense of what this h is. So unlike the entropy chi, which is either minus infinity or it's bigger than minus infinity. So here.
00:27:28.044 - 00:28:26.010, Speaker A: So chi of x being nontrivial, bigger than minus infinity, implies that this entropy h is plus infinity. Yeah. So this is something which. Yeah, and so then I should also say, well, maybe an obvious remark is that, well, if you're, if you're venom in algebra, not con embeddable, then you don't have any matrix approximations. You know, if n is larger or if your neighborhood is small enough, then there will be no matrix approximations. Right. So not con embeddable, then there will be no matrix approximations.
00:28:26.010 - 00:29:05.982, Speaker A: And so then this will be minus infinity. And this will be also minus infinity because, yeah, you have no matrix approximations. So if you take the log of zero, you get minus infinity. Okay. Right. And this h, by the way, though, this h is, uh, you know, it's either greater than or equal to zero or it's equal to minus infinity. And then, and the h is minus infinity if and only if you're not condemning, right? You have no things like this.
00:29:06.158 - 00:29:07.914, Speaker B: Some of those will have a cartoon.
00:29:09.814 - 00:29:53.102, Speaker A: Yeah. So I'll get to that kirtan sub algebra in a moment. First, though, I wanted to say, well, what about, you know, is. So it's actually open. Does there exist from x with the h being finite and positive? So this would be kind of interesting because the positivity of h implies that it has a lot of similar properties, like the free group phenomenon, algebra. Like that. It doesn't have cartans of algebra and so forth.
00:29:53.102 - 00:30:11.034, Speaker A: But h being less than infinity would also mean that it isn't a free product. So let me actually, let me talk about some properties and applications of it. I'm not going to get, I will give the definition eventually, but I wanted to give motivation first.
00:30:15.694 - 00:30:18.754, Speaker B: So the third board is giving someone to the board.
00:30:19.394 - 00:31:11.574, Speaker A: There is a third board, but you can only see two of them at once. I eventually have to erase them all anyway. Okay, so some problem. Yeah. So anyway, more generally, hay is defined h of n in the presence of m for n, which is a von Neumann subalgebra of M, which again, has a given tricks. So there's this notion here. And what does this mean, so the idea of this is that it looks at matrix approximations.
00:31:11.574 - 00:32:24.504, Speaker A: So it looks at the amount of matrix approximations of n, which have some extension to a matrix approximations of n. Right. So this is approximations in non commutative law using these microstate spaces. Exactly. And I will like make this more precise, but I didn't want to sketch properties first. This is just the intuitive picture. Yeah.
00:32:26.204 - 00:32:32.804, Speaker B: So it looks a lot like embedding.
00:32:33.824 - 00:33:15.084, Speaker A: Exactly, yeah. Right. So con embedding is similar to saying, it's equivalent to saying that any non commutative law can be approximated by non commutative laws from tuples of matrices. Yeah. Okay, so about the properties of this. So one nice property is a monotonicity. So if you have n one inside n two inside m, then you have this inequality.
00:33:15.084 - 00:33:58.404, Speaker A: I guess I should also mention that the h of m that I was discussing before is equivalent to h of M colon M. Yeah, because I mean, if you have matrix approximations of m, they extend to itself. Right. You just know extension, you have to. Okay, so then there's another monotonicity property. Yeah. If n is contained inside m one inside of m two, then h of n with respect to m one is greater than or equal to h of n with respect to m two.
00:33:58.404 - 00:34:20.424, Speaker A: And the reason is if you have to extend your matrix approximations to a bigger thing, then that's harder. So then that means that you look at like fewer things. So you have this inequality. Okay, so let's see, what else do I want? Ah, I should look at my notes so that I write everything correctly.
00:34:21.764 - 00:34:22.744, Speaker B: Come on.
00:34:29.284 - 00:35:31.034, Speaker A: Ah, yeah, this is very good, a very, very important property here. Okay, so if n one and n two are inside m and the intersection of n one and n two is diffuse, meaning it doesn't have any minimal projections, then you have sub additivity. So this here is the von Neumann algebra generated by n one and n two together. This is less than or equal to this entropy plus this entropy. Okay, another one.
00:35:32.104 - 00:35:55.124, Speaker B: This is, here's a very naive comment, but the second line reminds me of the fact that when I, algebraic one is contained in the other, when the combatant of the other is contained in the combatant of the verse. That's some of that.
00:36:02.004 - 00:36:11.060, Speaker A: Yeah, yeah. Well, I'll talk about this, I'll talk about the proof of those properties in more detail later. But I'm not gonna, I better had.
00:36:11.092 - 00:36:12.304, Speaker B: Nothing to do with everything.
00:36:13.204 - 00:38:17.454, Speaker A: Yeah, not, not exactly, but not it, this doesn't, I mean, I think there is some similarity, right? It's like, you know how inclusions work with adding additional conditions? But anyway, where is that? Yeah, so yes, normalizer, yes, and this is now another, another great property. So if n is inside m, let's assume n is diffuse. And then if you have, so then the normalizer of n, and I know I use n already, but let me just write the norm for normalizing. So the normalizer of n will consider unitaries in m such that unusar equals m, right? And so then the fact is that the algebra of the normalizer of n inside m, the entropy of this inside m is the same. So if you add in things which normalize it, it doesn't change the entropy. And if you think about, well, cartoons of algebras, that means the normalizer generates the whole thing, right? So this kind of makes sense why this could be useful. And in fact, you know, the main, the most interesting results in Hayes paper in 2018 were actually for more for generalizations of the normalizer, right? So this actually works for various weak normalizers.
00:38:17.454 - 00:39:23.134, Speaker A: So for instance, if you replace this condition of equality with just saying that this intersect n is diffuse in here, this statement is still true. But all of these normalizers are sort of contained in this master object, which is called the singular subspace or anti core subspace. I'm not going to go into that, but I just want to mention here some of these applications with normalizers. You read the statement, you're like, oh, well, we can prove that in a different way, so it's fine. But really the power of this comes from the results here that cannot be proved in another way, come from understanding that this works for very much for extremely general version of normalizers. Okay, so color property. So property example.
00:39:23.134 - 00:41:01.484, Speaker A: So if m one and m two are non embeddable, then their free product has this entropy, which is infinite. Okay, and another property, or example, if m is amenable, then h of m is zero. Okay, um, so, so we'll see in a moment why this is, but let me guess, sketch this briefly, right? Is that here what we're going to do, we're going to look at these microstate spaces and we're actually going to consider that, uh, we're going to kind of quotient out by unitary conjugation, right? So if two matrix approximations are conjugate by a single unitary, and we'll basically consider them to be the same. And if you're amenable, then any two embeddings into some von Eumann algebra, let's say with trivial center to one factor will have to be approximately unitarily conjugate. And then based on this, you can also deduce something approximate for the matrix approximations. If you're amenable, then basically all your different matrix approximations are approximately unitarily conjugate with better and better approximation as n goes to infinity. And so then that will mean that, whatever.
00:41:01.484 - 00:41:18.274, Speaker A: However, you're measuring the size of this microstate space, if you're protein out by unitary conjugation, it's basically trivial. So then amenable things will have zero here. And of course it won't be minus infinity because these are conimatables. So, yeah, so this is zero.
00:41:24.414 - 00:41:27.954, Speaker B: Approximately.
00:41:28.774 - 00:41:36.814, Speaker A: Yeah, yeah. In the sense of, yeah, it's a measure of like how many matrix approximations there are. In the sense the conference, be sure.
00:41:36.854 - 00:41:41.794, Speaker C: That if you have h equals to zero, then it is more than equal to.
00:41:42.654 - 00:41:52.214, Speaker A: Well, this congress is not true in general, but actually, this is a. Sure, this is a good thing to mention here before you reach the whole.
00:41:52.254 - 00:42:10.102, Speaker B: Board proposing other analogy related to this order reversing. If you have a librarian function, anything to anything, then it's sometimes expiry and first variable and quantum variable.
00:42:10.278 - 00:42:44.764, Speaker A: Yeah, yeah, yeah, sure. It's similar to that. Okay, allow me to address this question of amenability. And now I think I will use the other board because I don't want to erase this stuff yet. So, so as you might have. Yeah, so you might have heard of the Peterson Tom conjecture. Peterson Tom conjecture has to do with, if you take amenable sub algebras in a free group phenomenal algebra, then it will be contained in a unique maximal amenable subalgebra.
00:42:44.764 - 00:43:39.634, Speaker A: Okay, so, okay, but let me first of all write this statement. So this is now a theorem. So if n is this inside a free group pronoun in algebra, and n is non amenable, then h of n is crucial. So the statement that you were saying is not true in general, but we now know that it's true. For in subalge was inside a free group full name in algebra. And this is a non trivial statement. So where does this come from? So this is a combination of two results, basically.
00:43:39.634 - 00:43:54.096, Speaker A: Well, certainly it could be infinite. We don't even know if it's possible for h to be finite. Right. We don't have any examples. Yeah. So we don't. It doesn't.
00:43:54.096 - 00:44:05.896, Speaker A: I'm not asserting anything if it's finite or infinite, just that it's not. Yeah. Negative values for this are out of the question. Except nine is infinite h of n.
00:44:05.920 - 00:44:07.764, Speaker C: Not equal to zero, right?
00:44:10.944 - 00:45:09.446, Speaker A: Yeah, sure. And it's also not equal to minus infinity because it's common. Okay. Yeah. So this was a paper random matrix approached to the Peterson Tom conjecture. And this was like one another statement in that paper, which is shown to be implied by the theorem that if you take, you take independent gaussian random matrices and let's say x one through xd and y one through yd, then the operator norm of some. Yeah.
00:45:09.446 - 00:45:52.976, Speaker A: So maybe I should say some polynomial of x, one tensor one of the tensor one and one tensor y. Right. So the operator norm here in the tensor product of this. Now, I mean, as you probably know, for a circle, for a finite dimensional sea star algebra, and in general for nuclear c star algebra, there's unique tensor products. So the sea star tensor products here, this, right. And these are random matrices. So they're random and also depend on n.
00:45:52.976 - 00:46:34.884, Speaker A: So I'll write here, n. This converges as n goes to infinity to the norm of. I'll just abbreviate here. This to x tends to one and one tensor y, where you're looking now in the vanillman algebra of X, which is the three group phonomenal algebra, min tensor product itself. All right, so here, this is, you know, x is this 3d semicircular family.
00:46:36.904 - 00:46:43.524, Speaker B: This reminds me to some extent something in a long paper.
00:46:47.304 - 00:47:38.876, Speaker A: Exactly right. So this is like Hogwarts and Sorbjorn sin looking at the operator norms of things, except now you have to do it with a tensor product of the two matrices and not just, you know, and not just one tuple of matrices. And so then this theorem in turn has been proved. So at least there were two different proofs announced to it, and at least one of them is correct. And so Wolinski and Kaputan here and Gordon Avenue, that they showed that this statement is true. And then Hayes. So Hayes actually earlier showed that this statement implies.
00:47:38.876 - 00:47:39.544, Speaker A: This.
00:47:41.844 - 00:47:51.570, Speaker B: Must be quite early, Hayes, because my recollection is very wild working on every other.
00:47:51.682 - 00:47:52.818, Speaker A: No, no, no.
00:47:52.986 - 00:47:53.562, Speaker B: Okay.
00:47:53.658 - 00:48:23.264, Speaker A: I mean, Ben Hayes has done a bunch of things in dynamics as well, but I don't think he's done sister algebra specifically. Yeah. So anyway, this is, well, group sister like. We mean, we proved lots of things about group annoyment algebras. I don't know so much about c star algebra. Okay. So we have that result.
00:48:23.264 - 00:49:12.474, Speaker A: I think I should wrap up the first half, but if you'll allow me a couple more minutes, because I know we started late a couple minutes I wanted to give you some examples of like how you apply the properties listed here. Sorry. Okay, so first of all, let me revisit the previous statement that I said about a free group one element. Algebra does not have a kertons of algebra. Okay. In fact, I'll prove them more general statement here. So this is now, I don't know, proposition.
00:49:12.474 - 00:50:24.124, Speaker A: If m has a cartoonist of algebra, then h and m is less than or equal to zero. So here, that is, it is zero or minus infinity, minus infinity if it's not condominium. Okay, as usual. So how does this work? So, yeah, so, so let's say let a be the cartoons of algebra. Okay, so then h of m or h of m in the presence of itself. Well, m is generate. So by definition of cartoons, that algebra, it's generated by the normalizer of a.
00:50:24.124 - 00:51:07.542, Speaker A: And so then this would be the same as this. And then this would be less than or equal to h of aa. And this thing here would be equal to zero. Since the abelian a, the commutative a, is amenable. Yep. So I use the, I use the normalizer property here, and then I use just the monotonicity properties. Yeah.
00:51:07.542 - 00:51:28.034, Speaker A: And looking at this. Yes. If you have three products of conempettable things here, then this shows it doesn't have cartons of algebra. Hello. Okay, let me do one more I think I can erase.
00:51:29.814 - 00:51:43.116, Speaker C: So I think she had a statement that if we're living inside of a free group algebra, and then non amenability would imply greater than zero.
00:51:43.270 - 00:51:44.112, Speaker A: Yeah, yeah, yeah.
00:51:44.208 - 00:51:46.080, Speaker C: Is there a confusion that I can make?
00:51:46.232 - 00:52:28.384, Speaker A: Yeah, sure. So, so here. So you could say here that the normalizer of an amenable thing inside the free group phone m and algebra has to still be amenable. And I think, I believe that may have been proved already in a different way. Right. But you know, so one example would be if a is a subset of LSD and it's amenable, then the normalizer. Right? And also, you know, the, all these weird generalized normalizers, this thing will also be immutable.
00:52:28.384 - 00:53:35.364, Speaker A: Yeah. Okay, so that's a very good observation. And then let me go back and do one more, which is using this property here. So now here's another such proposition, right? Right. So if, in particular, if n one and n two are inside m, n one intersect n two diffuse, and n one and n two are immutable, then that would imply that h of n one join n two and m is less than or equal to zero. Right. So this will be less than or equal to h of n one m plus h of n two.
00:53:35.364 - 00:54:07.644, Speaker A: In m, it will be less than or equal to zero because n one and n two are amenable. Yep. And so in particular, right. You take your free products and then it's impossible to generate this free product by taking two amenable things which intersect diffusely. Yeah. And then if you were to, you know, iterate these operations, you can come put these together in various ways. Okay.
00:54:07.644 - 00:54:19.884, Speaker A: So. Okay, I think I should pause here for the break. Continue after this.
