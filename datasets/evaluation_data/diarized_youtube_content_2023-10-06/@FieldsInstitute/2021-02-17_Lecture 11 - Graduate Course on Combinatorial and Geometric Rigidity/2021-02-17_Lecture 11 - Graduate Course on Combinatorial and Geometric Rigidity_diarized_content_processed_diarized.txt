00:00:00.200 - 00:01:00.852, Speaker A: Remember, we're talking about global rigidity. So I give you a framework in D dimensional space, and I want to know if every equivalent framework is congruent. So I want to know if every framework with the same edge lengths arises just from isometries, from translations, rotations and reflections. And so in the last lecture on Friday, what we went through Hendrickson's conditions, which says, if you're a generic, globally rigid framework, then you must be D plus one connected, and you must be redundantly rigid. And then at the end of the lecture, we talked about dimension one, where two connectivity, the D plus one connectivity condition on its own was enough to characterize generic global rigidity. So the main aim of the next three or four lectures is to characterize generic global rigidity in two dimensions. So to do that, I want to use a really big technique in rigidity theory called stress matrices.
00:01:00.852 - 00:01:47.378, Speaker A: So these are, you can avoid them in the proof, but these are really important in lots of research and rigidity. So I do want to include them, even though I don't want to spend a lot of time on stress matrices. So this lecture, and then hopefully later in the course, Bob Connolly will give guest lectures about stress matrices as well. But this is the one lecture I'm going to tell you about stress matrices, and there's a specific purpose to it because I'm going to use it to understand the one extension operation for global rigidity. Okay. And so this was a result in one dimensions that we took as for granted on Friday, but we're going to prove it in this lecture. Okay, so, yeah, so I guess this is, I've just said this that we're looking at.
00:01:47.378 - 00:02:27.224, Speaker A: We did look at one dimensional global rigidity and showed it was a property of the graph. It was exactly two connectivity, and we want to have an analogous result for two dimensions. But before we really get into the combinatorics of that, we're going to develop this algebraic sufficient condition based on equilibrium stresses. Let me remind you what an equilibrium stress was. I define a stress just to be an assignment of weights to the edges. It's just some function that puts a real number on every edge, whereas an equilibrium stress is a stress which satisfies an equilibrium condition. And so you look at any vertex of your graph and you have to check this for all of your vertices.
00:02:27.224 - 00:03:07.162, Speaker A: Then the weights, the omega I, j on the edge from vi to vj times by the vector difference pvi minus pvj as you sum over all j's. You can think of this as all edges incident to your given vertex I, you need to get to zero. So you need to be in equilibrium at every vertex. Okay? And so as hopefully, you know, the, the weights omega I, j are the weight on the edge if vij is an edge and zero if it's not. So we really, you can think of this sum as going over all neighbors. So that's what I had here. J equals one to v.
00:03:07.162 - 00:03:51.976, Speaker A: By just putting zeros on the non edges, it's possible that an equilibrium stress, some of the actual edges have zero, but certainly the non edges must have zero. Zero. Okay, so when we introduced equilibrium stresses, we mentioned that, and then this is where the motivation for them comes from, that equilibrium stresses is a nice way of describing vectors in the co kernel of the rigidity matrix. So equilibrium stresses are the dependencies in the rigidity matrix. You have a linear dependence in the rows. It gives you a vector in the co kernel that gives you an equilibrium stress and vice versa. Okay, so of course if you're independent, if the rows of the liquidity matrix are linearly independent, then the only stress would just be the all zero.
00:03:51.976 - 00:04:34.014, Speaker A: So put weight zero on every edge and obviously that's not an interesting stress for us. Okay, so going back to the eighties and work of Bob Connolly, he defined a matrix that encodes these equilibrium stresses. So given a stress omega for our d dimensional framework, we define an n by n, a mod v by mod v. Symmetric matrix omega. And I'm going to usually have omega of little omega because it depends on the stress. And so if I want to edit the stress I'll use the same big letter, but the matrix will change because the omega would change to an omega dash or something. So I'll do it that way.
00:04:34.014 - 00:05:17.172, Speaker A: So it's a square matrix where the rows and columns are both indexed by the vertices. It's symmetric and the off diagonal entries are the negative of the stress. So the equilibrium stress, the weight on the edge from vertex I to vertex j, so omega I j, the entry in row I column j is minus that stress. And the diagonal entries are chosen. So the row and column sums are zero. So the diagonal entries are chosen to be the sum of the stresses incident to that, that vertex. Okay, so we'll do a small example soon, but yeah, and just to repeat what I just said, every row and column sum is zero.
00:05:17.172 - 00:05:53.624, Speaker A: So this tells us something about the kernel of the stress matrix, which is going to be important for us. Okay, so if you like, you could think about these stresses as applying to every coordinate like we did when we worked out what an equivalent stress was in a particular example. And then you could really think of the simple tensor product. But each of the things in the tensor product is just the stress matrix itself. It's just omega itself. So we really do just factor out and just work with this n by n. We're not like the rigidity matrix where it's got d times the number of vertices, columns and then some number of edges.
00:05:53.624 - 00:06:18.186, Speaker A: This time it's just the number of vertices. So it doesn't matter what dimension we're in, the stress matrix doesn't change the, we're going to talk about the rank of the stress matrix soon. And the rank depends on the dimension. The maximum possible rank depends on the dimension. But the actual matrix itself is always n by n. Its entries depend on the equilibrium stresses. And whether a stress exists or not, that depends on the dimension.
00:06:18.186 - 00:06:32.494, Speaker A: But you don't see the dimension directly in the pattern of the matrix itself. Okay, so we call, as we said, we call omega the stress matrix. Sorry, was there a question?
00:06:32.654 - 00:06:35.038, Speaker B: Yeah, can you go back up a little bit?
00:06:35.126 - 00:06:35.794, Speaker A: Yeah.
00:06:38.054 - 00:06:43.474, Speaker B: Can you expand on, technically we may apply omega to each coordinate?
00:06:44.894 - 00:07:27.566, Speaker A: Yeah, I think that's just bad wording, probably. So all I'm sort of saying is that this equation is really the pvis of vectors in RD. So you really have a sum of d equations equals zero and this is a vector zero. So you've got d different linear equations equaling zero. And so really you could make this into a sort of a big matrix like this. But I mean, so probably I shouldn't have even said this because I don't want to dwell on it. So I just wanted to make the point that the definition here, it looks, I mean, omega is just a square matrix that's symmetric and has this pattern of entries.
00:07:27.566 - 00:07:39.434, Speaker A: And so that doesn't depend on the dimension. The bit that depends on the dimension is the actual definition of a stress, an equilibrium stress, and the possible ranks, which is what's going to be important about this matrix.
00:07:39.894 - 00:07:46.838, Speaker B: Okay, so, but with this omega tensor identity, that's a d by d identity matrix, right?
00:07:47.006 - 00:07:47.638, Speaker A: Yeah.
00:07:47.766 - 00:07:49.230, Speaker B: Or no, wait.
00:07:49.382 - 00:07:59.742, Speaker A: Yes, so this is, this is d D. Well, let's say to mod v like I had. So this is d mod v and d mod v. All right, so it's.
00:07:59.758 - 00:08:04.794, Speaker B: A size of the vertex identity matrix and then the column, like if we're going to multiply this.
00:08:06.094 - 00:08:10.226, Speaker A: Yeah, you're correcting me here. You're saying that.
00:08:10.330 - 00:08:11.374, Speaker B: Yeah, exactly.
00:08:11.674 - 00:08:12.362, Speaker A: Yeah.
00:08:12.498 - 00:08:23.094, Speaker B: And then, sorry, what is the column vector that we would multiply with? It would be the vi minus vj for each eye in the vertex set.
00:08:24.914 - 00:08:26.814, Speaker C: Hang on, that. That's not correct.
00:08:27.754 - 00:08:29.202, Speaker A: It was the d. Was it, Sean?
00:08:29.298 - 00:08:33.714, Speaker C: It definitely is the d, because otherwise you're doing a tensor product of a v by v matrix.
00:08:33.754 - 00:08:38.010, Speaker A: Oh, yeah, that's true. Because then each of these things would be. Would be d d here, wouldn't they?
00:08:38.042 - 00:08:41.513, Speaker C: So, yeah, sorry, it is d. Yeah, yeah.
00:08:41.553 - 00:08:46.493, Speaker B: Okay. Okay. And then what is the column vector we're multiplying by?
00:08:49.113 - 00:08:50.333, Speaker A: I'm sorry.
00:08:50.953 - 00:09:00.333, Speaker B: So we take this tensor product matrix and we multiply by a certain column vector and we recover the equations above. What's the column vector?
00:09:02.793 - 00:09:06.933, Speaker A: Oh, oh. So you're wondering how to get to this.
00:09:07.834 - 00:09:08.690, Speaker B: Yeah.
00:09:08.882 - 00:09:28.546, Speaker A: Yes. So can you, can you shelve your question for maybe? I don't know if it's two or five minutes, but I think it might be in what follows. So I'm going to define a matrix a bit further down. Yeah, here. And I think it's that one, but so you can see it's just coming straight up. So if it is not clear, then ask again.
00:09:28.690 - 00:09:29.642, Speaker B: Okay, great.
00:09:29.738 - 00:10:17.964, Speaker A: Sorry. Yeah, so where are we? So we've got the stress matrix. So, by the way, any other questions? Is everyone happy with the stress matrix or the definition so far? At least we're going to see in a moment, in a couple minutes the calculation that shows why it's a sort of sensible thing to do. But first, I wanted just to do an example. And so this is an example we did before, maybe you don't remember, this is the 11th lecture, and maybe it was in lecture two, but we took the complete graph on four vertices in two dimensional space. If you remember, we realized it as a square. So we had 001-1011 etcetera, and we calculated the stress and we got weight one on the four edges around the outside and weight minus one in the middle.
00:10:17.964 - 00:10:48.244, Speaker A: So this, we can then put this into our stress matrix for this particular stress. And you just get this nice pattern. So say at vertex v one, you've got a one from vertex one to vertex two. So that gives you minus the stress, a minus one from vertex one to vertex three. So minus, minus and a one from vertex one to four, so you get minus one, and one plus one, minus one gives you the one on the diagonal. And you can, obviously, you can do that all yourself easily.
00:10:49.024 - 00:10:50.712, Speaker B: So can I ask another question?
00:10:50.808 - 00:10:52.312, Speaker A: Yeah, sure.
00:10:52.368 - 00:11:00.128, Speaker B: So, from Sean's lecture, if there, we know that there's no. So this is one equilibrium stress.
00:11:00.296 - 00:11:01.096, Speaker A: Yes.
00:11:01.280 - 00:11:08.644, Speaker B: But we can also say that there is no equilibrium stress that will have a zeros on the diagonal of omega.
00:11:12.224 - 00:11:15.184, Speaker A: There exists no which graph or for.
00:11:15.264 - 00:11:21.520, Speaker B: In general graph and displacement that you've drawn because none of them are affinely dependent. Is that right?
00:11:21.592 - 00:11:55.044, Speaker A: Oh, so for this graph, there is what we solved in the second lecture, and it was a one dimensional space of equilibrium stresses. And to make it the one one ones, all I did was take one of the variables and set it equal to one. So in this case, yes, so in this case, because it's a one dimensional stress and we know it's non zero on every edge and. Yeah, I mean, I guess I could come up with a realization that was horrendous, but as you say that it would, in this case, it's okay. The diagonals will all be non zero.
00:11:56.124 - 00:12:05.224, Speaker B: Okay. And if we took a triangle and put a vertex in the center of the triangle and did the same graph, so a different realization.
00:12:05.604 - 00:12:07.684, Speaker A: You want to take an arbitrary triangle.
00:12:07.844 - 00:12:21.100, Speaker B: And then do the same graph, but take v four and put it in the center in here. Yeah. Now, now there's an equilibrium stress with a zero diagonal. Is that right?
00:12:21.292 - 00:12:54.634, Speaker A: I don't know. I would have to do the calculation. Sean, if this goes back to your lecture, do you know the top of your head? So, I mean, my guess is that this is sufficiently generic, unless you put some extra conditions, Alex, that this will still be a stress like this one. It might not be so nice as what, ones and minus ones, but it'll still be non zero on all the edges. And it would be surprising if they sum to zero on the diagonal. But I'd have to do a calculation.
00:12:55.294 - 00:12:59.234, Speaker C: Yeah, I'm not totally, totally sure.
00:13:00.574 - 00:13:01.434, Speaker B: Okay.
00:13:02.054 - 00:13:06.750, Speaker C: It's a rare condition that you sum to zeros on the diagonal. That's the usual condition.
00:13:06.902 - 00:13:11.954, Speaker B: I was just trying to connect Sean's last lecture to this one, but I'll think about it more.
00:13:12.784 - 00:13:13.360, Speaker C: Yeah.
00:13:13.472 - 00:13:18.284, Speaker A: Let me know if you see something nice. But as Sean said, it's quite rare.
00:13:18.824 - 00:13:22.404, Speaker D: I think if they sum to zero, you'd have to have three collinear points.
00:13:26.784 - 00:13:27.564, Speaker A: Maybe.
00:13:28.424 - 00:13:30.104, Speaker C: Yeah, yeah.
00:13:30.144 - 00:13:30.640, Speaker D: No, you would.
00:13:30.672 - 00:13:31.712, Speaker A: You would. I think.
00:13:31.848 - 00:13:33.324, Speaker C: Yeah, I think Tim's right.
00:13:35.464 - 00:13:39.244, Speaker B: I'm not saying all the diagonal entries will be zero, just one.
00:13:40.264 - 00:13:49.164, Speaker D: Yeah. So I think if one of the diagonal entries is zero, then that. So that, that diagonal corresponds to a particular vertex and the other three vertices would be collinear.
00:13:50.824 - 00:13:56.164, Speaker A: So that is actually what I thought Alex had meant originally. I thought this vertex was going to lie here and it would be like that.
00:13:57.984 - 00:14:08.114, Speaker C: Yeah. Yeah. I think for k four, that would have to be something like that. Yeah, but this is just for k four. It's not obviously for a general graph, just in this case.
00:14:10.134 - 00:14:12.550, Speaker B: Okay, cool. I'll keep thinking about it.
00:14:12.662 - 00:14:46.570, Speaker A: Yeah, thanks. And thanks for the comments. Yeah. So where was I? So probably I was here still. So I wanted to just make explicit, this is a rank one stress matrix, because even though it's a four x four matrix, this is actually the maximum possible rank for the, given the constraints you have from the definition of a stress matrix. And we'll see what the maximum rank is in general soon. But we will want to know that small complete graphs have maximum possible rank.
00:14:46.570 - 00:15:52.004, Speaker A: And so this is just something I wanted to point out. Okay, so I think this is going to start answering Alex's first question. So I take a d dimensional framework and let's say it's got n vertices, and let's say just to have the d coordinates, I'm using xi one up to xid for the location of the framework point pvI, then omega is an equilibrium stress if and only if this matrix, where I think of the pvis as column vectors times omega is zero. So I've got my x eleven, x twelve down to x 1d, my first pv one and then xn one, xn two down to xnd here. So I times that matrix by omega and I should get zero. And so this is really a calculation that I don't really want to say anything about. But if you do, recommend you try it for yourself because you'll sort of see the how the equations naturally match up with the matrix multiplication.
00:15:52.004 - 00:16:43.010, Speaker A: Okay, so given that this matrix, I want to also add in the fact that the row and column sums are zero. So the fact that these sums all give you zero tell you that it's also true if you take the all ones vector and you times that by omega, then you'll get zero as well. So I'm going to find the configuration matrix to be that this matrix I had up here with the p's augmented by one extra row with the all ones vector. Okay, and so we've just had it that this is equal to zero. So the configuration matrix times the stress matrix is equal to zero. So we know that every row of this configuration matrix belongs to the co kernel of the stress matrix. And from the co kernel we get a, we get an upper bound on the rank of the stress matrix.
00:16:43.010 - 00:17:22.884, Speaker A: So of course the dimension of the co kernel is at least the, the rank of this configuration matrix, because every row belongs to it. There could be other things, but these things are always there. And so the rank of omega is at most it's n by n for n vertices minus the rank of this configuration matrix. And so here you had one, two up to d plus 1d plus one rows. And so if you sort of have a generic situation, you would expect these d plus one rows to be independent. So our n minus rank c is going to turn into n minus d minus one. But I think I said, yeah, I say that here, so I should have skipped ahead.
00:17:22.884 - 00:18:11.854, Speaker A: Yeah. So I've also said if equality holds, then not only does every row of c belong to the co kernel, but the co kernel is actually just the row space of c. So it's exactly those linear combinations of those rows. When p is generic, the rank is equal to, to d plus one. So the rank of the stress matrix is always at most n minus d minus one. So even though omega itself, the pattern of it doesn't say the dimension, when we're looking at the rank that we're going to be caring about soon, then the dimension does affect what the maximum possible rank can be. I mean, to me it's slightly odd because as d gets bigger, the maximum rank gets smaller, but it's just something you have to get your, your head around as you go.
00:18:11.854 - 00:18:48.062, Speaker A: Okay, so, yeah, I guess, as I sort of said, all this may be already so rigidity. We cared about maximum rank rigidity matrices for generic global rigidity. It'll turn out what we're caring about is exactly maximal rank stress matrices. So I'm going to say that a framework GP has a full rank stress. If there exists an equilibrium stress such that the rank of the stress matrix is maximal. So if there exists is crucial, because obviously our co kernel could be quite large dimensional. So there might be many different stressors, many different equilibrium stresses.
00:18:48.062 - 00:19:43.606, Speaker A: They may all have different ranks, but what you need is for it to be a full rank stress. For the framework to have a full rank stress is for there to exist one stress which achieves the maximal rank. Okay, so here's a really difficult theorem to prove. I mean, the statement is not, not hard to understand, but the proof is really complicated. So I take a generic framework on at least D plus two vertices, then the generic framework is globally rigid if and only if it has a full rank stress. So the direction that says if you're globally rigid, then you have a full rank stress, that's the hard direction. And that I'm not going to say much at all about, that's due to Gaultley, Healy and Thurston the other direction, if you have a full rank stress, then you're globally rigid, is due to Connolly.
00:19:43.606 - 00:20:13.924, Speaker A: And that I will give like a three line sketch of the result that is the main result of a paper. So you can tell I'm not going into any great detail. So, roughly, we're going to take this theorem as, as given, we actually are only going to use the Connolly direction. So the other direction we're not going to need in the results where we're going to prove. Okay, yeah, so I've said this. So here's a sort of. Oh, it was not proved in 2010.
00:20:13.924 - 00:20:42.004, Speaker A: What was it? 2005. So, apologies for that. So, Connolly proved that having a full rank stress in a generic framework guarantees you're globally rigid. So, I don't know if this is worth saying. It may be just better to look at Connelly's paper and get a sense for yourself. But I did want to just say a few words about the proof. So, suppose I have a generic framework in RD with a full rank stress.
00:20:42.004 - 00:21:36.274, Speaker A: Now I'm going to choose some equivalent framework. And so, for all equivalent frameworks, since I want it to be globally rigid, I have to be able to show this equivalent framework, GQ, is congruent. So I take some framework with the same edge lengths, and somehow the full rank stress property has to tell me congruence. Okay, so the way, the way Bob Conley does this is sort of in three steps. So, the first step, I think, is quite hard, and that is to show that this particular stress omega that we have a full rank for GP, is also an equilibrium stress for GQ. So, if you just have it, knowing that GQ is on the same graph and it has the same edge lengths, but you don't know anything about the genericity or anything else about Q, just the lengths of the edges, you get an equilibrium stress. The same equilibrium stress works for this equivalent framework.
00:21:36.274 - 00:22:16.538, Speaker A: When you know that it's not so hard to show that GQ is actually an affine image of GP. And so by affine image, I just mean you can get, you take each point and there's some matrix plus a vector that takes one point to another via this affine transformation. So, showing that from the fact that you have the same stresses is not so hard. And then at the end, the third step is, it's also not so hard. But it goes into a little bit of projective ideas. So it uses what's called conics at infinity and things. And the point is that when you know that GQ is an affine image.
00:22:16.538 - 00:22:51.218, Speaker A: So affine images need not preserve edge lengths. So you're constrained by your equivalence condition. And then you also know you have generic, which gets you away from these conics at infinity, whatever they are. And so Bob can avoid them. And you just read it in that part in his papers, not so hard, and will show you that you must be congruent. Ah, so there's a question in the chat. So Jim asked, does the first step need p to be generic or does it work for non generic P? So as far as I know, this first step is exactly where you need generic.
00:22:51.218 - 00:23:20.156, Speaker A: And so this is where you need generic in the proof I'm going to present of the two dimensional characterization of global rigidity as well. So somehow, in a lot of what I said, in the course for rigidity, you can get away with weaker conditions than generic. But this is exactly the point where, as far as I understand it, you definitely need generic. Not and weakening generic. I think it would be really interesting if someone could do it, but as far as I know, it's open to weaken the generic hypothesis there.
00:23:20.300 - 00:23:25.144, Speaker D: So, Tony, in principle, Q could be non generic, but p could be generic.
00:23:25.524 - 00:23:30.364, Speaker A: Yes. So in Bob's argument, the Q here doesn't have to be generic now.
00:23:30.524 - 00:23:31.700, Speaker D: Okay, thank you.
00:23:31.772 - 00:23:38.948, Speaker A: But the p you start with has to be. And if it's not, I don't know how to show this step. Sean, did you want to say something?
00:23:39.076 - 00:24:01.772, Speaker C: So you can slightly weaken it, but I don't know how much good it is, because he doesn't need that. It's generic per se. What he needs is that the configuration space of points equivalent to GP is a smooth manifold is what he needs, or a differentiable manifold. That's what he needs.
00:24:01.908 - 00:24:02.740, Speaker A: Yeah. So he.
00:24:02.812 - 00:24:04.740, Speaker C: Generic gives that for free, basically.
00:24:04.892 - 00:24:10.124, Speaker A: So generic. Generic allows him to use the sort of tasky Seidenberg stuff to get that.
00:24:10.244 - 00:24:10.984, Speaker C: Yeah.
00:24:11.764 - 00:24:15.988, Speaker A: But do you have a weaker condition than generic that allows you to do the same thing?
00:24:16.116 - 00:24:28.244, Speaker C: No, not really. It's one of them that, yeah, there's a weaker condition, so you can show there's more points that satisfy it, but then you're then still stuck in the same problem, really?
00:24:28.364 - 00:24:40.918, Speaker A: Yes. You can very definitely make small weakenings. Like I defined this idea of quasi generic last time to prove Hendrickson's conditions, and Bob's proof will work in this condition context where it's very slightly less generic.
00:24:40.966 - 00:24:45.270, Speaker C: But, yeah, quasi generic will be fine. You just need to. Yeah, yeah.
00:24:45.382 - 00:24:57.054, Speaker A: A big improvement. I mean, so it's probably not true if you just put regular there. I guess, but if you put some stronger version of regularity, then maybe there's an open problem there.
00:24:57.214 - 00:25:06.284, Speaker C: Yeah, you need like, it's kind of like a strongly regular, you need to be regular, but everything else equivalent is also regular is what you need. That's the exact condition you want.
00:25:07.184 - 00:25:11.844, Speaker D: Okay, so I have another question, but I don't want to hijack the lecture, so maybe I'll save it till later.
00:25:12.184 - 00:25:13.704, Speaker A: Okay. It's up to you.
00:25:13.784 - 00:25:17.256, Speaker C: Yeah, sorry about hijacking, Jim.
00:25:17.280 - 00:25:19.280, Speaker A: I'm happy to answer now, or at the end, it's up to you.
00:25:19.312 - 00:25:25.404, Speaker D: No, no, it's okay, because it's kind of an open ended discussion. I just wanted to ask Sean about what he said there, so.
00:25:27.624 - 00:26:03.092, Speaker A: Yeah, I'd be interested in talking about that later as well. Okay, so, yeah, so that's Bob's part of the theorem. I just mentioned that if you're, if you have a generic framework, maximum rank, full rank stress, then you're globally rigid. The converse that every generic, globally rigid framework that's not some more complete graph has a full rank stress is the more complicated direction. I think I said this bit already, so I'm not going to go at all into that proof. It's. I find it still hard to, to follow step by step the argument, but I do want to mention a corollary of theirs again.
00:26:03.092 - 00:26:49.924, Speaker A: So I mentioned this right at the start of the course as well, that it follows from their results that generic global rigidity depends only on the graph. So either every generic realization of a given graph is globally rigid, or none of them are, or all generic realizations are not globally rigid. So we can think of generic globally rigid as a property of a graph rather than a framework. And until this result in 2010, that was an open problem. It's a nice thing, but we're not going to use it for anything. We knew it was true from last lecture in one dimensional space, because two connectivity was a complete characterization. And when we finished characterizing global rigidity in 2d generically, then we'll know it's true in two dimensions as well, but that they proved in all dimensions.
00:26:53.134 - 00:26:54.158, Speaker B: Hey, Tony.
00:26:54.326 - 00:26:54.894, Speaker A: Hi.
00:26:55.014 - 00:26:57.422, Speaker B: Can you remind what is a full rank stress?
00:26:57.598 - 00:27:21.874, Speaker A: Yes. So let's just scroll back up to it. So remember we showed that the stress matrix has rank at most n d one. We say it has a full rank stress, GP has a full rank stress. If there is some equilibrium stress for that framework which achieves equality up in the bound, so it has the maximum possible rank of the stress matrix.
00:27:23.454 - 00:27:30.234, Speaker B: Okay, thanks. Also, is there any hope to simplify the proof of Gortler, Healy and Thurston.
00:27:32.854 - 00:27:51.714, Speaker A: I think. I'm not the person to ask. I don't know if there's any experts on that sort of stuff on who might Lewis Farrand might be a person to ask, or Shlomo or Bob Connelly. But I mean, I don't know of a huge simplification of the proof and it was over ten years ago now, so maybe not.
00:27:53.854 - 00:27:54.994, Speaker B: Okay, thanks.
00:27:56.974 - 00:28:57.154, Speaker A: Okay, so now we understand what the stress matrix is, and we know if we have a maximum full rank stress matrix, then we're globally rigid generically. So we want to, to use this to show another result of Connolly's from the same 2005 paper, which is the one I used on Friday, without proof in one dimensional case, that the one extension operation preserves generic global rigidity. So now I want to prove this. Okay, so it's going to be, there's going to be several results. The first one sort of does the sort of mainly will convince you and then the rest is kind of technical details, I guess. So the first lemma says I take a generic framework with at least d plus one vertices and I form a new graph g by doing a one extension on g. So I delete the edge from v one to v two and I add a new vertex of e zero, and its neighbours are v one, v two and d minus one of the vertices.
00:28:57.154 - 00:29:32.620, Speaker A: Okay, then we already saw, I don't remember which lecture now, but quite early on, that this preserves infinitesimal rigidity. And in particular we could do it. We could preserve infinitesimal rigidity by a geometric one extension which subdivided the edge from v one to v two. And the new vertex was placed collinear to v one and v two. So that's what this sentence says. So part of the lemma is there exists some map q such that the rank of the rigidity matrix for the new framework, g is d more than the rank for the first one. So the rank has gone up by d.
00:29:32.620 - 00:30:31.070, Speaker A: So if we were infinitesimally rigid beforehand, we still would be afterwards. So we know that and we're going to use the same special position, but in the next part to prove the next part. So if omega is an equilibrium stress for Gp and the edge e that we delete has a non zero weighting in the stress, then there exists an equilibrium stress omega dash for g q, whose rank, the stress matrix is one bigger, so it's got one more vertex, so the maximum rank would be one higher. And we're just going to show that the rank goes up by one from whatever it was. Okay, so this is, as I say, as a result of Bob's I'm going to give a very different proof to what he did, but I think I'm giving a simpler proof that than what Bob did. But feel free to have a look at his paper to see the original argument. Okay, so we're going to define g q.
00:30:31.070 - 00:31:10.770, Speaker A: So this is our new framework after we've done the one extension. So put everything at the same place as it was in our starting generic framework, apart from the new vertex which we put on the midpoint of the line through the edge, we delete. Okay, so we talked about this for infinitesimal rigidity. We talked about the collinear triangle giving us a minimal dependent set of rows in the rigidity matrix. And we used that previously to show that the rank of the rigidity matrix goes up as we, we asked. Okay, so we're going to work with the stress matrix. So let omega dash be the equilibrium stress for g q, defined as follows.
00:31:10.770 - 00:31:44.424, Speaker A: So we're going to put the, the stress on every edge of the original graph apart from the one that we're throwing away, is just going to be the same as it was before. We're going to put the stress, let's just see it from the picture. So everything that was over here gets the same stress over here, the new edges. So this is v one and this is v two and v zero in the middle. The edge from v zero to v one gets twice the stress you had here. So that's what we put here. So the point v zero is in the middle.
00:31:44.424 - 00:32:29.430, Speaker A: So if we put a two on on it, and then the half in the direction means we'll match things up and we do the same over here. So v zero, v two has twice what the edge we deleted had, and all the other d minus one edge is instant to v zero, get weight zero. Okay, so now if we look at the picture, we want to check that this actually was an equilibrium stress. The only vertices we have to care about are the end vertices of things that were changed in the graph operations. These ones, they got a zero, so that didn't make any difference. Here we had omega e. So at this vertex we've replaced omega e times that length, that vector difference with a half, a half and a two.
00:32:29.430 - 00:33:12.724, Speaker A: So it's fine here and it's fine here for the same reason, and obviously because the free vertices are collinear and we've just put the same thing on both sides, it's fine at v zero as well, because these were all zeros. So that's a very easy check. So we've defined a sensible equilibrium stress and now we need to show that it has a max, that the rank goes up by one. So if it was maximal before, it still will be okay. Okay, so this is what we're, we're going from. So what I want to do is write down the matrix and do some manipulations, and that's all. So here's the stress matrix for my new stress omega dash.
00:33:12.724 - 00:33:37.604, Speaker A: So this is the row and column for the new vertex. So for example, this was the row for vertex one and this column for vertex two. So that's the edge that we got rid of. So we got rid of it. So it's now become a zero, whereas it wasn't before. On the edge from the new vertex to one, we had omega one two and omega one two. So sorry, I've.
00:33:37.604 - 00:34:13.300, Speaker A: I didn't say this, but I've let omega ij be the ij Venturi. So this omega e up here as this was from v one to v two has become omega one two. This is just so I can have a consistent labeling in this matrix. So this is the edge that we, we split before that was in this zero position here and here. But it's now become twice on each of the two new edges between the new vertex to the ends of the deleted edge. And of course they come here as well. And then roughly everything else, all the sort of stuff over here is unchanged.
00:34:13.300 - 00:34:39.356, Speaker A: Notice that there is an extra bit on the diagonal because of these zeros, has had an effect. But then the new vertex had zero. Everywhere else. Everywhere else is zero. Okay, hopefully everyone's with me that this is the correct matrix. So now all we're going to do is do some row and column operations. So in particular, what I'm going to do is I'm going to use this entry to make this entry back into what it should be.
00:34:39.356 - 00:35:05.354, Speaker A: So I'm going to add half of the first row to the second row. So if I add half of this row to here, then the four halved and the minus two will cancel. The minus two halved and this omega one two will cancel. And the minus two omega one two halved will replace this zero of what we want. Sorry, I kept going in the wrong way. So. And this minus one will turn this one into what we want.
00:35:05.354 - 00:35:53.050, Speaker A: So that was adding a half of the first row to the second. So this fixed this, this fixed this and this fixed this. And then I do the same thing with a half of row one to the third, and then half of this fixes this to zero, a half of this fixes this to what you want, and a half of this fixes this bit to go away as well. So that takes you to this matrix. Probably you could have done it quicker than yourself without listening to me. So if you did that, then great, you end up with this, where everything is now down here is now exactly what it was in the first graph. So we can also then just use this to make this zero and this zero just by doing column operations.
00:35:53.050 - 00:36:42.112, Speaker A: These are all zeros, so the rest of it is completely irrelevant. And that gives us this nice block structure where we have the original matrix and then this one extra row and column with only this one non zero in the corner. So it's very obvious that the rank of this, this matrix is the rank of the original thing plus the rank of this one by one submatrix. But we chose to subdivide an edge omega one where the stress omega one, two was non zero. So because that was non zero, we definitely add a one to the rank and we get the rank change we want. Okay, so hopefully that's okay. But notice that this was at a very special position and it's not so easy to do the what we did before.
00:36:42.112 - 00:37:22.284, Speaker A: So in the rigidity case, if you find a realization where the rigidity matrix has full rank, then you perturb to a generic position. You're happy here. We have to be careful, because if you just take an equilibrium stress, that's maximum rank, at some non generic realization, and you try and perturb it, you might change the infinitesimal rigidity property. So you might change the rank of the rigidity matrix, which would change the dimension of the co kernel. And so you're stressed that you had as maximum rank as you perturb might just disappear. So that's why we had to work with infinitesimally rigid realizations that were full rank as well. And that's what I kind of say in this paragraph.
00:37:22.284 - 00:38:34.740, Speaker A: So if we didn't assume rigidity at the special position, when we move to a generic framework nearby, the rank of the rigidity matrix could go up, and hence the number of dependencies would go down. Hence the dimension of the space of equilibrium stresses would go down. And so the particular stress we have could go away. But there's a result here that I think I should, should assign credit to Connolly and Whiteley, is that if you have an infinitesimally rigid realization, not necessarily generic, and it has rank, full, full rank, n minus d minus one, then any generic framework at all would have an equilibrium stress with the full rank as well. So if you have an infinitesimally rigid realization, then you can perturb to a generic realization. In fact, you can go to any generic realization at all. And then there's another little technical bit that what I want to do the, this one extension operation on edges we have non zero stress.
00:38:34.740 - 00:40:09.994, Speaker A: So I don't want to have a stress of maximum rank where some of the edges are zero, because I might want to do a sequence of one extensions, and at some point I might have to extend on an edge that had a zero in it. And then this argument that I gave in the dilemma doesn't work. But if you, just a little argument I won't go into, you can show that you can choose such an omega dash here, where every edge has a non zero shaft, so it's nowhere zero. Okay? And so then if you put the lemma together with these two parts that I know I didn't really go into the proofs of these two, but putting these two in the lemma together, you can obtain this proposition, which is really the result we're going to use. So if you have an infinitesimally rigid framework, generic or not, you have an equilibrium stress, which is full rank. So rank is mod v minus d minus one, and you get a new graph, g from a one extension, and you take any generic framework of g, then that generic framework is infinitesimally rigid, which is easy, but also it has an equilibrium stress of full rank. So now, now that we have that, we can give the sort of combinatorial results that I want to have, which says if you're a graph however many vertices you want, and you're constructed from a sequence of these one extension operations.
00:40:09.994 - 00:41:00.484, Speaker A: And the starting graph is something that, you know has a, generically has a full rank stress. Then the result of this sequence of one extensions generically has a full rank stress. And hence we can apply Connolly's theorem to say it's globally rigid. So you give me, I mean, kd plus two here is the easiest thing to check, has a full rank stress. But if you want to check any graph for yourself and find it has a full rank stress generically, then you replace that KD plus two with that graph, and you have the same corollary, you get some graph from this sequence of operations. It can be very large, very complicated, but from what we've been talking about, we now know that it's globally rigid in D dimensions for any generic realization.
00:41:00.604 - 00:41:01.264, Speaker D: P.
00:41:02.964 - 00:41:42.404, Speaker A: Okay, so, I mean, I've gone through the proof here, so, but it's really been, been set up by what we've done. So we're going to use induction on n and we'll show that this thing we want to be globally rigid is infinitesimally rigid and has a full rank stress. And hence we'll apply Connolly's theorem to get the global rigidity. So we're generating everything from, in the lemma from KD plus two. So let me just show that one for you. KD plus two has exactly the right number of edges. So it's got the, the Maxwell count plus one.
00:41:42.404 - 00:42:18.264, Speaker A: So it's got the right number of edges to have a one dimensional space of equilibrium stresses. So like in for k four, we can just find a unique stress by just setting one of those entries to a one, say. But if you, you also think about it, kd plus two. So d plus two vertices. And we want to get to rank d plus two minus d minus one, n minus d minus one, which is just one. So as long as KD plus two has an equilibrium stress with a non zero entry in it, it'll have a non zero row in the stress matrix. And so you will have got the full rank you want.
00:42:18.264 - 00:42:49.768, Speaker A: So this bit is easy. And then we use the proposition to complete the induction. So every graph generated by one extensions, we know from the proposition we'll have a full rank stress generically and hence we'll be happy. So the only thing we have to do is worry about edge additions. But if you do an edge addition to a graph with a full rank stress, just assign that new edge to have zero as its stress, and your new thing has a full rank stress. You might worry that then. I was worried about zeros before for the one extensions.
00:42:49.768 - 00:43:18.600, Speaker A: And you want to mix one extensions and edge additions. Yes, so, sorry, I didn't actually say it. So when we're doing the corollary, I did mention edge additions. I didn't say it verbally, but we're putting it in there and when we do need it in there. So you might worry about flipping between the two. But that goes back to this point. I said that you can always take a, take one of these stresses and then up here that you can, when you've got this omega dash that works generically, you can then move it around a little bit.
00:43:18.600 - 00:44:19.142, Speaker A: I guess you can show that it can be chosen to be nowhere zero as well. So we can flip between one extensions and edge additions in our sequence generating our given graph, the graph we are aiming for, G. Okay, so to finish on to just sort of flag what we're going to do next time, I guess, but also say a bit about higher dimensions. So we know now that if you start from KD plus two and you just do some arbitrary sequence of one extensions and edge additions in whatever order you like, as many times as you like, what you get will be globally rigid. So what graphs can you and can't you get from this? Well, when d equals one, we saw last time, we get everything. When d equals two, then we're going to spend several lectures showing that you do in fact get everything again. But that's going to be the purpose of some detailed combinatorial analysis in the next few lectures.
00:44:19.142 - 00:44:59.828, Speaker A: But when d is at least free, you don't. So there exist minimally globally rigid graphs with minimum degree too high to do a one reduction. So minimally globally rigid, I mean, if you delete any edge, you fail to be globally rigid. And if you by one reduction saying, well, I do one extension, I always add a vertex to degree d plus one. So if I wanted to reverse the sequence in the corollary, then I'd have to find a vertex of degree d plus one to reduce by inverse of a one extension. Or I'd have to find an edge to delete because it's minimal, I can't find an edge to delete. And because the minimum degree is too high, I can't find a vertex to reduce.
00:44:59.828 - 00:45:52.654, Speaker A: And so I mentioned a bit about these examples to finish the lecture. So the comment is slightly more general. We showed last time the necessary conditions of d plus one connectivity and redundant rigidity. So if you're globally rigid generically, then you satisfy these two conditions. But for dimension at least three, they're not sufficient to guarantee global rigidity. So just like in the rigidity case, where you have these double bananas that show that d tight is not sufficient to imply d rigid, we have these, there are graphs which are d plus one connected and redundantly rigid, as long as the dimension is at least free, which show which are not globally rigid. And hence this is in general an open problem for these high dimensions.
00:45:52.654 - 00:46:53.450, Speaker A: Okay, so what are the examples? So the first example was found by Bob Connelly in 1991. So he showed that k five five as a counterexample. And so this does tie into Shaun's lecture. The k 55 is a four connected and redundantly rigid graph in three dimensional space, but it's not globally rigid and when you, you have this example, then we can use. Oh, I haven't introduced coning actually, yet, so I'm sorry, I should have, I shouldn't write a, written this, but since I have, and I have three minutes and I've nearly finished, let me say it anyway, so I don't think I have to find coning yet in this course. So if I give you a graph g, then, and I know you want to use Gc to denote the cone of g, which means you add one vertex and you join it to every vertex of g. So unless someone shouts, I haven't defined this in this course, I don't think, but my memory is bad, so shout if I have.
00:46:53.450 - 00:47:58.604, Speaker A: But what's true, and we will talk about this in a later lecture, is that rigidity properties are nicely preserved by this coning operation. So you have your framework g in dimension d, and you have this cone graph g G c. And you think about that in dimension d plus one, then the properties you like are true in dimension d for g if and only if they're true for the cone in dimension d plus one. So it's a result of Connolly and Whiteley that g is globally rigid in d dimensions if and only if the cone of g is globally rigid in d plus one dimensions. I probably should have, should not say this until the coning lecture, but I had written it already, so I've sort of said this vaguely here. But what this means is that we know k five five is a counterexample in three dimensions. So just apply this theorem and take the cone of the graph k five five, you get a counter example in four dimensions and repeated iterate the coning operation, you get counter examples in other dimensions as well.
00:47:58.604 - 00:49:10.954, Speaker A: But at some point, I think in one or two of their papers, Connolly and Whiteley proposed that maybe k 55 was a unique counterexample in Friedi, and there was essentially no other counterexamples in other dimensions. But Frank and Jiang managed to find families of counterexamples for all dimensions greater than or equal to three, which is effectively a bipartite kind of examples. And then more recently, Jordan Kirai and Tani Gower found example infinite families of examples that satisfy the four connectivity and redundant rigidity condition, but are not globally rigid in three D. And so I think their examples were d dimensional as well, but their examples did actually find new ones in three dimensional space. Okay, so like I said, I plan to on Friday, talk about. Start talking about the combinatorics of global rigidity in two dimensions in particular, I'll be talking about combinatorics of circuits in the two dimensional rigidity matriid, and we'll move on from there to global rigidity characterization in full. But that'll be after the rigidity workshop next week and the materials mini symposium, the start of the week after.
00:49:10.954 - 00:49:22.054, Speaker A: So I will stop there and happy to take any questions about anything but. And then after that, maybe we can discuss Jim's and Sean's comments a bit more.
00:49:24.394 - 00:49:35.514, Speaker D: Tony, just one thing I just want to clarify. We talked about counterexamples here. The K 55 is a counterexample. Exactly. What is it? A counterexample? Two.
00:49:35.814 - 00:50:11.624, Speaker A: So K 55 is a four connected graph which is redundantly rigid, but it's not globally rigid in 3D. So. And when you say counter example, you might think that the Hendrickson conditions are if and only if for global rigidity. So I think it probably was believed at some point, maybe in the nineties, that if you were globally rigid, you would be globally rigid if and only if you were redundantly rigid and four connected in 3D. So maybe even Hendrickson himself conjectured that in his paper. I'm not 100% sure of that, but when I say counterexample, it's to this if and only if.
00:50:12.484 - 00:50:17.744, Speaker D: And is it easy to see that K 55 is not globally rigid?
00:50:19.844 - 00:50:42.542, Speaker A: Not to me. So I did not try and draw a second equivalent, but non congruent realization. I think Connolly's paper is very short and what he does is work with the stress matrix. Sean, is it obvious to you from the Bolkarov stuff? There's something about the diagonal of the stress matrix, but I'm not sure how to explain.
00:50:42.598 - 00:50:44.190, Speaker C: What was the question? I was answering an email.
00:50:44.262 - 00:50:50.474, Speaker A: Sorry. Is it obvious why K 55 is a counterexample to Hendrickson?
00:50:51.294 - 00:51:01.830, Speaker C: No, I don't think it is. It's a whole construction, as far as I remember. Very non trivial. For me. It was non trivial.
00:51:01.942 - 00:51:05.702, Speaker A: Yeah, that's roughly what I tried to say for me as well.
00:51:05.838 - 00:51:06.374, Speaker D: Okay.
00:51:06.454 - 00:51:08.422, Speaker A: I was wondering if someone else had the intuition.
00:51:08.598 - 00:51:18.350, Speaker C: The proof is quite nice, if I remember correctly, but it's been a while since I read it. But, yeah, it's a non trivial result.
00:51:18.542 - 00:51:19.314, Speaker A: Yeah.
00:51:22.074 - 00:51:35.674, Speaker B: But wait. I mean, could it been. Could it have been proved by just finding two realizations of K 55 which are the same edge lengths?
00:51:35.794 - 00:51:46.482, Speaker A: Yes. Okay. I think Bob used stress matrices to. That was his intuition for why? Why you'll be able to find them, but I'm not even sure. In his paper, he draws a second.
00:51:46.538 - 00:51:48.494, Speaker D: One, two generic ones.
00:51:50.504 - 00:51:51.404, Speaker B: Okay.
00:51:52.344 - 00:51:54.684, Speaker D: And how do you. How do you write down a generic?
00:51:56.664 - 00:51:57.804, Speaker B: Yeah, good point.
00:51:58.264 - 00:51:59.776, Speaker C: This is where the problems come in.
00:51:59.840 - 00:52:33.980, Speaker D: Like, yeah, I mean, the problem with global rigidity is that you have. What generic means for global rigidity is not at all clear. It's not so easy to say what, what the, um, you know, for. For infinitesimal rigidity, there's kind of a notion of regular, which is easier to understand. But I think the. The notion of, you know, what, what, um, what algebraic equations must be not satisfied is much more complicated for global rigidity. I think.
00:52:34.172 - 00:53:11.424, Speaker C: Yeah, I think we actually got a kind of a count. We got a kind of a weird result with the MLT group that we were looking at something. And basically the reason was because some non genericity had snuck in while we weren't looking, and it gave us a bad result that didn't work, which was quite an interesting example of why genericity is so important. If you're not careful, it can creep up on you. Kind of the lesson from that.
00:53:11.844 - 00:53:43.704, Speaker D: Yeah, yeah, I think. I mean, yeah, it's worth pointing out as well, I think, like, in infinitesimal rigidity, if you find that. If you have a. If you have a graph and you find any non generic rigid framework, then the. The graph is generically, infinitesimally rigid. But it's not so easy for global rigidity. You could find a globally rigid framework with your graph, but you can't then conclude that that implies generic global rigidity.
00:53:43.824 - 00:53:51.524, Speaker A: Yeah. So I showed this example in one of the very first lectures, where you just make a couple of points coincident and strange things can happen.
00:53:54.344 - 00:54:07.944, Speaker C: Yeah. So this is what happened in the example I was talking about as well. It was. It seemed fine, but then, yeah, we had to full rank stress and everything, but we didn't have generic. And that's what caused the problem.
00:54:08.804 - 00:54:13.804, Speaker D: Yeah. I mean, as long as your graph is connected, you can just put every vertex at the same point, and that's going to be globally rigid.
00:54:13.924 - 00:54:14.624, Speaker C: Yeah.
00:54:30.624 - 00:54:40.844, Speaker A: Okay, so thanks, everyone, for coming. I will see you on Friday. But I guess, Jim and Sean, if you want to hang around anyone else who's interested, maybe we could talk a little bit briefly now?
00:54:42.904 - 00:54:46.448, Speaker D: Well, I think, Sean, you have to give a presentation now, Sean, don't you?
00:54:46.576 - 00:54:51.424, Speaker C: Yeah, actually, yeah, I've always forgotten. Yes.
00:54:55.884 - 00:55:24.996, Speaker D: Yeah, no, I was just trying to get my head around that, where the genericity comes into that first step in the. In Connelly's theorem, I guess. I guess I see what you're saying, Shana. Just saying that so the stress, the stress, if the thing is a smooth manifold, if the measurement variety is a smooth manifold, then the stress is kind of a vector in the normal space to that manifold, right?
00:55:25.140 - 00:55:54.672, Speaker C: Exactly. Yeah. This is what it proves. But you only need that it's a smooth manifold because basically you're not having the rank drop down at any point. And also because it's a smooth manifold, you can do like a local diffeomorphism from any point in the manifold to any other point in the manifold just because it's all smooth. And then you do some algebraic who witchery with that and you're fine, basically. But yeah, all you need is that is a smooth manifold to prove it, basically.
00:55:54.672 - 00:56:09.734, Speaker C: And then that shows that every, every element of your equivalent of your configuration space has the same stress. That stress will be a stress of all of them. And that does the trick, basically.
00:56:16.194 - 00:56:28.494, Speaker D: Is that, is that really true? You just need a smooth manifold? I mean, I mean, I know that works for the first step, but then you would be able to write down that. Would you not be able to write down the equations that define kind of generic very easily then?
00:56:29.554 - 00:56:52.604, Speaker C: No, because the step of finding, so the step of finding something which you know that all the points in its configuration space is smooth is not easy to do. That's, that's not trivial. So most of the results to do with that stuff is to reuse Sard's theorem to use existence and stuff like that.
00:56:53.624 - 00:57:01.938, Speaker D: Yeah, but wouldn't it would be enough to say that the, the rank of the jacobian of the measurement map is maximal?
00:57:02.136 - 00:57:24.654, Speaker C: Yeah, but you got to say it's the, it's maximal for every point in your configuration space. So at that point, you're really working out every point in your configuration space, which is not easy. I mean, you can do it hypothetically, but to do it with a concrete example, I don't know how you would do that. You also need this. It doesn't lie on a cone infinity, which I not.
00:57:24.814 - 00:57:26.150, Speaker D: Say that again, I didn't hear that.
00:57:26.222 - 00:57:49.224, Speaker C: You need to say the points don't lie on a cone at infinity, so don't satisfy some strange algebraic equality. Back out the details on that. So I think he uses generic to get around that quite easily. But I think it's fairly easy to avoid that in some ways that's not so much of a problem. But I need to reread the paper again.
00:57:51.124 - 00:58:18.836, Speaker D: Yeah, that bit I understand. The cornet infinity. I understand because. So, so he, first of all shows that if you have another kind of equivalent framework, then the stress condition shows that the other equivalent framework would be a linear image of the original one. And so it could be inequivalent, but it would be a linear image. But then he shows that if it's a linear or an affine. Affine image of the first one.
00:58:18.836 - 00:58:32.862, Speaker D: But if it's an affine image, then he shows that under certain conditions, and it will, the points will lie on this conic at infinity. And then that's not. If the points lie in a conic at infinity, then it's not going to be generic.
00:58:32.998 - 00:59:05.218, Speaker C: Yeah, yeah, that's what he does. I don't know how you would go about if you just have the condition that you're. If we say strongly regular, so everything in your configuration space is smooth. I don't see why that would guarantee that you don't have your points on a conical infinity. That wouldn't necessarily come from that, because he uses genericity to prove that part, but that's fine. So there's also that condition that you need to get around, I think. I can't remember his name.
00:59:05.218 - 00:59:07.614, Speaker C: I'm going to mention him in my talk, actually.
00:59:13.474 - 00:59:14.780, Speaker D: Abdul Fakir.
00:59:14.922 - 00:59:15.816, Speaker A: Yeah, him.
00:59:15.960 - 00:59:26.944, Speaker C: He has some results on this, I think, because he uses general position instead. And he has some results, but I think he has examples where it doesn't go both ways and stuff like this.
