00:00:00.640 - 00:00:00.856, Speaker A: Okay.
00:00:00.856 - 00:00:44.390, Speaker B: Good afternoon, everyone. Welcome to the Fields Institute Quantitative Finance Seminar series for the March edition. This is a flagship event for the center for financial Industries and the financial, mathematics and quantitative finance activities at field. Going on for over 25 years and continuing strong even through the pandemic. We hope that in the fall, we're going to be back in person, but for now, we are pleased to continue in virtual format. And the speaker tonight is Lars Tenthoff from Western University, and we're going to hear about simulated Greeks for american options.
00:00:44.462 - 00:00:47.406, Speaker C: So, Lars, please. Excellent.
00:00:47.430 - 00:00:55.114, Speaker A: Thank you very much. Matthews. It's a great pleasure to be part of this flagship ship operation.
00:00:57.264 - 00:00:57.688, Speaker C: To be.
00:00:57.696 - 00:01:02.044, Speaker A: The captain of that for 45 minutes, I think, is what I have. Is that the way it works?
00:01:03.704 - 00:01:04.648, Speaker C: That's right.
00:01:04.816 - 00:01:06.584, Speaker B: And then have some discussions for it.
00:01:06.624 - 00:01:37.948, Speaker A: Yeah. And again, you're more than welcome to interrupt me as I go along. You should be able to see my slides, so let me know if you can't see those. So, yeah, this. This is a paper I should first of all say, which is joined with my former PhD student, Pascal Letourneau, who's now at the University of Whitewater in Wisconsin. And it's stuff we've worked on for quite a bit. And the reason why it's taken a.
00:01:37.956 - 00:01:41.036, Speaker C: Long time to finish off this work.
00:01:41.100 - 00:01:45.236, Speaker A: Is that it's computationally quite complicated to.
00:01:45.260 - 00:01:48.400, Speaker C: Sort out something that we felt confident.
00:01:48.472 - 00:01:51.832, Speaker A: Presenting to people and confident about enough.
00:01:51.888 - 00:01:53.936, Speaker C: About that we could pitch it and.
00:01:53.960 - 00:02:40.444, Speaker A: Say that this is something we feel would work and something that people could potentially use. So our objective here is to use simulation based methods for calculating the Greeks. So the sensitivities of american options. And since european options are just american options that are never exercised, essentially, or where the optimal stopping time is, the maturity, it will also work for european options. But key here is that it's based on simulation method. When I very, very young and green came to Canada, I had the pleasure of speaking to one of the.
00:02:42.784 - 00:02:43.488, Speaker C: Fathers.
00:02:43.536 - 00:02:51.536, Speaker A: Of simulation for option pricing. So Felin Boyle came to HCC Montreal when I was there at the time.
00:02:51.600 - 00:02:54.176, Speaker C: And I was doing option pricing, and.
00:02:54.200 - 00:03:16.574, Speaker A: He said, that's very nice. Pricing options is very nice, but hedging is really important also. And he compared it to falling in love is the pricing, but then staying married is the hedging. You need to work on that all the time through the life of the option.
00:03:17.794 - 00:03:20.026, Speaker C: So while it's important to price an.
00:03:20.050 - 00:03:33.250, Speaker A: Option, again, hedging and calculating these sensitivities is clearly also important. Greeks for american options can be very different from the european counterpart.
00:03:33.282 - 00:03:35.994, Speaker C: And this even under a standard black.
00:03:36.034 - 00:04:01.566, Speaker A: Scholes type of geometric, brown in motion setup. So the figure that I plot here is from our paper. Our paper is available at SSRN. And if anybody wants it, let me know. I'm happy to share a copy of it. And these are basically the prices and the delta and the gamma of these options. So the sensitivity with respect to the, the underlying asset, or the first and.
00:04:01.590 - 00:04:04.030, Speaker C: The second derivatives of that.
00:04:04.102 - 00:04:24.970, Speaker A: So, sensitivities, Greeks, that people would want to know and get an idea about what they are. The red line shows the prices in Greeks for american options, and they look very different. So this is an option that has a strike price of $40. So whenever the stock price is way.
00:04:25.002 - 00:04:29.194, Speaker C: Below $40, the american option would essentially.
00:04:29.234 - 00:04:39.082, Speaker A: Just be exercised if it's too low right away. And that's why you get these kinks in the Greeks.
00:04:39.178 - 00:04:41.234, Speaker C: And that, of course, makes it somewhat.
00:04:41.274 - 00:04:48.894, Speaker A: Difficult to work with these. And they are, again, and this is key, they're very different from what they would be in a european option.
00:04:51.754 - 00:04:53.814, Speaker C: I mean, this is important also, because.
00:04:54.874 - 00:05:20.478, Speaker A: Prices, as you know, as you know, they're observed in the marketplace. You can get a price of an option, but whenever somebody's presenting you with a greek derivative, one of the Greeks, it's essentially an estimate of these sensitivities. As an academic, I've been working with data providers like option metrics, that will provide you with huge sample, a long.
00:05:20.526 - 00:05:22.966, Speaker C: Sample of option data.
00:05:23.150 - 00:06:30.530, Speaker A: And they also report Greeks in their data. And when you read the description of how they actually calculate these Greeks, what they do is they invert the black Scholes formula if it's a european style option, or they invert the binomial tree constant volatility model if it's an american option. And then they calculate the Greeks based on those implied volatility measures. So again, it's essentially a Greek that comes out of a constant volatility model that has been calibrated to the option price. It's not a Greek that's based on some sort of more elaborate model than the constant volatility model, or even something which would be non parametric, sorry, not non parametric, but model free. And so it's not similar to, say, the way we would calculate the VIX for the S and P 500. So, brokers, also market makers, will need Greeks for all of the options that they're traded, because option traders would want to have those.
00:06:30.530 - 00:06:45.634, Speaker A: You want to have good estimates of Greeks. If you're doing risk management, the better the Greeks are, the more realistic they are, the better you expect the management of any kind of risks involved in option positions to be.
00:06:47.174 - 00:06:50.206, Speaker C: And while what I'm doing here is.
00:06:50.310 - 00:07:09.446, Speaker A: We'Re taking market traded options as the object of interest, in reality, the method that we develop here can be used for any other type of products that have option like features. It doesn't have to be an actual standard option. You could, you could calculate Greeks of.
00:07:09.470 - 00:07:12.030, Speaker C: Any kind of other, I mean, all.
00:07:12.062 - 00:07:32.926, Speaker A: Other types of options that you would, you would come across multiple exercise options, whatever you would, you would, you would feel like. And it, you know, the method, the method should work, and the method should work because it really only relies on being able to simulate the underlying data generating process that you use for, for pricing.
00:07:32.990 - 00:07:35.814, Speaker C: So as long as you can simulate.
00:07:35.854 - 00:07:53.954, Speaker A: These risk neutral dynamics, the model that we develop can essentially be used. It doesn't rely on being able to solve anything in any other way, doesn't rely on being able to construct.
00:07:56.574 - 00:07:57.014, Speaker C: Finite.
00:07:57.054 - 00:08:33.626, Speaker A: Difference methods or whatever you want to, you want to do. It's purely relying on simulation. So that's why we think there's some obvious use for this type of model. Now, here's a short review of what we actually do, in case I don't have that much time towards the end to tell you about it. So we develop a method that we can use to estimate the Greeks for american style options. And it relies on nothing but flexible simulation based methods. And we combine those with initially dispersed state variables.
00:08:33.626 - 00:08:53.422, Speaker A: So I'll explain how we do that. But you can basically think about this. That instead of simulating from the value of the underlying asset right now, we spread out possible values of the underlying assets, even though we know what the value is today, we say, okay, assume that it had been something else. That's our initially dispersed state variables, and.
00:08:53.438 - 00:08:57.114, Speaker C: Then we price the option based on that.
00:08:58.814 - 00:09:03.438, Speaker A: And all that's involved in then getting the Greeks is basically a cross sectional.
00:09:03.486 - 00:09:11.534, Speaker C: Regression at time zero. And the difficulty there is to basically.
00:09:11.614 - 00:09:13.342, Speaker A: Think carefully about that regression, how we.
00:09:13.358 - 00:09:15.558, Speaker C: Should think about that regression, how we.
00:09:15.566 - 00:09:30.490, Speaker A: Can use that regression and results from the literature to prove that the method actually converges. So we do that, and we also, just for the fun of it, provide some numerical estimates of convergent rates.
00:09:30.522 - 00:09:37.090, Speaker C: And what's key here is that we can actually get optimal rates.
00:09:37.122 - 00:10:13.374, Speaker A: So for anybody who's used to working with simulation based methods, you get root n convergence rates. And we can, we get the same thing here. So without, you don't have to be that careful. You have to be a little bit careful. About how you pick things. Can't pick the initial state dispersion too large, but you can actually then get optimal convergence rates. So we show that that can numerically be obtained, and then we present a simple, and I call it consistent.
00:10:13.374 - 00:10:39.514, Speaker A: And consistent is not as an estimator as such, it's more that it actually works. It's a two stage method that works for finite choices of the number of simulated paths and any other key choices of variables that you put into this method, and it automatically controls imbalances. We write here off the bias of the estimates against the variance. Any kind of estimate here is going to be potentially biased, and it's going to have some variance.
00:10:40.134 - 00:10:42.366, Speaker C: And I'll talk a little bit more.
00:10:42.390 - 00:10:45.478, Speaker A: About how you can trade off bias for variance.
00:10:45.646 - 00:10:54.774, Speaker C: And our two stage method does that automatically in such a way that we deliver basically for a wide range of.
00:10:54.814 - 00:11:39.636, Speaker A: Options, estimates that are insignificantly different from the true value source estimates that we can believe and we can use for something. And again, it's robust. We show that across a bunch of alternative specifications, and it actually also compares exceptionally well with existing alternatives. We can discuss if it's exceptional, but it's definitely very well. As the author of the paper, I'm probably a little bit biased here, but anyways, I'll show you some results on that. And then the final thing we do is we apply the method to a realistic case with time varying volatility. So, I grew up with discrete time.
00:11:39.700 - 00:11:46.132, Speaker C: Models and specifically models formulated in the Garch framework.
00:11:46.188 - 00:11:48.780, Speaker A: So we apply that to real life.
00:11:48.812 - 00:11:52.452, Speaker C: Data, and we demonstrate that there are.
00:11:52.468 - 00:12:33.586, Speaker A: Large differences between estimated Greeks in different classes of models. So some of you will be familiar with the class of affine models. And affine models are nice to work with because they have closed form solutions for european prices, and also european, or the Greeks for these european option prices. So we use that and compare our simulated method to that and say that it generalizes. But then, since our simulation based method does not rely on being able to solve this in almost closed form or any way, we can calculate Greeks for non affine models or any other kind of models, and we find that not only are the large differences in prices, but the Greeks are also very, very different.
00:12:33.650 - 00:12:35.906, Speaker C: So if you were to say, okay.
00:12:35.930 - 00:12:43.106, Speaker A: I'm going to accommodate for time varying volatility and affine type of garage models, because I know that constant volatility is.
00:12:43.130 - 00:12:51.868, Speaker C: Wrong, then you still should potentially worry about whether or not the affine framework.
00:12:51.916 - 00:12:55.076, Speaker A: Is actually the right framework to use. And if it's not, then you will.
00:12:55.100 - 00:12:58.704, Speaker C: Still make big errors when you estimate the Greeks.
00:12:59.924 - 00:13:02.756, Speaker A: Greeks also vary significantly through periods of.
00:13:02.780 - 00:13:04.996, Speaker C: Crisis, which is something that we can.
00:13:05.020 - 00:13:27.752, Speaker A: Now start to think about more carefully, since we have a model that allows for volatility to vary. And the bottom line of it is that if you were to use those Greeks that I was talking about before, that data providers will give you that are calculated based on misspecified models, then the errors that you make can be extremely large.
00:13:27.808 - 00:13:30.464, Speaker C: And if I was a practitioner going.
00:13:30.504 - 00:14:03.264, Speaker A: Around and hedging my option positions, and I was to use those Greeks, I would potentially be, even though I thought I was hedged hugely, exposed to risks through these sensitivities. So what's the state of the, not the state board. Let's compare to some of the literature that's there. So, to motivate why we keep talking about american options.
00:14:05.004 - 00:14:05.892, Speaker C: Well, think about the.
00:14:05.908 - 00:14:16.204, Speaker A: Fact that if you had, if you knew. So of course, the key challenge in an american option is to get the optimal stopping time to estimate when I should actually exercise this option.
00:14:16.284 - 00:14:18.692, Speaker C: If I know that, then that option.
00:14:18.748 - 00:15:07.676, Speaker A: Is no more difficult to price or get the Greeks off than a european option. I could use simulation to do that. And there are a couple of methods. Basically, if you look at glass and Land's book, then you'll see that it's fine. Finite difference method. There's the pathwise derivatives method, there's the likelihood ratio method. Finite difference methods are simple to use, but they may not always give reasonable Greeks in particular, because if I am simulating data, simulating american options, continuity of these, continuity of the prices with respect to any of these parameters that are in there, is not necessarily guaranteed in these settings with a finite choice of paths.
00:15:07.676 - 00:15:19.904, Speaker A: And you may get end up getting something that is just pure rubbish. Pathwise derivatives requires that you can differentiate the payoff. So you can use that to get the delta, but you cannot get the gamma.
00:15:20.804 - 00:15:22.172, Speaker C: There's a bunch of other Greeks you.
00:15:22.188 - 00:15:36.614, Speaker A: Can'T get, unless, depending on what the payoff of these things, of the option is. So it's not as flexible. And then there's the likelihood ratio method, which differentiates the PDF instead of the payoff. And I would work generally.
00:15:38.994 - 00:15:39.618, Speaker C: For every.
00:15:39.666 - 00:16:21.726, Speaker A: Type of option payoff that you're thinking about, but it may not be very efficient. The Peterbark paper and the kennel and Tompaidis and Semiliano paper all basically assume that I know the early exercise strategy and then they do these things. There are a few papers, and this is where we contribute, that consider the joint problem of estimating the stopping time and the Greeks, I list a few here. Some of them use the methods that listed above. Some of them use other properties. The Wangen caftisch paper is the paper that's most similar to what we do. So, compared to the Wang and caftisch.
00:16:21.750 - 00:16:24.382, Speaker C: Paper, what we do is we provide.
00:16:24.478 - 00:16:25.394, Speaker A: An actual.
00:16:27.094 - 00:16:28.390, Speaker C: Proof of convergence.
00:16:28.462 - 00:16:39.766, Speaker A: We estimate the convergence rates and we derive this two stage method, which we think is, which is robust, and which.
00:16:39.910 - 00:16:43.110, Speaker C: Turns out to, in our numerical examples.
00:16:43.182 - 00:17:14.724, Speaker A: Be more robust than the Wong and cathles method is. And besides that, we also report greeks on a lot of other things than Hwang and Kaflisch did, since they only essentially, if I remember correctly, talked about delta and gamma. So, sensitivities with respect to the underlying assets. Since we're moving into the time varying volatility model, we also naturally talk about vega and bomber and banner. So I'll present you some results on that.
00:17:14.884 - 00:17:18.276, Speaker C: Okay, so I'll go quickly through the.
00:17:18.300 - 00:17:43.549, Speaker A: General setup of simulation based pricing. So if I have an american option at any point in time where I can exercise that option, I would need to figure out if I should get the payoff value set, or if I should hold the option and get the holding value f, and the price of the product or the option at any given point in time is then the maximum of these two values. And that basically gives us this optimal.
00:17:43.581 - 00:17:48.045, Speaker C: Stopping time algorithm, which is described here.
00:17:48.069 - 00:17:55.174, Speaker A: And that tells us when should we along a given path, when should we then exercise? And key here is to figure out.
00:17:56.154 - 00:17:58.610, Speaker C: An expression for this holding value function.
00:17:58.642 - 00:18:34.740, Speaker A: We typically won't have that in closed form. So what we do instead is we have to estimate that and the method we choose to work with. We could have worked with other methods, but a standard method in the literature nowadays is to use the least square's Monte Carlo method of Longstaff and Schwartz from 2001, published in Review of Financial Studies. And in that paper, what they do is they say that holding value, I can approximate that at any given point in time using a cross sectional regression where I have in my n paths.
00:18:34.772 - 00:18:38.304, Speaker C: That I've simulated, I take future payoffs.
00:18:38.644 - 00:18:56.474, Speaker A: So the payoffs that I've estimated along a given path, and I regress that on basically a basis of current state variables. So x here are the state variables. So in a typical option that would be the stock price. If I have a model with time rank volatility, be the stock price and.
00:18:56.514 - 00:19:02.898, Speaker C: Volatility, and I would basically regress future.
00:19:02.946 - 00:19:43.274, Speaker A: Payoffs on some function of this. And we're going to use simple polynomials here, but you could use stuff that's a lot more elaborate for the convergence theorem that we prove. It's nice if you work with polynomials, especially if you work with polynomials formulated in deviation from the current value of the stock price, that becomes super easy to work with. If I'd started all my paths at a given price, the price being what the value is today. So the parentheses here should actually be here. So I apologize for that, that's wrong. It would just be the average of these future payoffs.
00:19:43.274 - 00:20:13.998, Speaker A: So this set, n comma tau hat, is the payoff from exercising the option at the estimated optimal stopping time along a given path. But in the general case that I was alluding to, where I initially spread out these stock prices, I can't average these things because they're going to be generated from different, different initial stock prices. So I need some more, I need to think more carefully about this. And one way to think about this is each of these being a realization.
00:20:14.086 - 00:20:17.446, Speaker C: Of the value of the stock, of the option.
00:20:17.510 - 00:20:20.254, Speaker A: For some, if the initial stock price.
00:20:20.294 - 00:20:22.750, Speaker C: Had been whatever it was used when.
00:20:22.782 - 00:20:37.568, Speaker A: We started this, and then you could think about those as helping us approximate the price function at time t. And that's how we're going to be thinking about these things. And if I can approximate this function here, then I can evaluate it at.
00:20:37.656 - 00:20:40.416, Speaker C: Any, if I evaluate it at the.
00:20:40.440 - 00:20:55.680, Speaker A: Current value of the stock price, I get the price. And if I differentiate it once I get the delta, if it's with respect to the stock price, and I can get the gamma, and basically, since x can be any of the state variables, I can get all the Greeks. So with respect to any state variables.
00:20:55.712 - 00:21:00.036, Speaker C: That basically affect the, affect the stock price.
00:21:00.180 - 00:21:08.748, Speaker A: So you wouldn't initially, you know, it wouldn't immediately be possible to get the Greeks with respect to time, for instance. So since time is not a state.
00:21:08.796 - 00:21:11.860, Speaker C: Variable in here as it is right.
00:21:11.892 - 00:21:16.452, Speaker A: Now, that's not really something. But the other thing. So if volatility is a state variable.
00:21:16.508 - 00:21:22.424, Speaker C: Then you can get the, sorry, the vega.
00:21:22.724 - 00:21:38.912, Speaker A: And you could actually, even in a, in black scholes model, you can initially spread out the volatilities, even though you know, volatility is constant and therefore shouldn't really be a state variable. You can still get the vega of the option in a black Scholes world.
00:21:39.088 - 00:21:43.480, Speaker C: So this regression here, how can I think about that?
00:21:43.592 - 00:22:00.014, Speaker A: So we can think about that simply from the point of view of, let's assume that I have some price function here, and these, this is then this p of x, I can use the Taylor series approximation or an expansion to approximate this price function. And then I naturally have my greeks, right?
00:22:00.054 - 00:22:03.622, Speaker C: Those are the p one, p 02:00.
00:22:03.798 - 00:22:06.594, Speaker A: P.M. Those are the derivatives that I'm after.
00:22:07.094 - 00:22:10.190, Speaker C: And if I do a weighted least.
00:22:10.222 - 00:22:15.326, Speaker A: Squares regression, if I take my future payoffs along all of these paths that I passed, that I realized, and I.
00:22:15.350 - 00:22:20.466, Speaker C: Regress those on the state variables in.
00:22:20.490 - 00:22:23.474, Speaker A: Deviations from their true value, the value.
00:22:23.514 - 00:22:25.850, Speaker C: At which I want to estimate prices.
00:22:25.882 - 00:22:27.794, Speaker A: In greeks, well, then, these b ones.
00:22:27.834 - 00:22:33.094, Speaker C: Here, basically very closely connected to the.
00:22:34.434 - 00:23:19.056, Speaker A: Derivatives that I want and simple to get. So essentially, I'm going to estimate, I'm going to do this repression, and all I'm going to do is I'm going to keep the coefficients, and those are going to be my estimate probably transformed. Those are going to be my estimates of the greeks. So we can prove using local polynomial regression theory. And this is the theorem that's in the paper, that under some assumptions about how we generate the data, if the initial spread, the initial spread, the size of the initial spread is governed by this alpha. If alpha tends to zero, and the number of simulated paths tends to infinity such that n times alpha to the.
00:23:19.080 - 00:23:21.936, Speaker C: Power of three tends to infinity.
00:23:22.000 - 00:23:31.992, Speaker A: So alpha can't basically shrink to zero too fast, then the estimated I derivative of the option value. So that's the sensitivities that I had.
00:23:32.008 - 00:23:38.388, Speaker C: In the previous equation are asymptotically unbiased. And if, and this is a further.
00:23:38.436 - 00:23:54.652, Speaker A: Restriction on how fast alpha can go to zero, if this holds. So I, here is the number of the derivatives. So if I'm going after the price, I is zero. If I'm going after the delta, which.
00:23:54.668 - 00:24:01.100, Speaker C: Is the first derivative, I is one. If I'm going after a higher order.
00:24:01.132 - 00:24:14.334, Speaker A: Derivatives, then I become, I order. If this is satisfied, then the estimated price sensitivities, then the variance is also ten to zero, and the estimates are essentially means we're convergent.
00:24:14.874 - 00:24:20.254, Speaker C: Okay, so if I combine this with.
00:24:21.394 - 00:24:36.184, Speaker A: Basically this lemma here that says that under certain assumptions, I actually estimate my stopping time the right way, then I now have, and there's, in the paper, there's a full blown proof of that.
00:24:36.644 - 00:24:40.588, Speaker C: Demonstrating that under certain assumptions, as n.
00:24:40.636 - 00:24:51.144, Speaker A: Tends to infinity, alpha shrinks m. So the number of regressors that I use here, sorry, I'm going back and forth. M tends to infinity also, but not too fast.
00:24:52.204 - 00:24:55.212, Speaker C: I actually get in the limits, I.
00:24:55.228 - 00:25:00.814, Speaker A: Get convergence to the true underlying prices and greeks.
00:25:01.594 - 00:25:05.494, Speaker C: Okay, so this means that you can.
00:25:05.914 - 00:25:08.786, Speaker A: Rest assured that the method actually works.
00:25:08.930 - 00:25:11.458, Speaker C: So, convergence rates here.
00:25:11.506 - 00:25:13.094, Speaker A: So, I want to show you this.
00:25:13.474 - 00:25:18.922, Speaker C: To demonstrate that this comment from before.
00:25:18.978 - 00:25:38.470, Speaker A: That you get almost optimal rates here. So the theoretical results provide lower bounds. It could be faster, and it turns out to actually be faster than that. So we set up so that everything is satisfied. So n tends to infinity. Alpha has to shrink in a way.
00:25:38.502 - 00:25:42.054, Speaker C: That is governed by these numbers here.
00:25:42.094 - 00:26:05.374, Speaker A: So we increase m when we do the stopping time regressions quite slowly, so that the estimated stopping times will converge and we increase, or we shrink alpha with a function of p. So the larger p is, the faster alpha shrinks. And then we look at how the convergence rates are.
00:26:07.154 - 00:26:08.658, Speaker C: For different values of.
00:26:08.706 - 00:26:31.958, Speaker A: P and m naught. So m naught is basically the order of the regressor that I use at time zero. The table here shows these rates. So top panel is across values of p. So for m equal to nine. So I do use a 9th order polynomial at time zero. This is a black scholes model.
00:26:31.958 - 00:26:39.334, Speaker A: So geometric, brown in motion. So the underlying acid is the only thing that varies. So, I only have delta and gamma.
00:26:39.374 - 00:26:40.514, Speaker C: That I'm looking at.
00:26:42.054 - 00:27:15.878, Speaker A: I know what my true price delta and gamma is for these options. I estimate that with a black schole, sorry, with the binomial tree with 50,000 steps in it. And then I plot for various different m. So you'll see down here, m goes from 123-4567 up to eight, and as such, also naturally then increases n. For each of these values, I calculate what is my bias, what is my standard deviation, what is my rmc? And then I do a log log.
00:27:15.926 - 00:27:20.214, Speaker C: Regression of those biases, standard deviations and.
00:27:20.294 - 00:27:27.574, Speaker A: Root mean squared errors on the number of paths. And if I have, quote unquote optimal.
00:27:27.614 - 00:27:30.774, Speaker C: Convergence, I should expect the slope of.
00:27:30.814 - 00:27:49.490, Speaker A: That in that regression to be minus one, two. And for convergence, what we should be looking at, I guess, is the rmse. And you see that basically all of these Rmse convergence rates estimated, granted. So there could be and maybe should be a standard deviation, a standard error.
00:27:49.522 - 00:27:52.866, Speaker C: On these estimates, but they're all below.
00:27:53.050 - 00:28:11.870, Speaker A: -0.5 so faster essentially than root n convergence. Now, this holds for most of the, for the prices across NEp and Nem. Now, if you look at the delta and the gamma, you will see that.
00:28:11.902 - 00:28:17.862, Speaker C: If p is too low, remember that p had to be quite large in.
00:28:17.878 - 00:28:36.560, Speaker A: The sense that p is essentially what governs this thing up here. So p has to be quite large for this to actually converge. And if p is too small, delta and gamma doesn't converge, and that's to be expected. But if p is reasonably large, say eleven, the convergence rates there are for delta quite close to 0.5.
00:28:36.632 - 00:28:40.400, Speaker C: And if p increases to 21, then.
00:28:40.592 - 00:28:43.264, Speaker A: Even gamma has almost root n convergence.
00:28:43.304 - 00:28:48.280, Speaker C: Rate across M. So this is for.
00:28:48.312 - 00:28:54.894, Speaker A: P equal to eleven. Down here you see that you can actually have situations where if you have.
00:28:54.934 - 00:28:58.942, Speaker C: Too large an m, convergence rates start.
00:28:58.998 - 00:29:10.526, Speaker A: Shrinking back down again. So there's a balance between how large an m you should pick and how large to get the fastest possible convergence rates. But if you pick a reasonable number.
00:29:10.550 - 00:29:14.990, Speaker C: Of m, you can get almost root n convergence rates.
00:29:15.022 - 00:29:17.514, Speaker A: And if you do a large p.
00:29:17.894 - 00:29:19.952, Speaker C: You can basically get that.
00:29:20.118 - 00:29:24.988, Speaker A: We're quite happy about that. So you're not sacrificing necessarily a lot.
00:29:25.036 - 00:29:26.924, Speaker C: On this initial regression.
00:29:26.964 - 00:29:30.024, Speaker A: You can still get almost optimal convergence rates.
00:29:31.004 - 00:29:33.372, Speaker C: So that's asymptotically.
00:29:33.468 - 00:30:08.116, Speaker A: So what about if I want to apply this method? Suppose I want to do this with 100,000 paths instead of an infinite number of paths. And I do not want to have an infinite number of regressors either. I want something that I can use. Well then the theory. So local polynomial regression theory. So we use that again, as I said that before, to prove this. This is non parametric regressions, this theory that tells us that these how this work and how these coefficients can actually be used to approximate derivatives.
00:30:08.116 - 00:30:36.650, Speaker A: And that's how we get the proof of our theorem. We know that the variance is inversely proportional to the density of the initial state variable at x. So you want something that is peaked around the current value of x, where you want to estimate things in LPR, you accommodate that by having kernels that have appropriate properties. We're in a situation here where we decide ourselves how we're going to simulate.
00:30:36.682 - 00:30:39.122, Speaker C: The data, because we decide on how.
00:30:39.138 - 00:31:11.704, Speaker A: To initially disperse this data. And we're going to disperse this in a way that it's naturally peaked around the value where we want to estimate the delta. We need it to be spread out somewhat so we can, we can estimate these derivatives of the function, but it has to be peaked. You do not want to do something which is uniform or something which is bimodal even or anything, because that's going to decrease, sorry, that's going to increase the variance of whatever estimate that you have. The variance is proportional to the variance of the data. So the data that I use in my regression at t equal to zero.
00:31:12.004 - 00:31:13.892, Speaker C: So the more noisy that is, the.
00:31:13.908 - 00:31:41.370, Speaker A: More noisy my estimates are. That seems very reasonable. So that's the motivation for implementing variance reduction techniques. And we have a specific variance reduction technique here, which we show works extremely well for these greeks. And if I have time, I can talk a little bit more about that. And then the choice, the trade off between variance and bias, that if you're familiar with non parametric approximations and regressions.
00:31:41.482 - 00:31:43.610, Speaker C: Of descent, there's always this trade off.
00:31:43.642 - 00:31:50.094, Speaker A: So if I have a very large, sorry. If I increase my bandwidth, then.
00:31:53.074 - 00:31:53.386, Speaker C: I'm.
00:31:53.410 - 00:32:27.004, Speaker A: Going to increase my bias. But if I'm going to increase my bandwidth, I'm going to decrease my variance. So I want to have a large variant, a large bandwidth to decrease the variance of my estimators. But if I have a large bandwidth, I get biased estimates. So I want those to, or the bandwidth to be small. And bandwidth here translates into how large the initial state dispersion is. There's a trade off between spreading those numbers out very widely to get low variance or keeping them kind of close by to get low biases.
00:32:27.004 - 00:32:28.100, Speaker A: But if I get a low bias.
00:32:28.132 - 00:32:29.990, Speaker C: I get a high variance, I spread.
00:32:30.022 - 00:32:35.674, Speaker A: Them out, I get a low variance, I get a high bias. So there's a trade off between that. And it turns out you could use.
00:32:36.134 - 00:32:41.470, Speaker C: Simple rule of thumb rules for how to select bandwidth.
00:32:41.502 - 00:33:11.908, Speaker A: And then that's how we build our two stage method. So here's some results in just a naive method that says, okay, let me simulate some data with some initial spread. This alpha here is an initial. So the larger alpha here is, the larger the, the spread is. It doesn't completely translate into dollar spreads on the stock price, but it's almost like that. So these are small spreads. These are very large spreads.
00:33:11.908 - 00:33:26.364, Speaker A: And again, we stay within a binomial model. We get prices, deltas and gamma. And what you see is very low spreads. I get huge variances on my greets, so large that they are essentially completely useless.
00:33:26.524 - 00:33:30.124, Speaker C: If I have large spreads, I get.
00:33:30.204 - 00:33:37.100, Speaker A: Low variances, but I get very biased results, right? So I can get lucky and choose.
00:33:37.132 - 00:33:39.252, Speaker C: Something that's in between, for which I.
00:33:39.268 - 00:34:16.538, Speaker A: Get reasonable estimates of the Greeks, not only the deltas, but also the gammas of this option. And, you know, reasonable standard deviations. They're like, you know, in the ballpark of what the standard deviation would be on the actual price estimate. But that's somewhat lucky, right? So what we need is a method that can be used across like that doesn't, it doesn't depend on the initial choice here. So the method we develop is a two stage method. And I'm going to give you the results here. To show you that if I start with an alpha of 0.5,
00:34:16.538 - 00:34:31.226, Speaker A: the method now, because it's a two stage method and it adapts, it's actually going to go in and say that alpha was way too small, I'm going to increase that for you. And then it's going to generate estimates that are still reasonably close to the.
00:34:31.250 - 00:34:36.534, Speaker C: True value and have much lower standard deviations.
00:34:37.354 - 00:34:54.178, Speaker A: And again, you can pick them so low that it becomes difficult to get. You still have something which is biased. But if you start out with an initial value, even if that initial value, initial spread is very large, you using our method, two step method, you basically.
00:34:54.346 - 00:34:59.162, Speaker C: Get the estimates that are as unbiased.
00:34:59.258 - 00:35:09.704, Speaker A: As the estimates were before, but now you have reduced the variance. So here I've got a variance of 0.0043 in the naive method.
00:35:10.684 - 00:35:12.676, Speaker C: Sorry, I'm making a mess here.
00:35:12.700 - 00:35:26.944, Speaker A: So it's slightly larger but the bias is much lower. That's what I should have said here. But look at these numbers here. That's the numbers I wanted to say. So standard deviation here, 0.53, that's now down to 0.029. So the method adapts.
00:35:29.644 - 00:35:30.324, Speaker B: Just a question.
00:35:30.364 - 00:35:30.964, Speaker A: So I understand.
00:35:31.044 - 00:35:43.324, Speaker B: So going back to that table, so the alpha that you have there is the alpha that you start with and then you do this two stage and then you're going to end up using an effective alpha that is different, right?
00:35:43.444 - 00:35:43.980, Speaker A: Yes.
00:35:44.092 - 00:35:48.344, Speaker B: Do we have a sense of what that effective alpha is for each of those cases?
00:35:48.804 - 00:36:00.500, Speaker A: Yes. So since it's based on the rule of thumb, it's not going to give you the exact same alpha, irrespective of which was the initial alpha.
00:36:00.612 - 00:36:03.284, Speaker B: I just wanted to know like by how much the meta.
00:36:03.324 - 00:36:05.044, Speaker A: I believe that this alpha in this.
00:36:05.084 - 00:36:09.580, Speaker C: Situation is around to ten or somewhere.
00:36:09.612 - 00:36:10.844, Speaker A: Between five and ten.
00:36:10.964 - 00:36:11.744, Speaker C: Okay.
00:36:12.924 - 00:36:34.540, Speaker A: But that's something that's going to depend on the maturity of the option, the volatility of the option, it even depends on the strike price. And it just turns out that this two, this choice that we have work quite well, irrespective of those variables. So. Yeah, so great question. Exactly what we're doing.
00:36:34.572 - 00:36:37.952, Speaker C: Right, so we set, we know that.
00:36:37.968 - 00:36:47.296, Speaker A: We need to set a wide initial dispersion to get good estimates of the stopping time. So we do that and then we estimate the stopping time and then we.
00:36:47.320 - 00:36:50.624, Speaker C: Use variant reduction to get the data.
00:36:50.664 - 00:37:25.244, Speaker A: At time t equal to zero. And we use that to find this optimal bandwidth that you were talking about, an alpha hat or whatever you want to call it, and that's going to depend somewhat on how large the initial state dispersion was. And then we're going to use that alpha to scale all of our simulated paths back so that they lie in that optimal bandwidth. So, basically, you're multiplying them by a coefficient, and then you're going to use the stopping time you already estimated. So you don't need to estimate that again.
00:37:25.784 - 00:37:27.472, Speaker C: And then, based on the payoff that.
00:37:27.488 - 00:37:33.012, Speaker A: You get from this method, you're going to estimate the prices and the Greeks in that cross sectional regression at time zero that I.
00:37:33.128 - 00:37:39.308, Speaker C: That I showed you. And that works quite well.
00:37:39.356 - 00:37:50.476, Speaker A: It works well across these three options. It works well across a bunch of other options. We've got robustness checks with that, showing.
00:37:50.500 - 00:37:52.252, Speaker C: That it holds for a whole sample.
00:37:52.268 - 00:38:05.250, Speaker A: Of options with different maturities and options with different choices. Strike price. Sorry. Of number of paths. So, this is all estimated with 100,000 paths.
00:38:05.282 - 00:38:07.274, Speaker C: I should say that this is all.
00:38:07.314 - 00:38:34.672, Speaker A: Estimated with 100,000 paths and a 9th order regression polynomial to get the. To get the optimal stopping time and a 9th order polynomial at time t. So, yeah, so you could have chosen fifth order polynomials. You could have chosen 250,000 paths or a million paths. And the method we have, once it's applied, is robust to the choices of these things.
00:38:34.728 - 00:38:37.792, Speaker C: As long as you choose, essentially not.
00:38:37.848 - 00:38:40.184, Speaker A: Too high a polynomial, for too low.
00:38:40.224 - 00:38:48.244, Speaker C: A number of simulated paths, it'll adapt. Okay, so.
00:38:51.424 - 00:39:05.500, Speaker A: I got to speed up. I've got some more slides I can tell you. If anybody wants to know a little bit more about how we actually. What the two steps are. We can talk more about that at the end. But I wanted to jump to. Well, actually, before I jump, I wanted to just compare with the existing literature.
00:39:05.572 - 00:39:05.828, Speaker C: Right.
00:39:05.876 - 00:39:11.984, Speaker A: So here we've got these three options that I've been talking about. We've got our two stage results that are down here.
00:39:12.484 - 00:39:16.944, Speaker C: We've got the partial differential model.
00:39:17.684 - 00:39:28.924, Speaker A: So the PDM, which is the one where you differentiate the. Sorry, not the partial, the pathwise derivative method, you can't get the gamma there, differentiating.
00:39:30.704 - 00:39:33.484, Speaker C: The option payoff, so you can get the delta.
00:39:34.304 - 00:39:36.616, Speaker A: Interestingly enough, in this application here for.
00:39:36.640 - 00:39:40.192, Speaker C: This choice, and we're trying to implement.
00:39:40.248 - 00:40:16.084, Speaker A: This in the way that it was implemented by Peter Berg. And we get something which is quite close to the true values, but it's biased because the standard deviation are very low. So low, actually, that they give us a biased results. There's the likelihood ratio method, which has much larger standard deviation, so it's less efficient, which is what I alluded to before. But still works quite well. Then we've got this modified least squares Monte Carlo method, which is the Wong and Caflich method. And in Juan Kaflisch's paper, they have a suggestion for what that initial bandwidth.
00:40:16.124 - 00:40:16.984, Speaker C: Should be.
00:40:18.684 - 00:40:32.744, Speaker A: Not a two stage method to calculate it. It's a heuristic, it's a suggestion based on observations of the data of how to pick alpha and in the options that we have, that suggestion just doesn't work.
00:40:32.784 - 00:40:37.284, Speaker C: It leads to significantly biased results.
00:40:37.984 - 00:40:48.008, Speaker A: So essentially, coming out of this is the likelihood ratio method. And our methods are the ones that deliver for this sample and for many other samples that we tried.
00:40:48.056 - 00:40:51.754, Speaker C: Also statistically unbiased estimates of delta and the gamma.
00:40:51.794 - 00:41:05.354, Speaker A: And if you compare the standard deviation of the estimates that we get with either the LRM or the two stage method, ours are consistently lower than the LRM. So by doing this trick of adapting.
00:41:05.434 - 00:41:08.466, Speaker C: And rescaling, we end up getting methods.
00:41:08.490 - 00:41:56.526, Speaker A: That are about as, I mean, where the bias is the same order of magnitude, but where the standard deviation is much, much lower. That's the reason why we think that our method compares very well to the existing methods. And again, this holds for a much larger sample. And I'm happy to show you some tables that not only talks to the robustness of the method, but also justifies that it works well for different choices of parameters. But for the rest of the time, and you cut me off whenever you, you think it's fair. But if I could just have like two, three, four minutes to talk about this, this application with time rank volatility, that would be great. So what I have here is a plot of the VIX from end of.
00:41:56.550 - 00:42:01.846, Speaker C: 1999 to 2020, end of that month.
00:42:01.990 - 00:42:59.916, Speaker A: You see the global financial crisis, you see the kickoff of the pandemic here. And it's obvious we all know that variances are not constant through time. So what happens if I was to think about pricing options and getting my greeks in different periods, low volatility, high volatility. And in particular, what happens if I was to use our method, which allows, can be adapted to the situation? I'll show you that first. And if I compare that to the choice of, say, assuming that volatility is constant, but it's just high or low in these different, in these different regimes. So how can we think about that? So the first thing is what type of volatility? So I'm going to, as I said, I grew up with Garch models, so that's what I'm using. I've got a method here, or, sorry, I'm writing up here, the particular formulation of Hestan and Nandy.
00:42:59.916 - 00:43:11.724, Speaker A: So Heston and Nandy has this particular form of the Garch model, which renders it affine. And you can actually calculate closed form solutions for european option values. You can also get the european greets.
00:43:12.424 - 00:43:16.752, Speaker C: Our method can be used to, you know, any kind of general type of.
00:43:16.768 - 00:43:21.376, Speaker A: Garage model that you might have. So does it work then? And I hope you can see this table.
00:43:21.440 - 00:43:26.896, Speaker C: So these biases here should be estimates.
00:43:26.920 - 00:43:32.604, Speaker A: They should not be a bias. It's not that the bias is the order of the estimate. If you look at these.
00:43:33.024 - 00:43:34.896, Speaker C: So this is the benchmark values, the.
00:43:34.920 - 00:43:56.404, Speaker A: Price, the delta, the gamma, the vega, the gamma, the banner. So first and second derivative of the stock. First and second of the volatility, and the cross derivative stock and volatility. And then what you would get if you were doing this with our method. So now I've got stock prices and I've got volatilities, and I do an initial state dispersion, a bivariate initial state dispersion.
00:43:56.824 - 00:44:00.924, Speaker C: Then I do my choice of bandwidth on each of these margins.
00:44:01.424 - 00:44:11.002, Speaker A: So I could do it more efficient if I jointly pick the bandwidth of these things. So if I, instead of basically creating a square initial spread, I could create.
00:44:11.058 - 00:44:16.426, Speaker C: Something that was better shaped. But this is the simplest possible way.
00:44:16.450 - 00:44:23.774, Speaker A: And it's very likely you could do much better if you cared about it. And the fact is that it still works quite well.
00:44:25.234 - 00:44:26.778, Speaker C: Pick the number of paths here.
00:44:26.826 - 00:44:39.516, Speaker A: 100,000 is basically what we did, what we've had and been working with up until now. And what you get is basically a bunch of, these greeks are very close to the true Greeks, and they're not statistically different.
00:44:39.660 - 00:44:41.012, Speaker C: The vama is.
00:44:41.188 - 00:45:02.034, Speaker A: So the vama is the second derivative with respect to the volatility. The vama is statistically significant with this number of paths. If you increase the number of paths, and it kind of makes sense that if you've got bivariate regression, you need more paths, you've got more regressors, right? You actually get something which is statistically insignificant.
00:45:02.534 - 00:45:04.934, Speaker C: Okay, so going forward, what we're going.
00:45:04.934 - 00:45:16.534, Speaker A: To do is we're going to up the number of paths a little bit to 250,000. And we're going to be using that going forward. This shows that we can actually generalize this type of method to the bivariate case.
00:45:16.654 - 00:45:21.798, Speaker C: So then the question is, what happens if the world is not an affine world?
00:45:21.846 - 00:45:30.538, Speaker A: It was actually a regular garchomp. So here's a table that compares european options with Hestan Mandy's. This is the affine garch that we could get.
00:45:30.666 - 00:45:32.282, Speaker C: These are the estimates that you saw.
00:45:32.298 - 00:45:40.314, Speaker A: In the table before. What if this had been an american option? If it's an american option, I don't have any closed form solutions. I don't have any way of calculating these things, so I need to simulate.
00:45:40.354 - 00:45:43.010, Speaker C: Those again, as expected.
00:45:43.202 - 00:45:47.534, Speaker A: We know that from the european options. We saw that in those plots I showed you.
00:45:48.474 - 00:45:53.758, Speaker C: Greeks are very different, right? It can be very, very different between these options.
00:45:53.806 - 00:46:04.838, Speaker A: Look at the vana here -0.44 for an in the money american option, -0.07 for the same money ness european option. So very, very different.
00:46:05.006 - 00:46:05.914, Speaker C: Greeks.
00:46:06.614 - 00:46:18.386, Speaker A: That's great. That's interesting, right? It shows that we need methods that can actually get the Greeks of the american options. We can't really rely on using the the Europeans Greeks as approximations.
00:46:18.530 - 00:46:20.614, Speaker C: But what if it had been a different model?
00:46:21.074 - 00:46:37.094, Speaker A: So here's the straight up Ngarch model, which is not affine, empirically seems to fit option data much better than the affine model does, but doesn't promise any kind of closed form solutions for european options. And in any case, it wouldn't even matter if we have american options.
00:46:37.394 - 00:46:38.506, Speaker C: What about the Greeks there?
00:46:38.570 - 00:46:48.200, Speaker A: Well, they're still different. They can be very different. They are like of the same order of magnitude, but they can be different. Right.
00:46:48.232 - 00:46:52.280, Speaker C: The difference between 0.2 and 0.17 or.
00:46:52.432 - 00:46:53.924, Speaker A: What else are big differences?
00:46:54.304 - 00:46:56.008, Speaker C: There are some differences along the line.
00:46:56.056 - 00:47:05.056, Speaker A: And it's not the same using these two methods. The last thing, this table shows that these methods supposed to have asymmetries. What if it was, if they didn't.
00:47:05.080 - 00:47:09.228, Speaker C: Have asymmetries, you know, Greeks would be very different.
00:47:09.276 - 00:47:23.900, Speaker A: So the last thing I'll show you and then I'll finish up, is how about doing this through time? So I showed you this plot here of the VIX, and the orange line here is the backed out conditional volatility.
00:47:23.972 - 00:47:26.220, Speaker C: That a garage model would have if.
00:47:26.252 - 00:47:52.000, Speaker A: It was matched to this Vix value. So I'm cheating a little bit here because I don't want to estimate the model necessarily. So I'm basically using my VIX data to back out what the volatility would be and the time series of volatility. So this HT has an average value of 19.5%. So very close to the 20% that I've implicitly been working with, has a minimum of four and a maximum of.
00:47:52.032 - 00:47:55.648, Speaker C: 90, and basically is highly skewed with.
00:47:55.656 - 00:47:57.352, Speaker A: A median of 17%.
00:47:57.528 - 00:48:00.368, Speaker C: And if I rounded my volatility through.
00:48:00.416 - 00:48:08.728, Speaker A: Integer values, then the mode would be around eleven. So, huge differences and very varying, very volatility that varies a lot through time.
00:48:08.776 - 00:48:13.320, Speaker C: So we try and assess that in this Nguyen model.
00:48:13.352 - 00:48:15.248, Speaker A: And we pick a model that's then.
00:48:15.296 - 00:48:19.296, Speaker C: Started at a 10% annual volatility.
00:48:19.360 - 00:48:20.712, Speaker A: So we pick age as if it.
00:48:20.728 - 00:48:25.576, Speaker C: Had been implicitly 10% annually, or if.
00:48:25.600 - 00:48:31.384, Speaker A: It was very high, 40. So we don't go to four or 90, we go to ten and 40.
00:48:32.324 - 00:48:35.084, Speaker C: And then we compare these results here.
00:48:35.164 - 00:49:14.746, Speaker A: For the Nguyge model with what would have happened if I had assumed that it was actually a constant volatility model, but it was started at different levels of volatility. And again, what you see is huge, huge differences. Some of these estimates, some of these banners, or in particular are very, very large. If I was in a constant volatility model, in low volatility regime, much larger than they would be if it was actually a time varying volatility model that was generating this. So I'm not saying that the Nguyen model is the right model, but it does accommodate for time varying volatility. And if it had been something that.
00:49:14.770 - 00:49:18.250, Speaker C: Was closer to real life, then the.
00:49:18.282 - 00:49:32.656, Speaker A: Errors that, then these would be the more appropriate greeks to think about. And the errors that you would make as you go through life and hedge your option portfolios with the constant volatility.
00:49:32.720 - 00:49:40.880, Speaker C: Implied peaks would be huge, very, very large. And essentially, you would be thinking that.
00:49:40.912 - 00:50:11.068, Speaker A: You had a hedged portfolio position and options. But essentially, we would be hugely exposed to any of these greeks. So that's summarized up here. Again, what did we do? We developed this method, we proved convergence. We established this two stage method. We show that it's robust, and then we hopefully provide some interesting results with what happens if volatility is actually time varying, which we think. So this would be the empirical, relevant application of our model.
00:50:11.068 - 00:50:16.164, Speaker A: So I will stop now. I only went seven minutes over time, so I apologize for that.
