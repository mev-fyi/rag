00:00:00.280 - 00:00:22.714, Speaker A: Okay, again, I hope slides are okay. I hope everyone can hear me. So, okay, so let me start again by thanking you all for accommodating this talk even, because again, Singapore, there's 12 hours time difference. And I do apologize for not really being able to attend the talks. But I mean, somehow the conference starts 09:00 p.m. singapore time. So it's, somehow it's very, very difficult.
00:00:22.714 - 00:01:05.082, Speaker A: Okay, so in any case, today I'm talking about kind of some new things we are working on together with Yongsheng from the National University of Singapore. So it's going to be about a non commutative kind of extension of Lisson's algorithms for NMF, but we're kind of tailoring it now for the positive, semi definite matrix factorization problem. Okay, so, okay, so, okay, so this talk is going to be about an algorithm for PSD factorization. So let's kind of first start by defining what this problem is. So, okay, so in this setting, right, so we are given as input, you know, like a matrix x. It's a rectangular matrix m times n and its enter is non negative. So the entries of x are non negative.
00:01:05.082 - 00:01:53.046, Speaker A: Okay, so that's our input. And we're also given as input some kind of user specified parameter r. Okay, so this r is some natural number, you know, you know, 2345 or your favorite natural number. So these are the two things we have and what we are, what we are kind of asked to find is a family, two families of PhD matrices. So we have the a's and the b's. And we have, notice we have one, one of the a's for each of the rows of x. And we have one of the b's for each of the columns of n, for each of the columns of x, okay? And these guys, they need to be positive, some are definite and their size needs to be r times r, the user specified parameter, okay, and the main thing that we're asking from these two families of matrices is that, you know, for every, for the IJ entry of matrix X, I want that, I want that to be equal to the tracing product between AI and Bj.
00:01:53.046 - 00:02:31.054, Speaker A: Okay, so that's, that's uh, that's the goal here. Okay, we're trying to find these two families of pastime matrices that somehow recover the, the entries of the matrix x, okay? And again, r is fixed, okay. It's some kind of, some, someone has told me, you know, find a five dimensional phase de factorization. So that's what we're looking for. And now it's kind of easy to see that, you know, you can always, you know, you know, if your r is like, you know, less than minimum, minimum number of rows and number of columns, you can always find the PSD factorization. That's not that difficult. Okay? But what is more difficult is to find the smallest star for which your matrix actually admits another dimensional PSD factorization.
00:02:31.054 - 00:03:49.144, Speaker A: And that is known the PSD rank of x. Okay? It's a very, very nice, very cool quantity, and because it kind of related to all kinds of stuff, but my two kind of favorite applications. So the first one is that PSD factorizations in general, so they kind of capture, you know, the expressive power of some of the definite programs against some of the programs. You know, it's just linear optimization over the cone of PhD matrices, okay? So, and what I mean by that, you know, there's some kind of this canonical correspondence that, you know, if you have some polytope, you kind of associate the matrix to it, the so called slack matrix. And then, you know, PSD factorization of the slack matrix, they somehow, they give you a way to express your polytope as a projection of enough and slice of like a PSD cone of the size of the p's de factorization, okay? But somehow, also there's some kind of very cool applications also somehow very, very close, very interesting connections between P's de factorizations and somehow quantum and quantum information theory. Okay? So somehow, again, if you have like some kind of matrix corresponding to a probability distribution, a PSD factorization kind of gives you a way to realize this thing as some kind of quantum protocol. Okay? But again, you know, I don't really have time to go into the details, but somehow, okay, this kind of PSD factorization is kind of relevant for kind of many areas.
00:03:49.144 - 00:04:26.783, Speaker A: So both optimization and information theory. And perhaps, you know, the more geeky mathematical reason is that somehow PSD factorizations are like a non cumulative analog of non negative matrix factorizations. And, you know, what are, so what are these? So what is a non negative matrix factorization? So it's basically the same thing. It's kind of the same thing as we had before. You know, we have like a matrix x, which is enterprise non negative, and some, you know, user specified a parameter r. But before we were trying to find PSD matrices that somehow we're kind of realizing the, you know, x in this, but in the context of NMF, we are trying to find non negative vectors. R dimensional entry was non negative vectors.
00:04:26.783 - 00:05:33.170, Speaker A: And we have, you know, the a's and the B's, the a's kind of label the, the rows of x. The b's label the columns of x. And what we, what we're asking for from these vectors is that, you know, the adj entry of x is, is given by the inner product between AI and BJ, okay? So this is like the NMF problem. And, you know, if you just think about it for a second, you kind of see that nmfs, they're basically very, very, very special PSD factorizations. And it's exactly the case when you kind of restrict the PSD factors, the AI's and the BJ's, to be diagonal PhD matrices, okay? Because diagonal PhD matrices, they're nothing but non negative vectors. So, okay, so like, you know, so NMF is basically basic. It's like a, you know, special case of PhD factorizations, okay? But somehow, but nmfs are somehow like, you know, very, very popular, I would say much more popular compared to PSD factorizations, okay? And they're basically, and the popularity is mostly because somehow you can kind of use them as a dimensionality reduction tool, okay? And that kind of, you know, brings about many, many cool applications, you know, in areas like, you know, document clustering, musical analysis and many, many others.
00:05:33.170 - 00:06:33.956, Speaker A: And the reason somehow why NMFs somehow come into play is somehow because they give this kind of very beautiful parts based representations of your input data, okay? So, so meaning that kind of, you kind of explain your data by kind of adding, by kind of aggregating some like a finite number of atoms. Now, again, this is kind of vague, and if time allows, I will kind of meditate a bit more on this. I will explain more precisely what this means. But somehow, again, NMF is very important problem beyond optimization. It's very important in engineering and has many, many applications. We introduced the PS de factorization problem and somehow we also introduced NMF and realized that there's a special case of the PS de factorization problem kind of slowly work towards kind of, you know, kind of coming up with an algorithm for, for finding PSD factorizations, okay? And so again, and in this context, again, always remember that, you know, r, so the size of the p of the PSD matrices. So that is fixed.
00:06:33.956 - 00:07:25.324, Speaker A: Okay? So we have kind of made a choice. You know, r is like five or seven or two or whatever, okay, so we have made a choice and now we are trying to find another dimensional PSD factorization. So now, okay, so obviously like, no, the canonical starting point would just be, you know, you know, to set up some kind of non convex problem where you kind of, you, you kind of minimize the square laws over all the entries of your matrix. But of course there's so much you can do with this. But again, the standard thing you might do is you're going to try some kind of coordinate descent approach to solve this problem. And probably the first thing you would try if one showed you this, it would do some kind of alternate optimization between the ace and the b. So, so you kind of alternate between minimizing with respect to the, to the a's, okay, it's this N PSD matrices, a one up to a n.
00:07:25.324 - 00:08:06.874, Speaker A: So in this context, like in the first program here, the b's are fixed and I'm just minimizing over the a's. And after I update my a's I kind of minimize again over the b's. Okay, so just do kind of coordinate descent where I kind of have two blocks, the s and the b's. So that would be kind of a natural thing to do, right? So and again, I don't want to be talking about both of these things, so let's just focus on one of them because everything is entirely symmetric. So for concreteness, let's just say we have updated the AI's. So the AI's are fine and now we need to worry about the bis, the BJ's. Okay, so I kind of need to, need to solve this problem.
00:08:06.874 - 00:09:20.800, Speaker A: And then you know, you kind of stare at this for, you know, for a second and you basically realize that, you know, this guy is separable with respect to the bjs. Okay? So, okay, so in the end of the day, you know, you basically just need to solve this for every, you know, for each, and here you only have one matrix variable, you have the pjs, okay? And just, you know, just to, just to kind of simplify notation, let's just omit the dependency on j and you know, just, I can just rewrite this as minimum x minus ab euclidean norm squared. Okay, where here this is just all, just simplify notation. Where this small x here is just the jth column of the matrix x. These are exactly all the entries that appear here. And script a is this linear mapping that you fitted a symmetric matrix of size r and it spits out the inner products of your matrix B with a one, a two and an fixed PhD matrices that we have updated already in the previous step. Okay, and you know, this norm is just the usual euclidean norm, okay, and, okay, and again, hopefully you see that this is just notation.
00:09:20.800 - 00:09:42.578, Speaker A: Okay, so this is just exactly the same as this, as this program. Okay, so long story short, right? So doing this kind of coordinate descent approach, right? So this is the type of thing we need to solve. Okay, we need to solve this. I mean, it is iteration, but we need to solve an optimization problem of this form. Okay, so, so again, any questions, comments, please, just interrupt. Okay. Feel free to interrupt.
00:09:42.578 - 00:10:30.974, Speaker A: I'm not really keeping track of any of the chats, so. Okay, so now these like subproblems we encounter, they are convex. And in fact this is what we would call like a comics quadratic SDP. But, okay, I mean they are convex, but we don't really have a close form solution for these guys. So in the end of the day, if this was like your subroutine and your coordinate descent, you would need to be using some kind of numerical optimization to solve this. Okay? And somehow there is some work out there like dealing with how to find PSD factorizations and all of them, at least to the extent that I'm aware of. So this is basically the starting point, doing coordinate descent, ending up with a problem that looks like this.
00:10:30.974 - 00:11:14.440, Speaker A: And in one case, let's say in this paper by van der Ley, Giles and Gliner, they give two algorithms for this problem. So this was the first paper. So somehow the algorithms, they give, they're kind of simple, right? So they give like the first one is like a projected gradient method where, you know, you take a gradient step and then you project back into the PSD cone. The second method is like a method, but that kind of also allows you to keep track of the rank of the PSD factors. Okay. Which is something I haven't really talked about because the idea here is that in the piece of factorization, so for somehow the way I introduced the problem, I only fixed the size of the matrix. So the size is r, and that's fixed.
00:11:14.440 - 00:11:22.312, Speaker A: It's like, it's like five. But perhaps, you know, and depending on the application you are looking at, you might also want to talk about the rank of the PSD factors.
00:11:22.408 - 00:11:25.264, Speaker B: Okay, so, you know, can I ask you a question?
00:11:25.424 - 00:11:26.724, Speaker A: Yeah, yeah, go ahead. Yeah.
00:11:27.224 - 00:11:33.554, Speaker B: So you're saying this is a convex problem, but you're restricting the rank to the r and it's still conduct.
00:11:33.714 - 00:11:39.706, Speaker A: No, no, I haven't restricted the r. I haven't restricted anything. Yeah, so when I say convexity, I'm talking about this problem here.
00:11:39.730 - 00:11:40.562, Speaker B: Oh, okay, sorry.
00:11:40.618 - 00:12:21.298, Speaker A: Yeah, yeah, yeah. So I'm, so I'm just saying here. I'm just talking about the second algorithm here. Okay. I'm just saying okay, first of all, I'm saying that depending, there are certain applications where besides, okay, so that you want to talk about the, you want to find the PSD factorization of size r, but you also want to say something about the rank of the PSD factors, okay? So, you know, somehow low rank and in this context. So basically you will go here and you would do like, you know, the, you know, like the typical, let's say bureau montero change of variables. So, you know, you replace b with like, you know, like a huge transpose where the intermediate dimension is the rank you care about.
00:12:21.298 - 00:13:26.034, Speaker A: And this is exactly what they do here. Okay? And then they basically solve that using coordinate descent with respect to all of these new variables that you introduce. Okay? So that's what's happening here. And in like some kind of later papers by Fevold, which works a lot on music applications in musical analysis, together with Vincent Tan and some other collaborators, they give some other algorithms for solving, like this guy here. And these algorithms, basically they kind of just come from the fact that somehow they kind of realize that this is the type of thing that you get working in signal processing when you work with problems like alpha and rank minimization, facing trivial and stuff like that, problems of this kind of type. These are the type of relaxation that you kind of end up with, basically. So they kind of look into the signal processing literature, and they kind of dig out algorithms from, uh, for affine rank minimization and phase interval, and they kind of apply it to this, to this problem.
00:13:26.034 - 00:14:20.064, Speaker A: So these are like the kind of the existing, uh, things that are out there. So, um. Okay, so, um, like, okay, so, but now what we're going to do is it's going to be kind of, kind of different, okay? So we're going to be using, we're going to, again, we're going to try to solve this problem, but using the majorization minimization approach, uh, which I'm guessing some of you might have heard maybe in the context of gaussian mixtures, probably. That's a type of thing many people have probably seen. Okay, but in any case, in one slide and without any pictures. So the Mm approach, on a very high level, it's kind of a very generic method for solving an optimization problem. So let's say you minimize f over some set scripts, script x, okay? And you know, if you want to apply the Mm approach, what you need to find is you need to find the so called auxiliary functions.
00:14:20.064 - 00:15:12.534, Speaker A: So you need to find a formula of auxiliary functions. So let's say one for every, for every x in your domain, okay? And the auxiliary functions, they just, they need to satisfy the following two things, okay? So the first one is what we call the domination property. So the auxiliary function just needs to be, to lie entirely above your function f. Okay, so f of y is upper bounded by ux of y for every y. But the second thing is that, is that at the specific point x, when you evaluate the auxiliary function at x, it needs to touch your function, okay, so these are like the two things, okay, that you need. And as long as you have these two properties, then I mean, essentially you kind of have an algorithm, but again this is low, like high level, right? So and the algorithm is the following. It's like, you know, you have your case iterate, which is, which is xk.
00:15:12.534 - 00:16:24.224, Speaker A: And then what you do is like you kind of set up the auxiliary function corresponding to xk, you minimize that, and that is the xk plus one, okay? So that is your new iterate. And I mean, and it's like, you know, it's kind of trivial to see that. Like if you do that, you know, this kind of update rule, somehow along the trajectories of this update rule, if you, if you will. So your, your, you know, the function f is non increasing, okay, so f of x k plus one is upper bounded by, by f of x k. Okay, so, okay, but again, like now of course now to make this work, you know, there's a lot of things here, obviously here, you know, somehow the auxiliary functions need to be tractable in some way to be able to even kind of calculate, you know, this, you know, x k plus one. There's, then there's also all kinds of convergence issues because, okay, so here we do, we kind of see that f is not increasing in value, but you know, there's issues whether x is actually converging to a minimizer and stuff like that, but, okay, but that's basically all I'm going to need from MM at the moment. Okay, so this is basically definition, this is like the MM framework, okay? It's just a genetic framework.
00:16:24.224 - 00:17:26.370, Speaker A: Okay, but now I'm just going to show you how we can actually apply the MM approach to, again, to solve this kind of subproblem of interest that we encounter when we try to do PSD factorizations. Okay, so again, the first thing we need to do is, again, we need to find an appropriate family of auxiliary functions. So that's the first thing we need to do, right, okay, so now like the baby lemma, which kind of motivates somehow the core, I guess this is like step number zero. Okay, which is going to lead us to the, to the algorithm is that, you know, if you could find like a linear mapping, you know, which is invertible, so that, you know, t is upper, you know, t minus a transpose a is positive. So in other words, you know, t, t is at least a transpose a in the, in the PhD order. Okay? So then the function of interest is non increasing with respect to this update rule. Okay, so, and this is actually just very easy.
00:17:26.370 - 00:18:03.952, Speaker A: And this just follows, just follows straight from the definition of the MM framework because, right, so, you know, this is our objective function. Okay, so, and again, notice that I'm not worrying about psdns at this stage. It's like I completely forgot about the psdness and I'm only like minimizing over symmetric matrices. So this is like unconstraints. So this is my, this is my objective function. And, you know, I'm just going to do a teller expansion at the old, okay, so, and I'm just going to get this identity because my function is a quadratic and then, right, so, but then I am assuming that t is larger compared to a transpose a. And that's, that's going to give me basically this inequality.
00:18:03.952 - 00:18:34.474, Speaker A: Okay? Okay. So essentially I just replace a transpose a with t here, which is fine because t is larger compared to a transpose a. And obviously if you plug in b, all for b, we have equality. Okay, so these guys here. So this is really a valid family of auxiliary functions. Okay? And again, if I just follow the recipe of the, you know, the MM approach, right? So the MM approach tells me that your new iterate is the minimizer of the auxiliary function of the old iterate. Okay? And of course that's something you can calculate and this is what you're going to get.
00:18:34.474 - 00:19:05.274, Speaker A: Okay? So again, if you had this t, t larger compared to a transpose a. So then again, you can kind of bound the Taylor expansion and you just minimize that. And this is, this is the type of thing you get. Hopefully this makes sense. Again, any questions, feel free to ask, but fine, that's okay. But that's definitely not really what we want because of course we are going for psdness. I'm trying to find a PSD factorization of my matrix.
00:19:05.274 - 00:19:46.264, Speaker A: This type of update definitely is not going to guarantee any type of psdness, even if they all die terrace be old was PSD. I mean, there's just no way that b new is going to be PSD. So then the way we're going to try to enforce PSD miss is by basically enforcing additional structures. We're going to ask more things on the linear map t. For now we only ask that t is largely compared to a transpose a in the learned order. But now we're going to ask two more things. And you know, I mean, somehow, kind of no surprise, we're going to basically ask that t kind of acts as like congruence with a positive definite matrix q.
00:19:46.264 - 00:20:20.066, Speaker A: So that's the first thing, that's the first additional kind of requirements. And the second requirement is that, you know, when I, when I hit a transpose a b o with t inverse, I want to get exactly b. Okay, so, so, okay, so these are the two things, I mean, and they look technical, but I mean somehow let's, let's, it's kind of, let's see where these things come from. So somehow they don't come from nowhere. They basically come from this update rule which we had. Now again, remember T was linear. So kind of, I can kind of distribute t.
00:20:20.066 - 00:20:58.136, Speaker A: And now, you know, you see that condition three basically says essentially that, you know, the first, the first two terms here, they're going to cancel. They're equal to zero. And then, so the only, the only thing that remains is that B Nu is equal to t inverse acting on a transmose x. So this is what remains just by using three. Okay, so if T satisfies three, so then this is my updates. Okay, and now the last thing here to notice is that, you know, if you kind of work out what a transpose X is, well, a transpose X just basically forms this conic combination of the AI's. Okay? And remember, the AI's, they have been updated in the previous step and they are PSD.
00:20:58.136 - 00:21:19.560, Speaker A: So X, the vector X is supposed to be the jth column of the input matrix, which is non negative. So this is a conic combination of positive semi definite matrixes. So it's a self, this is a PSD matrix. And then basically I have a PSD matrix and I hit it with t inverse. But I have assumed that t inverse kind of access is congruence with a PD matrix. So psdness is preserved. Right.
00:21:19.560 - 00:21:59.090, Speaker A: So somehow we are in good shape if t satisfies these two traditional properties. So then this is the update. And this update is PSD. Okay, so I hope that makes sense. Um. Okay, so, so, uh. Okay, and then, and then basically the last thing is that, right, so again, kind of summarizing right? So if we had a t that satisfies basically these three things, so then somehow we are in good shape, right? So essentially we have, we have an mm, algorithm, okay, and, okay, and, but then of course, you might wonder, you know, is it, you know, can you actually find a t that kind of satisfies all of these things? And as it turns out, you know, not only you can, but some, but it's also uniquely specified.
00:21:59.090 - 00:23:04.942, Speaker A: Okay? And in the sense that, so this q here is going to be uniquely specified. Okay, so the q matrix. And that's going to follow essentially by the definition of the matrix geometric mean. Okay? Which is, you know, algebraically, this is the following, right? So if you, if you have two positive definite matrices, so you know, this is the matrix geometric mean. Okay, kind of algebraically, but I guess more, kind of more relevant for us is the following definition that if you kind of set up this kind of ricotta equation in the matrix variable x, again c and b are positive definite, then this beast here, the geometric minus is the unique positive solution of this matrix equation. And then if you just go back one slide, if you basically combine these two things, right? So you'll see that, you know, if you subs, if you substitute t inverse with, you know, with, you know, as, you know, congruence with, with, with q inverse. And then you use equation three, you basically get that, you know, q actually satisfies this equation here.
00:23:04.942 - 00:23:26.610, Speaker A: Okay. Or in other words, by exactly, by the unicity process property of the geometric mean, you basically see that q is nothing but, you know, b o, the inverse geometric mean. A transpose a, b odds. Okay, so it's the, so q is the matrix geometric mean of these two guys here. Okay, so, okay, so that's, that's, that's. Yeah. Okay.
00:23:26.642 - 00:23:31.974, Speaker B: Yeah, so you've got a, I guess. Yeah. Time is going.
00:23:32.474 - 00:23:34.314, Speaker A: Okay. Okay. Sorry, my phone.
00:23:34.354 - 00:23:38.294, Speaker B: Take another two, three minutes and maybe.
00:23:39.074 - 00:24:17.036, Speaker A: No worries. Yeah, so thanks. Thanks. Okay, so q is uniquely specified. And again, it's given by, by, by the mean between build and a transposition builds. Okay? And basically, that's basically, essentially we have the algorithm, right? So, so, and again, I mean it's, it's a bit of a mouthful, but all of this is saying this is just the formula, right? So it's just, you know, t inverse acting on a transpose x, if you remember that x is the j column of, of your input matrix. And then if you also remember that we are, you know, we asked for t inverse to be, to be of the form, you know, you know, so this is going to be q inverse acting on n transpose y, q inverse and q.
00:24:17.036 - 00:24:57.324, Speaker A: We kind of agree that it's given by this geometric mean. So this is basically the algorithm. Okay, so this is the algorithm you get for updating your PhD matrices. Okay. And of course there is, okay, so I don't really have time to go through the details, but of course there is something remaining to show, which is somehow the non trivial part of the, I mean, I guess the technical part of the paper is that somewhere you still need to show the domination property for this specific choice of Q. You get this mapping and you do need to satisfy, again, a is fixed and you do need to satisfy that the domination property, meaning that t is larger compared to a transpose a in the PSD order. Okay.
00:24:57.324 - 00:25:19.512, Speaker A: And I mean, that takes some work. So, but somehow, you know, you're going to need to invoke like libs concavity theorem and, you know, formal master's inequality and, you know, pass things together in a nice way and it kind of works out beautifully. Okay. Okay. And maybe some comments about this algorithm. Okay. It's like kind of simple and easy to implement.
00:25:19.512 - 00:26:07.796, Speaker A: Right, so this is really, this algorithm is really simple, right. Because I mean, it's basically, you know, it's just this formula and it's just a matter of having a subroutine for calculating geometric means. So there's no, there's no projection on the PhD cone or anything like that, at least not explicitly. Okay. The psdness of the Atlantic is automatically okay. Another cool thing is that if you actually look at fixed points of the update rule, they are basically points that satisfy the KKT conditions of the optimization problem we are trying to solve. And another very cool thing, which we like a lot and we are trying to explore in applications, is that this algorithm actually also respects block diagonal piece de factorizations, meaning that if you initialize with a certain, you know, because you do need to initialize, and if you initialize with a certain block diagonal pattern.
00:26:07.796 - 00:26:49.874, Speaker A: So, you know, this pattern is going to kind of, you know, it's going to remain, there's an invariant throughout the execution of this algorithm. Okay, so this is going to kind of allow you to calculate block diagonal, place de factorizations, which are relevant also in quantum and for, for various reasons. Okay. I don't know. And if I have, like for my last slide, I guess, uh, because I'm out of time, uh, I'm not going to do Lee song any justice. Uh, Lee song's algorithm is, is somehow it's like super, super useful and super, uh, uh, widely used algorithm for calculating non negative matrix factorizations. Okay? And this is like, you know, and this is, you know, this is how it looks like.
00:26:49.874 - 00:27:19.948, Speaker A: Um, and again, it's kind of an mm algorithm. So it has like all the properties you, you can expect an MM algorithm to, to enjoy. Okay? But the cool thing is that, you know, if you basically just take our algorithm and you just specialize on two diagonal PhD matrices, okay? So you just recover, you recover Lisson's algorithm. And again, I'm not doing lisson any justice here. I mean, this paper is like super important. It's like more than 10,000 citations. Okay? But again, special case of our algorithm, when you plug in diagonal piece the matrices, you do recover.
00:27:19.948 - 00:28:00.974, Speaker A: Listen. Okay, okay. And you know, we have some numerics. You know, somehow things are nice, but we're still kind of trying to apply this for more, like more and more examples and two things we are working on. And I mean, I guess maybe I focus on the second one is that we basically, we have been able to kind of extend this also for arbitrary symmetric cones. Okay? So if you were trying to find PSD factorization, I mean, conic factorizations for symmetric cones, so then somehow, you know, things kind of work out. Although, although now you have to go through the older kind of Jordan and algebra language.
00:28:00.974 - 00:28:08.474, Speaker A: Okay? So that's all. I'm probably one or two minutes late, so I apologize for that. But thanks a lot and please, if you have any questions, please go ahead.
00:28:09.174 - 00:28:10.454, Speaker B: Thank you very much.
00:28:10.614 - 00:28:11.094, Speaker A: Yeah.
00:28:11.174 - 00:28:22.966, Speaker B: For a very interesting talk. And I guess we have time for one quick question. We also will have time for questions after Anthony's talk, so hopefully you can stay around.
00:28:23.150 - 00:28:24.160, Speaker A: Yeah, no problem.
00:28:24.262 - 00:28:26.784, Speaker B: Yeah. Anybody have a quick question?
00:28:30.924 - 00:29:20.660, Speaker A: How would this compare to just off the shelf convex optimization? I mean, I don't know, that's, that's probably hard to say. I mean, you're probably going to do experiment, you're going to see extensive experiments when the, when the abstract, when, when the paper comes out. I mean, I guess, I guess the main difference, I would say, because again, even you can ask the same question for NMF, right? So you could still be doing NMF by solving essentially non negative list squares. Okay, but that's, that's kind of a completely different beast, right? So you, then you, it's going to be about interior point algorithms or whatever numerical subroutine you are using. I guess this has the advantage that it's just easy to implement even Lisa, you know, you just look at this, whatever. You know, you just implement this. It's like five lines of code and kind of runs.
00:29:20.660 - 00:29:32.804, Speaker A: Well. Of course, you don't have very good theoretical guarantees. You only have the Mm guarantees, whatever they are. Yeah, but I think the advantage here is simplicity.
