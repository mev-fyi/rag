00:00:00.080 - 00:00:53.076, Speaker A: So I'm going to talk today about types of rigidity. The main goal is to present the proof of the Azeroth Roth theorem. So let's begin. So we start with a graph with v vertices, with n vertices and e edges, we call p to b realization. If we put z in Rd, not exactly with an embedded, because the vertices could be at the same point, but we just place all the vertices in Rd. Then we have a bar join framework, I'm going to call it framework and we shall refer to the vertices as PI. And we're going to call the image of the edges, we still going to call it edges.
00:00:53.076 - 00:02:03.584, Speaker A: I hope there will be no disruption with that. And as Tony defined, we have the rigidity map, which is the function fg, which sends the realizations to the vector of the pairwise distances squared. So it's a quadratic function if we think about it. So now because I want to have some connection between z and the complete graph with the same n vertices, let kn be this complete graph. Check that if we have two realizations of complete graphs p and q as I call over here, then these q as we call them, they are congruent, meaning that they come from an isometry which sends its PI to ui. And by saying isometry, I mean that there is a function that supports, that fixes the distances. So by Mazur Lav's theorem, this means that the isometry is pretty much a composition between a linear isometry, a linear operator, if you want any translation.
00:02:03.584 - 00:03:11.552, Speaker A: Okay, so let's continue. So start the graph with n vertices, place it, realize it in some Rd space. So the whole placement is a vector in R and D. We will call a framework riggit. If there exists an open neighborhood u of p that I write over here, I will use that say if locally, if I see the placement, the placement once again is a way that I have put all the vertices in one vector in R and D. So I'm looking enable of that placement of that realization and look at it locally. I see on the left hand side here where I say that the inverse image of f with respect to kn are all the congruent placements, which means that these are all the placements that fix all the distances between any two two vertices that are in the graph, no matter if they are edges or not.
00:03:11.552 - 00:03:59.188, Speaker A: So these are the congruent placements and I want locally this to be the same as the equivalent realizations. Which are the equivalent realizations are the realizations that respect only the lengths between the edges. Okay, and as I said, I want this to happen locally. So if you want, from another point of view, someone would say, since the left hand side locally, I can think of it as that. It comes up from the isometries that I have in the space. And since I'm locally, there are no reflections there. So there isometries that respect the orientation of the space.
00:03:59.188 - 00:04:56.574, Speaker A: I want the same to happen with a graph. With the graph, I mean with the non complete graph possible. And we say that a graph is flexible if there exists a continuous path such that x of zero would be our placement. But when I'm going for bigger t, then I'm taking placements that are equivalent, but they are non congruent, meaning that the distances between two vertices that they are not connected by an edge, they change. Such paths are called non trivial motions. If Olxt would be inside the congruent placements, then this would be a trivial motion. Okay, let's see some examples.
00:04:56.574 - 00:05:40.734, Speaker A: The first example is to take two triangles and connect them to a common edge, e three here. So as you can see here, I have realized this graph in two reals. I have taken a realization on the left with the four vertices. The graph of course is rigid because as tony, so the previous time it's two three tight. And of course, if I see also p two and p four, since I am on the plane, the distance between p two and p four is fixed. No matter, I cannot move anything. So p two, p four would have always the same distance, which is two here.
00:05:40.734 - 00:06:34.414, Speaker A: So if I have any path, then this would respect the distance between the missing edge v two, v four. So it would be like an isometry, since it would move around the whole complete graph with four bits. Now if I take the same graph and move it to three dimensions, so I have just canonically embedded it inside three reals, then if you think the axis that comes through the edge epsilon e three, then I can just rotate the one side. Think of it as a napkin, like this. And I just, here is e three and I'm rotating just one side with respect to the axe e three. This of course is not coming from an isometry because the distance between p two and p four would become smaller. Hence this one is a flexible framework in r three.
00:06:34.414 - 00:07:20.062, Speaker A: So as we see, it's important what's the underlying space every time. Let's see another example. Of course, k four is a complete graph. By definition, it's all, it's all the equivalent frameworks are congruent. And now if I go from this was k four, the change is that over here I have one more vertex, p five. So I broken down the previous edge into two edges, and I have put p five in the middle. As we can see, this one is rigid in two reals, but it is not rigid in three reals.
00:07:20.062 - 00:08:11.366, Speaker A: Once again, I can do exactly the same for length with respect to the axis e three. So let's go to the first preposition. Start with a graph with n vertices and the placement and the realization p. We have that the following phrase sentences are statements are equivalent. The first one is that the framework is not rigid. The second is that it is flexible. And the third one says that in the definition of flexibility, you don't need to have a continuous path such that yt would be an equivalent, not congruent framework for all t, but just for some t, that's the difference.
00:08:11.366 - 00:08:57.744, Speaker A: For some t would be enough instead for all. So let's see how this works. So from a to b, there is some algebraic magic over here. Start with a framework GP that's not rigid, meaning that the congruent placement is a strict subject now of the equivalent frameworks, no matter what neighborhood you take. So that's the negation of the definition. But since it's a strict subset and both sets are algebraic varieties, which means that there are sets of solutions of polynomial equations, this means that I can always find an analytic path. So there's some algebraic argument over here that is needed.
00:08:57.744 - 00:09:59.336, Speaker A: It's, if you go in algebraic geometry, probably will be in many books, it would be about two pages, the proof. So it would take another hour to prove that one. But I think it's natural to think that since we are talking about solutions, polynomial equations of the solutions, if it's something, is a bigger manifold, let's say that it's smooth enough in order to find analytic paths. So that's, that's the first part of the proof, from b to c, as we said, it's evident because as we said before, the definition says the same as three, but instead of four sum, it says that for all t that lie from the half open zero to one interval. So it's evident. So what is left is to go from c to a. If I would like to make this proposition, this proof, with just a few words, I would say, okay, remove the first part and reparameterize, and you're done.
00:09:59.336 - 00:10:54.174, Speaker A: So that's what we're gonna do. Take all the t's such that the path would give congruent placements for every t from zero up to the max one t zero will be the max one. So that will be a placement that would be congruent, which means that there is an isometry that could still send qi to PI. Now, since since this is the inverse image of a close set, it is closed. So there exists an epsilon such that whenever I go inside the small neighbor, the zero comma epsilon neighborhood, above that t node, then I would be in a non congruent placement. So now all we need is we can define a new path. It's not defined in zero, but we can re parameterize.
00:10:54.174 - 00:11:54.464, Speaker A: We don't need it even here because we just want it to be inside enabled of zero. And the reason is that the way that we define it, which is exactly what we said just we dropped the first part from zero to t zero, and we use the isometry in order to have that x of zero is actually our realization p. And now, since this is a continuous path that x of zero is equal to p, every neighborhood of p intersects with that path. And that path, as we said, for every t bigger from zero, has frameworks that are equivalent but not congruent. And hence this means that we cannot find such a neighbor that everything would be congruent. And so the framework is not rigid. Any questions? Okay, let's continue.
00:11:54.464 - 00:12:49.164, Speaker A: So, for the second part, we want some of the differential geometry magic now to happen. So, we will talk about proposition two, which is of separate interest. It's much more general from the rigidity theory that we are doing here. So, start with two smooth manifolds and let f to be a smooth map, so, partial derivative continuous of every order. And we, we denote by rank of df to be the maximum rank of the derivative at x. So we are taking the maximum over all the x's that are in the domain. And we see that nx in the domain is regular if the rank of its derivative hits the maximum rank of the derivative singular otherwise.
00:12:49.164 - 00:14:46.440, Speaker A: And the proposition says that if I have a function from rm to rm, and this function is smooth with a rank of its derivative equal to k, then an x not is a regular point. So if x node is a regular point of f, then the image that we have locally for of the image of f that have locally around x node is a k dimensional manifold. So, let's see how this goes. It's pretty much, I would say, an application from the inverse function theorem. So, since df zero is the maximum k, we can assume that f can be written as f one comma f two, where I have just separated the coordinate functions f one consists of the first k coordinate functions that actually I have gathered this, that would satisfy that the rank of the derivative, a text node would be equal to k, which means that this submatrix over there would be the reflection of that matrix over there would be invertible if we think about it, which means that there is a derivative over there, that it's non zero. And since the derivative is a continuous function, I can think that the rank is equal to k in a neighborhood close to x node. Okay? And now since it's non zero, it's equal to k close to x naught by the local immersion theorem.
00:14:46.440 - 00:15:47.966, Speaker A: The local immersion theorem says exactly what I'm saying below is that this yields local coordinates. I can find system of coordinates, if you want, such that f x 100 period, that sends x one, x two just to x one. So it's a projection to the first one to the rank. Okay, the local immersion theorem, as I said, starts by I want to have the rank to be equal to k. So in order to have the maximum rank, and then I will have this projection result. So if I take now the derivative in local coordinates again, then I will have in the entry one, one of this block matrix, the identity matrix, okay, this would be the derivative of f. And I know that the rank of this one, the maximum is k, and the rank, sorry, the entry eleven has rank k.
00:15:47.966 - 00:16:44.804, Speaker A: So this means that the partial derivative of f two with respect to x two is equal to zero. And this means that the function f two is actually depends only on x one is constant with respect to x two. Hence, if I think about it, I have that f of x one x two is f one f two of x one x two, f one of x one is x one and f two would give us just g of x one. So f would have image near x zero. The graph of a function z function z that has domain k coordinate. So it's a k dimensional manifold. And that's the proof of the first of the second proposition.
00:16:44.804 - 00:17:52.062, Speaker A: So we are ready now to go to the main theorem, the Asimov Ross theorem. So we start with a graph with n vertices e edges, the rigidity map as before. And we suppose that we have a placement which is, sorry, a realization that's a regular point. So the rank of the derivative of z would be maximum. And we say that m is the dimension of the affine hull of that placement of that realization, meaning that take the points p one p n, and the way that I would like to say about the flying hull is translate p one to zero, translate every other point to PI, to PI minus p one. Then you have a subspace, that's the dimension of the subspace, and then move it up by adding again p one to the subspace, and you have the affine how the dimension remains the same. Okay.
00:17:52.062 - 00:18:52.190, Speaker A: And we say that the framework is rigid if and only if the rank of the derivative of ft is equal to this. That seems a bit weird right now. The reason that seems a bit weird now is that we have that the affine hal could be smaller, strictly smaller than R and D than rd. Sorry. So in that case, we have that it's rigid if and only if it's equal to nd minus m plus one times two, d minus m over two, and it's flexible if and only if it's smaller than that. So in order to see something more that we see more, usually in our cases, if we have everything the same, and now the affine hall of the realization is rd, then this would be the corollary. Instead of having two d minus m, here we would have two d minus d, and here we have d plus one.
00:18:52.190 - 00:19:43.190, Speaker A: So, naturally, we get these formulas, which is nd, which is pretty much the dimension of the domain fg minus the isometries that preserve orientation over here. So, let's see how this, the proof of the main theorem, goes. As we say. We want to talk about rigidity over here. The standard theory says that it is rigid. The standard definition says that it is rigid if that locally the congruent placements, the congruent realizations are equal to the equivalent realization. So what I'm going to think about is I'm going to talk about these two manifolds and see what happens with the dimensions.
00:19:43.190 - 00:20:49.674, Speaker A: Okay, the first one, we know it already from the, from proposition two, we, if k is the rank, the maximum rank of fg, so the rank of fg at p, because p is regular, then locally we know that fg of p is, is a k dimensional manifold. So the inverse image would be nd minus k. And we have done the first part. We know what is the dimension for the, for the equivalent frameworks that are locally. So what we need now is, what I say below is, I want the dimension of the, of the congruent placements in a, in a neighborhood of, of p. So, we start with the isometrics. Id are the isometers on RD.
00:20:49.674 - 00:21:46.990, Speaker A: We know that this one is d times d plus one over two dimensional manifold. And we define the function f that goes from the isometers to R and D, and pretty much. What does it evaluate? Every t in every coordinate with the placement. Okay, so it's easy to set up the image of this fix is just all the congruent frameworks of the realization p. And the inverse image of the element p is the subgroup, the closed subgroup, because p is just closed. It's just one element is the closed subgroup of the isometry group that fixes p. Right? Because I would have that f of that isometry could be equal to p.
00:21:46.990 - 00:22:48.494, Speaker A: So it fixes p. Stabilizer, if you want. And since it fixes p, it fixes all the affine. Hal, if you think about it, the elements that fix p are not the translations for sure. A translation would just move p around. And by mazurulam, we know that the isometries, yeah, we know that the isometries are translations plus linear operators, right? So if we think about, we can think like that locally, what we have is just linear isometries. So I could identify f minus one of p with the elements that, with the isometries that lie in this, in the space rd minus m, which has dimension d minus m times d minus m minus one over two.
00:22:48.494 - 00:23:28.734, Speaker A: Okay? And now here comes the magic from differential geometry. So I define here a natural projection that I say from the isometry set d that goes to this quotient, the isometric of d minus, sorry, not minus quotient, the re inverse image of p. Let's be careful about this one. This inverse image over there is a closed subgroup, but it's not a normal subgroup. Okay. You can find easily examples in order suite. So when I define this set, let's be careful.
00:23:28.734 - 00:24:17.196, Speaker A: I'm not defined a new group. I define a new manifold. Over here, there is a theorem which says that there exists a manifold here, a topological manifold, when such that this natural projection is smooth rejection, we need some properties that are satisfied from the isometries, like that they act properly and properly, meaning that they inverse compact set to compact sets. And that. And that I want, for example, that the subgroup, I want it to be closed, otherwise there would be too many technicalities in that manifold. The topology already, since it's quotient is quite difficult to see. I have put some references in the back if someone wants to see about it.
00:24:17.196 - 00:25:30.434, Speaker A: But what we need to know for now is that I know that there is this natural projection to that object which has a manifold property, and that there the dimensions work well as we expect to be. So the question would be the dimension of the quotient would be the dimension of the id minus the dimension of the f minus one of p, the inverse, inverse of p. So now we define the natural projection, and we define now the over line of f over there, which is the function that would give me this, this diagram that f would be equal to overlaying f composed with PI. So now what I'm trying to do is to define a function that is a different amorphous in order to count the, the dimensions. So f is smooth. This means that f, the over line of f, is also smooth. And so we are okay from the first part.
00:25:30.434 - 00:26:35.450, Speaker A: And now we have the core restriction in the, in the image instead of having R and D. Now I'm going to the image of over line f just to define the different morphism. And what's the reason? The reason is that the image of f is actually equal to the image of f. And as we said, the image of f is exactly what we want. It is the manifold of all the congruent placements of b. So all we need to do now is that is to count the dimensions. We have, as we said, that the manifold of the quotient is t times d plus one over two, which is the dimension of the isometrics on d minus the dimension of the isometries that give peak fixed, which is the inverse image of f at p, which is, as we said, d minus n times d minus n minus one over two.
00:26:35.450 - 00:27:14.214, Speaker A: We do the calculations and we have it. Okay, so now we have both dimensions for the congruent and the equivalent placements. And pretty much we are done with the proof now, because all I'm going to do here is to say that since submanifold, the dimensions should be smaller or equal, it is equal when they are rigid. And I'm solving with respect to k. So I have that here that thus k is equal to n. D minus n plus one times two, d minus m over two if it's rigid. Otherwise it is flexible and the proof is complete if you want.
00:27:14.214 - 00:28:20.854, Speaker A: It would be nice to see how easier it would be the exercise, if the proof. You would try just to do the corollary. So we did all these things with the quotients and stuff in order to have the realization in smaller dimensions. Okay, so we come up to the point that we see that the derivative is actually two times the rigidity operator. I know, I know that this one is not a new to most of you, but that's, I think that that's a good way to present it. That's the intuition of why we define the rigidity map. And that's pretty much the idea of how to work with how to define a rigidity operator in different norms, to find the rigidity map according to the new norm, and then to try to find the derivative.
00:28:20.854 - 00:29:27.724, Speaker A: Okay, so these two, of course, is we're looking for ranks and dimension place no role. So we just need to check that it's map every time. And now again, some definitions in order to, that we have seen before from Tony, in order to go back and see what happens with taking rigidity through infinitesimal flex and motions. So, a vector now is infinitesimal flex if it is in the channel of the rigidity map. And of course, if I have a framework with a regular realization, and I'm given an analytic path and an analytic motion, if you want, for example, an analytic motion would be a good parameterization of a trivial motion. It comes from an isometry. Then we can find a flex by just taking the derivative and evaluating at zero.
00:29:27.724 - 00:29:37.456, Speaker A: So that's exactly, for example, what is a trivial flex? Trivial infinitesimal flex.
00:29:37.640 - 00:29:38.032, Speaker B: Okay.
00:29:38.048 - 00:30:34.394, Speaker A: And of course, okay, okay. Framework, about joint framework is rigid, is infinitesimally rigid, if every flex is trivial, is coming. So from a trivial motion. And as a corollary, just rephrased, what Azeroth and Roth said is that if I have a regular point in the rigidity map, then the following are equivalent. The framework is rigid and the framework is infinitesimally rigid. And before we end, just some examples over here. So, we saw before the framework with the two triangles that are collided in e three.
00:30:34.394 - 00:31:28.924, Speaker A: So we have found the rigid map. We have calculated this one, we see that is equal to five. We do the calculations also, as we have from the corollary, we see that five is the magic number, in order to say that the framework is rigid here. Now, if I take the canonical embedding over here in r three, we see that the rigidity matrix would be, actually would have the previous realization in r two as a submatrix and zero in every other coordinate. So you see p two, p three, p one, p four, everything would have zero of the last coordinate. So the rank would still be five. And of course it would still be regular because I didn't add any edge over there.
00:31:28.924 - 00:32:14.634, Speaker A: But now five is not enough. And since, since the right hand side of the azeroth equations would give six, which is smaller, and hence the framework, by the theorem, is flexible. Let's go to k four. The first realization is in two dimensions. I have calculated once again the rigidity matrix, even though it has six rows. Since it has six edges. Its rank is equal to five, since I have some row linear equivalents, as we have seen here.
00:32:14.634 - 00:33:49.984, Speaker A: But five is enough as we do the calculations. And so, from the Azerothlow theorem, once again, the framework is rigid. If I take now the canonical embedding of k four to three dimensions, which means that the realization is in a plane, on the xy plane of three reals, then the rank is equal to five again. But once again, if we do the calculations nd minus n plus one times two, d minus m over two, we would have six, which is bigger from five, which means that k four, even though we know that, is rigid, by definition, it's not infinitesimally rigid. It's infinitesimally flexible, which means that there is an infinitesimal flex and an element in the kernel of the rigidity matrix. It's easy to find, actually. You can think that I will take velocity vectors that would go orthogonal to the plane where the graph is embedded here, and I will just ensure that not all vectors that would be orthogonal would point the same direction in order to know that I would have a translation but non trivial flex.
00:33:49.984 - 00:34:50.710, Speaker A: Yeah, of course, this does not mean the question says, ask if this would mean that it's flexible, which means can we apply Azeroth theorem? And the answer, of course, is no, because the placement that we had over there is not, the realization is not regular because we didn't hit the maximum rank. So I have another realization here where I moved p one on a different plane. I have now the last coordinate to be one, as we see over here, and calculate the rigidity matrix. Now, the rigidity matrix over here is equal to six. Okay? And six was the magic number that we had before, so it is rigid displacement. And finally, the last example of eligibility matrix. Here I have the graph that I had before.
00:34:50.710 - 00:35:30.504, Speaker A: I have pretty much like a four, but I have splitted this edge below into two edges. I have put a vertex in between. So it's like helen back move here for p four and p two. And I have calculated the rigidity matrix. The rigidity matrix would be six, because I found, once again, some dependence between the rows. However, as we see, as we do the calculation, six is not enough, because I should have five. So we know that this one is not infinitesimally rigid, despite the fact that this rigid in two reals.
00:35:30.504 - 00:36:06.854, Speaker A: And in order to find the infinitesimal flex, we can see the graph over here. Because these three points are collinear, I can just put an orthogonal vector to the edge described by p four, p five, and p five, p two, while everything else would be a zero vector. Again, as an exercise, I just leave that we can find another realization that would hit the maximum rank. And that's pretty much it.
00:36:10.514 - 00:36:15.534, Speaker C: Could we go back a few slides just to k four, do you mind?
00:36:18.034 - 00:36:19.274, Speaker A: In two reals or three reals?
00:36:19.314 - 00:36:39.794, Speaker C: In three reals, yes, in three reals. We saw that k four in three reals is infinitesimally flexible, but it's. So what's the situation for flexibility?
00:36:43.254 - 00:37:00.514, Speaker A: It's not flexible in r three. In every rn, it's rigid. It's continuously rigid because by the definition, every motion that would have, would come out from an isometry. Right? No, no, it's length.
00:37:01.774 - 00:37:05.902, Speaker C: Right. Okay, so it's, it's rigid but not infinitesimally rigid.
00:37:06.038 - 00:37:19.994, Speaker A: Exactly. And the reason why we have this is exactly because our placement is not regular. That's not the derivative. The rigidity matrix does not hit its maximum rank in the realization that we have.
00:37:20.694 - 00:37:27.364, Speaker C: I see, I see. Otherwise, it would be also infinitesimally rigid.
00:37:28.624 - 00:37:29.432, Speaker A: Yes.
00:37:29.608 - 00:37:30.528, Speaker C: Got it. Okay.
00:37:30.576 - 00:37:52.584, Speaker A: Yes. Think about it. If I go by, let's say, by luck and put a placement almost everywhere, my placements would be regular because I need some, some linear dependence with the rows, right. So I need to be in some hyperplane in order to do this. If you want, the hyperplane is, has measured zero in the whole space.
00:37:52.884 - 00:38:13.060, Speaker C: Right. So this, so the infinitesimally, infinitesimal rigidity of this realization really depends on the realization. If you just randomly throw down k four into r three, it's going to.
00:38:13.092 - 00:38:13.664, Speaker A: Be.
00:38:15.724 - 00:38:20.204, Speaker C: Infinitesimally rigid as well as rigid. Is that right?
00:38:21.024 - 00:38:31.884, Speaker A: Yeah. Almost everywhere would be, it would be a regular realization. So it would be infinitesimally rigid if and only if it's rigid and the other way around.
00:38:32.304 - 00:39:26.484, Speaker C: Right. Okay, cool. So, yeah, that's very interesting. So it, this specialness here comes because of the rigidity matrix and the rank of the rigidity matrix. And in some sense there's no relation to symmetry. But it seems like this is a pretty symmetric realization in some nice way. Is there some way to characterize those, those realizations that are not, that aren't regular, using symmetries instead of just using the rank condition?
00:39:31.464 - 00:39:41.374, Speaker A: I don't know. To be honest, I don't know if Tony or anybody else want to join on that question. I usually use the rank because I find it easier for me.
00:39:41.534 - 00:39:41.942, Speaker C: Yeah.
00:39:41.998 - 00:39:42.206, Speaker A: Yeah.
00:39:42.230 - 00:39:43.154, Speaker C: It makes sense.
00:39:44.174 - 00:40:24.862, Speaker A: Sometimes when we look at some graphs, we see some symmetrics that say there's too much symmetry over there, so it should be flexible. So for example, some really symmetric graphs that, like w the wheel graph with six vertices in the row, I think that this one has some nice symmetries over there. So this one would not be in finitesimal region, there would be non generic placements. But I don't know if there is anything more combinatorial than the rank or about symmetries that you can talk about. Tony, I don't know if you, no.
00:40:24.998 - 00:40:41.854, Speaker D: Not for the specific question will asked. I mean, symmetry often has a very big effect on whether a given framework is infinitesimally rigid or not, or whether it has a continuous motion or not. But I think it is best just in terms of the rank for this, at this part.
00:40:42.474 - 00:40:43.374, Speaker C: Okay.
00:40:47.114 - 00:42:26.164, Speaker E: So I also have a question, not, not related to the, I mean, curiosity, which is, so, if you start to write the rigidity matrix, often it becomes pretty complicated. I mean, it becomes a big metric. But yes, if you think about this function that you described in the lecture, then like you suppose you end up in a regular point, then you find, and suppose I have my own configuration, for some reason I have my p given, and then I can try to find the path of regular points that joins this p to one that is easy to compute. And then if it is all made of regular points because of the, of this differential geometry setting, it will have the same rigidity. Or, I mean, suppose that one is rigid, then the rank is not going to change between different regular points, right? And so, is there a way that, you know, to a canonical, if I have my g, is there a canonical canonical g that will be easier to compute than the others in some way? I mean, it's a vague question, but is there a recipe for finding the simple to calculate g? Or what, what do you, how did you do, how did you come up with this k four instead of another k four.
00:42:32.264 - 00:43:14.734, Speaker A: As usual, how we find these k forces, as you say, is that. Just be a bit careful not to, you know, not to hit a non regular case. And usually then we calculate the resistive matrix. In some works that we do, it's really a pain in order to calculate, especially if you change norms. But for sure, I would always choose to go with the rank instead of trying to find a map, unless it's not obvious to find the continuous path, then the rank. I think that it is easier because there's a standard strategy in order to find the rank.
00:43:15.634 - 00:43:42.054, Speaker E: I see, so there isn't. I can, I, like for example, say, now I embed my, say, my graph is always a subgraph of a complete graph, and then I embed the complete graph in space and from that one I can. But of course, maybe it's hard to compute the rigidity matrix. Okay. Yeah. Interesting though, to find like a canonic something that is canonical. Maybe a sparse matrix.
00:43:42.134 - 00:43:43.142, Speaker A: It would be, yeah.
00:43:43.198 - 00:43:43.914, Speaker E: Okay.
00:43:45.214 - 00:44:53.962, Speaker C: I like your question a lot. It reminds me of, in numerical algebraic geometry, there's this technique of deforming one polynomial system of equations to a second polynomial system of equations that is easier to solve, that you know how to solve, and then you trace back the solutions using what they call homotopy continuation. In numerical algebraic geometry, there are these paths that they prefer to use so that the paths never hit singular points. So the paths kind of are smooth so that you can keep tracing the solutions back. Um, so, so it's kind of in that, in that, uh, spirit, this is your question. Yeah. If you're looking for references for that, uh, the thing to look up is bertini, the bertini, um, system of, it's like a computer algebra system that implements numerical algebraic geometry.
00:44:53.962 - 00:44:59.444, Speaker C: And there's a science, there's a couple siam books that are really nice that introduce the ideas.
00:45:00.664 - 00:45:07.008, Speaker E: Yeah, that would be so. Okay. Homotopy continuation is the word.
00:45:07.056 - 00:45:07.264, Speaker A: Right.
00:45:07.304 - 00:45:08.324, Speaker E: Okay, thank you.
00:45:08.824 - 00:45:40.944, Speaker B: Yes. So comment on, because you mentioned other norms, and you gave at the start the result that the different types of rigidity are the same for the Euclidean norm, and that isn't true in other norms necessarily. And basically the step that's missing is the step that you have that involves algebraic geometry.
00:45:41.284 - 00:45:41.740, Speaker A: Yes.
00:45:41.812 - 00:45:55.674, Speaker B: That step is missing. It's the only thing that misses. So all the other, like, you know, this implies, this implies this. That's fine, but you missed that last key step. And you can build norms where it's not true, but they have to be pretty weird.
00:45:56.654 - 00:46:03.678, Speaker A: Yeah, yeah. I think that the other notes, I said that you can define the identity matrix. I didn't say that there is.
00:46:03.766 - 00:46:24.184, Speaker B: Yeah, maybe I've got a bit topic, but yeah, it's just quite an interesting thing. So I think the proof, any sort of proof of that result about the equivalents will have to use algebraic geometry at some point. It is basically required. And it's the norms where they're not algebraically defined. You can do some weirder stuff and make it not true.
00:46:25.724 - 00:46:29.824, Speaker A: So what do you need there? Too much smoothness on the norms.
00:46:31.564 - 00:47:19.684, Speaker B: So I think if the norms, if the norm can be defined as some sort of algebraic equation. So like an LP norm, but the p is like an even number, then you should be fine. I think you can do the same trick because it's defined by variety. But I've got, in a paper, I've got a norm where I effectively, I think of the unit ball and then I keep shaving off points until I end up with like very weird points that are smooth, but you can approximate them by corners. And then what happens is you have a framework that can't. It can't continuously move, but you can approximate it by these rotations kind of thing. But it's a very, very weird norm.
00:47:19.684 - 00:47:24.000, Speaker B: And I purposely had to construct it. So it was bad. Effectively.
00:47:24.152 - 00:47:29.720, Speaker A: So you said it works for LP, where? Pr. Nice. Like, even as you said. Yeah.
00:47:29.752 - 00:48:04.334, Speaker B: I think you can use the same argument because you're effectively dealing with. Because it basically comes down to the fact that you need to say your configuration space, if it's. If it's connected, that any connected component is going to be locally connected, and then that will. That's another way. That's basically what you're saying in your proof. And as long as you've got a variety, that's true. But you can make norms where the configuration space can be connected but not locally path connected, and you can cause weird stuff like that to happen.
00:48:04.334 - 00:48:22.384, Speaker B: This is a bit odd, but you really have to go out your way to build these things. So even if the norm's like analytic, I think you can't. Or maybe you could. I'd have to think about that. But yeah, you can. You can create some weird stuff. That's what I'm saying.
00:48:25.884 - 00:48:27.708, Speaker D: I can stop sharing ideas.
00:48:27.756 - 00:48:28.344, Speaker A: Right.
00:48:29.484 - 00:48:30.264, Speaker B: Yeah.
00:48:32.964 - 00:48:38.384, Speaker D: I think maybe you could go to your reference slide, because we didn't see that for long before there were questions.
00:48:40.384 - 00:48:43.284, Speaker A: Have you written anything there, son, in the second one.
00:48:44.144 - 00:49:06.124, Speaker B: Oh, that's me. Yes, that's the paper I was talking about. So it's in that. Yeah. So this was actually a comment? It was. It was from the referee, actually asked the question and then I thought the answer was yes, and I worked out and it wasn't yes. So it was a very good referee comment, actually.
00:49:13.604 - 00:49:22.584, Speaker A: Actually looking at the second one, I think that the proof is more egorous than the first one. You have a lot of details over there.
00:49:23.284 - 00:49:44.754, Speaker B: Yeah. So I found the Asima and Roth proof to be a bit quick. So I kind of. Because I was dealing with other norms, I had to be very careful. So it is a lot more rigorous. But then it doesn't have the last step because it's not possible in norm spaces. So you have more restrictions, so it's a trade off.
00:50:00.914 - 00:50:17.774, Speaker D: Okay, so thank you very much, lef Terrace, for a great talk. So the lecture course will continue on Wednesday, but obviously, feel free to ask any, any more questions in the next few minutes if you, you have any. Otherwise, I'll see you all on Wednesday afternoon. Thanks again, lef Terrace.
00:50:18.314 - 00:50:18.930, Speaker A: Thank you.
00:50:19.002 - 00:50:20.894, Speaker B: Thank you. It was a really nice talk. Cheers.
00:50:22.914 - 00:51:07.544, Speaker E: So I have another question. Sorry for you. It just came to my mind. So for example, suppose I am, I mean, can I impose that some of the points are sitting in a sub manifold? And then I. So at the p, if I impose that the points are living in a submanifold, is there, is it just somebody started developing this kind of theory? So, for example, I have, my points are, some of them are in the inside of this, all the points are in the inside of the sphere and some are on the boundary or something like that.
00:51:12.444 - 00:51:16.340, Speaker A: So placements on spheres or on some.
00:51:16.372 - 00:51:20.964, Speaker E: Manifold, I guess the sphere is just the first that comes to mind.
00:51:22.544 - 00:52:00.318, Speaker D: So, yes. So in this course we will talk about a little bit frameworks on the sphere. So you imagine that you fix the sphere in euclidean space and then you say the vertices of the graph are mapped to live on the surface of the sphere and they're also constrained to only move around on the sphere. So they can't move just randomly off into space. Then yes, this problem is pretty well studied and there's. So I will talk about it in the, later in the module you can replace the sphere with whichever manifold or surface, et cetera, that you like. And there are some results in this direction.
00:52:00.366 - 00:52:30.024, Speaker E: Yes, no, no, but so that's nice. That's part of it. So, but I would like to think about if there are like five points inside and five on the boundary, and I prescribe which of the gym vertices are mapped to the boundary and all the edges and so on, but, so that some of them are on a submanifold. Right. Often people are fixing some of the points, only some manifold dimension zero, but.
00:52:32.564 - 00:52:42.922, Speaker D: You can. So you want to have, say, some points in three dimensional space to be completely free and some points to be restricted to a hyperplane and some to a line and some to a point, for example.
00:52:42.978 - 00:52:52.054, Speaker E: Yes, or maybe, or a sphere. I was thinking just the sphere and not so much variety, diversity of things, but yeah, in general.
00:52:52.514 - 00:53:30.274, Speaker D: So this is also studied and again, I'll mention it a little bit. So the, if you imagine that the points are on the sphere, then, and I said before to restrict the points to only move on the sphere. The way you do that infinitesimally is to say that the infinitesimal motion must move you in the tangent space to the sphere. So in the tangent plane, yeah. So if I say I'm going to in 3d I'm going to restrict the points to some to be free, some to move on a hyperplane that's fixed, some to move on a fixed line and some to move on a fixed point. Then that sort of covers, you can imagine these hyperplanes being the tangent planes to some surface, whichever you like. So you just have to put the hyperplanes in the places you want.
00:53:30.694 - 00:53:31.342, Speaker E: Right.
00:53:31.478 - 00:53:34.914, Speaker D: So in some sense it is kind of answering your question.
00:53:35.734 - 00:53:36.254, Speaker E: Nice.
00:53:36.334 - 00:53:37.074, Speaker A: Okay.
00:53:40.644 - 00:53:43.692, Speaker D: These sorry questions, right, I think.
00:53:43.708 - 00:53:46.184, Speaker A: That you have also work in these types of questions that you.
00:53:49.244 - 00:53:58.284, Speaker D: So I call these things linearly constrained frameworks. So you have a lot of linear constraints on single vertices as well as constraints between edges.
