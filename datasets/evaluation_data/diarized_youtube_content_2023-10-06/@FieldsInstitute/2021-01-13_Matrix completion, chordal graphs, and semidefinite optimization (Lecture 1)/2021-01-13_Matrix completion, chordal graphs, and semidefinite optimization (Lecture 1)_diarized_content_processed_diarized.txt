00:00:00.600 - 00:01:06.124, Speaker A: So the main topic of these lectures is symmetric matrix completion. Of course, that's a topic that has a very long rich history and applications and connections with many different areas. So my own interest in this topic started in the context of semi definite optimization and looking at methods for using matrix completion theory to speed up algorithms for sparse semi definite optimization. So I'll try to explain that in the introduction before we start the actual lectures. So this work also started with Joachim Dahl. He was a postdoc at UCLA many years ago, and then continued with former and current PhD students. So, a semidefinite optimization problem is a convex optimization problem that is usually expressed in this form.
00:01:06.124 - 00:02:11.620, Speaker A: So the variable is a symmetric n by n matrix x. The inequality denotes that x must be positive semi definite. And then we have a linear cost function, linear equality constraints, written here as the usually usual, using the usual trace inner product of symmetric matrices, inner products of x with coefficient matrices c or AI. So it's sometimes called linear programming with conic constraints. And this is a problem that has been studied for the last 20 years, mostly in three groups of applications. First, there are applications where positive semi definite constraints arise very naturally, for example in control and studying stability using lyaparnov techniques in statistics. Obviously, if x is a covariance matrix, it must be positive semi definitely.
00:02:11.620 - 00:03:19.814, Speaker A: In distance geometry problems when x is the grammy of a set of points to be positive semi definite, so problems are applications like this often lead to optimization problems with positive semi definite constraints. So the second group of applications may be the largest one in terms of actual number of semi definite optimization problems solved in practice. So semi definite programming is used in convex modeling tools like CVx or CVx PI and yell, MIP and related tools. And often these tools work by reducing a general convex optimization problem to a semi definite optimization problem, and then solving that and returning the answer to the user. So there, semi definite optimization is used, but it's not in a transparent way. But underlying these tools are solvers for semi definite optimization. And then maybe the last group of applications is in relaxations of quadratic, non convex quadratic constraints, polynomial optimization problems.
00:03:19.814 - 00:04:44.044, Speaker A: So that was one of the main applications from the early days of semi definite programming, and it continues to be very active field of research. So the connection with completion problems is that in many of these applications we encounter problems that are quite large but have sparse coefficients c and AI. And then one would hope to exploit sparsity in much in the same way as for linear programming. For example, if you have a very large sparse set of constraints, we know that interior point methods can be implemented very efficiently. So here there's a complication in the case of a semi definite optimization problem, because if the entries of the coefficient matrices AI and c are very sparse, then the cost function and the equality constraints imposed only involve a small subsets of the variables in the matrix x. But x still must be, in general a dense matrix to be feasible for the inequality constraint. So, written like this problem quickly leads to a very large scale problem because you have this n by n dense matrix variable.
00:04:44.044 - 00:05:31.754, Speaker A: So that sort of motivated some techniques that exploit completion theory that I've explained in a few slides. But maybe I can first give one application that actually falls in this third class of problems, and that maybe explains the sparsity. So this is a problem that has received a lot of attention in electric engineering. It's the power flow distribution problem, optimal power flow distribution problem. So it's a problem with an optimization problem that's actually very non convex. And here I just summarized the reason why it's non convex, and I give the non convex constraints. In addition to this, there are several convex constraints.
00:05:31.754 - 00:06:03.492, Speaker A: So the variables, so it's a distribution network. The variables are the voltages at each node in the network. They're complex numbers. So you could also represent them as in a complex plane, as two vectors. And there's a complex power in each link of the network between nodes I and j. These are also complex numbers. And there are two actually constraints that make the problem non convex.
00:06:03.492 - 00:06:52.564, Speaker A: One is that these node voltages, these complex numbers, are upper and lower bounded by limits. So the upper bound would be a simple convex constraint. The lower bound is a non convex constraint. So if you represent the vi as a vector in a complex plane, it would be a lower bound. On the euclidean norm of vi, a second constraint are the power balance equations. So for each line in the network, the total power entering the admittance of the line must be equal to the complex power that's dissipated in the admittance. So it gives a nonlinear choradic equality constraint of this form.
00:06:52.564 - 00:07:48.790, Speaker A: If you're right in terms of real variables, it would be true equality constraints that involve the square distance between these two vectors in the plane. But in any case, it's a nonlinear quadratic constraint. So, a general technique that also underlies a lot of applications of semi definite programming is to relax these quadric constraints by introducing a matrix variable. In this case, the real part of the v times v complex conjugates. And if you so the matrix with these elements, if you have these as extra variables in the problems. Then these two constraints actually become very simple, become actually linear. So the bounds on the models of vi becomes simple.
00:07:48.790 - 00:08:53.732, Speaker A: Upper and lower bounds on the diagonal elements of x, the bounds, these equality constraints on the distances, squared distances become also linear in the original variables and then the matrix x. But all the non convexity is now in this added equality constraint that relates x to the original variables. So we obtain a relaxation if we drop this constraint and replace it with a weaker constraint that just says that x is a positive semi definite matrix. And then we obtain a semi definite optimization problem. So that's the technique that's been used in many recent papers for the purpose of this talk. It's interesting to note that the problem you obtain is very sparse in the sense that only a few entries in this positive semi definite matrix x appear in the constraints. So the diagonal entries and then the off diagonal entries xi j.
00:08:53.732 - 00:09:47.404, Speaker A: If there is a connection in the network between node I and node j, all the other entries are don't appear explicitly in the variable, and they're constraints. They only have to be in general non zero to make the entire matrix positive semi definitely. And then this is in general a relaxation. If x happens to be a rank two matrix, you actually have found the exact solution, and in general you can use it as an approximate solution to find a sub optimal solution. So that's one sort of typical application in this last category. And it shows that the sparsity often arises in problems of this form. And there's an underlying graph or network structure that makes the leads to this kind of sparsity.
00:09:47.404 - 00:10:49.234, Speaker A: So you're interested in exploiting that kind of structure in a semi definite optimization problem. And one very interesting approach that connected with completion theory is to just reformulate this problem in a very small but important way by writing it in this form. So in these two formulations of the problem are actually, there are two differences. One is that in the original problem, the original SDP x is a dense matrix variable that can be very large for if the dimensions are large. In the second problem, we restrict x to be a sparse matrix with the sparsity pattern of the constraints in the problem and the cost function in the problem. So all the coefficients here in the problem are assumed to be very sparse. They have a common sparsity pattern that will always denote by e in this talk.
00:10:49.234 - 00:11:34.910, Speaker A: So in the first formulation, x is a dense matrix. Here x is a sparse matrix with that sparsity pattern e. So I only use in x as variables the entries that matter for the constraint and the cost function. So that makes x a much lower dimensional optimization variable. And this doesn't have to be actually necessarily be a large scale optimization problem. But in order to do that, I have to replace the inequality constraints by the constraint that x, this sparse matrix x must have a positive semi definite completion. And then the question in this formulation.
00:11:34.910 - 00:12:03.524, Speaker A: So the benefit is we have a much smaller dimensional problem. It's still a conic optimization problem with linear constraints and objective. But the question is how difficult is it to handle this cone of positive semi definite completable matrices as compared to the original simple dense positive semi definite cone? And that's where the positive semi definite completion theory then becomes very useful.
00:12:04.504 - 00:12:17.404, Speaker B: Given can I ask a quick question? When you say positive semi definite completion, are we only allowed to modify the zero entries of x?
00:12:17.744 - 00:13:14.762, Speaker A: Okay, and here it's the standard positive semidetic completion. The zeros in the matrix in the sparsity pattern can be replaced by other elements to make it positive semi defects. But I'll explain some of the notation in a few slides. And there, of course the, and so there have been several techniques proposed or different approaches to exploit sparsity. There's actually two that are very closely related to completion theory. The first one here would be to look at the original problem and try to exploit sparsity as much as possible in the implementation of a general primal dual tier point methods. That's typically not much later to completion problems.
00:13:14.762 - 00:14:57.108, Speaker A: But two and three here are very much related. In the first one, we tried to implement an interior point method for conic linear optimization, but over this cone of positive semi definite completable matrices. So it has been studied since around 2000. The first paper, the second approach, is to use completion theory for the decomposition results that we'll discuss from matrix completion theory. And that's a general strategy that can be applied in combination with many types of optimization methods, has been used for interior point methods, but also for first order methods, splitting methods like the alternating direction method of multipliers, and they rely on the classical decomposition results for sparse matrix completion. So in this, these three lectures, we'll actually discuss in more details two and three, and then the underlying theory. And then the third topic I mentioned in the title is chordal graphs, and maybe that's familiar to most people here, but chordal graph theory have of course been studied in many disciplines, often under different names, since the 1960s, maybe first in combinatorial optimization and actually underline many of the like Perez and Loewals introduced the topic and discussed the key results.
00:14:57.108 - 00:16:27.536, Speaker A: So coral graphs are an important class of perfect graphs, and that means that there is a list of combinatorial optimization problems that are very difficult in general that become very easy for choral graphs. So, for example, graph coloring, finding all the clicks in the graph, the number of clicks in the graph are all very difficult in general. But for coral graphs can be solved by simple greedy methods, and of those may be only the enumerating cliques is the little encounter, or see why that's the case for chordal graphs and linear algebra? Also very early from the 1960s, people noticed the connection with the graph elimination and cholesky factorization of sparse matrices. And then later in the 1980s, there is a series of very important papers on different types of matrix completion problems, positive, semi definite euclidean distance, matrix completion, and so on, that we'll also discuss. There are some applications in database theory that I actually don't know much about. In machine learning it is important. They're called actually decomposable graphs, usually in machine learning, in the study of graphical models, and also euclidean distance matrices, and in just nonlinear optimization.
00:16:27.536 - 00:17:51.214, Speaker A: They also have been studied since the 1980s, when people look at partial separability as an important type of structure in large scale nonlinear optimization problems. And actually the use and semi definite optimization goes back to this first paper in 97. So for these three lectures, they'll more or less coincide with these three parts, although the first one, the third one, is actually the shortest section. So in this lecture I'll start with maybe just reviewing the basic definition of chordal graphs and also some connections with sparse matrix algorithms from just sparse matrix analysis. And then in the second part, which will be the main part, actually, we'll discuss the different results on matrix completion and then the algorithms for solving them. And then we look at some of these applications that I mentioned in the introduction. So let's start with definition of the caudal grass.
00:17:51.214 - 00:18:40.264, Speaker A: First give the definition, I think it's probably familiar to many people here. Then three representations of caudal grass will be very important. So the click trees is one example. And then in the sparse matrix algorithms, numerical algorithms, people often use elimination trees and supernodal elimination trees, which are very related to click trees, but a little difference. And then we'll explain how we use it for, in, for example, semi definite optimization. So first some graph notation. So an undirected graph is denoted the standard way by vertices and a set of edges.
00:18:40.264 - 00:19:36.084, Speaker A: Here edges are pairs of vertices. Because it's an indirect graph, the neighborhood is a set of vertices adjacent to a vertex, excluding the vertex itself. So the neighborhood of a is b, e and c in this example. And then another thing I should mention is when I use the term click in this talk, I usually, I mean the maximal, a maximal complete subgraph. It's more common to use clique for just any complete subgraph, but here, even if I don't mention maximal explicitly, I usually mean the, I mean the maximal complete subgraph. And of course we're interested in sparse matrices. And then the graph undirected graph represents a sparsity pattern of a symmetric matrix.
00:19:36.084 - 00:20:32.834, Speaker A: So the vertices are the indices in the graph row and column indices. The edges represent possible non zero positions. And more precisely, I should say that it's the absence of an edge that has the information. So if there's no edge between two vertices, it means that in that position the matrix must be zero. So the two five element and the five two element in this matrix must be zero. The other entries that are adjacent or the diagonal entries may or may not be zero. So the, we always use e for a sparsity pattern and this notation for sparse matrices of size n by n with sparsity pattern.
00:20:32.834 - 00:21:50.794, Speaker A: And then I mentioned cliques as maximal complete subgraph. These correspond to maximal dense principle sub matrices. So for example, this is a click in the graph corresponds to a maximal dense principle submatrix, and again dense in quotes, because it doesn't mean that the entries must be on zero, they're allowed to be zero. Okay, so then a chordal graph is defined like this. It's an undirected graph that doesn't contain as a subgraph a cycle of length greater or equal to four. So this is a non chordal graph because it contains as a subgraph cycle of length four. If we add a chord, so an edge between non consecutive vertices in the cycle, then it becomes, we destroy the cycle and it becomes a chordal graph.
00:21:50.794 - 00:23:08.864, Speaker A: So other names are rigid circuit graphics in some of the early papers, triangulated graphs, because the only cycles you can have are actually cycles of length three. Decomposable is a common name in machine learning. There's some common examples or simple examples, for example k trees. And then maybe we can skip this for now, but then we'll see later what, how common or how coral grass actually usually appear in our applications. And there are a few very classical results from the early papers on caudal grass that I'll mention here before we get to click trees. So first one by dirakin, one of the first papers on this is in terms of minimal separators. So a sub graph of an undirected graph is a minimal separator between two vertices v and w.
00:23:08.864 - 00:24:21.064, Speaker A: If removing that subgraph results in a disconnected graph with v and w in separate and different connected components, and it's minimal, at least minimal for these two specified vertices. And then early on it was proved that chordal graphs can be characterized by the property that all minimal vertex separators are complete graphs. And another result is that every minimalist vertex separator is a subset of at least two clicks in the graph. So that's something that we won't use very much in this talk, although it's implicitly a property that is important. So this is an example of a chordal graph. These would be all the minimal vertex separators. A second property that is more directly relevant for later is in terms of simplicial vertices.
00:24:21.064 - 00:25:17.314, Speaker A: So a vertex is simplicial if the neighborhood is a complete subgraph. So it means actually also that a closed neighborhood, if you add a node itself, it becomes a clique. And in this graph we have three simplicial vertices I, f and a. And Dirac again proved that every choral graph that's not complete has at least two non adjacent simplicial vertices. So this example has three, but there are at least two in every choral graph unless it's complete, and there's only every vertex is that every non complete has at least two. So some of these properties will come back. So these are some of the early characterizations I will come back when we talk about click trees.
00:25:17.314 - 00:26:14.994, Speaker A: And click trees actually summarize many of these properties in a very nice way. So click tree is defined like this. So here we have a chordal graph on the right we have all the clicks and the graph, and they're arranged in a tree. And this tree has what's known as the induced subtree property. So that means that if we look in this clique tree and we look for the cliques that contain a given vertex, for example the vertex e, then the cliques that contain this given vertex, that's the induced subtree induced by that vertex form a subtree in the clique tree. So therefore we call it, and that's true for every vertex in the graph on the left. And therefore it's called the induced subtree property.
00:26:14.994 - 00:26:26.394, Speaker A: And very important property of chordal graphs is that they're exactly the graphs that have a click tree with this induced subtree property.
00:26:27.714 - 00:26:38.890, Speaker B: And say something more about the way that these cliques are joined. Yeah, how do you get this graph.
00:26:38.922 - 00:27:41.234, Speaker A: On the yeah, so we'll get to that later, actually, because that can be done very efficiently. Here I just showed the result of a click tree that has this property, and it's of course an important restriction on which tree we picked on the set of clicks. But you see how this is done in practice. So how all the cliques are actually found or represented, and then how they're connected in this click tree. But this result simply says it's always possible. And it actually is a very characterization of chordal graphs. And it's also abstractly on a high level, the reason why many computational problems with chordal graphs become easy, because they're solvable by greedy methods that really exploit this induced subtree property in different applications.
00:27:41.234 - 00:28:40.644, Speaker A: So the next slides don't really explain yet how this is constructed, but explain that this tree with an induced subtree property actually summarizes many of the structural properties that I mentioned in the previous slides. Suppose we pick a root of this tree. We just pick any of the clique as root, and then we represent it in the usual way as a rooted tree. Then if you have a root, then we can separate or partition. Every click in two sets are indicated here as two rows. So the top row is always the intersection of the clique with the parent and the click tree, and we'll call that the separator of the clique. So every click has a separator except the roots and the click tree.
00:28:40.644 - 00:30:23.834, Speaker A: And then the remainder of the vertices and the clique are called residual click residuals. So that's a definition as possible. If you have a click tree and we pick a root, and then from that structure we get, and the induced subtree property we get a lot of interesting information about the chordal graph. For example, because of the induced subtree property, one can show that these residuals, so the second row in every click partition all the vertices in the graph, because by the induced subtree property a given vertex e can only be in a residual of one clique by the induced subtree property, it must be in the separators of the other clicks. So these residuals partition the vertices in the graph. And therefore this gives a simple proof that a chordal graph has at most n clicks, in fact n minus one if it's connected, because the residual subsets of each clique partition the entire vertex set, then these click separators. So the first row and every click happen to be the minimal vertex separators that are defined.
00:30:23.834 - 00:31:13.594, Speaker A: So we can have both n minus one of them. And then also I mentioned or defined as simplicial vertex. A simplicial vertex from the click tree can be found as the vertices that belong to don't belong to any click separator. For example, in this case a, f and I were the simplicial vertices in the graph. So this is one example of a tree representation of a chordal graph. It's not the only one, and we'll see one or two more later. And in general it's an example of a tree or a representation of an undirected graph as a tree intersection graph.
00:31:13.594 - 00:32:08.154, Speaker A: The tree intersection graph is defined as follows. So we start with the tree t. We have a family of subtrees in the tree that are indexed by some label v, and then the tree intersection graph defined by this. This family of subtrees is a graph that has these labels v or the subtrees rv in the set of subtrees as its vertex set. And in the tree intersection graph, two vertices are adjacent if those two subtrees intersect. And another basic result on chordal graphs is that three intersections graphs are chordal. And that's actually if and only if, every chordal graph can be represented as a tree intersection graph.
00:32:08.154 - 00:33:00.964, Speaker A: And the clique tree is one way of doing this. In the click tree we are actually obtaining. We're starting with the click tree and then obtain this chordal graph as a tree intersection graph. So we think of every vertex in this graph on the right as representing a subtree in the tree on the left, and in this case it's the subtree induced by that vertex. So each subtree induced by a vertex corresponds to a vertex in this graph on the right. And then two of those subtrees are adjacent if they intersect in the click tree. So that means that those vertices are part of the same clique and therefore they're adjacent.
00:33:00.964 - 00:33:56.514, Speaker A: So we could start with the click tree and then obtain this as a tree intersection graph of the click tree. So that shows the one direction of this result. This is another example just to illustrate the definition. So this is a tree t, we define five subtrees and then for these five subtrees, this would be the tree intersection graph. Every node in the represents one of these subtrees and they're adjacent if they intersect. For example, r1 and two reals have be in common, therefore they're adjacent. So tree representations or representing the graph as a tree intersection graph can be done in different ways.
00:33:56.514 - 00:35:18.394, Speaker A: We already mentioned click trees in machine learning. People often call them junction trees. And then one difference often is that the vertices of the junction tree are complete subgraphs, but they're not necessarily maximal complete subgraphs as in the click tree. In sparse matrix algorithms, people often also use elimination trees that we'll discuss later and also closely related but different from click trees. So this finishes that part of the, this lecture. And now we get to a topic that's much closer to sparse positives and definite matrices, and we'll show that, or see that chordal graphs are exactly the graphs for which there exists a perfect elimination ordering, and that's defined as follows. So if you have an undirected graph, we can order the vertices by just assigning numbers to them.
00:35:18.394 - 00:36:22.004, Speaker A: And we have an ordered directed graph. We can represent it by just writing the number next to each vertex. Or we can represent them by, in an array like this that could represent the sparsity pattern of a symmetric matrix. So we put the vertices on the diagonal in the order that we selected and then the dots represent the edges in the graph. So this has the advantage that it's easy to find the adjacent edges is adjacent vertices to given graph that precede or follow it in the ordering. So for example, we use this notation. So the vertex c, the neighborhood of c is the vertices a, d and e.
00:36:22.004 - 00:37:27.986, Speaker A: But it's very easy to in this graph to see that a is the vertex in the neighborhood that precedes it in the ordering, and d and e follow it in the ordering. And we'll denote that by the plus and a minus. So the neighborhood with the subscript superscript plus is the higher or monotone neighborhood vertices that are adjacent and follow v in the ordering. And with the minus sign it's the vertices in the neighborhood that preceded. So that we can also define a higher and a lower degree. And then if we include the vertex itself in the neighborhood and we call that the closed neighborhoods, we'll just use the notation call and row because in this array they appear as a row and non zeros in a row or that column. And then of course we are interested in symmetric sparsity patterns.
00:37:27.986 - 00:38:55.604, Speaker A: So it means that after an ordering represents a symmetric reordering of the rows and columns in a matrix. So if you use disordering, for example, of a matrix, we obtain a symmetric reordering of the sparse matrix. So a graph, we can do this for every disordering, for every undirected graph. It's an ordered undirected graph. An undirected graph plus an ordering is called monotone, transitive or filled if it has the property that every higher neighborhood of every graph induces a complete subgraph. So for example, in this vertex b, if you look at the edges connected to b that follow it in the ordering, then they are d, c, and e, and that's a complete subgraph. Or another way to say this, if you have two vertices in this higher neighborhood, for example, d and c are adjacent to b and follow it in the ordering, then d and c must be adjacent themselves, so also d and e and c and e must be adjacent.
00:38:55.604 - 00:39:50.662, Speaker A: If that's true for every higher neighborhood. So for every column in this array, then the graph is called filled or monotone transitive. And then this gives us to our last characterization of coral grass, again from the 1960s. A very important result that says that the graph is is coral if and only if it has a perfect elimination ordering. It's related to some of these other results that I mentioned. So it's related to the fact that every choral graph has a simplicial vertex. Because if we just want to pick the first vertex in the ordering, then that of course the higher neighborhood is the neighborhood of the vertex.
00:39:50.662 - 00:40:35.894, Speaker A: So for that first vertex, the vertex must be simplicial. And in a chordal graph you can find at least one simplicial vertex. If it's not complete, we actually have at least two by Dirac's result, and then we can combine it with a second important but simple property of caudal graphs. And it is that if a graph is coral and every subgraph is also chordal. So if you start with a simplicial vertex as a first vertex in the ordering, then remove that vertex, and the remaining graph is still choral. So it again has a simplicial vertex. And that way we can find a simplicial perfect elimination ordering.
00:40:35.894 - 00:41:35.724, Speaker A: So in practice, there are many other algorithms that are actually more faster than or more efficient than the simplicial elimination. So maybe the best known one is the maximum cardinality search by Toriana Yanakaukis has linear complexity, linear in the size of the graph. It's guaranteed to find a perfect elimination ordering if the graph is chordal, and for that reason it's also a very efficient test of the property being chordal. We can try to apply one of these algorithms. If it succeeds, it proves it's chordal. If it doesn't find a perfect elimination ordering, then it proves that graph is not coral. So maybe we can go to this next point and finish for the first lecture.
00:41:35.724 - 00:42:41.626, Speaker A: So the purpose of the next part is then to relate some of the theory of chordal graphs and make it more computational by relating it to algorithms in sparse linear algebra. People in Kantorov have studied structures that are closely related to click trees, but usually defined in terms of elimination trees or supernodal elimination trees, and can be manipulated and constructed very efficiently. So suppose we have a filled graph. So an ordered graph, undirected graph with an ordering. Then we can define the elimination tree like this. So the trees, the nodes and the tree are the vertices in the graph, and the parent structure is defined like this. For every vertex in the graph, the parent and elimination tree is the first vertex in that column.
00:42:41.626 - 00:43:18.644, Speaker A: So the first vertex that in the higher neighborhood of the vertex that follows it. So the parent of a is b, the parent of b is d. In this ordered graph, the parent of d is c, and so on. So that's called an elimination tree. We'll see. It's closely related to click trees. But one difference, of course, is that this only contains some of the information about the complete graph.
00:43:18.644 - 00:44:23.544, Speaker A: It's not possible to get the complete graph by just looking at the elimination tree, but we get some important information. Obviously we know from definition that each parent is adjacent to the vertex. But from the perfect elimination property we can say more. If you look at the vertices in a column, then all those adjacent vertices must be on the path from the node to the roots in the illumination tree. So the elements of the higher neighborhood of a vertex must be on the path somewhere between the vertex and its root. So by just looking at the elimination tree we know that for this graph, these three vertices, f, I and h, are certainly not adjacent to a. For the other vertices.
00:44:23.544 - 00:45:17.254, Speaker A: We cannot really tell just by looking at the elimination tree except for the first one, of course. But d may is in this case not adjacent. C and e are. So we get some important information about the graph, but it's not really complete. We can actually construct from it a graph that a tree that would look very much like a click tree if we expand it. So this is the same elimination tree with the, that I had here, ab and so on. The only difference is at the top of each block I added the entire higher neighborhood.
00:45:17.254 - 00:47:06.834, Speaker A: So we can think of elimination tree, add this information, and then the monotone transitivity, or the filled property of the perfect elimination means that every block in the first row is actually the intersection of this, each node in the this expanded elimination tree and its parent. So it means this, we can have a kind of induced subtree property each, if you look at, in other words, all the blocks in this expanded elimination tree that contain a given vertex e, then they form a subtree of this expanded elimination tree. And the subtree is exactly the vertices in the original graph that contain that given vertex e in their higher neighborhood. So that's another way of saying it's just the non zeros in that row of the sparsity pattern. So the, if you pick a certain vertex and look at the vertices in that row, then they form a subtree of this expanded elimination tree. So it's very much like a click tree. The difference is that these are not clicks, they're just, we have one of these blocks for each vertex in the graph.
00:47:06.834 - 00:48:09.714, Speaker A: They're still complete subgraphs by the monotonicity and they have this induced subtree property. And for many purposes it can be used instead of a click tree. But it may be more efficient to work with this or construct it. And just to finish a few things we can, people in sparse matrix algorithms have developed very efficient methods for constructing elimination trees, getting some important information from the elimination tree. So for example, it turns out that it's quite easy to find all the clicks or maximal clicks from this elimination tree. And we actually don't need the full expanded elimination tree. It's sufficient to have the higher degrees of every vertex.
00:48:09.714 - 00:49:13.314, Speaker A: So if I just know these numbers, the number of vertices in this top row, I don't need to know the exact elements of it, only the higher degree. Then knowing the elimination tree and these higher degrees that I put next to each book here, there is a simple test to allows you to find the columns in the original pattern that define the clicks in the graph. The details already matter, but they're indicated in this click tree. I indicate those higher degrees next to each vertex. And then there's a very simple test for, if you have that additional information to find the vertices in the graph that define the cliques in the graph. And they're called representative vertices of each clique. And here they're indicated by squares.
00:49:13.314 - 00:50:34.354, Speaker A: So in this case a and the vertices adjacent to a are a clique, b defines a clique e and so on. So that's one nice result from sparse linear algebra. And another one is that we can also get more information about, we can actually relate this also to a click tree. And that brings us to definition of a supernode. So if we have the elimination tree with the additional information that we know the higher degree of every vertex, then I can find the representative vertices indicated by squares here and I can partition the vertices or the nodes in the elimination tree in different ways. But I can, for example from every vertex, look at the every representative vertex, look at the nodes on the path between that vertex and the next higher representative vertex. And those are called super nodes.
00:50:34.354 - 00:51:06.174, Speaker A: So for example, a, c and d here is a super node. B by itself then would be a super node. I could also have chosen b, c and d, and then a would by itself be a super node. So it's, the partition is not unique here. L, m, n, p, q would be a supernode, and so on. So I can partition the elimination tree or the nodes in the elimination tree like this. They're called maximal supernodes.
00:51:06.174 - 00:53:00.474, Speaker A: I can then collapse or compress the elimination tree by what's called a supernodal elimination tree that actually treats each of these supernodes as one node in the tree in sort of an obvious way. So ACD would be a super node and b would be a child of that supernode in the supernodal elimination tree. And then that's related to a click tree, because if I do the same as when expanding a standard elimination tree, so each of these super nodes is expanded and I also list not just the supernode, but all the entries, nonzero entries, in that column as a first row, then this is exactly a click tree. So the bottom rows are just the supernodal elimination tree. Each top row adds to it the other entries in the column. So this maximal supernodal emanation tree, actually when expanded, gives us click three. And then maybe the last thing to mention is that for this part, if we, after all this pre processing of the elimination tree or supernodal elimination tree, we can actually modify the ordering without any extra work and bring the pattern in a very nice form like this that we'll use when assume, when discussing the algorithms in the next lecture.
00:53:00.474 - 00:54:14.984, Speaker A: So here I, after all this preprocessing of the elimination tree, I rearranged the ordering like this. So the diagonal blocks in this pattern, these dense diagonal blocks are called the supernodes, maximal supernodes, they define dense diagonal blocks in the pattern. And then the other columns, the rest of the non zeros in each column defined by block column defined by a supernode, have the same non zero structure. So they, so that can be achieved very efficiently and for chordal sparsity pattern. And it's a very convenient form to start and discuss sparse matrix algorithms. So here we actually treat every supernode as more or less like a node and a standard elimination tree. But it's a block column of columns that have a very identical, almost identical structure.
00:54:14.984 - 00:54:38.084, Speaker A: I think I should stop here? Probably there's one topic left in this first part, but it's very closely related to the Schlesky factorization and then the topic of the next lecture. So this is probably a good time to stop for today. Thank you.
00:54:39.664 - 00:54:50.644, Speaker C: Thank you very much Levin. So, anybody have some questions?
00:54:52.564 - 00:54:53.304, Speaker A: It.
00:55:35.304 - 00:55:49.184, Speaker B: Just one brief one. It's more on matrix completion than chordal graphs. But if you are actually completing a matrix, do you use software to do that?
00:55:53.444 - 00:56:30.624, Speaker A: Yes, in some of the applications discussed later, we have a package that we wrote with some python code for manipulating sparse matrices and solving different types of completion problems. Some of those basic preprocessing results that I mentioned, you know, are available in Matlab or other packages. So you can find the elimination tree and first sparse matrix.
00:56:40.904 - 00:56:42.044, Speaker B: Great, thanks.
00:56:49.544 - 00:57:02.424, Speaker C: So a lot of these completions are for semi definite completions. Are there some other quality? Yes, so the matrices that you would used for completion.
00:57:03.004 - 00:58:25.094, Speaker A: So the types we'll discuss are posted semi definite and the euclidean distance matrix completion. So in the next second lecture. So those are the types of completion problems we'll discuss and use, of course, parts. Matrix completion is also many other types of completion. For example, the minimum rank completion of the general sparse rectangular matrix is very important, much more difficult than the types we'll discuss here. So in terms of types of completion, we look at the posted semi definite completion, actually two types, the maximum determinant positive semi definite completion and also the minimum rank positive semi definite completion. And then for euclidean distance, also we can look at the minimum distance matrix completion with minimum dimension or just a feasible euclidean distance matrix completion, or a completion with minimum dimension for choral graphs.
00:58:26.234 - 00:58:34.090, Speaker C: So for general minimum rank completion, the general minimum rank using nuclear norm, yes.
00:58:34.162 - 00:58:57.874, Speaker A: For a non symmetric case, but it's not something I will discuss in the lectures. That's a very common technique. But for coral grass and symmetric matrices, the minimum rank positive semi depth of completion is actually easy to determine, as we'll see.
00:59:16.034 - 00:59:35.866, Speaker D: Sorry, I have a very elementary question. It was just at the very start you said something which has kind of piqued my interest. So I'm totally new to chordal graphs. So chordal graphs are ones where the minimal, you said the minimal, minimal separator, minimal vertex separators are complete graphs.
00:59:35.890 - 00:59:37.174, Speaker A: Yeah. Yes.
00:59:38.054 - 00:59:48.574, Speaker D: Yeah. So for, for, so just to be clear, I, this confused me for a while by, by I, so I misread this to mean the separators of minimal size, but it's actually the minimal.
00:59:48.614 - 01:00:13.294, Speaker A: Yeah, so there's something in the definition. So minimal applies to the whole thing. Right. It's a minimal separator for two, given. That's why sort of, this example is sort of motivated by this. So xy is a minimal separator between a and c, but y by itself can also be a minimal separator between, for two different nodes. So it's not minimal.
01:00:13.294 - 01:00:14.506, Speaker A: Yeah.
01:00:14.530 - 01:00:21.034, Speaker D: So minimal, sorry. Minimal means minimal with respect to inclusion rather than minimal size or anything.
01:00:21.114 - 01:00:28.378, Speaker A: Yes, minimal respect to inclusion, and also for a specified pair of vertices that you separate. Right.
01:00:28.546 - 01:00:48.704, Speaker D: Yeah, no, I was just interested in this because for a different reason. I've, with other people, I've been looking at graphs where the, say, graphs of a certain vertex connectivity, where the vertex separators of minimal size are all complete graphs, but I guess that's a different concept.
01:00:52.444 - 01:00:57.076, Speaker A: Yeah. It's inclusion here, not minimum of size.
01:00:57.220 - 01:00:59.444, Speaker D: Okay. All right, thank you. Thank you.
01:01:05.504 - 01:01:42.774, Speaker C: So when you talk, when, when you will be talking about completions, will you discuss anything beyond. Well, I see you're taking advantage of sparsity, but what about also the difficulty of the problem? Is there anything hiding in the sort of hiding condition numbers in the problem, like certain graphs, even though you can express them as a completion problem, will be harder numerically to solve them than others, just from some hidden structure. Numerically, I mean.
01:01:43.514 - 01:02:02.574, Speaker A: Yeah, we want to discuss that. There are many interesting questions like this. Right. So we'll use coral grass, but coral grass will appear as an extension. We start with a graph that's just general. That's not necessarily coral. Then we'll use a coral extension.
01:02:02.574 - 01:02:30.714, Speaker A: And then there's a question of how you have a choral cycle. But I think the difficulty of the problem depends also on the length of this cycle, more efficient chordal extensions than other chordal extensions.
01:02:32.854 - 01:02:51.594, Speaker C: Like, for example, what if you didn't know that the graph was almost rigid, or even rigid? If it was hiding that it was rigid, would you, would you, would it be hiding the fact that numerically it's more stable, you get a more accurate solution?
01:02:51.894 - 01:03:41.104, Speaker A: Yeah, I don't know. It's really. So it's triangular. This graph called cordograph triangulated, is sort of a different, another name for coral graph from triangulated, as it's used in some of these, is more common in the early literature, but in those papers, it's just another term for corelograph.
01:03:42.204 - 01:03:43.124, Speaker D: Okay, thank you.
