00:00:19.440 - 00:01:29.674, Speaker A: There is a question a little too. Is the space of sequences. Yes, with the Euclidean norm. Yes, that's true. And f and gr sequences. It's better at this generality not to use the word sequences, because sequences are used for functions for which the domain is countable. I mean, let's, let me write more specifically, where's my share for, for a sequence or suite? As you said, usually we write it this way.
00:01:29.674 - 00:02:27.214, Speaker A: So either we write it x index n or x of n, the function whose value at the point n. Both are acceptable version the important thing is that x defined on n with values on r or c, the domain is countable. And in this case, yes, we can use the word sequence or sweet in French. But the way I defined f f is defined on omega with values in r or c. The restriction sounds very similar. It means that the sum of absolute value f of x squared x in omega is finite. That's true.
00:02:27.214 - 00:03:20.604, Speaker A: If, if you call, you want to call it the Euclidean norm, I don't mind. That's okay. But we cannot call f a sequence because omega in the general setting is not countable. And if this is not the case, then the word sequence or suite is not suitable for these elements. But they have several properties which are similar to the case of little l two I defined in the previous page the inner product. So a task is to, to show that the inner product is well defined, and another task is to show that the norm is complete. These are, I mean, just steps that you need to take.
00:03:20.604 - 00:05:46.024, Speaker A: And it's a very good exercise in measured theory to show that l two of omega is indeed a Hilbert space. The completeness is, I mean, takes a bit of time, but I mean, it's doable. And when you did this, then it's immediate to see that it's actually reproducing kernel Hilbert space, because for each x in omega x, fixed absolute value of f of x is less than or equal to the sum absolute value of f of y squared y is in our set omega and then root square, because when we consider all of these y's, one of them is y equal to x, which gives precisely this value, and the one we have on the right side is precisely the norm. So another way to write the same inequality with a different language is to say that e x of absolute value is less than or equal to the normal f in l two of omega. That's the same thing, but written in the language of function analysis. And this means that ex is a bounded functional and nor of ex as a functional or operator from l two of omega two, our field, which is r or c is less than or equal to one. And again, as in the previous case, which was cn, if we consider the the canonical basis, then we see that actually equality holds.
00:05:46.024 - 00:06:43.734, Speaker A: But I do not need that. Actually the norm of the x is equal to one. But I mean, that's enough for us. So we have therefore l two of omega is and rkhs, nrkhs or a reproducing kernel space. This is a very good example because it covers many, many special cases. As we saw before, if omega is just one two up to n, then we obtain cn. I mean, I write it here again because it's really important for us, is omega is equal to one two and.
00:06:43.734 - 00:08:20.704, Speaker A: What happened? Oops, sorry, I don't know what happened. I try to. Do we see the screen now? Yes. Okay, thank you. If Omega is equal to one two up to n, then l two of omega is our ordinary euclidean space cn in the complex case or rn in the real case, if omega is equal to n one up to infinity, then l two of omega. We usually write l two. Sometimes we, we want to be more explicit.
00:08:20.704 - 00:09:19.904, Speaker A: We write l two of n but little l two. Without any further indication, it means l two of n in our application, because of the Taylor series. Taylor series start in functional spaces, start from zero up to infinity, so we can see that omega equal to n union. 00123 and this is pretty much the same as above. And it's a bit misleading because even for this one we write little l two. So it's not a huge source of confusion. But when you see little l two, be careful that we start from zero or from one in the context.
00:09:19.904 - 00:10:11.794, Speaker A: And another one, which also is used in harmonic analysis, is l two of z. In this case we go from minus infinity to plus infinity. So zero plus minus one plus minus two, and so on. In this case, l two of omega and is l two offset. So these are the important cases that we usually consider in our course. That's not all. I mean, the next step is to consider when omega is not countable, for example, when omega is equal to r.
00:10:11.794 - 00:11:27.564, Speaker A: This is a very interesting example to consider. In particular, if you consider this set of functions ex defined by this formula, ex of y is equal to y at one if y is equal to x and zero if y is not equal to x. By diagram, this means that at point x, the function is equal to one and in all other points is equal to zero. So it's a function like this. And at the first glance, it said, oh, what's the use of this? If we consider the measured theory, Lubeck measure theory, it's pretty much the same as the zero function. But wait, that's not the case here. The collection ex, when x is in r, you see, it's not countable.
00:11:27.564 - 00:12:58.628, Speaker A: It's a collection which the cardinality is the same as the cardinality of r. This is an orthonormal basis for l two of omega. You see, it could be your first example of non separate case, that you have a Hilbert space for which you have an autonomous basis, which is not countable. So, more explicitly, ex et in l two of omega, it's Dirac notation is delta x t, which means that if x is equal to t, is equal to one, and if not, is equal to zero, and moreover, that's a complete set. It means that their combination is dense in this space. To you remember, in the previous case I had here it is a diagram like this. F lives here on the yellow part, g on the.
00:12:58.628 - 00:13:41.904, Speaker A: I mean, h on green part, g on many different colors. We can consider blue holes. They are countable. They might have intersection, but on a huge territory, f or h, I mean, they are equal to zero. Now, come back here. I say linear combination and linear combination means finite linear combination of ex is dense in l two. What are these elements? If you look at this, you see, look at this.
00:13:41.904 - 00:14:45.584, Speaker A: What is the linear combination? A finite number of these elements. So if I use lin for linear combination of ax, x is in r. Some books use also span. I also use it, but I try not to use it that much, because in many, many texts, span means linear combination and also topologically closed. I don't want to put the closeness here, just linear combination means that elements of this are of this form, alpha one ex, one plus alpha k ex k. Then k is arbitrary. Alpha k or alpha k are also arbitrary, but a finite number of them.
00:14:45.584 - 00:15:29.704, Speaker A: And this element has this form. Here is x one. Here is x two. Here is x k. It's alpha one here, for example, alpha two here, and alpha k there, a finite number of values. And for the rest, which is really huge, is equal to zero. So it's a function which is basically equal to zero, except at finite number of points.
00:15:29.704 - 00:16:36.566, Speaker A: And yet these functions are dense in this space. This way you can, you can understand the complexity of this space. So that is our second example. And it's better to say, indeed, the generalization, our first example of rkhs. Now we go, we stop studying examples, and we develop certain tools which we need in studying rkhs. So what is a kernel? You remember, in the world in the combination we have rkhs. We talked about hs Hilbert space, but we didn't talk about archaic kernel or reproducing kernel.
00:16:36.566 - 00:17:16.954, Speaker A: What does that mean? This is connected to duality and frizz theorem. A bit of preparation to give the definition of kernel. Then you have a banach space x. Then it's dual. X star is the set of bounded linear functionals on X. So everybody is familiar with that. Characterization of X star is very important.
00:17:16.954 - 00:18:11.284, Speaker A: And for a few spaces we know X star, but mostly it's an abstract setting. We know it's a space, we use it, but actually we don't know what it is. Let me see, what is the question? The orthonormal basis? The question now Sushil, is this. The orthonormal basis of l, two of omega will have a same cardinality as that of omega. For any set omega, the answer is yes because of this. Because of this. Or more generally, did I mentioned about.
00:18:11.284 - 00:19:23.702, Speaker A: Well, I mentioned this just for R, but it's not just for R. This ex is defined for any omega and the number of ex is the same as the number of omega. Or the cardinality of this set is the same as cardinality of omega. So the answer to the question of social is yes, the dimension of this space, put it that way, is the same as the cardinality of Omega. So I can add it here. The I can add it here, that the dimension as a Hilbert space of L, two of Omega, as you said, is the set same as the cardinality of Omega. But be careful with dimension, the dimension as a Hilbert space, not the dimension as a vector space.
00:19:23.702 - 00:20:44.644, Speaker A: The dimension as a vector space is usually higher because as a vector space, we should consider Hamel basis and the number of elements of Hamel bases are very bigger. Back to the definition of kernel. We start with x, a Banach space and x star is the dual. Characterization of dual is important, but not always doable for some spaces at earlier stages. Rees Efric gave the characterization and this is one of his important contributions to the analysis. For example, he showed that in the low space setting, the dual Lp is lq for p between one and infinity, not including infinity, just between one and infinity and p and q are conjugate exponent. This means that one over p plus one over q is equal to one.
00:20:44.644 - 00:21:36.624, Speaker A: So the dual, he characterized the dual of Lp as Lq and another example that he did, which is very interesting, is this the case of continuous function on a complex set k like unit circle. In this case, the dual is the set of Borel measures on k. I mean, we don't need these details here. But if you ever take a course on measure theory, and sometimes in function analysis, you see these two examples. The one we need is about a Hilbert space. And the answer is rather interesting. Again, it's a theorem of Ephesus.
00:21:36.624 - 00:22:13.674, Speaker A: It showed that if we have a Hilbert space, the dual of h, naively speaking. So I write what happened to my eyes here. I write the identity with red. We will see momentarily why. But naively speaking, the dual is h itself. This is the contribution of frisk. The dual of the Hilbert space is itself.
00:22:13.674 - 00:23:03.174, Speaker A: Why do we need to be a bit careful here? Because his theorem is like that. Fix y in H. Define the functional lambda from h to C. I consider the complex case, but the real case is the same, such that here is the definition. Lambda of x is the inner product of x and y in H. And it is better to, to call lambda lambda index y because it depends on y. So I put a little y here.
00:23:03.174 - 00:24:12.894, Speaker A: So lambda y acts on H. Its value on x is the inner product of x and Y. It's very simple and it's easy. It's just the elementary calculation. The inner product spaces to show that lambda Y is continuous and moreover, its norm. When I say its norm, it means that as a functional from h to c is equal to the norm of y as an element of h, you see? So therefore, I have been able to find a good collection of functionals on h indexed by element of H. And this indexing is one to one, because if lambda equal y one is equal to lambda y two, I take this to the other part.
00:24:12.894 - 00:25:33.372, Speaker A: And knowing that this is, I mean, really linear with respect to the second component, this means that lambda of y one minus y two is equal to zero. And this identity tells me that value of y one, y two, it's norm equal to zero. So y one equal to y two. So it's injective if now I define a map from h to h, star mapping y into lambda y up to here, we know that this map is one to one. And the reason for using a red color here is that it's not completely linear. If you have a complex number here in front of y and it comes out, it becomes if you have alpha here, it's conjugate comes out, you have alpha bar. So it's not completely linear, but not, that's not a big deal.
00:25:33.372 - 00:26:31.298, Speaker A: So the red, the red color is connected to this. And I write in red again here. The mapping is conjugate linear, not completely near. Well, we can live with conjugate linear. That's not a big issue. What is missing by now to, to give a sense to this identity? What is missing? What do you think? So if we close our eyes to the conjugate part, we have something which is one to one and linear. I mean, keep in mind it's really conjugate linear.
00:26:31.298 - 00:26:58.130, Speaker A: But still we cannot say h is equal to h star. Something is missing. And what is missing? Isometry. Isometry is addressed. Yes, thank you. I add this isometry is addressed here. And thank you.
00:26:58.130 - 00:27:29.274, Speaker A: I added here, it's isometric onto part. I'm sorry. Yes, yes, yes. And that is important. Onto. In other words, there is no other bounded linear functionals. What Rees constructed here by this formula, this formula, it's a functional, it's a collection of functional.
00:27:29.274 - 00:29:02.464, Speaker A: Reese showed that there is no other and there is no other is equivalent to this is onto. And therefore, when we put all of this together, we can naively write hook is equal to h star. So if, I want to summarize, the most important part of this observation of FrIs is that if lambda from h to c is a bounded linear functional, then there is a unique. The uniqueness is also important. Sometimes I use it a unique y in h such that lambda is equal to lambda y. Any linear functional is there. So this is something that you can find its proof in many books, including Rudin's book.
00:29:02.464 - 00:29:47.374, Speaker A: Why do I need this here? To see why I need here, I go back to the previous slide and go back to the definition of rkhs. I go back, here we are. The definition had three parts. Algebraic property, good. Topological property. That's good. And then the connection between them, the connection says that ex is a bounded linear functional evaluation functional, which is bounded.
00:29:47.374 - 00:30:51.634, Speaker A: So I go back to my slide. So if h is an Rkhs on omega by assumption ex, from omega to, I mean, I write c. But you keep in mind that it works for real one too. No, no, no. Sorry, sorry. Not from, not from omega, from hook is a bounded linear function. Therefore, this theorem applies by f rise theorem.
00:30:51.634 - 00:33:08.874, Speaker A: The theorem that we saw here. There is an element, there is indeed a unique element, which we denoted by k little k index x, because it depends on x in age, such that x of which is f of x is indeed equal to the inner product of f and k x in h. That's the beginning of a studying kernel rkhs. The k is for this part kx, we simply call it kernel or more in more details, the reproducing kernel for the point x, point x, and using little k, we also define a capital k. Note that capital K has two variables defined on omega times omega with its values in c or in r. By simply defining k of x and y to be the little ky evaluated at point x. That's a definition of capital K.
00:33:08.874 - 00:34:19.164, Speaker A: And capital k simply is called the reproducing candle for h. So capital k for h. So when we say reproducing kernel for h, we are referring to capital x. When we simply say reproducing kernel or reproducing l at the point x, we refer to legal k of x. But I mean, they are not two different objects in a sense, because they are closely related by this formula. And now several properties of little k and capital k, and we verify them in the spatial case we saw before. Let's start with this one.
00:34:19.164 - 00:35:18.424, Speaker A: Always keep this formula in mind. This one f evaluated at x is the same as the inner product of f and k x. So what is k? I need ky at point x. You can look at this as a function f. You know that f at point x is f inner product with kx. Now you replace f by ky. So ky at point x if ky inner product with kx.
00:35:18.424 - 00:36:33.674, Speaker A: And now you see another formula for capital k. Therefore, capital k at point xy is defined to be little ky at point x, but it is also the inner product of ky and kx. You need to be careful about the ordering. It's usually a source of mistake, even though in the, in the real case it really doesn't matter. But in the complex case, it does matter. For example, let's see, what is the relation capital? What is the relation between capital k, x y and capital kyx? Is there any relation between them? Well, we look at the formula with which we have about k, x and y. The ordering says that is little ky and little kx their inner product.
00:36:33.674 - 00:37:37.854, Speaker A: But k, y and x is, you change the order of x and y. It's little k of x and little k of y. And we know that f g is equal to g f bar. Therefore, we immediately conclude that k x and y is equal to k, y and x bar. That's in the complex case. But in real case, we have k is equal to, it's symmetric in the real case. So it's a function of who variable, which is either having this property or this one.
00:37:37.854 - 00:38:27.010, Speaker A: This one is called symmetric. The other one conjugate symmetric. Another consequence of this. Yeah, we use this formula a lot. Another consequence of this, which is related to Reese theorem. Somebody mentioned isometry here is the norm of lambda. Y is the same as the norm of y in h and we have an incidence of this.
00:38:27.010 - 00:39:30.202, Speaker A: Here we have ex of f is the inner product of f and k of x. I write it again below because I will need it when you study several of spaces and other examples. So either x of is the inner product of f and k. Therefore, immediately we see that the norm of ex as a functional from h to c is equal to the normal kx in h. This is a special case of what we wrote about that the norm of lambda Y is equal to the norm of y. Very good. We can continue further, but keep this formula in mind.
00:39:30.202 - 00:40:08.952, Speaker A: We'll use it several times when we go to examples. What is the norm of kx? Still we can say a bit more. The norm of kx squared is the inner product of kx and kx. That's the definition of norm. And now we can write this inner product in two ways. Either we can say that this is the function f inner product with kx. So it's the value of this function at the point x.
00:40:08.952 - 00:41:21.724, Speaker A: So it's kx evaluated at point x. We can also say that it's the capital x on the diagonal on x and x. Both are correct. And therefore, when we mix this with the, with this formula, we see that the norm of example is actually is equal to the square root of kx of x and is equal to the square root of k at the point of diagonal x in x. So that's another more explicit formula for the norm of x. We will see why this formula is important, because usually in operator theory and in functional analysis, it is very difficult to find the precise value of an or. We define an operator, we show that it's bounded or continuous even we obtain an upper bound for the norm.
00:41:21.724 - 00:42:19.350, Speaker A: But the precise value of the norm, usually we do not know even. There are celebrated examples in which doctoral thesis was on evaluated the precise value of a norm. I mean there has been doctoral thesis in Michigan State University on the risk projection to find the precise value, or Mars and Reese, I mean orthogonal projection. And we sent p to p plus whatever it means. You need a bit of complex analysis. Marcel Rees showed that it's a bounded operator, obtained an upper bound, but didn't obtain the precise value. It was obtained way later also for the conjugation.
00:42:19.350 - 00:43:06.184, Speaker A: And here is a case in which we have a functional, a functional is an operator for its norm. We have a very precise formula. We will see when we study the examples that this formula in parks is very useful, because we have some operators. If out of nowhere I come and say, okay, find the norm of this operator. Probably you cannot do that. But if you keep in mind that it's a reproducing kernel Hilbert space, we can apply this and immediately obtain the value of the node. You will see, I mean, do not worry about the examples, but you will see that it is true.
00:43:06.184 - 00:44:38.604, Speaker A: So, back to our specific examples that we saw before we saw Cn. What is the kernel for Cn? The kernel for Cn you have already seen, but under a different name. Now let's, let's find it more explicitly. What do we want to find? We want to find, I mean, fix, say j in our omega, which is a finite set. What do we want to find? We want to find kj, an element of Cn, such that for each z in Cn, if we calculate inner product of z and kg, it gives us the evaluation of z at the point g, or if you wish, z index g. We want to find this, and it's not difficult to find this. Just look at what is the inner product z.
00:44:38.604 - 00:45:04.184, Speaker A: Either you think it as a function z at the .1 at the .2 at the point n, or as a vector of the dimension n. This inner product in Cn is written this way. Z at .1 kg at 0.1 bar, plus z at .2
00:45:04.184 - 00:45:56.664, Speaker A: kg at .2 bars up to the last one. That's the definition of inner product. And note that z one, z two and zn are free. I mean, they can change, absolutely no restriction on that. It can be anything. And now we have to find kg of one, kg of two, kg of three, such that this identity or this set of identities holds for all possible values of z one, z two and z.
00:45:56.664 - 00:47:50.604, Speaker A: And this happens if. Probably, if I write this identity this way, you see what should happen. This part, I keep it as it is. Z one kg 1 bar. And the other part, I write z zero times kg of one plus zero up to, up to still z up to z g times one plus last one, which is, again, no, I'm sorry, I wrote it badly. I write again, I write it this way, I have just one term, zg. So I write z g times one, and all other terms disappear.
00:47:50.604 - 00:48:40.994, Speaker A: They are zero. So I write z one time zero plus here, plus plus zn times zero. Sorry, I should have written it that way. I want this identity to be true. For all values of z, one z two and zn. And you see, because of the freeness, this happens if and only if kg of I equal to zero. If I is not equal to j, and if I is equal to j, kg of g is equal to one.
00:48:40.994 - 00:49:30.564, Speaker A: Either you think of kg this way, or equivalently you think of kg as the vector 0001 here, that's the j's place. It's the same thing. Either you, you think of kg as a function as here, or as a vector like here. So I obtain my kernel, my reproducing kernel. But this is not something new. You have seen this before. That's precisely the canonical orthanormal basis that you have seen before.
00:49:30.564 - 00:50:30.914, Speaker A: And big capital k ing, what is the formula for this? Again, we have seen this before, this little kg evaluated at point I. And we know what it is. It's written here. If one if I is equal to j and zero if I is not equal to j. If we think of k I g as an n by n matrix, we can say that k is the identity matrix, because on the diagonal is equal to one and elsewhere it's equal to zero. That's the interpretation for Cn. The interpretation for l two of omega is the same.
00:50:30.914 - 00:51:20.822, Speaker A: Precisely what is written here is correct. That's the kernel for l two of omega, except that our omega is not necessarily one two n. It can be any set. So in this case, we already saw the definition of what's going on. In this case, for any case for any x. We already saw the definition of ex in previous slide. I repeat, ex of y is one and zero if x is equal to y.
00:51:20.822 - 00:52:28.854, Speaker A: If x is not equal to y, and reproducing kernel kx is precisely equal to ax for all. In other words, even in this case, the canonical orthanormal basis is the same as kernel. That's not the case all the time. I mean, we will see different examples, but in little l two, we have this interesting phenomenon, and capital k, capital k is, is the same as identity, but identity on the set omega times omega if it's n dimensional, is an n by n matrix, but it would be an infinite matrix with countable indices or even uncountable one. It's a matrix which is huge but equal to one on the algorithm. Cover this, cover this. Aha.
00:52:28.854 - 00:53:40.634, Speaker A: And now after the break, I will start talking about a very interesting example, which is Sobolev space, several different versions of this. And then, I mean, that's the beginning of, indeed a series of, series of serious examples. I mean, the examples we saw up to now cn and l two of Omega. Well, they are good, but in a sense it doesn't show the real power of rkhs. To see the real power of rkhs, we start with good, I mean at least maybe ten examples. And each of these examples will take time because I need to explain what is the Hilbert space structure and then convince you that the evaluation functionals are continuous. And after doing this, I need to find the kernel.
00:53:40.634 - 00:54:29.114, Speaker A: And finding the kernel is the biggest task. It's not that trivial. We will see there are some difficulties in each case we will cover, I mean, try to cover the difficulties and find an explicit formula for the kernel. And by doing this, I mean after having so many examples, you will be more familiar with explicit examples of RKH and different applications of this theory. This will take two, maybe three weeks. And in the middle, of course I will talk about some properties of rks two, but my emphasize is on the examples. After that we enter the more abstract setting.
00:54:29.114 - 00:54:39.594, Speaker A: Okay, now, I mean ready? Feel tired? And another stop, let me stop the.
