00:00:00.680 - 00:00:30.176, Speaker A: When we think about graph analysis, we think about ETL. We have to get the data from our existing databases into that graph storage. Well, I'm going to show you a way to do it with zero ETL, which actually outperforms what you'll see with traditional graph databases. So a little bit about me here. So my name is Matt Tanner. I am the head of developer relations at Puppygraph, which, which is a graph query engine. If folks are familiar with Trino.
00:00:30.176 - 00:01:00.230, Speaker A: It's very similar to that. But for graph, my background, I actually have quite a bit of background in banking. I actually worked at Scotia for a few years and then I was also an architect at TD for a few years as well. I didn't work in the graph space at the time though. And then I moved to work for a company called Dgraff and that's what got me into the graph space. And now I'm working with the folks overdose at puppy graph to kind of bring this into the hands of folks like yourself. So we see the traditional pains of graph dbs.
00:01:00.230 - 00:01:55.330, Speaker A: Like many of you have probably tried to bring in a new graph technology. It's pretty tough to do because there's a lot of stuff underneath it. You have to have a lot of ETL pipelines. The scalability of this stuff is not always super great. So we got the data ingestion stuff and then the performance of it is a few of you probably have used Neo four j, where you start to kind of dunk out on some of the higher complexity queries and then the cost of it, obviously the cost of running a graph database and all that underlying infrastructure is pretty expensive to do. So what we have is we've built and I'll show you kind of what the architecture looks like in a moment. But if you have any of these data sources here, you can actually use that as your underlying storage for the graph database and then use puppygraph as essentially the compute engine for it.
00:01:55.330 - 00:02:39.702, Speaker A: So no ETL is required. All you would do is I'll show you, I'm going to show you how to quickly do it with iceberg today. You have your instance, you create your schema and boom, these data sources now give you that underlying storage to run graph queries and we can run them extremely efficiently. I'll show you some breakdowns of, of what we can do compared to neo four j so supported query languages. We support Gremlin and Opencypher and then we also have some client libraries that you can plug in as well. So Java, JavaScript, Python and go and we also have some supported integrations. So a lot of the time you might want to take that graph data and move that into another platform.
00:02:39.702 - 00:03:25.040, Speaker A: We can also, if you're using any of these, we can actually integrate with each of those, too. So now you don't have to worry about having the graph storage there. You can actually just go ahead and deploy our solution and then link it up with some of these existing technologies if you're using them. So how does it actually work? Dan, who's on the call, knows just monumentally more than I do about the underlying infrastructure, but I'll give you kind of a brief overview of it. So we have the query languages, then we have the actual query engine itself. And you'll see that the logical plan and physical plan are divided. And then you have the execution nodes which then come out to the data sources themselves.
00:03:25.040 - 00:04:07.710, Speaker A: What this doesn't do is it doesn't take a graph query, translate it to SQL, and then run that query. That would be extremely inefficient. We actually are able to tie into the underlying table structure and then grab the data that way. Dan, I think we're actually going to be publishing some videos in the next little bit that talks a bit more about how it's physically possible for us to do that. But that's kind of the high level of what we're looking at here. So when we look at what it traditionally seems like, many of you are probably running graph databases similar to this, where you have a bunch of sources, you've got these ETL pipelines that are dumping it into likely a data lake, and then you're dumping it into the graph database as well. You have to maintain those ETL pipelines.
00:04:07.710 - 00:04:59.930, Speaker A: What we do is just slightly different. You're still going to have your ETL between your SQL sources and your SQL data lake because you're probably going to still want to run SQL queries on it. But if you want to access the SQL source, so let's say you have a postgres database sitting here and then maybe you have Delta Lake sitting there. You can either connect to them individually, or if you know that all your data is going to be sitting in your data lake, you can just connect it directly to puppygraph. That is going to give you the ability to take graph queries that you've already written in Gremlin or in cypher and run it without having to have this underlying infrastructure there. So it makes it a lot simpler. And what we actually are able to do is we're able to do some ten hop queries in under a few seconds.
00:04:59.930 - 00:05:23.838, Speaker A: And I'll show you a case study as well in a second here. But this kind of speaks volume. So we actually did a benchmark against neo four J. Actually we didn't do this benchmark. We actually had a customer that came to us and said, hey, we ran this benchmark and you probably want to see what you're doing. And we saw it and went, whoa, it's much quicker on some of these queries. So you can see here, here's the runtime in seconds.
00:05:23.838 - 00:05:53.620, Speaker A: So neo four j is the blue bar, which means that's a higher runtime. And then this is a source vertex id here as well. You can see that for many of you in the back, you probably can't even see the little red marks here. And that's how fast our query engine was able to execute on it versus neo four j. We did not anticipate it being as quick as it has been, but we were able to. We've got a pretty good crew of four core guys that are building the engine and we've been able to refine it quite a bit. It's even quicker now.
00:05:53.620 - 00:06:28.060, Speaker A: So I can't tell you exactly who this case study is on, but if you go to our website, you'll see their logo there. Maybe the blue gives it away. It's one of the world's largest crypto trading platforms. They were building a project with a more traditional graph project. They were having a hard time getting it off the ground. They were taking, I believe it was a couple hours to run some of these queries we came in. We're able to run them in sub minute type of timeframes.
00:06:28.060 - 00:07:07.302, Speaker A: And this is what they got. After using us, instead of taking, they actually had an in house graph database as well as some other vendors they were using. They were actually able to release their automated system that they'd been working on for a very long time. They were able to release it in production like very quickly. I think it was less than a few months, achieved five hop paths between a and b in 3 seconds across a few hundred million edges. And then they did a POC with us. So if you guys have ever done a POC with a graph database, usually it takes a while, right? You have to procure the solution itself, then you've got to provision all the infrastructure with this.
00:07:07.302 - 00:07:58.840, Speaker A: They were able to do it in less than a day and they shipped to production in less than six months. So yeah, there's a bit more on it if you go to our website as well. And now let me show you really quickly how this works. So I'm going to show you, I'm going to actually, I'm basically going to tether into a iceberg instance that we've got running on tabular, which just got acquired by databricks, but we're running it on there. And I'm going to just show you how I can run a couple of graph queries without having any actual graph infrastructure underneath it. Really? And I will exit out of this for a second and I will come over here and first I actually want to show you this. So what kind of powers all of this underneath? You can see here that, oh, I'm sharing the wrong screen.
00:07:58.840 - 00:08:02.190, Speaker A: One sec, let me see if I can switch this.
00:08:02.850 - 00:08:09.898, Speaker B: Okay, stop. Share, I will share my other, I'll.
00:08:09.914 - 00:08:32.450, Speaker A: Just share my whole desktop for a second, actually. Great. Okay, so might be a little hard to see. Let me see if I can make it a bit bigger for you. So in order to actually connect to the database, I'm running this on Docker right now. I'm running this on Docker locally. All of you in here can access this product completely for free.
00:08:32.450 - 00:09:28.050, Speaker A: Now we do have an enterprise version of it, but you can run it on a single node and do most of probably what you're doing already. I put in my catalog info. Now I can connect to multiple, so if I have multiple data sources I want to stitch together as a graph, I can do that here. I'm only using one, but you can see I'm tossing in my connectivity details at the top and then I'm defining another object which is an array of vertices. So I've got my label and then here this mapped table source, it says this is the catalog, this is the schema, here's the table, and here's the meta fields that I want to include in that edge, sorry, that node itself. And then you can see here that I've got all my attributes. So I can just go through and define this schema and then I upload it into the platform which is right here.
00:09:28.050 - 00:10:21.220, Speaker A: And again, very simple schema on this one. But this will create the schema in here and then it will go to the underlying data source and map that stuff over into our query engine. So at this point I can start querying. Now if I had done this in front of you guys, I would have brought this up in Docker, I would have logged in and I would have basically come to this right away, except I would have taken that file, dumped it in and away. We'd be going maybe five minutes more on what we did here, but I can come over to query and I'm going to grab one of these pre canned queries that I've got here. And again quite simple, but I'm going to query for confirmed financial fraud users. So push this here and then I will do, I'm going to run this and then I will run this here.
00:10:21.220 - 00:11:02.218, Speaker A: And I can see now that this is actually pulled from the cloud instance that I have running of iceberg and I was able to pull in this graph based on the query that I have here. You also see that I've got Gremlin console, cipher console, and then I've got graph notebook. So if you want to run certain queries in Python or something like that, you can go to graph notebook and it will use this underlying query engine to run them as well. Now that first one, relatively simple, but what I want to do is maybe go there. These here, let me see, let me.
00:11:02.234 - 00:11:03.770, Speaker B: See if I can zoom up a bit here.
00:11:03.930 - 00:11:08.378, Speaker A: So I don't have any connections on these ones because let me see, I.
00:11:08.394 - 00:11:21.230, Speaker B: Can start to expand here. So expand it. Expand with edge label. This should work for me.
00:11:22.090 - 00:11:28.346, Speaker A: I just started playing with this dataset last night. I'll be honest with you, we should.
00:11:28.418 - 00:11:29.266, Speaker B: See, there they are.
00:11:29.298 - 00:12:03.400, Speaker A: Okay, so now I can start to see some of my edges that are coming out here. Move this over. These might actually be completely unconnected right now. I think that's probably what it is. So what you'll see here on the side though is it actually runs a query. So if I click one of these nodes, I can expand with all edge labels, expand with edge label and specify which edge label I want to expand on. I can also do query and view properties.
00:12:03.400 - 00:12:40.156, Speaker A: So if I have this visualization there, I can go and actually search for specific fields if I want to. And let me run another query for you here. So we can also run these cipher queries as well. So this one here establish a relational pattern based on the following criteria. So one user transfers money to another user who shares the same credit card. So if I come over to cypher console, hopefully this runs relatively quick. I don't have much memory allocated to my docker instance right now.
00:12:40.156 - 00:13:19.670, Speaker A: So let's see here. I think I've only got, yeah, there we go. So you can see here that I'm able to get back my query result again without having any graph storage. I'm able to leverage the existing SQL storage to run these queries and I'll skip. Yeah. So, and we can also run, so here we have, we're running a weekly connected component algorithm here. So group accounts with transfers, with transfer records and shared credit cards using the weekly connected component algorithm.
00:13:19.670 - 00:13:58.024, Speaker A: So if I run this, come down here, I will then be able to, we have a decent amount of support for algorithms. We're slowly building out the library more and more. Like I said, if you're running, if you bump up the ram a little bit, you'll get a much quicker query result. But here is my result here from that specific query. And then I'm just going to run one more for you. So here I'll run a Gremlin query for if there are confirmed fraudulent users within a group. So query users within a specific group.
00:13:58.024 - 00:14:39.178, Speaker A: If there are confirmed fraudulent users in the group, then other users might be fraudulent users as well. So this is going to be based off that weekly connected component id, which is this id here. I will take this and come back over here. One sec. Gremlin query. Now you can run this in a Gremlin console as well if you want to, but I'll run it here so we can see the visualization of it on this. And there we can see, we can start to explore this graph here if we wanted to.
00:14:39.178 - 00:15:03.740, Speaker A: And of course, if you want to. If I do expand with all edge labels, it will run a secondary query. Oh, there might not be any edge labels in that one. Let's see. There we go. And you'll see that it actually runs a subsequent query here. So as you explore the query interface, it will actually save your queries here as well.
00:15:03.740 - 00:15:33.620, Speaker A: Oh, there it is. Perfect. So then we can start to build out and explore this graph further and further. So the gist of it is, if you want to be able to run graph queries without actually having to have underlying graph storage. So if you have a database that you're currently running SQL queries on, you'd like to explore as a graph, this is going to allow you to do it even on your local machine. Spin up a docker instance of it, connect to it, and away you go. That's all.
00:15:37.440 - 00:15:42.740, Speaker C: Thank you very much. Matt. We have time for maybe one question.
00:15:45.680 - 00:15:51.496, Speaker A: And if you do have more questions that come up, you know, you can always connect with us on socials and stuff as well.
00:15:51.608 - 00:15:52.912, Speaker C: I have a question.
00:15:53.016 - 00:15:53.660, Speaker A: Sure.
00:15:54.040 - 00:16:11.580, Speaker C: So typically when we go, when we do real world graph models, the graph resides. You showed my SQL, it's a tabular data in mysql. How do you go from mysql to a graph.
00:16:12.520 - 00:16:42.686, Speaker A: Yeah. So what would happen is based on the schema that you have there, you would then define what the graph schema would look like. So the same way that if you were to build some type of transformation in your ETL pipeline, essentially you're getting around that by going, here's how I want that to map into the graph. Same thing you would do if you were doing the transformation in the pipeline, but instead we actually handle it on our side without the ETL portion. Does that answer for you? Yes. Okay. Thank you very much.
00:16:42.686 - 00:16:43.030, Speaker A: Awesome.
