00:00:01.520 - 00:00:26.454, Speaker A: Given a matrix. Right. So you can assume that it's a real symmetric matrix. Given. Whoops, what did I do? Did something by which the whole thing went out of the window. Okay, so there we go. Start again.
00:00:26.454 - 00:01:31.898, Speaker A: Not quite sure why my tablet every once in a while reacts in certain ways. It has its own cantankerousness. Okay, so given, I think maybe if I just start it given a matrix, and I'm just going to call it Dij. It's a weird notation, but you understand what I mean. So how many test if it is metric? Okay, so I j go from one to n. Okay, so we want to know, let's, you can assume basic things. You don't have to worry about it being symmetric.
00:01:31.898 - 00:02:27.681, Speaker A: So you know it's symmetric real, and you can also assume the diagonal zero. Okay, so the question is, how quickly can you do this? So do you want. So to be a metric, you know, the triangle inequality has to hold. So dij, so for every ijk, so and in any order, so basically you have. So every, for every order triple, essentially you have dij has to be less than or equal to the sum of d I k plus djk. So you might wonder what I'm asking. So it's obvious that I can do this.
00:02:27.681 - 00:03:11.898, Speaker A: I can check for every triple, check every triple. So ordered triple. So there are n, you know, n cubed, order n cubed. I would say order triples. And that should, that will do the test. So in other words, you would just go one by one, look at every order triple. You can sort of assume that I naught equal to j naught k, I guess, but doesn't matter to the big o complexity of this algorithm.
00:03:11.898 - 00:05:22.474, Speaker A: So now the question would be, the question to you is, is it the case maybe that some of these inequalities, once you have checked them, automatically imply the others? So in other words, is it possible that you don't need to check every ordered triple, that there is a subset, a significantly smaller subset? Okay, is it the case that there is a significantly smaller subset of checks that you can make? By the way, you can interrupt me any time you want and talk or the problem I have is not being able to look at the chat so well, but if you talk, I can hear you right now. I hear somebody talking, but don't know what they're talking. I can read any chat questions, if any come up, if that helps. Yeah, okay. Yeah, that will help. So this is a sort of a basic, very basic question that we will deal with when we were talking about when you're talking about distance matrices, where, what types of distances? So, in other words, is there something underlying about the type of distance that if you know some of these inequalities hold, that the others automatically fall? Okay, so if you want to say no, you do need omega log n checks. Okay, suppose you are going to claim that you still need to check omega triples.
00:05:22.474 - 00:06:09.944, Speaker A: I'm going to close the chat so that I have a little more space here. Okay, so if you see that, you know, if you want to claim that, in fact you're going to need all of them, what are the tools you're going to use to show this? Okay, so how would you show this? Any ideas? By the way, you can call me anything you want. You can call me Mira or sitaram or professor or whatever, but I don't really care. You don't have to follow William and call me doctor Sitaram either. You can call me anything you want.
00:06:10.404 - 00:06:15.104, Speaker B: Some constructive examples, some family of examples.
00:06:18.564 - 00:06:56.404, Speaker A: I see. So you would give an example that shows that. And what form would this example take? So, remember, showing something like this is the equivalent of saying that any algorithm that tries to use a smaller number of checks will not succeed. So in other words, you have to construct your example to. As. As an adversarial example. So, to the algorithm, right? So you'd have to.
00:06:56.404 - 00:07:19.820, Speaker A: Whatever algorithm is used. So you want to essentially saying that no matter what algorithm you use, no matter what order you check these triples in, no matter what algorithm you use, there exists a distance matrix. I will call this matrix, something like delta, like that.
00:07:20.012 - 00:07:31.174, Speaker C: One can prove it is a positive semi definitely, then you can. If it is a positive semi definite, then it is a distance metric.
00:07:33.194 - 00:07:59.126, Speaker A: No, I'm not asking that it is a. That it be. I'm not demanding that it be an euclidean distance matrix. All I'm asking for is that it satisfy the triangle inequality. You see? So I'm not asking that it should be embeddable in euclidean space at all. There is a difference. There are.
00:07:59.126 - 00:08:20.680, Speaker A: So we'll talk about these differences. So you. I think this was Aditya. Yes, yeah, Aditya, you are talking about, you are sort of equating, in a way, distances that come from points that are in euclidean space.
00:08:20.832 - 00:08:21.564, Speaker C: Yes?
00:08:22.104 - 00:08:28.564, Speaker A: With distances that merely satisfy the triangle inequality.
00:08:29.824 - 00:08:30.464, Speaker C: Yes.
00:08:30.584 - 00:09:08.111, Speaker A: So there are lots of distances that satisfy the triangle inequality, right? That do not, cannot be embedded in euclidean space. So, all I want to know is whether this is true for every triple. Okay, okay. So you want to show that for every algorithm there is an example. Or there is a metric space, if you will, or, sorry, distance matrix. There is a distance matrix. The, I called it delta just a moment ago.
00:09:08.111 - 00:09:38.864, Speaker A: So there exists a delta, which requires. So you can build the delta as the algorithm proceeds and essentially say, well, until you check all the triples, you can never be sure that, in fact, all these inequalities. Is the question clear to people. Okay, so any ideas as to how you would show such a thing that you require all the triples to check all the triples?
00:09:41.444 - 00:09:43.316, Speaker C: I have a vague idea.
00:09:43.460 - 00:09:48.092, Speaker A: Okay. I mean, could you just construct some.
00:09:48.268 - 00:09:52.700, Speaker C: Point that satisfies every single one of these inequalities except one of them?
00:09:52.852 - 00:10:24.560, Speaker A: Ah, good. Okay, so what we're going to do is ask ourselves, what does this collection of triples look like? So basically we've got this distance matrix delta. You can think of it as living in our essentially n choose two. But it's symmetric. I told you it's symmetric. And all this other stuff. So it's probably going to be something like n minus one, choose two, or something like that.
00:10:24.560 - 00:10:51.784, Speaker A: So it's essentially all the entries, the upper triangular entries of a matrix, n by n matrix. So I don't know what that number is. Is it n minus one? Choose two. Yeah, something like that. Okay, so it's not n squared. It's only this part. So that's, so Delta is living here.
00:10:51.864 - 00:10:57.152, Speaker C: I think it's n. Choose two. That's plus one if you have.
00:10:57.208 - 00:11:08.880, Speaker A: Oh, yeah, yeah, you're right. So if it's n by n, it's just n. Choose two. You're right. Okay, so delta is living because it's all pairwise distances. Yeah, exactly. So it's, it's n.
00:11:08.880 - 00:11:45.554, Speaker A: Choose two. Okay, so Delta is living in r n, choose two. And it has all these inequalities. So for every pair. So it's got all these di j's sitting there. Think of it as just one long vector, and this vector is satisfying these inequalities. So it's satisfying for every ijk, as we wrote, dij is less than or equal to dik plus d jk.
00:11:45.554 - 00:12:28.558, Speaker A: It's satisfying all of these. So essentially, if you think of one of the one such delta, it must lie on some side, one half of this hyperplane. Hyperplane in rn, choose two, defined by this inequality. So as all the combinatorial optimization people in this class. No. So this is just going to be the intersection of a bunch of these half spaces. So it's going because it's an intersection of a bunch of these half spaces, the set of all delta that we are thinking about are lie.
00:12:28.558 - 00:12:53.714, Speaker A: I can't draw in n, choose two dimensions. So I'm drawing in two dimensions. So think of these coordinates in my thing, there are going to be n. Choose two of these. So you're living in entrust two dimensional space. And delta is a point that lies inside. It's an intersection of these various half spaces.
00:12:53.714 - 00:13:24.954, Speaker A: So each of these represents a half space. It doesn't like me drawing these things. Okay, there we go. Okay, so it's an intersection. So there's one half space like that, and then there's another half space like this. Let's say here for these guys, it's an intersection of each one of these inequalities represents a half space. So it's an intersection of these half space.
00:13:24.954 - 00:14:17.522, Speaker A: So it's a convex cone. It's a cone. So and a cone is convex. So the question, what Daniel said, one way to show that, in fact, you're going to have to check every triple, is to notice that the boundary facets of this cone, how many boundary facets of this cone are there? So this is living in n two's two dimensions. But you can always take some triple and make this an equality, you know, make this inequality actually hold. So in other words, you can make IJ actually equal to ik plus jk without making any of the other inequalities into an equation. Equation.
00:14:17.522 - 00:15:04.094, Speaker A: So in other words, all the other inequalities are proper and only one of them, exactly one of them is not proper. So in other words, is equal is an equality. So in other words, what we have, that simple observation shows that there are actually n. Choose three boundary facets of this cone. Boundary faces or fast. I prefer to use facets because then we can, I use facet as just a kind of a well defined boundary area, regardless of whether it's a polyhedral object. In this case, it happens to be a polyhedral object because these are all linear.
00:15:04.094 - 00:15:32.754, Speaker A: But in general, that may not be the case. It may still be convex. And I'm thinking of the boundary sort of boundary. If you think of it as this complex, you can think of specific boundary facets. So that's one way to show that you actually have to check. None of them implies. So none of these inequalities is somehow hidden inside the cone.
00:15:32.754 - 00:16:28.924, Speaker A: So it's not like usually when you say, oh, there's a bunch of inequalities. So, for example, you could have some inequalities like this, each of which represents a half space. And. But there could be another inequality. So you could have sort of finished this, gotten some kind of a closed region or something like this, through these inequalities. And then you may have, you know, another inequality here that sort of cuts through here and does that, which, which is a useful inequality, but there could be another one that's sitting here, for example, which doesn't really do add anything. So, in other words, all of these inequalities already imply the last one that I just drew.
00:16:28.924 - 00:17:20.084, Speaker A: So, but what we are saying is that that's not the case. All of these inequalities are actually useful. They actually are important or necessary boundary facets that define this cone. Okay, so this is the kind of, and so, beyond saying all that, the question of what are the extreme rays of this cone? So how do you. So it's a convex object. And you suppose you want to write this every that, because it's convex. You can take any vector in here and use a different color, any vector in here, any delta, and write it as a convex combination of some extreme rays of this cone.
00:17:20.084 - 00:17:53.624, Speaker A: So, for example, it would be good to know what these extreme rays are, how this facet structure here is, what the. So let me try to write these. The facet structure, the geometry, right, facet. So some reason it's acting up. Okay, it's a different color, maybe. Okay, facet structure. Extreme raise.
00:17:53.624 - 00:19:31.734, Speaker A: Extreme raise. You know how this project, if I took some subset of these distances, for example, I happen to pick some smaller number, some collection of these distances, let's say, and I want to project the cone on those. What does that projection look like? How does it depend on the collection of distances that I took, which is really a graph? Because I picked a set of pairs, right? So, projections, projection and fiber. So if I took a projection of this cone here, I could take a projection of this cone here, and then I can take what's called a fiber, which is the intersection of the, the, in a sense, the orthogonal direction space. With this cone, we'll define all of these things later. Sections or fibers, what do they look like? Their projections on some of the other coordinates and so forth. So these are the kind of questions that are turn out to be extremely interesting, and not because we're looking at arbitrary dimensions.
00:19:31.734 - 00:19:45.094, Speaker A: Many of these questions are still very much matter of research. Okay, so I want to point people at this point to.
00:19:49.914 - 00:20:07.078, Speaker B: Mira. Can I ask a question? I don't understand two things here. One is why there are n. Choose three boundary facets. Is that supposed to be clear to us? Or is it some result that we'll later.
00:20:07.206 - 00:20:33.974, Speaker A: No, no we're not. It should be clear to you. So I can choose one triple and make this an equality. So in other words, I can take dij to be equal to dik plus DJK without affecting any of the other three. Other n two, three minus one triples. In other words, they will all be inequalities. Strict inequalities.
00:20:34.394 - 00:21:19.684, Speaker B: That sounds not so obvious to me that you can do such a thing. That seems to me to require some proof. Maybe I'm missing something. Also, the picture that you drew right above the end. Choose three. It seems to suggest that there could be, even if you could prove that there were boundary facets, there could still be these hyperplane inequalities that are forced to be satisfied by the others. I don't see why that.
00:21:21.424 - 00:21:29.084, Speaker A: Was wrong. Really. Because this is all ordered. I mean, yeah, there are.
00:21:29.624 - 00:21:30.272, Speaker B: Okay.
00:21:30.368 - 00:22:02.304, Speaker A: Yeah, so I think I want to leave. We can talk about this later, Will, but I'm going to leave this as a homework. Okay. So it is not, it's not a difficult thing to show. Okay, so the next thing I want to do is just point people to. So at the moment I'm going to do a new share and.
00:22:05.764 - 00:22:07.676, Speaker C: Hey Mira, could I ask a question too?
00:22:07.740 - 00:22:08.384, Speaker A: Yeah.
00:22:10.274 - 00:22:23.530, Speaker C: If we were investigating this, could we just take like a slice, say, and require them all to sum up to one because it's invariant, like there's a scaling factor.
00:22:23.682 - 00:22:43.104, Speaker A: Oops, what happened? Yes, so you have to, guys, you have to sort of be a little patient with me with all this sharing here that I'm going doing here, so. Okay, so new share, we're back to the whiteboard. Okay, there we go. Yeah. What was your question again?
00:22:44.564 - 00:22:52.388, Speaker C: I was just wondering if we could say required all the entries to add up to one so that we're taking a slice.
00:22:52.516 - 00:22:53.564, Speaker A: Yes, yes, yes.
00:22:53.604 - 00:22:54.524, Speaker C: And get rid of.
00:22:54.644 - 00:22:58.460, Speaker A: Yes. Then you have a polyhedron instead of a cone.
00:22:58.612 - 00:23:03.664, Speaker C: And then I was also wondering what's kind. Could you just repeat the homework question?
00:23:04.004 - 00:23:06.944, Speaker A: Homework question is just the question that I initially asked.
00:23:07.404 - 00:23:08.464, Speaker C: What was that?
00:23:08.764 - 00:23:26.944, Speaker A: Which was that in order to determine that a given matrix is a metric, it's not possible to do so without actually looking at every set of every triple.
00:23:27.934 - 00:23:33.982, Speaker C: Oh, I see. Okay, so none of the triples, none of the inequalities follows as a follow from the others.
00:23:34.038 - 00:23:36.154, Speaker A: Like you don't have a situation like this.
00:23:36.654 - 00:23:38.214, Speaker C: Okay, that makes sense to me.
00:23:38.334 - 00:24:02.844, Speaker A: Okay, so that's the question. And I suggested an approach to answering that question. Yeah. By looking at the. By noticing that one of these, any. You can pick one of these inequalities and make it an equality without affecting any of the others, which is essentially what Dan proposed. And so we'll.
00:24:02.844 - 00:24:06.524, Speaker A: Because we'll ask the question. I'm just making it a homework.
00:24:07.104 - 00:24:10.204, Speaker C: Okay. So you can maybe look at sort of a linear system.
00:24:10.544 - 00:24:11.176, Speaker A: Yes.
00:24:11.280 - 00:24:15.984, Speaker C: That has two ones in every row and then a one on the other vector. Okay.
00:24:16.064 - 00:24:55.268, Speaker A: Yeah, yeah, that's right. Okay. So, yeah, so I wanted to. Let's say there's something called a new share. Yeah, new share. So, I want to show you guys some of the very classical book by Deza and Laurent, which I think is this one, which is. It's not a book.
00:24:55.268 - 00:25:27.704, Speaker A: This particular thing that I have here is not a book, but it's, you know, it's one of the lecture notes or whatever it is that is by Deza and a bunch of other people who talk about, you know, who have described the metric cone. The cone that we are talking about is called the metric cone. So this, for example, for all three sets, IjK. Do you see this, by the way? You can see this, right? You can see the paper that I'm showing?
00:25:28.524 - 00:25:29.604, Speaker C: I can see it, yeah.
00:25:29.684 - 00:25:37.020, Speaker A: Okay. And so here you can see my cursor. Yep.
00:25:37.052 - 00:25:37.940, Speaker C: We can see that, too.
00:25:38.012 - 00:25:59.564, Speaker A: You can see the cursor. Okay. So I don't want to draw on it, because then I have to clear it and do all that. So, for all three sets, I, J, K, we consider the inequality. So the inequalities are exactly. So instead of D, which he's calling it x. So x I, J minus x I, K minus x, JK is less than or equal to zero.
00:25:59.564 - 00:26:31.224, Speaker A: This is exactly the same inequality we had. And this is exactly the section that Alex was proposing. You simply say that I'm only going to look at one small section of the cone. I'm going to ensure that the sum of all the triples of distance is less than or equal to two. He's actually looking at a section. This is a portion, so to speak, of the cone, right, has an interior of the same dimension as the cone, not an affine slice. So, given these conditions.
00:26:31.224 - 00:27:27.814, Speaker A: So, essentially, this induces how many facets? And that defines the metric cone. Now, the interesting thing about the metric cone is that it's very close. I mean, it's basically in one to one correspondence with something called the cut cone, which is heavily used in combinatorial optimization. The cut polytope is basically, you can think of it as its extreme rays. I can just describe it. So the n choose two dimensional cut polytope is the convex hull of the incident vectors of all the cuts of the complete graph. Okay? So if you think of the cuts of the complete graph, and a cut of complete graph, by the way, is a collection of edges which essentially separates the, you know, makes the graph disconnected.
00:27:27.814 - 00:28:17.064, Speaker A: So if you think of the incidence vectors, which means, you know, you think of a long vector of edges and you, you, it's a zero one vector where you put ones corresponding to all the edges that are in the cut and zeros elsewhere. So essentially you have given a subset of the vertices vn. The cut is determined by cut, quote unquote determined by s, consists of the pairs of elements of vn such that exactly one of ij is an s. Okay, sorry. This is defined on the vertices, not on the edges. By delta of s, we denote both the cut and the incidence vector in r and choose two. So this is the edges, all the possible edges.
00:28:17.064 - 00:29:07.316, Speaker A: So he writes it as delta s I j is one. If exactly one of ij is in S and zero, otherwise zero means that that particular edge is not part of the cut. So now you can think of the convex hull of that set of vectors, and that's called the cut cone. And so essentially, you know, it turns out to be essentially identical to the metric cone. So as you can see, this one to one correspondence, and there's a one to one correspondence between the metric cone and all the semimetrics. And the semimetric just means you don't require some of the distances can be zero. That's all semimetrics.
00:29:07.316 - 00:30:38.464, Speaker A: Mean metric will not allow some distances to be zero, except for the distance DII. So essentially this gives you another immediate combinatorial motivation to be studying the metric cone, rather than just say, oh, I'm just interested in all the possible metrics. Okay, so that was a little, and I just wanted to point, show you this book. There's an entire book called cuts and metrics which discusses these very basic questions that you might have concerning the metric cone. Okay, so the next thing, okay, so let's go back to the whiteboard for a second. So now we can move on and ask ourselves, um, what if I want add a little more conditions on the, um, on the, so I add a little more, few more conditions on the metric, right? So let me ask the following question. Given a metric.
00:30:38.464 - 00:31:31.264, Speaker A: So now I tell you that it's a metric. So in other words, it satisfies the triangle inequality. Real symmetric and so forth. Metric matrix, I'll call it. Can it be embedded in. I mean, you know, is it a l one? Does it correspond. Sorry? Does it correspond to l one, lp distances? So let's see.
00:31:31.264 - 00:33:21.716, Speaker A: Okay, lp. I'll just put it down here. And here will be the dimension. I'm not talking about the dimensions at this point in time. Okay, so does it correspond to lp? That's a weird way of asking the question. All I'm asking is, okay, so what about l infinity? So in other words, can one always find a set of points, endpoints in l infinity, such that the given metric matrix is exactly those distances? So we want p one through pn. Okay, p one through pn, l infinity, such that, you know, PI minus pj is exactly equal to dij.
00:33:21.716 - 00:33:50.874, Speaker A: So isometric. It's called isometric embedding. So what do you think? I'm not telling you anything about any. Any dimension. Or you can choose whatever dimension you want. Are you seeing my white whiteboard?
00:33:52.334 - 00:33:53.414, Speaker B: We can see it.
00:33:53.534 - 00:34:31.334, Speaker A: Okay, great. So what do you think the answer to that question is? So what this means is that you can think of each of these p I's as vectors in l infinity to something. I don't really care. You can. I'm not telling you what the dimension has to be. It can be anything you want.
00:34:39.524 - 00:34:41.944, Speaker B: Is the l infinity metric the max?
00:34:42.484 - 00:34:59.304, Speaker A: Max? Not exactly. So these are vectors in here. And when I say PI minus pj, this is the max over. Okay, some other index, let's say that you give to each one of these guys. Let's call it. Yeah, okay. Or something.
00:34:59.304 - 00:35:49.624, Speaker A: Why is this behaving this way? Sorry, I have to maybe. I don't know if it has to do with the slowness of my computer or whether it has to do with the slowness of my network. Okay, Max over k of. You can think of p each of these PI minus pj, PI minus pj. You know, the kth index of it, right? So p. I mean, minus pj one, p minus pj two, etcetera, dot, dot, dot, dot, pj n. And we are basically taking the maximum overdose.
00:35:49.624 - 00:35:52.384, Speaker A: Right? So that's what the infinity norm.
00:35:52.424 - 00:35:59.884, Speaker B: Do we. I'm sorry, do we also take the absolute value of the PI minus pj or to make them all positive, or.
00:36:01.864 - 00:36:37.184, Speaker A: Yeah, yeah. The infinity norm will take the max of the absolute values. Yeah. So then if you want to know what the infinity norm of a vector is, basically, max over I. So x, if you think of it as x one, x n, max over I. Yeah, perfect. Okay, so the question is, can one.
00:36:37.184 - 00:36:56.934, Speaker A: How about the analyst there, Sean? And castes, can this always be done? If I gave you a metric, can you put it in L infinity? Yes. And yes. The name of it. The fresh a embedding. That's called the fresh a embedding. Good. So.
00:36:56.934 - 00:37:26.622, Speaker A: And do you remember, how do you do that? Do you know? Not off the top of my head. Not off the top of your head. Okay. So we're going to do that. It's very easy enough that we are actually going to do it. So, but since my thing is acting up, I'm just going to go to the. There we go.
00:37:26.622 - 00:37:50.386, Speaker A: New share. I should have this up. Okay, there we go. So here, this is called the fresh. Do you see my other screen now you see a text screen. Yes, yes. Okay, so this is called the fresh a embedding, any endpoint metric space, X comma D.
00:37:50.386 - 00:38:35.864, Speaker A: So this is a slightly different notation. I just call it delta or dij or something going from one to n. They say x comma d, where x is. Think of these as points, a set of elements one through n, so that I call them p one through pn. They call them capital x comma d, which is the delta, the distance function. It says that it can be embedded into L infinity. Okay, so how do we do this? So essentially, you simply define for each point in the metric space you define.
00:38:35.864 - 00:39:09.064, Speaker A: So this notation is a little funny, but basically they're saying that you define a coordinate. Actually, by the way, Leo talked about this, he had a pretty long proof of this, but this is quite short. Leo is Leo liberty. Yeah. In his lecture, he actually talked about the fresh a embedding. So this can be embedded. So essentially the dimension of the embedding, by the way, we are going to use as many dimensions as the number of points.
00:39:09.064 - 00:40:14.184, Speaker A: So for every point little x in, they call it x, I called it p, it doesn't matter. So every point little x in capital x is, there is going to be a coordinate. So for a given point u, the xth coordinate is simply going to be the given distance between x and u. So if you think of the another way to think about this is that you can just take, maybe I can annotate here. Okay, so basically you take, if you think of x as a point, and that point is going to have, is a vector in your l infinity, which basically is going to have many coordinates. And one of those coordinates, or I think they're calling this u is the point, and one of the coordinates is going to be corresponding to x. So think of it as u sub x and that u sub x.
00:40:14.184 - 00:41:12.516, Speaker A: That coordinate is simply the distance. So another way to think about it is that you take your distance matrix and you know, there's going to be a row corresponding to the point u and it has all the distances to all the different points. And you simply take this to be your, you know, the vector corresponding to u in the embedding. So that's all you've done. So basically, the infinity norm of this embedding embedded points f u and f v is the max over all the possible coordinates. As I said, there's one coordinate for every point in the metric space of f of x, u minus f of xv. And that is by definition just dx u minus dxv.
00:41:12.516 - 00:41:47.726, Speaker A: And we are taking the max overall x's. And this by the triangle inequality has to be less than or equal to duv. And this is actually attained by simply setting x to be equal to u. So if I set this maximum is actually attained when you set x to be equal to u, because du, u will become zero. So you'll simply get duv. So this is less than or equal to, and this shows that it's actually attained at one point. So therefore it's an isometric embedding.
00:41:47.726 - 00:42:26.964, Speaker A: So now we know that the distance in the embedding, the infinity norm distance in the embedding is actually the given distance dx duv. This is called the fresher embedding. So it's a simple proof that simple embedding, you simply take the youth row in the given distance matrix, you assign it as the coordinates of the point u in the embedding. And so that means the dimension of the embedding is actually the number of points. Now that's not such a great embedding. Usually you don't want a dimension that's equal to the number of points. And I think Leo actually pointed this out.
00:42:26.964 - 00:43:23.694, Speaker A: So one's usually interested in minimum dimension in which you can embed. This is saying that no matter what the metric space you give me, no matter what finite metric space you give me, you can always embed it into l infinity, provided you give me sufficiently many dimensions, namely n. Okay, so now we will talk somewhat about some of these. Cone. There's a corresponding cone for all the l infinity matrix. Put it that way, the vectors or the matrices that correspond to metrics. Did I say l infinity? So any metric space can be embedded in l infinity, but you can think of the other lp norms, and they all have their own not all lp norms are universal.
00:43:23.694 - 00:44:14.436, Speaker A: In this way, l infinity is said to be universal because you can take any finite metric space and embed it into l infinity. The other lp's are not universal, but even if, although they are not universal, they have their own cones because they satisfy. I mean, the set of vectors that satisfy the norm correspond to the embedding in that norm space is a convex set. Okay, so proving this, uh, it's not straightforward, actually. Um, there is another very nice paper, um, which I want to point out to you guys. Um, let's say I can clear this. I have to remember to clear this, um, clear my drawing.
00:44:14.436 - 00:44:50.204, Speaker A: Okay, so, um, and I want to close this. Okay. And there's another, um, paper by ball. This is ball's paper. It's a classic paper by Keith Ball, which actually talks about isometric embedding in LP spaces. It was written only in 1990. It's surprising that so many of these questions were not answered in general LP norms until this time.
00:44:50.204 - 00:45:13.974, Speaker A: So, for example, it starts out by proving that every endpoint subset of LP embeds isometrically into lP. M. M is the dimension where m is at most n. Choose two. So even showing that, you can always embed any. Sorry, every endpoint subset of LP. So, you know, in other words, that it can be embedded in LP already.
00:45:13.974 - 00:45:56.034, Speaker A: And now you're trying to show what the maximum dimension you need in order to do the embedding. And they prove that this is at most. This is actually at most, half. I mean, and choose two. Basically, you would think this is totally obvious, but it actually requires proof. So for l one, for example, we'll do this. So we'll definitely spend some time on some of these results by Johnson, Lyndon Strauss Johnson, Shechtman, Whitson, Hausen, and all these people later when we talk about this, these different, different p norms.
00:45:56.034 - 00:46:37.804, Speaker A: Okay, embedding in these p norms and other types, not just p norms. We'll also talk about other metrics. We'll talk about tree metrics briefly, a very common type of metric, metric space, and. And their relation to the normed spaces. And we'll also very briefly, touch upon approximate embedding. So, not isometric, but where you allow some amount of distortion in the embedding and the corresponding complexity results on that. Okay, now I'm going to look at the chat because.
00:46:37.804 - 00:47:15.394, Speaker A: Okay, so this is. Is there any question for me here called a fresh. A burning. Kurotowski, I'm burning. Okay, got it. Okay, so now we just want to go to back here to the whiteboard. Okay, so now we're going to move on to euclidean l two.
00:47:15.394 - 00:48:59.890, Speaker A: Okay, so what is known about l two? So if I go back to the previous thing here, we now, you know, already have touched upon some of, some basic results that involve metric cones and other norms. And now we are, and we haven't talked about the dimension issue yet, we're just allowing the dimension to be anything of the embedding. Now we're going to talk about euclidean distance. So euclidean distance, the same as you can think about it in these terms as l two. So we're now going to talk about embedding in l two. Okay, so, okay, what is it that we want to talk about? So again, same question. So there is a famous theorem called Schoenberg's theorem, two theorems, and I think both, I think Leo talked about at least one of them and certainly talked about some aspects of the Caylee manger.
00:48:59.890 - 00:49:57.550, Speaker A: Okay, so Schoenberg's theorem essentially, I don't know why I'm doing pink. Let's just go back to blue. So Schoenberg's theorem says that if I give you. So there are different ways in which, slightly different ways in which people state Schoenberg's theorem. And it's essentially saying that delta, the given matrix, you know, dij, you know, real symmetric, whatever is euclidean, is a euclidean distance matrix. And it's usually written as EDM, if and only if it is negative semidefinite. It's not directly negative semidefinite.
00:49:57.550 - 00:50:52.934, Speaker A: It's negative semi definite on the subspace of vectors that are orthogonal to the all ones vector on subspace orthogonal. This is one way of stating it. He may, I think Leo Liberty stated it slightly differently because it's so closely subspace orthogonal to all one spectre. It has a, you know, one to one correspondence with, you know, the PSD cone, the positive semi definite cone. So these are all real symmetric matrices with positive non negative eigenvalues. Okay, so semi just means, even though it says positive, it means because the semi says that they're non negative eigenvalues. Okay, semi definite.
00:50:52.934 - 00:52:04.264, Speaker A: Okay, so that's Schoenberg's theorem, and we can add to it the condition that the, about the what ball called m, the dimension, the embedding dimension, okay, so is embeddable in cliadium space. Is EDM in embeddable in dimension, embeddable in, let's call it rm, where now this part, and I think I'm going to put that in a different color, embeddable in rm. We can talk about just the original, what I had in blue, without talking about where it's embeddable. But if you add that embeddability thing here, so this becomes negative, semi definite, and off rank m. Okay, so this is Schoenberg's theorem. And you can see that Schoenberg, and we're going to prove this. In fact, Leo, I think, proved it.
00:52:04.264 - 00:52:52.540, Speaker A: He seems to have spent quite a bit of time on proving a version of it, at least. And the other sort of, you can think of this as coming from analysis. This was proved in for infinite dimensional spaces, where this is what they call negative type metrics, right? So, but we are just using it for finite dimension. There are only endpoints in what we're talking about. Everything is finite. So this, Kaylee Manger, let me go back to blue. Kaylee Menger says that they are not looking at the dij.
00:52:52.540 - 00:54:11.762, Speaker A: So think of it as a vector that, I mean a matrix that is dij over here, and then it's got these all ones, sort of an extra row column added on here like this. Okay, so this matrix, let's call this the delta matrix. Let's call this the C matrix or CM matrix or something like that. It's basically the delta matrix here with this. So this guy, so again, the question is, delta is, okay, delta equals dij is EDM, and then again, embeddable in rm, in rm. And we can think of this as an extra condition, which we can have or not have, and we still have a theorem, okay, so if and only if, again, we have two conditions. This negative semi definiteness condition, as you can see, is a type of an inequality.
00:54:11.762 - 00:55:06.082, Speaker A: A sequence of inequalities here corresponds to all the minors. So, cm, I'm just going to use the word cm. Cmk. Cmk. The determinant is non negative for all k. So these are all the top left minors like this. So look at all the, any sub matrix of dij, sub matrix of delta, and then plug in this top left column and top row, looking 011-0111 like that.
00:55:06.082 - 00:55:59.694, Speaker A: And you're looking at the determinants of those, okay, it's greater than you. It should look familiar to you, those kinds of determinants. These determinants usually compute. What this look familiar? These determinants compute. What if I gave you a bunch of distances between, say some k points, all pairwise distances, whatever, and I computed, and I put them in a matrix like this, and I computed the determinant, what does that compute? Typically, just look at four points or three points, even volumes. Volumes, right. So it computes sort of a simplicial volume using sometimes you, I think Liberty talked about this too.
00:55:59.694 - 00:57:10.860, Speaker A: Heron's formula for the volume given all of these distances. So for a triangle, if you're given the three distances like this, you can compute the volume of area of the triangle using this type of determinant. So basically you'll have a small three by three matrix with the zeros and ones plugged in, and you have that determinant, and that gives you the volume of something. So you have this determinant, the all of these determinants for all k is sort of a sloppy notation. I'm just saying, you pick any subset of points here, any subset of rows or columns, any sub matrix, and then add on these guys and you get the corresponding determinant and you make that equal to zero, I mean at least zero. So that corresponds to this negative semi definiteness condition here. And now the other condition embeddable in rm here we had of rank m, that corresponds to equalities.
00:57:10.860 - 00:58:35.912, Speaker A: So I'm just gonna go back to that color. So that corresponds to the determinant of, determinant of CMJ, I'll call, it is equal to zero for all j less than or equal to m. Okay, so if it is actually embeddable in Rm, then we are saying that these volumes, no, sorry, for all j greater than or equal to m greater than or strictly greater than. It's one of these. We'll figure it out. Okay, so what this is saying is that these simplicial volumes, if it is embeddable in rm, if these distances correspond to a point set pairwise distance of a point set in euclidean space Rm, then the simplicial volumes of all j subsets for j greater than M should vanish. What was that? Okay, sorry, excuse me, I have to get rid of this.
00:58:35.912 - 00:59:16.854, Speaker A: Okay, there we go. Um, so, so why do we, so you can immediately look at this theorem and say, oh, I kind of see why. So essentially this, if this one direction is saying that if you can get a set of points embedded in Rm, so let's say r two, then if you can embed in two reals, so this may be j strictly greater than m. Let's see, j strictly greater than m for two reals would be three, right? So three is not quite right. So this should be m plus one.
00:59:20.154 - 00:59:24.706, Speaker C: Correct me when you have a chance. Can I ask a question? But you can finish this if you want.
00:59:24.770 - 00:59:58.720, Speaker A: Yeah. So essentially, if you can embed in two reals, you know, any simplicia, any tetrahedral volume should be zero. There are no tetrahedra. Right. So this would be saying that, you know, if I look at four points, the fourth point lies in the affine span of the first three points. So this, the volume of four points, the simplex defined by four points has to vanish. Okay, so maybe this is m plus j strictly greater than m plus one.
00:59:58.720 - 01:00:24.864, Speaker A: That's correct. Okay, so this has to vanish. That's one direction. So, but the other direction, which is basically saying that if we have these inequalities and these things vanish, then it is embeddable in RF. So you have essentially these two parallel theorems, one in a more, you know, more from an analysis perspective. And this is more from an algebra perspective. Okay, Alex.
01:00:26.184 - 01:00:53.292, Speaker C: Okay, thanks. I had two questions. The first is really easy. The CM matrix on the left and the bottom, the top left entry, what is that, a zero or a one or zero? It's a zero. Okay. And then the other question is, okay, so if we, the vanishing of these minors of this matrix cuts out a variety in the variables. Dijon.
01:00:53.378 - 01:00:54.216, Speaker A: Exactly.
01:00:54.360 - 01:01:02.456, Speaker C: And do we know about the dimension? Do we know the degree of this variety? And does it have a name?
01:01:02.600 - 01:01:10.604, Speaker A: Then that's basically Kaylee manga variety. So is Dan here?
01:01:13.624 - 01:01:18.616, Speaker C: Yeah, I've heard Dan say this about 18 times, but I sort of always was afraid to ask this.
01:01:18.680 - 01:01:46.634, Speaker A: Exactly. The Caylee Menger variety. Okay, so he has two subscripts here, or superscripts or whatever it is. One of them is this m and the other one is the n, which is the number of points. So he'll typically say cm, some two numbers. And those two numbers, one of them is m and the other one is the number of points. Nice.
01:01:46.634 - 01:01:55.492, Speaker A: Remember when we started out, dij, I and j go between one and n. There are n points. This is an n by n matrix.
01:01:55.628 - 01:02:02.500, Speaker C: Okay. And I imagine it's the miners of size one bigger than m. Yeah, this.
01:02:02.572 - 01:02:12.804, Speaker A: Yeah, it's m plus one. These should all be zero. All the larger, I mean, larger minors should all be zero. Somebody has put another new message was that.
01:02:14.184 - 01:02:20.164, Speaker C: And then what about the dimension and degree? Do we know this as a function of the two numbers?
01:02:21.744 - 01:02:31.844, Speaker A: The. Yes, we do, because we know that the degree is just the size of the determinants, right?
01:02:33.504 - 01:02:35.164, Speaker C: Not necessarily, no.
01:02:38.964 - 01:03:19.914, Speaker A: So this is going to give you the same cone here, right? This, this, what you have here is going to give you the same cone. So this equality is correspond to the boundary facets of this cone. So we will under, we will, I can tell you what the dimensions of those are. The dimensions are, is d plus one choose, I mean, sorry, m plus one, choose two will be the dimension of that variety, the variety that is given by the union of select set of boundary facets of this cone. We'll talk more about it.
01:03:22.974 - 01:03:24.314, Speaker C: Okay, thanks.
01:03:24.934 - 01:04:04.642, Speaker A: This here, when this vanishes, the set of all delta. Where this vanishes, these vanish, I should say all of these polynomials vanish, are exactly the boundary facets of this cone of dimension d plus one choose two, or m plus one, choose two. Sorry, m plus one, choose two. And there's a direct correspondence between these two theorem. The theorems are basically on the left hand side of the theorem is exactly the same thing, saying when is a matrix, an euclidean distance matrix. Here the same thing. The right hand side conditions have exact parallels.
01:04:04.642 - 01:04:50.414, Speaker A: So the negative semi definiteness here corresponds to this inequality, and the embeddable. Sorry, the off rank m. The rank condition here corresponds exactly to these guys vanishing. So it's a determinantal variety in a way, you can see that it's a determinant variety because of this rank condition. And that's exactly what you have. Okay, so we'll talk more about this later after we prove this. Okay? Right, so where are we at? We are at already at 1142.
01:04:50.414 - 01:05:04.564, Speaker A: And we never took a break. Anybody? I guess there were many questions. How do you find it boring people? I mean, I'd like some reactions. Is this interesting? Boring.
01:05:04.984 - 01:05:06.968, Speaker B: This is very interesting, Mira.
01:05:07.136 - 01:05:07.952, Speaker A: Okay.
01:05:08.088 - 01:05:09.432, Speaker C: Not bored at all.
01:05:09.568 - 01:05:44.540, Speaker A: Not bored. Okay. I mean, it's talking on. Zoom is really, you know, you can't see people's reactions pretty bad. Okay, so. Okay, so let's move on. So how about the others? Anyone else here that say non postdocs? Oliver? Okay, so if you have any questions, please stop me.
01:05:44.540 - 01:06:08.888, Speaker A: I mean, I didn't realize. I was just rambling on and on. And we are already at 1143. Is it really true? Yes. Okay, so now let's go on to proving. So I could just play what's his name, Liberty's video for you. It's been recorded and very obligingly present.
01:06:08.888 - 01:06:47.204, Speaker A: Maybe we can do that for a second, just so that I can point out to you where it is. Right, so let's go here. And I'm going to do new share, and I think it's Google Chrome. Okay, so let's go here. Why am I showing you my gmail? That's no good. Okay, so. Oh, maybe now is the time to do this.
01:06:47.204 - 01:07:20.404, Speaker A: Let's take a. So since we have only 20 minutes left and we have one more agenda item. We have one more agenda item in today's lecture. It may be good to do that. Let me briefly play this video just so that you know what I'm talking about. So he actually talked apparently about fresh a metrics embedding. Anybody pay attention to that, by the way, that he actually talked about it.
01:07:20.404 - 01:07:44.988, Speaker A: Okay, so which video was this? So I think I need to go back here. I think the very first day. So let's go back to the first day. The first day you can. So just. I'm just telling it to you. I will do the proofs.
01:07:44.988 - 01:08:42.366, Speaker A: Right? We will do the proofs. I just didn't realize it was so late. So we will do the proofs, but in the meantime, I just want you to go here and I just want to jog your memory for those of you who are in his lecture as to. So let me recap just one notion, the definition of distance. I'm going to reduce his volume. So I just want to scroll through his video so that we can actually, I can point out the place partition is NPR constructor realization. I should have bookmarked the exact points where this was.
01:08:42.366 - 01:09:06.378, Speaker A: So he talked about the fresh a metric. And he also talked about the proof. I think it was the very first day, if I'm not mistaken, that he actually talked about. It's very slow. Um. I don't know why it's taking such a long time. Never mind.
01:09:06.378 - 01:09:19.482, Speaker A: Okay, we give up on that. Stop. Share. You see that? It's loading, loading, loading. It's not loading very fast. Okay, sorry. Who is he that you are talking about? Leo Liberty.
01:09:19.482 - 01:09:36.881, Speaker A: Oh, yeah. Perfect. Yeah. Okay. So in his first talk, I mean his talk, you have to pay attention a little bit because he has so many applications and this and that. But he did talk about these two things. He talked about the heron's formula.
01:09:36.881 - 01:10:02.194, Speaker A: I don't think he talked about the Cayley Mengar formulation of this euclidean ness of a matrix. I mean, the EDM characterization. But he did talk about the Schoenberg theorem. Yeah, yeah. And he also talked about the fresh air embedding. So he did both. So it's just to give you another jog of your memory.
