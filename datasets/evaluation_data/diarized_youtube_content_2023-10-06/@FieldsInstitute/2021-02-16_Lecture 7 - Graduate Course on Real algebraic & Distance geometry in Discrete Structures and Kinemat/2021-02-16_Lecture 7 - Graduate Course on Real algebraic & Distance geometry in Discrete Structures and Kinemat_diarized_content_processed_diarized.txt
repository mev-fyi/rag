00:00:00.280 - 00:01:05.144, Speaker A: We were going to talk about beyond three decomposable graphs to decomposition recombination plans. So just to recall where we were, we had considered, talked about how tree decomposable graphs have the nice property that, you know, just to give you, remind you what a tree decomposable graph is. It's a graph that can be decomposed into three parts, each sharing each pair sharing exactly one vertex. That's the definition. And the base case is that the graph is an edge. So it's a recursive definition with a base case. So it has many nice properties, and there are nice algorithmic properties, which I won't talk about right now, but the part that we talked about was just to remind you, maybe I should decrease the size a little bit.
00:01:08.084 - 00:01:10.836, Speaker B: Maybe if you click Google Chrome.
00:01:10.900 - 00:02:13.104, Speaker A: The part that we talked about is that we can find a solution or realization of distances specified on the edges of the graph by realization point coordinates, such that those distances are realized. Another alternative way of thinking about it is that you're completing the distance matrix, a partially specified distance matrix. Some distances are given and you're trying to figure out what the remaining distances are. And the standard, you know, it's equivalent to the problem of finding a realization so that the remaining distances that are not specified are obtained by measuring the distances of the realization, the non edges of the realization, and they put there. So basically, realization and partial matrix completion are essentially the same problem. So now, of course, if they say find all realizations, then the equivalent of finding all completions and so forth. So anyway, this is just to recall.
00:02:13.104 - 00:03:36.464, Speaker A: So if your tree decomposable graphs have the property that when you can realize or find the point coordinates of one point at a time, or in this kind of situation, it's a little bit different than what I drew there. You're sort of having the realizations of the three parts, having found solutions of the three parts, you can find the positioning, the relative positioning of them, so to speak. So in other words, the position of that point with respect to these two points, very quickly, simply by solving another quadratic, because you have solved this, so you have a realization of this, which means you have this distance, you have this distance, and so you have this distance, and so you just, the question of solve, finding coordinates of this just corresponds to intersecting two circles, which is two quadratics, by the way, everything that we're talking about right now is in two dimensions. But the same idea of three dimensional, three decomposable graphs is identical. There's no difference, except that instead of having three of these guys come together, you'll have four in general, for D dimension D, it's d plus one. I just can't draw like that. So all examples that we'll be looking at will be just for D.
00:03:36.464 - 00:04:22.090, Speaker A: By the way, one important thing that I forgot to mention, which we did before, but now in the recollection I forgot to mention, is that because of this property, just this recursive property that we talked about automatically ensures that these graphs are minimally rigid. Okay? So since you have been in Tony's class, you understand what minimally rigid is. And this is not a hard thing to prove. It's quite easy to prove. You just start, do it by induction, starting from the base case. The base case is that each one of these is an edge, and then you proceed from there. So it'll just become something like, it's not quite a Henneberg construction, but it's practically a Henneberg construction.
00:04:22.090 - 00:05:23.974, Speaker A: Henneberg zero. Okay? And so essentially we can, when we do some of the proofs, eventually, when we talk about flexible graphs and we have to talk about their configuration spaces, we will revisit three decomposable graphs. And at that point, we're going to do a little more detailed proofs about these constructions, how the Henneberg one type construction, how it helps us later for describing the configuration space of these things. But for the moment, it's very simple. You understand what it is. And I don't want to belabor the point. The key ideas we talked about was that, you know, this, what we showed here of solving one quadratic at a time is essentially a triangularization of the original distance constraint system, which was a quadratic system into.
00:05:23.974 - 00:05:53.180, Speaker A: By triangularization, it means that we produce something like a matrix with all zeros over here, which. But these are all polynomials. They're not linear linear polynomials, they're nonlinear polynomials. But the idea is that these are the variables in the columns. And at any stage, you know, you add one variable at a time. Okay? So. Or in other words, you're solving a quadratic, a single quadratic in one variable.
00:05:53.180 - 00:07:13.420, Speaker A: And we, I gave it as a homework long time ago that saw, you know, intersecting two spheres, intersecting two circles, or, you know, that you would need to get produced this point, or intersecting three spheres and so on and so forth. You can write as in this way, as a triangularized system, where at each stage you're intersecting, you're solving one quadratic in one variable. And this is the case, as I said, in any dimension, it doesn't matter that we're still following that very original algorithm that we had with the sphere intersections. And so the idea is that this triangularization, which you can do, by the way, for polynomial systems using grobna bases and Wurrich characteristics set or whatever it is, there's a standard way to find a triangularization. The problem is those triangularizations all end up with very high degree. So these, these, as you keep going down, this last polynomial, univariate polynomial will have typically exponential degree in the size of the original system that you had. But these particular examples, these distance constraint systems, if you sort of input the original graph, the constraint system corresponding to the graph.
00:07:13.420 - 00:08:33.804, Speaker A: So here, for example, is the actual graph with the edges and so forth, and you have distances there, and you wrote the polynomial system down. The triangularization actually ends up maintaining the degree, preserving the degree. It doesn't bump up the degree, like, you know, grobna basis or other methods would do. So that's the key idea here with these three decomposable graphs, that you can obtain a triangular system that keeps the degree as quadratic, and that gives you this radical, you know, this, this kind of solvability by quadratic roots, square roots, as a tower of field extensions. Okay, so the advantages of going back, the advantages of graphs, the triangularization of a polynomial system without increasing the degree. And then this you can think of as a recursive decomposition of the graph. Now, thinking about the graph, we always go back and forth between the polynomial system and the graph, right? Genetically, the properties of the graph will be used to talk about how many solutions the polynomial system has.
00:08:33.804 - 00:09:30.274, Speaker A: We want finitely many solutions. As I said, these are all rigid subgraphs, rigid graphs and everything that's immediate to prove from the description, the definition of three decomposable graphs. So we decompose into a constant number, in this particular case, three rigid subgraphs. And each one of those has finitely many solutions because they are rigid, by generically speaking. And then, now the new graph that you're creating after putting the three of them together, is also rigid, also has finitely many solutions. And then we also know that adding one additional edge at each stage, one additional constraint, or one additional, the equivalent of one distance at each stage, then gives a unique solution. You reduce that finitely many solutions to a unique solution, genetically.
00:09:30.274 - 00:09:36.894, Speaker A: So these are the. Okay, William, you have a question? Yeah.
00:09:36.934 - 00:09:38.514, Speaker B: Are you able to hear us?
00:09:40.094 - 00:09:47.392, Speaker A: You have to. I can't hear you. Wait a second. Have I. Yes. Sorry. Okay.
00:09:47.392 - 00:09:56.056, Speaker A: I had this. I had the volume down. Sorry. Yeah, I was just checking because Alex was trying to ask a question, so.
00:09:56.120 - 00:09:57.644, Speaker B: I didn't think you could hear us.
00:09:58.584 - 00:10:01.832, Speaker A: All right. Okay, Alex, what's the question?
00:10:01.968 - 00:10:27.664, Speaker B: Okay, thanks. It was a little bit a while ago when you mentioned that would produce a polynomial of high degree. I'm guessing this is by. Well, anyway. But when you're saying that you keep the degree at two, you're not finding all solutions, but just one.
00:10:29.244 - 00:10:49.304, Speaker A: No, you find all solutions. You solve that system. It's a degree two. It's got two solutions. Right. And then if you plug in either one of those two solutions or both, each one of them will, at the next stage, potentially give another two solutions.
00:10:51.284 - 00:10:57.064, Speaker B: Okay, so. But you do have to solve many degree. Two polynomials.
00:10:57.444 - 00:10:59.864, Speaker A: Yes, but degree is all I'm talking about.
00:11:02.624 - 00:11:06.024, Speaker B: And each stage has potentially two solutions.
00:11:06.184 - 00:11:20.448, Speaker A: Yes. And the number of solutions, by the way, you don't gain anything from groena basis either. It's the same thing. They'll also have, at the end, lots of solutions. And if you're back solving, back substituting, you're going to have to try every single one of them.
00:11:20.616 - 00:11:21.328, Speaker B: Right.
00:11:21.496 - 00:11:35.248, Speaker A: Okay. So in that sense of number of solutions, there's no real. In this case, we actually can count in a way, because the potential number, there's an easy upper bound on the number of solutions, which is two to the n, which is the number of equations.
00:11:35.416 - 00:11:35.992, Speaker B: Yep.
00:11:36.088 - 00:11:43.284, Speaker A: Where n is the number of equation. But the point is here that the degree doesn't increase.
00:11:44.424 - 00:11:50.984, Speaker B: Okay. But there. Yeah. Okay, so it's just the degree of each polynomial.
00:11:51.144 - 00:11:51.680, Speaker A: Exactly.
00:11:51.712 - 00:11:52.424, Speaker B: That you're solving.
00:11:52.504 - 00:11:56.724, Speaker A: Yeah, exactly. The degree of each polynomial that you're solving remains quadratic.
00:11:57.214 - 00:11:58.542, Speaker B: Okay, thank you.
00:11:58.718 - 00:12:15.150, Speaker A: There are other advantages. They are nice polynomials because you're solving them in a simple way. Right. I mean, I am just, you know, intersecting. It's sort of a nice quadratic, too. In addition, you know, there are other things, but I don't want to go into details about those.
00:12:15.262 - 00:12:30.084, Speaker B: In other words, I was just kind of. Because you could view this as an algebraic variety with finitely many points, and then the degree of that variety is the total number of solutions, which is definitely not two.
00:12:30.504 - 00:12:32.352, Speaker A: Just two to the n, potentially.
00:12:32.448 - 00:12:34.096, Speaker B: Exactly. Or something lower.
00:12:34.200 - 00:12:44.104, Speaker A: Yeah, it's. It's the degree of this triangularized system. I think they call it the triangularized degree or something like this. Okay.
00:12:44.264 - 00:12:48.564, Speaker B: Which should potentially be the degree of that polynomial coming out of grobner basis.
00:12:50.364 - 00:13:16.036, Speaker A: The number of solutions won't be a single polynomial coming out of Grosvenor bases. Anyway, from Grosvenor basis, it will be the bezoot number that comes out of your original system. Just to get the degree, which is essentially the number of solutions, as you point out, you can simply look at the. Which is, you can just get the Bazut number, which we had in the.
00:13:16.060 - 00:13:19.488, Speaker B: Last lecture, two to the n. Okay.
00:13:19.576 - 00:13:21.604, Speaker A: In this case, it'll just be two to the m.
00:13:25.384 - 00:13:27.032, Speaker B: Okay, thanks. That was helpful.
00:13:27.168 - 00:13:40.364, Speaker A: Yeah. So, um, anybody else had a question? Sorry, I had you. I hadn't. I didn't have my sound on.
00:13:41.424 - 00:13:42.844, Speaker B: I think it was just me.
00:13:42.984 - 00:13:54.624, Speaker A: Okay. Yeah. By the way. Yeah. You have to ask a lot of questions because will told me that he had to meet some kind of admiral or something. So you have to ask all the questions. Alex, it's on you.
00:13:54.624 - 00:14:21.424, Speaker A: Okay. So the. Well, everybody should ask questions. I am just pointing to these two people who ask the most questions in general. So here. So adding an idea. So this part maybe explains a little more, Alex, is that there are two solutions in general, at each stage, finite number of solutions, typically two could be two or one.
00:14:21.424 - 00:15:02.556, Speaker A: But if you add this additional edge at each stage, which means one more constraint. So, which we also did in our original euclidean sphere intersection algorithm, it reduces to a unique solution. Remember, in the original sphere intersection algorithm, where we didn't have a fixed dimension, in order to decide whether to increase the dimension at a particular stage, we looked whether we had two solutions or one. And if we only had one solution, then it sort of signaled that you had to. Sorry. If you had two solutions, it signaled that you have to increase your dimension at that stage. I'm talking about the original algorithm that we had to even prove the Cayley Menger theorem.
00:15:02.556 - 00:15:40.014, Speaker A: The same similar algorithm that we're using here. But with that one extra edge, we actually get a unique solution. Here. The dimension is fixed, right? And this is, this one extra edge business is the same for any dimension. You know, we fix the dimension at that dimension, you have a triangular three decomposable, will involve typically d plus one distances, in this case, three distances. And the unique solution would then involve one more distance. So d plus two distances.
00:15:40.014 - 00:16:27.694, Speaker A: So the disadvantages of this, of course, is that it's a small class of graphs. You know, it's not this, this particular. The graphs of these polynomial systems is very special. They are so simple graphs that don't have these properties. So basically, hang on, I think I should open a new slide, actually. I'll open a. I want a new share, and I just want to show you some examples, very class, simple examples of graphs that are not three decomposable? So, for example, you could have just a simple graph, you know, with two.
00:16:27.694 - 00:17:04.624, Speaker A: This is called the c two cross c three graph. So this guy, or you could draw it this way if you want. This graph is a cycle of three crossed with a cycle of two. And that thing is not three decomposable. So this is the classic, the reason I'm not mentioning what another thing that you're all very used to, which is the k 33. This is called c three class c two. The reason I'm not mentioning this.
00:17:04.624 - 00:17:06.844, Speaker A: Any idea why I'm not mentioning this one?
00:17:12.904 - 00:17:14.444, Speaker B: Maybe it's not planar.
00:17:15.024 - 00:17:50.547, Speaker A: No, it's not decomposable at all. Okay, so in other words, the only decomposition of this is by decomposable. It's not only a trivial decomposition where the graph, if you think of the graph here, the only sort of rigid subgraphs inside it are the edges. So you've got the six edges. You'll get all the six edges here. So you just got a trivial decomposition, which would be the case for, say, let's say a triangle or something. It's just got a trivial decomposition.
00:17:50.547 - 00:18:50.566, Speaker A: But this has a non trivial decomposition, which is the two triangles are both rigid. And there are two rigid subgraphs, very clearly, two rigid sub graphs here. And those are non trivial. I mean, trivial is edges. So this actually has a non trivial decomposition, but it doesn't have that form that we had for three decomposable graphs, which is this kind of thing. Okay, so there are many, many graphs in the, in the slide, I had an example of, you know, you could have something like this, for example, that's three decomposable, right? Because you can recursively, now you've got the three, three parts, and each of the three parts looks like this. And then there you can, there you can split them up into these three parts.
00:18:50.566 - 00:19:11.110, Speaker A: That part, that part and that part. And then again, now these two parts are already at the base case, which are edges. Now you've got that one. So you've just got the triangle. And then the triangle is clearly three parts. Okay, so this. So there are all kinds of graphs that are three decomposable.
00:19:11.110 - 00:20:01.414, Speaker A: You know, they're not. They are pretty, you know, if you just barely look at the graph, basically they're non three connected. They could be two connected, but they're not three connected. Three connected means, three connected means that in order to separate, I mean, I should say maximally non three connected, which means it is three connected means there exists no way to separate the graph into two parts by removing only two vertices. Right. But what we are now saying is that in fact, you know, any pair of vertices in order to separate them, you know, you're going to, you can separate them with just removing two vertices, basically. So it's a little more, it's stronger than simply saying not three connected.
00:20:01.414 - 00:20:43.772, Speaker A: Okay, so that's the class of graphs. And then this one has a two, two trivial, non trivial, sorry, two non trivial components here. And you see we are starting to draw these kinds of pictures. And so for this, for example, I can, I would draw such a decomposition picture that looks like this for this guy. So let's see, let me move this so I don't have the chat sitting there. So for this I would have the graph at the root and then I would say, okay, these three guys go to blue. So the three guys are sort of the next level rigid subgraphs inside it.
00:20:43.772 - 00:21:38.916, Speaker A: So there are three rigid subgraphs and each of those rigid subgraphs is going to decompose in this manner. So again they divide into three and one of them is a triangle. It divides into three. Edge, edge, edge, edge, edge. And each of these looks exactly like this. So this part is duplicated in all of these places. Okay, so you start, you can see that we are start starting to draw the decomposition as a type of tree, right? And you know, it's not necessarily a tree in a way, because these, you know, not strictly speaking a tree in the sense that the leaves don't share anything.
00:21:38.916 - 00:22:38.386, Speaker A: I mean, there are leaves from here. Leaves are edges and the edges may share vertices that can happen. And in fact, this sub graph and this subgraph shares a vertex and this subgraph and this subgraph shares a vertex. So it's not a tree in the strict sense of, you know, somehow the object here and the object here don't create any cycles. That's not what we're talking about in that tree. This tree is just a way to schematically representing the decomposition into rigid subgraphs. So, so this decomposition here is going to look like three sort of, in the beginning you've got two of these triangles, so each of these is a triangle and then you've got a bunch of edges, three edges, one, two, three.
00:22:38.386 - 00:23:32.706, Speaker A: So the edge, edge, edge, and each of these is a triangle. So it's got edge, edge, edge, three edges and this one's three edges. Okay, so that's the, so we are starting to think about a graph in terms of a type of a recursive decomposition of the graph. Into rigid pieces. Notice that we are also not talking about very trivial decompositions, because if you simply say decomposition of the graph into rigid pieces, you could have divided, you could have given a sort of a trivial decomposition of this graph by simply right in the beginning, you know, just putting the graph and then putting all its edges. So it's, how many? It's got five edges, 15 edges down here. You know, I could have put 15 edges down here.
00:23:32.706 - 00:24:13.534, Speaker A: If all I said was I want to decompose the graph into rigid pieces, then I put all the 15 edges. That's not what we are doing. We are decomposing in the graph so that at the children, so to speak, of this, the full graph in the tree, these children are themselves sort of rigid subgraphs that are maximal proper sub graphs of G. So this is a maximal rigid subgraph, proper subgraph inside G. And so is this. And so is this. Right, so, so that the decomposition is somewhat, you know, for the tree decomposable case, this follows the definition.
00:24:13.534 - 00:24:45.794, Speaker A: It follows the definition. It says the graph divides into three parts, each one of them, which is rigid. And we know that those three parts are themselves maximal proper subgraphs inside G. Here, we have done the same thing. These are triangle, these are the two triangles. They are the maximal proper subgraphs inside and so forth. So for k 33, what we have here is the maximal proper subgraphs that are rigid inside it are in fact just the edges.
00:24:45.794 - 00:25:33.378, Speaker A: Okay, so it, so this, we could say it's not three decomposable, but it's kind of pointless because it's only decomposition. And in any way, shape or form is just the trivial decomposition, while this one has a non trivial decomposition, but it's not three decomposable. Okay, so this thing has a non trivial decomposition. C two cross c three. That's not three decomposable. Not three decomposable. Okay, so for the longest time, you know, people in the CAD community, computer aided mechanical design.
00:25:33.378 - 00:25:59.354, Speaker A: Computer aided design community. So let's see, how do I new share this community? They know. New share. This is the one I want. Why is it giving me this weird thing? I have no idea. What are you seeing? Guys.
00:26:01.194 - 00:26:07.294, Speaker B: We still see the diagrams you've been drawing for a while.
00:26:07.954 - 00:26:08.934, Speaker A: Okay?
00:26:09.674 - 00:26:11.094, Speaker B: You want a new screen.
00:26:11.914 - 00:26:38.356, Speaker A: For some reason, when I say new share and I see the screen that I'm trying to share is not showing me on the screen. I mean, it's not showing it to me on the screen. Weird. Ah, okay, so now, so you're seeing the slide that says triangularization with quadratics and radical solvability.
00:26:38.540 - 00:26:39.344, Speaker B: Yep.
00:26:39.884 - 00:26:45.252, Speaker A: Yeah, you're seeing that screen. Okay, you. You're not seeing the screen that I just drew on a moment ago.
00:26:45.388 - 00:26:47.092, Speaker B: No, we don't see that anymore.
00:26:47.228 - 00:27:26.636, Speaker A: Okay, good. Okay, so. So the next. So the disadvantages is that this is a very small class of graphs. There is a theorem, famous theorem by Owen and power from, I don't know, 2006 or something, that says that such graphs are exactly triangular, composable graphs, in other words, are exactly those for which you have this type of a triangularization by quadratics. So it's called quadratically, radically solvable. But the theorem doesn't go all the way.
00:27:26.636 - 00:28:16.260, Speaker A: It only proves this for topologically planar graphs. So, in other words, the k 33, which we know is not topologically planar. Well, that, in that particular case, there's no interesting theorem to prove, but it, you know, you can think of other three connected graphs which are minimally rigid, you know, decompose nicely and so on and so forth, but they're not, um, they are not included in this theorem. The conjecture is that you can remove that condition of topological planarity and still prove the theorem, but nobody has done it so far. So, in any case, I just wanted to say that it's a very small class of graphs, because there are tons of polynomial distance systems which are not. Which are not of the form that you can express. The solutions in this extension field, where you add one square root at a time there.
00:28:16.260 - 00:28:56.704, Speaker A: You know, this. This is a very small class Galois, theoretically. Okay, so this is, for example, a graph, another graph, which is not three decomposable, as you can see. You know, we have, you know, essentially, you've got the c two cross c three sitting here. You see, there's a triangle triangle connected by three edges. It's not quite the same c two cross c three that we saw, but it's just more or less the c two cross c three that we saw, and then it's connected. But this part alone, by the way, is, in fact, triangle decomposable, because you could decompose it in a different way.
00:28:56.704 - 00:29:21.366, Speaker A: That would cause. So, for example, you could have taken the central part, you know, just this triangle here and this edge. And. Sorry, this triangle here. Sorry. This entire rigid subgraph here, which is this quadrilateral with a diagonal, would have taken that one and this edge, and that. That's a triangle decomposition.
00:29:21.366 - 00:29:39.956, Speaker A: Sorry. Three decomposition. And then proceeded in that fashion. But so this one is not. And so. But. And the putting them together by adding these two edges would also, these three edges also prevents that from happening.
00:29:39.956 - 00:30:12.692, Speaker A: So this is a non tree decomposable graph. And the important thing in this slide is it's showing that this decomposition that I talked about is not necessarily unique. Okay. Doesn't appear, it doesn't clearly look unique because here's two such decompositions that I have drawn here that are both possible. Okay? So now I want to take a step back and remember this quadratic system that we're trying to come up with.
00:30:12.868 - 00:30:13.828, Speaker B: Right, Amir?
00:30:13.956 - 00:30:14.664, Speaker A: Yeah.
00:30:15.204 - 00:30:16.612, Speaker B: On the previous slide.
00:30:16.748 - 00:30:17.464, Speaker A: Yeah.
00:30:19.604 - 00:30:34.138, Speaker B: Is there anything that we can say, for example, like on the left and the right, they seem to have different length paths from the top node to the leaf.
00:30:34.186 - 00:31:00.542, Speaker A: Yeah. There are other things. So if you introduce this maximality condition that I talked about, right, a moment ago, you want the maximal subgraph, maximal proper sub graph. If you put that condition, then this won't be a possibility because now you don't have maximal proper. This is a maximal rigid subgraph, you see? But without that. Yeah.
00:31:00.678 - 00:31:06.074, Speaker B: Will the length of the, of the longest path be invariant?
00:31:07.014 - 00:32:01.500, Speaker A: No, none of these things is invariant. I mean, in general, non trivial decomposition, by which I mean anything that sort of doesn't just decompose into edges, they can have, they can have, they can be widely different. In fact, we will talk about that. There's a problem of optimizing or finding decomposition that minimizes not the measure that you're talking about, which is the length from the root to the tree due to the leaf, but minimizes the maximum number of children of any given node because that sort of dominates the algebraic complexity of solving the system eventually. Right. We want this. If you think of a triangularization, we would like it to be as close to being just a triangular system.
00:32:01.500 - 00:32:14.388, Speaker A: We're allowing a block triangular system, maybe. But then we want the number of solution polynomials that you're simultaneously solving at any given time to be as small as possible, because that's where the complexity blows up.
00:32:14.516 - 00:32:17.024, Speaker B: So if we have three children.
00:32:17.544 - 00:32:17.904, Speaker A: Yeah.
00:32:17.944 - 00:32:20.724, Speaker B: Then in the plane, that'll be good.
00:32:21.104 - 00:32:22.960, Speaker A: Three is good in general.
00:32:23.112 - 00:32:25.884, Speaker B: What are the other numbers and which ones are good and bad?
00:32:26.784 - 00:33:12.554, Speaker A: Any, anything bigger than three just starts becoming problematic, but maybe unavoidable, depending on the graph. Okay, is there, is there some connection between these two pictures? Like they're both decompositions into rigid subgraphs? No, I'm talking about the upper level, the origins here. Is there some function that can take me from the left one to the right one? They are the same. Pardon? They are the same graph. The same graph, yes. How can they be the same graph? There is a circle here. No, the circles are not.
00:33:12.554 - 00:34:06.434, Speaker A: The circles are showing the rigid pieces, rigid sub graphs of the graph. Okay, the left inner circle, if I stretch it, I get another extra circle here on the right side. First of all, I would like you to understand what the graph is. The graph is these little edge, these little vertices, these are the vertices of the graph, and the edges are here. So the graph on the left and the graph on the right are identical. They are the same graph. Okay, the same graphs, but the circles, the circles, extra circle here, the circle, the circles represent rigid subgraphs of the original graph.
00:34:06.434 - 00:34:50.762, Speaker A: Okay, I'll think about it. Thanks. Okay, so this is a rigid sub graph, but this is also a rigid subgraph. Right? Okay, so I'm not, at the moment, I have not put that additional condition that I mentioned to you a moment ago, saying that I want the children of a given, or the subgraphs chosen of a given sub graph of a given graph to be maximal proper subgraphs that are rigid. I have not put that condition. So, for example, this is not a maximal proper subgraph of the original graph. That's rigid.
00:34:50.762 - 00:35:02.354, Speaker A: This could be a maximal proper sub graph that's rigid. Yes it is. In fact it is. So this thing here is in fact a maximum proper subgraph that's rigid.
00:35:02.734 - 00:35:17.754, Speaker C: So it's maximum. Hi, mira. Yeah, so it's maximum rigid in dimension like v, v minus one. So the number of vertices minus one.
00:35:20.494 - 00:35:51.264, Speaker A: It's, in this case, it's just two dimensions. It's. At the moment we're talking about, the whole thing has moved to fixed dimensions. We're talking about a fixed dimension. And this particular example that I have drawn here and most of the examples that I'm drawing, because I'm unable to draw graphs that are triangle decomposable or whatever, in more than two dimensions. I'm only drawing two dimensional graphs. A two dimensional constraint system.
00:35:53.244 - 00:35:54.060, Speaker C: Uh huh.
00:35:54.172 - 00:35:57.788, Speaker A: Okay, I'm only talking about rigid in two dimensions.
00:35:57.956 - 00:36:04.064, Speaker C: But then if you have a, for your s seven, then you add one, one node. This is like.
00:36:05.764 - 00:36:25.572, Speaker A: Yeah, you're probably right. Yeah, yeah, you're right. I'm not seeing the picture correctly. You're right. That's in fact, since this is connected by one node, so all of these are also a maximum. So in fact, neither one of these decompositions is maximum. I mean, ensures that the children are maximum.
00:36:25.572 - 00:36:44.664, Speaker A: So you're right. In this case, I don't know what this vertex is, but you could have taken, for example, this entire thing, excluding only this vertex. And that would be the maximal, a maximal proper subgraph that's rigid.
00:36:44.964 - 00:36:45.744, Speaker C: Yeah.
00:36:46.044 - 00:36:56.548, Speaker A: Okay, you're right. Yeah. So in this case, both of these decompositions do not have that extra condition that I mentioned.
00:36:56.716 - 00:36:57.452, Speaker C: Okay?
00:36:57.588 - 00:37:02.944, Speaker A: But they are, they do decompose into rigid subgraphs. That's true in both cases.
00:37:03.924 - 00:37:04.804, Speaker C: Okay.
00:37:04.964 - 00:37:55.362, Speaker A: Okay, so in both cases, all these nodes here, the children of this represent these two big circles, which are both rigid, and here they represent that one and that one, which are both rigid and so forth. So they're both decompositions into rigid subgraphs, but they are not decompositions into proper vertex maximal subgraphs. Yeah. Okay. Right. So anyway, stepping back a second to what we want, sort of algebraically, we want to ensure that every time we want, we're picking a small. And when I say solvable subsystem, I mean it's got finitely many solutions, which is, we understand that as rigid, generically rigid.
00:37:55.362 - 00:38:54.692, Speaker A: So rigid graph for that dimension. Okay? So we want to pick something that has finitely many solutions, which means rigid, and then, in a way, solve it or do somehow replace it by a smaller subsystem, so, which I call a simplification of ti, of si, and replace it into the original system, ei. So currently you have a system, overall polynomial system, ei, and you're taking a small piece of it, and which you're guaranteed has finitely many solutions, a subsystem, a direct subsystem of it, which means a set of subset of equations with those variables. Yeah, that's what I mean. Direct subsystem of it. And then you do something to solve it. I mean, you wanted to have finitely many solution.
00:38:54.692 - 00:39:52.004, Speaker A: You can replace it by something else, which is simpler in the simplification to get the next big polynomial system. And what the key thing you want is, you want somehow the set of real solutions of the new system to be preserved. Okay? So if you go here, this is just a simple picture. Say I find a subsystem which has finitely many solutions. You simplify it somehow and put it back in there such that you're not, you're not changing the set of real solutions of this one. And then you find another subsystem here, simplify it, and, and then you get a new system, and you find a new one, and so on and so forth. Right? So the, the second transformation that simplifies this system might also end up simplifying the previous one.
00:39:52.004 - 00:40:36.004, Speaker A: That's fine. Okay? But the key thing is you're preserving real solutions as you proceed. So this is essentially a schematic of how you do this. We have linearly ordered this, but it doesn't have to be a linear order. I mean, you could pick s one and s two at the same time, for example, if they don't, if you can keep the other conditions of simplification and so forth, preserve those. So these are some kind of subsystem simplifiers. So if you look at the desirable properties of the decomposition, essentially we want to make sure that these transformations or simplifications that you do preserve subsystems.
00:40:36.004 - 00:41:46.694, Speaker A: And if you apply this transformation to two different subsystems, it's the same as applying transformation to the union of those subsystems. If you apply the transformation to the intersection of these subsystems, then by intersection I mean induced. So if you have a subsystem and another subsystem, the intersection would be a subsystem of a subset of equations that belongs to both of the original subsystems and a subset of which is supported on a set of variables which also belong to the original subsystems. That's what I mean by union and intersections of subsystems. And. Sorry, I apologize for that. Okay, so, and the other thing I want is that the overall system, at any given stage, ei, which is the overall system, you can sort of split it up into three parts, which is the subsystem that I picked up, the solvable subsystem, or has finitely many solutions, union some kind of a part, that is that, that has share some equation, sorry.
00:41:46.694 - 00:42:08.046, Speaker A: Share some variables with it. And another part that doesn't share any variables with it. Okay, and so this is solvable Ri is a maximal subsystem such that Si and. Oh, sorry. Si and Ri don't share variables. UI shares some variables, sorry. And all three of them do not share any equations.
00:42:08.046 - 00:43:05.144, Speaker A: So in other words, the set of equations has been split into three parts. These two don't share any variables. These 2 may, Ui may share some variables with sin, Ui, and for any a subset of ri, you also have that the transformation or the simplification applied to a gives a itself. Okay? So in other words, the simplification at any given stage only simplifies si. And it may simplify ui somewhat, but it does not simplify ri, which does not share any variables with Si. And then we also have the pre images of these. If you just keep taking the pre images of si, you can get the, essentially, you can get all in all it's saying is that you keep, essentially preserve the set of solutions.
00:43:05.144 - 00:43:46.414, Speaker A: Okay? So what we would like to do is these properties. We would like to sort of write them down generically so that. And express them graph theoretically. Okay, so that's the idea of how this whole doctor decomposition recombination plans came about, which is that here's what we would like to algebraically decompose. This is how we would like to algebraically decompose. And we want to sort of do this genetically beforehand by just looking at the graph. We get a decomposition just based on the graph.
00:43:46.414 - 00:44:14.292, Speaker A: And then after we have decomposed, now we have a polynomial system, which we have decomposed based on some properties of the underlying graph. And then now we take. We solve the polynomial system. Hopefully, it gives us some kind of block diagonalization where the blocks are not too big. Sorry. Block triangularization of the system so that the blocks are not so, so big, and then we solve it. So that's the overall strategy.
00:44:14.292 - 00:44:41.796, Speaker A: Right. So again, to repeat, we want to recurse. Whoops. Somehow, it's very sensitive to my fingers moving on the pan here today. Okay, so we want to recursively decompose the system into separately, independently solvable subsystems to do this. We know. So, in this picture, we could do s one and s two together.
00:44:41.796 - 00:45:49.764, Speaker A: But as we want them to be somehow solving s one or simplifying s one, we don't want it to interfere much with s two. So, to do this, we no longer care about the system, but only the underlying combinatorics or the graphs. So here I say hypergraph, because this particular talk was prepared in a slightly more general setting than just distances, right now, we're only caring about distances. So just think of it as a graph. And so we can use combinatorial rigidity to translate this, you know, having finitely many solutions, generally having a solution, or genetically having no solutions, or just one solution into properties of. So over constraint doesn't necessarily mean just one solution. But generically, if you think of having more equations than unknowns, unless those extra equations are somehow consistent with the others, the ones that they depend on, you will have zero solutions.
00:45:49.764 - 00:46:52.440, Speaker A: So, okay, so other uses of this, doctor planning, by the way, is decomposition, recombination planning. All this means is that you have a polynomial system to solve, but you're going to look at the graph underlying it and beforehand come up with some kind of a decomposition that has the properties that we talked about two slides ago. Okay, so the uses are, you can sort of, by looking at the doctor plan, get a sense of how difficult it's going to be to algebraically solve, eventually solve the system if the graph is under constrained, which means flexible. And this is where we're going to spend a little more time later, more deeply on the properties of tree decomposable graphs and others for which these completions, by which I mean the, it's not like the partial matrix completion. This is slightly different. What this is saying is the graph is not rigid. I want to add enough edges so that it becomes rigid.
00:46:52.440 - 00:47:54.404, Speaker A: Right. So, and I want to add them in a nice way and we can define nice later. And so the, and these new edges that you're adding can be used as kind of parameters to understand the infinite or more than zero dimensional configuration space of this under constraint graph. Under constraint just means not rigid, so independent and not rigid. So the other thing is you have dependent edges or dependent constraints, and we want to have some way of removing these dependent constraints or recognizing them and removing them if needed. And this doctor plan helps you do that as well. And finally, you want to allow, you know, sort of a decomposition, the decomposition into subgraphs of this type allows you some kind of underlying decomposition of the rigidity matrix, the stress matrices and the flex matrix.
00:47:54.404 - 00:48:23.956, Speaker A: You've seen all of these in Tony's class. Okay, so your recursive decomposition of the graph into rigid pieces allows you to sort of recursively decompose the rigidity matrix, the stress matrices and the flex matrices. So it has many, many, many uses, and it's, it's a heavily used concept now in computer aided design and other fields that do this on a regular basis.
00:48:24.060 - 00:48:24.724, Speaker B: Mira?
00:48:24.844 - 00:48:25.544, Speaker A: Yeah.
00:48:26.164 - 00:48:28.024, Speaker B: What's a flex matrix?
00:48:29.124 - 00:48:32.424, Speaker A: The right null space of a rigidity matrix.
00:48:35.804 - 00:48:40.124, Speaker B: So just the kernel of the rigidity matrix is a subspace.
00:48:41.744 - 00:48:48.216, Speaker A: The kernel of the rigidity matrix is a, is another matrix. Yeah.
00:48:48.400 - 00:48:51.872, Speaker B: Well, so you choose the basis and write it as a matrix. Okay, I see, yeah.
00:48:51.928 - 00:49:57.648, Speaker A: All right. Yeah, so they just, like they call, well, stress matrices are slightly different. They're not just putting together the stress vectors into a matrix. You write the stress vectors in matrix form. You could do a similar thing for flexes. But in either one of those cases, it doesn't matter whichever way you want to think about it, whether you want to think about it as just putting the stress vectors down as rows of a matrix or putting the flex vectors down as a columns of a matrix, or you want to think about writing the stress vectors as a stress matrix, you know, or flex vectors as flex matrices, which either way, whichever way you think about it, you get decompositions from the doctor plan. Okay, so now I can sort of define, you know, you will see in different papers, the doctor plan is defined slightly differently because which of these conditions have been imposed is not clear.
00:49:57.648 - 00:50:28.604, Speaker A: Okay, so the original d is not always uniform is what I'm trying to say. So you have to sort of pay attention to this. So here, just, a doctor plan just says that each node is a rigid sub graph of g. So in fact, you know, it's not saying that it should be rigid. Maximal, maximal, proper sub graph, it says. No, that doesn't say that. So you could in fact have that trivial decomposition that I had where the k 33, you know, I divided it up into all the children.
00:50:28.604 - 00:51:09.544, Speaker A: Well, in that case, that's the best you could do. But I could have taken even c two, cross c three and divided up into all leaves, which is the edges, and that would perfectly satisfy this definition. Root node is a maximal rigid subgraph. An internal node is the union of its children, and a leaf node is a single constraint. And in this case, it's a single edge, which is a single distance. So that's the definition of a doctor plan of a, in our case, just a distance constraint graph. And it could be a forest because, you know, in this description, we're not starting out with any assumption that g be rigid.
00:51:09.544 - 00:51:59.512, Speaker A: So if g is not rigid, it's going to end up with multiple roots, so to speak, each one of those being the maximal rigid subgraphs. A root node is a maximum rigid subgraph of the graph. But if you started out with g itself being rigid, that you'll have a tree kind of thing with a single root node. Okay, here's an example doctor plan. This simple three decomposable case that we looked at, this is a perfectly valid doctor plan. According to this definition. You could just split it up into your nine edges, or you could have a slightly nicer doctor plan here in a way that splits it up into the two maximal rigid proper subgraphs, which are the two triangles and then three edges and so forth.
00:51:59.512 - 00:52:54.302, Speaker A: So both of those satisfy the properties of, in the definition, the conditions and the definition of the doctor plan. Okay, so now an optimal doctor plant is one that actually minimizes the maximum fan in by fan in. I mean, here's a tree. So we have a tree here, and a fan in is the number of children that any, I mean, sorry. The fan in for a given node is the number of children it has and minimizes the maximum fan in means it minimizes the maximum fan in over all the nodes in the doctor plan. I want to use the word nodes for the doctor plan rather than vertices, because the vertices are the vertices of the graph and the nodes are the vertices of the doctor plan. So I just want to keep that separate so that we don't get confused.
00:52:54.438 - 00:52:55.354, Speaker B: Amira?
00:52:55.774 - 00:52:56.554, Speaker A: Yeah.
00:52:57.654 - 00:53:02.878, Speaker B: If there's a fan in of two at a node of a doctor plan.
00:53:03.006 - 00:53:03.334, Speaker A: Yeah.
00:53:03.374 - 00:53:06.594, Speaker B: Does that mean that we have a flexible graph?
00:53:07.254 - 00:53:34.834, Speaker A: No, it just means that they could. So look at this. So if you go back to this, it says nothing about how much the children overlap. So it simply says each node is a rigid subgraph of g. So you could have two nodes which have a lot of intersection. Okay. So just having two doesn't mean that it's flexible.
00:53:34.834 - 00:53:46.002, Speaker A: So for example, you could have two rigid sub graphs that intersect on it. On it. In the case of d equals two on a triangle, then or three vertices, then the result is rigid.
00:53:46.138 - 00:53:52.842, Speaker B: Okay, so this, so tree decomposable had like three subgraphs that intersected each pairwise.
00:53:52.898 - 00:53:55.610, Speaker A: At one, one vertex. Exactly.
00:53:55.802 - 00:54:00.012, Speaker B: This is not, this isn't like, none of those requirements are here.
00:54:00.148 - 00:54:29.180, Speaker A: None of those. We have generalizing well beyond that because the three decomposable class is a very small class. Okay. Okay, so an optimal doctor plan is something that minimizes the max. One thing that we want to, what we know is that even if you have solved, completely solved the two subsystems of a given system, or, you know, so find out. Found all the solutions, finitely many solutions. We do want them to be rigid.
00:54:29.180 - 00:55:19.384, Speaker A: So there's generally finitely many solutions for each of the subsystems. And if you have solved them, you know, getting the solutions for the parent is, you know, directly, the size of the system for the parent is directly proportional to the number of children, which is why you want to minimize that. The. So in other words, in the block diagonal, if you do a block triangularization, the blocks, the sizes of the blocks are directly proportional to the number of children. You know, so what we want to make sure is that those sizes of those blocks are small. So which is why you want to. An optimal doctor plan minimizes, for some reason, this is jumping around, minimizes the number of children across all the nodes.
00:55:19.964 - 00:55:21.854, Speaker B: And Mira, can I ask another thing?
00:55:21.964 - 00:55:22.306, Speaker A: Yeah.
00:55:22.370 - 00:55:26.974, Speaker B: So maybe you can go back to that. This to our. Slide 107.
00:55:27.354 - 00:55:33.458, Speaker A: Slide 107. Okay. Why is it jumping around like this? I have no idea. Okay. This one?
00:55:33.586 - 00:55:35.014, Speaker B: Yeah, yeah.
00:55:43.914 - 00:55:44.834, Speaker A: Hopefully I.
00:55:44.914 - 00:56:06.534, Speaker B: There any, is there any requirement, I don't see that the parent can be produced from its children. So if, you know, realizations of the children can. Do we require that the parents, that the realizations of the parent can be found from its children?
00:56:06.694 - 00:56:21.304, Speaker A: Yes, that's what we're trying to get at, you know, so remember that original algebraic conditions that we had. I don't know why this. Anybody have an idea why this is jumping around so much?
00:56:23.084 - 00:56:28.184, Speaker B: No. You could go to Google Chrome and try to make it full screen.
00:56:28.764 - 00:56:35.612, Speaker A: I did, I had full screen a moment ago, so. But now maybe it's better behaved. I don't know.
00:56:35.748 - 00:56:39.260, Speaker B: No, I mean, if you literally, in the Chrome menu, if you click view.
00:56:39.292 - 00:56:41.992, Speaker A: Oh, you mean just go to present or something?
00:56:42.148 - 00:56:50.272, Speaker B: Well, no, actually if I just click view and then I click enter full screen sometimes that's okay.
00:56:50.448 - 00:56:56.520, Speaker A: Oh, okay. Hopefully this has stopped now. I'm hoping. Okay, you can, but either way, I don't.
00:56:56.592 - 00:57:00.124, Speaker B: That requirement in this list is it.
00:57:03.064 - 00:57:37.844, Speaker A: This list is, it's trying to write the original algebraic conditions purely in terms of graphs. So the internal node is, the union of its children is probably what you're trying to get at by union means. An internal node is a graph. The children are all subgraphs. You take the union of the sub graphs of the children. You have to get the original graph. Okay, union of vertices, take the union of edges then.
00:57:38.504 - 00:57:42.592, Speaker B: So is this the definition of doctor plan? This slide, or was it slide, is.
00:57:42.608 - 00:58:08.144, Speaker A: The definition of doctor plan. We will come to this, adding this extra condition about maximal subgraphs in a minute. This is the most commonly used definition. Sometimes they automatically add this maximality condition to doctor plan, but it's not usually necessary.
00:58:09.364 - 00:58:14.664, Speaker B: So a leaf node is a single constraint. That means one edge, because we have a. Dr. Plane of a graph.
00:58:15.004 - 00:58:16.764, Speaker A: This is. Yeah, this is.
00:58:16.924 - 00:58:18.724, Speaker B: And the nodes are sub.
00:58:18.844 - 00:58:38.984, Speaker A: Yeah, I'm trying to, I'm using some old slides, which is doing something more general than just our graphs. So think of our usual graphs which, where every edge represents a distance. Yeah, yeah, yeah. And so these are single edges that represent one distance. Yeah.
00:58:39.924 - 00:58:42.344, Speaker B: And the involved primitives.
00:58:44.364 - 00:58:50.620, Speaker A: Where is the involved primitives? Oh, involved primitives are the vertices, like.
00:58:50.652 - 00:58:53.324, Speaker B: The coordinates of the vertices in the.
00:58:53.404 - 00:59:15.894, Speaker A: No, sorry. Yeah, I should have changed the slide, I'm sorry. So this is just because we are talking in general about hypergraphs. This constraint could involve more than two vertices. That's why it says involved primitives, but they are just, in our case, it's an edge and the corresponding two vertices. That's it. Okay, so in all the pictures, that's what we've got.
00:59:15.894 - 00:59:17.674, Speaker A: It's an edge with two vertices.
00:59:18.174 - 00:59:19.074, Speaker B: Okay.
00:59:21.574 - 00:59:24.354, Speaker A: So the leaves are all edges.
00:59:24.974 - 00:59:30.190, Speaker B: Okay. And there's no requirement right now that the parent can be produced from its children.
00:59:30.342 - 00:59:36.710, Speaker A: What do you mean by produced from its children? We need the realization. The parent is the graph, which is the union of the children.
00:59:36.782 - 00:59:44.158, Speaker B: Graphs and its realizations do not need to be produced from its.
00:59:44.206 - 00:59:48.606, Speaker A: There is nothing about realizations at all in this. This is purely graph theoretic.
00:59:48.750 - 00:59:49.990, Speaker B: Okay, thanks.
00:59:50.142 - 01:00:13.254, Speaker A: Okay, so this is purely graph theoretic. It's a statement about graphs. And we're going to try to show you later on, we'll try to show that this is enough generically to give the properties, algebraic properties, that we talked about earlier, which is that we preserve the set of solutions as we go.
01:00:15.514 - 01:00:16.802, Speaker B: Okay, thank you.
01:00:16.938 - 01:00:30.334, Speaker A: Okay, any other questions? This is just a story about this, this terminology here with constraints and primitives. This is just a single edge. That's all. Leaf node is a single edge.
01:00:32.954 - 01:00:37.934, Speaker C: Amir, when you say graph underlying a polynomial system, what is that?
01:00:39.554 - 01:00:50.794, Speaker A: The underlying polynomial system in our case would be a distance polynomial for this edge, which says in. Because.
01:00:51.534 - 01:01:00.034, Speaker C: Yes, I heard you said given a polynomial, find the graph, or something called a given polynomial system, find a graph.
01:01:00.414 - 01:01:01.246, Speaker A: Yes.
01:01:01.430 - 01:01:05.354, Speaker C: Underlying, like. So there's a graph for any polynomial system.
01:01:08.614 - 01:01:50.734, Speaker A: We are at the moment only talking about distance constraint systems, so it's not arbitrary. For anomalous citizen, there are. I mean, if it comes from a geometric system of geometric constraints, then yes, we do have a decomposition of those systems using the same ideas. The ideas generalize. So if you have, I mean, I. Later on, I will show you other examples of constraint systems, but we're not going to get to that until the very end of the class, I think. Let me see in my own thing here.
01:01:50.734 - 01:01:58.134, Speaker A: So, rigidity beyond distances, beyond distance constraint. So we won't get to that until we get here.
01:01:58.634 - 01:01:59.498, Speaker C: Okay.
01:01:59.666 - 01:02:05.134, Speaker A: Everything from now to then are going to be only distance constraint systems.
01:02:05.834 - 01:02:06.810, Speaker C: Okay.
01:02:07.002 - 01:02:17.374, Speaker A: Okay. So whenever I say polynomial system, you can assume it's a bunch of distance constraints, distance polynomials representing distances between pairs of points. Of course, the dimension may change.
01:02:18.194 - 01:02:19.054, Speaker C: I see.
01:02:19.474 - 01:02:58.860, Speaker A: Okay, thank you. Yeah, but you'll see that even three decomposable graphs, there are many examples of general geometric constraint systems which involve other types of constraints, which can be written as three decomposable graphs. We'll get to that eventually. Right. But I don't want to clutter our thinking right now, because we have been keeping this only distances so far, distance matrices, euclidean distance matrices, and so forth. So let's just stick to distance constraints for the rest of the class until we get to the end. Okay?
01:02:59.052 - 01:02:59.740, Speaker C: Okay?
01:02:59.852 - 01:03:29.818, Speaker A: Yeah. Okay. So here's where we were. So here's a graph, and we have divided. These are two doctor plans satisfying the conditions of the definition of the doctor plant. An optimal is. It minimizes the maximum fan in.
01:03:29.818 - 01:04:33.188, Speaker A: And it turns out that finding an optimal doctor plan is NP hard. And it's a very simple proof that takes a reduction from the click problem. And it's in, it's in a paper that we wrote some time ago. So next talks about history. So, in the eighties, this idea of triangle decomposable graphs came about simply because of its nice properties, the ability to solve the corresponding underlying distance polynomial system, distance constraint system, and that it had this quadratic radical solutions, at least in most planar graphs, and believed to be the case for other graphs, too. And I mean, one direction is straightforward, by the way, if it's three decomposable, all three decomposable graphs, planar or not, are quadratically radically solvable. It's the other direction to show that in order to be quadratically radically solvable, you should be tree decomposable.
01:04:33.188 - 01:05:19.594, Speaker A: That, sorry, this should have been tree decomposable. Three decomposable systems can be triangularized. Yeah. So to go in the converse direction is where you need the planarity so far, I mean, but it's believed that the converse also holds without planarity. And so they struggled after three decomposable, trying to generalize, was kind of using various combinations of things, and different things were confused in a way. The ability to find rigid subgraphs was confused with finding the doctor plan. And then we sort of came and this is my grad student, by the way, and this is a collaborator.
01:05:19.594 - 01:06:34.826, Speaker A: We collaborate from computer aided design community. So we formalized this idea of decomposition recombination plan, which has since been very widely adopted, at least in the computer aided design community. And so we separated the algorithms that find the rigid subgraphs from getting the decomposition. So assuming there's an oracle, so to speak, if you might have, if you've been in an algorithm class or theory of computation class, you've used the word oracle. So assuming that there is some magical thing which if you give it a graph, will tell you whether it's rigid or not for the correct dimension, you can then ask, okay, how do I do the decomposition, assuming the existence of this oracle? So we sort of separated the parts, and we said, okay, here's what we gave two papers. One talks about sort of, what is it that we want algebraically. And then how do you capture a graph theoretically? And then give some algorithms for coming up with these decompositions.
01:06:34.826 - 01:07:27.926, Speaker A: So they're a two part paper. And this slides are from another paper, which was a later paper with my grad students, which essentially cleans up some of, I mean, gives an alternative way of thinking about these doctor plans in the algorithms. The original algorithms from Hoffman, Lomnosov, Sitaram did a bottom up kind of building up of the doctor plan. And these, this one does a top down building up of the doctor plan, which means it starts the graph, splits it up into parts, and then splits it up in parts and so forth. And these were sort of building it up bottom up. But both use this idea that there is a rigidity detection heuristic. I mean, sorry, Oracle.
01:07:27.926 - 01:08:20.814, Speaker A: Okay, so, and so for this, I preferred to talk about the later algorithm, this top down algorithm first, before we talk about the bottom up algorithm, simply because it's a lot easier to understand. Okay, so the top down algorithm works if there is an underlying abstract rigidity metroid. These are the conditions under which it works. So what does work mean? Depends on having an algorithm versus having a certain complexity. Sorry, the top down algorithm, which is what we're talking about now. It's a later paper. I'll give you the references in the piazza, or add them to the slides later.
01:08:20.814 - 01:10:13.904, Speaker A: It requires an abstract rigidity made to actually prove the things we're proving, sort of the structural theorems we're proving about the, about the system, about the decomposition, you know, to show that it is. If the original graph is independent in the d dimensional rigidity metroid, we achieve, this algorithm will achieve optimality. And if we have an underlying sparsity metroid, which we have in two dimensions, but we don't have, we have no idea what it is in higher dimensions, then we get, actually a polynomial time algorithm. So what I mean is, we can give a definition of a doctor plan with some extra conditions which has certain properties, and the properties all go through if the first two are true, if it has an underlying abstract rigidity metroid and it's independent, but doesn't, you know, if it has a sparsity metroid, we actually get a polynomial time algorithm. Um, let's go to the next one. Okay, so this structure that we're going to talk about now adds this extra condition that the children of any given node are, in fact, vertex maximal proper sub graphs that are rigid, and we call them romps because they're so, so long. Okay, rigid vertex maximal proper subgraphs RV MP's, we call them rumps.
01:10:13.904 - 01:11:06.244, Speaker A: Okay, so we have two additional properties. First of all, the children of any given node are rigid vertex, maximal proper subgraphs. And then the second thing is, basically comes from the abstract rigidity metroid condition is that if talks about the intersections between pairwise intersections between these drums. So if all pairs of them intersect trivially, trivially means the size of their intersection is d whatever d is. Sorry, d minus one or fewer. Right, so that's trivial intersection. So in case of d equals two, the intersection would be one vertex and d equals three, the intersection would be two vertices and so forth.
01:11:06.244 - 01:12:09.456, Speaker A: Otherwise, exactly two that intersect non trivially. If what is all of them are children. Otherwise exactly two that intersect non trivially are children. Okay? So in other words, what this is saying is that there is either two children which may or may not intersect non trivially, but if they do intersect non trivially, there are only exactly two. And otherwise every pair wise they intersect trivial and intersection. Trivial intersection, as I said, was the size or the number of vertices in the intersection is one less, I mean, at most one less than the dimension. Okay, so in the case of three decomposable graphs, for example, the intersection between any pair is one vertex and that's a trivial intersection, okay? And in, in two dimensions, in order to be a non trivial intersection, the intersection should contain at least two vertices.
01:12:09.456 - 01:12:28.204, Speaker A: And you know this condition from the abstract rigidity metroid, that in general, if you have two rigid graphs and their intersection is greater than or equal to d, if they intersect on more than d vertices, d or more vertices, then the result is also rigid.
01:12:28.364 - 01:12:41.864, Speaker B: Hey, mira. Yeah, so this condition too is what I was thinking about before. This guarantees that knowing the realizations of the children will give you realizations for the parent.
01:12:43.164 - 01:13:24.824, Speaker A: Um, I don't think so. I don't think you need this. I think just the doctor plan guarantees that, because simply you're maintaining all the union of all the children are part of the parent. I mean, give the parent, the parent includes all the constraints in the subgraphs of the children. I don't think you need this. This is just, these are additional ones that, you know, help prove optimality and other things like that.
01:13:26.804 - 01:13:27.664, Speaker B: Okay.
01:13:28.084 - 01:14:07.454, Speaker A: Okay, so notice that at this point we are not, we've sort of moved into a purely graph world. We're going to have to tie it later to the algebraic world in the, so when we say decomposition and recombination, the recombination part is. Okay, now we have found a doctor plan. And now we have to take the corresponding distance polynomial systems and put them together and get the solutions. So that's the recombination part. We'll come to that later. So at the moment, we are living in a graph world and we are defining certain graph decompositions.
01:14:07.454 - 01:15:28.404, Speaker A: And we want to see, and we have defined the optimal doctor plan here, purely using a graph, theoretically. And now we're going to try to ask, can we get this optimal doctor plan fast? For that purpose, we are defining an extra condition on the doctor plan, which we call canonical doctor plan, which these are the two extra conditions. So this nice theorem is what we showed, which works whenever the graph has an underlying abstract rigidity matriarch. That's what this, this intersection thing is based on. So what we say is that a canonical doctor plan exists for a graph, and any canonical doctor plan is optimal if the original graph that you started out with is independent in that, in that d dimensional rigidity matriarch. Okay, so we will, we are, you know, today what I want to do is give a quick overview of the algorithm itself and then idea of a canonical doctor plan, how that helps with the algorithm. But in terms of actually proving it, we're going to do that later, probably in the next class.
01:15:28.404 - 01:16:42.076, Speaker A: Okay? So I might also spend a little bit of time today in talking about the bottom up algorithm instead of the top down algorithm. We'll come to that later, I mean later in today's lecture. So when we, the importance of the canonical doctor plan, we restrict the space of doctor plans to this special class of, instead of thinking about all doctor plans, we are only looking at the special class. So in this example, as you can see, basically three c three cross c two s, familiar c three cross c two s, all of which intersect on a triangle here. And if you ask yourself, what are the maximal proper, I mean, rigid subgraphs here, you cannot, I mean, these are maximal proper rigid subgraphs, because if you take this one, then the only thing you've got is these three guys attaching to a triangle. So these are the maximal properties subgraphs. One of them and then the other one is this one.
01:16:42.076 - 01:17:28.916, Speaker A: There's a third one, which is this one and this one. But the canonical doctor plan says in that case, if the intersection is non trivial, you just take two, any two doesn't matter. So we have taken this one and that one, two of those, and they intersect, as you can see, on the entire middle part. This entire middle part is common between the two of them. Then you split those, each of those into their maximal two of their maximal, which also have non trivial intersections. And this one, again, two, and they happen to share, this is their intersection. And the intersection happens to be one of the subgraphs.
01:17:28.916 - 01:18:14.592, Speaker A: In this case, it's drawn like this, but we could have just drawn it like a tree with this intersection sort of appearing twice here and there. And then after that, there's not much you can do. The maximal proper subgraphs are these triangles and the edges. And so that's what it looks like in all of those cases. So this is a doctor plan that sort of satisfies this canonical property. And we want to, as you can see, you know, there is, in order to get the optimal doctor plan. So the goal is to get this, where was the optimal doctor plan? So the goal was to get the optimal doctor plan that minimizes the maximum fan in.
01:18:14.592 - 01:18:58.908, Speaker A: So intuitively, it's kind of clear why you want to choose maximal proper subgraphs of a given graph, because you want the minimum number of children coming in. And you know that the children have to basically cover, because union of the children and the parent. The children have to cover the parent. So that's where the intuition of doing this canonical thing comes from and what we can show. So we will eventually prove this theorem, that a canonical doctor plan exists for a graph. And any canonical DDR plan is optimal if G is independent in the d dimensional rigidity metroid. So just these two sufficient.
01:18:58.908 - 01:19:45.842, Speaker A: We'll prove that later. But as I said today, I would like to motivate why we want to go through, it's a graph theoretic kind of proof, and why we want to go through that proof. Because the canonical doctor plans give us an optimal doctor plan in a fast algorithm for finding an optimal Dr. Plan. Now, the theorem says, if you have a good oracle. So that's what this underlying sparsity metroid is talking about. So you could say either that there is an underlying sparsity metroid, therefore you can get using pebble game or network flow or whatever, recognize rigidity quickly, or you have an oracle.
01:19:45.842 - 01:20:50.180, Speaker A: I mean, I could have changed the star to, provided there exists an underlying sparsity metroid, or there's an oracle for detecting rigidity, d dimensional rigidity of a graph. Either one of those two. Then we have computing an optimal doctor plan for a d independent graph in whatever dimension has complexity, order v cubed. Okay, so another, as a byproduct, we also get that this finds minimum and maximum non trivial rigid subgraphs of the input graph, which are also shown to be NP hard in the same paper. In, in general, okay, by minimum means, you know, sort of finding a, not just a minimal, non trivial rigid subgraph, but actually one that has the fewest number of vertices. Believe it or not, that's a hard problem. So I give you a graph and I want you to find sort of the minimum sized subgraph in it, which is non trivial, which means it's more than an edge, let's say, and is rigid.
01:20:50.180 - 01:21:40.264, Speaker A: That's a hard NP hard problem. Finding the maximum one, again, not maximal, but maximum, which means among the maximal ones, one that has the most number of vertices is also NP hard. All of these problems are np hard or optimal. Doctor planning is np hard. All of them are generally np hard. But we can show that using this notion of canonical doctor plan, we can get it down to order v cubed for independent graphs, assuming the existence of rigidity oracle rigidity detection. Okay, so for doing this, this algorithm relies on yet another twist of a canonical doctor plan, which we call a pseudo sequential doctor plan.
01:21:40.264 - 01:22:52.214, Speaker A: So it is a canonical doctor plan split up, I mean with a twist. So here's a canonical doctor plant, and what we're going to do is essentially think of the, we're going to create a doctor plan which takes one of the two non trivial rigid subgraphs, sorry, one of the two is, sorry, rigid subgraphs with non trivial intersection and thinks of them as, you know, splits them up completely into the next level. Right? So this one at the next level is going to split into, you know, it's going to split into all of these components. So in other words, here we've got the two rigid subgraphs with the non trivial intersection. We keep the non trivial intersection in one of them and then in the other one, the intersection has now become part of the left subgraph. So this is the remaining part. And the remaining part is just, you think of it as just split into its next level.
01:22:52.214 - 01:23:32.450, Speaker A: Okay, so again here you've got two parts and they intersect on the triangle. Sorry, you keep the one of them and then the rest of it is just the triangle with the three edges here. So this is the sort, think of it as the remaining part. So whatever you keep, this is the, all the remaining stuff, remaining stuff decomposes like this. So if you think here, you take this, what is remaining? What remains is these three edges plus this triangle. If you just take that, those three edges and this triangle, and think of that as a graph, it's under constrained, of course, but it splits up. This is how it splits up.
01:23:32.450 - 01:24:11.254, Speaker A: Similarly here, you know, you take this and then you take the remaining part, which is just, again, the three edges with a triangle that's under constraint. It's flexible. It's a non rigid graph, and this is how it splits up. And so this is a regular doctor plan for that. So I just want to point out for you that the canonical doctor plan also doesn't offhand what is the definition of the canonical. You remember, the doctor plan itself doesn't require the original graph to be rigid. The starting graph, so it could have, the doctor plan might end up with multiple roots.
01:24:11.254 - 01:24:39.398, Speaker A: And this is the same thing. I mean, canonical doctor plan doesn't require that the doctor plan has a single root. It could have multiple roots. So the original graph could not, need not be rigid. We're splitting it up into rigid sub graphs, okay? And the original graph itself may have come up, come with some as rigid subgraphs. So that's something that we are maintaining here. So this is called a pseudo sequential doctor plan.
01:24:39.398 - 01:25:29.714, Speaker A: So now we need a definition of something called a branch in order to describe the algorithm. I'm getting to the algorithm. Okay, so the branch is, so, branch of a tree, so it's a branch of a tree is every node on the path from, so the branch includes the tree and two vertices on it. So every node on the path from a to b, and their children. So in this case, you know, the branch here, you know, you can think of this as a and b, and here's your tree, this is your t, this is a and b. So basically you'll have all of the nodes from a to b and their children. So that's, that's the branch.
01:25:29.714 - 01:26:51.204, Speaker A: Okay, so these guys are not included in the branch. So the crossed ones are not included in the branch. So it turns out that the leaves of the branch are exactly the set of rigid vertex maximal proper subgraphs of a with a, you know, a minus b. So in other words, if you took the graph a and took away from it, cut away from it the graph b. If you cut away from it the graph b sub graph b, then the leaves of the branch, which means these black nodes here we can show, are exactly the rigid vertex maximal proper subgraphs or rumps of the graph that's left over after you've removed b from a. This clearly, this sort of tells us that if you can find the rigid vertex maximum proper sub graphs, you know, then you can sort of construct the, you know, sort of think of a wave going this way, starting from here, and you construct this branch, and for each of those you think of its rigid vertex maximal proper subgraphs and go like this. And you can sort of construct this canonical pseudo sequential Br plan in a kind of a wave going branch by branch this way.
01:26:51.204 - 01:27:45.538, Speaker A: So once, so, so the, and we can show that the rigid vertex maximal proper subgraphs, these black ones, can be found in time order v v squared. So that sort of gives the sketch of the entire algorithm, which is what I'm going to do next. So we have to spend a little bit of time on this slide. So the algorithm, we can find the pseudo sequential doctor plan by computing the, the rigid rumps of g difference e for all e. You don't have to necessarily do all e, but in the worst case you do all e and doing a quadratic amount of work for each leaf. Okay, so we, here's the, here's the algorithm. You start with the g as the single node in the doctor plan, right? So it has nothing else.
01:27:45.538 - 01:28:56.958, Speaker A: And then the recursive step is you compute a branch for each leaf in the doctor plant. So start with the, you choose an arbitrary leaf, and for each edge f in the subgraph that corresponds to that leaf, it could be a single edge, in which case when you start right in the beginning, it will be a single edge. So f in l is l itself is f. Then you compute the rigid vertex maximal sub graphs of the l, the leaf from, with that edge taken away. And then for each of the rigid vertex maximal subgraphs, you choose an arbitrary edge g and compute the rigid vertex maximal subgraphs of l difference g, and then compute the branch from l to f. And this uses linear number of graph intersections to position each leaf. Okay, so essentially, as I said, the algorithm sort of moves in a wave, if you will, going from one side, one sort of path from root to leaf.
01:28:56.958 - 01:29:36.742, Speaker A: It will start with that, and then it will find all these rigid vertex maximal subgraphs. And then for each of those it computes a branch and so forth. So it proceeds in this manner. This is all the slides that I actually prepared. I wanted to quickly tell you a little bit about the original algorithm. So this is a much later algorithm. This was published in 2014 or 2015 or something like this, but we had an original algorithm which may, and we implemented it and everything.
01:29:36.742 - 01:30:32.758, Speaker A: So I wanted to go and tell you roughly what that algorithm is like. So does anybody know a way of quickly scrolling down to. I guess not. Otherwise I have to. So this is the software, by the way, we had software to solve these geometry constraint systems by using doctor plans, you know, two dimensional and three dimensional cases. Okay, so we had this other type of. So this is the complete decompositions are very similar to rigid vertex maximal decompositions.
01:30:32.758 - 01:31:09.924, Speaker A: And the idea is more or less the same. So it's not the decomposition itself that's different, it's the. So as you can see, the definition, we call it complete decomposition. This was back from 2001 or so. So is that either you have exactly two children and the children intersect on a non trivial subgraph and together their union is everything. Or there's the case two is, if there's, for each pair, they only intersect in a trivial subgraph, and again, each one of them is maximum. So we call that a complete maximal doctor plan.
01:31:09.924 - 01:32:14.796, Speaker A: But the key thing is that the building up of this was done bottom up. So the algorithm that we had, which we call the frontier vertex algorithm, to build this up, we did that, unlike the branch algorithm, which we now have, this was done sort of building up the whole thing, bottom up. Right. And this is showing, okay, after you get the doctor plan, what do you do? How do you sort of use the doctor plan to actually realize or find the solutions? So here's an example of which we call the Hextet example. I just wanted to run over, because we are going on and on about doctor plans to at least get a little bit of an understanding of how. We'll use the doctor plans later, which is related to a lot of the questions that Alex has been asking. So this is a three dimensional structure, by the way.
01:32:14.796 - 01:33:00.526, Speaker A: So then d equals three in this case, unlike many of the examples I've been drawing recently, which are all d equals two examples. So d equals three. So suddenly you get a, you know, this you would consider to be over constrained in two dimensions, but it's well constrained, rigid, minimally rigid in three dimensions, the tetrahedron. So you can see this decomposition here. We got a tetrahedron here, another tetrahedron here, another tetrahedron here, a triangle in the middle, which is the green one. And then you've got these three pink triangles. Okay? So essentially this is a canonical decomposition, because you can't really do much better than this in three dimensions.
01:33:00.526 - 01:33:43.206, Speaker A: You know, think of it, remember that this is in three dimensions. So by saying you can't do much better than this is, this is optimal. It's an optimal doctor plan. It's also canonical. So once you have that, you know, how do you then go and get a realization? Or how do you solve the systems that correspond to this? So in other words, here we've got the system triangle tetrahedron, triangle tetrahedron, triangle tetrahedron and triangle in the middle. And this is a rigid vertex maximal. These are all rigid vertex maximal subgraphs.
01:33:43.206 - 01:34:42.580, Speaker A: This is canonical doctor plan. It's optimal everything it's got the best thing you can have. So now you want to realize or find the solutions. So what you do is you sort of, of course these are in r three, the coordinates of the points, the coordinates of point a parameterized in Cs coordinates, right? So c is the parent, remember CI are the children. So you have, when you think of Xaci, we're thinking of the points coordinator within their own cluster or within their own subgraph. And when I say this, I mean that it's parameterized in the parents coordinate system. And xacci depends on rotation angles and will only be resolved into a final position, xac in cluster c by solving active constraints.
01:34:42.580 - 01:36:04.436, Speaker A: In C we have the word called active constraints. And active constraints means that you have a subsystem that has already been solved, which means you don't really have to worry about most of the constraints inside that subsystem, but they're going to be some other constraints that are shared between subsystems and those will come into play. So those we call active constraints offer doctor plan. So if you go back to the doctor plan. So what this is saying is that now we're back to 2d again, these pictures, if we have already solved something, so in other words, I've already put these guys together into this structure, then this constraints inside the structure, in some sense this is already, we have found the solutions to this. So this constraints that are not shared between structure, these two structures that are required to then solve this system using the solutions of these two systems, those sort of inside constraints, we don't care about the only constraints we care by inside, I mean unshared, we only care about the shared vertices and shared edges. Those are, those are what we call active constraints.
01:36:04.436 - 01:37:08.744, Speaker A: Okay, so here we, so that's what we mean by active constraints here. So if you haven't, so let's consider the situation where we didn't have a doctor plan. So just making it very clear why the doctor plan is so crucial. So suppose you didn't have an unoptimized, sorry, we have the doctor plan, sorry we have the doctor plan, but we are not thinking about it much further when we are solving the algebraic system. So you pick an arbiter minimum minimal covering set of clusters. In this particular case, say c one, c two, c three cover all the vertices, right? So c one, c two and c three alone will cover all the vertices, you set the coordinate system c to one of the child clusters. So let's call it c one, right? So let's c one b, sort of like c one's coordinate system is now going to be the parents coordinate system.
01:37:08.744 - 01:37:59.762, Speaker A: And then we get c two and c three's position and orientation within C's coordinate system by using a 3d composite 3d rotation matrix m and a translation vector t. So if you look here, so we have fixed c one and we are now trying to position c two and c three in three dimensions. So you've got a rotation plus translation matrix for c two, rotation plus translation matrix for c three. You have to position it so that point a in c one and point a in c two coincide, point b in c one and point b in c three coincide and point c and c one coincide. But that's not just it. This is not a triangle, a three decomposal system. It additionally has all of these constraints.
01:37:59.762 - 01:38:53.414, Speaker A: In addition, those constraints have to be satisfied. Okay, so, so that's our active constraint. So the active constraints now between these three systems that have already been solved are the constraint that a is shared between these two, c is shared between these two, b shared between these two, and these distances have to be satisfied, I mean have to be met. So you can, each of these rotations are basically, can be written using the sines of the different angles of between them. And then you just have in addition the sines and the cosines. You'll have additional equations of the type s I squared plus CI squared equals one. Because we want an algebraic system here, we can't have sines and cosines.
01:38:53.414 - 01:39:37.766, Speaker A: So we just have an algebraic equation that forces them to be triggered and trigonometric variables. So this is just standard rotation matrix. Okay, so if you did that, you would get six active distance constraints per c, three distance constraints of this type. So what are the six active distance constraints? So we've got, so we got c one, c two, c three. I see only three. This one, that one and that one plus, I guess. Okay, so now we're writing the coincidence here as active constraints.
01:39:37.766 - 01:40:35.084, Speaker A: Okay, so we will get three distance constraints plus these overlap constraints which say that those a points, the a, the coordinates of the a point in one cluster, the same as the coordinates of the a point and the other cluster. Same thing for b and same thing for c. And each of these is itself three constraints because there's three coordinates that you're equating. So you've got nine constraints like this, plus you've got three more constraints like this. So altogether, twelve constraints. Plus of course, you also need the, the si squared equals CI squared constraints. So those will be another, you know, one for each pair sic.
01:40:35.084 - 01:41:03.818, Speaker A: But from the doctor plan, from our doctor plan, we can actually do quite a bit better than that. Okay, so here's what we do. So if the difference. This is a simpler example here, simpler schematic showing the same example. So this triangle here just represents one of the tetrahedra. This triangle is another tetrahedron, just a rigid in three dimensions. They share.
01:41:03.818 - 01:41:59.964, Speaker A: These two share one vertex, these two share one vertex, these two share one vertex. And then you've got these three extra constraints. So instead of thinking about this as, you know, what we did, which is think of this as sort of a rigid body, and you're sort of positioning this triangle with respect to this rigid body, it's got all of the rotations here. And with respect to those three, instead of doing that, we can think of it as there's a rigid body in the middle. So it was in our doctor plan, in our canonical doctor plan, we had that triangle in the middle. So we take that triangle in the middle and we think of each of these triangles as, you know, having one degree of freedom, or, sorry, one rotation. It's like a quaternion.
01:41:59.964 - 01:42:56.944, Speaker A: That corresponds to rotation about this edge, and same thing, rotation about this edge. And so basically we just have three variables, one representing one each representing these three, these three subsystems, plus a distance, a distance from here, distance from here, distance from here constraint system. So basically we have three variables and three distant three constraints. So it becomes a three dimensional. So now instead of choosing, now we're just showing this advantage of having these canonical doctor plans, as you know, essentially capturing all the maximum subsystems that are there. So we can choose, instead of using just c one, c two, c three, we choose all four, c one, c two, c three. It's not a minimal covering set.
01:42:56.944 - 01:43:25.550, Speaker A: It's got more than just minimal. And then you choose one of them as the home cluster, the one in the middle. So c four is the one in the middle. C four is the one in the middle. You choose that. And so this tetrahedron was shown as a triangle in that schematic picture, but it's basically this tetrahedron. So we do that, and then you use the basic trigonometry.
01:43:25.550 - 01:44:20.160, Speaker A: And then what I just said with using those three rotation variables here. And so using that, you basically end up with much fewer algebraics, much, much smaller algebraic system with only six constraints and six variables and all quadratic. So. And in the previous case, the degrees were also high. The reason is that you have those three distance constraints, um, where if you used all of these rotations and so forth, you know, you have the rotation matrix here, which already is quadratic. You know, the rotation matrix here already has all of these triples and so forth, degree three stuff in it. And so, and then you're squaring that.
01:44:20.160 - 01:45:43.540, Speaker A: So you end up with much higher degree than you do if you do it this way. Okay? So you have this optimized algebraic system. So in general, what you can do is take your canonical doctor plan that you have and impose another combinatorial structure on it, which then allows you to optimize the way in which you actually algebraically solve that, the systems that have been solved, the systems at every node using the solved systems of the children. So essentially, that's the other thing that we will do. So essentially, the remaining lectures on this doctor will be split between D and r. This part that I have talked about, the last part of this lecture are the recombination part after we have the decomposition. So once we have this canonical decomposition, there's a, there's a part that talks about, given that decomposition, how do you optimize the corresponding algebraic system that you will solve? That's the recombination part.
01:45:43.540 - 01:46:22.930, Speaker A: And then the first part is, how do you even get the decomposition? Optimal decomposition. So, and the first part is purely combinatorial. There's nothing, there's nothing other than graphs. The whole thing will be graphs. What we talked about here, and that proof, the key proof that we will do is that, you know, this computing doctor plan is in order, v cubed, that this is the key theorem. And the other theorem is that the canonical doctor plan exists for a graph. And any canonical doctor plan is optimal if g is independent.
01:46:22.930 - 01:47:33.714, Speaker A: So these are the two proofs that we will do here. And for the recombination part, I have to tell you about this extra combinatorial structure that we impose on the canonical doctor plan after we find it, which then allows us to optimize the number of equations and number of variables that you solve at every stage. By stage, I mean you have a parent and you have its children, and you're talking about, how do you solve this? We've already optimized the minimum number of children and so forth, because the doctor plan is optimal and it's canonical and everything. So now we are trying to figure out at that node how to place the children. I mean solve the algebraic system that corresponds to that parent given the solutions for the children. So these are the two pieces of d and r, and we're going to talk about both of them in the upcoming lectures. There's going to be something that I didn't yet say, which is here.
01:47:33.714 - 01:48:25.554, Speaker A: We primarily, I talked about the top down algorithm and we're going to explain that further. But we'll also talk about the bottom up algorithm, which is what the current code, you know, some of the pictures that I showed screenshots of this frontier vertex algorithm code, that's a bottom up algorithm. In fact, while we believe that the bottom up algorithm and the top down algorithm essentially give the same, I mean, sorry, the bottom up algorithm and top down algorithm are based on slightly different definitions of the canonical doctor plan. And we believe that the two definitions are the same, but we haven't proved it. Okay, so one is based on. Well, I won't go into the details. We're almost at the end here.
01:48:25.554 - 01:48:39.544, Speaker A: And I'll just take some questions and then we'll go from there. Any questions?
01:48:44.724 - 01:48:46.404, Speaker C: Hello Mira. I have a question.
01:48:46.564 - 01:48:47.344, Speaker A: Yep.
01:48:48.124 - 01:49:02.536, Speaker C: So in your algorithm you showed in the, in the last minute. So the output is a polynomial system with lower degree than the original polynomial system.
01:49:02.680 - 01:49:04.680, Speaker A: It has both lower degree and fewer.
01:49:04.712 - 01:49:12.856, Speaker C: Equations, but it has the same number of the distribution sets in the real field. It's the same as the original one?
01:49:12.880 - 01:49:29.830, Speaker A: Yeah, yeah, it's identical. So just somehow you're somehow, how shall I say, you can write the same system in different ways. It's just a rewriting, but the original.
01:49:29.862 - 01:49:31.006, Speaker C: One has a higher degree.
01:49:31.070 - 01:49:31.558, Speaker A: Right.
01:49:31.686 - 01:49:33.550, Speaker C: So has some multiplicities.
01:49:33.622 - 01:49:34.234, Speaker A: Right.
01:49:34.894 - 01:49:38.078, Speaker C: You said the original has some degree four polynomials and.
01:49:38.206 - 01:49:38.914, Speaker A: Yeah.
01:49:40.694 - 01:49:42.954, Speaker C: Only degree two at most.
01:49:43.494 - 01:50:20.834, Speaker A: No, it's more than degree two. I think it's, it's lower degree, but it's not two. Let's see. So when we moved beyond triangle, when we moved beyond three decomposable graphs. For three decomposable graphs, you only have to solve the decomposition is about as good as you can possibly get. Right. So if you think of the three decomposable graphs, and where was that share screen? Go to the whiteboard.
01:50:20.834 - 01:50:54.372, Speaker A: When we looked at three decomposable graphs in those decompositions, when you solve at any given stage, you're always solving one quadratic in one variable. So three decomposable graphs are very nice. And it's a real triangularization. It's not a block triangularization. What we get from the doctor plans is a block triangularization. And depending on how. And you cannot guarantee that you get.
01:50:54.372 - 01:51:31.564, Speaker A: Only. You can solve only quadratics. I mean, you're solving eventually a system of quadratics, but then how you solve that block. You know, the recombination that I talked about a moment ago is how do you solve that block? So we can go back here. Maybe I can. It came from moving this thing here. So if you think of this triangular three decompose this.
01:51:31.564 - 01:51:55.664, Speaker A: Maybe I should spend some time next time. So there's a graph picture, there's a doctor plan picture, and there's a polynomial system. What happened to it picture. Right. So in general, you know, you have the original graph, and then you have the doctor plan of the graph.
01:51:56.764 - 01:51:57.544, Speaker C: Yeah.
01:51:58.524 - 01:52:19.168, Speaker A: And then you have the. What happened to the. I mean, corresponding to the doctor plan, the polynomial system. That the way you solve the polynomial system. Right. So you can think of the doctor plan as essentially talking about either. In the case of a triangle, decomposable graph, three decomposable.
01:52:19.168 - 01:52:34.236, Speaker A: Sorry, three decomposable graph. You actually get this. Polynomial systems where each one of these is a quadratic in one variable after you have substituted in one way.
01:52:34.300 - 01:52:34.904, Speaker C: Yeah.
01:52:35.724 - 01:53:11.928, Speaker A: And this is the case for three decomposable graphs. If the graph is three decomposable. If the graph is not three decomposable. We are struggling, but we have managed to now define a doctor plan, but the corresponding thing that it will give you will look like this. So at each stage you're solving still you're solving not just one quadratic in one variable, but many, many equations here. Okay, you can. And these equations are now no longer quadratics.
01:53:11.928 - 01:54:00.294, Speaker A: Like, you can see some of them are quadratics, but then you also have, you know, when you have this situation where you had three rigid objects which are, you know, coincident on these points, and then you have some extra constraints like this, whatever. So these are quadratics, but then they are based on this. But then this thing here, you have this rotation matrix that corresponds to this guy. And you need to use the rotation matrix to get the coordinates of this point. And then when you equate them, you get cubics there. And, you know, it depends on whether you're talking about quaternions this way or how you express this, will change. And so this, in any case, this block that you're trying to solve here may not be quadratics.
01:54:00.294 - 01:54:31.994, Speaker A: Now you can rewrite it, right? So that by rewrite, I mean, you're not changing the solutions, the real solutions. Okay, so the real solutions are exactly the same. And you can do it in multiple ways. And what I was talking about at the end, and that part of it is what we call the recombination part. And this part, you know, you can, how you rewrite makes a big difference.
01:54:33.494 - 01:54:34.274, Speaker C: Yeah.
01:54:34.694 - 01:55:36.278, Speaker A: And that's the part, that's another optimal recombination, which we can also attack in a combinatorial way by imposing another combinatorial object on top of this canonical Dr. Plan. And we'll come to that later. Okay, so, so you have some properties of the graph, you have some properties of the doctor plan, and correspondingly you'll get other property, other combinatorial properties, the actual. So this, this block here, what I have here is in, what I'm talking about in the block is just solving, knowing the solutions of all of these systems in the doctor plan, solving this system, which is the union of the children. Right? Just that part is what I call recombination. At any given stage, you're using the solutions of the children and coming up with the solution of the parent.
01:55:36.278 - 01:56:08.484, Speaker A: So just this one block. So that block optimization of the rewriting of that block is what we call the optimal, I mean, has its own combinatorics that you can optimize, and that's what we're going to. And it has its own underlying matriarchs and stuff, by the way. And we have some couple of papers on that. So I'll talk about all of these, you know, coming up. So this is the good easy picture. Everything is easy in this case.
01:56:08.484 - 01:57:01.720, Speaker A: Coming up with this is easy because, as I said, you know, we'll spend a little more time on three decomposable graphs when we come to the next topic on under constrained or flexible graphs. And how do you find the configuration spaces of them? And there, you know, we'll talk a little more about how quick it is to find a doctor plan of these 3d composable graphs. You know, it's very fast because you can pick finding these three points that you can partition. There are many such three points that you can partition, but the key, beautiful thing is that it really doesn't matter what you pick. At the first star, it has what is called the church rasa property. So it has a very fast algorithm for coming up with a decomposition. And then after you have the decomposition, there's nothing to be done, really.
01:57:01.720 - 01:57:17.164, Speaker A: It just breaks it down for you right away. So, but not every graph, there are very few graphs of this type. So this is just an attempt to generalize and it gives rise to many nice. Interesting combinatorial problems.
01:57:18.644 - 01:57:32.144, Speaker C: So may I ask also a question? Yeah, I wanted to maybe to just make clear, how can we know if a graph is three decomposable?
01:57:32.964 - 01:58:39.464, Speaker A: Oh, yeah, that's what I was just talking about. The recognition of whether a graph is three decomposable is exactly the same, same algorithm as coming up with the 3d composition of the graph. Okay, so there's a. I wasn't going to spend time on that right now because we're going to spend more time on three decomposable graphs later. And another reason I'm not going to spend time on that right now, it's because it's so special that it does not generalize. Okay, so it's a paper by Fudos and Hoffman, I think, where I will try to put the definition where they show that, how to find these three vertices that split the graph, and there are many subsets, many possible triples of such vertices. But they show that it doesn't matter which, you take the beginning, that you greedily choose three.
01:58:39.464 - 01:58:53.584, Speaker A: Right, and, and then you proceed in this fashion, and then you can show that if it is three decomposable, you will find. You'll find that out. And at the same time, you produced the doctor plan for it as well. At the same time.
01:58:54.164 - 01:58:54.948, Speaker C: Perfect.
01:58:55.116 - 01:58:56.356, Speaker A: Does it make sense?
01:58:56.540 - 01:58:57.484, Speaker C: Yeah, yeah.
