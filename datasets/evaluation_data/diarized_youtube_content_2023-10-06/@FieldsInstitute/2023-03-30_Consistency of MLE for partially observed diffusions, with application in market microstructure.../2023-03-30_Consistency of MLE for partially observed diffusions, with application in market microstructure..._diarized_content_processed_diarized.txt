00:00:00.200 - 00:00:23.654, Speaker A: Thank you very much, Sebastian. Thank you all for coming. So, as Sebastian mentioned, this talk will be multifaceted. It's about statistics and finance. And so from time to time, I will play a role of a statistician. I will talk about general theory, and then I'll switch back to finance. So the key thing in this presentation is just not to get lost and sort of switch back and forth.
00:00:23.654 - 00:00:57.642, Speaker A: Okay, so it's a joint work with my student, Johan Yun, and the paper is available on archive. Also, maybe I should mention that the empirical study and all the data used in this work is actually taken from Sebastian's website, where he posts nicely, very nicely. He posted the data for his book on algo trading. So that's the one we gladly used here. Okay, so let me start with the financial part, with financial story behind this investigation. And that's, in fact, how this project was born. Somehow, we had a concrete question in finance, tried to answer that question.
00:00:57.642 - 00:01:30.644, Speaker A: It led to certain estimation inference problem. We realized that there's a general statistical result missing, lacking there. We went back into theory, developed that theory, verified that condition. We developed for that particular model, and then did the numerics. So what is the financial model? It starts with a question. Oop, sorry. Start with a question of what is price? Okay, so imagine a situation where the information that comes to the market happens at a much, much higher frequency than the frequency at which the price is updated.
00:01:30.644 - 00:02:24.470, Speaker A: Now, that has several consequences. One of them is just logically, it's a problem, because we like to think of price as a measure of something that determines consensus between buyers and sellers, between supply and demand. So if supply and demand are updated very frequently, but price doesn't change, that's not really a price, right? This doesn't make sense. It also causes problems when you try to build various models for quantitative trading, and when supply and demand obviously affect the price, you want to find the relationship between the two. But if one, if those guys change and price doesn't, you have to build very awkward, strange models. You can still do it, but it will not be linear regression will not be something classical like if you want to do Paris trading, for example, or any kind of predictive trading where demand buys and sells are supposed to predict the price. Well, if buys and sells happen, but price doesn't change, you know, you cannot do simple ar type of models, regression type of models.
00:02:24.470 - 00:02:54.904, Speaker A: You have to do something weird, or you could change what price is. Right? And so then you are led to the question, what is price at high frequency? In high frequency trading, markets, right? And so people have different notions of price. You can think of best bid, of best ask prices. This is the best price at which you, you can immediately sell, and this is the best price you can immediately buy in the market. You can take an average and think of a mid price. But yeah, they have a problem. They don't change very frequently.
00:02:54.904 - 00:03:25.602, Speaker A: You can think of a last transaction price which changes more frequently, even though still not very frequently, because other things like supply and demand are updated in the high frequency markets much more frequently than transactions actually occur. And I'm going to show you this on the next slide. So, to avoid this price, people introduce something called microprice. So let me explain what this is. So it's really an average between bid and ask. Best bid and best ask. It's a quantity that lies in between.
00:03:25.602 - 00:04:09.876, Speaker A: It's a weighted combination of the two where the weights are determined by the volumes. So this v are the volumes. So this is how many shares are available for you to buy, and VB is how many shares are available for you to sell at the respective prices, B and A. So in the market, I probably should draw this sort of elementary kind of picture. So this is your price. Typically you have two quantities, right, located at b, you have VB. Located at a, you have Va.
00:04:09.876 - 00:04:43.324, Speaker A: Okay? And this is really the number of the total volume of limit orders. Okay, sorry, downside of this thing. So this is the volume of orders available at the best bit. And this is the volume of orders available at best task. So now what happens is the true price is not necessarily in the middle. If these quantities change very often, which they do, I would like my price to reflect those changes. So now if I take, if I define the weights like this, every time VA or VB changes, this quantity will fluctuate.
00:04:43.324 - 00:05:36.208, Speaker A: And so that corresponds to my idea of changing the price whenever supply and demand changes, because these guys represent supply and demand, so they present how much you can buy, how much you can sell, and the quantity that I call microprices in between B and A. And let's see, if B, if VB becomes very large relative to VA, it means that more people want to buy than to sell. There's much more supply on the buy side, which means the true price is probably higher than the mid price because more people want to buy. So if you look at this formula, that's exactly what will happen. This quantity will go closer to a if VB is very large, and the opposite happens if there are more sellers and buyers. Right? So this thing reflects the nature of what price should be. It lies between bid and ask, and it changes at the same frequency at which my orders are updated with supply and demand are updated.
00:05:36.208 - 00:06:03.576, Speaker A: So it kind of fits all the boxes. Let's take a look at how this looks. So in this picture, I take a data from Microsoft stock on a particular day, actually in fact only 1 minute. Right? And in blue. So in orange and green I have bid and ask prices and blue is the midpoint. So I, I mean, of course it's just in the middle between the two. And you see that these things don't change very often.
00:06:03.576 - 00:06:26.420, Speaker A: They stay flat for quite a while. And this marker price that I find in red is in red here, you see it changes quite often. And it also tells you that these guys, these volumes are updated much more frequently than transactions happen or price change. Prices change. Okay, so it's sort of the side that, or the picture here just sort of confirms empirically everything I said on the previous slide.
00:06:26.492 - 00:06:27.224, Speaker B: Okay.
00:06:27.524 - 00:06:32.516, Speaker A: And by the way, if you have any questions, please jump in. Of course. So in a sense, the problem you're.
00:06:32.540 - 00:06:46.504, Speaker B: Trying to solve is finding that information from demand. The VP is a demand price speed, and the agent demands by itself.
00:06:49.084 - 00:06:49.468, Speaker A: To.
00:06:49.516 - 00:06:55.864, Speaker B: Inform you, what do you want to use it for? You want to use it to predict prices or something like this.
00:06:55.904 - 00:07:00.128, Speaker A: I want to create a notion of price that is going to create this notion.
00:07:00.256 - 00:07:01.484, Speaker B: For what purpose?
00:07:03.024 - 00:07:21.564, Speaker A: So the purpose I try to motivate at the beginning. So there are two reasons. First, just purely logical curiosity, scientific curiosity. Price must reflect the consensus between buyers and sellers. So if I want to have more people standing here on the buy side, my price should reflect that consensus. Right. It should move higher.
00:07:25.904 - 00:07:29.128, Speaker B: But the whole video.
00:07:29.296 - 00:07:47.670, Speaker A: Oh, you mean shouldn't I look at all the other ones? Yes, for sure. You're coming to. Very good observation. Let me just clarify it and extend it, because this is exactly what comes next. This definition that I gave is still very, very specific. Right? There's no particular. So it kind of checks all the boxes.
00:07:47.670 - 00:08:02.566, Speaker A: Right. But there are obviously other definitions of price that would also check those boxes. Right. And for example, what Jose is suggesting is why don't I look at all the volumes here and there, right. And include all of them? Yeah. And discounting or not discounting also. Absolutely.
00:08:02.566 - 00:08:23.684, Speaker A: So I just gave an example of what checks those boxes. But it's very important to realize it's an arbitrary choice. This particular definition of price is an arbitrary choice popular among partitioners because it has predictive power, blah, blah, blah. And it's easy to compute, but it's an arbitrary choice. Now, in this talk, I'm not going to make that choice. It's for importance. I'm not going to call this price.
00:08:23.684 - 00:09:07.610, Speaker A: I gave it as an example of what one could define as a price, but I'm not going to do it. I'm not going to choose this one, because what I'm going to do is follow the following methodology. Instead of saying what price is as a function of observed quantities, I'm going to say price is something I don't see, and it should follow certain laws. I'm going to list those laws, justify them, come up with a simplest possible model that fits all those laws, and then try to filter out the price for my observations. So it's exactly, I believe, in spirit, what of addressing the criticism you just mentioned? Right. So why this? Why not something else? Right? Okay, so what are these? So this is exactly what is listed here. So I'm going to list properties.
00:09:07.610 - 00:09:34.228, Speaker A: I'm going to come up with a simple model, estimate parameters on the model, and then filter out the posterior distribution. So I have to say in this presentation I will only go through one to three, not to four. So filtering is still in process. As a part of Johan's thesis, my student, who's a co author on this, but it's not going to be in this presentation. I'll just show you the estimation. But let's start with the properties. First, what I want, based on those discussion we had so far, what do we want from price? First of all, price must lie between best bin and best ask.
00:09:34.228 - 00:10:01.020, Speaker A: That's a natural property. Now, we also want to capture something that is known as an impact. Whenever a buy happens, it should push the price up. Whenever a sell happens, it should push the price down. This is something we call price impact, right? At least on average, this should be true. Now, what I also want is price itself to be a predictor of what happens next to the to the demand and supply. In other words, I want the third property.
00:10:01.020 - 00:10:28.348, Speaker A: Third property is less trivial. Now, if the price moves up within the bid ask spread, right. It should increase the probability that the next order is a buy. So in some sense, buy pushes price up, but the fact that price went up also should increase the probability that the next transaction will happen at this price. Okay, so I claim price should be. There must be consistency. Not only the demand, supply and demand price, but price also produce supply and demand.
00:10:28.348 - 00:10:33.988, Speaker A: So they must be coupled in this way. By the way, the micro price that I showed you before does have that property.
00:10:34.076 - 00:10:40.984, Speaker B: Just to clarify as well, when you say buy and sell in points two and three, you actually mean liquidity.
00:10:42.084 - 00:11:04.284, Speaker A: Yeah, so what I mean, yes, I get this question all the time. So, yeah, so whenever this transaction, somebody buys, somebody sells. But we call it a buy. If it was initiated by a buyer, we call it a sell. It was initiated by a seller. So when, for example, a market order comes in at this place, it will be a buy order, which will eliminate some of the sell orders. But we'll call it a buy order because a buyer initiated it.
00:11:04.284 - 00:11:28.814, Speaker A: Right? So, good point. All right, so now let's give things names, right? Let's try to capture these three properties more mathematically. So why do you know the total order imbalance? This is the difference between total volumes of buy and sell. So I take all the volumes here on this side, minus all the volumes here. This is imbalance between supply and demand. Okay, this is one measure of imbalance. We can come up with others.
00:11:28.814 - 00:12:03.492, Speaker A: And here we being a little bit arbitrary, because at some point we have to use actual data. But if you, for the theoretical purposes, you can just think of why as some measure of the imbalance of between supply and demand, you can do weighted sums, whatever you want. Right? But it's your observation. Now, I claim these three properties lead to the following conclusions. There must be a positive correlation between dx and dy. Indeed, a buy will push y up because it will eliminate some of these orders. And so this size will be larger than this size by more.
00:12:03.492 - 00:12:51.894, Speaker A: Right. So a buy pushes your total order flow up y. So I will refer to y as order flow, but people typically call it total order imbalance. Now, I also claim the combination of two and three will lead to the micro drift of x that pushes it towards a or b. And let me draw a graph here as well, because this will be very helpful. So if I put b here and a here, and this will be the mid price between the two, I claim that there must be a drift or some sort of force, if you wish, right. If you like to think in terms of physical terms, that pushes your minecraft drift away from mid price, which is periodic.
00:12:51.894 - 00:13:38.752, Speaker A: So if you assume that the sort of size of your bid and ask is kind of the same, so size of the spread, so it's the same, then a buy will push the price up. But if the price goes up, the next order is more likely to be a buy, which pushes the price up. Again, what happens is the following. If you find your price above the midpoint, it means the next order is more likely to be a buy, which will push the price further up, but that will even increase further. The probability that next order is buy will push the price further up. So you can start, and you have to just believe me. There was a series of works of myself and my student from Michigan, Roman Guy Duke, where we start with a lot of game theoretic formulations of this concept.
00:13:38.752 - 00:14:15.630, Speaker A: And you get take limits, and in the limit, you get a diffusion, basically for the price, you get a diffusion with this kind of drift. So these two properties give this positive feedback loop, which means that there must be a force that whenever price is above midpoint, it's pushed further up. When it's below, it's pushed further down by just these two principles. I'll skip all those works, all those papers, and just formulate everything in plain English. Right? So I just say I can prove proof that we can go from here to here. Any questions? This is really the most crucial part in terms of model to justify why the model is the way it is. Everything else will be quite standard in terms of modeling, at least.
00:14:15.630 - 00:14:42.268, Speaker A: Okay, so how do we put all this in equations? Right, let's assume that the spread between a and b remains constant. Now, you can ask me, okay, there are many assets for which it's not true, spread between best bid and best ask changes. But there are also many assets for which this is true. So Microsoft is one of them. There is intel. There are plenty, plenty of liquid assets with fairly small prices in us and at least the largest us companies. They actually are stable in terms of spread size.
00:14:42.268 - 00:15:08.604, Speaker A: So if we make that assumption, let's model the price. Again, price is not observed at codex as according to my principles. So, this new is what I drew here. This is my function. Mu, it's going to be periodic by changing between b and a, etcetera. I'm going to assume, by the way, the difference between a and b is always one. So this is just one, just to normalize things.
00:15:08.604 - 00:15:52.912, Speaker A: Mu is this increasing function, which is zero at one, two at the midpoint. And my price is simply a random walk, fastest drift. So the simplest you can do, right, render walk would be a priori model for the price, and I simply add that feature of the drift. Okay, now, what about order flow? Order flow has to be correlated with the price. So I say DX is alpha, dy alpha is the coefficient of price impact, right? Moving the. So moving order flow by one will move the price by alpha, and then it has some sort of independent random walk, in addition times some coefficient. I choose it in such a weird way for convenience, because of the way I will do statistical estimation.
00:15:52.912 - 00:16:13.134, Speaker A: Okay. All right. So I have a model with four parameters, alpha, beta, sigma, sigma, bar. But if you look at this model carefully, you'll see that actually. So, in this model, I have to say, though, x is not observed and y is observed. Okay? Okay. So let's again say a few words about this model.
00:16:13.134 - 00:17:02.432, Speaker A: If I set beta to zero, beta here controls alpha and beta together control the size of this mule. How strong is this drift? So, if you don't like the drift, the data should tell you that beta should be zero and drift is gone. Alpha controls the impact, the price impact. And the sigmas, they control the volatilities. How noisy is your order flow? How noisy is your price pressure? If you take beta to be zero, you end up with a, well, relatively standard model where it's called uncertainty zone model. It was analyzed by several people, in particular, Matthew Rosenbaum, where they looked at exactly the same thing, where x is just random walk without this drift part. Y is around the walk, and the prices are determined as surroundings of x.
00:17:02.432 - 00:17:25.028, Speaker A: So your ask price will be nearest integer above x. B will be nearest integer below x. So this is known. The noity here is the presence of this drift and the justification I just gave you. Now, I also have to say a few words. In this model, it turns out the presence of this drift can explain something called concavity of expected price impact. Something interesting happens, at least it's argued to happen.
00:17:25.028 - 00:18:10.806, Speaker A: Not everybody agrees with this, but some practitioners argue that if you submit a sequence of a long sequence of individual buy or sell orders of the same sign, or buys or sells, and then you measure by how much, on average, you change the price, for some reason, this change will not be a linear function of the volume, but will be concave. Okay? And it's really difficult to understand, because here, everything is linear, right? This is a linear relationship between the two, but somehow, in expectation for large sequences, you get concavity. So this model can explain that. It's another paper that I'm mentioning here, but not presenting here. This model can actually explain why concavity occurs, but it requires a bit of a discussion, so I'm not going to go there. Okay? So. And that's the reason, by the way, I introduced mu.
00:18:10.806 - 00:18:28.984, Speaker A: This drift explains concavity. Without drift, you don't get concavity with the drift. You get it? Okay, so, all right. So this is sort of. I hope I convinced you that this model makes sense to analyze. Now, the question is, what do we do with this model? In this model, I see why. I see a, I see b, I don't see x.
00:18:28.984 - 00:19:04.484, Speaker A: And now, what are the parameters I have to estimate? Well, these four right sigma bar actually is easy, because if you look at it carefully, you'll see that the variation of y is exactly sigma bar. So by just looking at the increments of y estimating quadratic variation, I can easily get my sigma bar out. So I assume it's known. The question is, though, how to estimate the remaining three. Alpha, beta and sigma alpha is the price impact. Alpha times beta is the strength of this drift. And then sigma is the microscopic volatility of the unobserved true price.
00:19:04.484 - 00:19:31.228, Speaker A: And that's where I. Any questions? Yeah. If you read this backwards, right, you can say d x t is alpha times dytom. Right. It tells you that if I buy one share, one share of asset, buying one share will eliminate, will reduce this quantity by one. Right. So effectively, y will increase by one.
00:19:31.228 - 00:19:47.052, Speaker A: Right. I will move my price by alpha dollars or alpha cents in this case. Okay, good. Okay. Any other questions? Because now I'm going to take off my financial hat and put on my mathematicians the decision head. Okay? Yes.
00:19:47.108 - 00:19:56.596, Speaker B: Particular choice of mu. This is one form that has this periodic structure at zero at x equals, right?
00:19:56.700 - 00:20:00.064, Speaker A: Right, yeah. Zero at midpoint.
00:20:01.644 - 00:20:05.316, Speaker B: Is the analysis going to be contingent on that specific form?
00:20:05.460 - 00:20:17.916, Speaker A: So, right. So, good point. So what am I. So, I'm giving you this particular choice of mug. So all the theory I explained holds for any mu. That is, somehow you need to have some symmetry, just for logical reasons. Right.
00:20:17.916 - 00:20:42.594, Speaker A: It has to be zero here. Yeah, but we have to be concrete. When we use data, use something just like we chose y, we have to choose concrete mu. It's true that the verification. So I'm going to derive the general result now and the verification of that result, and the numerical analysis is done for this particular mud. I see no reason why it wouldn't work for other mus. But we haven't really proven it.
00:20:42.594 - 00:21:03.342, Speaker A: Yeah, we work with it just for concreteness. But in principle, yes, you should be able to choose other mus, two. In a non parametric form, very hard. In a fully non parametric form, very hard. Yeah. So we choose that. But what we do is we at least say, okay, the magnitude of mu, we should be able to estimate.
00:21:03.342 - 00:21:05.414, Speaker A: Right. How strong that thing is. Right.
00:21:05.454 - 00:21:08.782, Speaker B: Because you observe y, mu is partially observed.
00:21:08.798 - 00:21:11.514, Speaker A: It's hidden because of the Brownian. Yeah.
00:21:13.494 - 00:21:14.582, Speaker B: That's the filtering thing.
00:21:14.638 - 00:21:33.084, Speaker A: Exactly. Exactly. Right. Exactly. So if at the very least, you to have a whole four twice so.
00:21:33.124 - 00:21:38.424, Speaker B: You can identify the alpha, beta mu, beta mu, we don't even care.
00:21:40.164 - 00:21:41.868, Speaker A: Mu is fixed. I'm not estimating.
00:21:41.916 - 00:21:45.344, Speaker B: Yeah, beta mu, you can't identify something with beta mu.
00:21:46.244 - 00:22:08.480, Speaker A: Yeah, you're right. Right. But you could come up with different parameters, I guess what Sebastian saying. I could come up with a different parametric, or even non parametric form of beta times mu and say it's just anything. Right? That satisfies some monotonicity properties and so on. And that to be a non parametric question, which I don't even know how to approach. But even if you just choose a different parametric form, I mean, your problem may or may not end up being identifiable.
00:22:08.480 - 00:22:39.748, Speaker A: By the way, this is a good start of the discussion that follows. Before we even do estimation, how do we even know if we are able to actually estimate our parameters from known data? Because the data is partial. I don't see x. If I saw x, I could do many things, but I don't see x, so I only see y. And this weird function also x. So how do I even know that I didn't put too many parameters in my model? Maybe they are redundant in the sense that from my observation, there's just no way. Even if I observe my system for infinite time, there's no way to actually distinguish those parameters.
00:22:39.748 - 00:23:15.874, Speaker A: Right. And that comes from a question of identifiability, which, in essence, is, in fact, equivalent to consistency of maximum likelihood estimator, which is known to be the best estimator asymptotically. So that brings me back to statistics, or forward maybe to statistics. Okay, so let me kind of forget about my previous problem for a while and just think of this as a. As a related problem, but much more general one. So let me look at more general problem, where I have two diffusion process, x and y. They could be multidimensional.
00:23:15.874 - 00:23:37.086, Speaker A: X is a signal which is not observed. It's a diffusion. Y is another diffusion, which is observed. And because I can always estimate covariance function here, I just assume it's identity. So, all right, theta is the unobserved parameter, which affects all the coefficients here. So I see y, and I try to estimate theta. It's very similar.
00:23:37.086 - 00:23:44.334, Speaker A: It's not exactly the same as the problem we had on previous slide because there are these roundings that we also see. Okay, but it's very much related, right?
00:23:44.414 - 00:23:46.862, Speaker B: Is it w w field correlated, or.
00:23:47.038 - 00:23:56.284, Speaker A: Yeah, good question. We can make them correlated if necessary. But for simplicity, let's think they're not. If it makes it easier. It's not. In fact. Yeah.
00:23:56.284 - 00:23:58.156, Speaker A: You have to take care of that a little bit.
00:23:58.180 - 00:24:01.104, Speaker B: But since you're not observing x.
00:24:03.324 - 00:24:04.012, Speaker A: Yeah.
00:24:04.148 - 00:24:11.064, Speaker B: So make it uncorrelated for it. You have to fix it.
00:24:11.364 - 00:24:40.074, Speaker A: No. So for what I will present here, it doesn't matter. They could be correlated. Yeah. For what I'll present here, when actual computations happen, it does affect your filtering problems and so on. Yeah, yeah. What are b and sigma? So, okay, say again? If what if we can find theta that we know b and sigma, b and sigma given something.
00:24:40.074 - 00:25:01.240, Speaker A: Yeah, yeah. Let me explain. So we don't observe the path of x, but the model is known to us. Right. So we know functional relationship b, sigma and h, right? Yeah. But when in our estimation problem, we're not allowed to use information coming from observations of x, only from those of y. Right.
00:25:01.240 - 00:25:12.768, Speaker A: It's called a partial observed diffusion model. It's quite classical. It's been around for a long time. If you know the parameter theta, you can filter out x. This is called filtering problem. Right. But we're not solving filtering problem even though the two are related.
00:25:12.768 - 00:25:35.124, Speaker A: We're actually estimating parameters in the, in this filtering problem. Right. So first thing you can do, and this is also classical, you can actually project everything on the filtration of y. You can actually find another process, h, another brownian motion, such that in fact, y can be written like this, where everything is adapted to y. So h and w adopted to y. So kind of project x out. Everything is on y filtration.
00:25:35.124 - 00:26:04.714, Speaker A: This is well known classical result. Now, if you think of y like this, it's actually a brown emotion with a drift, the law of this process. Now remember, now, what is likelihood? Let me remind you, likelihood is the density of your observation with respect to some reference measure. When you observe finite number, when you make finite number of observations, typically a reference measure is just a product of low back measures. So we write actual density and products of densities for IAD observations. But now we deal with processes. So observation is a path of process.
00:26:04.714 - 00:26:36.260, Speaker A: When we write down its likelihood, we have to find the reference measure, which is non trivial because think of infinite dimensional space. Now, but this particular process is just a brownian motion plus drift. So by Gearsheim theorem, its law is actually absolutely continuous with respect to winter measure, and its density is given by this expression. And what's nice in this expression is that even though it's complicated, everything here is y measurable. So you can think of h as a function of y, h, theta, and everything really is a function of y. So it's really think of as just a function of a path of y, right. It's a complicated function within, it's a function.
00:26:36.260 - 00:27:08.910, Speaker A: So you see a path away, you plug it in, and now it only depends on theta. That's why I think of this as a function of theta. And that's likelihood in this case. Okay, now maximum likelihood is a maximizer of this function. And so this is again standard, pretty much. But what is not standard? It turns out in the setting where you make only partial observations, because this h, theta becomes very implicit. Basically, it's a conditional expectation for which there's no formula, right? You have a very, like, you don't have an explicit question for l.
00:27:08.910 - 00:28:08.820, Speaker A: And it turns out if you look at existing theory, there's no sufficient, tractable, sufficient condition that will tell you that maximum likelihood actually is a good estimator. We expect it to be not only convergent to the true parameter as t goes to infinity, but in fact converging at the fastest possible rate, being asymptotically efficient. But there's no actual proof of this in general. So every time you have a more complex, more abstract setting, you have to prove it again. And the proofs, well, they based on some assumptions, because you can easily imagine situations where it will not be true. For example, if nothing actually depends on theta, obviously you cannot estimate theta from something that doesn't depend on theta, right? So there must be conditions and non trivial conditions that guarantee that MLE actually converges, where it should converge to the true parameter, right? And the question is to find those structural conditions. So abstract conditions exist, but they are stated in a very complicated way, through properties of the invariant measure of, through the invariant measure of a conditional distribution of x given fy.
00:28:08.820 - 00:28:36.358, Speaker A: So very abstract things. You know, you write down some conditions and then you look at this, you don't even know where to start, where to start verifying them. So we had to rederive conditions, get a different, more trackable condition, and use it for our model to check that the moe actually is consistent. Okay, so let's see how this is done. So I'm not going to say how fast here, I'm just going to say whether it will converge or not. How fast is a much more difficult question for the future. All right, so this is the theorem.
00:28:36.358 - 00:28:57.500, Speaker A: So again, this is a partially observed system. X is signal, y is the observation. Here's the result. We make a crucial assumption which infuriates statisticians as far as I understand now, so that the set of unknown parameters is finite. Okay, so that's just the nature of our proof. We have to assume it and we are working on extending it. We are actually, actually working on it on Friday.
00:28:57.500 - 00:29:18.664, Speaker A: I'll be working on it. So it's, so, so it is ongoing. But the proof. Yeah, yeah. Something in fact, for practical purposes, I agree. Right. Machine precision limits everything to finite grid anyway, right? But for theoretical.
00:29:18.664 - 00:29:48.842, Speaker A: Yeah, maybe, maybe that's the reason. But anyway, so under this assumption, we pretty much can find a tractable condition which we can successfully verify. In our model. We assume that something is bounded. And we assume that you can generate something out of your y without changing its information, but making it more useful. So to just tell you what this y is, it's just this. We basically take a exponentially weighted average of the increments of y.
00:29:48.842 - 00:30:32.798, Speaker A: Why do we do this one? One simple reason is because that makes your y ergotic, okay? You add a drift to y, which pushes it back somewhere, which makes a long term behavior of a meaningful, meaningful object. So in the limit, you get some sort of a limiting distribution, time average conversion, you get ergodicity out of this, okay? But you don't change, you don't change information because. Because, you see, y lambda is driven by y and you can express y through y lambda and vice versa. So you're not changing information. So if you observe y, you also observe y lambda and vice versa. Now let's assume you have. So what are the assumptions here? There exists such a family, and we actually use this family in our verification such that h of x and the y lambda are strongly ergodic.
00:30:32.798 - 00:31:06.564, Speaker A: Now, because h if h is bounded, for h of x to be ergodic, it doesn't take much. Okay? But of course, you have to make some assumptions on x itself. But why? As I explained, you make ergodic. So in a sense, this is not really, it's a mild assumption. We denote its invariant measure by nu and we ask the following. Separation property or identifiability property, if you wish. If I integrate Z with respect to nu, then this, as a measure of y, can only coincide with the nu at theta prime integrated over z.
00:31:06.564 - 00:31:30.094, Speaker A: If theta and theta prime coincide. So just like saying this sort of z times nu integrated over z as a measure, if it coincides with different thetas, then thetas must coincide as well. It's an aversion of identifiability. It's not the same as nu being the same for different thetas. It's a. We, it's slightly, slightly, yeah, it's slightly weaker. I don't know.
00:31:30.094 - 00:32:00.534, Speaker A: Okay. But on this assumption, basically MLE converges to the true theta. Now what I want to emphasize here is that the condition has stated in terms of the stationary measure, or invariant measure of the fully observed system. So I pretend actually no x and y. Why is it good? Because for x and y, I can write down pde for this new, I can do a lot of things. I can have a tractable characterization of this new. I can approximate numerically alternative, existing, abstract existing assumptions which guarantee consistency for partial MLE.
00:32:00.534 - 00:32:31.360, Speaker A: For partial observed system involve stationary measure of partially observed system, where again, you involve measure value process itself and look at its stationary measure, which is a measure of an infinite dimensional space. So much more difficult to characterize. Right? So the benefit here is that we don't do that. We say something about partial observed system through fully observed system. Okay, good. Now how do we prove this? I think I have enough time to go through the proof, at least quickly. Okay, so here's the likelihood here.
00:32:31.360 - 00:33:34.000, Speaker A: Here's the definition of h theta as a projection. Here's definition of MLE. Now let's take two different thetas, theta and theta prime. And it turns out that if you take a ratio of likelihoods, it's not difficult to see it's actually martingale because it's actually a radonicadium density of p theta prime versus respect to p theta. So you have this expression where delta theta is the difference of two h's. Okay? And now it's easy to see that if I want to show that my, let's think of theta as a correct one, as a true one, right? And theta prime as alternative theta. If I show that this ratio goes to zero, then I show that ultimately, for large enough t's, the correct theta will have higher likelihood than any alternative theta, theta prime, right? If this ratio goes to zero, it means that denominator becomes larger than the numerator, right? So then if I ma, if I maximize my likelihood over all thetas, I will find the correct one because it will be the largest one.
00:33:34.000 - 00:33:58.764, Speaker A: Okay? It's true, at least if your state is finite. If your set of possible thetas is finite because you do it two by two, if it's not finite, then you need additional regularity, sort of uniform continuity or something, which is much more difficult technically. Okay? But for finite number, it's true. So it suffices to prove that some martingale goes to zero. Now back to stochastic. Analysis, right? So if you look at stochastic analysis, you say, oh, you have exponential martingale, you have to prove it goes to zero. Amazing.
00:33:58.764 - 00:34:27.013, Speaker A: This is what we know how to do. So how is it done here? So what one can easily do is of course show that because h is bounded, that this integral is capital o, square root of t. So for large t's, it behaves like square root of t. This guy is o of capital t. So it should be, right. This guy, like the second one, should be the first one over large t's. And why is it good for you? Because it means this negative term will dominate and what's inside the exponential will go to minus infinity.
00:34:27.013 - 00:35:18.148, Speaker A: So exponential will go to zero, right? Easy. The problem is though of course it may behave less, it may behave, may grow slower than actual square root, actual t, right? So if you can show that this, the lim super of this average goes to strictly positive number, then indeed this term dominates, right? So it's enough now for you to prove this, that with probability one, this is true. Okay, now it's not straightforward to ensure the above, there's a paper by Chigansky which gives abstract sufficient conditions for this. But again, in terms of stationary measure of the filter of this measure of valid process, which is impossible to verify. So I claim for finite set of parameter values, I can do it by contradiction without using any of those complicated assumptions. It's very simple. So this is just a reminder.
00:35:18.148 - 00:35:56.618, Speaker A: What is my martingale? What is my age theta, what is my delta? And what is what we would need to do? We would like to do this. Okay, like to prove this. Okay, let's argue by contradiction. Let's assume there's a set, a positive measure such that, such that, well, let's assume that for this a, its probability is positive. Let's put this way, right? So with some epsilon, the limit of m is above epsilon with positive probability. Then on a, I must have this equality. Why? Because if I don't have this equality on a right, then this limit is strictly positive.
00:35:56.618 - 00:36:29.334, Speaker A: And then my martingale goes to zero, which is contradiction of a, right. So I must have this. Now what is this? It's a limit of some l, two norm of something l to norm goes to zero, which means that a scalar product of my, of my vector with anything else also goes to zero. So let me choose convenient test function here. And from here I can easily deduce that this is true. Where I chose as a test vector, this f of y lambda times indicator of at where at is the event that mt is above epsilon. So we have this now.
00:36:29.334 - 00:36:52.412, Speaker A: Okay, good. Now let me again summarize. This is by definition of mix. This is my h, my delta H. Now this is what we have established. This goes to zero. Now what I can do now is I can take this expectation and expand it, okay? I can expand delta H into the difference and I can change my measures.
00:36:52.412 - 00:37:41.870, Speaker A: So here the expectation is taken under the true measure p theta. And I can actually introduce the change of measure m is a change of measure that goes from theta to theta prime and change my expectation from theta to theta prime. Why is it useful? Because then I can recall the definition of h, right? And it's, you see, for h, theta prime is defined through a conditional expectation under theta prime. And if I change it here, then I can use power property and get rid of conditioning. So now I have these two quantities and I know that they somehow, after this averaging, converge to zero. Okay, good. These quantities are not exactly equal to zero, by the way, you could say, oh, I can now remove m inverse and go from theta prime to theta.
00:37:41.870 - 00:38:11.930, Speaker A: But you cannot do it because m is only a change of measure on fy. And here's x present here. Okay? So in fact, that's exactly the reason why the whole thing works. So maybe a little quick. But let me then say what, okay, so we have shown this, that the limits of the two coincide or the limit of the difference goes to zero. Now you can choose your epsilon carefully so that the indicators actually converge as well as t goes to infinity. I mean, indicator is not a continuous function.
00:38:11.930 - 00:38:56.420, Speaker A: So you say, well, how come? But you can choose, you can do it. It's a simple measure, theoretic exercise. Now what I can also notice is that if I look inside these quantities, right? Let me take a look at this second one first, right? Because indicator goes to indicator of a, I can kind of in the limit, replace it by indicator of a and take it outside, right? Inside, I have a time average of f times age, right? So it's this guy. After I remove the indicator, I get this left hand side here. But because of ergodicity, this will converge to integral of my. So this h becomes z, right? This is the value of my process, h of x and this f of y. And I integrate respect to invariant measure.
00:38:56.420 - 00:39:31.554, Speaker A: So by the definition of strong ergodicity, time averages converge to integrals with respect to invariant measure. So I get this, okay? And so that means, but don't forget about the indicator of a. It still remains. So when I take expectation, I get this is deterministic, it comes out of expectation and I guess just expectation of the indicator of a under theta. So I get this. By analogous argument, I analyze the left hand side and I obtain this. So, which is exactly the same, except that the invariant measure is under theta prime.
00:39:31.554 - 00:40:18.834, Speaker A: So then from the fact that the two limits are the same, what you get immediately is that these guys are equal. But if you remember, that's exactly the condition of my theorem. Equality of these guys means equalities of theta. So it's a contradiction to my initial assumption that thetas were different. Okay, so it's a kind of convoluted, but, and I know that you probably cannot appreciate, comprehend all the details in such a quick presentation, but the main point, I guess I want to make is that actually, it's a very nice, like, very clean proof that you can actually present even, you know, within ten minutes. It's not that difficult, right? So given enough time, several reads, it's actually not, you know, nothing complicated, right? So, okay, let's just leave it there. Okay, so, good.
00:40:18.834 - 00:40:48.794, Speaker A: So we have this proof with this condition, right? Of course, verifying this condition, right, saying that if this is true for all f and all lambda, therefore, theta equals theta prime, is not an easy job. It's a doable job, unlike existing other conditions where you have to deal with abstract measures on spaces of measures. But it's still not easy. Nevertheless, it's done for our concrete model. We did this for our concrete model. And let me show you how this is done. So let's go back to finance now.
00:40:48.794 - 00:41:27.002, Speaker A: Let's go back to this model that I described, this latent price model, and let's see how we can use those results here. So this is our price, unobserved price. This is our order flow, and this is our unknown theta. And the additional observations we have are a and b. So the first thing I want to do is, I want to notice that, okay, these kind of observations are really outside of the framework that we considered in previous slides, right? Because this is not just some other diffusion which is correlated with my diffusion. These are strange kind of singular functions of observation. So what I want to do is I want to use information contained in a and b separately to reduce the dimension of my theta.
00:41:27.002 - 00:41:59.370, Speaker A: And then I will solve the estimation of remaining parameters by looking at x and y. So it's a two step procedure. I first use information containing beta mask spread to get one equation essentially for my theta, which reduces number of parameters. And then I use, I look at x and y and use mle to estimate the rest. How is this done? It turns out that you can define the so called macroscopic volatility. This little sigma here is a microscopic volatility of the unobserved price. But if you look at the mid price and define its variation, let's say.
00:41:59.370 - 00:42:34.678, Speaker A: So you look at increments of this price of large size, square root of t. You take square root of t increments, so total use t of data and send t to infinity. This thing will actually converge to a quantity I call capital sigma theta, which is a function of theta. So it's kind of funny, right? The actual volatility of unobserved price is different from the volatility that you will obtain if you just measure volatility of your mid price. It's a very interesting phenomenon. Microscopic thing is not the same as microscopic thing because of the drift. And, okay, if, by the way, if alpha beta is zero, then you get the same exact quantity.
00:42:34.678 - 00:42:58.114, Speaker A: So capital sigma equals little sigma. But, you know, this thing I can measure, okay, because p is observed. So what I'm going to do is I'm going to equate my estimate of sigma hat. I'm assuming I know it and I'm assuming it's correct. So get an equation of this function. This function of theta is known. Well, of course, I use this functions phi and gamma.
00:42:58.114 - 00:43:30.936, Speaker A: Okay, so they are just explicit pretty much functions. Okay, so it gives me an equation. Now, how do I use this information? I reduce the number of known parameters by matching. This is something I observe. This is this function. So, I take this equation and I prove the following. That if I look at the reduced mle, in other words, I take arg max of my likelihood over only those thetas, which kind of are the closest to a fixed s.
00:43:30.936 - 00:44:18.670, Speaker A: Okay, so I pretend I know the value of capital sigma, right? I quote s, and I only look at those thetas which produce the values of capital sigma the closest to what I believe is true. So it reduces my overall number of parameters, and I take mle over this reduced set. And now, instead of s, I plug in my estimator sigma. Hat. Then, if my initial mle, if I knew my sigma, my sigma precisely, and my estimator was consistent, then replacing it by an estimator will still give me a consistent estimator. Okay, so I proved this theorem, okay? So if you can pretend you know this capital sigma, or you can replace it by a consistent estimator, you get consistency as long as in this setting, when you pretend you know true sigma, you get consistency. So it's enough now to prove this part.
00:44:18.670 - 00:44:54.612, Speaker A: Okay, consistency of this reduced maximum likelihood, where you estimate over a smaller set of parameters. All right, how is this done? Okay, so this is just a reminder of the model. This is x and y. This is the integrated y, the y lambda, which is the average, your parameter. This elementary functions phi and psi. And we have to make this strange assumption. Okay, so I couldn't figure out, I mean, this is just a shame, right? We have to assume that this mapping from z to this kind of expression, which uses this phi and psi, is invertible.
00:44:54.612 - 00:45:05.556, Speaker A: Okay, now it's monotone. Every time you try to numerically plot it, it's monotone. You do Taylor expansion up to the order ten. It's. It's monotone. Right? But I cannot prove it's monotone. So we have to just assume it's monotone.
00:45:05.556 - 00:45:38.806, Speaker A: It's an elementary function for which we have to assume that something, some property, holds true. But of course, if your theta is actually finite, you can actually numerically check that this monotone on that finite set. Then this theorem says that actually this equality, the separating property, is satisfied. So you can choose test functions to be just equal to y. Okay, it's enough to choose f of y equal y. And this equality for all lambdas will imply that theta equals theta prime. So that separating property that was assumed in the main theoretical result holds for this model.
00:45:38.806 - 00:46:20.894, Speaker A: Okay, that's what it says. The way it's done. It's actually, this is actually, most of the technical work of the paper is proving this result, not the theoretical result I showed you at the beginning, which is quite easy, but actually this one, because checking the condition exactly, it's pro. Yeah, because, well, you have to show that somehow, you know, this is true. It turns out what you do is you represent these quantities on the left and on the right as the limits of time averages of the functions of your process, x and y. And then you, you look at lambda going to zero into infinity and somehow interchange the limits, because then you get explicit expressions on both sides. And then you can actually argue that the only way that this could hold for lambda equals zero and infinity is if theta equals theta prime.
00:46:20.894 - 00:46:39.728, Speaker A: And this function obviously shows up at some point. All right, so that's something I skip. It's a technical part, but in fact, most of the effort is there. All right, so, numerical results. What do we get now? Computing the likelihood. So this is your model again. This is the likelihood.
00:46:39.728 - 00:47:07.892, Speaker A: In this particular case, some constants appear because you have the volatility is not one here. It's not identity. What is mu theta? Mu theta is exactly what I call h theta, because beta mu is your age. So mu theta is the conditional expectation of projection of mu of x on fy, and it's an integral of mu with respect to posterior distribution. So this PI is the posterior distribution of x. It's your best prediction for the distribution of x given the information contained in y. Right.
00:47:07.892 - 00:47:30.754, Speaker A: So that's the definition. And for this PI, of course, if we were to actually use it, we have to compute, literally, to compute l, I have to compute mu. To compute mu, I have to compute PI. Good news. For PI, there is an equation, it has to satisfy, which is called Zakai equation, by the way. So, okay, so I have to correct myself a little bit. PI is a normalized version of a solution to the chi equation.
00:47:30.754 - 00:48:00.404, Speaker A: Zakai equation is from unnormalized density. So for this u, basically there's this equation which has this heat type, heat equation type in the dt, right. Second differential in x, and this linear in u term in dynamic, which in principle is complicated. But in dimension one, remember, x and y have dimension one. Actually, you can solve it by splitting. The splitting scheme that we use is, first, you pretend there is no dy component and you solve this equation. It's just a heat equation.
00:48:00.404 - 00:48:23.618, Speaker A: You can do that. Then you pretend there is no dt term, and then you solve this part and you evolve your equation u with two steps every time. Right. This is well known splitting scheme. What's nice is about the second part is that it's actually a simple linear equation used. So the solution will be just stochastic exponential. So if you pretend there is no dt, a solution to this equation is just u, explicit.
00:48:23.618 - 00:48:50.954, Speaker A: Write down the stochastic exponential. So it's very nice, actually, to do it numerically. And what do we get? So, I'm going to give you numerical results, this first slide, maybe not the most interesting one. Let me say a few words about data, I guess. So, we look at three tickers here. These are the tickers that have constant spread, spread of size, $$0.01. It's taken over a course of a month because that's the data we have from Sebastian.
00:48:50.954 - 00:49:14.122, Speaker A: First, I estimate the quantities, which are easy. Remember, sigma bar is the volatility of the order flow of supply demand. I observe it, so I just construct the estimate. I see it's more or less stable, but there are spikes, occasionally different days. So this horizontal axis here is days. Okay, I have 20 days in total. And these are tickers.
00:49:14.122 - 00:49:40.918, Speaker A: So this is MSFT or intel. Sorry, this is Microsoft. And this is this ETF mu. And so this is Sigma hat, which is a macroscopic volatility of your assets. Now, this is the magical part. Okay, so after all these approximations, computing this, solving Zakai equation, plugging, computing the smew, plugging it into likelihood, right. You might get some really crazy pictures you might expect.
00:49:40.918 - 00:50:10.462, Speaker A: Okay, I'm going to optimize now over theta, right, something that computer from data, how ugly is it going to be? I was very happy to see that. Actually, it's not that ugly. I don't know what your opinion is on these graphs, but the good news is that. So, first of all, what are these graphs are? So they're cross sectional graphs. In general, I have three parameters. But remember, I can estimate macroscopic volatility, which reduces parameter by one. So effectively, now we have a function of two variables, likelihood, let's call them alpha squared and sigma square.
00:50:10.462 - 00:50:40.754, Speaker A: This is the square of the price, impact and square of the microscopic volatility. Now, I fix here sigma squared and go over alpha squared. So all these curves, each color corresponds to one particular value of sigma fixed. And I change over alpha. And I look at my function over all alphas for each sigma squared fixed. Here, I do the opposite, and I do it for intel, Microsoft, and this ETF. And what's nice is that in most cases, you see pretty clear location of the maximum.
00:50:40.754 - 00:51:05.736, Speaker A: Right here it's not so clear, but here it's better. So maxim is actually well defined here. It's not some sort of crazy thing that kind of jumps and you have maximum here, and then it appears all the way at the end, at a very, very close location, at a very close value. Right? So actually, maximizing this function is a meaningful process, of course, because also we have a very small dimension here. It's only two parameters. Maximization is done by exhaustive search. We basically fix a grid.
00:51:05.736 - 00:51:27.954, Speaker A: We work on the grid on a finite capital theta, and we just search for the maximum. Okay? And you get the results. So this is how the optimal price impact moves. There are some spikes. These are exactly spikes on which other parameters also spike. So it's kind of nice to know that it spikes not because your likelihood is somehow unstable, but because parameters really change. This, this sigma's change.
00:51:27.954 - 00:52:03.510, Speaker A: And so this is for optimal beta. This is the MLE for beta. And this is MLE for sigma squared. And what's interesting again. So I should comment a little bit. So, once you translate these numbers back because the units of measurement here are a little bit off, right, a little bit strange prices and cents and volume is in shares but kind of after normalization actually get meaningful value for alpha. They kind of in the same range as you would get if you just take a regression of your mid price changes on your volume, changes on the order flow.
00:52:03.510 - 00:52:29.142, Speaker A: So it makes sense. And so here also if you plot, I don't have it here, but if you take this value of beta and you plot the result in mu, you will see that mu actually is not close to zero. It has a meaningful value, meaningful impact on your market price which kind of sells the data actually supports the idea of having this drift. At least it doesn't tell you it should not be there. It should be eliminated. Okay. And I think that's the last slide I have.
00:52:29.142 - 00:52:31.014, Speaker A: Sorry if I went over time. Thank you.
