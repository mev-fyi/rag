00:00:00.520 - 00:00:39.184, Speaker A: So we shall discuss Alan Kahn's own contribution to this particular genre as time goes on over the next two lectures. Yeah, he writes about this. He. Alenkan writes about this in his book. And of course, he has to be a little cagey about how he does it in view of this sentiment. So when you read what's in his book, it's very offhand. He doesn't want to make a big deal of it.
00:00:39.184 - 00:01:09.424, Speaker A: He doesn't want to announce it with trumpets. He just does a certain calculation. He says, I'm only showing you this for a certain narrow purpose. I'm not really saying anything. I'm only showing you this because I wanted to make one narrow point, which I'll make on his behalf today or on Thursday. Okay, so this is our goal this week. It's index theorem week here in Toronto.
00:01:09.424 - 00:01:33.364, Speaker A: Oh, by the way, everything is good with the Internet out there, apparently. Nobody's complaining. I can't hear you. Well, thumbs up. Okay, great. Or maybe it's just sometimes when you do this to your computer, it automatically sticks up a thumbs. Mine does.
00:01:33.364 - 00:02:26.750, Speaker A: We're living in an incredible modern age where you just do that to your iPad and it goes right back. Glad you're having a good day. Okay. Oh, yeah. One more announcement. So next week is for this class, spring break. And so there will be no classes next week.
00:02:26.750 - 00:02:57.334, Speaker A: I don't know how you're going to do it, but you're going to have to find another way to amuse yourself. Next week. I don't know. Yeah, you can all follow down to the. To the clubs next week. Okay, good. I was actually.
00:02:57.334 - 00:03:15.944, Speaker A: There are some rules, but I was allowed to have a week of holiday, full reading week or something. Yeah. So that's what it is. Next week. I'm actually. I do have permission to not teach next week. Good.
00:03:15.944 - 00:04:20.502, Speaker A: We'll not get to index theory immediately, not in the first half of today's class. And we won't reach the grand conclusion, the 103rd presentation of proof of the index theorem, until Thursday. But we'll make some headway in the direction of the index theorem today. Oh, and by the way, I've spent my entire life, I should say, by the way, I've given several of these 100 talks, so I should confess to that right away. But I've also spent my entire life trying to teach the index theorem to people, and I've concluded it's not possible to do it in one semester, let alone in two lectures. So we're going to just catch a glimpse of index theory over the course of this week, I wouldn't say that we're going to dot all of the I's and cross all of the t's, as they say. Do they say that in French? Dot the I's, cross the t's.
00:04:20.502 - 00:04:45.344, Speaker A: It means take care of all of the details. No, it's not an expression. German? Canadian. Do they do it in. Okay, good. All right, good. So I want to go back, to begin with, I want to go back to where we were on Thursday.
00:04:45.344 - 00:05:30.472, Speaker A: And put what we did in a slightly different context. And talk just very briefly about continuous fields of sister algebra. What we did last time on Thursday is most of what was required to write down a certain, well, short exact sequence of sea star algebras. And the rest of what is required is in the notes. And I labored over this. And after I labored over it, I decided I didn't like what I did. But I believe it's mathematically correct.
00:05:30.472 - 00:06:32.056, Speaker A: However, with Chi chi and ed, we thought of a better way of doing it. So I'll probably edit the notes at some point. But. But I want to just dwell because it's relevant to index theory on this thing we made a certain funny. It's not going to fit there, is it? Short, exact sequence of sea star algebras. I shall now attempt to write down in all of its full glory. So this thing is what we were mainly talking about last time.
00:06:32.056 - 00:06:56.504, Speaker A: The sister algebra of the tangent groupoid. It's built using some strange convolution multiplications. It calls to mind the way you build the convolution algebra. The sister algebra of a group. But it's not exactly that. And by the way, in the homework I go into the last lecture a little more detail about the extent to which this is not exactly that. Which is to say the sister algebra of a group.
00:06:56.504 - 00:07:20.754, Speaker A: Some funny. We had a discussion about modular functions and a confusing issue. I believe I straightened it all out in the notes for your benefit. Because it's just the kind of person that I am ever ready to serve my students. Right. Yeah. I don't know how you could answer that with such a straight face.
00:07:20.754 - 00:07:38.570, Speaker A: Have a career in acting. Probably make a lot more money there. Aha. Okay. Okay. Here I didn't quite finish. Oops.
00:07:38.570 - 00:08:31.256, Speaker A: The tangent bundle of a manifold is a bunch of vector spaces. And each vector space is of course, an abelian group. And those abelian groups have their own convolution algebras. And you can fit them all together to build one c star convolution algebra of the tangent bundle, that's what appears there. And then zero. And what you have to labor over, which I did labor over, not so much in the class, but in real life, in the notes. If you're dotting those famous I's and crossing those t's, you have to worry about exactness in the middle, which is odd, because, you know, in the rest of mathematics, you're taught that that's never where a problem with exactness arises.
00:08:31.256 - 00:09:51.282, Speaker A: It always exact. The problem enters one end or the other. Not so here it enters right in the middle. I don't know what that means philosophically, but that's the way it is. So what's the story here? Well, we built, we came up with this sequence by studying a bunch of representations. For every point in the tangent groupoid, we built a representation, excuse me, for every point in the object space of the tangent groupoid, for every point in the manifold and every real number, we built a representation of this c star algebra, the one in the middle. And where the representation ends is actually in compact operators in l two of m.
00:09:51.282 - 00:11:00.874, Speaker A: This is what happens if t is not zero. And in this case, something funny happens, which is that this is actually independent of m. There's only one of these. And so let's just call it lambda t, like that. And then there was special ones that you get when t is zero, and they're valued in the sea star algebras, like I was telling you a moment ago, of the fibers of the tangent bundle. And these guys do depend on m, because, well, if you change the point little m, you end up in a different convolution algebra with a different fiber. But you can combine them, and after you've combined them, you have a single representation, which is the thing that appears up here.
00:11:00.874 - 00:12:09.710, Speaker A: Sorry for squishing this in hope. That's sort of legible. And the ideal that you see above in the short exact sequence is all of the elements in the seaster algebra which go to zero under all of these regular representations here. And it happens to be the case that once you go to zero at the end, what you have can be identified via these homomorphisms with continuous functions into the compact operators which vanish at infinity. That's the critical point. It's not obvious at all that something in the kernel of the map lambda zero is a family of, does have the property that if you then take that element in the kernel and apply lambda t to it, t not equal to zero, you'll get a function into compact operators which vanishes at infinity. By the way, infinity also means at zero because it's infinity in the sense of locally compact spaces.
00:12:09.710 - 00:12:16.478, Speaker A: Yes. Dan, sorry to keep you waiting, because it's my job to serve you. And I feel bad about making your weight.
00:12:16.566 - 00:12:18.914, Speaker B: Do you want a little subscript lambda on the screen?
00:12:22.254 - 00:12:54.434, Speaker A: Do I want it? No. Oh, yeah, I see. Let me put it here for consistency. There's a theory called the theory of amenability, which said. And there's also something called the full group's eastar algebra, which I don't have to tell you about. Because the theory of amenability says that the full group's Easter algebra, which is the thing without the lambda, which I didn't tell you what it is, is equal to the thing with the lambda. For the groups and groupoids we're considering, this means the reduced C star algebra.
00:12:54.434 - 00:13:30.034, Speaker A: Lambda stands for left. Yeah. Which I learned from Pierre Zulc. He didn't write it g, but, you know, there you go. Okay, so far so good. And the content, really, of what we did last time is two things. Well, the first is a triviality.
00:13:30.034 - 00:14:44.014, Speaker A: It's a triviality because of the way we defined the sea star algebra norm. If you happen to have something which represents to zero for all t, I mean, including t equals zero, then that's because f is zero. So these values that we're calculating, these values of images, if they were calculating lambda t of f, determine f completely. And it's the second thing which is not so simple, which is that the. Maybe the norm. Suppose you take an f and you apply lambda t to it. Now you have lambda t of f.
00:14:44.014 - 00:15:11.678, Speaker A: I'd like to say that lambda t of f varies continuously with t. And that's true as long as t is not zero. Because it makes sense. At least it makes sense to ask whether lambda t of f varies continuously with f. Because, well, that's a path in this single sea star algebra. So we know what a continuous path is. But when you get to t is equal to zero, all of a sudden lambda zero, that lies in a different sea star algebra.
00:15:11.678 - 00:15:39.704, Speaker A: So we don't exactly know what, what it means for the function to be continuous there. On the other hand, if we take the norm, and where you take the norm depends on what t is. If t is equal to zero, you're supposed to take the norm in this sea star algebra of the tangent bundle. Excuse me, this sister algebra of the tangent bundle. And t is not zero. It's supposed to take the norm here. But now it makes sense because the norm is just always a number to ask that this thing be continuous.
00:15:39.704 - 00:16:10.024, Speaker A: And it is. Function is continuous. There's no problem improving continuity away from zero. That's a doddle. The whole problem is continuity at zero. And there's no problem in saying that the norm at zero is, let me get this the right way around. Less than or equal to the limb of the norms as you converge to zero.
00:16:10.024 - 00:16:40.250, Speaker A: But the other inequality, that the norm at zero is bigger than or equal to the limp soup of the norms, that's a bit of a chore. And that's what the, that's where the work is as carried out by me for your benefit, in the, in the notes. Okay. And thanks to, I was just singing your praises before ed. Thanks to the help of Shiqi and Ed, we'll make a better version of that for some unspecified future version of the notes. Okay, good. So I'm not going to discuss the proof of this.
00:16:40.250 - 00:17:20.138, Speaker A: It's just a fact that this is so. It's not. It's a theorem that this is so, and the proof can be pieced together from what's in the notes already. Great. And now that is for the record, let's just put down the condition, the appropriate definition here. Well, this is straight out of the famous textbook of dixmeet. So let t be a topological space for us.
00:17:20.138 - 00:18:10.544, Speaker A: It's going to always be the real line. It's going to be the place where all of these little t's live. So a continuous field of sea star algebra is two things. It's like a. We're trying to define a bundle of sea star algebras, but we want to consider bundles like the one that's being discussed here, which, where the actual sea star algebra can vary even up to isomorphism. So this guy here is not at all the same sister algebra as this one. After all, this one is commutative and this one isn't.
00:18:10.544 - 00:18:52.868, Speaker A: So we want to take into account that possibility. These things are not even sure it's true. Isomorphic. I'm just going to boldly say this. Not even isomorphic as barnark spaces. They're just not even the same Banach space, I think. Anyway, we're not going to worry about that is what? It's just a family of sea sterogebrists not glued together in any way whatsoever, just floating in space like in the space station.
00:18:52.868 - 00:20:03.444, Speaker A: Each at is Sort of floating independently of the others. And a family or a vector space usually put in vector space. Let's give it a name s of sections. What do I mean by a section? I just mean a function from the base space t into the disjoined union of the ats, which sends little t to something in big a t, just a function. I don't know. And then there are some axioms, and it's worth just writing them down just to dwell on them briefly. This definition is not difficult at all, but it's rather unexpectedly interesting.
00:20:03.444 - 00:20:33.724, Speaker A: So we'll see. So, such that what? Well, it's, how did I, how many parts did I put this into? I don't know, because I didn't print out that. Oh, I did print out that page. Yeah. Okay. There are a bunch of conditions. The way I've written it, there are three conditions, and the first two are as boring as heck, and you can always figure out a way to make them work.
00:20:33.724 - 00:21:38.350, Speaker A: So for every t in t, if you look at all of the values of all of the sections at t is dense in 80. Let me put the other boring condition here and then make a comment on both of them at the same time. This collection, apart from just being a vector space, is closed under point wise multiplication. After all, the sections take values in algebras, in fact, c star algebra. So you could also take stars in volitions. And if these weren't satisfied, of course, you could easily fix them. First of all, if you wanted the second condition to hold, well, you could just look at the s generated by all of the s's you started with.
00:21:38.350 - 00:21:56.142, Speaker A: That's typically what you do. You have a, have a bunch of s's. It's not closed on the pointwise multiplication. Fine, close it onto point wise multiplication. Same thing with involution. And then you have a t such that this span is not dense in 80. This subspace is not dense in 80.
00:21:56.142 - 00:22:22.424, Speaker A: Fine, just take the norm closure and call that thing 80. And now you've fixed that problem. So these are not very interesting at all. But the third thing is super interesting and not so easy to handle, as we know from this example, which is that for every. Oops. No, there's a, why did I say three conditions? How did I get it? Oh, there are four. Good, I missed one.
00:22:22.424 - 00:23:10.494, Speaker A: So let me in my mind. So these sections s are supposed to behave like the continuous sections, and continuous functions have certain properties. Namely, continuous functions are closed under local uniform limits like you learn in when you're small from your mother and so maybe not your mother. So we want to build that in. I'm going through this because I think Dixmere and his pal Duadi got this exactly right. And it's okay. So we should just admire this.
00:23:10.494 - 00:23:50.502, Speaker A: Given any section s, I mean, not a priori, the conclusion is that s is going to be in big s. But suppose you're starting off with any section. This is a criterion which forces membership in big s. And the criterion is exactly what I said about approximation. So if at every point and every. Are you ready for this, Dan? Second epsilon.
00:23:50.638 - 00:23:51.314, Speaker B: Right.
00:23:52.854 - 00:24:59.334, Speaker A: Every epsilon, if there is a neighborhood of t in t and a continuous section s prime, that is to say, a section in this distinguished collection, such that so s is approximated by s prime by a standard of epsilon or all, let's call this t zero. Now, for all tnt in the neighborhood. In the neighborhood. Let's give the neighborhood a name. Sorry about that. So that's a property that the section s may or may not have, that at every point it can be approximated by in quotation mark's continuous section. If that's the case, then s has to be continuous.
00:24:59.334 - 00:25:38.074, Speaker A: This grammar is if. If blah, blah, blah, blah, blah, then s is in s. Suppose you had an s which was not in little s, which was not in big s, but it had this property. What should you do? Well, you should just throw it in. You should close the previous big s under this operation to get a bigger big s. So again, this is not a big deal. This is just something that you would like a collection of continuous sections to have, because you know that continuous functions ought to behave in this way.
00:25:38.074 - 00:26:29.484, Speaker A: Okay, so none of these are big deals. But the fourth guy, formerly the third guy, but then I forgot something. The fourth guy is the big deal, which is this function is continuous. Norm function is continuous. So this is the norm in at it's continuous. So in our example, this example here, of course, the topological space is the real line. The a sub T's are these algebra.
00:26:29.484 - 00:27:07.668, Speaker A: Most of them are just the compact operators, but one of them is c star of tm. And so that's what the a's are. As for the S's, what do you do? Well, you take the smooth, compactly supported functions on the tangent groupoid things which live there in the middle. That's what you would start with, just like we did on Thursday. And for each smooth, compactly supported function, you get for yourself a section of the collection of sea Star algebras I just mentioned by taking that function f and representing it on the various fiber. Sea star algebra. So that's how you get a bunch of sections.
00:27:07.668 - 00:27:46.494, Speaker A: And when you get a bunch of sections that way, they're automatically closed under additional vector space operations. But also multiplication, because we have this convolution multiplication on the tangent groupoid algebra and the junction, all of this stuff, one and two is automatically satisfied. We don't have to do anything. There density is a doddle that we also have that, if you think about it for a moment. What about this limit condition? Well, this isn't satisfied because we were only talking about smooth elements in the convolution algebra to begin with. Fine, you just complete according to this prescription here. You add to s all of the sections you get in this way.
00:27:46.494 - 00:29:08.496, Speaker A: And then it's a fact when you take by elementary analysis, that when you enlarge the collection, if you already knew the old collection had this continuity property, so also the larger connection collection of sections, big s will have continuity property too. That's how you build a continuous field. When we actually proved this type of thing, this part two of the proposition, we did only imagine that f was a smooth, compactly supported function. And then we I secretly, without telling you, extended by continuity, by taking limits to make this assertion here for all elements in the tangent group by sea star, if you have a continuous field like this, and if the space t is not so ridiculous, if it's a compact Hausdorff space or a locally compact Hausdorff space, then you can build a new sea star algebra out of the continuous field. Namely, you just look at the siesta algebra of all continuous sections coming from s, which vanish at infinity in the sense that this scalar function here vanishes to infinity. So that's the sister algebra of C, zero sections. And if you were dutiful and you threw in all of these sections in part three, then you can check that what you get really is a sister algebra.
00:29:08.496 - 00:29:45.058, Speaker A: It's complete in the obvious sister norm. What is the obvious Siesta norm? It's just the soup of the norms of sections. And if t is a locally compact and half star space, you can go backwards and forwards. Once you have big sister algebra of sections, it's an algebra, not just a sea star algebra, an algebra over the complex numbers. It's actually an algebra over the continuous functions on t, which vanished infinity. So you can localize the Siesta algebra at any point, you can just divide by some vanishing ideal corresponding to a point and get back the fibers. So you can go backwards and forwards.
00:29:45.058 - 00:30:11.560, Speaker A: Continuous fields, the sister algebra of continuous sections. When you do that, you need to not every sea star algebra is going to give rise to a continuous field. I mean, here we're talking about the starting with a continuous field and building a sea star algebra of sections. If you start with something which you wanted to be a sister algebra of sections. Now you want to check that. You get a continuous field. Well, that's not obvious, because this norm condition, you don't get for free.
00:30:11.560 - 00:30:34.112, Speaker A: You don't get this for free. You have to work to get this. You get one or the other semi continuities. I never remember which is which. Anyway, so one half of continuity you get, but the other half you don't get at all. Sir, sir. You could say that's a beautiful feature.
00:30:34.112 - 00:30:54.534, Speaker A: There's some interesting analysis involved here. Or you could say this is a defect of C. Easter algebra. This annoying technical issue arises very often, not in the easy situations we're considering, but in more fancy situations. I hear you, but I suspect Nigel can't. That's absolutely right. I cannot hear anyone.
00:30:54.534 - 00:31:13.054, Speaker A: Sorry, but you can hear me? Yes. Yeah, you can hear me. Can anyone hear me? Yes, we can hear you. Yeah, somebody says, yeah, we hear you. Okay, good. Sorry, samik, I cannot hear you. No, it's fine.
00:31:13.054 - 00:31:38.346, Speaker A: It's fine if he types. Yeah, exactly. Ask a question in the chat, if you have a question, and Dan will interpret it. Dan will answer it, in fact. Okay, good. All right. What else did I want? Oh, yeah, that's temporarily all I want to say about continuous fields.
00:31:38.346 - 00:32:50.792, Speaker A: We're not done with continuous fields, because soon we're going to talk about k theory, and the main issue for us will be how k theory interacts with continuous fields. So we're not done with this, but we'll come back to it. Oh, I did want to say one thing, which is kind of fun. It's kind of interesting. Maybe we'll just go over here. This is a remark which is about a closely related concept, not exactly a continuous field of sea star algebras, but about continuous fields of Hilbert spaces. What's a continuous field of Hilbert spaces? Well, you take this definition, and all you have to do is erase point wise multiplication and involution, and then the rest makes sense.
00:32:50.792 - 00:34:01.214, Speaker A: If the fibers are Hilbert spaces instead of sea star algebras, if you have a topological space which is a reasonable space, maybe not too big. I mean, compact and matrizable. An aves is not too big either. Maybe these fiber Hilbert spaces are all separable. You can ask, what kind of a field do you get? Now we're talking about Hilbert spaces, so it's like we're talking about infinite dimensional bundles. And you might ask, what kind of a thing is it that you get in this way? Which is some kind of infinite dimensional bundle of Hilbert spaces. And it's interesting to figure out the answer.
00:34:01.214 - 00:34:49.394, Speaker A: Well, one thing that can happen is that the fiber dimensions can jump in a slightly unpredictable way. Just because you have a continuous field of Hilbert spaces, it doesn't mean that all of them have to have the same dimension. So this thing is definitely not locally trivial. For example, you could have a continuous field. Maybe you have a reasonable, nice continuous field, a satisfactory continuous field in your eyes to begin with, maybe the trivial field. And then you could change the trivial field by taking one point t zero and changing the fiber Hilbert space over that point to just plane zero. And then you define the new continuous sections to be the old continuous sections, which assume the value zero at the special point t zero.
00:34:49.394 - 00:35:16.374, Speaker A: And that's a new continuous field that you built that way. It satisfies the same axiom. So this definition allows the fiber dimension to jump. And that's actually a feature, not a bug. Yep. Good. Is the continuous field of topological spaces some kind of bundle? No attempt is made to glue the fiber sister algebras into a single topological space.
00:35:16.374 - 00:35:32.714, Speaker A: I would say, indeed, following Dan, that the answer to that question is no. You don't want to think about doing what I think you're thinking about, which is gluing the fibers sea star algebras together into a single topological space. But, yeah, go ahead.
00:35:40.594 - 00:35:42.386, Speaker B: Is this like what Riofel's?
00:35:42.490 - 00:37:01.572, Speaker A: Yeah. When reopel talks about deformations, he insists that his deformations be, well, among other things, continuous fields in this sense. Yeah, there's, of course, other, you know, there's some press on bracket in the picture as well. But back to this, which is a remarkable fact, which shows you that despite the unpredictability of continuous fields, the fiber dimensions can jump. The following is true, that this field, within the world of reasonable spaces, every collection of Hilbert spaces, collection of continuous sections, is a direct summant, an orthogonal direct summant of a trivial field. Maybe a better word is constant. Constant field is one where all of these fiber Hilbert spaces are the same.
00:37:01.572 - 00:37:24.814, Speaker A: Hhh. And where the continuous sections are just the continuous functions from t to h. And that satisfies these axioms so that that really is a bundle, and every other continuous field, at least, is an orthogonal direct cement of one of these. And not only that, but you can make the orthogonal complement into a trivializable field, too. Yeah.
00:37:24.974 - 00:37:29.150, Speaker B: So is this sort of like a projective statement from Kakarot?
00:37:29.222 - 00:37:58.612, Speaker A: Yeah, it's weird. This, first of all, this is. What's the theorem of Kasparov. Kasparov. So stabilization. And what it's trying to tell you is that these continuous fields of Hilbert spaces are like projective objects. Projectives in the category of infinite dimensional, I don't know, in quotation mark bundles over a reasonable base.
00:37:58.612 - 00:38:09.492, Speaker A: Of course, if the base is some horrible, monstrous nonsense, then all bets are off. But in reasonable context, these things turn out to be miraculously kind of reasonable. It's very strange.
00:38:09.628 - 00:38:13.504, Speaker B: Is it no longer true if you still have like a, like a r or something?
00:38:13.804 - 00:38:39.046, Speaker A: Oh, locally compact is fine. Yeah, locally compact is still reasonable. Yeah, yeah, that's. Yeah. I hadn't mind much worse things than that is there is a. Kasparov built a theory which we'll touch upon later, which at its center involves the. These continuous fields and generalizations of continuous fields.
00:38:39.046 - 00:39:41.844, Speaker A: And in doing so, he got k theory exactly right. He really figured out the right way of handling things and issues that were a little bit muffed by a tier and singer, for example, issues that come up in index theory for families. Kasparov got exactly right using this definition, for which we thank Dixme and Duadeep for figuring out the details here. So this is quite an important foundational definition, as you can tell, because I keep rabbiting on about it. Okay, enough. There's an awful lot of siesta algebra theory that you can have whole thematic programs lasting entire semesters on sea star algebras. But when it comes to using sea star algebras in the outside world, in nature, the things that you typically need from day to day as you help other people, strangers, solve problems using sister algebra theory, the things that you need are mostly in Dixmier's book from the 19.
00:39:41.844 - 00:40:15.364, Speaker A: I don't know what the english edition is from the 1970s. The book was probably written in the fifties. So there we go. And this business with continuous fields ranks pretty highly. It's almost. Oh, maybe I just. No, I should give one more example, as this example here may convince you.
00:40:15.364 - 00:41:21.204, Speaker A: Suppose you have the following situation, which we're going to consider anyway, and which we already have been considering a little while ago on Thursday. Suppose you have a submersion of manifolds, I think we called it z to wise. And suppose, let's just remember all of this stuff from first day. We talked about densities. I want to build some integrals and some Hilbert spaces and so on. So for that you need an everywhere positive family of densities, smooth measures, if you like, on the fibers PI. So now you can do this.
00:41:21.204 - 00:42:24.618, Speaker A: Call these five. This is ew. Now you can form l two of zw in the way that we discussed using densities, and there's one of these for each w, and you can form this guy. And it's a family of sister algebras. And these are examples of continuous fields. And to be continuous fields, you're not allowed to stop the way I stopped here, because you have to say what are the sections? Otherwise it doesn't make sense. These Hilbert spaces or sea structures are compact operators.
00:42:24.618 - 00:43:20.304, Speaker A: They're just floating around in the international Space Station, and they're not connected to one another at all. We're not going to glue them together into a topological space, but at least we need to say what are the continuous sections? And so now let's do it. So, s will be generated by smooth functions on the big space. If you have a smooth, let's say compact supported function on the big space, you can restrict it to each fiber. And now you have a family of smooth, compactly supported functions, which is to say a family of vectors in here. And you insist that all of those be continuous, and you're not allowed to stop there because you have all of these axioms to satisfy. So the actual continuous sections are all of the sections you get by the item three process from smooth, compactly supported functions.
00:43:20.304 - 00:44:36.804, Speaker A: So, and that's what generated by means. This discussed in detail in Dixmier's book. So, the final thing is r continuous fields. And what about compact operators? These are continuous fields of Hilbert spaces and seastor algebras, respectively. Well, you know, how do we specify some continuous operators as we go from fiber to fiber? There are lots of ways of doing it, but the simplest and standard way is to do the following thing. Suppose you have two smooth and compactly supported functions on G. Then you can build an operator in the following way.
00:44:36.804 - 00:45:32.050, Speaker A: If I have two functions on I said g, I should have said z. If you have two functions on z, and if you have a point in w, you can restrict both of those two functions, let's call one of them f and the other one h. You can restrict them both to the fiber z, sub w. Now, you have two functions on the single fiber, which is to say two vectors in this Hilbert space, l two zw. And if you have two vectors, you can make an operator out of them, because you can use one of the vectors as a linear functional that gives you a linear map from l two z w into the complex numbers, and you can use the other vector to map the complex numbers back into the Hilbert space l two. So for any two vectors, there's a corresponding operator with one dimensional range, which is usually called something like t sub vector one, comma vector two, or t sub vector one, tensor vector two. And that's what I'm talking about.
00:45:32.050 - 00:46:22.822, Speaker A: And you take all of those guys, and now when you're doing the generation, you've got to be a little more energetic than we were before, because this collection, it's a little bit interesting. Each of these individual operators that I described has the property that if you compose it with another, then you get a third. So the collection of operators I just described is closed under multiplication, and it's closed under adjoint, but it's not a vector space. So the first thing you have to do in the generation is take the span of the operators I just talked about. Now, that thing is does satisfy one and two, and then you do the same completion operation for three, and you're done. Oh, is this a continuous function? Is the norm continuous? That's not completely trivial, but the answer is yes. And so this is how you do it.
00:46:22.822 - 00:48:07.264, Speaker A: And this was an endlessly difficult problem for a tira and singer, which they never really figured out. You have the fibers of a submersion like this. What kind of a bundle do the Hilbert spaces l two of z sub w make? And the answer is, they don't make a particularly satisfactory bundle. And this is a much better way of proceeding. If you want to prove the families index theorem the right way, then this is the language to use, as Kasparov pointed out. Yes, that's exactly in fact what I wanted to write down here, as a small prelude to something alpha and this beta, you could form this guy, this space, which is just pairs of points in zoom. This is a smooth, topologically closed submanifold, embedded submanifold, closed, embedded submanifold of z times Z.
00:48:07.264 - 00:49:08.616, Speaker A: And it's a lig groupoid, for what it's worth, which is a lot, actually. And a lig groupoid, what's the composition? Well, I wrote it this way to make it obvious what the composition is. If you have a z one and a z two and a z three, so you have pairs z two z one and z three z two, it's obvious how you compose them to get the pair z three z one. It's a group word. What's the source and the range map? Well, first of all, what's the objects? The objects are w, and there are two maps to w, namely PI on the first factor and PI on the second factor said that wrong, didn't I? Just erase the last we'll fix this up in post. Erase the last 30 seconds. The object space is z, and there are two projections to z.
00:49:08.616 - 00:49:40.574, Speaker A: And that's what the source and range are. And I gave you the composition. And it's the smooth groupoid. Yeah, it's. It's a subgroup point of z times z. Okay, thank you. And then use exactly as echoed suggested cc infinity of g to generate the continuous sections.
00:49:40.574 - 00:50:19.856, Speaker A: So we'll see this groupoid in a little while. This is not a particularly interesting groupoid. And Alain, who's still up there, makes a big deal over that fact. This is a much easier groupoid to think about than the tangent groupoid. As you travel through the fibers of the tangent groupoid, something odd happens when you get to t is equal to zero. The nature of the fibers, the nature of the groupoid structure changes. Here.
00:50:19.856 - 00:50:56.866, Speaker A: The groupoid multiplication is just all the same everywhere. But in the case of the tangent groupoid, something odd happens at t is equal to zero, which is where all the trouble lies. According to our length, this groupoid in the language of category theory, is equivalent to a much easier groupoid. Namely, just the groupoid whose objects are w. And it doesn't have any morphisms, just has identity morphisms as required by law. So this groupoid that I just wrote down is equivalent to a commutative space, whereas tangent groupoid doesn't have that property. And according to ln, that's the fundamental issue of the index theorem.
00:50:56.866 - 00:51:31.538, Speaker A: The tangent groupoid has the extra complexity that it's not equivalent to a space. And in an ingenious way, just to make a cut to the end of the discussion for this week, in an ingenious way, Alain will change the tangent groupoid into one of these things. And in that way, make the index theorem, which I haven't told you what it is, transparent. Okay, maybe I shouldn't have said that. We're getting way ahead of ourselves. Yes, Morita equivalent. Or I mean, in his.
00:51:31.538 - 00:51:55.694, Speaker A: In his book, Alain's very. He doesn't mention Morita equivalence. He just says equivalence in the sense of categories. But yeah, Morita equivalent in the sense of the groupoids. All right, what else did I want to say? Oh, just. Yeah, I have one more sister algebraic topic to mention. I lied a little bit when I said that it's all in Diximier's book.
00:51:55.694 - 00:52:02.830, Speaker A: Because there's one topic which you use all the time, which is not in Diximier's book. And I'll tell you about that next after I've answered Dan's question just.
00:52:02.862 - 00:52:11.550, Speaker B: Just really quick. So you kind of mentioned this briefly. But you have different sections that give you the same continuous field of Caesar algebra.
00:52:11.742 - 00:52:35.694, Speaker A: Oh, yeah, yeah, yeah. You could have. I mean, after you've built the. Let's consider the simplest case. You have the in which w is a single point. Now, different generating families is what I'm thinking. After you've generated.
00:52:35.694 - 00:52:58.284, Speaker A: They have to be the same. But you could start with, I don't know, two smooth structures on the same manifold. And then you'd get two generating families which weren't. Well, you know, situated with respect to one another at all. And it's another benefit of sister algebra. So you don't have to care about that. It all sort of goes away in the wash when you take this completion.
00:52:58.284 - 00:53:22.048, Speaker A: This result of Gennady's caspar was very, very striking and impressive. This, I think, should seal the deal that we're onto something. When we talk about continuous fields. Along with this construction, which you can just do. You don't have to trivialize and introduce some artificial trivial. You can. All of these Hilbert spaces are really the same.
00:53:22.048 - 00:53:38.156, Speaker A: And you could trivialize this in some way. But it's unsatisfactory to do that. But Gennady says you don't have to. It's just fine the way it is. It's a projective object in the world of families of Hilbert spaces. Enough. Good.
00:53:38.156 - 00:53:59.732, Speaker A: So the one other topic I want to mention just. Oh, sorry. Yeah. Since continuous field topologist basic. Are we making it into some kind of bundle? No. Well, we're not. You should free yourself from always wanting to think about bundles.
00:53:59.732 - 00:54:15.030, Speaker A: A continuous field of siesta algebra. Is a family of sister algebras. Or a continuous field of Hilbert spaces. Is a family of Hilbert spaces. Which are tied together in a certain way. But they're tied together through this family s. Of continuous sections.
00:54:15.030 - 00:54:39.144, Speaker A: And as for asking for anything more, I don't think you should do it. This is the right way of tying together those infinite dimensional fibers into one thing. And it's not a bundle. But not everything in life has to be a bundle. And this is an example. Yeah. I mean, that's also true.
00:54:39.144 - 00:55:09.994, Speaker A: They're not bundles in the sense that most people understand bundles. The fibers can change, of course. The fibers of a groupoid. Are glued together into some single geometric object. Namely the smooth manifold of allomorphisms. But here you should, through meditation and therapy, learn to not worry about the fact that we haven't topologized the disjoint union of the fibers. That's okay.
00:55:09.994 - 00:55:14.374, Speaker A: Just takes practice. But then eventually you can do it. Then you're happy. Don't worry.
00:55:16.434 - 00:55:18.654, Speaker B: Just make one kind of ones that.
00:55:19.754 - 00:55:29.474, Speaker A: Yeah, don't, don't. That's what you need therapy for. You want to get past that. I will not apologize.
00:55:30.134 - 00:55:32.414, Speaker B: I like what you're doing by building this.
00:55:32.454 - 00:55:33.034, Speaker A: Like.
00:55:36.774 - 00:55:42.262, Speaker B: I mean, I missed last part, but is the tantrum ways to start over? It's an example where I guess at.
00:55:42.358 - 00:56:17.072, Speaker A: Is it's built out of something, namely the groupoid, which is more than just a family of source fibers. They all fit together in a beautiful way, but when you. But it's finite dimensional. And no bad thing happens when you contemplate gluing finite dimensional things together in infinite dimensions. It's just not. I just don't think it's productive or correct to use that finite dimensional idea of a bundle in this infinite dimensional context. Let me put it this way.
00:56:17.072 - 00:56:21.576, Speaker A: A Tiran singer tried to do it and they pretty much screwed it up. So. Yeah, don't.
00:56:21.680 - 00:56:31.552, Speaker B: So again, this last fact is part four. Does that follow? Because we had a manifold, or at least.
00:56:31.688 - 00:57:14.044, Speaker A: No, no, this follows. This last item is an item from harmonic analysis. Very simple harmonic analysis in the case of the tangent groupoid. But as you move from example to example, you'll find examples where an increasing amount of stress is put on you to try and prove this. And eventually you'll find examples where it's just not true. You build, you start off with some perfectly respectable c infinity thing, like a Lee groupoid, for example. And now you build from the Lie groupoid, some family of sea star algebras, and it isn't a continuous field.
00:57:16.384 - 00:57:18.404, Speaker B: Why did we work so hard to put in?
00:57:19.584 - 00:57:48.784, Speaker A: Because when you've got that far and you've built some terrible counterexample like that, you're in the world of count crazy counterexamples, like counterexamples to the Baumcon conjecture and so on. You've ventured too far in a certain direction. Okay, you're beyond infinite dimensional. I don't know where you are, but. But that's, you know, come back, don't go that far. Then you need therapy to correct the previous therapist's damage. Yeah, that's how it is, I suppose.
00:57:48.784 - 00:58:44.764, Speaker A: All right, let's move ahead and wait. More time on that than I expected. Okay, let's now say goodbye to Professor Dixmere. So, one of the topic piece of language that's very handy, which you won't find in Dixmere's book is the following thing. And you can talk about left, I'm going to talk about multipliers. And you can talk about left multipliers or right multipliers or two sided multipliers. And let's just talk about two sided multipliers.
00:58:44.764 - 00:59:38.352, Speaker A: So it's a pair of maps. I apologize. This definition looks kind of clunky. But here's what you're supposed to be thinking of. You're supposed to be thinking of this. You have an algebra a, like perhaps the Alex Easter algebra of compact operators on Hilbert space, and it sits inside a larger algebra, b, like the bounded operators as a two sided ideal. If you have that situation, then you can take anything in the larger algebra and you can, let's call it x.
00:59:38.352 - 01:00:05.294, Speaker A: And you can multiply elements of the ideal on the left or the right by that element x. And you'll get maps from the ideal to itself. That's an example of a two sided multiply. So it actually only comes from one thing, one thing in this larger algebra, which contains a as an ideal. So you should think of l and r as just two manifestations of one thing. Left multiplication by one thing and right multiplication by one thing. So sorry about that.
01:00:05.294 - 01:01:14.252, Speaker A: Anyway, here's what the laws are. If you left multiply ab, then that's just the same thing as left multiplying by a by whatever x, and then multiplying by b, that's called associativity. And then the same thing for the right. And if you're multiplying on the left and the right by the same element, then there's one other law, isn't there? Which is that a times the left multiplication of b. So this would be axb is the same thing, is the right multiplication of a by x times b. So these things form an algebra. How do you write down the multiplication law? If you have two left multiplications and you compose them, that's another left multiplication.
01:01:14.252 - 01:01:47.096, Speaker A: So the product left should be the left one times left two. But the product rights should be the other way around. Right two times right one. You know how it goes with these things. And that makes, that puts a multiplication on these things. Addition is obvious and you're in business instead of. Yeah.
01:01:47.096 - 01:02:57.116, Speaker A: Let me give you one non trivial but very interesting example. Instead of developing an elaborate theory, let's just jump straight to an example involving the convolutional algebra of the tangent groupoid of a manifold M. Suppose we have letter, we also call that a darn. I shouldn't use a different letter, but let's let's go back to our scaling families. Remember them? Of order. I don't know. R agree.
01:02:57.116 - 01:04:24.554, Speaker A: I guess we will call you. This is a family of operators on the spaces which are cc infinity of M and it has the property which I'm going to fix in a sort of simple way. Well maybe I was going to just monkey with what I've already written, but I'll. As we saw he determines a smooth family, a bigger smooth family of operators, one on each of the source fibers of the tangent groupoid which we were calling MMT. Like that. Most of the M sub M t's are just copies of M and p. T is already an operator on M.
01:04:24.554 - 01:05:07.714, Speaker A: And so you're in business. You don't have to do any extension when t is not zero. But when t is equal to zero you don't have a p zero. It's not in the collection. But the definition of scaling family is that there's a smooth extension to t is equal to zero. So we have such a family now which is a smooth family. So in other words it's a certain operator big p.
01:05:07.714 - 01:06:10.914, Speaker A: What's a smooth family? Well you're supposed to say that all of the operators can be glued together. I use that term loosely. Into a single operator like this. There is an operator big p like I've written at the bottom, which localizes on each source fiber to the operators I've just discussed. If you have a scaling family then you get a whole family of operators like this. Or if you like one operator like this with some additional properties. And this thing is compatible.
01:06:10.914 - 01:07:09.718, Speaker A: When we were discussing scalable families, scaling families, we didn't have any multiplication on the tangent group point to speak of. But now we do. And this thing is compatible with that. It's well what people would call a left multiplier. That's supposed to be the big piece. Well that's kind of interesting. Why do we have this property? How could we dare expect to have something which is a right module map? Because we never talked about anything.
01:07:09.718 - 01:07:51.830, Speaker A: When we were talking about scaling families we didn't mention convolution at all. Where the heck is this coming from? Well we did mention something which is that this family is equivariant family that we started with. All of these operators. It's not just a random family of operators. One on each of the source fibers. They're equivariant in the sense we discussed. You can build a map from compos smooth functions on one source fiber to smooth functions on another source fiber.
01:07:51.830 - 01:08:52.354, Speaker A: Just by composing in the sense of groupoids with an element of the tangent groupoid and all of those maps from one source fiber to another, intertwine the operators PMT in the obvious way, which we did write down. And you have to remember, does it go from source of gamma to range of gamma or the other way around? So I'm not going to repeat it. Okay, so that's kind of compact. Moreover, this family of operators, as we saw, is adjointable. It's not obvious, and we actually used a little bit of elliptic theory to prove it. But this is adjointable. And the adjoint is another scaling family, or comes from another scaling family.
01:08:52.354 - 01:09:33.312, Speaker A: And now you can do the following thing. You can right multiply by p by left, multiplying by the adjoint family for p. This is a right multiplier. In other words, a left module map. Not sure what to call this. R. Let's just do it like this.
01:09:33.312 - 01:11:13.198, Speaker A: So l which is p involved, and r which is exactly what I described. You take p star and then you convolve with it. Then you take the adjoint, sorry for multiplicity of stars there. This is a two sided multiply. It's just algebra. When you work out what it means, ah, it just means that p star is the adjoint of pull. And in fact, these are all of the multipliers subject to one thing, which is that to be a scaling family, there's an additional condition having to do with scaling, having to do with the integer r, which appears over here.
01:11:13.198 - 01:12:24.944, Speaker A: And we modeled with that in a variety of ways. And one of the things you can say, of course, every element of the algebra a. To go back to the definition of multipliers, every element of a obviously gives a multiplier. If you have an element little c of a, it gives a left multiplier by left multiplication by c, and a right multiplier by right multiplication by c. So a always sits in the algebra of multipliers. And so when I say an element of, I should be a little bit careful smoothing element, what I'm worried about here is the compact support condition in the t direction. So decided to fudge that by just saying here, smoothing element, fix that later.
01:12:24.944 - 01:13:38.204, Speaker A: So when we talk about smoothing families of operators, one operator in each source fiber, in fact, that's exactly the same, sorry. When we talk about smooth families, one in each source fiber, that's exactly the same as an operator, which is a right module map like this. These operators are exactly the families of operators, which I forgot to say, are equivariant. If you have one of these things, it necessarily is, in our language, a smooth, properly supported, continuous family of operators on this source fibers, which is equivariance. So we're not talking about anything different when we write down this fundamental relation than what we were looking at before. I said a small lie there to get the proper support has two sides to its definition, and you'll be able to guarantee that you obtain from an operator satisfying this law, a properly supported operator if you know that p is a joint, if it's part of a two sided multiplier. So every two sided multiplier, like in the definition over there of the tangent group or sister algebra, is a smooth family of operators.
01:13:38.204 - 01:14:44.200, Speaker A: That's what I'm saying, which is continuous in the sense we were describing, and properly supported in the sense we were describing. So that's nice. We can now say, thanks to our adjointability theorem, we're just dealing with certain two sided multipliers. Which two sided multipliers are we considering? Well, they satisfy a side condition having to do with scaling, which is not going to be particularly relevant because what we're going to describe in index theory is something which applies to any multiplier whatsoever. Did I gobble that enough that everyone is completely confused? We had a long discussion at the beginning of the semester about scalable operators and scaling families. That long discussion involved many adjectives like properly supported and continuous and adjointable and smoothing. I don't know what.
01:14:44.200 - 01:15:42.588, Speaker A: And what I'm saying now is that the language of multipliers, which is not in Dixmier's book, but it should have been in Dixmier's book, conveniently packages all of that into one statement, which is that scalable operators are two sided multipliers of the tangent groupoid algebra, which satisfy, excuse me, scaling families are two sided multipliers of the tangent groupoid algebra, which satisfy an additional condition having to do with the scaling transformations. This one parameter family of automorphism. Oh, sorry. What is e? Oh, thank you very much. Eabcde, fifth letter of the Roman Alphabet. This is an Euler like vector field. Ah, no, no no, no.
01:15:42.588 - 01:16:12.794, Speaker A: It would. It's, it's the generator. Sumi Masen said, Sumi Masen in a go off getting up. This is the generator. I should have called it something else of. In fact, I will in just a moment. This zoom action, the action of positive real numbers on this space, maybe I'll call it.
01:16:12.794 - 01:16:47.666, Speaker A: Thank you, Heath, for drawing attention to that z for zoom, it's a vector field. It's an operator on sections, which is a vector field. My apologies. And we discussed, or we observed that the scaling condition on a scaling family could be written in infinitesimal form. And it was this infinitesimal form up to a sign. I can't remember if it was plus r or minus r. It depends on how you define the zoom action.
01:16:47.666 - 01:16:51.934, Speaker A: So I guess I'm kind of safe as long as I don't specify anything else.
01:16:52.474 - 01:16:53.694, Speaker B: I talked about it.
01:16:58.554 - 01:17:22.914, Speaker A: Yeah, that's the opposite of the way I did it. So with either Dan's definition or mine, one of them. This is correct. Okay, I shall. You're confused. Yes. So that's a condition on p as a linear map from cc infinity of tm to itself.
01:17:23.454 - 01:17:30.910, Speaker B: Right. So I should think of this as the conditional multiplier. I'm just confused. Every multiplier gives one of these.
01:17:31.022 - 01:18:23.176, Speaker A: Yeah. Let me try to unconfuse you. Suppose you have a multiplier of the convolution algebra of the tangent groupoise. Suppose you look at that multiplier and temporarily you put the right part of the multiplier to one side and study only the left part of the multiplier. Okay? If that left part of the multiplier, which is a linear map from cc infinity of tm to itself, satisfy, and therefore it's not just a linear map, it's a linear module map, right module map from cc infinity t of m to itself. And therefore it's actually a smooth family of operators on the source fibers. If that operator, that smooth family of operators on the source fibers, considered as a single operator, satisfies.
01:18:23.176 - 01:18:57.300, Speaker A: In addition, this condition up here, where p means the left part of the multiplier, which is the subtle linear map from t of m to itself, cc infinity of t of m to itself, if it satisfies this condition, then the left part of the, a multiplier is a scaling operator of order. Degree is a scaling family, excuse me, of degree p. And conversely, if you have a scaling family of degree p, you get what I just said, a multiplier whose left part is the action of p and it satisfies somehow, like.
01:18:57.332 - 01:18:58.504, Speaker B: All of these attributes.
01:19:01.644 - 01:19:25.434, Speaker A: Yeah, they're just in there. It's kind of, I mean, it's a small thing, and if they weren't automatically in, we'd have thrown them in anyway. But it's kind of nice that they're all just swallowed up. And all you have to say is it's a multiplier. And you're in business. You know, that's why we make definitions, to express packages. Put a package of ideas in a little tidy suitcase to carry around.
01:19:25.434 - 01:20:32.878, Speaker A: Yeah, we have like a big chunk of ideas in a bag, and we can carry it around. Okay. And one more little theorem going back to our original idea of a scaling family. Or. Yeah, or I could have written big p. I don't know. If this thing is about a zero degree zero, is a scaling family of degrees zero, then the above, I clearly need some better notation because I don't have any notation for what I'm about to write down.
01:20:32.878 - 01:21:47.834, Speaker A: The associated above two sided multiply of this algebraical thing extends to the sea star algebra. And just like in the baseball records, I'll decorate this with a small asterisk, because I'm making a small lie here, which has to do with an inessential part of the story, which is what happens in the t variable far away from t is equal to zero. And that's all I'm going to say. The star means a small detail needs to be fixed. But this statement is essentially morally true. And it is true if you make a very small modification, which maybe I'll actually make right now. This is another one of these analytical facts.
01:21:47.834 - 01:22:36.342, Speaker A: We know that order zero operators are bounded. So point wise, fiber wise, so to speak, each of the p sub t's, or each of these p m t's, wherever they are over here, extends to an operator on l two. We know that for sure. And, but the following question arises, and this is actually the pivotal question. As you vary the t variable, do the norms of these p sub t's remain bounded, uniformly bounded in t? And the answer is yes, the norm of p sub t is uniformly bounded in t. At least if you vary t over a compact region, maybe from minus one to one, or will do soon from zero to one. Okay.
01:22:36.342 - 01:23:15.634, Speaker A: As t goes off to plus infinity or minus infinity, we don't have much control, and the norm can go to infinity. And that's not good, because multipliers of sister algebras are automatically bounded operators. That's what the star means. So it's a fact that the norm of p sub t over there. Yeah, this over here, I mean, is not only finite in degree zero, but it's uniformly finite in any neighborhood of t equals zero. And that's what you need to make this theorem true. Why is it uniformly bounded in t near t is equal to zero? Well, that's in the exercises from lecture.
01:23:15.634 - 01:24:03.438, Speaker A: I don't know what. So there it's your job to explain why it's uniformly bounded. There's an extra little analytical fact here, and it's like the facts we were just discussing. It's a non trivial fact, and it's an item of harmonic analysis. And in the exercises, this small item of harmonic analysis is conquered. And what we did in class, not quite enough to conquer it, but in the exercises, this small point is conquered. Let me, I don't have time to talk about k theory, but let me at least finish the, the last part of the preparations here I went to, my plan got a little disrupted because I sort of wandered all over the place.
01:24:03.438 - 01:25:39.550, Speaker A: But there's one more thing I do want to say, which I'll do over here. Oh, so, yeah, maybe I should say, apropos of nothing, really, we don't need the following fact directly, but it's a beautiful little fact, which is the star algebra of all multipliers of a c. Star algebra is the seat star algebra. So what's going on here is that the norm of a pair, literally, r is just the operating norm, let's say of l as a map from a to a, which is equal to the operator norm of r as a map from a to a. And I'll leave it to you to think about what the adjoint of a multiplier should be. We have this trick which converts left to write multipliers, and it involves p stars. In some way.
01:25:39.550 - 01:26:44.182, Speaker A: It seems pretty clear that you can work out an adjoint of a multiplier of a sister algebra using the adjoint on the underlying sister algebra. This should have been in Dixie's book, but it isn't. However, it is in the book of pedestal and apropos of continuous fields and the asterisk here, there's just one final remark to make. Oh, yeah, right. Fine. I didn't give an example. So the multiplier algebra of the sea star algebra of compact operators on a Hilbert space is indeed the algebra of all bounded operators.
01:26:44.182 - 01:27:52.936, Speaker A: That's a terrible beat. The bounded operators on H. I'm not sure I can do a good balance, which is the example we were discussing. But that's a good exercise and it's my job to make exercises for you, so I'll stick that in the exercises. Okay, now just a final point. So for the next lecture, we're going to work with a small variation of the sea star algebra of the tangent groupoid. And what we're going to do is we're only going to look at fibers of this tangent group in the sense of continuous fields between zero and one.
01:27:52.936 - 01:29:07.430, Speaker A: So this is the c star algebra of continuous sections of the associated continuous field that we were discussing a while ago over not the entire real line, but just the unit interval. Things just work out better for these. And to go back to the theorem here, each one of these families pt is an honest multiplier, no asterisks or extends to an honest two sided multiplier of this thing. The problem with this theorem, as I said, is what happens when little t goes to infinity. But now we took care of that. Little t can't go to infinity. It's trapped in a box, bouncing around in the unit interval.
01:29:07.430 - 01:30:04.244, Speaker A: So now the asterisk can be removed if we work with this thing here, is there, it's okay. And what we're going to do, just to tell you what you can forget before Thursday, is proving quotation marks. Take a walk through Alan Kahn's presentation of the index theorem. And the way things are going to work is that we can forget about pseudo differential operators or scalable operators or this or that, because the entire program from now on will apply to anything which is a multiplier of this sea star algebra. And it satisfies a certain Fredholm property that I'll make precise next time. So, as for scalable operators, it's good to know that they fit into this context, because it means that the index theorem that Alam proofs applies to pseudo differential elliptic pseudo differential operators. That's the Fredholm condition.
01:30:04.244 - 01:30:33.024, Speaker A: But we can unburden ourselves of the theory of scalable operators for the next lecture and concentrate only on groupoids and k theory in the next lecture. I mean, don't forget the other stuff. But we can unburden ourselves and think only about k theory when we meet for the last time before spring break. Thank you very much. Thank you very much everyone. Thank you, Internet.
01:30:34.004 - 01:30:35.904, Speaker B: What if we take the multiplier algebra?
01:30:37.244 - 01:31:14.774, Speaker A: What if you take the multiplier algebra of b, of h, or indeed any unital sea star algebra, or any unital algebra for that matter? If you have a multiplier of a unital algebra, an algebra with a multiplicative identity, you can apply l to that multiplicative identity and you'll get some element of a. Surely that is the special element of a. Indeed, it's obvious that left multiplication by that special element is the left part of the multiplier. So this doesn't do anything for units allow algebras with multiplicative identities. But it's rather interesting, let's say, for compact operators or many other things.
01:31:16.094 - 01:31:17.774, Speaker B: Way of making an algebra unit.
01:31:17.934 - 01:31:57.780, Speaker A: It's a way of making an algebra unit. What if you do this? Yeah, by the way, please wander off if you want. If you do this for the sister algebra of C zero functions on a locally compact space, maybe the locally compact space of integers is the most famous example. What do you get for the multiplier algebra? Well, it's a little exercise to see that this multiplier algebra is a commutative C star algebra. If you start off with a commutative a and Gelfinder Nymark tell us that commutative siesta algebras correspond to some kind of spaces. I mean, they are spaces. Certainly the multiplier Siesta algebra is a unital seaster algebra, because the identity comma, the identity is a multiplier.
01:31:57.780 - 01:32:13.770, Speaker A: So this is a compact space which Gelfand and Naima give us canonically out of the topological space of integers, totally disconnected space of integers, discrete space, face of integers. And it's the stone check compactification. That's what you get.
01:32:13.922 - 01:32:16.574, Speaker B: So like the idea of like compact.
01:32:19.714 - 01:32:39.154, Speaker A: Yeah, all, I mean, you have to define, if a compactification means compact space which contains your original space as an open subset, a dense open subset, then compactifications are just unital sea star algebras which lie between the unital Cser algebras which sit inside of the multiplier algebra.
01:32:41.014 - 01:32:44.714, Speaker B: So like the DoM unitization of just adding one.
01:32:46.174 - 01:32:46.694, Speaker A: Yep.
01:32:46.774 - 01:32:48.822, Speaker B: And this one is the stone check compact.
01:32:48.998 - 01:33:16.268, Speaker A: Yeah. The, the one point compactification of a locally compact space is the least you can do. It's the laziest possible way of compact finding space. The stone check compactification. I wouldn't say it's the most energetic, the one that requires the most effort, because it's also pretty easy to build. But it's at the other extreme, it's the compactification with the most points. The interesting compactifications are somewhere in the middle, of course.
01:33:16.268 - 01:33:23.004, Speaker A: Typically very good.
