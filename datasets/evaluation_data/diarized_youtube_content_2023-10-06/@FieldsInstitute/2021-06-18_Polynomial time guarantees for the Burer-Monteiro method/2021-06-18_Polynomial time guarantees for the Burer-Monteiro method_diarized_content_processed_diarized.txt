00:00:00.560 - 00:00:44.604, Speaker A: I'm going to talk about polynomial time guarantees for the wehrmaterial method, and this is joint work with Ankur Moitra from MIT. So this talk is going to be mainly about semi ethnic programming. So let me start simply by introducing the notion of a semi definite program, a semi definite program which I'm going to abbreviate as SDP. It's a special class of optimization problems that has the form that you may see on this slide. And perhaps the most important for now is that, is that the optimization variable, which I'm denoting as capital x, is actually a matrix. So the variable capital x is a matrix. Furthermore, it's actually a symmetric matrix of size n times n.
00:00:44.604 - 00:01:16.282, Speaker A: That's formally how we write an SDP. But the exact formulation is perhaps not totally important for us. For the purpose of this talk, what matters for us is that semiphenate programs are actually very well behaved. They are very well behaved class of optimization problems. In particular, they are convex optimization problems. Furthermore, they can be solved in polynomial time using techniques such as interior point methods. And this is something that we have known for a while.
00:01:16.282 - 00:02:05.684, Speaker A: We have known for a while that interior point methods solve SDPs in polynomial time due to the work of Nestorov and Emiraovsky. Well, and importantly, also, SDPs actually appear in many, many different applications in science and engineering, and I will illustrate a few of them in the next slide. The main purpose of this talk is going to illustrate, is to illustrate that we can also solve SDPs in polynomial time using a more recent class of methods. And this more recent class of methods is what we will call the Bohr Montero method. And I will introduce this later in the talk. So, first of all. So here are some applications of SDPs in case that you haven't heard about them before.
00:02:05.684 - 00:02:48.804, Speaker A: For instance, semidefinite programs appear in the context of the max code problem. So this is a classical program from computer science and combinatorial optimization. And it turns out that the best polynomial time approximation algorithm that we know is based on SDPs. They also appear, for instance, in the problem of matrix completion. There are very good SDP relaxations that approximate this problem. Similarly, they have also been used in the context of phase three, trivial interrotation synchronization, which is a problem that appears often in robotics and also in cryo electron microscopy. And they appear as well in problems from machine learning, such as clustering problems.
00:02:48.804 - 00:03:35.114, Speaker A: So these are some of the, of the applications of SDPs. And there are many, many other practical applications of semi definite programming, solving, semi definite programs. As I mentioned before, like one approach to do it is by using interior point methods. And we know, like as I mentioned before, we know that interior point methods can solve sdps in polynomial time. And for instance, Frank talked about this on Tuesday, I believe. And interior point methods, they are widely available. So there are many different solvers that, many different implementations based on interior point methods.
00:03:35.114 - 00:04:39.274, Speaker A: But one inconvenience of interior point methods is that for large scale sdps, they are typically intractable, they typically do not perform very well for large scale problems. And in particular, the reason is that very often they run out of memory. And as a consequence, in the past years, let's say in the past decade, there has been a sort of new techniques for solving semi ethnic programs that scale better for large scale problems. And in this slide you may see a few of the alternatives that have been proposed for solving large scale sdps. The main topic of this talk is the first of these methods, namely the low rank factorization method, which was pioneered by Brewer and Montero. And I'm going to refer to this method as the Bohr Montero method. And importantly, these Bohr Montero method is one of the most widely used methods in practice for solving large scale semi definite programs.
00:04:39.274 - 00:05:27.420, Speaker A: So let me try to illustrate what are the main ideas behind this technique. So here on the top, I have a semi definite program. This is the formulation that I had at the beginning, and as I mentioned before, the exact form is perhaps not terribly important for us. Now, what matters the most again is that the optimization variable x is a symmetric matrix of size n times. Nice. Furthermore, this matrix x is actually positive semi f, and consequently it's possible for us to factorize this matrix x in the form y times y transpose for some other matrix y. And we're going to make the assumption here that the matrix y, it's actually going to be a tall and skinny matrix.
00:05:27.420 - 00:06:07.622, Speaker A: We're going to assume that this matrix y is going to be of size n times p, where p, which is the number of columns, is significantly smaller than n, which is the number of rows. So that's one of our assumptions. So we are going to assume that there is a solution of the SDP that has a small franc, and it turns out that this will always be the case. And I will discuss this later. We are going to assume that there will be a solution of franc p where p is much smaller than n. And we're going to use this in order to factorize x in the form yy transpose where y is a tall and skinny matrix. Now we simply rephrase the problem in terms of y.
00:06:07.622 - 00:07:01.714, Speaker A: So here I have the new problem written in terms of y and the Bohr Montero method. The main idea is simply to solve this new problem. We now are going to solve this new problem by using some local optimization method. Now, why is it the case that this might be a good idea? Well, the main advantage of solving this new problem is that the matrix y that we're using, it's smaller than x. Since we are assuming that p is significantly smaller than n, then the matrix y, which has dimensions n times p, is going to have significantly less entries than x, which was the original variable. And consequently, when we are going to try to solve this new problem, we are going to get an immediate gain in terms of memory usage. So, so there will be a direct gain in terms of memory usage.
00:07:01.714 - 00:07:57.134, Speaker A: And as I mentioned before, memory was actually the bottleneck for interpoint methods. And what that means is that this memory improvement is actually very significant. Okay, so now what's the next question? So, another question that, one question that I forgot to mention before is how is it that we are supposed to choose pull. So the assumption was that this optimal solution of the SAP had a small rank, let's say rank p. But how is it that we are supposed to know this rank p in practice? Well, in some cases we know something about the problem. There are some problems in which we know something about the problem, and we may have an estimate about this value piece. But it turns out that in the worst case, we can always use this bound that I have here in the bottom.
00:07:57.134 - 00:08:50.484, Speaker A: And what this says is that if we choose p to be about a square root of two times m, where m is the number of constraints, that's always going to be enough. So there is a bound, there is a value of p that we can always use. And if we choose this value of pull, it turns out that the problem that I have on the top and the problem that I have in the bottom are going to be equivalent in the sense that they have the same optimal value. So as long as we choose this value of p that satisfies this bound, which I'm going to call the Barwin pataky bound, the two problems are going to be equivalent. Nonetheless, there is an important distinction between the problem on the top, which is the SDP, and the problem in the bottom, which is the Borer Montero problem. The important distinction is that the original problem, the semifinal program, is a convex problem. It's a very well behaved problem.
00:08:50.484 - 00:09:38.478, Speaker A: But the problem in the bottom, it's actually a non convex problem. And what that means is that if we use local optimization method to solve this problem in the bottom, in general, we might get stuck in a local minimum, which might not be global. And if that's the case, we will not be solving the semi definite program to global optimality. So that's one important issue. Well, the boromonter method was introduced around 20 years ago, and since then there has been quite a lot of research in trying to understand the behavior of this technique. And because of that, we now understand that there are two important regimes to take into account. The first regime is when we choose the parameter p, so p is the number of columns of the matrix y.
00:09:38.478 - 00:11:04.864, Speaker A: This is a very important parameter of this method. And if we choose this parameter below the Barda q out, it turns out that the behavior is actually quite bad in the sense that even if we assume that the Bermond error problem is equivalent to the original problem, in general there might be a spurious local meanwhile, which means again, that if we use local optimization, we might not get the solution that we want. But as long as we choose the parameter p above the patachy bound, it turns out that the situation dramatically improves. So let's assume for the remaining part of the talk that we choose p above the bargain of patakyva, which is a square root of two times m in this regime. Early work by Bureau Montero and also by journey and co authors gave a strong indication that this method was likely going to be actually quite successful. But a formal proof came later, and it was given by Bumal, Voroninsky and Bandera. And what they showed is that under certain conditions, in particular, if we assume that the cost matrix is generic, they were able to show that the Boer Montero problem is going to have no spurious local minima.
00:11:04.864 - 00:11:48.292, Speaker A: Hence, if we find any local minima, that's going to be enough. Sorry, give me a minute. I apologize. Okay, so let me continue. This work by Derea says that under some conditions, there's going to be no local minimum. And there was some subsequent work, in particular by Bumalenko and quotus, which proved that some extensions of this result, which deal in particular to the case where we may have some approximate local minimum. So, yes, let me ask you a question.
00:11:48.428 - 00:11:56.442, Speaker B: This is something I never understood. So, in here, this assumption that the feasible says set is smooth. This is a feasible set of the.
00:11:56.458 - 00:12:07.174, Speaker A: Semi definite problem of the borough Monterrey problem, the feasible set of the bohemontero problem. So the re parameterized set.
00:12:09.554 - 00:12:25.094, Speaker B: Yes. But you have the constraints, right? This is with the constraints, yes. So isn't it true that there's cases where this set is never smooth, like for certain combinations of the dimension and the.
00:12:26.754 - 00:13:19.094, Speaker A: No, that's not the case. If we, for any dimensions that you want, if you take generic matrices, the feasible set will be generically smooth. Yes. And actually the smoothness of the re parameterized feasible set actually is going to agree with the primal non degeneracy of the original SDP. So there's an interpretation of this smoothness also in terms of the original SDP. Okay, so let me perhaps move on. So, above the situation is a lot better behave.
00:13:19.094 - 00:14:21.164, Speaker A: But I still would like to point out that even though we know about these results prior to our work, actually it was still unknown whether it was possible to solve sdps in polynomial time using the Bohr Montero method. We have known for a very long time that we can solve sdps in polynomial time using, let's say, interior point methods. But until quite recently, it was still unknown whether we could achieve similar guarantees using this widely used borer Montero method. And it turns out that one of the main obstacles was actually feasibility, because all the previous works assumed that we were able to obtain a solution that satisfying the constraints exactly. And doing that, in practice, it's not going to be visible. So our main result is stated on this slide is this theorem on the bottom, and it has three main assumptions. Let me perhaps just focus on the first assumption, which is the most important one.
00:14:21.164 - 00:15:08.326, Speaker A: And what this assumption says is that I want to choose a parameter p slightly above the barbecue patache valve. So notice that I want p to be above a square root of two plus eta times m, where eta is a positive constant that is fixed, but it can be arbitrarily small. For instance, it can be 0.01. So that's the main assumption. And under this assumption, what we can show is this theorem on the bottom. And what does this theorem says? So, the theorem says the following. If you give me an arbitrary semi program that satisfies some smoothness conditions and you perturb and you then perturb the cost matrix by using a small random perturbation.
00:15:08.326 - 00:15:57.824, Speaker A: So this is important. We're going to take a small random perturbation of the cost matrix, and we are now going to try to solve this perturb problem. And we are going to solve this perturb problem by using the Brewer Montero method. And in the sense that we are going to initialize some local optimization method with an approximately feasible point. What this theorem shows is that the Boroman terror method is able to compute a point that is approximately feasible and approximately optimal with high probability in polynomial time. And this high probability result. This is with respect to the random perturbation that we took at the beginning here, we have to take a small random perturbation, and with high probability with respect to this perturbation, the method will conclude in polynomial time.
00:15:57.824 - 00:16:49.604, Speaker A: Any questions here? Okay, so, let me perhaps discuss a little bit more this issue of the random perturbation. So, this idea is actually, it's not known. It's something that is known as a smoothed analysis. And this is an idea that was introduced by Spielman and Tenkin in 2001. The motivation behind smoothed analysis is that there are some algorithms that are very practical, but that might be slow in a few very specific instances. One notable example is the simplex method. The simplex method for solving linear programs is a very, very efficient method in practice, but there are some very specific instances for which the method can take exponential time.
00:16:49.604 - 00:17:42.170, Speaker A: And what Spilman and Tank said is that in order to analyze these methods, it's better to take a worst case instance, but then to take a random perturbation around this worst case instance. And that's the notion of smoothed complexity. That in a smoothed complexity, we start with a worse case instance, then we take a small random perturbation, and we analyze the complexity of this random perturbation. And in particular, they use this technique in order to show that the simplex method takes polynomial time in the setting of smoothed analysis. And the theorem that I showed you in the previous slide, what this theorem is doing is exactly the same. We start with a worst case semidefinite program, and then we slightly perturbed the cost matrix using a random perturbation. And we showed the polynomial time optimality of the Borough Montero method.
00:17:42.170 - 00:18:36.024, Speaker A: And what this says is, is that the Bohr Montero method takes polynomial time in the setting of a smoothed analysis. Let me also point out another important issue here, which is that this theorem alone is actually not enough for us to guarantee that we can solve sdps in polynomial time. And the reason is that there's actually one assumption here that it's a little bit tricky, which is that we are assuming here that we are initializing the borough Montero method with a point that is approximately feasible. What that means is that we need to find this approximately feasible point. And this is something that is non trivial. Simply finding an approximate feasible solution of a semi thin program can be quite complicated. Let me make this more precise.
00:18:36.024 - 00:19:16.646, Speaker A: We want to find a matrix x that satisfies some constraints approximately. So we want something that is approximately feasible. And also, I want the matrix x to be positive semidefin. How is it that we can do that? One possibility is to formulate this problem as a least square problem, where we minimize the sum of squares of the infeasibilities. And once we have this new formulation, we can apply again the Bohr Montero method by factorizing x in the form y times y transpose. And if we do that, we obtain a new optimization problem in terms of y. In this case, this is an unconstrained optimization problem in terms of y.
00:19:16.646 - 00:20:20.514, Speaker A: But as before, this is a non convex optimization problem. But despite the fact that this new problem is again non convex, we can show that under the same assumptions as before, if we randomly perturb the data of the SDP, then we can find a point which is approximately feasible for the SDP in polynomial time using the Bohr Montero method. And that's the second theorem that I have here in the bottom. Now, if I combine these two theorems, polynomial time optimality and polynomial time feasibility, we have a two stage method for solving worst case semi definite programs using the Bohr Montero method and everything francine polynomial time. So, first, we find an approximately feasible solution using the second theorem. And then using this initial point, we can find an approximately optimal solution using the theorem on the top. Any questions here?
00:20:22.394 - 00:20:25.242, Speaker C: Can I ask a question? I don't know if you can hear me, Diego.
00:20:25.378 - 00:20:26.730, Speaker A: Yes, I can hear you.
00:20:26.922 - 00:20:37.654, Speaker C: So, in the, in the first theorem, what is approximately feasible and approximately optimal? So what is the approx. I mean? And so is this. And how does this affect the number of iterations? I mean.
00:20:38.714 - 00:20:59.674, Speaker A: Yes, so, yeah, so the accuracy is going to be proportional to the perturbation. It's going to be proportional to Sigma, to how much you're going to perturb. So, essentially, you can fix an arbitrary perturbation, you can fix an arbitrary precision, and then you can choose stigma to be proportional to that.
00:21:00.374 - 00:21:01.366, Speaker C: Oh, I see. Okay.
00:21:01.430 - 00:22:03.492, Speaker A: Okay, good. Any other questions? Okay, so what I want to do now is to try to give you some of the ideas behind the proofs of this theorem. And, yes, and actually, the main ideas behind our theorems actually rely on the results of bumal, Boroninski and Bandeira. What they showed is that they provide a very explicit characterization of when is it that the semi ethnic program may have created spurious critical points when we solve it using the bureau montero method. So this theorem here says that the bureau Montero problem is going to have spurious critical points only when a certain condition is satisfied. The condition is that the cost matrix of the SDP has to lie on a certain set. And this set is what I'm calling here, m plus l.
00:22:03.492 - 00:23:11.384, Speaker A: So this is a certain variety, and it's an explicit one. So m consists of all matrices up to a certain rank, and l is the linear span of the matrices that define the SDP. So there's an explicit condition that characterizes, when is it that spurious critical points may exist? And now, importantly, so this theorem also tells us, why is it that the boromonte method. Sorry, why is it that the barbie on Pataky mount is relevant? Because it turns out that when we are, when p is above the Vervinian Pathaki mount, then this set m plus l is going to be a low dimensional set. It will be a variety of low dimensions. And if we have a low dimensional algebraic set, and if I pick now a random point, let's say a random cost matrix, then with probability one, this random cost matrix is not going to rely on such a set, because it's a low dimensional set. And that's the way in which boom, Alvaro and Bandara were able to show that that for a generic cost matrix, the bohemontorov method doesn't have any spurious critical point.
00:23:11.384 - 00:23:58.894, Speaker A: But as I mentioned before, this is not enough to guarantee polynomial time optimality. Because for polynomial time optimality, we have to acknowledge that a numerical solver will never be able to obtain a point that satisfies the criticality conditions exactly. There will always be some mistake that the solver is going to make. So in order to address that, what we do is that we provide a characterization of spurious approximate critical points. And the characterization is actually quite similar to a characterization from before. What we showed is that Espurius approximate critical points may only exist when the cost matrix lies on a tubular neighborhood around the set n plus l. From the previous slide.
00:23:58.894 - 00:24:51.388, Speaker A: What do I mean here by a tubular neighborhood? Well, I mean precisely what you may see on the picture on the right hand side. So we have a set, for instance, a curve, and then we simply thicken such a set by an epsilon amount. That's the epsilon neighbor. And what this theorem tells us is that if we want to understand how often the bohemian method is going to be successful, then we need to understand the volume of such a tube. Now, importantly, it turns out that volumes of two large neighborhoods actually have quite a rich history in differential geometry, and also in algebraic geometry in particular. Here I have two theorems. The theorem on the top gives us an explicit formula for the volume of a tool or neighborhood around a manifold.
00:24:51.388 - 00:25:32.636, Speaker A: So this is a theorem from, from a book of 1939. So this is a relatively old result, and it gives us an explicit formula for the volume of a tubular neighborhood around a manifold in terms of the curvature constants of the manifold. Now, in practice, computing these curvature constants can be complicated, so applying these theorem is not so easy to do. But in the case of algebraic varieties, there is a more explicit formula that it's easier to apply. And that's this theorem that I have here in the bottom. This is much more recent, as you may see here. And what this theorem says is that if I have a variety, there's again going to be some explicit bound on.
00:25:32.636 - 00:26:14.690, Speaker A: Well, in this case, it's on the probability that a random point lies on such a tubular neighbor. And the bound now depends on some very explicit constants, which are in particular, the ambient dimension, the dimension of the variety, and, for instance, the degree of the defining equations. And this second theorem is actually the key for us in order to prove the polynomial time guarantees. So, let me perhaps just give you the main sketch of the proof. So, this is our main result, where we showed a polynomial time optimality of the Wuhr Montero method. And the main idea for this proof is, is. Well, is the following.
00:26:14.690 - 00:27:08.154, Speaker A: So, first, we showed that the boromantor method is able to find an approximate critical point in polynomial time. That's the first step. This is actually not so trivial, but let me skip this part. Now, assuming that we have an approximate critical point, our hope is that such a point will be the global optimum. So what we will try to do is to understand what is the probability that this critical point is not the global optimum, or equivalently, the probability that this point is spurious. Now, since we have a characterization of spurious critical points in terms of tubular neighborhoods, we can upper bound this probability in terms of some tube condition. And finally, using the explicit bound from the previous slide on the volume of a tube, we can find an upper bound on such a probability.
00:27:08.154 - 00:28:12.572, Speaker A: And if we just evaluate the constants, we will figure out that this probability is actually a tiny number, which means that the probability that the point is spurious is going to be a tiny number. And hence the poorer Montau method will find the global optimum with very high probability. That's the main idea behind the proof. So let me perhaps remind you that there was a second theorem regarding feasibility, and it turns out that for feasibility, a very similar analysis holds. So there's also going to be a tube characterization of when is it that there might be spurious critical points for the feasibility problem, for the least square problems I have here on the top. And then we can use this to characterization in order to prove that we can find an approximately feasible point in polynomial time. So we also provided some experiments, some computational experiments, in order to complement our theoretical results.
00:28:12.572 - 00:29:01.444, Speaker A: So let me perhaps briefly talk about this in this first slide. We are trying to illustrate what is the role of this Barbie not pataky bound. And for that we generated a set of random semi definite programs. So we generated some random semi ethnic programs, and we solved these random SDPs using the boromontero method for different values of the parameter p. And what you may see on this plot is the percentage of experiments which are solved to global optimality when we use the Voron Montero method. And what you may notice here is that there's actually a very steep jump when we cross this vertical line. And this vertical line, as you may expect, actually corresponds to the Barbie pataky bound.
00:29:01.444 - 00:29:45.004, Speaker A: And this shows that the Barbie path accurate is something that is not an artifact of the analysis. It's something that actually you can also see in practice. Now let me illustrate a second experiment. And in this experiment we are trying to illustrate what is the role of the random perturbation that we are taking. And for that, what we did was that we considered a semi Devnet program that was actually quite ill behaved. We consider a quite problematic SDP, for which the bureau Montero method was only successful 50% of the time when we tried different random initializations. And then what we tried to do was to add different levels of noise around such an SDP.
00:29:45.004 - 00:30:46.924, Speaker A: And what you may see on this plot is that by adding increasing values of noise, we are able to overcome the inconvenience that we have with this original problem. What this means is that the role of the novice is simply that it allows us to move away from problematic instances. The picture that we have here on the right hand side, it simply compares how are the residuals behaving when we with the number of iterations for the original amper Torp problem and for the Perturb SDP. And what you may notice in this plot is that, is that the residual decay much faster for the perturb problem. So that's really all I wanted to say today. So just to summarize, we proved the first polynomial time guarantees for the borough Mont error methods. Importantly, these guarantees work arbitrarily close to a barbing pataky bound.
00:30:46.924 - 00:30:59.924, Speaker A: And our proof actually relies on very geometric ideas, such as the notion of varieties and tubular neighborhoods around varieties. And that's it. Thanks for your attention.
00:31:01.504 - 00:31:02.056, Speaker C: Great.
00:31:02.160 - 00:31:05.684, Speaker B: Thank you, Diego, for your presentation. Any questions or comments?
00:31:08.764 - 00:31:32.984, Speaker D: I have a quick question on picking the rank here. So, practically, in some instances, you'd want to pick the smallest rank that you could. But is there any advantage to picking even larger than this minimal Barnikov Pataky bound in the sense of you might converge faster? There are fewer bad instances. Is there any advantage to picking a higher rank?
00:31:35.314 - 00:32:31.440, Speaker A: Well, yeah, in practice, typically, you want to choose the smallest frank that you can. And actually, the variable pataky bounties is quite high compared to what people actually use in practice. In practice, people don't even go to that level. In terms of the analysis. It turns out that the probability of, the probability that the point that the method will be successful is actually going to depend on that rank. So, yeah, so this inequality that we have here, there's like some approximation here in the sense that somehow the higher we are from the bound, so the higher that we are from a square root of two times m, the higher the probability that we will be successful. But in any case, for any value that is larger than the square root of two times m, it will be high probability, just that it will be slightly larger if you choose the rank a higher.
00:32:31.440 - 00:32:40.544, Speaker A: So that could be one. That's perhaps one advantage. Does this answer your question? Yes.
00:32:40.584 - 00:32:41.324, Speaker D: Thank you.
00:32:43.504 - 00:32:45.164, Speaker C: Can I ask a question?
00:32:45.544 - 00:32:46.284, Speaker B: Sure.
00:32:47.304 - 00:33:19.908, Speaker C: Yes. So it's about. So, I mean, again, the accuracy that I mentioned. So, I mean, so, in the interpoint method, when we say it's polynomial time, usually, I mean the polynomial time, it's predominantly log of one over epsilon. So epsilon is the accuracy. And the fact that it's log of one over epsilon, not one over epsilon, I think is kind of important here. So is there actually a hopes in sigma, you said, is the accuracy, is it possible to actually improve this sigma inverse to, like, a log one over sigma? Or is there, I mean, is this possible? Or this.
00:33:19.908 - 00:33:21.224, Speaker C: I mean, there is kind of.
00:33:22.084 - 00:33:58.394, Speaker A: I think that it's possible that, that this will be. I think that it's possible, but our current analysis, the ideas that we're using right now, are really not going to lead to such a bound. Ideally, we would really like to have a polynomial dependence on log of epsilon, as you mentioned. But our techniques in particular, the use of these two bounds, which is one of the key parts of the paper. These two bounds are going to have a dependence, which is polynomial in epsilon. So whenever we use this to bounce, we will not have this logarithmic dependence.
00:33:59.374 - 00:34:14.714, Speaker C: And so, maybe another question is. So you have this phase one where you find an approximately feasible point. So here there are no actually constraints. So you apply a factorization again. So I don't worry. Yes. In this one.
00:34:14.714 - 00:34:25.954, Speaker C: No, the previous slide. Yes. So. Okay, so, yeah, so you see in the. In the last display, the question you have, your y is actually of the same size as x, right?
00:34:27.294 - 00:34:35.326, Speaker A: My y. No, it's not of the same size as x. I'm doing exactly the same factorization as before. Y is of size n times p. Okay.
00:34:35.350 - 00:34:38.654, Speaker C: Okay. Because you wrote y is in sn. I mean, I thought it was.
00:34:38.734 - 00:34:44.583, Speaker A: Okay. Okay. Yes. So this is a typo. Thank you. So this y should be of size n times p. Okay.
00:34:44.583 - 00:34:45.015, Speaker A: Yes.
00:34:45.039 - 00:34:55.763, Speaker C: I mean, you don't have. I mean, in SDP, ls the least square, you don't have, actually, any constraints. Right. So. So there are. I mean, this bargain oak pataky kind of argument is not.
00:34:58.503 - 00:34:59.759, Speaker A: Yeah, you're completely right.
00:34:59.791 - 00:34:59.983, Speaker C: So.
00:35:00.023 - 00:35:18.014, Speaker A: So, actually, prior to our work, I don't think that it was known whether there was an optimal solution to this problem that had a small rank, but actually, it is. So there will always be an optimal solution of this problem that has ranked, at most, square root of two times m, where m is the number of squares that you have in this summation.
00:35:18.314 - 00:35:19.174, Speaker B: Okay.
00:35:21.514 - 00:35:22.734, Speaker C: Okay. Thanks a lot.
00:35:23.034 - 00:35:23.894, Speaker A: No problem.
00:35:28.754 - 00:35:37.514, Speaker E: Is there anywhere where you needed an assumption that the original SDP had strict feasibility.
00:35:41.614 - 00:36:04.434, Speaker A: Strict feasibility. I know. I don't think that we are making the assumption. I'm trying to think, but I don't think that we are making the assumption. We do need that there's strict duality. But.
00:36:04.474 - 00:36:09.574, Speaker B: But then how do you know, when you perturb it, that it remains feasible? I mean, that there's a solution?
00:36:12.634 - 00:36:39.264, Speaker A: You're right. You're right. I. Yeah, I think that these results. Well, in particular, these results, assumes that you start with a point which is approximately feasible. So there's. There's some assumption here already that we are assuming that there's an initial point that is approximately feasible.
00:36:39.264 - 00:37:02.884, Speaker A: We are assuming that the perturbe problem has a point that is approximately feasible for this theorem here. But, yeah. No, I don't think that we're making any assumption about the strict visibility of the problem.
00:37:07.144 - 00:37:11.304, Speaker B: So that should be an approximately feasible point of the perturbed problem.
00:37:11.424 - 00:37:18.924, Speaker A: Exactly. It might be infeasible, but as long as there's a point that it's approximately feasible, then that's enough.
00:37:20.624 - 00:37:24.054, Speaker E: But you're perturbing C. That's the objective, right?
00:37:25.314 - 00:37:34.290, Speaker A: Yes. Oh, yes. So here we are, perturbing C. You're right. So we have these two theorems. We have these two theorems in the first. Yeah.
00:37:34.290 - 00:37:54.646, Speaker A: In the optimality, we only perturb C, so that doesn't change feasibility. So in principle, this problem could be. It could be feasible as long as there's an approximately feasible point that. That's enough. Sorry, Papa, because in that case, you.
00:37:54.670 - 00:38:06.794, Speaker B: Shouldn'T expect to get the kind of polynomiality that Hamza was talking about. Right. Because we don't know, actually, if, when you have a semi definite problem which is not strictly feasible, if you can solve it in polynomial time.
00:38:08.534 - 00:38:09.434, Speaker A: I see.
00:38:11.574 - 00:38:12.314, Speaker B: Right.
00:38:14.374 - 00:38:33.846, Speaker A: Yeah. I think that I will probably have to look more carefully about the assumptions that we're making, because, of course, these assumptions are a bit simplified. So I probably have to look more carefully about whether we are, whether our assumptions imply strict visibility. I don't think they do, but I will check. Yeah.
00:38:33.910 - 00:38:45.406, Speaker B: Or let me say the other way. Perhaps if you assume strict feasibility, then you can get the kind of things hans I was talking about, but without assuming it, then I think it would be too much to expect that.
00:38:45.590 - 00:38:57.134, Speaker A: I see. Yeah, maybe. But as I mentioned before, the techniques are not going to allow us to get this poly log epsilon accuracy.
00:38:58.474 - 00:39:13.454, Speaker B: So there's no refined version of those results in those tube results. The variety could be extremely singular. And those results, if variety is nicer, then under, like.
00:39:15.554 - 00:39:21.694, Speaker A: I don't think so. No, I don't think that there are, there are not too many results in this area.
00:39:24.194 - 00:39:24.934, Speaker B: Great.
00:39:26.514 - 00:39:37.454, Speaker E: Can you also, is there anything that you can say about when you can get second order guarantees? What does it mean?
00:39:37.754 - 00:40:21.274, Speaker A: Oh, yeah. So, so, yeah, so what does this mean? So this means that somehow the Bohr Montero method is not a single method, because in principle, we can use any local optimization method to solve the non convex problem. But our polynomial time guarantees are not going to apply to any local optimization method. We know that we need a local optimization method that satisfies some properties, some second order guarantees. And in our paper, we gave an example of one method that satisfies the second order guarantees. We believe that there are probably more, but at the moment, I don't think that there are many results regarding methods with second order guarantees for qCQPs.
00:40:22.534 - 00:40:31.102, Speaker E: So if you have a second order guarantee. That means you have essentially optimality with respect to perturbations.
00:40:31.278 - 00:40:47.454, Speaker A: I think by second order guarantee, I mean that the. Yeah, that the second order. The KKT conditions are satisfied with second order. That the Hessian of the Lagrangia will be positive semidefinite on the tangent space.
