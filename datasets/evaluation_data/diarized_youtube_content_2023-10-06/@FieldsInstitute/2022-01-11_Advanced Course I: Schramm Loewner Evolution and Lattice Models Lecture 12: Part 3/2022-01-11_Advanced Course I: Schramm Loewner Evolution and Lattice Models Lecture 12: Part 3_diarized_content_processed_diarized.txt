00:00:01.800 - 00:01:00.072, Speaker A: Okay, so let me describe the proof of this framework, and it's done in steps. So let me, the first step is we just have an observable in the beginning. Why would it imply convergence of the driving function, and more than that would imply polynomial convergence of the driving function. And for this step, we actually do not use case condition. Case condition campaigns of condition will only be used to prove that second step of the way. So from convergence of driving function to convergence of the interface, again, I talked a lot about multivariable absorbables. If I use them in the proofs, that would be totally incomprehensible.
00:01:00.072 - 00:02:25.410, Speaker A: So let me let h be a one variable observable, and the variable will be inside the domain. Of course it's one variable, it has to be. Okay, so fix a point which is far enough from the boundary, and let wn of t be the drawing function of this command or the curve command. Now, if we combine the fact that the discrete observable is almost martingale absorbable, and that continuous absorbable is close to discrete, we get this exciting result, that expected value of h at time t prime, conditioned on gamma. And notice what I am doing here. I said, okay, I take h at the point gt prime of five w zero. So I look at the evolution of my point, and now I have to shift, because h again assumes that my start of the interface is zero.
00:02:25.410 - 00:03:40.494, Speaker A: So I consider shifted version of jt prime. Again. Remember we had this, but then again, this closeness of the absorbers would lead to the fact that this is close to this up to this correction, and to the minus s correction. Now what do we do? So this is the main technical thing. We look at our observable, so exactly like we did for percylation, we look at our observable, but instead of expanding near infinity, we expand them near this point, w naught. Well, technically speaking, phi of w naught. And here we use the fact that h satisfies this exciting, horrible looking equation.
00:03:40.494 - 00:04:40.736, Speaker A: So let me show it to you again. So this h satisfies this equation. And notice that here we finally have kappa. So we expanded near this point and we get that essentially we have some coefficient a of t, which depends on T but not zero, and some coefficient bt, such that this is expectation of v n of t plus epsilon minus v n of t. And here the expectation of v n of t plus epsilon minus v n of t squared. So, quadratic equation minus k epsilon. Now, if you take two different points, w naught w one, and use non degeneracy condition.
00:04:40.736 - 00:06:37.944, Speaker A: So again, we see that non degeneracy condition essentially says that if you take it at another point, this a of t and b of t, which would depend on the point, they're independent. And so now, since we again allowed to use not degeneracy condition, we see that at least for small epsilon expectation of vn, epsilon minus v, n of t squared is about k. Epsilon expectation of vn of t plus epsilon minus p, n of t is about zero, with correction up to n to the minus s. Okay, so this is all very familiar to what we just did at the first hour. But now, since we want to make this close to b of Kappa T, here comes the tool which would allow us to do it. It's a theorem of skrachotzo with mathematician Antoli Scrachot, which says that if you have any martingale, such that it has bounded jumps. So the martingale which has bounded jumps, then you can find a collection of sloping times for the standard brownian motion, such that this joint collection, so it will just finely minus x would be the same as the law of the brownian motion, stopped at the stopping times.
00:06:37.944 - 00:07:32.846, Speaker A: And more than that, in terms of quadratic variations. Here, expectation of b of Tk plus one minus b of Tk squared, conditioned on the beginning, is the same as expectation of Tk plus one minus tk. And that's very important. TK plus one is bounded by constant. This is equal. That's the first time b of t exits outside of TK. Remember, our is far away from TK.
00:07:32.846 - 00:08:16.834, Speaker A: Remember, the jumps of our martingale is less than delta. So we really couple them close. Okay, now I'll step quite a few steps, but this theorem. So, when you apply this theorem, it gives you existence of coupling between brownian motion and this w and t. Again, because you simply. Okay, so you know that at near each point, you can expand your wn. You know that the jumps, everything is bounded.
00:08:16.834 - 00:09:05.782, Speaker A: Now you just introduce stopping times for w and stopping use cross. And you see that the Prohorov distance is essentially small. So again, this would be granted essentially by this. So this was technical, but relatively easy part. What about the next step? So why would polynomial conversion of driving function imply polynomial convergence? Of course, this is not true in general. So again, let me not give examples. But for arbitrary curves, imagine if your curve is relatively wild.
00:09:05.782 - 00:10:02.098, Speaker A: Even when your driving function is very close, your curve can go back and forth inside itself. So this is essentially the main enemy. So the curve goes back and forth. Your driving function didn't change much, but the curve introduced a new jump inside. So, to be able to control this, you need what is called the tip structure models. And the concept is due to Frederick Wickland, who was already mentioned here. So, let us take a cross cut in omega t of small diameter such that it separates gamma of t from infinity.
00:10:02.098 - 00:11:05.418, Speaker A: So let us do it in the upper half plane, and then, well, in the domains it's about the same. And so let s not would be the last time when our crosscut, the closure of the crosscut, intersect the curve. So this is gamma of t and this would be gamma of S naught. And then we look at the diameter of this curve. So we look at the diameter of the curve which was cut by a crosscut. And then Tip Modulus at delta is the supremum of the diameter of this piece of the curve. For supreme over all C's, this diameter less than delta.
00:11:05.418 - 00:12:01.730, Speaker A: So this is what tip structure model says. Okay, so if you remove Tip from here, this is a very classical construction, which is due to Wachowski. And basically, instead of the curves, he considered locally connected domains. And he looked at the maximum diameter of the piece of locally connected domain, which can be cut by cross cut of diameter delta. So this is called structure, sometimes crossed or remodels. And again, tip structure module says modification of this for the tip, which was produced by Viklund. And then if you have control of the tip structure modulus, you have control on the closeness of the curve.
00:12:01.730 - 00:13:07.324, Speaker A: So more specifically, there is this lemma, which is due to Frederick Wicklund again. And the lemma is the following. So suppose that you have two curve gamma and gamma tilde corresponding driving functions. W tilde, w tilde. And I want here to consider the maps which zip up. So if remember in previous formulations, we can say that maps gt which unzipped ft the maps which zip up for the Leonard chain. And assume that driving terms of the curve are epsilon close and tip modulus at epsilon to the p is about epsilon to some pr, so at some epsilon where the epsilon close.
00:13:07.324 - 00:14:20.874, Speaker A: And then for epsilon to the p, your tip modulus is bounded by epsilon to the primary pr. And suppose also that you know that supreme of the derivative close to the tip. So, okay, so let me draw the picture. So this is your curve, this is ft, this is unwrapped, this is wt, this is comma t wt. So ft maps wt 280. And then when you go by d, so I add d here. Then I want the derivative to have some bound on the derivative.
00:14:20.874 - 00:15:45.004, Speaker A: Okay, and then there exists some c two prime, which depends on beta r p c prime and all these constants such that the curves are close. So both of these rates are polynomial. Okay, so that's the setup that we suppose that we have closeness control on the derivative and tip modulus. Notice one interesting thing here. Control of the derivative happens for map ft for the curve gamma control of the tip happens for the other curve. So if you want to show that two curves are close, we just need to control the derivative here. And we just need to control tip modulus here.
00:15:45.004 - 00:16:52.154, Speaker A: And so how do we use this level? Well, Viklund showed that actually sle kappa always satisfies condition three for all couple as n eight. And so this is even explicit bound. So let me be very specific. What does it mean that it satisfies this condition? Probability that this supreme of the derivative is less than d minus beta is exponentially close to one. So this is what is important, that of course the derivative is random, so the derivative can explode. But the probability that it didn't is really huge. So this large probability for ethyle kappa, that would be our limiting curve.
00:16:52.154 - 00:18:23.264, Speaker A: This is small. So this is true for any kappa. And then part two is this easy observation that actually, well, an observation, let me not call it easy, that companion Smirnov condition actually implies condition two of the theorem that suppose that your family, of course, companies Smirnov's condition. Then probability that the tip modulus is bigger than r, where r is some cr, one plus epsilon is bounded by constant times r to the beta. So this is exactly, exactly this type of estimate. So everything depends here only on the probability companion smell of condition. And so what does it mean? If we combine this estimate of equivalent and our estimate, it means that if family, of course satisfies companions Mernov condition, then probability that the tip modulus is bigger than n minus p u I is big, so that the tip model is large, is small.
00:18:23.264 - 00:19:01.244, Speaker A: And then we can use the lemma. So we know that the driving functions of our discrete interface, gamma tilde would be discrete interface and this are close. We know that the discrete interface satisfies condition two. We know that sla kappa satisfies condition one with high probability. So this high probability, this closeness, would imply the closeness of the curves. So we don't need to recouple anything or to do anything else. It's just automatic.
00:19:01.244 - 00:20:22.582, Speaker A: There we are. In this instance, tip modulus was small, derivative was small, and so we had this. And that's the end of this proof. Okay, so you see that, of course, I skipped a lot of technicalities, but the idea is essentially that you just carefully study what it means for two luvner curves to be close, and then you immediately get sprint. Okay, so I have about half an hour left, and this is definitely not enough for what I plan to do, but I really, really want to touch on gaussian free field. Okay, so for some reason. Okay, so gaussian free field, it is very closely related to SLE.
00:20:22.582 - 00:21:08.798, Speaker A: And maybe I'll mention it at the very end, how it is related. But for now, let me just talk about what it is. It's a beautiful object by itself. So, let me start with the dream, which comes from physics. Suppose that you have so not, suppose they want to start with domain omega, and they want to create a function which is random on this domain, such that for every point in omega, phi of x is gaussian with min zero and expected value of the product. So, correlation of phi of x and phi of y is a green function. So, green function is the usual thing.
00:21:08.798 - 00:22:24.508, Speaker A: It's a fundamental solution of Dirichlet problem. Zero on the boundary delta function at the point x when you take the Laplacian. So, in two dimensions, this is just, you know, logarithmic singularity at the polytechs. Okay, so that's a dream. And why we only need to know pairwise correlations, because of course, remember that we had weeks formula that we want, if we want to know correlation of the values of phi at a few different points. Well, so you take the product and you can easily compute this product by just considering all perfect pairings between the indices here, taking pairwise expectations. Okay, shouldn't forget to take the product and then you take the sum of all such pairings.
00:22:24.508 - 00:23:07.174, Speaker A: And since you already know that this expectation is green function, well, easy to see. And amazingly, this can be realized in one dimension. So, let me again, in the interest of saving time, just say what it is. So what is the green function in one dimension? It's simply for the interval zero one. You want a fundamental solution of just second derivative is equal to zero. So this is second derivative is equal to delta function. The function is this x to the multiplied by y minus y, where x is less or equal than y.
00:23:07.174 - 00:23:40.208, Speaker A: So this is a green function, and then such an object exists. It's brownian bridge. So it's brownian motion conditioned to start at zero and end at one. And, well, here I outlined the construction. Again, let me not dwell too much about on this. So you simply take for example, standard brownian motion. You take it at time x of one minus x, multiply by one minus x, and then it's very easy to compute correlation.
00:23:40.208 - 00:24:20.192, Speaker A: And this is exactly your green function. So there are many ways to do it. So you can also construct it by correcting brownian motion by independent gaussian minus x b of one. That's another way to get the same thing. So, in one dimension, it's a perfectly nice object, just brownian bridge. So here you can think of it as maybe two dimensional three dimensional brownian motion. Oh, well, brownian motion condition, say to be zero in the boundary because, well, that's what green function does.
00:24:20.192 - 00:24:44.884, Speaker A: It zero on the boundary. So this probably implies that this is zero on the boundary. And of course, I wouldn't call it a dream if it was possible. It's not, because look what phi should be. Phi. Let's take correlation with itself. It's green function at the point xx opposite infinity.
00:24:44.884 - 00:25:32.404, Speaker A: So there is no such function. But can we save it? Let's try to see. And there is a beautiful general construction which is called gaussian Hilbert space and which will help us here. So we start with Hilbert space and ovar for now, and let us define gaussian Hilbert space as a map. So, it's a way to index gaussian functions. Gaussian, sorry, distributions. So, we consider a map from Hilbert space to l two, which should satisfy the following condition.
00:25:32.404 - 00:26:32.514, Speaker A: For each element here, phi of h is zero mean gaussian variable, and they correlate as the scalar product. Okay, so this phi is actually isometric. So this gaussian Hilbert space index PI h is actually just an isometry from h to all square integrable variables. Integrable random variables, which only takes values in zero mean gaussian variables. And again, isometry. Now, why does it exist? The construction is very easy. What you do, you take a nurse, a normal basis in h.
00:26:32.514 - 00:27:23.114, Speaker A: You consider countably many independent standard Gaussians, so distributed like standard normal variable. And the map would be the following. You take your age, you take the Fourier series, and you replace your basis elements by Alpha N's. So this adjust. So this would become a Gaussian. Why is it well defined? Because, well, the sum of Gaussians, of finitely many Gaussians as gaussian mean zero. Now, you of course, take an infinite sum, but the variance of the nth partial sum, it's, well, just the partial sum of this.
00:27:23.114 - 00:28:17.162, Speaker A: And now, when you let n go to infinity, this, of course, would converge in l two, because again, this series converges sum of n squared. So the l two norm of this would be just norm of h. So this is isometry. And again, as usual, bipolarization expectation of g, phi of H. It's just color product of H. So this is a way to create isometry from your Hilbert space to l two. And actually this is a very nice isometry if H is finite dimension, because you just take your h phi of H is just sum of alpha K.
00:28:17.162 - 00:29:10.756, Speaker A: Alpha K. So this is h scalar product with this random guy, sum of this alpha kek, right? So this is just, so this is a random element of our space, h capital. Okay, so you, so your phi here in case of finite dimensional h is nothing too exciting. This is just scalar product with some random element of H. And more than that, no, this is a really standard thing to see that this is random variable. It's actually multi dimensional gaussian. So it has density, this with respect to the back measure.
00:29:10.756 - 00:29:44.844, Speaker A: And the density is again, it's just norm of x in h squared, well, exponent over two. So in this ek coordinates, just the thing. Now, in the infinite dimensional case, in physics literature, they also write this, but this is total nonsense. Okay, I'm also on the record. No, it's not nonsense. It's physical intuition. Let me explain what's wrong here.
00:29:44.844 - 00:30:49.398, Speaker A: So suppose that now you want to do the same thing in infinite dimensional space. So you want to say that now, infinite sum of this is just the product of h with the random element of your space. The thing is that this random element never belongs to H. Well, never, almost surely, because you look at the l two norm of this, that would be sum of alpha n squared. And the sum diverges almost surely, because again, the probability that alpha n is bigger than one, it's some constant. So you now, some of these probabilities is infinite. So if you use barrel cantele with independent guys, this guy is bigger than one infinitely often.
00:30:49.398 - 00:31:29.758, Speaker A: So this of course diverges. So now this is something which took me, I remember some time to observe. I always learned that functionals on Hilbert space are elements of Hilbert space. But random functions of Hilbert space are not random elements of Hilbert space, amazingly. So, phi is not an element. So we just saw it. Phi is almost surely not an element of your Hilbert space.
00:31:29.758 - 00:32:25.254, Speaker A: So it's just something. And for now, this is just functional on your space, which is defined by this, a random function. Okay? So now let us consider very, very specific situation. So let us pick a domain, let it be bounded so that nothing is too exciting. And let us look at this obelisk space on this domain with zero boundary values or w zero one two. So this is closure in two smoothly, compactly supported functions with respect to just the norm, which is richly energy. So you just integrate gradient of f squared.
00:32:25.254 - 00:33:19.014, Speaker A: So this is called Dirichlet space. And now one thing which we want is the following, the product of the elements of the space. Well, this is just integral of the dot product of the gradients. By Green's formula, this is minus f laplacian of g or minus laplacian of f g. So this is minus f delta G, where this is a normal scalar product. So this is just l two's color product of the functions f and g. And so this is Dirichlet space.
00:33:19.014 - 00:34:49.347, Speaker A: And this observation actually would help us to understand why Gaussian Hilbert space with respect to it is Gaussian Freefield as dreamed by physicists. But first, observe that this is a conformal invariant object, because, well, it's an easy computation that if you have a conformal map between the two things, then the gradients, of course, they are both multiplied by phi prime. So we just, the adjacent variant notation which I would use for this node to write w twelve is h of omega. Okay, so now the definition. So for now, this would be gaussian free field on omega with zero boundary values is the gaussian Hilbert space indexed by this h of omega. So it's simply a random functional which for each h in h of omega produces this product. And as usual, this phi, as we just proved, does not belong to H.
00:34:49.347 - 00:35:52.284, Speaker A: So this is just some functional which we for now write as the sum in this basis. So formula and this phi, again, it's just a gaussian free space. It is conformable. So this is rigorously defined object. It's conformally invariant, since h is conformally invariant. So if we look at this, find omega one, then it's just phi in omega two composed with phi again, because everything in the definition of the scalar products is conforming invariant. Now, what is this phi? It's not an element of h of omega, but what is it? What is the sum? So let us consider an example.
00:35:52.284 - 00:37:08.130, Speaker A: Let's look at the square, and if we take this basis sine, mx, cosine and y, then phi is this sum. So this guy is a Gaussian with variance one over m squared plus n squared, the sum diverges. So phi cannot be a function, right? So, but we knew that if you wanted to have something like gaussian free field, it cannot be a function but if you multiply by any small power of m squared plus n squared, any small negative power, then the sum suddenly converges. So now, okay, let me be quick here. Antiderivative for phi of any order is alto function. So if you look at the, again, Fourier transform. So, you know, you take the Fourier transform, you take antiderivative of any order, fractional order, it's a function.
00:37:08.130 - 00:37:51.254, Speaker A: So phi actually belongs to ws two for any negative s. So it barely stops being a function. So it's almost a function. It's very nice distribution. But this, of course, we proved it only for the square. But of course, then by conformal invariants, in any simply connected domain, this would still be true, because both classes again are conformally invariant. Well, and in any domain we can should add locally.
00:37:51.254 - 00:38:47.184, Speaker A: Now, this was kind of unsatisfactory definition, because eventually, of course, well, as mathematicians, we probably want to have a functional which acts on test functions. We want to have a random distribution. So is JFf a random distribution? Of course it is. But what sort of random distribution? So let's see, let us take any test function, and we want to compute phi on rho. So how would phi act on rho? Okay, let's do this. Let's introduce green potential. So this is usual green potential.
00:38:47.184 - 00:39:50.314, Speaker A: Take your green function, you integrate it with the weight rho, and this, this is green potential, Laplacian of, this is minus rho of x. Again, this is why, because this is green function, fundamental solution, flatwash. And it always belongs to h of omega, and this zero belongs to h omega with the following norm. So you take this norm, and now you use green function, so zero delta zero. So this is zero rho. So this is simply green's energy, j of x y rho of x rho, five dx dy. So the norm of this green functional in your Dirichlet space is green energy.
00:39:50.314 - 00:41:15.144, Speaker A: Okay, now define phi on delta h to be phi h with respect to this color norm. So, okay, here we remember that this, this is a functional random function on Dirichlet space. For a laplacian of a function from Dirichlet space, phi can be defined as this. So what does it mean? What is phi rho? Rho has derivative, so rho has anti Laplacian, so to say, green potential of rho. So phi acts in conjunction. So this is a well defined object, again, because Geofroy belongs to directional space. That would be our phi axis of row.
00:41:15.144 - 00:42:22.074, Speaker A: So that's the way we can think of phi is a random distribution. So for every row, this phi row is a normal distribution, mean zero and variance of this random distribution of this normal distribution is simply green energy. So let me actually correct it. It's not green potential, it's green energy. And now let's look at the covariance. Well, this is just, we know that this is, this thing preserves scalar product, but between what? Between zero one and zero two in the richness space. And the same calculation shows that this is just integral of j of x y rho one of x rho two.
00:42:22.074 - 00:42:57.030, Speaker A: And finally we come to this. So I want to show you that this is again realization of physics dream. And for a second imagine that phi is a function. What would be then expectation of phi rho one, phi rho two. Well, this expectation of integral phi of x rho one of x. Expectation integral of phi of phi rho two of phi. Now useful beanie.
00:42:57.030 - 00:43:45.054, Speaker A: So to say this is just expectation of phi of x five rho one of x rho two of phi. Aha. So in the sense of distributions, namely in this sense, this expectation of fire one, fire two is this. So in the sense of distributions, g of x y is indeed expectation phi of x phi. So we just compare this and this. But of course phi is much more than the distribution. So suppose for example that mu is a measure with finite green energy.
00:43:45.054 - 00:44:38.624, Speaker A: Then you can integrate phi with respect to mu. Again, a five year function. This would be the honest integral, but here it would be just the Dirichlet scalar product of phi and gmo. So this would be again normal variable, this variance, which is green energy of the measure. So you can integrate five with respect to any measure of finite energy x. So for example, you can, in two dimensional case, you can take the average of phi over any interval, over any circle. That's a well defined object.
00:44:38.624 - 00:45:51.924, Speaker A: We cannot evaluate at a point, because that would mean integrating five with respect to delta measure, which has infinite energy. Okay, so now I am running out of time. So let me try to quickly go through this. What we can do, we can also restrict our phi two subdomain. So how to do it? Suppose that omega prime is subdomain. Then you can write Dirichlet space as Dirichlet space and omega prime plus what is orthogonal to the Dirichlet space of omega prime. What is this orthogonal? Well, if f is the nature toggle, then easy computation using Laplacians and interchanges just shows that f actually has to be harmonic function in the domain omega prime.
00:45:51.924 - 00:47:01.972, Speaker A: So h of omega prime are all f in h omega such that f is harmonic and omega prime. So we can do this decomposition and then, well, so, well, we can easily see what is the projection, for example, to this orthogonal from h. You just take h outside of the domain omega prime and then extend it inside as a harmonic function. And projection to h omega prime would be simply h minus this harmonic extension. Now, what does it mean? We can take the basis of h of omega to be the first. We take the basis of h omega prime, then we take the basis of the complement. This would mean that phi Omega would split into two things, phi omega prime and phi omega prime orthogonal, which is created from this.
00:47:01.972 - 00:48:13.364, Speaker A: And so phi omega prime of h would be just phi omega applied to h minus h tilde and orthogonal one would be just applied to h tilde. And these two, they are independent because we took the, to create this guy, we took bases on one direct summon and took independent random variables there, Gaussians in the other, we took the other ones. And more than that, an easy calculation shows that. Remember I told you that this is distribution, but this is a very good distribution in the domain omega prime. This orthogonal thing is actually a random harmonic function because its distribution Laplacian is zero. So if you take any rho in c infinity, then since we have this distribution Laplacian of this would be zero. So essentially, well, we started with phi having zero boundary conditions.
00:48:13.364 - 00:48:58.514, Speaker A: But this shows you that actually, when we restrict to a domain, what happens? Our gaussian free field becomes the following. You take whatever happened outside, you extend it to the inside, and we add an independent copy. And so that's how you in general, extend gaussian free field. Gaussian free field. This boundary data in h is simply your gaussian free field. Zero boundary condition, plus this extension, plus harmonic extension of this boundary data. That's all.
00:48:58.514 - 00:50:25.634, Speaker A: And then domain Markov property, which we just observed, is that if you are given phi omega in the complement of domain omega prime, then phi omega is the sum of this psi. So what this and harmonic extension of sine omega prime and an independent copy. So, independent of this harmonic extension, copy of gaussian free field in omega prime. Okay, so now in the remaining negative minutes, more or less, let me quickly say. Well, one way we can observe SLE couples in gaussian free fields. So this is beginning of the beautiful theory developed by Scott Scheffeld first, and then Schaffelt, Jason Miller, and there are many other great people working on this. But just to show you the connection, suppose that you do the following amazing thing.
00:50:25.634 - 00:50:59.642, Speaker A: You run your gaussian free field with this following boundary conditions. You take two prime ends, ab. You put boundary condition lambda naught here and minus lambda naught here. That's a gaussian free field with this boundary condition. Now, you run sle four from a to b, and you do the same. You put lambda naught here, minus lambda naught here. You have two domains, you have two gaussian free fields.
00:50:59.642 - 00:51:37.734, Speaker A: And you just say, okay, so let now our gaussian free field be this gaussian free field here, left gaussian free field, and right gaussian free field here. Then the distribution of this triple object. So you take triple sla four, and then these two gaussian free fields. That's exactly a regional gaussian free field. So, again, this is the easiest way to observe. There are much stronger connections. More than that, if you introduce winding.
00:51:37.734 - 00:52:20.914, Speaker A: Let me not talk about it. Everything would be written in the nodes, which I will put online. Then you can observe any Sla cap this way. And again, it goes much, much further. And it's probably more natural to actually define SLA using gaussian free field. Okay, so, let me finish the course on this note, of course covered about a sort of what I plan to cover this term. But so, some of the material which I didn't cover will be in the notes, which will be on the course webpage.
00:52:20.914 - 00:52:26.134, Speaker A: And for now, let me stop recording and thank you for your attention.
