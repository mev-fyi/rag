00:00:00.960 - 00:00:49.954, Speaker A: And the same for the phagoc. And we have some oscillation, I mean, this one is, I mean, at least on the zero parts, it's decreasing, it's a monotone function, but this one, it's an oscillating. You remember we had sine of n plus one t over two in denominator. And sine says what we why we have so many oscillations here. But despite having oscillation, we have the same phenomena. If we fix a data, a delta, I mean, outside delta, the part plays no role and the function concentrates around the origin. And all the kernels are like that.
00:00:49.954 - 00:02:05.046, Speaker A: And this is a manifestation of what we wrote here. For the strong sense l infinity case, or I mean, a bit weaker previously, with l one norm, most of the kernels satisfy this strong evasion. And now let's go to what we promise. This is the DRM that we saw construction of h one function. I mean, you have a measure, you construct something in the interior of disk and it satisfies the properties we saw. And now we go for a specific cases, various convergence. This is the interesting part, because, well, the measure here, the measure that we put to define u or pr stubule, can be a function in l one, and more specifically can be in lp or in l infinity.
00:02:05.046 - 00:02:55.058, Speaker A: In each case, when we put more and more restriction, we should be able to say more about the function we obtain. And this is precisely explain the title. Why do we have various convergence modes? Let's start with the best one, and in a sense, the smallest class, if the function is continuous on t. So I emphasized yesterday, continuous function is the smallest class that we can see there. And then you do the convolution with an approximate identity. This can be either the previous two cases, or in any general case. What do we obtain? Several properties.
00:02:55.058 - 00:04:01.098, Speaker A: First, phi k star f stays in this space, is a continuous function. Second, is norm is controlled. The norm of v star f, nor infinity is controlled by the norm of f for all k. And finally, it goes to the normal f in l infinity. Now note that this one, I mean phi f goes to f, does not imply directly the first one. Of course we can say that the normal phi k star f is bounded, but such that uniformly bounded by a constant time f is something more important, and it's not immediately a consequence of this. So we have this convergence just for this theorem, the uniform convergence.
00:04:01.098 - 00:04:47.084, Speaker A: I give the detail of the proof, but for the one which comes afterward, it's the same philosophy, the same trend, and I mean, I go rapidly for that. So let's see, one by one, why do we have such properties? This is a theorem which is proved before that if a function is continuous, the convolution is continuous too. It's a part of young inequality. In the young inequality, we saw that if one of them is in Lp, the other is in Lq, then the function is defined everywhere, and also it is continuous. So this is something which is done before. But for the norm. Again, this is a version, this one, this is a version of young inequality.
00:04:47.084 - 00:05:43.434, Speaker A: Phi K star f norm infinity is bounded by norm of k in L1, and norm of f in infinity. And now you remember the second assumption. You see why we put that assumption. This less than or equal to c is precisely the condition two that we put on the approximate identities. The norm of k in L1 is bounded by c, and so we obtain what we want here and now. The proof of this is really the essence of theorem, and it has a nice ingredient in it. Why do we have the uniform convergence of phi k star f to f? First, this is one of the result in analysis.
00:05:43.434 - 00:06:26.344, Speaker A: One maybe. If a function is continuous on t compact set, it is automatically uniformly continuous. And uniformly means for every epsilon, there is a delta, which just depends on epsilon, such that this difference is small. F of e to the it, minus f of e to the is provided t and s are close to each other. And you see the delta appears here. This is the same delta that we put as condition three or three prime in our definition of approximate identities. Let's see how we use this.
00:06:26.344 - 00:06:57.804, Speaker A: Well, phi k f at point e to the it. That's the definition. This one and this one. If you multiply them and integrate, you obtain this. Where does f to the e to the it come from? I mean, this doesn't depend on the variable toe. And you see integral of phi k with respect to toe d two is equal to one. That's our first condition.
00:06:57.804 - 00:07:53.760, Speaker A: The first condition of identity says, the approximate identity says that integral from zero to PI, phi k dt, d toe is one. So it's one times f to the it. And we have it here. So it's a trick to move f to the it inside. And now we have this combination. So having this combination, and you know that integral from zero to two PI is the same as integral from minus PI to PI, because the function is two PI periodic. And now we decompose our interval into three parts from minus PI to minus delta.
00:07:53.760 - 00:09:08.834, Speaker A: And delta to PI is one of cases we consider. And on the minus delta, delta is another one. So that is why I wrote this integral this way. And then what can we say about the integral from minus delta to delta of this combination? Well, if tau is between minus delta and delta, the difference between these two arguments is less than delta, t minus s is less than delta, and therefore the difference of f's is less than epsilon. So if this is the case, just for the second integral, the one in, in the middle, this combination is less than epsilon. So we end up with epsilon integral from minus delta to delta, absolute value of vk for the second integral. And now the job of delta is done.
00:09:08.834 - 00:10:16.670, Speaker A: Be a bit generous and replace delta by p. And why being generous here is not dangerous, because integral from minus p to p of e k by the second assumption is less than c. So the whole thing is majorized by c epsilon, which I wrote here. That's how to deal with the part from minus delta to delta, what to do with the rest, what to do with this part and this part? Well, I wrote it this way. And note that now we are generous. For the other part, it means that. It means that now I replace this part by the biggest possible triangle inequality.
00:10:16.670 - 00:11:06.012, Speaker A: I replace it by normal f plus normal f, two y's, two divided by two. It gives me normal f divided by p, and the remaining part is the integral of phi k over the remaining interval from delta to PI and its mirror image. And now we use the assumption three or three prime assumption. This is assumption three. It says that this part is small, so the whole thing will be epsilon times this coefficient. If you put them all together, we have the difference is bounded by epsilon times a constant. And since epsilon is arbitrary, works for all t.
00:11:06.012 - 00:11:52.164, Speaker A: We obtain what we wanted. This is a technique that appears in analysis many, many times. You have what we have here. What we have here is the integral or the summation even for the sum appearance of one function times another one. And over interval of the integration, we can divide it into some parts. For example, in the first one here, f is small and on the second part, g is small. This is precisely what we did here.
00:11:52.164 - 00:12:46.646, Speaker A: On the interval minus delta to delta, this combination is small and the other one is bounded on the complement from minus PI to minus delta and delta to PI phi k is a small and the other part is bounded. So, this is a general technique that we have applied many, many times. So this section, all the time is like this. We have a general theorem like this for an approximate identity. And immediately after that there are at least two. Corollary, because we can replace this phi by Poisson kernel or by Feyre kernel, at least. So that is why here we immediately have two corollaries.
00:12:46.646 - 00:13:29.354, Speaker A: The first one is the theorem, the same theorem. We replaced phi by the Poisson kernel. So u is defined by this formula on the disk and on the boundary, I mean, is the small u that we have before. And the conclusion, u is continuous on d bar. I mean, this is precisely what we did here. When we say that phi r star f or pr f goes to f, this means precisely what I wrote here is continuous on d bar is harmonic. We saw this before.
00:13:29.354 - 00:14:05.898, Speaker A: And the norm of u on each circle is less than the norm of u. This is because of this part here, c is equal to one. Remember this all the time for failure. And for Poisson kernel, c is equal to one. So the norm of pr star f is less than or equal to the normal f. This is what is written here. And finally, ur goes to u in l infinity norm.
00:14:05.898 - 00:14:44.542, Speaker A: Indeed, number four and number one are the same thing. Said differently, they are the same even though the appearance is different. And also, we saw this. This, I mean, I able Poisson means a lower dimension. That's the theorem applied to the Poisson kernel. And now the same theorem applies to Feyer kernel. And we obtained this very nice formula by Feyer, already obtained in different form by Weierstrass.
00:14:44.542 - 00:15:27.228, Speaker A: Weierstrass theorem is an existence theorem, which means the polynomials are dense. But give me the polynomial. And here is the polynomial constructed explicitly by failure. Pn is the convolution. This is the convolution of kn and f. So the formula we have about in here, this formula, phi k star f. If instead of v k, you put the failure kernel, you obtain this combination, and pn goes to f uniformly.
00:15:27.228 - 00:15:56.124, Speaker A: That's uniform approximation. Pn is bounded in l infinity norm by f. This we know. And also the limb goes to f is a consequence of this. The important part is this one. And indeed, in some application, we precisely need this, that pn in as in norm is bounded by f. It's better to put it this way.
00:15:56.124 - 00:17:10.828, Speaker A: This is very important. Before going to the next type of convergence, weakestar convergence, I would like to make a comment, comment about this. I mean, Weierstrass theorem is, I mean, very old to 19th century, when he proved that polynomials are dense in the space of continuous function, then failures around. I mean, if I'm not mistaken, it was 19 oh 419 oh five when he presented his construction kernel, which gives us explicitly foreign, so more than 100 years ago. And so we obtained this, and then, I mean, about two decades later, Hardy and Littlewood, they work on polynomial approximation in hardy spaces in Hp. We will see it today. I mean, everything works well, but in h one we need these polynomials.
00:17:10.828 - 00:17:52.088, Speaker A: These polynomials converge in s. One, which means failure means. So everything goes back to, I mean, 80 to 100 years ago. Sounds a very old subject. It's interesting to know that for Derrick Les spaces, the fact that these polynomials are convergent and go to the function they were approved just two years ago. I mean, it's published last year. So that's, I think, an interesting historical thing to know that the topic is still active.
00:17:52.088 - 00:18:58.274, Speaker A: And if you're interested, there are open problems on this. For example, for Dobron wolfniacke spaces, which we absolutely do not touch it here, but there will be a session a week on this topic, model spaces and Dopplerskovniac spaces. Some aspect of polynomial approximation is still open for such spaces. Some comment and now forget about it. We go to wicked star convergence. We should not expect something strong like this for all functions or measures, because we saw, I mean, analysis one many, many years ago, when we had this course, that if you have a sequence of continuous function which goes uniformly to another function, the target is also continuous. Therefore, in a sense, this is a necessary and sufficient condition.
00:18:58.274 - 00:19:51.810, Speaker A: So if phi is continuous, we have uniform convergence. That's very good. If we have uniform convergence, it is continuous through the end of story. Now, what happens if we remove continuity and put something else, for example, f being in one of the Lp spaces, which comes after this part, or even more, we consider a measure. So when we enlarge our space, we should expect less and expect less, is summarized here. If you have an approximate identity and you make the convolution with a measure v k star mu, well, one good thing is that the measure you obtain is absolutely continuous. You have, you stay in this space l one.
00:19:51.810 - 00:20:39.754, Speaker A: That's one thing. Still, you have some good properties. First, one, uniform boundedness of the norm, I mean phi, k star mu, they are norm bounded from above, and also in this sense, from below. Also, they are controlled by the norm of mean. And instead of the uniform convergence, we have this weakest star convergence, this one. The technical word is hidden here. The measures muk converge to d mu in the weakest star topology of m of t.
00:20:39.754 - 00:21:39.882, Speaker A: What does this mean? Convergence in weakest r topology is precisely what is written here. Remember, m is the dual of c. So weakest star convergence means that for any continuous function phi, if you multiply phi by p k star mu and integrate, and then take limit with respect to k, you recover the integral of feed emu. That's the meaning of weaker star convergence. Yeah, and a bit more, which is a consequence of this convergence. The supremum and the limit of star mu is the same and is the same as the norm of mu, which I mean indeed implies this condition too. So we assumed less, we obtained less.
00:21:39.882 - 00:22:50.074, Speaker A: As I said, I don't go into the detail of the proof, but as you see, to show that it's norm bounded, we again use a version of young inequality, and the fact that norm of k in one is bounded by c is one of our assumptions to show the convergence with star convergence. And now there is an explanation why I have this ordering uniform convergence first. Then we can start convergence of measure, because I need the previous one here. It's like to prove in analysis one to prove the intermediate value theorem. Sometimes we prove the Boltzano theorem first, which means that if a function is positive, somewhere negative, somewhere, it's zero in between a continuous one. And then we go with the generalization intermediate value theorem step by step. This is the same story here.
00:22:50.074 - 00:24:00.384, Speaker A: There is a reason I started with uniform convergence at the very beginning, because in the next step, when I prove weakest or convergence, I need that result. And need I have phi here, which is continuous. What is the integral of phi with phi k star mu? I mean, these are some simple justification. I mean, by Fubini theorem, if you do the integration, you obtain the integral of v k star psi, where psi is almost equal to fever is just, I mean, a reflection with respect to the origin. That is because in the definition of convolution, we have t theta minus t, not theta plus t. Theta minus t forces us to, I mean, consider such a function, psi instead of phi. So psi is pretty much equal to phi with just one reflection.
00:24:00.384 - 00:24:59.084, Speaker A: And what we obtain at the end of the day for this is the convolution of phi k with psi again at point e to the minus PI, t really doesn't matter. This is just technicality here. What is important is that based on the previous calculation, previous convergence, moon phi k star psi goes to psi uniformly on t at every point. And therefore, when we take limit limit here, we can take limit inside and replace this by psi. So this identity is by the first part, uniform convergence, and then we are done. This part is the other parts that, for example, the integral is bounded by suprema momma. All of these are elementary calculation.
00:24:59.084 - 00:26:02.270, Speaker A: The main message is here. So we assumed less. We obtained less again, I mean, in all cases we have two spatial case. What if we apply the Poisson kernel, we obtain a function which is harmonic l, one norm is bounded and we have convergence. As before, we can start convergence for every continuous phi integral of phi with ur is equal to integral of phi when r goes to one. Before jumping to the next corollary, which is this application of our theorem with failure kernel. You saw two other corollaries, and because they are simple consequences of this one and they imply important result in harmonic analysis, both of them are type of uniqueness theorem.
00:26:02.270 - 00:26:57.174, Speaker A: The first one, if you have a measure mu such that it's integral with respect to the Poisson kernel, is equal to zero, then mu is identically equal to zero. Of course the other way around is trivial, but why? Why it should be this way? Let me see. There is a question in chat. Yes, I will, I will share this PDF file with everybody after the lectures. And the reason is precisely the corollary. When we say that this combination is equal to zero, this means that u is equal to zero. The harmonic function is identically equal to zero.
00:26:57.174 - 00:28:03.424, Speaker A: And then when the harmonic function is identically equal to zero, the left side here is also identically equal to zero. So integral of phi d mu is zero for all phi, and therefore, by again, back to this theorem, mu has to be zero. So that's one uniqueness theorem. The second one, which is more popular, is that if you have a mu and all the Fourier coefficient of mu are zero, then mu is equal to zero. And again, this is a corollary of the corollary, because when you develop this, this is our u, u is equal to the summation of u hat r to the absolute value of n e to the I n theta. That's another formula for u that we saw before. And if all the coefficients are zero, u is equal to zero identically on d, and we are in the same situation as before.
00:28:03.424 - 00:29:00.594, Speaker A: So mu has to be zero. So, uh, this general theorem, I mean, in a sense looks a bit dry. And yes, what can we get out of this? But as you see, if you apply it to a special case, I mean, you obtain something still you have the feeling that maybe again, this is something dry. So what? And immediately after that you recover important uniqueness result from harmonic analysis. So that's, I mean, some levels of abstraction that I mean, gives us nice result. And the same for with failure kernel, this is convolution with failure kernel. And we have the same result.
00:29:00.594 - 00:29:31.934, Speaker A: So we did two cases by now, uniform convergence and weakest or convergence of measure. So, in the first case, we started with a continuous function on the boundary. In the second case, with a measure. Now again, we back to something similar to the first case. We consider a function on the boundary, but the function is not continuous. This is in Lp. We consider a function in Lp, and then we do the convolution.
00:29:31.934 - 00:30:33.650, Speaker A: What do we obtain? The same techniques tells us that the function stays in Lp. Lp norm is uniformly bounded. And then, more importantly, we have convergence in Lp, norm vks f goes to f in Lp, nor, and the supremum is the same as limit is the same as the, the normal function. So the main message is here, but the other parts are important. And if you look at the proof, is very similar, the proof that we had before. And again, we use part one, the corollary that we had in part one. Every function can be approximated by Nlp by continuous function.
00:30:33.650 - 00:31:24.346, Speaker A: So instead of f, replace it by phi. So this is a very standard technique in analysis. You replace f by f minus phi plus phi. Why we do this? Because in a sense, f minus phi is small, and we then we treat phi differently. If you do this, you have some convolutions with f minus phi and some conclusions, and some part just with phi. So this is just with phi and the others with f minus phi. The others with f minus phi can be majorized by the normal f minus v, which is small.
00:31:24.346 - 00:32:06.762, Speaker A: That's the way phi is chosen. And to deal with convolution with phi, we replace it even by something bigger, replace norm p by norm infinity. That is okay, because by part one, we know that this is small, because phi is continuous. And therefore, I mean, everything works well, and we obtain the required results. So that's another part of it. Again, to a special case, if the function is in lp, we use the Poisson kernel. We obtain a harmonic function whose piece norm are bounded.
00:32:06.762 - 00:33:19.590, Speaker A: In our technical language, this means that the function is in little hp, and we have the norm convergence. And if you apply the failure kernel, then you obtain a polynomial, and the polynomial convergence in lp norm, proving that, for example, in the analytic Hardy spaces, proving that Taylor polynomials convergence, it's not that easy. We need master risk theorem. But proving that failure averages are convergent, I mean, based on the theory that we developed by now is easy. I mean, now it's easy. Based on corollary 13, we can show that they converge to the original function. Uh, consequence of this corollary, it's a Riemann Lobe lemma, which I mentioned in one of the previous parts.
00:33:19.590 - 00:34:03.304, Speaker A: If f is in l one, then f hat goes to zero. When n goes to plus or minus infinity. I promise to give a proof of this. The proof is based on this corollary. Given f, we can approximate f by one of its failure means pn zero. N zero is fixed such that the difference is less than optimum. And now, what can we say about f hat? I mean, the technique which even in the previous page I mentioned, we write f equal to f minus p and zero plus p and zero.
00:34:03.304 - 00:34:53.333, Speaker A: Why we do this? Because then we can take the hat. So f hat is hat like this for any n. And note that if n been written somewhere, it's here. If n in absolute value is bigger than n zero, the spectrum of pn zero is zero. I mean pn zero is from minus n zero up to n zero. So beyond that is identically equal to zero. So we can write f minus p and zero hat if n is bigger than zero.
00:34:53.333 - 00:35:46.774, Speaker A: And then if we take absolute value is less than the norm and the norm, we know that it's less than epsilon. That's the way p and zero is chosen. And well, we are done. F of n hat f hat of n is equal to hat of this at the point of n, which is less than epsilon, and we are done. I think there is a question I mentioned at the beginning. This is not a book. This is a PDF file that I will share with you, but the PDF file is a modification of some result in the blue book representation theorem in hardened spaces.
00:35:46.774 - 00:36:31.978, Speaker A: Well, in the ten minutes remaining, this is the last one. Let's see what we have considered by now. Continuous function then measures and then lp. The reason that we do ping pong and we go from one to another, because in each step we need the result from previous steps. And the last one is weakest convergence of l infinity function l infinity functions, I mean, are good in a sense, a small class with respect to the other cases. So we should get something better. But still we cannot get the best one, which means uniform convergence.
00:36:31.978 - 00:37:36.278, Speaker A: And the reason is that in l one there are functions which I mean, are bounded, but like not continuous. So we should not expect uniform convergence. However, we have this nice result. If f is in l infinity, pk f is more than being bounded, is continuous, and the norm is uniformly bound it, and in this sense also the supremum is bound, is controlled by the normal f. And then for the convergence, the technical wording is here. V k star f converges to f in the wicked star topology of l infinity. What does that mean if we consider our space l one as the first space is dual x star is l infinity.
00:37:36.278 - 00:38:26.534, Speaker A: That is one of the risk theorems. So we can look at l infinity as a dual of l one. So on l infinity there is a weakest star topology. So we can, when we consider x double star l one sits there and l one, put a topology on l infinity which is called weakest our topology. And with this topology we have the convergence, which is that. I mean, we use a lot of technical words, but everything is summarized here. Integral of phi phi f goes to integral of phi f for all phi in l one, phi in l one.
00:38:26.534 - 00:39:55.730, Speaker A: That's the explicit meaning of weakest or convergence on l infinity. In this case it is important to emphasize on the word weak star because on l infinity, weak topology and weak star topology are not the same, but in lp between one and infinity, even though in the previous parts I said weakest our convergence, on that case weakest our topology is the same as weak topology. They coincide, but here they are different. Sheldon also mentioned a nice reference of chapter eleven of the book on the book. Sheldon, after my talk, could you please share the picture of the book so that everybody can see that the reference is on the chat and we try to share with you the front cover of the book after my talk, I mean about 510 minutes. Thank you Sheldon. The proof again, the same technique.
00:39:55.730 - 00:40:43.910, Speaker A: I mean, we fix phi in l one, we do the usual Fubini theorem and we end up with the integral of Phi K star with psi. Psi is pretty much the same as phi and Phi k star psi. And you see in this part you need the result about l one convergence or generally lp convergence that we had before. That is why this part came at the end. The last one we saw before that psi goes to psi in l one more. And that is why here when we take the limit, we can take the limit inside and we obtain what we want. So that's the steps.
00:40:43.910 - 00:41:30.992, Speaker A: We took four steps and in each step we needed the result of the previous case to finish as before. Two special cases. If u is bounded and we do this, then we obtain a harmonic function which is bounded, uniformly bounded. And then the convergence of ur to the boundary is in the weakest topology of l infinity, which is summarized by this identity here. Or I mean these are application of the theorem to the fair kernel. The same story. I added a section here about l two convergence.
00:41:30.992 - 00:42:38.714, Speaker A: Two is always different because it gives us a Hilbert structure and we can say more. L two here is not an exception. There are many interesting results that we can prove for l two norm, which are not valid if we replace two by pieces. The usual definition of inner product in l two, and the first important thing is Bessel's inequality, which I mean in the orthonormal case is summarized as this. In the general case, we can have a constant here, but here the constant is one, because we have an orthogonal family. The proof is very simple and is based on this identity. If you define g as the projection of f on that space, we have the pythagorean theorem.
00:42:38.714 - 00:43:21.144, Speaker A: The normal f squared is the norm of f minus g plus the norm of the projection. This is on the other side, but I put it on this side. And then, since the norm is, is always positive, you obtain the result here. If you are worried about convergence, which I mean, is not the case here. But if you're worried, you can consider finite sum, and then let n go to infinity for the case of e to the int. If you apply Bessel inequality, we obtain and this one. But that's not the whole truth.
00:43:21.144 - 00:44:06.264, Speaker A: In fact, this is true, there is no mistake in it. But it's more than that. We have identity here, and this is the content of two important theorems, which I merged them together. The first one is by Lysfischer, which basically says that little l two and capital l two are the same thing. The magic words that I used yesterday, isomorphic isomorphism. And then we have this identity, which is due to parcel wall. This part is for the isometric part, and this one is for the isomorphism, which is between little l two and capital l two.
00:44:06.264 - 00:44:53.422, Speaker A: This theorem, this is the point I mentioned. You cannot replace two by p here. This doesn't work for other values of p. If you put integral of f exponent p, you do not obtain f hat exponent p. And this gives us different theory. One is, you can follow with this definition, you obtain hardy spaces, the usual one, which we study. But if you adopt this definition, it's another class of spaces we shall call the lp.
00:44:53.422 - 00:45:39.966, Speaker A: And sometimes, if you consider analytic LPA spaces, they are not the same, these are different objects. Well, it's pretty much the same. This completes half of representation theorem, which was, which were in my mind, I mean, the one that we start on the boundary, we go inside and we have some properties. Tomorrow is the second part. We start from inside and go to the boundary to see what happens. And when we put them together, the first part and the second part, we obtain a complete picture. Thank you very much.
00:45:39.966 - 00:45:46.694, Speaker A: This is the end of my talk today. Shalom. Can.
