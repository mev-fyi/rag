00:00:01.480 - 00:00:48.970, Speaker A: Thank you very much for the invitation, and I also want to take a few seconds to thank you. Well, Sebastian and people here who are accommodating me with coming with a kid because I'm with my five year old, and for me it's a nice opportunity to show him what it's like to be in an academic environment. Yeah, I know it takes a bit of a bit more of like. It takes a bit more time to accommodate for that. So thank you very much. What I'll present is I wasn't sure which risks I wanted to talk about when sending the title, so I stated it vague, but the MCT. So the minimum capital test is a measure that is used for solvency capital requirement calculation.
00:00:48.970 - 00:01:37.492, Speaker A: And so I knew what I was coming here for seminar in the financial field, so I don't do much of, as I was saying, option pricing and stuff like that, but what I do that's related to finance are things that are related to the solvency capital requirements, which financial institutions and insurance companies must comply to. So that's why I chose this topic and it's a project that I did in collaboration with a colleague of mine, Debbie Dupuy, working at, and a PhD student who will be soon graduating. Roba by Ragdar. Yeah, Carlos. Yes, he's a former master's student. He's now. What are you.
00:01:37.492 - 00:02:09.912, Speaker A: Hi Carlos. So it comes down to. Well, we had initially, almost ten years ago, I started collaborating with the OSFI. They wanted to update their regulation for property and casualty risks. So they had this problem that it was a framework that was very old and especially for earthquake risk. They had a measure, which I'll show later, that had to be updated. And so we started talking.
00:02:09.912 - 00:03:40.456, Speaker A: And then they also had this objective because they wanted to go from the capital requirements, where the insurance companies needed to either use prescribed formulas from the regulator or use their internal models. They wanted to have something that was a bit more structured, and then they wanted to have a flow to include these in the pricing policies for the insurance companies. So here what we did was to start and saying, well, where we want to focus on the liabilities, the losses, and we have to go from the losses to claims, and then we have, we wanted to have this solvency capital requirements and then to know how to go to the pricing of the insurance products and then to get the claims, the premiums, which would respect those, the amounts that would be set aside for the solvency capital requirements. So with the earthquake risks, here is a map of the earthquakes. It's the 167 earthquakes that happened in the last 200 years. So not much. But earthquakes are rare events with very intense damages.
00:03:40.456 - 00:04:31.204, Speaker A: Well, the probability there are lots of earthquakes, actually, those are the 170. Wait, those are the earthquakes with moment magnitudes that are higher or equal to six, which is a standard, what we call catastrophic earthquake, because we often have ground shaking. That's an earthquake, but low magnitude, we didn't count for them since we don't have any damages, we don't have claims coming from those earthquakes. So earthquakes that are causing damages are the ones that are either from 5.2 with damages or starting at six. So this is the map representing the risk. We see that there's a huge portion in the west, there's also a portion in the east.
00:04:31.204 - 00:05:16.466, Speaker A: We'll see also a bit later why there's the actual calculation of the minimum capital test and why there is the need to update, because there are still other regions that are affected. So we have a lot of exposure in the east. In the west, however, we don't have a lot of insured, especially in the east. Only 3.4% of homeowners are insured against earthquakes in Montreal and Quebec, and we have 65% in Vancouver and Victoria. So a couple of challenges. Well, the insurers only rely on their own historical data to establish those amounts.
00:05:16.466 - 00:06:04.500, Speaker A: And as we've seen, we don't have lots of claims, so it's very difficult to have a fair amount. And then that is then reflected in the premium. And the earthquake insurance premium is an add on that you choose to add to your regular insurance policy. So if you want to have that add on, you must pay. And the amount is quite high. And we wanted to know, well, is this justified also? So going from, again, the ground shaking to the distribution of the losses in space and time was a challenge. We didn't want to call ourselves engineers, so we choose to use the physics of existing models.
00:06:04.500 - 00:07:02.914, Speaker A: But still we wanted to use the losses and the claims from what we had as information gathered from several different organizations. So we had Catiq, we had the insurance Bureau of Canada, we had information from lots of different sources. We wanted to have something that was gathering all the information. And we also wanted something that could be open sourced either for the insurance companies or for the regulator to be able to have a standard model overall. Canada also the capital requirements calculation based on the minimum capital test and pricing, we needed to update those. This was the first thing that the OSFI asked. So the office of the superintendent of financial institutions and also the AMF in Quebec, de Marchie, financier wanted to update these calculations.
00:07:02.914 - 00:07:36.642, Speaker A: I'm forgetting something, but I'll get back to it later, I guess so. Here is a bit. If we only look at the data that was gathered from one of our sources. I'll get back to our methodology later. So this is why there's this here, because I'll jump from one slide to the other. So we have Catiq's information where we can see from the residential point of view. We have lots of insured of exposures.
00:07:36.642 - 00:08:58.933, Speaker A: Sorry about that. We have lots of exposures in the west, whereas we have much more non residential exposure in the east. Are these the same vertical scale? Yeah, yeah, and it's in billions dollars, but we'll see later that the amounts, it's just to show that we have more of residential and non residential. So the objectives were to provide an open source earthquake risk model, and we wanted also to have this simulator which could be used by insurance companies for rare but extreme events. And we also had this objective to have a spatial temporal model to see if there was something happening there that hadn't been taken into account in the past, and then to provide, to adapt the MCT and solvency capital requirements to IFRS standards. So IFRS standards are the standards that are in Europe. And we wanted to see if we would adapt their calculations to Canada, to our country, if again the capital requirements and the premium would be very different.
00:08:58.933 - 00:10:41.454, Speaker A: So what we did is to have to gather all the information from different sources for the number of significant earthquakes and also from the exposure. So those are from the census. Jesus Haz camp different sources who have the information of the building structures by FSA and the information which could help us to calculate loss amounts and then claims how to simulate the earthquake in space and time. So it's in bold because we spent a bit more time on this aspect and we wanted also to have to apply a new method to evaluate or to compare models, spatial temporal models for those specific types of events. So we've also estimated the ground shaking intensity, calculate the damage rates, estimating losses and insurance claim payments. So from losses to claims, those are very straightforward calculations where we have deductibles, policy limits, and then to calculate the solvency capital requirements using the new and adapted MCT framework. So for the spatial temporal point process, we use the intensity function, the lambda, which is simply so, the expected number of events on each grid cell.
00:10:41.454 - 00:11:35.094, Speaker A: We could assume, because those are rare and extreme events that in space and time was independent, were independent processes. So we had a gaussian kernel which bandwidth was calculated using MSc while minimizing the MSC. And then where we spent a bit more time was on the quartic kernel to compare for the spatial mapping of the earthquakes. So we compared the bandwidth with either the MSC or minimizing the cross validation by cross validation. And first, we started with this map very like, very broadly. Yeah, yeah, exactly. Thank you very much.
00:11:35.094 - 00:12:33.814, Speaker A: And we have a grid with, it's a pixel based representation. And so we said, if we want to have, let's say if we want to compare in space, it was very difficult for us because we had pixels where we had lots of earthquakes and other where we had none. And so either we would use a residual analysis, but then the number was not. We had very skewed distribution, because at some places we had more, lots of places we had none. And then if we wanted to use the deviance residuals, by taking the likelihood of each one of the deviance between the two models. So, with the two bands, with the quartic kernel, we took the difference and to evaluate if one model was better than the other. But then by doing so, we also had the same problem for some.
00:12:33.814 - 00:13:22.824, Speaker A: For some pixels, we had lots of earthquakes, and for others, we had none. So the deviance was a bit better. But still, we couldn't get nice results. Even a good result was not a very good result. So what we did was to use this Voronoi tessellation spatial representation. And so the Voronoi tessellation takes different polygons, where each polygon contains one earthquake at most, and each point in the polygon is closer to that spatial, to that point. So x that longitude latitude x than to any other point.
00:13:22.824 - 00:14:15.294, Speaker A: So the tessellation, the polygon, is constructed such that it's the closest, so that each point in one polygon is closer to one earthquake than to any other polygon around that. And then using this, well, we couldn't use the residuals, because, of course, we have one or zero. We have. Well, in each polygon, we have one earthquake. So what we did was to use the, and develop the framework to use the, the residual, but with those polygons. So with the Voronoi representation. And so this is by summing then the deviances, and we could find the model that would fit best our spatial mapping of the, our intensity for the spatial representation of the earthquakes.
00:14:15.294 - 00:14:54.144, Speaker A: So, once we had that residual analysis, we used the ground shaking intensity using a generalized pareto distribution where we wanted to see. So we had this large quantile, and we had already a point from which we knew that was considered as extreme. So we had this threshold and we could isolate to estimate for. To find. Sorry. We could isolate to find the XM that we were looking for here. Here.
00:14:54.144 - 00:15:49.386, Speaker A: And then we had also, by the geological Survey of Canada, several quantiles from which we could estimate by minimizing the MSc of the quantiles. We could estimate the parameters, the parameters of the GPD here. So the sigma and the Xi were estimated using the MSC of the quantiles that were already provided. So we could find. We could, with this, have a model of the losses for each point on a gridded map in Canada. So once we had the PGA, we wanted to know then what was the propagation of the intensity of the ground shaking. And we had, this is very straightforward, no functions there.
00:15:49.386 - 00:16:57.894, Speaker A: It's really just deterministic calculations. And again, basic actuarial math using deductibles, policy limits, we also had market penetration and we could calculate the claim payments for each point on a gridded map. So this led to this simulation, this simulator that was okay, that was coded by robot. Let me just reload maybe. Oh, yeah, it says so. It takes lots of computing power, but with this free version, and for now, since it's just anonymous, it's not yet. This.
00:16:57.894 - 00:17:37.154, Speaker A: This simulator is not yet publicly available. So this is the free version of it. We can, this is UBC's actually GPS coordinates that I just entered here so we can simulate, for example, an earthquake in UBC. I tried like, I wanted to do it here, but it took it at home. It took me a bit of time for the calculations to run. So I. I went in a place where we had more history of earthquakes, and I felt that UBC would be good guess.
00:17:37.154 - 00:18:31.426, Speaker A: So we can simulate as much earthquakes as we want. And then if we have, let's say we have the percentage of insured, let's say we have 25%, a deductible of 10%, a policy limit of 90%. And we enter same thing here. You can enter whatever number you want in here, and then you calculate the claims. And this is just a straightforward calculation. And then you get, why is it taking time? Well, you get, okay, the split of the claims. So from the losses to the claims.
00:18:31.426 - 00:19:29.834, Speaker A: And we have all these, this information. So you can do this pinpoint on any point on the map. Let's see if it works here. Anyways, so, very fun. So this was one of the first objective that we had to have this available simulator for the earthquakes. And then the next objective was to see if we, if adapting the solvency capital requirements with an updated version following the european standards would be how actually, we wanted to look at how it would change the capital requirements. So, yeah, go back here.
00:19:29.834 - 00:20:20.654, Speaker A: This is how they used to calculate. Well, how they are still calculating the minimum capital. This is the minimum capital test for earthquake entrance risk. So it's actually an lp norm with 1.5. And when I asked. Yeah, when I talked to the person who set up this equation and I asked why? And then initially, and this is a funny story, because in 1989, when they asked to start having those solvency capital requirements, then they started looking at the numbers and they wanted to justify. So are we using an arithmetic mean or what kind of.
00:20:20.654 - 00:21:15.624, Speaker A: What are we using? So they provided a couple of numbers based on different calculation schemes, and then the amount that they suggested, the objective was that insurance companies, federally regulated, would agree to comply to those standards. Right? And another objective was to have enough capital to pay for losses and claims in case of a danger. And they also had the information of what the defense was able to pay, because if the insurance company is not paying, then it's the government who pays. Right? So they provided this. Well, with this equation, they provided a number that insurers just agreed to jump in. So that's what they kept. So, yeah.
00:21:15.624 - 00:21:48.830, Speaker A: And I spoke on the phone with this person at that time, it was a couple of years ago, Roba was doing an internship in an insurance company. And then we wanted to figure out. And then I talked to the person at OSVi, and he. He knew, he said, well, I know the guy who suggested that equation. I talked to him over the phone and that's what he said. So that's why it's too low. Well, yeah, but still, see, no one.
00:21:48.830 - 00:22:27.564, Speaker A: No one takes this entrance product. So there's something here, right? And then east is split and west is split, like from the center of Canada. So that's pretty much it. What we wanted to do is to have this new calculation. So in Europe they have. So here what we have are, you know, we have provinces that are all in line. In Canada, what they have in Europe are just like a bunch of countries here and there, and they have something that is a bit more, well, a bit more complex.
00:22:27.564 - 00:23:28.706, Speaker A: You calculate a correlation, right? So it's the correlation between those countries, the losses and claims from those countries. What we did was to do the same thing here. So it's very basic, but we tried to adapt and to consider other dependence measures than the correlation. So in the paper, we have lots of other tools that could be used. This is just the corresponding method with the IFRS standards. So if we have the probable maximal loss as calculated with using the GPD, that we estimated the parameters by minimizing the MSE of the quantiles that we have from the Geological Survey of Canada, we have those pmls for each province, then we calculate the correlation between each province, and then we can have this countrywide. And it, it used to be one over two, an event that could happen once every 250 years.
00:23:28.706 - 00:24:28.288, Speaker A: And then as time went by, from 1989 to 2017, they said, well, we need to update this because we don't have enough. And then they said to the insurance companies, you have five years to evolve and then increase the capital up to an event that could happen once every 500 years. So that's why it's one over 500 here. And so what we did is to have, so we had this distribution, we knew that we had a sequence of independent random variables satisfying. So basically we used extreme value theory to model the losses. But what we could have is that the number that we were looking for was following a Poisson process. So it was very convenient.
00:24:28.288 - 00:25:35.538, Speaker A: We could have this model quite straightforward, using the probable maximal loss, because we knew that the maximum of the, the amounts that we would pay would follow gev. So once we had these gev parameters again estimated using MLE, we could find the probable maximum loss at level epsilon. And as I said, well, we transitioned from once every 200 years to once every 500 year events. So if I come back here to. Well, this is the simulated claims that we had. And if we go back to this first slide on exposures. So we have lots of exposures for residential in the west, lots of exposure for non residential in the east.
00:25:35.538 - 00:26:13.338, Speaker A: We have the split between east and west, just to have the comparison after with the AUSVi's calculations, we have the residential and non residential. It still holds here. We have much more, much more claims for non residential than we have for residential. And if I go back to the exposure, this is something I did not talk about. Our methodology is now with the simulation, the simulation algorithm that we had, we could compare. What if we had. We had, we had losses from the.
00:26:13.338 - 00:27:03.914, Speaker A: Not losses, sorry, exposure from the west and from the east. But anyways, okay, so let's get back to the claims. And again, we have the same split from west and east for the residential and the non residential, because we don't have much of non residential and we have much more. So the average, the average insurance claims per province. And we had to simulate 100,000 years of events which is something I should have mentioned earlier when talking about the estimation of the parameters of the GEV. This is what we used. We simulated 100,000 events to estimate our parameters and then obtain our losses and obtain our claims and obtain our solvency capital requirements.
00:27:03.914 - 00:28:02.634, Speaker A: This is just to show you the correlation. So we have other types of dependence measures in the paper here. We just want to show that it makes sense because neighboring provinces have a correlation that is non zero, and then when we have further apart provinces, they are zero. So what happens if we use OSV versus. So the actual versus the IFRS type of capital requirements for the losses? We see that there's a bigger difference, right, between. So the Pearson correlation or the osses calculation. And again, for the claim, we see that there's a much closer gap between the two to fill.
00:28:02.634 - 00:29:10.998, Speaker A: So. Well, the guy who said this lp norm was a bit random, but still we're not so far if we rely on other countries assumptions and calculations. So what we're able to do now to have. What we can show, actually, is that we would need to set aside for an event that happens once every 500 years. An amount of insurance claims are around 28 billion, but 152 billion in uncovered losses can be partially covered. So what happens is that we still have, if we want to calculate this once in every 500 years event, we would need to set aside 180 billion, if we have. So at this rate, we would have claims for 28 billion.
00:29:10.998 - 00:29:57.440, Speaker A: And then what? Claims that losses that would be covered of 28 million and then the remaining 152 would need to be covered by someone else, so by the government. So that's still something to think about. Oh, yeah. And that's mostly it. I just wanted to talk about a bit. So this is all part of a big project that I'm now continuing with Silvana. We're working on natural catastrophes, and I've been doing also well with robot, two wildfire models to have the probabilities, to simulate probabilities of wildfires in Canada and in the US.
00:29:57.440 - 00:30:39.128, Speaker A: And we have different models for that. So we model Canada man made fires or natural wildfires. We have the US manmade wildfires in the US, natural wildfires. We also split the large wildfires for Canada and the US. So, yeah, we're focusing on rare events with extreme damages, which is, again, something that I've discussed this morning. This morning, my flight, I had a 10:00 flight, which got canceled at midnight. So I received an email and then they said, well, you have to book.
00:30:39.128 - 00:31:13.800, Speaker A: And of course I tried to book, but everyone on that flight wanted to book sequent flights, right? So trying to figure out, I booked the 01:00 p.m. flight and then sent the email to Sebastian saying, well, my flight got canceled, but here is my next flight. And he's like hoping that it doesn't get cancelled. And five minutes later I get an email, your flight is. Your 01:00 flight is canceled. I'm okay, so let's try another one. And I had scheduled another flight at five, which would have made me late anyways, but then.
00:31:13.800 - 00:31:44.646, Speaker A: And when they said, they also said, but this flight is under review because of winds and rain. So I'm like, okay. And then, yeah, so I said to the little one there, you want to take the train? He said, yeah. And we hopped in. So all these rare events. That's what I said. Also, so many things like, yeah, so Christian Genet once told me, working with extremes will make you become one.
00:31:44.646 - 00:31:56.974, Speaker A: And, yeah, things happen in my life. I'm always, I'm always thinking about that. So. But that's it. That's all I wanted to share with you today. Thank you.
