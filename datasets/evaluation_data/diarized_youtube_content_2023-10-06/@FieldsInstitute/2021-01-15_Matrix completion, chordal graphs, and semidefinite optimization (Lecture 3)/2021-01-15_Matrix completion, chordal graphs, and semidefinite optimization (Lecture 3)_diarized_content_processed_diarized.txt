00:00:00.160 - 00:01:31.764, Speaker A: Last time we went over the second part, we looked at positive semi definite matrices with choral sparsity pattern. And an important result was this, this decomposition result or clique decomposition result that says that every matrix with a choral sparsity pattern that's positive semi definite can be factorized or written as a sum of very simple positive semi definite matrices with very simple sparsity patterns, one for each clique. So geometrically it means that we write this quite complicated convex cone of positive semi definite matrices as a sum of very simple convex cones. So that's one of the classical results in for choral sparse matrices. And then we also made some connections with multifrontal cholesky factorization. And the conclusion, or the goal was to find efficient methods for evaluating the logarithmic barrier function for this cone of sparse positive semi definite matrices. And so conclusion was that we can evaluate the gradient and the Hessian very efficiently by a recursion over the click tree or an elimination tree in a way that's very similar to the multifrontal Cholesky factorization and has a similar loss of complexity.
00:01:31.764 - 00:02:41.788, Speaker A: So the next topic then is the positive semi definite completion problem. So again has a nice geometry that's closely related to the positive semi definite matrix cone. So if we define the cone of sparse matrices with pattern e that have a positive semidefinite completion, then we can represent that or view that as the projection of the positive semi definite cone on that sparsity pattern. And the projection simply means that everything that's outside the sparsity pattern, that's all the zeros in the sparsity pattern, are set to zero and we keep the elements that are non zero. So here we don't have to assume this is a chordal pattern. So it's clear that that's a convex cone because it's the projection of a positive semidefinite convex cone for n by n matrices on a subspace it has non empty interior because the identity matrix is in this completable cone. By our assumption that the diagonal is always interpreted as a non zero, in our definition of a sparsity pattern, it's pointed.
00:02:41.788 - 00:03:41.478, Speaker A: That means that if a matrix is in this cone and the negative is in the cone, then the matrix must be zero. And again, that easily follows from our assumption that the diagonal is treated as non zero. And the least obvious of these properties is that actually it's a closed cone. And that relates to some important question in convex analysis, that on conditions that guarantee that image of a closed convex cone under a linear mapping is still closed and it only holds under certain assumptions. But they're satisfied in our case, again, because the diagonal is treated as non zero. So the condition is that if we there's no non zero matrix in the cone that we're projecting, that's also in the null space of the linear mapping. That's a mis projection.
00:03:41.478 - 00:04:23.962, Speaker A: So in this case it means there is no positive semi def. If a positive semidefinite matrix is projected to the zero matrix, then that matrix must be zero. And that's true because we it would mean that the diagonal of this positive semidefinite matrix is zero. So it's a regular or proper cone, and that's true for any sparsity pattern. It's also the dual cone of the positive semi definite sparse matrix cone. So if you take the or if we first take the dual of the completable cone, then it's easy to show that that's the positive semi definite matrix cone with that sparsity pattern. And the dual cone here is defined like this.
00:04:23.962 - 00:06:01.084, Speaker A: It's the dual cone of the positive semi definite completable cone is a set of all sparse matrices and make a non negative inner product with every matrix a in the positive semi definite completable cone. So that holds if and only if the matrix b in the dual column is positive semi definite. And then from the fact that the post is semidefinite completable cone is a closed cone, we get that both directions are satisfied, right? So the dual of the positive semidefinite completable cone is a cone of positive semi definite matrices, and the dual of the dual is the closure of the original cone. So since the positive semi definite completable cone is closed, then we also have the other direction that the dual of the positive semi definite matrix cone is the positive semidefinite computable cone. And again, these are true for any spot varsity pattern. Then if the pattern is chordal, we have also very fundamental decomposition result that comes from actually famous paper by Henry that says that the matrix with a chorus parsity pattern as opposed to semi definite completion if and only if every click in the pattern, every fully specified positive semi definite principle matrix principal matrix is positive semidefinite. So again in a simple example here, we have a simple chordal pattern with three clicks.
00:06:01.084 - 00:06:57.164, Speaker A: So for this matrix to have a positive semi definite completion, we need that it's necessary and sufficient that each of these diagonal blocks is positive semi definite. Obviously that's a necessary condition for any pattern that the fully specified principal sub matrix must be positive semi definitely for the matrix to be compatible, but for the choral patterns, that's necessary and sufficient. So it actually is closely related to the earlier result. It actually predates it historically. But one way to prove it is from the previous result and then the duality property. So we've seen that the completable cone and the positive semi definite cone are duals. For the positive semi definite cone, we have this decomposition result of the cone as a sum of simple matrices.
00:06:57.164 - 00:08:11.704, Speaker A: And then in general with some technical conditions, the dual of a sum of convex cones is the intersection of the dual columns. So in this case it gives us the characterization of the positive semi definite completable cone as an intersection of very simple convex cones, small dimensional dense positive semi reference cones. So we can skip the proof then related to this result. So this gives the feasibility of an existence of a positive semi definite completion. Then we can look at different ways for completing a completable matrix and different kinds of type or types of completions that we may be interested in so many applications. You might be interested in the minimum rank completion. So the matrix with x with the smallest rank, that gives us, that completes sparse matrix a.
00:08:11.704 - 00:09:23.884, Speaker A: And there is also an result from, I think the earliest paper I know about it is this paper from 92 that says that the minimum, if the pattern is chordal, then there exists, and it's a completable matrix, then there exists a completion with rank equal to the largest rank of the maximal clicks in the maximum dense sub matrices. So again, clearly for any matrix that would be a lower bound on the minimum rank of a completion. Because if you take a completion, any positive semi definite completion of a matrix, the rank cannot be smaller than the largest rank of the fully specified matrices. But for choral matrices, you have the interesting property that that lower bound is also achievable. So that's actually the minimum. So for general patterns, you might still be interested in the minimum rank completion, but that's in general very difficult optimization problems, not convex. For coral patterns, you can actually find this minimum rank completion by again recursion over a click tree.
00:09:23.884 - 00:10:10.940, Speaker A: I'll skip the general algorithm, but just explain it. For this simplest type of chordal pattern with two overlapping principle sub blocks, and then by combining the result for the two block completion problem with the data structure of the click tree and choosing the right order to complete it, we can show this more general result. For any coral pattern, the notation gets more complicated. So if you just look at the simple results. So we have a matrix that number the blocks. Like this, we have two overlapping sub blocks. They're both positive semi definite.
00:10:10.940 - 00:10:43.728, Speaker A: So the matrix has a positive semi definite completion. I want to replace this three, one and 13 block to minimize the rank of the matrix and keep it post the semidefinite. And the claim is that the minimum rank is a maximum of the rank of these two matrices. So let's call that r. So the completion follows like this. So each of these sub blocks is completable. So we can find each of these sub blocks is a completely specified dense matrix.
00:10:43.728 - 00:11:24.864, Speaker A: So we can factorize it as factorizations with column dimension r. Because this has rank r1, this has rank two reals. If one of these ranks is less than r the maximum, then we can just add zero columns to the factorization. So these factorizations exist. Then that means for the block a 22, we have two different factorizations, vv transpose and v tilde. V Tilde transpose for the same positive semidefinite matrix a 22. So by a useful result in indian algebra, there exists an orthogonal matrix that relates these two matrices v and v two.
00:11:24.864 - 00:12:46.296, Speaker A: So there exists an r by r orthogonal matrix q that relates these two matrices. And then we can verify that if we use this orthogonal matrix and combine these four matrices u, v tilde w in this way, then we get a completion of the original matrix. It's a completion because the bottom part, v tilde and'w were a completion of the this block in the matrix. The first two blocks also form a completion of the other specified matrix because of this result, right? So we can write v tilde as v q transpose. So if you make that substitution and use the first fact here, then we see that this also agrees in the other this first leading matrix and then the completed block follows stress from the product. But the fact that we have an rank r factorization shows that is completable with rank r. So in to do this, for a general matrix, we would have more than three blocks.
00:12:46.296 - 00:13:51.684, Speaker A: We have an entire matrix that we the factorization that we complete block row by block row and we can do this by again click three recursion and it in the right, in this case the inverse topological order, which means we start at the top, the root of the click tree and then go through the tree in inverse topological order. And in each step we apply this simple method for the two by two case and by the properties of the click tree. We never have to revisit choices we made before. In the same way as in this simple factorization, we didn't change v tilde w. It has already completed from earlier steps in the recursion, we only change the new part that we're adding. So it's kind of a greedy recursion that gives us the minimum rank completion. So a second, so that gives us the minimum rank posted semidefinite completion.
00:13:51.684 - 00:14:45.686, Speaker A: A second very useful type of completion is the maximum determined positive definite completion. So here we assume that a is in the interior of the completable cone. So there exists strictly positive definite completion, and we might be interested in finding the completion that maximizes the determinant. So this optimization problem, maximizing the determinant or log of the determinant of w, subject to the fact that w is a completion of the matrix state. So that's a convex optimization problem in general. So the, even for a non coral pattern is always a convex optimization problem, but in general, you will have to solve it by convex optimization algorithms. There's a very interesting duality and optimality conditions.
00:14:45.686 - 00:16:04.884, Speaker A: So if we derive the optimality conditions by just writing the KKT optimality conditions, we find the following conditions. Obviously w must be positive definite, it must satisfy the completion identity, and then the inverse of w will be a sparse matrix with sparsity pattern e. And those are the three conditions, w positive definite, the equality constraints in the optimization problem, and then the inverse of w is a sparse matrix with sparsity pattern e. And the inverse actually also arises in the optimality conditions as the dual multiplied. So it also gives us an interesting so these are the optimality conditions. The dual problem is also of interest, or will be of interest later. If you take the dual of this convex optimization problem of finding the maximum determined completion, then the dual problem is a maximization problem with a sparse positive semi definite matrix with a sparse matrix as variable and this cost function.
00:16:04.884 - 00:17:23.884, Speaker A: And so the duo has an interesting, both have an interesting interpretation in graphical models of closing distributions, and we also encounter here in this talk, we then encounter the dual problem in a different context. So all of this is again true in general. So for a general sparsity pattern, we might be interested in finding the maximum determined completion, and then we have to solve it by general optimization methods for a choral pattern. Again, we have the special property that we can solve this much more easily. We in fact have from the early papers on this explicit expressions and formulas for the positive maximum determinant completion. But we can also find it by a similar recursion as we used yesterday for the, for example, the projected gradient or the projected inverse of a positive semi definite matrix. And it's very related to that problem actually, because here the optimality conditions of the problem will be that w inverse, the inverse of the maximum determined completion is a sparse positive definite matrix.
00:17:23.884 - 00:18:55.174, Speaker A: So it has a zero fill factorization, Chileski factorization, LDL transpose. So instead of computing the maximum determined positive definite completion, which will be a completely dense full matrix, it's more efficient to compute Cholesky factorization of the inverse, which is a sparse matrix. And we can use the same equation that we used yesterday to find the projected or selected inverse of a positive definite matrix. There we had the same equation, but there the Cholesky factorization of the matrix was given, and we tried to use this equation to find a subset of the entries and the inverse of the matrix by examining this equation and determining the important elements in the inverse in the right order from the Schlesky factorization. Here we use the same equation, but we are given the matrix a because that's part of the, that's the completable matrix that we're trying to complete. And from this equation we're finding the Cholesky factorization of the inverse. And that can also be done and actually expressed, if can be expressed by just explicit formulas.
00:18:55.174 - 00:20:13.760, Speaker A: In practice, you would compute the Cholesky factorization of the inverse of the maximum determinant completion by a similar recursion over the click tree in inverse topological order. And we can give a method for doing this that has the same, in the same style as these other we call multifrontal methods for, similar to the multi frontal method for Cholesky factorization, and has a similar structure. We have recursion over the click tree and we try to optimize the efficiency by introducing small dense auxiliary variables that we create and delete, and they're no longer necessary for the recovery. So this is sort of a similar product as we had yesterday for the projected inverse. So here we compare the cost of this inverse maximum determined completion and the cost of a cholesky factorization for the same sparsity pattern. So we see it's a little more expensive, but the trend is the same. And roughly speaking, you could say that you can compute as maximum determined completion at a similar cost as a Cholesky factorization for the same pattern.
00:20:13.760 - 00:21:38.144, Speaker A: So often that means it gives you an idea of the efficiency or the complexity of this maximum deterrent completion. So I mentioned this dual problem, which is sort of well known in graphical modeling, where the inverse of y, so the w would be the estimate of an covariance matrix in a gaussian graphical model. This dual problem also arises in convex optimization as the dual barrier of the log dead barrier for the posted definitely semi definite cone. So I mentioned that the cone of positive semi definite completable matrices and the positive semi definite matrices are dual cones. Yesterday we looked at the log dead barrier for the positive semi definite matrix cone in convex optimization. This is how we define the conjugate or the conjugate barrier of a dual cone, if we know the barrier function of the primal cone. So it's related to the legendre transform or convex fascial conjugates, but with this difference in sign.
00:21:38.144 - 00:22:50.974, Speaker A: So the formula says to evaluate this barrier function, this logarithmic barrier function at an element in the interior of the completable cone. We will have in general to solve this optimization problem in the definition. So you will have to find a positive semi definite matrix s that maximizes this concave cost function for the x at which we evaluate the barrier. So in general, this is actually a convex optimization problem. But it's not trivial to solve for a coral barrier. It's related by duality to this maximum determined completion problem we discussed, and we can find this s that maximizes this cost function in the definition of the dual barrier from the previous output, where we find an inverse factorization of the maximum determinant completion. So in other words, the optimal s in this optimization problem is exactly the inverse of the maximum determinant positive definite completion of x.
00:22:50.974 - 00:24:31.234, Speaker A: And since we can for a choral pattern efficiently compute the inverse of the positive definite completion, so the matrix s that maximizes its cost function, or a factorization of it, we can efficiently evaluate this barrier function, and also from the theory of conjugates, the gradients and the Hessian, and this summarizes the set. So we can evaluate the conjugate barrier for positive definite complete matrix by solving the maximum determinant completion problem, or the inverse of the completion, and then this matrix s that gives us the maximizer in the definition of the conjugate also gives us the gradients and the Hessian of the dual barrier. So this will be used in sort of an application I mentioned at the end of the talk. And then the third type of symmetric matrix completion is euclidean distance matrix completion. I think this is familiar to everyone. So, Euclidean distance matrix is a symmetric matrix that can be written as entries that can be written as the square distances between a set of points. We'll call the matrix y with rows yi realization of the euclidean distance matrix.
00:24:31.234 - 00:26:04.928, Speaker A: And so if you expand this product, we see that a and then this this Gram matrix Yy transpose satisfy this linear relation by just expanding the definition element by element. We note that the realization of course is not unique, because if you have one realization of a, then we can always translate it, translate every point. So you write it as adding a one vector times the same vector a to every row in the y, and we can also rotate them by an orthogonal matrix. And if you have one set of points that satisfy this, then the rotated and translated points also satisfy them. Now there's some of course, famous characterizations of this due to Schoenberg, who showed that symmetrics n by n matrix is a euclidean matrix distance matrix if and only if the diagonal is zero, and also the it's negative semi definite on the complement of the once vector. So if p is a matrix whose columns span the orthogonal complement of the ones vector, then that matrix here must be negative semi definitely. And a very useful choice of p would be this one, this very sparse matrix.
00:26:04.928 - 00:27:21.564, Speaker A: Because with this choice of p, one can show from this equation here that by factorizing p transpose ap, we get a realization that has a specified entry, a specified row at zero. In this case, if you choose ek to be the kth unit vector, k basis vector, then if we factorize this matrix, then y will have a zero k rho. So that's the realization that places the k point at zero. And then the so that means that there also the rank of this matrix gives us the dimension of the embedding. And since we'll, we'll be dealing with matrices of different sizes, we'll call that just a dimension of a the euclidean distance matrix. So it's a rank of this matrix b transpose ap for a matrix whose columns span the orthogonal complement of ones. So then the EDM completion problem is given matrix a, find an EDM matrix x.
00:27:21.564 - 00:28:07.174, Speaker A: That completes it. So in general, that's a convex feasibility problem. We're specifying some entries of x, and then x must be positive semi definite on the orthogonal complement of one. And actually in addition a x must be has the zero diagonal. But we assume that's part of these conditions already satisfied by the matrix a. And of course in many applications we might be interested in the solution that minimizes the dimension of the embedding. So again, there is a famous decomposition result that's similar to the positive semi definite completion from 95 that says an EDM matrix with a choral sparsity pattern.
00:28:07.174 - 00:29:10.244, Speaker A: A matrix with a chordal sparsity pattern is completable in euclidean distance matrix. If every fully specified principal sub matrix is in euclidean distance matrix. And as for the postive semidefinite completion, there exists a completion with a dimension equal to the largest of the dimensions of these submatrices. So again, we can try to formulate this in the style of the algorithms that we've been using so far as iterations on the click tree. Again, we only do it for a two by two, the simplest case, and then by combining this with the general data structure of a click tree, we can extend it to any coral pattern. So in the simplest case, again we have two overlapping diagonal blocks. Each of them is euclidean distance matrix.
00:29:10.244 - 00:30:20.294, Speaker A: And we try to find a completion of this matrix that gives us an euclidean distance matrix completion with dimension equal to the maximum of the two dimensions of this these given idiom matrices. So it's similar to what we did for the minimum rank post, this semidefinite completion. But we have to, in addition, worry about the translation between the realizations. So we assume we take these two fully specified matrices and we take two EDM realizations of them, and we can pick the first column of v to be the first row of v to be zero. As I mentioned, we could also make the first row v tilde zero. But in the if we use this two block algorithm in a more general recursion, then this v tilde w is something that we've compute is something we assume we've computed already earlier during the click recursion. So we like to not change v tilde Mw in the next step.
00:30:20.294 - 00:31:29.964, Speaker A: So you don't have to change any computations that we did before in the recursion. So then we have two EDM realizations of the same matrix a to two, and that means they're related like this. Or one can show they're related like this. So the first row of v is zero. So if we shift v tilde to have the first row zero by applying this matrix on the left, then they should be related by an orthogonal matrix q. And then I can define this matrix, this three block matrix, where we take the part that we already had from the second block, v tilde w. I add this matrix as a first block, which is the new part rotated by an orthogonal matrix and translated and I can verify that this matrix y now is realization of oracle gives us a completion of the matrix.
00:31:29.964 - 00:32:33.170, Speaker A: So this is a realization of a completion of the matrix, because the second and third block hasn't been changed. So that was a realization of the trailing block. Here the first block is defined like this, the second block by this relation. If I solve for v tilde in terms of v can also be written like this. And because uv was a realization of the leading block, I have the, we can see that this is also correct realization of the leading block because it just translates the realization that we had and multiplies it with an orthogonal matrix. And then in the general algorithm you would continue like this, so this, and build up the realization of the entire coral matrix. So most, these are some references that in addition to the ones I mentioned.
00:32:33.170 - 00:33:50.524, Speaker A: So the complete references can be found in this paper, except for the minimum rank completion algorithms, which are actually from the PhD thesis of Ifon sun. And for most of these algorithms except last one, I think we have some Python implementations in this package. So in the last part of the talk, I will just list a few applications in convex optimization and semi definite optimization of all the different techniques we discussed in part two. So the first connection with convex and modular nonlinear optimization is via the idea of a partial separability in objective functions. So this was introduced by Grivanc and when they studied sparse quasi Newton updates. So this is an example of a partial separable, partially separable function. So we see that f can be written as a sum of functions that each depend on a subset of the variables.
00:33:50.524 - 00:34:57.733, Speaker A: And it's partially separable because, for example, we see that four and three, x four and x three always appear in different components of the function. So if you fix x the other variables x one, x two and x five, then this would be separable and x four and x three. So in general definition would be like this. F of x is a sum of functions that each depend on only a subset of the variables, and these subsets can be overlapping, right? So it's not completely separable. We can also use it for define partially separable sets in the same way, because by applying this to the indicator function of the set. So for a set, it would mean that we have a vector x that's in the set c if some certain specified sub vectors of x lie in sets ck. So we have different overlapping sublocks of x that must be in the in set circuit.
00:34:57.733 - 00:35:29.574, Speaker A: So now we could call c a partially separable set. It's not a product of the set cks because they're overlapping subsets. And this can be sort of represented using graphs. So we can define an interaction graph. For example, in the function that I, on the previous page, this would be the interaction graph. So it means there is two vertices. The vertices represent the variables in the function.
00:35:29.574 - 00:36:29.514, Speaker A: Two vertices are not adjacent. If x three and if those vertices, those variables never appear in the same component function, like for example x three and x four in this example. So it also means if we have a non adjacent vertices then and we fix all the other variables, then the function would be separable in those two variables. And if you write it in general, it will give us a relation like this. So if I j, the edge ij is not in the an edge in the graph. If I and j are not adjacent, then it means we have this kind of separability condition. If we keep x fixed instead, except for the ith and the j component, and we look at this function of two variables s and t, then it's separable in S and t.
00:36:29.514 - 00:37:41.874, Speaker A: So it's similar also in graphical models. If we this would be the log of a distribution that you write as a product of probability density functions, and this would be correspond to conditional independence of the variables. Okay, so we can do this for functions. We can do this also for partially separable sets. And one example that we've seen is this positive semi definite completable cone with a choral pattern, because we did exactly for the matrix. What we have here in the definition, we have a sparse matrix x, and this clique decomposition result says that x is posted semi definite completable if these overlapping sub matrices in the matrix are positive semi definite. So we express the cone of positive semidefinite matrices as an intersection, overlapping intersection of poster semi definite dense matrices.
00:37:41.874 - 00:38:29.108, Speaker A: And the interaction graph would be actually have these sub matrices as vertices. But it's very close related to the click tree that we already have. So this type of structure that we had here. But in general, this partially separable structure is very useful in optimization. Suppose we just have an optimization, but the notation is a little bit easier if you just use the general notation here of partially separable function. But everything will apply to this important case. So if you have a partially separable optimization function, then it means this.
00:38:29.108 - 00:39:51.698, Speaker A: So we can write it in, put it in the correct form for decomposition algorithms, especially dual decomposition algorithms, by introducing splitting variables and splitting variables for these components. These sub vectors of x, we call them x tilde k. We substitute that for p beta x k. So the sub vector beta k of x, and we have a separable cost function that we have to add equality constraints in some form that specify that these different sub vectors of x are all sub vectors of the same overlapping sub vectors of the same common vector x. Right? So several of the variables appear multiple times. And one way to express these equality constraints is just like this, right? So we concatenate all the splitting variables and then we have a very simple and very sparse matrix that just relates or expresses the fact that these splitting variables are sub vectors of this actual original optimization variable x. But in any case this is a sort of a subspace that can be represented in many different ways.
00:39:51.698 - 00:40:58.884, Speaker A: So this is sort of the right starting point for dual decomposition algorithms, also algorithms like ADMM in general, decomposition algorithms of many types, because we have a separable cost function and very simple equality constraints because p is a very simple matrix. And so this can be then written in many different ways. But it doesn't matter how we actually express the equality constraints, because they just express this fact that the splitting variables are, must be equal in the places where they overlap. So this is a good starting point for decomposition methods and in the context of semi definite programming. And the click decomposition has been used in many types of algorithms. For sparse semi definitive optimization. It was actually first proposed and used for interior point methods, and people often refer to this as the click three conversion methods.
00:40:58.884 - 00:42:47.424, Speaker A: So in that application it means that we use this characterization. So we express the posted semidefinite complete well matrix by introducing splitting variables, different variables for each sub matrix in the matrix x, sparse matrix x, and then we add equality constraints to express the fact that where these submatrices overlap, they must have the same value. But it's really the same idea here for a vector optimization problem. So let's go back to around 2000, and there have been some also recent papers where people use this actually to carefully to solve some very large sparse SDP by actually this paper, for example by Altvet for study this for this power flow distribution problem and just used reformulation, careful reformulation of this type and the mosaic solver for just standard positive semi definite optimization. And the second paper also used a variation of this to actually solve some very large sparse sdps of special structure. So for interior point methods it's useful. It's actually even more natural to use it in combination with first order and speed splitting methods, because this type of structure is exactly what first order decomposition method and splitting methods try to achieve.
00:42:47.424 - 00:43:55.412, Speaker A: So we have an separable cost function and linear equality constraints on which it's very easy to project a variable because projection on this subspace is very easy. And that can be done with splitting methods like ADMM or Douglas Ratchford methods, but also block coordinate descent type methods where we do the projections sequentially. And it's actually very well suited for all those types of methods. And there's a very recent solver that actually for sparse semi definite programming via ADMM by using this decomposition methods by group from Oxford. So an application of the same idea, not in the context of just semi definite programming. But this was a question that was asked yesterday. Also, suppose we're interested in, as an example, in a sparse nearest matrix problem.
00:43:55.412 - 00:44:58.320, Speaker A: So, nearest matrix problem means we sort the nearest matrix in Frobenius norm to a given matrix subject to some constraints on the nearest matrix. So for example, we can try to find for a given matrix a, the nearest positive semi definite matrix that has a specified sparsity pattern, this problem. So it means we take a matrix, a sparse matrix a and projected on the positive semi definite cone of sparse with a given sparse sparsity pattern. The dual cone. The dual problem has this simple interpretation. So in a dual problem, we project a on the polar cone, the negative dual cone. So we actually find the nearest matrix to a that's positive semi definite completable.
00:44:58.320 - 00:46:06.296, Speaker A: Or that's the first problem here I project on the completable matrix. In the dual problem, I project on the positive semidefinite matrices. So without the for a small problem, this can be done by just alternating methods. In an obvious way, we alternating alternate the projection on the positive semi definite matrices and projection on the sparse matrices. But the problem is that for a large example, we'd like to avoid projections on the positive semi definite matrices because that requires an idea composition. So these clique decomposition methods are very useful here, because if the pattern is chordal, then we can rewrite this problem and apply the clique decomposition theorems for the primal or the dual cone. So for the primal cone, for example with the post is semi definite completable matrix.
00:46:06.296 - 00:46:58.844, Speaker A: We know that a matrix is completable if every dense principal sub matrix is positive semi definite. So then we have this much simpler optimization problem where we have a quadric cost function and then much simpler constraints that says every click in the graph we have a positive semi definite dense matrix. And we have a similar structure in the dual problem. But there we are using the sum decomposition of a positive semi definite matrix. So in this formulation, the problem is very well suited to many types of decomposition methods. So we have dockers, Ratchford or ADMM. Since we have a sort of differentiable cost function, we can also use projected gradient methods.
00:46:58.844 - 00:48:25.344, Speaker A: Actually, the dual is well suited for projected gradient methods because we had have separable or separable constraints, and we can just project the gradient of the cost function very efficiently on the constraint set. Or we can use Dijkstra's algorithm and Dijkstra's algorithm is actually equivalent to block coordinate descent on the dual. It's a primal interpretation of the dual block coordinate descent. So here also block coordinate descent on the dual is very natural choice for solving this problem, because if you solve for one of the variables hi at a time, so hi would be one of just a small dense matrix. That's much easier, because then we project just hi is a projection on a smaller positive semi definite cone. So in all of these methods allow us to solve a problem like this by sequence of projections on small positive semi definite coins of order equal to the size of the clicks in the graph. But we don't need an I value decomposition of the entire matrix x only of the smaller sub matrices, so the clicks are not too large and we don't have too many of them.
00:48:25.344 - 00:49:42.560, Speaker A: These methods can be allowed, can allow us to solve much larger problems. And this is from a paper by efan where we took some matrices from the seed sparse collection. These are some statistics of the size of the matrix that goes up to size 100,000, but they're very sparse and the maximum click size is important. So the larger matrix actually has it's not necessarily the most difficult to solve because the clicks are smaller. So the conclusion is that by any of these different methods we can solve these matrix nearness problems by and the cost is dominated by eigenvalue dense eigenvalue decompositions of the side size of the cliques, or much smaller matrices than the original matrix. And then for the different versions, circulation, Ratchford or Dijkstra or gradient projection, we have a similar complexity per iteration because it's dominated by just these smaller dense height value problems. So that's one important application of everything discussed.
00:49:42.560 - 00:51:07.444, Speaker A: So it's used in these, in the context of sparse semi programming, in these conversion methods in combination with interior point methods or interior point methods, where you try to reformulate a sparse SDP by an equivalent problem with a much that's larger but has a simpler sparsity pattern or simple structure that might make it easier to solve by any tier point methods, or by just first order or splitting methods. The second application that actually will skip, or at least except for one slide, is another option in when we discussed at the beginning of the lectures, the post sparse semi definite programming, you can also view the original SDP as an conic linear optimization problem where x and s the variables are both symmetric symmetric matrices. So this is actually the primal problem in the usual notation. This is the dual and the usual SDP convention. But both x and s are sparse matrices, the cone and this cone k. So the usual dual cone would be the cone of positive semi definitely sparse matrices. The corresponding dual cone is a cone of completable sparse matrices.
00:51:07.444 - 00:52:17.014, Speaker A: So the difference with the standard SDP is that these cones are no longer cell dual. But we can use interior point methods for non symmetric optimization where the cones are not self dual. And this is a good example of such problem because for if the pattern is choral or a choral extension, then we've seen we can evaluate the barrier functions for the primal and the dual cone efficiently barrier functions and their derivatives. So it opens the possibility of solving these problems directly by non symmetric interior point methods. And this was actually the PhD of Martin Anderson who implemented this in the solver. And then the last thing I want to mention is an application of this first result I mentioned this morning. So the fact that for a choral pattern we can find the minimum rank positive semi definite completion efficiently.
00:52:17.014 - 00:53:50.974, Speaker A: So if you go back to a sparse SDP in the usual standard form, so x is the variable, the coefficient matrices are very sparse. So c and AI are all sparse with some common sparsity pattern e, then the key observation in sparse semidefinite programming was that we can actually look at the dense matrix x, the variables in the dense matrix x as including two types of variables. We have the variables that are important for the cost function and the equality constraints, and those are the actual important optimization variables. And the other variables that don't appear in any cost functional constraints are must be given values that given values that make the entire matrix positive semi definite. So that allowed us to replace the dense constraint x with the constraint that x is a positive semi definite completion. It also means that if we have a solution of this SDP, that we can always replace any feasible x by a completion of the important subset of the entries of x. So if you just take the entries of x that appear in the constraints and the objective, so that's the projection of x on the sparsity pattern, then we can complete replace x with any completion of that matrix, and we get another feasible solution that has the same objective and the same is still feasible.
00:53:50.974 - 00:54:55.464, Speaker A: And then we've seen for choral sparsity pattern that we can always find a completion that can be factorized as y y transpose, where the column dimension of y is the maximum rank or the largest fully specified sub matrix. So it's certainly bounded by the largest click size. So that means that every feasible solution of the sparse SDP can actually be written in this form. So the problem is actually equivalent to this non convex formulation where y is a rectangular matrix with column dimension equal to the largest click size. But this is completely equivalent. So every solution of the problem can actually be expressed in this form. So the minimum rank completion is actually useful, for example as a post processing of the result of an SDP solver.
00:54:55.464 - 00:56:10.380, Speaker A: For example, it might happen that this original problem is in SDP relaxation that we solved by any of the standard solvers. If the solution of the SDP relaxation is not unique, the optimal solution is not unique. There's a set of optimal solutions, then an interior point solver will often return an optimal solution of higher rank because the central path converges to a point of with a maximum rank actually in the optimal sets. So as a post processing, if the problem comes from an SDP relaxation and you're interested in actually low rank solutions, we can use this to round a solution from a standard SDP solver, any SDP solver, to a solution of lower rank. We're just using just this fact that for coral patterns we can efficiently compute minimum rank completion. It's also useful in these decomposition methods that I discussed, because in those methods we typically decompose or use the completion formulation of the SAP. So you're not actually computing X as opposed to semi definite dense matrix.
00:56:10.380 - 00:57:08.554, Speaker A: We're only computing the projection of X and the algorithms will return a positive seminar completable matrix. And then in the application we are actually interested in the full matrix x or actually a factorization of the full matrix x because we want to use it as a, for example, SDP relaxation. So there also it's important to extract from the solution of this decomposition methods the completion of low rank. So the maximum determined completion would be less useful because that would give us a maximum rank. Here we would be interested in the low rank completion. We have one slide with or two slides with an example of the first application. So, to go back to the power flow problem that I gave in the introduction.
00:57:08.554 - 00:58:00.154, Speaker A: So here we look at a set of benchmark problems for this optimal power flow SDP or relaxation of the optimal power flow problem. This is the size of the SDP, the maximum click size, and we just solve them by standard SDP solvers, Mosex Lumi or SDP T three. And the first column for each of those solvers gives us the rank of the solution computed by the solver, the numerical rank computed like this. Right? So the number of eigenvalues greater than a certain threshold. That depends on the maximum eigenvalue and the size of the matrix. So we see, for some of the cases we have an exact relaxation, so the rank is one. So that means that the relaxation was actually exact.
00:58:00.154 - 00:59:10.342, Speaker A: But in most cases we get a non exact solution and in fact the rank of the solution is larger than the rank of the maximum clique in the problem. So that means that the solution is not unique, because we've seen it's always very easy to find a solution with rank equal to the or less than or equal to the maximum click size. Every feasible solution can actually be rounded by this minimum rank completion to a solution with rank, certainly not greater than the size of the largest creek. So what we did in the second columns is that we took the solution from the dense SDP solvers and rounded them using the minimum rank completion to get something that has lower rank. And in many cases, or some cases, we found an exact solution by doing this. In other cases we reduce the rank to something much less than the original rank. So you can think of this as a rounding of a solution of an SDP solver to have a low rank.
00:59:10.342 - 00:59:59.324, Speaker A: It's not necessarily the minimum solution of the original problem, but it's an equivalent optimal solution of the relaxation with lowering. And this illustrates it. For the second example where we in Mozek or any of these solvers, we went from rank five to rank one. So these are the eigenvalues of the solution returned by Sudumi. So here we would say the numerical rank was five. If we use this threshold after rounding to an equivalent feasible solution or equivalent optimal solution, we get smaller I values. So this is the sort of the end of the lectures.
00:59:59.324 - 01:00:58.318, Speaker A: So this is summary we looked at the some results from completion theory with choral sparsity patterns. We have a number of very useful results from the theory of matrix completion and sparse matrix computations. So the decomposition results for the different types of completion and then fast algorithms that I formulated in way that extends the multi frontal factorization algorithm for positive semi definite matrices. Positive definite matrices. So for example, you can evaluate barrier functions efficiently, we can evaluate different types of completion, positive semi defect or euclidean distance matrix completion efficiently. And all of this is for chordal patterns, but it's useful for non chordal patterns. If we first do a choral extension, and then depending on the problem, this choral extension gives us an equivalent problem.
01:00:58.318 - 01:01:48.048, Speaker A: That's the case in sparse SDP, for example, there is no loss of generality in assuming the pattern is choral. In other applications, the choral extension gives you a problem that's no longer equivalent to the original one, but can be used as a way of finding suboptimal solutions. But in sparse SDP it's actually equivalent. And then these results are all very useful in sparse, large scale sparse methods, interior point methods, as well as the first order decomposition and splitting methods. So that's it. Thank you. Okay.
01:01:48.048 - 01:02:46.144, Speaker A: Thank you. Even so, there was one question which I tried to answer. I'm not sure I did a good job. So the connection about, I think the rank constraint completion problems. So I think it can be related to this slide. So this is very similar to the low rank formulation of optimization problems where we force a variable to have a certain low rank by just factorizing like this and solving this non convex formulation over y. So it's very similar to the famous Montero Burr formulation of sparse of SDP.
01:02:46.144 - 01:04:50.724, Speaker A: So one difference here is that for a sparse matrix, the conclusion was that we can actually write this problem in this form, and it's completely equivalent to the original one, because I can always, if I use for the column dimension of y, the largest clique size, then this is equivalent because every matrix x can be replaced with an equivalent feasible solution that has the same, that's still feasible and has the same value of the objective value and has the form yy transpose. So it could be useful for showing exactness of some relaxations, but only for when the largest click is very small. So I don't know if it's really directly useful in that context, but in some simple cases it might actually give you a quick way of showing that some relaxation is actually guaranteed to be exact in the usual way of using this formulation. In the Montero formulation people use, for example, the Papaki Borvino bounce on the rank of extreme points of feasible set of a sparse SDP. So there it's used differently. We don't use the largest click size as the column dimension of y, but we just make some assumptions on the rank or an upper bound on the rank for the optimal solutions and then solve it by the same formulation. So then the problem is not really exactly equivalent, because it's only at the optimum that we can assume that we have a low rank solution.
01:04:50.724 - 01:06:14.818, Speaker A: But I'd be interested to know the exact connections, I think. So what is the noteo formulation? Sorry? The material formation. Yes, that's a very usual useful method to solving large sdps in general. So without necessarily without, it's usually not done in the context of sparsity. But instead of using with this, if x is very large, instead of using this dense matrix x large dimensional dense matrix x as variable, we factorize it as y y transpose and the column dimension of y is chosen to be an upper bounds on the rank of the optimal solution of the problem. And there are some results on the maximum rank of the extreme points of the feasible set of an SDP by Pataky and Bardinok, and that can be used as an upper bound on the rank of the optimal solution of the SDP. And then the advantage of solving of this non convex formulation is that the size of the problem, so the dimension of the variable y is much smaller than the original matrix variable.
01:06:14.818 - 01:06:26.834, Speaker A: It's only n times r if r is the number of columns in y, but it's a non convex formulation, so you can have to solve it with different methods.
