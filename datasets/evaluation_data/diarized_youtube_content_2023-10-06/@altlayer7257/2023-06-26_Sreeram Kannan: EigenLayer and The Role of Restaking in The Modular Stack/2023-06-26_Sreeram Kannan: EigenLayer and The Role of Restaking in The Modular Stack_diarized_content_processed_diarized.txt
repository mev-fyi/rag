00:00:27.410 - 00:01:04.820, Speaker A: Hello. Good morning, everybody. Welcome to this talk on Eigen DA, which is a data availability mechanism for roll ups. In this talk, I'll try to motivate why we think this is an interesting mechanism. The talk is subtitled hyperscale an innovation for roll ups. And Amrit gave a great talk just before this on alt layer. And first, thanks to Amrit and Yauchi for inviting me to this great day, the roll up as a service day here.
00:01:04.820 - 00:01:54.030, Speaker A: So what I'm going to talk about is how do you design a data availability layer that supports this kind of an aspiration that altlayer presented, which is you want to support a large amount of throughput. You want to bring an insane amount of activity into the crypto ecosystem. So that's why we call it hyperscale open innovation for roll ups. Firstly, I want to thank several of my team members who contributed to this. It's always a joint effort and thanks to all of them. Okay, let me jump into the applications. So we want to start this with we are talking about hyperscale open innovation and the question is, why would somebody be interested in this? And I want to start with a motivating thing.
00:01:54.030 - 00:02:57.090, Speaker A: If you want to try to measure applications in the crypto ecosystem, you can kind of do this on two axes. One axis is what throughput do they consume, and the other axis is how much value per bit is transacted. What is value per bit like per bit of information that you're sending through the system? How much is a single transaction worth? Hundreds of dollars or it's worth like a penny. Okay, so why is this important? If you take the wavy or operating in blockchains today, very clearly we are in the top left quadrant here in the figure, which is we're transacting systems, mostly financial and NFTs, which have high value per bit. These objects that you're trading or sending are worth a lot, but it's not a lot of throughput. So that's the axis on which blockchains today are operating. But where we want to be, we want to enable new and interesting applications like gaming, social, high throughput DeFi.
00:02:57.090 - 00:03:36.674, Speaker A: Not only we want to go to the high throughput regime, but also we want to get to the low value per bit, like a Tweet. How much is a Tweet worth? Maybe it's worth a cent, maybe it's worth nothing. So you want to go to that regime. You are talking about a very different kind of system architecture which would get you there. But even if the value per bit is low, because the throughput is high, the product of these two is what the total value transacted through the system is. So if it's value per bit times bits per second, right? And so that's value per second. So that's this curve, right? Okay, so we want to get to this regime where you have very high throughput but low value per bit.
00:03:36.674 - 00:04:08.262, Speaker A: But the fundamental thing is therefore the transaction cost per bit needs to be small. Okay? So if you look at transaction cost per bit, you can look at what are the fundamental contributing factors to this. I think there are three basic contributions to the transaction cost. Number one is the capital cost. The capital cost is basically you need staking. If you want security, you need staking. And if you have staking, then there's a certain opportunity cost of capital.
00:04:08.262 - 00:04:56.282, Speaker A: So that's the first contributor to the capital cost. The second contributor to the capital cost is the second contributor to the cost is operating cost. You actually run the system, you have to download the data, you have to do all these things. Another third contributor to cost is actually congestion cost. And essentially what we've done is to think about how do we reduce these three axes. If you look at the third one, the congestion cost, the congestion cost is basically just if you look at the congestion cost, it's fundamentally coming from everybody who's transacting creates an externality on all the others. And if you're creating congestion, you need to pay for it.
00:04:56.282 - 00:05:33.798, Speaker A: So right now, pretty much all the blockchain is fundamentally priced for congestion. So you're basically running at congestion cost. So let's look at all these three costs and how we think about at Eigen layer of reducing these costs for the capital cost. If you want a certain amount of stake, really, and if you have $1 billion staked and you have to pay off the opportunity cost of that capital, you may have to pay a 10% Apr to actually compensate. That a 10% Apr of a $1 billion. You're talking about $100 million annually in fees that needs to be paid for it. So one way we can reduce the capital cost of staking is by having shared security.
00:05:33.798 - 00:06:11.774, Speaker A: You have the same pool of security supply not only to one application, but to many, many applications. Same thing, that same economy of scale that really powered general purpose proof of stake blockchains can actually power other things as well. So that's the capital cost. If you have shared security, the principle that Eigen layer builds on is called restaking, which is the idea that if you stake in ethereum you still have that capital that you can actually make additional credible commitments with. For example, you download and run and validate a new system. You can actually promise that you're running that system correctly. Okay, so we have capital costs, we have operating costs.
00:06:11.774 - 00:06:46.218, Speaker A: If you have every node needs to download and store the data, that actually adds up quite a bit over the entire system today. Actually, the operating cost is nondominant. Like even for a pretty expensive system like Salana, you have thousands of nodes. Each node maybe annually costs like $10,000. So you're talking about the annual operating cost is $10 million. It is high, but it is not as high as the capital cost. Because you have several five or 10 billion worth of stake, that means you're paying off like $1 billion in the staking Apr.
00:06:46.218 - 00:07:48.542, Speaker A: Okay, so you have the capital cost, you have the operating cost, which may not be dominant, but as you increase the throughput, it'll start to become dominant and finally you have the congestion cost. And the way to reduce congestion cost is actually by enabling shared throughput. You actually build high performance systems so that there is very little congestion to begin with. And you also build an economics where you don't have this unpredictable pricing of, oh, I don't know when the price is going to go up or down. And this unpredictable pricing actually leads to a large number of problems. Amrit in his talk earlier pointed out how these blockchains can actually reduce these blockchains can actually learn from existing cloud systems like AWS. And I think we want to do something like that here where for example, in AWS, you can actually reserve compute for a long enough time and you know that there is no congestion because you have reserved instances.
00:07:48.542 - 00:08:48.230, Speaker A: There's also another concept called a spot instance where you can go and buy an instance at the moment, but if there is congestion, that spot instance may be very highly priced. Okay, so that's the core thing. We want to think about the fundamental cost basis of the system and make sure that we are actually designing a system which is very low in the fundamental cost per bit analysis. Okay, so you just saw the roll up architecture earlier, so I'm not going to go over this, but the core idea is that right now, roll up sequencer takes the data and then posts it on a data availability layer. Why do we need to do this? You need to publish for the computation to be transparent, you need to publish the inputs to the computation because if you have the inputs to the computation, anybody can replicate the computation or continue the computation onward. So there are ways to slice and dice this. So you could either publish the inputs to the computation or the outputs to the computation depending on whether you're running an optimistic or a zero knowledge roll or a succinct validity roll up.
00:08:48.230 - 00:09:51.154, Speaker A: But you do need to publish the inputs or the outputs of the computation on a data availability layer. So that's what does the data availability layer do? It enables anybody else who wants to know the inputs or the outputs for this computation to come and acquire it. Okay, so what properties do we need from a DA layer? Okay, actually I'm going to just swap to different presentation. So here we go. I think the animations probably work better on this. Okay, what properties do we need from a DLA? We need the DLA to be hyperscale. Okay, what do we mean by hyperscale? We want the bandwidth of the system to grow linearly with the number of nodes.
00:09:51.154 - 00:10:21.870, Speaker A: The more the nodes in the system, the more the throughput that it should acquire. What do I mean by nodes in the system? The nodes which are serving for data availability. The more the nodes, the better the system should be. In fact, it should be the most ambitious goal would be exactly linear. You have N nodes, each of them have C bandwidth. You multiply the two N times C should be your system performance. That's the most ambitious, right? You're aggregating all the bandwidth fluidly, but there's at least this factor tool loss.
00:10:21.870 - 00:11:15.310, Speaker A: Why? Because we want to operate the system even when as long as the majority of the nodes are honest. So you do take a factor tool loss, but not that much. More like that's what hyperscale is, it's basically exact linear scaling for a DLA. Okay, so that's the scaling. What about the cost, right? If you compare to a system like AWS, which is a single node downloading and storing data, maybe there's like a couple of copies or five copies which is storing it and you want to get to that kind of a cost basis. Even though you want your system to have thousands or maybe hundreds of thousands or even millions of nodes, the cost basis should not be as though every node needs to download and store the data. So just to pick on the first point here on hyperscale, for example, what we are expecting is as the number of nodes increases in the system today, most of the systems don't have linear scaling.
00:11:15.310 - 00:11:51.902, Speaker A: They actually have no scaling. The more the nodes in the system, the capacity of the system remains the same. But what we're looking for is the more the nodes in the system, the system bandwidth scales linearly, but the cost does not scale at all. So that would be an insane system, right? Like you have more performance as you get more nodes. The cost doesn't increase and the security increases. So you're just like slicing the trade off perfectly in your favor. What would be the latency limit? If you're looking at an AWS system, you send a request, the request goes to the server and the server says yes, I stored the data.
00:11:51.902 - 00:12:35.242, Speaker A: So that would be like a round trip latency. And you'd want a system which operates at that kind of like a performance, so that your roll up nodes are not stalling on some data availability layer, waiting for 12 seconds or sometimes even twelve minutes for finality. It should be verifiable nodes. Anybody who wants to verify that the nodes in the DA system are doing their thing, you should be able to verify it without having to download all the data. So this is the property we call Verifiability, which is a simple light node can download and verify all the data. And finally you want to have it be customizable. Customizable basically means like somebody wants a different quorum, somebody wants to use their own token.
00:12:35.242 - 00:13:05.506, Speaker A: Somebody wants different kinds of properties. We want to be able to allow for all of these different kinds of properties. So this would be an ideal DA layer in our view. Hyperscale bandwidth, not a limit. Low cost, low latency verifiable and customizable. We are building a system called Eigenda, which is built on top of Eigen layer. Eigen layer is this core platform which is based on restaking.
00:13:05.506 - 00:14:07.962, Speaker A: So the idea of Eigen Layer is you stake in Ethereum, you set the withdrawal credentials to the Eigen layer contracts. Eigen layer is a set of smart contracts on Ethereum. So you stake in Ethereum, you set the withdrawal address to the Eigen layer contracts and in the eigen layer contracts you set your own wallet address as the withdrawal address. So what you're adding is really like a step in the withdrawal flow. And what this does is it really allows the system to enforce a system of checks and balances where if you're not operating for the services that you opted in correctly, then the system can enforce a penalty on you. So that's the basic structure of Eigen layer. Basically we call it restaking, which is you stake in Ethereum and then you set the withdrawal credential to the Eigen layer contracts, adding a step in the withdrawal flow but then enabling trust transfer to all kinds of other systems including like a data availability system.
00:14:07.962 - 00:14:54.538, Speaker A: Okay, how is INBA achieving this hyperscale property? The core idea is using erasure codes to distribute the information so that no one nodes need to download all the information. So you encode it and send it to all these nodes and each node only downloads a little bit. But they also come with cryptographic proofs that they've been encoded correctly so you don't need to worry about it. So we use KCG polynomial cryptography. This whole architecture of the cryptography underlying eigenda is basically built off of the Ethereum roadmap. In the Ethereum roadmap we have this thing called Donk Sharding, which is a mechanism to actually scale the data availability bandwidth of Ethereum. Eigenda is basically like an opt in system of Donk Sharding that we've built.
00:14:54.538 - 00:15:34.534, Speaker A: And because it's out of protocol, there's a lot of degrees of freedom that we have which actually lets us optimize even more than Dong Sharding. So the Dong Sharding throughput is roughly one megabytes per second. 1.3 megabytes per second. We are already at ten megabytes per second with eigenda. And whereas the node bandwidth requirement to run an eigen DA node is actually much smaller than all the other systems. Okay? So really one of our vision for Eigen layer is that anybody can come and build these new features on top of Ethereum security or borrowing, at least aspects of Ethereum security.
00:15:34.534 - 00:16:34.442, Speaker A: And Eigendia is basically and what we want to do is to enable people to experiment with new kinds of distributed system technologies that can then be built on top. And the best ideas can then be internalized back into ethereum over a longer scale. So Eigenv is our first kind of product on top of Eigen layer, which basically tries to achieve this awesome. So let me skip ahead to some of these other properties I mentioned. It's low cost, it's low latency, but I think one of the most important dimensions that we need to consider when building things like roll ups is how does it interact with the economics. And if you look at the roll up economics today, there is a bunch of problems with the roll up economics. And the first one is the roll UP's dominant cost is DA and DA cost is high, right? And if the DA cost is high, basically you have a problem like you cannot make it cheaper than the DA cost.
00:16:34.442 - 00:17:14.882, Speaker A: So that puts a floor on your cost basis. And the second problem that we find is the DA cost is uncertain. So you don't know because the DA cost changes based on congestion. Also you have same site externalities Amrit in his talk was mentioning about Yuga Labs. For example, if there's a Yuga Labs roll up and you have another you're checking along, doing your own other roll up, and suddenly there's a Yuga Labs Mint and the DA layer is congested, and then now your DA cost becomes uncertain. Maybe it blows up by ten x and you cannot satisfy your users. And that's really problematic.
00:17:14.882 - 00:18:06.438, Speaker A: So the DA cost being uncertain is actually not very good. And finally, roll ups, unlike an L one, an L one can fix a certain amount of inflation as the rewards for data availability inside their own system if you're an L one. But if you're a roll up, you can't pay in your own token for data availability. Even if you allocate a treasury from your own token, you still take exchange rate risk of oh, I need to exchange my token for ETH and maybe the ETH price goes up and you need to kind of eat it on your roll up. Okay? So roll ups with Eigenda can completely transform all these economics by getting the best benefits of shad security while retaining a lot of degrees of freedom. First thing is with Eigenda, the DA cost is low. Why? Because the cost basis underneath Eigenda is low.
00:18:06.438 - 00:18:56.706, Speaker A: And really by optimizing on all three axes of the capital cost, because of restaking the operational cost, because of the better distributed systems architecture and then finally for congestion costs, because throughput is high, there is not congestion. So you actually get the underlying cost basis is low. Not only that, there is no uncertainty in the cost basis. You can actually go to Eigenda. Like you reserve a reserved instance on AWS, you can actually reserve hey, I want 10 Kb/second bandwidth for the next one year and you can go and reserve that on Eigenda because Eigenda has plentiful data bandwidth, we are able to actually do this and many, many systems can. Many, many roll ups can come and reserve this kind of bandwidth and what you get is actually positive. Same side externalities.
00:18:56.706 - 00:19:58.770, Speaker A: The more roll ups come, the capital cost of the system is split among many more roll ups. The cost per byte actually goes down rather than going up. Finally, we allow roll ups when they're doing long term DA reservation to actually pay fees in their own native token. What this means is a roll up may come and say, hey, I want to bootstrap my system and I want to give the Ethereum stakers 5% or whatever of my token economy as a DA fees for the first year, but also that helps me bootstrap the DA and bootstrap the roll up. This is something that you can actually do on IGAD. Okay, so that concludes my talk. Basically, the core idea here is by optimizing the underlying cost basis of how you build a DA, you can actually achieve all the favorable properties hyperscale, low cost, low latency Verifiability, and customizability on a common data availability layer which is built on top of Shad security from Ethereum.
