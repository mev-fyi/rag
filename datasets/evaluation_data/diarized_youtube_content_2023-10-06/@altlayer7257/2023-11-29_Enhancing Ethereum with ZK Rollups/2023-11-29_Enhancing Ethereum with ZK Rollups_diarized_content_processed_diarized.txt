00:00:08.090 - 00:00:26.178, Speaker A: Hello. Good afternoon, everybody. Thank you very much to the organizers. It's a pleasure to be here. As Dorothy kindly already said, my name is Luke. I'm a research cryptographer and partner at Polychain. Today I'm going to be moderating the panel on enhancing Ethereum with Z K rollups.
00:00:26.178 - 00:01:23.874, Speaker A: We've got ourselves about half an hour, mostly of questions, hopefully some from the audience. At the end, we'll be speaking all about what zero knowledge roll ups are, what they use for, and how they're used for the fixing, or in this case, I guess the enhancing of Ethereum in a variety of ways, which will be represented by our wonderful panelists. Is it going to be for privacy? Will it be the scalability, perhaps verifiable, compute, perhaps something else and allow them to introduce themselves individually and what they're working on? But for this topic, we're very fortunate to have a panel that is absolutely drowning in technical expertise. So I think it's a great opportunity to take everything and explain it all like we are all five, some of the difficult things we've been thinking about and some of the hot topics and questions at the moment. So to start with, I'll let the panels introduce themselves and then we'll go into some rounds of discussion.
00:01:24.002 - 00:01:56.610, Speaker B: Good afternoon, everyone. My name is Ismail. I'm one of the founders of Lagrange Labs. So we build proving systems that allow us to verify massively parallel computations, things like SQL, Mapreduce, RDD, spark in zero knowledge when run over blockchain data. So this is very applicable when you think of kind of this new trend of zero knowledge applications called coprocessors. And when you want to increase the amount of data that your application sitting on a major blockchain can access at the point of execution.
00:01:57.990 - 00:01:58.642, Speaker C: How's it going?
00:01:58.696 - 00:01:59.250, Speaker A: Everybody?
00:01:59.400 - 00:02:18.122, Speaker C: Check. Can you hear me? Yeah. Bret Carter. I'm a technical product manager, at Risk Zero. If you were here a little while ago, you probably heard my talk. Worked at one labs prior to this at risk Zero. I'm primarily focused on bonsai, which is sort of a sandbox proving environment that scales horizontally quite well.
00:02:18.122 - 00:02:25.100, Speaker C: And then also our ZKBM or our platform team. Yeah, I think that's the basics for me.
00:02:25.870 - 00:02:27.494, Speaker D: Hey, everyone, I'm Bret.
00:02:27.542 - 00:02:28.662, Speaker E: I work at Taiku.
00:02:28.806 - 00:02:47.940, Speaker D: Taiku is not like a type one ZkvM, so we're trying to be as equivalent as possible to Ethereum. And we also try to focus on the decentralization, or at least the permissionless of L2s. In general, we have some interesting ideas about how to actually use roll ups in the most efficient way.
00:02:49.430 - 00:03:03.990, Speaker F: Hello everybody, I'm Nicola Leoson, I work at linear. Linear is a ZKVM deployed on Ethereum Mainnet since July, with over 1000 applications deployed on it and a few millions of transactions executed.
00:03:06.090 - 00:03:20.810, Speaker E: Hi everyone, my name is Ian, I'm the co founder of scroll. So scroll is a general proper scaling solution for ECM. We are building a Bico level compatible with the KVM to scale seamlessly. So if you are a developer, it's just Ethereum, but cheaper, faster, with a higher throughput secured by ZK.
00:03:22.210 - 00:03:54.230, Speaker A: Okay, thank you very much. Just starting looking backwards a little bit for zero knowledge roll ups and many other scaling solution currently used on ethereum. What would you say were some of the most fundamental components and largest breakthroughs in the creation of these technologies? Or I'd say more definitely their application in blockchain in recent years, less so. The 1970s and 80s work and what fundamental pieces there have been vital to the application and creation of your particular projects.
00:03:55.370 - 00:04:33.410, Speaker B: So for us, because we're focusing on proving massively paralyzable computation, things like recursion of the composition of proofs are one of the most important factors for us as we start structuring our proving instructions with a higher arity than we've traditionally seen used in zero knowledge. I think when we think of computation broadly, computation being structured as a data structure is a very important component of how we think of compiler design and how we think of language design. And being able to have the proving of that computation, also structured as a data structure, allows us to do things that were otherwise impossible.
00:04:35.450 - 00:05:14.618, Speaker C: Yeah, I think from our perspective, I guess what was missing before was actually the accessibility of ZK technology itself. So clearly there's been tons of innovation, improved systems. We're in some sort of renaissance when it comes to zero knowledge. Cryptography, the sort of blessing of blockchains and token economics is there's been tons of money poured in the field, lots of innovation. It's a very burgeoning space, but what's really been lacking is inaccessibility for the technology itself. And so the performance of proof systems are just getting to a point at which you could do something like a general purpose ZKVM. Not a lot of folks know this, but we have sort of a hand rolled proof system underneath our ZKVM.
00:05:14.618 - 00:05:57.678, Speaker C: So it's a stark based proof system. Our CSO, Jeremy Brustol, has been implementing the snark papers and the stark papers for years prior to founding the company and finally realized that these stark based proof systems were getting to a point at which we could actually have really acceptable performance. And so he innovated this field called Baby Bear. It's a very small field, it's very efficient for the work that we do in our ZKVM. And so definitely the lower level proof system performance itself has been a huge enabler. And I think the next major step will be the. I don't think we yet realize that we're about to experience a sea change in ZK based off of not just risk zero, but all of these other general purpose zkvms as well.
00:05:57.678 - 00:06:24.070, Speaker C: They're going to make the technology so much more accessible to developers. You're going to be able to iterate so much more quickly. We're going to see tons and tons of innovation. And I think that's a really exciting primitive that maybe this group will wake up to today or already knows, but definitely in the next year or so, proving performance as proving performance numbers for real world applications come out, everybody's going to be very impressed with how fast things have gotten and how easy it is to develop in ZK.
00:06:25.770 - 00:06:59.060, Speaker D: Yeah, one obvious one is obviously the tools are important, but yeah, if we're building some kind of scalability system and then obviously the performance and the cost is super important because otherwise it's just simply not worth it to do it. That's kind of like what makes this. Using it as a scalability solution just makes it feasible. I think where we are not there yet is kind of like this approved generation time or this latency. Because if we can also have low cost plus very low latency, then it opens up lots of new applications for ZK as well.
00:06:59.910 - 00:07:32.486, Speaker F: So I have to double down on recursion. It's kind of magic. The idea of a recursion is that you're going to prove that you have proven something, and usually the second proof is smaller and more easy to verify than the first one. And this allows to manage a lot of complexity because what you put at the end is the final proof. And you've been able to divide your proof system in 1020 50 small proofs. So that's a really big part. And another part that I think was quite important for the ZKVM is the design with modules.
00:07:32.486 - 00:07:49.780, Speaker F: ZKVM is very complicated and actually when you work on it, you divide it in independent modules that are very similar to what you have in cpus. And that's how you can manage the complexity. It remains complicated actually, but the way we can divide this makes it manageable. And it was quite important.
00:07:51.590 - 00:08:43.698, Speaker E: Yeah, for us, I think it's just basically already covered a lot. So for us, the QVM enabled by this recursion, like faster approver hardware, all those kind of innovations. But I think going forward, we still care about one thing in the performance side. That's why we are working on internally, we have some experimentation going on, working on some GKR plus smaller field, like new exploration around how fast the QVM can be. Maybe we believe that in one year we can get to a point where it's like in ten second you can generate a Ziki VM proof, which means for every block block builder, they can generate proof within 10 seconds. So we are kind of on the research side, doing very aggressive research about what technology is required for us to get there, and instead of doing just incremental improvements over our current proven stack. So that's something we care about.
00:08:43.698 - 00:09:28.930, Speaker E: And another thing is definitely we are in collaboration with the PSE team as well as some other project on is kind of like how we can kind of doing something like residual continuation where you can prove larger gas limit block. Because currently, because currently I think already exists leaky EVM has some upper bound for how many catches you improve per block and all those things. And if you make that become dynamic upper bond, then you can prove any transaction. So you turn this, you can't prove some transaction problem into like it just take a longer time to prove this transaction. So which means which will make this kind of layer to become more censorship recent because you can enforce any transaction and also in general enable more off chain computation.
00:09:30.150 - 00:09:58.460, Speaker C: Just a quick note, seeing ye talk over there, I think a huge necessity in the space is for open source proving systems and circuits and scroll deserves a lot of credit for that. Ye and the team right up front committed to open sourcing the vast majority of their work and have really opened up the space for a lot of others. And it's definitely a model we hope to follow. At risk zero. We want to open source all of the work we do that's such that everybody can build decentralized systems on top of our technology. So yeah, I think that's been a huge unlock for the space overall as well.
00:10:00.290 - 00:11:24.710, Speaker A: Yeah, I think starting with what Bret said at the end, the open source underpins what the thematic reason for the advancement in these. The efficiency and scalability as of the question of these system from a change improving system because the brain drain from 2016 up until now just became larger and larger. And more people with more ideas across a variety of projects are working together, bringing things like, as Ismail said, arity inside approving systems with the very original wide plonky two circuits that's brought in with different plankish constraints, and we change from having just enormous r one cs list into something that's sort of composable and you're able to now sort of separate into different modules of the ZKVM or different sets of opcodes, as Nicholas meant. I think all of that would not be achieved if people hadn't worked together so effectively. And the last piece, but I don't think is put into full production, maybe because of the actual tangible physical limitations, is maybe the hardware it's been worked on for a while. But understanding how and where is best to execute possibly a very expensive cost of thousands of GPUs or even Asics, especially if the proving systems and these huge field elements you spit out change, it churns at the top, you'll have an awful sunk cost. But I think hardware sees a lot of ultimate acceleration.
00:11:24.710 - 00:11:44.860, Speaker A: And the next question on ZK more broadly, not necessarily within the scaling of ethereum or even scaling systems as a whole, and not even necessarily in blockchain, what do you think is going to be the most powerful use case of zero knowledge for the foreseeable future?
00:11:45.310 - 00:13:06.950, Speaker B: Yeah, I mean, I think one of the things in the last question that I don't think anyone touched on, but I think is also very relevant and ties into this one, is the fact that the application of zero knowledge in crypto has made it far more relevant to actual industry problems than we've previously seen. Because fundamentally, with verifiable computation, we can shift the safety of a computation away from a lower trust proving environment onto a higher trust verification environment. And what we've done with these proof of stake chains is created a very high degree of safety and liveness over the consensus that nodes are participating in. And so functionally, with zero knowledge, you're now able to actually apply the verifiable computation you're doing into an environment where you have high degrees of certainty over the verification of what you're doing. And so you then also kind of open the question up is, okay, if you have these authenticated data structures that are being updated at a regular interval based on the consensus and some predefined set of rules, what can you then do with that type of authentication of that data structure that inherits the same safety as the consensus? And so some of the work that our team works on is, if you have now a blockchain that you can say with a degree of certainty, there is some commitment to some chronology of blocks. You can now then compute over that and inherit from that safety the same result of the computation that you want to then prove.
00:13:09.130 - 00:13:13.366, Speaker C: Awesome. Yeah, I have a hard time. It feels like almost predicting crypto prices.
00:13:13.398 - 00:13:13.594, Speaker F: Right?
00:13:13.632 - 00:13:48.674, Speaker C: Like trying to predict the future for exactly which use cases which will take root and sort of change the space. And so I guess the approach I would take is sort of trying to start from first principles and first understanding the technical primitives that we're just now beginning to unlock. One of them for us, as ye mentioned earlier, is like for risk zero. We implemented something called continuations. I believe it's a first in proof systems. And basically the way that it works is typically in snark based proof systems. In many proof systems, you have an upward limit on the amount of compute you can do in this Zk environment.
00:13:48.674 - 00:14:27.982, Speaker C: And what continuations does is it breaks an arbitrarily long program down into individual pieces, and you can just prove them in parallel and then recursively combine those individual steps. So we have that working in production, a succinct version, basically. Like for a computation of any length, we can recurse down to a succinct size proof. And that's now open source as of about three weeks ago. And so thinking about that, one of the exciting use cases for it are just coprocessors. You can take any operation that's very expensive to do on chain, and you can now do it off chain at roughly 99% savings over the gas costs you would have spent on chain. I think a really simple example that you all would understand is signature aggregation.
00:14:27.982 - 00:14:44.870, Speaker C: So literally, I can show you some code. We can batch thousands of ECDSA signature verifications in a for loop. And just like a few lines of rust code. One signature verification. For us, we have a standard unit of measure. You can think of it as one risk five instruction. We call it a cycle.
00:14:44.870 - 00:15:10.400, Speaker C: One ECDSA signature verification is about 800,000 cycles. We have an optimized version of it, and we've done proofs on bonsai, which is our horizontally scaling proving system, or, excuse me, proving back end. We've done proofs over 20 billion cycles at this point, so I can't quite do the math. I'm sure somebody up here could do it better than me, but that's thousands of ECDSA signatures we can verify sort of in one go with a really great security model.
00:15:13.010 - 00:15:48.274, Speaker D: Yeah, it's kind of like difficult and need to predict how it's been used for blockchain. It's kind of like mostly for interruptibility reasons. So it's kind of like, why do you generate the ZK proof? It's kind of like, okay, because this smart contract has to be able to somehow verify that something was computed correctly. I think you can also use that in other applications like off chain, because then it allows you to pick what data you share, but it also makes things composable because you can do interesting stuff on it. So, yeah, depending on how that could be useful, I'm not sure that's like something like SGX or like EE environments.
00:15:48.322 - 00:15:49.560, Speaker B: That is available.
00:16:01.570 - 00:16:15.940, Speaker D: Which is I guess in some ways quite similar to like Ziki pros. Well, yeah, some ways, and it's not used that often. So it's kind of hard to predict where things will go when these kind of like Zeke things are available.
00:16:17.910 - 00:16:48.286, Speaker F: Scalability and privacy, it's really the main things for Zkam and privacy. You can really go all over the places. There are some stuff that is obvious, like private payments, private operations, some stuff intuitive as well. Proving your nationality, prove your age without revealing it. You're over 18, got the right to work, you don't have to reveal any information about yourself. So there's a lot of very small things that can be done, and it's really an integration thing that anyone can work on.
00:16:48.308 - 00:17:33.246, Speaker E: I will say, yeah, I believe that definitely in a long, long future, like verifiable computation, for any general type of computation will become possible. Like you can run any program on a cpu and then you get the proof and everything is verifiable, but that's a long future. And what I see where we are here is that I definitely believe that Dick EVM is definitely the largest scalability application. But I also see more experiment on some ZK identity, something like use AK to help some real problem, for example. Objection. You might also need some privacy. And for payment, you might can use AK to prove some information without reviewing your kind of banking information or doing something like that.
00:17:33.246 - 00:18:52.758, Speaker E: There are some experiments, for example like ZkpdPR, where those are like smaller ZK application, but I think only if you get to use the key in your daily application and in a smaller scale, and then you are kind of starting moving this kind of needle, and then you keep improving on proving stack, and then the next big thing will be maybe mitigate machine learning, because it's structured circuit and for some specialized computation. So practically you can actually lower the overhead a lot, and then eventually you will get to the point where you can prove a general type of computation. But I believe that needs a lot of effort. Like for example for other zikirap like us, when we build this kind of really robust proof market where people are incentivized, build ASIC or build better and better hardware, and then combine it with all those factors and then you can get to a point. So I want to see, you need to see some practical way, you need to start with some small experiment and then you do something specialized and then eventually combining all those effort you can achieve there. And also another thing I want to bring up is that it's not like Aris application really need to be proved. So that's why I still believe theykee EVM will become the biggest use case because it's not like OpenAI want to prove their model like in they can prove to you that I have some model and this is a result because there are no incentive for them to really do so.
00:18:52.758 - 00:19:27.620, Speaker E: So that's why I think for Amazon and for a lot of things happening in cloud, it's not necessary to kind of really prove. But why ZK can tie so closely with blockchain contact is that blockchain really need trust. And ZK can have this kind of really nice publicly verifiability for any node to verify this very, very small proof. So that can be helpful in blockchain context. But I want to really emphasize that it's not everything need to be proven. So you really need to come from first principle that what do you need, what you are hiding, and then you can build some useful applications from there.
00:19:29.590 - 00:20:03.470, Speaker B: To add something. I think the value to your point of verifiable computation is constrained by two things or three things. It would be the input data to that verifiable computation has to be verifiable to some extent, and then the verification environment of that computation has to be able to be secure enough such that what you're verifying has any value. And then the cost of doing that verifiable computation, the value of doing that verifiable computation has to be high enough to offset the cost of actually computing it. And so as we decrease cost, we still need places where there is sufficient value behind doing this verifiable computation.
00:20:05.010 - 00:21:32.370, Speaker A: I think linked to the example he gave about the OpenAI, I maybe have a counterpoint. We see that the same founder, when working in crypto, sees the value of people monetizing the addition in a trustless manner of their own data into some kind of system with worldcoin, and you wouldn't be able to do like the OpenAI training model because you couldn't put that into a zero knowledge proof. But if there was some inference or just the input of the private data to be able to get a greater network effect, which is what blockchain as a coordination mechanism provides over just collecting data from various centralized streams, you could get faster trained models and have it private and know, have people monetize on it if that's what they wish. But over, yeah, I think great points on the co processor side, great points on just verifiable computation. And as Nicholas said, most of this is underpinned, or at least around something in scalability and privacy. We spoke sort of at the start about these different movements that we've had along the tech cycle or the development cycle of these zero knowledge proofs, the field sizes, the arity, and what they've done over the past four or five years to make the technology before us sort of viable in kind of systems. And that to me is sort of going down levels and making things faster, at least for the infrastructure as I see it.
00:21:32.370 - 00:22:41.840, Speaker A: But I want to take the last question I think we have going back the other way and think a little bit about the DAP side, both from the developer and from the user experience. What in your respective projects and systems, different choices have you made from the starks versus the snarks, to everything I see from real custom cryptographic operations inside of handwritten assembly circuits, all the way down to full general purpose egg vms and everything in between with different LLM compilations. LLVM compilations, sorry. And also LLM and machine learning models with compilers like EzKL and then coprocessors like Axiom are doing, and I think usually, but I think Bret and Risera would argue now a little bit against people say that the performance decreases a little bit as you encompass more and more and just put whole instruction sets inside of these circuits. What different decisions have you made, and how do you think that they're going to affect the DApps from both the developer and the user experience?
00:22:43.410 - 00:24:09.420, Speaker B: So with what we do, where we focus on massively parallel computation, we really are trying to push the boundaries on how much data we can compute over within one of our proofs. And so the constraint on this has historically been how relevant will this computation be with respect to the current state of the chain at the point that the proving of it finishes? So if the proof takes too long to generate and it's over just way too old data, then we're unable to verify that with respect to a commitment that's present in the execution environment. So we have to be fast enough on our proving such that we can prove over sufficient scale of data that can then be verified back at the point of transaction. And so in line with that, we've focused on some new properties of our approving systems to make them updatable with respect to long running computations that happen over very large sets of data on chain. So if you think about things like moving average of prices or volatility, or anything involving a ring buffer where you want to iteratively update that computation and the data that computation is being run over as the data in the blockchain changes, those are things where the majority of what you're computing, if it has any arity to it, is reused. And so with what we did with a recent paper we put out, and something that we're going to open sourcing later this year, you're able to now have very large and expressive proofs that can be iteratively updated as the data that they're being computed over, or the subset of data that they're being computed over changes.
00:24:12.990 - 00:24:49.990, Speaker C: When I think about what's unique about risk zero in particular, there's just a lot of things too we need to get into. I guess one of the ones that's most obvious is we really believe in our general purpose approach. I mentioned it earlier to some degree. But I think the reason we can do that is because we have a highly performant stark based proof system under the hood. What's nice about that proof system, it's agnostic over field, so it's quite configurable. We've also pretty cleanly abstracted the ZKVM over the proof system. So say, for example, plonky three comes out, which the Polygon team is doing terrific work on that, and plonky two, and say it just happens to blow our proof system out of the water, we can swap that out pretty easily.
00:24:49.990 - 00:25:29.890, Speaker C: Your program stays the same, you've written it in rust, you're always compiling it down to risk five. And so there's just a number of benefits that come from that general purpose approach, and we believe the performance. While it's actually available today, in many cases there are lots of use cases in which the cost of proving is totally acceptable. Obviously in a ZK validity roll up context, you're doing constant proving. And so that's like sort of the most exacerbated version of proving. Proving costs most difficult, sort of use case to make fit right now. But I think within the next year, we want to achieve roughly a one 50th optimization in terms of economic performance.
00:25:29.890 - 00:25:59.370, Speaker C: We want a 150th the current cost. And I can think of a roadmap that gets us most of the way there. We basically need to two x the performance, or one half the cost five to six times over the next year. And I can think of about four ways we can do that. So we should be getting pretty close. And sort of last observation is there definitely is some penalty in performance for taking the generalized approach. But what's nice is under the hood, we've made some really great innovations when it comes to recursion primitives.
00:25:59.370 - 00:26:24.520, Speaker C: So we have like an aggregation primitive for combining proofs from a like program. We have sort of a proof composition primitive, which allows us to, in the ZKVM, recursively verify a proof from a different program or a discrete program. And these recursion primitives allow a lot of flexible sort of use cases that I think we're really just now beginning to understand.
00:26:29.210 - 00:27:30.762, Speaker D: From the starting point. Like tech was trying to be like a fully ethereum equivalent, Zkevm, which at the time was pretty optimistic, but some other projects were even more optimistic where they said, we're trying to generalize ZkevM. But yeah, we found it very important that we kind of stick as close as possible to Ethereum, because developers kind of be able to deploy their applications without anything changing. But also from our side, where we can just reuse a lot of the existing tools and infrastructure provided by Ethereum. So that was forward looking, but the way we did it was kind of like with custom circuits. So instead of trying to target a general instruction set, we handwritten the circuits to be able to interpret EVM bytecode and everything around it. So yeah, that does bring you more performance, but in the future, probably it will be moving more towards a general CQVM approach, because it adds more benefits to your system.
00:27:30.762 - 00:27:59.190, Speaker D: Because as an ethereum equivalent roll up, you also try to keep up with ethereum itself. So Ethereum keeps updating. So every time we have to update the circuits, it's a lot of work. So we kind of try to avoid that. And also being equivalent also allows us to do more interesting stuff, so we can kind of execute the same thing that is possible on Ethereum, directly on L2. So we have a direct interoperability between layer one and L2 that allows us to more easily built an ecosystem.
00:28:00.410 - 00:28:32.954, Speaker F: So I think I named three things, actually. So first being exactly equivalent to the EVM and to the global experience that you're having with layer one. It includes, for example, the RPC calls. Strange RPC calls that actually a lot of people are using. It's actually quite important for DApps and it's a choice that we made at the beginning to be 100% compliant. And we've got really a very good feedback about this. Second point is on the kind of internal architecture, we decided to have a lot of very different modules, very isolated.
00:28:32.954 - 00:29:13.766, Speaker F: It means, for example, that the prover is totally isolated from the tree generation. We've got actually standard clients and it's something as well on which we have a very good feedback. We've got 50 people running nodes outside of a cluster because they want to have access to the data. So that was quite useful as well. And at a very internal level, the choice that we made was to reuse, to use very advanced, but ignore in the ZK space, in the, I would say web free space primitives. A lot of people are speaking now about GKR, for example, it's something that we introduced in 2020. GKR is done by Rollwasser on the verse.
00:29:13.766 - 00:29:42.226, Speaker F: It's totally safe, incredibly good, but nobody was looking at it in the web3 world. And in 2020 we said, hey, it's actually very useful. Look at what you can do. You can do GKR for the first level, then you do a recursive proof to use it on Ethereum. And it was something that had, I will say, game changer on the performances of the system. Something that we've done on the ZKVM is to use lattices. Lattices are very well known when it comes to quantum safe cryptography.
00:29:42.226 - 00:30:10.670, Speaker F: It's used for signatures and so on, but nobody was looking at this inside the Webfree environment. And it allows actually to have very fast polymal commitments. So it's something that is both very secure, but as well a big differentiator in terms of performances of approver. And it was choices that were kind of complicated to do instead of, oh yes, just reuse stackware stuff. But I think at the end of the day, it makes a lot of differences.
00:30:12.450 - 00:31:01.678, Speaker E: Yeah. From user and developers perspective, I think our EQM definitely want to be as close to because we are bico level compatible. All the EVM toolings can be reused and we are already reusing ECM's client to kind of produce our blocks. So it's easier for us to have client diversity and all those kind of nice upgrade oeips to kind of merge with scroll and as we develop, I think one thing which I want to quickly mention is that everyone talk about their hyperscaling like their L2, layer three, build on top. But I think one fun fact is that we are the only ZK based L2 that support EC pairing verification. So which means all the ZK stack, polygon, CDK, all those stuff can't deploy on their own Ziki rob, they can only deploy on scroll. So which means that's one interesting application.
00:31:01.678 - 00:31:46.490, Speaker E: So we are not only targeting at the solidity application, but also if you are building some Vic application, you can easily verify proofs on scroll. So which enable a wide range of vic application. So that's also something which is very unique to us right now because it's not just a narrative, it's like something real, like leaving our magnet. I think another thing which I want to mention is we keep a very high standard to security because I think for user and developer I think security is definitely something very important. And it's also not so directly feeling by people like users see how fast you can withdraw money but they can't feel how secure you are. But we are keep doubling on security side. For example, one concern ZQM has is maybe your ZikvM has some bug.
00:31:46.490 - 00:32:40.266, Speaker E: So we are working with automata on some kind of IGX prover. It's already successfully proved all the testnet blocks and so basically can prevent even if you have bug, there are some way you want to suffer from this problem. So I think security definitely something and as I also mentioned, like performance, but we want to improve the performance to a level that we think is necessary. Because if you are making incremental improvement, for example, your proof generation move from 15 minutes to five minutes, it doesn't really influence anything because you need to wait for aggregation and then you will have like 30 minutes block time. But if you really make that like 10 seconds, that can change a lot of things. So we are kind of doing experiment on whatever is needed to move there like at the end goal, not just like the local incremental move for user and developers.
00:32:40.458 - 00:32:50.670, Speaker A: Well, we are out of time. But thank you very much for the journey and justifications for all of your designs and particularities. Please give a round of applause for our panelists.
