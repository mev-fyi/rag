00:00:00.330 - 00:00:43.500, Speaker A: Sam hi, everyone. This is here the head of investments at Mata Labs and today we'll be talking about the economic considerations of modular roll up services. Now, normally I'd begin by trying to define what a roll up is, but given what we've seen on Twitter recently.
00:00:43.580 - 00:00:44.736, Speaker B: It turns out it's a little bit.
00:00:44.758 - 00:01:30.740, Speaker A: More difficult than what it seems at face value. Now, all one has to remember is that a roll up tries to move parts of the execution from the base layer from the l one off chain while saying something about the guarantees of the data availability. And then it ultimately ends up posting a smaller version of the data that is required to reconstruct the state or at least say something about the validity of the state back to the L one. So the raison deterral is to guarantee the correctness of off chain execution as well as say something about the data availability behind that execution.
00:01:31.560 - 00:02:57.228, Speaker B: And a few other things to remember. The roll up nodes actually depend on the host chain in order to be able to update their state. Because the host chain defines the state transition function and as a place to nominally store their history, the history of the actual roll up, we know that the actual layer one, its consensus and therefore its security, does not. In any way rely on the roll up, because it's not waiting on a block by block basis to determine what happens on the roll up. Instead, every once in a while, the roll up will post its state, which has to be consistent with the rules defined by the L one. And the bridge between them, if you were to use the word bridge, is the ability for the layer one to check the validity of the state transitions that are proposed by the L two every once in a while. Now, usually people talk about this idea that the roll up in a sense, partitions the host chain's consensus or really its state by allowing it to apply a subset of rules that only really apply on the roll up so long as they are globally consistent with the rules that are prescribed by the L one.
00:02:57.228 - 00:04:23.140, Speaker B: Now, because the roll up needs to know about its own state as well as the state of the L one at all times, then the cost of running a roll up is strictly higher than running just an L one because you're basically now having to take care of two different chains. So this must mean that there must be some utility that's generated by using a roll up that justifies spinning it up. Now, this might seem obvious, but since we're going to talk about the economics of roll ups, I think it's important to highlight. Now, something to remember is that blockchains by design are extremely computationally intensive. It costs approximately 10 million times more to do a calculation per byte on ethereum than it does on AWS. That is the cost of achieving the properties that are promised by a distributed ledger. And this is really the sole motivator of considering roll up economics because a minor improvement in the execution environment can reduce the costs dramatically and you can see that this gap is enormous.
00:04:23.140 - 00:06:17.890, Speaker B: And so there is sort of a lot of margin to be gained by trying to cheapen the execution environment while preserving censorship, resistance, decentralization and so on. Now, the basic idea is that instead of submitting transactions or a series of transactions on the L one and incurring the sort of base layer cost you move this process off chain and there is a set of actors that are responsible for essentially doing this and then posting the results back on chain. Now, depending on the type of layer two whether it's sort of an optimistic design or a pessimistic design pessimistic here refers to validity proofs. Of course these operators behave in slightly different ways and use very different technological architectures but the basic principle is sort of the same. And as economic agents they have to think about the following question how do I provide these services while pricing them in such a way that it is still profitable for me to do so? And while the user experience is at least as good as what they would experience if they were executing directly on the L One. Now, there is this concept of application specific blockchains which I refer to more generally as specialized execution chains. Whether roll ups or not while using validity proofs or fraud proofs or whatever type of mechanism they handle the execution in a modular way.
00:06:17.890 - 00:08:25.156, Speaker B: And this modularity relies on a simple principle that you are able to share the security of the underlying L one which has a cost. And perhaps there is also a way to communicate between these different layers with or without going through the base layer. Now, the utility of these modular setups lies in their ability to remove the constraints that are normally imposed on a network that's trying to bootstrap a cheaper execution environment but also incurs fragmented, liquidity, different UX, additional latency. And the premise is that these new primitives that are being built from a technological perspective allow these modular networks to build a stack that is vertically integrated and does not detract away from the user experience and certainly offers a much cheaper execution environment. Now, when it comes to the economics of these layer twos, the name of the game here is to achieve positive network effects for unit economics without materially sacrificing decentralization and certainly not security. And what I mean by positive network effects for unit economics is that you basically escape the paradigm that you're normally constrained to on the L one where the more transactions that you have on a monolithic L One on average the higher the cost of a single transaction becomes. And that's basically a statement of network demand, right? For a fixed computational supply.
00:08:25.156 - 00:09:19.640, Speaker B: As the demand for computation increases the price increases on the L two. And that is really the fundamental insight. There is this ability to batch transactions together and then there is this economic regime where it is possible that as the number of transactions increase across the network you can actually have lower costs. It's simply due to amortization. You compress some parts of the data that's required and you do so by allocating costs to multiple users. Now of course if there's only a single user of the network this is sort of moot. But in the case that you have many many users each with a small subset of transactions then you can actually have decreasing costs.
00:09:19.640 - 00:10:47.396, Speaker B: Now by definition again the utility of the user of the L two has to be strictly larger than the cost of running the L two individually and the cost of publishing data to the L one which I really term here as the cost of inheriting the security. There's sort of no free lunch. When people say that the layer two in some way inherits the security of the L one what they're really saying is that there is a cost to publishing data back to the l one as well as the cost to actually reading the state of the L one. Now, why is all this important? Well, a layer two economically must be designed such that the utility of using layer two for a particular user is strictly higher than the utility generated from using the L one. And the same is true for the operator we call a layer two economically sustainable. When the utility of both the operator of the roll up and of the user is strictly higher than operating on l two than it is on the L one. Now here's where we get into some of the architectural trade offs or design trade offs associated with these different agents.
00:10:47.396 - 00:12:23.232, Speaker B: So in general we have somebody who's responsible for the data somebody who's responsible for ordering the data somebody who's responsible for the state of the roll up itself and then somebody who's responsible for generating some sort of proof that's associated with the validity or the correctness of the state transition. Now again, depending on whether we're talking about a validity roll up or an optimistic roll up the generation of the proofs is a sort of fundamentally different process. But when it comes to sequencers in many ways they're sort of the same. The sequencer is the agent that is solely responsible for ordering the transactions and then they send that sequence of transactions to someone who ultimately uses them in some capacity to update the state. And the idea behind running a sequencer from an economics perspective is straightforward. Basically the sequencer has to design usually algorithmically on an order of the transactions that is strictly different than the order in which the transactions come in that maximizes some sort of revenue. Now of course there might be some cases or some instances in which the ordering itself is constrained by some sorts of fairness algorithm.
00:12:23.232 - 00:14:31.820, Speaker B: I mean in the case of centralized sequencers where the user simply trusts that the sequencer will order transactions, let's say as first come, first serve basis, the sequencer is simply paid for the fact that they do this. Of course, the sequencer runs a particular set of hardware which requires some off chain compute costs and they at least need to cover those costs and perhaps make a small profit on top of that. The rules by which they order the transactions highly depends on the environment and also the actual designation of the agent as a sequencer. So in the case where you have a single sequencer it's sort of trivial. This person is guaranteed where this operator is guaranteed to be sequencing their transactions and so their economics are sort of straightforward whereas there are cases in which the sequencer is decentralized so that you have multiple agents that can perhaps bid or can be randomly selected to generate the canonical ordering of transactions. And it is in that case that the economics start to become nontrivial first because you now have to ask multiple agents to participate knowing that only some of them might be able to propose the sequence of transactions. And so if that's the case, then there also has to be some sort of profit sharing scheme amongst them so that the ones that are idle or at least the ones that are not at the front of the line are able to cover their opportunity cost or at least their sort of hardware costs.
00:14:31.820 - 00:16:11.448, Speaker B: Now, this in and of itself is an entire sort of area of research. But the thing to remember is that the sequencer even if they were to try and be malicious, really, at best they can only censor transactions or they can slow down or in some cases, stop the operation of the roll up, but they can't insert malicious transactions because the process to guarantee the integrity of the transaction is sort of completely separate from the sequencer function. However, what that means is that we still have to worry about censorship, resistance and the liveness of the roll up. The roll up is useless if it is completely secure but doesn't do anything. And so you want to design the sequencer architecture so that a it maximizes liveness and B it also enables multiple sequencers to come in not only for the sake of decentralization but actually for redundancy. And this is where the economics become very very important. And there are different sequencer design approaches, some of which include bonding or a proof of stake type sequencer where the sequencer guarantees some ordering of transactions.
00:16:11.448 - 00:18:09.112, Speaker B: And if that guarantee is sort of not upheld by the sequencer, then they are slashed. They can also guarantee certain transaction selection criteria, they can price their own transactions, they can use a global market for determining fees or a local market for determining fees. And most importantly, as we mentioned before, they can also say something about the fairness of transaction processing. And what that means basically is that they commit x ante to a scheme that orders the transactions and you can go back in time and audit that and make sure that that's indeed what the sequencer has done. Now, the really interesting part about the economics of running a sequencer is that now that you have basically separated the security of the underlying L one from the execution environment, you now have opened up a design space of different types of economic instruments like derivatives that could be used to run a sequencer. A sequencer in many ways has to hedge against the fluctuation of their variable costs and their variable costs are of course tied to the L one and they're also tied to the transaction demand. And so you can easily see that having something like a collateralized sequencer obligation which is an instrument that's sort of analogous to a debt obligation but in this case the debt is not monetary but rather the users commit to using a particular sequencer and their commitment has to be guaranteed somehow.
00:18:09.112 - 00:20:09.380, Speaker B: And then the sequencer in response to that commits to some sort of soft finality and deposits collateral as a bond. And what that does is instead of being subject to the fluctuations of the spot market whether it's on the L one costs or on the transaction demand costs on the L two the sequencer has some sort of forecasting ability in terms of the demand for their services. So this becomes very interesting because you can now have a set of sequencers in a competitive market go out and issue different types of instruments that are tied to their future performance and these instruments can be priced based on their previous performance. So in the same way that you have credit rating agencies issue different analysis and ratings for the credit worthiness of different financial institutions or companies based on their balance sheets, based on their workforce, based on their previous performance. And that enables the marginal creditor to think about the risk that they're taking by allocating capital or committing to a long term contract or some sort of service agreement with this particular provider. You can now develop a reputational system upon which you price the future performance or rather discount the future performance of these different operators. So that becomes quite interesting not just from a pricing perspective but also it makes the economic forecasting problem for the sequences themselves much more tractable.
00:20:09.380 - 00:21:40.876, Speaker B: And this is particularly valuable in the case where you're using some sort of proof of stake based sequencer where it becomes very important to know upfront what the cost of running the sequencer is going to be for a given period of time. Because there is also the cost associated with hedging that exposures, particularly if the bond is not in a numerair but rather in a different currency, something like ETH, so that the operator can hedge their exposure to that currency as well. So now it's probably a good time to talk about mev. And what that generally refers to is the capacity to reorder transactions in such a way that the person who does the reordering or the entity that does the reordering benefits at the expense of another entity. And of course, from their perspective this is a positive and from the counterparty's perspective that's a negative. But it's useful to make the distinction between basically the two different types of mev. There's accretive, what I call accretive and what I call parasitic mev.
00:21:40.876 - 00:22:52.700, Speaker B: Parasitic mev is exactly what it sounds like. Where all of the agents external to the sequencer experience a net negative, or at least the vast majority of them, it's net negative due to the reordering of the transactions and accretive is sort of the opposite. Now, a good example of parasitic mev would be something like sandwiching right on some sort of exchange. And a good example of accretive mev would be something like interoperability as a service, whether it's with different types of roll ups, auction clearing is another good example. These are sort of accretive to all of the participants external to the sequencer. Now, my hypothesis there is that there is enough diversity already in roll up providers and now I'm referring to different roll ups and not even necessarily sequencers. That parasitic mev.
00:22:52.700 - 00:24:52.020, Speaker B: The operator has no choice from a social perspective but to rebate it or pass it back onto the user because there is enough of a competition for roll up providers, let alone different sequencers, that will incentivize users to move away from the environments in which there are parasitic mev. Now, obviously, in the case where for a single roll up you have a market for sequencers, that just becomes even simpler. In the case of accretive mev, that's where the argument could be made that in the same way that a private company provides some value for a user and it gets to keep a bit of that value in the form of profits. The operator or the sequencer that provides accretive mev to the users will be allowed to keep that as part of their P L and it shouldn't be surprising, because something like auction clearing obviously requires running some sort of algorithm as well as incurring hardware costs. So of course the operator should be paid for that and the users would be happy to, so long as it's within their utility framework to pay a little bit extra for the profit of the operator. Now, speaking of profits, we know that as we mentioned before, a marginal improvement in the cost of execution off chain can provide a dramatic improvement in the overall cost of execution because the cost of execution on the L one is so high. But another thing to remember is that on the L one, the inter machine workload variance is actually quite small.
00:24:52.020 - 00:26:40.442, Speaker B: So by that I mean most validators are running similar workloads. And why that's important is that on the L Two the variance is actually quite large. And the reason for that is the same reason that we mentioned before, this idea of decoupling between the execution, its efficiency and security. This leads to what I call a sparse market for L Two computation in the sense that you now have a very wide spectrum and therefore economic opportunity for the operators to prove different types of transactions, different types of workloads and sequence different types of transactions as well. So now the name of the game becomes what different optimization routes or strategies could be used to reduce the ultimate cost of running the L Two, whether it's from the sequencer's perspective, whether it's from the proverbs perspective and so on and so forth. And this is where of course, we have validity proofs that come in all different kind of flavors and forms. We have data availability solutions that change the architecture and therefore the economics of storing and querying state data, how you can become extremely efficient in terms of sampling some of that data to reduce the burden of storing the entire state.
00:26:40.442 - 00:28:28.350, Speaker B: And perhaps maybe you only need to store a small subset of that state, which then leads to this interesting problem of state growth management. So L Two S, while the way in which they operate is sort of fundamentally different from the L one because of the decoupling that we discussed before, at the end of the day the roll ups themselves have a state that they need to keep track of and the state will grow over time. It might grow slower than what you'd expect for a normal L one, but again, that sort of depends on the architecture and the operational model. But at the end of the day they run into exactly the same state management issues as the L one does. Basically it becomes more difficult to sync the full history of the node because you simply have a large amount of data. So the name of the game then becomes how do I incentivize external operators to efficiently store parts of the state and enable querying parts of the state and how do I do so without burdening the roll up itself? Because if you need a roll up for the roll up, then we haven't really achieved very much. So instead, in your palette of resources and operations that one considers in designing a roll up, they also have to consider how do you manage the state of the roll up so that you don't run into exactly the same problem that you would normally on the L one.
00:28:28.350 - 00:30:35.330, Speaker B: And this is where the data availability problem comes into play. And ultimately the cost of verification of the validity proofs or of the fraud proofs as well as the cost of publishing the data back to the L one, always provides a sort of ceiling for improvement because you then rely on the L One itself to improve in order for you to reduce those costs. So instead you have to think about, okay, how does one improve state management on the L Two side and incentivize off chain data availability using different types of models, volition, validium and so on. And how that introduces coordination problems, latency problems, but also how that allows different roll up operators to bootstrap their systems from scratch. So now another topic that we touched on was how does one enhance liveness and finality on these layer twos? And again, returning to this decoupling, we now have this interesting opportunity for different types of economic agents to come in and actually improve the liveness properties on the L two S without necessarily having to coordinate or having to participate in the day to day operation of the roll up. And here's what I mean by that. You can now have what I call dedicated failsafe sequencers that are paid a small fee that is weighted towards liveness without actually processing any load.
00:30:35.330 - 00:31:27.640, Speaker B: Day to day. They sort of behave as an insurance sequencer. And if you think about insurance contracts in general, you generally pay a premium for something that you hope that you will never use or never need to use. And that's interesting because in that model you're basically paying for an agent to just sit there and do nothing. And so long as they can provide certain guarantees that in the event that something happens that they're able to intervene, you're happy to pay that premium. These sequencers are basically being paid to be redundant and just sort of sit there. And the reason that's interesting is because you can pay these agents in a non native token so that it's not tied to the security of the L One in any way.
00:31:27.640 - 00:32:53.090, Speaker B: The other interesting aspect is that we know regardless of redundancy, regardless of decentralization, you can never guarantee 100% uptime. That is a real problem for L ones. But for L Two S you can also never really guarantee 100% uptime. But what you can do is actually make the net cost of downtime much lower when you use these sort of clever economics. So you can essentially socialize in the event of significant downtime, you can socialize the loss across a multiple set of insurance providers and that are willing to underwrite that process. Now, when it comes to finality, we know that it's ultimately determined by when the proofs are posted back to the L One and then the last roll up state that's considered to be valid is now sort of canonical and immutable. So the name of the game here becomes how do I provide a market for finality? And that's simply determined by the transaction inclusion.
00:32:53.090 - 00:35:03.390, Speaker B: And we know from a user's perspective that they can force inclusion in the L Two at the expense of some minimum cost. So assuming that they have no time preference, it is strictly cheaper for them to send their transaction to the Sequencer. But in the case that they do have some time preference, they now have to weigh that against the cost of including the transaction themselves and the probabilistic guarantee by the Sequencer that they're going to include the transaction. So, in the same way that we discussed these issuance of these derivatives tying the performance of the Sequencer to the native demand, we can also include a similar type of instrument that guarantees inclusion within N blocks or within a finite period and these themselves could be priced. So not only can the sequencer guarantee or forecast something about their future costs and the transaction demand on the L Two, the user can also instead of using this very coarse gas scheme lever that they're normally used to using in mempools on Ethereum, they can now have an entire spectrum of inclusion guarantees. Which gets us into the topic of transaction quality. And by transaction quality, I'm really referring to the spectrum between a pristine transaction, which is a single atomic transaction that is very cheap to execute and is included effective immediately, versus a series of complicated transactions that are expensive to perform and atomicity is not guaranteed.
00:35:03.390 - 00:37:06.254, Speaker B: And most transactions, or most series of transactions as a subset sort of lie somewhere in between. And again, you can now not only have a market for single transactions, you can also have a market for bundled transactions. And the reason you can have this is because you can now have these specialized Sequencers that offer you these sort of services. You can have in the case of multiple Sequencers where one of them specializes in DeFi like transactions or in NFT like auction like transactions and so on, they can offer you these bundled services as the most market efficient way of executing them. So you can see, just based on the discussion that we've had before, that the added benefit of having multiple Sequencers is not just from a redundancy perspective, but also because it adds this new vector of customizability of transactions that on the L one were normally handled through off chain agreements or through something like flash bots. Whereas now you can have a much more general purpose, a much more robust way of doing so and more importantly, you can price it. And so now in an era in which applications will start to really think about how to subsidize or even eat the entirety of the Gas transactions and just incorporate them as part of their PNL in Order to improve user experience, they really need to be able to price not just single transactions and there are latencies and atomicity, but also they need to be able to price bundles.
00:37:06.254 - 00:38:07.462, Speaker B: Atomicity of course, is sort of trivial in the case of a single transaction, but it's very, very important in complex. So now that we're talking about pricing, there is one thing that I want to highlight, which is that the network fees on layer two S fundamentally differ from those on the L One. And it's really for a very simple reason. The fundamental difference between L One and L Two economics is that the native L one assets command something that looks like a store of value supremacy, whereas the L Two simply does not. And the reason for that is quite simple. If you inherit the security as an L Two from the L One, you must be economically subservient. And what that means is that the security layer has to be guaranteed by an asset that is not native to the roll up.
00:38:07.462 - 00:39:46.742, Speaker B: But what it does do is that it opens up the design space for off chain data availability and other off chain services, because now the roll up can serve as the vendor for these types of services. Which brings me to perhaps the most important question. How do these L two S, if they were to issue tokens, how does value accrue to these tokens? And more importantly, does one even need a token for a layer two system? And there are sort of two schools of thought. One school of thought says that simply because security is inherited from the L One, the L Two token can at best serve as a bootstrapping mechanism for the L two ecosystem. But it is designed fundamentally so that it becomes less useful over time. Once enough activity is aggregated onto the L two, the L two token itself, its value will sort of decay. The other school of thought is that the layer two itself, because it can serve as a vendor for these modular services that we described before, that it now effectively acts as the marketplace for all of these off chain computational services.
00:39:46.742 - 00:40:50.440, Speaker B: And so that can be accretive not just to the L Two itself, but ultimately to the L One, to the extent that it enables more economic activity on the L One than what you would nominally get without the existence of the L Two. So then here the question becomes can a layer two justify the existence of a token not just purely based on the mechanics of running the roll up, but instead, can it offer value to the L One that wouldn't exist otherwise? And with that, I will leave you with some open questions. And again, it was an absolute pleasure to be here. If you do have any comments or questions about anything that I discussed here, as well as these open questions, obviously, please do reach out. Thanks again.
00:40:51.930 - 00:41:07.770, Speaker A: Bye. Subscribe.
