00:00:08.730 - 00:00:47.530, Speaker A: Good afternoon, everybody. I'm Sriram, founder of the Eigen Layer project. Today I'm going to talk about some myths in data availability. And I just made this talk in the last half hour. So there may be errors, but this is based on what I've been hearing kind of through the conference and trying to set the record straight on some of these things. So the first one is restaking doesn't secure Eigenda. So eigenda is a product which is built on Eigen layer, which is a restaking protocol that you stake ETH.
00:00:47.530 - 00:01:23.794, Speaker A: And stakers and operators opt in to serve new services. But some people are saying, for example, that restaking doesn't really secure Eigenda. I want to explain the trust model of Eigenda. So how Eigenda works is it's a service built on Eigen layer. Stakers opt into Eigen layer and then opt in to serve Eigenda. And Eigen layer itself is built on a dual trust model. There are two aspects of trust that one can borrow from Eigen layer.
00:01:23.794 - 00:02:05.198, Speaker A: Number one is economic security. Stake a bunch of ETH and then make a claim that you are going to validate these services correctly. And if you don't, your ETH will get slashed. So for objectively attributable faults, ETH staking provides economic security. There's another dimension to there's another dimension to Eigen layer, which is actually decentralization. You can borrow decentralized node operators who are actually serving Ethereum. And because these are decentralized node operators, you get a certain amount of collusion resistance, which means they're not all the same, and they're not going to collude together.
00:02:05.198 - 00:02:36.986, Speaker A: So Eigen layer itself serves both of these trust models. It's economic security for objectively attributable faults. And collusion resistance comes from decentralization. Eigenda is a data availability service, which means it does two fundamental things. It downloads and stores the data. The eigenda nodes download and store the data, but also they have to serve the data. And the way Eigenda borrows trust is it borrows trust on the economic security from ETH.
00:02:36.986 - 00:03:19.062, Speaker A: Staking by using a protocol called proof of custody. Proof of custody ensures that the proof of custody protocol is kind of conceptually simple. If you're claiming to store a bunch of data, you have to compute a function on the data and some private secret, and then you have to raise your hand or send a message if the hash of this function is zero, for example. So what proof of custody does is to ensure that you're actually holding custody of the data. And if you don't hold custody of the data, you will get slashed. So I canda borrows economic security from each staking. However, there is another dimension needed for data availability.
00:03:19.062 - 00:03:56.294, Speaker A: It's not enough for nodes to store the data. Nodes have to serve the data. What if we all just storing the data and then never serve it to anybody? Then the data is not really available. So how do we get nodes to serve the data? Each of the nodes have an incentive to serve the data, especially when each of the node can operate its own fee market for service. This is because the data is not held only by a few nodes. The data is dispersed across many nodes and any subset of nodes. If you're erasure coding at ten x ratio, then any 10% of the nodes are sufficient to serve the data.
00:03:56.294 - 00:04:41.334, Speaker A: So there is no monopoly in service. What this does is to create a competitive market to serve. And this requires decentralization, because if everybody is the same guy, then they can all collude and then refuse to serve data. But because it's a highly decentralized set, you can actually get the incentive to serve. So eigenda is actually built on the security that comes from eigen layer, not only the economic security, but also decentralization. So one of the things you have to do if you want to borrow decentralization is to keep the node requirements to be the minimal. Because if you say that I need a one gbps bandwidth to run a node, very few people will be able to do it.
00:04:41.334 - 00:05:15.978, Speaker A: But eigenda node requirements are kept very low so that you can actually have a highly decentralized operator set. So that's the first myth busted. Okay, what's the second myth? DA on alternative data availability chains increase ethereum l two security. Oh, if you have to be an Ethereum l two, you have to use data availability sampling. Otherwise you're not secure. Okay, so let's see. Data availability sampling helps.
00:05:15.978 - 00:05:47.526, Speaker A: What is the use of data availability sampling when a majority of the nodes are malicious? Even if a majority of the validator nodes are malicious, you can check for yourself that the data is available. However, data availability sampling cannot be performed by the Ethereum. Roll up contracts. Imagine you want to be an l two. The main thing you're writing is a roll up contract. Roll up contracts cannot do data availability sampling. So if a majority of the DA nodes are malicious, the roll up contracts will be fooled.
00:05:47.526 - 00:06:19.842, Speaker A: It doesn't matter that you're sampling and you realize that, yeah, the data is not available. You cannot go do an intervene on the roll up contracts because they are static, immutable contracts on Ethereum. So data availability sampling does not influence Ethereum roll up security. Myth busted. Data availability sampling does not increase ethel to security. Okay, myth three eigen DA is a DAC. Other das are not daCs.
00:06:19.842 - 00:07:03.778, Speaker A: DAC is a data availability committee. The image that is supposed to conjure or is just a multi sig or a small number of nodes. That's just saying something from the Ethereum viewpoint. Only thing that the Ethereum rollup contracts can see whether you're running any other DA chain. It doesn't matter from the Ethereum roll up viewpoint, it just knows that a majority of the validators have signed off on the data. Any non native data availability solution appears like a data availability committee to know you're just basically getting a signature of a group of nodes. These group of nodes may be staked, they may be rotating, they may have economic consequences for doing this.
00:07:03.778 - 00:07:45.490, Speaker A: All that is true, but there is no way to get any better security than like a committee. Different DA systems will have different trust assumptions on these DAX. So from the viewpoint of Ethereum, everything is a data availability committee. Okay, here is another myth. Myths may be true or false. So let's examine the myth. Data availability sampling on alternative DA chains helps secure roll ups which are built natively on these chains.
00:07:45.490 - 00:08:38.130, Speaker A: Okay, so what data availability sampling does is to help you secure against a malicious majority of nodes of validators. And if you're building a native roll up on a chain which performs data availability sampling, what happens is light clients which use the data availability sampling will not be fooled. So if you build a native roll up on a data availability chain, you actually absorb the strong security that the data availability sampling gives you. This myth is confirmed. So actually it is true that DAS data availability sampling on a DA chain secures native rollups. It doesn't secure Ethereum rollups. So the only way to secure Ethereum rollups is to do data is to use the Ethereum data availability layer.
00:08:38.130 - 00:09:27.358, Speaker A: What's the next myth? Data availability sampling is a mechanism for scaling consensus performance. If you do data availability sampling, as you have more nodes sampling the network, the performance increases. This is not true because data availability sampling is fundamentally verifiability scaling. What it means is a network is actually maintaining the data from the outside. You want to know whether the network is actually storing the data or not. One way you get to know is by asking questions, hey, give me this data, give me that data, and you're able to get it. That means you can be sure that the data is available, and instead of downloading the entire data, you're only downloading a few chunks.
00:09:27.358 - 00:10:31.894, Speaker A: So data availability sampling is a scaling of verifiability. Nodes outside the system actually don't need to download the data, but inside the system, the consensus nodes, which are actually holding the data, depending on the design, may need to actually download all the data and propagate it through a peer to peer network and so on. And so data availability sampling by itself does not scale network performance because consensus nodes are still downloading all the data. Okay, eigenda cannot have data availability sampling. Myth six data availability sampling is actually a relatively simple primitive to build if there are erasure coded commitments. If the block header contains the commitment to an erasure code, you can actually build sampling. Data availability sampling does not help Ethereum l two s, which is what we saw.
00:10:31.894 - 00:10:58.410, Speaker A: And eigenda is purely a data availability system which is adjacent or adjunct to the Ethereum roll up ecosystem. It's not a chain on its own. It doesn't do anything else. It doesn't do consensus. There's no blockchain. I canda is purely an ethereum adjacent, Ethereum centric data availability system. And so we didn't build data availability sampling.
00:10:58.410 - 00:11:36.662, Speaker A: However, data availability sampling can be built out of protocol permissionlessly. What does it mean? For example, let's say you're building a blockchain like Ethereum. It's very important to get, let's say, the block size right, the consensus protocol right, and so on, because anybody else cannot change it. But anybody can build a wallet. To Ethereum, data availability sampling is something similar. Anybody can build a light client. Once you have erasure coded commitments in the network, busted eigenda can have data availability sampling.
00:11:36.662 - 00:12:24.650, Speaker A: It'll be, to the extent that it's useful, people can build it. Myth seven eigenda scales with the number of nodes. So I mentioned earlier, the data availability sampling itself doesn't actually help scale. What helps scale the network performance is making sure that each node's footprint, each consensus node's footprint is very low. And that's exactly what happens in Eigenda. Every node only downloads a little bit of the data, so the total da bandwidth of the system scales linearly in the number of nodes, and eigenda keeps node bandwidth low. Remember earlier I was saying that we need more decentralization in our trust model, and so you can actually get it with Eigenda.
00:12:24.650 - 00:13:10.402, Speaker A: Okay, meteor data availability challenges rock. So what's a data availability challenge? Is basically an optimistic data proof. What is an optimistic data proof? Somebody claims it's kind of like an optimistic roll up. In an optimistic roll up, somebody claims that if I execute this, this is what happens, anybody can challenge it, and whoever is correct, they keep the money and the other guy loses the money. It's exactly the same idea for doing data availability challenges. However, there are two major problems with data availability challenges. The first thing is data availability is not attributable on chain, unlike execution.
00:13:10.402 - 00:14:01.000, Speaker A: If I run something and put it on Ethereum, then Ethereum can check later whether that is correct or not. Whereas if I claim that the data is available and somebody challenges, Ethereum does not know whether I am correct or the challenger is correct. So this leads to a complex incentive problem. The second problem is the throughput problem. Imagine I run a data availability layer with 10,000 times Ethereum's throughput, and the challenge window is one day. Now, if somebody challenges and you ask me to post all the data on Ethereum, and if you post the challenge after the one day period, I may have to post 10,000 days worth of data onto Ethereum, it's going to take me 10,000 days to post all the data. What are we talking about here? Simply doesn't make any sense.
00:14:01.000 - 00:14:45.750, Speaker A: Busted data availability challenges don't work in the last 1 minute. I'll just explain what we are actually doing. I just mentioned a bunch of myths here. Eigenda is a hyperscale Ethereum centric data availability system. It's built purely as an adjacent adjunct data availability layer to Ethereum. What do I mean by this? Ethereum has its own consensus protocol which orders all these roll up commitments. And so we don't need to actually go and reinvent the wheel and build exactly the same thing by building a new consensus protocol so we can completely ride on top of the consensus in Ethereum and only do data availability at a stations.
00:14:45.750 - 00:15:12.058, Speaker A: So it allows us to make trade offs. Simply a separate chain cannot do, because it couples consensus and data availability. And we've optimized the system for high scale, low cost reserved bandwidth. In Eigenva. You can reserve how much bandwidth you want for a whole year. You can pay in multiple tokens with fixed rates negotiated a priori. You can have multi quorum staking.
00:15:12.058 - 00:15:35.460, Speaker A: You're building your own roll up. You want your own roll up token to have utility, you can use it in Eigenda as a part of dual staking. Okay, last slide. We've been working closely with Alt layer, and one of the things that we are envisioning to do is everybody can build an l two on top of Ethereum using iconda. Thank you.
