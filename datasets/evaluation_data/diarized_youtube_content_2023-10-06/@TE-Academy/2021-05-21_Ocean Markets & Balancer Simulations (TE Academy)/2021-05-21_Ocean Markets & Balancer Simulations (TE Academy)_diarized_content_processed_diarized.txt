00:00:00.170 - 00:00:03.200, Speaker A: Thing and then we are good to go. Great.
00:00:03.970 - 00:00:18.158, Speaker B: Yeah, that will be part of my talk, so you can go ahead and record. Yeah. So let's get going. So I will share my screen. Let's see. It's always different with everything. I think you might need to give me permission to.
00:00:18.158 - 00:00:19.102, Speaker B: Oh, there we go.
00:00:19.236 - 00:00:25.120, Speaker C: Perfect. Okay.
00:00:25.930 - 00:01:15.950, Speaker B: And can you guys see my slide slides? Perfect. Okay, so yeah, this talk is about Ocean market balancer simulations for Te academy. In it I'm going to cover a few things. I'm going to give a sort of very short summary of what the research questions are, which is the basis of research, of course, an intro to a few things, ocean simulation verification for electrical engineers, which is the world I come from, token engineering simulation and verification. And then I'm going to talk a bit about ocean system token engineering, ocean market token engineering, and for the balancer simulations work, this is kind of the bulk of it here. So I'll drill into that quite a lot. Then I'll be drilling into the roadmap part of this research and beyond and conclude timing wise, just to set an expectation.
00:01:15.950 - 00:01:17.814, Speaker B: We have an hour and a half.
00:01:17.852 - 00:01:20.326, Speaker A: 2 hours, an hour and a half.
00:01:20.428 - 00:01:24.502, Speaker B: Okay, great. I'll just talk 25% faster than if that's all right. Yeah.
00:01:24.556 - 00:01:25.160, Speaker A: Okay.
00:01:25.850 - 00:01:26.806, Speaker C: As always.
00:01:26.988 - 00:02:01.314, Speaker B: Yeah. So just record this and play it back at half speed later on. Okay, let's get going. I can talk really fast, and I have had caffeine today, so let's get going. So I'm just going to minimize the screen here for myself. Okay, here's the summary of research questions and so on for very quick context. Ocean market under the hood uses balancer amms doing token engineering to model these, verify and optimize them in the context of ocean market is highly useful on its own.
00:02:01.314 - 00:02:38.206, Speaker B: Ocean is powering this open data economy, which is growing exponentially. And you can also view it as a well defined subset of the broader balancer ecosystem. So we can extend the scope to that once we've got a handle on ocean market dynamics. And of course, once you've modeled that well, you can keep going right to broader and broader defi, et cetera, et cetera, as you'll see. So here's the specific research questions that are more near term. I talk about some future stuff too. The two main ones are can we capture the dynamics of ocean market and data ecosystem in the current release of ocean, which is v three?
00:02:38.308 - 00:02:38.960, Speaker C: Right.
00:02:39.730 - 00:03:22.618, Speaker B: And that can be viewed as the system identification problem. A key tool for this will be token swice with EVM and loop as I will talk about later, this includes capturing of some of the observed issues, things like when people publish, they become a bit of a data token whale, and then it gives them the opportunity to rug pull. So that's the problem. So, as part of our ocean v four work and ocean v four plus, we have proposed mechanisms for that. And we're interested in seeing how well do these mechanisms work, at least in simulation. So these mechanisms, of course, can be modeled simply by having the solidity code running that in the EVM and seeing what the dynamics look like. So there's a few different mechanisms, and I'll get into that later.
00:03:22.618 - 00:04:05.882, Speaker B: So that's the big picture about this talk. But there's a lot of context, a lot of extra information, and I'll drill into that now, building up towards these research questions. So, first of all, two slides on what'socean, if you go to the web page, this is actually a few months ago, but it's tools for the web3, data economy, where we are treating data as an asset class. And there's two main sets of tools. There's ocean market app, which is a web app that has on ramps and off ramps for data into ERC 20 tokens, and essentially a dex to swap and stake these data tokens. The off ramp is to consume them as data sets, basically. So ocean market app is for that.
00:04:05.882 - 00:05:06.266, Speaker B: And of course, people can earn by selling data, curating, staking, et cetera. And you can also build your own app by forking the front end and the other libraries for your own data exchange, data market, et cetera. And at the heart of this, it is these ERC 20 data tokens, which means that metamask becomes a data wallet, balancer becomes a data exchange, Aragon becomes a data dow, et cetera, et cetera. Here's a good mental model to think about how Oshun is sort of on the left is step one, on the right is step three, and in between is all of defi, which we can essentially leverage fully for data. So on the left, basically, it's an on ramp where you have a file or some rest API or otherwise, as a service that you wrap with an ERC 20 token, which we call data tokens. And we have the libraries, et cetera, the web app, et cetera, to do that. And then once you have those data tokens, then they can flow around like cells in a bloodstream, blood cells in a bloodstream, et cetera.
00:05:06.266 - 00:05:54.746, Speaker B: In the broader defi ecosystem, whether this is data wallets, like I said, where data mask becomes a data wallet, et cetera, data dows, data exchanges, data provenance for tracking things like the history of exchange of data, things like Etherscan and Coinmarketcap help a lot there, and even going a little bit crazier, things like using Nexus mutual for data insurance or stablecoin based on data, using dai or reflexor or something. And then finally, the off ramp is simply taking 1.0 of your data tokens for a given asset and saying, I want to consume this, please. That is the three steps, and this is the basis of what ocean market looks like. Ocean market and v three has been live since last fall. And yeah, we continue to steadily build it out. Hey, Angela, I see that there's questions and comments popping up.
00:05:54.746 - 00:05:58.240, Speaker B: Should I try to address them as I go, or should I get through the talk first and then do questions?
00:05:58.770 - 00:06:09.906, Speaker A: I think it would be a good idea to go to questions before it's too late. But in this case, the chat messages are mainly me dropping links there. So for the moment, I think we are good.
00:06:10.008 - 00:06:33.046, Speaker B: Okay, cool. All right, and. Sounds good. I'll keep going then. So this is ocean in a nutshell. I come from the world where I spent almost 20 years in the world of semiconductors, building CAD software for people to design and verify their chips, focusing on analog and memory and custom digital circuits. And there's verifying design side, and I'm going to talk a little bit about that to give you guys a flavor.
00:06:33.046 - 00:07:29.126, Speaker B: So one of the challenges is whenever you manufacture a chip, there's variation, plus or minus tolerances, et cetera, and it's basically atoms out of place. When you are manufacturing the thing, those atoms get out of place just due to the process variation. When you're doping these chips, et cetera, which is actually truly a statistical phenomenon, which leads to performance variation of the transistors, which leads to performance variation of the actual circuits, the amplifiers, the filters, the bigger radios, et cetera, right? And for the longest time, for decades, this was a very small problem. It was like less than 1% variation overall, less than 1% performance variation. But starting in about 2004, 2006, it started to get really bad. It went from, in one or two process generations, from about 2004 to 2008, it went from yields of 95% to yields of 5%. For example, when Sony went to ship its PS three, they were delayed by about a year simply because they couldn't ramp their yield.
00:07:29.126 - 00:08:05.078, Speaker B: So they had the thing manufactured, but only five in 100 ps three s were actually good enough to ship. And that was a big problem for showing at Sony, incidentally. They became one of our very first customers, and PS four came out and they didn't have issues. And I'll get into that. So one of the challenges, for example, memory circuits, is you've got a memory circuit where there's a bunch of these bit cells on the left. Each b is a bit cell, and if you have a one gigabit memory, that's 1 billion bits, right? And you have some other circuits around it, you have the bitline precharger on the top, you have a San samp on the bottom. You don't have to understand this, just understand that overall you've got a billion of these bit cells, each with six transistors or more.
00:08:05.078 - 00:09:13.278, Speaker B: So you've got 6 billion circuits, sorry, 6 billion transistors total plus. And you want to try to verify this, right? And you want to verify it where it's even extreme variation, because each bitcell, if you want the overall chip to say, have 95% yield, that means each bitcell has to have. So 95% yield is 5% failure rate, right? So if you say 5% failure rate for one bitcell and 5% for the next, very quickly you get to 100% failure rate across the board, like 0% yield, right? So you really need a failure rate on the order of one in 10 billion for each bitcell, right? If you want to estimate that failure rate, it takes you 100 billion or 1 trillion samples if you're going to just do simple Monte Carlo sampling. So even if you have a simulator simulating 1 trillion samples would take on the order of a month on a cluster of 1000 machines, if you're lucky. If you're not lucky, it's a year or more, right? So really a big problem. So how do you go about doing that? And the answer is you have to basically pull some mathematical tricks, use some AI, and be very smart about what you simulate, and then build the right mathematical models to handle that. And this is actually what I did for a living in my previous work, and so on.
00:09:13.278 - 00:10:09.230, Speaker B: So here's an example of verifying a memory bit cell. This is representing the PDF, the distribution, but instead, look, it's this monotonically increasing set of dots, right? That's because it's not a description of the PDF, but it's a description of the CDF, the cumulative density function, where the y axis is scaled in a log fashion. And that makes it much easier to visualize. If this line was fully, fully straight, it would mean that you have a linear circuit, small perturbations in lead to linear responses on the outside. But here, it's not a straight line, which means that it's actually a nonlinear circuit, right? So, basically, we were able to construct this CDF overall, we can construct it with about 10,000 samples, takes about 10 minutes on a laptop, and that's from about ten years ago, actually, versus running it, the full thing for a million samples or more on a small bit. So, like this, it would take you. Yeah, you're talking weeks, months.
00:10:09.300 - 00:10:10.014, Speaker C: Right.
00:10:10.212 - 00:10:53.802, Speaker B: So we managed to chop down the simulation very well. But the key thing is that we built these as tools for practicing circuit engineers, memory engineers, to use as they develop their memory. So, you know, these sorts of tools are used by. This tool in particular is used by, for example, engineers in TSMC as they develop the next generation of Moore's law, right? The next generation chips. They're used by engineers in Qualcomm, Sony, Nvidia, et cetera, all to verify their chips, right? If they didn't use these tools, they'd go to manufacturer and they'd have 0% yield, and then they'd go broke, right? So these tools are absolutely necessary, right? And in circuit land, simulators have been around. Good simulators have been around since 1972, called spice is the first one that was really good. Ever since then, people have extended spice to make it faster, faster, faster.
00:10:53.802 - 00:11:30.098, Speaker B: And now the verification tools use spice in the loop and have higher level, smarter sampling algorithms, visualization, et cetera. And that's what I described here. It's a smarter sampling algorithm using AI tricks, et cetera, in order to simulate fewer circuits and at the same time get results that are useful to verify the design, right? So this is a standard practice in circuit land for about ten years to do this, in part thanks to the work that I had done. This is an example in memory chips. Here's an example. This was memory bit cells. So that's the things in the middle, the billion bit cells that you might have in a one gigabit memory chip.
00:11:30.098 - 00:12:08.898, Speaker B: There's also sense apps, and they have a different shape and they have a different purpose. So in this case, this is an example of a result on the bottom, where basically it's simulating and verifying the sense amp once again, in this case, 150 random variables to account for with their own pdf. And then you have to propagate that up to the higher level performances like power consumption, delay, et cetera. And you want to verify how's it doing? Am I going to yield or not? Et cetera. And once again, to verify this thing, use spice in the loop. And AI tricks to reduce the number of simulations so that it doesn't take too long. Once again, standard practice.
00:12:08.898 - 00:12:43.166, Speaker B: So that's examples where you have uncontrollable variables that are random. Right. Modeling these random processes, besides those uncontrollable random variables, you also have, worst case uncontrollable variables where you have to work across the whole space, for example, from a temperature of -40 degrees celsius to plus 40 degrees Celsius, or a power supply voltage of 1.0 volt to 1.2 volts or whatever. And there might be five or ten different variables. And if each variable has, say, ten possible values, that's ten to the ten possible combinations to look at.
00:12:43.166 - 00:12:48.254, Speaker B: And you simply can't look at all of those. That's just too many. That would be 10 billion simulations.
00:12:48.302 - 00:12:48.466, Speaker C: Right.
00:12:48.488 - 00:13:21.100, Speaker B: It takes too long. So what do you do? Well, here's an example of a problem. If you're trying to verify the vco of a PLL, which is circuit jargon, but basically a PLL is a phase lock loop, trying to get a circuit to its sort of oscillations to get in sync with another circuit's oscillations. And the VCO is one of the building blocks of that. And in this case, there were 3375 corners, which sounds small, but the challenge is, simulating one of these suckers takes about, I believe, an hour. I forget. Exactly.
00:13:21.100 - 00:14:09.338, Speaker B: So it would just take too long to do all of these. So how do you do it? Well, one way to solve it is cast it as a global optimization problem where you're just sampling iteratively, and then you basically try to minimize performance. You try to keep searching through the space to make it perform as worse as possible. And basically, that is the opposite of what you normally do in optimization, where you're trying to optimize the goodness of something, right here, you're trying to optimize the badness of it. And in the end, once it's converged, then you know that you found the badness with a degree of confidence and you can stop. How do you do this? Well, it's spicing the loop once again, running around in the space of variables of changing temperatures, changing voltages, et cetera, and then optimizing. And in this case, there were, let's see if you were going to use all of them.
00:14:09.338 - 00:14:20.382, Speaker B: It was 3375 simulations. In this case, it verified here, it stopped at 160 simulations. So that's about a 20 x improvement, which makes a difference, right? It's 20 days versus one day.
00:14:20.436 - 00:14:21.040, Speaker C: Right?
00:14:21.570 - 00:14:56.426, Speaker B: And yeah, I won't get into the details, but once again, it's spice in the loop with some AI optimization tricks to reduce the number of simulations. So this is once again standard practice. For example, this specific tool Nvidia has used for more than ten years to verify every single chip that it chips, right? Every AI chip, every other chip too, every graphics card, et cetera. And yet most of the big semis use it as well. It just gives you an example. So that's the world of electrical engineering, CAD tools and simulation. The overall CAD industry for electrical engineering is about 6 billion, 7 billion, growing 15% a year.
00:14:56.426 - 00:15:16.366, Speaker B: The simulation submarket of that is about 600 million a year. Actually, that was ten years ago. It's probably 900 million or so now. So let's then go token engineering, simulation and verification. And once again, I'm coming from the electrical engineering field. So taking my learnings from there into token engineering. And I wrote a blog post about verifying token based systems.
00:15:16.366 - 00:16:15.906, Speaker B: Came out about half a year ago, actually closer to a year now, I guess. And basically I define token engineering verification as evaluating the token based system to find out whether it meets the specified requirements. And of course you can have uncontrollable variables, et cetera too, or just simple straight up simulation. And you can, even as a starting point, we can recognize this is a dynamical system, right? A failure is when it gets stuck in a region of state space that doesn't reflect design intent. For example, in circuit land, if it completely fails in performing at the temperature of -37 degrees celsius and 1.2 volts or something, right? And then it just fails, that means it's failed overall, right? It wouldn't work as a design. And you might say, okay, well, in token land, where do things like do, where does all the different verification types fit in?
00:16:15.928 - 00:16:16.258, Speaker C: Right?
00:16:16.344 - 00:16:57.634, Speaker B: So I'm just going to talk about three types of verification. There's digital, analog, and mixed signal. So digital verification in circuit land is things like digital circuits that are on and off and so on, whereas analog circuits are continuous in time or continuous in value. Mixed signal has a combination of analog and digital. If you've got a bigger system on a chip and it's got some analog subcomponents and some digital components, that means it's mixed signal. Yes. If you're doing pure digital verification is much easier, right? You can do things like in token land, you can do things like security audits where they're just checking for bugs or Sat solvers and other sort of formal verification that people have been using for the last few years.
00:16:57.634 - 00:16:59.854, Speaker B: But that doesn't capture the analog dynamics.
00:16:59.902 - 00:17:00.162, Speaker C: Right.
00:17:00.216 - 00:17:03.134, Speaker B: And the incentive designs is really analog dynamics.
00:17:03.182 - 00:17:03.826, Speaker C: Right.
00:17:04.008 - 00:17:40.478, Speaker B: And we see failures in analog dynamics every time. There's, like, someone taking advantage of, say, a flash loan to hack some defi protocol, right? And we see that every week or every month or whatever in various ways. So you can verify this, too, using tools of circuit land and otherwise. And this stuff has not become very widespread yet in token land. But it's my hope that they do. And in fact, if you think about tokens, any given token design has both the incentive side, which is an analog problem, and the smart contract logic, the ons and offs, et cetera. So you really do have a mixed signal verification problem.
00:17:40.564 - 00:17:40.862, Speaker C: Right.
00:17:40.916 - 00:17:56.070, Speaker B: You've got to verify the incentives and the logic at the same time. And that's really a harder problem yet. But we have to acknowledge that that's the problem we're trying to solve. Fortunately, there, once again are solutions from circuit land that we can port over to help leverage.
00:17:56.490 - 00:17:58.118, Speaker A: Trent, can we pick a question?
00:17:58.204 - 00:17:58.918, Speaker B: Yeah, sure.
00:17:59.004 - 00:18:02.790, Speaker A: How do we identify confounding effect of variables?
00:18:04.330 - 00:19:13.534, Speaker B: Yeah. So simulators themselves account for them, right? What you have is a mapping from, say, ten controllable variables, design variables, et cetera, and, say, 50 uncontrollable variables, random variables or worst case variables. And all these variables together go through a mapping, a function, to an output, say, power consumption, right. Or many outputs, power consumption and area and a bunch of other things, right? And so if that mapping is linear, then you don't have any interactions among the variables. But if that mapping is nonlinear, depending on the nonlinearity, you have interactions, right? If you have variable one times variable two, then those two variables are interacting, right? So that's how I view it, as simply, the variables are not independent. Well, they can vary independently, but they have interactions that lead to. And the interactions matter in the outputs all over the place, right? So whenever you can, electrical engineers try to design circuits that are as linear as possible, right? Because linear design is a heck of a lot easier than nonlinear design, right? But you can't get past that.
00:19:13.534 - 00:19:37.202, Speaker B: There are very nonlinear aspects of circuits, right? For example, you go back to, let's see here, this bit cellar, right? It's curving. That indicates nonlinearity. Same thing with a sense amp. You've got a couple of step functions here. That's nonlinearity, although in the middle here, there's a linear ish behavior, and that's actually the main operating region, right? And a bit cell in general.
00:19:37.256 - 00:19:37.426, Speaker C: Right.
00:19:37.448 - 00:19:44.834, Speaker B: It's flipping on versus flipping off. That's strongly nonlinear. Right? And all digital is nonlinear, but analog, you're trying to stay linear.
00:19:44.882 - 00:19:45.046, Speaker C: Right?
00:19:45.068 - 00:19:53.690, Speaker B: In fact, a lot of people call analog circuits, linear circuits, because the aim is to try to make them behave as linearly as possible. Yeah, that's useful. Thank you.
00:19:53.840 - 00:19:54.250, Speaker C: Great.
00:19:54.320 - 00:20:20.238, Speaker B: Yeah. So by the way, too, for a lot greater detail about this, I did write a textbook on verification of circuits. It's now, I guess, several years old, but it's probably very pragmatic here. Happy to reference it later. So in token engineering verification, you don't have to rely just on simulation. And depending on how hard that is, you can do other things. And I've come to frame it as three steps, human based, software based, and economic.
00:20:20.238 - 00:21:07.374, Speaker B: So human based is typically starting with subjective discussions among yourself, your friends, increasing number of people from one person to two to more stakeholders, writing a white paper, sharing that with friends, getting them to shoot it down, token engineering sessions, board games, all this. That's all the human side of things. Then you can go into software modeling with increasing fidelity. Maybe start with a spreadsheet, then go to agent based simulation, and then even higher fidelity simulation, such as agent based with EVM in the loop. And I'll talk more about that later. And then finally is economic or live, right? And if you want, you can ignore the first few steps and just dump your smart contract onto Ethereum mainnet and hope for the best, right? And if there's not much value at stake or whatever, that's fine. But a lot of projects they do in ICO and they work for two or three years.
00:21:07.374 - 00:21:15.798, Speaker B: If you just dump the thing there and the thing fails or gets hacked, you're in real trouble, right? So it's very pragmatic to try to figure out, okay, can we ratchet up value at risk over time?
00:21:15.884 - 00:21:16.520, Speaker C: Right?
00:21:16.970 - 00:22:15.814, Speaker B: So for example, you could put out a testnet where you have bounties, maybe, for example, about a year ago, gnosis, when they were releasing their dutch auction, maybe that's two years ago now, they had $150,000 bounty for, if anyone found an issue, a major issue, they'd get 150,000, right? Or you can have sort of like a canary network like Kusama originally was, where the idea was way less value than the polka dot network. And if any issues were found, then they would be found there first, right? Since that time, Kusama has evolved such that it's much more of sort of a long tail network now with billings of value, but that was its original intent. So this happens all the time. There's a lot of different tricks to ratchet value at risk over time. The blog posts that I referenced back here talks about them and other tricks, too, wherever you can. You can also give the option for your users to choose risk reward trade off. For example, maybe you deploy your smart contracts to both Ethereum main net and some as yet unproven side chain that barely anyone uses yet.
00:22:15.852 - 00:22:16.054, Speaker C: Right.
00:22:16.092 - 00:22:28.374, Speaker B: And that might be useful so people can say, okay, I can use Ethereum main net, I'll pay higher transaction fees, but I know at least that network is really secure versus maybe the new side chain, maybe it's not yet secure, or maybe there's going to be bugs or l, two, et cetera.
00:22:28.422 - 00:22:29.066, Speaker C: Right?
00:22:29.248 - 00:22:49.860, Speaker B: And you can do this in a phased approach. And in fact, what we often see happening pragmatically, software modeling hasn't been used that much. It's changing. Thanks, token engineering community. So often we see things going straight from step one to step three and step three where sometimes it's very abrupt, right? Yeah. And increasing fidelity. I talked about that spreadsheet based and so on.
00:22:49.860 - 00:23:06.230, Speaker B: Let's go into where the rubber hits the road on ocean. So I'm going to talk about ocean in two parts, the overall system level model, and then ocean has many subblocks, and I'm going to focus on the subblock of ocean market. But I guess before I get there, maybe there's more questions on just verification.
00:23:06.650 - 00:23:12.540, Speaker A: Yeah, right. Octopus. Are there formal verification methods for te as well?
00:23:14.750 - 00:23:46.594, Speaker B: Formal verification implies. So actually formal verification of digital is easy. It's running itself to over those scale like crazy, right up to billions of transistors, for example. So I shouldn't say easy, there's still gotchas, but it's solvable and tractable and scalable. Formal verification on the analytics side, people have been chasing that for 30 years. It's been really hard. So what people have done as a workaround instead is approach it from the perspective of spice in the loop, because simulator in the loop in general has been found.
00:23:46.594 - 00:24:32.100, Speaker B: People were basically trying to not use spice in the loop and it was just too tedious. All their models were never accurate enough. All the problems emerge there. So everyone uses spice in the loop now. The thing is, spice in the loop takes time, right? So then you have to figure out how do you verify this thing where you try to minimize the time and none of the formal verification approaches to analog or mixed signal scale well enough. So what you do is you say, okay, what's the closest I can do. And the closest you can do is things like this where you have global optimization that maybe doesn't give full global optimization guarantees, but in practice is super effective and just works, right? And so this particular algorithm that we have here, when we developed it, we had a benchmark against 278 different industrial circuits that we got from our different customers.
00:24:32.100 - 00:25:11.870, Speaker B: And when each customer got comfortable with it, usually in the span of a month or two or three, then they would always use it to verify everything, right? Like I mentioned with Nvidia, et cetera. That's one example for that same thing here, right? With verifying these long tail distributions out to six sigma, seven sigma more. And once again, they would use it just to get comfortable. But these visualizations, like you see here, these cdfs that are log scaled in the y axis, they're called NQ plots. Those help to build intuition as well. And by the way, these plots, these are part of the CAD tools that we develop because they really help people to gain trust from them. So the summary is, there's great formal verification for digital.
00:25:11.870 - 00:25:22.514, Speaker B: There isn't great formal verification for analog that scales, but there's good enough for real world usage for analog verification, which is what I've showed you here.
00:25:22.552 - 00:25:22.850, Speaker C: Right?
00:25:22.920 - 00:25:37.170, Speaker B: And then the mixed signal approach is basically, you use spice in the loop still, but you can actually decompose the verification part hierarchically and maintain accuracy. And I didn't talk about that here, but there's tricks for that, basically. Yeah. Any more questions?
00:25:37.340 - 00:25:40.410, Speaker A: Octopus added two notes to this question.
00:25:40.560 - 00:25:41.258, Speaker B: Okay.
00:25:41.424 - 00:25:51.382, Speaker A: Is there a particular global optimization algorithm? And I think he thinks more about gradient descent or simulated annealing.
00:25:51.526 - 00:25:57.614, Speaker B: Yeah. So gradient descent is insufficient because it's not a global optimizer, it's a local optimizer, second order.
00:25:57.652 - 00:25:58.238, Speaker C: Right.
00:25:58.404 - 00:26:15.566, Speaker B: Simulated annealing is a global optimizer. It purports to be a global optimizer that has some degrees of guarantees. In practice, its guarantees are not very good. So you can use whatever optimization you want as long as it has the properties where it's sufficiently trustworthy.
00:26:15.598 - 00:26:16.190, Speaker C: Right.
00:26:16.360 - 00:26:45.518, Speaker B: In practice, what we found works really well is model building optimization, which actually goes back to a research paper from 1999 called bayesian global optimization. Jones, Shonen Welsh. It's an awesome paper. And how it works is basically, I can show the picture here on the left here, the first 20 samples, it takes random samples, and then it builds a model. In this case, it's a gaussian process model, because that's a very accurate interpolator. It's sort of like local linear modeling. But local linear model means nonlinear overall, it's very, very nice.
00:26:45.518 - 00:27:21.382, Speaker B: It gives you confidence intervals, et cetera, too. And then after that, what it does is it says, okay to optimize next, what's the worst place? Like, what does the model think is the worst place? And then after you simulate there, you update the model, and then you ask the model, what's the next worst place, et cetera. So if you do that, you'll be okay, but it still won't find you the best because it's not accounting for uncertainty. So it might also get stuck there. But if you start saying, okay, I'm also going to look around in places where the model has a lot of uncertainty. So you basically search for new places that have a mixture of uncertainty and optimality in a worst case sense. And in the end, you get really reliable global optimization.
00:27:21.382 - 00:28:10.370, Speaker B: And that's what you see here, right here, the pink background is the uncertainty, the confidence intervals, and the orange line is the actual simulation value, and the dark pink line is the estimate. And you can see actually the dotted orange line along the top. This is the worst case scene overall, right? It's trying to maximize that for the worst case. So this is sort of the thing in practice that works. And as time goes on, people are coming up with better, better global optimizers. But it depends on the problem class, right? In, say, neural networks, where they have a billion parameters to optimize, they're happy with local optimization, because if you have a billion parameters, it means in any step of the way, you have a billion times two or 2 billion possible directions to go, right? And in fact a continuous version of that. So the more dimensions you have, the more optionality you have to just optimize.
00:28:10.370 - 00:28:22.814, Speaker B: And in training a neural network, you don't need the global optimum, you just need something to fit here, you need something more of a global optimum. But you tend not to have as many variables, right? You have ten variables, 100 variables, maybe 1000, right? In this case, so you're okay in this case.
00:28:23.012 - 00:28:25.806, Speaker A: What's the paper again you've mentioned for.
00:28:25.828 - 00:28:58.662, Speaker B: Our reference, the paper is bayesian global optimization. No, sorry, efficient global optimization ego by Jones, Sean Lowe and Welsh. But if you just type in efficient global optimization, Jones, you'll find it. And interestingly, even people use this now for automl to basically have machine learning that automatically figures out higher level parameters, including and also training neural network parameters at the top level. So it's become quite widespread. I used it a lot in my AI work in the stuff. So it's just a hyper useful thing under the hood people use.
00:28:58.662 - 00:29:29.954, Speaker B: The model they use tends to be gaussian process models. Those scale linearly in the number of dimensions, they scale quadratically in the number of samples, which is a problem, but there's tricks such that you can get it down to linear there too. Basically, in the end you can have a pragmatic algorithm on that, and I'm happy to share information on that later. And if you want to see sort of a summary of this overall, you can actually go to my personal website, Trent street, and it points to several of these papers and talks from earlier years, as well as the summary of it all in one book is this book I wrote more than ten years ago now.
00:29:30.072 - 00:29:31.220, Speaker A: Cool, thanks.
00:29:33.990 - 00:29:45.682, Speaker B: So I talked about token engineering. The summary there is. Yeah, here. Right. Circuit land has great tools for digital verification and great tools for analog and for mixed signal.
00:29:45.746 - 00:29:46.310, Speaker C: Right.
00:29:46.460 - 00:29:51.722, Speaker B: Token engineering so far is coming along, but it has a long ways to go.
00:29:51.776 - 00:29:52.426, Speaker C: Right.
00:29:52.608 - 00:30:12.254, Speaker B: Three, four years ago, people started using formal verification a lot more, and I actually had pulled in friends from circuit land to help out and give guidance to the people doing formal verification in Tokenland. And part of my mission here is to help bring the practice for analog and mix signal from circuit land to here. And I happen to have spent a lot of time myself on that.
00:30:12.292 - 00:30:12.590, Speaker C: Right.
00:30:12.660 - 00:30:56.542, Speaker B: I was less focused on the digital side, but where I spent my career, a lot of my career was in the analog and mixed signal side. And that's why I can really help to guide that. And that's partly why I built token spice, because I have a particular background that lets me do it, build it some of it quickly, and show what can be done. So that's why I've done it. Let's keep going. I'm going to talk about the top level building block and how we verified it, and then the ocean market building block, how we verified it so far, and then talk about open questions and what can be done on that. And then where it can lead to and some of where it can lead to is super interesting things like evolution of solidity in the loop, stuff like that.
00:30:56.542 - 00:31:24.082, Speaker B: So let's get to it. So here's the system engineering token engineering work that I did. This was from about February to May last year. So just over a year ago, and I was collaborating a lot with Julian Tavenard from fabric. So the first thing in a three step flow, there's different ways to break down the steps in token engineering. Here's one way you write down your goals, you have a first cut design and then you verify.
00:31:24.146 - 00:31:24.470, Speaker C: Right?
00:31:24.540 - 00:31:50.654, Speaker B: So writing down the goals. Yeah, I talk about that in the next slides. I talk about all these writing out the goals, exploring the designs to achieve them, and then verification, and let's just get to it. So in terms of goals in the top level, we wanted a design to have an ecosystem that is sustainable and growing towards ubiquity. And a great way to achieve ubiquity is to simply to keep growing exponentially over a long period of time.
00:31:50.692 - 00:31:51.038, Speaker C: Right?
00:31:51.124 - 00:32:29.590, Speaker B: Facebook didn't get to 2 billion users by growth over one year. It took them more than ten years, right? 30%, 50% growth a month. So that's how you get to ubiquity. And you want an ecosystem that is going towards that in a sustainable way. You want to have funding going to teams that keep improving at all the different layers, the base substrate, the smart contracts, the apps, et cetera, over the long term, ten years, 20 years, 100 years. And also the token health itself is really important for this, because if the token health evaporates, then kind of the ecosystem itself becomes much weaker. And it's a great tool to help drive towards sustainability.
00:32:29.590 - 00:32:35.834, Speaker B: So what you want to make sure of is that the token itself grows as the usage of the network grows.
00:32:35.882 - 00:32:36.238, Speaker C: Right?
00:32:36.324 - 00:33:23.630, Speaker B: And by the way, the wrong way to do this is make your token a unit of exchange. Because Chris Berniski has showed if you do that with MV equals PQ modeling, you achieve the opposite effect, where velocity goes up, traction goes up, but the value of the token itself goes down, exactly the opposite of what you want. So you want to actually have dynamics that capture this correctly, whereas traction goes up, value goes up, and those exist. And I'll give a design. And you also want to make sure that the basic design is simple to understand and communicate, can be implemented pragmatically over time. It gets people to do work, gets people to do stuff, just like bitcoin gets people to add to its security and encourage skin in the game by users. The users themselves should be holding ocean, et cetera, right? So these are all important sub goals.
00:33:23.630 - 00:33:50.520, Speaker B: And a choice of system level design can lead to the goals of the subblocks in the system. So we basically went through iterations to design this. And I'm not going to get into great detail on this, I'll just end up with the final model here, which is the following. This is the web3 sustainability loop. And I'll just describe it. The core is by analogy Amazon. How Amazon works is a very nice analogy so I'm just going to talk about Amazon for a second.
00:33:50.520 - 00:34:28.830, Speaker B: Amazon, you've probably heard about the famous two pizza teams, right? And so each team has enough people where two pizzas can feed that team. You don't need more. And so they have a sea of these two pizza teams and each one of them has their own profit and loss, their own balance sheet, their own KPIs, right? And then they're getting resources from a resource allocation algorithm that Bezos devised and has executed along with his lieutenants. That's how Amazon works. And then what do these two piece of teams do? They do things that drive revenue to Amazon. So what does that mean? Well, Amazon's ecosystem is Amazon.com and otherwise.
00:34:28.830 - 00:34:37.990, Speaker B: And in that ecosystem they have an ecosystem of buyers and sellers, et cetera. They take their cut, 2%, 5%, whatever, and that's their actual revenue to Amazon itself.
00:34:38.060 - 00:34:38.438, Speaker C: Right?
00:34:38.524 - 00:34:53.462, Speaker B: They take that revenue and they feed it back almost 100% back to their resource allocation algorithm of their two pizza teams. The goal for them is growth, growth, growth, growth, growth, growth. And if they slow down the growth over time, then they start thinking about doing dividends, et cetera.
00:34:53.526 - 00:34:53.754, Speaker C: Right?
00:34:53.792 - 00:35:35.862, Speaker B: And that's how Amazon works. So what you see in front of you is a lot like that in many ways, where instead of the two pizza teams, the sea of two pizza teams, you have workers in the ecosystem. You have people building tools on top or apps on top, improving the core software, doing outreach, adding data, everything, all these different things that add value to the ecosystem, right? And by adding value to the ecosystem, what that ultimately means is that the revenue goes up. And which means if you say, okay, the fundamental valuation of the network is a multiple of the revenue. That's the price to sales approach to pricing or valuation. Then it drives up the value of ocean, for example.
00:35:35.916 - 00:35:36.278, Speaker C: Right?
00:35:36.364 - 00:36:10.180, Speaker B: There's other ways to arrive at this, too. Discounted cash flow, et cetera. But you can model any of those. But ultimately, if you get revenue increasing over time and there is good revenue growth, then you arrive at very good fundamental valuation values. So that's the name of the game is to grow that, right? The KPI is this growing network revenue and growing ocean. In this case, it's in the context of a data ecosystem. So whatever you put in there should be growing, growing, growing such that there can be a small percentage, a non extracted percentage of network revenue, which is network revenue going to the community.
00:36:10.180 - 00:36:44.094, Speaker B: Most of that goes back to growth. In this case, it's going to Ocean Dao, which is another building block. That is a grantsdao, the community itself chooses people, make proposals, the community votes on which proposals should get funding. And then those people who did the proposals go ahead and build the things or do the outreach or add the data or whatever it is they proposed. So that's the main part of this. A couple other things, other revenue going to Ocean Dao. You can also have held back some pre mine in the treasury that also goes to Ocean Dao to help kickstart things.
00:36:44.094 - 00:37:38.158, Speaker B: And we have that in Ocean's case, Ocean protocol foundation has kickstarted Ocean dow for that. Also, critically, to help drive value to the overall token on the bottom left, it has buy and burn ocean. And what that does is if you have 5%, say, of the revenue that buys ocean in the open market and burns it, then it actually drives the fundamental value of ocean. And a good modern example of this is EIP 1559, right? So as network revenue goes up in Ethereum main net, I. E, transaction revenue for verifying transactions, a portion of that is burned, which then drives up the value of ether, right? So it's a major upgrade to Ethereum's token dynamics in a really, really positive way. And that has almost certainly been one of the factors towards the rise of eth. Among the fact that we're in a bull market too.
00:37:38.158 - 00:38:18.714, Speaker B: But the fundamental token dynamics of Ethereum got a lot better, or are about to get a lot better with EIP 1559. Interestingly, Filecoin implemented that inside filecoin as well, and they've had it live for more than a year. So pretty interesting that they've already verified that it works for their case. And what that means is then you have to make sure that revenue goes up as traction goes up, right? And one way to do that is simply taking a cut of all sales, right? So if you take 0.1%, you take 5%, whatever you want, and then you've got your revenue growing, growing exponentially as the overall sales traction goes exponentially. So that's the heart of the ocean design, but it's a very general design, and we designed it. We started thinking about this two years ago.
00:38:18.714 - 00:38:50.326, Speaker B: We published it about a year ago after a lot of internal evolution and modeling, et cetera. And since we published it, there's, I don't know, 510, 15 teams that I know of that are using this for their own token models. Probably the most notable one is boson protocol, but there's many others as well because it is just simply quite general and you can kind of use it off the shelf, keeping going. Then we wanted to know, will this work? Right, can we verify this? So there's subblocks too. I won't get into that, but it led to some goals in the sublocks. But then we wanted to verify this.
00:38:50.348 - 00:38:50.534, Speaker C: Right?
00:38:50.572 - 00:39:25.498, Speaker B: What did it look like? And so there was three levels of verification, in a sense, subjective discussions, right. We did that software modeling, and we actually did two versions of this. Julian, my collaborator at Fabric, had a spreadsheet based approach, and I built an agent based simulator in Python, token spice. And then we've been putting this live economically, so which of course is the main thing over time. Yeah. So let's go into the agent based simulation token spice. So you might ask, why did I call it token Spice? It's probably obvious by now, given that I come from electrical engineering.
00:39:25.498 - 00:39:55.418, Speaker B: Land spice is the de facto simulator there. There's a whole bunch of variants, right? There's h spice, n spice, ng spice, all of this, right? Berkeley Spice. So for me it's just the obvious name. As an electrical engineer, if you're going to have spice for tokens, token based systems, it's going to be token spice. So that's what I call it. I initially built it to model the ocean ecosystem, but you can adapt it quite readily. There's just a page of code in one place where you wire together the different building blocks as a netless, and I'll get to that.
00:39:55.418 - 00:40:16.866, Speaker B: It's agent based simulation like mentioned. So each agent has its own wallet. Each agent basically at each time step can take an action. Is it going to do X or Y or z, et cetera. It's all written in Python, right? So each agent is a class, and it has methods in the class. And then you simply model the system by wiring up the agents. It's like a netlist in circuit land.
00:40:16.866 - 00:40:51.874, Speaker B: And then you track the metrics, which are KPIs, things like token price or money going to ocean, dow, et cetera. And it's actually easy to adapt for, super easy to adapt for other projects doing web3 sustainability loop, because all you need to do is change the parameters, but then you can also adapt it to other stuff simply by using the backbone and fork it and change your netlist however you want, right? And it's open source at this link here, GitHub.com, Oceanprocol, Tokenspice. And it's all like Apache two, open source, all that. We had a block diagram for this model. Overall, this is what it looked like. Initially.
00:40:51.874 - 00:41:28.278, Speaker B: I was drawing with a pencil and paper, iterating, iterating, iterating, and you can see the value flow. The thicker the value flow, the more value is flowing there. And at first I was having a high fidelity model for the publishers and the buyers and all that, but then as I was building, I realized I don't need that fidelity right now. I really have higher level questions about the system level design rather than the market. So I simplified the model. And here's the simplified model. It's basically like this, right? But taking that and then making it realized this is actually what it looks like.
00:41:28.278 - 00:41:35.690, Speaker B: So each one of these gray boxes is a different agent. So we actually have a single agent to model all of the data ecosystem right now. And that's fine.
00:41:35.760 - 00:41:36.380, Speaker C: Right.
00:41:37.070 - 00:42:14.278, Speaker B: We can make it higher fidelity later. And that's what I'll get to later, actually. But for token spice to model ocean system level, this is what it is. And yeah, basically these value flows and so on. So then you can take this and say, okay, well, what under the hood, what variables are going to model, et cetera? And this is a mapping of all the models. By the way, I can share these slides later so we don't have to look at it all now and all the links, right? But basically the backbone is the following. On the top left of the picture, you have revenue coming from the marketplaces, right? So you start with one marketplace, starting with, say, $10,000 revenue a year, right, sorry, a month.
00:42:14.278 - 00:42:37.760, Speaker B: And then maybe it's growing 15% a month. Okay. And then you might say the number of marketplaces themselves is also growing 15% a month. Okay, great, that's on the top left box. And then you say, okay, ocean network revenue, that's simply a percentage of the marketplace's revenue. What's the fundamental valuation? You might say, okay, a 30 x price to sales ratio, pretty conservative. You could even go 50 x like Zoom or finance have.
00:42:37.760 - 00:42:59.250, Speaker B: But you arrive at a fundamental valuation. Of course, the overall valuation is fundamental plus speculation valuation. And you can ignore speculation valuation. In fact, it's really important because that's going to be most of the valuation for the first several years of the life of a project, because people are very hopeful about the future. So they're basically pulling future expectations of revenue, token value, et cetera, to the now.
00:42:59.320 - 00:42:59.602, Speaker C: Right?
00:42:59.656 - 00:43:08.722, Speaker B: And that's, okay, so total valuation is fundamental plus speculation. And then you say, okay, well, you've got a total valuation for the network. Divide by the number of tokens, and that gives you the price of the token.
00:43:08.786 - 00:43:09.400, Speaker C: Right.
00:43:09.850 - 00:43:18.490, Speaker B: But interestingly, where do you arrive with the number of tokens? Well, you've got a baseline that's out there already, but then you've got minting and burning from some of the things happening like I described before.
00:43:18.560 - 00:43:19.034, Speaker C: Right.
00:43:19.152 - 00:43:45.838, Speaker B: So that's the general idea. And marketplace's revenue. You can actually make it higher fidelity by modeling what is the growth rate as a function of sales, et cetera, et cetera, et cetera. But this is the overall idea. It's all in python, straightforward and so on. And yeah, this is another picture of just the web3 sustainability loop like I showed you before, but not just ocean specific, but anything. And then there's fine grained modeling.
00:43:45.838 - 00:44:15.786, Speaker B: In this, for example, we say, okay, instead of growth rate being fixed at 15%, we can say it's going to be negative if you have no money put into growth at all or no sales. But if you actually have, or, sorry, the ratio of r and D to sales. So if that ratio is negative or zero because you have no sales or no R D, then it's negative. But as it goes up, at some point it's going to cross the threshold such that you'll have a growth rate of greater than zero. But it's diminishing returns.
00:44:15.818 - 00:44:15.966, Speaker C: Right.
00:44:15.988 - 00:44:20.666, Speaker B: As you put more and more money into R D over time, it's diminishing returns.
00:44:20.698 - 00:44:20.942, Speaker C: Right.
00:44:20.996 - 00:44:25.582, Speaker B: Just like Amazon isn't growing as fast now as it did in its first few years.
00:44:25.636 - 00:44:25.854, Speaker C: Right.
00:44:25.892 - 00:44:32.702, Speaker B: Although actually Amazon, it's amazing how well it grows because it's able to discover new markets almost like any other, unlike any other company on earth.
00:44:32.766 - 00:44:33.090, Speaker C: Right.
00:44:33.160 - 00:44:34.686, Speaker B: Google still only knows really, ads.
00:44:34.718 - 00:44:34.866, Speaker C: Right.
00:44:34.888 - 00:44:55.878, Speaker B: Et cetera. But Amazon just keeps swallowing markets. It's really, yeah, this is an example of adding fidelity to the model for revenue of marketplace's ecosystem. And you can also say, okay, what about in Ocean's case, we've held back 51% of the token supply for feeding it to ocean dow for long term sustainability.
00:44:55.974 - 00:44:56.620, Speaker C: Right.
00:44:56.990 - 00:45:21.282, Speaker B: But then what should that schedule look like, right. Should it be flat for ten years? Should it be exponential like bitcoin? Or should it be something else if it's flat? The problem is that after ten years, you have a huge drop off. It's really painful if you make it exponential like bitcoin. The problem is that it's actually too generous in the first few years because a lot of people just don't know. Sorry, already a lot of people know about ocean, so it's a bit too generous. Whereas bitcoin was an unknown at first, basically.
00:45:21.336 - 00:45:21.940, Speaker C: Right.
00:45:22.550 - 00:45:52.058, Speaker B: But there's heavy mediums, and I'll show you an example of that. So let's look at some results. So, by the way, there's parameter settings. So if you're going to do your own runs of token spice to model your own token design web3, sustainability loop, or otherwise, this might be what you have, right? So the simulation time, is it ten years, 20 years, 150 years, growth rate information, the toll that you might take from marketplaces revenue, is it 0.1%, is it 5%? What's the speculation volume evaluation now?
00:45:52.144 - 00:45:52.490, Speaker C: Right?
00:45:52.560 - 00:46:28.546, Speaker B: So for Ocean, we could just go to coin market cap and see, et cetera, et cetera, right? And then with these parameters, do a run, and this is results we get. So we see, okay, what's the monthly r and D spend on the left? It's linear. And because there's exponential growth, it goes extremely high up after a while. But that's hard to reason about for the first ten years. So if we have a log scale, which is on the right, then it's easier to think about. And we actually have a design, it looks a little bit jagged. And that's because the final design we have right now is the schedule to release tokens is ratcheting.
00:46:28.546 - 00:46:44.030, Speaker B: So rather than it's like bitcoin, but then it only gives 10% of the supply to start with, and then it goes to 25%, then 75%, and then 100, based on meeting particular milestones that are humanly observable. And when you get to 100%, though, that's when you throw away the key and it just goes.
00:46:44.100 - 00:46:44.720, Speaker C: Right.
00:46:46.130 - 00:47:24.410, Speaker B: And those particular milestones and timings and numbers were set based on token spice simulations, actually. So that's why it looks roughly flat for the first seven years here. And in practice, things are a little more jaggedy and stuff, but this gives a feel. So this is monthly r and D spend. And I won't tell you exact numbers, but this spend means that it's enough to fund token spice and other communities, sorry, enough to fund ocean, dow community, et cetera, for people to keep improving ocean very well. Over the years, minimum 510 people, but as many as 100 or more. And once you get beyond your ten, it grows 100,000, 10,000 people even, which is pretty exciting.
00:47:24.410 - 00:47:54.820, Speaker B: What about the r and d sales ratio and marketplace's growth rate? So let's look on the right. That's the log side. And we see that at first it's capped out at the max, which is 50%, I think we put it at. And then as time goes down on where the overall ratio goes down of r and d to sales, then the growth rate slows in line with the models of before. This is just helpful to analyze things. You can look at the token count. So this shows the ocean token count over time.
00:47:54.820 - 00:48:12.082, Speaker B: So basically, the top level line, the blue, is the total supply, which is what we started at in year zero, sort of at the beginning of the simulations, and then number minted for ocean dow, and then number burned based on revenue, et cetera.
00:48:12.146 - 00:48:12.760, Speaker C: Right.
00:48:13.870 - 00:48:43.378, Speaker B: You can see on the left is the linear version, and on the right is the log version. And in the end, actually the number burned, it's a fraction of overall. Right. And this is because we have 5% burned. We did simulations, for example, of what if we have 95% burned? And then you'll see that that green line goes much higher, but the total revenue to the network, et cetera, is lower, because if you're burning, it's only a one time gain versus if you actually plow it back into growth, if you can, then it's this exponential, a gift that keeps on giving.
00:48:43.464 - 00:48:43.906, Speaker C: Right?
00:48:44.008 - 00:49:09.974, Speaker B: So that's why it's useful to have a baseline of burning to drive token value, to help make it easy to calculate token value, et cetera. But beyond that, you want to just max out growth. So that's the aim. And then you can also see what's the monthly ocean minted and burned. Right, and that's what this is. And the minting here is for ocean dow. And one key question is, we wanted to make sure that we didn't have too much ocean minted per month in a way that would saturate the market and drive the overall price downwards.
00:49:10.022 - 00:49:10.186, Speaker C: Right.
00:49:10.208 - 00:49:33.934, Speaker B: In sort of a qualitative judgment way. And that's sort of looking at, like, what's the volumes on binance, et cetera, versus what you're minting for ocean Dow? And these numbers come out, and it's like, yeah, okay, we're good to go, right, but before we had these numbers, it was a big worry. Right? So, yeah, having these simulations really helped allay these concerns. And then you can also look at the Dow income versus time. Right, which is basically the community income.
00:49:33.982 - 00:49:34.434, Speaker C: Right?
00:49:34.552 - 00:50:03.510, Speaker B: And we can see here there's income as USD income is ocean and total income. And the total income is for the first eight or nine years, it's roughly flat. Roughly. And then after that, it actually goes up exponentially with the revenue of the network. It answered a lot of questions for us, right. Things like, okay, what's the benefit of having 51% of our supply versus what if we just burned all of that now and didn't do any of that, right. And the answer is, it's 30 x more value because of the growth you get.
00:50:03.600 - 00:50:04.046, Speaker C: Right.
00:50:04.148 - 00:50:08.654, Speaker B: So we had a very definitive difference based on burning versus not.
00:50:08.692 - 00:50:08.942, Speaker C: Right.
00:50:08.996 - 00:50:15.594, Speaker B: And that was really nice to see, like, 30 x. It's, like, dead obvious then, right. And then we said, what about the 51% distribution?
00:50:15.642 - 00:50:15.818, Speaker C: Right?
00:50:15.844 - 00:50:42.322, Speaker B: What's the schedule? And I hinted at this before, so do we do uniform for ten years? Do we do pure exponential, like bitcoin, or do we do ratcheted exponential uniform? If you look on the bottom left, you see there's a sharp drop off of the green, which means it's actually about a 20 x drop off in ocean Dow income. So at year ten, people basically experience a 20 x reduction in funding to the various teams getting money from ocean Dow, and that could be catastrophic to the ecosystem.
00:50:42.386 - 00:50:42.818, Speaker C: Right.
00:50:42.924 - 00:51:08.670, Speaker B: So you say, okay, well, let's smooth that out. Let's just have an exponential. And that's what the run two is about, right, where you have this exponential curve. But then the challenge is there's actually too much up front, and the ecosystem simply can't swallow it, right? So run three is basically say, okay, ratchet up over time so that you know that the ecosystem can swallow this stuff initially, and then once you've got to 100% of the ratcheting, then you throw away the key, and everything is just fully automated.
00:51:08.750 - 00:51:09.090, Speaker C: Right.
00:51:09.160 - 00:51:45.146, Speaker B: And that's the best. So that's what we went with. So this gives examples of questions that we had and then definitive answers that we got, thanks to using token spice. So, yeah, that's token spice for the web3 sustainability loop. But then the next question was, what about this building block, which is ocean market, which has undertud balancer amms, right? So let's look at the design process, et cetera. We had a lot of goals, right? We wanted the market itself to drive the value of ocean. As the market volume goes up, ocean value goes up, gets people to do work such as curation, et cetera.
00:51:45.146 - 00:52:05.442, Speaker B: Drives virality. Simple to understand. And each third party marketplace should get all the characteristics above. We have about 15 teams building third party marketplaces on top of us, so we want to make sure of that. And the fact that there are 15 teams is a good sign of the success of the design so far. And the third party marketplaces should also drive data liquidity to ocean market. They shouldn't be silos on their own.
00:52:05.496 - 00:52:06.050, Speaker C: Right.
00:52:06.200 - 00:52:17.554, Speaker B: And then there's a lot of secondary goals, too. We explored a ton of different designs right here's like just small screenshots of a bunch of them, right? Canada, ABCDeFG, et cetera.
00:52:17.602 - 00:52:17.862, Speaker C: Right.
00:52:17.916 - 00:52:50.418, Speaker B: So a lot of explorations in what this could look like. And the chosen design was, I guess it can simply be summarized as an amm. That's the simplest way to think about it, because when we had this design, we're like, okay, we have this shared market backend on a blockchain that then gives a percentage of revenue to the community. There's staking on the data sets as well, et cetera. And then we realized, you know what, what's a good way to implement this? Oh, interesting. If we simply use an amm like balancer, then it will work out of the. So, you know, because we know Balancer well, we really love balancer, all that.
00:52:50.418 - 00:53:07.938, Speaker B: Then we just simply use it. And that turned out to meet our goals. So lots of details on that. I won't get into the details here. Like I said, we implemented as an amm, and on the bottom, it's like how it meets them. Yes. So we just had the checks, classic token engineering type checklists.
00:53:07.938 - 00:53:19.830, Speaker B: Are we meeting all the. Yes. Good. And then how did we go about verifying overall. Right, so step one, humans, it's those checklists there, right. As well as talking to people and so on. Software modeling.
00:53:19.830 - 00:53:42.770, Speaker B: We actually did some there initially with like Python drivers and Javascript drivers for balancer, but we also knew Balancer itself already having being deployed before that, and Uniswap and Bancorp, et cetera. That gave us a lot of confidence that the basic idea would work because we were using this off the shelf code and token design.
00:53:42.840 - 00:53:43.314, Speaker C: Right?
00:53:43.432 - 00:54:21.182, Speaker B: And then of course for the economic live thing, when we launched the thing, we launched it with a lot of caveats. This is beta test and production, et cetera, and choose your own risk reward, right? So some ocean holders went in there staking, but a lot haven't because they want to get understanding. Some people got burned too, here and there. That's fine, right? You expect that we observed the community response to the market and dynamics made a bunch of adjustments. For example, at first we were 90% weighted to the token 10% ocean. But what that meant was if you only put in $100 of ocean liquidity, it means you've got another $1,000 worth of value in the market, because the data token is most part of the value.
00:54:21.236 - 00:54:21.598, Speaker C: Right.
00:54:21.684 - 00:54:54.794, Speaker B: And that was just basically way too sensitive to ocean tokens being put in. So we flipped it around to be 70% ocean, 30% data token, which is easy in balancer, right? It's something that none of the other pools can do, or at least could do except for balancer. And that really made a big difference because then it was way less sensitive to changes in ocean liquidity, but at the same time had some sensitivity to it, which is important and many other things. But we also noticed that where there are several issues to account for. And I'll get into that and then ideas and solutions. So this thing went live last fall. This is what it looks like.
00:54:54.794 - 00:55:22.818, Speaker B: You go to market oceanprotectocol.com, you can see it and you can publish your own data sets, et cetera, et cetera. There's about 5 million, 6 million liquidity in there these days. And yeah, the dynamics are pretty exciting, right. If you stake on a data set, it basically means you're adding liquidity to it, but it also means it's a curation signal, right? Because no one is going to stake on a garbage data set. They're only going to stake on data sets that they think that there's going to be transaction volume, and there's only going to be transaction volume if people think those are quality.
00:55:22.904 - 00:55:23.394, Speaker C: Right?
00:55:23.512 - 00:55:55.142, Speaker B: So that's actually one of the main things that we leverage, staking liquidity, providing as a very strong signal for curation. There's other curation signals, too, but that's a very good one. And it's actually not unlike balancer itself. If you go to pools balancer exchange, you'll see that the pools are sorted by liquidity. So the metric of how much liquidity there is in a pool is arguably the number one metric for overall quality of a pool. But certainly, like, it's a very dominant metric, right? There's obviously other ones, but it's a very important metric. So we leveraged that here.
00:55:55.142 - 00:56:19.574, Speaker B: And that was really important because it avoids spam. People can spam all the garbage data sets they want in ocean market, but they're not going to show up very easily. Right, because the spammers aren't going to put real liquidity in those, right. And if they do, then they're tying up their own liquidity. That's their prerogative. But why not just put in quality data sets and make real money? So that's a v three. So v four and v four plus, because we've actually split v four into two things.
00:56:19.574 - 00:56:51.466, Speaker B: Now, v four, the base of v four is basically nfts to wrap the base IP, the copyright, and then v four plus is better ideas, et cetera. And that's what I'm going to focus on here. So when I see v four, I really mean better idos, better staking. I'm going to talk about a few goals, et cetera, and then what this leads to here. So some of the goals, it needs to be simple, easy to understand mental models, plays well with existing, et cetera, et cetera. Here are some of the issues we observed, and I framed this as constraints to be met. Checkboxes.
00:56:51.466 - 00:57:34.394, Speaker B: And this was based on observations. We want to avoid large price swings when people want to stake ocean. We partly solved that already with the 70 30 instead of 1090. But there's still some sensitivity and we want to get past that, right? Because right now, if people want to stake just ocean under the hood, it's still buying data tokens in order to do that. And then that drives the price up, right? But ideally there should be some way to stake without necessarily even buying data tokens. You want to solve price spikes at the beginning. So right now, when someone publishes a data set, it's basically you've got the data set, the publisher creates a pool, they add data tokens as liquidity, these ERC 20 tokens to represent a file or whatever, and they add the ocean as liquidity.
00:57:34.394 - 00:58:17.402, Speaker B: So it's a pool with two tokens, and they add it and then it's boom, go. Right? There's no smoothness at all. It's not like a lot of icos and stuff where there's a dutch auction or a fixed price auction or a channel auction or an LBP or something, right? It's just jump off the deep end, boom, right? And that causes price spikes up and down and huge variance at the very beginning, right? But also later on, even when you get past that initial burn in period, it's useful to solve price spikes later on. You want to get past sensitivity there, too, if you can. You want to avoid exit scams after idos. So if you think about it, right now, when someone publishes a data asset, they own 100% of those data tokens. They are a data token whale, right? So they can rug pull.
00:58:17.402 - 00:58:28.922, Speaker B: And that happened in practice. We saw it. There was people pretending to be people they weren't. We fixed that, actually with good identity, three box, et cetera. But even after that, some people managed to get the trust of the community and then still rug pulled.
00:58:28.986 - 00:58:29.454, Speaker C: Right?
00:58:29.572 - 00:59:21.870, Speaker B: So is there a way to do token engineering to completely remove the possibility for that, or at least greatly mitigate it? And the answer is yes, I'll talk more about that. But you still want to have a good incentive for the publisher to publish initially, right? So they should still have maybe at least some of those tokens, 10%, 20%, but probably not 100. And then finally you don't want to make it where the market is like there's 10,000 x more volume on speculation versus on consumption, right? Maybe 100 x is healthy. And speculation, by the way, is a good thing in that it's kind of a prerequisite to establish automatic price discovery. And this is a lot of the reason that we use amms in the first place, because fixed price is a problem. People don't know what price to set the data at. So how do you go about automatically discovering the know? Traditionally you would say order know, just like Tesla stock or whatever is arrived at.
00:59:21.870 - 01:00:24.734, Speaker B: But thanks to amms, it allows you to get the benefits of an order book exchange. But for the long tail, where you don't need a bunch of people putting in bids and asks and stuff, you can just have this robot that's there to always be the buyer and seller. And then people just add liquidity, right? That's super, super valuable, right? But even under that regime and stuff, people might focus on speculation back and forth of how much they think the data token might be worth rather than value creation. And the real value creation is when data is consumed, right. Someone buys a data token and then takes it and redeems it and consumes it to actually see the data set, right? So we want to make sure that consumed volume is maybe say 1100 of speculation volume or something. And that's roughly what ether is, right? The gas volume in ether, that's sort of the consumed volume and then the volume on exchanges, et cetera. For ether, that's the speculation volume, right? And it's somewhere between 100 x and 1000 x on any given day, right? So that's probably a useful number with ethereum as a comparison.
01:00:24.734 - 01:00:51.478, Speaker B: But you don't want to be 10,000 x or something, right? So that's another goal. So these are the different goals with the v three design, the design I showed you already using amms, et cetera. This is how it does. It's simple, simple mental model, et cetera. It has automatic price discovery. I didn't show that here, but it's implicit. But it actually has several fails, right? It has the large price swings at the beginning, at the equilibrium, et cetera.
01:00:51.478 - 01:01:26.086, Speaker B: It has the risk of exit scam after ido initial date offering. There's an extremely good incentive for the publisher to publish initially, right? Because they are the data token whale. But then there's also the risk that they can rug pull. And then there's also the risk of data token price decoupling. Basically speculation volume outweighing consumed volume by too high of a ratio. So that's the status quo, right? And the way we put it here, it's like, that's a bit painful, but we are honest with ourselves about where we're at, right? So then it's like, okay, what can we do to fix this? And we went through a design process, a classic token engineering process, against this checklist. What can we do?
01:01:26.108 - 01:01:26.390, Speaker C: Right?
01:01:26.460 - 01:01:53.226, Speaker B: So we started saying, what are the different things to do? And we have a mental model of three phases. Publish the initial data token, contract the ido, or burn in phase, sort of like a regular tokens ico, and then the equilibrium phase, right? So there's details on that. We can frame the issues in many ways, but then we went through a whole bunch of different design iterations, and that's each of the columns here. So there's the ocean v three. And then we said, what about one sided staking?
01:01:53.258 - 01:01:53.406, Speaker C: Right?
01:01:53.428 - 01:02:37.386, Speaker B: What if there's a robot that's owned by the balancer pool that also owns all the tokens, every single one, and whenever you stake an ocean, the robot will match. It will mint data tokens such that the price doesn't change when liquidity of ocean and data token is added to the pool, right? So whenever ocean is added, there's a minting of data tokens, and those get dumped into the pool. And then if people want to unstake ocean from the pool, then the robot burns the data tokens such that price stays the same. So that's one sided staking in a very clean way, right? And it actually solves a few issues in a nice way. It avoids the larger price swings when people just want to stake ocean. It doesn't solve price spikes at the beginning. It does solve price spikes at market equilibrium.
01:02:37.386 - 01:02:43.602, Speaker B: And it fully solves the risk of exit scam after ido, aka rug pull. But it still misses out on three things.
01:02:43.656 - 01:02:44.066, Speaker C: Right?
01:02:44.168 - 01:02:46.238, Speaker B: What about liquidity bootstrapping?
01:02:46.254 - 01:02:46.434, Speaker C: Right?
01:02:46.472 - 01:03:19.638, Speaker B: Like balancer style. Well, it solves price spikes at market equilibrium, and that's kind of all. You could say that it helps a bit on the beginning stuff, too, but that's all right. What if you combine the two of one sided staking and liquidity bootstrapping pool? From our analysis, it doesn't do any better than just one sided staking on its own. Basically. What if you say, okay, what if we have a dutch auction where all the proceeds of the dutch auction, say, selling 10% of the tokens go to the publisher. Well, that actually solves price spikes at the beginning.
01:03:19.638 - 01:03:54.914, Speaker B: But you still have the issue of two things. You still have the risk of the publisher doing an exit scam after the ido because they have a bunch of money, then from that ido, maybe too much. And then we also have the problem of price decoupling. What if we add in a reversible ICO like Luxo did, where people can get their money back anytime? And interestingly, in this case, it actually solves the risk of exit scam, but it adds a ton of complexity. Right. Luxo, they actually had to do a lot of work to make that happen. They had to have a lot of GuI, for instance, as an explanation.
01:03:54.962 - 01:03:55.318, Speaker C: Right.
01:03:55.404 - 01:04:19.886, Speaker B: Very cool. But you don't want to have that for long tail assets where you're having ten or 100 idos every day. Right? So it doesn't really fit for our use case. It fits for broader token use cases and stuff. Probably, but not here. Then what if you say, okay, let's chop this down again and say just one sided staking and the dutch auction, so we don't have the liquidity bootstrapping, we don't have the reversible ICO. Interesting.
01:04:19.886 - 01:04:31.682, Speaker B: It actually does. Okay, but it's still got a couple of flaws. What if we say if the proceeds from the dutch auction, they don't go to the publisher, but they go directly to the pool itself, owned by the robot, if you will.
01:04:31.736 - 01:04:32.340, Speaker C: Right.
01:04:32.950 - 01:05:14.990, Speaker B: It actually shifts around the problems. Now you have just two problems at the bottom. The publisher doesn't have any incentive because he doesn't own any data tokens up front, and you still have this price decoupling problem. But then if you say, we have a vested pre mine, so the publisher, they get vesting of, say, 10% tokens, but it's vested over time, say over one year, four years, whatever. Just like a lot of icos, the team is vesting over time, then you're good to go on that and you have just one problem left. And then to solve that final problem, I think I have a slide on that. Yes, you basically have a tax that basically says we're going to charge a higher transaction fee if this ratio goes beyond 100 x or 1000 x.
01:05:14.990 - 01:05:17.390, Speaker B: And the higher it goes, then the higher the fee.
01:05:17.470 - 01:05:17.906, Speaker C: Right.
01:05:18.008 - 01:05:58.110, Speaker B: And that's a very nice way. Basically what will happen is people will be disincentivized to trade if it goes beyond a certain volume and they will trade elsewhere or focus on getting more consumed, volume or whatever. So it shifts the dynamic nicely away from speculation, away from extreme speculation, towards actual value creation, which is nice. So that's the token design following the token engineering process. What about verification of this thing? So we said, okay, well, I sat down and started to model this with just token spice. And I started actually writing Python models, pure python models of balancer. And then I spent, I don't know, two, three, four days doing this.
01:05:58.110 - 01:06:58.014, Speaker B: And after I was realizing, like, I was copying and pasting solidity code, I was revising, revising banging my head against the wall, worrying that, oh crap, I'm going to have to have unit tests like crazy, et cetera. What if I make mistakes? What if Unitex makes mistakes to make it really good? It's probably going to take me three weeks, four weeks, five weeks, and that's really problematic. And I'm like, well, what the heck, right? I've got this solidity code sitting right here, right? I know how to make the Nash run in the loop. I've got unit tests for this elsewhere in ocean py and stuff. Why don't I pull it in here? So I did, right? I basically took token spice and said, okay, you can have it where an agent itself, rather than just being a pure python agent under the hood, it's solidity. What do I mean by solidity? I mean there's a smart contract deployed onto ganache with EVM, and then you've got a python wrapper for that, which is basically the drivers for that smart contract, right? So you've got, some agents are simply solidity with python wrappers. And what agents are they? It's all the ones that are smart contracts, which I'll get into that, I think, the next later slide.
01:06:58.014 - 01:06:59.854, Speaker B: But it's basically the balancer amm.
01:06:59.902 - 01:07:00.114, Speaker C: Right?
01:07:00.152 - 01:07:20.530, Speaker B: Like bpool Sol. So that's that. And actually overall, right, you can compare two different simulators. Each has their own pros and cons. In terms of software simulators, you've got Google sheets and Excel as the spreadsheets for custom software. For agent based modeling, you've got token spice with ocean v three, you've got CAD. CAD.
01:07:20.530 - 01:07:43.482, Speaker B: And I think CAD. CAD is awesome. I love how fleshed out it is. I love that token engineering is really drilling down on this. That's great, right? And then a third type of simulator is where you have spice in the loop, right? Just like circuit land, where it's basically super high fidelity EVM in the loop in this case. And that's super valuable. And CAD CAD community has talked about doing this with CADCAD.
01:07:43.482 - 01:08:05.400, Speaker B: Great, right? I believe Gauntlet has talked about doing this with their stuff. Great. And I've built preliminary support for this in token Spice. I call it token Spice, too. I'll get to that soon here. And to give you some background here in analog circuit land, people were building a lot of non spice in the loop tools to do verification. Throughout the 90s.
01:08:05.400 - 01:08:31.198, Speaker B: There was probably ten or 15 companies trying to chase that as well. None of them succeeded. Zero, because they were never accurate enough. And it would take too long to write the model. And by the time the model was written, you missed your market window because it might take two months, four months, or whatever. And even by the time it's written, it's probably not accurate enough. But people started to realize, actually, I can spend the extra computation cycles to simulate this thing.
01:08:31.198 - 01:08:57.314, Speaker B: I just have to be smart about what I sample and stuff, right. So rather than trying to speed up the simulator by having simpler models at the level of the analog circuit itself, you have an accurate model at the level of the analog circuit, and then you get smarter about what you simulate, right, about what samples you draw. And so that's what happened in circuit land, and that's where I spent my time, was with Spice in the loop throughout the. Even a bit before and a bit after, and that was highly successful.
01:08:57.362 - 01:08:57.622, Speaker C: Right.
01:08:57.676 - 01:09:38.002, Speaker B: And that's what the industry has adopted. And it's not just the work that I did with my companies, but also other companies in the industry that also went for simulator in the loop. I see that they're all useful and complementary depending on what your goals are. But if you're going for super high fidelity or you want rapid iterations against actual solidity code, then you've got to have EVM in loop. So this is a block diagram showing how these value flows are. And what this is is basically each Dodika Hadron. Sorry, Dodikagon.
01:09:38.002 - 01:09:45.382, Speaker B: That's a polygon with ten sides. With each of these, it's a different agent.
01:09:45.436 - 01:09:45.606, Speaker C: Right.
01:09:45.628 - 01:10:08.270, Speaker B: So there's publishers on the left. So this is actually for a given pool. So there's one publisher for the pool that's on the left. There's a pool in the middle. There's a staker speculator on the right. In fact, there's a bunch of them doing staking and speculating because it's kind of coupled in ocean V three, and on the bottom, there's several people who are the consumers of the data, and then there's various value flows back and forth. For example, top right, sorry, top left.
01:10:08.270 - 01:10:19.374, Speaker B: There's the publisher publishing data token one and pool. And they're also staking. So they put in their data token one tokens and their ocean tokens, and they get back the balancer pool tokens for that pool.
01:10:19.422 - 01:10:20.066, Speaker C: Right.
01:10:20.248 - 01:10:42.054, Speaker B: And then they can, of course, sell those tokens themselves. And they can also sell the data tokens themselves. So that's examples here. And then the pool itself. To model this pool under the hood, this would be bpool Sol, right, the balancer pool, whereas the publisher on the left is a pure python agent. And same thing with all the other agents here, staker, speculator, and data consumer. They're pure python.
01:10:42.054 - 01:10:53.294, Speaker B: I heard someone saying something. Was there a question? No. All right, I keep going. So that's Ocean market V three, right. The different value flow and stuff.
01:10:53.332 - 01:10:53.726, Speaker C: Right.
01:10:53.828 - 01:11:22.178, Speaker B: And it's of great interest to us to model this, right. So I've written out basic models for this in token spice with the EVM and stuff, and I haven't fully connected the wires yet, but that's kind of where it's at. I'll get to that. This is what Ocean market v four looks like in terms of the agents, how they're wired together. So if you look on the right, the stakers and the speculators are split apart. And that's possible because you've got this. If you want to speculate, you can buy and sell directly, but if you want to stake, you're not accidentally buying.
01:11:22.274 - 01:11:22.726, Speaker C: Right.
01:11:22.828 - 01:11:28.466, Speaker B: If you want to stake, you can just simply stake in ocean, and you've only got the vested in ocean.
01:11:28.498 - 01:11:28.934, Speaker C: Right?
01:11:29.052 - 01:11:38.342, Speaker B: So that's a key thing there. And the pool itself also has the robot inside that is minting and burning the data tokens.
01:11:38.406 - 01:11:38.730, Speaker C: Right.
01:11:38.800 - 01:12:08.166, Speaker B: So we've got a bit of different stuff happening here now, but it's largely the same overall. So then I took token spice, what I did earlier, what I showed you for web3 sustainability loop, and I modified it to support EVM. So it's got ganache under the hood. And actually, roughly, generally you can very quickly, if you want, actually make it play with, like robston or rinkabee too. Right. It doesn't really care, which just ganache is local, so it's easier yet. And if you want to be really crazy, you could put it onto polygon or something too, right.
01:12:08.166 - 01:12:44.414, Speaker B: And that's actually part of the aim over time, actually. So what I've done so far, I've written the v four. We have actually first cut sort of prototype smart contracts for the better staking. I've drawn out the schematics like you saw, and I've started to adapt the token spice code, right. So we've got the EVM hooks. A lot of unit tests are on that, where you use Ganache, the Abis themselves are wraps as classes, which are inside these python agents. And it already includes ocean data tokens, data token factory, a friendly fork of balancer, v one Amm balancer, Amm factory, and more extensive unit tests for all of this.
01:12:44.452 - 01:12:44.750, Speaker C: Right?
01:12:44.820 - 01:13:12.534, Speaker B: And in this case, actually, we already had tons of unit tests. Like we had built a python driver for balancer, right. Because we needed that for ocean. So we actually had that, and I just put that code into here. And so this, you know, I could go fairly quickly because this is a straight up software engineering task, and it didn't involve having to accurately replicate port solidity code to Python or something. It was just wrap evm.
01:13:12.582 - 01:13:13.034, Speaker C: Right?
01:13:13.152 - 01:13:33.706, Speaker B: So did all that. And right now where I'm at is writing Python level agent behaviors. For example, how should a publisher behave, right? Should he be greedy? Should he be random? Should he be dumb? And ultimately what you want is actually, you want a mix of them, right? You've got a certain percentage that are greedy, very rational greedy. You've got some that maybe are altruistic, other ones that are just operate randomly, et cetera.
01:13:33.738 - 01:13:34.126, Speaker C: Right?
01:13:34.228 - 01:13:40.322, Speaker B: Same thing for all those. And you can have whatever arbitrary behavior you want. You don't have to write equations for it. You just write Python code.
01:13:40.376 - 01:13:40.594, Speaker C: Right?
01:13:40.632 - 01:14:08.278, Speaker B: So that could be equations, but it can be whatever you want. And so what still needs doing is it's not small. So finish writing the Python level agent behaviors, replicating the dynamics, which is a lot of work, because it's a system identification problem. And I'll get into that. And then put in the ocean market before dynamics. And there's some preliminary contracts, but there's more work there. Iterate on this, simulate and design until satisfied.
01:14:08.278 - 01:14:36.482, Speaker B: This is a lot of work, right? So I started down this path last December, and in January of this year, and then basically a whole bunch of other stuff came my way. And I also realized that, oh, man, this isn't just like three, four weeks of work. This is like six solid man months. Twelve solid man months, whatever, right? It's a lot of work. And so it's been sitting there. It matters to us, but we've had other things that are just more urgent. So when Angela discussed with me the possibility of ocean working with balancer simulations, I'm like, oh, I have a good problem for everyone.
01:14:36.482 - 01:15:00.954, Speaker B: So this is the challenge, right? This is the goal. And by the way, there's a bit about the architecture, but how the agents work. There's controllable agents, there's uncontrollable agents. I describe how this stuff all works with EVM, so there's more details on that. And these agents that all have agent wallets, which is actually under the hood, can be even wallets inside EVM and stuff.
01:15:00.992 - 01:15:01.482, Speaker C: Right?
01:15:01.616 - 01:15:18.194, Speaker B: Private keys, all of that, right. So it's pretty cool. I'm pretty excited about this. And to me, this is very exciting because I'm used to working with simulator in the loop in circuit land. And now token spice is my new simulator in the loop with a very, very preliminary analysis algorithm around it.
01:15:18.232 - 01:15:18.820, Speaker C: Right?
01:15:19.670 - 01:15:31.654, Speaker B: Analysis algorithm. I guess overall, it's the simulator, right. Because you have to have this sort of iteration over ten years, 20 years, 50 years, and that's the simulator. So the simulator is the loop with EVM as part of that.
01:15:31.692 - 01:15:32.280, Speaker C: Right.
01:15:33.610 - 01:16:10.114, Speaker B: So then going to the research questions, which is the main goals here, I needed all the context to get there. So there's the basis, like I talked about, right? Ocean market has balancer, amms, et cetera. Here's the two big questions. Can we capture the dynamics of ocean market for ocean B three? This is a system identification problem. It's a white box one, because we can actually see the structural behavior. So we only really need to tune the parameters, or partly because the agents, we actually can't see the agent structures in a sense, but we can see at least the amm stuff. And ideally, that includes capturing the observed issues.
01:16:10.114 - 01:16:13.294, Speaker B: Ideally, the simulator has people doing rug pulls now and then in the simulator.
01:16:13.342 - 01:16:13.554, Speaker C: Right.
01:16:13.592 - 01:16:18.466, Speaker B: You want that. And then once you have that captured well, and that's a very interesting problem on its own.
01:16:18.488 - 01:16:18.638, Speaker C: Right.
01:16:18.664 - 01:16:22.626, Speaker B: It's a lot of work right there. Once you have that, though, ocean V four has mechanisms.
01:16:22.658 - 01:16:23.046, Speaker C: Right.
01:16:23.148 - 01:16:29.302, Speaker B: So with one sided staking, will that address the issue that we're simulating now of the rug pools, et cetera?
01:16:29.366 - 01:16:29.930, Speaker C: Right.
01:16:30.080 - 01:16:39.370, Speaker B: And what about the IDO, the initial data offering? How well does just jumping to a straight up amm versus a dutch auction versus an LBP work?
01:16:39.440 - 01:16:40.060, Speaker C: Right.
01:16:41.070 - 01:16:43.114, Speaker B: And the other mechanisms that I talked about.
01:16:43.152 - 01:16:43.354, Speaker C: Right.
01:16:43.392 - 01:16:57.614, Speaker B: How well do each of those work? And so those are the big research questions, and I'm very excited about them. I think they will be very cool. And token spice can give some very nice simulation results. And from an academic perspective, I see it's very interesting, too.
01:16:57.652 - 01:16:57.854, Speaker C: Right.
01:16:57.892 - 01:17:55.166, Speaker B: This is the sort of stuff that can get published quite readily in my view. I also want to talk about just one final thing that I think is super cool and where this all can lead. So once you have something that has a simulator, a high fidelity simulator like token spice with EVM in the loop, that's a high fidelity simulator, right? Once you have that, then you can start saying how do we explore the broader design space, right? And we could just do it manually like I'm talking about here. But what if we start to automate that? What if we have evolution of the structure of the smart contracts themselves or the agents and stuff, right? And I'm going to give you a few examples from the past for that and what this can mean for token land. So here's an example for the past. A couple of friends of mine at NASA did this in about 2003, 2005. It turns out that designing antennae is really hard, right? You have to have it where they have a linear capture of the signal independent of how much power there is coming in from the signal.
01:17:55.166 - 01:18:45.506, Speaker B: You have to be in a particular frequency that you have to listen to, actually a frequency band, and you have to have a bunch of other characteristics. Basically you have to pick it up. And the traditional approach to doing an antenna was a big PCB board with almost black magic for what's on that PCB board. And then one big rod going straight up about 5 cm long. And there is like 2000 page textbooks on sort of like the handbook for RF design and stuff, right? But it's literally black magic, right? People talk about token engineering as an art for sure, and so is RF design and largely so is analog design. But in all these, you try to still apply as much as you can engineering practices, right? Really good simulation, better design methodologies, et cetera. So what my friends at NASA did was they said let's just evolve these antenna and let's basically call it the bent paperclip design.
01:18:45.506 - 01:19:30.546, Speaker B: That's what you see here. So this design you see on the right, this is a bent paperclip and the exact bending, et cetera, really matters, right? So they basically had a software simulator, an EM simulator for this. And in the end this is actually the world's best antenna for its use case for its particular bandwidth, et cetera, right? It is so good that one of those two people, Jason Loan, quit his job at NASA, started a company called X five Systems, and for the last 15 years has been consulting, doing antenna design for various space companies. And many of those antennas, I think the first antenna went up in space in 2008. And so this is now one of the standard practices for rf design, right? Basically literally evolving them because it's such black magic, you might as well evolve it and have the best designs. So this is the possibility of evolution. It doesn't end there, though.
01:19:30.546 - 01:20:13.738, Speaker B: You can also do this for coming up with your own white box equations, right? So you can actually take spice, simulate it with spice, do 100 simulations, 1000 simulations, run it through something called symbolic regression to extract functions, which is basically evolution of functions, and then you spit out functions. So this overall setup for circuits, it's called caffeine. And this is work that I did in 2005. And so on the bottom, you see different expressions. For example, it's a closed form expression for gain. That's the first one, and then the next one is frequency response, and then phase margin, et cetera. And each of those is a closed form expression that is super useful to analog designers because, a, they can look at it, get insight into it, for example, they can see, okay, cool phase margin.
01:20:13.738 - 01:20:25.902, Speaker B: It's a function of. That's the third thing down pm. It's a function of the ratio of the drain current of transistor one divided by a voltage on transistor one, and the addition of that with also transistor two, same ratio.
01:20:25.966 - 01:20:26.338, Speaker C: Right.
01:20:26.424 - 01:20:27.842, Speaker B: That's super useful to them.
01:20:27.896 - 01:20:28.500, Speaker C: Right?
01:20:28.870 - 01:20:32.662, Speaker B: So now you can do this, and it'll spit this out to you in just a matter of minutes.
01:20:32.716 - 01:20:32.982, Speaker C: Right?
01:20:33.036 - 01:21:13.694, Speaker B: So that was part of my PhD, and I'm really proud of it because it helped also push forward the state of the art. But you don't have to stop at evolving equations or evolving antennae. You can also directly evolve circuits. And so this is also an output of my PhD research in the 2000s, where I was literally spitting out trustworthy circuit topologies. What was it doing? I actually designed a grammar, a language of building blocks, hierarchically organized of all the different building blocks of analog circuits, from the level of transistors, to one level up with current mirrors, et cetera, to one level up with op amps, et cetera, and higher and higher and higher. And then basically the search algorithm runs through this space, this grammar. And any given point in the space is a tree.
01:21:13.694 - 01:22:24.460, Speaker B: If you evaluate that tree, it looks like a circuit, like shown here, these schematics, if you simulate it, then you get different performance numbers, things like the gain bandwidth, the x axis, or the power consumption, the y axis. If you put this into evolution, you have evolution to run through the search space and find the optimal designs. And why stop at just one optimal design? Why not a whole trade off. So in this case you see trade offs of optimal designs between gain bandwidth and power. So for example, the blue curve on the right, you can see it says okay, for a smaller amount of gain bandwidth you want a particular topology, and that's this one, it's basically half the size of the other one, basically, right? But if you go farther along to the far right, we get a different topology yet, right? And within that there's five or ten points, each of those points is the same topology, but different parameters, giving different trade offs between power and gain bandwidth. So this is actually very exciting to analog designers because they can very quickly evaluate where is each topology strong versus not and what are the parameters for that topology, right? Yeah. So the point is overall you can use evolution to evolve antennae, to evolve functions, to evolve circuits and a whole bunch of other stuff.
01:22:24.460 - 01:23:12.234, Speaker B: A friend of mine from the genetic programming world, he's not a quantum algorithm expert, but he wrote an expert, he wrote a textbook on quantum circuits, how he evolved them all. And then he got a quantum expert to tell him whether they were good or not, and he kept adding objectives and constraints until they were good. And then he published this textbook, and it's actually a very useful textbook. It's kind of crazy, but this is what you can do. Here's the cool thing, you can also do this for token land. So if you want, you can say, okay, I'm going to search across the grammar of solidity, or I'm going to have building blocks of different ERC 20, ERC 721, other higher level patterns, right? Whatever you want, beepool, sol, et cetera. So you can evolve there by searching across the space of trees, and then you can simply evaluate against it with EvM the loop using token spice, right? So token spice can let you do this, right? Well, not right now.
01:23:12.234 - 01:24:07.766, Speaker B: You have to have the search algorithm like genetic programming, but once you have that, then you can evolve solidity, which is super exciting for whatever objective function you want, right? You can also, if you want, go a bit crazier and evolve EVM bytecode itself. And it turns out that evolving at the level of the bytecode level might even be easier in genetic programming. Land people have found it easier to evolve with stack based programming languages like assembly, et cetera, versus higher level languages like javascript, et cetera, and EVM, the bytecode, et cetera. It's all stack based, right? So you're going to have a better time, better success evolving there. You might not understand the results, but it will actually probably perform better. And to me this is fascinating because it points to a future of smart contracts deployed to Ethereum, blockchain, et cetera, that literally no one understands. But they are the best in class circuits, right? And I see this just as a natural progression.
01:24:07.766 - 01:24:31.678, Speaker B: And there's enough financial incentive to do this, right, for like flash bots, et cetera. And of course, you can evolve wasm bytecode too, if you want, or C plus plus, whatever you want. You can also even do other stuff without EVM in the loop. You can do evolving networks of attackers and defenders. And unume O'Reilly is a genetic programming researcher out of MIT. She's been doing gp for 30 years. Really world class, amazing woman.
01:24:31.678 - 01:25:24.690, Speaker B: And she evolved this in the context of network attackers and stuff. But imagine this for a world class bug bounty approach or finding problems. This is going to be really awesome, right? So once again, you would have be evolving EVM bytecode, but you would also then be modeling at the level of network latency, et cetera, et cetera too, right? And there's models out there, right? For example, Sam and his team at Arweave have actually built an erling based model of the whole rweave blockchain, which I just think is awesome, right? So out there, there's sometimes quietly things going on that you might not hear about. For simulators getting built. CaDCAd is well known, but there's other people just doing things with their own one offs that are actually kind of amazing. And I think Sam and his team did something amazing there, too. So overall, right, starting to wrap this up, there's roadmap with this work, right? There's three big research questions.
01:25:24.690 - 01:26:23.174, Speaker B: You can frame it as the following. There's a system identification problem. We want a high fidelity model of ocean V three, right? What does that mean? Well, you've got the smart contracts there, you've got some agents written down, but what you need to do is go and find the observed behavior of ocean V three that actually happened on chain, right? So this is from October of last year until now. Dump that all, like evaluate all that, have time series models based on what's going on, et cetera. And then you want to run your simulator and see if it's actually having dynamics like that, right? And of course you're going to probably have to take this in a statistical direction too, because what actually happened is sort of one random sample of one of many possible samples, right? So overall, this is a system identification problem that actually does have an element of randomness to it, right? So you actually have to account for that. It's a really interesting problem. The second one is that first thing is not trivial, right? It's going to be a lot of work.
01:26:23.174 - 01:26:55.166, Speaker B: It's really interesting on its own. And you want to observe and model things like rug pulls, et cetera. The second thing is verification. So take this, starting with the results from step one, you then inject in the v four stuff, right, the one sided staking robot, et cetera. And then you see how well it works. And if you add in the one sided staking robot, does it address rug pulls? Right, et cetera. And then finally, once you've done those first two things, you can start tuning ocean v four.
01:26:55.166 - 01:27:06.354, Speaker B: You can say, okay, now we're going to optimize the parameters of this to make it really awesome, right? And you can choose whatever parameters you want to wiggle, but you have an optimizer in the loop, at the outer loop and just do it.
01:27:06.392 - 01:27:06.834, Speaker C: Right.
01:27:06.952 - 01:27:38.960, Speaker B: And here's the cool thing, you don't have to stop there once you've got this, this is sort of the nonlinear future, especially sep four. Then you get to system identification, where you extend not just from ocean market, but to the broader balancer ecosystem. You start with V one balancer. So up till when V two was released in the past month. And you basically try to fit your model to all the observed on chain dynamics there, right. So the scope of the problem is ten x 100 x bigger now, right, in terms of the amount of data, et cetera. But fundamentally, you've already got an approach to it from the work in step one, step two here, right.
01:27:38.960 - 01:28:01.750, Speaker B: And you make sure that you've got the uncontrollable variables, probabilistic and worst case, I think you might need that earlier on. But if you can get away with it just now, that's good, too. So that's the v one. And then also tack in the balancer V two stuff, right. And balancer V two has a much richer variety of strategies in the pools, right. Astrategy. And Balancer V two is a pool, right.
01:28:01.750 - 01:28:20.390, Speaker B: That's a pool as sort of a facade to the balancer treasury. And then you try to fit the model to the observed on chain dynamics there, right. Although we have less data there. But it's going to be interesting to balancer. Then step three, right. You start doing designs, place exploration to tune balancer V two strategies. Right, towards minimizing impermanent loss, et cetera.
01:28:20.390 - 01:28:36.814, Speaker B: We see a basic version of this already with gauntlet, right. They're working with balancer directly to basically tune a single parameter, which is the weight of pools in balancer, to try to minimize impermanent loss. Right. Depending on the relative ratios of the tokens in the pool.
01:28:36.862 - 01:28:37.122, Speaker C: Right.
01:28:37.176 - 01:29:05.754, Speaker B: So that makes sense. It's a very cool start, right? And gauntlet has their own closed source simulator, but I would love to see this open source with sort of a big community effort around this. Right? So I think that's going to be super interesting. And then finally, once we have stuff there, once we've sort of done design space exploration and just tuning parameters, let's go more open ended yet, right? Why stop at just tuning parameters? Let's tune structure itself. Let's evolve solidity, bytecode. Let's evolve evm at the level. Sorry.
01:29:05.754 - 01:29:08.762, Speaker B: Let's evolve solidity. Let's evolve bytecode and more.
01:29:08.816 - 01:29:09.082, Speaker C: Right?
01:29:09.136 - 01:29:19.134, Speaker B: And that's what this is about, step four. And you can kind of go nuts and eventually you end up with AI dows that own themselves and whole new approaches to solve Mev and all this.
01:29:19.172 - 01:29:19.470, Speaker C: Right.
01:29:19.540 - 01:29:25.454, Speaker B: I think it's going to be really fun at that level. And that's huge on its own. Right? That's a research field that can go nuts for 20 years.
01:29:25.492 - 01:29:25.694, Speaker C: Right?
01:29:25.732 - 01:30:29.298, Speaker B: In step four there, right? And I think it's going to be really fun, but one step at a time, right? So that's why we have these first three steps here, then these next four steps. And the final step is like fully open and super interesting. So to wrap up, what I talked about today is I started with a summary of the research questions. Then I gave an intro to ocean, and then simulation and verification from the world of electrical engineering, analog, cad, et cetera. Then from the nascent world of token engineering, simulation and verification, where we are so far. And then I drilled down into token engineering practice within ocean for the system level, the web3 sustainability loop, and how we built token spice to verify that, but also how token spice is more general than that. And then also around ocean market, the token engineering there, the problems, the token engineering work we did, and then where we're currently at with a verification using token spice, having EVM in the loop and we're only partly there, which has led to the near term research questions, I think the scope of the work for the next while, but then also what it can lead to beyond that which is much more broad.
01:30:29.298 - 01:30:32.802, Speaker B: Fidelity models across all of balancer and eventually defi, et cetera.
01:30:32.866 - 01:30:33.478, Speaker C: Right?
01:30:33.644 - 01:30:37.080, Speaker B: And that's it. We're at the conclusion, so I will stop there and open to questions.
01:30:38.910 - 01:31:01.390, Speaker A: Awesome. Thank you so much. Now, everyone, shoot your question. I guess there's a lot, and certainly I'll share the link as fast as I can so that you can rewatch it and dig into the material trend. Please share the slides.
01:31:02.690 - 01:31:03.438, Speaker B: Okay.
01:31:03.604 - 01:31:35.820, Speaker A: And yeah, just raise your voice if you have a question. In the meantime, we'll continue next Tuesday in our working session, same time, 01:00 p.m.. CST. Then we can review those research questions. And I'll guess from there we form teams and specifying who is working on what. And certainly there will be a lot of questions that we either drop into the discord or we set up another session. Whatever fits best.
01:31:35.820 - 01:32:00.340, Speaker A: And, yeah, in total, we have nine weeks, guys, so I think we are ready to cover everything. Yeah. But certainly very great inspiration for where this can led us to and a lot of exciting research questions. Last chance. Do you have any immediate questions at the moment?
01:32:01.830 - 01:32:40.938, Speaker B: Yeah, I have a question. Hey, Trent, that was such a great presentation. Thank you. So, I was wondering if you had any thoughts on, since the EVM takes so long to do transactions and stuff, I was wondering if first simulations, if you had any thoughts on maybe how to make that transaction time shorter or maybe even simulating the EVM in some way. Well, here's the thing. Right there is an EVM simulator. If you call, that's 100% fidelity accurate.
01:32:40.938 - 01:33:16.922, Speaker B: It's called ganache, right? Ganache is basically a local EVM simulator, if you will. It's an ethereum network of one node that you're running locally, right? And then to run this thing takes, like, I don't know, a thousandth of a second. It's super fast, right? So depending on the smart contract that's running, right. So you can do iterate through this, like in my unit test, et cetera. I've got token spice running with simulator in the loop. With EVM in the loop, and it's doing thousands of simulations, and it only takes 10 seconds for unit tests, all that. So it's already super fast, and that's the point, right? Like, ganache is your friend here.
01:33:16.976 - 01:33:17.578, Speaker C: Right?
01:33:17.744 - 01:33:23.774, Speaker B: And then once you want to simulate on more broad networks and stuff, or play in the real world and broad networks, you can.
01:33:23.812 - 01:33:24.014, Speaker C: Right.
01:33:24.052 - 01:33:31.534, Speaker B: But when I say EVM in the loop, the main focus is for rapid iterations locally against Ganache, which is all you need.
01:33:31.572 - 01:33:31.822, Speaker C: Right.
01:33:31.876 - 01:33:35.938, Speaker B: So it's not slow. It's fast. Super fast. Cool. Thank you.
01:33:36.104 - 01:33:36.820, Speaker C: Yeah.
01:33:42.310 - 01:34:08.074, Speaker B: And that's the exciting thing I should mention to me. Like I said, I was playing with balancer, trying to get it, just write the python models for it. I knew there had been some research work as sort of token engineering work to have balancer models in CaDCAd and stuff. Great, right? But then I knew Balancer V two was coming down the pipe, all these strategies and stuff. So it seems that all the modeling work for CADCAD is always going to be two months, six months, twelve months behind the state of the art of what the solidity is.
01:34:08.112 - 01:34:08.410, Speaker C: Right.
01:34:08.480 - 01:34:39.990, Speaker B: And that means you can't easily iterate rapidly on the solidity designs either. Right, but what if you could, right. What if you could come up, write a new solidity design, and then literally simulate it a minute later and see what the result is in terms of the effect on token price and ocean Dow funding or marketplace volume or whatever it is? Right. And this is what, really, to get there, you need to have EVM in the loop. Right? So you need an agent based simulator where some of the agents can be EvM based. And that's exactly what token spice two is.
01:34:40.060 - 01:34:40.680, Speaker C: Right.
01:34:44.650 - 01:35:05.210, Speaker A: Makes sense. Anything else at this point? No. Okay, then. Thanks a lot, Trent. This will be only the starting point. Thanks for sharing this. We'll see each other soon in this group next Tuesday.
01:35:05.210 - 01:35:08.746, Speaker A: One C-E-S-T. Thank you. Bye.
01:35:08.938 - 01:35:09.850, Speaker B: Thank you. Bye.
