00:00:04.280 - 00:00:48.978, Speaker A: All right, welcome. Welcome to the Te Academy study season and the session defi concentrated liquidity from scratch with Mark Richardson today in the fourth session already. So welcome again, everyone who is following the program since session one. And welcome to everyone who is new here. In case you are following this session via Twitter or YouTube, welcome to Te academy. You can register for the study season via tokenengineering.net to get full access to the learning material and to the session recordings, and to also have the chance to mint an NFT proof of knowledge.
00:00:48.978 - 00:01:16.044, Speaker A: We'll discuss the options and how to receive such a proof of knowledge later in the session today. For now, let's start with the program. We had an exciting homework to do, building two separate liquidity pools. There was an office hour last week, so I think a lot of stuff to discuss. Mark, I hand over to you and welcome everyone in the call.
00:01:17.984 - 00:01:19.688, Speaker B: Thank you very much. Can you hear me okay?
00:01:19.816 - 00:01:20.524, Speaker A: Yeah.
00:01:22.364 - 00:01:41.972, Speaker B: Okay. I'm going to share my screen here. Okay. Can you confirm that you can see the title slide perfectly well, thank you. Wonderful. Okay. Yeah, so quick.
00:01:41.972 - 00:02:58.424, Speaker B: So we have probably a pretty long session today. It's our second to last session, and what I'm going to try and do with this, and what I said I wanted to do last week is to kind of bring some of the engineering aspects to concentrated liquidity to the forefront. And the purpose of last week's homework assignment was to give you an opportunity to engage with some of the concepts that I think you can only learn by doing. Right, by actually trying to make some mathematics perform a certain way under certain circumstances. But the overall structure of the course has been to really look at the mathematics component. We've spent sort of, I think, a lot of time dealing with the algebra and calculus and geometry and so on and so forth. The trajectory that we've taken has been pretty much a, you know, has followed the history of the development of concentrated liquidity, you know, beginning with Bancor's invariant, which was published in 2020, which looks like this.
00:02:58.424 - 00:04:25.944, Speaker B: And then we've just started to look at the Uniswap environment, which looks like this. And while I have provided you an enormous amount of course material via the textbook that goes through these derivations, I think that now that we have sort of our sea legs under us with respect to the algebra and how to navigate some of these equations and transformations, that there's really no purpose doing too much more algebraic manipulation. But that's with respect to these models, because they're pretty much the same thing over and over again. This is going to change a little bit with the last lecture. The last lecture is where I'm going to be presenting what I call the natural concentrated liquidity invariants, meaning that they have the way that they are parameterized, or the variables that we use to describe them are not like, asserted from the perspective of a protocol implementation, but are sort of naturally expressed as functions of other well known concepts. And, sorry, they are expressed with respect to well described and well known functions in things like. In things like trigonometry.
00:04:25.944 - 00:06:08.212, Speaker B: It is a good time to sort of pause and reflect, though, on whether or not we have actually sufficiently covered the theory up until now, and because there's been a piecemeal participation in the homework assignment, either because people are just kind of auditing the course and they don't want to do the proof of knowledge, which is totally fine, or because people are feeling quietly overwhelmed, and some of the material is very challenging. And so what I've decided is to actually return to this section of the homework and do a full tutorial with you right now. And it's going to be a little bit similar to the office hours that I held during the week. But if you've already watched that video, I'm not going to repeat too much. We're going to take a slightly different approach, and I'm going to add some commentary that I may not have had the opportunity to do that. Now, there's obviously a bunch of different ways that we can approach some of the problems that I've been offering people as homework assignments, and you will note that there are those who have their preferred methods and tools. So, for example, we've seen a really terrific contribution from Andreas, who has implemented everything in Desmos, and it's actually a very beautiful description that he's provided there.
00:06:08.212 - 00:07:19.354, Speaker B: And then people like Thiago, who are very comfortable with Python and would prefer to use Matplotlib and so on, but there's something that's very, I think, has a really good pedagogical value about Excel, meaning that it's so accessible. And I think even if people don't have a Microsoft 365 subscription, they can still use Google sheets for free and things. And just being able to sort of click around and do things means that we can sort of describe the process, I think, in a slightly more helpful way than we could if we were sort of commenting on like a map lib code base or something like that. So what I want to do is to essentially walk you through. Not necessarily, we're not going to complete the whole homework assignment, in this tutorial. But what I do want to show you is how you might think about approaching it. Because one of the things that I, you know, that I'm not sure I have made clear is that all of these exercises that you're doing when plotting things, there are transferable skills here.
00:07:19.354 - 00:08:36.434, Speaker B: Often, when you're trying to visualize data, you're going to be making decisions about, you know, how the data is distributed, like, what you should approximate, what should be hard coded, and you have to sort of make those same decisions when you are implementing something in a smart contract that the protocol is never going to be infinitely precise. And this is something we've already discussed at length. And so to give it, for example, a lookup table or to constrain the scope of its activity to within certain well understood boundaries is a very normal thing to do when you're developing these systems. And while it's very difficult to sort of teach you that in the context of actually implementing something in a smart contract, because we simply don't have the time and many of you won't have the expertise to participate in something like that, I feel like graphing things and plotting things is totally accessible. So I think that that's enough of a preamble. Let's just begin. Remember, the point of the homework assignment was to create two liquidity pools back to back, such that they shed a common price boundary.
00:08:36.434 - 00:09:14.684, Speaker B: And I said I wanted you to do it first for the Bancorp two formula, and then do it for the Uniswap v three formula, and get a feeling for which one was easier to work with and which one was harder to work with. Let's start with the uniswap b three formula here. And recall that there are three constants that describe that invariant or that parameterize that invariant one. Is this p high value or actually the square root of that p high value. But we'll come back to that. Let's just choose any number. It doesn't really matter.
00:09:14.684 - 00:10:41.792, Speaker B: P low can be, you know what? Let's use? Let's use a geometric sequence. And remember, P zero is always the square root of the product of these two things. So, you know, in some sense, you know, six is halfway between nine and four, but it's halfway between nine, nine, and four in sort of a multiplicative way rather than an additive way. Now, l, remember, is just a number, right? Obviously, it represents something, but we never really spoke about what l means, and it's just sort of the size of the curve, right? It's that invariant that gets larger and smaller, and so we can choose, like, any number for l and still come up with a reasonable result. And one of the things that I want to show you, and this is going to contrast a little bit with some of the material that Andreas has provided us, is that you don't necessarily need both of these to both of these liquidity pools to meet at the same point in virtual space. You only need it to meet at the same point with respect to the price. So, okay, how to begin plotting with something like this? These are the variables that we know about, and we know that we're going to need the square root of these things as well.
00:10:41.792 - 00:12:00.644, Speaker B: So let's go ahead and create variables for that. Okay? And when we're. You know, when I'm doing this, I want you to think about, like, how much. How much data we're using. You know, like, if this was a smart contract and we were going to need to store all of these values, like, what can we get away with not storing? And what calculations would you prefer to not have to do again? So, for example, it's not very difficult to take the square root of nine in that I think even in a modest implementation, you'd be able to figure out, like, a very fast algorithm for finding that square root without too much trouble. But if this was a stranger number, like, if this had decimal places and things, then performing a square root on the smart contract is not necessarily a great idea. Taking the square root first, maybe this is something that you can perform using off chain infrastructure, and then taking that as an input, and just storing that data in the smart contract as it's received from the off chain infrastructure, is probably a better way to handle things.
00:12:00.644 - 00:13:34.568, Speaker B: I'm going to start by creating the virtual curve, and in order to decide how wide the window is, which part of this curve we should be graphing, we can start to make use of some of the other relationships that I've already taught you about. For example, we know that the real curve is the one that we're most interested in, because this is the one that actually tracks correctly. The token balances with respect to the invariant function, and that curve is characterized by an x intercept and a y intercept, meaning, what is the x coordinate when the balance of the y tokens has been driven to zero, and what is the y coordinate when the x tokens have been driven to zero? And while I didn't derive this for you in the lectures, I have done it for you in the textbook, and I think it's the l term multiplied by the reciprocal of the square root of p ler minus the reciprocal of the square root of p higher. That looks about right. And then for the y intercept, it's slightly different, but you can tell that they're related. This is also l, except that it is multiplied by the difference between p high, the square root of p high, and the square root of. Okay, yeah, that looks right to me.
00:13:34.568 - 00:15:12.596, Speaker B: Okay, so this is already telling us kind of where our, you know, where our real curve is likely to be, to be plotted so that we don't have to plot all this useless data outside the window that no one needs to see and no one cares about. The, the other useful term are these asymptotes. And this was one of the things that we discussed last week. And remember, if you're dealing with concentrated liquidity in a conventional sense, these are expected to be negative, and they are also the sort of a substitute or a code name for the horizontal and vertical shift parameters that have been added inside the invariant in order to drag that virtual curve back to the x and y axis. And so you can just look at the uniswap invariant and just see, just by visual inspection, that the x asymptote is equal to negative l over the square root of p high, and the y asymptote is equal to negative l times the square root of p low. And again, this is something that we're going to use to start constructing, you know, to start constructing the plot and make informed decisions about where things should be. So, for example, we can already ascertain what the very largest x value is, or the window of x values that we want to plot between with respect to the virtual code.
00:15:12.596 - 00:16:35.032, Speaker B: So let's do that. First, if we take the x intercept, right, which is the highest x value, in order to translate that back to the virtual curve, all we need to do is reverse that horizontal shift that we've been speaking about. So what I'm going to do here is I'm going to write max, the Maxx plot, something like that, and, and just do that process that I just said. And so by reversing the shift, that's the same as subtracting off the asymptote, because the asymptote has moved back, and because the asymptote is a negative number, subtracting a negative number means these things add together and we're going to get some maximum x coordinate. That's going to help to orient us with respect to what we're trying to put and gives a little bit more of a little bit more efficient, make us a little bit more efficient in our exercise. So if that's the maximum, then what is the minimum going to be? Well, we know that the smallest x value that we're going to be plotting on the real curve is going to be zero. And so to get its minimum virtual, we do exactly the same process.
00:16:35.032 - 00:17:19.242, Speaker B: We just subtract off the asymptote. And now we have a range of x values through which we're going to try and plot something. Now, for those who haven't yet seen the office hours recording, and I apologize, it was a 90 minutes call, so it is a little bit on the long side. One of the things that I did introduce, and this was kind of specific to the reason we had the call, was this idea of choosing a better sampling space rather than just always assuming that to linearly sample stuff is the best way to do things. Depending on the function that you're plotting, you might actually lose some of the fine structure. And so. Oops, sorry, Max.
00:17:19.242 - 00:18:32.650, Speaker B: Max index. So instead of distributing your data points by a constant difference, you can instead distribute them by a constant quotient. Now, I'm not going to describe that process in as much detail as I did in the office hours, so if you are interested in what I'm about to show you, please go back in and have a look at the office hours reporting, because I do think it's worthwhile. But what I'm going to do here is I'm just going to choose, let's say 100 points, and then I'm just going to add an index column here, beginning at one, sorry, beginning at zero, and then enumerate this up to 100, 100 points. And this is going to essentially give us the size of our data set. Now, because we enumerate from zero, we actually are going to have 101 data points. But just like I said at the end of last week's lecture, just because you got 101 data points doesn't necessarily mean that your data set is 101 items large, because maybe it's the interval between these data points that matters.
00:18:32.650 - 00:19:27.024, Speaker B: And that would be, you know, the change in perspective. And it depends on the specific context of the conversation. But just know that that's an important realization. Okay. So to distribute the x coordinates geometrically, and, you know, it will become obvious why this is a, a good idea in just a moment. The formula for doing this is that you basically take whatever your minimum value is and then you multiply it by the quotient of the maximum value that you're trying to hit and the minimum value that you're starting from, and then you've raised that to the power of whatever the index is that you're currently at. So whatever data point that you're at as a quotient of the maximum index that you're trying to reach.
00:19:27.024 - 00:20:34.826, Speaker B: And you should see already that this is giving us exactly the, you know, the starting value that we were looking for. And I need to quickly make some of these cells constant so that I can just pull it down the rest of the column. The only thing I need to be variable there is the index. And this will give me a distribution of points that ends at exactly the, at exactly the value that which rhino hits is 61,728, which just happens to be our max. But the distance between each of these, and I can show you this really quickly, is such that it is a constant quotient. So if I take, for example, each one of these values and subtract it from the value that occurred before it, what you will see, let me get rid of this cell here. What you will see is that the distances are all different, right? These are all, you know, there is no constant difference between these values.
00:20:34.826 - 00:21:53.944, Speaker B: But instead of subtraction, if I do a division, the division is constant, right? And so this, you know, each value is some constant multiple larger than the last value. And this has, this is, you know, this has some important consequences with respect to the curves that we are looking at. But I wanted to just show that, because I did notice it in a lot of people's plots that, you know, this kind of, you know, efficiency in data distribution hadn't really been considered, which is fine, because it's not something that we explicitly spoke about, but it is something that I think that would be good to know. Okay, now for the virtual curve on the uniswap. On the uniswap invariant, we know that the, like, the real, like invariant, the thing that's on the right hand side of the invariant function, the way that we've been writing them, is just l squared, right? So if we take that l value, square it, and then divide it by these x values, we should be able to get all of our y code coordinates relatively quickly. And indeed we can. And then we can get a quick look at our plot by inserting, let's say, a scatter plot here.
00:21:53.944 - 00:22:33.898, Speaker B: Good. Okay, so this is our virtual curve, and we could use this as is if we wanted to. In fact, I think that. I think that we will. But I want to again, return to quickly the purpose of the homework assignment, which isn't to create just one of these things, but to create two of them back to back. And so what I'm going to do is I'm going to, let's move this out of the way for now. Actually, I might need to move it over there.
00:22:33.898 - 00:23:21.972, Speaker B: I'm going to copy all of these and I'm going to paste this here. And I'm hoping, okay, need to move some of this stuff over. Okay. So the thing that I want to show you is that when we come to plotting this stuff next, we want either this p high to be the same as that p, or we want this p low to be the same as that p high. And it doesn't really matter which one we choose. So let's go ahead and, you know, let's do it the other way around. I'm going to set this p low to be nine and then I'll put this one up to 16.
00:23:21.972 - 00:24:56.894, Speaker B: So again, we're following a geometric sequence. Okay, now what you will note is that our x intercept has already updated and so has our y intercept because it's just referring to these rows, which makes this a, you know, a fairly efficient way to explore some of these things. You know, if you've got a spreadsheet that's already set up with these, you know, that's already set up this way, then, for example, if I was asking in an exam question what the relationship is between two different variables, then you can actually just feed it some values and experiment with it to try and get to see if there's a reasonable result there. And so this plotting exercise, it's not just about creating nice looking plots, but also to give you an exam question answering machine. If you know how to interpret the information in the exam and you know how to feed it into your, you know, into your plotting program, then it should be able to spit out results that will help you to work out what's true and what's not. Okay, for our virtual x on this other plot, I'm just going to move across each one of these cells and drag this down. And this l needs to go there.
00:24:56.894 - 00:25:25.154, Speaker B: Perfect. Good. Okay, so what we have now, and remember, we're referring just to the virtual curves. And for uniswap v three, that's actually a very helpful way to do it. Uniswap v three is kind of the easiest to understand from its virtual curve perspective just because of the way that it's constructed. But that's okay. Okay.
00:25:25.154 - 00:26:19.242, Speaker B: So what I want to show you here is that we've now got both of our, we have both of our plots set up here, but they look very, you know, they look very different. And if I set this to, let's say, 10,000, and I set this to 10,000, you can see that they actually match up perfectly. This is a really good example of joining these two things up. It just so happens that when this l term is equal in both, they do match up. Right. It's the same as having one continuous curve, but they don't need to match up. Right.
00:26:19.242 - 00:27:02.154, Speaker B: This could be, you know, out slightly. Let's do 12345 for this one, and we could do 54321 for this one. That's probably a bit big. And these are still continuous with each other. They're just not continuous with each other with respect to their virtual token balances. And I could show you this. What I could do is create a dy DX column here and actually measure the marginal rate of exchange at all of these different points.
00:27:02.154 - 00:28:23.718, Speaker B: What you would find is that when we plot that, it's always continuous, the derivative of this thing is continuous, but its virtual token balances aren't. But I don't have to demonstrate it, because you already know that it's true, because it's how the curve is parameterized. We know that p low here is nine and that p high here is nine. And we know that we plotted between these two extremes because we've already pulled out all of the information that we need in order to just graph that part of the curve, the part of the curve that's contained between those two price bounds. And so our window is limited there. And there's a bunch of things that we can do to confirm for ourselves that we're not making a mistake. And one, I think one helpful one, is to start drawing the real curve, because we know that the first, at index zero, our x real, should be zero, and at index 100, our y value should be zero.
00:28:23.718 - 00:29:35.994, Speaker B: Right. Just because of how those intercepts work and how we've defined the terms, or how we have decided to plot our stuff, how we've organized our data to do that, remember, the only thing that we need to do is to take whatever the x virtual token balance is and add the asymptote, which is the same as doing that horizontal shift. And so that gives us the zero time that we were looking for, which is, again, it's nice to already have in your mind what you're expecting to see before you attempt one of these things, because otherwise it might take you a little while to notice a mistake or notice something fishy's happened. For the yreal balance, it's exactly the same thing. We just take the y virtual token balance, we add the asymptote. And again, I need to make that constant and then drag this down. And so if I scroll to the bottom, to the 100 data point, we indeed see that this is our y balance returning to zero.
00:29:35.994 - 00:30:31.494, Speaker B: And we may as well add this to our plot. So if we take our x values from, from this column and our y values from this column. Good. So this green and this and this blue and this, this blue component are actually, you know, the same curve. It's just that one of them is pretending to have tokens that it doesn't remember where this came from. And we're going to do exactly the same thing for our, for our other curve here. I just need to shift the asymptote to a different location and same for the y axis.
00:30:31.494 - 00:30:52.794, Speaker B: Okay, let's check to see that we've got a zero at the bottom. And we do. So now let's add that to the plot as well. Okay. Adding our real x coordinates. Adding our real y coordinates. Okay, perfect.
00:30:52.794 - 00:31:42.906, Speaker B: And so this, like, light blue curve that we've just added is, you know, is coming from, from this orange one here. Okay, I'm just going to quickly change our chart design so that it's a little bit more pleasing to look at. There we are. Okay. Now the important thing to realize is that even though these things are so discontinuous that, you know, you could be trading, you could be trading on this curve here and then suddenly end up all the way up here. And even though this looks like a discontinuous function, it doesn't matter. I mean, there are lots of reasons why it doesn't matter.
00:31:42.906 - 00:33:05.014, Speaker B: But what I mean is, you might not notice if you were the trader or it might not be immediately obvious because the marginal rate, right, the derivative of this curve as you leave it, is actually the is. I think this is the other way. The marginal rate on this end, on the left end at the low x, the marginal rate there as you leave and teleport suddenly over here are the same. So if that was a hill, right, and you were climbing it, you wouldn't even notice that you had like, teleported from one virtual coordinate to another virtual coordinate because the slope of the hill hasn't changed. And that is kind of the, like, if I had to express my opinion of like, the uniswap v three implementation and like, and what it's done, it's that clever realization, right, that you can have all of these different virtual curves with all of these different amounts of liquidity in them, because users are choosing which buckets they're going to, you know, they're going to contribute their tokens to that. It doesn't really matter. The market can still interact with it in a continuous way, and it will mean that there will be, you know, some.
00:33:05.014 - 00:34:27.486, Speaker B: Some parts of the. Of the. Of the price chart become more stable because the market is, like, finding more liquidity at that price point, and there will be some parts that are, like, relatively vacant, right. Or there's no liquidity there at all, and the price might skip. But if you're making the assumption that there is going to be a competition between people that are selling their tokens to consumers, then it makes sense that they will cluster around the current market price, and depending on their risk appetite, and depending on their, you know, their specific outlook, either the media outlook or even the long term outlook, that will change how they distribute their liquidity with respect to each other. And that's okay, right? So, as long as you have an implementation that puts all of these, that organizes these curves such that you can guarantee that the slope at one end of a curve that's now like leap being left is equal to the next slope, right. As you're joining the next part of the curve, irrespective of how relatively large they are with respect to each other, you're going to be okay.
00:34:27.486 - 00:35:27.634, Speaker B: Like, it's going to be a product that people understand and don't mind interacting with. Now, why go to all this effort of showing you all of this stuff and asking you to. To plot it and things for uniswap v three and then ask you to do it for Bancorp two? Bancorp V two didn't have that assumption. Right? Like I've said in previous lectures, it was a single liquidity pool that used a different rebalancing mechanic in order to try and keep the market satiated. I think I've repeated ad nauseam that these equations that we're talking about, they're always one in the same mathematical object. There is fundamentally no difference between, you know, the mathematics that describes uniswap v three, then describes Bancorp v two, that describes carbon d five. I'm going to talk about that a little bit today.
00:35:27.634 - 00:36:39.656, Speaker B: But the choice of the parameter here, right, actually giving the curve the ability to determine its behavior based on the prices that you want it to operate within, made drawing this chart pretty easy. What about when we don't use those let's use the Bancor v two version. So y zero, x zero, and the amplification coefficient. And again, we can just choose, you know, random numbers. So let's do one, two, three, and three, four, two, and let's have a be something like 2.5. Now, again, we're going to try and think of, like, what is the part of the curve that we're trying to plot? Like, what would be a window that would make sense to visualize so that we're not, like, spamming useless data that we're not interested in seeing? And so, again, it's helpful. Like, there's a reason why I give you these characterizations.
00:36:39.656 - 00:37:27.784, Speaker B: And obviously, it's not just for plotting, but also because of the things I've said earlier. These will change how you think about implementing these things should you ever come to try and do that. So, remember, the x intercept and the y intercept are defined in pretty much the same way it is. Oops. So, the x intercept is defined as x zero multiplied by twice the amplification coefficient minus one over the amplification coefficient minus one. And the only difference between x zero. And so the x intercept and the y intercept is whether or not you're multiplying it by the geometric middle x coordinate or the geometric middle y coordinates.
00:37:27.784 - 00:38:00.604, Speaker B: Okay, so those are the intercepts. And then, remember, the asymptotes were also relatively easy to do. In fact, I think in the Bancor case, they're even easier, because it's just the negative of whatever that geometric middle coordinate is multiplied by the amplification term, minus one and the same. Oops. And the same for the y asymptote. Here we are. Good.
00:38:00.604 - 00:39:02.504, Speaker B: After that, we can again start going through some of the process of defining our window. But note that we haven't spoken about a price anywhere here yet. We have enough information to determine p high and p low, but we've had to go through, like, we didn't have to do, necessarily, the intercepts and the asymptotes, but that will be helpful. But we're now determining what these, what these price bounds are by almost, like, by guessing. So, for p high, this is going to be a squared divided by a minus one squared multiplied by the quotient of y zero and x zero. So this is our p high value, and then our plo value is more or less the same thing. You just flip the a times up around the other way.
00:39:02.504 - 00:39:59.224, Speaker B: This will give us our plo value. Okay, so let's say, let's just pretend that we've already plotted all of our stuff here, right? And we've got this beautiful, you know, curve. When we go to create our other plot, we need to perform that same process. Remember I said on the uniswap b three one, I'm going to just choose the p low of one of these to be equal to the p high of the other one. And so I need to do the same thing here. But I don't know, maybe that's really hard because I can't just, in the way that this is parameterized, I can't just hit equals on my keyboard and set it to be p high over here because it's not updating correctly. These p high and p low parameters are produced from these other parameters.
00:39:59.224 - 00:41:51.580, Speaker B: That's what makes this more difficult an exercise for this specific parameterization of concentrated liquidity. Whereas for this specific exercise, Uniswap v three's parameterization of concentrated liquidity is much easier to do. And just that realization, just thinking that, ok, I had to plot something and it was very, very fast to do it using variables defined a certain way. And if I use exactly the same mathematics, exactly the same geometric, the same geometry, the same underlying principles, truth of the universe, even though these are redundant formula, it's now just more difficult for me to achieve what I wanted to do. And so the mathematical equivalence of these things, like just never confuse that for meaning that you can take any one of these formula that have been expressed in someone else's white paper or in someone else's smart contract and then transfer it verbatim to a new product and expect it to behave the same way. The more important point here is that even if, even if we did go through the trouble of redefining x zero and y zero here so as to actually meet our, to match up the p high and p low, it will never be quite right. And the reason is that for this parameterization, the p high and p low versions, like the PI p low values that we've chosen, they are hard coded.
00:41:51.580 - 00:42:45.444, Speaker B: This is an absolute value even in excel, it's actually referencing this other cell. So I can guarantee to an infinite amount of precision that this p low value, even if there's a rounding error in it, that the rounding error that it has is going to be exactly the same rounding error that this one has because it wasn't calculated. It's a number that I have written to the contract. And so it is the gospel truth. Whereas if we have to write these variables right, the amplification time, the x zero and y zero times to the contracts, then the p high and p low boundaries will never be gospel. They will always be approximate. Even if the algebra says no, this is the actual value, right? If you do this calculation, you're going to determine these boundaries.
00:42:45.444 - 00:44:10.020, Speaker B: Remember, if you're performing that calculation in a computer, it's never the right answer. You only get approximately the right answer, and that exposes undefined behavior. That if your system is assuming that these boundaries are extremely well defined and reliable, that even if there is just a fraction of a millionth of a percent of error in it, someone might spam your smart contract with deliberately chosen inputs so as to exploit that bad assumption and do something like drain the smart contract of the liquidity. And for those, I'm sure many of you know, this is not an uncommon occurrence, these kinds of bad assumptions in this kind of lazy implementation, using someone else's work to try and like shoehorn it into your own code in sort of a haphazard way, it does have devastating effects, and we usually refer to these as hacks and exploits. But it can also just be an accident. You know, sometimes the market itself, people just using your protocol will stumble across one of these errors that cause that cause, you know, bad behavior. And so don't think that it's like, I mean, in 99.99%
00:44:10.020 - 00:45:06.674, Speaker B: of cases it is true that there are people with malicious intent who are studying people's implementations hoping to profit from their mistakes. I'm not naive, but it doesn't have to be. Sometimes people just make a bad smart contract and it gets exploited accidentally by people using it for its use case. And if nothing else, that was the purpose of this exercise, right? It's not just that it's easier to do and that you are, you know, that I'm trying to get you to take shortcuts. It's that if it's easy, it's probably easier for a good reason. And you should not be, you know, trying to do difficult things in the products that you're building. And at the same time, you need to be mindful of the, you know, the precision and accuracy of the calculations that you're, that you're doing.
00:45:06.674 - 00:46:09.020, Speaker B: And so if you're, if your smart contract is very, very reliant on right, if it's built into the bones of the architecture, that there's a certain number that needs to be, you know, you know, that is the waypoint for a lot of other app logic, then you need to be 100% confident in that number. And if you're not, because you've calculated it, then maybe you need to reflect on whether or not you have parameterized your curve correctly. And that is the end of that tutorial. And I think also sort of rounds out the discussion a little bit surrounding why these different curve parameterizations are not good or better than each other. But in a very specific situation, one can be easier and in a way that makes it more secure. Okay. Um, that concludes the uniswap v three discussion.
00:46:09.020 - 00:46:44.020, Speaker B: Um, I still recommend, if it's some of the things that I've said, um, about these things, or if you didn't actually manage to complete the plot or complete that exercise, I do want you to reach out. I do think we should talk about it. If you're interested in token engineering, you may be. You know, you're. You're part of a small pond, and I would prefer that you have good knowledge and are able to disseminate these ideas in a thoughtful way. So don't talk to me. I am making myself available for this stuff.
00:46:44.020 - 00:47:17.706, Speaker B: I do take it seriously. We're going to move on to the third and final unnatural parameterization of concentrated liquidity. And this one's very close to my heart, because it's a parameterization that I worked on very hard when we were developing Bancor's new product, carbon defi. And while we're there, I get asked this a lot. Carbon is a product name. It's not a company name. So it's an anagram of Bancorp.
00:47:17.706 - 00:48:05.334, Speaker B: And that rearrangement, that jumbling up of the letters, is supposed to be sort of a symbolic retrospect on how the project has changed. Because everything that we discussed up until this slide has been specifically amms. In a way, this is Bancor's legacy. It developed the Bancor formula way, way back in 2017. It started with the concentrated liquidity stuff. In 2020, obviously, there are more popular, more successful projects now that have adapted that technology, but it's still an amm, right? Uniswap V three is an amm. In Trader Joe's thing and balancer.
00:48:05.334 - 00:49:20.034, Speaker B: These are all amms. Carbon D five is not an amm. And that's why this, in my opinion, the rearrangement of the letters is important, because it's meant to symbolize a shape shift into something new. And in a way that maybe that will feel confusing to you, because we're still going to be talking about concentrated liquidity. And it's like, how could all of these products that we've been discussing in the lectures up until now, how can they all refer to or make use of the same mathematical objects, that some of them be categorized as amms, while others are not amms. And I'm hoping that by the end of this, uh, of this lecture, you will be able to begin to start thinking about just how flexible it is or how underutilized or under explored, um, some of this, you know, um, some of this engineering space really is, and, like, how, how much tunnel vision, you know, the, the industry has at times. Okay, so, yeah, the carbon defi equation, it was developed in October of 2022, shortly after I took leadership of the project.
00:49:20.034 - 00:50:15.284, Speaker B: And you'll note straight away that the way that I generally present it, which is this form here in white, it has a very different sort of phenotype to the other two. Just by visual inspection of the Bancorp v two or uniscorp v three formulas, you can kind of, you can see the invariant. You know, it's like, okay, the right hand side, it's l squared, or it's a squared times x zero times y zero. You can see the asymptotes. And, you know, there's a way that's presented in these two equations that's, like, very intuitive. Whereas the carbon d five equation, the way that I usually present it, it doesn't, it's almost like it doesn't care about those characteristics, or it's not advertising those characteristics, or that it's trying to advertise a different set of characteristics. And maybe all of that is true.
00:50:15.284 - 00:51:17.234, Speaker B: We can first, let's derive the equation, and then we can start discussing how, you know, how you present the formula might reveal certain characteristics about it that aren't necessarily obvious. So one of the ways, obviously, there's an infinite number of ways this can be derived, but I think the fastest way to do it in the context of our lecture material is to actually just start from the uniswap b three formula. And remember, this was a natural continuation from the Bancorp two one. So after you've already got the price, the price constants inside the invariant, what I needed to do with the carbon invariant was to introduce the y intercept as a hard coded variable. And I'll explain why later in this lecture. So, in order to do that, we can derive the, the expression for the y intercept in terms of p high and p low and l. And this is something that I've done for you in the textbook.
00:51:17.234 - 00:52:44.958, Speaker B: And once you have the y intercept defined, that way you can actually just rearrange it to get the l term isolated or make l the subject. And once we've done that, we can actually just do a straight substitution. We can actually just push l into the invariant where it is, and also push it out of the invariant where it lies and substitute it for its definition in terms of the price boundaries in the y intercept. Now, that gives you a pretty messy looking equation, and you'll see that there are ways to clean this up, especially when you have an appreciation for the fact that you have some flexibility as to what is computed off chain and then stored in the contract versus what the contract has to calculate every day. So, cleaning this up just a little bit, you can see that it still has that presentation of, you know, you can actually see the, you know, the asymptotes or the horizontal and vertical shift components and the invariant expression on the right, but it's still like, it's cumbersome. And so what we want to do is we want to find a way to very concisely express these same ideas. Because the square root of p high and the square root of p low and these kinds of things, that's just a symbol at the end of the day.
00:52:44.958 - 00:53:24.984, Speaker B: Sure, it's an intuitive one, but if you need to combine variables to create other variables, that's totally reasonable. And remember, this has already been done. I've already demonstrated this with the uniswap b three invariant, where we showed that the l constant, or the liquidity number, is actually just a times the square root of x zero times the square root of y zero. So we're doing something very similar here. We're just combining some of these other constants and giving them new names so that we can present them a little bit easier. And we're going to do the same thing with the Y intercept. And this is so that we can get everything in the formula to use single letter variables.
00:53:24.984 - 00:54:05.984, Speaker B: And it kind of also just looks nice to have, like, a and b and, you know, x, y, z. Okay, so after making that substitution, our formula boils down into this, which, again, has the same form as the invariants that we've been studying in the first three lectures. But these are actually the same as each other. And if you want to, it is, you know, an interesting exercise to actually rearrange that equation in purple into the form that it's presented in white. But you don't have to. I don't think that's necessary. Just know that is the derivation where carbon comes from.
00:54:05.984 - 00:55:08.524, Speaker B: Okay, so after you've got these variables out, you can. It's a relatively simple process to define all of the points on all of these parts of the curve, but the virtual curve and the real curve, as we have previously, just by substituting our new identities for old ones or, sorry, substituting old identities for new ones. And, you know, there's nothing like magical or like, you know, it doesn't require an enormous amount of, like, of intellect to do that. It's a very grimy, sort of a very grindy, the sort of thing to spend your time doing. And I guess I'm a little bit self conscious of that in the writing of the textbook. I acknowledge that there's a lot of redundancy there, but I think it's necessary redundancy. But the point is that once you've completed that process, you actually do end up with a map of all exactly the same things that you already knew about, just with a different name.
00:55:08.524 - 00:55:45.524, Speaker B: Now. So instead of having the square root of p high and square root of p low, instead of having a and x zero and y zero and so on, we now have everything expressed in terms of z and lowercase a and lowercase b. Now, from that invariant that we, that we just derived, you can get out exactly the same, like marginal price equations. That should be a negative, by the way. I'm just realizing I left that up. But you can get the marginal price equations as before. You can get the swap equations as you have previously, and so on.
00:55:45.524 - 00:56:43.286, Speaker B: And one of the things I want you to pay attention to here is that the marginal price equation looks very, very different to how it has in all of the other examples that we've looked at. And anecdotally, I can tell you that I actually started at the marginal price equation and derived the, the derived the invariant form of the equation during carbon's development. The derivation that we just did, there was more like a proof. The actual development process actually started with a marginal rate equation. And have a look at it right now. You know that, that z term is the y intercept. And so what we're really doing is like, we've got two terms, a and b, where a is some width, right? Some difference in the square roots between two price bounds, and b is just the lowest price bound.
00:56:43.286 - 00:57:40.114, Speaker B: And so what we're saying is that your y balance as a proportion of the maximum y balance is how far through the price range you are, right. It's almost like a progress bar, like a loading bar in, you know, in, you know, when things are downloading or something, it actually measures the progression through the price range as a fraction of some, you know, of some coordinate versus its maximum coordinate versus its intercept. And that is a requirement. I needed this equation to do that so that I could make carbon perform its functions and. And split away from the amm paradigm. You know, you can also get the other trade equations. And again, I apologize.
00:57:40.114 - 00:58:19.798, Speaker B: It's made such a big deal out of leaving negative signs in, and now I realize I've left them out of this slide. But the used textbook is gospel truth. I tend to make these slides in a hurry, and sometimes I make mistakes. So I apologize for that, to revise, because I know that some of this information is now lacking or is no longer on the slide. But just know that the lowercase a and the lowercase b, it is familiar. I know that these equations now look very different, but they shouldn't, because it's just a rearrangement of the stuff that you just spent the last week working on. It is p high and p low.
00:58:19.798 - 00:58:58.800, Speaker B: There is no difference between the information, just the way that it's organized and presented. Remember, the z term is just the y intercept. You already know what that is. It's when you draw the real curve. It is the point where that curve touches the y axis. It's not special or mysterious or anything. Now, what is different about carbon? And this is going to be what we're going to be focusing on for the remainder of this lecture, and this is the thing that splits it out of the amm paradigm in a very profound way, is that you don't know what the y axis and the x axis are yet.
00:58:58.800 - 00:59:37.664, Speaker B: I told you what one of them is, which is the y axis, refers to a token balance, but the x axis doesn't. In fact, the x axis doesn't really refer to anything. We don't even store that coordinate in the smart contracts. We never refer to it, and there isn't a single function that needs it. And based on what you know and what we've examined so far, that might seem like an insane thing to say after all of this work. We've always been comparing two different token balances and using that to determine a price. But that was a design choice of the Bancor founders back in 2017.
00:59:37.664 - 01:00:09.996, Speaker B: You don't need it. All you need is a way to determine a price. And instead of referring to two token balances, the equation that I just showed you only refers to one. This is how it works, and this is how it's able to perform exchange. Again, with reference to the Bancor V two formula, we have the system that you're familiar with. Number of tokens on both the x and the y axis. By convention.
01:00:09.996 - 01:01:41.844, Speaker B: In carbon defi, the only axis that refers to a token balance is the y axis, and the x axis refers to, if you like, a token delta. But really it's just, I still like to think of the x axis in carbon Defi's case is just the extra dimension that you get from performing an integration over the price curve, which is why there are so many, I do go to so much effort to show you how to integrate the price curves in the textbook, because the price curve is really the one that carbon D five uses. Even though it's swap equations and things all look like they refer to the normal bonding curves, they really don't. Okay, so there's consequences here because both of these bonding curves are still linked together, right? The, the tokens that are coming, you know, it's still a smart contract that performs exchange, which means someone's going to send tokens into that smart contract and the smart contract is going to send tokens back out. And there needs to be some logic there that determines what the tokens that were sent to the smart contract are worth and then return the amount that the trader was expecting to get. So this is how I want you to visualize it. The tokens that are sent into this, what we call a strategy, but let's just call it a smart contract, they're sent to the other curve.
01:01:41.844 - 01:02:51.004, Speaker B: Okay, so this Green arrow, this is like the traders Delta X, like the amount of, you know, the tokens that some consumer is selling to the smart contract, but they don't go into this bonding curve, they go into the other one, which also means that it has to invert its dimension. So if, because of the convention that we're using, that both token balances are always displayed on the y axis, then this means that necessarily, if there's a delta x happening on one of these curves, that means that there is a corresponding delta y happening on the other curve and in the opposite direction. So if this is a, sorry, in the same direction. So if this is a plus dx, which it is, then this corresponds to a plus dy on the other curve. What about the negative delta y? This is the tokens that are being sent out of this curve. Remember, we display the token balance by convention on the y axis. So this delta is literally coming from this token balance.
01:02:51.004 - 01:03:41.334, Speaker B: Now that will still translate to, or there will be a corresponding shift left, because this is still a bonding curve that you have to be on the curve somewhere. But this left facing arrow is a completely different number, right. It has no correspondence to the size of the down pointing arrow on the curve from which the tokens are being removed. So yeah, this green arrow, same magnitude. If this is 100 tokens in, those hundred tokens are being sent to this curve on the right, and this is where they're going. But if this is, let's say, 200 tokens out, I don't know, that looks about twice as long as the green arrow. If this is 200 tokens out, you have absolutely no idea how wide, how long the corresponding red arrow is on the other curve.
01:03:41.334 - 01:04:33.386, Speaker B: And once you realize that, you can completely do away with that as a necessity. Once you realize that there doesn't need to be a correspondence, you realize there's a lot more flexibility in how these pricing algorithms work. That actually unlocks a lot more user agency than people in DeFi have been offered in recent years. Now, because of that lack of correspondence, these things can look really weird. A very small green arrow over here, let's call it 100 tokens, could still mean that this is still necessarily 100 tokens on the green arrow in the right place. But proportional to the size of its curve, might be very different. Right, 100 tokens on the left might be a very small number, whereas 100 tokens on the right might be a very large number.
01:04:33.386 - 01:05:27.480, Speaker B: So proportionally they might look very different. You can also end up with this situation where the shift, what do I call it, the traversal, upon this bonding curve, now moves it into an out of bounds condition. So that edge of that arrow there, which would correspond to this traversal, it is literally going out of bounds. And there's no way for the spark contract to detect it because we do not store the x coordinate anywhere. It's only looking at y coordinates. And so the, you know, from its perspective, the y coordinate is still positive. Nothing weird has happened.
01:05:27.480 - 01:06:28.644, Speaker B: That if you were to then extrapolate the bonding curve, you would note that the contract is technically reporting a negative token balance. Now there's nothing or not a negative token balance, but a negative x coordinate. Now the reason why that's not necessarily a problem is because like I said before, the x coordinate doesn't refer to its token balance anyway. So it's perfectly reasonable to let it go negative if we wanted to, but we don't want it to. And I want you to think for a minute, because I'm going to ask a question. What does it mean if we let the X coordinates go to go negative? And I'm telling you that the smart contract doesn't even notice because it doesn't even store that coordinate? What does it fundamentally mean for the way that that curve would behave? And I'm hoping, I'm not expecting to get an answer, but I'm hoping to get one.
01:06:35.884 - 01:06:42.704, Speaker A: Any thoughts? Drop it in the chat or switch on your mic, raise your hand.
01:06:43.764 - 01:07:12.444, Speaker B: Is Thiago with us? I'm gonna call Tiago. Yeah, I was thinking a little bit. I mean, it doesn't fill my mind that there could be extra amount of tokens going beyond the bounds. I'm not really sure. Yeah, so there are no extra tokens. We're just filling it up with more tokens. On the y axis, we can go as high as we want.
01:07:12.444 - 01:08:14.584, Speaker B: But what does it mean if we're not exploring lower and lower x balances? Because there is no x balance, what are we exploring? What is the consequence of heading further and further up that curve? What else changes with the coordinates? The invariant as well? Not the invariant. This is actually the same invariant. Let me put it this way. We're on a. Let's say that we're in a normal constant product curve where, you know, x zero and y zero are both three and four, right? So k equals twelve. What can you say about the point x equals one, y equals twelve versus the point x twelve. Y equals one.
01:08:14.584 - 01:09:12.134, Speaker B: It's the same. The same curve. Is it? What if someone's trying to sell tokens into it? Where do they get a better rate at the lower price when it's y below x? This is the important component here. The price is still changing. So even though these x coordinates can go out of bounds, and that's not a problem, the thing that is a problem here is that the price is also going out of bounds, and it's not going out of bounds in any significant way. It just means it's going out of bounds with respect to the intercept or where it was supposed to be, because that's where the user wanted their price range to stop. If we allow this out of bounds condition to continue, it means that the user quoted a price and then is getting a different price.
01:09:12.134 - 01:10:08.216, Speaker B: So what to do about it? This is basically that situation. You could imagine this blue curve sort of continuing up, and this is pointing to that point over there. And so if we were to go to this y coordinate, that would mean that we're now quoting a price over here at this slope when the user wanted their highest price to be this one. And this is why we parameterized this curve with the intercept in it. Because if we move the intercept up to match that new Y coordinate, you will find that the slope of the curve at that point does not change. Right. The curve just got larger, but it didn't warp.
01:10:08.216 - 01:10:55.084, Speaker B: Right. It has not transformed. It's still similar to, it still has the same proportions to where it started. So both the p high and the p low, and, in fact, every other coordinate, every other matching coordinate proportionally will still have the same first derivative value. And that means that we can very accurately fine tune the liquidity that it has available or report the liquidity that it has available with respective to some maximum in order to maintain the price bounds that the user had decided on. Okay. And this is the important point, right? And this is why the parameterization in carbon is so, is so important.
01:10:55.084 - 01:11:43.812, Speaker B: If the user says, these are the price bounds that I. That I want, and the protocol then goes outside those price bounds, we've got a problem. We could try to maintain those price bounds even though we've got two completely different bonding curves talking to each other by using either the Bancor V two invariant with a and x zero and y zero. Or we could try and do it with the uniswap invariant, having to calculate l on the fly and that kind of stuff. But it's hard, right? And you don't want to do a hard thing. And also, you don't want to calculate anything. You don't want the smart contract with something as sensitive as price quoting to have any opportunity for error.
01:11:43.812 - 01:12:28.144, Speaker B: Right? You need to get the, you need to get the margin for error down to zero. And that was the take home message from the tutorial from the Uniswap v three. Um, you know, homework assignment that we did is that, it's not that, you know, it's not that there's minimal error there. The error is zero because it is hard coded what the, what the price bands are, and you can make the edges equal to each other in order to get the error down to zero. In a. In a system like carbon D five, where we're no longer an amm and we don't have, you know, and these price boundaries are arbitrary, right? They're set by users. And we've got all of these, you know, all of these edge cases and things to deal with.
01:12:28.144 - 01:13:08.302, Speaker B: We needed to find a very, a very clever, very direct way, right? A very simple way to make sure that the prices update correctly and that they always update without a calculation being performed. And there's nothing safer than this computation. There's nothing being calculated here. There is no precision loss when you're just comparing two numbers in memory. We can just look at the y coordinate and compare it to the y intercept. There's no division, no subtraction. Nothing's being multiplied.
01:13:08.302 - 01:13:37.468, Speaker B: There's no overflow or underflow. It's just two numbers. And the program can look at them. And if one of them is higher than it should be, then we make the other one higher to match. And that is how we maintain these price boundaries. Now, this means that there are, you know, that the contract does have to update this parameter every once in a while, but every time it updates it, it updates it in a way where no computation was ever performed. Right.
01:13:37.468 - 01:14:24.074, Speaker B: No measurement was made, no calculation was done. Comparing true numbers and asking which one's higher doesn't qualify. And this gives you the same precision as hard coding values. Okay, so I said that there was a reason why we did this and that we were trying to unlock different behaviors. And I've given, like, I borrowed these slides from other presentations that I've done, or at least the slides I'm about to show you. And I think a lot of the time, people sort of quasi understand what I'm talking about. You know, some people who have, like, a lot of experience in industry, you know, get it, but, like, a lot of people don't.
01:14:24.074 - 01:15:22.732, Speaker B: And I think it's, it's cool now because you're probably the first, like, captive audience that I've had that we get to sort of, you know, build up to, you know, what the significance and, like, how, how different it is to be operating without those kinds of amm assumptions and what it can let you do. So what this image is supposed to demonstrate is basically where liquidity is distributed. So in a constant product case, and we're going to be referring to this as Uniswap V two, because I think that product has definitely sort of become known for that specific deployment. But that is enumerating prices from zero to infinity. Right. This is something that we, we've been over, I think, at length, whereas with concentrated liquidity, you're only offering liquidity between certain price bounds. And we haven't represented it like this in the lecture yet, but I think we all know what a price chart looks like, and we all know what p high and p low means now.
01:15:22.732 - 01:16:27.506, Speaker B: And so just appreciate that you cannot just draw that on a chart as like a parallel channel, as a horizontal parallel channel. So this is what confines liquidity. And you're going to be trading at any of the prices within there. The reason why I was motivated to reprimanitize this equation and develop something like carbon was so that you could do something like this, determine where you want to buy and where you want to sell separately, independently of each other, even things like just pure limit orders or whatever, or creating these kinds of fading or scaling strategies. These are things that did not exist in defi, at least in any significant capacity, and even, to an extent, are kind of absent from centralized exchanges. Some of these products still don't exist anywhere except on carbon. Okay, so these animations are how I've is what I think usually makes people's eyes glaze over.
01:16:27.506 - 01:17:04.104, Speaker B: But given the speciality of our students here, it may have more meaning for you now that we're coming to the end of lecture four. So what I'm doing here is just printing. This is just a sinusoidal curve. This isn't actually any price chart. I just made this up. But what I want you to look at is the mechanic. You can see that in the bottom two plots, I'm tracking the position that we are on the curve with respect to the constant product x times y equals x zero times y zero case.
01:17:04.104 - 01:17:49.340, Speaker B: And I've reproduced the plot on the left, in the plot on the right, but then taken the transpose in the plot on the left, we've got the quote token on the y axis and the base token on the x axis, and then the opposite for the right curve, the base balance on the Y and the. And the quote balance on the x. And you'll. And what I want you to see here is that there's a symmetry to it. Right? I is a symmetric operation that's happening as we move back and forth, as the market breathes in and out. We're just moving up and down on this curve. And in a way, this is very similar to the, the animation that I prepared for you guys and showed you at the beginning of lecture two.
01:17:49.340 - 01:18:38.460, Speaker B: But now it's got price and market context. And so if we were to generate a plot for what this thing looks like, the green shaded part is meant to represent our bidding prices, the quote token. And the red shaded area is the base token. Okay, what about concentrated liquidity? Well, you already know how this operates, right? Because you've been doing this now with me for a few weeks. With concentrated liquidity, you only have liquidity available between certain price bounds. And so when the market moves outside of that price bound, you basically stop offering prices that anyone's going to trade with you at. If the market comes back, fine.
01:18:38.460 - 01:19:47.484, Speaker B: And as its showing you right now, and if the market moves right through your range, then youll end up with all of the other token. But its the part where the market appetite is inside of your liquidity range that I want you to pay attention to because you can see, and if I pause it here, I think quite clearly that as the market is cutting through your liquidity, the bids, right, our cash token is being flipped into the base token and it appears pretty much immediately behind, right. It becomes, you know, it's almost like these two segments are supposed to fit together. And again, like this was a design decision in 2017. It's not a mandate, it is not a requirement, it's not even a feature in my opinion. It's just a design choice of the type of protocol that you're building. If you want to have continuous liquidity, then fine.
01:19:47.484 - 01:21:14.304, Speaker B: But ask yourself, if that's what people want, who is your protocol for? Because if it's for token projects, right, or if it's for people that like incentivize market makers and stuff like that, then sure. Like, they will probably want to do this because their product, their business is to provide pretty looking and, you know, pretty looking order books and you know, stable and stable marketplaces. But for people that are just normal people, people that are just like day trading or, you know, speculating on cryptocurrency and stuff, they probably don't think this way, you know, and if someone wasn't giving them like token incentives to interact with these protocols, then it's worth asking, you know, would they, would they have ever enjoyed working? You know, would they have ever enjoyed actually interacting with this protocol? So did they have a specific goal in their mind? Right. Is, was there an objective that they had that they realized a protocol designed this way was going to help them achieve? Because I stipulate that they don't, and that most people that are interacting with concentrated liquidity protocols don't really know the financial instrument that they're buying. Okay, so one last time, I want you to just pay attention to these bottom two plots as the price comes back through this region. And just note the symmetry here. Right? As the, oops.
01:21:14.304 - 01:22:14.532, Speaker B: As the, as one of these token balances wanes, the other one gains, okay? And they move like in sort of lockstep with each other. And it's because this is a, you know, this plot is a transpose, so it can't do anything else. And I know that seems obvious, and we haven't really discussed anything so surprising just yet, but ask yourself, what do you think is going to happen when we look at carbon defi here? We've actually separated the bidding liquidity and the selling liquidity into discrete regions. And we still allow the market to approach it from any direction. And so nothing's happening at the moment because the market price is between these two batteries. And here we'll start to notice our first big difference. So the market has expressed an appetite to trade with this liquidity provider at these price points.
01:22:14.532 - 01:23:15.874, Speaker B: And so you can see this green triangle is starting to contract, but there is no corresponding symmetric change in the selling liquidity in the asking liquidity. Instead, what you will notice is that the triangle is getting larger. If you pay attention to the y scale, you can see it growing there, which is exactly that mechanic that I showed you a few slides ago, where to stop the price going out of bounds. Instead, we let the curve get larger, and then when the market turns around, it comes back the other way. If this was a typical concentrated liquidity position, then this asking liquidity would be all the way back down here. That actually it now exists in a different curve at a different price point. So now that the market is turning around and coming back, cutting back through this liquidity, nothing happens.
01:23:15.874 - 01:24:03.604, Speaker B: And maybe that's what the user wants. Maybe the user only bought things down here because they're trying to sell it again up here. And if that's a reasonable attitude to have, then separating the tokens into their own separate bonding curves is apparently a very reasonable way to go about building a decentralized exchange. Again, I'm going to come back here so you can see that part of the animation. Just note that when it comes up here and just kisses the asking prices, it is going to start cutting in, and then watch what happens. We're going to lose some of our base asset. And as it's converted into cash, it's going to start filling up this triangle.
01:24:03.604 - 01:25:09.746, Speaker B: And notice that it fills up at a different rate, because now this curve is a different size to this curve. So it only takes a small movement here to get a large movement here. This gray rectangle, or this gray triangle, sorry, is supposed to be a similar size to this green triangle, but it's absolutely not. And as we get halfway down, we've now completely filled the old curve. And as we continue to move through these asking prices as it's converted into cash, it's just going to keep rescaling this other curve until it's a much larger curve. And so it's a different mechanic. Yes, but the reparametrization that we've used allows us to very efficiently and very securely allow for this kind of back and forth passing of tokens between two different bonding curves without ever going outside of the price mandate that the user has prescribed for the protocol, which is a very different way to think about decentralized exchange.
01:25:09.746 - 01:25:55.134, Speaker B: Usually it's the protocol that prescribes to the user how the thing is going to trade. And I think if you reflect on a lot of the work that we've done over the last few weeks, that that's a relatively fair statement to make sure you can decide which buckets you want to contribute liquidity to in a concentrated liquidity system. But you cannot decide that if you buy something at one price that you want to sell it at a completely different press. And in a way, the, you know, the. It's a very natural, like, economic principle, you know, throughout all of human history, to acquire something at a low valuation and then try and sell it again at a high valuation. That's kind of, you know, how finance and economics is born.
01:25:56.634 - 01:26:00.654, Speaker A: I hate to interrupt, I must say, but we only have three minutes left, Mark.
01:26:01.034 - 01:26:10.510, Speaker B: I know, I know. We're actually, we're on the very last slide. Yeah. So we're good. And. Yeah. And then, so this is the summary plot for this.
01:26:10.510 - 01:26:31.792, Speaker B: And again, I want to draw your attention to these grayed out regions. Right. So in a typical situation, we would force the height of this green liquidity and red liquidity bands to be equal, and we would jam them together. Right. Almost like cogs. Right. Or like, you know, like a jigsaw puzzle.
01:26:31.792 - 01:27:15.164, Speaker B: So that they kind of. So that as an asking price is being taken, there's a bidding price catching up to it in order to keep the market liquid. But we've taken a completely different approach here, and it's still concentrated liquidity. We're using a re parameterized version of the concentrated liquidity equation from 2020, or if you like, a reparametrized version of the uniswap e three concentrated liquidity equation. It's the same math, but the behavior of the protocol is so divergent. And that is kind of the perspective that I wanted to give you with this, you know, with this course. It's that, you know, don't be.
01:27:15.164 - 01:27:51.166, Speaker B: Don't let your creativity be, like, stifled by what people tell you is all that's possible, or that there's only certain examples of things being done a certain way. Here is an example of taking something that was years old and forcing it to do something that people didn't think was possible in a blockchain context. And so I do have homework for you again. And it's probably what you expected. I want you to choose any two hypothetical tokens like foo and bar. Whatever, Ethan, BTC. It doesn't matter.
01:27:51.166 - 01:28:15.834, Speaker B: Create separate concentrated liquidity bonding curve for each. Remember the convention, the token balance is always on the y. And then perform a swap by sending tokens into one of those curves and removing tokens from the other and plot the overall result. I think that's it. We're right on time. Right? That's exactly 90 minutes.
01:28:16.374 - 01:28:43.508, Speaker A: Yeah, we just are missing the exams information, but that's not a problem. So I would suggest that first of all, thank you so much for the session. Again, it was awesome to follow. We have some information that we'd like to share prior to our next session. We'll drop a note in the chat. This is how the exam works and is structured. Our idea how to make it work next week.
01:28:43.508 - 01:29:02.084, Speaker A: Feel free to make comments and otherwise in case you have questions on the homework. As usual, just reach out on the discord or directly to Mark and we'll meet again next week for the final session. Same time, same channel. Thank you so much, Mark.
01:29:03.504 - 01:29:07.896, Speaker B: Yeah, my pleasure. And I'll see you guys all in discord, I'm sure.
01:29:08.080 - 01:29:26.654, Speaker A: Right? And now we directly jump on the next session with Roderick McKinley. Today we have back to back program. That's why I swiftly stopped the recording. And we continue in a moment. Bear with us. Thanks, Mark.
01:29:27.274 - 01:29:28.714, Speaker B: The recording has stopped.
