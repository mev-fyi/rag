00:00:00.560 - 00:00:42.062, Speaker A: Welcome back to track number six at the study season Token Engineering Academy. Today this is our second session with Mohamed. Welcome again. We'll work on modeling and simulating decentralized economic systems with the application of AI and machine learning. Some more words in case you are following us, you are following this session via Twitter or YouTube. This is part of the study season 2024 program at Token Engineering Academy. It's a cohort based program open for anyone interested in this field.
00:00:42.062 - 00:01:23.340, Speaker A: You can still register via tokenengineering.net register and you'll get access to all the materials and the recordings of the session. One more note, TE academy will be at EFCC with a dedicated token engineering track. A full day of token engineering talks and you can be there. Actually, it's hard to get ECC tickets. They are sold out very quickly and we are giving away five tickets for referrals of our newsletter. Last edition went out yesterday.
00:01:23.340 - 00:02:09.894, Speaker A: We are covering a new variant of the token flywheel. If you have been studying token engineering fundamentals, you know the web free sustainability loop and we published an update with the latest insights how to drive web free economies in a sustainable way. And if you subscribe, or if three of your friends subscribe, you take part in the lottery and can win an EFCC ticket worth 500 euro as far as I know. Okay, that's it for now. Happy to drop you the link for the referral program and I think. Mohammad, we are good to go. Let's start the session.
00:02:09.894 - 00:02:12.154, Speaker A: Thanks for joining us today again.
00:02:13.974 - 00:02:28.472, Speaker B: Thank you Angela. Let me just share my screen. Please do. Let me know when you guys can see that. Just give me a moment. Is it visible?
00:02:28.608 - 00:02:30.244, Speaker A: Yeah, we can see it. Perfect.
00:02:36.704 - 00:04:28.814, Speaker B: Okay guys, so welcome to the second session of our track on modeling and simulation decentralized economies with the application of AI and ML. So today we will be diving into the world of applications of AI and ML in token engineering. And in our previous session we explored that the foundations of foundational concepts of decentralized economic systems and the importance of modeling and simulations, and how AI and ML can address challenges of this domain of modeling and simulation in decentralized system. So in this session we will build up on that knowledge and focus specifically on application of ML that can be applied to optimize various aspects of token engineering. So we will be looking at a practical example of ILP framework yesterday to build an understanding of methodology of the way we can approach this modeling domain. So first of all, I will give brief recap of our topics we discussed on previous session. So in previous session, we discussed the difference between centralized and decentralized economic systems and established that this centralized economic system, they have single governing authority which controls and regulates different aspects of the economy, and while decentralized system distribute this control across a network of participants and providing greater transparency and security.
00:04:28.814 - 00:06:20.184, Speaker B: So from this comparison, we established that the dynamics of decentralized systems are very different from their centralized counterparts. So we therefore the mechanism to design and simulate these systems are very different from their centralized counterparts. So we also discussed how existing use cases of blockchain are being offer, how traditional use cases of web two are being transformed to web3 domain, and there are many new use cases which were being built on this technology which were not possible on our previous infrastructure. So the challenges of making these system robust and sustainable, they are very high and we have to find ways to optimize them. So in order to make the system robust and sustainable, modeling and simulations are very important in this domain as complexity of interaction within these systems is of much higher order and due to lack of centralized control these systems, we cannot rely on traditional analytical methods to optimize and verify these systems. So we emphasized the importance of modeling and simulation in this systems. So, modeling will help us to understand complex economic interactions and predict outcomes, while simulations allow us to test these models in various scenarios to ensure their robustness and reliability.
00:06:20.184 - 00:07:44.184, Speaker B: So we discussed different modeling and simulation techniques like agent based modeling and system dynamics, and which are used to analyze the emergent behaviors of the system system and its components. After that, we explored the token engineering process foundational tools which are used in this domain like CAD, CAT, token, Spice, RedCAD, UTM, and we also identified several challenges faced by token engineers using these tools related to the complexity and different nature of the system and managing dynamic behaviors, ensuring the security and scalability, defining mathematical definition, building user interface and incorporating live data in these models. So we at the end of session established that we can leverage AI and ML in order to deal with these challenges and make these challenges, address these challenges using AI and ML. So can you see my next slide, guys? I just need to check here.
00:07:44.564 - 00:07:47.304, Speaker A: We still see the session two title slide.
00:07:48.044 - 00:07:54.704, Speaker B: Okay. Okay. Okay. So let me just get here. Yeah.
00:07:57.304 - 00:07:58.364, Speaker A: Now it's fine.
00:07:59.144 - 00:08:57.410, Speaker B: Okay, just let me. Okay. So now once we, in previous session, we discussed all these challenges and at the end we established that we can leverage AI and machine learning to address these challenges. And these are the objectives of this session. So our primary objective is to explore different approaches for leveraging AI and machine learning in modeling and simulations. And to achieve this, we will delve deeper into the case study of intelligent liquidity provisioning framework. So we will begin with discussing AI ML applications in token engineering.
00:08:57.410 - 00:10:32.938, Speaker B: This will give us a broad understanding of how these technologies can enhance different aspects of decentralized economic systems, and then we will explore how to integrate AI and machine learning into the simulation and modeling process. So this section will focus on practical methods and tools to implement these technologies effectively. So following this, we will provide an overview of Uniswap V three and reinforcement learning to set the context of our discussion and understanding the foundational concept of Uniswap V three and reinforcement learning to define the complexities of the systems and how we can leverage AI in Uniswap mod Uniswap v three modeling and optimization. So after that we will have a brief overview of our framework. Then we will discuss different components and by the end of this session we will have a comprehensive understanding of how to apply machine learning and AI token engineering projects and different type of protocol and simulate decentralized economies effectively. So let's get started with the applications of AI and machine learning in token engineering. So these are a few algorithms which I will give a overview from the higher level and then we will apply them on the intelligent liquidity provisioning framework.
00:10:32.938 - 00:11:48.974, Speaker B: So, various applications of AI and ML in token learning domain which we can explore are following. So first of all, clustering and classification techniques to understand user behaviors so they include both supervised and unsupervised models. We can use these clustering and classification techniques to understand user behaviors by analyzing patterns and segmenting user based on their interactions and preferences. So we can tailor economic models to better meet user needs and to user experience by doing this, reinforcement learning is. It's a very powerful tool, traditionally being used in gaming and robotics. So that is something which can be really used to model dynamic behaviors and emergent dynamics of the system. These RL agents operate on certain policies which can be translated into the mathematical specifications that will that describe the behavior of participants and they enable more precise and effective economic strategies.
00:11:48.974 - 00:13:22.244, Speaker B: Reinforcement learning can also be used in agent based models by creating adaptive agents. So we will see that practical implementation of this in our framework. So these agents can learn and evolve based on their interactions within the system, leading to more realistic and dynamic simulations. Generative AI we know generative AI from the reference of nlms, but Gans and variational encoders, they are very powerful to model and simulate system because they we can use them to generate synthetic data for our simulations. So this synthetic data can be used for scenario analysis stress testing and parameter optimization by providing data sets for simulation without relying on any real time data, especially in the design phase where we don't have any real time data. So we can use these models to generate this data and then use that in our simulation symbol symbolic regression is another powerful tool that allows us to discover mathematical relationships within data without any predefined models. So the challenge which we discussed related to the definition of mathematical specifications of different mechanisms, we can use symbolic regression to define these mathematical specification without, with the help of AI and ML, without defining any concrete functions and definitions.
00:13:22.244 - 00:14:47.334, Speaker B: So another thing which is really, which can be really helpful in token engineering flows is integrating LLMs and RL that can generate policy frameworks for agents. So this combination leverages the strength of both technologies to create well rounded and effective policies. So we can also use lms, that is the use case which is specific to token GPT, and we will be discussing that too in this session if we have time or in the next session. So, building user friendly interfaces for simulations for users to interact with these models and simulations through natural language inputs, so they don't have to run these simulations or have a lot of knowledge of code, what is going on. So we can just give them an interface where they can tweak the parameters and perform different kind of action with the text based interface. So with these applications, with these some of the applications, AI and ML can provide comprehensive toolkit for advanced token engineering. And let's now see that how we can integrate these agents.
00:14:47.334 - 00:16:11.124, Speaker B: So this is a general outline of using AI and ML for in modeling and simulation. So the first step is data collection and pre processing, which we use in every model and simulation. So we can use AI and ML to categorize and classify data and filter and transform it into a suitable format by using different techniques like clustering, unsupervised learning, KNn k, mean clustering, and other algorithms, we can use this to extract and transform this data. This can also include predictive models for the market behavior, clustering models for user segmentation, and generative modules for scenario creation. For example, we can use AI and ML to detect outliers, missing values and anomalies in all data to apply appropriate model or simulation technique. So there is another problem which is related to the modeling and simulation is choosing an appropriate model for simulation. So here, AI and ML can help us to select and apply appropriate methods for simulations.
00:16:11.124 - 00:17:18.836, Speaker B: For example, we can use AI to perform data driven simulations and modeling which use historical and current data and give us more realistic scenarios and outcomes. Parameter tuning and optimization. This is another aspect of modeling and simulation. And this in here, AI NML can help us to automate process by using techniques such as grid search, random search and Bayesian optimization framework or evolutionary algorithms. So these techniques can help, can help us to find the optimal values of parameters and maximize our, minimize our desired metrics, so such as accuracy, efficiency or profitability, whatever the use case of our system is. So after that, validation and verification. This is the fourth step in modeling and simulation any system.
00:17:18.836 - 00:18:32.730, Speaker B: So here we can leverage AI to evaluate the quality and reliability of our model by using methods such as cross validation, sensitivity analysis and uncertainty quantification. Error analysis also can be used with the help of these AI algorithms to validate and verify our system. So these methods can help us to measure the performance, robustness and confidence of our simulation and modeling outputs. This is one of the challenge which we discussed that you don't, you don't find out that what is the correct model, is it correct or not? And how complex it should be. So here we can test these models using these frameworks to validate and verify models so that we know that how much complexity is enough to model a system. Visualization and interpretation the use case which we discussed about the token DPT that can help us in visualizing and interpreting the results of the simulation. So that is pretty much straightforward.
00:18:32.730 - 00:19:49.490, Speaker B: We can leverage LLMs to interpret the results of a simulation and then convert them into the visualizations and of course the feedback and improvement. We can use AI models to monitor and update our simulations by using methods such as online learning, reinforcement learning and active learning. So by these methods can help you to learn from the new data and feedback based on the feedback and actions of the users. So these are the different high level methods which we can use throughout the lifecycle of a model through a modeling and simulation exercise. So we, once we will go into this framework. Before doing that, we will keep a higher level understanding of Uniswap e three and its significance in Defi. So all those who are attending Mark Richard Richardson's sessions, he has quite comprehensively explained the dynamics of Uniswap V three and amms.
00:19:49.490 - 00:20:35.890, Speaker B: So I will just give a brief overview of Uniswap here. Automated market makers. They are decentralized exchanges which allow users to trade cryptocurrencies directly from their wallets without the need of any centralized intermediary. So unlike traditional exchanges that use order books, AMM use algorithms to set prices and facilitate trades directly from liquidity pools. These allow for decentralized permissionless trading providing greater access and efficiency in the market. So concentrated liquidity. Uniswap introduced this concept of concentrated liquidity in Uniswap v.
00:20:35.890 - 00:21:30.124, Speaker B: Three, where liquidity providers can allocate liquidity within the specific price ranges rather than across the entire price spectrum. So this allows for more efficient use of capital and higher potential returns. One of the drawback of this concentrated liquidity is that we have to actively manage our positions. So that needs more active participation of liquidity providers and to achieve the capital efficiency which this Uniswap promises. So with this active management, LP's have more control over their capital and they can manage their risk by optimizing their liquidity provisioning strategy. And they can. This can.
00:21:30.124 - 00:22:32.532, Speaker B: This can increase the depth of liquidity where assets are currently being traded by reducing slippage. So this is the one of the pro of using these consolidated liquidity positions. So in uniswap v three, LP's provide liquidity across a range. They have to select a price lower and price upper in order to concentrate their liquidity. And based on that, they earn fees when assets are traded within that range, and as soon as assets current price leaves that range, so they don't earn any fees. And apart from that, their assets is their assets are completely converted into the other asset, which results in impermanent loss or divergence loss, which is inherent risk of this uniswap v. Three.
00:22:32.532 - 00:23:50.118, Speaker B: So our framework is basically built based on reinforcement learning, which we will be using to optimize this liquidity provisioning strategies. So, to give you a basic overview of reinforcement learning, it's a type of machine learning where agent learns to make decisions by performing actions and receiving rewards or penalties based on the outcomes of these actions. So it's an iterative process where agent continuously improves the decision making by learning from the feedback it receives from the environment. So it has four basic components. There is an RL agent, which is a learner or decision maker. In the RL process, the agent's goal is to learn the optimal policy, which dictates the best actions to take in various situations. Environment is everything that the agent interacts with, and the environment provides the state, which we call the state space or observation space, which includes all the possible situations that an agent can encounter and action.
00:23:50.118 - 00:24:54.644, Speaker B: They are all set of possible actions that agent can take. It's defined by the action space of agent, and these action impact the environment and lead to different outcomes. So these actions change the state of the system by doing, based on what the policy RL agent is performing, rewards, reward function is feedback from the environment that indicates that how good or bad an action is. So reward guides the agent learning process by reinforcing desirable actions and discouraging undesirable ones. In reinforcement learning, agent repeatedly makes decision, observes the its outcome and terms in terms of rewards or penalties, and adjusts its action accordingly. So this process will iterate for a number of steps which we define in our traditional training process. So until its strategy is defined and it has maximized its, its cumulative rewards.
00:24:54.644 - 00:26:21.590, Speaker B: Typical applications of RL are in gaming and robotics. So it's been used for chess playing, chess and other environments which can be simulated in the computer, because applying these in real world, there are a lot of samples which an agent requires in order to learn, and it's not economically feasible to give all these interactions in the real world. So therefore simulated environment are used and gaming applications are the best simulated environments which can agent interact with for number of steps it wants to. Some of the benefits of using reinforcement learning include RL is well suited for making complex sequential decisions, the problems which we define by the MDB Markov decision process, so where each action can affect future outcomes. And RL agents have this quality of adapting to new changing environments, so making them robust and dynamic in uncertain scenarios. So we will be in this case study, there are two objectives which we will try to achieve. The first one is to build an approach to model and simulate these systems.
00:26:21.590 - 00:27:42.190, Speaker B: So to develop another understanding to how to model or simulate any system with the applications of AI and ML, and learn skills to abstract these concepts, concepts of economic interaction within the DeFi and any blockchain system. So the second, the second goal, which is the secondary goal of this session, which is basically to learn how to optimize liquidity provisioning in Uniswap v. Three. So we don't expect anyone to understand that, how liquidity provisioning in uniswap is optimized, but to learn the methodology or framework, how it's done and how it can be applied to other environment, that is the primary objective which we will be exploring here, because these models and simulations can be adapted to other environments without much amendment. So we can take them to solve a very totally different use case with these minor adjustments. So this will be the primary objective of this presentation. So let's start with the intelligent liquidity proving framework.
00:27:42.190 - 00:29:00.414, Speaker B: What is it and how it works. So this framework is designed to optimize deputy provision strategies in Uniswap v. Three, and by leveraging agent based models, reinforcement learning and Uniswap v three simulator so these are three major components of this framework. And it optimizes this liquidity provisioning by maximizing the utility function which we define as the reward function which is fees earned by a liquidity provider during its position in the pool minus impermanent loss. So that will be the reward function which will be, we will be trying to maximize in this framework. This ILP framework has some, has some assumption based on which it was developed. So the first assumption which we used is that the samples which are given to this RL agent are realistic enough that this agent can learn from this environment, then apply these strategies on a actual uniswap environment.
00:29:00.414 - 00:30:17.126, Speaker B: And the second assumption was that all the actors which were model of Uniswap, they are rational and they perform these action based on defined policies. So like if you have a arbitrage agent in your city, so these agents are trying to maximize profits and they are, they are trying to maximize their profit and they don't have any other intention which we, which are not covered in this framework. So next see the different components of the system. So we have Uniswap v three simulator which is basically an interface to Uniswap v three core contracts. So this provides a detailed and functional simulation of Uniswap V three protocol and it replicates the market conditions of uniswap pools. This component is very crucial in a sense that it provides us for environment for testing and refining liquidity provision strategies. And the second component of this system is agent based model.
00:30:17.126 - 00:31:35.304, Speaker B: So agent based model creates a dynamic and interactive environment by simulating behaviors and interaction of various agents within the use Uniswap v three ecosystem. This model includes different type of agents like there are swapper agents and there are liquidity provisioning agents. So these agents interact with this our environment, within this our environment and their interactions are modeled by this agent based model. And swapper agent performs swaps based on the policy which we have defined for them. There are different type of swapper agents like there is retail swapper agent, then there is whale trader, and then there is a arbitrager and other type of agent which are modeled within this swapper agents functionality. And liquidity provider provisioning agents are also responsible for providing liquidity and removing them based on their policies. They are also categorized in different different categories based on our analysis of different type of users, which we did using clustering and different classification algorithms.
00:31:35.304 - 00:32:20.448, Speaker B: So RL agents, that is the most important part of this framework. They are trained to learn optimal liquidity provisioning strategy through interaction with this simulated environment. The training process involves continuously improving their decision making by learning from the outcomes of their interactions. So there are two type of RL agents which we have used in this framework. There is DDPG agent, deep deterministic policy gradient. So this is a reinforcement algorithm that is used for training RL agent. It combines the strength of policy gradient methods and q learning methods.
00:32:20.448 - 00:33:37.938, Speaker B: So it allows agent to handle high dimensional stair spaces and continue section spaces. So we have used, we have to use a continuous action space in our algorithm because the price range which you are selecting, they don't have any discrete limits. So DDPG is very effective in dealing with these continuous action spaces. And the second algorithm is proximal policy optimization, which is another reinforcement learning algorithm. It is more efficient and stable in terms of DDPG and it takes less samples to learn and optimize its policy. It uses a clipped version of the objective function, so preventing drastic policy changes which is required in environment, because one drastic policy changes can lead to the consequences which can, which can be financially very expensive. So the lived objective function ensures that the no policy changes are very drastic and sudden.
00:33:37.938 - 00:34:09.854, Speaker B: So this makes it suitable for this complex and dynamic environment like Uniswap v three. By integrating these components, this ILP framework creates a system for modeling and simulation simulations to optimize liquidity provisioning in Uniswap e three. So this combination of realistic simulator, diverse agent behaviors and reinforcement learning allows for the development of adaptive liquidity provisioning strategy.
00:34:11.154 - 00:34:24.098, Speaker A: Mohammed yeah, I just checked the chat. We have one question by Musa. Feel free to switch on your mic and ask if you.
00:34:24.146 - 00:34:25.370, Speaker B: Hey guys, how are you?
00:34:25.522 - 00:34:26.254, Speaker A: Hello.
00:34:26.954 - 00:35:52.574, Speaker B: Hey Adris, how are you? Yeah, I wanted to ask like this engine is tailor made for project owners prior to the project launch or after the project launch? Or is it for the token holders or can it be customized to their needs as well? Yeah, we can use this framework to optimize liquidity provisioning. So anyone who is providing liquidity, either as a project owner or as a user, so they can use this framework and once they, they have some capital to inject in this tool, any, so they can use the strategies from this framework to, in order to optimize this framework, their liquidity provisioning. So anyone who wants to launch his token, you can use this framework to provide liquidity in a pool or if you want to provide liquidity in an existing pool like each USD USDC, so you can leverage this framework for your own strategy. For product developers like liberty managers, they can use this framework in their vaults for custom strategies of their vault. So anyone can use it based on his requirements. So it is for every kind of user who wants to do this. Is this.
00:35:52.574 - 00:36:31.184, Speaker B: Are these any similar to DLN? Musa, can you. I don't know about DLN. I just recently studied that they are using a concept of dynamic liquidity provisioning. So is this any similar to that or is that different? Actually, yeah, actually the. All the liquidity managers have some kind of strategies of optimizing their liquidity provisioning. So this framework specifically is unique because it leverages AI and ML. There is a product with the name of Giza agents.
00:36:31.184 - 00:37:20.416, Speaker B: I will share the name of that in chat. So they are using different kind of machine learning algorithms for the liquidity provisioning strategies. But apart from that, all different liquidity managers have some kind of deterministic strategies which. Which are not that efficient. And in our evaluation of this framework, I have used mavericks both strategy and compared the returns of their strategy with this RL strategy. And it was our strategy outperformed that based on the over evaluation results. So RL algorithms are much better in dynamic environments where volatility and all these parameters, they can't be handled by a deterministic model.
00:37:20.416 - 00:37:40.634, Speaker B: So this is the only project which is leveraging this machine learning to solve this problem. I hope that was. Yeah, yeah. Thanks for the right answers. Thank you.
00:37:43.694 - 00:37:48.194, Speaker A: Mohammed, can we check another question or.
00:37:48.974 - 00:37:50.714, Speaker B: Yeah, we can take questions.
00:37:51.694 - 00:38:19.242, Speaker A: Gosium, you also have a question. Please switch on your micro. You can't talk. Okay. I tried to read it out. What was the comparison to another project company? I think you are referring to what Mohammed just mentioned. How reinforcement learning agents or dynamic liquidity provisioning can be more efficient.
00:38:19.242 - 00:38:21.974, Speaker A: Just mentioned Mohammed. Do you know what I mean?
00:38:24.274 - 00:38:57.566, Speaker B: I think he's asking about the evaluation comparison of different strategies. So it was mavericks. Maverick has three different strategies that has more left mode right and more both. So I use mavericks mode, both strategy for evaluating our agent. It's also being used by a 51 finance, another liquidity manager. They have a very similar strategy to mold both with slightly different names. So I tested this agent against these both strategies and it outperformed them.
00:38:57.566 - 00:39:03.674, Speaker B: So I think we should continue with this.
00:39:04.374 - 00:39:21.814, Speaker A: Just one final question. Is this comparison. I know that there are some write ups about your work on Uniswap B. Three liquidity provisioning. Is this included in the articles you published?
00:39:22.114 - 00:39:22.874, Speaker B: Yeah.
00:39:23.034 - 00:39:23.774, Speaker A: Cool.
00:39:24.194 - 00:39:31.218, Speaker B: It is included in the evaluation strategy and results of these evolutions are there in these articles. Perfect.
00:39:31.266 - 00:39:37.654, Speaker A: Then egosium. You should find it on notion at the studies and information hub. Okay, let's go. Let's continue.
00:39:39.394 - 00:40:48.668, Speaker B: So, we have discussed different components of this framework. So first of all, the, we will see the first component of this simulator of this framework, which is uniswap b three simulator. So this uniswap b three simulator basically uses ETH, Brownie and a local ganache blockchain to deploy v three core contracts on this blockchain. And it mimics uniswap v three operation by allowing token deployments, pool interactions. And we can perform all different type of action like add, remove, liquidity, swap and pool get boosted. So we deploy these tokens in this simulator in a local environment and then we interact with these pool contracts through this uniswap v three interface. So I think I will, I will take you through the code of the simulator after this because it will be difficult to switch things here between slides and there.
00:40:48.668 - 00:41:42.462, Speaker B: So we will take a look at that, how the simulator is configured. And I have also, there is an also article on my medium which is related to the use of this simulator to launch new tokens. If you are as a, as a retail user trying to launch a meme coin or something like that. So maybe you can do this simulator in order to optimize your liquidity provisioning. Like what feature you should select, how much supply of tokens you should put in your pool and what should be the initial price of token. And these kind of parameters, they can be optimized using this simulator, which is basically our local Uniswap V three pool. Okay, so the next component which we discussed was agent based model.
00:41:42.462 - 00:42:46.464, Speaker B: So ABM is, it simulates action and interactions of various agents within Uniswap V three ecosystem. So by modeling the behaviors of different participants, the ABM creates a dynamic and interactive environment that mirrors real world market condition. So what happens in RL that when once your agent takes an action like it adds liquidity into the liquidity pool, then the, to model that dynamic nature of our environment, we have to basically take a step of environment like a real Uniswap V three pool. So unit in Uniswap pool, people are trading, they are adding liquidity, they are removing liquidity. So all these actions are being performed by the actual agents. But we are in an environment where we don't have these kind of agents. So agent based model is used in order to simulate those behaviors, so how these behaviors are modeled.
00:42:46.464 - 00:43:34.526, Speaker B: There is an article from Gauntlet on user cohort analysis which they performed a research on optimizing liquidity, mining incentives for Uniswap v. Three on the DoW platform. So I used insights from that specific article to model the behaviors of different agents in our model. So this agent based model performs action based on the policies which are defined in the policy function of each of these agents. And by incorporating these agents, the ABM provides realistic market dynamics environment of Uniswap. V. Three, the interaction between liquidity provider agent and swapper agents.
00:43:34.526 - 00:44:35.840, Speaker B: They create an environment to for testing and optimizing these liquidity strategies. And it is done through the token spice agent based simulator which has different components like you can define your own agents, you can bind them together through their net lists, you can configure their actions, how many steps they will perform and in what order they will perform these actions. So all these actions are orchestrated through the net lists of token spice. Okay, so we have policies of all different type of agents. We used mathematical statistical models based on this gauntlet analysis to define all these agents. These are the agents which we have in our agent DOS model. So there are noise traders, mev boards, whale traders, arbitrageurs, and there are static LP's, grid strategy LP's and dynamic strategy LP's.
00:44:35.840 - 00:45:46.422, Speaker B: So these are different actors which we use to model the behavior of different type of actors inside a uniswap environment. Uniswap et environment. Okay, so reinforcement learning agents, they are two type of agents which we discussed earlier, DDPG and PPO. So I will briefly explain the learning process of these two agents and once we see the code then we can find out that how they are doing it. So DDPG agent has an actor network that suggests actions and given the current state, so actor network gets the current state of the pool and then based on that it suggests an action, the critic network, that it evaluates the action of potential reward. And this critic network basically adjusts the action of proposed by the actor network. So the combination of these actor and critic models which are basically neural networks who learn their weights of their different neurons over the time.
00:45:46.422 - 00:47:03.444, Speaker B: So by, by a combination of actor and creating a network, this DDPG algorithm learns how to perform optimal action. PPO is known for its stability and efficiency and it learns by optimizing up policy incrementally. So it's our own policy algorithm which directly optimizes its policy based on the feedback from the environment. And it has more potential of balancing between exploration and exploration because GDPG is prone to exploitation unless you introduce a version noise in each section. So once you have learned a strategy, PPO can. PPO explores different action in order to learn a better but DDPG, most of the time it tries to exploit the existing strategy instead of exploring new actions. So, using a combination of these two different type of agents, we can also see that how the Uniswap environment is represented by a mathematical function, because the DDPG has more static mathematical definition as compared to the PPO.
00:47:03.444 - 00:48:07.598, Speaker B: So we can define the properties of a uniswap pool based on the performance of these two different type of agents. Okay, so agent environment is basically the environment in, agent performs all those actions in our, in our code, we have defined it as a discrete environment, which is basically used to interact with the Uniswap simulator and agent based model. So this environment has two basic components. It has an action space and another, and state space. So what is action space? You know, in our case, our action space consists of price lower, which is tick lower of your liquidity position, and price upper, which is a tick upper of your liquidity position. So if you are using reinforcement learning for any other use case, you can define the action space of agents within the constraints of. You can define it discretely, or you can define it with a continuous function.
00:48:07.598 - 00:49:04.362, Speaker B: So like, if you want to model staking mechanism. So there, the action space consists of stake and unstake, and it has a certain value, you can discretize it between a certain range, or maybe you can keep it continuous to allow agent to perform any type of action. And the second component is state space. So, state space include various market indicators such as current market price, liquidity, depth in the pool, and free growth variables like free growth zero and free growth one. There are two variables which are tracked by any uniswap pool. So though they basically accumulate all the volume up trades which are being conducted in a uniswap pool. So the, this state space is basically what an agent observes at every step.
00:49:04.362 - 00:50:12.414, Speaker B: Based on this state, it takes its action and it tries to optimize based on the, based on the results, which it gets from its reward function. The reward function, which we have, it considers fee, income, impermanent loss and portfolio value, and potential penalties by providing, by providing penalty in case of an invalid action or action out of range, which is very beyond the limits of current price. So this reward function calculates the efficiency of a liquidity position. After this environment, we have a training step. So the RL agent operates within a simulated environment by learning this optimal strategy, by exploring different type of actions and observing their outcomes. So for training, we have to define three major steps. So, first of all is defining states.
00:50:12.414 - 00:50:59.692, Speaker B: The state space includes market conditions such as token price liquidity and trading volume free growth. And this environment is dynamic in nature. So agent based model is what makes this environment dynamic. And it provides, provides the action at every step by leveraging its policies. So dynamicity of this environment is modeled through this agent based model. And during the training loop, agent learns its policy. And we can monitor the learning process through the loss function, which is basically the training loss of your actor and critic network.
00:50:59.692 - 00:51:59.004, Speaker B: So you can monitor that loss and see that whether your RL agent is converging or not. And these loss functions are basically tracked using different tools like you can use ML flow or tensorboard to track these loss functions. Okay, so evaluation the evaluation process of this it assesses the performance of our RL agent based on its training results and compares it against a baseline strategy which we described earlier. So this helps us to understand how well the agent is optimizing liquidity by liquidity provisioning strategy. So there are two types of evaluation which we can do. First, one is agent actions. By we can analyze the agents action over extended periods to see how they align with the market prices.
00:51:59.004 - 00:52:45.418, Speaker B: This includes examining how the agent adjusts its liquidity position in response to changing market conditions and agent. We can also monitor the agent actions. We can determine if the strategy it employees are effective in maximizing returns and minimizing risks over time. So basic realization of agent action as compared to the current market price and the fees earned by agent, we can. We can assess the performance of our rna. We can use. We can also use the baseline strategy to assess the performance of RL agent so is compared against.
00:52:45.418 - 00:53:48.152, Speaker B: It's basically comparing performance of RL agent with the baseline strategy which is Mavericks mode growth strategy. This baseline provides a standard to measure the effectiveness of RL agents decisions. And there are different matrices which we track during this training and evaluation process, like raw rewards of agent fee income in permanent loss in order to determine the performance of agent against the baseline strategy. So the final step of our complete RL framework includes inference which is basically the final equity strategy where user interacts with the trained model. So it has multiple components. The first one is user profile assessment. So every user who is providing liquidity in a uniswap pool has some kind of risk tolerance profile of how much risk he wants to take and his risk appetite.
00:53:48.152 - 00:54:46.312, Speaker B: So it can be low risk agent. It can be a medium risk agent or high risk agent. So we can set this profit taking and stop loss thresholds according to the preferences of users and another defining factor of which is defined by the user is investment horizon that how long you want to provide liquidity? It could be short time, midterm and long term investment. So based on that, the parameters are adjusted of this RL agent and every user has its risk preferences a bit. Based on that he can select pools of different volatility. Like in stable coins, pools are high volatility pairs and after that the integration of RL agent predictions. So, predictive modeling is used regularly.
00:54:46.312 - 00:55:59.576, Speaker B: We use DDPG and PPO agents to predict optimal liquidity ranges based on the current pool state. And we decide that how often we want to curie this RL agent for adjustment based on the market preferences, market conditions and user preferences. So implementation of this strategy starts with the initiation position where user enters the liquidity pool based on initial RL agent prediction and user preferences. And after that we set up a system to monitor the pool state and equity position performance. And then we use the adjustment protocols like, based on different parameters like profit taking, price range, exit and stop loss to rebalance the position periodically. We can do it periodically based on some kind of redefined strategy that how often we want to monitor our position. So that time horizon is defined by the users and we collect data from the user based on, based on the returns he has earned.
00:55:59.576 - 00:56:27.516, Speaker B: And that feedback is then again looped into this RL agent to optimize its strategy. So now we will move towards the actual implementation of this algorithm. For that, I will share my screen with you where you guys can see that, how this agent operates. Let me know you that you can see the screen here.
00:56:27.660 - 00:56:28.584, Speaker A: Looks good.
00:56:29.204 - 00:57:18.824, Speaker B: Okay. Okay, so let me just get this. Okay. So first of all, let's see this Uniswap V three model which is basically simulator of Uniswap V three locally deployed on Ganache blockchain. So here we are configuring the basic parameters like connecting it with the Ganache blockchain which is running locally in the system. So here we have this blockchain running in our terminal. You can see that all the transactions which are being performed by the, this environment, they are uh, being running on this blockchain.
00:57:18.824 - 00:58:18.870, Speaker B: We are connecting this blockchain with our simulator and after that we are deploying the tokens. So user provides the symbol lock tokens and all the parameters of the token. These tokens are deployed. And once they are deployed, then we deploy a pool with some initial condition, like what fleet at this pool you want to be and what should be the initial liquidity distribution inside the pool. And then there is a sync pool state function which basically gets the data from the actual uniswap pool through Uniswap subgraphs and then syncs the pool of this locally deployed pool with this, with this actual uniswap pool. So once the pool state is synced, then we are ready to perform different type of action. So these are the actions we can perform on this pool.
00:58:18.870 - 00:59:19.838, Speaker B: Like we can add liquidity, then we can remove liquidity and perform swaps. So all these functions related to the actions of oral agent which it can perform like provisioning and swap, and all the state function which are used to get the state of pool like liquidity positions, we can get tick state, we can get global state of price data and all those variables by using this interface. So this complete interface helps us to perform different kind of actions on uniswap pool and then simulate them in our RL agent. Okay, so the, this is the notebook which runs on the simulations and all the training process. So this is our training environment. So in training environment we have defined an action space. So in what train the action RL agent can perform.
00:59:19.838 - 00:59:59.282, Speaker B: So we have defined a lower limit and upper limit for this action space. And then we have a state space which basically the current state of the pool. So we are getting current price of the pool. It's liquid distribution, its free growth value, growth variables. So this is the state which is observed by the RL agent. So once we have set all these parameters, we have this step function which basically performs all the steps sequentially. So first of all, the RL agent gets the state of the pool and based on that it gives us an action.
00:59:59.282 - 01:00:42.678, Speaker B: So we perform that action in this environment. So here the take action function basically performs the action which is, which is suggested by the RL agent. Here the raw action is suggested by RL agent. Then we pre process it to convert into a proper format and then we take this action on a actual uniswap pool. After the action of RL agent, the environment takes a step which is basically the agent one step of agent based model. So this engine dot run performs one step of our agent based model. And after that the news, we get the new state of the pool.
01:00:42.678 - 01:01:36.400, Speaker B: And based on that new state of the pool, we calculate rewards that how much fee income this section has performed, this performed action has produced, and what are, what is the implemental loss. And based on this scale reward. We, we provide this reward to our agent again, and based on that iterates over to optimize this reward function. So these are the functions related to the getting the observation space from the global state function which we have seen in the uniswap v three interface. And after that the actions of agent are executed in the environment. So these are all helper function related to this. So this is the or second environment which is evaluation environment.
01:01:36.400 - 01:02:11.642, Speaker B: So it is very similar to what we have seen in our training environment, but it has a baseline policy also. This is a baseline agent policy. So apart from the agent taking an action, the baseline agent also performs an action. And then the returns of both, we can, we are calculating the returns of both the agents or RL agent and a baseline agent. And then we are computing the rewards of both of these agents. And based on that we are evaluating. So this is where I have defined the baseline policy, which is mavericks both.
01:02:11.642 - 01:03:04.210, Speaker B: So this is implementation of that policy. So after the training environment, this is, these are the definitions of our RL agents. So first of all, we have this actor model of our deep deterministic policy gradient. This is actor model. So we, I have tuned these different parameters of number of layers on number of neurons in each layer, type of layer and what type of activation is used. But we can experiment with these parameters while fine tuning our model. So this actor and critic model which is defined right here, the combination of these two different models is basically is used by this choose action method in order to perform an action.
01:03:04.210 - 01:03:48.994, Speaker B: So the actor model suggests an action and that action is performed by the agent. And based on return, static model sets its weights to, to evaluate the quality of that action. And that based on that evaluation, it adjusts the parameter of actor model. So this combination of actor and critic models is used to perform the learning functionality of this agent. So here basically adjust the parameter of both the models. You don't actually need to know in depth that what is happening here. But these are basically the parameters adjustment of over both the models.
01:03:48.994 - 01:04:33.318, Speaker B: And this is our training loop. So this training loop basically initializes over environment. Then it defines our aral agent, initializes our DDPG agent, and then here we can provide that for how many episodes and for how many steps this training loop will. Training loop will perform. So once we are done with the training, we can visualize the results of this RL agent. So the PPO, I will not go into the depth of the implementation of PPO. It has some, some it has.
01:04:33.318 - 01:05:21.764, Speaker B: It is slightly different, but it is not much different from that of PPO. So here we have interface where we run the training loop and then visualize the results. So I am here, I will simulate this for just five steps because it will take a lot of time to perform these. So here it will start training. And once we have this training here, you can see that it is performing different token transfers. RL agent is helper from this action. So this is the price range it has selected.
01:05:21.764 - 01:06:03.830, Speaker B: It is bit 590, price lower and price upper is 1434. And based on this action of RL agent, it has opened a liquidity position in this pool. And this mint function is basically the event emitted by the contract. Then environment takes steps, environment basically take steps based on the agent based model. So environment has data position, it has performed a swap, then it has removed a liquidity position, then it has performed another swap. So it takes certain steps. After that we have collected some fee.
01:06:03.830 - 01:06:41.930, Speaker B: This is the amount of fee which we earn from this step. And based on this collected fee and impermanent loss, the we give feedback to the RL agent and this training process continues for the number of steps which we have defined. So once this will finish, I can show you the visualizations with of training. So maybe, maybe we should wait for a minute or two. In the meanwhile we can take any questions if we have.
01:06:42.082 - 01:07:08.404, Speaker A: I was just suggesting everyone, this is the time for questions. Maybe I have some words for one. So the key KPI to observe performance and also to train the agents is fees plus or minus impermanent loss as a value, as a metric for profits, right?
01:07:08.944 - 01:07:59.504, Speaker B: Yeah. It's fee minus impermanent loss plus portfolio value. So yeah, is another thing we considered in the reward. So basically this application of this RL in this specific environment, I really don't want to dig deep in there that how it's been done. What is the utility function and theory of Uniswap v. Three? But the goal is to see that how we have implemented this, in this, for this specific use case and then learn to use it for any other purpose, for any other kind of optimization or verification inside our token engineering process. So that is the primary goal of this session.
01:07:59.504 - 01:09:02.832, Speaker B: So I will try to focus on that because the implementation is very abstract and it changes from one use case to another. So that is not the focus here, but the basic idea of how we have the approach of using a reinforcement learning agent and then agent based model and then a simulator and how they all are interacting with each other. Defining a utility function which is in our case is fee minus impermanent loss. But if you have any other kind of system, there are different utility function which you optimize like user engagement, like staking amount of token stake or assets bought inside your environment. So we can define those, those objective functions inside our RL problem and then we can optimize all those leveraging this. So this is a very useful tool. I have worked on this for liquidity provisioning use case and for some of the use cases also.
01:09:02.832 - 01:09:18.764, Speaker B: But this is the one use case which it really has performed well and that's why I try to explain that how it can be successfully used for solving different kind of problem.
01:09:20.064 - 01:09:21.324, Speaker A: Yeah, makes sense.
01:09:22.024 - 01:11:34.384, Speaker B: All those who are interested in diving deeper into this, they can go to my articles and GitHub repository, they can set up an environment locally and then try this RL agent for any different use case which they want to, they don't have to build it from scratch, it has all the components, so you just have to tweak with your objective functions and your agent policies. So once you have those policies defined for your specific use case, you can use that in order to, in order to test your own, perform your own simulations. Okay, so let me see that. We got, it's taking longer than usual to train, so I will not be able to show you the visualizations of this training. But they are, I have read them in our, in my article and on GitHub. So you can see there, and this is an example of inference, performing an inference. So you set these parameters of your preferences like risk tolerance, profit taking, stop loss, investment horizon and risk average and threshold.
01:11:34.384 - 01:12:42.046, Speaker B: And based on that the RL agent will suggest you an action. So in this case we have selected breadth BTC pool. So for this pool it has suggested a range of 21 to 24 ETH BTC. So that is around 63,000 to 72,000. So based on this model that currently based on these user preferences, the ideal liquidity position in Ubisoft pool is between 63,073 72,000. So if you change your risk parameters or if you change pool, it will give you different, different suggestions for different type of pools and based on your risk parameters, and also the train model you are using, that will also result, subdition models will vary. So you can tweak with the parameters of the training and, and then see that how different strategies are being deployed by this RL agent.
01:12:42.046 - 01:13:36.054, Speaker B: So that is pretty much it from this session. And if we have question answers, I can do that. And the, for next session you can, it's not an assignment or homework or something like that, but if you are interested in this, you can just set up, set this up. I have Django application of this, so you don't have to perform all these steps manually. You can just play with the parameters and everything through that application interface and try out different strategies and if you are actually interested and in using this simulation for any other project. So maybe we can discuss that in next session. Also that how, how you are planning to implement this strategy, this simulation and modeling strategy for any other use case.
01:13:36.054 - 01:14:16.108, Speaker B: Apart from that, we have our next session in which I will be representing the token GPT demo of token GPT. So for starters, if you want to test that. So I have a basic implementation of rag inside chat GPT. So rag is implemented on that token GPT, custom GPT which I made on chat GPT. So it is public, anyone can use it. So try out that custom GPT which has token engineering data augmented in them. I have used it and it was really helpful for me.
01:14:16.108 - 01:15:04.224, Speaker B: So you must also try that. I will share the link of that custom DBT inside in your discord channel. So maybe you can, you can go there and see that how it is better from normal chat DPT. The knowledge it has about token engineering specifically was injected through different documents which I uploaded. And then after that it has knowledge of all the tools like CAD cat. If you ask from it about the actual implementation of CAD Cat simulation. So it will give you much more precise and accurate results as compared to normal chat GPT which you are using.
01:15:04.224 - 01:16:32.362, Speaker B: So that is really helpful for those who are trying to, trying to start with this, with this domain because that it has specific knowledge of token in. So even if you ask very best basic questions like of token engineering. So it, it has a very good response as compared to the other models, that token GPT which I will be presenting in next session, that will be the manual implementation. So it will not have a user interface like chat DPT, but it will be inside the environment, but its functionality will be much more enhanced by the all the tools which are integrated with it. So you can use it to extract data from different uniswap subgraphs or do an analytic dashboard or defy lama. So these all public APIs will be integrated in that. So if you ask it within plain text that you want to get data about a specific token or about a specific protocol, so it will give you that data based on that tax prompt and you can perform that analysis on that data and you can also use different models and you can integrate your own tools inside this model.
01:16:32.362 - 01:16:43.204, Speaker B: So I will be sharing all this in next session. Do we have any questions? Should we.
01:16:47.424 - 01:17:35.196, Speaker A: I mean, only experimenting with this framework. Now back to intelligent liquidity provisioning. I think it's really worth setting up a dedicated session. So just everyone to let you know you might have seen it in any track, feel free to open up and schedule learning collaboration sessions. Just schedule something. Invite everyone to come on board and then experiment with the intelligent liquidity provisioning framework yourself. I think, Mohammed, you've done a great job in documenting this very well, so you can just start getting your hands dirty.
01:17:35.196 - 01:17:43.584, Speaker A: And I think then there is plenty to discuss and I'm sure Mohammed would be happy to provide feedback.
01:17:44.564 - 01:18:27.314, Speaker B: Yeah, I will be. I'm available on discord and platforms, so anyone who wants to discuss, we can schedule a meeting and do it. And all those who are trying to start with this, they must attend Mark Richardson's sessions on this part of this track because I have attended his first session and that was very comprehensive. And the way he's explaining the dynamics of Uniswap v three with visual aids and all those things, they are very helpful. Who want to learn about the Uniswap v three or amms or different type of market making protocols in decentralized finance.
01:18:27.474 - 01:18:46.774, Speaker A: So this was not bland, actually, but it's perfectly complementary, I realized today. Yeah. Okay, so no homework until next week, but any questions or any experiments, most welcome, right?
01:18:47.474 - 01:19:22.196, Speaker B: Yeah, yeah. There will be no homework, but we can discuss. Discuss ideas from all the participants that. How they think that they can leverage this. This framework for their use case and how they are thinking about designing different kind of objective functions and what kind of environment they want to use it on. So we can discuss that, just to share ideas and see that. Just to see that, how everyone is thinking.
01:19:22.196 - 01:19:57.390, Speaker B: And by learning in public, we can enhance our skills. Would you push the current folder seeming like you made significant changes? Yes. This v two of framework, it isn't public. Now, I will try to make it public as soon as possible. Actually, I was working with the product who are building on this idea, which I mentioned earlier for visa agents. So for that purpose, I have to make it private for some time. But I will.
01:19:57.390 - 01:20:30.214, Speaker B: I will share you the public repository very soon, or if anyone in specific who wants to get added in this repository. So just send me your email into the discord and I can add you until it's private so you can see and collaborate on that. Okay, Tiago and I will add you. I just copied your email and I will add you disposal after this session.
01:20:35.954 - 01:21:10.330, Speaker A: Good. Last chance to drop any questions. Don't be shy. No stupid questions here. In case we don't have any, I'd say we can close the session for today, but again, we have our Discord channel to continue the conversation. Check out the GitHub. Let Mohammed know if you'd like to have access to the latest version.
01:21:10.330 - 01:21:40.838, Speaker A: And yeah, see you on Discord. And wait, let me check our next session. Mohammed. Oops, I'm just pulling up the calendar. Next session will be next week. Same time next Thursday? Yeah, one week to experiment with the intelligent liquidity provisioning framework. All right, thank you.
01:21:40.926 - 01:22:03.636, Speaker B: Will not be boring as these because they are very technical, but they were very user friendly and they will. Everyone can interact with that because it will be a simple user interface, text based. So I want everyone to join that and don't get intimidated by the complexity of this one. That will be very user friendly.
01:22:03.820 - 01:22:22.756, Speaker A: I was just about to say it was not boring at all. It's a lot to digest and it's probably why everybody was staring at the screen and trying to follow, but perfect. Thank you so much. Good. See you next week or on discord, everyone. Thank you.
01:22:22.860 - 01:22:24.304, Speaker B: Thank you. Thank you.
01:22:25.464 - 01:22:26.000, Speaker A: Bye.
01:22:26.072 - 01:22:26.724, Speaker B: Thanks.
01:22:28.504 - 01:22:29.944, Speaker A: The recording has stopped.
