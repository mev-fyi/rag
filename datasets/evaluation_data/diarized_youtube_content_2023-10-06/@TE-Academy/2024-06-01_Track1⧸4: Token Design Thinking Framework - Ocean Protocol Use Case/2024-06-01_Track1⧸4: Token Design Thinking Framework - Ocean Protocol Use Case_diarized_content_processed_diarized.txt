00:00:04.480 - 00:01:19.478, Speaker A: Hello and welcome everyone back to the study season at Te Academy. We'll have the final session today with Sherman on the token design thinking framework, a structured approach for crypto founders, protocol designers, and token engineers. This is the fourth session we run on this topic, and today we'll see who has passed the criteria for the submissions we've received, working on a case study and exploring a token model via the token design thinking framework. So, Drumroll. I'm excited to see the results that we'll share in a moment or highlights from submissions and also, of course, the total lists of students who have passed. Before we dig into that, I just want to share that we'll run an event on the future of crypto impact measurement next Tuesday. Together with open source Observer, Metagaf and hyperserts, we'll discuss how we can make quantitative measurement of impact work in crypto.
00:01:19.478 - 00:02:11.962, Speaker A: And this is of course an important one. What tools do we have? What approaches seem to be valid? And what do we have to keep in mind when trying to measure impact in a quantitative way? This, of course also is related to what we are trying to achieve with the reputation based voting mechanism that we are currently working on. And we have some great projects on board to discuss this topic with next Tuesday. You can sign up. It's again a Zoom call via the QR code. All right, now I think you, I bet you're all excited about did my submission pass? And just some more words on this NFT thing. So every live track in the study season offers an NFT.
00:02:11.962 - 00:03:10.392, Speaker A: So if you passed proof of knowledge criteria, you are eligible to receive an NFT. And in this case, we had a submission date last week and we received a great number of submissions working on the case study we have set up for this live track and we'll now share some highlights in submissions. And after the session we'll share the final list of recipients on discord. And of course we'll contact everybody also via email so that you have all the information you need to receive your NFT. Just generally it works like this. We'll airdrop this NFT to all students who are eligible, so no further action required on your end. We'll let you know by when you can expect them, then you'll find them in your profile and you'll get a note once the airdrop has been completed.
00:03:10.392 - 00:03:16.816, Speaker A: So you just don't have to do anything in this respect and we'll keep you posted.
00:03:16.960 - 00:03:17.360, Speaker B: All right?
00:03:17.392 - 00:03:46.604, Speaker A: And that said, I'm now keen to hear Sherman. First of all, welcome back to the session. Thank you for joining us again. Thank you for this series of sessions about the token design thinking framework. And let's dig in. So, let's take a look at the submissions and the case studies and. And just explore, step by step, your observations and good, or maybe things to improve.
00:03:46.604 - 00:03:48.872, Speaker A: Stage is yours.
00:03:49.048 - 00:04:03.484, Speaker B: Thank you. All right, let me share the screen. This is a bit weird.
00:04:04.854 - 00:04:06.634, Speaker A: Screen sharing should be on.
00:04:07.694 - 00:04:52.624, Speaker B: Yeah, but I don't find the. Ah, here it is. All right. Okay, so today we will look at the use case of ocean protocol for those who are attending the live screen and don't know the people attending the token engineering kind of class or batch, they had two weeks or a week to study ocean protocol. Use the design thinking framework to analyze the use case. According to this framework. They submitted the results.
00:04:52.624 - 00:05:31.394, Speaker B: Most people passed in the sense that they would get an NFT. Some people didn't, probably had too little time to really, because it is. It is a lot of work. A, to study the use case, b, to really go through all the questions or the selected questions. It wasn't the whole design thinking framework, selected questions, but it's still a lot of work. So I understand that some people did not have the time and or the patience to do this next to their jobs or regular life. So, unfortunately, those did not pass.
00:05:31.394 - 00:06:17.494, Speaker B: Angela will be sharing the results. As she said before, I asked Angela to verify my kind of, my assessment, and she agreed. So it was four eyes principle on deciding who passed and who didn't. Okay, so before maybe we dig into, I would like to share a few examples of three different students. But, baby. Angela, before we start, I would like to ask, ask the crowd what maybe two or three people who could comment what the greatest challenges were in fulfilling this task.
00:06:18.514 - 00:06:33.018, Speaker A: Yeah, great idea. Just raise your hand, switch on your mic, drop a note to the channel. Biggest challenge for you? Working on the design thinking framework and case study.
00:06:33.146 - 00:06:42.726, Speaker B: Any other feedback? Was it easy or. Yeah. Anyone?
00:06:42.870 - 00:06:44.998, Speaker A: No complaints? No challenges?
00:06:45.126 - 00:06:47.750, Speaker B: No complaints. No challenges. Okay.
00:06:47.902 - 00:07:17.604, Speaker C: All right, I'll just share my observation. So, I would say I was sort of familiar with the ocean protocol, and. But still, it was. It's a very complicated, uh, ecosystem with five tokens. So some questions were specific to the tokens, and, you know, to get the answer to those, I had to do a lot of searching, and there were times where I was not really successful in getting the specific answer.
00:07:17.644 - 00:07:39.494, Speaker B: Yes. Yes. I'm not surprised. I think most people should have had. The research on this use case is quite complex. Also, because the protocol changed so many times and the documentation is, it's unclear. There is no timeline for the documentation.
00:07:39.494 - 00:08:25.214, Speaker B: It's really hard to research. I spent a lot of time also researching it, so I know that quite well, any kind of use case is quite challenging to research, especially the use cases where there have been many protocol changes over history. It's hard to know when, what happened and which information is still up to date. So I had two interviews with Trent, personal interviews or one interviewer and or one kind of face to face session and then an email session where I had to clarify which stakeholders still exist, which don't exist anymore, et cetera. So I fully understand. Vishu.
00:08:26.914 - 00:08:30.190, Speaker A: Kerry, you are raising your hand. Would you like to share?
00:08:30.362 - 00:09:14.368, Speaker D: Oh, yes. I just wanted to kind of amplify what you just said about the idea because I did look at ocean protocol in the past, in the last two years, and I went through the white papers and everything else, and a lot of things have kind of changed. So that I think was like one of the challenges. And also, I mean, I wanted to work with someone and, you know, like I finally did, but, you know, kind of connecting with them in their time zone and kind of splitting the work was, was also, I mean, it wasn't a major challenge, but it was, you know, something that had to be dealt with also.
00:09:14.456 - 00:09:55.644, Speaker B: So, yeah, and you didn't have a lot, a lot of time. You have to. I will be presenting kind of my analysis of the use case, which is also outlined in the book. But like, I spent weeks researching this, so I fully understand that your kind of results after only one week of doing this and probably not full time, very likely not full time, will not be the same as my results. This is quite, and that was not the idea. The idea was to make you dig into another use case using this framework. And I would like to share with you now kind of my results or my analysis.
00:09:55.644 - 00:10:55.384, Speaker B: So, and also go over the challenges of how to come to those conclusions. All right. So if there are not any other comments, maybe I would like to share. So Giser Schneider, I selected, I liked Gesa Schneider's approach. She really, she or he really used or made a lot of effort not only listing the stakeholder types but also grouping them. And I like that quite a lot. Also use the existing stakeholder metrics template from them, from the Google sheets to analyze the stakeholders.
00:10:55.384 - 00:12:17.264, Speaker B: There was some clarity with the rights and permissions, but we will go over this. I saw that there was quite a level of detail put into doing this analysis when the economic design was, I would suggest and I think everyone, most people struggled with analyzing the properties of the economic properties of the token. It does not help if you don't stick to the template, because what I see here is that kind of the different criteria, the different properties were taken and then in a, instead of like using one template per token type. I think that's easier when you analyze it. Maybe this was done to cover for the fact that there weren't answers for all of the tokens, but it's better not to answer a question where you don't know how to answer it, but use the template as it is. It really helps in the analysis whether you're designing your own token system or analyzing an existing token system. And we will go through the results or through my analysis a bit later.
00:12:17.264 - 00:12:49.062, Speaker B: But still, I like this presentation quite a lot. Obviously there were many more slides to that. These were just a few examples. One LV. So I assume that's also was quite meticulous in analyzing the stakeholders, the roles, functions, rights and permissions, rewards and obligations. So that was quite well done. Analyzing the token, just two tokens were selected.
00:12:49.062 - 00:13:36.904, Speaker B: We will see that there are five tokens. And yeah, other than that, the analysis was quite good and also the purpose and reality. I quite liked the analysis of the original purpose and how reality panned out. I could see that this person went through an effort of understanding and applying the design thinking framework. The last showcase I would like to make was from Sebastian. Yeah. Also took some time in identifying the stakeholder types and grouping them according to the different criteria.
00:13:36.904 - 00:14:20.084, Speaker B: And in the economic design, like the people before. Yeah, we have three token types here, there are five. We will see why it's so hard to identify. And then again here, I would highly advise in the future, take the template and try to fill it out per token type. It's at least for me, maybe it's different for you guys, but you don't lose the overview so much. You can focus on one token type at a time and the properties of a token type. Other than that, the analysis was quite good.
00:14:20.084 - 00:15:45.994, Speaker B: Okay, so I won't be spending too much time, you know, the use case. So just for the audience, the people who have tuned in in the live stream, I will be repeating a few things that most of the people who did analyze the use case already know. So when it comes to the purpose, ocean protocol's purpose is quite clear. To provide a peer to peer data exchange protocol. Their idea was to create a decentralized marketplace that allows anyone to easily share the data they own and monetize the data they own in a privacy preserving manner, while maintaining the provenance also of this data, who published it, who purchased it, and who consumed it, thereby tackling a lot of principal agent problems of the data economy that we have in web two and sub purpose is to kind of create higher quality data for data scientists via publicly verifiable audit trails, and to provide this in the privacy preserving manner. The political principles, yeah, I'm not sure the formatting is a bit off. Sorry for that, the political principles.
00:15:45.994 - 00:17:29.524, Speaker B: So the founders stated that for them, data is a common good, and they wanted to find ideal licensing schemes for these common goods and data to be shared for profit, nonprofit, as well as mixed models. And another very important political principle of ocean protocol was to really create a decentralized and permissionless protocol, on top of which other people could build their own in a permissionless fashion, and on top of which anyone could create their own specialized data exchange protocol. So the idea was to do this to, in the long term, create a data exchange protocol where you have the minimum possible human intervention, let's say. So as such, the idea of ocean protocol was to create kind of this distributed Internet tribe, they originally called it the DAO, went away from the terminology because of, you know, the terminology was not, was misused a bit. But as I said, one of the political principles was really to create a permissionless and decentralized system. Now, the history of ocean protocol is very important to understand, as I said before, because it also is one of the reasons why it is so hard to research. It is one of the older protocols, application protocols.
00:17:29.524 - 00:18:29.874, Speaker B: The Ocean Protocol foundation was set up in 2017. The first version of the protocol was deployed in 2018, but back then, not all features that the protocol has today were available yet. Since 2018, the protocol has undergone massive changes. It had four major upgrades, version one, version two, version three, and version four. If you're interested in the details of what changed across the versions, it is analyzed in more detail in the book daos and purpose driven tokens. What is really interesting here in the history of ocean protocol is that they started out with one token only, the ocean token. Now today they have five tokens after version four, the data nfts, data tokens, ocean, the original ocean token, Vocean, and h two o.
00:18:29.874 - 00:19:40.564, Speaker B: And we will analyze all five tokens in detail a bit later. So I think what's also important to. So Ocean wants to be a decentralized organization, or wants to be decentralized in the future. But in the beginning, in the early years and now, the governance layer has very much been shaped by the Ocean Protocol foundation and before even Bigchaindb. So it started. If we look at the governance feedback loop with Ocean Protocol foundation set up in 2017, the first rule sets were kind of identified and designed, and eventually in 2018 deployed with the version one of the protocol constituting the first algorithmic constitution. Eventually, the first data scientists and data providers started to experiment with the protocol.
00:19:40.564 - 00:21:05.298, Speaker B: With that feedback from the community over the years, the first adaptations were kind of worked into the protocol and subsequent versions were released. When it comes to the functional design, so the most important function is the data market, like where the data providers and data users meet. So the idea was to create a collectively managed and publicly verifiable data market. So content provision, content curation is content consumption is public, and content curation had specific mechanism referred to as data farming. They have, Ocean Protocol created an interesting functional design of how to have control over your data in a privacy preserving manner. Also via your Ethereum wallet, it builds on EVM compatible, Ethereum infrastructure and EVM compatible infrastructure. Yeah.
00:21:05.298 - 00:22:10.624, Speaker B: To make this work, because a lot of data is also part of this exchange protocol, and not only tokens. They needed to create ocean provider, which is kind of a system, kind of a middleware system architecture that you can either set up yourself or have someone set up for you as a third party provider. Yeah, when it comes, yes. The idea was to have a permissionless infrastructure, a protocol stack that is open source of offering necessary developer tools that anyone can create so that anyone can create their own third party marketplace according to their own market rules. The only thing that is not central or will never be decentralized is the question of IP or privacy violations. Intellectual property violations and or other privacy violations need to be resolved in a centralized fashion via the Ocean Protocol foundation. And they have a so called purgatory process.
00:22:10.624 - 00:23:21.726, Speaker B: This is something that cannot be decentralized yet, probably, or in the near future also for regulatory reasons. Yeah, I think it's basically, I think everyone knows how ocean market works. Ocean Protocol foundation set up a first application called Ocean Market, where they created a token based system to match data owners and data providers and create kind of curation principles around this market. But anyone can really take the ocean market as a template, tweak it, tweak the market rules and create their own third party marketplace. And many other party marketplaces or some third party marketplaces exist today, often industry consortia or automotive etcetera. Okay, let's go into, I think, what is most interesting, the stakeholders and the tokens. Before we look at the tokens.
00:23:21.726 - 00:24:14.474, Speaker B: It's very, I think, more important maybe to start with the stakeholders. I'm sure that many of you were confused analyzing the stakeholders, because many stakeholders that existed in version one or two didn't exist later on anymore. So the kind of stakeholders that you see here are the current stakeholders or the most important current stakeholders. Here we see the internal stakeholders, first and foremost, the Ocean Protocol foundation. In the early years, Bigchaindb, a private company in Germany, still had a lot of governance powers, but over the years this shifted towards the Ocean Protocol foundation. But that might have been a bit confusing when you research this. Yes.
00:24:14.474 - 00:25:26.004, Speaker B: Another stakeholder is, are the marketplaces. The first marketplace was Ocean market and any third party market that built on top of the ocean protocol and the latest, very specialized type of market that ocean protocol brought out as ocean predictor. It's kind of a marketplace for very special purpose predictions, but it's also a marketplace for data exchange. But the data is not random data, prediction based data. So these marketplaces, they are a stakeholder in the system because they, and these marketplaces, they have their own sub stakeholders, data publishers. So data publishers, could they publish data assets they own, or they collect it, or they predict it on predictor. It's data they predict on ocean market, third party markets, or ocean predictor.
00:25:26.004 - 00:26:21.914, Speaker B: Then you have the data consumers, the people or institutions that would consume data assets that others have collected, own, or have predicted via those different marketplaces. Another stakeholder group are ocean token holders. We will see later what ocean is, but basically it's the native token of the protocol. It was the first token. It has various functions. Basically you can use it to invest in for investment purposes, hold on to ocean. But if by staking, for example, you can participate in data curation or execute various protocol or application functions, we will see that in detail a bit later.
00:26:21.914 - 00:27:17.234, Speaker B: Then there is this one stakeholder group that still exists, the ambassador program. It's community contributors, members whose mission is to generate interest and activity in the ocean protocol and or inform other people about how it works. And then there is a stakeholder group around third party ocean provider services. The important middleware is hard to set up. Most people might not be willing or have the time or the know how to set it up themselves, and therefore they can use third party ocean provider services. And stakeholders have, or kind of institutions or service providers have emerged around this service. On the periphery of the network.
00:27:17.234 - 00:28:19.124, Speaker B: We have peripheral or external stakeholders, policymakers, as always, national, local, national, international policymakers are relevant. Since a data exchange protocol involves data. Obviously, privacy laws, IP laws, and privacy laws on a national international basis are relevant. So any changes in policymaking on a national level or international level will affect ocean protocol or ocean markets and the market rules. But not only IP law, privacy law. Since ocean has five different tokens, securities or financial market law might also apply on the periphery. We also have DeFi and CEFi services that exchange those tokens, for example, or accept those tokens, or where you can maybe stake those tokens.
00:28:19.124 - 00:29:43.164, Speaker B: So anything happening on the periphery of these decentralized or more centralized financial services exchanging ocean tokens will also directly or indirectly affect the system. Ocean builds on Ethereum and EVM compatible infrastructure. So any changes in the ethereum ecosystem, for better or worse, will also influence the ocean kind of socioeconomic dynamics. Not only the ethereum ecosystem, but other web3 protocols that might be used or might become more important all of a sudden. And then obviously we have non human institutions. The original idea was that ocean is being used by data scientists, and so any advances in data science or AI ecosystem evolution might influence that. Formal protocols, informal protocols just like with any other web3 protocol cryptographic algorithms, changes here might affect the ecosystem in one way or the other.
00:29:43.164 - 00:31:04.744, Speaker B: Okay, so if we look at the single players a bit more in detail, so the Ocean Protocol foundation, and previously also bigchaindb, they are really governance is centralized around Ocean Protocol foundation. We can say that their role is protocol design, testing and adaptations. They bootstrap, or their idea was that they will at least bootstrap the ocean ecosystem until it can fly alone with minimum intervention. That is their purpose. The activities of Ocean Protocol foundation is to allocate funds to protocol development and ecosystem evolution. These funds were raised in token sales in the early years of the foundation, and they allocated the tokens, or the proceeds of the token sale to different kind of to the treasury, and allocated different types of percentages for different types of purposes and or stakeholders in the future. Ocean Protocol foundation also filters illegal datasets, ip violations that I mentioned before, and they distribute ocean tokens on an annual basis according to the schedule and the predefined allocation.
00:31:04.744 - 00:32:56.504, Speaker B: They are the sole policy, they have sole policy making power, the representatives of the Ocean Protocol foundation, and they have certain executive powers, for example, filtering illegal data sets, their rewards and obligations. In theory, it's non kind of, it's a nonprofit foundation, they have non financial goals. However, it is worth mentioning that active founding team members who work on ocean protocol kind of in the foundation, in the core team, they received initial tokens, so there might be vested interests of people who might be bigger token holders, even though I'm sure most of them also have the non financial goals. Might be, yeah, you cannot negate the fact that you have some big ocean token holders who also might have financial goals. Ocean token holders are also a very important stakeholder group. They buy the network token and hold on to ocean, and they can, with ocean, they can participate in data curation or execute certain monetary policy activities such as locking ocean to earn interest, and they can lock ocean to actively vote over data assets. They can also use ocean in other DeFi and CeFi services, potentially earning interest, etcetera.
00:32:56.504 - 00:33:34.524, Speaker B: The rights that are attached, and we will look at the rights in more detail later. Also, they have revenue rights, voting rights, executive rights. So revenue rights. If you have ocean and you lock them, you can earn revenue on locking the ocean tokens. Voting rights are not voting rights over protocol changes, but basically over data assets. Kind of the importance of a data asset. We will look at the details later.
00:33:34.524 - 00:34:58.674, Speaker B: Executive rights in the sense of if you lock your ocean, you can also earn interest. And why is it an executive right? Because interest rate policies are kind of the fiscal policy of the network and that is executed decentrally via the ocean token holders who want to participate in it or not. And obviously they also have market power to create, they create demand for ocean by kind of buying ocean and selling ocean ocean token. As for rewards and obligations, ocean token holders can earn rewards by locking up ocean within the in kind of within the protocol. They are the beneficiaries from potential appreciation of ocean price if the protocol is more successful or gains more traction in the future as a growing usage of the protocol. So ocean market, yeah, and other third party markets provide decentralized marketplaces for matching data providers and data users. These markets can be more or less specialized and are independent from each other.
00:34:58.674 - 00:35:49.974, Speaker B: The activities they develop each marketplace can develop and maintain an application. Their own smart contract logic adapt fee structures where needed onboard. New types of users market to new types of users. Obviously they have being a data exchange. They have market power by determining the market rules and defining the rights of the buyers and the sellers on their marketplaces. They also define which token is accepted as a payment token within the market and they set the market fees. So ocean market and other third party markets, or a predictor have a lot of, kind of a lot of market power in the system.
00:35:49.974 - 00:36:57.052, Speaker B: The market operators earn fees on some or all market transactions. Depending on the market rules. They need to comply with the General Ocean Protocol meta rules let's say now data publishers on those different marketplaces, data publishers are very important stakeholders. They publish, they provide the main asset, right, the data assets or the predictions on the different marketplaces. The activities are, they share their data, they define the access price for that data that they provide and the access rules, access duration, access rules, access frequency, etcetera. They have obviously, market power. They own the property rights and the attached management rights to the data sets that they provide, and they can monetize access to their data sets.
00:36:57.052 - 00:37:23.574, Speaker B: So this is how they can make money, but they have the obligation to comply with local laws, such as privacy laws, intellectual property, etcetera. They can try to breach the laws, but then there is this purgatory process where data sets can be taken down if a data publisher publishes a data set where they over which they don't hold the IP.
00:37:24.314 - 00:37:46.234, Speaker A: Sherman, is it okay to take a quick comment on this stakeholder? I read it out, Susie, if that's okay. I thought that since ocean retype passive and volume df vocean was no longer in use, and there's a link. I think what we have to take into account is that this analysis is from dating from which month? Sherman?
00:37:46.934 - 00:37:55.302, Speaker B: I did this analysis end of last year. So if anything changed. Yes, maybe I should have. Okay.
00:37:55.438 - 00:38:15.158, Speaker A: Yeah, I mean, this is a typical problem. Things are moving fast and the locking v token aspect has changed over time. So keep in mind that this analysis is six months old. Still should apply in 90%.
00:38:15.246 - 00:38:40.314, Speaker B: But I might be. This table, though is very superficial, right. We will look at the different token types. This is just to get a general overview of kind of the role and the functions and the general rights and rewards. This is not detailed. This is kind of very high level. So maybe something gets lost in generalization.
00:38:40.314 - 00:39:10.944, Speaker B: Yes. Then the next stakeholder said, data scientists or the data consumers are obviously very important. You have two sides of the marketplace and both sites need to exist. Otherwise you don't have a market. So on one hand, we need people who provide the data. On the other hand, we need the people who buy the data. And they would generally be the data scientists or consume the data services via the different marketplaces.
00:39:10.944 - 00:39:58.292, Speaker B: Yeah, I'm just wondering if I should do this very much in detail because we will later go to the five different tokens. Maybe we have more time for the tokens. They obviously pay the data consumers, mostly data scientists, pay for data assets on the market with ocean or other accepted tokens. The accepted tokens can be defined by the marketplaces. They can develop prediction algorithm rhythms. If for predictor. They can take data assets they bought at any marketplace and use this data to develop algorithms for specialized marketplaces such as predictor.
00:39:58.292 - 00:40:45.356, Speaker B: They can also develop novel data science applications on ocean protocol. They have market power. They're the second side of the market, a very important side of the market. And they have usage rights, access rights, and certain management rights, depending on what rights are granted to a data set. These rights are bought or temporarily kind of accessed. Their rewards and obligations can vary depending on the role they fulfill in kind of the network, because data scientists can have different roles, they can be data consumers, or they can build different applications on top of the protocol. Yes.
00:40:45.356 - 00:41:14.622, Speaker B: Ocean provider is a service provider. I'm not going to go into the details. You will have time to read this much more in detail, obviously, in the book. This table is a super, it's a super condensed summary. Ocean predictor is a specialized marketplace. I'm not going to go into the details of ocean predictor here. Then obviously, regulators, they define the regulatory framework of what is allowed on those markets.
00:41:14.622 - 00:41:54.028, Speaker B: So they have a lot of market power. Also, I already talked about most of that and Defi and C, five exchanges. Obviously, I also talked about that. Okay, let's look at the different tokens. Maybe spend the rest of the time analyzing the tokens in detail, because I know that this was where most of, most of you struggled, a, in identifying all five tokens, which was already hard, and then really looking at the properties in detail. And maybe, Angela, if anyone has questions, we can address them directly. I think in this case it would make sense.
00:41:54.156 - 00:41:55.140, Speaker A: Okay, perfect.
00:41:55.292 - 00:42:39.144, Speaker B: Yeah. Data. So I'm not starting with the first token that existed, ocean token, kind of the protocol token. I'm starting with data NFTs because I think this makes sense from a marketplace perspective. So the purpose of data NFTs is to represent the property rights for a unique data asset. It basically represents the base IP of a data asset that I might own. It is issued upon me, claiming copyright when I upload that data asset to any ocean, to ocean marketplace or other third party marketplaces.
00:42:39.144 - 00:43:40.234, Speaker B: The data set, the data NFT is obviously fungible. Why? Because each data asset is unique. These data NFTs are, in theory, transferable, but that really depends on me as, for example, data, the base IP owner. Do I want to sell the whole IP to the data set? Am I? And I can only do that if I'm allowed to. You know, there might be privacy laws that prevent me from doing so. So in theory, it could be transferable, but transferability could be limited kind of by the publisher or by regulators if it would be illegal to do so. Data NFTs are not convertible.
00:43:40.234 - 00:44:56.694, Speaker B: The supply is question is not applicable because it's an NFT, it's not a currency. The stability question, for the same reason is not applicable. The rights that are attached to the data NFT is obviously the base IP, the property right to the base IP, but also any type of management rights that I might issue together with that base id role assignment for data token issuance, et cetera, but also revenue rights. I can ask kind of the data NFT issuer. I could decide to attach revenue rights to people who consume this data NFT via data tokens. We will see the data token roll later so I can define whether or not somebody who gets access to this data NFT can generate revenue through selling through using the data. These accessrite tokens expiry date or event data NFT only expires when a data publisher withdraws the data asset from the actively withdraws from the marketplace.
00:44:56.694 - 00:46:04.972, Speaker B: A data NFT. Maybe it also expires if it's delisted by a given marketplace operator because of, for example, copyright infringement or other legal reasons. Whether or not it technically expires is another question, but like it practically expires privacy some privacy features. Okay, when it comes to privacy, depending on the underlying, yeah, it has the same privacy features as the ethereum network and other supported protocols allow. Okay. The second important token type is once you have the data NFTs, you, as a kind of owner of the base IP of a data asset, can issue a number of data tokens. The purpose of the data tokens is to represent licenses to access the data assets.
00:46:04.972 - 00:47:00.584, Speaker B: The data nfts, right. So you're not selling the data, but you are selling the access rights to the data. Usually you're not selling the data NFT themselves. You might be in certain cases, as I said before, but usually you're selling excess rights to these data assets as a kind of data NFT owner. You can define the number of data tokens or access rights you attach to a certain data asset. You want to have it scarce, less scarce, expensive, less expensive, how many people should be able to access it. The data tokens are issued upon creation of a data NFT when the data publisher has to also define the number and type of life as licenses attached and the licenses are represented by the data tokens.
00:47:00.584 - 00:48:07.542, Speaker B: These data tokens are in themselves within a group, fungible within the group of the same data asset. You could attach 100 licenses to one data set and these 100 licenses would be fungible within that group. Data tokens are in theory transferable and transferability is unlimited. Unless a data publisher explicitly limits the transferability of these access rights. You might only want to grant access rights to your data sets to special people who are identified or institutions that are identified, maybe also for privacy reasons. If you want to only share, for example, data you have for research purposes, you might want to limit transferability. In this case, you might only want to identify the buyer of the people who can access that data should only be researchers, for example.
00:48:07.542 - 00:48:44.424, Speaker B: That would be an example in limiting the transferability. In theory, they can be unlimited. In practice, you could limit the transferability. Data tokens are convertible. When do they convert upon expiry date or expiry event? If you used an access write, the data token expires, or if the data token comes with an attached expiry date by the data token issuer, then it would kind of burn or get burned and thus converted upon expiry date.
00:48:45.004 - 00:48:55.092, Speaker A: Okay, I think, sorry. Okay. Vishnu raised the question, but I think it was now answered by this explanation of expiration of the data tokens. Yes, all good.
00:48:55.228 - 00:49:31.394, Speaker B: We will go. There is a section on expiration date and event a bit later if you still have questions. I'm happy to so, supply, in this case, it's not a currency, but obviously you have the supply of, or kind of the amount, let's say, of data tokens issued. This is determined by the data publisher. Depending on the data asset and the revenue preferences. It's very, very subjective, so the data issuer will define the supply. Stability is not applicable because it's not a currency.
00:49:31.394 - 00:50:09.918, Speaker B: It represents the data tokens represent an access right. So the price can be determined by the data publisher. And your pricing policy will depend on your, yeah, your personal revenue preferences or distribution preferences. Do you want many people to use it or not? The price could be zero. It could be for free. Rights that are attached are obviously access rights, usage rights, and actually potentially also revenue rights. I forgot that here, we should include revenue rights here.
00:50:09.918 - 00:50:58.936, Speaker B: If revenue rights, if you can build products on top of it and make money, obviously generate revenues on top of that data, expiry date and event. As I said, event one, burned upon consumption, event two, time based if defined by the license terms, and if time has expired. Does that answer the question? Any question regarding this? Okay, privacy. Again, same privacy features as the Ethereum network and other supportive networks. Ocean token. As I said, that was the first token that was issued in version one of the protocol it repeats the purpose. It has various purposes.
00:50:58.936 - 00:51:49.614, Speaker B: It's the internal currency and medium of exchange of the ocean protocol that can be used for network payments. However, I think that's interesting. It's not the only marketplaces or marketplace payments. It is not the only accepted token. I think the protocol founders what Trent told me, they really wanted to make it inclusive and not tie it to their token, not create any lock in effect ocean. The second purpose is to create a convertible governance token for data curation or for staking and slashing of prediction fees. So by locking the token, we will have a look at that a bit later.
00:51:49.614 - 00:52:47.194, Speaker B: Ocean tokens are issued upon. Initially they were issued upon, they were pre mined at Project Genesis, and then the continuous issuance, and then it was identified how many tokens over time will be distributed to which sub wallets and sub kind of stakeholder groups and purposes. So based on the initial issuance, there is a continuous issuance on a yearly basis in decreasing manner according to the pre determined issuance and distribution rules. The token is fungible in value. However, transaction history is public on the Ethereum blockchain, so discrimination, as we know is possible. In theory, tokens can be tainted, but in theory it's fungible. Ocean tokens are transferable.
00:52:47.194 - 00:54:01.524, Speaker B: Transferability is unlimited unless they are temporarily locked up for creating vocean for voting purposes. And we will look at vocean in the next slide. This means that they're also convertible? Yes, because by locking them, I convert the ocean token temporarily into a vocean token for voting purposes. Supply was limited to 1.41 billion tokens. It was, as I said, pre mined, held in various multisig wallets for various beneficiary groups, which are all allocated over time according to the predetermined ratio and schedule. Already said that ocean token was designed without a stability mechanism, which means that the prices determined by market forces, supply and demand on internal and external markets.
00:54:01.524 - 00:54:56.166, Speaker B: The rights attached to the ocean token, obviously it represents a property, right? It's an asset. It's the internal currency of the system and derived revenue rights. If the tokens are locked or saved, you can earn interest. We will see that later. You also have convertible voting rights. Voting, as I said, we will see that also later with vocean does not refer to protocol change voting, but rather voting over the quality of datasets. For example, there is an expiry event because 5% of the network taxes collected in ocean are burned on a regular basis.
00:54:56.166 - 00:55:34.614, Speaker B: This is to decrease the supply. These tokens are burned, therefore expire privacy is the same. Okay. Yeah. Vocean is the last one before we get to the ocean, because we discussed ocean tokens. Ocean tokens did not come with an inbuilt stability function. Eventually, I don't know, I don't remember recall exactly now, but h two o was designed to provide a native stable token for the ocean protocol system.
00:55:34.614 - 00:56:14.194, Speaker B: It's a stable unit. The purpose is to be a stable unit of account and medium of exchange to determine the price of data assets. It is issued upon locking ocean into an escrow contract. The token is fungible in value. It is transferable. It is convertible because it can be burnt and converted back into ocean. So it is created by a locking ocean, and it is destroyed and converted back into ocean when it's being burned.
00:56:14.194 - 00:56:56.628, Speaker B: The supply is limited, is not really limited. It's only kind of. It's limited to the amount of ocean token holders in existence and the amount of token holders willing to swap their ocean or lock their ocean against h two o. So it has a stability mechanism, obviously being a stable token. The mechanism is a crypto collateralized mechanism without a hard peg. It's more or less free floating around the price range. And I think they copied an existing stable token protocol, more or less the rules of.
00:56:56.628 - 00:57:17.534, Speaker B: Which one was it? I forgot. I forgot the name. I know which one it is, but I forgot the name. Rights. Obviously, it's a property rights, there no other rights attached to it. The expiry event is when being swapped back for ocean and privacy. Same again as ethereum.
00:57:17.534 - 00:58:17.468, Speaker B: Last but not least, the complex topic of vocean v oceans. The purpose of v oceans. There are different applications of vocations, but basically we can sum up that vocations can be used to vote on the quality of data sets. This is one big purpose range, and the other one is to engage in macroeconomic policies by locking them up. They're issued by locking up ocean into a vocean escrow contract for a predetermined time or event upon which you can earn interest. This is why it's also part of a macroeconomic policy of the network. The tokens are fungible, they are not transferable, and they are converted.
00:58:17.468 - 00:59:00.994, Speaker B: Yes, they are convertible because they are converted. When the voting period or the lock up period for earning interest expires, they're converted back to ocean. The supply is capped or limited to the total number of people willing to contribute their ocean tokens and the amount they hold or lock up their ocean tokens and the amount they hold. In certain cases, the ocean balance decreases over time until the lockup period ends. I think they changed that several times. So I'm not quite sure if that is still applicable. Stability is not applicable.
00:59:00.994 - 00:59:31.984, Speaker B: It's kind of a. It's not applicable. The rights attached are voting rights. Voting rights over quality of data sets in data farming programs. Not sure if the data farming. Yeah, this is changed so much also, but I think it's still applicable also revenue rights for ocean rewards in data farming programs. And they're passive rewards and active rewards, the expiry date or expiry events.
00:59:31.984 - 01:00:11.784, Speaker B: So event one is when voting period ends if it is used for voting on data sets. Event two is when staking, aka locking period ends if it is used for saving and or voting, which is always why I get confused because two things are mixed up. Kind of voting is mixed up with earning money. This is a design principle that some protocols do. I'm not a big fan of it, but this is my personal opinion. Yeah. Same privacy features as the Ethereum network and other supported networks.
01:00:11.784 - 01:00:29.724, Speaker B: So I hope this gave you some kind of an overview of the five different tokens and the different properties. I know it's quite a lot, but I urge you in the future to really take the properties and analyze them in detail. To sum up.
01:00:30.304 - 01:01:00.984, Speaker A: Yes, we have two or three questions. Maybe I can read it out, please. Vishnu asks, how is stability achieved here? And this refers, I guess, to the stable coin h two o. You're muted, Sherman. You're muted. I guess just check to unmute yourself.
01:01:01.484 - 01:01:07.548, Speaker B: I'm in a hotel room and in the next room there are three children yelling, oh, I see you.
01:01:07.596 - 01:01:09.856, Speaker A: We can't hear it, so it should be fine.
01:01:10.020 - 01:01:10.440, Speaker B: Okay.
01:01:10.472 - 01:01:11.608, Speaker A: Stability h two.
01:01:11.656 - 01:01:16.964, Speaker B: Yes. It's a crypto collateralized floating free floating without the hard peg.
01:01:20.064 - 01:01:21.044, Speaker A: Vishnu.
01:01:25.424 - 01:01:48.964, Speaker B: But if you're interested in knowing more, their website provides more interest and I analyze it a little bit in more. I don't think so. If you want to. Yeah, I don't know what exactly you want to know because we can now go into the details of stability mechanisms.
01:01:49.344 - 01:01:59.440, Speaker C: I mean, the price of the token, it remains stable over time. That's what is meant by stability. It's not as volatile as the.
01:01:59.632 - 01:02:34.940, Speaker B: Yeah, it is less volatile than the other ones. Okay. Yes. If you're interested in the topic of, yes, stable tokens, in the end are designed, there are different types of stable tokens and stable tokens. The purpose of stable tokens is to, you have kind of a target asset against which you try to stabilize it. Stability can be kind of hard pack, for example, to the us dollar or any other type of asset or ethernet or. Yeah, maybe it doesn't make sense.
01:02:34.940 - 01:03:11.694, Speaker B: Usually stable tokens are hard packed to some kind of a fiat currency that has less inflation. Right. And. But a stability is always relative because against which asset, right? And in this case, it's a crypto collateralized. Algorithmic crypto collateralized without a heartbeat, but free floating. They took the protocol, what is amines? Amine stable token. I forgot the name, but they basically took forked the rules of that stable token.
01:03:12.354 - 01:03:32.344, Speaker A: Yeah, I can drop a link. Okay, thank you. And the other was rather a comment, then a question now, what happens to the ocean token and properties of this token once the merger is complete and we have the ASI token in place?
01:03:32.804 - 01:03:52.360, Speaker B: This is something you should ask Trent, because I don't know. I analyzed the use case before I knew about the merger. Trent told me something big is happening. He couldn't tell me what. I honestly don't know. You will have to ask Trent or any other people involved in the Ocean Protocol foundation.
01:03:52.532 - 01:03:58.048, Speaker A: It will be certainly exciting to follow as this is all in the making now.
01:03:58.136 - 01:04:28.558, Speaker B: Okay, yeah, it is all in the making. Maybe to sum up also the use case. So if we look at the power structures we have. So I looked at four aspects, policy making power, voting power, executive power, and market power. Policy making power. As we said, only core team members of the Ocean Protocol foundation have coordinate all protocol evolution. Trent said that the aim is to.
01:04:28.558 - 01:05:33.736, Speaker B: Yeah, obviously, as more and more mechanisms solidify, less and less kind of policymaking or rule changes will be necessary so it can become as permissionless as possible. So they expect that the kind of this policy making power that the Ocean Protocol foundation has at the moment eventually is already fading and should eventually fade even more so that policymaking beyond the fine tuning of certain economic parameters should be impossible. Like just economic parameters that are. And then it is still unclear if and when this fine tuning of the economic parameters, by who will be executing this fine tuning. Currently it is being executed by the core team members of the Ocean Protocol foundation. Maybe in the future it will be a decentralized voting process. We don't know, right? I think they don't know yet.
01:05:33.736 - 01:06:43.646, Speaker B: And post merger things have changed anyhow. Voting power. So here the question is, who can vote and how has that changed over time? What is maybe worth mentioning is that initially ocean token holders could vote on the allocation of funds through a grantsdale that used to exist. This grants Dao doesn't exist anymore, only for the allocation of funds grants to certain projects. Today, ocean token holders have no voting power over protocol evolution or funding allocation in any kind of of way. And as I said before, and here, I can only quote Trent that he said that they want to minimize the probability. Oh, the reason that they stopped the grants Dow was that they wanted to minimize the probability that the purpose of the ocean protocol gets hijacked by special interest groups because the grantsdale wasn't running for a long time.
01:06:43.646 - 01:08:22.234, Speaker B: But even through this short period of time, they already saw that special interest groups were trying to hijack the direction of the protocol, or at least the grants, and this is why they stopped it in the end. So the idea is to limit voting power to data curation via vocean tokens. As for executive power, initially the founders and team members of Bigchain DBGenP in Berlin had a lot of executive power over the evolution and maintenance of the protocol. Community management, marketing, etcetera. As the protocol was deployed, stakeholders participating in the market mechanisms such as data publishers and data consumers, also received decentralized executive power over provision of data assets and consumption of data services or assets according to the predefined market rules. Ocean token holders have, as I said before, executive powers over certain macroeconomic policies because they can engage in contracting or expanding the amount of ocean and circulation via locking up their ocean or unlocking their ocean. And the mechanism here is to incentivize locking up of ocean tokens against interest paid.
01:08:22.234 - 01:09:54.088, Speaker B: Today, Ocean Protocol foundation has executive powers to decide on potential sanctioning of illegally uploaded data sets where once, but only once, an IP claim has been made. Yeah, so these are, as for market power, obviously the data markets kind of operators, data publishers and data consumers make and break the systems. They have most of the market making power. Ocean token holders have market power to enter or exit the ocean protocol ecosystem via buying, holding on to, converting or selling their ocean token tokens. And last but not least, if we look at what was Ocean's purpose and after so many years, what is the reality of what have they achieved and where might be challenges. So when it comes to token and market design for data assets and data services, I can say that ocean Protocol has succeeded at providing, from my point of view, a token design for representing data assets and data service that are publicly verifiable. They've really set up a solid kind of use case and best practice for IP exchange and IP access to IP over this token design and the token exchange.
01:09:54.088 - 01:11:02.214, Speaker B: They've been very successful, I think also in providing, in setting up this data exchange in a privacy preserving manner. They introduced a set of practical and necessary privacy mechanisms for the consumption of data services provision and consumption. In the end, traction seems to be the biggest challenge. When I spoke to Trent, the protocol is still lacking broader user adoption. That might be the case because they started as a general purpose data market, and data is a very broad asset class, and more specialized market mechanisms around specialized data classes are probably needed to emerge before ocean protocol will see significant user adoption. And I think that this is the reason why a few months ago, they introduced ocean predictor as such a specialized market for prediction data. Prediction for prediction markets.
01:11:02.214 - 01:11:57.614, Speaker B: The foundation is currently focusing, or at the time of interview end of last year, was focusing on supporting the community to build more specialized data market applications on top of ocean protocol to mitigate this challenge of lack of traction. So they have a great idea. It's very timely, it's well done, token wise, and when it comes to, on a technical level, from a privacy preserving mechanism. But they probably started too general and need to create more specialized marketplaces. The only real challenge that I see or criticism that I would have is the way they try to resolve the curation of quality data. This is my personal opinion. Right.
01:11:57.614 - 01:13:37.664, Speaker B: And I've discussed this a bit with Trent also, because the way they're trying to curate data in quality data is through their data farming incentive mechanism, for example, which they introduced for ocean market. But such, this mechanism, in my opinion, does not sufficiently incentivize the curation of niche data assets. For example, active data farming mechanism only incentivizes the curation of data sets that are expected to be popular and generate more traffic via financial betting game, right? Like you can basically bet what is going to be more popular, but what is more popular is not relevant in a niche market, right? So I think that the curation mechanism try to be, to try to lean on financial mechanisms. But quality data is subjective and it's not objective. So data that might be interesting and relevant to some data consumers or data scientists might not be interesting or relevant to others. So popularity is not necessarily a quality criteria, right? It's just a popularity criteria, it's a quantitative criteria. The problem for me is that the incentive mechanism of the data farming program operates on one objective function only, the popularity via the measure of data volume consumed.
01:13:37.664 - 01:14:24.536, Speaker B: DCV. Right. The problem might be mitigated in case, for example, enough specialized data markets exist or are deployed that cater to very specific niches. Because I think in the niche you can take data this criteria, but in a general purpose market, you cannot reflect niche interests with this criteria. So maybe this will be resolved, maybe not. But this would be my biggest criticism of ocean protocol. I have to mention, however, that the question of curation.
01:14:24.536 - 01:14:56.588, Speaker B: Nobody has or decentralized curation has not been resolved, neither in web two nor in web3. So it's not necessarily an ocean protocol challenge only. Right. It is a general challenge because curation, when a curation is subjective, it is really hard to objectify with one mechanism. And this is the biggest challenge, I guess. Okay, so I think this is the end of my presentation. If you're interested to read up on more.
01:14:56.588 - 01:15:11.154, Speaker B: In case you haven't, this use case is analyzed in more detail in the book dao and purpose. Thousand purpose driven tokens. It's one of six use cases analyzed. Okay, brilliant.
01:15:11.234 - 01:15:33.654, Speaker A: Thank you so much. We have one more question on the execution power. The principal dropped a note here. Doesn't this executive powers of sanction and illegally uploaded data by the founding members bend towards centralization? And is there any chance that their decision will be biased?
01:15:35.014 - 01:16:14.924, Speaker B: Yes. I mean, this is what I said in the beginning. This is the only thing that they can't really decentralize or haven't decentralized. Maybe there are efforts to decentralize this a bit more, but basically you have to see, I don't know how you want to decentralize a mechanism. You can automate it, maybe, but it's like you have to check IP law, right? So why should you do this in a decentralized fashion? I don't. Yes, it's a point of centralization in the protocol. I'm not sure there is a mechanism to decentralize this, but again, I think it would make more sense to ask the founders of OSHA.
01:16:14.924 - 01:16:17.096, Speaker B: Right.
01:16:17.160 - 01:16:27.500, Speaker A: And there was Suzy sharing a tweet from our account. Basically, I have to check. This tweet was about.
01:16:27.672 - 01:16:28.364, Speaker B: Yeah, right.
01:16:28.444 - 01:16:53.864, Speaker A: We published, just recently published a piece in the token engineering reads about the value creation loop. This is summarizing Trent's latest work and update on how to capture value in the ocean token ecosystem. Looking at the predictor case. So basically it's a complementary read to this analysis of Sherman.
01:16:54.494 - 01:17:01.634, Speaker B: Yeah, I mean, ocean predictor is a very special marketplace that they created after everything. Yeah.
01:17:03.374 - 01:17:13.554, Speaker A: All right, I think we still. We have around ten minutes left, so I don't know if we have time for some final comments.
01:17:13.934 - 01:17:20.666, Speaker B: Yes, any final comments? Questions? This is our last session. You're welcome to an extra.
01:17:20.790 - 01:17:23.054, Speaker A: Yeah, you raise your hands, which on your mic.
01:17:23.914 - 01:18:06.926, Speaker E: Yes, thank you. Again, I have a general question. It's actually the question I did like a couple of weeks ago. My impression is that incentive mechanisms needs to be really good, well designed because there are economic implications. So if you do it wrong, then there's a lot of money or cost. At the same time, I am seeing in this example that the token is evolving in so many ways. New tokens are created, now there is a merge.
01:18:06.926 - 01:18:33.378, Speaker E: So I don't really understand what is like the minimum viable model in order for you to actually start implementing because I feel sometimes that waiting it like, because it's a complex system, you will learn more by interacting with the, with the population. So do you have any observations, suggestion?
01:18:33.546 - 01:18:58.824, Speaker B: No, I think actually Trent is again, I only analyzed this use case as an external kind of expert. I can't say for sure because I think minimum viable. I think economic. You mean minimum viable or the minimum activity around a market? Are you asking for that?
01:18:59.644 - 01:19:17.270, Speaker E: Maybe not considering this specific use case. Yes, but like for example in a vow where you want to have like a purpose driven token, you obviously need to start interacting with the community to tweak usage.
01:19:17.422 - 01:19:56.180, Speaker B: Exactly. So I think I said that in the first or second session. So we have this criteria of minimum viable economy. Minimum viable economy. Because any system you start you need. It's probably going to be some marketplace around something, block space for blockchain networks, data assets for data exchange, and you need some, probably some minimum stakeholders who participate in this to make it economically feasible or to start some traction. That depends really on the use case.
01:19:56.180 - 01:20:53.764, Speaker B: I don't think there is an object, unfortunately. We said that also in the beginning. For example, in the case of an data exchange where you have data providers and you have data buyers, you need both. If you have nobody selling data, you might have a lot of people interested in data, but there is no asset they can buy. So I think rather than minimum economy, you have to look at enough supply of your initial asset class, whatever it is, and creating enough demand. So it makes it is more attractive for people to supply whatever asset, right. In a social network, the asset would be content, content curation content that you provide to a social network.
01:20:53.764 - 01:21:22.484, Speaker B: Obviously people will only be interested to provide to a social network if enough people are using it, right? And there are not exact numbers, but the balance needs to be right and you need to create a minimum noise and traction and activity in the beginning and then keep engaging so it grows. You know, I guess that is the magic, but there are no exact numbers. From my experience.
01:21:24.904 - 01:21:32.084, Speaker A: It would be so cool to have some. We don't that's why token engineering is fun and it's so challenging.
01:21:33.104 - 01:21:34.072, Speaker B: Another question.
01:21:34.168 - 01:21:54.194, Speaker A: Yeah. Gisa raised her hand. Yeah, I have a question regarding the verification of data. How exactly does this happen? So how does providers of data provide this data then? How does it get verified?
01:21:54.354 - 01:21:57.534, Speaker B: You mean verified if they own the base IP?
01:21:58.114 - 01:21:59.394, Speaker A: Yeah, for example.
01:21:59.474 - 01:22:15.374, Speaker B: But no, basically in ocean is designed in a way. You upload data. By uploading data, you claim that you have the copyright. I don't think you have to prove it. Right. But then if somebody.
01:22:17.474 - 01:22:18.146, Speaker C: Is of the.
01:22:18.170 - 01:22:39.734, Speaker B: Opinion that you infringed copyright, they can make a claim for that data set. That moment the data set goes into a purgatory process and into a purgatory process and is not available until the matter is resolved by the Ocean Protocol foundation.
01:22:41.314 - 01:22:46.694, Speaker A: Yes, I've muted you swiftly because there were some background noises, but feel free to switch on your mic again.
01:22:48.194 - 01:22:50.294, Speaker B: Okay, yeah, now I got it.
01:22:50.794 - 01:22:55.506, Speaker A: Okay, so ocean protocol verifies then the data.
01:22:55.650 - 01:23:01.930, Speaker B: No, they don't verify the data. They only check if somebody made a claim.
01:23:02.082 - 01:23:03.642, Speaker A: I made a claim if this data.
01:23:03.698 - 01:23:05.162, Speaker B: Was provided somebody else.
01:23:05.298 - 01:23:08.094, Speaker A: But how do we know that the data is incorrect?
01:23:09.434 - 01:23:20.786, Speaker B: Only if somebody makes a claim. Okay, it's a claim that, it's a claim that they don't own the ip and it's taken down. It's like YouTube.
01:23:20.930 - 01:23:21.578, Speaker A: I understand.
01:23:21.666 - 01:23:22.090, Speaker B: Okay.
01:23:22.162 - 01:23:25.574, Speaker A: So it's rather that somebody else says, this is my data.
01:23:28.444 - 01:23:29.344, Speaker B: Thank you.
01:23:35.964 - 01:24:40.164, Speaker A: There are so many more flavors to explore for the ocean protocol looking at this new data merger, the new plans, and the new potential for creating an AI economy. So I think next year we could run another round and could look at the ocean economy and detect entirely new aspects to it. For now, I think we are fine. It was a lot of content and I'm so glad to have you. Sherman, thank you so much for sharing this amazing analysis and sharing this framework. I'm sure this is super useful for so many when designing or analyzing tokens to structure it, there are so many aspects who analyze and it helps the overall total field of token engineering. So thank you so much for hosting this live track and contributing to the study season.
01:24:40.784 - 01:24:42.240, Speaker B: Okay, yeah, thank you.
01:24:42.272 - 01:25:21.864, Speaker A: I'll now move to discord and invite everybody to do the same to check out the list of recipients of the NFT. And again, we'll share more information how to receive this NFT and how to take part in the voting for the fellowship prize winner. The NFT gives you additional voting power. Don't forget this. And we'll also share more information where to get Sherman's books and how to follow up with her activities in the future. So thank you so much, Sherman. Thank you so much, everyone, for participating in this live track.
01:25:21.864 - 01:25:27.084, Speaker A: Thanks. See you soon at te academy.
01:25:28.504 - 01:25:29.364, Speaker B: Bye.
01:25:32.144 - 01:25:35.204, Speaker A: Thank you so much, everyone.
01:25:36.504 - 01:25:37.944, Speaker B: The recording has stopped.
