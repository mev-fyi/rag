00:00:00.090 - 00:00:03.230, Speaker A: Once again, I'll start recording.
00:00:09.730 - 00:00:16.270, Speaker B: In the token engineering discord, but I didn't see option for chat.
00:00:17.970 - 00:00:59.440, Speaker A: Okay. All right, welcome again, everyone, to the balancer simulations. We have somewhere an echo, so please make sure, everyone, you switch off your mic, yeah. Today we'd like to introduce the catcat model and simulations for balancer pools. This is a project kicked off by the token engineering community ethic hub. And we got a grant from Balancer and Powerpool to build this model and to run first simulations. And this is what we'd like to present now.
00:00:59.440 - 00:01:30.754, Speaker A: So go to the next slide, Raul. So this is our team, Vasili Sumanov, crypto economic research and member of the board at Powerpool. Raul Martinez, co founder and CTO, ethic hub. Angela, myself, modeling and ecosystem, and Andrew Chi, developer, Python and on chain data. And we collaborated. Next slide. Building this model that we want to introduce.
00:01:30.754 - 00:02:35.760, Speaker A: Now, our agenda today is we'd like to give you some more information about the general scope of this project. And before we dig into that, I'd like to hand over to Fernando and Marcus, who are here from Balancer. Then we take a closer look to the model, to the model architecture, the transactions we are including. In this model, we take a look at the available notebooks and first metrics, and then we take a closer look towards the research roadmap over the next couple of months, because this work for balancer simulation is just starting. So with this model, we are able to run a variety of simulations to verify new pool designs that will be relevant for version two, Balancer version two. And there are already a couple of questions on the table we'd like to explore with simulations. And then next step and call for applications for a newly founded research group.
00:02:35.760 - 00:02:47.250, Speaker A: Yeah, project scope. Briefly, four token engineers. Basili, Raul.
00:02:49.750 - 00:02:51.246, Speaker C: I'm in Zoom.
00:02:51.278 - 00:03:51.842, Speaker A: Right. And we had two projects with a need for running simulations and find out if a particular design works. Well. There are some background noises. Can you please hold on? Can you kick David, please? Okay, there somebody, can you please all switch off your mic? Okay, let me check. All right. Now, we had two projects on board Powerpool with dynamic weights changing and ethic hub with particular questions, in ethicub's case, how to build a safety module to cover loan defaults.
00:03:51.842 - 00:04:37.860, Speaker A: And so we had our research questions in mind. We were already familiar with tools we can use for token engineering to run simulations. And so we thought, okay, let's not only build these models and simulations for ourselves, instead, let's open source this model and tools as a starting point for infrastructure for token engineering, balancer pools and, yeah, let's share it with the community. And maybe now, Fernando and Marcus, I'd like to hand over to you to say some more words about balancer simulations and our joint project.
00:04:41.130 - 00:04:49.320, Speaker D: Yeah, maybe I can go first because I'll probably be shorter than you. More succinct, Marcus. Okay, Maria, so.
00:04:51.850 - 00:04:52.600, Speaker A: Wait.
00:04:55.530 - 00:04:57.350, Speaker D: There'S the dude dancing.
00:04:58.090 - 00:04:58.840, Speaker A: Yeah.
00:05:03.410 - 00:05:07.600, Speaker B: Wait, Maria, can you stop sharing screen?
00:05:08.370 - 00:05:16.818, Speaker A: I need to get this guy. Okay. Sorry, Fernando. Go.
00:05:16.904 - 00:06:08.100, Speaker D: Good. Yeah. So I was just going to say that it's a pleasure to be collaborating. I think we've always wanted for there to be more design, more modeling, more testing before people start things, especially in blockchain, where people can lose money. It's so important that we have the tools, the right tools to allow projects to do testing and modeling before they test in prod, which is unfortunately the case in many projects that we see here. And balancer is a very complex project and there's lots of people asking different questions, and it's always hard for us to say, well, answer questions. Every David is trolling us.
00:06:08.100 - 00:06:33.740, Speaker D: So, yeah, I think it's great to have one tool that the community is supporting and for free. I think it's great that there are those grants that make this possible. So, yeah, I just wanted to thank you guys for your amazing work. And maybe, Marcus, you have more to say as well.
00:06:35.950 - 00:07:12.950, Speaker E: Yeah, I think Fernando said it very well, especially with VQ coming on. We open a lot of doors to a lot of new flexibility with how Balancer's architecture is designed. And a lot of economic incentives and crypto economics will start playing more significant roles, which is something that you already see in the integrations that you discussed, Angela, when you opened the session.
00:07:14.970 - 00:07:15.286, Speaker F: The.
00:07:15.308 - 00:07:30.460, Speaker E: Kind of thing that we will see gain a lot more importance as a fundamental part of balancer, and we hope to be able to discuss all this in the research questions in the coming months, like Angela said.
00:07:37.890 - 00:07:44.478, Speaker C: Vasily, is this your screen we're being.
00:07:44.564 - 00:07:53.074, Speaker A: Trying to chase down? Thanks. Sorry to interrupt you. Marcus, go ahead.
00:07:53.192 - 00:08:12.380, Speaker E: No worries. I think I had said it all. It's exciting to be building this with you all. Thank you very much for supporting this, and I hope the token engineering community can dive into all this with us and build more together.
00:08:12.750 - 00:08:32.766, Speaker A: Thanks. Cool. Thanks a lot, Marcus. Yeah, we really enjoyed the time building the model. Thanks also for your support. I think it's a great step forward and we can't wait to continue this work. Now, let's take a look at what we have now.
00:08:32.766 - 00:08:46.180, Speaker A: After a couple of months of working on the first version. Raul, can you pull up the slides again? Give it a second. We will make sure to.
00:08:46.550 - 00:08:49.940, Speaker B: Someone is exploiting because.
00:08:51.990 - 00:08:52.462, Speaker G: Marcus.
00:08:52.526 - 00:08:56.150, Speaker A: No, I don't think it's another Marcus.
00:08:58.250 - 00:08:59.858, Speaker D: There are two Marcuses.
00:09:00.034 - 00:09:02.210, Speaker C: Yeah. Zoom, obviously somehow.
00:09:02.370 - 00:09:03.080, Speaker B: Yeah.
00:09:04.030 - 00:09:16.800, Speaker A: Wait, yeah, so we are trying to make sure to kick all people who should not share their screen. But now, Raul, can you please share the screen again?
00:09:17.810 - 00:09:35.762, Speaker D: I think you can deactivate it, Angela, for non hosts. So you can say that only hosts can share the screen and then you can give it to whatever. Yeah, whoever you want to be able to share the screen.
00:09:35.896 - 00:09:37.810, Speaker A: Only host. Only host.
00:09:38.150 - 00:09:38.986, Speaker D: Only host.
00:09:39.038 - 00:09:39.640, Speaker A: Yeah.
00:09:42.650 - 00:09:53.340, Speaker D: I think we published this link somewhere public and then trolls are getting okay now with our names just popping our names and trolling us.
00:09:54.030 - 00:09:57.180, Speaker A: Okay, Raul, can you share again?
00:09:58.910 - 00:10:02.860, Speaker B: Not promising. Nobody dancing is going to show or something.
00:10:03.330 - 00:10:35.778, Speaker A: Okay, it looks good, looks good. All right. Yeah, so we already had this and then next one more. Okay, so what do we have now? Balancer simulations. So we published a GitHub repository with a catcat model and we will explain more. What is a catcat model and what can you do with it? In a minute. This catcat model enables to run simulations.
00:10:35.778 - 00:11:50.110, Speaker A: It's a python package with several Jupyter notebooks to run simulations yourself and then also build on top of this model and adapt this model to answer on your own research questions. Next to it we have a Gitbook documentation, which is documentation around balancer simulation. So the core functions we implemented keep in mind at the moment, this is balancer version one, the balancer math, the notebooks, everything you need to know to plug in on chain transactions and price feeds, and then some more additional information left and right. We have a discord channel, I hope you already know it. Otherwise, make sure to join the channels. We will share information regularly. Updates, and last but not least, a medium publication where we published some first articles about simulations and of course plan to continuously publish more on the work that is ongoing.
00:11:50.110 - 00:12:35.322, Speaker A: All right, maybe one step back, Raul. So publishing this model and introducing this model is just the first step to establish token engineering for the balancer ecosystem. And what we are planning from now on, April until July, is establish a research group and you can apply to become a member of this research group. Go to tokenengineering.org te Academy and you'll find more information about the goal of this research group. How can you take part? How can you apply research roadmaps and so on. So make sure to check out tokenengineering.org
00:12:35.322 - 00:13:29.658, Speaker A: if you are interested to join this group. The first iteration will run over three months, and this should be a group that works on various research questions, applying models and simulations, and just grow a network of token engineers in the DeFi sector, particularly for balancer pools, but also for other AMM related questions. Okay, Raul, I think. Oh yeah. Some final words about catcat. So catcat is an open source python package that is basically a key element for the token engineering domain. So Python, you are building a digital twin of crypto economic systems.
00:13:29.658 - 00:14:18.510, Speaker A: You can apply various systems modeling approaches, and in addition to that, use this whole universe of data science tools and libraries to run simulations, to build agents, to run agent based modeling and analytics. So it's great that we have this tool and we are using it for this first model. So make sure to check out catcat. So it's cadcad.org, there's GitHub. There's also a discord community. If you're interested in all things related catcat, check out this repo.
00:14:18.510 - 00:15:18.910, Speaker A: Okay, and now let's talk briefly. I'd like to give you an overview on the general model balancer version one. So our goal was to build a model that is available and useful for many research questions related to balancer. So we always had in mind, okay, let's make these models modules that you can combine. And first, we replicated balancer version one, transactions and functionality in a Python digital twin, which is, for example, adding liquidity, withdrawing liquidity swaps. We have fees implemented and also weights and the ability to change and adapt weights. And in addition to that, our model covers also external prices.
00:15:18.910 - 00:15:35.430, Speaker A: And we will talk more about why we did that and how we did that. Now I'm handing over to Raul, who will now walk us through this model, this Python digital twin of balancer pools.
00:15:36.970 - 00:15:52.650, Speaker E: Okay, if I can interrupt real quick before role goes, I just wanted to say goodbye to all. And unfortunately we can stay in longer because we have another call at balancer. But again, looking forward to continuing this conversation with you all on the Discord channel. Thanks again for having us.
00:15:52.720 - 00:15:55.630, Speaker A: Thanks for joining and looking forward to the launch.
00:15:56.290 - 00:15:57.040, Speaker E: Yes.
00:15:59.330 - 00:16:00.110, Speaker D: Thanks, guys.
00:16:00.180 - 00:16:00.586, Speaker A: Bye bye.
00:16:00.618 - 00:16:01.150, Speaker D: Take care.
00:16:01.220 - 00:16:01.790, Speaker E: See you soon.
00:16:01.860 - 00:16:03.760, Speaker D: Bye bye bye. Thank you.
00:16:05.830 - 00:17:26.710, Speaker B: Okay, so, as Angela said, I'm Raul, I'm the CDO from ethic hub. And I'm going to introduce you a little bit on birdside view of how this model works and a little perspective on the philosophy of how we architected it, too. So basically, before the design goes, the situation we were trying to figure out first, should we do a liquidity bootstrapping pool. If we have a lending platform and a pool used to put token in the market for collateral and liquidity mining programs, a lot of what ifs start to happen. What if the liquidity mining is too high, the mining incentives? What happens if there is other defaults? What happens if there is a spike in people that are selling your token? You run into first thing. These kind of systems are more and more complex, so it's hard to do a simulation. Well, it's almost impossible to do it in an excel file, so you need something that has time in the picture.
00:17:26.710 - 00:18:32.400, Speaker B: Carcass is a great tool for that. And then what we are seeing that this is very new and it's a good idea, we think, to have the same as we have now, like money legos, to have simulation legos. So instead of doing an ad hoc simulation just for one case, we thought it was great and balancer and Powerpool to help us bring together this team to do this first piece. So the idea is to build modular reusable code. We plan, either the research group or us, to create a python package so you can import pool, and then in the future maybe you can import like arbitrage agent or liquidity mining program. And the ideas to have a lot of pieces that can help projects to build simulations. We made it easy to understand, at least we think so.
00:18:32.400 - 00:19:55.302, Speaker B: And we have a documentation, a gitbook, like it was any other software project. And we wanted to verify the mechanisms with real data, because we feel like this component, even if it's very ubiquitous, the amm, it's cool to have some notebooks to gain a better understanding of how they work. So this is a little bit of the flow of information, or the parts of the diagram. If you know catcat, you know, there's policies, there are state update functions and state variables and metrics that are changing over time. So first, we will talk about later. Our first model is basically reproducing state from a pool, but it's not only from our existing pool, but it's not only being plotting the output of what happened, it's trying to get the real inputs to see if the outputs were more or less the same. So for that, the policies here, as we will say later, this is the column where why the state is changing.
00:19:55.302 - 00:21:10.210, Speaker B: So in the first simulations, we don't have really behaviors, we don't have agents making decisions, we just have what we call action decoder that is going to take the integrated data from several sources, price data, the pool actions, what kind of trades or exits were in the pool and starts plugging it into the simulation. Every time step is going to be a different action and every action will have date time. So you can later do like time related plotting. And here, depending on the type of action, you will have, for example, a price update that will add the variable external token prices. You will have what type of change when this change was done, and the meat of the model here is the pool methods. This is where we ported basically what is in the banzer pool contracts in solidity to python as close in how they behave as possible. And we have all types of bouncer v one interactions.
00:21:10.210 - 00:22:11.942, Speaker B: Weight changing is a work in progress yet, but we have all the types of joins, exit, single asset joins, and multiple asset joins and exits. And two versions. You can know how many tokens you want to send, but you can doubt how many tokens you want out. And the same when joining, you may know like okay, I want to get one pool share. And the pool method will calculate how many tokens you have to send. So here in the first model that we said, the decision of what kind of method is going to happen is going to be a reproduction of what happened in the history of a pool. After every price changes, the relationship between the tokens in the pool are going to change because the balances will change.
00:22:11.942 - 00:23:16.570, Speaker B: So this will trigger two different calculations. It will change the spot prices, that is the trading pairs inside of the pool, and we will record how many tokens are accumulated in the pool as a fee, because this can help you. For example, if you have, okay, should I create a pool? What is the optimal fee? How much can I provide? How will this help my liquidity providers to not to cover impermanent loss? These kinds of questions. And here we can see if we start adding behaviors. For example, an arbitrage agent will be an agent. That is a policy that will track relationships between the prices in the pool and external prices. And if an arbitrage profitable arbitrage trade is detected, it will trigger a swap.
00:23:16.570 - 00:24:13.260, Speaker B: Or in the future you can extend this model to have like, okay, dynamic weight change. Something is going to alter the use one of these pool methods to change the balances or the weight, sorry, of the pools. And you can, for example, prototype pools that follow external price data, pools that react to sudden changes in volume, or even have some kind of backstop for your pool to exit tokens. There's a lot of interesting research that could be done with these kind of methods. And of course, you are free to add your agent. The model is open source, so just go to GitHub and fork away. I'm going to talk a little bit about everything.
00:24:13.260 - 00:25:03.050, Speaker B: Usually in calcul, or at least the models we saw before, tend to have one state variable for each variable you want to track. But we decided to, even if it is a little bit object oriented. For this, we wanted to keep the pool state variables together. This way it's easier to generate the initial states with a script, either from existing pools. Andrew will talk about that later. Or you can just generate an initial pool with fictional weights and balances. And it will help you to know if the relationship, if the pool is well constructed, because the relationship holds.
00:25:03.050 - 00:26:00.046, Speaker B: Also, we group this because balancer math modifies several up to n tokens, several state variables in one operations. So was like the correct way to go for us. Well, this also is in line with the idea to have this as a piece. So you could easily have two pools in the same simulation interacting between them, maybe arbitrage between pools or something like that. Or maybe you can want to do a router for trades. So grouping it may make sense in the policies. As we said before, this is the place where why the state changes are like the reasons for why we should trigger a swap or join or something like that.
00:26:00.046 - 00:27:02.420, Speaker B: In the beginning, the action decoder has three methods, three operation modes. This will be explained a little more, also by Andrew or in the documentation. But basically you can get the state from a pool and just plot the output. No operation here is just like based on the events that happen, you can see the output. So you can just use this as a data analysis tool, or you can get contract what the users of a pool actually with the exact method they use and how many tokens wanted to go in the pool. There is a difference. Well, simplified version is using the events or the theorem events that happens in the pool as both a little bit both as input and an output, because these events are the things that happen.
00:27:02.420 - 00:28:05.170, Speaker B: It's after the fact. But if you're interacted with uniswap, balancer, or every pool, you launch a transaction with some parameters and you have some security window of how much the price you want to pay, or the minimum or maximum tokens you want in. Because how ethereum works, you will have transaction reordering, or some transactions can go first. So if the trade doesn't have this kind of security measures, you could end with a very unfavorable trade. So here we wanted to have these three methods to see if it's going to be a difference between our model and ethereum in the way that of course catcat. At least yet that I know the time steps do not have transaction reordering. There is no concept of gas cost or priority.
00:28:05.170 - 00:28:41.198, Speaker B: So whatever happens sequentially is going to be included in the pool. And as you can see in our verification notebook, the conclusions is like between simplified and contractal. There is not much difference because you are getting in the entry sequence of events. So you are free to use both modes. And well, we are working on an arbitrage agent. As we said, the state of the functions is where the mechanism, how the state is changed, the rules. Okay, I want to do a swap.
00:28:41.198 - 00:29:16.238, Speaker B: What is the math applied? Which state variables will be changed, the spot price calculations methods. And I think, yeah, this is pretty much it. Andrew. Or do you have any questions regarding this? I've been talking for a long time, I don't know if I lost you. Please, anything. If you have any questions, feel free to ask. If not, Andrew, you guys can also.
00:29:16.324 - 00:29:31.106, Speaker A: Drop your questions in the chat. And we will have still time to answer any questions. Maybe Andrew. Now continue. And let's see. Happy to pick up questions later on.
00:29:31.288 - 00:30:11.406, Speaker C: All right, so, in order to compare the simplified mode, the contract call mode, with each other, we first had to get historical transaction data. Also to run the simulation. We had unit tests to test like that. The model followed the math of the smart contract exactly. But we still needed to plug in real historical data. Because we didn't actually know if we could assume exact in all the time. Perhaps there was a big difference.
00:30:11.406 - 00:30:48.490, Speaker C: So Raul sat down for a couple of nights and there's an anonymous log event which had no labeling at all. And he managed to decipher what it meant and the important information inside it. And yeah, that really helped us for our contract. Call action decoder mode. Next slide. So, as mentioned before, our simulation does not understand anything about ethereum. It just knows about actions.
00:30:48.490 - 00:31:32.650, Speaker C: And actions are like what happens to the pool. And I call them actions to distinguish them from events. Because they could be confused with ethereum log events. Well, they're kind of the same thing, but I just decided to call them actions. So join, swap, exits, fee changes, weight changes are actions. Now, even though the token price changing doesn't actually have anything to do with the pool itself, it's also an action because it's something that happens to the system. But let's say if you transfer your pool shares between accounts, that has nothing to do with the pool or the system itself.
00:31:32.650 - 00:32:37.146, Speaker C: So that is not an action. So on the right you have a sample action on the right side. This is a swap for 15,000 die for 25 wrapped ether. As you can see, it has a timestamp transaction hash and the denorms are the weights of the particular tokens in the pool at this particular time. The contract call section is derived from Raul's hard work in deciphering the balancer anonymous event that was emitted. And the action part is what the action decoder simplified mode only pays attention to. So if you activate contract call mode it will look at contract call and action.
00:32:37.146 - 00:33:39.410, Speaker C: But if you activate simplified mode, it will only look at action. So the action part is derived from Google Bigquery, which there are some SQL queries you have to run, which is much easier than going to an Ethereum archive node and pulling stuff and sorting it there. It was Marcus from Balancer who first clued into the existence of there being an SQL database for this stuff. But still the anonymous event was not there. The anonymous event that Rahul had to decipher for the swap exact amount in or swap exact amount out was not there. So we still had to connect to an Ethereum archive node to fetch that. The price data, it can come from Tradingview or Coingecko.
00:33:39.410 - 00:34:26.400, Speaker C: Coingecko. Both of them have limitations and that five minute data is not actually available after some like I think it's three months. So if you really want data, you really need to pay attention and pull it regularly yourself. Coingecko data in particular is a bit dirtier and I had to do some pre processing, tidy things up. Who knows if Tradingview is actually doing the same kind of stuff in the background. So after all this, I became a pandas expert again. It seems that every time I forget how to use pandas, here comes another project which requires me to relearn everything about pandas again.
00:34:26.400 - 00:34:40.040, Speaker C: I will have a blog post that details more on how the data parsing script was written in tokenengineer.com. Publish it later.
00:34:42.410 - 00:34:43.510, Speaker B: Next slide.
00:34:45.450 - 00:35:36.470, Speaker C: So here is a structure of how the data parsing script looks like the data is in a big SQL database. In Google Bigquery, some of the data is not actually there. There are really long SQL queries to derive the stuff that we need. Yes, for example the fees which are not exactly simple events that you can pull from. So we pull the SQL data, we take the initial state of the pool and that becomes one JSON file. Then we take all the events. The events like the log join, the log exit, log swap, beeple, Erc 20, transfer.
00:35:36.470 - 00:36:28.178, Speaker C: These are all events. Ethereum log events. Then we merge them together into one long list. And for each transaction hash which may contain multiple log events, we query the Ethereum archive node for the anonymous event, Raul's anonymous event, and then we transform all of these into pool actions. And once we have a long list of pool actions, we can then pull the pricing data from either trading view or Coingecko. Each of these had their own idiosyncrasies, so we need to further process their data a little bit. And then at the end we get price actions.
00:36:28.178 - 00:36:40.170, Speaker C: And then we interleave the pool actions with the price actions to finally get the historical transaction data input for the simulation.
00:36:44.610 - 00:37:38.170, Speaker A: Okay, any questions so far on the model architecture, how this is built? Just feel free to ask. Otherwise. Of course, feel free to check out the GitHub repo and also the documentation. All right, if you don't have questions, then I'll continue and show you around in the notebook. Let's see, let me pull up my screen. Okay, can you see it?
00:37:39.360 - 00:37:40.108, Speaker F: Yes.
00:37:40.274 - 00:38:32.828, Speaker A: Okay, so this is the notebook available via GitHub Balancer simulations. And it's just one example of a notebook containing the general Python model of a balancer pool. So the general model replicating all transactions for version one. And in addition to that, that bigquery and pricing data Andrew and Raul were just talking about for a particular pool. And this is this pool. It's an 80 20 wrapped eth die pool that we just took as an example of. Okay, here you have a real pool with real transactions that we plucked and injected into the data.
00:38:32.828 - 00:39:24.984, Speaker A: And now let's see what you can find out via using this model. So in the first part, you see we have some information on system specification that is then available via the gitbook documentation. Of course we have some dependencies that we need to load pandas and CadCAd, which I was talking about. This package, you need to run simulations. You initialize your pool, you import the pool transactions in these steps here you can see the JSON file we have produced using the parsing script we have provided with the model. Then we see here the state variables and the initial values. I'll come back to that in a minute.
00:39:24.984 - 00:40:15.150, Speaker A: Also, the token prices are also added to the model. And now we have the state update functions Raul was discussing. So we have the transactions, the on chain transactions. We are triggering the right method and balancer math in order then to update the balances in all the states in our pool, all the state variables. This is configuration of the catcat model. For example, here you can later on define Monte Carlo runs, you can define the number of time steps so the iterations of this simulation and some more parameters. We already can see that we have 49,000 steps for this pool recorded.
00:40:15.150 - 00:41:25.984, Speaker A: And then with this execution part, we run the catcat simulation, which I already did beforehand, and we end up with a table, a spreadsheet of the pool. The action types, the change date time. So of every transaction we see in this pool from onchain data, we have this information recorded. These are the token prices available via using these price feeds. We are tracking fees we are tracking then having the balances and the price defined by balancer math multiplying via us dollar, we will end up with a dedicated price of every token in the pool. And finally we calculate metrics such as TVL and D invariant. Now let's now run some of the analysis we've prepared here in this notebook.
00:41:25.984 - 00:42:52.812, Speaker A: So you can always pull this notebook from GitHub, play around yourself, fork it, and do whatever you want. First I need to of course need to load my libraries. Then I'll limit for the sake of demonstration, I'll limit the number of steps here and now. Let's keep in mind again, the time steps represent a state update, and state updates can be the pool functionalities like swaps, adding removing, liquidity exits, also initializing the pool. All these transactions trigger state updates and in addition to that, any update in a us dollar price we see from the trading view price feeds also triggers a state update, and all those actions result in a dedicated timestamp. Right? So one timestamp represents a state update can be a swap liquidity, add, liquidity withdrawal, and it can be an external price update. Now again, we have 80 20 rep e die pool 20% die means this is a pool that has way less die liquidity than you'd normally need to lend at the spot price initial values.
00:42:52.812 - 00:43:33.832, Speaker A: So we have less die and a lot of eth in this pool. This is why you have this 80 20% and let's check out the initial values. So pool creation, this is the first action on 7 December. We have zero generated fees. At this point we have 100 pool shares. So those are the BPT tokens. We have a swap fee pretty low, and we have the following tokens, die and wrapped ether with the weights as mentioned here in decimal, and the denorm weight.
00:43:33.832 - 00:44:26.700, Speaker A: This is the balance, the initial balance of die and this is the initial balance of wrapped ether. Right, the spot price is this price for of course infinitely little trade defined by the value function of a balancer pool and then token prices. Here is the US dollar price that we took from the price feed. Now let's first check out, okay, some key information about this pool. So the observation time here in this notebook in the simulation starts at 7 December. This was when this pool was initialized. So this real pool and our observation time now ends at 20 February.
00:44:26.700 - 00:45:19.820, Speaker A: Of course here the only limitation is what data you have available. So we have data available for this pool until 4 March. And of course it just depends on what data you are willing to plug into the model. But our observation time now 7 December until 20 February, 74 days and number of timestamps including price updates 44,022 transactions in this pool, real transactions so that we recorded from on chain data. Okay wait now first, now let's check out some metrics about this pool. First you probably would look at TVL. So if you look at pool happiness, TVL is always relevant.
00:45:19.820 - 00:45:50.488, Speaker A: Let's take a look at the TVL. And now this data is coming from, not from the chain, but from our simulation here. Looks awesome. TVL grew in our 44 timestamps recorded here. This is the y axis TVL in us dollar. Now this looks pretty nice. TVL growth all fine.
00:45:50.488 - 00:46:48.528, Speaker A: But of course if you are a token engineer, you want to take a closer look and see if this really reflects your pool healthiness. So let's now dig into the data and let's check out if this growth in TVL reflects properly the direction your pool is currently taking. Now we all know that you can have for a balancer pool, you can have three sources of growth. One is the US dollar value. And if you look back how the Ethereum value grew in the course of December, we can be pretty sure that our TVL growth is driven by the increase of ether us dollar value. Dai not that much since it's a stable token. In addition to that, of course, it's not only us dollar value, it's of course liquidity adds.
00:46:48.528 - 00:47:44.430, Speaker A: So any liquidity provider adding tokens to the pool will increase the overall pool value. And then last but not least, the fees that are charged for every single transaction like swaps or also single assets. Liquidity ads now let's check out what have been the sources for this growth we are seeing here. And you can for example explore now pool shares. So let's see if adding liquidity was the root cause of growth here. Now we started with 100 pool shares, this was the initial values and we end here in our last timestamp with 100.27 four seven meaning we didn't have that much liquidity added to the pool during the observation times.
00:47:44.430 - 00:49:26.812, Speaker A: Now, another element to explore the growth is not only the pool shares BPT token, but the value function itself, where we have the product across all b of t. B of t is the balances of all the token assets in the pool, in our case, wrapped ether and dai and w is the weight of this particular token asset. And now if you look at the TVL versus the invariant, you can compare the growth of these two metrics, where we again see the significant growth of TVL and the very moderate growth of the invariant. And this is also reflected by the growth in percentage where TVL grew almost tripled, whereas the invariant only grew by 2%. And overall though, the conclusion here is that growth in TVL was really mainly driven by the growth in us dollar value, and only to a very tiny little extent by liquidity providers adding value, adding liquidity to the pool. Now let's check out what actually happened in the pool via looking at the actions. So Andrew and Raul already talked about how to detect actions from on chain data.
00:49:26.812 - 00:50:15.672, Speaker A: And here in our simulations, you can analyze the type of transactions that took place. So we remember we had this observation time, December until February, and we clearly see that this pool is dominated by swaps. So look here, one pool creation makes sense, but then 22,000 swaps compared to. We ignore the price updates for a moment since this is not internal transactions. Okay, we have 22 swaps and only 40 joint swaps. So 40 events. When liquidity providers added a single asset, 25 joints.
00:50:15.672 - 00:51:02.844, Speaker A: So full basket liquidity adds 13 exits and eleven exit swaps and eleven exits. So there isn't that much going on in this particular pool when it comes to adding and removing liquidity. Whereas this pool is mainly used for swaps. And this can be different of course, for imagine we have pool for liquidity bootstrapping, where your key value drivers should be adding liquidity. Then of course you should have an entirely different picture. What's also possible is to map the action type to timestamps. As you can see, it here takes a moment.
00:51:02.844 - 00:52:15.376, Speaker A: So here, all the 44,000 transactions are broken down into what action type do we have here? And price updates have been very important here. But if we zoom in, as I did here, then we see timestep per time step, what actually happened in the pool? And you see, okay, the dark ones are swaps. So we have a lot of swaps. And in between ordered by the time information of every single transaction. We have price updates at external markets, price updates of the US dollar value of dai and wrapped ether and then swaps. And of course here we have a first information on, okay, whenever a price of a particular token is updated on an external market, of course an arbitrage opportunity opens up. And so these price updates can result in a swap, which is an arbitrage trade.
00:52:15.376 - 00:53:16.460, Speaker A: Here we have another swap. Again, price updates, a swap, price updates, a swap. And what we have here in this tiny step, or here in these larger steps, I zoom in a little bit, are then these particular activities who are increasing the invariant, in this case a joint swap. So somebody added liquidity to this pool and immediately the invariant was updated and was brought to a new level. Okay, so we see swaps, not swaps, joint swaps, adding liquidity or joints resulting in these steps. We have it here step, this should be an exit here, step that is dropping in the invariant. Here are steps, here are steps.
00:53:16.460 - 00:54:21.340, Speaker A: And so this gives you a picture of what's going on, what action type triggered, and the results on the invariant. Now adding value to the pool, let's wrap it up. Here are join events, liquidity ads, deposits, and then there's a second transaction that is adding value to the pool. And this is this continuous growth here without having these larger steps, a continuous growth and these are the fees that are continuously. A swap normally doesn't change the invariant, but adding fees to the pool value is continuously growing the value and then of course growing also the value of each individual liquidity share. And this is another source for growth of the invariant. And here, this tiny little picture is another way to look at the invariant.
00:54:21.340 - 00:55:35.664, Speaker A: Here we have the price curve. So you all know this amm curve showing how the price of token x and token y is correlated in the value function discussed beforehand. And normally if you only have swaps, you would only move up and down at this price curve, right? But since swaps are adding value via the fees, with every swap, you have a little bit different price curve here, the next one a little bit different price curve. Basically they are mainly changing the position and a little bit the curvature. But this is another picture that shows, okay, with different transactions, you are changing the invariant, the value function constraint, and thus the value in the pool. This is another way to look at this invariant. Okay, and last but not least, now we had, we checked out the TVL, the US dollar value in your pool.
00:55:35.664 - 00:56:42.520, Speaker A: We checked out the pool shares, we checked out how different transactions have an impact on the value in a pool. Now finally, still, there is one element you should explore when looking at the healthiness of a pool. And these are the balances themselves. Because even if the invariant grows, even if TVL grows, you still need to check out the balances to be sure if your pool is still healthy. And we can now run this tiny script where we then finally see, okay, here's the growth of the dye balances. Here's the growth of the wrapped ether balances. And by using these results outputs of the simulation, we can also see that despite the fact that this anti pool growth, it mainly grows in dai, and the wrapped ether balances decline.
00:56:42.520 - 00:57:44.830, Speaker A: And in a way, this makes sense. So people are using this pool for changing, for selling dai to eth. So selling dai into the pool and buying wrapped ether out of the pool. But in the long run, of course, this also means with continuous us dollar value growth of your wrapped ether and the balance decline, you'll end up with more and more and more dai in the pool. And for a liquidity provider, for impermanent or even permanent loss. Since once you have all your tokens in dai, or the majority of your BPT tokens representing only dai, then the growth potential of this pool is just exhausted. Okay, so these have been some of the metrics you can already find in our simulations and what you can do with notebook number one.
00:57:44.830 - 00:58:26.280, Speaker A: If you have questions, feel free to ask. Also happy to answer your questions in the discord. Any questions right now? If not, then I'll hand over to Vasili and he'll now introduce what's on our research roadmap, what we are currently working on, and also what's on our roadmap for the next couple of months. Vasili, your.
00:58:26.430 - 00:58:31.396, Speaker F: Yeah, yeah. Hi, Angela. Hi, all guys. So, can you hear me perfectly?
00:58:31.508 - 00:58:32.728, Speaker A: Yeah, sound is.
00:58:32.814 - 00:58:32.980, Speaker F: Yeah.
00:58:32.990 - 00:58:33.404, Speaker A: Yeah.
00:58:33.522 - 00:58:39.164, Speaker F: So can I please ask you to demonstrate the screen? Because I'm from phone, so I cannot demonstrate the screen.
00:58:39.202 - 00:59:08.786, Speaker A: Yeah, let me check. Oops. Okay, wait. Once again. Okay. Yeah.
00:59:08.808 - 00:59:36.330, Speaker F: I think that we can start with the first slide about dynamic. Mm. The next one. Yeah. So all stuff that was presented was mainly describing already existed pools with already existing data. So it is just like a framework for analyzing all the stuff inside the pool on atomic scale. So you can derive all the stuff, as Angela short.
00:59:36.330 - 01:00:12.774, Speaker F: And as Raul and Andrew said, that you can get some on chain data and process it. But what if you want to create something new on top of balancer, some pool that never existed. And for example, in Powerpool, we were focused on dynamic. And because I personally, I think that balancer is not only the protocol for just liquidity provisions. So for exchanging assets as we all understand it. So you can change eth to die and so on. But it is like the tiny use case from our point of view.
01:00:12.774 - 01:01:15.930, Speaker F: So balancer offers great flexibility to build some financial products much more complex than liquidity pools for exchanging assets. And one of big scope of application is portfolio management indices and some similar stuff. But classic IMM balancer pool is not a perfect solution for portfolio management as it was demonstrated know for example, traction of power pools in Mainnet. And we came up with the idea what if we will take IMM and start to change something inside it, because IMM is something static. So you have the environment and you play with it and you just have the mask behind the environment. But what if you can change the environment during the pool life and fix some problems based on this changing? So like dynamic MM is IMM that can adjust parameters during the operation. For example, you can adjust fees, you can adjust weights, token set or even curve properties.
01:01:15.930 - 01:02:08.534, Speaker F: And the main goal for researching this is that in our vision, it can decrease the impairment loss problem, improve capital efficiency, and mainly provide more benefits to liquidity providers. Because at the current moment, arbitragers can get some arbitrage gaps or some profits. But if each providers often end with a permanent loss, and they're not happy with that. So at the moment, we consider two types of dynamic MM systems. So open loop control systems and closed loop control systems. So, open loop control systems are dynamic mms that change some parameters based on external inputs, like for example token prices or TBL projects or any other on chain metric that you can get, for example in Ethereum. And closed loop control systems are the systems that change parameters based on some events inside the IMF.
01:02:08.534 - 01:02:57.018, Speaker F: For example, Kyber network, new JMM paper, for example, money swap with virtual balance, and for example Bancor V 2.1. That mark will describe after my talk. For open loop, we have our index v two, we have Bancor V 2.0, and we have dollar classic pools. Also, there are some other projects that also try to adjust something, mainly weights based on external input signals. So yeah, Angela, can you go to the next slide please? Yeah. So what do we need to build a model? At least a model for dynamic mm? So we need input signals, data.
01:02:57.018 - 01:03:28.306, Speaker F: I mean, it can be market cap, TVL, and any other metrics that you select. For this, we need parameter adjustment strategy. It is basically an algorithm. Why and how you will adjust parameters of IMM. For example, if you want to adjust weights, the conditions for weight adjustment. So how you will calculate new weights, how you will set them up, how fast you will change these weights, and so on. If we're talking about real systems, this parameter adjustment strategy influences pool controller that influences the pool.
01:03:28.306 - 01:04:20.102, Speaker F: Finally, and of course, you need to have a pool that supports all these parameter adjustment calls. For example, if you want to change weights, you need to be sure that your pool can change weights. Because for example, in balancer, there are smart pools and ordinary pools that you cannot change after you launch them. So you need also to consider this. So, on the figure, we can see changing weights in pool with three tokens with ETH, WBTC and Bal. This chart demonstrates how weights are changing in this pool. As I can see, from 30% to 20% for one token, and from 20% to 30%, from 30% to 40% and from 30% to 20%.
01:04:20.102 - 01:05:18.934, Speaker F: So this is like continuously we're changing block by block. So this is mainly what, for example, you will get based on some external signal. So, and talking about balancer v two, as Marcos and Fernando said at the very beginning of this call, it offers next level of flexibility in pool design. So, since the custody is separated from the pool logic, you can add any custom logic that you need for your pool, because you can make some smart portfolio that trades itself based on market caps or TVL or something like that, or you can make some synthetic financial products or anything else, right? So, basically, I think this research is especially important in scope of balance review, that I think will bring a lot of new products to the market. Not only just things for trading, but also things for portfolio management, asset management and even maybe new complex financial products. Angela, next slide please. Yeah.
01:05:18.934 - 01:05:55.960, Speaker F: So, dynamic research, roadmap and also implementation. So what we need inside our model to build dynamic waste changing, for example, so we need balance rm cut, cut model that is already presented and done. So it works. Also we need to implement arbitrage policy. And also we need to implement parameter adjusted policy. So, what I mean, for example, if we start to changing weights, it means that if we change weights inside the invariant, we change effectively token prices. And some arbitrage gap appears based on this.
01:05:55.960 - 01:06:34.898, Speaker F: And we need to understand how this will be traded. So, the arbitrage are the main force around imm. So, if you change weights, they will trade the gaps all the time. So you need to understand how many value they got from this pool, how many fees they paid, how LP shares behave, right? So, allocate prices losing some value or they getting some value and so on. So, in our first model, there will be external market prices, input data, for example, from trading view arbitrage policy. So it is like our model for arbitrage agents. So we understand.
01:06:34.898 - 01:07:00.822, Speaker F: So we got some unchained data, how arbitrage really work, what methods do they use, and make a digital twin of this arbitrage in our python code. So also we're adding waste changing strategy here. And waste changing reflects all these impute signals. So we have, for example, changing in market caps, we start to change ways. And after that we see how arbitrariors trade with this pool. So it is like a game. So you create a pool.
01:07:00.822 - 01:07:39.990, Speaker F: After that you add the weight training strategy, launch an arbitrage agent and just see how it works. And if you understand that result that you want to get, you can change parameters, or you can add some optimization function for some parameters and finally get what you want. And only after that you start implementing the pool. Because as it was said before by Fernandez, it is something about money. So people can lose money or just on arbitrage even without any. So, and the main research questions here is like analysis of dynamic MM pools, defining optimal weight changing strategies. Because as I said before, in dynamic weight changing, there are a lot of questions.
01:07:39.990 - 01:08:33.062, Speaker F: So how change weights, what is the trigger, what is the rate of changing weights and so on and so forth. So, it's like a little universe inside balancer, imm, basically. Also, the optimal fees in IMM is big question, because for samples, for example, you need big fees for samples, you need low fees and even dynamic fees, for example, to get more volumes if volatility is low. But prevent some arbitrage rates if volatility is high, and force arbitrarily to pay for that. And finally, we want to get to closed loop systems modeling. So, to get some algorithms for adjusting weights based on IMM as a price sensor, for example, somebody made a trade, balances, change it, and you can adjust weights right after that. And you don't need oracles in this case.
01:08:33.062 - 01:09:06.494, Speaker F: You don't need to rely on some external price sources. You just use IMM as a price sensor to execute your weights chaining strategy. So, this is also in scope of the research. And for the last, I want to say that implementation in Powerpool. So it already works. And this is special digitalized oracle with poker agent that calls the oracle gets some time weighted average prices from Uniswap and sushiswap. And after that calls DMM strategy contract.
01:09:06.494 - 01:09:45.120, Speaker F: This strategy calculates new weights. For example, if market cap of some assets grew a lot, it can increase weights of this asset and decrease weights of all other assets respectively. So after that he signed transaction for controller contract. This contract controls the pool and can for example, and have an ability to change weights or some other parameters inside the pool, or even stop, join, exit and swap events in some cases if it is required. So this controller contract finally initiates the weight changing inside the pool. So this is like a system that maintains itself. It works without any manual input, just updating weights all the time.
01:09:45.120 - 01:10:25.574, Speaker F: At the moment it is once per day to save on gas, because every transaction costs gas and it costs a lot. And there are three ways of changing strategies implemented. Market cap adjusted weights as I said before, TvL based ways changing it is important for your v one wall. So it is an index of LP tokens of stablecoin wall, so not of protocol tokens. So these tokens are not volatile, they're just accumulating interest. And IMM changed weights according to TVL inside each wall to reflect the community choice to have a portfolio that copies all this TVL distribution. And the third strategy is market cap weight rebunder.
01:10:25.574 - 01:11:15.354, Speaker F: It is something new, I can say. So basically IMM itself trades tokens. For example, if you need to change weight significantly and you understand that arbitrage will get a lot of profits and liquidity projects will lose all these profits, right? Because if somebody earns, somebody lose, and you can just trade some tokens to other tokens to adjust balance according to new weights by IMM. So trades itself and it remains all profits inside the pool and decreases this value drain from arbitrage and saves liquid price capital. But at the moment, these strategies, of course, they require more solid basis based on simulations. And this is what we are working on right now. So thank you all.
01:11:15.354 - 01:11:16.714, Speaker F: And if you have some questions, just.
01:11:16.752 - 01:12:33.326, Speaker A: Ask any questions on these new strategies for weight changing or also fee optimization. Feel free to also check out the article introducing balancer simulations on medium, which has a great overview on the current strategies that have been introduced from various protocols and projects. So this is also a cool source. And then also feel free to reach out at any time in our discord. All right, if you don't have questions now, I'd say Mark, mark will introduce us to his latest research. So we thought, okay, would be nice to have somebody from another protocol, in this case Bangkok, showing how other projects are using simulations or verifying their assumptions on new designs. And Mark has been.
01:12:33.326 - 01:12:38.360, Speaker A: So join us tonight. Right mark, feel free to share your screen.
01:12:40.410 - 01:12:42.438, Speaker G: Yeah, can you guys see this?
01:12:42.524 - 01:12:43.960, Speaker A: Yeah, we can see it.
01:12:46.970 - 01:13:22.820, Speaker G: Let me see. Okay. Yeah, so I thought I'd give myself a quick introduction because you're around here. I don't know anyone in these channels, so my name is Mark. I've been working since January for Bancor as a research lead. I actually don't have any traditional finance or programming background. I came from total synthesis, so I was a synthetic organic chemist for 14 years, and during 2020, I basically decided to finally learn the blockchain space.
01:13:22.820 - 01:14:15.620, Speaker G: Over Christmas, I developed a concept for liquidation free leverage on the Bancor network that we now call the Bancorp vortex. And shortly after publishing that, I was offered a job working for Bancorp. And I'm now developing sort of a range of different products to sort of add to Bangkok's core offering. It's not clear to me. It wasn't clear to me until just now just how familiar everyone was going to be with amms in general and Bancor specifically. But I have assumed that everyone is very familiar with amms, but knows nothing about Bancor. Over the course of the presentations that I've seen, I've decided to sort of change direction a little bit, which is fine.
01:14:15.620 - 01:14:44.830, Speaker G: I only woke up a couple of hours ago and had to throw its presentation together in quite a hurry. It's currently 05:00 a.m. In Melbourne, so please forgive me if I'm not quite as articulate as I usually am. I'm functioning on very little sleep right now. It's been quite a week. But for Bancor, I tend to use characters from the grainy universes. So we're going to use Bender here as a representation for the Bancorp protocol.
01:14:44.830 - 01:15:35.834, Speaker G: And I need to introduce the idea of a dynamic token supply. So Bancor is no longer fixed supply. For the first leg of its history, it had 69 million tokens permanently in circulation. But with the launch of V 2.1, following the failure of V two, we've introduced an elastic token model, and this enables some really important features that we think are going to help take Bancor to new markets. So the first feature, obviously, of a dynamic supply is the ability to mint tokens, but then it also needs the ability to destroy those tokens later in order to maintain a sustainable monetary policy. So the first thing that this allows Bancorp to do is to offer single asset exposure.
01:15:35.834 - 01:16:30.234, Speaker G: We think that this is something that's still extremely undervalued in all of DeFi, the ability to make profits just by providing a single risk asset and not asking users to take exposure to any of the other assets. So in protocols like Sushiswap and Uniswap. I'm sure we're all familiar with the idea that you're forced to buy Ethereum and provide liquidity in that asset as well. At Bancor, we've tried to find a way around that so that people that are super enthusiastic about one token and refuse to own any other assets can still participate in DeFi. So this is kind of how this works we can imagine. Tony here is a ethereum enthusiast and he refuses to own any other assets, but he wants to still participate in liquidity provision. So what we allow the protocol to do in this instance is to mint BNT tokens in order to provide the other side of the pool.
01:16:30.234 - 01:17:37.590, Speaker G: This is a flexibility that's unique to Bancor, because BNT is the base asset in every single one of its pools. You can't do this on uniswap or sushiswap, for example, because those protocols have no control over Ethereum, and if they did, it would start to conflict with Ethereum's tokenomics. So this is something that's kind of built into the BNT design and helps to facilitate these features. So the Ethereum enthusiast and the protocol will both contribute an equal number, or an equal value, I should say, of tokens to the pool. So this is technically an inflationary event. When these tokens are minted and you wanted to check the token supply on Etscan or coingecko or something, you will see the number of bnts in circulation increase. But I'm sure from your understanding of amms you should realize that this is kind of a fast in order to actually get those tokens, the BNT tokens that it minted out of the pool and onto the open market, especially if you wanted to get all of them, would actually drive the BNT price to infinity.
01:17:37.590 - 01:18:37.626, Speaker G: And so it's very, very difficult to make a coherent argument for why this is actual inflation. The minted tokens that are supplied to the pools are not in circulation. They are available for sale, though, but they don't create any selling pressure. One way to think of it is that they respond to buying demand only, but they don't have to be sold in order to discover a price. So this type of inflation is a new type of inflation that there isn't really any good vernacular to describe it yet, and so we're helping to develop that language. There is a way that even though it's not true inflation, there is a way that we can deflate it, and that is if a BNT enthusiast, in this case represented by Martin from the Simpsons when he comes to provide his single asset exposure in BNT, the protocol will remove its own BNT and destroy it, while Martin provides the BNT in place. So this is why the elastic supply is kind of important.
01:18:37.626 - 01:19:29.654, Speaker G: The protocol's bnts become something like a placeholder that sort of reserves Martin's spot while he's purchasing, while he's realizing that position. The other thing that the elastic supply allows us to do is to provide impermanent loss protection for all liquidity providers, both BNT, ethereum, Link, USDC, or basically everything on the network. This is a gifted status and it requires Dao approval. We don't offer it to just any token. There are pretty significant security exploit risks associated with this mechanism. And so we have to go through a pretty thorough vetting process to make sure that, for example, there's no rebasing mechanism which screws with the contracts. So ampleforth, for example, will never be whitelisted on Bancorp.
01:19:29.654 - 01:20:23.930, Speaker G: We need to make sure that there isn't like one single whale wallet that controls a ridiculous amount of the supply. So, for example, if XRP was an ESC 20, we never would have whitelisted that. We're never going to whitelist hex or something like that because of the rug pool exploit effectively becomes magnified. If someone knows ahead of time that they're going to perform it, they can buy BNT and stake that as well, and basically double the profits of performing these types of things. We also, in general, would like to not whitelist anything that SBF alimator holds for the same reason. Okay, so for impermanent loss protection, this is the part that I think most people struggle with, and it is a very complicated part of the tokenomics at BNT. So we're going to come back to this concept that the protocol and some token enthusiasts are both going to contribute liquidity to the pool.
01:20:23.930 - 01:20:52.142, Speaker G: Then over familiar the familiar impermanent loss situation pays out. There's some price divergence, but it actually matters in our case which direction that price divergence is in. So let's consider first the case where BNT is going up. This is the situation that we're currently in. BNT, I think, is currently about 14 x higher than it started when B 2.1 was launched. But that doesn't mean that there is an impermanent loss.
01:20:52.142 - 01:21:29.630, Speaker G: So even when BNT is mooning, it still means that there is a liability to the contract that has to be covered. So when this happens, because the BNT is being sold out for the underperforming token in this case, Ethereum. It means that the protocol actually then takes a stake in Ethereum. So we now have hundreds of millions of dollars on the protocol that's not owned by anyone. And this is true of the USD, stablecoins, ethereum, rap, bitcoin, everything that BNT has outperformed. In a way, this is money in the bank. This is unique to Bancor in the sense that it has now this huge hedge fund in all of the different cryptocurrencies that we currently insure.
01:21:29.630 - 01:22:30.450, Speaker G: But if the Ethereum liquidity provider wants to withdraw his liquidity, and we are insuring his position against impermanent loss, then the protocol is forced to liquidate at least part of its position to reimburse the liquidity provider for their losses. So these ethereum tokens would be returned in the case where BNT price is going down. So underperforming the market or a particular asset, the situation is slightly different. The best way to understand this particular situation is to just assume that the protocol and this one liquidity provider own 100% of the liquidity between them. Obviously, this is insane. In reality, we have thousands, sometimes tens of thousands of liquidity providers. But if BNT did underperform it in the case where they do control 100% of the liquidity between them, then there simply isn't enough ethereum in the protocol to reimburse the liquidity provider.
01:22:30.450 - 01:23:07.230, Speaker G: Because of the impermanent loss, the BNT value is also going to be insufficient. And so the protocol is forced to mint additional BNT. And so this is a true inflationary process. This is new bnts that are being created to supplant the US dollar loss that the liquidity provider has suffered. And this would all be returned to Tony in this case. So it is possible, like there are these edge cases where some liquidity providers who are providing single asset exposure will be reimbursed in BNT. We haven't actually seen it yet, and I doubt that we ever will.
01:23:07.230 - 01:24:09.230, Speaker G: There are probably going to be some fringe cases in the future where something like this could happen. But you have to be so unlucky, you have to basically be the last person in the liquidity pool to withdraw your funds, and the liquidity pool has to basically cease to exist. So while it can happen and I have to pay, that, it's just so unlikely. So what we have created is what is, in the financial lingo, is called an american perpetual option. It's an american option in the sense that you can access the value of it at any point before maturity. And a perpetual option in the sense that it has no expiry date. And so this actually is a unique product in defi, most liquidity providers don't know that they're actually in an option straddle, but on bancor, you can actually just be exposed to the single risk asset.
01:24:09.230 - 01:24:53.694, Speaker G: And so this is something that we think that is still underappreciated about Bancor. And as people start to warm up to the idea of it and realize how profitable it can be, that we're going to see much more significant traction. Having said that, you currently can't really get a seat in Bancor. The Dow controls how much of these options it sells by limiting the inflationary capacity of BNT. And so at any one time, if you wanted to try and deposit your link, for example, to the link pool, you'd find that there's zero space. If we wanted to increase the TvL twofold, we could just mint a huge amount more of BNT. But we have to be very careful that we're not attracting too much unproductive capital.
01:24:53.694 - 01:25:45.890, Speaker G: Amms in general are extremely capital inefficient. I don't think that's a controversial statement, especially within this group. But we are working on ways that the pools can become much, much deeper than they are and still have access to other revenue sources, sort of similar to what balancer is doing with their asset management system. And it looks like people are interested in b three, or at least the market makers are interested in uniswap b three. If the market wants an order book model, we can easily build an order book model. So there are things that we can do to increase the trade volume and profits for lps using this system. So to understand exactly what the cost of the protocol is of offering these perpetual options, it's really not very difficult math.
01:25:45.890 - 01:27:03.750, Speaker G: The first equation here, p, is just the price of the token that Tony here has provided. So the price of Ethereum at the time that he chooses to exercise his option, and p, zero is the price that he deposited it at. So once you have that x term, then it's really trivial to calculate the impermanent loss. And this is essentially the cost of the protocol, what you have to appreciate about this. And because at face value, often it seems that this can't be sustainable, right? Like, how is it that we can absorb all of the impermanent loss into the protocol and take all of that risk away from lps and still maintain a stable token supply? We have to realize that the value of Tony's option is only increasing as the square root of time, whereas the fees associated with that are increasing with linear time. And so this means that the actual overall position is you can defeat the impermanent loss if the stake remains in the pools for long enough. So you can invert this from the protocol's perspective, the cost of offering this option to Tony is dropping off as the square root of time, but it is also earning fees alongside Tony.
01:27:03.750 - 01:28:02.974, Speaker G: So both of these liquidity providers, the protocol and Tony, are both earning fees, but the option cost to the protocol is asymmetric. So you can easily run simulations to work out based on what the APY is on a certain pool as to how long you should force a liquidity provider to remain there in order to enter a breakeven situation for the protocol. So this would be neither inflationary or deflationary. We should also point out that when you are entering, if you have failed to defeat the breakeven point, it's not like the protocols failed. It just means that there's a slightly higher emission of BNT onto the open market than we would have liked. But I think the crypto community has kind of got the wrong idea about inflation and deflation in general. It's not the case that when a token supply is inflating, that its price has to depreciate, or the other way around either.
01:28:02.974 - 01:28:41.500, Speaker G: We've got dozens of examples now where burning tokens has achieved literally nothing in terms of the token's price appreciation. So I tried to keep this short. It's super abstract, thousand foot, because I thought I was only given something like ten minutes to talk. But if you want to see that the full economic and financial analysis for this, we do have an economic and quantitative paper that deals with everything in detail. But if I may, I actually wanted to talk a little bit about some of the simulation work that I've been doing as well.
01:28:42.110 - 01:28:43.100, Speaker A: Oh, yes.
01:28:43.470 - 01:28:44.906, Speaker G: Do we have time, by the way?
01:28:45.008 - 01:28:50.974, Speaker A: Yeah, we are already a little bit overtime, but I would love to take a look at it. So, two, three, go.
01:28:51.092 - 01:29:52.802, Speaker G: Yeah, sure. So one of the things that you need to appreciate about Bancor is that it is extremely difficult to simulate because you can no longer deal with things at the pool level every single time a liquidity provider provides liquidity. That's a unique position that we now need to track, and we need to know the price that they provided it at, because this is the option that they bought. And this means that the impermanent loss associated with each one of these positions contributes to the overall impermanent loss. And if you try and aggregate it into just the pool, overall, it becomes completely meaningless. And so, at the moment, because we're still using pool tokens on chain data for this is just impossibly difficult to analyze. We've got a network upgrade coming out soon that should make these going to improve the composability of the protocol and also drop the gas fees a little bit, which is good.
01:29:52.802 - 01:30:45.506, Speaker G: But more importantly, it's going to make our token supply much easier to analyze. So, yeah, I also use Jupyter notebooks. Unfortunately for this particular notebook, I'm going to be unable to explain the context of it, because this is for a secret project that hasn't been announced yet. But I can still show you some of the ways that I'm handling some of the situations, because a lot of it is very similar to the kind of stuff that was presented already, but maybe some things that I can contribute. So, for example, one of the things that I choose to monitor is the profits made by arbitrageures over time. And one of the reasons why this is important is that we know that all of those profits come from the protocol. Right.
01:30:45.506 - 01:31:36.466, Speaker G: Every single time that an arbitrage is making money, that's the protocol, or a protocol losing money. There is a misnomer, I think, in the Amm space, that one protocol can be sort of like the target of the arbitrage and another one can be the beneficiary. That in reality, both of them are losing. The arbitrator would never execute a trade that didn't profit them. So in these cases, I'm running hundreds of millions of transactions at a time, and this particular simulation takes sometimes a few days to complete. But I'm also exploring edge cases here. So, for example, this is a situation where the price of bb and t basically drops to zero, starting at a price of six and coming down to 0.5.
01:31:36.466 - 01:32:07.930, Speaker G: Because we need to make sure that if these situations occur and they have occurred. Right. We can run historical price data through these simulations and see what happens to the token supply. We need to be certain that these kinds of black swan events won't kill the protocol. So we stress test these with extremely challenging situations. That's not the right one. Let's try this one's.
01:32:07.930 - 01:33:24.050, Speaker G: Okay, so some of the other things that I have to monitor are. Let's see what I can actually show here. Yeah, so one of the other things that I do is to make sure that things that we know should be true from basic amm math. That, for example, if the price oscillates around a certain region, but returns back to where it started, that there should be zero impermanent loss on a pool. And this is really interesting because it actually flies in the face of something that I said before, which is that if you track the arbitrage profits, that you can measure your impermanent loss, and in this case, you can't. And there seems to be this weird paradox that occurs where if you do end up in a price oscillation system, where you return back to where you started, the arbitrage of profits are actually measuring money extracted from somewhere else on the market. And so this is actually something that I'm building into the simulations now, which is not only trying to simulate the bancor system by itself, but also what is the influence of what's happening on Bancor, on centralized order book systems and the other way around.
01:33:24.050 - 01:34:20.358, Speaker G: So trying to capture these kinds of market behaviors, I think, are really going to be important for even just the academic sense, right. Of understanding how amms perform and what their role is in greater financial markets. And I think that as DeFi sort of matures, if it becomes a competitor to traditional financial products, that this level of understanding of how different parts of DeFi are interacting and what their influence is on the transfer of value and losses and profits across the system is going to be critical. So, yeah, I'm really looking forward to once we've announced what this simulation was actually built for. I'm really looking forward to being able to tell you guys more about it. And I apologize that I'm kind of teasing you with it, but, yeah, all in good time. I promise that this is coming out relatively soon.
01:34:20.524 - 01:34:26.040, Speaker A: Yeah, we'll definitely get back to you. Mark, thanks for sharing today.
01:34:26.670 - 01:34:27.930, Speaker G: Yeah, my pleasure.
01:34:28.270 - 01:35:13.398, Speaker A: I think there has been already people wanted to take a look at the paper you've mentioned, Bancor research paper. So feel free to share it in the discord. And, yeah, for sure, we'll pick up this topic again once you're ready to share and in the research group. So let's wrap up this call. Thank you to everyone attending this call. Thank you for staying with us through all those different topics and new simulation approaches. And I hope you now call your interest and check out the research group.
01:35:13.398 - 01:36:08.250, Speaker A: As mentioned, we will start end of April, and it will run, the first iteration will run across three months. So we will gather a group of researchers, it can be individual people who are interested to dig into DeFi simulations from economics, from an engineering background, from finance, of course, from computer science. So feel free to apply. I've dropped the link in the Discord channel. And then, of course, we'll collaborate with projects like Bancorp to discuss the research topics on the table, the interconnectedness of DeFi money legal and make progress in this space and make progress in our infrastructure and knowledge around how to make sure that all these models are sustainable and robust. All right. Thank you all for being here.
01:36:08.250 - 01:36:12.650, Speaker A: Thank you, everyone who presented, and see you at the discord.
01:36:13.870 - 01:36:14.806, Speaker F: Thank you, Angela.
01:36:14.838 - 01:36:15.610, Speaker G: Thank you, everybody.
01:36:15.760 - 01:36:16.780, Speaker A: Thanks, everybody.
01:36:18.290 - 01:36:19.770, Speaker F: Bye bye.
01:36:19.930 - 01:36:22.554, Speaker C: Bye bye.
01:36:22.602 - 01:36:23.790, Speaker A: Thanks for joining.
01:36:37.200 - 01:36:44.690, Speaker F: Yeah, Angela, by the way, did you see the TVL of Bancorp before 2.1 and TvL of Bancorp after 2.1?
01:36:45.620 - 01:36:46.736, Speaker A: Tell me.
01:36:46.918 - 01:37:24.200, Speaker F: Yeah. So at 1 January 2021, TvL of Bancorp was $125,000,000,000 and now it is 1.7 billion. So looks like this implement loss insurance is like $1 billion idea and implementation. So this is, I think, why it is so market votes by TVL. Right? If you build something cool and really fulfill the needs of liquidity providers, who are the main source of liquidity, obviously you will get huge TVL.
01:37:25.260 - 01:37:43.440, Speaker A: Awesome. Yeah. These elastic supply and also the various approaches how to tackle connectedness like this. Very much so. We'll get back to that soon, I'd say. All right, see you all on discord.
01:37:43.780 - 01:37:44.620, Speaker F: Yeah. Bye.
