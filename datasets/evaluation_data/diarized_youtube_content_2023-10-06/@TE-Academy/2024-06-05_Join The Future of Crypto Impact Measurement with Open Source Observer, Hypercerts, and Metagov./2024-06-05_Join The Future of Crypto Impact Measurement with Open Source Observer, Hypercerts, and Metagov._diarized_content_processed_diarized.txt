00:00:02.200 - 00:00:06.598, Speaker A: How is sound quality? You guys all seem very good, but is my sound quality okay too?
00:00:06.726 - 00:00:07.434, Speaker B: Perfect.
00:00:13.574 - 00:00:14.634, Speaker C: All right, done.
00:01:06.254 - 00:01:34.162, Speaker B: Hi, everyone. Welcome to today's meeting discussing the future of impact measurement. We have a number of really cool guests here today. Jolke from hyperserds. Great to have you here. Eugene from Metagaf joining us in a minute and Carl from open source observer. And I'm Angela from token engineering Academy.
00:01:34.162 - 00:02:15.644, Speaker B: Yeah, and today, big topic is how can we quantify impact in our crypto communities? How can we measure it, how can we make it accessible for evaluation and then ultimately land at rewarding impact in our communities and ecosystems? And there's a lot going on in the space over the couple of months. We are all working on various projects and I think great chance to share it and discuss it. And I think we should open with a brief introduction around some words about who you are, what's your project, what you are currently working on. Holke, would you like to start?
00:02:16.864 - 00:02:17.392, Speaker D: Sure.
00:02:17.488 - 00:03:41.860, Speaker C: Thank you for having me and having us. So, I'm Holke. I'm a political economist by training and worked in multiple spaces from consulting and social impact and working also with the german government. This meeting is being recorded. So I come from a background of like with multiple experiences and made it into web3, really thinking about what are the systems that we can build, that we can test, where we can experiment with new ways of funding, funding what really we collectively value, and that doesn't really have a business model today, how can we build new systems for that? And that was how I ended up being a research scientist for public goods funding at protocol labs, where I started the project on hyperserts, which are digital impact certificates. And I think we will dig deeper into that during this call. But yeah, really this motivation was funding the things that we really, really value, and also having this discussion of what really is value, and not just saying everything that makes a profit is valuable, but taking you this concept of value much broader, having that discussion and then making it happen and experimenting, and experimenting in the real world of how we can build these systems.
00:03:41.860 - 00:04:05.952, Speaker C: And yeah, I started at protocol labs also together with Carl, who's on the call, and then we created the Hypersets foundation that I'm leading now, where we push the, the concept of impact certificates further and try to push it towards use cases and implementations of those awesome, Holke.
00:04:06.008 - 00:04:11.204, Speaker B: And we'll learn more in a moment. Let's continue with the introduction round first, Carl.
00:04:11.984 - 00:04:35.314, Speaker A: Great. Well, it feels naturally that I go after Holka. As he mentioned, we work together at protocol Labs Hyperserts. I'm now working on a new project called Open Source observer. Before getting into that, I'll share a bit about my background. Previously. Before getting into crypto, I worked in the world of impact measurement in traditional supply chains.
00:04:35.314 - 00:05:51.634, Speaker A: And so we took a very closed source approach to defining metrics and then measuring companies and entire supply chains against those metrics. And now one of the ways that I described the work that we're doing is trying to trace the software supply chain and identify metrics that are meaningful in measuring the impact of different types of contributions long term. Much like Colca, we see a world where you're actually deploying funding on the basis of some combination of metrics and a set of preferences that communities have, which are likely changing over time. Right now, we're only about one year in. It's been incredibly exciting just seeing the amount of interest and the desire to not reinvent normal grant making, but actually bring a new set of skills and disciplines into decentralized grant making. The thing that we're most excited about is building a network of impact data scientists. So people that are kind of motivated by the potential of changing public goods, but also want to develop the dark arts of doing data science and being able to take quantitative analysis and apply that on large datasets towards the goal of identifying the most meaningful signals about what products are having an impact.
00:05:51.634 - 00:06:00.034, Speaker A: And I'm going to have to jump off for like a few minutes. I'm going to come right back, but really excited to be here. And I guess over to you, Eugene.
00:06:02.014 - 00:06:07.270, Speaker E: Great, thanks and sorry for being late, Eugene.
00:06:07.302 - 00:06:16.974, Speaker B: We just started with a brief introduction round about yourself. What brought you to impact measurements and metagoff?
00:06:18.474 - 00:07:09.456, Speaker E: Yeah, sure. So hi everyone, I'm Eugene Leventhal. I'm currently the executive director over at Medagov, which is a governance research nonprofit where I'm leading a group exploring kind of research on grants and impact measurement with my own journey. I mean, I did my undergrad in psychology and finance and kind of going both into the corporate world and simultaneously working with some nonprofits. It was very interesting and kind of both seeing very different resource availability and different amounts of rigor needed to prove that you deserve some amount of funding to go do a thing. And a lot of it just very much seemed like social network based and not actually based on any reasonable quantitative differences of things. It's just that like, hey, if you're buddies with VC's versus if you're just a local community person, the amount of funding that's available to you is grossly different.
00:07:09.456 - 00:08:00.864, Speaker E: And so the first, my first sort of foray into the world of web3 was in 2016, 2017, working on a project called Edudao, which is different now than the hackathon oriented.edu dao. But our whole idea was, how can we create a community crowdfunding platform where the governance of what proposals get approved of, sort of the entire community involvement of the platform would sit in local neighborhoods. And we were specifically starting with the Bronx in New York, and we had a lot of great initial traction, both from schools, nonprofits and local community efforts. Um, but it was just way too early, right? This is before ETH was registered as an asset class by, you know, regulatory bodies in the US. There were no stable coins yet. So when we talked to the lawyers or the finance people, they politely told us to go really far away.
00:08:00.864 - 00:08:46.054, Speaker E: But it was a really interesting foray into thinking about, you know, how can we build these kind of infrastructures to put both access to capital and decision making power over what happens to that capital for local communities that are not tech forward into the hands of those communities. And so then from there, I ended up doing a public policy masters and slowly getting involved in a couple of different nonprofit projects, excuse me, in the web3 space that were getting closer to kind of both research and granting. And that was sort of what led me here to Medigav. And about a year ago, I started shopping around the idea of the state of web3 grants report, which, yeah, Gitcoin and a private backer were willing to kind of fund and seed. And that kind of led to what is now the grant innovation lab at Mediga.
00:08:46.954 - 00:10:02.636, Speaker B: Awesome. An amazing journey, Eugene, and thanks for joining today. I will remember this term non project profits, because this seems to be something to aim for, maybe. Okay, about myself, I'm Angela, founder of the Token Engineering Academy. Our goal is to establish token engineering as a crypto native discipline because we need this combination of computer science and economics mechanism design of AI and data science of control theory, systems engineering to build robust systems. And we have to understand human behavior or deal with human behavior and incentives decision making to build great, sustainable token networks. And for me, my starting point was I was pretty excited about the idea that we can build economies, networks that have their own incentives, that have their own currency, and not only the products and services, rather build networks where we have a much broader variety of value contributions compared to web two, and can make these systems and these networks permissionless globally.
00:10:02.636 - 00:11:28.764, Speaker B: And that's an awesome opportunity. And of course, as token engineers, we are focusing on how can we manage the token distribution, how can we balance supply and demand, how can we create token utility and award value contributions to the network? And this of course brings us to the question of impact. So what is value add to a network? Sometimes this is not as easy to measure as just you pay fees or you provide liquidity. For sure we have a lot of, let's say, easily quantifiable contributions, but then there's so much more. And I think what we are currently observing in the big L2s who have established amazing, exciting retro PGF and similar retrofunding grant systems, is that they try to go beyond the simple to measure contributions. They try to build systems where we have a great variety of contributions and also a great variety of perspectives on what is value add. And then we as token engineers try to create suitable mechanisms so that we can encode these perspectives and these ways to measure value add.
00:11:28.764 - 00:12:27.012, Speaker B: And this of course then leads ultimately to impact metrics. So how can we define and quantify the purpose and the goals of a network? So some more words about te academy. At the moment we have more than 4500 students enrolled in our programs. We have launched te fundamentals as a bachelor level program in token engineering. And currently we are running the study season, which is a cohort based program where we design a voting algorithm together with our students that gives weight to reputation. So we know that in our own ecosystem, what matters most is knowledge, experience, contributions to the field, reputation as researchers and practitioners, and we make this verifiable and quantifiable via nfts. And here we are building a voting system on top of it.
00:12:27.012 - 00:12:56.854, Speaker B: And this is another, I guess, activity where we have, where we are asking and addressing very similar questions. Now, I'd like to start with Holki and hyper certs because I guess I'm not sure if everybody who's watching at the moment is familiar with hyper certs. Let's talk a little bit about the core idea of hyper Certs and impact claims. Ok.
00:13:00.354 - 00:14:11.660, Speaker C: So the core idea is really that what we can now do is that we can bottle impact and have an asset or like an asset or like a token that represents this impact that then can be sold. And by selling it you can finance the project and finance activity that created that impact in the first place. And that is the core idea. And to make it more lively, let's actually start with an example. An example that will be released soon is voicedeck and voice tech tries to support journalism in India and not the journalism to just spread whatever happens right now, and that needs to get a lot of attention. But the kind of journalism that is solution oriented. So solution journalism and how we can support that, and solution journalism has a certain impact that can be measured like a impactful story that changes something in a community, and where we can assess how impactful was that they will use.
00:14:11.660 - 00:15:24.562, Speaker C: They build a system on top of hyper certs, where journalists can create a hyper search for their story, then that will get evaluated for the impact that it had on the community. And then it enables funders to buy this impact. And by buying the impact, the funder can then say, I am partly responsible for this impact because I funded it. And I can now claim that I have this impact because I own the hyperset. So, the hyperset in the story is what accounts for the work that was done by the journalist, and then is used as a retroactive funding instrument, because the journalist, after the impact was observable, sold that hyper cert to the, to the funder. So, and then we have basically a very transparent supply chain of impact where you can, you know, who did what, who evaluated it, and who funded it. So, and this is kind of the ecosystem that we want to create.
00:15:24.562 - 00:16:39.770, Speaker C: We don't want to create that just for journalism. This is just one example. We want to create that for basically any impact domain. And we recognize that every impact domain needs to have specific funding mechanisms, and not just like one funding mechanism, and that solves it all. But each impact domain needs multiple funding mechanisms that ideally are also interoperable with each other. And hypersearch is kind of the base layer that we imagine for these systems, because if you give out a grant, so prospective funding, classic, you give out a grant, you can also have that as you are selling prospectively part of your hyper cert to someone. So to the, to the grant maker, and then you have, for example, a similar retroactive funding mechanism, where after the impact is observable, you sell more parts of your hyperset, because a hyperset in itself can be split up into multiple fractions, so that you can prefund your project with selling 20% of your hyperstrate and then fund, or like get more funding if you really prove that you were impactful with what you did.
00:16:39.770 - 00:17:11.618, Speaker C: So this is what you want to do for multiple impact domains, as I said. And the impact domains that we are thinking about very actively is, for example, open source software. It is nature, regeneration and biodiversity. It is AI safety. It is journalism. And those things are exactly this category of things that don't often have a business model today. But we really value and we want to see it in the world.
00:17:11.618 - 00:17:25.634, Speaker C: And the hypersert as a primitive helps to build these funding mechanisms and helps build multiple different funding mechanisms. If that is a kickstarter like platform, if that is a price competition or anything else.
00:17:30.374 - 00:17:46.874, Speaker B: Over the past couple of months we have been exploring the hyper search concept and we found so many interesting and very unique aspects to it. For example, that you basically allow to create impact markets via hypersearch.
00:17:48.434 - 00:17:48.794, Speaker D: Yes.
00:17:48.834 - 00:18:47.952, Speaker C: So an impact market for us is first of all really that those who create impact can sell it. So and this, you can think of a market where the US, yeah, selling your impact similar to a carbon credit that you can sell. And yeah, in the way like how we specify and define these is, yeah. Very much dependent then on the impact domain. And I think something that is important for all of these impact markets is that there are of course kind of the impact, like the project developer, those who create the impact and then the funders. The funders can now specialize into prospective funders who find who is the best next team and those who retroactively fund where impact has been proven. So I think there is a specialization of the funding side.
00:18:47.952 - 00:19:46.494, Speaker C: And then a third actor of course is important, the evaluator. The evaluator who says, oh, this was actually impactful. And there also a huge variety of approaches can be taken, more objective. One where what is it that we can measure about the impact, but then also what is the subjective impact, for example from to the community, to beneficiaries or to experts, because there's something that we cannot measure directly. But kind of the experts, they have a good feel for this was an impactful contribution to a domain. So I think when we think about these actors and how they play together and how we can kind of realign them and also make sure that all of these functions can be done in a financially sustainable way, especially the evaluation, then I think we can make steps towards really funding those projects that have the most impact.
00:19:47.554 - 00:21:17.684, Speaker B: Yeah, this is awesome. I guess we'll discuss some more tiny and super exciting details in a moment. For those of you listening here and wondering, wait, what is this whole hyper certs thing all about? You can basically go over the full concept of hyper certs and other NFT based systems and ways to reward for impact or for other contributions in our new course update. So@tokenengineering.net we offer courses learning token engineering te fundamentals and there's this course NFT based reputation in web3 and our latest update that you can access via this page is hypersearch. So basically this course provides an overview on generally what are nfts NFT use cases, our own certificates which are nfts non transferable, ERC 1155 and we run our nfts or our nfts live on optimism. And here you'll find more information on how we can design and use reputation, how we can assign reputation, how we can prove reputation in token based systems, and we have several case studies with different approaches how to leverage nfts in such systems, and one of them is hypersearch, our latest update.
00:21:17.684 - 00:21:50.474, Speaker B: And on top of that, this is not only a usual TE fundamentals course, we have created it together with students and Aniki. If you are here in the session, please feel free to switch on your mic and say hello. Aniki has been the key contributor and he's been in touch with Holke for a while, digging into hyper certs, what hyper certs are, how they are implemented, the claims and creating interoperable funding systems.
00:21:51.094 - 00:23:21.674, Speaker D: Aniki hi everyone. So I participated to this course regarding the hyperset use cases. So basically the course is really an overview of the hyperset on the micro level, as a data layer as well on the macro level. So what we can do with it, the different dimension of hypersets, also how it is used in the context of antero interoperable funding system and also how is it implemented in the blockchain, how it is used. So really to have all the dimensions regarding Viper set, explaining outside in an easy way with different diagrams to have an easier idea on how to use it and what is the goal of it and why use it, such to say, yeah, yeah, there is like seven different parts which are really relatively short, but really, I think, easy to grasp, even if the concept is a bit abstract in itself. But I think that that way it is, yeah, interesting for everyone who wants to know more or use iperset to understand it. And I can say a word about myself.
00:23:21.674 - 00:24:11.424, Speaker D: Otherwise I have more of a technical background in programming. And then I was interested in the blockchain ecosystem, so I learned a bit with different cohorts, the ecosystem and also some different cohort about crypto data analysis. And then I discovered token engineering academy. So I did pass some modules on it and then there was an NFT party. So there I meet Angela and then there was a proposition to review some courses. So in particular the NFT courses. And then we talk about extending the courses with different new primitives such as hyperset.
00:24:11.424 - 00:24:22.884, Speaker D: And then I jumped in and then I contributed to the course and I got in contact with Orc and since then we were working on this course on ipos.
00:24:23.704 - 00:24:56.334, Speaker B: Yeah, and I must say I'm super glad we have this additional module. So a big shout out to you Aniki. It took us a while to put the content together, but I must say now it's really super easy to digest and still has the depth we expect from our courses. Check it out@tokenengineering.net. dot and big thank you to you, Anike. You definitely provided great impact for the token engineering community. Thank you so much.
00:24:56.914 - 00:24:59.770, Speaker D: You're welcome. So let's see how it goes.
00:24:59.922 - 00:25:02.294, Speaker B: Yes, I'll keep you posted.
00:25:03.114 - 00:25:06.698, Speaker D: Yeah, so if you have a need also feel free to contact me.
00:25:06.866 - 00:25:55.364, Speaker B: Perfect. And by the way, anyone who's taking this course can mint an NFT by passing an exam with newly proving newly acquired knowledge about hyper certain NFT based reputation. Now I'd like to continue the conversation with yeah, let's share what challenges you or what basically what you are working on when it comes to measuring impact. Eugene, we have been in touch last week already discussing impact measurement on wider scope, the future of public goods funding. Right. And of course, measuring impact is a big part of it. So share a little bit about your current work with respect to measuring impact.
00:25:56.544 - 00:27:53.744, Speaker E: Yeah, thank you. And you know, from our perspective, we're really interested in exploring what kind of systems of feedback loops need to be generated and how to do the right stakeholder mappings and getting feedback from the right people to try to get more, not just qualitative inputs, right? We don't want just open ended surveys of was this good or was this not? But taking a much more systematic mapping approach towards understanding whose voices need to be heard and effectively, what are the dimensions of impact? Right? If you know, our world is a research generation, research community, research application. So in the context of these things, right, if you talk to traditional research funders on how they measure impact, a lot of it will be, oh well, citations, right? Let's put up and see how many folks have references, how many papers were result of this grant, and, right, talk to those funders about how happy they are with those metrics and they'll all unanimously say not marry. And so kind of thinking about how you dig down a little deeper from there, especially for something like research, which is effectively saying how do you track the impact of an idea? And so putting that into strict metrics, especially if looking at like on chain metrics, it is very unclear how you get from the idea to the on chain metrics mapping. And so that's why I'm very excited to kind of see the work of both Holk and the hypersearch team and Carl and Ray and the OS observer team and others that are very focused on the technical side. We're building out the infrastructure for there to be more data in the first place. And where we hope to contribute in that landscape is experimenting both on assessing our own impact, but also working with some web3 grant programs, whether it's utilizing newer tools also like either one of theirs or like something like impact garden with attestation based mappings of, hey, you know, we put out a paper, someone can attest to the fact that they interacted with that paper and it helped them do something.
00:27:53.744 - 00:29:17.352, Speaker E: Or, you know, we organized an event. And a concrete example, we organized a web3 grant summit in Denver this year and we had multiple people who went there personally attest to us the value that they got. So, you know, how can we do some more of this attestation in public? And if we build and contribute to these kind of webs of attestations and explore other forms of systematically capturing feedback from there, how can we actually potentially deduce some more quantitative approaches? And, you know, there are, through survey design and other data science and statistical approaches, there are methods towards taking large bodies and qualitative information and quantifying them in some way. But we just feel as though the systems of capturing the qualitative are still very weak and they're usually the grantor sending out a survey to their mailing list or some emails that they collected or put together and that doesn't feel systematic enough in terms of approaching and mapping. Whose voices do we need to hear? How do we hear it? What nuances might we need to take with different community members in order to make sure that we get some at least somewhat consistent qualitative data and from there start thinking about what we can do more quantitatively. We're still in the pretty early days, we're in talks with a couple of grant programs. Nothing is set in stone yet, so I wouldn't feel comfortable throwing out the names publicly yet.
00:29:17.352 - 00:29:53.524, Speaker E: But we're trying to pretty much take this approach of let's do a full review of what you're currently doing for impact measurement, then let's do a stakeholder mapping of who are all the voices you want to hear as part of that. Then how do we start mapping five to ten projects impact, and from there how do we systematize this for your grant program? And, you know, we also want to put our money, where our mouth is again, and really try to understand what does it mean to measure the impact of Medigu's research or Mediguv's events. So, yeah, those are some of the things that are top of mind for us and that we're really excited to experiment with throughout the rest of this year.
00:29:53.874 - 00:31:10.754, Speaker B: Yeah, and this sounds like a pretty wide and general challenge, so enough for years of research, I think. I mean, you pointed on the special opportunity we have in crypto land, so we can represent contributions on the chain and in a digital fashion. Have you? I mean, we thought about how can we represent learning achievements in our community? And there is no predefined way to do this. Or it's hard to say, okay, you measure clicks in a course or something like that. How can we provide meaningful, find meaningful indicators? And we decided to define them ourselves via the NFTs and then under the hood, implement a certain hard to game way to track activities. But then all these activities lead to an NFT and to a certain proof. What are looking at? Okay, the metrics we have already, you mentioned surveys, qualitative input.
00:31:10.754 - 00:31:31.750, Speaker B: What's your observation here? Rather define your own metrics versus go what you have, what you can find out there. Yeah, that's a tough, probably, I should say data points, not metrics even, I mean, taking.
00:31:31.902 - 00:32:08.892, Speaker E: Right. If you have actual data, that is always a great starting point, though. I think most people who even take that approach will be the first to say that that's not enough. And that's just, you know, like we're taking things like citations and papers published just because those are the easiest numbers to find. But we recognize the limitations of those numbers. So I think in general, and I know, I feel like I've spoken to both Carl and Holke about this offline the idea that we're seeking the right metric or the right number, that itself becomes a false promise. And why I keep coming back to the term of systems is because dynamic systems change and move.
00:32:08.892 - 00:33:48.366, Speaker E: And so what, is today a target? Could tomorrow no longer be a useful target? And so how do you build the systems that can be reactive around those kind of changes? And so from our perspective, again, trying to take this more stakeholder mapping and not just say, if you take that description overly literally, a stakeholder mapping of a grant program can just say, well, all the people working at the grant and the people we give a grant to, and that, in my opinion, is actually a fundamentally constrained view of the system. And one thing that we've been talking about with a number of programs, and publicly, whenever we can, is sort of the lack of theories of change in Web three, especially around grant programs, people don't usually have a robust system of, hey, you know, if we put out this tool or this protocol or this thing into the world, this is the not just the first order effect of like, oh, well then here's the crypto degens who get to play with it, or like, here's someone in one part of the world who gets the benefit because of their constraints on access to markets internationally or something like that. Well what then, right? And I think a lot of grant programs will kind of conflate impact and output, and those are not the same thing, right? If you take kind of a program evaluation lens, you have resources that fund activities that then produce outputs, which then produce outcomes, which then eventually lead to impact. And in traditional impact evaluation, like, that's a decade, that's literally a ten year long window a lot of the time. And so if you're thinking in your grant program, they're like, hey, we funded a thing and three weeks later, three months later, we expect to see the impact. Well, that's just not really realistic. Impact takes time to form.
00:33:48.366 - 00:34:43.860, Speaker E: That's kind of the window for outputs to happen. And so just trying to add more rigor and thinking about the temporal nature of this and the fact that true impact isn't just like, we give you money, what did you do with it? It's, well, who did you interact with and who did they interact with? And trying to really map the web of systems, of interactions. And again, I just keep always coming back to systems because if you really think of it as a massive dynamic system, then you recognize, well, we can't map the whole world. So what is the most relevant portion of, say, multiple rungs of interaction that we can try to map, whether qualitatively or quantitatively, and then find ways to kind of keep generating new updates to that information. And I think one of the most important things is just like the humility to say that we're almost always going to be wrong relative to the perfect thing. And that's okay because this is an imperfect science. For now, no one has solved this problem.
00:34:43.860 - 00:35:05.584, Speaker E: So we're all fundamentally working in a very difficult domain that is going to take many years to make progress on. And so no one should feel ashamed to say that, like, hey, these are the things that we're using and we don't think they're great, but what else do we do for now? And so I think even just that mindset of publicly working and publicly failing is going to be a healthy approach for us all to improve.
00:35:06.924 - 00:35:13.504, Speaker B: Great call, Eugene, everyone. Holke, Karl, feel free to chime in with your own questions.
00:35:14.764 - 00:35:16.756, Speaker C: I would love to do that if I can.
00:35:16.780 - 00:35:18.184, Speaker B: Ok Holky, go ahead.
00:35:19.244 - 00:36:33.024, Speaker C: I mean, it's like, I just love the theory of change and I think more people should know what a theory of change is and look up theory of change and logic models and what that means and how you can apply that if you really are serious about creating impact and maybe like to formulate that as a question, or like Eugene, maybe you will just say, yes, you see it the same way. But I think we need to find different funding models along the full spectrum of the period of change, because sometimes when the impact is only in ten years and you do only retroactive funding, then what do these projects do for the ten years that the impact is not shown? So that's why we need funding models that fund your activities and your resources. Or we observe the output and we're like, we don't know if this is really impactful in ten years, but the output is already great. And according to our theory of change, it actually does what we wanted it to do. So I think there are funding models that specifically fund along the theory of change. And that's also why it's so important that we actually like, know that concept and know what we really want to do. And sometimes we have retroactive funding models where we don't actually look at the impact, but on the output.
00:36:33.024 - 00:36:49.004, Speaker C: So, and I mean, we always talk about impact as kind of the whole thing. So I think we can be more specific in our language, even though like on the, on the marketing side, we probably will still talk about impact. And I don't know if I formulate this as a question now. Eugene.
00:36:51.054 - 00:36:52.382, Speaker A: Yeah, I mean, I know for sure.
00:36:52.438 - 00:36:53.566, Speaker E: Carl, do you want to jump in?
00:36:53.630 - 00:36:58.914, Speaker A: Yeah, there's a couple of things that I'd like to jump in on here. I think that, first of all.
00:37:00.734 - 00:37:01.046, Speaker B: We.
00:37:01.070 - 00:37:45.250, Speaker A: Really need to not just make things that are ten or 20% better, but we need to make things that are ten times better. The fact that we're dealing with new technology, there's skepticism about whether it's actually going to lead to anything that is radically different. I think gives us just a much higher burden that we need to prove that this is actually a better way of finding things. And reality check, we are nowhere near that at this point. And so I think it's fantastic. There's a lot of energy around mechanism design and experimenting with different funding mechanisms. I think that we also just need to ensure that we are funding things on a recurring basis more quickly.
00:37:45.250 - 00:38:45.566, Speaker A: And to Holca's point, it's not really helpful to design the best retroactive funding mechanism if that lives on an island and is the only source of funding you're going to get, because like you said, 98% of projects are not really going to want to work on something for ten years in the hopes that they get some uncertain payoff. I'd love to see more experiments kind of further, like earlier on in the lifecycle. So looking at hackathon projects, how many of those actually survive and what can be like the next set of funding for them? Maybe it's a fast grant, maybe there's a way to go from getting some kind of smaller builder grant to immediately graduating for some kind of larger multi year funding. I think that's where we have a lot of projects that are maybe stranded. They're kind of at that point where they're not quite sure whether to go all in or whether to keep their options open. And I think being able to separate the wheat from the chaff, as we say, and, you know, fund more of those things that really have that promise, seems like a good area to invest in. Yeah.
00:38:45.590 - 00:40:32.192, Speaker B: And I must say, what we, I mean, looking at what you proposed, Eugene, I think we have to come up with, I think it's not true that we can't measure anything apart from output and ultimate impact. Right. So we should be able to find indicators of making good, reasonable or not only reasonable, making great progress with projects and help them survive and help them to grow. And I think this is somewhat not sufficiently addressed yet, because we on one hand either want to define ultimate impact in a crypto network when usually leads to, okay, fee to make a protocol sustainable. But how can any early project immediately, within the first six months contribute to fee revenues? This applies to a very narrow group of types of projects. So can we come up with metrics that help us to understand a project's progress and step by step progress? And I think what's great here, it's great that crypto is moving fast, because we are learning a lot about how projects evolve and how projects evolve in a healthy, promising way, and this can lead to measuring their progress and maybe not immediate impact. Question is, what would we call it if not impact, but still help them to grow? Unsolved problem? I think at the moment, yeah.
00:40:32.208 - 00:40:56.794, Speaker A: I mean, it's not, there's a lot of interesting signal if you just look at inputs, I think we can all agree that that's not the only thing that matters. And you shouldn't just be funding people because they are continually putting effort into something. On the other hand, like persistence is one of the best signals of a team that's really committed to something. So I agree that there's definitely, there's definitely something there.
00:40:59.654 - 00:41:00.086, Speaker B: Right?
00:41:00.150 - 00:42:58.434, Speaker E: And I think just if it's okay to add a few things, I mean, I think part of what we're talking about as well is the idea that mechanisms, well, there's two things here. First of all, mechanisms either aren't either or. Right? A lot of the time you can use multiple mechanisms for different things. And I think what Carl was just alluding to with the hackathons, that brings up another point, which is the term granting itself can sometimes unintentionally be limiting because granting is just one sub aspect of resource allocation. More broadly, and depending on the kind of impact you want to have as an organization that has capital to issue and you're figuring out what to do with it, is it most suited towards get as many hackathons going and just see what things emerge and then think of the systems of support you can build for people coming out of hackathons and maybe that can lead to more actual growth in TBL or users on your system or something like that, because you're actually spurring new innovative ideas from the bottom up versus thinking of just a retro program where you're, even if you're putting up a large pot of money, but if you're expecting hyper sophisticated outputs, who's going to have the luxury of doing that? It's going to be some small player. I think all of these things exist. I don't know if a spectrum is the best way to put it, but at least in a landscape, or I think of them as like tools on a toolboat, right? You have different options depending on different types of effects that you want to create, and you can mix and match these things, right? And in an ideal world, I would also love to see more collaboration amongst funders because I think funders aren't sharing enough honest notes on here's what's working, here's what isn't, here's what we're trying, here's why we just did retro, here's why we're doing prospective, here's why we're doing research grants, and the more we can actually share this, the more as a broader ecosystem we could actually say, hey you, Funder X, you're actually really comfortable just kicking projects off.
00:42:58.434 - 00:43:46.332, Speaker E: You're not as worried what? Like, you obviously want to see change ten years from now, but you're willing to take the risks upfront, whereas Funder Y, you're willing to put up massive pots of money for stuff that has already happened. And then how do we get funder Z to fill the gap somewhere in the middle to get more of those early stage projects to get to qualify for the retro? And I think, I mean candidly, right, we're right now trying to shop around the idea of a web3 grant consortia. And some of the initial feedback I got was honestly shocking, where people are like, oh, well, if they're here then I don't want to be here, and if they're here then I don't want to be here. And it's just like we're not competing on the products. The whole point is helping each other learn on impact measurement and most efficient allocation of capital. These concepts don't care about your technology. It doesn't matter what l one or l two or whatever you're building.
00:43:46.332 - 00:44:24.826, Speaker E: None of us know how to do this. Well, let's just collaborate. So I actually see a lot of the roadblocks just being personal and zero sum, which is especially frustrating because that's like exactly what we're supposedly trying to get out of with web3. And yet we all fall back into these traps because they are so hard to get out of. And so, you know, like the malloc is real when it comes to not just collaborating more and supporting each other. And that seems like such a basic first step that could already get at least like a, at least get to that ten to 20% that Carl was mentioning. And that is not the end all be all, but we're literally just missing the low hanging fruit because we're not willing to talk to each other.
00:44:24.826 - 00:44:36.434, Speaker E: And if we are, I think from there, coming up with those ten x plus ideas is going to get easier. So yeah, that's just a quick call to like, don't forget to collaborate, folks. It's real important and I guess to.
00:44:36.474 - 00:45:40.334, Speaker B: Make projects sustainable and to find the, you know, the most talented people and the most interesting projects. I think it makes a lot of sense to combine different funding mechanisms. On the other hand, you could say, wait. To be able to grow the projects, the number of projects, quality of projects, contributing to a certain ecosystem, you have to be very specific. So there is no general best metric for impact, I would claim. It's always context specific and it's a matter of making further develop and continuously develop it for an ecosystem. And I think, Carl, you can speak to maybe give some examples we have, looking at the optimism retro PGF, and the evolution of assessment of impact and measuring impact.
00:45:41.274 - 00:46:29.624, Speaker A: Yeah, sure. Maybe. One thing to start with is that we've tried to come up with a set of principles for coming up with good metrics in this space. And I think the reason that'd be a good starting point is because a lot of these principles, if you try to apply them in the traditional public goods world, that they're actually pretty difficult to either to implement or to kind of implement, I guess. And so one of them is really just around like, verifiability. So basically, if there's a metric that is out there, then you should actually, anyone who's kind of looking at that should be able to verify all the way back to the source, where that data is coming from. It could be difficult, it could be convoluted, but like that idea that you can verify, this is a very kind of crypto native concept that doesn't exist in kind of the traditional world of public goods funding.
00:46:29.624 - 00:48:05.128, Speaker A: Another one is reproducibility. So if I'm getting a result, and anybody else should be able to, again, given the same data and the same formula, should be able to reproduce that, and then a third one is really just around like completeness. So basically, if you have metrics that really only apply to one project, or are highly subjective in the sense that it could be a quantitative metric, but it might be the one where it only works for my project, because I've defined things in a certain way that are only relevant to my project or to my, to my community. All of those things have very, very limited scope. And so the exercise that we tried to do with optimism and more broadly, is come up with a set of metrics that are going to be applicable to a very wide range of projects where you can actually audit all of the logic and the formulas, and they're all built from public data that would obviously include anything which is on a block explorer, but also things like GitHub activity and software dependencies and so on. So that's kind of like the design space is, how do you come up with a set of metrics that fulfill those basic criteria and are going to make sense to a community of people? So I guess that's like the fourth principle, is that they need to be simple enough that you can explain them, and you don't need to read a white paper to understand where it's coming from, where we've landed is basically trying to come up with around 15 or so different metrics which are designed to counterbalance each other. So on the one hand, you have metrics that are really focused on growth, and that might include things like pure transaction numbers and gas fees and so on.
00:48:05.128 - 00:49:01.862, Speaker A: And obviously those are very, very important economic indicators for a L2 or chain like optimism. But on the other hand, you have things that are related to network quality, for instance, things that are trying to capture composability, gas efficiency, the quality of those transactions that are happening and so on. So you take something which is pretty subjective, like how do you determine a spammy transaction from a high quality one, but then you try and apply that consistently across a broad swath of projects. The other access is just around users. So you can imagine trying to grow the number of users, the number of active addresses, other ways of measuring pure user growth. On the other hand, you also have the quality of users. If you can find the best five or 10% of users on the network, what features do they have and what products do they prefer? So all of these things are intended to offset each other.
00:49:01.862 - 00:50:13.844, Speaker A: If you do really well in one area, it's going to be very difficult to do well, and some of the other ones that are a counterbalancing force. Perhaps the best example of that is if you're really good at getting a lot of addresses and a lot of users, how many have you actually retained, or how many of those are trusted addresses that are interacting with a lot of projects, it becomes difficult to actually be good at a number of different indicators. So that's like the experiment around coming up the metrics and trying to apply them across all the products that apply. The final twist is that the badge holders are going to be voting on these metrics. So instead of picking individual projects, the big twist here is that they are going to be looking at these metrics, seeing how the products compare, and then actually deciding how much of the portfolio they want to weight based on any given metric. So it could be that the best metric is, I don't know, trust the transactions or the number of new users onboarded. But what you actually see is that the way that badge holders vote is, oh, we're concerned that people have optimize too much for that one metric, and so we are going to use our voting power to counterbalance that.
00:50:13.844 - 00:50:49.874, Speaker A: So you add all this up, and I think it leads to something which is pretty difficult to gain. In the very least. If you're good at one thing, it's going to be hard to be good at all of the things. And if everybody is kind of optimizing, for one thing, the most likely the badge holders are going to try and correct for that and they're voting, on the other hand, like, it's a, it's a hugely ambitious undertaking to try and get all these metrics for all these projects, handle all of the many edge cases, support many different networks. So we feel like a huge burden right now to kind of get the data good enough or right so that it holds up to this kind of an exercise.
00:50:50.454 - 00:51:18.766, Speaker B: Right. And just for the current picture, this new approach of bachelors voting on metrics instead of projects is brand new. So it's for this current round where I think applications are open for this week only. How many metrics do you track or do project bachelor's vote on at the moment? In this very first instance of this voting design?
00:51:18.910 - 00:51:26.134, Speaker A: Yes, it hasn't been fully, it hasn't been finalized just yet. I think it'll end up being around 15 metrics.
00:51:26.254 - 00:51:26.942, Speaker B: Okay.
00:51:27.078 - 00:51:51.500, Speaker A: And one of the other, like, interesting design choices is that you can take a metric like transactions and you can apply it on a linear scale or you can apply it on a logarithmic scale. And so if you believe that you want to reward momentum in products, kind of getting from, like, you know, a small level to a bigger level, then a log scales me better for that. If you just think it should be, you know, pure linear scaling, then you do it that way.
00:51:51.612 - 00:52:16.524, Speaker B: Yeah, that's, I mean, here the devil or the exciting challenges are in the details of how the results are calculated. But basically, the idea is a voter doesn't ever see and look at projects. They only look at metrics and rank or vote. How relevant they see this particular way to measure impact for the development of the ecosystem.
00:52:17.064 - 00:52:24.112, Speaker A: Yes. I mean, they will see the projects, they'll see the distribution that's produced by their waiting, but the goal is, okay.
00:52:24.168 - 00:52:29.632, Speaker B: They see the outcome of their voting. Okay. And translate it into project ranking.
00:52:29.768 - 00:52:55.016, Speaker A: And I think, like, the bias that they're trying to correct is that you can design something which will favor projects that you think are very impactful. But the problem with that is that there's often there's going to be good projects that get in that you might not know about and that without being able to look at these things in an unbiased way, would otherwise slip through the cracks. And so that's kind of one of the things that they're trying to crack for. We'll see how well it goes.
00:52:55.160 - 00:53:00.524, Speaker B: Yeah, pretty exciting. I recommend everybody to follow round four of red.
00:53:02.954 - 00:54:34.034, Speaker C: If I can jump in there. So what I think is exciting here is the interplay between kind of what can we measure and how do we still trust kind of the subjective evaluators, the batch holders in this case. And I think this is like, in general, something that I think I want to just like, stress, because I think, like, sometimes when you talk about impact metrics, we lose this part of, like, we should really measure and, like, improve what we can measure. And, like, what we can measure, we should measure and, like, take that into account. But then we shouldn't pretend that we can measure everything and there's a risk that we actually pretend that we can measure all the things, and then we actually end up in a pretty big dystopia, from my perspective. And that's why I'm so excited that this, like, how we can combine, this is what we can measure and this is what we cannot measure, and that's why we need the subjectivity in it. And when I think about it, when I, if I would, for example, fund AI safety, it's like, what would actually be the metric for this in research, like, AI safety research? Well, the way I would fund it, I would actually look at, like, who do I trust? Who knows the most about this area? And so there's local knowledge, and, like, how can we build a funding network where, like, the funding actually ends up potentially through multiple steps at those nodes in local networks where the people who then ultimately make the decision to fund this project or not are able to make the decision.
00:54:34.034 - 00:55:02.492, Speaker C: So, and I think this is something where we have the metrics, but we also have sometimes just like, to trust individuals that are experts in a certain area or communities that are directly affected. And I think getting the balance correct is really like a challenge that we all face. And that's why, like, this experiment with bad shoulders interacting with metrics is so exciting because it addresses exactly this in a new way.
00:55:02.668 - 00:55:03.504, Speaker A: Absolutely.
00:55:04.764 - 00:55:56.442, Speaker B: I just want to highlight this. If we, I mean, there's so much data we can assess, so what role play do we want to eliminate? The subjective element and the potentially biased element of human evaluation. And in some cases, it's not possible. In other cases, we could replace human decision making entirely with just tracking numbers over time. The question here is, I think this is also underrated in impact measurement, is incentive alignment. So what's the incentive for decision makers to take good decision, to make a good evaluation at high, you have compensation for evaluators, right? At optimism. As far as I don't know.
00:55:56.442 - 00:56:07.774, Speaker B: We don't have any skin in the game for evaluators. And this is another element here that I think is at the moment, not sufficiently addressed.
00:56:08.554 - 00:56:39.738, Speaker A: Yeah, just two things there. You bring up a couple good points, Angela. The first one is that optimism. They have, I think, four rounds that are planned out for this year, and the one that comes immediately after this is going to be purely expert based. So it's going to look at their VOP stack. There's going to be a group of people that are deeply knowledgeable about that, and they're going to do effectively what you said, holka. They're going to look across a set of contributions and have their own ranking mechanism.
00:56:39.738 - 00:57:33.030, Speaker A: I don't know exactly how that's going to work, but that's like a complete 180 in the other direction, just around purely subjective, expert led assessment. I think what is important to remember here is that they are not taking a middle of the road approach. They are trying very extreme experiments, because I think you're going to learn very, very powerful things about the limitations of each of these approaches from it. Now, obviously, it's expensive deploying this amount of capital to do these kind of experiments, but I think it's an underappreciated part about this, is that they are actually testing these things with significant funding on a real scale. And if the metrics goes perfectly well, then I'll be very, very, very surprised. Like, there's no version of reality where that actually happens. There's going to be a lot of issues, and I think the important thing is, actually, this is only round four.
00:57:33.030 - 00:57:59.494, Speaker A: Zoom out. If you do 100 rounds, then this is going to be pretty important in terms of what we learned from it. Yeah, but to answer your specific question, Angela. Yeah. There isn't like real skin of the game in the sense that the batch holders don't represent people that have very large economic interests and optimism. By and large, some do. But it's kind of a mix.
00:57:59.494 - 00:58:06.896, Speaker A: So it includes people that are much more just engaged in governance, but don't have large token allocations and so on.
00:58:07.040 - 00:59:00.854, Speaker B: I would not even say it's about, you know, collusion or so, or conflicts of interest. It's also about how professional and well equipped do we want to make decision makers. So at the moment, I guess for many endows, it's at least when it comes to a broad variety of contributors and decision makers, it's a side project. It's not their main activities. They can't really invest into tooling and building up their own expertise and probably even their own research. And yeah, I think you're right, we are in the early stage and there are many iterations needed. But I think aligning incentives is something that we should keep in mind.
00:59:00.854 - 00:59:11.384, Speaker B: On top of the right metrics to measure impact. Some final words, anything you would like to add as we are coming top of the hour?
00:59:12.084 - 00:59:29.544, Speaker D: Okay, Eugene, I have a question regarding the impact. If there are some impact models that are simulable, that means that we are able to simulate some kind of.
00:59:31.614 - 00:59:31.926, Speaker E: I.
00:59:31.950 - 01:00:47.812, Speaker D: Would say the different aspects which contribute to an impact. And from there, because I mean for every project there are some generic patterns. That means for example, I just announced a project, I find a ground. Then I found some workers. The workers did the job, then they produce some output. The output then have some impact on different people or product or entity or services, and then it generates another level of impact status. And I wanted to know if there are some work in the field in order to try to outset that to help the other existing, I would say devices in the impact system in order to make it easier to grasp this potential impact without having to spend a lot of resources for it.
01:00:47.898 - 01:01:40.234, Speaker A: So yeah, no, it's a great question. I just shared a link right now to some docs that show you how to get going in our data science environment. You're absolutely right. We have tried very, very hard to make it so that you don't have to replicate the infrastructure, you don't have to pay for storage, that you don't need in order to at least test and train models. And then if you want to deploy something on a larger scale, either you can invest the resources then, or maybe we can even discuss doing it on our budget. But that's a really big issue with these things is that oftentimes, sure the data is all public, anybody can look at it, but actually getting everything together, we're at twelve or so terabytes right now, and that's growing every day. And that's not something that you can trivially spin up over weekend.
01:01:43.374 - 01:02:06.856, Speaker D: And to understand everything. So I don't know if there are already some people working on it in order to try to formalize it, in order to automate somehow some step of the work process in order to be able to measure impact faster or at least make some prediction on it.
01:02:07.030 - 01:02:07.744, Speaker E: So.
01:02:10.324 - 01:02:16.984, Speaker A: Okay, we would love to collaborate on that, jump into our discord and we have some things you can check out.
01:02:17.644 - 01:02:18.704, Speaker D: Okay, cool.
01:02:20.684 - 01:02:44.774, Speaker B: I think there's a lot more to discuss. I think we touched on around half of the topics that I actually wanted to discuss, but there's so much to share. I hope this wasn't the last time we discussed measuring impact. Check out everyone. Check out open source observer. Check out hyperserts and Metagoff NTe Academy. Thank you so much, everyone, for joining us today and see you soon.
01:02:45.674 - 01:02:47.054, Speaker C: Thank you for having us.
01:02:47.514 - 01:02:48.494, Speaker E: Thank you.
01:02:48.874 - 01:02:49.586, Speaker A: Bye, everyone.
01:02:49.690 - 01:02:50.154, Speaker B: Bye, everyone.
