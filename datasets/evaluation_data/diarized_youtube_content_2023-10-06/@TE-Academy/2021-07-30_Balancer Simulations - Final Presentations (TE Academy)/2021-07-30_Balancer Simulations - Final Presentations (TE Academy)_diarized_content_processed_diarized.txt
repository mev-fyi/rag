00:00:00.170 - 00:00:31.078, Speaker A: Seeing 22 people. Sorry I pushed the button. No worries. Seeing 22 people dedicate their free time to learn more about token engineering simulations. Rigorous engineering design on top of balancer on top of it all is very rewarding for us. Fernando apologizes that he couldn't be here on time time. He'll be arriving shortly and will be able to watch most of the presentations, I believe.
00:00:31.078 - 00:01:30.166, Speaker A: But I just wanted to thank everyone for your time. As you know, Balancer was built from the beginning following this rigorous token engineering design principles, while it was originally a research project at block science, and we do our best to stick to that in the new developments at Balancer Labs and at the protocol at large. So it's very good to see people carrying on that torch and building new things on top of balancer following those same principles. Thank you all very much. Thank you too, Marcus, for your support and the whole balancer team for supporting this initiative. And with that, I'd like to call Joseph presenting the first project. His results.
00:01:30.166 - 00:02:01.614, Speaker A: He was working on Balancer v two dynamic parameter adjusting pools. Joseph. Okay, stage is my request to share. Screen should work. Cool. Is it on my slideshow? Okay, we can see your screen. All good.
00:02:01.614 - 00:02:30.150, Speaker A: Okay. Is it the slides that showing? Yeah. Okay, perfect. All right, I'll get started. Cool. So, yeah, the topic I went with was to do with the dynamic weights adjusting. So in version two, we've got the ability to change the weights associated with each of the assets in an underlying pool.
00:02:30.150 - 00:03:39.360, Speaker A: Bit about myself. I come from a software developer engineering background. I've just been spending these couple of months studying some of the web three technologies, looking into Amms, Dexes, and some other use cases for blockchains. I don't have any financial background, but I have interests across the board in algorithms, game theory, and decentralized technologies. So I was more than interested to join this research group and get engaged with CADCAD. So, a bit more on the research question that I chose, I chose the question of Leah. Can we dynamically adjust the weights of assets in a balancer pool to reduce impermanent loss for liquidity providers? So you're probably familiar with the formula and how we derive what the impermanent loss is, namely the difference between, or the change in the value from what the balances were initially worth at the time of provision to what it is worth now.
00:03:39.360 - 00:04:58.962, Speaker A: On the right. I've just got the two cases that I'm sort of trying to try to use as a visual explainer for where I think weights adjusting can be beneficial, which is where in the case that when we can perform a weight adjustment to effectively change the price offered in the pool to bring it down closer to what the real market price is, in the topic of the effective dynamic weights rebalance, where the price is being brought to closer to that real price, and then destructive dynamic weights rebalance, where it's being pushed further from the real one. So I came up with these key metrics for the success. Namely, we want to measure whether we can reduce the amount of impermanent loss through this method. Also wanted to look at evaluate our success based on the decentralization factor. Things like oracle price manipulations are obviously quite an issue with this approach. So how easy is it able for a single or small subset of actors to perform these kinds of malicious attacks? And the other is also gas efficiency.
00:04:58.962 - 00:06:09.482, Speaker A: So whether it's actually feasible in many instances to actually perform these weight adjustments, does the benefit justify the on chain transactions involved? So bid on the simulation made quite a number of assumptions. I actually made the assumption that there was zero gas fees on the transactions. This is a work in progress. But another assumption was I took a 1 minute granular price history from a centralized exchange and I used that as the troop price oracle, from which all the agents in the system that I'm simulating draws their insights front and back. Running opportunities for arbitraging agents are also performed ahead of any weight adjustments to account for things like medif. So on the right, these are the two agents and some of the parameters associated with them. So we are performing a weight adjustment on a number of key intervals, 1 minute, five minute, ten minute, 30 minutes, as well as no weight adjustment as our baseline for comparison.
00:06:09.482 - 00:07:11.650, Speaker A: And there's also a sophisticated arb agent I've implemented that considers a number of factors for perform, when to perform swaps. I haven't gone with any real or like real agent trade simulation. So here, this is for my specific simulation I performed. It's on this price history between the 15th to 30 June, obviously 1 minute granular, and it's from binance. So here we've got those weight adjustments over time in the five different simulations I ran, the first being the baseline simulation without any adjustments. So you can see here that the weights aren't being changed. Obviously the weights are being changed in all the other ones where at different interval, at different uniform intervals, those weights are being adjusted to pull back the price to wherever the oracle price is every 30 minutes, every ten, five and one.
00:07:11.650 - 00:07:59.858, Speaker A: So you can see here it diverges a lot more in the case where it's being adjusted every minute. Here's the chart on the impermanent loss. This is nominal, and I've got, I think the next one. This is probably more interesting. So this is the comparative chart percentage wise. So you can see here that it's changing a lot. Obviously, impermanent losses is reduced severely when the window, the weights are being adjusted very regularly, but it's also inverse to the price action of the, the actual performance of the assets in the pool themselves.
00:07:59.858 - 00:09:10.230, Speaker A: If I go back to the price history chart, you can see it was sort of in a downward trend for the majority of the part, with a bit of an uptick at the end. And we can see that here in the chart, that, for instance, the weight adjustments every minute, it severely reduces the impermanent loss experienced from the pool, but they all end up sort of equalized towards the ending. Some of the major learnings I took, the more frequently the weights are get justice, the more sensitive the weights in the pool are to divergence, reducing impermanent loss. It's possible, but obviously at a cost you create, allow it to be more the system to be more prone to oracle attacks, and you're also prolonging the unidirectional weight adjusting. The prolonged unidirectional weight adjusting also introduces biases to the strategy over time. So some of the future directions I want to take, there's some existing model improvements that need work. I want to incorporate some natural, non sophisticated trading entities.
00:09:10.230 - 00:10:07.382, Speaker A: I've only got the Arb agent in there at the moment, equalizing the price, and then I want to incorporate gas fees into the equation. Obviously, sometimes the weight adjusting is not going to be effective, as well as profitability of the arbitrage opportunities. Other system designs I also want to explore include some of the closed loop variants that only take into consideration the model's own price action and volatility, as well as look into some other models, like where weight adjustments get performed behind or in the same atomic transaction with a swap. And here are the resources. I've used Saliz Nico's balancer v two python implementation to perform the simulation. Thanks. Any questions? Awesome, Joseph, thanks for presenting.
00:10:07.382 - 00:11:16.602, Speaker A: Yeah, we have some time for questions. I've also shared. Yeah, go. How fast is your av agent? Fast? Like terms of calculation? No. Is the ArV agent's actions always commensurate with the granularity of the weight adjustments? So what I mean by that is, let's say you're going with a very slow weight adjustment. Are there multiple arbitrage? Does your arb agents execute multiple trades, or does he only execute one trade? In the simulation, does the price just rebalance before the weight readjustment occurs, or is it more continuous? Yeah, that's a good question. On every time step, the order is that the new price gets introduced.
00:11:16.602 - 00:12:23.990, Speaker A: The first step is the arbitrage agent is able to evaluate whether there's a mispricing occur and then it's able to perform the swap. And then in the second step on that same timing tool, you've got the weight adjustment that occurs afterwards. You should consider randomizing that the order. Yeah, you might get a different result if you allow the arbitrage agent to do their actions followed by a weight adjustment, as opposed to multiple weight adjustments in a row or multiple arbitrage actions in a row. Although you don't have any traders in your system, do you? Yeah, there's no regular trading agents. I guess I've nearly got the arbitrage agent performing swaps. Okay, so you're assuming kind of worst case scenario, which is the arbitrage agent always acts and then the weights are readjusted.
00:12:23.990 - 00:13:35.930, Speaker A: That's correct. Okay. So if that's always the order that it occurs in, like, where is the opportunity for the readjustment to prevent the arbitrage from extracting value? In an ideal case, shouldn't the readjustment occur before the arbitrager has an opportunity to act? Yeah, that's a good question. I think that's what's ideal. I went with the assumptions I made because I feel like on every time step or every block, I think an arbitrager will probably always have the opportunity at first slide in with mev. If there's value there, they'll extract it before the weight adjustment would occur, unless you can slip a weight adjusting transaction in through similar methods. So you should also have a look at the opposite.
00:13:35.930 - 00:14:54.586, Speaker A: What if it's not an arbitrage, but someone who's detecting to exploit your system? What happens if they move the price in the wrong direction first, and then wait for the weights to readjust and then arbitrage? So if they move the price deliberately away from the market, wait for you to readjust and then extract value, is that more profitable or less profitable? Yeah, definitely. That is something I want to look at. When the weight adjusting agent is performing their weight adjustment and they set the price, it might differ from whatever the real market price is and the effect of that. Yeah. And also just someone manipulating it because they know that the oracle is going to do this. They know that your pool is going to readjust its ways, right? So maybe they have an incentive to not arbitrage it and in fact move the price even farther away from the market and then wait for you to readjust the pool before extracting additional value after the weight has been readjusted. Just something to consider, right? Pretend the arbitrator isn't necessarily trying to keep the pool balanced, but just trying to extract the maximum amount of value.
00:14:54.586 - 00:16:06.210, Speaker A: If they know that you're about to readjust the weights, think about what they're incentivized to do. Are they incentivized to rebalance the pool first or are they incentivized to move it further off kilt and wait for you to rebalance it? Yeah, definitely interesting to consider. I guess I have the unpleasant job of taking the time, but I guess there is so much room for improving this model still, I think great job in building the fundamentals here Joseph, and I think these are good recommendations. It's pretty exciting to see how dynamic weights can really be practically implemented. Also taking all these countermeasures and then playing the dynamic weights, taking this into account. Thanks for your feedback mark. Now I'd like to call vasily on stage the second presentation Powerpool funds a fun strategy.
00:16:06.210 - 00:16:41.022, Speaker A: Yeah, so can I share the please? Yeah, yeah, as I say, I can. Yeah. So I decided to go with Hagmd doc, not with the slides, but all stuff is collected here and the GitHub link is also published. So my talk is about fund of funds imm based strategies. It is not the classical use case for imms at all, but it is structured investment product. So the use case is neuralizing a pool. It is a pool deployed to the main net which acts as a fund of funds structured investment product.
00:16:41.022 - 00:17:18.326, Speaker A: So fund of funds is when you hold different shares of third party funds. So in this case, third party funds are urine walls. So there are four of them. And the question is how to update this basket, how to maximize the yield. So yeah, it is crazy amount of layers and I'm working alone on this. So my main occupation is Powerpool protocol. So the main research question here is to compare different fund of fund strategies applied to urinary ape to this particular basket of walls.
00:17:18.326 - 00:18:15.558, Speaker A: But of course you can take any other imputed data so and task to create building blocks fund of fund strategies on python test strategies and their combinations using real on chain data for the last 30 days and compare outcomes and find out what is more efficient to use for your lazy a. So the simulation methodology was based on key metric the absolute profit in USDC generated over a period by the pool or API. Basically. So since we have the basket of walls inside the pool and these walls grow slowly but surely every day based on claiming the rewards and capitalizing these rewards inside the pool. So all of this is made by urine. Of course our metric is profit. So we basically need to answer how to continue to allocate while a funds to set of walls.
00:18:15.558 - 00:18:43.960, Speaker A: So our input data was only TVL and share price. So share price is the price of urinal p token. It means that how many EDC will get if you will go to the contract and redeem it. And of course this share price can decrease, it only increases. Of course the vault is not hacked or if it works correct, it is only increased every day as I said, slowly but surely. And TVL also is present here. So basically this is only two metrics that we have for the research.
00:18:43.960 - 00:19:20.494, Speaker A: So the simulation methodology is based on updating the world composition every delta step. So delta is a number like n. Like if delta is one it means that we are updating the world completion every day. So our simulation step is one day because it is not efficient to check the TVL and the share price more frequently from my point of view. So if the delta is three, it means that we update every three days based on past performance. So there are three basic fun of fund strategies. So first of all, just TVL based strategy.
00:19:20.494 - 00:19:59.214, Speaker A: So we calculate the sum of all TVL involves and get a share of TVL in our vault and defining weights by this formula. So the assumption is that the bigger TVL is involved, the bigger is community trust, the bigger is probably API and we will see that it's highly correlated. So we just follow the market. So if this vault has bigger TVL it means that we also need to allocate more money. So DTVL or TVL derivative is basically how TVL was changed because TVL can grow or can go down based on. So if people want to deploy capital in this world, the tv grows. If they take their money out, the tv goes down.
00:19:59.214 - 00:20:30.238, Speaker A: So we also need to take this into account and the d share price or rate of growth it is how fast the price of all change. Basically how fast it accumulates interest. So an example of transaction updating the basket based on tv based strategy you can find here. So it is life and mainnet. Yeah, and based on these three strategies you can make a combination of strategies. For example, allocate part of the capital based on the first strategy, part of the capital based on the second one and part of the capital based on the third one. This is internally composable.
00:20:30.238 - 00:21:03.726, Speaker A: So in the code you can just change the ratios or even optimize the ratios. So I have a function for optimizing the ratios. So the first strategy of allocation capital by TVL is presented here. So we got like almost 15% API. And the capital mainly was in one vault, the USDN one. And it also really good correlates with the real product, because the USDN vault is like the biggest vault in terms of TVL. And the black line is for profit and vault.
00:21:03.726 - 00:21:42.958, Speaker A: So how it is accumulating? And the axis for profit is here, and the axis for TVL in absolute numbers here. So the total TVL in yern lazy was defined as 10.5 million, very close to the real amount in the pool. So the second strategy was like we allocated capital into three different strategies by TVL, by change of TVL and by change of share price. And after that we just sum all these weights and get these results. So basically the USDN is champion as well. Here it looks like all metrics outperform other ones.
00:21:42.958 - 00:22:10.120, Speaker A: And we got slightly bigger API here. So the third one was based on changing of share price. So it means that how fast vault grew last delta days. For example, if delta three lasts three days, if delta one lasts one day. So yeah, here we have a slightly different picture. The basket is more diversified. So anyway, the APO is slightly lower.
00:22:10.120 - 00:22:40.114, Speaker A: And also I compared the strategies here. So TVL, the combination of territory strategies and D share price. And we can see that the combination is better. Of course I can say that this set of vaults is probably not the ideal one to demonstration, because one vault is clearly outperforming other ones. But yeah, it is flexible. So we probably can get some other vaults here and simulate them. So here is the GitHub and I have three conclusions here.
00:22:40.114 - 00:23:20.970, Speaker A: So using only two and chain metrics, TVL and share price, you can create an infinite amount of strategies by combinations and using the derivatives. This is outdated. Also, I had strategies with some limits to the capital. For example, let's allocate not less than 10% of the capital to each world or something like that. I also tested them, but they didn't provide good results. Because if I allocate like 10% into this BuSD or USDP world, which have quite low profitability, the overall API goes down. So basically the majority of funds in this particular case will be in the USDN world because it's just best performer.
00:23:20.970 - 00:24:01.002, Speaker A: Yeah. So future work is focused on migration. Urinary zip to balance rev two stable sub pool. So we are connecting research on this point at the moment how to adjust amplification factors, but it's not the point of the stock and also take into account the gas cost of basket adjustments. So at the moment we assume that is quite low in comparison with the overall capital and the profits that the pool generates. But probably we need to account it and also the organic swaps because this pool is not only the basket for holding, it is also the liquidity pool. Of course the constant project imam is not good for walls which have price near to one dollars.
00:24:01.002 - 00:25:01.454, Speaker A: But for stable swap pool with clearly correctly defined application factor, it probably will work and we can get additional profit from swaps. Yeah, so I think I finished spot on. We have 2 minutes for questions with TVL. You're referring to total value lost, right? Yeah. So basically it means that if vault has bigger tvl, and of course it is easily verifiable on chain, we allocate more capital. And if for example Walt has low QL, we not allocate too much to it. So it was our first approach and this strategy is really working in mainnet and it looks like it is not so less profitable than more complex ones.
00:25:01.454 - 00:26:01.580, Speaker A: So probably the simplicity is decay and less is more. I don't know any comments or other questions. By the way, you can post your questions to the chat or just switch on your mic, whatever you prefer. And I've also shared the hackmd in the chat. Feel free to check out Vasilisweach Angela, probably I can add a couple of comments if I still have time and nobody have questions, if you don't mind. Sorry, you said you're optimizing the parameters. What were you using to optimize the parameters? Basically I just calculate.
00:26:01.580 - 00:26:32.870, Speaker A: You mean the optimizing the parameters. So I'm trying to find the appropriate values of a, b and c. In this case, a, b and c is a shares of strategy. For example, if a is one third, b is one third, and c is one third, it means that I calculate 33% of my fund management assets under management using this strategy. 33% using this strategy and using this strategy. So basically it's a question how to combine them properly. It just makes no.
00:26:32.870 - 00:27:06.384, Speaker A: So I assume that c is one minus a, n minus b. And after that I just manually calculate all possible combinations and calculate the API and the profit and just select the best one. So this is the approach. It's quite simple. So you have, for example, you start from, I don't know, zero and you limit each coefficient at 0.5, for example, and the granularity is 0.1 and you just make cycle in the cycle and go.
00:27:06.384 - 00:27:34.328, Speaker A: So you can go to the code and pick and show. Thank you. Very cool. Okay, have a comment if this time. Yes, go. Very interesting project. I was just going to say that we've had two presentations on dynamic weights and I was just going to make a comment that one seems to be adjusting the weights short term to changes in price.
00:27:34.328 - 00:28:25.720, Speaker A: And then Vasili's one is adjusting the weights depending on, I think, I guess, long term, sort of asset management type strategy. So I just wonder maybe potential future research could kind of merge the two. Sort of. Sometimes you're sort of chasing short, you're adjusting your weights for short term trends and sometimes you kind of want to adjust based on long term trends because that's your prediction. You don't mind having different weights because that's your long term play. So, yeah, I was just saying that maybe a dynamic weight sort of combining a short term and long term play might be sort of an interesting avenue to think about. Yeah.
00:28:25.720 - 00:29:07.764, Speaker A: First of all, I want to comment really briefly here. So first of all, when we're updating the weights, we're also updating the composition. So we have poker agent that updates weights and composition by redeeming the walls that we need to decrease our allocation in and minting the walls that we need to increase our allocation. So basically we don't rely on arbitrage, in particular this case, but we also store all this value in the pool and try and to avoid this possible arbitration value drain. And since all these walls lp tokens, you can easily mint them or redeem them. It is not some other protocol token that you cannot print. You can print this money.
00:29:07.764 - 00:29:44.700, Speaker A: You just need to take UCC from one vault and supply the CC to another one and that's it. The gas costs are not so big, so now the gas cost is like $150 taking into account it's more than million dollars of capital. And even if you make it more efficiently by 1%, it will be $100,000 per year. So I think it's good. Okay, time's up and we should hurry up. Moving on with the next presentation since again, we have around ten today. Thanks Vasili, for sharing.
00:29:44.700 - 00:30:18.350, Speaker A: Everyone check out his hack MD and I'm pretty sure that this won't be the last time we'll see you at balancer simulations or token engineering events with your work. Thanks, Vasily. Our next team is Vasilios and Chris, initial parameters of liquidity bootstrapping pools sensitivity analysis. Yeah. Thank you, Angela. So, I share. Let me share my screen.
00:30:18.350 - 00:31:11.420, Speaker A: Yeah, we can see it. Good. Let's make it a little bit bigger. That takes some time. Okay, well, what we did, we used the balancer V two cat model to run a sensitivity analysis using catcat. Moreover, we created a streamlit web application to make the results more accessible to people. The main motivation, at least for myself, was because I'm new in balancer.
00:31:11.420 - 00:31:57.532, Speaker A: I tried to find a way to get familiar with the balancer protocol and the balancer liquidity bootstrapping pools. And I thought sensitivity analysis and a good way to start. So, what is the use case? The use case is that we have Alice. Alice would like to crowdfund her project, and she's planning to conduct an auction using balancer liquidity bootstrapping pools to distribute hundred tokens. This is the token a of our case, also in notebook. You can find it. And she would like to distribute 100 tokens within 99 hours.
00:31:57.532 - 00:32:46.350, Speaker A: This is our 99 timesteps in our simulation at an average price of one token b. So our team, myself and Chris, maybe a few words about my person. I am environmental engineer. I have an MBA with focus in finance and e business. I used to work for many years for utility companies in Switzerland. And since 2015, I'm working at the intersection of blockchain technology, decentralized finance and renewable energies. So, Chris, if you are available, you could also say something about yourself.
00:32:46.350 - 00:33:29.880, Speaker A: Yeah. So, my name is Chris Gabe. I'm passionate about technology. I studied at Sorbo University and K eleven before coming back to my home country where I decided to found a startup that will accelerate the digital inclusion. I also graduated from MIT boot camps innovation leadership where program where I learned about web three and all their use cases. And I simply fell in love. And in this research with Vasilis, I helped him with the infrastructure for the catcad model and setting up the streamnets.
00:33:29.880 - 00:34:22.108, Speaker A: Okay, thank you, Chris. So, our research questions very simple. Understand how distribution of token a changes over sweeping within the range of the initial parameters. The initial parameters are the weights of token a, the weights of token b, the swap fee, and we have something like the trading volume. Additionally, Alice would like to know which initial parameter is the most important parameter in this token distribution, achieving 100% token distribution. So the key metrics of our ratio question. To be honest, I did not exactly understand what the purpose is to say, but I think accuracy of our calculations is very important.
00:34:22.108 - 00:35:11.660, Speaker A: We don't take accuracy of models for granted. That's why we also spent some time to test our results. So this is also important to say. Additionally, I have to say that this tool is more for newcomers in balancer, it's not for experts. And this is more a high level overview than a deep analysis of the results. And this is more, we can use the results more quantitative and less qualitative, at least at this stage of the project, because a lot of things have to be done. So for our simulation, we use the balancer v two cat python model, also the cat cut standard notebook layout.
00:35:11.660 - 00:36:06.250, Speaker A: We calculated in the policy, in the policy function, using the math weighted class, we calculated the change of token a for every swap. And then in the state update function we updated the balances of token a. And actually that's it for all parameters. So the results. Let me go how to jump to the application? I don't know how to. Yeah, exactly. So let's go very fast to the application here.
00:36:06.250 - 00:36:39.290, Speaker A: Yeah, just say the first simulation, this is the weights of token a. I think you have to switch the screen sharing. No worries, 2 minutes left. Yeah, I'm not using this, not using the zoom very. Just have to stop screen sharing, I think on top of your window. Exactly. And now pull it up again with the application.
00:36:39.290 - 00:37:14.690, Speaker A: Okay, here we have the first simulation. This is the weights of token a. Then you can run the slider, see the results. Token a now from 0.5 to 0.1. You can find all the results explanations in the hackmd file. Now we have token b the same.
00:37:14.690 - 00:37:57.088, Speaker A: And also for the other direction, for token b. We see here, this is very much sensitive swap fee. I think this is really easy to understand. This is direct proportional to the amount you get out from token b. So that's why you also distribute 70%. And the final is our trading volume. So increase the value of one to two.
00:37:57.088 - 00:39:01.680, Speaker A: So the buying power of token b. So let's do the same again. So it's relative difficult to interpret the results, but we can say that we have. I take one example that when we increase the parameter of weights token a 100%, you can also distribute 50% of your tokens. We have here the situation where when you increase the weights of token B 100%, you distribute all the tokens a, but in less time, so you need less swaps. So most learnings once again don't take accuracy of models for granted, just test it. I think modeling keep it simple because it can be very complicated.
00:39:01.680 - 00:40:01.530, Speaker A: And I'm convinced that we can learn from each other. I learned a lot from Chris and Chris for myself here you can find all the links about the documentation, so open for questions. Yeah, thanks Vasilius, and thank you Chris for this presentation. We have time for at least one question. Certainly any questions? Here's how do you determine how does the simulator know when to stop selling tokens? I didn't understand this point. Yeah, the amount of token, in our model we keep everything constant, so that means that the amount of token b is also constant. There are 100 token b available, always.
00:40:01.530 - 00:41:28.836, Speaker A: Okay, got you. Any other questions or comments? I have one, maybe real quick. Congrats, lasidus. If you had one thing to recommend us to do for the real, like would you think of something from the learnings you had? Or maybe nothing can be applied already in practice. I think of course we can create a tool that helps people to understand how changing of parameters, what is the effect of distribution of the total value locked and other things. So when you have a liquidity, put something pool, a smart pool, and you want to change all these parameters, you have to really understand what is happening in your model. So I think this is a very good starting point to have something like a tool that people can set up their pools and also make sensitivity analysis with combined parameters, because in this case it just had only one parameter.
00:41:28.836 - 00:42:07.216, Speaker A: So we can also combine parameters. And I think it's also important to provide additional information, for example, how the spot price is changing, because this is really relevant. Changing the weights means you are making your token more expensive or less expensive. And this has an effect of course on the attractivity of the token and how you want to react with the market. Thanks. There's two teams working on building uis for lbps. Maybe if you want to get in touch with them, could collaborate.
00:42:07.216 - 00:42:20.380, Speaker A: Copper and Prime Dao. Yeah. Thanks for answering. Thank you. Ok, awesome. So thanks again, Vasilios and Chris. Thank you, Angela also.
00:42:20.380 - 00:42:41.250, Speaker A: Thank you. Great to have you in the group. And I'd like to continue now with a first piece of infrastructure built in the course of this research group. It's balancer V two js SDK. George. Thank you, Angela. Let me share my screen.
00:42:41.250 - 00:43:16.360, Speaker A: Okay. Is it up? Yeah, we can see it. Okay, cool. So I've been working on an SDK which basically tries to mimic the exact on chain behavior of various balancer V two pools, that is weighted pools and stable pools. This could be used for a variety of purposes. For example, you could run simulations with this. Another use case would be arbitrage bots.
00:43:16.360 - 00:44:18.400, Speaker A: And this SDK is actually going to be integrated into balancer smart order router. So I'm going to jump right into a demo of it. So basically, one nice feature of this SDK is that you can directly initialize local pools from onchain state. So here I just have to pass the pool id. And basically the SDK will initialize a pool with the exact current state of that pool. You can choose a block number and if that is specified, then the pool will be initialized with the exact onchain state of the pool at that exact block number. So to give you an example here, I'm initializing from the weighted diwet 40 60 pool.
00:44:18.400 - 00:45:01.048, Speaker A: And let me run the example. Okay, so here we can see the tokens in the pool. The first one is die, the balance that is in the pool, its weight, and the same for wet, its balance number of decimals and its weight. Then we can try to simulate swaps. So here we are simulating giving die to the pool, giving 1000 die and getting wet. And here we can see the result. That is, we will get 00:44 wet for those 1000 die.
00:45:01.048 - 00:45:59.052, Speaker A: And again in the reverse detection swap given out, we basically give wet to the pool in the exact same manner as before. We give diet to the pool and we will be getting wet. And again we get result of 00:44 this also includes the swap fee. And next we can also simulate adding liquidity with join exact tokens in for BPT out. So in this case, we are giving 10,000 die and ten wet as liquidity to the pool. And we'll get 315 bpt tokens in exchange for that. You can also simulate withdrawing liquidity from the pool.
00:45:59.052 - 00:46:48.108, Speaker A: So here we are withdrawing 315 ppt tokens from the pool. And in exchange for that we will actually get this amount of die and this amount of wet. TezK has support for weighted pools here. It also has support for stable pools here. In this example, I'm using the DaI USDC USDT stable pool. Let's run this as well. Basically, the pools follow the exact same APIs, no matter whether it's weighted pool or if it's a stable pool.
00:46:48.108 - 00:47:31.624, Speaker A: And one other feature of the SDK is that you can customize pools. So here we are not initializing from onchain state, but we are actually providing our own parameters. So we are creating a weighted pool with two tokens. We can specify the balance that we initially want in the pool. The decimals of the token, the weight, the initial supply of BPT balancer pool tokens. And also we can customize the swap fee and this would basically work exactly the same as before. It's just that we have more control over the exact parameters of the pool.
00:47:31.624 - 00:47:58.342, Speaker A: Let's run for this example as well. And, yeah, basically it follows the same API, and this gives you more control over simulating if you want. So, yeah, mostly that's it. Let me know if you have any questions. Great. Thanks for sharing. Go.
00:47:58.342 - 00:48:23.680, Speaker A: Thanks, George. It was me again. Great work. Two questions. I have one is, did you coordinate with. Oh my God, I forgot his name. He was doing the python SDK as well, right? Nico, right? Yeah, Nico actually.
00:48:23.680 - 00:49:06.720, Speaker A: With him, I think. Not necessary, but it would be nice if maybe it's too late now, but it would be nice to make sure you guys use the same structure, even though it's two different languages. But having a similar kind of structure would be nice. But, yeah, that's maybe too late now. And probably you did use a similar structure. The other question I wanted to make is you used everything in not in way, but in units. So if you have one die, you just input the number one.
00:49:06.720 - 00:50:08.580, Speaker A: Right. So I'm just wondering, is there a way to use the SDK? Using the way, if I have a one USDC, I would put a million and one die ten to the one with 18 zeros, or. Yeah, just curious how you thought about that, because the actual on chain calculations we use the way. Yeah. So basically for good ux, I decided that the SDK will deal with scaling. So basically here you give an amount of 1000 and the SDK will take care of scaling this 1000 to 18 decimals, which die has. So if you want to provide one way, you would have to actually put zero, eighteen zero, and then one.
00:50:08.580 - 00:50:21.174, Speaker A: Awesome. Yeah, from the real pool it gets the decimal. Yeah, exactly. It scales automatically. Yeah. That's really cool. Yeah, no, great work.
00:50:21.174 - 00:50:37.420, Speaker A: Thanks for that. Thank you. Thanks. I've shared the GitHub repository to this SDK in the chat. Check it out, try it out. And thanks again for sharing it and for building it. That's great.
00:50:37.420 - 00:51:16.670, Speaker A: Now, this was a first infrastructure presentation, and we'll continue with three additional research projects. Mark and Sebastian have been working on reimagining Amms for carbon finance. Mark, stage is yours. Thank you, colleagues. And I hope you can see my screen. Let me just pull this up into presentation mode. It's fine.
00:51:16.670 - 00:52:36.576, Speaker A: Great. So, good morning, good afternoon. And yes, I will quickly be presenting on our research on reimagining Amm driven carbon finance for sustainable development goals, sdgs. And it's very pertinent right now because there's been a lot of high level discussion on carbon finance and recently some interesting reports on how to both drive voluntary offset markets, but also in reimagining sort of carbon finance in a way that would both provide a global price and ultimately support the carbon financing transfers that are sort of agreed upon in the Paris accords. First of all, just to go through our inspiration, it is an SDR pool. So the SDR special drawing rights. It was the idea of a sort of one design for a global currency that is still used actually to sort of liquidity bootstrap central banks and belong to this committee in the IMF, which all do.
00:52:36.576 - 00:53:31.560, Speaker A: All members of the IMF do. And so anyway, it was using that same model to see if we could then replicate that as a carbon SDR pool of pools. So what we first took inspiration of was thinking around having wrapped fiat. So we just sort of simulated in our head, having wrapped stablecoins, or under the EU emerging regulation, the asset reference tokens, and then using the IMF SDR weights. There's a certain weighting that's already established on the currency baskets you're seeing there the euro, yen, dollar, pound and remembrance, or yuan, and then having paired markets. So as we quickly learned, the balancer math is inherently binary. It's sort of made for two token pairs.
00:53:31.560 - 00:54:35.820, Speaker A: And when you go beyond that, it changes sort of the calculation. So we wanted to sort of stick with that in making our pool of pools really effectively a pool of paired pools. So for example, we would use a fiat wrapped euro paired with that european carbon token. And that's how we establish the initial pools. And that pool of pool was then ultimately the larger pool of pools was each given the appropriate weight in align perfectly with the IMF balance. So in this amm driven carbon solution, what we are trying to hopefully show as an example of the power of amms potentially, is how it could bridge disparate carbon market jurisdictions. It is good at creating bridges between different markets, so long as there is fluidity between those markets.
00:54:35.820 - 00:55:19.900, Speaker A: This has an opportunity for efficient and effective policy intervention. And I'll explain why policy intervention comes into our simulation. But there was recently a report in how states could individually intervene into their markets to make sure that the price doesn't go too low or too high. So this is very much something that they're looking at, their market mechanisms. Usually it's auctions. So we're thinking about how amms might make that more efficient and effective, as it's ultimately an automated policy, and then having a larger vision for global price equilibrium, sorry. It was internal market mechanisms and then an international redemption scheme.
00:55:19.900 - 00:56:28.796, Speaker A: And this touches upon, as I said, this recent report on 100 billion dollar potential market for voluntary carbon offsets. So what we were trying to, in the grand scheme of things envision was on the left hand side. These are the. And this I took a little inspiration from actually the two presentations beforehand, because it does get both into the intervention policy that Vasili was talking about, and actually the sort of continuous flow that Vasilius was just explaining. So on the left hand side, that actually is supposed to represent carbon, and then you're having the various commitments and allocations that are given to the member countries in their local jurisdictions. And ultimately what we want to then move towards on the other side are the offsets that would be creating the credits. So it's actually very similar to having a slow transfer from the right hand side to the left hand side that would replace that carbon with actually just tokens on that ledger at the end, and all those offsets then created and paid for on the right hand side of the ledger.
00:56:28.796 - 00:57:28.240, Speaker A: So that was sort of that we wanted to first map out what was our grand vision. But then, of course, the simulation was going to be much more simple. And unfortunately, we did have a few issues running into this simulation portion, but we had at least worked out what we wanted to sort of test in the stock and flow modeling. And here. So what we were looking to drive was the carbon SDR policy. In just a buy and sell, we were looking at sort of marbles and robots. And basically it would be an intervention in the market on how they could intervene in either buying or selling to maintain price stability within those pools that are actually the carbon SGR is the full compositional value of those pools, of each individual five mini pools.
00:57:28.240 - 00:58:14.396, Speaker A: So thinking of it like a liquidity token for all of those pools wrapped together. So that was actually a bit. What we wanted to test was just sort of a buying and selling of that to see if we could then stabilize internally those dynamics towards price stability. Let me see, how are we done? We have 2 minutes. Okay, yeah, sorry. In the actual, what we were doing before we did in CadCAd, we were working it out. We forked the Cele's excel and we were just playing with that a bit further in making each of those binary weighted pools.
00:58:14.396 - 00:59:11.300, Speaker A: As I said, what we found out very quickly is that the balancer math is, as I said, made for two token pools. And otherwise, if you want to find spot prices in multi token pools, it actually exponentially increases because you have to then calculate for each paired pool. So we were trying to work that out just sort of individually, so we could see how this would play out in a five token pool that would ultimately, then the liquidity token would represent the value of all those. So that was a bit on the, and then this was the other side of that. This was then how we were going to be driving the individual carbon tokens against those basically liquidity token. And that would be what would be purchased and sold in our simulation model. What we then were also asking a question which was posed in the research was on hi ms up results.
00:59:11.300 - 01:00:17.224, Speaker A: And here we were looking at the difference between spot prices upon what we wanted to simulate was also market mechanism as purchases are made, and to make that a random variable. So to sort of simulate chaotic market behavior is the difference between the swap price at timestep a versus b. And we were thinking around that as often, providing insights into how liquid your pool is, and then between, at the same time step, how it is between the spot price and the effective price. And we sort of were looking at that from a perspective of how efficient and effective it is, because ultimately that's your spread. And if your spread was too large, this wouldn't really serve the efficiency and effective market. So we were trying to understand how we would look at those two to interpret the health of the pools from two different angles. Unfortunately, all right, we're not able to get the CAD, CAD in stock and flow in place because we had some issues on that.
01:00:17.224 - 01:01:22.220, Speaker A: So I regret that. But that is sort of our next step. And we've already written to commit to that. We want to randomize that market environment, like I said, in each of those pools, and then looking so various random changes in the pool swaps while then looking at the policy, which would sort of take this looking at buying and selling SDR with an aim towards equalizing the pool prices between each other. We also wanted to test, therefore, the liquidity provider stability, the policy intervention loss risk. So if you were to have an interventionist entity, what under various market mechanisms potential loss, would they be exposing themselves to? And is this an effective way to actually have price convergence forward looking? There was this voluntary ecological markets overview that is actually looking at the different standardizations. So then adds another level of complexity as you would have different price tokens.
01:01:22.220 - 01:02:05.836, Speaker A: So that would actually maybe require a further layer of pools that would have their own market mechanism in between the different standards. So that's one more wrinkle in the story. But we hope to then actually keep plugging ahead at this. And we've already raised it sort of within the hyperledger climate accounting special interest group. So we'll hope to maybe once we've taken this next step on the CAD CAD, on these initial assumptions, what will then get feedback from broader stakeholders. So, with that, I thank you very much and open to any questions. Unfortunately, we can't take questions here since we are over time now.
01:02:05.836 - 01:02:41.000, Speaker A: Thanks for sharing, Mark, anyways, and it'll be exciting to follow up, let's say, some weeks down the road, and see how your project is making progress. I'm excited to see more work. Thank you on that one. All right, we continue with another project. Again, a new use case for amms. It's QT. Mark and Octopus working on liquidity for alternative risk transfer instruments.
01:02:41.000 - 01:03:11.810, Speaker A: Brilliant. Thanks. Thanks, Angela, for the brief introduction. Yes. So this use case knits very tightly to our bigger initiative, which is Project Athena. Project Athena is more focused on trading digital securities, digital assets in a regulated environment. And this piece is the last piece of the puzzle that we are actually trying to solve today, and hopefully you guys will like what we have done so far.
01:03:11.810 - 01:03:39.288, Speaker A: So, this is the team. So, other than me, the rest of the guys are really well known, so no introductions required. But if Andrew or mark needs a couple of seconds to introduce yourself, so feel free and go ahead. Let's just move on with the presentation. Okay, brilliant. Okay, so, over to you, Andrew. You can lead it from here on.
01:03:39.288 - 01:04:10.310, Speaker A: I'll just switch the slides for you going forward. So, we were interested in looking at the use case research. So we were interested in insurance linked securities. These are risk transfer instruments that typically allow large companies to invest in unlikely risks and receive regular payments. We're interested in extending these. These are extended to non institutional investors. These are really good because they offer returns even when the interest rates are low and they're not correlated all to the stock market.
01:04:10.310 - 01:05:04.656, Speaker A: As long as the risk against which you are insuring does not occur, you get regular payments. The problem with this instrument is that the resale market is very limited. So right now, investors have take a haircut whenever they need to sell this, because basically the only market that's available is brokers. So our hypothesis is that we could implement Amms as a secondary resale market that would make it more cost effective for people who wanted to sell their ILS instruments by creating an ILSX token in an AmM pool. So there are three things we needed to do. We needed to make sure that we maintained regulatory transparency so that some portion of the fund is never touched. We also wanted to measure our price discovery and make sure that we could maintain liquidity of this digital asset at a low transaction fee.
01:05:04.656 - 01:05:38.176, Speaker A: And now I'll be handing it to mark, who will discuss the model and the simulation results. Just before we get in with mark, I just wanted to explain two things. One quick slide on the simulation parameters. What we've tried to mimic here is a cat instrument. For the sake of simplicity, a cat instrument is nothing but a bond as to how a fixed income asset behaves. And what we have done for this specific instrument is we've removed the risk simulation element of it. So it's just the financial parts which we're talking about today.
01:05:38.176 - 01:05:56.404, Speaker A: And generally these instruments have a lifecycle of three years. So Mark's just going to cover all the brilliance behind how we simulated that piece. Mark, up to you. If you want me to stop sharing the screen, please let me know. I will shut it off. I'll go ahead and share. Let's do it.
01:05:56.404 - 01:06:23.032, Speaker A: I can run through it really quickly. Can you guys see my screen right now? I'm just going to stop sharing, Mark, and you can share it now. Okay, let me know when you guys can see it. Insurance company Dow. Is this the one? Yeah. Perfect. Okay, so really quickly imagine insurance company Dow, and they occasionally put up a proposal as a request for collateral.
01:06:23.032 - 01:07:19.740, Speaker A: And this at the moment is a prospectus. And so they're going to approve on that collateral, which then gets sent to an insurance trust. This insurance trust is, of course, going to be a system of smart contracts, and its job is basically to read the request for collateral and then correctly price the ILS. How does it price the ILS? Well, there's going to be a whole bunch of these prospectuses already in the vault, and it can read through these one by one and basically sum up all of the USDC that is received as collateral and all of the ILS tokens that were issued for that collateral. So if you do this for a very large number of these policies, it's pretty straightforward to go and calculate an average price for this token. So, for example, you can imagine 200 and 6400 thousand dollars or million or billion dollars in USDC being requested. It can calculate how many ILS tokens should be minted and sold in order to cover that collateral.
01:07:19.740 - 01:08:15.300, Speaker A: So this is then going to go into effectively, it could be a constant sum amm, or it could also be a dutch auction, but institutional players would basically bid for those ilss. And if the collateral is satisfied, then the ILS would be distributed to them. When institutional players fail to read the collateral requirement or the reserve for that auction, then they basically get their money back. Those ILS tokens are destroyed and the insurance has to go somewhere else. So the insurance companies then, over time, will be distributing premiums to these policies, which causes the amount of USDC associated with it to go up. So for each one of these different entries in the blockchain, you can actually see in real time their value going up and down. Of course the value can go down when someone makes a claim on their insurance policy, and so you kind of end up with an aggregate of all of those different insurances.
01:08:15.300 - 01:09:07.156, Speaker A: After six months to three years, these institutions are then going to be able to claim the ILS back for the USDC associated with those specific policies. After they claim it, that policy is then effectively destroyed or canceled, or reached maturity, however you want to call it. So it might seem like this is going to be kind of piecemeal, but of course there's always another policy immediately behind it. You could be selling one of these every single day, which means there's going to be claimable amounts of USDC every single you can. After redeeming that amount of ILS, you can always redeem another one the next time it becomes available. So if we zoom out a little bit, you can see that there's always going to be this very large backlog of policies with a lot of USDC about to be claimable, and a huge amount of ILS on the other side waiting to claim that USDC. But it's still bottlenecked, right? You can't actually claim all of that ILS at any one time.
01:09:07.156 - 01:09:54.060, Speaker A: And so the idea is to create a constant product bonding curve in order to support institutional transactions between each other. So if you deposit both ILS and the USDC, you then receive what we're calling the ILS X, which is just a pool token. And this will then allow institutions to basically get ILS off of their books without having to wait for the expiry of the contract. This is helpful because occasionally you want to do this right, maybe there's another investment opportunity, or your fund is restructuring its risk, and at the moment, as Kirtsy said, there's like a 45% haircut to brokers right now. Whereas if you're just exchanging between each other that you paid the 0.3% pool fee. What's interesting is that this ILSX might be, from a regulatory standpoint, compatible with DFI and available to retailers.
01:09:54.060 - 01:10:24.652, Speaker A: So you can easily deposit that ILSX token together with USDC in a standard amm. And this would allow institutions to then trade with retail investors and also retail investors to trade with each other. So we kind of have got this fence between the restricted space and the retail space, which basically separates the highly KYC, highly regulated players from the retail players. And this is super fascinating. 1 minute left. Can we see 1 minute? Yeah. Okay.
01:10:24.652 - 01:10:48.096, Speaker A: Yeah. The results. Okay, so, yeah, I've simulated every aspect of what I just discussed, so institutions can swap with each other, provide liquidity, remove liquidity, they bid on things. Okay, great. And, okay, this is the last result. This is the only thing you really need to see this red line here. This is the amount of money not paid to brokers.
01:10:48.096 - 01:11:20.476, Speaker A: And over the three years of this simulation, which was only $8 million in collateral received, these institutions saved $40 million in broker fees. And so there are lots of things that we can play with. You can got the institutions perform pretty good arbitrage, whereas DeFi doesn't. I think this is pretty accurate, but yeah. So overall, we've established that you can create this system. And left completely unguided, this is full Monte Carlo. It still appreciates in price, it still accrues value to itself.
01:11:20.476 - 01:11:50.136, Speaker A: And the liquidity aspect of it seems to function without any intervention. So I'd say that the model is pretty well established now. We've got a really good understanding of how it operates, where some of its weaknesses are. But otherwise, I'd say that the hypothesis has been supported in this case. Brilliant. Angela, can we have like 10 seconds just to give your learnings and next steps, if that's okay, let's go. I guess everyone is fine with it.
01:11:50.136 - 01:12:18.444, Speaker A: Okay, brilliant. Okay, so what we are saying, the next steps are. So obviously, we're clearly established and we have done some validations to say that capital is ring fenced. Great price discovery for ILS, of course, compared to what the brokers are doing now. We have a fixed fee based transactions within the process. So a lot of democratization of the holding. So obviously, the conclusion is we are looking at a great return on capital invested.
01:12:18.444 - 01:13:06.770, Speaker A: So it's a great case for us to move forward. The next steps are the interesting steps. We want to move and take this to Finma and FCA as a use case, to say that we want to try this in the related space. Obviously, we want to do a white paper around it and a broader risk and compliance assessment around these things. Of course, the second part is to enter some parts of this design into the hydro pool, Hydra DX and the Omnipool piece, which will start soon, and bigger piece is more to do with technology integration and choice of technology, whether it be ethereum based or substrate based, by looking at integration with Project Athena, which is more to do with digital asset trading, which is the bigger project. So that's all from me. I'll stop sharing now.
01:13:06.770 - 01:13:24.532, Speaker A: Thanks, everyone. Yeah, thank you, Kirti. Mark octopus. Awesome project. Let's take one brief question. Just give. Hurry up.
01:13:24.532 - 01:14:00.092, Speaker A: There are any questions? Not at this point, but I think there's plenty to. Yeah, Angela, there is like a constant sum usage because I unfortunately cannot go to the hackmd and check it out. I'll open the access up for you. Apologies for that. Please do right away. I didn't end up using constant sum because it was too hard to simulate. Honestly, I ended up just doing a dutch auction facility.
01:14:00.092 - 01:14:28.248, Speaker A: I can show you how I've done that, but the effect is the same. Basically, you just want to make sure that the ILS is issued without any slippage. Right. Because the aftermarket effects of being able to trade with each other is going to be different. Right. You need to be able to offer an effective discount to the people that are buying those ILS tokens as new collateral is being requested. So you need to give them an ability to acquire a large amount of ILS without impacting the price.
01:14:28.248 - 01:14:58.608, Speaker A: So, constant sum was originally the way I wanted to do that, but it was clumsy and computationally intractable, honestly. So dutch auction system was the one that I went for. Okay, thank you. Good. Thanks for that. Again, the document is shared. Kirty will open it up, and maybe we'll have the chance to, in the course of the Hydra DX project, come back to this, to the simulations, and to your next steps.
01:14:58.608 - 01:15:25.630, Speaker A: All right, we'll have our final research projects in this first batch. Zero intelligence, imon. Zero intelligence analysis of amms is now on our agenda. All right, let's go. Can you guys see my screen? We can. Great. There we go.
01:15:25.630 - 01:17:08.300, Speaker A: My projects on zero intelligence analysis. So, the basic idea is, for those who don't know zero intelligence, agent based modeling is essentially simulating random behavior into a protocol or market mechanism and seeing if you can replicate realism, sort of statistical properties out the other end, and then you can attribute that realism to the market mechanism as opposed to strategic behavior. So, my idea was to take the sort of open source digital twin models in CADCAD of Uniswap and balancer, essentially shuffle the existing order flow, simulate it back into the mechanism, and then analyze metrics such as impermanent loss and slippage at the other end. So I'll just report on numbers done on uniswap balancers. Work in progress. So this is from the open source digital twin, a breakdown of all the events for the data set used. And see, most of the events are ETH purchases or token purchases, as opposed to adding or removing liquidity, taking an off the shelf algorithm or open source algorithm that was on the cat CAD GitHub.
01:17:08.300 - 01:19:07.780, Speaker A: By looking at the position of trades or events, how many decimal places the requested tokens go up to, you can sort of classify or estimate if the event was an arbitrager or a convenience trader, and you can see it kind of plateaus around after four. So using a four decimal or 40 rule, you can see that in the data set, a high proportion of arbitragers in the data set running that same algorithm, classifying algorithm per block, you can see that it sort of steadily grows during the data set used and around 50%, but towards the end around 80%. So, looking at different strategies, comparing the real and then my shuffled model. So with my zero intelligence model, I'm running 100 Monte Carlo simulation runs, and then taking the average of these metrics, comparing real versus shuffled or real versus my zero intelligence model. Sort of strategies such as just e modeling or holding the uni token or 50. Actually can't remember if that's ether uni, but one's holding 50 die and 50 I think it must be e holding. You can see all the strategies in the shuffled zero intelligence model make more, but have a lot more risk skewers, turns negative to positive, and a lot more non normal in returns.
01:19:07.780 - 01:20:23.950, Speaker A: Looking at just the start to end token balance growth, it's a lot more in the shuffled model. Obviously, because I'm running 100 simulation runs, I can calculate confidence intervals, and you can see that the mean of the real is not in the sorry, that the real is not in the mean of the shuffled model. So it's significantly different onto impermanent loss. So this is start to finish strategy returns. And the impermanent loss you can see in the shuffled model there is less impermanent loss so far. The first result, I guess, is that liquidity provision is more profitable in a zero influence model, as you might expect. And another result, I think that could be of interest, is that there's less impermanent loss onto looking at slippage, this was one result that kind of bugged me for a while.
01:20:23.950 - 01:21:31.692, Speaker A: So if you split the different events, the trading events, into token purchase and ETH purchase, you can calculate slippage as a percentage of perfected price, over spot price, and the shuffled showed more. Sorry, less slippage in the serum terrace model for total purchases, and more slippage for ETH purchases, less and less and more. And I kind of was wondering why that was. You can see kind of steady is out around 20% or 0.2% in the shuffled model. So one thing I have to sort of do a bigger sort of deeper dive into is looking at. Forget my simulations, just the idea that just looking at the real data is die purchase slippage more than ETH purchase slippage.
01:21:31.692 - 01:22:14.930, Speaker A: And one thing I've just run, actually, today, was if I take all the token purchases and calculate the percentage slippage, take all the ETH purchase transactions on the real data and calculate the mean, these are the numbers I get. But if I run the whole rerun the whole experiment on medians, there's a sort of clearer result. So, meaning there's a big sort of. It's very volatile. So just taking the mean even of just the real data, there's a big sort of variation in trades, in slippage trades. It's a big, wide distribution. So that's something I have to do a deeper dive into.
01:22:14.930 - 01:23:36.260, Speaker A: I did run medians on all the other metrics, and it doesn't seem to be a big difference, but this one is something I need to do just to see the distribution of slippage. And I might have to move away from calculating slippage as a percentage, but maybe calculating dollar value for slippage to just officially say, what is slippage in the real market? And then how can it compare in a zero intelligence model? Last metric I calculated was price efficiency. So the idea is, how close do prices resemble a random walk? So, this is something finance academics do to check informational efficiency. The idea being, if it's closer to a random walk, this sort of variance ratio, a short term and a long term variance, will be close to one. So the smaller the numbers are in this metric, the closer it is, the closer the price dynamics are to a render walk. So you can see that significantly. So, in the shuffled model, you have a lower number for this metric, which suggests higher price efficiency.
01:23:36.260 - 01:25:17.530, Speaker A: So, of course, I'm not saying that in a zero intelligence model, there's higher informational efficiency, because, of course, there is no information, but it sort of shows that it's closer to a random war. So the takeaway, then, potentially, is that arbitragers and strategic behavior in the real world actually add more noise to the prices. And again, that's something I want to investigate empirically in a deeper dive. So to conclude, zero intelligence essentially allows you to investigate the effect of just the mechanism and bypassing strategic behavior. So I hoping to say that evidence shows that adding strategic behavior reduces liquidity provider returns, because actually in the zero intelligence models, random behavior means more lp returns, potentially increases slippage if we take into account that median table that I showed. And strategic behavior reduces price efficiency, perhaps again, arbitrages and strategic behavior adds noise to the price. My HecMD needs a bit more work, but I will update it with a link to some academic paper I'm hoping to write on this and I'll upload that to SSRN pretty soon.
01:25:17.530 - 01:26:23.292, Speaker A: And the extension is, I'm hoping to use not just the uniswap digital twin, but also the balancer simulations, open source digital tool twin and play with that. That has a nice feature of including a sort of feed of external price updates. So that's something to incorporate that might uncover more interesting things. And of course, the zero intelligence model is always a baseline model, so you can always add to it. So whether it's more intelligence agents, arbitrage agents, external oracle feed, or intelligence agents with reinforcement learning or machine learning or genetic algorithm type strategies is something I can also look to add. So, yeah, that's a whiz through what I've been doing. Thanks, Iman, for sharing.
01:26:23.292 - 01:27:24.244, Speaker A: We have time for a quick question. Anyone in the chat? Yeah, Angela, could I make the little suggestion? So I think that probably this work can be even more interesting if you will work with V three, as in a suit, you use insrb two. But if you will check out v three and think about strategic behavior, position management or something like that, it can be really cool. But it is of course the big extension of the work. Yeah, obviously the shuffled replay needs actual data, right, to shuffle. One option is to actually, in zero intelligence models, people don't typically use data, but actually just draw from random distributions themselves. Maybe I'll do that if there isn't sufficient data for v three, but yeah, I'll look into that.
01:27:24.244 - 01:27:54.520, Speaker A: Thanks. All right, thanks again, imon, for your presentation. And now somebody asked in the talk about reinforcement learning integrated to catcat simulations. We'll have this presentation just in a couple of minutes. Stay tuned. First, I'd like to call up Nico now. So a couple of presentations included or were working on simulations in catcat.
01:27:54.520 - 01:28:27.530, Speaker A: And Nico was this person who helped everyone in providing version two, balancer pools, components for python simulations. And this is now also available to everyone in an open source repository. Miko, please walk us through your Python model. Hello? Are you there? Yes. Okay. Yeah, here I am. All right, so, hey, everyone.
01:28:27.530 - 01:28:50.868, Speaker A: It's been a good last few months. So this is about the Python V two implementation. So Python balancer pools. It's been a great coordinated effort from everybody. Super happy to hear all your guys'feedbacks. And together we've been able to build something really cool. So add liquidity, mimic, swap, and now simulate.
01:28:50.868 - 01:29:30.792, Speaker A: We've seen so many great projects, and I'm super happy and super excited to see all your guys'work from today and to be hearing about what you guys are building with it. And it's more than I could ever imagine. So, yeah, super excited and super happy to be seeing all of the work right here today. So we have the starter notebook. This is in the repo. This can help you get started if you haven't already played with it. It'll help you from installing the module itself to actually starting to use it.
01:29:30.792 - 01:30:04.532, Speaker A: This was made by Vasilius. So many thanks to Vasilios. This is a great starting point, and I feel like it helped everyone kind of begin the journey into using this Python module. So, pypy availability. So this is new since last time. You can now install it from Pypy. Yeah, super, super awesome to be on Pypy and put ourselves out there in the world.
01:30:04.532 - 01:30:37.470, Speaker A: And now anybody can use it. Fees are updated. Octopus pointed the sell last time, and now they're updated. So whenever you make a trade, it's calculated in the fee of the funds going in, and that's put into a dictionary. So, yeah, python poetry. So now it's easy for anybody to make any changes on a branch and even be able to install this in their own virtual environment. It's really simple.
01:30:37.470 - 01:31:28.960, Speaker A: It's almost like the next generation in setup Py. It's a poetry. So, yeah, shout out to Williams, a little known name from the LTF crew, for hooking up the Python poetry. Yeah, so this is a little demo wanted to run with you guys while we still have some time because I know some of you guys weren't here last time. Let me just restart the kernel. Oh, no, it's okay. Oops.
01:31:28.960 - 01:32:31.104, Speaker A: All right. Yeah, I guess it's not a real Jupyter demo if it somehow fails to work, but that's okay. Cool. Just one secondary. It's how we can initialize the weighted pool. Then we can bring the balances inside the weighted pool. Here are the amounts.
01:32:31.104 - 01:33:09.920, Speaker A: So we have 800 wet 47 WBTC 6237 dpi. And then these are the weights, so it's split evenly into three. Can run that and then we can do a swap. We've just swapped 20 wet for 1.16 WBTC. We can also do a swap for some DPi. So we can swap one WBTC for 130 dpi.
01:33:09.920 - 01:34:02.752, Speaker A: And now for the newer feature, we can get the factory fees all in their own respective currencies. That's pretty much it, yeah, it's been a good ride since last time. Have answered lots of questions, have worked with a lot of great people, and I'm glad to be seeing all your guys'work, so I'll go over to the thank you slide to say thank you and yay. We did cool. All right, great, Nico, thanks for sharing and thanks for building this, since we are all benefiting from it. Yeah, a round of applause. Are there any questions to the.
01:34:02.752 - 01:34:40.042, Speaker A: I posted a repository link and also there's a nice skit book with the documentation. Pretty cool. Any questions here? Yeah, I have one question. I see that there's a contract, a solidity contract in there. Are you referencing that in Python code or connecting to that somehow? No, actually. So this is a bit of an older screenshot. That contract is no longer there.
01:34:40.042 - 01:36:26.106, Speaker A: But that's from when we were actually porting over the code for the weighted math and the stable math. We were using it from that contract, so we decided to put it in the repo, though this was something that we were thinking about of should we use maybe like ganage or one of these interesting solidity testing tools to actually interact with the EVM? Doing this, we could get accurate decimal points too, because the EVM acts in a very specific way that's a little hard to mimic at the very specific decimal point. But we decided to go against it and just create a fully organic kind of python representation for a like lightweightness and b, yeah, we thought that might be better for simulations as well, so that they wouldn't be having to send a lot of transactions and stuff. But it is still an interesting route and something we'd love to explore. But it's a very different road from where we are, and it's a road we left behind many kilometers ago. Okay, any further questions? All right, thanks again, Nico, for sharing this. And with that, this question was already leading towards token spice.
01:36:26.106 - 01:37:21.598, Speaker A: So you probably realized most of the simulations so far have been done in Python using catcat as one of the major frameworks developed by block science, now open sourced with a dedicated community. So I think already at least two years of work have been put into building CADCAD. Now there's a new framework we decided to test and further explore in this research group, which is token spice. And before we show you more examples of the work that has been conducted so far on token spice, Trent is here in this call to provide some more words and the context on token spice. Sure. So, hi Angela, thanks for having me. And hi, everyone.
01:37:21.598 - 01:38:01.174, Speaker A: A really good presentation so far. Really great, very cool to see and great progress. So I'm just going to take a few minutes and talk about token spice and point to it and so on. And basically by background, before I came into the blockchain world, I spent almost 20 years in the world of semiconductor CAD, building CAD tools for designing computer chips, software that's used by the likes of Sony and TSMC and Qualcomm and the rest. And in that world, simulation is a key thing. It was invented in 1972. The first good simulator was invented in 1972 out of Berkeley and called Spice.
01:38:01.174 - 01:38:43.482, Speaker A: And since that time, spice has been a key part of verification. And basically what you do when you run spice is you input a net list, which is basically a listing of the different building blocks that you say, I've got a resistor here, a transistor there, et cetera. And then you say, simulate, please. Maybe do a transient simulation over time or something else. And then it gives you results that you can then plot, et cetera. And that's become a standard tool for verification. And then starting in the 90s, people started to do spice in the loop, where basically you could have sweeps for design variables, sweeps for worst case parameters and so on, as well as Monte Carlo simulations and optimization and synthesis and so on.
01:38:43.482 - 01:39:24.562, Speaker A: And there was actually a lot of symbolic tools at the time that weren't quite as accurate. They tried to basically simulate faster, ten x 100 x faster, but they weren't nearly as accurate as spice because Spice made a point of being as close to the silicon as possible with really detailed equations, modeling actual silicon data. So spice took off, and actually a lot of the other modeling stuff kind of ended up being used a little bit in the very beginning. But the spice stuff that's really accurate ended up being used a lot. So given that I'd spent a lot of time in that world, I decided to start building something that mimicked that way of thinking for token land. And I called it token spice, given that I come from that world. And so that's a bit of the background.
01:39:24.562 - 01:40:10.406, Speaker A: I initially built it without EVM in the loop in order to build the web. Three sustainability loop a model for it, which is sort of the system level design for ocean. But then also I decided that I wanted to verify more at the level of the balancer amm staking and so on. My initial approach to that, this was before there was any digital twins of it or anything in Python, was simply to try to copy and paste port the solidity code to Python. And I found that quickly became very error prone and difficult. So I thought, well, I've got ganache, so I'm just going to get ganache running with the ocean contracts and the balancer pools, et cetera, and have a wrap around it. Ocean has a built in Python driver for balancer v one, basically, and run with that.
01:40:10.406 - 01:40:27.606, Speaker A: So that's built as well. And I've generalized it now to support more arbitrary netlists. So I'm just going to give you a bit of a walkthrough of what this looks like. It's only a couple of minutes given time, but give you a feel. And before I do, first thing I will do is share a link. Oh, there we go. Link token spice.
01:40:27.606 - 01:40:36.160, Speaker A: Perfect. Great. Someone already shared it. Thank you, I guess, Angela, thank you. So I'm going to share my screen here. Let's see here. Token spice share.
01:40:36.160 - 01:41:15.210, Speaker A: Okay, so it's a GitHub repo Apache, two fully open source, blah blah blah, and the readme basically walks you through what it does. It simulates tokenized ecosystem using an agent based approach. Each agent can be pure Python. Any arbitrary python you want doesn't have to be equations or anything. And those agents themselves can be drivers for solidity, including just like Nico just presented a balancer v two driver that could be integrated into token spice quite readily and then wrapped. So that's an example. So in the table of contents, it talks how you can do initial setup, do simulations, make changes and so on.
01:41:15.210 - 01:41:50.882, Speaker A: And we'll just go through the initial setup basically is you get the repo, you get ganache running, and then you deploy some more contracts to it. I give a couple of default ideas of using the ocean contract, but you can do other stuff too. And then its command line is just TSP, short for token spice. So you type TSP enter, it gives help. But if you want to run a simulation, here's what an example simulation looks like. TSP run assets netlists, simple grant netlist by. So we've got several example netlists in the repo, and they're all under the assets section.
01:41:50.882 - 01:42:25.986, Speaker A: So one netlist is simple grant. It's got simply two agents, a grant giving agent and grant taking agent, it will spit out the results to outdo a csV. And then once you have those results as a csV, then you can also run a plotting command, and it will plot the results using Matplotlib and plot py. And it basically grabs it from the CSV and puts it to the PNG. And here's a couple of examples from this. In this case, this is from the web. Three testimonial, loop and ocean token count over time, the total supply, the total minted, the total burned, that's on the left and on the right.
01:42:25.986 - 01:42:51.798, Speaker A: It shows stuff on a monthly basis, what's burned and so on. So this is very helpful here. And there's also plots you can get, of course, when using EVM in the loop. So you can have an iterative session of doing different simulations, making changes, trying different parameters, whatever you want, with or without EVM in the loop. And that's what this is about. It talks about the agents and netlists here, how the agents are defined. They're all inside assets, agents directory, like I mentioned, they're all in Python.
01:42:51.798 - 01:43:17.186, Speaker A: Every agent has its own web three wallet, and it holds a private key and creates transactions. So meant to be very simple. And then basically there's base agent on all the other agents inherit from base agent. Something like pool agent basically is a wrapper for bpool Sol. And that's it. It's a very thin wrapper and stuff. And in this case, right now, it's off of Balancer v one, but you can have v two and stuff too.
01:43:17.186 - 01:43:53.610, Speaker A: If you use mucos code. And then the netlist themselves, they're basically specifying a few things. What things you want to simulate, what things you want to measure. So things to simulate, as in what is the schematic, what are the parts, how do you want to connect them, what are the parameters of that? And basically under the hood, it can have agents, et cetera. We've got example agents and netlists, sorry. So for example, simple grant netlist, it has two agents, a grant giving agent, a grant taking agent. If you look at the code itself, it's basically about a page and a half here, and it's got stubs.
01:43:53.610 - 01:44:30.198, Speaker A: It's the one called Sim strategy, and it inherits from the base sim strategy and has a couple more parameters, how much initial ocean to give, how many seconds between the grants, et cetera. And this is just a basic demonstrator of the simple grant agent, but also as an example of simple pool agent is just a very simple example of an agent that has a single publisher agent. And then every now and then it goes and creates a new balancer pool. And that of course has Evm in the loop. So you have a very simple agent to go and use that. And I'll show you how simple this is. This is publisher agent, whoops.
01:44:30.198 - 01:45:00.818, Speaker A: I'll go to the netlist code and basically you can have some simulation strategies. How much initial ocean does the pool have? Sorry, the publisher have, et cetera. A few more things, the weights and so on. And then basically, yeah, there's some plotting instructions and a couple more things. I guess the main thing is here is where you wire up the agent. In this case it's just a single agent, it's just a publisher agent, which is its own class, of course. It's got some initial parameters here and the rest specified up here.
01:45:00.818 - 01:45:21.018, Speaker A: And then every now and then it creates actually a new agent which is stored in this dict of agents. So that's it. I encourage you to look into more detail. But that's a summary there. Yeah. And actually to give you a feel of some more plots, the web three sustainability loop. This is a much more complicated example because it's what the thing was first built for.
01:45:21.018 - 01:45:47.438, Speaker A: So here's the initial net list, and then it has quite a few different metrics, or KPIs being modeled. And then you get many plots coming out such as this. I think I've shown a couple of these, R D span, monthly, ocean minted and burned dow income, token price even, and more. Right. So that's very useful if you want know, analyze that. So I'll stop there. Basically, the summary is, this is a new way to do simulation.
01:45:47.438 - 01:46:54.050, Speaker A: I view it as complementary to CaDCAD, and I'm a big fan of CADCAD and the block science team, et cetera. I see CADCAD as a symbolic analysis tool, which is good for drilling on equations and so on. But with every new smart contract code you write, you have to go and somehow write a twin. But given that ganache exists, et cetera, this is much more useful for token spice, is much more useful for rapid iterations by changing your solidity code in the loop, the structure or the parameters, et cetera. And this is basically the benefits of what you get when you have EVM agent simulation, rapid iterations, high fidelity simulations, a very simple mental model, and the opportunity for real time analysis, optimization, et cetera, even against live chains. So you can even run that against live chains and update on the fly and stuff, right? And actually I'm quite excited because I see that an accurate simulator like this is the baseline for much more detailed higher level tools. And in circuit land, here's the high level tools, there's design entry, like schematic editors and waveform viewers.
01:46:54.050 - 01:47:13.262, Speaker A: Waveform viewer isn't design entry, but it's part of it. Then there's verification tools, system identification, corner extraction, and more. And then design space exploration tools. So there's about ten or twelve tools that are standard tools you see in circuit land. And as part of it's probably about $1.5 billion in sales for those tools total. And the simulator part is 800 million.
01:47:13.262 - 01:47:46.502, Speaker A: So what? Half of that? And I see an equivalent opportunity here in token land, but it's actually much bigger because token land is actually, the simulation verification is going to be much bigger than semiconductor land. So I'll stop there. I think token spice is exciting. It's still pretty young. Like Angelo pointed out, CAD Cat's been around a couple of years. Token spice, it's basically been mostly me for the last year and a half, only as a part time thing. I have other duties, but I've taken it to a point of maturity where the community can start leveraging and using it, and I continue to keep improving it as well.
01:47:46.502 - 01:48:25.760, Speaker A: So with that I will stop. Cool. Awesome. Thanks for this overview and also preview how this might become relevant in the future. And we can now already see first teams working with it, like the next team, Ocean Spice, who started since June working on a project that is dedicated to support ocean data markets. Who is going to present? Is it Robin? No, I'm going to go. You guys see my screen? Yeah, we can.
01:48:25.760 - 01:49:48.118, Speaker A: Okay, so thanks, Trent, for the introduction of token spy. And we're actually using token spy to simulate the ocean market ecosystem. So our team consists myself, Robin, and Richards, who already involved in ocean ecosystem. Sean is a well known token engineer who's supporting me in token engineering role. And Sarah? Sarah is not here and she's working on the web app things, and I will elaborate more detail later about her role. So our research starts with the question that how can we use token spy to analyze Ocean V three and Ocean V four? Analyze, compare them, and make simulation of ocean V three and V four. We have been doing it step by step.
01:49:48.118 - 01:50:45.420, Speaker A: The first is implement the rest of the data consumer agent, and with that being done, we go ahead and then simulate the v three market dynamic. And we want to make tokens buy more accessible for broader audience via a web app application. The web is actually upload on GitHub, can take a look on. And with this question, we actually got granted from Ocean Dow route seven to be what we want to be, basically. So pretty excited about that. So. Yeah, and actually the, the web app is being done by Sarah, who I mentioned before.
01:50:45.420 - 01:52:22.390, Speaker A: So with these questions being answered, we want to show here some of the scenario that we simulate with ocean v three with Ocean spy, token spy, of course, we have a couple of scenario. The first two are about the bad, let's say a bad publisher who create a better token and create a pool and then just want to get more ocean from everybody else. The third scenario is about, the third scenario involve data consumer, where we have publisher and then we have data consumer buying data token, but we got a pretty greedy speculator who tried to attract value from the pool. So we go with each of these scenario in detail. The first scenario is, let's say we have a publisher, publisher named Robin. Robin has initially ten k of ocean initially. And then Robin go ahead and then create a data token, 100 data token.
01:52:22.390 - 01:53:15.766, Speaker A: He create a pool with three to seven weight, and then with the pool of, he put everything in the pool, 100 data token and then ten k of ocean. And then he advertises. It's really good pool, it's really good data. And then everybody should participate in the pool. And then stakers start to stake ocean in the pool, 1000 ocean pool into the pool every day until day ten. As you can see in the orange line, it's going up every day. So at day number eleven, Robin decides to rock pool, and then basically he unstake everything.
01:53:15.766 - 01:54:53.450, Speaker A: And then he leaves the pool. And then he got away with almost 15,000 ocean. That's the first scenario. Second scenario is even a bit more severe that, let's say another publisher named Angela, she go to create a pool like Robin, but she only put 30 data token in the pool, and then keep another 70 data token staker in scenario one, every day stake 1000 token in either pool. And then in day eleven, Angela decide to unstake everything and sell everything, and then leave with almost 17,000 ocean number. Scenario number three is we involve consumer data, consumer agent. So let's say a good publisher named Sean, he created data, and then he put every data token in the pool, also with 10,000 ocean.
01:54:53.450 - 01:56:54.926, Speaker A: And then there are data consumer buying the data token every day from the pool. So that's why we have a blue light, this data in the pool going down and then the orange going up as the consumer keep buying data. But the staker got into the pool really early in the first day and then staker put 10,000 ocean in very early day. And then in the day ten he decided to unstake the pool token and then leave and then get about two and a half k of ocean. So these three simple scenario that you can see the disadvantage or a change that we need to make in ocean protocol. That's why Ocean decided to to go with the v four that essentially eliminates these kyrie. So this is also our future work that we will complete ocean v three dynamic and documentation and implement ocean v four mechanism and then continue with implementation of the web application and of course build more simulation with token spy which I think Sean will elaborate more on this later.
01:56:54.926 - 01:57:51.270, Speaker A: So that is all from our group. Awesome. And I think you started with all this end of June, right? So this was not the full three months. Yeah, actually I start from beginning of this batch about in the middle of June I think. Yeah. Any questions here or comments on this work? Again, you can find the link to this project in the chat. Maybe some of you can also add the web app or if there's any GitHub that would be cool to explore.
01:57:51.270 - 01:58:50.474, Speaker A: Yeah, I haven't pushed the work that I have done but yeah, we push it soon on GitHub. Sure. And there will certainly we'll continue collaborating here also for the community working on token spice, exploring token spice further, some announcement at the end of this call. Now before we close it, we have two more presentations, one infrastructure one, and as next team to present blocksmooth working on foundation behavior, definition and intelligent agents. And here we have a case. You guys decided to also compare catcat and token spice. I don't know if we hear some more thoughts on that.
01:58:50.474 - 01:59:28.722, Speaker A: One stage is yours. Okay, I'm sharing my screen. First of all, thank you very much everyone for attending the presentation and for the incredible presentation that you have shown before. Everything is very inspiring. On your question I would say that yeah, we were trying to compare katcat and token spice, but at the end it was doing the work parallel. So we just quit using token spice, not for any specific or technical reason, just because we only need to one. And it was a little bit more mature cut candidate token spice.
01:59:28.722 - 02:00:15.702, Speaker A: And as I said before, developing our platform in both of them would be like doing the same work twice. So the presentation today, as you say, we are focused on the foundation but mainly on the intelligent agents. And how can we integrate this intelligent agency in catcat? Let me check if I can. Yeah, sorry. So the team of people working here is a blockspop team where Harry is our CTO, and George and Manuel are also collaborating. And Daniel, that is me, is the one that presenting here. We all come from the engineering and computer science world, and we have pretty good experience in intelligent and autonomous agents.
02:00:15.702 - 02:01:19.190, Speaker A: Not applied to tokens, of course, but we can recycle some knowledge from our experience there. So, basically, which is our idea? Our idea is we work in a company, and we need to define the behavior of the foundation. And of course, the token ecosystem is composed by many different situations, let's say users, different kind of contracts, different kind of business that these users want to create. And for sure, a lot of technical issues with regard to the simulations, but also regard with the daily work of the platform itself. So our idea, based on our experience, is okay, as far as all the DeFi, or most of the DeFi situations, can be mathematically defined, we want to find optimal solutions, or try to find optimal solutions for all of them. And for that, based on the current state of the art in the simulations, we decide to apply some artificial intelligence and intelligent agents to the existing simulators. What we're going to show today is just a small example based on price stability.
02:01:19.190 - 02:02:22.238, Speaker A: We just want to show that it is possible to create intelligent agents and perform properly there. And of course, just by changing utility function of these agents, we can achieve different targets. So our research question, we can divide it in two different parts. The first part is, is it possible to use intelligent agents to really make a difference or to do some influence in the token ecosystem? And the metric that we're going to use here is basically the success rate. It's a pretty common metric in artificial intelligence, a prompt for the accuracy and so on. But at the end, you just want to check how many times your system were properly out of a certain amount, and you can measure it in a percentage. And the second research question is, once we have an intelligent agent, or starting from zero, would it be possible to train this intelligent agent inside the cutcat simulator or any other simulator where the metrics here are, of course, yes or no? This is just a technical issue.
02:02:22.238 - 02:03:09.066, Speaker A: And the second one, as I said before, is the success rate. We have proposed two different situations. Here is the first one, where we have the intelligent ecosystem for reinforcement learning, which is the most common one, is called gym. And for the people that is not used to work with artificial intelligence, we can just say that it's something kind of similar to katkat. There's an ecosystem where you can define the environment, you define the agents, and you define the utility functions that you want to maximize and also the functions that define the behavior of the environment. And then you just let the system run and you extract some knowledge. This knowledge is usually analyzed and classified through neural networks.
02:03:09.066 - 02:03:56.800, Speaker A: This is deep reinforcement learning. And then you have a fully trained agent that can perform an optimal behavior whenever you connect it to a new environment. The problem that we have here is that when you go to cut, cutcut, let's say in the hierarchical level, is at the same level as Jim. So cutcut is also running the simulation, and cutcut is giving access to one agent, or different agent, is giving access to the estate functions and the state update variables and so on. So the integration of these two systems is a little bit tricky. So our first approach was just to take the knowledge that we extract in the reinforcement learning and apply it with some changes of course in the architecture. Token to the token simulation, you have to cut that.
02:03:56.800 - 02:04:50.542, Speaker A: And this works prefined. Of course, it depends on how good the agent is trying, but this works. The second simulation that we have is, okay, how can we integrate these two things together? And as I said, due to the hierarchical level and the hierarchical programming of these two libraries, we need to include one inside the other. The easiest way for people with experience in artificial intelligence is just create an environment and a cutcat environment inside gym. Because Jim allows you to use some predefined agents, some predefined functions, that it's proven that they work better so they are easier to train and you can then achieve better results. The other approach could be to integrate kind of gym into cutcat. This is unfortunately not possible to do.
02:04:50.542 - 02:05:25.100, Speaker A: So what we can do is just modify the agent and develop all the policies, reward and utility functions. These are internal variables for the reinforcement learning. We can just hand coded them inside cut. Both of them represent a lot of programming effort and is more into the, let's say into the technical issues or into the architecture than into the research. So we just decided to focus in the first one. So let's say that the one at the left is out of our scope, mainly due to the technical effort to develop it. So we focus in the other one.
02:05:25.100 - 02:06:11.714, Speaker A: We are happy to share some results that are shown here. So as I said before, our use cases to show how the technology works was the price stability. So where we define a minimum price and say, okay, whenever the price goes down, you can autonomously decide to act, how to act, with which amount of tokens, and so on and so on. In the graph of the left, we can see the training loss. This means with the training steps. How good the algorithm is, the closer to zero the better. So as you can see, there is a super fast training at the beginning of the steps, and then it stabilized at a value really close to zero, because the scale here was minus seven as an exponent.
02:06:11.714 - 02:06:40.142, Speaker A: So ten to the power of minus seven. So it's a really low value. The example, of course, is not realistic and it does not represent the real market. So just consider this as a value and say, okay, it is learning. The knowledge that we extract from this agent, of course, is not that useful. And here on the right, we can see how these results extrapolate to the token simulation. We have a reference value, which is in orange.
02:06:40.142 - 02:07:48.470, Speaker A: We have a value of our token of interest, which is in blue, which is oscillating somehow there is kind of a stable price around 70, and something happens at some point. We are not analyzing what happens, just the final result, and then the price goes under zero point 65 with regard to the other one. And then the foundation performs some specific action and the price is again up. Of course, what we want to show here is the possibility to create these intelligent agents and to make these driven decisions completely autonomously. The use case, it is not that relevant because LBN is trading and everything is possible if you have enough amount of money and so on. And as conclusions, I would like to say that the main difficulties that we faced were the integration between GM and katcat, as mentioned, we could solve them somehow and we could optimize the problem. But the limitation that still exists is that there is no creation of knowledge.
02:07:48.470 - 02:08:43.100, Speaker A: Here are some more information about reinforcement learning. Reinforcement Learning is basically a trial and error problem, which is driven by the environment. Let's say that it learns in which direction it is learning more, and it decides to continue exploring in this direction. If we integrate the reinforcement learning agent, but not the environment, into cutcut, we are not able to explore the situations that could be optimal, let's say. So we can just learn from the actions that we can see, but we cannot create new knowledge. And this is the main limitation of the application of this solution. Here then active agents, of course, we're working with simulated environments or even with historical data, does not work properly, because the analysis of the influence of our actions into some generated data is not going to be realistic at all.
02:08:43.100 - 02:09:40.940, Speaker A: And the definition of intelligent driven functions is complex and does not scale. This is more or less the problem that we have been discussing in token spice in the last weeks when we are creating the net list. When we create the net list, we just define the number of agents and what each agent is doing. But if we are defining 1000 agents, for example, and we need to define intelligent, let's say utility function, for each of these 1000 agents, this is a tremendous work. So the automatic creation of these agents and these intelligent functions should be also implemented. On the positive side, we can say that yes, it is possible to integrate pre trained agents in katkat ecosystem and they work effectively. It is also possible to train them inside the simulations, which is a big advantage I would say.
02:09:40.940 - 02:10:34.702, Speaker A: And of course, just by changing the utility function of our agents, we can apply this structure to many different problems here. As I said, we are using a small example, which is really easy to understand and to see which is the price stability. But many other of the products that have been discussed here could benefit from this intelligent application, the application of these intelligent agents and optimization tools, and that's a little bit from us. Until here, we have a lot of ways to connect to us and also our GitHub, where everything is going to be updated. I don't know if we are going to publish everything also in HackMD, but in GitHub all the documentation should be available and also the source code. And yeah, that's everything from us. So thank you very much for listening and thanks.
02:10:34.702 - 02:11:12.568, Speaker A: Ready for questions? For some reason GitHub repository is not available. Not sure if it's public. Ok, I'll just check it again. No worries. Any questions here? I mean, I would be keen to learn more about your next steps. Maybe some words. Yeah, actually we have a new planning after next week, I would say, where we are going to analyze the internal results that we get and which is the most effective way to apply them.
02:11:12.568 - 02:11:54.856, Speaker A: Because as I said, this is a tool, this is not a final solution. And just by changing some small parameters and some small functions inside intelligent agents, we can solve. Not directly of course, some work is required, but we can solve different problems. For example, the problem that, sorry, I forgot the name that we're addressing with a dynamic weight weights in the pools can be somehow optimized with artificial intelligence and many others. So we just need to decide which is the most beneficial step for us now. But we will continue working here. Awesome.
02:11:54.856 - 02:13:10.946, Speaker A: I just see that Fernando has to leave. I don't know if you have a couple of seconds still to provide some wrap up from your point of view. Fernando, are you still there? Okay, he probably has to jump off, but I think Mark is still around. Okay, yeah, great. So any other question on blocksmooth? Daniel's work first, not at this point, feel free to check out the resources shared. Oh, there. So elder is asking, are you going to implement calcat Zoom environment despite difficulties? This is something that we need to decide and it basically depends in the final application because as I said, the difference between applying reinforcement learning as a proper environment or just applying artificial intelligence to analyze the data is that you can create knowledge or not if all the actions are pretty much defined.
02:13:10.946 - 02:13:59.550, Speaker A: There is not much sense in creating this whole ecosystem with reinforcement learning, but just with some classification with some intelligent agent that is much more simple to analyze and can directly work in the simulator. Okay, thanks for this. Thanks again. And we have a final presentation. And this one is actually work that has been produced by all the research group members. It's our repository for playing around with the core value function and gaining intuition on balancer pools in general. And it's BK to present this repository and the collection BK.
02:13:59.550 - 02:15:10.870, Speaker A: Got it. You guys able to see the screen? Perfect. So, yeah, along with the research sessions like the key academy planned the explorations. So we were trying to achieve the learning code, balancer AmM algorithms. And along with that, Angela and other members prepared the math exercise for us. And basically our goal was to understand how the balancer AMM works and attempt the math challenges and gain the intuition on how each algorithm would work, basically. So, moving on, how we developed these results, we had weekly sessions organized for sharing and reviewing the exercises and whatever work we did alongside.
02:15:10.870 - 02:16:50.280, Speaker A: And we had long public discussions on the Discord channel, particularly for te balances simulations, where everyone shared the results, the formulas, and everything which others can explore and comment on. And most importantly, many leaders like Mark Richardson, Vasili, shared their unique approaches on how to solve questions. So, moving on, this was a pretty important lesson, which Mark was explaining how value moves on the AMM curve. And yeah, this is just a snapshot. And moving on, I'll share one of the exercise so that everyone can know what we were working on. So I'll just pick small exercise, not taking so much long time. So this exercise was to like, we have the token ab with 100 balance and equal weights, and we were going to perform the 99 swaps one by one, swapping out token a each time.
02:16:50.280 - 02:17:47.800, Speaker A: So this is the solution by Vasilias. And he did initiated the initial weights and the balance, setting this web three as zero and calculating the invariant here, which would remain equal throughout. And first of all, he created the series a, which is given in the question. It would reduce by one each time step. And running this, we have initial data frame restored which has a token a balance decreasing from 100 to one in the pool. And now we would start removing token one by one. We are keeping invariant as the same.
02:17:47.800 - 02:18:38.982, Speaker A: And here we are using this formula for calculating b, one token balance. And yeah, this is it. And once we do it for each rows, we have this as a result. So we're having token b for each step. When token a was stepped out and we can see invariant. Yeah, it was kept same as it is. And another approach around this same calculations was offered by mark where he here we did all it all at each single time step.
02:18:38.982 - 02:19:47.550, Speaker A: But Mark did it all organically in python and setting the balances using the individual function in python to swap it out in a loop for 100 times step. But I have done this just here with the balancer v two CAD model, which Nico just explained previously. So I'm just setting the pool, initiating it with zero fees and token a and B with 0.5.5 weights. And I'm using swap for each time step in the loop for 100 times. And yeah, getting the. Okay, so this is the same result which we get by another method.
02:19:47.550 - 02:21:08.150, Speaker A: And yeah, basically this is how different solutions were said along the sessions we had. And next on, we had some challenge questions from the team, which is right here. I don't want to go in deep, I know you are all drained out with this long session. So yeah, please try to explore around if you want to gain intuition on how these amms would behave with. There are many different scenarios to explore how Amm would behave under different conditions. And this is all presented in the GitHub repo we have in token engineering communities, balancer pools, AmM explorations. And soon we will have all user solutions which we have on the main branch for everyone around the community who want to learn and gain the knowledge of amms and particularly balance pools.
02:21:08.150 - 02:22:16.686, Speaker A: So yeah, I'd like to thank everyone for presenting their work results and like to thank Angela for this opportunity to present our community based, valuable output for communities to learn along on balancer and amm pools. Thank you, Angela. Thank you BK for collecting all these components and contributions on this repository. Any questions on this one? I still think we are missing some of the solutions, so hurry up and make sure to submit them to BK. And if there's anything else then, I mean, we've seen 13 presentations today, a really awesome variety of use cases of simulation approaches. This is really awesome. And it's just a big leap for the community.
02:22:16.686 - 02:22:37.560, Speaker A: Token engineering on balancer. Poof. And with that, I suggest before we close. We switch on our mics and give a round of applause to everyone in this research group. I really enjoyed the time with you guys and I guess we can all be proud of the results. This is to you. This is to everyone.
02:22:37.560 - 02:23:31.714, Speaker A: David, great to have you finally. And now, first, maybe Marcus, some final words before I continue with what's next. Thank you, Angela. Thank you everyone. Again, very happy to see the results of all your projects. I've been following for the last few weeks, coming to the sessions and seeing some of the progress, but was happy to see you come across the finish line today and some of the projects that I wasn't aware of and only got acquainted with today. Thank you, Angela, so much for organizing all of this, putting the group together, making sure we interact with each other and can learn from this heterogeneous group that has so much to contribute with each other.
02:23:31.714 - 02:23:57.140, Speaker A: We are very excited with this and looking forward to the next phases of this partnership with the Token Engineering Academy. Thank you everyone. Awesome. Thank you guys too. Thank you, the whole balancer team again. And also thank you, Trent, if you're still around, I can't wait to continue with token spice. And with that, let me share some final words on what's next.
02:23:57.140 - 02:25:13.986, Speaker A: You notice that some of the teams are actually, at least 50% are planning to continue and work on their simulations, work on their use cases, and that's amazing. There's also token spice open sessions coming up, so we'll continue to gather the token engineering community on token spice simulations. Sean, would you like to add some more words on that one? Absolutely. Thank you, Angela. Yeah, I want to say this is a once in a lifetime opportunity coming up to be involved in a collaborative development team working on building out token spice to be a next generation, state of the art token simulator. So, of course, it's originally developed by Trent McConaughey, and it's used to simulate the sort of economic implications of ocean protocol and run simulations around that whole ecosystem. And Trent has been recently curating a whole backlog of issues for the next steps of developing out tokenspice as a fully rounded simulator.
02:25:13.986 - 02:26:16.170, Speaker A: Actually, the netlist has just been implemented, so it now has this modular injection of different simulations, and it's becoming really reusable and modular and expandable. So it's an interesting time. And also, Trent McConaughey is offering his time as a mentor along this project. And it's really an opportunity to meet some other software engineers, data scientists, token engineers who are just going to collaboratively do pair programming and share a lot of knowledge towards the build out of the backlog that's been identified. And there is an ocean DaO grant recipient which is oceanspice which we saw a presentation from Trang and we are looking for help and I think roughly about one third of that grant is allocated towards token spice development and running simulations and things like that. So definitely reach out to the team and stick around for updates on scheduling. Thank you Angela.
02:26:16.170 - 02:26:56.742, Speaker A: And yeah, again, sessions will be on Thursdays five to 07:00 p.m. Every week and we'll announce anything you need. So the link and all the repositories on Twitter. Follow us for upcoming news on the token spice open sessions. And there is another project or another research group just about to lot. I think Kirty will be part of it with his project as well. Omnipool Engineering Simulations this is a collaboration with block science and HydradX, in this case cross chain liquidity and optimizing fees.
02:26:56.742 - 02:27:48.440, Speaker A: But it's not only working on Hydra DX, it provides a really solid introduction into the engineering process, system design, generalized dynamical systems, parameter selection. So it's a great overview and we'll gather again 20 ish group of co researchers in the field. And again, the sessions are open August until October. We'll publish these events as well at token engineering and I hope we'll meet each other there. Again, I enjoyed the time, thank you so much. Don't leave, so if you want to stick around, you can hang out. We'll leave the call open until nine.
02:27:48.440 - 02:28:06.140, Speaker A: Feel free to stay in the call and let's have a casual final session for the balancer simulations. And thank you everyone for joining tonight. Great pleasure to have you all in the session. I.
