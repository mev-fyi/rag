00:00:00.330 - 00:00:49.066, Speaker A: All right, good morning everyone. My name is Eric and I'm an engineer at RISC Zero. Today we'll be talking about what RISC five has to do with RISC zero zkbm. And so before we begin diving into things that are super specific to zero knowledge proofs, I thought let's start with a background on programming languages. So we'll kind of build up some context around it, and then we'll kind of tie everything together to zkbms as well. And feel free to speak up if you have any questions. I have a hard time seeing the chat, so yeah, just feel free.
00:00:49.066 - 00:01:30.362, Speaker A: And there will be spaces for questions as well throughout the presentation. Anyway, so let's begin. Let's begin with programming languages one on one. So let's say that we have code in rust, all right? One of the features, nice things about programming languages is that it's meant for humans and it's not really specific to one computer architecture. So let's look at like a hello world. For example, in this hello world example, you can see that the code does not expose anything about the specifics of the underlying computer that we're using. And at times we could.
00:01:30.362 - 00:02:24.022, Speaker A: But in general, programming languages are intended to kind of hide that information a little bit so that we don't always have to think about it. Now, when we want to run the rust code, we use a compiler to generate assembly code. Assembly code is meant for machines and it's specific to one architecture. So the assembly code has notions of hardware registers or the instructions that will get executed or run by the hardware. So let's take a look at assembly code for a little bit. So this is a disassembly of a binary executable file. And this section right here in the blue box is a bunch of data, really, and encodings of these instructions.
00:02:24.022 - 00:03:09.658, Speaker A: And then to the right we have the disassembly of what is in the middle section. And so we can see that these data disassembles to things like addition, for example. So the top line says add I. So add negative 16 to the stack pointer and then save it to the stack pointer a little bit. On the way down we see MVA one a zero. That just means that we move the contents of the a zero register to the a one register. And so as you can see, we have a lot of details about the underlying system.
00:03:09.658 - 00:03:51.734, Speaker A: Now to the left we have locations associated with these opcodes. We can think of these as addresses. One of the advantages of writing in a language like this is that we have very fine grained control over the system, and it's actually really fast. There are some reasons why we don't want to write an assembly. Can anyone off the top of their head think of any reasons? We can just shout hard.
00:03:51.772 - 00:03:53.942, Speaker B: It's just hard to read and develop with.
00:03:54.076 - 00:04:49.420, Speaker A: Yeah, and I guess, Victor, thanks. And one of the questions that we can ask is like, what does this code even, you know, a lot of you may not be able to guess it, but this code, actually, it is code to print a character on the terminal screen for RzkpM. Now, the only way that you could kind of understand or even guess at that is to look at the top that says runtime putchar. A putchar function usually does some sort of printing of characters anyway. So yeah, one of the downsides is the limited expressivity. So a lot of the times when we're writing code, we're trying to communicate with other humans. And so assembly is limited in that.
00:04:49.420 - 00:05:41.050, Speaker A: Another disadvantage is that it's specific to machines. So if I were to write assembly code like this, it may not work on different, other hardware. And another thing is, with the limited expressivity and the specific to machines, it's difficult to maintain. Now, this is the reason why we program in programming using modern languages today. So anyway, to kind of view we have a compiler that compiles source code like rust code to assembly file. And note that we have this notion of ELF. And ELf is basically just like a formatting for executable programs, for binary executable programs.
00:05:41.050 - 00:07:01.538, Speaker A: So there are other types, but we won't get into that for this talk. Anyway, as I said in this diagram we see that it is targeting assembly, but compilers nowadays we can target many different architectures from the same source code. And in order to do that we pass through what is called an intermediate representation and called LLVM, which is an industry standard tool. So what's usually done is rust code gets compiled to LLVM code, and then LLVM can emit code for X 86, which are which is instruction sets for basically all desktops and laptops except for Macs, as well as servers and an arm, which is for cell phones or Mac devices. Or we can also target risk five. And notice this isn't just true for Rust, but we can also add other languages. If other languages can compile to LLVM, then they can have this advantage too.
00:07:01.538 - 00:07:36.240, Speaker A: So with C plus plus we have clang, and with go we have go or tiny go. So this topic is about RISC five. So let's zoom in on this a little bit. So what is RISC five? Let's start by breaking down the acronym. So, RISC stands for reduced instruction set computer. And we can think of this as each. This is a set of assembly code where each of the instructions perform simple operations rather than combining multiple at a time.
00:07:36.240 - 00:08:25.390, Speaker A: The latter is called a complex instruction set computer. And the five stands for the fact that this was invented in UC Berkeley, and it's their fifth risk ISA. And it's defined in a specification. It's freely available on a PDF online. And so let's take a look at the contents of this real quick. So, within RISC five, we have 32 registers, where each registers register can hold 32 bits, as well as a program counter that holds a 32 bit number. The difference between the registers and the program counter is that the program counter holds the location of the next instruction to be executed.
00:08:25.390 - 00:09:14.890, Speaker A: And we also have the instructions. So this is a grouping of the, this is all the rv 32 I base integer instructions. So as you can see, we have addition, we have move operators and store operators, as well as control flow. And this is all that is needed, as well as a few environment related instructions. And this is mostly what is needed to run a lot of simple programs. As you can notice, we don't even have the notion of multiplication or division, because those can be expressed by as addition or subtraction. The interesting thing about RISC five is that it's modular.
00:09:14.890 - 00:10:05.118, Speaker A: So there are a bunch of extensions, one of which we implement is the multiply extension. So we don't have to translate the multiplications into additions and subtractions. Kind of saves on the code sides a little bit. Another thing to know, as I mentioned earlier, is that there are instructions that are used for special functionality. So for example, we have a code like this, which moves a bunch of values into different registers and does an e call. And when you do that, then it will do something special that is specific to that chip or that environment. So anyway, that was a brief tour of RISC five.
00:10:05.118 - 00:10:42.570, Speaker A: But what does RISC five look like? We've seen parts of this specification. Well, as I said earlier, it could be implemented as hardware. So this is the low five r one chip. The processor executes risk five code. And I think the processor right there is like the black square below the writing, low five r one. Or it could look like a virtual machine. And that's what we implement is the RISC five as a virtual machine.
00:10:42.570 - 00:11:33.900, Speaker A: Then what that means is that all of the notions of registers and memory are implemented as software as well as the instructions. So let's kind of go back to the bigger picture here. So we have rust code that is compiled to RISC five and is executed through a vm, which results in some sort of effect. Maybe we return a number, do some complicated computations, print hello world and whatnot. So at this point, what questions do we have? We're kind of done with the programming language background. Does anyone have any questions?
00:11:36.830 - 00:11:59.890, Speaker C: So, we have the LLVM intermediate representation as well. Is the reason why you choose to arithmetize the RISC five code instead of the LLVmir directly, that it has a more succinct representation, so it's easier to kind of create polynomial constraints?
00:12:01.110 - 00:12:43.460, Speaker A: That's an interesting point. I don't have a great answer for that, because with constraints, it has to do with the specifications. Right. I think another thing about LLVM is that, from what I know, it is a little bit abstract, right? It extracts the notion of registers so you can technically have like infinite amount of them. So it might consume a little bit less memory during runtime, is my guess there.
00:12:45.510 - 00:12:52.626, Speaker C: Okay, thanks. I might point people to the ZK podcast that Jeremy and Brian did.
00:12:52.648 - 00:12:53.586, Speaker D: I can put a link in the.
00:12:53.608 - 00:12:57.960, Speaker C: Chat, but there's some discussion there about why specifically they chose Risc five.
00:13:00.730 - 00:13:07.880, Speaker A: Yeah, and we'll discuss that a little bit towards the end here. Any other questions?
00:13:09.290 - 00:13:22.714, Speaker D: Yeah, questions on the. Just curious how you guys implement the risk five emulation of VM. Are you guys using the cumul codes for emulating the risk five chip?
00:13:22.842 - 00:14:09.480, Speaker A: Great question. So, we actually have a domain specific language, an embedded domain specific language that is used for writing is. We're not using QMU or anything like that. It's a standalone program, and along with the circuit definition, we write also these constraints as well. Right. Anyone else? Let's move on. So, now that we've talked about programming languages, let's just connect this to the zkvms.
00:14:09.480 - 00:15:03.480, Speaker A: So, the goal of a ZKVM is to generate a receipt for program execution. And the goal for risk zero's ZKVM is to generate receipts for risk five program execution. So that means that in addition to all of this, we need trace of the risk five execution, as well as a set of constraints. So let's alter this diagram a little bit. Our risk five virtual machine will emit any sort of result along with traces and constraints. And then that is then used to generate a receipt. There's a lot of intricate math that goes on into generating a receipt.
00:15:03.480 - 00:16:01.630, Speaker A: We have a tutorial on how this seal is constructed. So for those of you who are interested, I highly recommend reading about that. Anyway, let's focus on the constraints. So the examples of what a constraint would be within our context would be things like every bit should be a zero or a one, and that can be expressed as a polynomial, like this. So since this is written in a domain specific language, x is not always guaranteed to be just a zero or a one. It's a variable that can hold values, right? So we have constraints like this to ensure that if x is a bit, then it is going to be a zero or a one. Another one is that each register should contain 32 bits of data.
00:16:01.630 - 00:17:16.740, Speaker A: And another more interesting one is the program counter should change after each instruction, because if you recall, the program counter is basically like a register that holds the address for the next instruction to be executed. Now, there are other constraints that we could also glean from this specification as well. But hopefully we see at this point that the constraints are from the specification and how that fits into the ZKVM. So let's put it all together now. So if we want to write code for the risk zero ZkvM, we start with languages and we compile that passes through all VM and goes to RISC five. This is the program to be executed. And so that is run on the RISC five vm, or risk zeros Zkvm, which results in a trace, and constraints which is used to generate the receipt.
00:17:16.740 - 00:18:24.936, Speaker A: So at this point, maybe quick pause to see if anyone has questions. Okay, so one of the questions that we get is why risk five? So compared to other instructions at architectures or other low level representations, it's minimal. As we saw, all instructions fit on a slide. Other instructions sets are very large. And what this does is it increases the size of the memory footprint of the ZKVM, as well as making it difficult to implement. Another important advantage of RISC five is that it's modular, and so we only use what we need. RISC five has many extensions, and some of them we implement and others we can leave behind completely.
00:18:24.936 - 00:19:37.220, Speaker A: So one of the features that we do not implement is the a feature which stands for atomic instructions. While this may not seem like a big deal, the atomic instructions can imply that we are in a multithreaded context and we have the ability to say, no, we'll just execute in a single threaded context and not implement the atomic instructions. And another advantage is that we have great language support. Existing languages and tools are very mature. The modularity is supported by the tools as well. Anyway, another question that we get is, how can we tell which code executed on the ZKVM? And so, in order to solve this problem, we have this notion of image ids. And so when we load an image id is basically the hash of the binary when it's loaded in memory, and this binary.
00:19:37.220 - 00:20:16.240, Speaker A: So then we have an l file, we take the shot of that, and we put that into the seal so that when the verifier comes along, the verifier has a notion of, oh, this is the image id that I'm looking for. Then it can check the seal and determine that the desired image was executed. So anyway, that is all I had, all the content that I prepared for this time. So does anyone have any questions or comments or reactions or thoughts?
00:20:16.900 - 00:20:37.156, Speaker D: Yeah, comments on the, I guess, paralyzing your execution, your ZKvM. So if the underlying hardware has a multicore. Right, can you take advantage of the multicore by. I don't know, is that possible to paralyze the execution of the ZkVM to make it faster?
00:20:37.188 - 00:21:15.190, Speaker A: I guess we do have, during the generation of the seal, we use the multicoloreal capabilities, and I guess you could, in theory, run the ZKVM in parallel, like multiple jobs at the same time. But we don't really have a notion of a multithreaded execution environment at this time.
00:21:16.120 - 00:21:26.170, Speaker D: All right, so I guess the ZkVM architecture itself sort of assume a single core. Single core architecture. Is that correct?
00:21:26.620 - 00:21:58.230, Speaker A: Right. Yeah, for now, yeah. It could change in the future. But we're also thinking of, one of the things that we have to think about is, what is the notion of time in a ZKVM, or a lot of these primitives for execution, for example, what does it mean to sleep for a second? Right.
00:22:01.320 - 00:22:02.070, Speaker D: Okay.
00:22:03.160 - 00:22:32.104, Speaker E: Perhaps to expand on that a little, I will add, if you're thinking about, hey, internally, to the ZKVM, do we have multicoring? Well, as Eric just pointed out, you don't really have a notion of time, but if you're thinking like, hey, I've got this nice multicore processor, can I take advantage of that? The answer is very much yes, in the sense of, we have some GPU acceleration support. If you poke around for how you can find, oh, we've got metal support. The only reason you would be taking advantage of a GPU is to have multiple threads.
00:22:32.152 - 00:22:32.412, Speaker D: Right.
00:22:32.466 - 00:22:45.970, Speaker E: And we totally are able to take advantage of that, but that doesn't show up as the ZKVM itself. Is multithreaded that shows up as we can use multiple threads to do proving about it.
00:22:48.020 - 00:22:53.840, Speaker D: Yeah, I see. So I guess, sorry, maybe sorry if I'm rambling.
00:22:55.800 - 00:22:56.548, Speaker A: I don't know if you.
00:22:56.554 - 00:23:20.270, Speaker D: Guys support right now, if you guys allow the multi thread threading in the, if I'm writing rust code with the multithreaded. Right. First I guess, first question whether it's to support it. And if you guys do support it, then I guess is it possible to have those multithread actually be executed or running on multiple core gpu? Like you mentioned.
00:23:27.760 - 00:24:38.788, Speaker B: One thing you think about here is why does multithreading exist? Multiple cores exist in a cpu, and multiple cores exist in a cpu because you have limits on how fast a given core can run, and so you get more performance by having multiple cores and splitting your jobs across those cores. If, in case, in the context of the ZKVM today, adding multiple cores as an internal mechanic to the arithmetic circuit, to the ZKVM as an actual physical construct, using the analogy from hardware, basically implementing multiple cores instead of just having multi threading on one core, you wouldn't get performance advantage today because the bottleneck is the proving time. The bottleneck is can you do the entities, can you do the witness generation? And so having multiple cores in the ZKVM circuit would not improve your performance. It would probably slow things down. And so, at least for today, there's no reason to implement multiple cores in the ZKV circuit. It actually would not provide a performance improvement. If you want to run multithreaded code, like code that is written to be multithreaded, you can.
00:24:38.788 - 00:25:02.270, Speaker B: It's just that basically the scheduler will schedule all threads onto one core, which is not necessarily a problem, but basically. So as far as true concurrent execution, kind of like what Eric was mentioning before, concurrency is kind of a weird concept in the ZQVM because time is not defined as usual, and so.
00:25:04.880 - 00:25:05.244, Speaker D: You.
00:25:05.282 - 00:25:07.112, Speaker B: Kind of think about things a little bit differently.
00:25:07.256 - 00:25:21.510, Speaker D: Yeah, I think that's a great explanation. Yeah, thanks, I guess. Like you said, for example, calculating the witness, right. That's pretty time consuming, right? If you guys somehow can paralyze the witness calculation, that's already a big win, I think.
00:25:22.520 - 00:25:38.104, Speaker B: And there are ways to do that. And basically, I think that's something I believe that we are working on, is basically ways to paralyze witness generation in the ways that it matters. Obviously, we're always looking at basically where's the bottleneck today in our proofing system, and how can we eliminate it?
00:25:38.302 - 00:25:41.704, Speaker D: Okay, great. Yeah. Sorry if I ramble too much. No problem.
00:25:41.902 - 00:25:59.170, Speaker A: It's like the diagram that I've shown, the process diagram. It's really not to scale. The math portion is like a huge bottleneck. Yeah. Compared to the overall system. Any other questions?
00:25:59.860 - 00:26:06.290, Speaker C: So, we have this instruction set, and then you combine the instructions, and you get a program, whatever.
00:26:06.980 - 00:26:11.264, Speaker A: I assume that there is some mapping.
00:26:11.312 - 00:26:22.840, Speaker C: From each instruction to some subsurface, but do you also combine, say, typically used combinations of instructions into subsurfits for efficiency gains?
00:26:23.900 - 00:26:54.560, Speaker A: So we don't combine the instructions themselves, but we do have special circuits for primitives such as Sha and things like that. Because if we were to just run Shaw straight on the ZKVM, we'd just run out of cycles. So we have those kinds of optimizations, but we don't. Yeah, go ahead.
00:26:54.710 - 00:27:08.630, Speaker C: So, apart from those particular primitives, it's pretty much like in isomorphic like, you have this instruction, then we add this subsurcuit, and kind of legoing that in that way.
00:27:10.700 - 00:27:21.800, Speaker A: I'm not sure what you're talking about with what you mean by subsurfits, but each of the instructions are represented within the circuit definition.
00:27:23.760 - 00:27:48.630, Speaker C: Yeah, I'm thinking in terms of. I believe you guys use heirs, right? Like the stark primitive for arithmetization, but thinking in terms of the more plonk like arithmetic circuits. You would say a subsurface is like, it's just a circuit on its own, but you can create a circuit by gluing many subsurfaces together.
00:27:51.480 - 00:28:18.190, Speaker A: Now, I could be wrong about this, but I believe our circuit might be monomorphic, except for some of those optimizations. This is a good question. Maybe we can get back to you on this. I'm not as knowledgeable in this part yet.
00:28:19.440 - 00:29:00.772, Speaker B: I think I might understand if you imagine an Alu, you could, for instance, implement, or be running a multiply and an add at the same time in the circuit. Is that kind of the idea? Yeah. I don't know. I guess I understand your question, although I don't know the answer either. One thing I can say is basically that there is an intermediate decoding step. Basically. So when the instruction comes in, the circuit basically decodes the risk 32 instruction format into an intern representation, which uses basically binary flags, like set this split flop to this level, set this split flop to this one, and that will perform an ad in the circuitry.
00:29:00.772 - 00:29:16.284, Speaker B: And so we definitely have that kind of thing. And you could obviously imagine, and as we do revisions on the hardware, which we are doing constantly you can imagine that we might do something like that where basically you take two instructions and actually squash them together. Do we did it today?
00:29:16.402 - 00:29:17.470, Speaker A: I don't know.
00:29:19.620 - 00:29:21.010, Speaker B: That's a great question, though.
00:29:27.600 - 00:29:52.144, Speaker A: Any other questions? All right, so thank you for attending, and here's some references as well as our social media next time. Victor will be talking about finite field implementations on the 15th, so thank you.
00:29:52.182 - 00:29:54.850, Speaker D: Everyone, for, thank you, guys.
00:29:57.140 - 00:29:58.510, Speaker A: And see you next time.
