00:00:00.650 - 00:01:19.810, Speaker A: And I'll just sort of give a very quick explanation of how to go the other way. In fact, and I won't be too serious about all of this, I'll just say that, okay, let's say that we have a bunch of evaluations and we want to get the polynomial back. And just to point out, this is sort of what the FFT is most like, I think, because with the FFT, we add a bunch of sine waves, and here we're adding a bunch of monomial terms. We're trying to find a bunch of coefficients for x cubed, for x squared to match the data that we have, basically. So just to remember the matrices, right, this was the matrix equation that we had, and what we really want to do is to invert this matrix, right? So if this matrix is m and we multiply it by this vector to get this result, then to go back from the result to the matrix we started with, we want to multiply by the inverse matrix. We want to do a sort of matrix that's the opposite of what we had before. And so what we need to do is we need to find a way of inverting this large matrix.
00:01:19.810 - 00:02:44.482, Speaker A: But it turns out to be very convenient, because this matrix has such a clear structure with its sort of multiplication table exponents, properties, that, in fact, the inverse of the matrix is actually very similar to the matrix itself. So here on the right is actually what the inverse of the matrix on the left is. You basically just replace the twos by threes, and then you multiply the whole thing by four, or really the inverse of four, but the inverse of four is actually four. And so this is all to say, right? You can check, if you want, to ensure that if I were to multiply this matrix by this matrix, I would actually get this sort of identity matrix that doesn't change a vector at all. But the sort of stunning realization is that because these matrices are so similar, I can just apply the same algorithm but with powers of three instead of powers of two. And that would give me the coefficient out of the evaluations. So it's sort of fascinating that this Fourier analysis problem, the forward problem, is actually the same in some sense as the backward problem.
00:02:44.482 - 00:03:30.640, Speaker A: You're just doing it with a slightly different generator. As we mentioned, two is one generator of this multiplicative group, but three is the inverse of two. And so I just do the same operation but with the inverse of the generator, and it gives me back the result. And so that is basically a summary of the discrete Fourier transform so thanks for coming, and definitely feel free to ask any questions you may have about all of this. Yeah, it was great to talk to you all.
00:03:32.530 - 00:04:09.770, Speaker B: Hi. Thank you for your presentation. Very impressive. Yeah, my question is, actually, for the last slide you gave, let's say inverse of the first one, I'm wondering, how can I come up with three and four? Is that easy to get such number, or I need to any rules or any formula I can use to just simply get three and four from the previous matrix.
00:04:10.450 - 00:05:24.180, Speaker A: Right. How do we get three here? I could have been more clear about this, I think. But the way that we get three is that three is just the inverse of two. So three is the number that, when multiplied by two, results in one in the finite field that we're working with. So how is it possible when you have a finite field, to, given a particular element of that field, find another element such that they multiply to one? To do this, you use what's called the Euclidean algorithm. So there's basically an algorithm which says that I have two multiplied, but I have two, and I want to find some coefficient to two such that when I have five and some coefficient of five, they sort of multiply together to get one. And so basically there's an algorithm for evaluating the inverse of any particular element in a field, or at least in a prime field.
00:05:24.180 - 00:06:36.458, Speaker A: So that's how you get three from two. And then what is four here? Four is basically just the size of the field here. How do we get four? I've written it as four here, but you could actually maybe think of it a bit more appropriately as the inverse of four. Or you could think of it as maybe just negative one would be a better way of thinking about it. So the key realization is that if I multiply, let's say that I was going to multiply a particular row of the matrix on the left and a particular column of the matrix on the right to find the coefficient here. Basically, the easiest way to think about it, maybe, is to just remember that the upper left most cell of the identity matrix has to be one. So if I take a bunch of ones, right, the top row here is a bunch of ones, really, and the side column here is actually a bunch of ones.
00:06:36.458 - 00:07:09.720, Speaker A: And so if I just multiply this matrix by this matrix, I'm going to get something that has a four in the upper leftmost column, because it's just going to be a sum of four different ones. And so I need to take something which is the inverse of the inverse of negative one. Right. The inverse of four, which is negative one. And so four here is sort of meant to stand in for negative one, which is in the field with five elements. Negative one is the same thing as four.
00:07:11.850 - 00:07:27.450, Speaker B: Okay, I get your point. So another question actually is where can I apply these rules to metrics? I mean, for general or for some specific situation, I can use this rule.
00:07:29.470 - 00:08:06.230, Speaker A: Yeah. What has to be the case in order to apply this rule? So basically it has to be a matrix that has this form of, all of the columns here are sort of ascending powers of a particular value. In the case here we have of. This is known as the DfT matrix. Right. The discrete Fourier transform matrix. You can also call it a slightly more general thing to search for is a van Dermond matrix.
00:08:06.230 - 00:08:39.906, Speaker A: And so basically the rule is it has to be some element where each column are just increasing powers of that element where the power always increases by a fixed amount. So if I look at this column, the power goes from zero to two to four to six. And. 0246. That's a linear progression. And the same thing is true with this last row. Here it goes.
00:08:39.906 - 00:09:17.022, Speaker A: 0369. And, and that's a linear progression. So whenever all of the rows of the matrix are a linear progression that starts at zero, the powers are Powers where the exponent is a linear progression that starts with zero, then it will be possible to do a version of this rule that inverts the matrix. And what you'll get is a matrix where all of the columns are that way. Right. So the inverse matrix, the columns are always, we have a linear progression. 0123.
00:09:17.022 - 00:09:23.230, Speaker A: A linear progression. 0246. So basically that's just going to be the general rule.
00:09:25.110 - 00:09:29.218, Speaker B: Oh yeah, I recall that. Thank you very much.
00:09:29.384 - 00:09:30.900, Speaker A: Yeah, thank you.
00:09:32.390 - 00:09:35.346, Speaker B: So is this method a recent invention.
00:09:35.378 - 00:10:45.190, Speaker A: Or has it been known for hundreds of years? That's a good question. So, of course, the sort of use of this technique for the sort of cryptographic process of Starks is pretty new. I mean, that's sort of where my expertise lies. It really came about, I think, in 2018 or so was when the research on the stark paper came out. But it's definitely the case that Fourier analysis and the sort of basic ideas behind these transformations, they have a very long history, I think going back even to the 18 hundreds. If you watch the veritasium video like I did recently, I think it was around the 70s when the sort of discrete version of the 48 transform was first sort of realized. They realized that it would be a convenient way, I guess, of doing sort of seismic analysis.
00:10:45.190 - 00:11:08.060, Speaker A: That was the first application, sort of like sensing earthquake waves, I think. And so that was how it started. And basically that was the first time that this had been implemented as an algorithm. And I guess since then it's been sort of very useful and.
