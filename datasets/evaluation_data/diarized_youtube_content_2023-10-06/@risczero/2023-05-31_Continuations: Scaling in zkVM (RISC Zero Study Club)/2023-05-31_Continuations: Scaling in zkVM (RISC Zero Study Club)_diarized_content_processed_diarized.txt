00:00:00.810 - 00:00:45.162, Speaker A: All right, well, good morning, everybody, or at least good morning in my area. Thanks for joining. I'm Frank Lob, the CTO and co founder of Persero. Yeah, today we're going to talk about continuations, which is an exciting new feature to allow large programs to be run efficiently within the RCKVM. All right. So, yeah, some motivating sort of background material. The problem we saw that bitcoin was an interesting solution to basically using economic means in order to solve the sort of, how do we verify that other people are computing things correctly? But we ran into some problems with that.
00:00:45.162 - 00:01:36.730, Speaker A: It wasn't exactly without issue. There's not really any privacy. The programming model was very limited in bitcoin, and it really just doesn't scale. Right. The idea is that everybody has to rerun all of the computations, and so doesn't really lend itself to being able to sort of farm out the work to lots of different participants. Zero knowledge gives us the possibility of solving a lot of these problems, but in its current state, it's kind of hard for developers to access because they need to have sort of expertise around how to build these arithmetic circuits, which are kind of difficult to build and to maintain. And a lot of the times, you don't really get full Turing completeness.
00:01:36.730 - 00:02:15.334, Speaker A: So, yeah, it's just kind of hard to develop these circuits. So we wanted to try a different approach, which is basically build a zero knowledge vulture machine based off of the RISC V ISA instruction set. Yeah. So based off of these sort of things that we've seen in the past, we wanted to try to leverage a lot of the experience that we've learned over decades of computer engineering. So, yeah, some of the things that we've learned over the years. Right. Open source seems to be here and here to stay.
00:02:15.334 - 00:02:54.862, Speaker A: Lots of people are using it. I think corporations, individuals, all sorts of people realize its importance and its power. Computer hardware. There's this kind of general theme here, which is that generality seems to be better than specific, or at least generality seems to win out in the long term. You kind of see this in machine learning. You can see this in various other things where basically generality seems to be the thing that once you have a general thing, you really get a lot more power out of it. So scale also matters.
00:02:54.862 - 00:03:57.406, Speaker A: The different kind of problems and different kind of solutions exist at different levels of scale. So we wanted to try to figure out how can we deal with scale? How can we make scaling really possible in decentralized systems and in general. So, yeah, let's take a look at the Apple two real quick, just to kind of show basically, if we look at how much computation has grown, like the appetite for computation and the appetite for data just keeps going up and we seem to find more and more uses of that computation. Right. And so initially we could say these old style machines weren't capable of doing very much. But as the hardware has grown, so has the demand and the capabilities of the software. And so, yeah, we can kind of see the Linux kernel is pretty big, the parameters going to gpT-3 or even bigger.
00:03:57.406 - 00:04:39.970, Speaker A: So scale seems to matter. Of course, trying to get zero knowledge to scale is a pretty hard problem. So let's kind of talk through that. A lot of the xero knowledge proof systems are based off of the current techniques. To try to make them run faster has to do with the number of constraints. It's like one of the main sort of parameters around how well your proof system is working. There's sort of this scaling ceiling in our case based off of the field that we chose.
00:04:39.970 - 00:05:24.290, Speaker A: So we're using this baby bear field, which basically is, it only allows up to two to the 28 values. And so we have to. This is where the cycle limit kind of comes from that you might be familiar with if you used the ZKBM. Currently we only allow sort of 16 million cycles before the cryptographic system can't even represent numbers past that point. People have tried to use recursion in order to kind of solve some of these problems, but it's a little tricky to use. Yeah, it's tricky in practice. So, yeah.
00:05:24.290 - 00:06:21.858, Speaker A: What if we could figure out a way to make zero knowledge proof systems or the programs that run within these systems kind of automatic? What if we could automatically make these things scale in such a way that developers don't even have to think about it? So that's kind of what we come up with. Know, there's like ICB, there's incremental computing, there's these ways of basically looking at it from the proof systems point of view. How do we make the proof system scale? I think what we did is took a more engineering approach. So we've got the CKBM. How can we get the ZKBM? If we look at the ZKBM, is there a way to sort of make the program execute in a way that we can make scale? And so our approach is basically let's split the execution up into separate parts. We call these segments. Once you split them up into separate parts, you can now prove each one of these parts independently.
00:06:21.858 - 00:07:06.994, Speaker A: So there's a first phase where we have to basically run the entire thing in serial. But once that's done, we can do the proving of each part in parallel. And then of course we can use recursion to do a roll up where we get a single succinct sort of proof at the very end. So yeah, what's nice about this is the programmer doesn't have to know anything about it. The execution engine can basically figure out where all the splits need to go. The user does can control basically how big each segment wants to be up to the cycle limit. But other than that, if they wanted to turn down the limit for each segment, they can do that.
00:07:06.994 - 00:07:56.066, Speaker A: And what's nice about that is that it uses a lot less memory. And of course we get parallel proving by doing this. So this is just kind of an example of the host side code, to be able to configure and execute the executor and then to request approve. So yeah, we can kind of see the execution environment. We're setting up a builder, we're basically giving it what the segment limit wants to be. And this is specified through powers of two. We add some inputs, we go ahead and run the execution, get provided a given elf file, and then we have this sort of session callback mechanism where we're going to be calling the executor.
00:07:56.066 - 00:08:41.810, Speaker A: And every segment that gets produced as we're running the execution gets collected here into this file segment. We can store these segments on disk. The reason for that is so that we can kind of keep our execution memory. The memory needed for execution can remain fixed so that we don't need to sort of, as you keep running segments, each segment is going to cost a certain amount of memory in order to keep all that resident. So if we kind of swap it out to disk, we can keep the fixed amount of memory cost for the execution. And then later I've got this session, I can now kind of prove the entire session. And what I get back is a receipt.
00:08:41.810 - 00:09:29.902, Speaker A: The receipt contains a bunch of segments, and I can also get the journal out of the receipt. So it's kind of similar to what we had previously. The API is a little bit different now, but the basic gist is that we have this execution phase, we have approving phase, and then you verify. Take the receipt, send it to the other party and do a verify. I should mention that there's two kinds of receipts that we currently support, and one is sort of this flat continuations, and the other one is like a rolled up continuation. So flat continuations means run the executor to split it up into multiple segments. And then when I call, prove what I get back is a list of a collection of receipts.
00:09:29.902 - 00:09:53.450, Speaker A: One receipt for each segment, and you can then verify each segment independently. The other thing you can do is ask for a rolled up proof, which basically returns a single receipt. And by verifying that single receipt, I can go ahead and ensure that the entire session was run properly.
00:09:55.070 - 00:09:56.634, Speaker B: Can I ask a clarifying question?
00:09:56.752 - 00:09:57.930, Speaker A: Yeah, go for it.
00:09:58.080 - 00:10:05.198, Speaker B: Yeah, I guess one thing we talked about, what actually is a segment in terms of memory, what's actually stored in.
00:10:05.204 - 00:10:06.814, Speaker A: A segment, and why is it that.
00:10:06.852 - 00:10:13.470, Speaker B: A chain of segments actually constitutes a full computation? Actually proves a computation.
00:10:14.770 - 00:10:40.214, Speaker A: So I have some material in a little bit that goes into the details of, I guess, what a segment is. Great. Yeah. Let me see if that answers your question in just a minute. And if not, we can dive into more details. Right. So we have this execution, which basically determines that it basically takes a snapshot of.
00:10:40.214 - 00:11:37.990, Speaker A: The idea is that we have memory state. So you can imagine that you've got the computation that you're doing. You've got some sort of program that says, go and run all these instructions. Those instructions are going to make side effects. They're going to cause different parts of memory to be read or written to. And the idea is that we want to take a snapshot of this memory state, and then when we want to continue, like if we pause one segment, we stop the execution of one segment, and then we want to continue that execution on a different segment. The trick is, well, how do we make sure that these two segments are related to each other? How do we make sure that, cryptographically speaking, everything is verified so that people can't sort of inject their own instructions in between, or they can't play around with the verification of the computation.
00:11:37.990 - 00:12:14.840, Speaker A: So the idea is, let's get a snapshot. Let's then prove through various means that the snapshot from one relates to the snapshot of the second. So, yeah, that's how we do the splits. We use a mercalized data structure. I'll get into that in a little bit on what that looks like. So, yeah, this is kind of a representation here of the execution. We do this kind of pre flight, we call it, which is basically running the execution in serial to figure out where all the splits need to go.
00:12:14.840 - 00:13:03.778, Speaker A: It also does things like record all of the host, guest I o the interaction with the host. All that stuff gets collected during the pre flight. Once that's done, we can then farm out. We can take each segment, segment one, segment two. All these segments can be farmed out to different machines, potentially, and each segment can be run in parallel, independently. The proof, I should say, once that's all done, you can imagine you've got all these leaf nodes of proofs for each segment. We can then do these join operations, which is kind of a recursive operation where we can run the verify of the a and b together into a single segment or session.
00:13:03.778 - 00:13:43.506, Speaker A: And that's what we call the join sort of predicate. If you do this enough times, you kind of do this in a binary tree fashion. You finally get a final succinct proof at the very end. Okay, so let's look at the memory snapshot. So what we did is we kind of looked at the idea of how do we verify that one segment is related to another? It kind of comes down to this whole notion of this image id. What we want to be able to do is say that segment two expects to start with the same image id. I'll go back to this one.
00:13:43.506 - 00:15:08.810, Speaker A: So if we say that a and b are both image ids, what we want to do is prove that when segment two starts, we expect this starting image id to be whatever the result of segment one was, and it's going to produce a new image id, in which case we want to make sure that we're always going from a to b to b to c to c to d, d to e. And so what is the image id? It's basically the root hash of a merkel tree. And that Merkel tree is made up of all of the hashes of pages in memory. So what we did is we kind of looked at, we kind of borrowed some operating system technology where they have this concept of a page, which is essentially, if you take all your memory and split it up into chunks, call it a page 1 size that we're currently using, then what we do is we can do a hash of that page that becomes an entry in the page table. The leaf nodes in the Merkel tree are all sort of the hash entries for all of the pages in memory. Now, there's this funny thing that happens eventually. There's kind of this overlap where we build this Merkel tree, but it's not a binary merkel tree.
00:15:08.810 - 00:16:13.620, Speaker A: The arity is pretty large. It's like however many entries, however many hashes that you can fit into a single page. That's the arity of this Merkel tree. So there's this kind of overlap that happens if you imagine the page table starts with all just the user memory, but eventually the beginning of the page table itself is actually inside the page table also. So eventually at the very end of this page table, you end up having sort of a root page, which is sort of the summary of all the other previous pages and the final hash. If we take that final root page and we hash it, we finally get this image id. And that basically tells us it's basically an efficient way to represent the verification of an entire memory image, which is currently it's about 192 megabytes, but yeah, that's basically what an image id is.
00:16:13.620 - 00:16:21.880, Speaker A: So I guess I'll stop there for a second and see if there's questions, or I can try to explain that a little better.
00:16:28.620 - 00:16:43.040, Speaker C: I'm curious if you can necessarily do this segmenting on all programs, or if this is just programs where you could parallelize it, and later instructions aren't dependent on earlier instructions.
00:16:44.180 - 00:17:27.080, Speaker A: So yeah, there's a concept of instruction level parallelism. There's different kinds of parallelism in our case, we only support like a single thread of execution. There's only like a single context, a single core you can kind of think of that the circuit implements. And so we don't yet have the ability to run multiple cores or multiple threads of execution at the same time. And so really what we're looking at is parallelism for the proving, not necessarily parallelism for the execution. So the execution is all serial. There's sort of this dependency.
00:17:27.080 - 00:18:13.304, Speaker A: Now your compiler may decide to reorder instructions or do whatever it thinks is best in order to achieve better serial performance. And currently we don't have any sort of vectorized instructions, so we don't even get instruction level parallelism either. That would require sort of vectorized instructions to be supported, which we don't yet do. Maybe that's something we'll do in the future. But it's pretty complex. It would cause the constraints to go up quite a bit for the proving system. We picked risk five because it's pretty challenging to kind of get all the constraints to run feasibly.
00:18:13.304 - 00:18:24.030, Speaker A: Right? So if we pick us, we pick a more complex instruction set, the constraints go up quite a bit. I don't know, does that answer your question?
00:18:24.740 - 00:18:42.150, Speaker C: Yeah, let me check. So it's not that you couldn't do this on Fibonacci, for example. It's that you do all of the segments, and then your verification of them is what you do in parallel, and you get some scale bonus from this.
00:18:42.520 - 00:19:15.500, Speaker A: Yeah. So the proving is the part that is the most expensive, and that's the part that we can parallelize. So the idea is that running, you can take any program you want as long as it's a risk five program. We can execute that entire program with however many cycles it takes. It may take millions and millions of cycles, billions of cycles. In fact. We can go through and run all of those cycles and record all of the memory transactions that are going to occur across that entire execution.
00:19:15.500 - 00:19:59.704, Speaker A: So that's kind of the serial part, but that part we can run pretty fast because we aren't doing all any proving. We're just purely emulating the actual RIsC five architecture. We could even run the execution on a real piece of hardware if we wanted to, provided that we had the sort of introspection, we could watch all of the memory transactions going by. But it is possible in theory, to basically run the execution phase in real hardware. So it goes really fast. But then doing the proving, we can basically split transparently. We can just basically decide where the splits need to go and inject some overhead.
00:19:59.704 - 00:20:49.880, Speaker A: There's like page ins and page outs. There's basically whenever a piece of memory is accessed, we need to do this page in operation, which is basically go and on demand. When a piece of memory is accessed, go and hash the page that that piece of memory comes from. And then we're going to do this sort of tree of that. It's kind of like an inclusion proof, where we hash up the tree through the merkel tree and verify that the image id is the one that the user has selected. So this is kind of on the read side, and then on the right side, when we go to write out the final page, there's also this page out kind of overhead. So for every segment there's like extra cycles that we inject in order to perform this memory verification.
00:20:49.880 - 00:21:11.700, Speaker A: But other than that, the user doesn't really need to see sort of where the splits are happening. It just happens automatically. And so, yeah, any program should be capable of doing that. And the parallelism you get is from the most expensive part, which is the proving, constructing a proof for a given segment. That's the part we can parallelize.
00:21:12.360 - 00:21:14.180, Speaker C: Thank you, that's very helpful.
00:21:23.800 - 00:21:58.176, Speaker A: All right, what can we do? What are the kind of applications we can do with continuations? Well, we have a working example of the EVM. It's like an EVM interpreter. I guess it's using REVM. It's like a rust interpreter for EVM. We have working examples of that. We have a wasm interpreter that you can run. In the past, it was kind of limited in what you could do with these things because we had an upper bound of 16 million cycles.
00:21:58.176 - 00:22:29.900, Speaker A: That means if you hit the 16 million cycle limit, the program would just stop and there's just no more, there's no more prove. We just couldn't prove anything beyond that. Plus, I would say that it took like 100gb of ram in order to run something with 16 million cycles. And it took a very long time. It took like 15 minutes, 30 minutes, something like that. So when we split with our default segment limit is now a million cycles. And so we try to fit that within about 8gb of ram.
00:22:29.900 - 00:23:11.244, Speaker A: We're trying to fit this on sort of consumer grade hardware like gpus. And so each one of these million sized cycle segments can run in parallel. Assuming that you have enough machines, you can go wide. We can make it scale in that way. So now developers can kind of write fairly long programs. One of the ideas is could we run GCC or like a compiler within the virtual machine? It'd be interesting because perhaps we could help solve some of the supply chain sort of issues that we've run across. If we could run a whole build inside of VM, that'd be interesting.
00:23:11.244 - 00:23:38.500, Speaker A: ZKML obviously requires lots of computation, so we shouldn't be limited anymore on sort of how many cycles it takes to run something. Yeah. So, yeah, that's the question. What can we do with this thing? Yeah, that's what's new. Awesome. Thank you, Frank.
00:23:38.840 - 00:23:45.030, Speaker D: Yeah, I think we can open things up for some Q A. We have some time.
00:23:45.640 - 00:23:46.390, Speaker A: Yeah.
00:24:01.380 - 00:24:03.010, Speaker D: That looks like we have a hand.
00:24:08.260 - 00:24:12.400, Speaker A: Hello. Go ahead, Vivek.
00:24:14.520 - 00:24:32.760, Speaker E: So, can this be compared with the performance of rewriting the proverb in terms of vectorized kind of functions like that takes supports from AVX two or any GPU vectorization, et cetera.
00:24:33.900 - 00:25:32.350, Speaker A: Yeah, so that's a different way to sort of make programs run more efficiently if you have more cores, if you have more. So generally, the way that people have tried to make computers run faster is we add more resources, we add more cores. That's one approach. Or we add more logic to a core so that we can do more pipelining, we can sort of super scalar execution, all these kinds of things. In our case, we don't really have that as something that's available to us because the number of constraints for the proving system is already really high in order to implement just the bare bones sort of risk five instruction set. And so vectorization would be something that would be interesting to try to make things run faster. But in our case, memory access doesn't really cost anything in the proving system.
00:25:32.350 - 00:26:29.224, Speaker A: Vectorized instructions wouldn't really give us much benefit, because really when we convert the software, we convert the set of instructions into the set of constraints that are run through the proving system. The performance characteristics change quite a bit compared to traditional computation. So for the execution phase, right, the place where I'm basically just running every instruction in sequence. Yes, there's a potential for making that phase go faster if I had vectorized instructions or if I had multiple cores or something like that. But the very large sort of bottleneck isn't really the execution, it's the proving. And the proving takes a lot of memory, it takes a lot of computation. And so really the trick is to make proving go faster.
00:26:29.224 - 00:26:57.656, Speaker A: We have two main things that we can do. One is we can try to reduce the number of cycles in our program. So like, just have less cycles. Essentially, vectorization doesn't necessarily give you less cycles, it just kind of combines, it combines multiple operations into a single instruction. It's kind of what vectorization is about, right? It's like single instruction, multiple data. That's kind of the idea there. And again, accessing data within the proving system doesn't really cost anything.
00:26:57.656 - 00:27:53.876, Speaker A: So we wouldn't expect to see a big benefit from single structure, multiple data in the proving system. And then the other approach would be like multiple cores, but again, that would just be lots more constraints. I suspect that multiple cores wouldn't really be a way to make the proving system perform any better. And so we don't really have those two traditional tools at our disposal. We really want to reduce the number of cycles or do this kind of splitting technique where we can now run each of the proofs, each segment, independently. When we go to prove each segment, we can have it run on a different machine or different hardware. And that's the main part that we're trying to parallelize.
00:27:53.876 - 00:28:08.110, Speaker A: So it's not really parallelization of the program or the instructions or anything that developers normally familiar with. It's more like we want to paralyze the proving part of it, the most expensive part of the process.
00:28:10.100 - 00:28:49.328, Speaker E: Yeah, sorry. So my question was different when you create this session, right? And then the prover will be like, you can invoke multiple prover at the same time on a single machine. And since prover will be on X 86, it will use all the codes and everything available to it to prove the things right. And you can do the same thing by porting prover to AVX, something like that. Right. Like your prover basically has lot of functions which can be done in parallel manner.
00:28:49.364 - 00:28:49.852, Speaker A: Right.
00:28:49.986 - 00:28:55.420, Speaker E: So how does those two speed up of the proving process itself are compared?
00:28:56.560 - 00:30:21.972, Speaker A: I see. So you're talking about on the proving side. On the proving side, is there ways that we can make use of AVX or parallelization techniques of the host machine, whether it's a GPU or a multicore machine or a machine with AVX instructions, all that kind of stuff? So we have this thing called the how. This is unrelated to continuations, by the way. So this is just purely, the proving system has the ability to basically run the most heavy parts of the computation on different kinds of accelerators. So for instance, AVX I would consider as a kind of accelerator, we don't really make use of AVX specifically because the proving system typically, normally, things like machine learning benefit pretty well from things like AVX, because you can pack the data in such a way that I can access, like I can do one computation across an entire sort of cell, nearby cells, very quickly, and there's a way to sort of pack the data to be able to do that. In the case of proving system, there's a whole lot of random access that's hard to predict, where it's hard to make sure that they all align.
00:30:21.972 - 00:30:52.580, Speaker A: Like we do a technique where we pivot. Normally you have rows and columns. We kind of flip everything around. So we have columns and rows. They kind of get pivoted so that we can get better cache coherency and get better sort of instruction parallelism. But the way that we handle all that is that we have this how, which is hardware extraction layer. And the idea is that we write a back end kernel either for gpus or even for the cpu.
00:30:52.580 - 00:31:42.044, Speaker A: On the cpu side, what we end up doing is using rayon, which allows us to run lots of threads in parallel. So we're using core parallelism. We're not really using instruction level parallelism, because again, we'd have to arrange for the polynomial constraints to be executed in a way that's amenable to AVX. I think that's probably something we could do. We haven't yet looked into that too deeply, but yeah, that's kind of where we're at with that. But again, this is kind of independent continuations. This is just kind of how do you make the proving system itself, once you have a single segment, how do you make that thing running really fast? Yeah, there's lots of ways that we can make use of gpus.
00:31:42.044 - 00:32:21.170, Speaker A: And things like that. We're more likely I o bound than we are computation bound. So this is why gpus end up giving us quite a bit of benefit versus something like fpgas or like AVX or things like that, because there's a lot of memory that we have to access in a random fashion, and it's hard to kind of coalesce these reads and writes into a cache hierarchy. We don't really benefit from cache hierarchies too much because there's too many accesses in order to fit within a single cache. So believe that makes sense.
00:32:28.120 - 00:32:29.750, Speaker E: Yeah. Thank you.
00:32:39.070 - 00:32:42.278, Speaker C: I have a question as well.
00:32:42.464 - 00:32:56.690, Speaker B: How does this compare to the other major way of getting unbounded length proofs at this point, unbounded computation, which is folding. So how does continuations and folding compare and contrast?
00:32:57.910 - 00:33:33.580, Speaker A: Yeah. So I'm not as familiar with the folding systems. So we might need to have somebody like Jeremy, who's our chief science officer, kind of give us an overview of that. That's more of sort of on the cryptographic proof system side. And I think that this continuations approach is more. Let's take an operating systems engineering approach to kind of making this thing work. So, yeah, it's a little hard for me to compare and contrast, I guess.
00:33:33.580 - 00:33:39.500, Speaker A: Good question, though.
00:33:49.880 - 00:34:10.368, Speaker F: If I could jump in real quick. I just want to introduce myself, everyone. I'm Bret Carter. Recently joined RiSC Zero, helping out the engineering team in a program capacity. But I have a background in product management, so I have a very product manager question for the audience. It's really cool to see people using our technology, and I'm curious if anybody who ended up on the call, if you were willing to share in the.
00:34:10.374 - 00:34:12.544, Speaker A: Chat, just sort of what's interested you.
00:34:12.582 - 00:34:19.460, Speaker F: In risk zero and why you're here. I'd love to just kind of hear what we're doing well and what's catching people's attention.
00:34:35.320 - 00:34:35.984, Speaker A: I'm Morgan.
00:34:36.032 - 00:34:44.276, Speaker G: This is my second day. Been awesome so far. But this presentation, everything that risk zero is doing is just, honestly, really cool.
00:34:44.298 - 00:34:46.728, Speaker A: It's like the Internet that building the.
00:34:46.734 - 00:35:00.030, Speaker G: Internet that we deserve with this kind of technology, we're going, I think, a lot of these problems that the engineers and the scientists are trying to solve. I just think it's really cool.
00:35:00.640 - 00:35:01.004, Speaker F: Yeah.
00:35:01.042 - 00:35:07.452, Speaker A: That's all I got to say on the matter. Thanks, Morgan.
00:35:07.516 - 00:35:07.840, Speaker F: Yeah.
00:35:07.910 - 00:35:11.590, Speaker A: Super to have you. Thanks, Ramka. Appreciate it.
00:35:13.080 - 00:35:41.310, Speaker H: Yeah, I can second that. I think the generalized nature of a ZKVM has huge implications. I'm not a product person, but just a very interested developer and not to mention the presentations and the publications that the risk Zero team has put out is incredibly appealing and it's just very interesting stuff. Very excited about it.
00:35:44.020 - 00:35:45.904, Speaker A: That's super helpful, Hans, thank you.
00:35:45.942 - 00:35:50.028, Speaker F: Yeah, we have a great devex team and Paul's doing a really good job with these ETA study clubs.
00:35:50.124 - 00:36:01.570, Speaker A: I would plus one that. Thanks Kai, I appreciate that.
00:36:01.640 - 00:36:07.462, Speaker F: I'm going to drop my discord id in the chat too. If anybody wants to just message and say hi, I'd be happy to connect.
00:36:07.516 - 00:36:23.340, Speaker A: There just to know who you all are there as well. Thanks everyone. Awesome.
00:36:23.490 - 00:36:34.850, Speaker D: Appreciate everyone coming. If we don't have any last minute questions, I guess we can sign off. Anyone have any more questions for Frank or for the team in general?
00:36:40.520 - 00:36:41.028, Speaker A: Cool.
00:36:41.114 - 00:36:48.100, Speaker D: Well, appreciate everybody coming. I think we're going to have another one of these in June. We'll have details coming out about that shortly.
00:36:52.540 - 00:37:01.250, Speaker A: Great. Well, thanks Paul for putting this together. Really appreciate it and thanks everybody for for attending. Hope you got something out of it. And yeah, have a great day.
00:37:01.620 - 00:37:02.610, Speaker C: Thank you.
00:37:03.380 - 00:37:06.220, Speaker A: Thanks Paul. Thanks Rand. Great stuff. Cheers.
