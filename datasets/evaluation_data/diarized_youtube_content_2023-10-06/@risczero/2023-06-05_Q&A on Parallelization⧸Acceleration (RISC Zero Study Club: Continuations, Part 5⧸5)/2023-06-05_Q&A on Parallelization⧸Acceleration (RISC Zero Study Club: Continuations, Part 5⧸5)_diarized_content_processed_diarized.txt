00:00:00.410 - 00:00:01.440, Speaker A: Yeah, go ahead.
00:00:01.810 - 00:00:20.110, Speaker B: So can this be compared with the performance of rewriting the prover in terms of vectorized kind of functions like that takes supports from AVX two or any gpu vectorization, et cetera?
00:00:21.250 - 00:01:07.602, Speaker A: Yeah. So that's a different way to sort of make programs run more efficiently if you have more cores, if you have more. So generally, the way that people have tried to make computers run faster is we add more resources, we add more cores. That's one approach. Or we add more logic to a core so that we can do more pipelining, we can sort of super scalar execution, all these kinds of things. In our case, we don't really have that as something that's available to us because the number of constraints for the proving system is already really high in order to implement just the bare bones sort of risk. Five instruction set.
00:01:07.602 - 00:02:07.922, Speaker A: And so vectorization would be something that would be interesting to try to make things run faster. But in our case, memory access doesn't really cost anything in the proving system. Vectorized instructions wouldn't really give us much benefit because really when we convert the software, we convert the set of instructions into the set of constraints that are run through the proving system. The performance characteristics change quite a bit compared to traditional computation. So for the execution phase, right, the place where I'm basically just running every instruction in sequence. Yes, there's a potential for making that phase go faster if I had vectorized instructions or if I had multiple cores or something like that. But the very large sort of bottleneck isn't really the execution, it's the proving.
00:02:07.922 - 00:02:37.886, Speaker A: And the proving takes a lot of memory, it takes a lot of computation. And so really the trick is to make proving go faster. We have two main things that we can do. One is we can try to reduce the number of cycles in our program. So like, just have less cycles. Essentially, vectorization doesn't necessarily give you less cycles, it just kind of combines, it combines multiple operations into a single instruction. It's kind of what vectorization is about, right? It's like single instruction, multiple data.
00:02:37.886 - 00:03:27.790, Speaker A: That's kind of the idea there. And again, accessing data within the proving system doesn't really cost anything. So we wouldn't expect to see a big benefit from single structure, multiple data in the proving system. And then the other approach would be like multiple cores, but again, that would just be lots more constraints. I suspect that multiple cores wouldn't really be a way to make the proving system perform any better. And so we don't really have those two traditional tools at our disposal. We really want to reduce the number of cycles or do this kind of splitting technique where we can now run each of the proofs, each segment independently.
00:03:27.790 - 00:03:55.400, Speaker A: When we go to prove each segment, we can have it run on a different machine or different hardware. And that's the main part that we're trying to parallelize. So it's not really parallelization of the program or the instructions or anything that developers normally familiar with. It's more like we want to paralyze the proving part of it that the most expensive part of the process.
00:03:57.450 - 00:04:28.050, Speaker B: Yeah, sorry. So my question was different. When you create this session, right. And then the prover will be like, you can invoke multiple prover at the same time on a single machine. And since prover will be on X 86, it will use all the codes and everything available to it to prove the things. Right. And you can do the same thing by porting prover to AVX.
00:04:28.050 - 00:04:42.710, Speaker B: Something like that. Right. Like your proverb basically has lot of functions which can be done in parallel manner. Right. So how does those two speed up of the proving process itself are compared?
00:04:43.930 - 00:05:59.790, Speaker A: I see, so you're talking about on the proving side. On the proving side, is there ways that we can make use of AVX or parallelization techniques of the host machine, whether it's a GPU or a multicore machine or a machine with AVX instructions, all that kind of stuff. So we have this thing called the HaL. This is unrelated to continuations, by the way. So this is just purely, the proving system has the ability to basically run the most heavy parts of the computation on different kinds of accelerators. So for instance, AVX I would consider as a kind of accelerator, we don't really make use of AVX specifically because the proving system typically, normally things like machine learning benefit pretty well from things like AVX because you can pack the data in such a way that I can access, like I can do one computation across an entire sort of cell, nearby cells, very quickly. And there's a way to sort of pack the data to be able to do that.
00:05:59.790 - 00:06:50.874, Speaker A: In the case of proving system, there's a whole lot of random access that's hard to predict where it's hard to make sure that they all align. Like we do a technique where we pivot normally you have rows and columns, we kind of flip everything around. So we have columns and rows, they kind of get pivoted so that we can get better cache coherency and get better sort of instruction parallelism. But the way that we handle all that is that we have this how, which is hardware extraction layer. And the idea is that we write a back end kernel either for gpus or even for the cpu. On the CPU side, what we end up doing is using rayon, which allows us to run lots of threads in parallel. So we're using core parallelism.
00:06:50.874 - 00:07:55.506, Speaker A: We're not really using instruction level parallelism, because again, we'd have to arrange for the polynomial constraints to be executed in a way that's amenable to AVX. I think that's probably something we could do. We haven't yet looked into that too deeply, but, yeah, that's kind of where we're at with that. But again, this is kind of independent of continuations. This is just kind of how do you make the proving system itself, once you have a single segment, how do you make that thing running really fast? Yeah, there's lots of ways that we can make use of gpus and things like that. We're more likely I o bound than we are computation bound. So this is why gpus end up giving us quite a bit of benefit versus something like fpgas or like AVX or things like that, because there's a lot of memory that we have to access in a random fashion, and it's hard to kind of coalesce these reads and writes into a cache hierarchy.
00:07:55.506 - 00:08:08.440, Speaker A: We don't really benefit from cache hierarchies too much because there's too many accesses in order to fit within a single cache. So believe that makes sense.
00:08:15.390 - 00:08:16.042, Speaker B: Yeah.
00:08:16.176 - 00:08:16.600, Speaker A: Thank you.
