00:00:00.410 - 00:00:45.574, Speaker A: We're going to look at the paper more or less like, sentence by sentence, starting from the beginning. And I'm going to try to unpack the context and references as much as I can as we go. So, the first sentence says, this document is an informal summary on the fry low degree test and deep algebraic linking. And it lists some references. And I just want to talk a little bit at the beginning about what are these references? So, the three references that it mentions here are the fry paper, the deep fry paper, and the proximity gaps paper. And I'll also throw into the list for discussion. The stark paper, and I think the academic literature of our protocol is pretty completely described.
00:00:45.574 - 00:01:20.710, Speaker A: Well, that's not true. There's some plonk and pluckup stuff that's separate, I guess. But within the stark and fry part, like, these four papers are the bulk of it. So, let me unpack a little bit of how these four papers are connected and then continue. So, the stark paper is really, like, that's what we're doing. We have a stark. It's a scalable, transparent argument of knowledge, is what the seal on our receipt is very literally a stark.
00:01:20.710 - 00:01:51.614, Speaker A: So, a stark is a way of proving computational integrity. Meaning a stark is a way of convincing a third party that the computation you ran is actually the computation that you said you ran. The first part is an algebraic linking protocol. And the second part is a. Read Solomon proximity testing protocol. And I'm just going to leave those as black boxes. I'm not going to talk about what an algebraic linking protocol is at all.
00:01:51.614 - 00:02:42.720, Speaker A: Today, we'll talk a little bit about the idea of a Reed Solomon proximity testing protocol. Because that's what fry really is. But for the purposes of this slide, I just want to contextualize that a stark has two parts, algebraic linking. And then Reed Solomon proximity testing. And specifically the algebraic linking part turns our assertion of computational integrity into a Reed Solomon assertion. And then the second part proves that Reed Solomon assertion. So, a lot of black boxes here, but hopefully, the sort of breakdown of a stark consists of two parts, algebraic linking and resolving approximity testing feels relatively okay.
00:02:42.720 - 00:03:00.130, Speaker A: Each of those has two versions. There's a regular version and a deep version. We have our algebraic linking without deep. And we have our algebraic linking with deep. We could do either of those. And we have a. Read solomon without deep.
00:03:00.130 - 00:03:28.094, Speaker A: That's fry. Or read Solomon proximity testing protocol with deep. That's deep fry. So, this is our first couple minutes of the presentation. A lot of technical words, questions at this point. Of course, there's the question of what is algebraic linking and what is deep. But I'm going to assume, because I haven't heard any voices or hands, that I can continue.
00:03:28.094 - 00:03:52.390, Speaker A: I had a little note here about, like, I could be a little bit more technical. I'm not going to talk about algebraic intermediate representation too much, but there's a little note. So I mentioned that there's these two different versions. There's the without deep version. And the with deep version. Deep stands for degree extension. For the elimination of pretenders.
00:03:52.390 - 00:04:38.230, Speaker A: When starks were originally created, there was no deep. Yet it was Ali and Fry. And then a year or two later, this deep technique was invented, created. And at that point, people were running starks using deep, Ali, and deep fry. And then a little bit later, the proximity gaps paper showed that, in fact, deep is not actually an improvement over the original fry. So where we are today is that when we run starks, we use the deep version of the algebraic linking protocol. But we don't bother with deep on the fry protocol.
00:04:38.230 - 00:05:15.554, Speaker A: So that's the kind of context of the timeline of how starks have evolved over time. There was the original, then the deep improvement. And then the realization that the deep improvement only actually applies to the first part of the protocol. Quick question. Yeah. When you say fry is actually better than deep fry. Does that relate to the execution time or execution time relative to security? Great question.
00:05:15.554 - 00:05:53.262, Speaker A: So deep is computationally more intense than not deep. So there's a little bit of extra computational burden in order to do deep. And the thought was that by doing that extra computation, you get better security. And the realization in the proximity gaps paper was that the original Fry protocol actually had better security than originally proven. And the improved fry security achieves the deep fry security. Without the need for the extra computation of the deep part. Does that answer your question? Yeah.
00:05:53.262 - 00:06:19.110, Speaker A: Great. Thanks. So let's just continue the second sentence. I'll read it out loud. Based on its most recent soundness analysis, we discuss parameter settings for practical security levels. How fry is turned into a polynomial commitment scheme. And the soundness of deep sampling in the list decoding regime.
00:06:19.110 - 00:06:55.490, Speaker A: That's a lot of technical jargon. Soundness analysis is a term that you may or may not be familiar with. Parameter settings for practical security levels is sort of ambiguous. Polynomial commitment scheme is new jargon to most people, probably. Soundness of deep sampling in the list decoding regime seems like a bunch of greek. Let's see if we can make sense of what some of these terms are. So soundness is something that I've been thinking about quite a lot recently.
00:06:55.490 - 00:07:50.130, Speaker A: Suppose you get a receipt from some third party that ran our ZKVM, and you run the verifier. And the verifier says, yes, the receipt checks out, everything's cool. We don't get 100% guarantee that the person who ran the ZKVM and generated this receipt was actually honest. We don't, in zero knowledge proofs in these sorts of cryptographic arguments, we don't get a full 100% guarantee. Rather, we get up to some probability bound up to some epsilon, which we hope is small. We can trust that the prover knows an execution trace that actually satisfies the constraints. So, soundness is the analysis associated with this epsilon.
00:07:50.130 - 00:09:29.688, Speaker A: Epsilon is referred to as the soundness error. And what we mean when we say we have 100 bits of Security, or 128 bits of security, is that this epsilon is really small. The epsilon 100 bits of security means that the soundness error is less than one over two to the 100th and 128 bits of security means the soundness error is less than one over two to the 120 eigth. So, in this bunch of jargon, the soundness analysis is the question of how small is epsilon? And this discussion of setting parameters is, what do we need to do in order to make that epsilon small? Questions about soundness. Okay, so then there's this term of a polynomial commitment scheme is the next one that comes up, and I want to just discuss what is a commitment scheme, and then add in the polynomial part. So, commitment schemes, like, if I'm playing, say, a guessing game with a bunch of children, and I say I'm thinking of a number between one and 100, and the kids start guessing, a kid might ask the question, like, wait, did you actually even pick a number? Maybe you're just cheating. And so the simplest possible commitment scheme that I can think of would be, before we start this game, I could just write down my number on a piece of paper, and then at the end, in order to convince you that I wasn't cheating, I could show you the piece of paper.
00:09:29.688 - 00:09:59.008, Speaker A: So by writing it down on a piece of paper, I've made a binding commitment to my answer, and then I can't change it after the fact. So the two kind of keywords that come up around commitment schemes are binding and hiding. There's probably more. I didn't actually look at any references. I was putting together these slides. I just sort of, like. I mean, I looked at some references, but this is based on my current understanding of these terms and not based on a specific reference from a specific text.
00:09:59.008 - 00:11:11.770, Speaker A: So, loosely speaking, a commitment scheme consists of a way to commit, a way to reveal, and a way to check that that reveal actually aligned with the commitment. So we use merkel trees. We commit by constructing a Merkel tree. The root is the commitment, the reveal is we show the contents of a leaf, and the verifier is able to check that the leaf contents match up with the root by checking the validity of the Merkel branch. So our commitment scheme is based on Merkel trees, and a polynomial commitment scheme is just a commitment scheme where the data that you're committing is evaluations of a polynomial. So in the example I gave in the class, like the kids guessing a number thing, I was just writing down a single number. But if I was actually thinking of a polynomial, and the thing that I wrote down was the polynomial evaluated at each point of my domain that I was interested in then, I'm now looking at a polynomial commitment scheme instead.
00:11:11.770 - 00:12:23.036, Speaker A: So the way that our commitment schemes work with Merkel trees is each leaf corresponds to some evaluation point, and the contents of that leaf are the evaluation of the polynomial at that point. Questions? This is maybe up there with the most technical few sentences in the presentation. Okay, I'll continue. So we discussed what a polynomial commitment scheme is. It's just how do we commit and reveal data within our proof system? And then there's this part of this last phrase has a lot of terms in it, soundness of deep sampling in the list decoding regime. Let's unpack this term of list decoding regime a little bit, and we'll look at this in a more concrete example in a few minutes. But just for the sake of getting through the abstract, read solomon.
00:12:23.036 - 00:13:13.330, Speaker A: Proximity testing is about how close there's these code words. And we're looking at something that may or may not be a code word, and we ask the question, is this a code word or is it close to a code word? And sometimes when we ask the question, what's the closest code word? There's a unique answer. There's like, oh yeah, that one's the closest. But other times I might be looking at some potential code word and ask what's the closest? And I might find, oh, there's like four or five that are actually equidistant, in which case I can't decode to a specific one. I can only decode to like, I don't know, the closest one is one of these five. And it turns out that the analysis of. Sorry, go ahead.
00:13:14.500 - 00:13:20.130, Speaker B: Can you just go I'm not sure I understand the meaning of code word.
00:13:23.960 - 00:14:00.716, Speaker A: I have a more explicit example here in a few. Let me put a pin in that question. And if it's still confusing, in a few slides. Now, let me just sort of invite the idea that unique decoding is where things go, as you would hope. And list decoding is. Oh, things are a little bit more complicated here. So, going back to the phrasing, will it click? Will I go to the next slide? So it says soundness of deep sampling in the list decoding regime.
00:14:00.716 - 00:15:07.156, Speaker A: For the purposes of this moment, let me just say that last part is soundness of deep Ali in the hairy part of the analysis. So, here's the first two sentences of the abstract, which we've discussed a little bit of a pin in Ash's great question about, what do you mean by code and code word, the last sentence of the abstract. And then we'll dive into a more example of a code. We illustrate the deep method applied to proving satisfiability of algebraic intermediate representations and prove a soundness error bound, which slightly improves on the one in the Eth stark documentation. So, in more intuitive words, we walk through a stark using deep Ali and fry to prove an assertion of computational integrity. We show that our security, Orbis's security, is slightly better than eth's. Stark security, at least needs to do less queries, I guess.
00:15:07.156 - 00:15:38.460, Speaker A: Is the result there? Speed through this a little bit, because I do want to get to the read Solomon code part of this is really the meat here. Okay, we still have. We're in good shape on time. Sorry. I popped out to check time. So we got through the abstract, more or less. Here's the first sentence of the paper itself, which is scary.
00:15:38.460 - 00:16:15.832, Speaker A: It's less scary if you take away most of the scary words and symbols, but it's still kind of scary. Fast read Solomon code interactive. Oracle proof of proximity is the acronym. So, let's unpack this acronym a little bit, and we'll get to what we mean by Reed Solomon codes. So, the term fast here is. There's this algorithm called the fast Fourier transform that, if you're aware of great. And if you're not great.
00:16:15.832 - 00:17:15.544, Speaker A: The term fast here is just a reference to the similarity between the fry protocol and the FFT algorithm. Read Solomon codes, which we'll look at. Well, we'll look at codes a little bit more explicitly in a moment, are a way of adding redundancy for the purposes of error detection and error correction. The word interactive here refers to sending messages back and forth between a prover and a verifier, although in reality these interactions are simulated. So we run a non interactive oracle proof or a non interactive argument. The term oracle refers to a mathematical idealized version of a hash function, more or less. So when you're reading the math papers, it will refer to oracle proofs.
00:17:15.544 - 00:17:55.400, Speaker A: When you're reading the security analysis of a real life protocol, the word oracle proof will get replaced with an argument. So if it is happening in terms of abstract ideal hash functions, it's an oracle proof. If it's happening in real life, it's an argument. And then finally, this word proximity is about closeness. There is this notion of closeness within this world of codes that I have yet to really explain. And within this world of codes, we get to ask about closeness. So it's a proof of closeness.
00:17:55.400 - 00:18:48.376, Speaker A: So let's get to Ash's question of what do we mean by a code and a code word. Mary Wooders, who's a professor at Stanford, has a fantastic course on algebraic coding theory. And if you want to dive deeper into learning about read Solomon codes and the associated bounds and those topics, I highly recommend this course. There's a YouTube channel, there's problem sets, there's lecture notes. It's great. I chose a few choice slides from that in order to give a brief introduction to what we mean by a code. So, stepping away from the specifics of starks and proofs of computational integrity and all that, the basic problem encoding theory.
00:18:48.376 - 00:19:54.530, Speaker A: Alice has some message that Alice wants to send to Bob, and Alice encodes this message. So the message here is short and it's encoded into some longer code word. But then before Bob actually receives the message, maybe there's some sort of signal noise that distorts some of the data. Maybe some entries get deleted. But so Bob's looking at this code word, Bob's looking at this thing that was received from Alice and is wondering what was Alice trying to say is the basic problem of coding theory. So here's that in a diagram, Alice is thinking of some string of zeros and ones. Alice encodes the message, Alice sends it with some potential noise, and Bob receives it and wonders about what was that original message.
00:19:54.530 - 00:20:46.930, Speaker A: So here's a formal definition. A code is basically, it's just a collection of code words of a specific length using a specific Alphabet. So there's an example at the bottom. The code consists of these three words that are each ten letters using the standard roman Alphabet, all capital letters. Let's look at another example here, and we can get a little bit more concrete about what we mean by distance, what we mean by decoding here. So here's an example of a code. It consists of strings of length four where every entry in the string is either a zero or a one.
00:20:46.930 - 00:21:29.020, Speaker A: And let's imagine that Bob, oh, sorry, before we. I'm going to skip that slide, actually. Let's imagine that Bob received this message from Alice. Bob received this string and looks at it and says, oh, there was an error here or there was an erasure here. I can't see the second bit. Bob is able to look at this, and because Bob knows the code or Bob knows the contents, Bob knows these eight valid code words. Bob can look at this and say, oh yeah, I can figure out what that missing bit was.
00:21:29.020 - 00:22:18.548, Speaker A: That missing bit. Well, if it was a zero, then it wouldn't have been in the list. So the missing bit must have been a one. Does that track? So in this case, if I have a missing bit, I'm able to fix it. But in this case, Bob receives one and looks at the code and says, I have no idea what Alice was trying to say. This was not one of the messages that is sort of allowed. This isn't something that if Alice ran the encoding procedure that she said she was going to run, she would have gotten one of these things on this list.
00:22:18.548 - 00:23:16.624, Speaker A: And what I received wasn't on this list. So you can look and say maybe Alice was trying to send, and in fact there's four possible candidates here, maybe the four likely candidates here. And what I mean by that is there's four entries on this list that differ in exactly one position. The first entry differs, differs in only the last bit. I won't go through finding all four of those, but we can detect that there's an error here. But if we try to actually decode this, we end up with a list of four closest candidates. So what I mean by closest, I'll go back to that slide that I fixed.
00:23:16.624 - 00:23:50.720, Speaker A: The distance in this context is just the number of entries that differ. And if we're looking at something that's not valid, we can try to decode it by asking what's the closest valid one? So sometimes when we ask what's the closest valid one, we get a unique answer, but oftentimes instead we get a list. So maybe I should invite ash. Does this sort of give a little bit of clarity to what we mean by code now, are you still feeling.
00:23:52.520 - 00:24:01.270, Speaker B: Yeah, I guess I'm still not clear on why we're measuring closeness. Like if it's wrong it's wrong. No.
00:24:02.280 - 00:24:43.990, Speaker A: Yeah. So the verifier receives this seal and has to sort of check what's going on here. And the verifiers, the amount of time that the verifier. We need the verifier to run incredibly quickly. In fact, we need the verifier to be able to run in less time than it takes to read the contents of the original computation. So that statement of, like, if it's wrong, it's wrong, is a really. I love that question.
00:24:43.990 - 00:26:00.460, Speaker A: The verifier doesn't have enough time to actually test whether it's 100% right. The verifier only has enough time to test proximity, and so we need to sort of understand what's the best we can do with a really limited amount of time. If we're only allowed to read, say, like, 10% of the original computation, is that enough time for us to determine its validity? And so that's a bit of a hand wavy, loose answer. But the rough answer to your question is the verifier doesn't have enough time to tell whether something is 100% valid. And so we need to use a test of proximity rather than a perfect test. That's the best I can answer for now. I don't know if anybody else wants to add to that, so here's that first sentence, fast read Solomon code interactive oracle proof of proximity.
00:26:00.460 - 00:27:09.724, Speaker A: I'm not going to talk about this definition, but I figured I would include it. For those who are interested, this is a formal definition of what a read Solomon code is. It's again drawn from that same course on algebraic coding theory, which I highly recommend for the sake of making this reasonably accessible to everyone. Discussion I'm going to avoid discussing. Solomon codes involves finite fields and polynomials in a way that I think we need to do a sort of primer on polynomials and finite fields before I want to have a study club that goes into the details of Reed Solomon codes in a very loose way. Reed Solomon codes are codes that are based on low degree polynomials. So what do we mean by this low degree test? It elaborates.
00:27:09.724 - 00:28:25.930, Speaker A: It says, fry is a low degree test for functions on an FFT domain boy I-E-A smooth multiplicative subgroup d, a finite field f. This is sentence one again, if you don't have a degree in math reading, this is probably pretty tricky, but maybe it's easier if I take away some of the harder words. What Fry does, given a function called f. Fry proves that f corresponds to a polynomial of low degree, and this word corresponds means is close to rather than is exactly the same as bringing in a little bit more language. It's not just low degree, it's low degree with respect to the size of d. And there's this function notation that you may or may not be familiar with. So what's the function notation? What's d and what's f? This notation says that we're associating each element of d with an element of f.
00:28:25.930 - 00:29:01.460, Speaker A: So d is a set, f is a set. Given any point in d, we can evaluate the function f with that point as an input, and the output, when we evaluate it, will be an element of f. D is our initial commitment domain. This is the size of our Merkel tree. We construct one Merkel leaf for each element of d, and f is where we do our arithmetic. So it was baby bear. It's going to be Goldilocks, I guess it's going to be field agnostic.
00:29:01.460 - 00:29:44.310, Speaker A: Adding in a little bit more of that language. Now, this is actually the beginning of the paper, an FFT domain. Again, not going to get into this too much, but it's a domain where you can do finite Fourier transforms more or less. It means you can split it into smaller pieces over and over and over again, and not going to get into the definitions of those terms. Questions. At this point, I want to sort of step back and try to invite a little bit of discussion, if it's possible.
00:29:51.560 - 00:30:17.896, Speaker C: Can I add a little that you correspond to? A low degree polynomial is related to what a Reed Solomon code is. Paul didn't go into, for good reason, what one is, but that's a relevant measure of closeness. So just sort of wanted to mention that to help people connect the dots.
00:30:18.088 - 00:30:19.870, Speaker A: Yeah, I appreciate that, Tim.
00:30:23.380 - 00:30:31.010, Speaker B: I feel like a dumb, dumb still. But why is close good enough? Don't we want everything to be correct?
00:30:32.580 - 00:31:52.700, Speaker A: Great question. There's this idea of redundancy in error correction contexts, and what read Solomon codes are doing for us is making it so that if there's any error at all before we do the read Solomon part, then the read Solomon part is going to drastically amplify that error in such a way that it's very easy to find it. So if there was one discrepancy originally, after we do read Solomon encoding, 75% of the thing is fucked up. And then we test it a little bit. And we test it enough that we are confident that we would find it, because most of it would be an error. There's a three blue one brown video about hamming codes that I think might be the best source of getting an intuition here for why closeness is enough for us. But this term proximity gaps is actually related here.
00:31:52.700 - 00:33:02.874, Speaker A: A proximity gap refers to the idea that if it's not, how do I explain what a proximity gap is? There's nothing. That's only kind of close is the nature of a proximity gap. So essentially, once you do the Reed Solomon error amplification process, either things are going to be pretty far or correct. And so by running the test, the test says that it's within the pretty close area, and because of the proximity gap, we get to sort of conclude that that's enough. So you're not a dumb dumb for having this question of why close is good enough. It's actually like a very good question. I also have a question related to this closeness that you were mentioning.
00:33:02.874 - 00:34:00.900, Speaker A: Is it somewhat related or similar to the Schwartz zipalemma? So the closeness, right. It doesn't super closely linked to me there. The short zip elemma, for those who aren't aware, essentially says polynomials don't have that many roots. And so if you're looking at polynomial stuff, you can bound things based on the number of roots. And the distance here is really just looking at how many entries disagree. So I don't know. Bolton, do you want to comment at all about that? Do those feel linked to you?
00:34:01.830 - 00:35:23.610, Speaker D: Yeah, I sort of agree. I think know the short simplemma is maybe a generalization to sort of multivariable polynomials of the sort of, as you were saying, paul, the sort of very simple fact that if I have a polynomial that is low degree, it only has a certain number of roots, which is basically the same thing as saying that the polynomial is only going to be zero, equal to zero in a very sort of small number of places. And what that tells us is that if you take a polynomial and you inspect it at a bunch of random places, and you keep seeing that the polynomial is equal to zero in those places, you can be pretty sure that the polynomial itself is actually equal to zero. The full sort of general Schwartzpool lemma does come up in some of this more complicated analysis. But I think for the purposes of this talk, it's probably better to just think about this simpler fact that polynomials tend to be only equal to zero in a lot of places if the polynomials themselves are equal to zero everywhere.
00:35:25.230 - 00:36:18.478, Speaker C: In some ways. The short symbol that lemma gives an intuition for why you can be close to just one code, where if these read Solomon codes are polynomial, low degree polynomial in some sense, that the fact that there are only a few roots is some intuition for why. If you have some random thing that seems kind of like a polynomial, it probably is that polynomial ish? Kind of. Probably only one. I don't know how, but can I go ahead. One comment on Ash's thing about. Good question on why is close good enough, which is, in some ways, how we do the code to run the verification and how the proof is framed are in some ways backwards of each other.
00:36:18.478 - 00:37:00.620, Speaker C: If you're running the verification and you see a single thing wrong, it's tails. It's bad. You know there's a problem, you're certain there's a problem, it's bad news. The proof is saying, hey, look, how much can we try doing stuff and still getting okay results until we've gotten close enough that we can be extremely confident that there isn't some problem hiding away from us? So close is sort of us getting close to the actual answer without looking at it, rather than being like, oh, well, if it mostly passes, it's good enough. No, it's not that if it mostly passes, it's good enough. It's that you don't have to check the entire world.
00:37:01.870 - 00:37:03.660, Speaker A: Yeah, that's a good point, too.
00:37:04.930 - 00:37:35.640, Speaker E: I'm wondering, is this related to the zero knowledge portion? Like, we add noise to kind of mix things up so that it's hard for people to recover the execution trace to recover the actual computation. So I guess my question is, if there was no zero knowledge, if it was just computational integrity, would there be a closeness? Is closeness related to the noise? We add that, mixing it up and everything?
00:37:37.930 - 00:37:50.970, Speaker A: Not that I don't believe so. I think that regardless of what, even if we wanted to do a non zero knowledge implementation of this, I think it would still have the exact same closeness and proximity discussions.
00:37:52.190 - 00:38:23.080, Speaker C: So it enables it to be zero knowledge, but for efficiency reasons. So if you actually check the entire polynomial, you can't really do zero knowledge. That breaks zero knowledge. So it is related to zero knowledge in that way. But you would still, for efficiency reasons, to make the verifier run fast, even if you aren't trying to do zero knowledge, you would prefer not to check everything. So in that way, you would still not check everything, even in a non zero knowledge case.
00:38:24.490 - 00:39:07.010, Speaker A: Yeah, Frank, I can maybe be a little bit more explicit on the do I want to actually? Essentially, the verify gets some information in the context of this test, and we add the random padding in order to make it so that they can't do anything with the information that they receive. That's not as explicit as I was thinking I was going to make it, but it's as explicit as I'm going to make it for now. We can talk more about that after if you want to, Frank.
00:39:08.490 - 00:39:09.094, Speaker E: Sounds good.
00:39:09.132 - 00:40:24.134, Speaker A: Thank you. I do have a few more slides that I could run through, but I think it might be more valuable to invite people to vocalize what they're either confused about, curious about, interested in. I'm thinking that it will. I'm thinking that I will facilitate some sort of introduction to finite fields over the next few weeks, and then maybe another one of these that goes a little bit more into reed solomon codes, assuming some finite field background. But for today, I'm curious what's on people's minds about either things that are feeling like a barrier for you in your current understanding. Things that felt confusing to you about what we've talked about so far. Today's just have a short observation, I think at know my opinion so far.
00:40:24.134 - 00:40:32.986, Speaker A: This has been one of the most exciting work related calls I ever had. Thank you.
00:40:33.008 - 00:40:38.960, Speaker B: That's really good, Paul. I would love to learn more math from you. You're really good at this.
00:40:39.490 - 00:41:34.876, Speaker A: I appreciate that. Thanks. It's something I'm pretty passionate about, so I'm happy to run through the last, like, ten slides that I have prepared if people don't have things that they want to vocalize. Okay, I will just go ahead and go back to my thing and wrap things up a little bit. So the next paragraph is just as scary as the previous one. I'll read it out loud. The oracles, that's idealized hash function sort of thing, kind of provided by the fry prover are, again.
00:41:34.876 - 00:42:07.240, Speaker A: Oh, actually, the way I'm using oracles is slightly wrong. Well, it's good enough. The oracles provided by the fry prover are again, functions on d, or a subdomain of it. And the verifier queries the values at points from their domain only due to the small size of d. The key tool for distinguishing one polynomial for another is statistical sampling. However, a statistical test can only assure proximity, which we measure by the fractional hamming distance. So again, don't panic.
00:42:07.240 - 00:43:04.680, Speaker A: I'll take away some of the complicated stuff. And this gets back to Ash's question of, like, why is. Well, it gets kind of back to the question. Ash's question of why is closeness good enough? What we're doing here is we're doing a statistical test, meaning we're choosing some random points and we're saying, did it pass there? We're doing some random tests, and we're saying, did it pass the test there? And by the nature of this sort of test, unless we test everywhere, all we can really get out of this is proximity. If we want the verifier to be able to run in any sort of efficient amount of time, we have to find a way to make a proximity test be good enough. And it turns out that Eli Benson et al. Have done that for us.
00:43:04.680 - 00:43:48.820, Speaker A: So to sort of boil it down, fry gives us the tools to test. Is the code word I received close enough for me to decode? And with a few dozen queries, I think in our protocol, we're doing 50 something right now, we can determine if the code word is in the list decoding radius, meaning there's a few viable closest code words. This was our example that we saw earlier of a list decoding sort of situation. And with more queries, we can determine if it's in the unique decoding radius. We actually find one code word that it corresponds to.
