00:00:00.330 - 00:00:44.890, Speaker A: So basically, finite field implementations. This is something that I've gotten interested in over the past year, basically answering the question of, like, okay, when you've done a brass tax, how things actually implemented. And there's some really interesting stuff going on. So, one thing I'll kind of preface, as with all these study clubs, if something isn't clear, definitely ask. When I first looked at these algorithms, I was very much so lost for a while. Why does that compute what it says it does? This is a common theme, especially when you get to optimization things, is that often things are done in very roundabout ways. So definitely ask about things.
00:00:44.890 - 00:01:59.540, Speaker A: My goal is that people can walk away with an intuition for it and not just have copy down the formulas, because that's because the formulas themselves are easy to Google and mostly useless to you unless you understand what's going on. So definitely ask questions. So, some context to definitely kind of remind ourselves of is what is a finite field? And we previously covered this in a study club talk. The finite fields, kind of the knowledge about clock arithmetic, the fact that you can do arbitrary algebra, and other kind of forms of mathematics on top of finite fields, fields, basically all that kind of was in the intro, and I won't really cover that today, except for just to mention, just to mention a quick reminder about what finite fields are at the basic level, and the fact that they are very important. And so kind of our motivation, which some of this is recalling what we did before, but also kind of driving, kind of further driving this point home. Finite fields are the basis for basically all the math we do in cryptography. Kind of like similar to how you would work with the real numbers or the real numbers in mathematics in grade school.
00:01:59.540 - 00:02:49.698, Speaker A: And you work with numbers of dollars and cents, like floating point numbers, if you're doing accounting. Well, not actually floating point numbers sometimes, but that's a different issue. Basically, though, every mathematical field will have its favorite number set. And for cryptography, finite fields are like our set of numbers that we do everything over, talking about addition, multiplication, negation, all these operations are defined in the context of a finite field. This is especially true in our particular subfield of zero knowledge proofs, where zero knowledge proofs are generally computation studied. Zero proofs are generally defined as arithmetic circuits. And so similar to how digital circuits, the basic operations are and or not.
00:02:49.698 - 00:03:37.330, Speaker A: In arithmetic circuits, our basic operations are addition, multiplication, and negation. And so basically, the finite field operations define the basic language of how we actually put together programs inside of zero knowledge proofs. One of the reasons why implementation is actually interesting and difficult is that real computers don't have finite field circuitry. Real computers often will have integer arithmetic. So you have add, multiply, shift, divide. Those are operations that are supported by essentially all architectures, although not all all, but definitely. But the vast majority that you're going to encounter are going to have basic integer operations.
00:03:37.330 - 00:04:30.270, Speaker A: However, these integers are not, these integers are not arbitrary length. Usually computers will have a word size, and the word size generally defines how big of integer it can handle at once. So for most computers we work with today, you have 64 bit words. You multiply numbers up to two to the 64, although the result you'll get will be truncated. But essentially the point here being that we have these operations that are defined in real digital circuitry, and we need to take those operations and actually define finite field operations on top of them. And doing so is not trivial. So one thing we really care about in terms of our implementations is speed.
00:04:30.270 - 00:05:59.194, Speaker A: And why do we really care about speed? A single finite field multiplication, a single finite field addition is probably very cheap. Basically, it might be counted in less than 100 cycles. By itself would be completely inconsequential. The reason why we care so much about this is being the most basic component of our cryptography. The performance of a finite field multiplication or performance of a finite field addition affects is the baseline by which all other performance for our algorithms is kind of defined. So if we're implementing an NT algorithm, or if we're implementing a multiscalar multiplication, which are the two heaviest operations in zero knowledge proof production, we can work hard to make the most efficient possible NTT or MSM algorithm that uses as few multiplications as possible, as few additions as possible, and other operations. But additionally, you're always going to have to use these basic operations, and if you can make your multiplication two cycles faster on your cpu, those two cycles get that little tiny win gets multiplied over the hundreds of thousands, millions into billions of operations you're going to have to do over the course of a large entity or a large MSM operation.
00:05:59.194 - 00:07:24.620, Speaker A: And so, because proving costs in our field are one of the biggest barriers to deploying zero knowledge proofs to the general public, especially when you're talking about proving things in the browser, proving things on a mobile phone, we really care about making sure that we make our cryptography fast. Making cryptography fast involves a lot of things. One of those things being having a really highly, highly optimized finite field. So what are we going to talk about in this study club? We're going to talk about a few things we're talking about like big integers, big integers being a way to represent numbers larger than your computer can natively handle. We're talking about addition big integers, multiplication on big integers, and then talk about modular reduction. So basically where addition, multiplication, modular reduction essentially form the basis of your finite field, we're talking about two different methods of doing modular reduction, and when you might want to use each of those. And we'll also talk about some additional forms of multiplication like caratsuba multiplication and tomb cook, that you might use in certain circumstances but aren't so common in the cryptography we see in the $0 proof field.
00:07:24.620 - 00:08:15.962, Speaker A: So let's start by talking about big integers. So the accountant webinar before basically all digital computers have a defined word size, which is basically how large of an integer it natively handles. Almost all computer modern computers we use, so our phones, our laptops, our servers are using 64 bit words nowadays and can often handle much larger integer values as well, if with specialized operations. But 32 bit computers are not entirely gone. Also, in the IoT world, you kind of never know what you're going to get. You might have some smart card device and for God, some God forsaken reason. It's a 16 bit computer still.
00:08:15.962 - 00:09:14.734, Speaker A: So basically the basic point here basically is that any given computer has a defined word size, which is how large of an integer it can natively handle. And if we want to represent values that are larger than the native word size, we need to break it down into a series of words. Basically, a single 256 bit integer can be represented as either was that 464 bit words or 832 bit words. And that's what you end up doing for Michael's curves. So if you're doing representing a curve point, it consists of two to three finite field values. Each of those two to three values are going to be represented as 464 bit words. Pretty commonly in the case of the zero knowledge virtual machine, we have the bay bear field, which is a 32 bit field.
00:09:14.734 - 00:09:43.670, Speaker A: So generally you can actually fit it in one word. However, occasionally we have to break it up inside the ZKVM into bytes or half words into 16 bit values, which would have a similar idea. But essentially the core idea here is that big integers allow us to represents values that are larger than the computer can natively handle by breaking the large value into words that the machine can natively handle.
00:09:44.810 - 00:09:55.510, Speaker B: Oh, one secondary.
00:09:58.510 - 00:10:00.090, Speaker A: Can folks still see my screen?
00:10:00.160 - 00:10:13.570, Speaker B: Need to start presenting in had some kind of issue. Yeah, it's back. Okay, let's try that again. Okay, there we go.
00:10:13.720 - 00:10:55.120, Speaker A: So here's a value. It's a number. This number is not a huge number, but in the context we're going to talk about here, these are about the size numbers we're going to have. This is a, was it 1234-5678 digit number? And normally in computers, like I said before, you have bit value words, 64 bits, 32 bits, 16 bits, eight bits. We humans are trained to work with base ten. And so for the purpose of this presentation, we're going to use actually a two digit word. So two base ten digits will be our word value.
00:10:55.120 - 00:12:00.014, Speaker A: All the numbers from zero to 100. And basically we're going to use the human computer and we're going to say that the human computer can work with values that are between zero and 100. Anything else with overflow? What's nice about this, for purposes demonstration, is that this number, like 227,311,837, splits into these four words. 27, 30, 118, 37, which is fairly nice for just like looking at it goes, it splits evenly. Obviously, if you think about it from a computer's perspective, 64 bit word, like taking a single 256 bit value, splitting it to 64 bit words is, is very natural because basically it can split along bit boundaries, along byte boundaries very easily, just like we can split digits in a large byte ten number. So the first thing we're going to talk about is addition. Addition being kind of one of your more basic operations.
00:12:00.014 - 00:12:39.786, Speaker A: So we're going to add these two numbers. And if you're going to do this, we kind of learned a method to do this in grade school. You line things up in columns. You take your number, you split the digits out in the middle. In this case, instead of using one digit at a time, we're using one word at a time, which in our case is two digits. And so you put each of them in a column, and then you add the first word, you carry the one, you add the second word, you carry the 1 second word, and the carry carry the one, the third word, and the carry carry the one, the fourth word. In the carry carry the one, you have a final carry.
00:12:39.786 - 00:13:05.062, Speaker A: And so our final value at the end of this is this 1160 00:16 26 value there. And this schoolbook addition algorithm is basically exactly what we do for beginners. It works great. It's a linear time, as best you can do. So schoolbook conditions is what we use in production for adding two values together.
00:13:05.196 - 00:13:05.880, Speaker B: Simple.
00:13:06.810 - 00:13:50.994, Speaker A: I'm going to use this basically as the first example of walking through an algorithm. All the algorithms that I talk about in this presentation are kind of lifted from this great book that I really like personally called the Handbook of elliptic and hyperliptic curve. Cryptography has a lot of good information about optimized implementations. It's focused on elliptic curves, but it includes a lot of good information about finite fields, which is generally applicable. So let's walk through this algorithm here. So, takes some inputs, takes a u value and a v value. Each of them are in word integers.
00:13:50.994 - 00:14:39.530, Speaker A: In this case n for us is four. So taking both forward integers and it's going to give us the addition result in the final value w. So to start the algorithm, we initialize the carry the carry bit k to zero. Then we're going to go forward. And this first step here, we add the u zero, v zero, and carry modulo b, which b in this case is the base. Our base is 100. And so basically you just take the w value as the last two digits of the result and the carry is the first digit role.
00:14:39.530 - 00:15:44.170, Speaker A: So our result in this case of adding 37 and 89 is 126 and 126 we split into the leading one, which is our carry, and then 26, which is the result word. And then you store those into the w value. So I do this again. They see add the carry, the carry, the v one, store the word and the carry, u two and v two and the carry. Store the result word and the carry, and then finally u three and v two, u three and v three and store the lower word, the carry and the carry. And then we drop out a loop and we move the carry into a fifth word for a result. And so now we have this result here, this 1160 00:16 26 just as we did before.
00:15:44.170 - 00:16:39.066, Speaker A: And one thing that's worth noting here is that the two inputs are both four word integers, whereas the output is a five word integer in general. In addition, basically the addition result can be up to double the size of the original result. And so if you have two values, which might be their max value or might be more than half the value of their container, basically. So we have two four word integers, the result is going to be a five word integer. You can see you need somewhere to put the carry and this growth will become important later on. So the other important algorithm to talk about is multiplication. In this case we're going to multiply some smaller values because multiplication usually takes longer and so smaller values to work with.
00:16:39.066 - 00:17:44.274, Speaker A: And so we're going to multiply 50 412 or 5412 times 3629. Now, if it's like a time, because I want to get to the really interesting stuff I'm not actually going to walk through this algorithm here, but I will promise you, this is a definition of school book multiplication. Basically in the same idea that basically here, this algorithm corresponds to what you might do in grade school to add two large numbers together. This algorithm here corresponds to what you might do in grade school to multiply two large values together. And though basically it results in the trace of execution like this. And one thing I will note here is in the final value, this 1964 148 is twice the size of each of either input. Well, it's basically the size of the, it is equal to the size of both inputs added together.
00:17:44.274 - 00:19:09.306, Speaker A: So the first input is two words, the second input is two words, the result is four words. So, multiplication in general doubles the size of the values you multiply, where addition adds one word or really one bit. So this talking about the growth of our numbers here, this kind of the doubling of numbers upon multiplication, doubling the size of numbers up multiplication brings us to the next point, which is that you need to basically do reduction or to get things back into a reasonable form, because basically if you had a long string multiplications and you don't reduce things, you're going to go from having two word values to four word values, to eight word values, to 16 word values. You get exponential growth of the size of your intermediate results. And that's completely untenable for anything beyond very small multiplications. Luckily, however, the way finite fields are defined is that any number is equivalent to basically you can always subtract any number of times the modulus from a number, and the result is equivalent. So in this case we have 1345 times 25.
00:19:09.306 - 00:20:02.190, Speaker A: One is this large number. But if we subtract 1000 multiples of the modulus, which in our case is the prime 29, 71, we get down to the much more manageable number of 673. And those two numbers, the large multiplication result and 673 are equivalent in the finite field. They are the same number for our cases. So whenever we have a multiplication result, we will almost always want to reduce it down to the smallest positive representation. And this is often referred to as like the canonical representation. In fact, if you think about a finite field value, this is generally what people think about is the canonical representation, which is between zero and the modulus.
00:20:02.190 - 00:21:14.146, Speaker A: So how do you actually go about producing this reduction? We'll talk about two methods. The first method we're going to talk about is called Barrett reduction, which can be applied to the integers you normally think about, basically maybe your first go to, and the second one we're talking about is called Montgomery form multiplication and also montgomery form reduction. Montgomery form multiplication reduction is very widely used. It is what we use throughout our code base in the risk zero virtual machine, although it has the disadvantage of requiring it to be in a special format, which we will describe later. Basically, Montgomery form requires is an alternative form from what you might refer to as classical form. And if you receive your values in classical form, then that conversion cost is not trivial. And so you might not want to use Montgomery form if that conversion cost.
00:21:14.146 - 00:22:02.270, Speaker A: If you need to convert ventures first, in this case, you would want to use Barrett reduction. Additionally, some folks have used Barrett reduction in hardware implementations and elsewhere. Generally, Barrett and Montgomery are not so far apart in terms of performance that both are not useful. And so people, although Montgomery form is most commonly used and it's usually the fastest in software, people have found reasons to use Barrett reduction in production as well. So let's talk about that one first. An obvious way to basically produce a reduction is to use division. And so this little Python esque program here basically uses division to influence reduction.
00:22:02.270 - 00:22:59.494, Speaker A: So basically what you want to do, like we had before, you have the quotient and the remainder in this modular division. And the quotient here is 1132, the remainder is 673. And basically you subtract quotient times the modulus from the original value to get the remainder. And so in this case, to find the quotient, we can just do a floored division. So basically we say q, the quotient equals u divided by n, and then we go ahead and subtract q times n from u, and that will give us the fully reduced result. However, division is generally expensive and compared to multiplication compared to other operations that we're doing. And additionally, you really cannot guarantee that it's going to be constant time.
00:22:59.494 - 00:23:43.730, Speaker A: And constant time is important when you're operating over secret data, because if something is not constant time, then how long an operation takes can reveal its values. Basically, time based information leakage has been shown multiple times, basically via avenue through which people can extract private information. And so basically, if you're, for example, producing a signature, this is especially important if you're doing something like encryption. If you're producing a zero knowledge proof and you really need your data to be private, constant time is an important property to achieve. And so for both those reasons, we want to avoid division.
00:23:48.150 - 00:23:49.220, Speaker B: Let's see.
00:23:52.230 - 00:24:35.910, Speaker A: The intuition behind the Barrett reduction method is that you can approximate division by doing, by doing a multiplication by something that is close to one over n, where n is our modulus. And so in this slide, basically, I actually made a slight little notational kind of hiccup, where basically m and r are actually the same value here. But basically there's two different notations for the same thing, but there's this m or r value. And basically, if you want to approximate one over n to achieve multiplication.
00:24:38.090 - 00:24:38.406, Speaker B: You.
00:24:38.428 - 00:26:03.282, Speaker A: Take your base, raise it to the power of two n, and then divide it by the modulus, and you have something that is close to one over n. And the way you actually do this is kind of in this formula below is we do like u times r bit shifted to the right by two to the n. And the reason why, or bit shift, or really base shift, is what you can kind of think about here, where basically in our digit, our digits, base ten notation, this would actually be a shift of digits. That shift basically influences the division. And the multiplication basically allows it to kind of the multiplication plus the division allows it to be close to a multiplication by one over n, which is equivalent to a division. And now, then, as before, you take the quotient, multiply it by the modulus, and then subtract it from the original value, and you get a mostly reduced form. And the reason why I say mostly reduced is that the actual final result will be somewhere in the range of zero to three n.
00:26:03.282 - 00:27:00.578, Speaker A: Let's see. Actually, this is also much easier to fix. So zero to three n, and we don't want zero to three n, we actually want between zero and n. And so if we do either one or sometimes two subtractions, we'll get down to that final value. And so in this case, what we do, as before, we do the partial reduction, and then we do up to two subtractions and we get the final result. So basically, just like to kind of reiterate, the intuition behind bear reduction is that instead of actually doing a division by n, we do what we refer to as an approximate division, which will be a multiplication by a value that is close to, close to one over n by virtue of basically doing multiplication first and then doing a division. And we kind of get within the ballpark of the number that we want.
00:27:00.578 - 00:28:09.838, Speaker A: And in the ballpark can be shown to be defined as up to three times the fully reduced result, which is definitely good enough because all you need at that point is up to two subtractions and you get your fully reduced result. So let's actually walk through the real algorithm. And so in this algorithm, there is some pre computation. And so the value that you need to pre compute is this r value, the r value being kind of the multiplication number that you multiply. Then the input with before you do the right shifting in order to make sure that the combined operation of multiplication plus right shift is close, is approximately a division by the modulus. And so you can see the formula here. You can see in the definition of the input here, the r value is defined.
00:28:09.838 - 00:28:42.414, Speaker A: It's defined as the base to the power of two n divided by the modulus, which is capital n. So in our case before the modulus, which you can see kind of in the computational table below, our input is 27, 30, 118, 37. So, a four word integer. Our modulus is 29 71. The prime that we talked about before our base is 100. As before, our n is two. Since what we want is the word size, we want to work with two word, two word integers.
00:28:42.414 - 00:28:55.720, Speaker A: At the end of the reduction, our r value is the result of this formula above. So r value is 100 to the power of four. So 10,000 or, sorry, not 10,000.
00:28:59.950 - 00:29:00.860, Speaker B: 10 million.
00:29:04.030 - 00:30:03.866, Speaker A: 100 million, I say 100 million divided by 2971, which happens to be which. We floor the value, rounding off the extra bits above the decimal point, and we get 33,658 and we're going to use that pre computed value. So we do this before we do any computation inside our algorithm. So now we have the input, and we do this first step here, which is, this is the approximate quotient. So just kind of breaking down this approximate quotient. We start with saying we take u divided by b to the minus one, which very simply is actually just chopping off the last word, basically. So you can see this, 27 30 118 is equivalent to our 27 30 118 37 minus the last word.
00:30:03.866 - 00:30:10.640, Speaker A: The reason we do that is this is the more optimal way to do it. It turns out that the last word doesn't matter, so we just get rid of it right away.
00:30:11.330 - 00:30:14.466, Speaker B: Then we multiply that our input with.
00:30:14.488 - 00:31:29.610, Speaker A: The last word chopped off times the r value, which results in this much larger number. A quick note that although we have these intermediate results which are larger than we can fit in a given word, so these themselves could be represented as big integers. The reason I don't make that explicit here is because when you actually go to implement this formula, you'll do this iteratively such that you don't ever actually have these large intermediate results. But basically, when you see a large intermediate result like this, you can imagine intuitively that you just represent it with another big integer, a mini word deconstruction. Basically you have this multiplication, you get this large value out. And then you do the divide by b to the power of n minus one, which is actually just equivalent to chopping off all of the first three words or the first six digits. And so we just take the highest two words, which is the highest four digits.
00:31:29.610 - 00:32:12.902, Speaker A: All right, so now we're going to calculate these two values, r one and r two. And when we subtract r one and r two, we'll get the partial reduced result. So r one is the highest, or, sorry, the lowest three words, we chop off the highest. Now, again, because it doesn't affect the end result of the input. So this is the 30, 118, 37, the lowest three words of the input, and then calculate r two value, which is our quotient times our modulus. So if you recall before, basically this quotient on the modulus is effectively like how many multiples of the modulus we are going to subtract. And a quick note as well, that.
00:32:13.036 - 00:32:15.366, Speaker B: In the finite field, any number times.
00:32:15.388 - 00:32:49.330, Speaker A: The modulus is always equivalent to zero. And so this r two value in the finite field is equivalent to zero. This is basically like basically adding or subtracting by r two doesn't actually change the number we're working with in the finite field. So we're going to subtract r two from r one and we get the partially reduced number 24 five. We will check to see if it's negative. So if we wince negative, we need to add a multiple.
00:32:51.910 - 00:32:52.466, Speaker B: We need to.
00:32:52.488 - 00:32:57.774, Speaker A: Add the maximum value of our words, basically get back to positive.
00:32:57.822 - 00:32:58.130, Speaker B: Again.
00:32:58.200 - 00:33:33.458, Speaker A: Basically, this is just the way this algorithm, this is a way of formalizing the idea of that wrapping around this wrapping around would happen automatically on a digital computer. Usually that words wrap around. If you do addition, and then the r value is not guaranteed to be fully reduced yet. So we need to check to see if it's, if it's larger than the modulus, if it is larger than modulus, we subtract. If it's larger than the modulus, again, we subtract. Again, we'll have up to two subtractions here. In this case, there's actually no subtractions because 24 five is already less than our modulus, 29, 71.
00:33:33.458 - 00:34:20.922, Speaker A: And so we're actually done already. And as a quick note in Barrett reduction, the probability that a number will be below the modulus after subtracting is about 90% or, sorry, the probability that the mostly reduced result will be below the modulus is about 90%. There's about 9% chance that it'll be one multiple of the modulus above the modulus. So basically I do a 90% chance, sorry, a 9% chance of one subtraction, and there's only a 1% chance of doing two subtractions. So most of the time, the mostly reduced form is actually going to be fully reduced, as is the case here. And so we return this r value. This is a two word integer.
00:34:20.922 - 00:34:22.640, Speaker A: 24, five.
00:34:24.530 - 00:34:25.440, Speaker B: All right.
00:34:27.810 - 00:34:39.910, Speaker C: Before we dive into monkey form, there is a question in the chat. Someone asks, thank you. You mentioned the modulo will always be prime. Is that for crypto reasons or finite field reasons?
00:34:40.650 - 00:35:19.858, Speaker A: So actually the modulo does not need to be prime for these algorithms. So this algorithm will work with any modulus. In cryptography, you generally do want to be prime, because if it's not prime, it's not a finite field, at least not tactically. We talked about the clock math before. If you do the 12:00 math, you end up with like four has no inverse. And this property is true of any non prime modulus. And so only prime modulus form a simply field.
00:35:19.858 - 00:35:44.266, Speaker A: And if you have prime power modulus, or if you want a field finite field of prime power order, so like, instead of being a field of modulus seven, it could be a field modulus seven squared. Well, not modulus seven squared, but basically of order seven squared. You can define this as well. But basically finite fields must have an order, which in the case of a.
00:35:44.288 - 00:35:46.794, Speaker B: Prime power is just the modulus, is.
00:35:46.832 - 00:35:51.710, Speaker A: Just the modulus itself that is prime.
00:35:52.210 - 00:35:52.960, Speaker B: Yeah.
00:35:57.010 - 00:36:25.286, Speaker A: But the reason why we use big n here in this algorithm tickler is that does not need to be prime. It can be any number we want. And thanks for calling the chat. And I want to actually stop right now, because we went through the first complicated kind of part of this, the first reduction formula, one of the ones that took me a bit to understand myself. One stop for questions. So if anyone else has a question as well, I definitely wanted to give time for.
00:36:25.308 - 00:36:33.478, Speaker B: That's okay.
00:36:33.644 - 00:36:58.180, Speaker D: Can we have more than two subtractions in step four? Is that the reason why we are using a while loop? Otherwise we could have maximum just checked if r is greater than or equal to n two r equals r minus n once, then again, and if r is still greater than or equal to n, then r equals r minus n. And stop. Is it possible that we can still go beyond this?
00:36:59.910 - 00:37:23.420, Speaker A: No, there will only ever be up to two subtractions. And so the while loop basically is just an artifact of. It's simple to write it that way. And also the fact that well, yeah, basically, that's just the way it's written here. If you were to write it in code, it'd be perfectly valid to say, to use two if statements and check once, check twice, if you want to do that way instead.
00:37:24.590 - 00:37:25.340, Speaker B: Yes.
00:37:27.630 - 00:37:28.620, Speaker D: Thank you.
00:37:38.750 - 00:38:32.798, Speaker A: All right, I'll talk about monk memory form. So this is the other major Montgomery form. Reduction and multiplication is the other major way we can reduce numbers to get them back into the canonical representation. One of the things about Montgomery form is called Montgomery form because it is an alternate finite field representation. This alternate field representation enables a fast combined multiplication and reduction algorithm, but does require you to basically do some pre processing to get your numbers into this representation. So what is the Montgomery form? So when you think about finite field elements, classically, they're represented as numbers from zero to the modulus. So this is kind of the most intuitive way I think about finite field.
00:38:32.798 - 00:39:18.794, Speaker A: And that's not the only way I think about it. Montgomery representation basically represents things, numbers, as their classical representation, times r mod n. And so basically, the number actually will still be, the representation will still be a number from zero to the modulus. So it's still, for example, a 256 bit number if it was before. However, it is stored with this r value, kind of mixed in, you might say. And we'll kind of show why we do this later, because mixing in this r value allows us to do, like I said before, kind of a faster combined multiplication.
00:39:18.842 - 00:39:25.086, Speaker B: Reduction. So, one thing before we talk about.
00:39:25.108 - 00:40:03.786, Speaker A: Montgomery reduction is that the Montgomery reduction algorithm is actually just a little bit different than the Barrett reduction algorithm that we saw before. And in particular, it does not actually calculate the reduction itself. It actually calculates the reduction. It calculates a reduction and multiplication by this r value at the same time. And so the result of this reduce, reduction algorithm, notated here as Redc, is u times r inverse mod m. And again, we'll talk about why this is later. It seems a little funky.
00:40:03.786 - 00:41:25.734, Speaker A: It's like, why would I want something that calculates u times r inverse? Doesn't seem that useful. But we'll try to build the intuition for why we want this or why it's useful. And actually, let's skip straight to that instead. So basically, this reduction formula can be used to define multiplication, as you can see at the bottom here, x times r mod p. So this is the Montgomery representation of x. So this is x in square brackets, as noted in the line above, times y r mod p, which p, in this case, is our modulus, just to be clear, times r to the negative one mod p is X-Y-R mod p, where X-Y-R mod p is the Montgomery form notation for xxy because it has a single r mixed in as before. And so what's kind of interesting here basically, is basically, if you do a schoolbook multiplication, just class multiplication of two Montgomery values, and then you do this Montgomery reduction which implicitly multiplies by r inverse, you get the Montgomery representation of the result value.
00:41:25.734 - 00:42:47.138, Speaker A: And so what this does is it basically allows us to define a multiplication formula which multiplies two Montgomery representation numbers and gives you the monk memory representation of their result. This kind of motivates the reason why we want to have this alternative. Why this alternate reduction formula is useful is because it allows us to basically define basically an isomorphic representation of numbers, basically that has its own kind of multiplication that gives you, that stays within this form. And this multiplication is slightly more, this multiplication plus reduction is more efficient than the classical form of the same algorithm. So that's kind of why reduction is also used to get, reduction can also be used, the Redc formula can also be used to get two into Montgomery representation from classical, and can also be used to convert from classical or, sorry, to get to Montgomery from classical, or from Montgomery back to classical again. All right, so now let's actually walk through this algorithm. So, as kind of before, there is a pre computation step which basically gives us some values we need.
00:42:47.138 - 00:43:49.446, Speaker A: So in this case, the r value, which, to be clear, is not the same r value as before, but r value in this case is simply our base to the power of n. So n is the number of words in our integers we're working with in our multiplication inputs, in which case, as before, we're working with two word, we want two word inputs and outputs. So we're reducing to that much in this case. So our value then is 10,000 or 100 to the power of 100 squared. We need a special value which is referred to as n prime or negative n to the negative inverse. And that value here is 69 or negative inverse mod our base. So mod 100.
00:43:49.446 - 00:44:18.030, Speaker A: So basically, just take the last two digits of negative one inverse, and we take that value, which is 69 in this case. And we're going to use that within our algorithm. And this is a constant that is used, that is determined by our modulus and our r value, and can be pre computed before we ever start the algorithm.
00:44:18.370 - 00:44:20.718, Speaker B: And so now we're going to go.
00:44:20.724 - 00:44:35.218, Speaker A: To the algorithm and I'll try to kind of explain the intuition behind this inverse value and all the operations that go on here. But the first step in this algorithm is simply copy your input into your output. So we start the t equals the.
00:44:35.224 - 00:44:35.970, Speaker B: Output.
00:44:38.550 - 00:45:10.554, Speaker A: In the first iteration of the loop, which we're going to iterate from zero to n minus one. So we're going to do zero and one. In this case, we calculate this k value and k is the ti, which t zero. So in this case 37 times 69 mod b. So take basically 37 times 69. Take the last two digits, which are in this case 53. And then you take this intermediate value, this ki, multiply it by our modulus.
00:45:10.554 - 00:45:32.790, Speaker A: So now ki n times b to the power of I be zero. So it's just ki times n. At this point. This is a multiple of the modulus. So as before, any multiple of the modulus is zero. So this number is actually equal to zero in the finite field. So then we add was effectively zero.
00:45:32.790 - 00:46:03.038, Speaker A: But is this number to our t value. And something magical happens, which is that the last word of our t value is now zero, which is very useful, because basically if we now we do a right shift or division, then it will actually divide cleanly. So the goal here is actually going to be to make the right two words of the t value zero. And in this first step, we just.
00:46:03.044 - 00:46:04.240, Speaker B: Did the first word.
00:46:05.030 - 00:46:09.300, Speaker A: So let's do it again. I equals one. So we're going to take the.
00:46:12.550 - 00:46:12.866, Speaker B: T.
00:46:12.888 - 00:47:22.570, Speaker A: One, multiply it by our n prime value, and take the last two digits, so that in this case is 93 times 69, and the last two digits of that number are 17. Multiply that by the modulus. So we have this special zero value that we then add to t and it sets the second word to zero. And so now we have this number 32, 52, which has the right two digits being zero, and we divide by r. And so this is where the r inverse comes in, where basically in the finite field we just did a division, which is in this case just a right shift by four digits or two words, and that division results in a number times r inverse or a number divided by r in the finite field. But also what's really nice is that the concrete representation of that value is now much smaller. Instead of being four words, it's two words, which means that basically we have done kind of the most important to us step of the reduction, which is get it smaller.
00:47:22.570 - 00:48:30.734, Speaker A: Now, in this case, like just as before, this gives you a mostly reduced result. In this case, the reduced result is going to be between zero and two times the modulus, so that we need up to one subtraction and in this case, we actually do need a subtraction because it's larger than modulus currently. And the result is 281 and that 281 value, as promised, if you multiply it by r, so 281 is the reduction of u times r inverse. But if you multiply it by r, which r is simply 10,000, and take that number modulo r prime, you get 24 five, which, if you recall from the Barrett formula, is the exact same number as we got from that classical reduction formula. So this Montgomery formula really does do what it says it does. It computes this radc, u equals u times r modulus, sorry, u times r.
00:48:30.772 - 00:48:35.870, Speaker B: Inverse mod n. So there is also.
00:48:35.940 - 00:49:44.166, Speaker A: Additionally a combined multiplication introduction formula. And I won't walk through this one, but what I want to note is that this algorithm is very similar to our reduction algorithm, except for on line three, there's a multiplication, or line three and line four. So basically, instead of just doing this trick of setting the right bits of your number to zero, you're setting the right bits of the multiplication of your two numbers. So, ui and v, zero or Ui and v in this case. So the multiplication result, you're setting those right bits to zero, and then you're shifting right in every loop. So the two modifications from the previous algorithm here to this one here are that instead of just doing this, like set right word to zero, tricked on the input, you're doing it on the result of the multiplication of the input value. And then additionally, this algorithm here, instead of doing a final addition by r.
00:49:44.268 - 00:49:50.134, Speaker B: On line five, it does a division.
00:49:50.182 - 00:51:03.554, Speaker A: By r, or division by b on line four, which over the course of the entire algorithm, doing n iterations of division by b is the same as dividing by b to the power of n, and so gives you the same result. But it's just done iteratively. And this kind of iterative computation is what you're going to see for basically any software implementation, because basically it allows you to avoid these large intermediate results like we saw in the formal or the pseudocode for the Barrett reduction. All right, and so this is kind of the other one. And this one, the Montgomery multiplication, for me personally, was a bit hard for me to understand at first, and curious if people have questions. Let's talk about a quick comparison before. So I mentioned before that Montgomery form is the most common in cryptographic implementations.
00:51:03.554 - 00:52:01.654, Speaker A: You'll see it's what we use personally in our risk zero, ZkvM and Zk cryptography implementations. This is a quick comparison of the number of multiplications which multiplications being actual like machine multiplications, what your computer will compute, how many multiplications each algorithm takes, and the classical form takes n times n plus 2.5. And it also requires n divisions. So basically where n is number of words. So in this case, our four word numbers are going to have four times four plus 2.5 multiplications and four divisions for classical, which those four divisions especially are kind of a major issue for performance. Whereas Barrett reduction has slightly more multiplications but no divisions.
00:52:01.654 - 00:52:40.280, Speaker A: So it's n times n plus four. And Montgomery form has fewer multiplications than that. So basically it's n times n plus one. And so Montgomery is for most cases the fastest formula to be using for multiplication. There are also some precomputations that are involved and additionally some restrictions. Precomputations are important if you want to do many modulus, although in cryptography we usually pick a field and stick with it, so we only have one. And we can do this pre computation even at compile time and have the constants already provided for us in our code.
00:52:40.280 - 00:53:35.062, Speaker A: And then this restrictions about how large your numbers can be before you reduce them, which can also come into play. But the key we want to focus on here basically is that Barrett Montgomery form, both have, both avoided divisions, which is a major key. Montgomery form is generally faster, but requires your inputs to be translated into Montgomery form before you can compute on them. I also want to briefly mention some other alternative multiplication methods. So, as we saw before, the asymptotic runtime for all of these algorithms is o of n squared. As n, the number of limbs gets larger and larger and larger. So if you have, we go from having one word integers to two words, to four words, to eight words.
00:53:35.062 - 00:54:53.258, Speaker A: If we have massive integers like our 40, 96 bit RSA keys, or maybe integers that are even larger for some other algorithm we're working on, you start to care about your asymptotic runtime in a number of limbs. Multiplication was quite, for quite a long time, believed to have a lower bound of, to have an epilogue runtime of o of n squared, like no matter what you did. However, in 1960, Karatsuba discovered a divide and conquer method that has runtime of o of n to the log to the log base two of three, so about 1.58, which is better than ovin squared. After that, tomb Cook created a kind of similar but improved version of this kind of method. And then later still, some folks discovered NTT based methods for multiplication, which have even better asthmatic runtime. However, all of these formulas are concretely slower than monk area multiplication and classic and schoolbook multiplication until you get to fairly large integers.
00:54:53.258 - 00:54:55.230, Speaker A: So for cartsua multiplication.
00:54:57.410 - 00:54:58.446, Speaker B: Caratsuba and tomb.
00:54:58.478 - 00:55:18.454, Speaker A: Cook can be applied to things that are kind of reasonable. So like, if you have like eight word numbers, it might already start to apply. Whereas NTT based methods really don't apply until you get into the order of maybe 1000 words or more.
00:55:18.572 - 00:55:20.970, Speaker B: And so if you have like a.
00:55:21.040 - 00:55:51.140, Speaker A: 49 six bit RSA key, you might use tube cook. If you have much larger numbers, even still, you might start using the NTT based methods. But concretely, Montgomery multiplication, classical multiplication and Barrett reduction are going to be faster for most of your needs, for most of the numbers we work with today in cryptography. And that's all I have for y'all. I'm curious if people have any last questions.
00:55:53.670 - 00:55:54.542, Speaker B: Thanks, Victor.
00:55:54.606 - 00:56:04.630, Speaker C: There are a couple of questions in the chat and maybe either Venky or Victor can speak those out loud because the chat won't be in the recording.
00:56:05.210 - 00:56:06.950, Speaker B: Yes, that'd be very helpful.
00:56:10.170 - 00:56:58.710, Speaker D: Yeah, there's Venki here. Let me repeat those questions. Sorry, I put on chat because there was noise outside my room. So one question is, since this moncomeary representation is an alternative representation of the normal of the integers in the field modulo n, can we safely say that the operation, the multiplication equivalent operation in this alternative field is your multiply. Take any two elements, xr and yr and form the product xr y r inverse mod n. Is that the operation in this alternative field? Because that ensures that closure property is there. Xr yrr inverse is equivalent to xyr.
00:57:00.090 - 00:57:00.840, Speaker A: Yes.
00:57:02.170 - 00:57:02.534, Speaker B: Right.
00:57:02.572 - 00:57:03.506, Speaker A: So the relevant.
00:57:03.618 - 00:57:05.400, Speaker B: Oh, I don't pass it. Right.
00:57:07.630 - 00:57:09.178, Speaker D: Yeah, that one. That one. Yeah, that's the one.
00:57:09.184 - 00:57:10.170, Speaker B: That's the slip.
00:57:10.930 - 00:57:34.210, Speaker A: That's the reduction. Then there's also this one as well. Well, the answer is yes. Basically this algorithm here, this combined multiplication reduction, this algorithm is closed over the Montgomery form integers.
00:57:34.950 - 00:58:00.700, Speaker D: Okay. And I had a second question where you were comparing the different operations and the different other algorithms on that slide. I would like to ask about one more algorithm, if you knew about it, or if you have any thoughts about it. And that is using the fast Fourier transform algorithm for multiplying two integers. Would that be an o of n log n complexity, which may be way better than these?
00:58:04.850 - 00:58:34.390, Speaker A: In this case, the NTT is what I referring to. I think we're referring to the same thing. In this case, I'm talking about NTT based methods are in my. I think we are also talking about as well. And yes, they have o of n times login times log login runtime. And they are asymptotically faster but concretely slower until you get to much to get into very large integer sizes.
00:58:35.610 - 00:58:36.600, Speaker D: Thank you.
00:58:46.370 - 00:58:47.760, Speaker C: Any other questions?
00:58:53.590 - 00:58:54.450, Speaker B: Mmhmm.
00:58:57.110 - 00:59:06.440, Speaker E: I've got one other question. If there's time, I know we're a minute over the time limit. It's not a deal if there isn't time, but if there is, let me know.
00:59:07.530 - 00:59:11.354, Speaker B: Yeah. What's your Russia? Yeah.
00:59:11.392 - 00:59:50.040, Speaker E: So the question is just more about how this actually translates in practice. Like say for example, the actual finite field arithmetic and reduction, all this kind of stuff inside of the risk zero vm. Is this just used for generating the proof? And when you're doing the actual computation you're not doing this reduction? Because I'm trying to wrap my head around how you would actually get useful results when doing the actual multiplication and then doing the reduction. I basically just asking a question of how this translates into practice. It might be out of scope of this talk, so if it is, just feel free.
00:59:54.010 - 01:00:37.590, Speaker F: I can speak to that really quickly. This is Jeremy here. In terms of every place we use finite fields in the entire prover, everything is always in a Montgomery form. There's only a couple of places where the constants are actually pre translated by the compiler into Montgomery form. Everything is generally done in Montgomery form wherever possible. In terms of the guest, there isn't really, I mean, that's just a normal risk five machine. And so you could write Montgomery form or not.
01:00:37.590 - 01:01:08.320, Speaker F: We do also currently have a finite field accelerator which does the work in the circuit, and so that doesn't use Montgomery form because arithmetic circuits doesn't make sense. They're already natively operate over finite fields. There's no need to do a reduction because it's all in finite fields in the first place. But yeah, so basically all of the implementation everywhere in our current stack uses Montgomery form for all the work.
01:01:10.610 - 01:01:11.934, Speaker A: Does that answer your question?
01:01:11.972 - 01:01:12.670, Speaker F: Hopefully.
01:01:14.790 - 01:01:19.620, Speaker E: Yeah, definitely enough for now. I think I need to dig more into some of those concepts a little bit more today.
01:01:27.580 - 01:01:55.330, Speaker C: Well, if there's no more questions, I guess we can wrap up. I will echo Monsie's comment in the chat that we are going to be hosting a dev meetup at ETH Denver, so you can find details for that on discord and on Twitter. And we'll get the recording and the slides up both on YouTube and on slash study club, likely by the end of the day, but certainly shortly. Thanks Victor, and thanks everyone for coming.
