00:00:00.090 - 00:01:18.294, Speaker A: Thanks Paul. My name is Roy Rodstein. I've been working on integrating machine learning algorithms into the RISC zero platform for the past few months, and hopefully we'll be continuing to work on some more sophisticated integration down the line as well. I wanted to present this morning on two specific crates that we've integrated into risk zero that enable pretty much every classifying regression algorithm that you would find in Python, scikitlearn, except for neural networks and deep learning capabilities. So I'm fairly certain that most of you are familiar with risk zero, but for those who are not letting me there we go. What exactly are we doing? What is risk zero and what is it? Not so quickly. In a nutshell, risk zero is a tool that enables a programmer to write any rust program and execute it, and convert and generate a zero knowledge proof attesting to the correct execution of that program.
00:01:18.294 - 00:01:48.906, Speaker A: And the way we do that is compile the rust code to RISC five, generate an execution trace, and use the stark protocol to generate a proof. RISC Zero is not a layer two. We're not a ZK roll up. It is a standalone tool that can be used to post proofs on different chains. Right now we have integration with sepolia, but next year we are aiming for Ethereum mainet as well. You can also just run proofs locally on your device. They do not have to be verified on chain.
00:01:48.906 - 00:03:07.266, Speaker A: You can even verify them within the risk zero tool. But it is meant to be a general purpose ZKVM, a virtual machine that enables you again, to write any arbitrary code. It can be something as simple as computing the Fibonacci numbers, or something as sophisticated as constructing ethereum blocks to prove that they were actually constructed correctly. And so the general purpose nature of the virtual machine enables us to tackle a whole host of applications, including and germane to this discussion today, machine learning algorithms. And if you want to explore the roughly two dozen or so examples that showcase the capabilities of risk zero, you can go to the GitHub, navigate to the examples section of the repo, and explore all of the above. I will just mention that the verification of external snark systems is something that's currently in the work, and I'll touch a bit on that later in the talk, because it is going to be relevant to the machine learning efforts that we are currently tackling. Okay, again, RISC Zero is a virtual machine.
00:03:07.266 - 00:04:19.290, Speaker A: You can think of it as essentially a computer that you run a program, and a proof is generated after the execution of that program. There are two components that you will be interacting with primarily when writing that program, the host, which will transmit all the relevant data to the guest, will execute the code that you want to generate a proof for. So if you are computing Fibonacci numbers, the logic for computing Fibonacci numbers will take place in the guest. If you are running a machine learning algorithm for inference, the inference calculation will be occurring in the guest. The host will be transmitting the input data, the host will also be transmitting the model parameters to the guest. But all the execution and proof generation, or, excuse me, the proof generation will be happening on the host side, but all the code that you want to zkify will be contained within the guest. Okay, so when we are thinking about ZKML, it's somewhat of a buzword.
00:04:19.290 - 00:05:24.766, Speaker A: On two sides of the equation. One side of the equation, we have machine learning algorithms, ranging from traditional linear regression extending all the way to the latest and greatest large language models. And on the cryptography side, we have signature schemes, we have fully homomorphic encryption, zero knowledge proofs, and all the good stuff that enables those capabilities. Now, the two fields address very different problems. Fundamentally, machine learning is looking to either classify data in a certain category, identify patterns, generate image or text based on some kind of an input. And in cryptography, we're fundamentally concerned with encrypting data, protecting data, ensuring that an adversarial party doesn't gain knowledge of some private communication or some private calculation that is not meant for them to know about. So what is the intersection? And I'd be happy to open it up to participants in the chat.
00:05:24.766 - 00:05:50.880, Speaker A: What do you think goes in the middle of this Venn diagram? And it could be related to blockchain. It could be related outside the blockchain. But if you want to post some ideas in the chat, Paul, you can relay them to me, and we can have maybe like a two minute quick discussion on what you think goes in the middle and also how big the middle of the ven diagram is or should be.
00:05:57.590 - 00:06:09.350, Speaker B: All right, we have so far, computation and data privacy, attesting to the model execution and verifiability.
00:06:12.640 - 00:06:34.980, Speaker A: So what benefit do we get from proving correct inference of a machine learning model? So if I can get an answer or two to that, then I'll switch over and kind of give my take on the next couple of slides.
00:06:38.860 - 00:06:40.330, Speaker B: Could you repeat the question?
00:06:41.500 - 00:07:04.976, Speaker A: Why do we need proof of inference? Why do we need to prove the correct execution of a machine learning model? And it could be related to some of the other things that were mentioned, privacy, verifiability, what would be a concrete, specific example so we can trust that.
00:07:04.998 - 00:07:06.850, Speaker C: The output was not tampered with.
00:07:09.380 - 00:07:35.454, Speaker A: Okay, I'm sorry, go ahead. Yeah. So then we're trusting that the model was actually constructed correctly. I see one more thing in the chat. Yeah.
00:07:35.492 - 00:07:38.110, Speaker B: Someone said models typically run on large servers.
00:07:39.830 - 00:07:40.820, Speaker A: Quite true.
00:07:43.030 - 00:07:52.470, Speaker B: And then there's a couple other comments here. One, Benjamin says that he's interested in how machine learning can be used for more efficient proving algorithms.
00:07:53.290 - 00:09:04.250, Speaker A: That's an interesting one. So that would actually be reversing it and doing MlZK. So actually using the corpus of research done in ZK and cryptography to somehow come up with a new snark or stark scheme, or maybe even write custom circuits. I think this actually came up in a discussion a few months ago in a ZK meetup, and the security auditing teams were quite skeptical of that proposition. But it's certainly an interesting one. Writing circuits manually defining constraints is not a trivial task. Paul, if more things come up, feel free to add on to what I'll say for this slide, because I think it's an interesting discussion that needs to be had, in part because it's a very nascent field that has garnered a lot of attention and might be, I wouldn't say overhyped, but maybe the capabilities or the perceived capabilities are somewhat overextended in the general conversation.
00:09:04.250 - 00:09:07.002, Speaker A: I want to just hold on.
00:09:07.056 - 00:09:33.480, Speaker B: Before you, Roy, go on, let me just sort of invite. It seems like we have quite a rich conversation here, and I want to invite people to use the microphone in addition to just using the chat, because the presenter generally is going to have a hard time seeing what's in the chat. So just in order to sort of make myself a little bit less of a bottleneck for conversation, I want to just invite people to unmike or to unmute and jump into the conversation as they see fit.
00:09:39.370 - 00:09:40.950, Speaker A: Go for it, don't be shy.
00:09:41.770 - 00:10:06.590, Speaker B: Yeah, I'll just sort of like. There are a few more comments in the chat that I don't think you've seen yet. Like somebody commented about zero knowledge pattern detection. For example, object is a vehicle, but doesn't say what type of vehicle, or like protecting trained data privacy. And then Andrew commented, the model impacts a scarcity metric of the system, which is different than the classic recommendation algorithm.
00:10:09.090 - 00:10:10.720, Speaker A: Can you repeat that last one?
00:10:11.330 - 00:10:19.810, Speaker B: The model impacts a scarcity metric, which is different than the classic recommendation algorithm. So those impacted by scarcity demands want to verify.
00:10:20.470 - 00:11:11.522, Speaker A: Okay, interesting. Cool. So the little matrix chart that I have in the bottom right hand corner is kind of how I think of ZKML as a general framework. We can have a private model where the weights are not revealed publicly, a public model that anybody can inspect, similar to how Twitter open sourced their recommendation algorithm. However, they don't have a ZK component to actually prove that they're using it. On the other side, you can have private data and public data. Now, I think the bulk of ZKML today, where we can claim that we have enabled trustless compute, will be on the right hand side.
00:11:11.522 - 00:11:59.906, Speaker A: With a public model and public data, it's kind of trivial. Anybody can verify that a model was executed correctly on a certain bit of data trivially, by just rerunning the algorithm themselves. And so ZK basically eliminates the need to do that, but it can be verified. So we have the trustless compute in that domain on private data with a public model. Similarly, we can attest to the correct execution of a model without having to reveal private data. I think this is the classic ZK problem that is generally solved with ZK proofs. Now, when we are dealing with a private model with public data, this is a bit of a question mark which I think there is a solution to, but not currently.
00:11:59.906 - 00:13:21.738, Speaker A: It will involve applying ZK to the training side of the equation as well. One thing I do want to mention, when we are talking about private data and running any kind of ZK program on private data, it is almost always going to be done client side, meaning that the person who is in control of that private data is the individual who's also going to have to generate the proof. Because otherwise you would have to send your private data to an external prover who would then generate the proof. But then is your data private anymore? Think of it this way. If you have x ray scans for medical application for some kind of machine learning model that will classify potentially tumors from an MRI or an x ray scan, you would typically send that data to some external service provider specialist, who would then analyze the scans and generate a result. However, you have now, if you haven't encrypted your name and identification data, you have had to actually transfer your private data to someone else, and technically it's no longer private. However, you do not have access to the model itself unless it's public.
00:13:21.738 - 00:15:36.990, Speaker A: And if it were public, you would still have to do the heavy lifting of generating the proof on your end if you wanted to preserve your private data. So I think this is something that doesn't get mentioned, at least in the general conversation with ZK proofs, and certainly ZKML. But if you do want the top right domain you're going to be generating, you will have to generate the proof client side on your own, using your own infrastructure. Private data on a private model. I'm not even sure what that would entail. I don't know who would be making use of such a proof, but in terms of tackling the bottom left category public data with a private model, I think the solution comes on the training side. And as of right now, training or applying ZK to the training part of ZKML is prohibitively expensive, rendering it essentially infeasible at this point right now, until infrastructure improves, until we are better able to optimize the ZK process, the ZK proof generation process how do you tackle training in a ZKML framework where you want to prove that the training was done correctly? It's a bit complicated, because unlike inference, which just comprises one calculation, albeit it could be a very expensive calculation, there are multiple components in machine learning training pipeline, gathering data, formatting the data, doing feature engineering on the data, and then the process of actually training the model entails setting a whole bunch of parameters, including whether or not you're using batch normalization atom optimizer, specifying a specific loss minimization function, and then of course running backprop on the training set in order to reach a correct set of parameters that can generate inference with very high reliability and precision.
00:15:36.990 - 00:16:01.040, Speaker A: So that is going to be expensive. You could potentially select one component or multiple components out of the training process and apply ZK to them, but it's not clear what ZK applied to the training pipeline would entail. And if somebody wants to jump in, maybe throw in a suggestion, I'd be happy to hear what they have to say about that.
00:16:02.050 - 00:16:09.300, Speaker C: I actually have a question on this. What is an example of when you'd want to use ZKML in the training side of the model?
00:16:10.870 - 00:18:33.460, Speaker A: So, OpenAI was recently sued by many parties, but more relevantly to this question, by a group of publishers who claimed that OpenAI had used the corpus of text generated by the publishers without proper licensing or paying for it. So I think the author of Game of Thrones was one of the signatories to this lawsuit, claiming that basically OpenAI was using copyrighted material without permission. So what could OpenAI do in the future to ensure that they are not running afoul of copyright claims or something similar? They could apply ZK to show that only permissioned data sets were actually used during the training process, and conversely, that restricted data was not incorporated into the training process itself that I think is going to be the most germane going down the line. How exactly you do that, it's not clear to me, but I think that is probably the most relevant aspect going down the line, especially when foundational models are going to become increasingly ubiquitous in different applications of deep learning, as more and more people will be building custom applications on top of them. So it's going to be important to ensure that the foundation has actually been trained correctly and legally. Now, in terms of enabling, going back one slide, in terms of enabling the bottom left corner, I think when you prove that a private model has been trained correctly, that red check mark or that Red Cross turns green, because now we can be sure that the model deployer or the model trainer wasn't including some kind of little backdoor into the model that would allow them to generate an inference. Say, if you're coming up with a pricing model that would generate some output that would potentially be advantageous to them and disadvantageous to whomever else might be using the model.
00:18:33.460 - 00:18:36.754, Speaker A: Does that make sense?
00:18:36.952 - 00:18:38.260, Speaker C: Yes. Thank you.
00:18:39.030 - 00:20:38.896, Speaker A: Cool. So that's kind of my overview, my bias of the landscape. Now, in terms of blockchain world, I think we actually have a bit more of a limited use case, in part because Ethereum is still largely an ecosystem for token trading. And yes, real assets are still being exchanged, but fundamentally it's still a defi ecosystem, and the supporting infrastructure around it is going to be tailored to those applications, credit score, risk analysis, pricing models for amms, that sort of thing. So do we really need deep learning for those applications? And if not, what's kind of the ceiling for ZKML inside of blockchain world? After all, ZK, the primary application right now, is to offload expensive compute on Ethereum and other blockchains, where the execution of computation is not only public but at points, could also become prohibitively expensive. ZK addresses that by offloading that compute and then posting a proof of correct execution on chain instead. So what is really useful to offload from Ethereum and other blockchains that machine learning can solve? Any thoughts? Am I still with you guys? Paul? Yeah, we're still here.
00:20:38.896 - 00:20:49.540, Speaker A: Okay, cool. All right, if notetakers, I'll just give my bias. And I saw 16, went to 17 in the chat.
00:20:53.160 - 00:21:00.984, Speaker B: It says, are you saying that the model use cases like risk model or pricing model are not a good risk zero use case?
00:21:01.182 - 00:22:28.468, Speaker A: Oh no, I'm saying that they are. And in fact, my next point is that my argument, a bit of a loaded statement is that we can actually enable all the relevant applications for the ZK space. So I'll go ahead and actually get going to that part of the talk. I want to highlight some differentiators where RISC zero has some different capabilities and offerings that separate itself from other protocols and frameworks that are also implementing ZKML solutions. So while we don't have direct ability to handle deep learning models, we can, through smart core, enable all your traditional classifiers and regression algorithms like support vector machines, logistic regression, decision trees, clustering those kinds of ML algorithms. And in the future we hope to be able to expand those capabilities with the next generation of our circuit where we can code custom accelerators specifically. In this case it would be for machine learning operations like matrix multiplication, Relu, Softmax, but that will be at a point in the future.
00:22:28.468 - 00:24:02.928, Speaker A: There are some projects right now that are offering deep learning solutions like Modulus Labs, Ezekiel and Giza, which currently is the deep learning Onyx Zkmo solution written in Cairo for the Starknet ecosystem. But one of the limitations with these protocols is that while they're optimized for generating machine learning models, if you want to do ancillary logic on top of it, it becomes quite difficult. An example would be, say in Ezekiel, which is a halo two based framework. You can transpose any deep learning model into an Onyx file and generate a ZK proof using Ezekiel. But if you want to do say you want to ZKFI a generative model and create an image based on some input, you will get pixels out from the model. But if you want to encode it into a JPEG or PNG or some other image file, or do some kind of post image processing, a virtual machine is much better suited for that application than having to write custom halo two circuits. Each time you want to do a different, say, not a degree of image compression, but depending on how much you want to compressor encode an image, you would have to write a custom circuit for that process.
00:24:02.928 - 00:24:56.470, Speaker A: So we can verify an Ezekiel proof within RisC zero and then add ancillary logic on top of that. So currently I'm going to be talking about two crates that we've integrated, smartcore and Forest ML. Forest ML is our XGBoost solution for smartcore. You will need a little bit of rust savvy to do the model training. The nice thing about the forest ML crate for Xgboost is that while it's a pure rust implementation of the Xgboost algorithm, it comes complete with python bindings. So you do not actually need to touch any rust during the training process. Okay, so I'm going to show you some code snippets, some screenshots of how to spin up a smart core project.
00:24:56.470 - 00:25:47.316, Speaker A: I would recommend if you do not have Jupyter installed. I like to use Jupyter notebooks for training a model, just a machine learning bias. Jupyter has a rust kernel, which you will need for smart core if you elect to do training inside of the notebook. The install instructions are here. They're also going to be found on the RiSC Zero examples repo smartcore ML. The readme will walk you through exactly how to get up and running with all the necessary dependencies. All right, so step one, if you're spinning up a new project, you will in command line write cargo risk zero, your project name.
00:25:47.316 - 00:26:59.080, Speaker A: I like to enable the standard library, and then also you will specify the guest code name as well. And after you've done that, you should get a screen that is similar to what is posted on the left. And in your vs code you should have a host folder, methods folder, git ignore cargo, toml file, readme tool chain, and license file. So for smart core, there are a few other folders and files that I add. One of them will be the Python, excuse me, the Jupyter notebook with the Rust kernel. I also add a res folder which will contain the trained model, and I also will be importing the inputs and classes for my training data. This presupposes that you've done all the necessary data pruning feature engineering, say, in Python, or you could do it in rust, but rust is a bit more challenging in terms of data preparation.
00:26:59.080 - 00:28:34.904, Speaker A: This will presuppose that you've already formatted your input data correctly into input data and classes or outputs as X and Y CSV files, so be sure to add those to your project directory. And most importantly for smartcore, RiSC Zero has a custom fork. The reason is that we enable support vector machine capabilities, which in the native smart core library there is a small issue with serializing and deserializing the support vector classifier and regression models, but our fork addresses that issue. So be sure that you are using the risk zero smart core fork, especially if you want to use support vector regressor or classifier algorithms in smart core. And then also be sure to add surday JSON and the surday message pack dependencies. So when you've done all that, your cargo tumble for the host should look something like the left hand side of the screen, the bottom left hand side of the screen. So hopping over to your Jupyter notebook, you will need to install smartcore polars, which is the database package rust based.
00:28:34.904 - 00:29:30.540, Speaker A: For those familiar with the Python version of Polars. Polars is entirely rust based, and you can work with data in rust. We will be using Polar's to import the CSV file. For the Y CSV file, make sure then you import all the dependencies listed right below. And we use the smart core CSV reader to read in the input values. And the reason we do that instead of using the polar's reader is that the smart core reader will automatically format the input values into the dense matrix format. The dense matrix type is a necessary component for training the data, so be sure that you're using the correct reader for the input.
00:29:30.540 - 00:30:31.660, Speaker A: And for the Y CSV, you can use the Polar's CSV reader and just be sure to follow the code and convert it to a vector of U 32s. So all you have to do is just copy and paste that code and you will have your X and Y ready for model training. And here we're using a decision tree classifier. You can specify different criterion, the max depth of your decision tree, the number of leaves, the number of splits, the type of entropy, the split criterion. Here we're using entropy, but you can replace this model with custom params with any type of model that you want. It can be a linear regression model, it can be a clustering algorithm. So this is the part that you will replace.
00:30:31.660 - 00:31:56.712, Speaker A: And once you have done that, you will then train the model and export it as you have the option to export it as JSON file as pen code here. I'm just doing it as a JSON file. JSON will just give you an easy visualization of what the model actually looks like, but you can export it as any kind of binary that you wish to use. So one of the caveats with smartcore is that when you are deserializing the models, you have to specify the types using the correct generic values. And so I have a list here of almost all of the algorithms that smartcore offers, destructuring the types as you would need to input them when you are deserializing the model into risk zero. And so it's important that, let's say you're using k means, which is the second model from the bottom that you're using the type definition as specified. You can find all of this in the project readme, but I just wanted to direct your attention to that because it's extremely important that you specify the type.
00:31:56.712 - 00:32:14.928, Speaker A: If you just try to deserialize the model without specifying the type, it will not work. And it can be a little bit tricky to figure out on your own exactly how to define all the types. So I've just listed all them for you. And the decision to classify that.
00:32:15.094 - 00:32:25.190, Speaker B: Yes, there's two questions in the chat. One is, what is a dense matrix type? And the other is, what is deserializing into risk zero mean?
00:32:25.560 - 00:33:25.860, Speaker A: Okay, I'll show you what deserializing into risk zero means in just a second. The dense matrix type is what smart core uses for your input data. So you can't just give it a vector of f, have to take that vector and put it into a dense matrix type. And that's basically a format that smart core uses in order to both train the model and run inference. Deserializing the model. Let me quickly show you what that entails. So if you look to the third line of code where I have type model equals decision tree classifier, you can see here that I'm specifying the type as a decision tree classifier with the correct generic values.
00:33:25.860 - 00:33:58.008, Speaker A: And then in the next line, I am deserializing the JSON model, which I've exported from my rust Jupyter notebook from the JSON file into the native rust model type. Does that make sense? Whomever was asking about deserialization, what we've done is.
00:33:58.094 - 00:33:59.544, Speaker B: No, say it again.
00:33:59.742 - 00:34:34.580, Speaker A: Okay, so in our Jupyter notebook, we've trained a model, we've exported that trained model as a JSON file, and now we have to import that model into RISC zero. And the way we do that is by deserializing the JSON file into the native rust type, so that we can now run inference on the model and generate a proof of inference inside of RISC zero. In order to do that, we have to convert the JSON file. We basically have to import the JSON file into risk zero into rust.
00:34:38.040 - 00:34:58.010, Speaker D: Just a clarifying question. So basically, it's the host that are actually deserializing, and the host try to send the data to the gas. I would have a question about the overhead, but I think I would like you continue. By the way, this is the new version, right? Because it's proof rather than proof of eelf, right?
00:35:00.220 - 00:36:19.776, Speaker A: That's correct, yes. Version 19 or zero point 19. Okay, one quick caveat. When you are using support vector machines, the reason that we have to use a fork is that one of the parameters in the support vector classifier and regressor struct does not get serialized with a smart core, and there's a complicated, boring reason for that. But we do have a workaround, and the workaround is that whatever kernel, whether it's a linear basis kernel, gaussian kernel, polynomial kernel, whatever kernel you used to train your support vector machine classifier or regressor, you have to recreate that kernel inside of the guest code, and I'll show you how that's done. But I want to specify that particular point with regards to svms. If you plan on using svms, you will need to define the kernel inside of the guest and then update the model to reinsert those parameters inside of the guest before you can generate inference on SVM.
00:36:19.776 - 00:37:24.660, Speaker A: So that is the sole thing that the risk zero fork is addressing. All we did was make this params field public, where in the original version of Smartcore it's a private field. So by making it public, we're able to essentially just reinsert the params back in the guest to build a complete model and then use it for inference. Okay, so this was the host side where we are deserializing both the model and we're deserializing the model, deserializing the data, sending it over to the guest. Remember, the guest is where we are executing code that we want to prove, and in this instance we want to prove that we are generating an inference correctly. And so this is what it will look like. On the guest side, we read in two parameters.
00:37:24.660 - 00:38:31.470, Speaker A: The first we're going to be reading in whether or not we're using a support vector machine or not. Again, that's the one caveat that you need to be aware of, that if you are using a support vector machine, you have to specify that. You have to specify whether or not you're using SVM one way or the other, because then the code will update the Param section for the sport vector machine model if you are using it. If not, then it just skips that section, reads in the model from the host, reads in the data from the host, and then calls predict on the training data on the input data for the model that you just imported. And that is in the line that has yhat equals trained model predict data unwrap. That is where we generate our inference. We can commit that output to the journal through end, commit the bottom line here.
00:38:31.470 - 00:39:25.104, Speaker A: Alternatively, you can also print it out. You can log it out to inspect it. If you are using a support vector machine, this is what the code would look like. You are reading in the model using the correct type generic parameters, in this instance a support vector classifier. And then you see below that we are defining the kernel parameters and reinserting those parameters back into the model before we have yhat equals model predict. If you're not using support vector machine, the only thing you need to worry about is the first part of the guest code. Okay, that covers the smart core part.
00:39:25.104 - 00:39:34.240, Speaker A: I can answer any outstanding questions that may have arisen and I didn't get to see in the chat. Before moving on to Xgboost.
00:39:36.440 - 00:39:41.860, Speaker B: There'S a question of how long, roughly does the trained model run in the guest?
00:39:42.600 - 00:40:30.420, Speaker A: This will depend if you are using a random forest with a whole bunch of trees. It can take up to 50 or 100 segments, which is a lot. Those are very large models. Each segment is approximately about a million cycles of compute, so you can do the math. However, most models support vector machines. Clustering algorithms all fall within one to two segments, so if you are proving locally, it should take no more than a minute or two minutes. Again, random forests tend to be very large models, so it shouldn't be any surprise that it comes with a lot more overhead.
00:40:30.420 - 00:42:13.040, Speaker A: Okay, possibly the most popular model out in the ecosystem right now, both in web two and web three that is not deep learning, is Xgboost, and so I'm quite happy to be able to offer XgBoost as an option for ZKML projects that want to make use of it. In order to use Xgboost with RiSC zero, we use a crate called ForestML, and depending on your preference, you can install it with PiP or conda. Forest ML is a pure rust implementation of the XgBoost algorithm, so there are no compatibility issues whatsoever in running it inside of the guest. Another thing that we did with this project is partner with Spice AI, which is a web three database data solution provider. And what we can do with Spice AI is query data from the blockchain in order to train our model previously with Smartcore, the presupposition was that you had already obtained data, you had done feature engineering data preparation on your own, and then formatted the input data and the classes or the outputs as X CSV and Y CSV. Here we are assuming that you are going to be collecting data from the blockchain and training your model. Your model.
00:42:13.040 - 00:42:16.000, Speaker A: What was that?
00:42:17.730 - 00:42:23.506, Speaker B: I think somebody just entered without muting, but I think it's now all good.
00:42:23.608 - 00:43:29.240, Speaker A: And the nice thing about this is that all the model training can be done in Python. Okay, again, spin up a new project. Your screen should look like something on the left hand side and add a Jupyter notebook, this time with a Python kernel in addition to the generated files from the RiSC zero template. And I also add a res folder that includes a home for where you can export a trained model. Be sure to add the correct dependencies to both the guest and the host. Unlike smart core model deserialization for forest ML, XgBoost is very straightforward, no generic parameters needed. All you have to do is specify that it is a gradient booster type and that takes care of all of your deserialization needs.
00:43:29.240 - 00:44:24.182, Speaker A: So I'll quickly go through what a bare bones Python notebook should look like. You import all the relevant dependencies. I already had instructions for how to install the Spice AI API. If you want to use it, you can sign up for a free trial on spiceai.com if this is something you want to experiment with, but all you have to do is write a standard SQL query that specifies what data you want from Ethereum from Polygon, and I believe they also have enabled a few other blockchains as well. Specify your API key and then call on the data. The data is automatically formatted into a pandas data frame, so it makes it super easy to work with.
00:44:24.182 - 00:45:18.794, Speaker A: You can use all of your standard Python data science developer toolkits that you may be familiar with if you're experienced in Python data science world. So here I'm just doing a very bare bones basic demonstration. I am querying the number of transactions in a block and the base gas fee per transaction in that block to build a very basic bare bones gas fee predictor model. So those are the two values that I've queried. I've structured my data frames accordingly, and now I'm ready to train the XgBoost model. The only caveat the max size is going to be the max number of leaves. This is set to two to the 32 minus one.
00:45:18.794 - 00:46:19.610, Speaker A: This is a little quirk in the ZKVM. It's a 32 bit processor, and for a reason that is still unclear to me. When you set this max size to two, to the 64, which is what the default is for a standard machine, it does generate an error, but I don't think many people are going to be generating XgBoost models with hundreds of billions of leads. If you are, maybe we can work on a way to address that issue. But for now, please keep the max size to that specified value, and then proceed with model training by simply calling fit on x and y x. Here is the gas fee number of transactions, and y is going to be the gas fee per transaction in a block. And then we export.
00:46:20.910 - 00:46:23.278, Speaker B: We do have a question from Fahim, it looks like.
00:46:23.364 - 00:46:33.650, Speaker C: Yeah, I have a question, actually. So I'm unfamiliar with how training a model works, but what is the purpose of max size in this case? Like, why do we need to pass in this variable?
00:46:34.390 - 00:47:03.226, Speaker A: It's a variable that's taken in by the Xgboost model itself. The default is set to two, to the 64, and that just causes an error when you are transmitting the model from the host to the guest, because it's sending over an integer that exceeds the word size for a RISC 532 bit processor. So this is just a little to.
00:47:03.248 - 00:47:07.020, Speaker C: Clarify the question itself. What does max size itself do?
00:47:07.950 - 00:47:13.280, Speaker A: It specifies the max number of leads in a XgBoost model.
00:47:13.890 - 00:47:15.246, Speaker C: Okay, thank you.
00:47:15.428 - 00:47:28.820, Speaker A: Yeah, and the only reason I am setting a manual, I'm manually setting a value here, is to avoid an error that is generated with the default value, which is two to the 64.
00:47:30.950 - 00:47:32.580, Speaker C: That makes sense, thank you.
00:47:33.990 - 00:47:41.960, Speaker B: And then there's one more question in the chat about it says, what are the approaches and strategies to convert the trained model to rust code?
00:47:42.890 - 00:49:04.070, Speaker A: So the nice thing is that because forest ML is a pure rust implementation of XgBoost, you do not need to worry about training the model in rust. The Python bindings are merely an interface for the user who the assumption is familiar with. The Python ecosystem offers an easy way to interact with a rust based model, but with Python bindings. So you do not need to alter anything regarding the model in rust itself. That is all handled by ForrestML behind the curtain behind the bindings. So unlike Scikitlearn, which is a pure Python implementation of all your standard machine learning models, Forest ML is a pure rust implementation of Xgboost with Python bindings. Okay, so after we've trained the model by calling fit, we export it using the built in saver booster function, and that will export the model as a JSON file.
00:49:04.070 - 00:50:55.934, Speaker A: I'm actually submitting APR to enable serialization in other formats as well outside of JSON. But for right now, JSON is what we've got, and this is what the host side will look like when you are importing the model into risk zero, going down to the line just under predict which is the second function, line 16. This is how we deserialize the trained model in JSON format, deserializing it into native rust, and we have one more step where we actually have to convert that model into bytes, and then we're transmitting the bytes over to the guest along with the input data, and then we're reading in the model bytes, we're reading in the input data. We are doing one more deserialization step to reconstruct our trained model, and then we can generate an instance of inference from our trained XgBoost model on line 34 where we call on predict. The one other caveat is that we need to use a matrix type in order to as an input into the trained model, and that's handled in line 31. The matrix type does not have, for those curious, does not have serialization enabled on it. So we actually have to transmit the raw vector of inputs from the host to the guest and then reconstruct this matrix type inside of the guest.
00:50:55.934 - 00:51:59.790, Speaker A: That does not take up too much compute time. So if you're worried about overhead from constructing this matrix type, it takes maybe a few thousand cycles, and then we can commit our output back to the journal. And that is how you run Xgboost in RiSC zero. So both of smart core and Xgboost examples are in the RISC zero examples repo. I encourage you to check that out, play around with it, and if you have questions, I'm more than happy to work with you to get a project running and to expand capabilities. So in the near future, just to wet your appetites a bit, we are working on a template that will enable verification of Ezekiel proofs inside of RISC zero. And what this would enable is the ability to add any kind of ancillary or additional logic on top of a machine learning proof.
00:51:59.790 - 00:53:27.210, Speaker A: Real simple, straightforward example would be to do some image processing on an image classification algorithm or a generative model that'll spit out pixels and you want to do some either formatting on those pixels and JPEG encoding into an actual image. So that's what we are working on. In the future, and hopefully in the next version of the risk zero circuit, we will be able to enable developers to design their own custom accelerators, which could potentially include machine learning operations such as matrix multiplication, Softmax, and all the other functions that are necessary in order to build deep learning models. So I'm happy to answer any questions, address concerns, debate the merits of ZKML in general, whatever you guys have in your mind. So, quick question. You had mentioned that you were removing the JSON serialization within the model. Were you talking about within smart core, or were you referring to something else? Were you referring to the kind of like JSON serialization, nd serialization outside of just before the host or within the actual model.
00:53:27.210 - 00:54:43.920, Speaker A: So for Xgboost, what I want to enable is the ability not to replace JSOn. But if you want to toggle between JSON and other formats like bin code or message pack, I want to have that ability. It's just a better way of transmitting data in general. I don't know why, but JSON appears to be a default way of serializing data. But I think having bin code and other formats enabled, especially when it comes to rust, will be very helpful. There's one issue when you are serializing Nan, not a value, into JSON, and then trying to deserialize it using will Nan becomes null, and then you cannot deserialize null back into Nan. So there's one kind of quirky caveat with Serde JSOn that I'd very much like to avoid by offering other serialization formats for Xgboost.
00:54:43.920 - 00:54:56.610, Speaker A: Gotcha. Okay, yeah, I was removing some JSON serialization from the example, and I wasn't sure if I had done redundant work or not. Okay.
00:55:01.630 - 00:55:03.420, Speaker E: I have a question about.
00:55:05.470 - 00:55:05.738, Speaker A: I.
00:55:05.744 - 00:55:36.130, Speaker E: Think you mentioned something about forest or tree like data structures and how that could be a little bit more expensive. Inside the ZKVM, we have this notion of computing things on the host side and having the guest check the answer. This is called advice or hints. Do you think we could use something like that to speed up parts of these machine learning algorithms?
00:55:37.190 - 00:56:18.738, Speaker A: I think certainly if it comes to deserializing a large model, because if you have a random forest with a few hundred trees, just the deserialization component is going to take up a couple of dozen segments on its own. So I think certainly for that, I think there's definitely room for cutting down some of the segment counts. But Eric, I'm not too familiar with that construction, so it certainly would be something worthwhile exploring. I think.
00:56:18.824 - 00:57:15.620, Speaker E: Yeah, for example, maybe, I'm thinking you have hundreds of these trees, and maybe you want to get a certain number back. You can just ask the host, hey, can you search for this on my behalf? Where some value is like, maybe the root value is less than, I don't know, five or something? Because a lot of times traversing these trees can be expensive or you have some other expensive operation, but the answer to those operations could be checked more easily. I wonder if there are any opportunities within these machine learning models to add a little bit of acceleration. But like you mentioned, deserialization is probably like the bulk of the work.
00:57:16.470 - 00:57:40.054, Speaker D: But that should not be bottleneck, right? Because you can run a program firstly, load the incline model. And then you stop right there. You already have 100 segments, and everything happened after that segment. Already have the proof for the first 100, right. Basically that you start with the image of the memory that already have the model there. Right. And then you can create with the computation.
00:57:40.054 - 00:57:45.214, Speaker D: So basically you can find a way to ignore all the live loading costs here.
00:57:45.412 - 00:57:51.200, Speaker A: Yeah. And this is where I think more the pause resume feature would come into play.
00:57:55.180 - 00:58:28.580, Speaker D: And then one other question is this, I think average mentioned one thing is that it's a little bit like this. We can call that holographic. Basically that the model is super big, but to do the inference, you actually only need to access part of it. It's a little bit like merco tray. Eventually, when you do an inference, you only access one pass of the tree. I don't know how the HG boosts work, but if that is the case, then I think in the current implementation, when the host provide any data, you don't track that data.
00:58:28.650 - 00:58:29.270, Speaker A: Right.
00:58:30.280 - 00:58:48.650, Speaker D: That's when the host free to provide any data to the guest. So that clamp already provide a channel to do this. The only problem is that whether or not the host can provide data in the middle. But I guess that should not be a big problem as well.
00:58:49.580 - 00:59:01.420, Speaker A: Yeah, that'd be an interesting avenue to explore. Would that have to be done with maybe a syscall back to the host?
00:59:02.000 - 00:59:34.356, Speaker D: You have a syscall read, right? You can already use that. I think the problem probably come back to Eric, is this, is that, is it possible actually a little bit like this. When you pause the program, then the host provide more input to the gas. Can the host provide input gradually to the gas? If that can be done with some API, then it would be soft because you can just use the ref function to provide any data to the guests.
00:59:34.388 - 00:59:35.544, Speaker A: Right? Yeah.
00:59:35.582 - 00:59:39.610, Speaker E: So we do something similar. Victor's done something similar in the.
00:59:42.700 - 00:59:43.016, Speaker A: You.
00:59:43.038 - 01:00:28.840, Speaker E: Know, replacing this part of the machine learning code with something like then. So what you can do essentially is pass the merkel root of the data to the guest and say, okay, well, here's the correct answer, but I'm going to give you all these other leaf values and then the guest can check as it's coming in. So, yeah, I think that's definitely one way. Yeah, I'm not exactly sure how much, I'm not knowledgeable about the inference algorithms or how it traverses the trees, but this I think could be helpful.
01:00:34.460 - 01:00:50.380, Speaker A: Yeah, we may need to actually potentially make a custom fork that would allow us to inspect some private data or data in some private struct. So we'd have to find a way to expose some of that data which might be hidden.
01:00:54.000 - 01:01:16.310, Speaker D: It's actually a really small problem. Right. In Russ, we already prepared that everyone will have a folded version and that's what happened in the real life. Everyone have their own version of the artwork because they need to change something into public. I think that's totally okay. And you can see that you just create more live.
01:01:19.000 - 01:01:58.510, Speaker A: For. That's exactly what we did for Smartcore anyway, so yeah, it's not a big deal. Okay. Yeah, Paul, I think we're at the hour mark. If there are any questions I'd be happy to feel them. Also offline as well, or online, but offline from the zoom call and I can go back to the front where I have my information. Feel free to reach out to me either email, preferably via email over Telegram, but telegram works as well.
