00:00:00.490 - 00:00:11.738, Speaker A: Thanks for joining us for this session of risk zero study club. We have Wolfgang wells giving a presentation on hash functions for zk. I'll hand things off to wolfgang.
00:00:11.914 - 00:00:39.630, Speaker B: Yeah, perfect. Thank you. So yeah, the hash function is the topic for today. And maybe the term hash function is a bit ambiguous, or there are different things called hash functions. So you'll be talking about cryptographic hash functions used in cryptographic primitives. There's also like, if you remember data structures and hash maps and stuff like this. They also use hash function that has a slightly different set of requirements.
00:00:39.630 - 00:01:27.058, Speaker B: We're not going to talk about those. And also there's password hashing if you want to store passwords securely on a server or something like this. Also again, slightly different requirements, but often called hashing. So we'll be talking about cryptographic hash functions and especially in the context of zk of zero knowledge proofs. So let's first start with a practical example. Probably the first time I stumbled across hash functions, that was before I knew what hash, or even a function was, was when you download big files like Linux ISO for example. And then usually on the website, they also provide you with the hash of the ISO file.
00:01:27.058 - 00:02:33.310, Speaker B: And the goal here is you want to make sure that the huge file you downloaded is actually the same like on the server. And of course you could download it multiple times, right? But that would be not very practical. So instead you get this rather short hexadecimal string and then you can compare in the end when it's on your machine to see whether the hash matches what it's supposed to be. And you can also use this to verify that the ISO downloaded from a mirror or whatever is exactly correct. Or maybe you have a file lying around on your hard disk and you want to check is this the most current one? Then you can also use the hash value of it. Recently there may be better cryptographic methods like signature, but if you look at the manjaro for example, download page, you're still able to get the checksum and you'll see something like this as the hash functions. And of course, depending on your platform and console or whatever, there are different commands you can use to get actually the hash of a string or a file.
00:02:33.310 - 00:03:27.386, Speaker B: So if we take a look at the hash function at first in this context, so what are the properties that are required? So the first one is what is a hash function? In general, a hash function takes an arbitrary sized input. So it can be a small string, it can be a huge ISO, it can be whatever you want, can be an empty string and basically maps it to a fixed size binary string. So here of n bits. And usually this thing you hash is called input, and the output is called the digest. And then you can perform this, for example, as I mentioned, on a string, and you get something back, or you can do it on an ISO and you get it back. And of course, it is important that the digest is shorter than the input. If the digest would be the same, I don't know, 600 megabytes or whatever that your input is, then it would be more or less useless.
00:03:27.386 - 00:04:14.446, Speaker B: Then you could just redownload the file or whatever. So it's actually crucial that it's shorter or fixed size and not as long as the input. So another property that we want in this context is deterministic. So of course, when we evoke the hash function at a different time, at a different location in the world, we need to get the same output, we need to get the same digest back, right? So this means it needs to be public. There cannot be any private keys or whatever involved, and it cannot be random. It needs to be deterministic. And the third property required in this context is what is often called like avalanche effect.
00:04:14.446 - 00:05:15.102, Speaker B: Essentially, even if there's a tiny change in your input for maybe just one single bit flip or you want to check that you didn't, I don't know, cancel the download a few bytes too early or whatever, the hash should look completely different. So about half its bits flipped, ideally, and that's why it's called an avalanche. Like, a small change should lead to a huge change in the output. And you can see that, for example here, if I just change the c to a d or the d to a c, you get a completely different hash here. And another consequence that actually comes with the previous three properties are collisions. So in general, it is impossible to prevent collisions. And this is due to the math is called pigeonhole principle, which is a terrible image, but nevertheless.
00:05:15.102 - 00:06:02.914, Speaker B: So if you have more pigeons than you have pigeonholes, then there needs to be at least one hole you stuff two pigeons in. So that is the general principle. And for us means basically the number of inputs, the number of ISO is huge, while the digest string is comparatively small. So there must be collisions, there is no way around it. And a collision means basically the same input leads to the different input, leads to the same digest. And we had this in the Avalanche effect. So we kind of want to make sure, of course, there are collisions, but at least it shouldn't be too easy to create them.
00:06:02.914 - 00:06:53.098, Speaker B: So, for example, a bit flip should not lead to a collision. Ideally it should lead to completely different digest. So what are basically the probabilities to get a collision? And there's another nice mathematical problem for this. It's the famous like, birthday paradox on. So the question is, if there are, I don't know, 20 students in a class, what is the probability that two of them have the same birthday? And the chance is, I don't know, around 40% or a bit over 40%, something like this. And if you generalize this very, very roughly. So we are not talking about 365 birthdays, but due to the n possible outputs, right, you roughly need square root.
00:06:53.098 - 00:07:37.774, Speaker B: So what you usually say, if you use the number of bits as a security. So if you have an n bit s function, then you get a collision security of n over two bits. Exactly. Because the square root thing leads to a significant chance that you get a collision. Okay, so we now have like, there must be collisions, but they shouldn't be constructible. So for the example so far, that might not be so relevant, right? We don't have like malicious actors on our download website. And if a hacker takes over the website, it's of course trivial to change the download link as well as the hash function.
00:07:37.774 - 00:08:42.706, Speaker B: So in this context, the requirements or the properties we stated so far are sufficient. But if we really want to go into the cryptographic realm, we really need to make sure they're as strong as mathematically possible. And so if you ask a theoretical mathematician, so based on these properties, what is the ideal hash functions? Then you would come up with this random oracle, which is a theoretical model. There's also like a stricter definition, but I actually like this pseudocode algorithms here a lot because it gives you really the impression what is happening. So this random oracle, it is essentially the hash function. So it gets the input, the arbitrary size string, right? And then it uses this global huge book in the sky, right? This is like a phone book or whatever, a key value store everyone in the world can look at. You can append stuff, but you cannot delete stuff.
00:08:42.706 - 00:09:32.194, Speaker B: So this is this ideal book in the sky that everyone can look at. And so if we want to compute the hash or the digest of x, then we do a lookup in this book and check, is there a value associated to x? And if there is no value, okay, then this is the first time we actually compute it. So we basically create a random digest. So eventually we flip the coin n times, right, for every bit to produce the digest, the output. And then we store this new value in the book for everyone else to see, and we output the digest. And now the other case. Basically, if x is in the book, then we just return the value that was in the book.
00:09:32.194 - 00:10:14.466, Speaker B: And this gives us all the properties we need, this global book or whatever, in order to achieve determinism. But apart from that, it will be as random as possible. So that is the ideal, the mathematical ideal for a hash function. But unfortunately, it is not tainable. It is infeasible to get. So as a next step, we want to formulate more precise definitions of what the cryptographic hash function needs to have, and those properties are actually obtainable. So this is the list of the properties we need.
00:10:14.466 - 00:11:09.314, Speaker B: The first one is one wayness, or sometimes also called pre image resistant. And essentially, given a y, given a digest y, it should be computationally infeasible. To find an X prime that gets hashed to y. It doesn't need to be the original value, it just needs to be any value that has the same digest. And if we look at our random oracle from before, then what would be the complexity be for that? So yeah, we basically need to loop through the entire book and see where is this x y. We cannot do a lookup by value, we can only do by keys. Right? So we literally need to iterate brute force over all the entries in the book to find it.
00:11:09.314 - 00:11:48.766, Speaker B: And this is like worst case, two to the n. So the second property is collisions resistance. That is the one we already talked about. So there must be collisions, but it should be infeasible to find them. And as mentioned before, a collision is like different input leading to the same output, to the same digest. And in the random oracle model, this is n over two, which is again, mathematically the best you can do due to the birthday paradox on. And yeah, it's pretty easy to do.
00:11:48.766 - 00:12:32.202, Speaker B: You need to loop through the entire book and see are there any collisions? And if you found them, okay, and if not, you also need to generate like random input and so forth until you find them. So I think it's pretty clear that this, again has the best mathematical complexity here. And the third one is target collision resistance. This is also sometimes called second pre image resistance. Yeah, but I guess I prefer the name target collision resistant because I think it's a bit more clear what it does. And here it is basically given an x. So you have an input given.
00:12:32.202 - 00:13:32.894, Speaker B: Now it should be infeasible to find an X prime that has the same digest. So it is a collision, but you're not allowed to find any collision, but it must be a collision with the input that is somehow given or provided. And again, you can see in the random oracle model, this is two to the n. So those three are, I'd say, the key properties that are usually stated, but there are two more that are basically, you cannot come up with a reasonable cryptograph or general purpose cryptographic hash function without four or five, but they're usually not so prominent. And so the fourth one is pseudo randomness. So essentially the digest should behave indistinguishable from a random function if you put in new input. There are several applications in cryptography, right, where you basically use the hash as a random number generator or pseudo random number generator, stuff like this.
00:13:32.894 - 00:14:33.810, Speaker B: So this is an important property and you can do statistic test or whatever to at least have some level of certainty that your function actually achieves this property. And the fifth one is actually very fuzzy. It is important, but very hard to formulate its non malleability. And that means basically, if you have the digest but you don't know the input, then it should still be infeasible to basically derive a digest from a slightly modification to the input. So let's say, for example, you don't know the input x, but you know that if you would add one to the input, you'd get out that digest. Or if you append the zero bit to the input, you don't know the input. But if the guy knowing the input would attend zero, he would get that output.
00:14:33.810 - 00:15:36.040, Speaker B: But of course, there are many relations. It's not only adding one or appending zero, they're basically all possible relations or whatever that inputs could have. And it should be obviously secure against all of them. And that is obviously also a crucial requirement for cryptographic hash function. Let's for example, consider like in a betting system or whatever, you basically have committed to the amount or auction system, right? You have basically committed to your bid. And so you compute the hash and basically public, made the hash public, then if someone could basically say, okay, I can now compute the digest of your bid plus one, that would be very bad, because then you could always be outbidden even though they don't know what your actually number was. So just as an example, as I said, it's a bit fuzzy, but this is kind of also an important requirement that you need.
00:15:36.040 - 00:16:25.538, Speaker B: Okay, and now those are the requirements. And now let's get into how hash functions are actually designed in practice to achieve all those properties. And there's really not like every hash function does things differently. And it's really more like an art than a craft to build a hash function. But there are certain frameworks that can help you. And one very important one is the so called sponge construction. And why it's called sponge because very visually, it has two phases, in one at which absorbs, and it cannot only absorb like in one swoop, but it can have multiple steps and absorb more and more water like a sponge.
00:16:25.538 - 00:16:57.490, Speaker B: And then it changes to a different mode. And there you can squeeze out the data again in multiple steps. If you squeeze harder, the next time, more water comes out. So that is the analogy that you have here. And essentially it helps you to deal with this pesky requirement that the input is arbitrary sized. It gets basically rid of this by the assumption, let's say you have a black box. Let's say it's also like a hash function.
00:16:57.490 - 00:17:25.686, Speaker B: But this hash function operates on a fixed input and produces the same size output. You have that as a requirement. And then you can use the sponge construct to get rid of the arbitrary sized input. And how it does this is the following. This is the entire state here. This is basically the entire input that your black box would operate in, right? And it consists of two parts. One part is called the capacity.
00:17:25.686 - 00:18:09.382, Speaker B: This is essentially your private state and rate, which will basically gets filled by the input data. And for that you chunk the input data into multiple chunks or blocks of the size, matching the rate. You put it into your state. Then you apply your fixed size hash function. It perturbs stuff in there, right? Comes out with the state, same size again, but now of course, different data in it. And then you use again the second block. You put it in here, you do it again, you do it again, and so forth until the entire message is consumed.
00:18:09.382 - 00:18:49.366, Speaker B: And if in the end you end up with maybe half a block, because the data is not 100% dividable by your rate, then you just pad stuff. You need to be a bit careful how you pad it. But it's relatively straightforward. You can pad, I don't know, zero, and then at least then the number of zeros you need, or stuff like this. There are different techniques to get the requirements you want, but yeah, it's relatively straightforward. And then you have absorbed your entire input method. Now your state is in a different state.
00:18:49.366 - 00:19:36.598, Speaker B: And then you switch to the output, to the output mode, the squeeze. And you basically just copy what you have like in your public state in the rate you copy. And this becomes your dicer, essentially. And if this is still too short, for example, then you do this again. It gets perturbed again and you copy out the new part of the digest. And here you can see that it's crucial that the capacity is actually not zero or sufficiently large, because otherwise it provides in some sense entropy to the rate. Otherwise, if the capacity would be zero, it would be trivial to just, I don't know, compute the second block of the digest right from the first one.
00:19:36.598 - 00:19:55.100, Speaker B: So this private state is actually crucial to provide it. And if the private capacity is long enough, then you get the properties you want and you can even squeeze out more blocks as you want.
00:19:58.110 - 00:19:59.034, Speaker A: A couple of questions.
00:19:59.152 - 00:20:00.234, Speaker B: Yeah, go ahead.
00:20:00.352 - 00:20:07.354, Speaker A: So, is it correct to say that as you grow the input, you need more rounds of absorbing?
00:20:07.402 - 00:20:10.142, Speaker B: It takes more rounds, yes, exactly. Yes.
00:20:10.276 - 00:20:20.050, Speaker A: And then the amount that I can squeeze out, I guess, can you comment a little bit on how many rounds I can squeeze out and whether that depends on how much I absorb?
00:20:21.190 - 00:21:05.234, Speaker B: Essentially it does not depend on how much you have absorbed, but how much your capacity is. But it is essential that it's called xoF, so, extendable output function. So essentially for certain use cases, you can squeeze out very, very long digest here. Then it's technically no longer a hash function, but this extendable output functions. And actually I have some numbers on the next slide. So what the requirements exactly are to be able to get secure output. But the gist of it is, as long as your private state is big enough, you can squeeze out as much data as you want.
00:21:05.234 - 00:22:01.710, Speaker B: Of course you won't get better collision resistance or whatever, but you can still use this data as pseudorandum data or whatever, depending on the use case you want. So this is basically, that's another reason actually why you use this sponge construct, because you get these nice mathematical guarantees. The assumption kind of is that the state permutation, so basically the f I had on my previous slide is good enough. So permutation in this sense just means bijective from the state size to the state size. And good enough, again means to be really able to prove those properties, you need, again something similar to a random oracle. So you need the random permutation. But if your function f is good enough, you get exactly those properties.
00:22:01.710 - 00:22:56.802, Speaker B: So let's say c, that is the size of your capacity. So your private input you have. And essentially you can then see the one way resistance depends on n. Obviously, if you don't squeeze out enough data, if you only squeeze out one bit, then it's obviously trivial to find a pre image for that. But if you squeeze out enough, then you see your capacity becomes the limiting factor, right? And the same for the other ones. And obviously, collisions is the most critical one because it depends on the hash lengths, on the digest length. So, as an example here, the most prominent user of this bunch function is sha three, or catch up, used in ethereum.
00:22:56.802 - 00:23:49.810, Speaker B: Actually, they are exactly the same. Just when ethereum came up, it was not fully standardized yet, so they choose a different parameter, a constant set. But apart from that, it's 100% identical. And that uses 256 bits of the digest is 256 bits. So actually it should be an n here and not a d. But yeah, so 256 bits, and the capacity is 512 bits. And then you get exactly those collision, those security requirements, namely 256 bit one wayness 128 bit collision resistance, again, because of the square roots thingy and 256 bit collision target collisions resistance.
00:23:49.810 - 00:24:23.626, Speaker B: But there are also other versions where you can squeeze out more data. They are called then shake, actually. And then you'd use the shake function, and you'd squeeze out more data. You'd squeeze out the 1024 bits. Right? Then you could get your collision resistance also up to 512 bits. So, yeah, those are those nice properties that you have. But mostly you use the sponge construct to be able to absorb a lot of stuff.
00:24:23.626 - 00:25:04.618, Speaker B: The squeezing out part is usually not required. So for ketchak or shah, it is just one round. You do, you're not interested in this arbitrary length output. This is a very special use case, but it could also be used, and there are applications for that. Okay. Yeah, so this is the spanish construct, but we still now need to come up with a good state permutation that is as close to the random oracle that you can get. And again, here are no real frameworks or guidelines on what you can do, but there are, I don't know, a few best practices.
00:25:04.618 - 00:26:14.718, Speaker B: So the first one is cryptographic hashes need to be as efficient as possible, essentially. So you should also use efficient operations on the cpu that you have. So, for example, bit rotation, obviously, like bit rotation is kind of a bijective or whatever, and it is very fast to do on modern processors. And then you combine those, mix and match them in different ways to come up with a state permutation function that is sufficiently good. And another trick that you can do to basically drive up the mathematical complexity of your state permutation without driving up the computational or the complexity. To write or audit the permutation is you use rounds. So you basically, you compose those functions multiple times, right? Since they're all bijective, nothing goes wrong.
00:26:14.718 - 00:26:49.390, Speaker B: And then, essentially, if you want to code the hash function, you just have a for loop around it, essentially. But internally, the complexity of course becomes much bigger because it gets applied many, many times. And just to give you some estimate, some ballpark estimates. So maybe 24 is, I'd say like a typical number of rounds for at least sponge construct hash functions.
00:26:50.370 - 00:26:59.140, Speaker A: There's a popped up in the chat that I'll read out loud. It says, is it possible to infer the size of the input from how long the functions take to run?
00:27:00.710 - 00:27:39.326, Speaker B: Yes, essentially that is possible. Exactly here. So basically this will linearly depend on the number of absorb steps you have, and you can then use that to more or less derive how long the input was. But this is only if you know how it's executed, right. If you only have the digest, then you have no way of knowing that. But in theory, you could have some side channel attacks if someone executes the hash and then figure out how long the image was. That is correct.
00:27:39.326 - 00:28:20.030, Speaker B: But I don't think there's a very good way around this. And this is typically not the attack vectors that you're worried. When hash functions, they need to be secure with respect to the output and not necessarily secure with the execution. But of course, that is something to consider, that you can somehow get information about the length from the execution time of the hash function. Yes. Okay. Are there any other questions up to this point? Because after this, we'll leave basically classical hash functions and go to the Zk realm.
00:28:20.030 - 00:28:26.020, Speaker B: Apparently not.
00:28:26.470 - 00:28:30.226, Speaker A: Yeah, this has been great so far. My questions are definitely answered so far.
00:28:30.328 - 00:29:21.294, Speaker B: Yeah, perfect. So, yeah, as I said before, those are like classical cryptographic hash functions that are also quite some. So now we want to take a look at Zk friendly hash function. And what does this actually mean? So you want to execute the hash function basically inside of ZK proof, right? So it is not so much relevant to use the hash function as a proving mechanic or whatever. The use case we're interested in is executing a hash function, computing a hash inside, for example, the risk zero Zkvm. Right? And why would you do that? There are actually many use cases. Probably the most prominent one is inclusion proofs.
00:29:21.294 - 00:30:30.250, Speaker B: Let's say you want to prove inside your ZkvM that the certain value was part of the state, right, without needing to load and verify the entire state. So instead you just get a Merkel inclusion proof, and then you just need to check that this one is correct. And for that you need to do quite a few hashes of hashes and so forth. So this is probably the most typical use case of why you'd want to do this inside the ZKVM, but there are tons of others, like cryptographic primitives, encryption, whatever. So there are quite a few. And so why is that actually different to the classical world? And this is how ZK proofs are basically constructed, is a bit different to how a regular computer does it. And of course, this is a huge topic, and we also have tons of other study clubs on this, if you're interested.
00:30:30.250 - 00:31:31.398, Speaker B: But just to give like a very brief and hand wavy argumentation is no matter what exactly your ZK proof is, one part of it is also like the arithmetization, or you need to construct an arithmetic circuit. And essentially an arithmetic circuit is very simple to the Boolean circuit that basically happens in your laptop. But instead of operating on boolean gates and bits, it does like algebraic operation on a certain finite field that is used. So it kind of looks like and or gates in a boolean circuit. Right, but that's why it's also called a circuit. But it's essentially here you just sum up a and b, and then you multiply it with c, for example. And why do you do that? Very briefly, it is a very good way to describe or deal with polynomials.
00:31:31.398 - 00:32:11.798, Speaker B: And polynomials are a crucial part in ZK proofs. And then, of course, how exactly this done is a bit different. That depends on how the arithmetization works, whether it's rake, one constraint system, or plonk or whatever. But the idea is always the same. You have like, algebraic operations on your finite field. And the key is, if you want to make it as efficient as possible, you want to come up with a hash function that uses the least amount of algebraic operations that you have. And the first construct that came out with this in mind was minc.
00:32:11.798 - 00:32:50.482, Speaker B: Actually, it stands for minimal multiplication complexity. It's from 2016. And, yeah, let's take a look how it's actually designed, or how we, with the knowledge we have so far, could design something similar. So, first point is, let's use the sponge construction, right? Otherwise we have to worry about fixed input sizes and whatever. Let's use that. And then we are limited to only this permutation. So we only need to build a permutation from b size input to b size, b bits sized output.
00:32:50.482 - 00:33:26.170, Speaker B: And that's pretty nice. Let's lose our finite field for that. And let's kind of assume that the p we have is sufficiently large. Either we use two to the b, or a prime number that is close to it for now. And if we have that, we actually just need to come up with a bijective function on this field. And this is then our permutation that we'll use. And the rest will be basically automatically from the sponge construction.
00:33:26.170 - 00:34:16.338, Speaker B: So what can we do? So the first thing we can do is basically we can add a constant, right, to our input. Obviously this is the permutation that is bijective, but it's also very simple to invert, right? So this would be not a good, it would be a terrible hash function because the one wayness would be trivial to break. You just need to invert it and then you can come back to anything or something that has the same input. So we need to make it actually a bit more complicated. And the next thing is, okay, additions maybe is not sufficient. So let's use multiplications. So we're looking for the smallest monomial that is basically a permutation for this field.
00:34:16.338 - 00:34:51.670, Speaker B: And it turns out essentially you're looking for a small prime. So three, five, seven, so, and essentially, as long as p minus one is not divisible by this d that you use there, then you're good. Then it's a permutation, right? So, yeah, perfect. Let's use this. Let's say we are lucky for our use case three works out and we can use three. Otherwise we had to go up to five or seven or whatever. But obviously this adds more multiplication.
00:34:51.670 - 00:35:14.340, Speaker B: So keeping this d as low as possible is generally desirable, as long as you can. And then you combine this, essentially. So you first add a constant and then you do the power of three. And that of course, is still too simple. But we use the other trick from before. We just apply many, many rounds. So we compose this function many times.
00:35:14.340 - 00:36:19.746, Speaker B: So always add a constant and do to power of three and stuff. And those constants obviously should be different, and ideally they should be random, right? So of course I could come up with a perfect, secure random number generator that generates them, but then I had no way of proving that they are actually random. So I have to prove that I have nothing up my sleeve, essentially. So what is typically done here is you actually don't choose them randomly, but you derive them from a pseudorandum number generator. So for example, you compute the shah hash, which is kind of assumed to be pseudo random, of this fixed string. And then you just append one, two, three, whatever. To get those numbers, for example, the key is just you need to prove that you have nothing up your sleeve, that they're generated, and not that you haven't come up with them in a way that could be malicious, right? And then you combine this quite a few times.
00:36:19.746 - 00:37:33.274, Speaker B: And this is essentially the drawback, obviously, because our f here is so simple, it just does this add and then the multiplication we need to do actually quite a bit of rounds. But it turns out if the number of rounds is larger than the log three of your p, then you're good, or at least then the classical attacks that can happen for lower rounds cannot occur. So this is quite a bit, this is like in the magnitude of hundreds, depending on how many bits you have. So compared to classical hash functions, this is quite a bit of rounds, but still, because the function itself is so simple, it still pays off. And if you actually then implement it, it'll have much less complexity for a zero knowledge proof than the classical stuff, because especially there we said, you use binary operations, you use soar, you use bitshift. Those are really ugly to describe in an arithmetic circuit. So this one obviously is much easier.
00:37:33.274 - 00:38:14.982, Speaker B: And even the high number of rounds still doesn't kill this. This is the general idea, and I think that is pretty clear. It has quite a bit of assumptions here. Our example, we basically assume that our field is large enough, so it mostly applies for, I don't know, like stark proofs that are living on a big enough field and stuff like this. But you can use exactly that idea now and continue with it. And so I have this as a quick outlook, for example. So there is GMIMC, which is a more generalized version of MIMC.
00:38:14.982 - 00:39:24.866, Speaker B: If your field is not big enough and you want to basically compute it not on a single element, but on a vector, right. Then how could you do that in a secure way? And then you can do a construction which is called like Feistel construction. So Shinsley, you see here, you have the same f, that is again, this, add a constant and power to the three or whatever, you use the input, basically the first element of your vector, you add it to all the remaining ones, and then afterwards you basically shift everything by one. And then you apply it again and again and again. And then you get similar security guarantees, but you're able to essentially combine multiple elements instead of one, depending again, on which proving techniques with arithmetization you're looking for. And one step further, is actually the Poseidon hash function, which is also used in risc zero. And it is essentially the same idea, is really the same it uses here.
00:39:24.866 - 00:39:46.860, Speaker B: It's called s boxes in the Poseidon paper, but it's again this to the three. Or again, you need to adapt the monomial if your field is not compatible with that. So you do exactly the same thing. Your state ascend. You use sponge constructions that is again the same. That's such a nice construct. That's always good to use.
00:39:46.860 - 00:40:31.350, Speaker B: The state again is like now a vector of elements, right? You do this and one change that have you have a maximal distance mixing in the end. So basically you perturb the stuff a bit that has nice mathematical properties. This is basically the addition you have. And this allows you to reduce the number of rounds. And in general it makes it, even though each individual round is more complicated overall. Again, it kind of depends a bit on your proving system. It is a bit simpler, but in the core it uses the same ideas which we've seen and derived in this presentation.
00:40:31.350 - 00:41:04.900, Speaker B: And there are new hash functions coming out, improving upon this, like Poseidon two and stuff like this, with new proving systems coming out. You again always need to revisit those. Are they still efficient on my system? What can I maybe change so this will also evolve in the future? Yeah, so this is all the slides I got. I hope that helped understand a bit how you can design efficient zk friendly hash functions. Do we have any more questions?
00:41:09.290 - 00:41:31.660, Speaker A: Yeah, I guess. I wonder, can you say a little bit more about how the S box from Poseidon fits into the absorb and squeeze portion? I think you sort of already did, and I just kind of missed that piece.
00:41:33.070 - 00:42:03.654, Speaker B: So the S box, that's maybe a bit misleading or. The term is usually used a lot in cryptography. This s box is actually. You'd actually do the x to the power of three. That is exactly what is happening here. So what I have here is the state permutation that you use in the sponge contract. So this is a bit of misleading because the sponge function has rounds in some sense.
00:42:03.654 - 00:42:44.178, Speaker B: And then again, the permutation function, the black box used in the sponge contract also has rounds. So here we are only looking at essentially the f that I had in the. Let me go back. There it is. So the picture of the pose, the entire thing with all its iterations and whatever, is exactly this f here. It operates on a fixed input and produces a fixed sized output. And one, what's left out is essentially how you'd implement the sponge construct to get the absorb in and the squeeze out if you want.
00:42:44.178 - 00:43:09.290, Speaker B: And. Yeah, so this is essentially the state permutation in the sponge construct. And here, again, it needs to be complex. And here, that is basically the difference between posadon and mimc. This f gets a bit more complex.
00:43:12.850 - 00:43:28.630, Speaker A: Awesome. Thank you. There's a question in the chat, it says, what's the main difference between sha one, shaw two, et cetera?
00:43:29.770 - 00:44:24.540, Speaker B: Actually, they are completely different. Sha just means this is the one that was standardized by Nist, right? So they always do, every few years or whatever, they do competitions to come up with a new hash function or new encryption algorithms, and then they standardize this, and then those functions get a new name. As I said, sha three, for example, is actually called catch up. So the hashing algorithm is ketchup, and it just gets renamed, or it gets this additional name, sha, if the standardization, if it gets chosen as the winner. And so, actually, sha one, two, and three have not much in common at all. So sha three is the only one that is sponge construction. The other ones use different constructions there.
00:44:24.540 - 00:44:59.910, Speaker B: They're not by the same authors, but however, there are also, like, families of hash functions. For example, there is Blake. And there's Blake one, Blake two, Blake three. And they are kind of similar. And Blake two, for example, was a contender for the shah three competition, but didn't get the cut. Ketchup did. So, essentially, they have the same properties, or they aim for the same properties.
00:44:59.910 - 00:45:48.100, Speaker B: But, like, for example, sha one is considered as no longer secure, so you shouldn't use it, while shah two still is considered fully secure. So bitcoin, for example, uses shah two five six, which is essentially sha two. Right. But still, to be on the safe side, they chose a successor which has even nicer properties, which is then sha three or ketchup. But, of course, those additional security comes at a price. Right? It is, in general, more complex to compute a sha three hash than it is to complete a sha two hash. And so that's why a lot of applications still use shah two hashes, because they are still deemed to be secure, even though there's a successor waiting already.
00:45:48.100 - 00:45:50.400, Speaker B: And.
