00:00:01.530 - 00:01:06.190, Speaker A: Cool. Yeah. Hey everyone, I am Stephen. I work at Risero as part of the growth team where I mainly handle the partnerships, solution architecture and product research. And effectively the motivation for this talk is to go over some of the common use cases, projects and general architectures that I see day to day, where we think ZK is going in the future and exploring the prominent use cases of ZKVM talk technology in blockchain and also outside of crypto. So I think a quick overview of this talk will go over the state of ZK as it is today, then just dive headfirst into a bunch of blockchain use cases. What does it look like beyond crypto? And then looking forwards, what are key enablers for ZK's future and what are the areas of disruption? So currently for the state of ZK, it really focuses on three main areas we've seen interest in scaling.
00:01:06.190 - 00:01:42.170, Speaker A: This is the ZKE evms of the world. Things like ZKsync, Scroll, Sarquare, but also the non ZKE EVM rollups. These are the rollups that our team works very closely with, such as Eclipse, sovereign Labs and layer N. Basically non EVM chains that run ZK roll ups as well. Additionally, we think about privacy shielded transactions, shielded assets. I think Aztec is a real leader in this space. And then finally provenance, things about proving validity, things about proving KYC, proving assertions.
00:01:42.170 - 00:02:24.490, Speaker A: And I think ZK identity is probably the biggest use case in this category. What are some disappointments with CK today? There's still poor devx, namely writing. Most CK circuits requires a lot of math, requires a lot of cryptography, and even strong developers struggle with building ZK. The space has a lot of hype. I think one or two years ago people thought that ZK is going to be a part of everything we do, part of every single tech, every single software. And that's probably still true in the future, but that's maybe five, maybe ten years out. And there's a few reasons for that.
00:02:24.490 - 00:03:00.018, Speaker A: One is obviously development times. ZK is hard to build, it's hard to upgrade, it's hard to extend. So naturally, the ZK development process is a little longer than usual. And then next is performance. As it stands, ZK execution is between 1000 to 10,000 x slower than non ZK execution. And over here we can quickly go over. What does that mean, right? Right now, in most CKE evms, I think polygon is likely one of the most performant.
00:03:00.018 - 00:04:05.764, Speaker A: Proving 100 transactions in a block takes between two minutes and 1 hour. While if you do it not in ZK, it takes a fraction of a second. Our ZkVM runs at about 100 khz on an m one MacBook and with parallel proving we can hit about 3 mhz. But if you think about this, this is still at least 100 to 1000 magnitude slowdown over non ZK execution. It's truthfully actually closer to a 10,000 x reduction in performance. So given all that, what are the use cases that these constraints enable? And what are the most prominent ones we're starting to see today? The first one, and probably the most obvious to everyone, is roll ups such as ZKE EVM rollups, ZKsync scroll polygon, those are just a few major ones. And then, as I mentioned before, non ZKevM roll ups like sovereign Labs and the architecture of these roll ups are all pretty common.
00:04:05.764 - 00:04:51.618, Speaker A: Effectively they are L2 networks that generate blocks. Once consensus is reached and the blocks are finalized, you run the blocks through a ZK program. In this case, you test the validity of the block construction as well as the transactions themselves. And this is the architecture that all these ZK rollups choose. There's some nuance into when the proofs are generated and how big each proofs are, but at the end of the day they run the chain and then the ZK proof handles the roll up mechanism. You take the block, you validate it, and then you post the proof on chain for verification. A design we've also started to see these days is ZK fraud proofs and hybrid roll ups.
00:04:51.618 - 00:05:55.070, Speaker A: This kind of stems on the point where you think about how much do ZK proofs cost for a block? Even at the lowest cost, we're talking probably five to $10, which ends up being five cents to ten cents for the end user. And in use cases where users are really price sensitive, or where those prices for gas fees are just too high, there's a new design called ZK fraud proofs that a lot of chains are adopting effectively. In this case, it's a traditional optimistic roll up where blocks are finalized after a seven day withdrawal window and only during times of fraudulent transactions are proofs of blocks created. Those proofs are then submitted on chain and then the proof is compared against the onchain states to detect any sort of fraudulent transactions. In doing so, you can basically reduce the need to generate ZK proofs from once every block to once every day, once every week. And technically you don't need to generate any of them at all. So this reduces your gas costs significantly.
00:05:55.070 - 00:06:41.018, Speaker A: But obviously there are weaknesses to this approach. The main one being there is a long withdrawal period and fraud proofs require detection. So you once again fall into the same pitfalls as traditional optimistic roll ups. But there are advantages. The optimism canon fraud proof system took, I think somewhere over two years to build, and it's still not very functional. While ZK fraud proofs, especially with ZK vms, are extremely easy to build, and this is just a matter of porting their execution layer onto rust, onto risk five, and then just running that normally through normal execution. So a ZK validity proof for fraud proofs is very easy to build.
00:06:41.018 - 00:07:11.730, Speaker A: They're also very upgradable and extensible. Traditional fraud proofs have a lot of dependencies. They have like WaSM and MIPS emulators written into EVM. When normal proof, you can just upgrade and extend it just by writing new rust code. And this kind of also leans into the future proofness of these things. Not only are they upgradable, so if you want to add account abstraction, you just write that in rust. But also ZK fraud proofs are stepping stones to full ZK proofs.
00:07:11.730 - 00:07:54.610, Speaker A: And I think we see this in eclipse, and excitingly, we also see this in optimism. Going back here, they basically released the RFP looking for a ZK fraud proof where you would ZK proof canon. We're doing something very similar where we have been able to prove optimism blocks. Eventually, if you just run these proofs every single block, you turn optimism into a ZK roll up. And I think this is very much in their roadmap. And the end goal here obviously being getting rid of the withdrawal period and really enabling better cross chain interoperability. So I think the next idea I want to go over is ZK bridging.
00:07:55.190 - 00:07:57.026, Speaker B: Before we bridge into bridging, can I.
00:07:57.048 - 00:08:00.434, Speaker A: Just ask a clarifying question about the fraud proof stuff?
00:08:00.552 - 00:08:21.194, Speaker B: Yeah, I think what I'm hearing basically is that the ZK fraud proofs is basically just a way to achieve an optimistic roll up. Is that right? It's basically like once you do it, it's exactly the same as what an optimistic roll up would do. It's just actually easier than doing it in the quote unquote usual optimistic roll up way.
00:08:21.312 - 00:08:31.950, Speaker A: Yeah, that's a good way to put it. It's easier to build a fraud proof mechanism, and you're actually a lot closer to being a full ZK roll up. So you can basically kill two birds with one zone.
00:08:32.530 - 00:08:33.086, Speaker B: Awesome.
00:08:33.188 - 00:09:21.550, Speaker A: Interesting. Cool. So for the next one, I kind of wanted to talk about ZK bridging. I think succinct is probably the largest leader in this space, as well as layer zero and polyhedra. As it stands, they basically all revolve around the same idea. To prove ZK bridging, what you want to do is prove that a user deposited their assets into some sort of escrow bridge smart contract on ethereum and the best way to prove that a user deposited their funds into a smart contract is proof of consensus. And the altar sync committee is a really good way to go about doing this, because effectively in the ZK context, you just need to aggregate and then verify a bunch of BLS signatures.
00:09:21.550 - 00:10:27.688, Speaker A: For alter sync committee you need to aggregate about 500 bLS signatures, which is relatively feasible, while for full block consensus you need, I think, somewhere over 20,000 bLS signatures. So as it stands, like Alter sync committee, you trade off some security, as alter sync committee is a reduced validator set in exchange for significantly faster and cheaper proofs. And I think as we look towards the future, full block consensus will be the optimal, most secure way, but in exchange you're dealing with significantly higher proof costs. So ZK bridging actually gets us really close to this idea that was proposed by Vitalik called the Verge. An idea of the verge is basically fully snarking and fully starking Ethereum. This means generating a Zk proof of all the blocks of Ethereum, and basically proving the entire chain in a single ZK proof. There's three main components here.
00:10:27.688 - 00:11:13.748, Speaker A: One is listed here, a snark for l, one evm. We've actually already accomplished this using zeth, which I'll get into a little later, but this is proving all the transaction execution in a ZK context. There's a snark for consensus state transition. This is what I mentioned previously, proving all the BLS signatures. And then there's a snark for vertical proofs. This is mainly because traditional mpts are a little intensive for ZK proofs, so moving to vertical trees will actually help reduce that and enables better scaling. And then the next step here is actually a quantum safe proof system, which risero already attains due to our stark basis.
00:11:13.748 - 00:12:24.620, Speaker A: So I think I wanted to highlight this because we're actually surprisingly close to the Verge, as if we take Zeth, which is our open source Zkevm framework. It was built by taking ref RevM and alloy porting that onto our ZKVM, as that's the rust Ethereum ecosystem, and just running reth normally with some slight changes. What this allows us to do is basically prove all of Ethereum blocks, their entire block construction and all the transactions within them. So that will prove the EVM part of the Verge, and then proof of consensus, as I mentioned before, just means we need to prove all the BLS signatures, which right now is a very expensive proposition. But hopefully in the future, with fpgas with better performance, we'll get there. And the end goal of the Verge is we can effectively prove every single Ethereum block by itself. And then using recursion, you can roll up all those proofs such that a light client down the road can sync fully with Ethereum by proving a single ZK proof.
00:12:24.620 - 00:13:19.170, Speaker A: So this really proliferates. Light clients allows any sort of hardware to sync with Ethereum. A stark proof is 200. Verification is extremely, extremely lightweight. So IoT devices, smart devices, really any sort of hardware in that case could theoretically sync with Ethereum in a trustless quantum safe manner. So I think the next architecture I wanted to go over is the ZK coprocessor architecture. What this really thinks about is how do you extend smart contract complexity? How do you move compute off chain? And the principle behind this is basically scaling for the layer one, or scaling for smart contracts without introducing the complexities of a roll up.
00:13:19.170 - 00:14:03.992, Speaker A: The design here is pretty simple. Effectively on the Ethereum side, on the blockchain side, you have a smart contract that acts as a relay. A user will send inputs to a smart contract. They will emit an event, and then the bonsai or the risk zero relay will monitor the chain for an event. They'll then send those inputs onto our ZKVM. The ZKVM will run any sort of computation given those inputs, and then it will submit the response and approve on chain through a smart contract callback function, basically testing to the validity of those results. So it's a little confusing.
00:14:03.992 - 00:15:04.288, Speaker A: So let's just dive straight into examples. One of the first ones we've seen, and also probably one of the most prominent use cases of ZK coprocessors is in the defi space and specifically a clob, which is a two sided order book. These are more liquidity efficient order books without Amms. They're actually pretty popular in the Salana ecosystem because salana you just have more gas and you can code more complex smart contracts. But in Ethereum, naturally, their smart contracts are rather restrictive and you don't want to pay $50 in gas fees just to submit a single order. So what we can do is we can take the order matching algorithm, which traditionally sits in the smart contract, and fully move that outside fully move that off chain. So in this case, a user will send inputs to a smart contract, say, I want to buy ten bitcoin at $500.
00:15:04.288 - 00:15:48.160, Speaker A: Those inputs will then be relayed to our ZK coprocessor. The ZK coprocessor will be running our order book and our order matching algorithm. So given those inputs, it will determine a set of outputs, it will find a counterparty for that trade, and it will submit that counterparty back on chain back onto Ethereum L1 for the exchange to occur. So the state transition, the exchange of tokens still remains on Ethereum L1. Simply the matching finding the person to do the trade with was done off chain. And there's a ZK proof that tests all this. So, you know, the order matching the order book were all run fairly and securely.
00:15:48.160 - 00:16:50.416, Speaker A: So in some sense, you're bringing off chain computation smart contract complexity to Ethereum layer one, like CK scaling to Ethereum layer one, without having to port to another chain. And then another real advantage of this is if you think about the gaming space, writing game logic in EVM is really tough. EVM is not a game programming language whatsoever. So we've actually talked to a lot of gaming companies in this space thinking about how do you port game logic into Russ? Because Rust is actually a really comfortable language for many game studios, and it's a very similar design. Whether it's a physics engine or like an in game item exchange, that logic can live off chain. It will live on our ZkVM in the ZK coprocessor, and the inputs to the on chain game remain the same. They go through the same smart contracts.
00:16:50.416 - 00:18:00.430, Speaker A: But whenever the smart contract needs to calculate the physics of the game, whenever the smart contract needs to calculate how the item exchange will work, it will just query the ZK coprocessor, and the coprocessor will respond with a valid response and a proof attesting to the validity of that response. And then another fun idea here is, I think everyone always says ZKML, but what are some practical applications of ZKML? Well, for the ZK coprocessor in Defi, I think there's actually a really prominent use case today. Effectively, if you think about Ave, there are a lot of risk parameters to adjust. Ave is a lending protocol, so it has factors such as reserve factors, health factors, liquidation thresholds, and supply caps. As it's done today, these risk parameters can only be adjusted through traditional governance. So this means that a Dow needs to vote on it. It's relatively slow, and they may not vote on the most optimal variable to adjust instead.
00:18:00.430 - 00:18:52.886, Speaker A: What a ZK coprocessor can do is it will take the inputs, the inputs being the on chain state. This can be the pool liquidity, volatility, token prices, historic token prices. It will take all those inputs, send it to our ZKVM, our ZK coprocessor, and you can run an ML inferencing model on these inputs. And in this case, we actually have an implementation of this already. This is called smart core, which is a ZKML inferencing library built on rust. So given those inputs, it will basically calculate what risk parameters need to be adjusted and send that response back on chain. And a proof that proves that this inferencing model took these inputs and gave us certain outputs.
00:18:52.886 - 00:19:57.698, Speaker A: So this is once again effectively bringing ML on chain in a verifiable, trustless way. And not only does this improve efficiency, because theoretically this inferencing model is more accurate than Dow voting, it's also a lot faster. During periods of high volatility, governance won't react quick enough to change the risk parameters of a protocol. Rather, a ZK coprocessor and ZKML can constantly be constantly adjusting these risk parameters given volatility in the market, so they react a lot quicker and generally create a safer protocol because of this, outside of Defi, there's just like traditional provenance use cases of ZKML in chat GBT today. There's no real way to prove that given my inputs. Given my question to Chat GPT, it gave us certain outputs. That sort of provenance connecting the inputs and outputs of inferencing model doesn't really exist.
00:19:57.698 - 00:20:51.470, Speaker A: So ZKML is kind of the only solution to that. Truthfully, for Chat GPT, we're still quite far away because it's extremely expensive, but as we get pgas and acceleration, I think we'll get closer there. And then also, for generative art, NFTs profile picks, things like that. You can prove that given certain inputs, my NFT picture were these outputs. So once again, proving the provenance connecting the dots in an ML inferencing model. Another pretty interesting use case we've seen is ZkYC and identity. Although this is kind of looked down upon in the crypto space, it is coming eventually in certain use cases, specifically due to government regulation.
00:20:51.470 - 00:21:46.480, Speaker A: And I think we started to see this in the Uniswap V four Hooks directory. There is this KYC hook which basically says users can only trade with certain liquidity pools given their ZK proof given their KYC. But there's a question of how do we get Kyc on chain, right? And I think ZK is a great way to do that. To accomplish this. Id me is one of the largest KYC providers in America, and they release their KYC in the form of a JWT token, which is OIDC. And effectively in our ZK proof we can verify that this JWT token, which is a KYC proof attesting to a person's identity. We can validate that inside of a ZK context, and then we can connect this KYC proof to a certain wallet address.
00:21:46.480 - 00:22:42.530, Speaker A: And this can all be pretty shielded. So when this final proof is posted on chain, all I reveal is this wallet address has been verified by KYC. But at least for the exchange for someone like Coinbase or Uniswap, they know that this wallet address is associated with this certain legal person who has gone through full KYC. So all of these projects seem pretty complex, right? So I also wanted to highlight some past hackathon projects, like what is feasible in a day or two, and a lot can be done, right. There is pretty good ML work that we've seen. These are all from Zkhack decision trees, weightless inference. You can do some pretty fun things like porting a completely different language such as brainfuck onto our ZkvM.
00:22:42.530 - 00:23:36.106, Speaker A: Another hackathon project is the P two P fiat on ramp, such as ZKP to P. They effectively allow you to on ramp funds using Venmo. All this is verified, the network transactions are verified, and it basically connects Venmo with the onchain requests for crypto, and then also reputation markets, prediction markets, something like a ZK credit score. All feasible. And these are all projects that people were able to hack together in a day or two. So although development periods are long, I think there's actually a lot of very useful use cases that you can build in under two or three days. So now I'm just going to quickly go over what does ZK mean beyond crypto? We've already talked about what ZK means beyond Zke Evian.
00:23:36.106 - 00:24:08.880, Speaker A: Now let's talk about what it fully means beyond crypto. Truthfully, a lot of these use cases are relatively new. The non crypto traditional tech, traditional government space move a lot slower. So these are maybe three to five to ten years out. But this is where we really will see ZK in every single part of our daily lives. So the first one is supply chain. I think supply chain has been around in the crypto space since it started.
00:24:08.880 - 00:25:14.610, Speaker A: But what we can do with ZK here is add a lot of privacy guarantees to a supply chain blockchain, let's say for the end user, for the end consumer, I only want to selectively reveal where the product was manufactured. I don't want to reveal any other stage of this process. With AZK proof, you can do that in a completely verifiable manner, or for the wholesaler, I only want to reveal certain information, left or right of this graph. And AZK proof is a very easy way to basically establish provenance while also building security around what information is revealed. Another idea here is secure control systems. There's actually been a research paper done by the national energy grid where they effectively think about running most of our energy control systems in a ZK context. This really ensures that no one is hacking the software, ensures that it's being executed correctly.
00:25:14.610 - 00:26:28.918, Speaker A: So before a power station shuts down, it will check that the ZK proof telling it to shut down came from the correct origins, came from the correct inputs, and there wasn't some sort of foreign hacker that was trying to get into the software. So these are all like research projects, but the use cases here are actually very valid and really bring true security to these systems. Similarly, in healthcare, it goes back to the idea of provenance and privacy. Whenever you go to a hospital, what if you don't want to share all of your health records, all of your history with the doctor? What if you just want to share selectively what's relevant to your current case, or with your insurance provider, you want to privately share relevant information. I think ZK is a great use case for this. You can also prove things such as your age is over 21 without revealing your age. So basically private provenance, giving users more autonomy, giving us as people more security, and being really selective.
00:26:28.918 - 00:27:24.190, Speaker A: I think we're also seeing with some new EU data privacy laws, that these use cases will become even more provenance. Because why do you want to give big tech companies your personal information when you can just selectively reveal certain aspects of it? Or you can just prove that you're over 18 without actually revealing your true birth dates? Another idea we've thought about is IoT devices. Truthfully, this is kind of far out. But how do you prove that your baby monitor is running correctly, or your ring doorbell is running correctly? That the data feeds are actually going to Amazon and not anyone else? This goes back to security control systems. But instead of a nuclear power plant, this is your ring doorbell, basically proving all of that software is running accurately all that is valid. And there's no hacker. Nothing about the software is broken.
00:27:24.190 - 00:28:18.850, Speaker A: Another possible use case is gambling. Once again, we have BRF functions. You can verify the randomness of a gambling site, but you can't verify what they do with that random number. Whether it's a slot machine or blackjack, or like hard deck shuffling. ZK proofs can basically prove that given a random string of numbers, we translated those random numbers into a usable output, into a game, into game logic. And then what is the future of ZK? What I wanted to go over here is key enablers in the space. I think there's three main sections, parity, versatility and functionality.
00:28:18.850 - 00:29:11.010, Speaker A: The key here with parity is we need to be able to run any program we want really fast performance, get as close to one to one performance with non ZK execution. And I think we're very far away from this. Maybe we can get to 100 to one in the next few years with fpgas and asics, but I think we're in a very early stage of the ZK industry, so our performance still has a long ways to go. And as I said, we can obviously accomplish this with hardware acceleration and then also just better devex tooling. We want parity in the devx experience. We want any sort of developer to be able to work with CK without new traditional math or cryptography backgrounds. I think we also need versatility in proof systems.
00:29:11.010 - 00:30:23.302, Speaker A: What this means is we need interoperability between proof systems, different softwares, different blockchains, different companies may be using different proof systems, their own proof systems. How do we enable interoperability between all of these? Whether it's some universal verifier or universal recursion, I think there's a lot of work to be done there. Additionally, with better security, some ZK proof systems only have 80 bits of security, which given a lot of coordination, could theoretically be hacked. So ensuring standards of security, quantum safe security is still key. Also just general system compatibility, vms, ensuring that ZK can easily be integrated into a lot of different systems. And then finally for functionality, right, building accelerators or things such as domain specific circuits for search and hash, or security or signature verification. This idea of an App Store, something where developers can easily ping accelerators, bring in specific domains, specific circuits.
00:30:23.302 - 00:31:16.380, Speaker A: It's kind of like a library that traditional developers would use when they're coding. And then finally we go into the realm of like MPC and fhe. This brings even better security and more functionality to ZK. I think fhe and MPC are real complements to the ZK stack, and they enable a lot of the privacy use cases I talked about previously, and also upgradability. Right now, if you look at most of the Zkevms that are written in plonky two, a lot of different stark circuits and snark circuits, it's really difficult for them to upgrade. Once you introduce account abstraction, you could potentially need to go into the code base and refactor everything. So moving towards a more generalizable framework will enable this better upgradability, and writing your code in traditional rust will definitely help in the ZkVM case.
00:31:16.380 - 00:31:56.514, Speaker A: So I just wanted to put some areas for disruption here. A lot of these are quite general and outside of the crypto space, but when we think about applied ZK, when we think about ZK beyond ZKE evms, these are a few of the areas we've been thinking about. Cool. I think that's all. Thanks for sticking through it, all of those different use cases, and I'm happy to take any questions. Thanks, Steven.
00:31:56.562 - 00:32:14.270, Speaker B: That was awesome. I think we probably will have a few questions if we linger for a moment. There's sort of like a lot of topics that you ran through, from CKML to gaming, to fraud proof, to roll ups to et cetera, et cetera.
00:32:15.250 - 00:32:15.566, Speaker A: Yeah.
00:32:15.588 - 00:32:20.590, Speaker B: I'm curious, what are people thinking? Any follow up questions to any of those ideas?
00:32:23.490 - 00:32:24.720, Speaker A: Can you hear me?
00:32:26.530 - 00:32:27.858, Speaker B: Yeah, we can hear you.
00:32:28.024 - 00:32:30.466, Speaker A: Yeah. I'm wondering, where do you see a.
00:32:30.488 - 00:32:33.080, Speaker C: Demand for devs in the ZK space?
00:32:34.730 - 00:32:36.738, Speaker A: Where do you see the majority of devs?
00:32:36.914 - 00:32:39.906, Speaker C: No, where do you see a demand for devs?
00:32:39.938 - 00:33:22.662, Speaker A: Oh, demand for devs. Yeah. I mean, truthfully, right now a lot of them are in ZKe EVM, ZK roll ups, but also like applied ZK. I think the ZK coprocessor idea is pretty prominent. Defi is where I'm seeing a majority of them. Defi you can think about, as I said, like variable parameter changes, dark pools, shielded defi, things like what Aztec and a company called Renegade are pushing. Yeah, but a lot of these are very new projects, and it just requires people to take it upon themselves to start hacking upon it.
00:33:22.662 - 00:33:39.090, Speaker A: It's a very early field. Outside of the Zkevms, everything else is kind of a startup. Thank you. I think there is a question in chat.
00:33:40.070 - 00:33:46.580, Speaker B: Yeah, I'll read the question in the chat. It says, can you give us more info regarding the DSL market space example?
00:33:50.470 - 00:33:53.590, Speaker A: Are you referring to domain specific circuits?
00:33:54.490 - 00:33:59.910, Speaker B: Yeah, it says, if I remember correctly. So domain specific circuits is probably what was being referenced there.
00:34:00.060 - 00:35:08.350, Speaker A: Yeah. So a DSL is a domain specific language. So that's something like circom. It's a specific language made to code for a specific use case. A domain specific circuit is something like that, but not really a domain specific circuit is a ZK circuit that's only used for one main use case. And what I was thinking about there with the App Store is something like in the future you can just call a domain specific circuit for shot 256 hashing or for ECDSA signature verification. It's an App Store in that programmers can get ZK proofs, very performance, very optimized ZK proofs for specific use cases in a generalized framework, like our ZkVM for example, we basically have this, and we call them accelerators, which you can call in your traditional rust program that will basically compute hashes and signature verification substantially faster than in a generalized context.
00:35:10.770 - 00:36:00.542, Speaker D: Thank you. Yeah, that's actually really interesting. Speaking of, let's say if I have a couple of just different sort of, maybe not domain specific circuit on ZkVM at this circumstance, but it's more like just, I guess just some pre written ZK proof services that is written, that is available to run on TKVM. And maybe if bonds has this sort of marketplace or just libraries that is public. And whoever uses writing their own program on Bonsai can actually call those sort of different Bonsai App Store services and then have it implemented into their bonsai stuff. I think that will be really interesting. But also, would that mean they're going to be like two different bonsai steps that is running, or.
00:36:00.542 - 00:36:03.840, Speaker D: I don't know, would something like this work out?
00:36:04.450 - 00:36:30.630, Speaker A: Yeah, it's actually something pretty interesting. Right. There is like the low level domain specific circuit that does hashing. There's also something very high level. Like what if there is an app that does KYC, like super generalizable, it's an App Store sort of thing, and users can just directly ask for KYC. They don't need to touch anything. And I think that is something we're thinking about.
00:36:30.630 - 00:36:46.460, Speaker A: Right. That really just falls into the App Store sort of idea how that's integrated into bonsai. Truthfully, I won't say anything because I'm not very sure myself, but there's definitely ways to build around that and build like an easy to use platform that developers can use.
00:36:47.650 - 00:36:48.158, Speaker D: Amazing.
00:36:48.244 - 00:36:48.654, Speaker A: Thank you.
00:36:48.692 - 00:36:50.990, Speaker D: I think there's another question in chat.
00:36:52.690 - 00:36:53.440, Speaker A: Yeah.
00:36:56.690 - 00:37:13.590, Speaker B: Let'S read it out loud. It appears on the recording. Do you see ZK coprocessors as direct competitors to decentralized compute providers? Or do you see them as suppliers and ZK coprocessors as the aggregation abstraction layer.
00:37:15.050 - 00:37:55.218, Speaker A: Right. So I actually think decentralized compute providers can become providers for ZK coprocessors. At the hardware level. ZK coproprocessors are basically infrastructure as a service, right? They're traditionally a big cluster of gpus. So ZK coprocessors will take in the input and take in the software, and then they will look for hardware. As it stands, usually it's AWS, but they can also query things like Akash or other decentralized compute networks. For compute, the only issue there is cost.
00:37:55.218 - 00:39:04.090, Speaker A: Right? I'd say most of the compute providers are a little bit more expensive than what AWS can offer in their centralized offering. So yeah, that's kind of how I would think about that. And they're basically like job aggregators, right? A bunch of ZK proofs, a bunch of ZK jobs are sent over to coprocessors, and then the coprocessors will aggregate those jobs and send them out, distribute them amongst infrastructure providers such as AWS, such as Akash. And also with our goal is we want to decentralize that fully eventually, so anyone can really hop on and generate ZK proofs. I'm curious how you feel about ad auctions as a use case, considering they're a lot of the economy of web two, and it feels like verifiability, privacy and succinctness all apply there. Sorry, ad auctions as a use case. Ad auctions like Facebook ads.
00:39:04.090 - 00:39:52.550, Speaker A: Yeah, entities bidding on ad space. Yeah, totally. I think that makes a lot of sense. I'm not terribly familiar with the ad space, but something we've thought about with Facebook ads, for example, is right now, Facebook ads will learn everything about you, like your location, your interests, everything. Or like they're super. But maybe Facebook can be a little bit more selective about it or a little bit more secure, like using a ZK proof, I can prove that I'm interested in cars without actually showing the exact car brand that I've been looking into. Generally that makes ads a lot more secure, a lot more private.
00:39:52.550 - 00:40:17.230, Speaker A: And then I'm guessing for the ad auction, you just reveal those sort of numbers, you reveal that sort of information instead of revealing all the browsing history of a user and things like, yeah, generally we think the EU is going to do a real push in that direction as well. Thanks.
00:40:30.970 - 00:40:36.870, Speaker B: Looks like there's one more question in the chat that says, can you point me to any open source projects I'd like to help with OSS.
00:40:40.450 - 00:41:09.430, Speaker A: The good news is our ZKVM is fully open source, and a lot of other parts of our recursion will be open source in the coming few weeks. So happy to have you contribute to our product. But also, a lot of the ZK systems are very open source. A lot of the dsls are open source. I'd recommend. I don't know, Paul, I think you have actually a lot of information here on good resources.
00:41:10.650 - 00:41:34.240, Speaker B: Yeah, I mean, if you're interested in the deep ZK side, I can certainly make recommendations. I think arcworks and Circom are both open source. And also though, yeah, I mean, we are about to be open sourcing even more of our stack. And definitely our intention is to be as open source as possible.
00:41:42.770 - 00:41:43.134, Speaker A: Yeah.
00:41:43.172 - 00:41:45.120, Speaker B: Dragon, you want to jump in?
00:41:45.890 - 00:42:34.160, Speaker E: Yeah. Thank you. Maybe I'm out of context, but I was really sending last presentation about zet and there was a mention that the ketchup takes more than 50% of cycle count, which is, I guess, more than 50% of the resources. And my question is, how would the project improve if there was a different hash function? If the different hash function would be used in zet, let's say Poseidon hash, or any ZK friendly, how different would it be? Like the performance of that?
00:42:35.090 - 00:42:54.610, Speaker A: Yeah, that's actually probably one of the easiest ways right now for us to get a very substantial proof performance improvement. Yeah. Ketchak. We have no accelerators. We have no nothing. That's just run with the Shaw two crate, that implementation. So it's super slow.
00:42:54.610 - 00:43:38.210, Speaker A: If we move to Shaw two, like Shaw 256, we actually have an existing accelerator for that. So that would get us really substantial performance gains and ZK friendly hashes as well. What we've actually been thinking about doing is basically building our own ketchack accelerator, which is a domain specific circuit outside of our ZKVM. And that would substantially improve performance at a maximum, probably reduce that 50% all the way down to 1%. Victor can probably comment a bit more on this, because he handles a lot of our accelerators.
00:43:41.380 - 00:44:30.560, Speaker C: Yeah, I mean, what you said is exactly right. So not a whole lot more to add to that. One thing I will say basically is obviously we would like to use a different hash function like ketchak is quite non ideal, but that's the choice of the theorem community, and we follow the choices that they make because we're trying to be compatible with them. I know other Zke EVM projects have changed out the hash function that they use for the inclusion proofs in the state Db, and that makes them incompatible with the base layer protocols. This is where you get the distinction between different types of EVM, and we strongly believe that basically that it is worth it to maintain compatibility and actually use the hash functions that ethereum sets forth.
00:44:36.110 - 00:45:09.570, Speaker E: Yeah, I understand. The reason I'm asking because I'm doing a research project how would change to vertical tree impacts. The Zkavm projects like that. In the vertical tree, the main hash, quote unquote hash function is the Pedersen. So it's a multiscalar multiplication. Do you have specific circuits for multiscalar multiplication or it's just compiled from rust?
00:45:10.870 - 00:45:12.802, Speaker C: That's really interesting question.
00:45:12.856 - 00:45:13.506, Speaker E: Sorry.
00:45:13.688 - 00:45:56.210, Speaker C: No, that's great. I love that question. We don't have specific circuits, so what you would really want, basically, is probably something that would basically be like a finite field accelerator. And we actually do have a finite field accelerator. We have a module multiplication 206 bit integer multiplier. The 206 bits is probably the main difficulty you're going to run into with this one, because I believe that the vertical tree proposal uses, I think, BLS twelve, which is too large for that. However, definitely in a future revision of our circuit, we're going to have a more flexible, larger bitwidth version of the same circuit, up to, I think, 1024 bits.
00:45:56.210 - 00:46:07.190, Speaker C: And so this finite field multiplication accelerator would be probably the main component you'd want to use to basically do accelerated vertical tree verification.
00:46:11.380 - 00:46:33.530, Speaker E: Okay, thank you. Just to mention that in current proposal, it's the embedded curve into BLS twelve three eight one. That's the curve specified for the Pederson. So it's less than 256 elements. And yeah, I plan to benchmark this with risk zero. So that's why I'm asking.
00:46:34.540 - 00:46:34.904, Speaker C: Yeah.
00:46:34.942 - 00:46:35.192, Speaker A: Awesome.
00:46:35.246 - 00:46:40.170, Speaker C: I'd love to see more about that. So when you have some initial results, I'd love to see them.
00:46:50.610 - 00:46:51.262, Speaker B: Very cool.
00:46:51.316 - 00:46:52.810, Speaker E: Thank you. Bye.
