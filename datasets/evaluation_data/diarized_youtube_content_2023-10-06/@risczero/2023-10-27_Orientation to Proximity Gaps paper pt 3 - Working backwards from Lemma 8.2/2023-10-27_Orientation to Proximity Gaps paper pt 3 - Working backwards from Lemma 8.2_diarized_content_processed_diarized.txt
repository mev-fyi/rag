00:00:01.210 - 00:00:57.128, Speaker A: Okay, but let's sort of circle back to. The major objective here is to try to track down how we're going to translate from the list decoding radius into the unique decoding radius. And hopefully, this is going to make that quadratic term epsilon disappear. Just to circle back to the beginning for a of like, wait, what's that epsilon that I'm pointing to, that epsilon is going to be this term. Ideally, we were looking at the proof of lemma 8.2. And we were looking at the part of the proof of lemma 8.2 where they define the actual fry soundness bounds here.
00:00:57.128 - 00:01:33.668, Speaker A: And the key epsilon term here was defined in equation 8.8. So let's bounce back to equation 8.8 and see if we can make sense of. Okay, this is looking nice. We have theorem 7.4 implies this probability, whatever this probability is less than some error bound. And this bound is exactly what we were looking at there.
00:01:33.668 - 00:02:15.330, Speaker A: Okay, great. So this looks like is going to somehow come from theorem 7.4. This is the term that's, like quadratic in our fry domain size. This n squared now is quadratic in our fry domain size. And when we move into the unique decoding analysis, this term should get replaced by something quite simple. And I think I can maybe actually even track down specifically what it is. If we look at theorem 7.4
00:02:15.330 - 00:02:53.870, Speaker A: at some point in this. It's quite a complicated little collection of proofs here. These proofs are going to come back in some way to theorems 5.1. Or in particular, theorem 5.1 is the key result over the list decoding radius. And here we see exactly that term that is appearing in our fry soundness. And the corresponding theorem 4.1
00:02:53.870 - 00:03:37.288, Speaker A: is our result in the unique decoding radius. And the corresponding term here is just this. Suppose the size of s is greater than n. In theorem 5.1, we had this much hairier suppose s was greater than this thing. To the 7th power here. Okay, so we were looking at lemma 8.2.
00:03:37.288 - 00:04:02.480, Speaker A: And in lemma 8.2, there was this epsilon. And this epsilon was defined in equation 8.8. And we went back to equation 8.8. And equation 8.8 was implied by theorem 7.4. So here's the key term in equation 8.8
00:04:02.480 - 00:04:54.980, Speaker A: that appeared. There's this extra kind of. It's wrapped in a little bit of extra stuff here that's somewhat unclear to me. To contextualize what the next steps are, it feels like we should be able to replace this first term with just n based on the like this looks like it's extracted from theorem 5.1 and we should be able to translate into the corresponding theorem 4.1 and replace this with n, which I think may mean that. Now, the bounding term for this maximum argument is actually the second term rather than the first term.
00:04:54.980 - 00:06:11.938, Speaker A: So it seems to me like the result that we might get may actually be, rather than replacing this term directly with n, I think we may actually be replacing the term with something that looks more like this. But I'm a little bit unclear really, of the details here. So maybe I'll stop my kind of dig in at this point and we can see where others can help push this analysis forward. I'll recap for a second from the top before I kind of close here. So I think the meat of what we're going to need to try to figure out where I'm sort of stuck is. Okay, if we want to translate. Where should I start here? If we want to translate this into a unique decoding focused result, how do we do that? That's the five second version of the problem.
00:06:11.938 - 00:07:26.660, Speaker A: What is the unique decoding analog of? In particular, what is the unique decoding analog of this part here? Is it just n, or is it something that includes the rest of that maximum argument that lives in theorem 7.4? So from the top, theorem 8.3 is the main price soundness result. We have a query error thing, and we have these two terms here, which come from batching and folding, and these two terms get unpacked in lemma 8.2. And the proof of lemma 8.2 is the very end of the paper. Did I.
00:07:26.660 - 00:08:14.580, Speaker A: It's the very end of the paper. There's this algebra here, and it's this epsilon which appears also, it's part of both of these, this proximity gap, or I guess this weighted correlated agreement thing is coming out of both here and here. This epsilon to the I thing is defined on the previous page in terms of this epsilon. It's like a combination of epsilon and one of. Well, yeah, I don't need to dive into that. Anyways, these terms, both are defined in terms of epsilon. So we have.
00:08:14.580 - 00:08:31.430, Speaker A: Yeah, that's where I'm currently stuck in my analysis, or as far as I've currently gotten. Thanks for listening and we'll see where we go from here. Bye.
