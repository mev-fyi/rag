00:00:03.450 - 00:00:22.686, Speaker A: Hi, everyone. This is Alex from near protocol. We're in Cupertino at Harmony office. With me today is RJ, one of the developers of Harmony. And we're going to go into very technical details of Harmony protocol today. RJ, would you like to quickly introduce yourself and give us an overview of how Harmony works?
00:00:22.788 - 00:00:39.398, Speaker B: Okay. Hi, everyone. I'm R. Jay from Harmony. I'm the core developer and co founder of Harmony. Harmony is a high performance blockchain that features sharding and secure proof of stake. And our consensus is a fast BFT consensus.
00:00:39.398 - 00:00:47.260, Speaker B: I also improve on the network infrastructure so that the sharded network can operate in an efficient way.
00:00:53.410 - 00:00:59.210, Speaker A: You are. So let's get the high level overview.
00:00:59.290 - 00:01:43.262, Speaker B: Okay, so, in harmony, we have two type of chains or two type of shards, right. First is the beacon chain, which is like overarching chain that controls the overall network sharding construction. So there are different shards like this. Each shard has its own chain like this. So, yeah, should point it back. So the shard chain will process transactions separately. So each shard has its own state.
00:01:43.262 - 00:02:20.874, Speaker B: Right. So, for the head of the block, it contain the state of the shard. So harmony is state sharding, which means the state, the blockchain state, are separated and stored separately in each shard. Those states are different, contain different data. And for each shard is separately processed transactions in parallel. So let's say you have a transaction here, and this transaction is specifically for this shard one. So this shard one will process this transaction by itself.
00:02:20.874 - 00:02:32.490, Speaker B: And shard two will ignore that transaction. So this will multiply the TPS by the number of shards because they're processing the transaction separately.
00:02:32.650 - 00:02:36.480, Speaker A: And short to medium term, how many shards do you expect to have?
00:02:37.650 - 00:02:46.370, Speaker B: Maybe at the first launch of the net, we'll have about ten or below 100.
00:02:46.520 - 00:02:51.380, Speaker A: But the design of the network, like, what is the largest number of shards it can support?
00:02:52.230 - 00:03:25.070, Speaker B: Yeah, it's a good question. It depends on the beacon chain's processing power. So I haven't said this. So for the beacon chain, beacon chain is just like normal shard, but it has additional functionalities. So one of them is for generates the secure randomness. Another is storing the cross link from the other shard. Cross link from the other shard.
00:03:25.070 - 00:04:33.970, Speaker B: So for the randomness, the beginning chain needs to run distributed randomness generation protocol to generate this secure randomness that's unpredictable, unbiasable, and verifiable. So that it's hard for people to bias this randomness or manipulate the randomness. Because the randomness is important in the sharding process. Why is it important? Because we want to randomly assign the validators to each shard, so that the attacker won't concentrate their voting power inside a single shard. So that will require this secure randomness. Another function for beacon chain is the cross link to the other shard. So for each of the block in the shard that's generated, you will send the header to some of the blocks in the shard chain.
00:04:33.970 - 00:05:37.030, Speaker B: In the beacon chain. Sorry. So with this, we can have a kind of global ground truth for which chain in the shard that's canonical. And also for the beacon chain to contain all of the headers of the shard chain. You can directly broadcast or sync those headers to the other shard, so that other shard get like a header chain or the other shard, so that they can act as like a light client for the other shard, so that they can verify some of the proofs from other shard. So one of the kind of performance upper bound is how fast the beacon chain can process the cross hash link, right? So like a header, in term of Ethereum header is about like 500 bytes. I think that's about the scale.
00:05:37.030 - 00:05:57.600, Speaker B: So if we include all the headers from all the shard chains, I think you'll scale like 1000 shards, like 500 block 500 or 1 mb, about that 1000 or 2000.
00:05:58.130 - 00:06:19.794, Speaker A: But there is also a BLS signature, right. In every shard. So also the processing power you need to be able to verify thousands of BLS signatures. Right. And you're saying that you snapshot every block from the Shard chain, right? And the beacon chain, does beacon chain produce blocks as frequently as shard chains?
00:06:19.842 - 00:06:24.150, Speaker B: Yeah, because it's running the exact same consensus.
00:06:25.450 - 00:06:43.680, Speaker A: And it also can accept, like it has also some state unrelated to randomness and cross links. So you can actually send transactions to the beacon chain. I see. Now the question is, why do you need to cross link? How does cross linking help?
00:06:44.130 - 00:07:32.214, Speaker B: So the cross link will make sure there's one canonical truth about which shark should be which chain. Right. Because there is cases where one shark have forks. It's hard to figure out which chain is canonical because we're not using poor work, we're using BFT. And also we're using staking to prevent the CBU attack. So it will be a little bit easier than pro work to create forks. And that's kind of hard to distinguish from the canonical one with the crosslink, because the beacon chain is like the global ground truth.
00:07:32.214 - 00:07:51.620, Speaker B: And also beacon chain has more security, so it's more trustable. So if you put the hash link from shard chain to the beacon chain, we have a better security guarantee or more kind of proof that a single change, the canonical one.
00:07:52.390 - 00:08:00.150, Speaker A: But because you're using PBFT, and I assume that every participant in the PBFT is weighted by the stake, right?
00:08:00.220 - 00:08:00.854, Speaker B: Yeah, exactly.
00:08:00.892 - 00:08:13.414, Speaker A: So for as long as one third of the stake is. For as long as less than one third of the stake belongs to people who are intentionally trying to behave maliciously, the fork is.
00:08:13.612 - 00:08:14.460, Speaker B: Yes, yes.
00:08:15.550 - 00:08:35.600, Speaker A: Effectively, the design of the system assumes that it is possible to have more than one third of malicious actors in the short chain. So the question is realistically up to what percentage of malicious actors you think can end up under reasonable assumptions can end up in a single short.
00:08:36.850 - 00:08:39.230, Speaker B: Can what, sir? Can happen in single.
00:08:39.300 - 00:08:45.278, Speaker A: Yeah, so yeah, underreasonable security assumptions. What do you think is the maximum percentage of malicious actors in a single shirt?
00:08:45.374 - 00:08:52.070, Speaker B: So our security assumption is we will have like one fourth of malicious token.
00:08:53.210 - 00:08:54.290, Speaker A: In the global pool.
00:08:54.370 - 00:09:29.170, Speaker B: In the global pool validator. So when they're doing staking, we have this adaptive thresholded Pos staking, which is working like this. I'll draw it here. We have three validators that has different amount of stake. So this sale, I have 100 tokens staked. And this have like 50 tokens staked. And this one have like 20 tokens staked.
00:09:29.170 - 00:10:24.082, Speaker B: And after this amount of token, we'll use a formula to formula is like this. This formula is for the price for a single ticket. We call them voting share. So this formula is like, this is the total amount of stake in the last epoch over the number of shard times a security parameter. So the total number of stake right here is like 170. And the number of shard, say, let's say ten. And we have the security parameter right here, right here.
00:10:24.082 - 00:11:30.726, Speaker B: If we want to make the shard secure after the random sharding, we need to make the security parameter large enough. Say we calculated it, it has to be 600. So that when the process is like this, after they do staking based on this price, they give voting shares. Let's say we have 600, let's say this lambda is 600 and each voting share, the price is like, let's see, 650, no, it's like 0.2 something. Let's say five. So this guy will get like 400 voting share, this hundred get 200, this guy will get 880.
00:11:30.726 - 00:11:36.874, Speaker B: So we get this amount of voting shares, 200 voting share, this 200, this.
00:11:36.992 - 00:11:46.720, Speaker A: 80 voting share, you need extra zero, right? Because you need 610 shirts. So it's going to be like 4000 2800.
00:11:49.330 - 00:12:40.110, Speaker B: So we'll randomly distribute these voting shares into the number of shards. So if we have like ten shards like this, these voting shares will be randomly distributed all to these shards. And the end result is for each shard, this first validator will have some amount of voting shares there. Because it's randomly in expectation it will be proportional to their original staking. Right? So here, let's say here this one, I have like 400 voting share here for the validator one. And here is like 380 validator one. Something like this.
00:12:40.110 - 00:13:07.490, Speaker B: The reason we're going to convert the staking to voting share and then distribute to the shard is because we want to kind of cut the big stakers stake into pieces and randomly distribute the stake into multiple shard so that they cannot concentrate their big stake into a single shard and then overtake that shard.
00:13:07.650 - 00:13:31.310, Speaker A: But if you're making so with your particular constants. Right. Let's say that the assumptions about malicious actors in the global pool is 25%. And let's say the security parameter is actually 600. Then the probability of having 33% of malicious actors is practically zero, right? It's like one e -18 or something. So why do we need to cross link? It seems like forking is impossible.
00:13:32.210 - 00:13:51.394, Speaker B: It's because even we have this security proof that when you are sharding the validator or their staking is secure. But over time, a single shard can get overtaken or get corrupted by malicious attackers.
00:13:51.442 - 00:13:54.982, Speaker A: Are you saying it's going to be corrupted adaptively after the shuffling happens?
00:13:55.036 - 00:13:59.110, Speaker B: Right? Using and not prevent attackers to corrupt some of the node.
00:14:00.670 - 00:14:14.510, Speaker A: So what we believe in here effectively is that after the validators were assigned, there is a chance that someone will adaptively corrupt more than one third. Do we also believe that someone can adaptively corrupt more than two thirds?
00:14:15.170 - 00:14:15.920, Speaker B: Yes.
00:14:16.290 - 00:14:18.574, Speaker A: Right. So go ahead.
00:14:18.692 - 00:14:57.020, Speaker B: Yeah. So as I said, the sharding need to. First, the distribution of the validator need to be done securely. But also we need to take care of the corruption of a single shard. Right? Where we have this resharding mechanism. Which means periodically, we're going to reshuffle the validators in the shard or their voting power in the shards across different shards. Right? So when attacker corrupt a single shard, you will not be able to get the whole shard's voting power.
00:14:57.020 - 00:15:22.130, Speaker B: How to say for a long time. Because we're going to reshuffle the voting power again and again. So that they had to keep catching up with the reshuffle. So we're going to keep the reshuffle frequency high enough so that with high probability that attackers cannot be easily overtaking a single shard.
00:15:22.710 - 00:16:11.170, Speaker A: But we do believe that they can take over the single shard for a very brief period of time. Right? So let's go through the standard attacks on the short side. So the easiest one is forking. Right? And so against forking we protect by cross linking. Now the beacon chain, when it snapshots the short chains, effectively, beacon chain acts as a light client, right? So what beacon chain does is it gets the header. Assume beacon chain doesn't download the entire block, right? That would be lots of information to download, right? So it downloads the header, it verifies the signature and it snapshots it. So let's say that two thirds of people were corrupted in a single shard, right? And let's say that what they did is they created a block.
00:16:11.170 - 00:16:57.394, Speaker A: And in this block the state transition function is applied in a completely wrong way, right? So like out of nowhere one of the malicious actors gets 1000 tokens for example. And on top of that, not necessarily, but let's say they also built another block which otherwise is valid. So from the state that was passed from this block to the state that was final, the state transition was applied properly. And now this block is being snapshoted or like I guess both of them get snapshoted to the beacon chain. The beacon chain has no way to detect, right. That the block was wrong. Are we assuming that that cannot happen? Or how do we defend against this?
00:16:57.592 - 00:17:27.818, Speaker B: So for a shard to have a wrong state, you have to have more than two thirds of the malicious voting power, right? So with that, we already calculated to have like one third is already almost impossible. So to have two third is like thousands of motor magnitude even harder, right?
00:17:27.904 - 00:17:50.674, Speaker A: But that is from perspective of sampling, right? So if sampling one third is very close to zero, sampling two thirds will be practically zero. But if we think that there's an adaptive adversary, then it seems like if we believe that adaptive adversary can corrupt one third, it seems to be reasonable to assume that they can also corrupt two thirds. Then there is no exponential decay in probabilities. Does that make sense?
00:17:50.792 - 00:18:17.690, Speaker B: Yeah, I see your point. So we try to make the resharding process frequent so that we're going to make sure that it's almost impossible to create forks to create wrong state. Because also we're using the PBFT or BFT consensus, which is by nature is hard to create forks. And it also provides finality.
00:18:21.550 - 00:18:39.650, Speaker A: The effectively, the assumption is two thirds of malicious actors would never end up in a single shard. Right. And also, you said that the security of the beacon chain is higher than the security of the shard chains. Where does that come from? Do they have more validators assigned?
00:18:40.710 - 00:19:12.698, Speaker B: As I said, all the beacon chain or shard chain are just like a shard. The sharding process or resharding process apply to all of them. So it's just a matter of how much stake or voting power you're going to assign to them. Right. So as I already analyzed, the security of a single shard is already kind of proved secure. Right. So for the beacon chain, it has the same security as the shard chain.
00:19:12.698 - 00:19:32.258, Speaker B: If we treat them as the same shard as other shard, basically it means putting the same amount of voting power or staking into them. But since the beginning chain is like the global, it can be the single point of failure. So we're going to put assign more.
00:19:32.344 - 00:19:35.346, Speaker A: Staking, like effectively lamp this higher.
00:19:35.448 - 00:19:50.890, Speaker B: So let's say each shard has the equal amount of voting share or staking that's assigned to them. We're going to assign like two times of the staking to the beacon chain. So that is a lot more secure.
00:19:52.270 - 00:19:58.934, Speaker A: Cool. So from here, and how often a block is produced, is it like every few seconds?
00:19:58.982 - 00:20:04.486, Speaker B: Yeah. Uses BFT consensus. Every few seconds we'll have a new block.
00:20:04.598 - 00:20:06.878, Speaker A: And you use some flavor of PBFT, right?
00:20:07.044 - 00:20:11.070, Speaker B: Yeah. It's like optimized BFT that scale linearly.
00:20:12.130 - 00:20:18.690, Speaker A: Yeah. Let's talk about the consensus. Or do you want, first, while we have the shards, do you want to talk about cross shard transactions?
00:20:20.310 - 00:20:21.026, Speaker B: We can talk about.
00:20:21.048 - 00:20:35.510, Speaker A: Yeah, let's do cross shard transactions since we have shards already drawn. So what happens if I want, like, let's say I'm Alice or Alex and I'm on this side and there is on this shard, and there is Bob, who is on this shard. How am I sending money to Bob?
00:20:36.970 - 00:21:35.590, Speaker B: It's actually very simple. So if I want to send some money from the sharp a shar one to Shar B, I just create a transaction specifying the source account is shar one, some account of yours, and then destination account in shard two, some other account, other person's account. And then the transaction will be, let's say this is a cross shard. Cross shard transaction will be sent to the source shard. Right. And the source shard will first kind of debit the money you're going to spend, right? So you're going to first deduce your balance first and then generate proof in the form of a receipt, like Ethereum receipt. So send this proof to the destination shard.
00:21:35.590 - 00:22:00.660, Speaker B: So destination shard, as I said, destination shard will act as a like client of the other shard, right? So you can check that proof is valid. And also based on the cross link, all those stuff, you can make sure that the money already is deducted, and then the shard two is safe to credit the amount to the destination address.
00:22:03.190 - 00:22:37.614, Speaker A: I presume that Harmon is also executing smart contracts. Right. When I'm sending a. So let's say I have a crypto zombies account on this chart, crypto zombies contract, and there's a cryptokitties contract on this chart. And I want to have a method in crypto zombies which would consume a cryptokitty to do something. Right? So as a developer, how do we express the crosshart invocations? Is it some sort of promises or how does that work?
00:22:37.812 - 00:22:56.918, Speaker B: So right now, for smart contract, actually, most of the work are done by the single shard, right? And for the quashar smart contract, we are only allowing kind of the read operation, like reading whether there is a crypto key in the other shard. We're not supporting the update yet.
00:22:57.004 - 00:22:58.614, Speaker A: And how does reading work?
00:22:58.812 - 00:23:40.100, Speaker B: Read. We have this mechanism of kind of. It's more like subscribe and listen, right? So let's say your smart contract will be reading some fields in the other shard. Smart contract. And you are going to specify that in the smart contract. And then the data that you're going to listen in this shard, those validator in that shard will publish that part of the data. You're interested to your shard, right? You're going to keep publishing that.
00:23:40.100 - 00:24:01.290, Speaker B: Whenever that data is changed, you're going to publish that, and your shard that's interested in this field will listen that. So the execution of the smart contract won't be blocked by anything because most of the cases have the updated data from the other shard.
00:24:02.670 - 00:24:07.882, Speaker A: But I can do write operations for the contracts which are on the same shard as I write.
00:24:07.936 - 00:24:09.514, Speaker B: Yeah, you can write.
00:24:09.632 - 00:24:20.366, Speaker A: Then the question is, you said that at the beginning it's going to be, let's say, ten shards. But then over time, I assume you want to go like, to thousands of shards. When you increase number of shards, the new shards, are they empty or you.
00:24:20.388 - 00:24:23.390, Speaker B: Rebalance, they will be created a new.
00:24:23.460 - 00:24:27.310, Speaker A: Oh, they're going to be completely empty. Okay, I see. That would work, I guess.
00:24:27.460 - 00:24:27.870, Speaker B: Cool.
00:24:27.940 - 00:24:31.310, Speaker A: Okay, so let's move to the consensus.
00:24:34.610 - 00:24:35.360, Speaker B: You.
00:24:45.930 - 00:24:48.374, Speaker A: So how does consensus work?
00:24:48.492 - 00:25:25.922, Speaker B: Yeah, I can draw like a message graph like this. Since it's BFP consensus, there will be leader. Right. And there will be the validator. Validator one, validator two, validator three. So because BFT is following the traditional PBFT kind of methodology, there will be two faces, there should be more. There is phase called prepare.
00:25:25.922 - 00:26:04.050, Speaker B: Right. And another phase called commit. And before that, the leader will announce the block first to all the validator. This we call the announce phase. So once the leader announced the new block to the validator, the validator first check whether it's valid or not. Right. And then they will sign the BLS signature on the header and return back to the leader.
00:26:04.050 - 00:27:05.170, Speaker B: Return back to the leader. So the leader will aggregate all the BLS signature into a multi signature. Multi sig, right. And after the aggregation, it will send that multi signature again to all the validator. And then the same thing, kind of same thing happened. All the validator need to check whether they're more than like two s plus one, where s is basically one third of the committee, whether they're more than two third plus one or equal to two third plus one, signing on that multi signature. If that's true, it will sign that multi signature again using BLS and return back to the leader.
00:27:05.170 - 00:27:20.570, Speaker B: So the leader, after getting two third plus one, you'll aggregate that signature again and broadcast to the validator for them to finally commit the new block.
00:27:20.990 - 00:27:23.558, Speaker A: Is it synchronous or is it weekly synchronous?
00:27:23.734 - 00:27:25.014, Speaker B: It's partially synchronous.
00:27:25.062 - 00:27:41.946, Speaker A: Partially synchronous. So there's some sort of a timeout here. Right. So at some point, clearly there's never going to be the case that the leader will accumulate all these signatures. Or like, it's going to be very rare. Someone will be a flying. Right, you have 600 validators.
00:27:41.946 - 00:27:50.238, Speaker A: Presumably someone is a flying. So at which point leader stops waiting for more signatures and starts distributing the block.
00:27:50.334 - 00:27:52.562, Speaker B: Once they have two third class one.
00:27:52.616 - 00:27:59.042, Speaker A: Okay, so the moment they get two third plus one, they immediately start distributing. They're not waiting for more. How is the reward distributed?
00:27:59.186 - 00:28:03.026, Speaker B: The reward will be distributed equally, including.
00:28:03.058 - 00:28:12.250, Speaker A: Those who did not participate. What is the motivation to participate? Like, why would they sign any? Why would they even be online once I'm assigned?
00:28:12.830 - 00:28:16.202, Speaker B: That's a good question. If you don't sign, you won't get a reward, right?
00:28:16.336 - 00:28:41.586, Speaker A: Right. But then the leader only waits for two thirds plus one, let's say 100% of people are online. Let's say everybody's online. But the moment the leader gets two thirds plus one, they already just sent the block, right? So two thirds like f people. F people did not get their signatures into the block. So the block is recorded only having two thirds plus one signatures. So f people who were online and participated will not get a reward, right?
00:28:41.768 - 00:28:42.500, Speaker B: Yes.
00:28:42.870 - 00:28:44.370, Speaker A: Isn't it unfortunate?
00:28:45.130 - 00:29:11.690, Speaker B: No, because you have to wait for exact this amount to proceed. You cannot wait more because the security assumption is we can have at most, or it's possible to have f malicious nodes, right? So if you wait more, you are basically waiting for at least one malicious guy. Then if that guy is stalling, you will never get the money.
00:29:11.760 - 00:29:22.622, Speaker A: Yes, but my question is more about the fact that, let's say that v one and v two signed, and so the leader progressed, but v three, now v three will not get the reward, right?
00:29:22.756 - 00:29:25.634, Speaker B: I said everyone will get reward, even.
00:29:25.672 - 00:29:31.234, Speaker A: People who were offline. But then we get back to the question, why would we three even be online?
00:29:31.352 - 00:29:54.138, Speaker B: Yeah. For the guy who are not signing, if in some rare instances where it's kind of slow for him to reply in time, it's okay, but we have the slashing mechanism. Or we're not slashing. We're going to drain their voting power if they continuously not signing for a long time.
00:29:54.224 - 00:30:02.990, Speaker A: So if they didn't sign multiple blocks in a row, then they will be punished. And the leader is rotating, right? The leader, is it rotating.
00:30:04.690 - 00:30:10.558, Speaker B: At first? We're going to keep it stable if it performs honestly, right?
00:30:10.644 - 00:30:14.754, Speaker A: So if the leader wants to punish the validator, they will just not include their signature, right?
00:30:14.792 - 00:30:36.082, Speaker B: Yeah, that's a good question. Then this will come to the filtering problem, right? The leader have the power to filter anything they don't like. So in that case, the nodes or validator who got filtered can show that some proof that he already signed that for a long time, for many rounds.
00:30:36.146 - 00:30:47.386, Speaker A: But how can you prove that you signed in the past, not now. You cannot prove that something happened in the past unless it was included on the chain, right? But it was not included on the chain because the leader rejected you.
00:30:47.408 - 00:30:53.290, Speaker B: Sign that proof every time when you are like, but I can sign your.
00:30:53.360 - 00:31:02.266, Speaker A: Message and withhold it. So imagine v three. Yeah, they will sign. They will withhold it. They will not send it. And then later they will say, look, I have the signature.
00:31:02.378 - 00:31:12.622, Speaker B: That's right. You have to broadcast your signature. So if you're honest, you're going to broadcast the signature. As you are persisting in the consensus.
00:31:12.686 - 00:31:13.906, Speaker A: Oh, you broadcast it to everyone.
00:31:14.008 - 00:31:15.814, Speaker B: Yeah. It's not just directly signed to the.
00:31:15.852 - 00:31:22.360, Speaker A: So it's a squared communication, it's login, right? No, because every person sends message to every other person. Right.
00:31:26.490 - 00:31:40.140, Speaker B: But it's broadcast. But the destination is a leader. So other validators shouldn't kind of rebroadcast that message.
00:31:42.510 - 00:31:55.300, Speaker A: The second question is, so let's say the leader is a flying. At which point the other validators, two questions. One is how they reelect the leader. And the second question is at which point they do that.
00:31:56.070 - 00:32:33.386, Speaker B: So whenever, after this commit all the validator, let's say. So the first round is finished here and it will come to a new round. You will set up like a timeout counter, let's say, based on how fast the consensus run normally will have a timeout. And if the leader doesn't reach consensus after the timeout, everybody is free to initiate leader change protocol, which will be the same as the traditional PBFT algorithm.
00:32:33.418 - 00:33:20.826, Speaker A: But in traditional PBFT, the person who is proposing a new leader at some point needs to communicate n cubed amount of information for 600, it's almost a gigabyte of information, right. That needs to be communicated like effectively. In traditional PBFT, the leader change, the view change is accumulating. Like every person will send some proof that the consensus wasn't reached to the candidate leader. And the candidate leader needs to send all those proofs to all the validators, right? So the proof itself is quadratic and it is being sent to linear number of people, which is n cubed of information sent. Right. So the candidate leader needs to send 1gb.
00:33:20.938 - 00:33:49.794, Speaker B: So you don't need that much of proof. Right. You only need to say, I think this leader is wrong because I didn't get the correct consensus by the time. And all the other validator, you will broadcast your opinion, right? And if more than two thirds of the people have the same opinion, then the leader is behaving dishonestly.
00:33:49.922 - 00:34:38.150, Speaker A: But that doesn't work because what will happen is that there's a reason why in PbFT there is a quadratic proof. The quadratic proof that you need to send is you need to confirm that everybody is on the same page that we're doing a leader change, because otherwise what will happen is that every validator will send, like the leader will be acting normally up until this point. At this point, everybody will send to the leader their signature parts. At this point the leader has the block and this block has a consensus on it like this block is irreversible, but the leader doesn't broadcast the block. So the leader knows the block. Nobody else does. And so by this time, by the time when the commit is supposed to happen, every validator says, well, consensus isn't reached.
00:34:38.150 - 00:35:05.360, Speaker A: The block wasn't broadcasted. And so it could happen that two thirds of them will message the candidate leader saying, we think that the consensus has stole. The candidate leader will say, I see two thirds of people thinking consensus stalled. They will do their own consensus, come up with some other block, at which point the old leader will broadcast their block. Now, you have two blocks, each of them having two thirds plus one signatures. It's a fault. Right.
00:35:05.360 - 00:35:15.602, Speaker A: So that's not the PBFT consensus. I mean, that's not the BFT consensus. Then the BFT consensus guarantees that you will never have two blocks with two thirds plus one signatures. Does that make sense?
00:35:15.736 - 00:35:20.210, Speaker B: So I don't get. How do you get, like, two block with.
00:35:20.360 - 00:35:37.026, Speaker A: Okay, once again, so this is the current leader. He's malicious. He's acting normally up until this moment. So they do broadcast the block. They accumulate the first set of signatures. They broadcast the signature, accumulate two thirds of signatures here. At this point, they have a block with two thirds of signatures.
00:35:37.026 - 00:36:09.390, Speaker A: That block is final, but they do not broadcast the block from what you described. So this is the first block. This is the first finalized block, but they do not broadcast it. So if I understand correctly how you do view change, then at some point of time, the timeout will be hit, at which point all the validators will think that the consensus stalled. And so all the validators, n minus one, will reach out to some other candidate leader and say, we want to kick out the existing leader. They're acting slowly and start a new consensus.
00:36:09.470 - 00:36:10.146, Speaker B: Oh, I see.
00:36:10.248 - 00:36:13.902, Speaker A: And then the new leader, like the carry out. Yeah. And then you have two blocks signed.
00:36:14.046 - 00:36:23.346, Speaker B: Then it's because the view change is initiated by all the honest validators. Right. So we're going to prefer the opinion of the honest validator.
00:36:23.378 - 00:36:34.250, Speaker A: But how does the beacon chain know about that? The leader will snapshot the header to the beacon chain. The beacon chain is the light client. How does the beacon chain know that it should not accept this block?
00:36:40.430 - 00:36:45.982, Speaker B: In this case, the beacon chain can invalidate that block. That's kind of.
00:36:46.116 - 00:36:56.114, Speaker A: But on which ground? Right, from the beacon chain perspective, there are two blocks, and each of the two blocks has signatures from two thirds plus one owners validators. On which ground will it choose? One of the two.
00:36:56.232 - 00:37:09.526, Speaker B: The view change. Have proof? Have all the cryptographic traits, right? To say, oh, really believe this leader is wrong. Right? So that can be used to distinguish this.
00:37:09.628 - 00:37:51.380, Speaker A: But then the leader, like, what could happen again is that the leader already cross linked the block. There's going to be some delay before the other block gets preference, right? Will it not be the case that malicious actors can already go to some other shard and do some malicious behavior based on the block? That is snapshot at this point? So let's call this block one, the one that was created here. And then after the view change, another leader produced block v two. Let's also say that there is certain transaction which did get into v one but did not get into v two.
00:37:51.910 - 00:37:55.878, Speaker B: Yeah, that's a good point. Yeah, that can happen.
00:37:56.044 - 00:38:27.706, Speaker A: But if that can happen, like effectively, if we're saying that the honest validators can send another block to the beacon chain, do we even need any sort of BFT consensus? Can we just say that the cross link always has, why not send block at this point, right? We got to this point, we send the block to the beacon chain. If beacon chain confirmed it, we're saying, yeah, that's final. And then we're saving a full round of communication. Right. What will break? In this case, you can produce blocks.
00:38:27.738 - 00:38:47.110, Speaker B: Twice as fast, but because we're following the PBFT process, right. You have to have this two phase, whereas the first phase is basically trying to make everybody agree on the order of the block, and then the second phase is actually checking the content and commit.
00:38:47.930 - 00:39:04.026, Speaker A: Well, my point was more of the fact that the fact that two blocks can be produced, that's exactly what BFT is supposed to protect against. Right. So if we're already not protecting against that, we can relax the complexity of.
00:39:04.128 - 00:39:20.458, Speaker B: Yeah, you're right. It's like really a trade off between security and the performance. We can totally use the real original view change protocol to attach all the proof, in which case it will be a lot slower.
00:39:20.554 - 00:39:30.660, Speaker A: But I guess the main question is, if you're committing to using a weekly partially synchronous algorithm, why not use tendermint, which does not have a view change, right.
00:39:31.350 - 00:39:35.458, Speaker B: So tendermint is changing leader every round. Is that true?
00:39:35.624 - 00:39:37.046, Speaker A: The point of tendermint is.
00:39:37.068 - 00:39:37.254, Speaker B: Yes.
00:39:37.292 - 00:40:10.222, Speaker A: Is that the leader is proposing, it's super similar, right? The leader is proposing the block accumulating signatures. Accumulating signatures. But there are timeouts after every step. And after this step, the consensus isn't reached. The next leader. The next leader is proposing the block, and they have certain guarantees that will not allow two blocks being produced, but the view change is gone. Without the view change, it's super cheap, right? Yeah.
00:40:10.222 - 00:40:27.990, Speaker A: Okay. I guess consensus makes sense with the consensus. Another problem is that let's say that the leader is malicious, but they're not stalling the consensus. Instead, what they're doing is they're just producing empty blocks.
00:40:29.370 - 00:41:05.306, Speaker B: That's the filtering question, right? So for the transaction issuer, you should be able to, because it's broadcasted to the network, every other validator will see that whether there is a transaction or not. And if an issuer found that his Transaction did not get included for a long time, or people see that the leader is producing empty block for a long time, then we'll have some criteria to say. At this point, everybody should change the leader. It's malicious, right?
00:41:05.348 - 00:41:31.786, Speaker A: But what if. Let's say that the system is at capacity, like the shard is at capacity, it cannot include all the transactions. Isn't it a problem that the leader has the sole power to choose which transactions get in and which transactions do not? You can prove that your transaction was not included, but the leader has an excuse. The leader says the block was full, right? And so now the leader can censor out any transactions without any penalty, right?
00:41:31.888 - 00:41:59.170, Speaker B: Yeah. I think it's a common question for all the blockchain that's leader based. Leader has more say on which transaction to include. But a leader producing empty block is definitely not should be prevented. But if a leader is acting like at full speed to include most of the transaction, but is kind of including a specific transaction, that will be a lot harder to prevent.
00:41:59.910 - 00:42:31.040, Speaker A: Another question is, how is this timeout chosen? Right? So let's say that for some reason, network conditioning became worse. Let's say timeout was 5 seconds. And let's say that the network conditions became worse. Like, for example, the validators were chosen in such a way that a large portion of them distributed in the world. Or like, let's say the cross ocean link got broken or something like that. Nuclear winter. How will timeout increase? What is the process?
00:42:31.570 - 00:42:47.646, Speaker B: So it's based on the previous blocks, like performance, right? It will be adjusted based on the history network condition. Say, for the last epoch, what's the average block time? And that timeout will be like according.
00:42:47.678 - 00:42:52.930, Speaker A: To that, in the previous epoch, that's the same validators. Or it's a different set of validators.
00:42:54.150 - 00:43:10.700, Speaker B: Like, a major portion of the validator will stay the same, but some of them need to be like, some of them get kicked out, right? Some new join, but it's still like a good estimate for the future time.
00:43:12.270 - 00:43:47.974, Speaker A: So let's say there was a leader who had a ridiculously good network, or even better, here's an example. Let's say there is a leader who has very good network and very good processing power, and so that can execute consensus very quickly to an extent that they finalize the full consensus at, let's say 0.3 seconds. But let's say that the majority of people have worse hardware. And so let's say for them it takes like a second. So let's say this leader produced multiple epochs. In every epoch, consensus was reached in like 0.3
00:43:47.974 - 00:43:59.782, Speaker A: seconds, and then this leader goes offline. And so my understanding is that the timeout will be, let's say 0.5 seconds, but then no leader will be able to produce a block in 0.5 seconds.
00:43:59.926 - 00:44:16.030, Speaker B: So the leader is only part of the process. Right. So most of the delay are actually based on the reply of the validator. So a faster leader doesn't guarantee like a lot faster consensus because it's like.
00:44:16.180 - 00:44:19.754, Speaker A: And this is goship network, right. This is not direct communication.
00:44:19.882 - 00:44:20.560, Speaker B: Yeah.
00:44:21.010 - 00:44:28.226, Speaker A: Cool. Yeah, I think that makes a lot of sense. One other topic I want to discuss is randomness. Randomness is interesting, right?
00:44:28.248 - 00:44:28.930, Speaker B: Yeah, that's interesting.
00:44:29.000 - 00:44:31.170, Speaker A: Is there anything we didn't discuss about consensus?
00:44:32.310 - 00:44:34.340, Speaker B: I think that's mostly it.
00:44:38.090 - 00:45:08.800, Speaker A: Actually. One thing about PBFT is that the proof, why the proof is expensive is that I think every, unless I remember it wrong, I think it's because every validator needs to prove to the candidate that they saw majority of validators wanting to switch. And I think that can be optimized with BLS signatures, and then it will be quadratic, which is way more doable. Okay, so randomness, you guys using the rundown with vdfs, right?
00:45:09.330 - 00:45:12.458, Speaker B: It's not rundown. It's not exactly rundown.
00:45:12.554 - 00:45:14.020, Speaker A: Can you walk us through the.
00:45:14.550 - 00:46:36.250, Speaker B: Yeah. Let's say this is the start of the new epoch, and at the start of the new epoch, the leader will pull from all the validators, let's say this is all the validators, and tell them hey, that we're starting the randomness protocol and it will send the previous block hash. Let's say this is the n minus one. This n the leader will send, let's say this is the leader. Leader announced that he's going to start the protocol with the hash of the n, with the block n minus one. And then every validator will use the VRf to sign on this message and return back to the leader. So the leader waits for f plus one Vrf results and then aggregate this into like, we call it like pre image of the randomness.
00:46:36.250 - 00:47:16.390, Speaker B: So you can aggregate like x or anything. Just combine all of this together and it will create this we call pre image of randomness. And then run consensus, the kind of same PBIP consensus. Just include that in the next block, right. And with all the proofs of the VRI proofs so that this get committed to this block. And then because it's in the block, everybody is free. Or the leader can just compute using the VDF verifiable delay function, right.
00:47:16.390 - 00:47:47.300, Speaker B: It's probably show that you spend some time to compute. You can never get a result instantly. So you input that pre image of the randomness into this VDF and then generate the true randomness. So we'll configure this Vdf so that it's at least more than one block time, right? So let's say it was configured to k block time.
00:47:49.030 - 00:47:51.220, Speaker A: What is the order of magnitude of k?
00:47:51.990 - 00:48:26.240, Speaker B: It can be like ten or something like that. It doesn't have to be that large. Say this is the n plus k block. Somewhere around the n plus k block you get the true randomness, and then you commit that, you run consensus again with all the proofs. And people will verify that, right? And we'll confirm, we'll sign and you get committed to the block. So right here you get the randomness for all the sharding or anything.
00:48:26.610 - 00:48:30.542, Speaker A: The question is, did you guys think which particular VDF to use?
00:48:30.676 - 00:48:33.410, Speaker B: We'll probably go with the Ethereum one, right?
00:48:33.480 - 00:48:49.654, Speaker A: So with the Ethereum's VDf, they build an asic, right? Which is going to be, give or take, 100 times faster than anything you could have, like traditional hardware. So is the expectation that the leader has to have the, we can, we.
00:48:49.692 - 00:49:08.970, Speaker B: Can configure the time, the VDF delay time so that it will take care of that ASIC at the beginning. If, say, the leader doesn't have the ASIC, right, we'll make it even longer.
00:49:09.120 - 00:50:13.726, Speaker A: But that means that the k needs to be, well, at the very least more than 100, right? Because if the ASIC is 100 times faster than traditional hardware, but also ASIC is 100 times faster than very good hardware, right? So if you're running on regular, like, let's say you don't have a super strong hardware, it's going to. But the bigger problem is in Ethereum's case, because they use exactly the same approach, right? I'm not sure about the signature, but they have precommits from multiple people. Then they compute the VDF and the output of the VDf on the aggregate of the pre commits. And the output of the VDF is randomness. In Ethereum's case, it is the expectation that the VDF is going to be run by a person with an ASIC, right? So it is not expected that anyone will run VDF without the ASIC. As a result, for someone to, and let's say k is 100, right. As a result, for someone to build faster, they will need to have an even faster ASIC, right? And the ASIC that Ethereum is building costs $15 million to build.
00:50:13.726 - 00:50:54.250, Speaker A: So to build even faster ASIC, it will cost like even more. And how much faster it's going to be, you need 100 times faster, which is barely impossible in this case. The problem is that, let's say the harmony becomes the protocol, right? So someone actually has a very strong incentive to break it. So if K is configured to be, let's say, 200, so you're saying Ethereum Isaac is 100 times faster. We're going to configure K to be 200. There is no way that Ethereum Isaac is going to beat us. Now, for someone who wants to break the randomness, for them, the cost is going to be building an ASIC, which is 300 times faster, which is expensive.
00:50:54.250 - 00:51:24.018, Speaker A: But we're talking about whichever protocol is going to be the protocol. We're talking about like $100 billion protocols, right? So what is 50 million invested? But one thing is that you don't have the leader to run DVD. You don't need the leader to run dvd. Anyone can, I guess that's a partial solution. Cool. And is it also the randomness which is going to be like the smart contract? Will it have access to the randomness?
00:51:24.194 - 00:51:46.750, Speaker B: We haven't decided that. Yes, because you don't have a defined period for when you're going to get the randomness, right? So you may not get randomness for each block. So that's a question we're still researching, but this is definitely used for the network sharding, because this only happened every epoch.
00:51:47.250 - 00:51:51.294, Speaker A: I see. And the epoch cannot rotate faster than k blocks, right?
00:51:51.332 - 00:52:00.274, Speaker B: Yeah. So this will be at most like ten minutes. So our EPA will be longer than ten minutes.
00:52:00.472 - 00:52:21.558, Speaker A: And also what happens if, imagine I'm executing a cross shard transaction. What happens if we already make an assumption that it is hard but possible to corrupt one third? Right. So it's possible to make a fork.
00:52:21.654 - 00:52:21.914, Speaker B: Yes.
00:52:21.952 - 00:52:23.242, Speaker A: So what happens if in a short.
00:52:23.296 - 00:52:27.030, Speaker B: No one third doesn't create forks, right. One third only stall.
00:52:27.190 - 00:52:50.702, Speaker A: One third has a theoretical chance to create a fork. Because what you can do is you can. Well, I will provide theoretical what you can do in theory. In practice, it's hard, but it gives you the theoretical ability effectually. What you can do is you can. This is like slightly less than one third of honest validators. This is slightly less than one third of honest validators.
00:52:50.702 - 00:52:55.786, Speaker A: And this is your slightly more than one third of malicious validators.
00:52:55.918 - 00:52:59.720, Speaker B: You can double sign on.
00:53:02.490 - 00:53:37.870, Speaker A: This is not practical because you cannot really split the network. But it gives you at two thirds, you can definitely fork. At one third below one third, you definitely cannot fork. But exactly how it grows from zero to one in between, we don't know, right? But let's say we believe that fork can happen, because, I mean, we cross link. The sheer fact that we cross link effectively means that we believe the fork can happen. Right? Then that means that the beacon chain can fork. Right? Because the beacon chain has only twice as many validators, so it is also feasible to corrupt them adaptively.
00:53:37.870 - 00:54:22.330, Speaker A: So could the following happen? Could it be that I'm in one shard? This is shard one, and it's building its blockchain. And this is shard two. Can I do this? Like, let's say I'm an adaptive adversary that is capable of forking. Then what I can do is I can create a block here in which I send money to the merchant, create a block on the beacon chain where it got cross linked. And so a merchant receives money over here, right? And the merchant receiving money here provides me the service. But after that, I'm forking here. And this happens to be the canonical chain.
00:54:22.330 - 00:54:43.106, Speaker A: I'm forking here. And this happens to be a canonical chain. What happens is we have a double spent, right? Because in this chart, there is no reason for this not to stay. The canonical chain, there was no forks. Right? And so money wasn't spent. In the canonical chain, money wasn't spent. But on the destination shard, money is received.
00:54:43.106 - 00:54:46.760, Speaker A: Right. So is it something that's possible?
00:54:47.470 - 00:54:50.906, Speaker B: I think in our security assumption, this shouldn't happen.
00:54:51.008 - 00:54:53.830, Speaker A: So the assumption is the beacon chain never forks effectively.
00:54:53.910 - 00:55:09.600, Speaker B: Yes, I know. This is more like the fork choices in the POW or Pos consensus, where they're easily forked, you have to continually resolve forks, right? But in your network, this shouldn't be a problem.
00:55:09.970 - 00:55:36.326, Speaker A: But that gets back to my argument. So my argument is the following, that one of the two things should be true. Either one forks are possible, in which case this can happen, which is a double spend or argument number two, you don't need cross links. Does that make sense?
00:55:36.428 - 00:55:50.060, Speaker B: So the beacon chain won't have forks. Right. We will guarantee with the high probability that beacon chain won't have forks because we're going to push more staking on that.
00:55:50.430 - 00:56:10.866, Speaker A: Right. The only concern is that the question is how much more stake you put on the beacon chain. If it's twice as much, and we're talking about an adaptive adversary, then the attack is only three times more expensive. Because if it costs x to corrupt this short, it costs two x to corrupt this short. Right. So that has to be exactly more than two.
00:56:10.888 - 00:56:26.086, Speaker B: I guess we haven't decided on the exact number. I'm just giving an example. It can be two or it can be like five. It depends on the correct analysis. So before the minute we'll have a clear analysis on how much the stake should be.
00:56:26.108 - 00:56:33.500, Speaker A: Makes sense. Awesome. And are there any other aspects of harmony you'd like to discuss?
00:56:34.110 - 00:56:38.826, Speaker B: Let's see, what else do we have enough. Do we still have time?
00:56:39.008 - 00:56:41.418, Speaker A: We have time to discuss one more component. Yeah.
00:56:41.584 - 00:56:42.940, Speaker B: Which area you.
00:56:43.650 - 00:56:47.454, Speaker A: I don't have any other technical questions. We can talk more about interesting stuff.
00:56:47.572 - 00:56:58.382, Speaker B: Okay, so what else? Maybe talk about the network.
00:56:58.526 - 00:57:04.820, Speaker A: Oh, yes, that's your signature technology, right?
00:57:05.190 - 00:57:10.360, Speaker B: Yes, we're optimizing on the network with some of the technology.
00:57:11.130 - 00:57:18.220, Speaker A: So you guys have a blog post about block propagation in 1 second, right? 1.3, yeah. Can you walk us through how that works?
00:57:20.110 - 00:58:29.886, Speaker B: So in the traditional peer to peer broadcasting, let's say this is the sender, this is the receiver. This is the receiver. So the sender will send a copy of, say this is the message m. The sender will send a copy of the message to each of the neighbor, and each of the neighbor will gossip again to the other neighbor. So this will incur like say we have D neighbor, we will have like m plus D network consumption on the sender, right? So when the block is large, say like a megabyte of the block. So it will be a bottleneck for the sender. So what we did is we are using the usage coding usual code to encode the block m with some extra usual code, say e.
00:58:29.886 - 00:59:05.370, Speaker B: And we're going to cut this into chunks and send like equally send to the neighbors, to the D neighbors. So each of them only have a part of the encoded block. So this will incur like this m plus e load where basically e is 30% or some percent of the block size. So it's like significantly reduced on the load of the sender.
00:59:06.830 - 00:59:09.126, Speaker A: And then how do they recover the block?
00:59:09.318 - 00:59:18.366, Speaker B: So this block chance. Similarly, these validators need to send, but.
00:59:18.388 - 00:59:21.146, Speaker A: They'Re also sending among themselves because it's.
00:59:21.178 - 00:59:36.134, Speaker B: Missing some of them. Right. It's the same because right here it's sending the whole block. So this guy already has the block he doesn't need to sign, but here it's sending the chunks. Right. Each of them are different. So this is the first and this is the second and third.
00:59:36.134 - 01:00:01.870, Speaker B: They also need to communicate to get a full picture. But when one node gets m over m plus e portion of the chunks, you can decode that chunks into the whole block.
01:00:03.170 - 01:00:08.090, Speaker A: And e exists here because we assume that some of these people will not be communicating.
01:00:08.170 - 01:00:12.110, Speaker B: Yes, because we assume like one third malicious.
01:00:12.690 - 01:00:21.998, Speaker A: And if I recall the blog post correctly, you were using fountain codes, right? Yeah, because of UDP or why, what's the trade off? Because just using razor codes.
01:00:22.094 - 01:01:11.854, Speaker B: So the traditional azure code is based on the risk Solomon encoding, which is a fixed encoding, say you can only add like let's say it's a fixed parameter, say 30% of actual azure code. And if in the rare case that's not kind of enough, then some node won't get the reconstructed block. So instead we're using the Raptor queue fountain code. Fountain code, which is more like a continuous adaptive encoding. So at first you can have like 30% and you can actually generate additional pieces a little bit by a little bit. And for those additional pieces it can also be part of the reconstruction.
01:01:11.902 - 01:01:13.298, Speaker A: And this is infinite, right?
01:01:13.384 - 01:01:32.614, Speaker B: Yeah, you can continuous generate. So for the sender if it doesn't like, if some of the validator are not getting the block, saying it's not getting response of the signature, you can keep kind of sending more blocks so that it may have a better probability that receiver received the whole block.
01:01:32.662 - 01:01:36.570, Speaker A: And the moment they have like m or m plus one, they can recover. Right?
01:01:36.640 - 01:01:38.266, Speaker B: Like any, yeah.
01:01:38.448 - 01:01:54.590, Speaker A: Awesome, great. Well, I think we can conclude the technical part here. Now let's have just very few non technical questions at the end. Would you be comfortable sharing? When is the main net launch?
01:01:55.570 - 01:02:00.362, Speaker B: We're planning to launch the main net in the middle of this year, around June and July.
01:02:00.506 - 01:02:02.590, Speaker A: Exciting. So when is the testnet then?
01:02:02.740 - 01:02:06.646, Speaker B: Testnet will be like in two months from now. Great.
01:02:06.748 - 01:02:13.506, Speaker A: Okay, so excited. Looking forward to it and thanks a lot for sharing the knowledge.
01:02:13.618 - 01:02:14.920, Speaker B: Okay, thank you.
