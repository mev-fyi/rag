00:00:02.970 - 00:00:15.774, Speaker A: Okay, welcome to another whiteboard series. I'm Bowen from Nier, and today we have with us Yi, co founder of Axiom. So, yi, do you want to introduce a bit of yourself and also talk a bit about what axiom is?
00:00:15.892 - 00:00:28.710, Speaker B: Hey, my name is Yi. I'm one of the co founders of Axiom. We allow smart contracts on Ethereum to access the entire history of Ethereum and do verifiable computation on top of it using zero knowledge proofs.
00:00:29.290 - 00:00:58.382, Speaker A: Okay, cool. So I guess maybe just to give people some background. So for those who don't know, e is definitely one of the most prominent figures in the zero knowledge space and has been working on blockchain related things for many years. Given there are so many things that zero knowledge proofs can do, why specifically do you choose to work on Axiom versus some other things like layer twos or whatever that is?
00:00:58.516 - 00:02:23.340, Speaker B: Yeah, as Bowen mentioned, one of the biggest use cases for ZK until now has been building roll ups and roll ups, generally scale Ethereum by allowing smart contract developers to build similar types of applications to those which exist on l one, but just at greater scale on l two. And so when coming up with the idea for Axiom, our north star has always been to allow smart contracts to do new types of things that weren't possible on l one before, not just because of scale, but because they're fundamentally a little bit different. And with axiom, what we're doing is just building a type of system that allows smart contracts to make asynchronous calls, to read more data and then do compute on that data. So to start, we're offering access, as I mentioned, to the history of Ethereum, and then we're allowing developers to do compute on that history. So maybe I can draw a little. Yeah, so in general, we have sort of the axiom contract, and later on we'll expand to this, to the real architecture. So for now, cartoon and so a developer will make a query, and this query will require some access to some historic data.
00:02:23.340 - 00:03:41.748, Speaker B: So imagine a receipt or a transaction, and then it will also access some computation over that data. You might imagine adding up a user's trading volume. And so what axiom is going to do is generate a ZK proof that whatever claimed data value or computation actually authentically belonged to the history of Ethereum. And after this proof is generated, it's going to send this proof to a verifier on chain. And once that zero knowledge proof is verified, we actually send a callback to the developer's contract. And so in this way we allow developers to actually access sorts of data that weren't possible to access before within the EVM.
00:03:41.924 - 00:03:42.312, Speaker C: Yeah.
00:03:42.366 - 00:03:55.308, Speaker A: So I think one obvious question people would have is probably regarding this part, right? So I think the natural question people want to ask is, well, why couldn't we just do it as is today? Where does they can help?
00:03:55.474 - 00:04:46.296, Speaker B: Yeah, so that's a great question. So I think unless you've tried to develop smart contract before, you probably wouldn't know that within the EVM. And actually within most smart contract development languages, you can't access any type of historic information. So that means the previous owner of an NFT or any type of transaction history just isn't present in the state of Ethereum or any other blockchain. And there's actually a fundamental reason for that, which is that suppose we just changed Ethereum so you could access any historic state. That would mean that every validating node on Ethereum would need to be able to access historic state extremely quickly in order to validate a transaction. And so that would mean that they would have to turn every full node into an archive node, which definitely hurts the decentralization of Ethereum.
00:04:46.296 - 00:05:46.930, Speaker B: So there's sort of a fundamental trade off between increasing the set of data that contracts can access and decentralization. So as Mon asked, how does ZK help this situation? Well, the answer is that when you currently run a transaction on Ethereum, we're relying on the Ethereum consensus to validate your data access and your compute instead. We're replacing that consensus here with ZK. So ZK uses cryptography instead to do that. So let me explain how that works. So first of all, why is that even possible? We're going to use actually the simplest possible property of a blockchain. Okay, so a blockchain is a chain of blocks.
00:05:46.930 - 00:06:59.380, Speaker B: And what that means is that the current block, well, it contains a commitment to the previous block, the block before, and actually the entire history of the blockchain. And so what that means is suppose we wanted to access a piece of data from this historic block. Well, in principle, this historic block is actually committed to in the current block. So we could supply a decommitment proof of this chain of block headers from a past block to the current block. And so what would that decommitment proof look like? It would simply say that, hey, I know, a sequence of block headers, let's say this intermediate one, and this intermediate one such that the hash of this block is contained in the header of this block. The hash of this header is contained in the header of this one and so on. So if I could exhibit that to you, then you would know that this historic block was authentically present in the history of this chain.
00:06:59.380 - 00:07:46.880, Speaker B: Now that in principle allows you to know the entire block history of Ethereum. And within the historic block, well, we actually have a commitment to the entire state of Ethereum at that block. So that takes the form of the states route, the transactions route, and the receipts route. So the state route commits to all accounts and account storage. The transactions route, of course, commits to the transactions in this block. And the receipts route commits to the receipts or log events in this block. So again, we could just decommit.
00:07:51.480 - 00:07:51.844, Speaker C: And.
00:07:51.882 - 00:07:53.990, Speaker B: Maybe we would get a receipt here.
00:07:57.480 - 00:08:21.388, Speaker A: I think one problem here is that if the decommit chain is very long, if we're trying to prove something like 1 million blocks ago, this structure by itself may become some problem. My understanding is that Ethereum blockchain by itself doesn't have the Merkel mountain thing that built in. So need to build some structure itself, I guess.
00:08:21.474 - 00:08:48.150, Speaker B: Yeah, that's exactly right. So if you think about actually performing these decommitments in the EVM, you're going to run into sort of two classes of challenges. So the first is that challenge one is the chain could be huge. So if you just want to put the block headers on Ethereum, that's going to be way too much data and your costs will explode. The second challenge is that.
00:08:51.740 - 00:08:52.216, Speaker C: Let'S call.
00:08:52.238 - 00:09:39.924, Speaker B: This decommitment number two. The decommitment number two for actually proving data from each block requires something called a merkel. Patricia triproof. So this is a Merkel proof into this degree 16 Merkel tree that Ethereum uses to commit to all data. And the problem with that is that it's also huge. And so if you actually want to do this on Ethereum, just in the raw evm, your cost is going to be exorbitant. So we have sort of two techniques at axiom to resolve these two problems.
00:09:39.924 - 00:10:23.260, Speaker B: So the first pertains to this first step of decommitting the chain of block headers. So what we do there is that, as Bowen mentioned, sometimes this chain could be like quite long. So let's say that this is not four blocks in the past, but let's pretend that this is 1 million blocks. Well, in this case, you have to exhibit 1 million block headers, which is not great. So what we do instead is we actually maintain a cache on Ethereum that commits to the entire history of Ethereum block hashes. So we call this axiom core.
00:10:26.640 - 00:10:27.004, Speaker C: And.
00:10:27.042 - 00:11:27.842, Speaker B: This is a cache of all Ethereum block caches. Now, when I say a cache, I don't mean that we literally store them all, since obviously accessing them is pretty costly. Instead, what I mean is we have a commitment to all block caches, and we want that commitment to actually satisfy two properties. The first is that we want updates to be cheap, namely more block hashes are postpended and we need to update the cache. The second is that we want the commitment of, effectively, of one size. I say effectively because in reality this is going to be log n size. But n login doesn't grow that fast for a blockchain.
00:11:27.842 - 00:11:44.908, Speaker B: And so what we use is something called a Merkel mountain range. And we actually use a small twist on a Merkel mountain range, which I'll explain in a little bit.
00:11:45.074 - 00:11:47.004, Speaker A: Yeah, maybe let's actually dive into.
00:11:47.042 - 00:11:48.430, Speaker B: Yeah, let's dive into what?
00:11:48.880 - 00:12:03.570, Speaker A: I don't think this is a very well known data structure that people use. And actually, even though we have this building to near, I think that was like at least three years ago at the time. I didn't know. This is like, there's like a name to this.
00:12:04.260 - 00:12:06.768, Speaker B: Yeah, I think I remember talking to Ilya about this.
00:12:06.934 - 00:12:08.630, Speaker C: Two years. Yeah.
00:12:09.560 - 00:12:48.616, Speaker B: Okay, so to explain what a Merkel mountain range is, maybe I'll draw a picture of all block hashes on Ethereum, starting from Genesis. So this is block number. So we have block zero to. I think we're on like 17 million now, maybe 18 million. Let's just say 17 million. So a Merkel mountain range is a structure which allows you to commit to any number of hashes while taking only log n commitment size. And it has these two properties that updates are relatively cheap.
00:12:48.616 - 00:13:25.200, Speaker B: The commitment is relatively small, and you can actually decommit any block hash in this interval in a cheap way. So here's how it works. It's basically just a sequence of Merkel trees, hence a Merkel mountain range. So first, you're going to fit the largest binary Merkel tree you can into this range. So in this case, we take the leaves from zero to. Let's imagine this is roughly 16 million. That's going to be the value my diagram is going to be.
00:13:25.200 - 00:14:11.662, Speaker B: Not so good. First, we chose the largest Merkel tree we can fit into this range, and we take the Merkel root. So this will be peak zero of the Merkel mountain range. Then we repeat, we take another Merkel tree that's as big as possible, and this will be peak one, and we just keep going and so on. So a Merkel mountain range is just a sequence of peaks, of Merkel roots, of Merkel trees, of descending size. Now, what's the benefit of this? Well, as you can see, the whole commitment is of size. Log n in particular.
00:14:11.662 - 00:14:58.666, Speaker B: It's the number of ones in the binary representation of the total number of blocks. The second thing is, as we append blocks here, you can sort of see intuitively that you're only going to modify those peaks which change. And that size is pretty small. So it has these two properties. And so if we want to prove that some block hash was in the history of Ethereum, all we need to do is give a standard Merkel proof into one of the peaks, and that would show, sort of, that it was committed to in this Merkel mountain range. So now I want to talk a little bit about a slight tweak we make to address this update issue. So what we actually store is not quite a Merkel mountain range.
00:14:58.666 - 00:15:22.630, Speaker B: We actually store a Merkel mountain range of Merkel roots it of 124 block hash chunks.
00:15:25.050 - 00:15:26.520, Speaker C: Okay, that's interesting.
00:15:26.970 - 00:16:06.820, Speaker B: And in addition to this, we always store the trailing 124. Sorry, how to say, we store the Merkel roots of a zero padded merkel tree of size 24, holding the last few leaves, last few hashes.
00:16:08.280 - 00:16:10.804, Speaker A: And what does the last few hashes mean?
00:16:10.842 - 00:16:33.804, Speaker B: Yeah, so. So this will store from zero to 124 times k for some k. Okay. And this will store from 1024 times k to 1024 times k plus m, where I see m is the trailing segment.
00:16:33.932 - 00:16:34.816, Speaker C: I see.
00:16:34.998 - 00:16:44.000, Speaker B: So what we do is that we will keep this Merkel route updated until such time as we need to update the full Merkel mountain range.
00:16:44.760 - 00:16:45.460, Speaker A: Makes sense.
00:16:45.530 - 00:16:45.860, Speaker C: Yeah.
00:16:45.930 - 00:17:08.164, Speaker B: So the reason for this is that in Ethereum, it's relatively expensive to update storage. So if you think about blocks being appended, these last ten peaks of the Merkel mountain range, to account for up to 1024, they get updated quite a bit. So instead of actually updating them, it's much simpler for us to just update this Merkel route.
00:17:08.212 - 00:17:08.970, Speaker C: I see.
00:17:10.220 - 00:17:14.108, Speaker A: And then once you get to the next one, you merge it back into the.
00:17:14.194 - 00:17:29.680, Speaker B: Yeah, exactly. So once we get to the next one, we merge it back into this Merkel mountain range. So we call this a padded Merkel mountain range, but this is sort of a non standard data structure. So we have a Merkel mountain range for almost all of it, and then we have this zero padded Merkel route at the end.
00:17:29.750 - 00:17:32.000, Speaker A: How much does this save you in practice?
00:17:33.060 - 00:17:53.624, Speaker B: It's quite a bit. Basically, suppose you do an update for, let's say, 200 block hashes, then an expectation you're going to update eight of the Peaks mountain. So that's eight s stores. They're warm, but.
00:17:53.662 - 00:17:56.632, Speaker C: Okay, that's quite a see. I see.
00:17:56.766 - 00:18:48.680, Speaker B: Okay, so far we've just talked about the data structure we're using, right. And these updates, in principle, could be done in EVm. But now, in practice, doing the updates in EVM has sort of two challenges. So, as we see, we have this chain of blocks, and let's indicate here which blocks are actually already committed to in our Merker mountain range. So let's say that this is in the Merkel mountain range is abbreviated MMR. So as well as we're in the Merkel mountain range and now some new blocks have occurred and we want to update. So the fact we can use is that in Ethereum, you actually have access to the last 256 blocks.
00:18:48.680 - 00:19:37.120, Speaker B: Ball caches. It's a bit of a mystery to me why it's 256. I think there's no actual issues if you make it all block caches, to be honest. But we have to work with what we have. So we're currently at 256. And so what we do is we use the fact that we have a single recent one. Okay, so now you might imagine that in EVM you can just access the last 256, do some merkel root computations in EVM, and start appending to this data structure.
00:19:37.120 - 00:19:54.468, Speaker B: The challenge with that is that suppose you just don't manage to do the update in a 256 block window. So that's sort of 45 minutes. Well, now you're kind of in trouble, because how do you connect back to your McCollum round merge? So that's not very robust system.
00:19:54.554 - 00:19:55.092, Speaker C: Right.
00:19:55.226 - 00:20:06.410, Speaker B: So instead, what we do is we just choose this recent one and we generate a zero knowledge proof of the chain of block hashes back to the last multiple of 1024.
00:20:07.500 - 00:20:08.472, Speaker C: Oh, I see.
00:20:08.606 - 00:20:20.606, Speaker B: So let's not so clear how to denote that, but let's say it's like this. So we do this in ZK.
00:20:20.798 - 00:20:21.780, Speaker C: I see.
00:20:22.390 - 00:21:03.230, Speaker B: So what that means is that in the typical happy path, if we're within this 256 block hash window, we check that this chain of block headers connects to each other, that the last block hash is in this 256 block window. And then we check that this starting point actually aligns with the endpoint of our Merkelman range. Now, suppose that we missed some. Then what we can do is we first prove this chunk of up to 1024 and we can sort of backfill with chunks of 1024. So no matter what happens, we can always get back to this where we were in the Merkel mountain range.
00:21:03.570 - 00:21:06.346, Speaker A: And in practice, how often do you do this update?
00:21:06.458 - 00:21:09.294, Speaker B: So in practice, we do it every 192 blocks.
00:21:09.342 - 00:21:10.494, Speaker A: 192 blocks.
00:21:10.542 - 00:21:12.798, Speaker B: And we could probably get a little bit more aggressive.
00:21:12.974 - 00:21:16.478, Speaker A: And how long does it take to generate this zero knowledge proof?
00:21:16.574 - 00:21:19.670, Speaker B: Oh, it's pretty fast. I think two minutes or something.
00:21:19.740 - 00:21:20.600, Speaker C: Oh, okay.
00:21:22.250 - 00:21:26.360, Speaker B: That's with some cost saving. If you really wanted to, you could do lower.
00:21:26.730 - 00:21:27.480, Speaker C: Okay.
00:21:27.850 - 00:21:29.810, Speaker B: Yeah, we rerun on, like, not a great machine.
00:21:29.890 - 00:21:30.760, Speaker C: I see.
00:21:31.630 - 00:21:49.150, Speaker B: Our philosophy is we always want to maintain the property that this commitment with the Merkle mountain range always overlaps the 256 block hashes. And that way the cache will always commit. The cache plus the EVM block hash opcode always commits to the entire history of Ethereum.
00:21:51.730 - 00:21:54.420, Speaker C: Okay, that makes a lot of sense.
00:21:56.230 - 00:22:05.714, Speaker B: There's maybe one more thing which we didn't discuss, which is when we deploy the contract, there's nothing in it. What do we do then? We have to build this from scratch.
00:22:05.762 - 00:22:06.360, Speaker C: Yeah.
00:22:07.290 - 00:22:25.094, Speaker B: Okay, so we actually have a separate way of bootstrapping, which proves the block hashes in chunks of 128,000. We start. Yeah, maybe we erase.
00:22:25.142 - 00:22:27.500, Speaker A: Erase some part, like, yeah, I'll erase here.
00:22:33.540 - 00:23:04.344, Speaker B: Suppose we start again at 17 million. We're going to prove this is 128 times 1024. Then we prove 128 times 1024, and we just keep going back. And, okay, there's some complications surrounding that. 17 million is not a multiple of 1024. But we just handle this, and we eventually get here. So now we have sort of in groups of 128 times 1024.
00:23:04.344 - 00:23:13.050, Speaker B: We actually write the groups every Merkel route of 1024, and then we construct the Merkel mountain range in EVM, actually.
00:23:13.360 - 00:23:13.964, Speaker C: I see.
00:23:14.002 - 00:23:16.028, Speaker A: But that is quite expensive to do, right?
00:23:16.114 - 00:23:22.508, Speaker B: That construction, it's like, okay, it's definitely a little bit expensive. I think it costs like 20k total.
00:23:22.594 - 00:23:22.988, Speaker C: Okay.
00:23:23.074 - 00:23:39.012, Speaker B: But it just simplifies the system so that we don't have to think about. Some parts of the system have chunks of 128,000, other parts have chunks of 1024. Yeah, we could definitely probably cost optimize by dropping some of these. But it makes sense. Yeah, the cost is not crazy. So we kind of just do it.
00:23:39.066 - 00:23:39.670, Speaker C: Okay.
00:23:40.040 - 00:23:50.760, Speaker B: So, yeah, we have to do, I think it's something like 130 of these. And then once we have bootstrapped the system to this state, all we need to do is maintain it in this way.
00:23:50.830 - 00:23:51.400, Speaker C: Right?
00:23:51.550 - 00:24:02.252, Speaker A: So basically, in this entire part of this chain proof. The only part where you use decay is like this part of maintaining the Merkel mountain range in light of the.
00:24:02.306 - 00:24:09.176, Speaker B: More recent blocks, both the recent blocks and this historic bootstrapping.
00:24:09.288 - 00:24:10.350, Speaker C: Okay, sure.
00:24:11.840 - 00:24:23.200, Speaker B: So we obey the principle that a user of our system shouldn't need to trust us really in any way. Like if something has been posted, it really has been verified either in EVM or by CK.
00:24:23.780 - 00:24:29.360, Speaker A: Yeah, makes sense. Okay, so should we move on to the other part of the proof?
00:24:30.340 - 00:24:30.704, Speaker C: Cool.
00:24:30.742 - 00:24:35.250, Speaker B: So so far we have this cache and we sort of maintain it to have this property.
00:24:37.660 - 00:24:38.410, Speaker C: It.
00:24:47.580 - 00:25:51.340, Speaker B: All right, I'll just redraw it. Now, the next step is that we actually need to access the history of Ethereum from this cache. So let's say we're in this situation, and let's say that the MMR committed to this much so far, and this is axiom core. And so this is just a vanilla smart contract on Ethereum. So now we want to actually ask like, hey, I want a transaction from this block, I want a storage value from this block, and so on. So the way that we allow users to express this is in something we call a query. So what's a query? Well, okay, it's a couple of things.
00:25:51.340 - 00:26:39.100, Speaker B: It's a list of data that the user wants. And so we allow pretty arbitrary data. So we format the data as just 32 byte chunks from the history of Ethereum, and they can come from block headers, accounts, storage transactions, receipts. And we're also handling solidity mappings. Okay, so it's a subset of storage, of course. But as you probably know, the mapping of solidity key value pair to storage slot is a bit complex, and it's actually better to handle ZK.
00:26:39.920 - 00:26:40.764, Speaker C: I see.
00:26:40.882 - 00:27:20.168, Speaker B: And so our principle is that anything in the history of Ethereum is parameterized in this way. And, okay, we have some pretty long complicated lists of how it's parameterized. And so the user can specify basically these 32 bytes at a time. So for example, if you want to read transaction call data, we let you just specify, hey, I want this index to this index in multiples of 32. Similarly for receipts you can access, I want the first log I see, and I want this data field in that log and so on.
00:27:20.254 - 00:27:23.530, Speaker A: And what's the reason why it has to be 32 bytes at a time?
00:27:25.180 - 00:27:29.608, Speaker B: We let you do multiple queries, right. This is just a way to make your query more sensible.
00:27:29.704 - 00:27:30.108, Speaker C: Okay.
00:27:30.194 - 00:27:35.500, Speaker B: So you can definitely write some sort of wrapper language around it that concatenates these queries.
00:27:35.580 - 00:27:36.304, Speaker C: Right?
00:27:36.502 - 00:28:41.316, Speaker B: Okay, so there's this data side of the query. And then we also allow users to specify compute. So we talked with a lot of users about what type of compute they want, and we decided that the answer is that they kind of just want arbitrary compute. So we created the system that you can actually plug in your own ZK circuit to specify the computation you want over this data, so you can. And so I think one thing that's maybe not so obvious is that if you want to use on chain data for your application, even if you don't think you're using compute, you probably are using some small amount of compute. So just to give an example, suppose you want to find a user's average ethereum balance over the last, over five blocks at some interval. Then what you'd want to find, let.
00:28:41.338 - 00:28:41.910, Speaker C: Me.
00:28:44.920 - 00:29:36.420, Speaker B: Say here's suppose we want to take the average with a ten block gap and with over five blocks. So what you would fetch in the data query is sort of the balance of the address at block, the balance of the address at block plus ten, and so on. So you might ask like what's the computation? Okay, of course we have to average these numbers, but what you also have to do is constrain that the block that you're putting in here really differs by ten.
00:29:36.490 - 00:29:36.820, Speaker C: Right.
00:29:36.890 - 00:29:44.152, Speaker B: And there's all sorts of these sort of bookkeeping constraints. And so we allow users just to specify this in an arbitrary way in the computation side.
00:29:44.206 - 00:29:53.596, Speaker A: Yeah, but I guess from user point of view they're not really writing a circuit directly. Right. Because if they do, then probably many people would actually do it.
00:29:53.698 - 00:30:07.180, Speaker B: Yeah. So we created a Javascript front end for users to, they are writing a circuit, but for this sort of circuit, it's not quite what you think of as writing a circuit. You're literally just constraining some equalities.
00:30:07.260 - 00:30:09.264, Speaker C: Right, I see.
00:30:09.462 - 00:30:23.830, Speaker A: But I imagine if they actually want to do a lot more complex computation, then it might actually get more involved. I don't know, what's a good example? Like they want to compute some complex function over some data.
00:30:24.200 - 00:30:32.052, Speaker B: Yeah, that's right. We think that the computations are going to split into two categories. One is just sort of this bookkeeping stuff.
00:30:32.106 - 00:30:32.372, Speaker C: Right.
00:30:32.426 - 00:30:35.080, Speaker B: And we think that a user can write that with no issue.
00:30:35.150 - 00:30:35.624, Speaker C: Sure.
00:30:35.742 - 00:30:44.808, Speaker B: But of course if you want to write, let's say a machine learning model or something complicated, then yeah, you need to use something different. You need to plug in something not written in this sort of framework.
00:30:44.904 - 00:30:45.356, Speaker C: Right.
00:30:45.458 - 00:30:49.692, Speaker A: And then do you have like a separate framework for that? Or what's the plan for that?
00:30:49.746 - 00:31:13.924, Speaker B: Yes, and I'll explain, maybe the plan for that comes in when we connect these two. Let's just assume that we can prove the data and we can prove the computation. So the way our system works is that within the computation, I'll erase this.
00:31:13.962 - 00:31:14.836, Speaker A: Yeah, erase that part.
00:31:14.858 - 00:31:15.430, Speaker C: Yeah.
00:31:19.880 - 00:31:44.540, Speaker B: So the idea is that here's the computation, and this is actually on a user's laptop. So we have sort of the computation, which is pretty trivial. Like we have block, block, block plus ten and so on, right? And then we have a bunch of sort of requests.
00:31:46.580 - 00:31:46.944, Speaker C: Right?
00:31:46.982 - 00:32:44.940, Speaker B: So in this request you would ask for, hey, here's, give me the balance, here's a request for the balance and so on. Then what the user does is actually send a proof of this, which carries these requests to our axiom query contract. So we index this contract and we glue this together with the fulfillment of those requests with data proofs. In here we do compute plus data. So what I mean by that is that the user is saying, making some claims about the history of ethereum, like, hey, my balance at this block was whatever was one. My balance at ten blocks later was ten and so on. So these are just assertions in this compute proof.
00:32:44.940 - 00:33:03.610, Speaker B: And what we do is we provide ZK proofs for those assertions in this data part. And then we also check that the assertions we provided are compatible with the request, and we wrap all of those together into a single proof and we feedback that back here.
00:33:04.060 - 00:33:04.810, Speaker C: Right.
00:33:05.580 - 00:33:17.468, Speaker B: Okay, so how does that answer your question about the more complicated computations? Our vision is that the user can just start adding more requests. So there's nothing about the request that is specific to the fact that it's data.
00:33:17.554 - 00:33:17.852, Speaker C: Right?
00:33:17.906 - 00:33:23.864, Speaker B: It could be, let's say, oh, I see, anything complex.
00:33:23.992 - 00:33:24.284, Speaker C: Right.
00:33:24.322 - 00:33:40.310, Speaker B: And so in general, we think that there should be this division between what's happening on the client side and the server side, right? Really, it's more like what's happening on the user interface side versus the Zk back end side.
00:33:42.040 - 00:33:51.192, Speaker A: And actually here, why does this part needs to happen on the user side? Because all the data is public.
00:33:51.246 - 00:33:51.368, Speaker C: Right?
00:33:51.374 - 00:33:53.050, Speaker A: There's no like, yeah, that's right.
00:33:53.420 - 00:33:56.696, Speaker B: It's actually just free option.
00:33:56.798 - 00:33:57.064, Speaker C: Okay.
00:33:57.102 - 00:34:00.404, Speaker B: So we have a flexible system, you can do it on the user side.
00:34:00.462 - 00:34:00.828, Speaker C: Right.
00:34:00.914 - 00:34:20.384, Speaker B: The reason that's really helpful for developers is they don't have to set up any improving infrastructure. Right. We developed a webassembly prover and so yeah, your user can just run it in their web app and, okay, it's a pretty small proof. And everything else that's heavy is delegated to our system.
00:34:20.502 - 00:34:21.024, Speaker C: Right.
00:34:21.142 - 00:34:36.176, Speaker A: But I imagine at some point, well maybe think about it in another way, maybe if what user wants to do is something that's more heavy, then I imagine most of the things would actually be delegated to the server.
00:34:36.208 - 00:34:36.452, Speaker B: Right.
00:34:36.506 - 00:34:40.808, Speaker A: In that case, what are they actually doing on the kind of the user interface side?
00:34:40.974 - 00:34:41.256, Speaker C: Yeah.
00:34:41.278 - 00:34:50.072, Speaker B: So I think there are two possibilities here. One option is that on the user interface side it's really just glue code.
00:34:50.206 - 00:34:51.512, Speaker C: Yeah. Makes sense.
00:34:51.566 - 00:35:06.960, Speaker B: Yeah. So it's just things like, oh, I wanted my balance, let's say I want 100 balances and I feed it into machine learning models. So then on the user side it would just be, hey, my balance at block, at whatever block at ten blocks later and so on.
00:35:07.030 - 00:35:07.360, Speaker C: Right.
00:35:07.430 - 00:35:13.680, Speaker B: And then also that, hey, the outputs from these balances really are the inputs to this machine learning model that are verified.
00:35:14.580 - 00:35:36.516, Speaker A: Yeah, makes sense. And coming back to the more heavy lifting part, because eventually you need to generate a proof that needs to be verified in a similar contract. I imagine you have to do some kind of proof compression or something. And also there's some limitation to how much compute you can prove in single proof.
00:35:36.628 - 00:35:56.368, Speaker B: Yeah, exactly. So let's talk about this part where I guess all the substantial stuff is happening. So there's two pieces of this. One is how are these data proofs generated? And the second is how is this compatibility between the compute proof and the data proof happening? Given what we have here? Maybe we start with the second.
00:35:56.454 - 00:35:57.090, Speaker C: Yes.
00:36:06.280 - 00:36:37.660, Speaker B: Okay, so as primitives here, I'm going to assume that we have a compute proof. And so this could be either from a laptop or server. So we don't really care where it comes from. If it's small, as we discussed, it could come from a laptop. If it's big, it should come from a server. Now we have, and I just call it a data proof, but it could include things like ECDSA.
00:36:39.280 - 00:36:41.870, Speaker A: Oh, the ECDSA would actually come from.
00:36:43.920 - 00:36:56.724, Speaker B: So, so you could imagine. So here we just currently on Testnet, we're supporting any piece of ethereum data, but we also have circuits for, you can imagine ECDSA here.
00:36:56.762 - 00:36:57.350, Speaker C: Okay.
00:36:59.160 - 00:37:40.240, Speaker B: And our system is flexible enough that you can just modular. Now what do we need to do? We need to check that the dataproof actually proved everything that was requested by the compute proof. So we do this operation called proof aggregation. And what that does is kind of exactly what I said. So it creates a combined proof, so an aggregate proof. So it does like a couple of things. The first is that the compute proof is valid.
00:37:40.240 - 00:38:00.440, Speaker B: And one special thing we offer is that the compute proof carries its own vericification key. And so we do an operation here, the aggregation, it's actually universal.
00:38:01.100 - 00:38:01.944, Speaker C: I see.
00:38:02.062 - 00:38:04.090, Speaker B: So what that means is that.
00:38:06.620 - 00:38:06.936, Speaker C: What.
00:38:06.958 - 00:38:25.280, Speaker B: We'Re checking is not that the compute proof is valid against a fixed verification key, but against the verification key that the user actually provides. The second statement checking is that the data proof is compatible with the compute proof, as we just discussed.
00:38:28.180 - 00:38:35.840, Speaker A: How is that checked exactly? You actually need to check some assertion of the data.
00:38:35.910 - 00:38:36.576, Speaker C: Right.
00:38:36.758 - 00:38:38.588, Speaker A: How is this part checked?
00:38:38.684 - 00:38:46.244, Speaker B: Yes, I will get to that. But basically, maybe the last thing to say is like, yeah, the data proof is valid, right?
00:38:46.282 - 00:38:47.110, Speaker C: Right. Yeah.
00:38:49.320 - 00:39:05.500, Speaker B: Now, when I say the data proof is valid, well, you can't prove in isolation, as you're alluding to, that some piece of data occurs in the history of Ethereum. You need some root of trust. So what's our root of trust? It's going to actually be a Merkel mountain range. It's a claimed Merkel mountain range.
00:39:06.000 - 00:39:06.990, Speaker C: I see.
00:39:11.650 - 00:40:12.450, Speaker B: And when I say claimed Merkel mountain range, it's just a string of hashes that we claim are a valid Merkle mountain range in the history of Ethereum. And this we are going to need to verify on chain. So we need to reconcile this claim against the data structure that we had before against this cache of block hashes in axiom core. I see, yeah. I want to talk about a couple of subtleties relating to this universal aggregation. The first is that why do we do this at all? And the main motivation is that we don't have to change our on chain verifier, even if the compute circuit is just completely swapped out all of these components, because they carry their v keys with them and we actually have in an output. Okay, we call it an aggregate V Key hash.
00:40:16.980 - 00:40:17.760, Speaker A: What they say.
00:40:17.830 - 00:40:23.200, Speaker B: Okay, what is that? That is a commitment to all of the v keys and the aggregation structure.
00:40:23.360 - 00:40:24.196, Speaker C: I see.
00:40:24.298 - 00:40:51.900, Speaker B: So if you know the aggregate v key hash, you know exactly what was proven, even if the compute proof structure is totally different, or if these components were different once, if the user knows the aggregate v key hash, then it knows what the proof was about. So when we verify on chain, we always give you, hey, this proof was valid. Here's what it claimed, and here's the hash of all the v keys and the structure that entered it.
00:40:51.970 - 00:40:52.396, Speaker C: Right?
00:40:52.498 - 00:40:58.370, Speaker B: So the user can know from this whether this query result actually corresponds to something they're interested in.
00:41:00.580 - 00:41:03.132, Speaker A: And this aggregates all the v keys.
00:41:03.196 - 00:41:36.220, Speaker B: From all of these okay, I see. And so what that means is that a user can use our system to do a compute proof that we don't have to know about. The user just sends us their v key, sends us their proof and to us it's all kind of the same. Yes, makes sense. So the second thing I want to mention is that they don't have to deploy a verifier. Here we have a verifier and it's universal.
00:41:37.440 - 00:41:38.990, Speaker C: Oh, I see. Yeah.
00:41:40.080 - 00:41:54.400, Speaker B: So that means that if you're a smart contract developer and you want to use the system, all you need to do is to validate this aggregate VK hash coming from our system. And there's no Zk specific contract deployments.
00:41:55.940 - 00:41:57.970, Speaker C: Yes, makes sense.
00:41:59.380 - 00:42:03.164, Speaker A: Okay, should we come back to some of the points here, like how exactly?
00:42:03.222 - 00:42:03.444, Speaker C: Yeah.
00:42:03.482 - 00:42:35.468, Speaker B: Okay. So far we've talked about here we have the compute proof and the data proof. We do this universal aggregation procedure that gives a commitment to what was aggregated, checks the validity of the compute proof and the data proof. And these validities are the trust is rooted in a claimed removal mountain range and in the validity. The compute proof obviously depends on the key. Now there's maybe two points left. The first is what does it mean that the compute and data proofs are compatible? That's actually easy.
00:42:35.468 - 00:42:52.636, Speaker B: Just the compute proofs make some requests and we check in Zk that those requests correspond to what the data proof was proving. Okay, so the data proof would say like, hey, here's the balance of account ABC. We just check that that is actually the same as what was requested in the compute proof.
00:42:52.828 - 00:42:53.316, Speaker C: I see.
00:42:53.338 - 00:42:58.788, Speaker A: But from like a ZK point of view, how exactly does it work? Because those are two separate proofs, right?
00:42:58.874 - 00:43:09.080, Speaker B: Yes. So from ZK point of view we have sort of a public instance and a public instance.
00:43:10.140 - 00:43:10.890, Speaker C: Okay.
00:43:13.180 - 00:43:23.992, Speaker B: So what the aggregation proof is doing is in the proofs, they don't say anything, they just verified. But now in the instances you're actually allowed to manipulate them within the aggregation procedure.
00:43:24.136 - 00:43:25.230, Speaker C: I see.
00:43:26.160 - 00:43:35.584, Speaker B: So these public instances are private inputs to the aggregation circuit and that circuit will just constrain certain equalities between them.
00:43:35.622 - 00:43:37.490, Speaker C: I see. Okay.
00:43:41.560 - 00:43:43.988, Speaker A: So those are the input to this.
00:43:44.074 - 00:43:44.484, Speaker C: I see.
00:43:44.522 - 00:43:49.124, Speaker A: So each aggregation proof has those different private, private inputs, right?
00:43:49.162 - 00:43:54.636, Speaker B: Yeah, exactly. So the compute proof as well as these instances are inputs to the aggregation proof.
00:43:54.688 - 00:43:57.930, Speaker C: Right, I see. Okay. Yeah, that makes sense.
00:43:58.300 - 00:45:04.438, Speaker B: Okay, so the details of how to do it are a little bit complicated because there's quite a bit of bookkeeping. But at the fundamental level, we're just checking that certain requests here actually correspond to the answers here. Okay, so maybe the last missing piece of all this is how does the data proof really work? This is sort of big black box right now. Okay, so I'm going to redraw the diagram. So remember we had this axiom core contract, we have this MMR, and now for the data proofs, they aggregate sort of a number of what we call sub queries. So subquery is just a request for a single piece of data in the history. So let me talk about how to just do a single sub query.
00:45:04.438 - 00:45:37.400, Speaker B: So let's imagine we're trying to prove a storage value. So we call this a storage sub query. And what does that have to prove? It has to prove that the value of slot. So maybe let me say how to parameterize it. When we talk about a piece of ethereum storage, we care about the block number, the address we're talking about.
00:45:39.930 - 00:45:40.246, Speaker C: And.
00:45:40.268 - 00:46:37.160, Speaker B: Then for each address, the local contract storage is a key value pair. So it's a key value pair from UN 256 to UN 256. And this is called a slot. So we care about the slot and the output is the slot value. Right, okay, so how do we prove this? So the data structure in Ethereum is that we have a block header. Within the block header we have a state route, and the state route is a commitment to a Merkel Patricia try, which holds all information about accounts. So I will attempt to depict this.
00:46:37.160 - 00:47:36.086, Speaker B: So this thing is the account try. So let's imagine that this address really corresponds to this account. Now this account will have the knots balance storage route and then code hash. So don't worry about the others. But what's important is that the contract storage is committed to in the storage route. And the storage route is again a root of a degree 16 Merkel Patricia try. And so somewhere, let's say here is the key value is the value that we're looking for.
00:47:36.086 - 00:48:40.916, Speaker B: So this is the storage try, and here we will get the value. So what is the statement we're actually proving? We're proving that we have a block header that's indexed by block number, that's this one. Then we're looking up an account by the address in the account try. Then for the storage route from that account try, we're looking up a slot, and so the slot is the key in the storage try, and we end up with the value. Okay, so that's a pretty complicated procedure and what do we actually have to do? So the first thing is that we look up the block header in this MMR and so I'm describing operations. We're doing just as pure computations, but in reality we're going to do all these computations in ZK. Now, the second is we have to do lookups into these account and storage tries.
00:48:40.916 - 00:49:42.006, Speaker B: So these will be pretty parallel. And so I'll describe them just kind of separately. Okay, so how does that work? We have a root, we have a degree 16 try, and then we have a leaf that we're interested in. So in a degree 16 Merkel try, what we have to prove or how to exhibit that this leaf actually is committed to in the root. What we have to do is just display all nodes that it visited on the path up to the root. So that corresponds to commitments or to the data of each of these nodes. And so there's basically two steps to this.
00:49:42.006 - 00:50:19.150, Speaker B: The first is, because it's a Merkel tree, we care about the serialization of the nodes. And so ethereum uses what's called RLP serialization. And RLP is recursive length prefix. So this is some type of serialization that is self describing. So the first few bytes tell you about the length of the whole structure. So that's what's used to serialize each node. And we have to NZK deserialize.
00:50:19.150 - 00:50:22.210, Speaker B: It's a little bit complicated because of the dynamic length property.
00:50:22.280 - 00:50:22.900, Speaker C: Yes.
00:50:23.290 - 00:50:49.100, Speaker B: Okay, so the next step is we have to do the hash checks with ketcheck. So the hash function in Ethereum is ketchup. So we need to check that the hash of this node really appears in this node at this proper location. And this is actually the most expensive operation, just because ketchak is the most expensive operation in.
00:50:51.870 - 00:51:10.810, Speaker A: I mean, this whole thing, obviously people do in different blockchains and pretty well known. But I think the problem in DK is that it's actually quite expensive because you're doing potentially a lot of catch, like both in the MMR, also in the different tries. So how expensive is it to actually generate approving in practice?
00:51:10.990 - 00:51:49.040, Speaker B: Yeah. Okay. So it's actually kind of complicated question to answer. So I'll describe some of the optimizations. I think the best way to answer is two components. One is how many catch acts can you do per second? And then how can you use those catch acts optimally? So we can do, I think it's around 30 catch acts per second right now. And given that, how do we actually do these? So, one complicated issue is that this is a dynamic depth and these are dynamic size.
00:51:49.970 - 00:51:50.720, Speaker C: Right.
00:51:53.010 - 00:52:00.690, Speaker B: So what we do is actually because ketchack actually takes something like 90% of the proving time. We create a catchack table.
00:52:01.270 - 00:52:02.210, Speaker C: Oh, okay.
00:52:02.280 - 00:52:34.830, Speaker B: So we have just a separate set of circuits that do just catch and their job is just to produce as outputs like catchack input output pairs. So remember I had this picture of, we had like the, we had the compute proof, we had the data proof. And the reality is we actually have a third proof which is the catchack proof.
00:52:37.330 - 00:52:37.998, Speaker C: I see.
00:52:38.084 - 00:52:42.942, Speaker B: And in the aggregation we reconcile the compute proof with the data proof.
00:52:43.006 - 00:52:43.394, Speaker C: I see.
00:52:43.432 - 00:52:53.582, Speaker B: And for each data proof we reconcile with the catchack proof. So the data proofs just do the sort of parsing logic and make requests for catchack input output pairs.
00:52:53.646 - 00:52:54.114, Speaker C: I see.
00:52:54.152 - 00:52:56.306, Speaker B: And they reconcile against this catch proof.
00:52:56.418 - 00:52:57.366, Speaker C: I see.
00:52:57.548 - 00:53:04.642, Speaker A: Okay, so basically each time you need to do a catch up proof, you kind of delegate that to the other circuit.
00:53:04.706 - 00:53:19.950, Speaker B: Yes, exactly. And that also helps us with the dynamic depth because if you think about, let's say this was depth four, but maybe we needed up to depth ten in some cases. So what we would have here is sort of dummy values.
00:53:20.370 - 00:53:21.022, Speaker C: Right.
00:53:21.156 - 00:53:28.110, Speaker B: And so crucially, when we have a repeated catch act request, we only use one row of this table.
00:53:29.010 - 00:53:29.678, Speaker C: I see.
00:53:29.764 - 00:53:36.174, Speaker B: And so what that means is that we can have a pretty large max depth without really imposing more catch extra costs.
00:53:36.302 - 00:53:40.162, Speaker C: I see, okay, yeah, that's quite interesting.
00:53:40.216 - 00:53:48.360, Speaker A: And then in practice, how expensive maybe in terms of time or cost to actually do this thing.
00:53:48.890 - 00:54:08.358, Speaker B: Yeah. Okay. So there's sort of two considerations for us. One is parallel latency and the other is just total latency and or cost. So in practice we have a configuration with up to 32 of any type of query.
00:54:08.454 - 00:54:08.810, Speaker C: Okay.
00:54:08.880 - 00:54:20.590, Speaker B: So we had the block header queries, account queries, storage queries and so on. And that takes six minutes to run end. So that includes all the proofs, all the aggregation to a form that can be verified on chain.
00:54:21.110 - 00:54:22.322, Speaker A: That's pretty fast.
00:54:22.456 - 00:54:23.106, Speaker C: Okay. Yeah.
00:54:23.128 - 00:54:31.010, Speaker B: And actually one thing that we learned that is not so obvious is that there's quite a bit of tradeoff between flexibility and performance.
00:54:31.350 - 00:54:32.050, Speaker C: Let's see.
00:54:32.120 - 00:54:50.060, Speaker B: So here we're allowing you to query any block in the history of ethereum in a totally heterogeneous way. So you can have a transaction from this block, an account from this block, and we have to pay for that a bit in the ZK side. So I think if we reduce some of the flexibility, we could probably get that number down maybe by half, maybe more.
00:54:50.430 - 00:54:52.842, Speaker C: I see, cool.
00:54:52.976 - 00:55:03.626, Speaker A: Given that we're kind of about to run out of time and we also kind of already dived deep into the two parts of the proof. Is there anything else you want to talk about regarding axiom?
00:55:03.818 - 00:55:15.410, Speaker B: Yeah, one thing I want to say about is, well, we're going to launch this on Testnet. I think probably we'll be already live by the time this video comes out. One thing I want to mention is that we're going to bring axiom to layer twos as well.
00:55:15.480 - 00:55:16.626, Speaker C: Oh, that's cool.
00:55:16.728 - 00:55:29.080, Speaker B: And, actually, one thing we were a little bit worried about how complex that would be, but it's actually pretty simple. So, this is all about layer one. And so the first step is we're going to verify layer one on layer two.
00:55:30.170 - 00:55:31.158, Speaker C: I see.
00:55:31.324 - 00:55:57.006, Speaker B: And so our design is that we're not going to change the ZK proof at all. Instead, what we're going to do is to take this axiom core structure and basically port it to optimism. Arbitrum, scroll, whatever. And once you have that, then you can verify all these proofs just on our contracts? On each of these.
00:55:57.108 - 00:55:57.920, Speaker C: I see.
00:55:59.250 - 00:56:03.614, Speaker A: So they're basically just like a regular smart contract in each of those layer twos.
00:56:03.662 - 00:56:19.602, Speaker B: Yeah, just a regular smart contract. It depends a bit on the bridge structure. So, for, let's say, arbitram or scroll or ZK sync, the bridge structure is via message passing from L one. So, what we have to do is just message pass a commitment to our MMR.
00:56:19.666 - 00:56:20.006, Speaker C: I see.
00:56:20.028 - 00:56:21.334, Speaker B: And we just verify against that.
00:56:21.372 - 00:56:22.038, Speaker C: I see.
00:56:22.204 - 00:56:30.070, Speaker B: For optimism. We can also do that, and there's some chance we can optimize a little bit, because on op stack, you can access l one just natively.
00:56:30.410 - 00:56:34.518, Speaker C: Right. Yes. That is very cool.
00:56:34.684 - 00:56:37.630, Speaker A: And excited to see that launch.
00:56:40.290 - 00:56:43.758, Speaker B: Yeah. Other than that, I think we covered most things. Went pretty deep, actually.
00:56:43.844 - 00:56:54.220, Speaker A: Yeah. Okay, I think then it's time to wrap up. And again, thanks, e, for joining me on another episode of the whiteboard session. And, yeah, thanks, everyone.
