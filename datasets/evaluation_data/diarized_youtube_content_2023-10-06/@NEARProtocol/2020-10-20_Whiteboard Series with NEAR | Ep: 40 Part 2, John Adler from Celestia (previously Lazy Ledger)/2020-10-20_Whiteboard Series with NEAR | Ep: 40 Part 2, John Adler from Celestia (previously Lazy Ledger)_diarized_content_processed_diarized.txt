00:00:04.650 - 00:00:46.460, Speaker A: So now we go on to kind of the main topic of today's conversation now that we know all this, which is lazy ledger. So as we said before, let's draw our blockchain again. Here is our blockchain. It's really nice that they have built in arrows. So here's our blockchain it. So we have our block headers, right? And each block header will have a transaction route in here, right. It's going to have a transaction route in here somewhere in the block header that commits to all the transactions here.
00:00:46.460 - 00:01:10.318, Speaker A: So now we kind of note something interesting, which is in order to solve double spending, do we need to exclude invalid transactions? By invalid transactions I mean a transaction that either double spends a UtXO or one that spends an invalid nonce. Do we need to exclude them to prevent double spending?
00:01:10.494 - 00:01:12.580, Speaker B: No, we just ignore them.
00:01:14.470 - 00:01:21.734, Speaker A: Exactly. We could have a blockchain where those transactions are still in the blocks, but we just ignore them.
00:01:21.852 - 00:01:24.822, Speaker B: Well, effectively we're saying there's no such concept as an invalid transaction, right?
00:01:24.876 - 00:01:41.130, Speaker A: Yes, that's exactly it. Right. There's no such concept as an invalid transaction. And we let the application that runs on top of this order data decide its own validity rules. The only thing that we're doing at the consensus process is ordering zeros and ones. That's it. And the answer is yes, we can do this.
00:01:41.130 - 00:02:14.354, Speaker A: And this also means that the blockchain no longer has to do execution. And this is really good because the bottleneck right now, by far, in any contemporary blockchain, is very much execution, right. We have some sort of virtual machine, say the EVM, we have an implementation of it, say in guess. And we target a particular amount of computing hardware for a full node, say, let's say a consumer laptop. And then we have a particular validation rate. So that's how fast we want to be able to catch up to the head of the chain. And in Ethereum it's somewhere around like 30 fold.
00:02:14.354 - 00:03:03.350, Speaker A: So if you take Ethereum's 30 transactions per second, or 15, multiply it by around 30 validation rate, you get four or 50, which is around the limit of what geth can do if you just launch it on, if you just run it locally. So we know that that's our bottleneck. There's no consensus bottleneck here. It's very much the execution. It's very much the execution engine. So being able to remove execution from the base layer and just saying, well, we're just going to order zeros and ones is good, because now it means that we no longer have that bottleneck. Okay, so how does lazy ledger work? So the way lazy ledger works is it commits to a list of messages, let's call it message route, where a message, just some zeros and ones, and the message does not have any effect on the lazy ledger state.
00:03:03.350 - 00:03:47.026, Speaker A: And in kind of like the ideal academic lazy ledger, lazy ledger has no state at all. So the messages are just zeros and ones, they don't affect the lazy ledger state. And in fact there is no lazy ledger state. So what it will do is someone, somehow, the exact consensus process is orthogonal, and you can use any consensus protocol on top of lazy ledger. Which one you use kind of influences how much state you'll need in practice. If you're doing proof of work, you don't need any state to manage block producers. If you're doing proof of stake, then you're going to need some state somewhere as part of some consensus critical state somewhere, because how else would you manage the validator set? But that's basically like the bare minimum that you need.
00:03:47.026 - 00:04:27.870, Speaker A: Everything else is just messages. So essentially you just have a bunch of zeros and ones, and then lazy ledger doesn't execute them, it just orders them. And then if you want to check to see is this block valid? You no longer need to execute the block to check if the block is valid. You only need to make sure that the block data is available, right? Yeah. So what this means is that a light node essentially has almost the same security guarantees as a full node. Like the light node is going to do some random sampling. And if they have 100 light nodes and 100 full nodes, the full nodes can also do just this random sampling.
00:04:27.870 - 00:05:07.486, Speaker A: And except for the one exception for this fraud proof here, which a, anyone can generate, and b, the nice thing is this is stateless. Unlike a state transition fraud proof, the fraud proof of incorrect erasure coding is stateless, which means if one person checks one full block and tries to construct a potentially a fraud proof for that one block, they don't need any previous blocks. They can do it entirely statelessly, which is very good, because it means that you can now distribute this work among many parties, as opposed to requiring one party with a very powerful computer that can fully validate, say, all transactions. So this being stateless makes it completely different.
00:05:07.668 - 00:05:11.498, Speaker B: And the particular implementation of lazy ledger, is it proof of work or proof of stake?
00:05:11.674 - 00:05:14.494, Speaker A: We can talk about the particular implementation in just a couple of minutes.
00:05:14.612 - 00:05:16.126, Speaker B: Okay, so we're starting from the general one.
00:05:16.148 - 00:05:37.800, Speaker A: Okay, so the general one is basically, except for this fraud proof stuff, which we can ignore again because of the fact that we can distribute it and we don't need one very powerful computer. A light node essentially has the same security as the same guarantees as a full node, because they both just check if the data is available. And so long as there's sufficient number of these nodes doing this, then yes, the data is available and yes, it can be reconstructed. And this is all you need to do.
00:05:38.970 - 00:05:45.020, Speaker B: Let's have an application which wants to use lazy ledger, right? So will fraud proofs be something that lazy ledger provides? To me.
00:05:46.830 - 00:06:37.958, Speaker A: The fraud proofs that lazy ledger has natively are only for the data available, like the erasure coding here, right? And in our implementation it's also for transactions that mutate the lazy ledger state, such as basically just modifying the validator set, which is again, there's like a trivial state. So this is easy to do in terms of if it can support application layer fraud proofs. And the answer is lazy ledger doesn't need to natively the application can define its own fraud proof scheme. And since the application now we're going to get into how applications work and we can talk about the namespace Merkel tree in a second. So what you can do is you can have applications running on top of laser ledger, on top of the ordered data. The application can define entirely its own execution system, which means you can get heterogeneous execution. You can't do this in a sharded context.
00:06:37.958 - 00:07:19.286, Speaker A: You may have heard, for instance, heterogeneous sharding from Polkadot, but they don't really have heterogeneous sharding because all of their shards or parachains, execution systems all need to compile down to wasm, which has a common not actually, they don't have a different metering for each parachain. They all have the same metering. So there's actually homogeneous sharding. With laser ledger we actually have true heterogeneous sharding. You can define any pre compile you want in your application. For instance, you could have a pre compile that does something like find a collision for SHA 256 for one gas, and you could do that in your application. And laser Ledger doesn't care because lazy laser just ordering zeros and ones, it doesn't actually execute anything.
00:07:19.286 - 00:08:07.320, Speaker A: So it allows for true heterogeneous sharding, and it doesn't care what you do at the application letter there. Okay, before we get a little bit deeper into your questions, we can talk about the namespace Merkel tree namespace name space Merkel tree so what this does is it's a Merkel tree, as you may have guessed. But essentially each leaf leaf, one leaf n. Here's our root. You have your tree here. Each of these leaves is associated with a namespace id. So there's going to be plus namespace id, one, it's going to be namespace ID N.
00:08:07.320 - 00:08:49.602, Speaker A: And you may look at this and you may say, well, this looks kind of like a Merkel Sumtree. For those of you who are maybe not familiar with Merkel Sumtrees, it's essentially a construction for doing fraud proof style things on bitcoin, where each transaction you kind of add on some metadata of, say, how many fees this transaction pays. And then what you do is when you want to hash, like when you have an internal node and you hash two leaves together, you concatenate this extra data. In the case of Merkel Sumtree, it would be the amount of fees. In the case of the namespace Merkel tree, it's the namespace Id. And you can kind of cadinate that together and you hash it. So you kind of get, as the root, you get a commitment.
00:08:49.602 - 00:09:23.374, Speaker A: In this case of a Merkel Sumtree, you get a commitment of all the fees that are collected in the block. So it allows you to have a fraud proof of the miner collected too many fees and they minted themselves too many coins. In our case, it allows us to have range proofs, essentially that particular messages or messages from a particular namespace, you can prove their inclusion. So each leaf essentially is one namespace Id associated with it. Internal nodes. So an internal node has, do those.
00:09:23.412 - 00:09:34.980, Speaker B: Namespaces in any way like the hash function that is used for the intermediate nodes in the Merkel tree? Is it still just a shot to 56, or does it aggregate namespaces in some special way?
00:09:35.430 - 00:10:38.120, Speaker A: You can think of it like a shot 256 that outputs okay, already wrote it outputs a min and max namespace Id. Yes, internal nodes will have both a min and max. So this is essentially a Merkel interval tree and allows you to have range proof. So then I can prove, assuming that leaves are ordered by their namespace Id, you can prove that you can provide a proof of all leaves for a particular namespace, and then whoever's verifying the proof knows that yes, these are all the leaves for a particular namespace. Of course, assume, or we construct it in a way that the leaves are ordered by the namespace Id. So what this means is applications aren't just blobs floating around, is that each message is going to be associated with a namespace id and kind of. This allows for the construction of what the paper calls virtual side chains, or what we call more recently, lazy ledger applications, which.
00:10:38.120 - 00:10:47.770, Speaker A: It's essentially just a blockchain. It's a blockchain, but it commits its entire block. It's associated with the namespace and just commits its entire block. Boom. To lazy ledger. Just the whole block.
00:10:49.310 - 00:10:54.086, Speaker B: Is this Merkel chip per block? Is it just all the transactions in a block? Or is it all the transactions in existence?
00:10:54.198 - 00:10:56.442, Speaker A: It's all the transactions in a block. This is per block.
00:10:56.506 - 00:10:57.694, Speaker B: Oh, that's per block. Okay.
00:10:57.812 - 00:11:34.502, Speaker A: Yes. We've been thinking, if we can extend this to cover multiple blocks to allow for better sampling across epochs and stuff, that's kind of still a little bit of research topic. But for now, this is per block. So how lazy Ledger works, again, is that you can have an application just however it wants to reserve its namespace. Lazy ledger doesn't care. Then. The simplest case is that you have a blockchain, and then it takes this entire block and it just puts it as a message on lazy Ledger.
00:11:34.502 - 00:12:01.618, Speaker A: And lazy Ledger will order them. So what this means is that if you run multiple of these together on top of lazy Ledger, they essentially have shared security because they have the same data availability guarantees. So that you can construct things like fraud proofs, which allow you to run secure lite clients for one virtual side chain that talks to another virtual side chain. So it's essentially, you can think of them like shards at that point, right?
00:12:01.784 - 00:12:11.800, Speaker B: Also, when you say sharding, right. You can shard different things, right? So in this case, lazy ledger natively shards processing, right? Because lazy ledger validators don't process anything.
00:12:13.210 - 00:12:20.046, Speaker A: Laser ledger doesn't even. Because at no point do lazy ledger validators are they ever forced to validate any virtual sidechain.
00:12:20.178 - 00:12:20.860, Speaker B: Right?
00:12:22.270 - 00:12:25.942, Speaker A: But it allows you to build an application that shards execution.
00:12:26.006 - 00:12:43.490, Speaker B: Yes, right. You also shard state, naturally, because again, laser ledger validators don't store any state. So it's only application validators that store state, but it doesn't shard networking, right? All the messages from all the virtual applications still have to go through the same ledger, right?
00:12:43.640 - 00:13:12.378, Speaker A: That's correct, yes. So in serenity, for example, in serenity, for example, if you're running a validator in serenity, commonly called e two, imprecisely, in my opinion, every single validator needs to. Well, they need to fully validate the shard they are assigned to, but they also need to run data availability checks on all the other shards, right?
00:13:12.464 - 00:13:55.960, Speaker B: But if you consider a validator in the serenity, they are a full node in the shards they track, but they light client nodes in all other shards. While in laser ledger, the actual validators that build the chain, the full nodes in terms of like if we separate downloading data than loading transactions, right, and processing them. So in laser ledger, every validator is a full node in terms of downloading all the transactions, but node, not even light node in terms of processing them, right? While in serenity in terms of both processing and unloading, I'm a full node in one shard and a light node in a lot of. I don't load less data.
00:13:58.650 - 00:14:51.800, Speaker A: I'm not really fully convinced of that distinction in practice for two reasons. The first reason is that, as I said above, the thing that you need to do here is the only reason that you need to fully download a block is to compute this fraud proof. If you're not going to try to compute a fraud proof, you never need to fully download a block. You just check to see if it's available, right? But the thing is, computing this fraud proof is stateless. So why don't you just take the validator set and just split it up into 64 different chunks of validators and have them. Each validator just randomly assigns based on some VRF or just even some internal randomness, 160 fourth of each block or 164th of the blocks, as in every 64th block, give or take on expectation, they just try to fully download the block and compute its fraud proof. And then you have the same thing, right.
00:14:52.170 - 00:15:02.910, Speaker B: You can also assign, if I understand the construction correctly, right, you can also assign actually 164th of the block. You can have every validator download some subset of rows and subset of columns.
00:15:04.530 - 00:15:34.630, Speaker A: Yeah, you can do stuff like that too. In practice, it's because of the fact that you can parallelize almost the things like computation here, and you can parallelize across even multiple blocks because of the fact that everything is stateless. It means that in practice it's not really a problem. Okay, I think you had a few questions earlier. I think I answered some of them, but probably some other ones are. Oh, so one thing you asked, which I remember now, is proof of work or proof of stake.
00:15:35.050 - 00:15:38.440, Speaker B: Yes. And what is the consensus? Yes.
00:15:38.970 - 00:16:09.486, Speaker A: So we can start with that and then presumably we have other questions and then you can ask them. So in terms of the technical stuff that I have to introduce, this is basically it. So now you can ask any questions you want. So proof of work, approve of stake, how we're implementing this, not how it's done in academia. Right. So we went with proof of stake and specifically the tendermint consensus protocol, because it offers pretty fast finality guarantees, pretty good security guarantees, and pretty good performance. And it's one of those things that it is working there in production.
00:16:09.486 - 00:16:35.674, Speaker A: Right. Cosmos uses tendermint. Many cosmos zones use tendermint, so it's been around for a decent amount of time, and they're a pretty good developer ecosystem and whatnot. So it's one of those things where we don't really care what consensus protocol the lazy ledger chain uses because it doesn't need to use a particular one. But in terms of if we had to choose one, tendermint is probably the most mature at this point. In terms of proof of stake. Now, you might say, why not proof of work? Proof of work seems amazing.
00:16:35.674 - 00:17:28.982, Speaker A: You don't need any state to manage the validator set in proof of work, right? Because it's managed implicitly through the work, and the blockheaders are cheaper and stuff. But the problem of proof of work is you kind of don't have this deterministic or immediate finality. As soon as a block is there, which you get out of BFT protocols, and proof of work is, it's probabilistic, and then you have things like variable block times and whatnot. So it makes things generally just not as good. And I recently posted a thread on Twitter about accountable safety. By recently, I mean at the time of this recording, not when there are YouTube recordings out about accountable safety. So one thing that the Ethereum space has kind of been pushing very hard, which we should admire them for, is the concept of accountable safety.
00:17:28.982 - 00:18:16.250, Speaker A: So if a reorg happens, if an equivocation happens, which that happens, then usually the chain stops at that point because it says, okay, there's a safety violation. So it happens. What this means is you can identify at least one third of stakers, and then through off chain coordination, you can burn their stake. And this is really good because it means that you can penalize specifically attackers. And this isn't something that you can do in proof of work, right? In proof of work, if a majority of miners reorg the chain, you can't really penalize just them. The only thing you can do is you either tank the price for the coin forever, which isn't really great, it's kind of brittle, or you change the proof of work algorithm, which is now you're hurting everyone. You're not just hurting the miners that attacked.
00:18:16.250 - 00:18:31.950, Speaker A: So proof of stake has this notion, or modern proof of stake protocols have this notion of accountable safety, which allows it to have kind of very strong security guarantees. It makes it very costly to attack the system and therefore it can have a much smaller security budget.
00:18:33.750 - 00:18:49.970, Speaker B: Yeah, that's great. Zaki from Cosmos once told me that if you are using BFT consensus, you should be using tendermint. I kind of agree with him, at least as of today. I think that is, it is indeed the most mature consensus.
00:18:50.130 - 00:19:04.230, Speaker A: Yeah, as the meme goes, it's all tenderament. And then Zachary goes, yep, it always has been. A lot of these proof of stake protocols reduce in some way and whatnot into some variation of tendermint.
00:19:04.310 - 00:19:39.670, Speaker B: Well, yeah, they use hot stuff, but it's similar. Cool. So now the question I have. So let's say I'm building an application, right? If I'm building an application, I use laser ledger as my data availability layer. Now, in order to have state validity fraud proofs, I need annotated state routes. So the simplest way I see is I will just submit an extra transaction which says, oh, up until now, this is the state route we have, right? Or rather I guess will, are we just submitting the state route as of after every transaction?
00:19:40.170 - 00:19:43.590, Speaker A: So how the application does it is application dependent.
00:19:44.250 - 00:20:02.922, Speaker B: Do they need to have. My question is, can I use. It's pretty easy to build if I have another network where people agree on something, like for example, they order transactions, they compute the state route and they submit it to lazy ledger. But can I do that without having an extra protocol between the nodes? Can I only do it using lazy ledger?
00:20:02.986 - 00:20:33.618, Speaker A: Plus, generally that makes things messy to kind of determine an ordering. And this kind of boils down to a lot of leader selection processes in optimistic roll ups, especially on Ethereum. And I wrote some posts about this on merge consensus and whatnot. But what you want is just ideally a permissionless consensus process. How you determine the leader is irrelevant for an optimistic roll up. This has to be deterministic and the leader selection should be verifiable by Ethereum proper. In the case of optimistic roll ups.
00:20:33.618 - 00:21:17.682, Speaker A: And I'll explain how this bridges to laser Ledger. So one way you can do it is you can say whoever submits the block first, they're the leader. Another way you could do it is you run some small process that does something, say round robin, or you run some rando or something, and you randomly select a leader. So you can do these things. But the trick is all of these processes to elect a leader the way it works in Ethereum, because of how Ethereum is kind of limited in its execution system, the leader selection needs to kind of be verifiable by Ethereum proper in laser ledger. If you're building a lazy ledger native application, you don't really care. Like laser Ledger doesn't care who the leader is.
00:21:17.682 - 00:21:31.298, Speaker A: So basically anyone can submit a block at any time. And kind of to give you some intuitions about this is imagine that you're in bitcoin. For instance, do you agree that someone could just create a bitcoin block and just distribute it up in the network?
00:21:31.394 - 00:21:31.846, Speaker B: Yeah.
00:21:31.948 - 00:22:27.850, Speaker A: So now imagine you take that block and you just put it as a message on lazy Ledger. So note that the person who puts it as a message on lazy ledger doesn't need permission from lazy Ledger, and it doesn't need permission from anyone inside this virtual side chain. Just like how a miner doesn't need permission from the other, it doesn't need permission from the bitcoin network to mine a block and distribute it to the peer to peer network. Right? The trick is you need to have it so that verifying if a block header is valid. In other words, if the presumed leader had the right to produce this block, needs to be exponentially cheaper than verifying the full block. As long as you have a condition like that, then there's no problem. If anyone can submit any message for any namespace ID and you don't care, it's like imagine someone goes on bitcoin and they try to distribute an ethereum blockheader throughout the peer to peer network.
00:22:27.850 - 00:22:52.610, Speaker A: Nothing bad happens, right? It just gets dropped. Like in this case, okay, someone includes an ethereum virtual sidechain block header in the bitcoin virtual side chain. So bitcoin nodes, the bitcoin virtual side chain nodes which are running the application, they see that, they see this thing clearly doesn't belong here, so they just ignore it, just like they would in the peer to peer network. Does that answer your question?
00:22:52.760 - 00:23:08.920, Speaker B: Yes. The point is, I guess my question was whether it's the case that there is some way of building applications which is simpler than building an optimistic roll up, right?
00:23:11.550 - 00:23:55.320, Speaker A: Yes and no. It's slightly simpler than an optimistic roll up because you're not limited by the EVM. How the application itself decides the fraud proofs is not. You don't have to implement, say, interpreter in the EVM, and implementing an interpreter for fraud proofs in the EVM is actually nontrivial, especially doing in a way that doesn't use up all the gas is non trivial. Here you have access to native code to execute the fraud proofs, which means that things get much easier on that front. Also, in terms of things like timing and whatnot, you can imagine, for example, a gossip network overlaid on top of the lazy ledger nodes for each virtual side chain. So it's like just some subset of the nodes, and they're gossiping based on topics where each topic is the same namespace ID.
00:23:55.320 - 00:24:40.562, Speaker A: So people can actually, or you can have fraud proofs for a virtual side chain be gossiped around in the peer to peer network, which is something that you can't do in an optimistic roll up. Right, an optimistic roll up. You have to post a fraud proof on Ethereum with laser ledger. Like native virtual side chains, you can actually distribute the fraud proofs in the peer to peer network and they'll actually take effect. Even if it takes a long time to get the fraud proof included in lazy Ledger, the kind of off chain applications are still going to take that into account. So in some ways, it's slightly easier in some ways, because you don't have to worry about timing as much for fraud proofs. You don't have to worry about building an interpreter for your execution system and gas limits and whatnot.
00:24:40.562 - 00:24:59.834, Speaker A: So in that sense, it's easier. And in terms of things like bootstrapping a new chain, one slight issue with cosmos owns is you have to bootstrap their security. Every single cosmos owns need to have a validator set, you have to run proof of stake, needs to have its own security guarantees and so on.
00:24:59.872 - 00:25:00.362, Speaker B: Right.
00:25:00.496 - 00:25:17.810, Speaker A: With laser Ledger, you don't really need a big validator set doing proof of stake. You just need some way of deciding a leader for the next block of the virtual side chain. And that can be very simple, right? And you can do that and say, like some other application, that's all it does. It just decides leaders for these virtual side chains.
00:25:19.190 - 00:26:11.362, Speaker B: And I have one more question, which is a little bit, I would say it's slightly orthogonal, but it's interesting because you mentioned sharding, right? So if we compare cosmos versus. Well, I would put Polkadot and near into the same bucket from this perspective on Cosmos, if you have two zones. So sharding is only meaningful, in my opinion, if there's way at least to move assets between applications, I ideally do something more than that. Right. But let's say we just want to move assets. So, on Cosmos, if you move assets from one zone to another, then what happens? If, then we later realize that the first zone was actually corrupted because the second zone, they will be acting as a light client right. So as a light client, they verified the route, it was correct.
00:26:11.362 - 00:27:10.854, Speaker B: Merkel proof matched. So let's say later we actually do realize that there was an invalid state transition. So in Cosmos, they effectively have a philosophy which says there's no such thing as invalid state transition. Any state transition on which tendermint consensus is reached is valid by definition, right? So if staterode does not match transactions, that is still a valid set transition as long as tendermint consensus is reached, right? So from this perspective, you don't need any fraud proofs. But in Polkadot and in near, if you submit a cross chain communication from one shard to another, or from one parachain, in case of Polkadot to another, and then later it turns out that the first parachain had an invalid set transition, what happens that in both near and Polkadot, both chains will get rolled back, right? But in laser ledger, the latter is definitely not something that is easily implementable, unless those shards in advance agree upon having the shared security. So do you have any thinking around how that will be happening?
00:27:10.972 - 00:28:03.394, Speaker A: Yeah. So first of all, kind of a few notes, which is that unlike, say, cosmo zones or just sidechains, if the virtual applications or virtual side chains on top of lazyledger make use of lazyledger as a data availability layer, then you can be guaranteed that since data is available, fraud proofs can always be made, which is something that you can't guarantee with sidechains. So one thing that's nice is cosmosones. If they want to, they don't have to, but if they want to, they can opt in and essentially just post all their blocks to lazy ledger. So this will allow them to share security. So in terms of how fraud proofs work and how different virtual side chains that are like shards would communicate, the answer is it specifically depends on what the execution system wants to do. So one potentially nice thing is you don't have to worry about rollbacks.
00:28:03.394 - 00:29:09.210, Speaker A: Lazy ledger doesn't have to worry about rollbacks, right? Rollbacks, in terms of the state of these virtual side chains, is only determined by what the virtual side chains run. You don't have to worry about lazy ledger rolling back because this one virtual side chain had some invalid state transition in it, right? And in sharding systems, one shard has an invalid state. That's when things get really messy, right? Because then it's like, do you roll back just the shard, then people keep the money they've stolen, you roll back the whole system, then things get a bit messy, right? So there's a little bit tricky. There's ledger, you don't have to worry about that because it's all done at the application layer. So essentially two applications would have to kind of agree to be part of a network with shared security where they'll accept being able to process fraud proofs. This is assuming that you don't go the optimistic roll up route where let's say you have one virtual machine that's turing complete with a high gas limit that allows for trustless two way bridges. And then different virtual side chains can use this as a hub.
00:29:09.210 - 00:29:56.934, Speaker A: One option is having a hub kind of like you just run like the EVM for instance. Right, some turing complete virtual machine. And this acts as a hub that various virtual side chains can use to move assets around indirectly. The other alternative is they can communicate directly with each other without going through this hub. And this essentially means that they would have to run to be secure, they would have to run both a light client of the other virtual side chains, which in this case a light client of the other virtual side chains is pretty easy because it's not much more work than just running a laser ledger light client, right. And downloading a few extra block headers. And they would need to have some way of executing fraud proofs for the state validity.
00:29:56.934 - 00:30:44.582, Speaker A: And how you do that is kind of an implementation detail on the one hand. On the other hand, it's a tricky implementation detail. Yes, because the ideal situation is each namespace or each application, let's say because namespaces are permissionless. Let's say each application defines in the ideal case, this is something we're still ironing out, but let's say each application defines in WASM code or EVM code or something, how to execute fraud proofs. And then if you want to execute a fraud proof, you just take the fraud proof and load it into this execution engine and see it does spit out true or false. So you can either do that or you can run native code. Native code is a bit trickier because there's possibilities for resource exhaustion, attacks and exploits and whatnot.
00:30:44.582 - 00:31:07.518, Speaker A: But you just need some standardized way, which technically doesn't have to be, it's not enforced by lazy Ledger. Applications can do whatever they want. Right. And I'm going to guess that in the future there's going to be multiple standards for how to define this. You just need to have some standard of being able to essentially load the execution engine and have it be able to process a fraud proof.
00:31:07.694 - 00:31:52.990, Speaker B: Right? So the way I hear your answer is that rather the next step I see happening is like Polkadot near serenity. All the systems that today have their own data availability schemes, they all started before there was any data availability standard. So if we imagine lazy ledger becoming the de facto data availability standard, then it's just that if there is a next generation of blockchains, they will just have one less problem to solve. They can just resort to using lazy Ledger as a data availability and then they can, Polkadot can build their parachains with their specific way of state validity proofs and rolling back the relay. Is it called relay chain? Yeah, relay chain.
00:31:54.050 - 00:32:47.358, Speaker A: And that's actually a good point that you bring up, which is what I want to touch on, which is that lazy Ledger doesn't inherently compete with any other layer. One, because it can't, it doesn't have a native execution layer. And in lazy Ledger, and this is kind of one thing I want to make extra clear, so no one calls me like a shill or anything. Lazy Ledger does not provide a scalable execution layer. What it provides is a scalable data availability layer. In other words, you can just fit a whole bunch of bytes per second through it with high security guarantees, like the highest security guarantees and the most bytes per second, because it completely removes execution. If you want to build a scalable execution layer on top of it, this could be shorting, this could be, you exploit a whole bunch of parallelism like the Solana guys, right? You have a really optimized down, bare metal execution system, and you can build that, you can build any execution system you want on top of laser ledger.
00:32:47.358 - 00:33:11.754, Speaker A: But lazy Ledger native doesn't provide on its own any execution system, let alone a scalable one. So in that sense, laser ledger is nice because it's completely agnostic in which blockchains can use it for a data availability layer, right? So like you said, nier can use it, Polkadot can use it. Serenity can use it. Ethereum can use it. Bitcoin can use it, Solana can use it. All of these chains can use it. Cosmos can use it.
00:33:11.754 - 00:33:38.722, Speaker A: All of these chains can use it. And now you can actually do this scaling by altcoins thing, which used to be terrible because you lose security. Now they all have the shared security and in fact, potentially additive security if they all pay fees together. And then you can actually now do proper experimentation with various execution engines, with various trade offs and whatnot, all without losing security guarantees, right?
00:33:38.776 - 00:34:10.086, Speaker B: Yeah, that's great. Well, the last question I have is lazy Ledger still needs to do some. Like, first of all, if it's proof of stake, right? Besides maintaining validator set, we also need to be paying them rewards, right? Also, as transactions are being submitted, it cannot be free, right? Transactions must be like something must be paid when the transaction is submitted, right? So that means that laser ledger at least implements the very basic transfer functionality, right?
00:34:10.208 - 00:34:46.258, Speaker A: Yes. So I can go over that briefly. First, the kind of idealized situation that was presented in the paper, which is that you run a virtual side chain, and that virtual side chain does things like manage the validator set handles fee payments and whatnot. And that's kind of the idealized version. In practice, it's the case that if you have something that's consensus critical, then you might as well just enshrine it. Just simpler, if you just enshrine it, then you don't have to go through different levels and direction, you don't have to worry about constrained execution environments and so on. You just enshrine it and just do it natively.
00:34:46.258 - 00:35:05.280, Speaker A: So laser ledger, since we're using proof of stake like you said, and since we want to pay for fees, both of those things, there is like a very minimal execution system in our current design of lazy Ledger. And basically the only thing it does is it manages validator set and it handles fee payments. That's all it does.
00:35:05.970 - 00:35:27.910, Speaker B: Right? So let me think how it fits until this moment, right? We assumed that, for example, that no validator needs to execute all the transactions, right? So other transactions that change the state, are they in a separate sort of zone in the block, which is heavily limited?
00:35:28.730 - 00:35:56.640, Speaker A: What we do is we have a reserve namespace for consensus critical messages. Any other namespace is used for arbitrary messages. So these consensus critical messages, like transactions and whatnot, we're going to heavily restrict how many of those can be in a block so that running a full node, maybe you can even do it on a phone, because just what, just moving balances around and you're just doing a small number of those, right?
00:35:57.170 - 00:36:28.198, Speaker B: And I presume that actual transfer of lazy Ledger, native currency, whatever it is, is not the intended use case, right? So if you want to be transferring or exchanging, you should be moving to one of the l two s built on top, right? So the transactions effectively should only be happening if a I actually want to become and or stop being a validator, or if I want to move my lazy ledger coins to some side chain or take them out, right?
00:36:28.284 - 00:36:35.402, Speaker A: So that should be relatively, it's basically that and paying for fees because each message needs to have a fee payment associated with it.
00:36:35.536 - 00:36:40.010, Speaker B: But can this not be solved using the sum tree. The sum merkel tree.
00:36:42.130 - 00:36:43.614, Speaker A: As opposed to what?
00:36:43.812 - 00:36:52.314, Speaker B: Also, I thought you're suggesting that the messages, that the transactions that pay for the messages also go into this restricted part of the block.
00:36:52.442 - 00:36:52.926, Speaker A: Yeah.
00:36:53.028 - 00:36:58.420, Speaker B: While it feels to me that because you're paying for every message included into the block, then we can just have.
00:37:00.150 - 00:37:31.690, Speaker A: Well, right now I'm discussing something that's technically you can consider like a design detail. So if you're saying, could you do this other thing? Then the answer is, well, maybe you could do this other thing. I'm just describing the way we've done it now. So the way we've done it now. You know how the opera works in bitcoin? No. Okay, very briefly, the opera is just like operand, and then some bytes of data, and then that output becomes prunable. It's unspendable, but it means that you can record those, like, I think it's 80 bytes or something along those lines of data in the bitcoin blockchain.
00:37:31.690 - 00:37:57.122, Speaker A: And there's no reason to stop it at 80. You can make it a gigabyte if you want. The problem doing it that way is that in order to fully validate this chain or the sanctity of the coin, you still have to download all those operand data, but you shouldn't have to. So the way we've done it in Lazy ledger is essentially you have a transaction that pays for a message, but you don't include the message in the transaction. You just include a hash of it, and the message is going to be somewhere else in the block.
00:37:57.266 - 00:38:01.334, Speaker B: I see. So the intended use case is that messages are pretty large, right?
00:38:01.532 - 00:38:02.902, Speaker A: Messages can be very large.
00:38:02.956 - 00:38:22.602, Speaker B: Yes, I see. Right. That makes sense. Cool. Okay, so I don't have any more questions. That was a lot of interesting information. And I guess data availability is not something we discussed frequently on the whiteboard sessions.
00:38:22.602 - 00:38:28.080, Speaker B: So I think it's overall was.
00:38:29.810 - 00:38:30.078, Speaker A: A.
00:38:30.084 - 00:38:41.230, Speaker B: Lot of new information for many people. So thanks a lot for coming. And we always ask one final question, which is not technical, which is how soon is the launch?
00:38:41.810 - 00:39:25.822, Speaker A: I'm going to pull out a meme and say 18 months, but unironically. The nice thing about lazy Ledger is that the system that I've described here, if you think about it, since there's no high performance execution system, since there's no rapidly switching gossip network of shards around and subcommittees and all that stuff, it's actually fairly straightforward. Right. The trick is you have to understand how data availability proofs work and the implications of them and make sure you design things like transaction systems that don't include entire messages and stuff like that. But once you have these details in place, then it's just really just a matter of implementation. There's not really any unanswered research questions in this. That's the nice thing.
00:39:25.822 - 00:39:34.314, Speaker A: So yeah, we expect somewhere keep on the lookout somewhere 18 months in that time range for a testnet. Pessimistically.
00:39:34.362 - 00:39:38.820, Speaker B: Awesome. Great. Sounds great. Okay, again, thanks a lot for coming.
