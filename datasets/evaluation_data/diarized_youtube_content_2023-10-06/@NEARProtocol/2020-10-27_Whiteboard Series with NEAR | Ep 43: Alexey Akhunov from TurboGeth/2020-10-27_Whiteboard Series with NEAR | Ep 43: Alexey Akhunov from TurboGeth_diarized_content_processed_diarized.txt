00:00:01.290 - 00:00:22.666, Speaker A: Hello, how are you? Good, thank you. And we're live right now. One sec. There we go. So, hi everyone. Thanks for joining us. Today we have a whiteboard session with Lexi working on turbo gas.
00:00:22.666 - 00:00:42.300, Speaker A: And joining us is Danny, Michael, Misha and Eugene, who would be asking all kind of questions that they have after they watch the recent video by Alexei about the turbo gas. So I'm going to let you guys kick it off and just don't mind me from now on.
00:00:46.770 - 00:01:26.300, Speaker B: Awesome. Okay, so I'll get started. So Alexei, I know we talked about how you've been researching this for Ethereum for the past three years, and you found really some proposals for improvement regarding constructing state in a better way in order to avoid having to deal with issues like state rent or staking for said state rent later on the line, just getting the design right. And so we talked about kind of learning from those best practices. So I wondered maybe if you could tell us about those best practices for the group or what were your findings? And that can be a jumping starting point for us here.
00:01:27.150 - 00:01:40.990, Speaker A: Yeah, so thank you. Can you hear me okay, yeah, she's kind of muffled. Okay, can you hear me?
00:01:41.060 - 00:01:42.910, Speaker B: Can everyone else hear Alexi?
00:01:44.210 - 00:02:05.830, Speaker A: Yep, sound good to me. Okay. So. Because I just come across muffled to myself. Anyway, so the conversation, I guess, started when we were thinking about the state rent. And a lot of people probably aware that we had a state rent project in Ethereum. And the reason why it started is because there was a realization that the state is very large.
00:02:05.830 - 00:02:56.934, Speaker A: And basically the size of the state was creating a lot of the issues, for example, synchronizations and processing. And so it looked like a very kind of crucial, very urgent problem. And nobody really knew exactly how big the state was. And that was actually interesting because depending on how you store it, the number would be different. So if somebody asked at that moment, but how big is it? Like, what are we talking about? And some people say, well, it's 100gb, some other people say, no, it's 60gb. So it was already the first sign of that. This is not very well defined in a trooper guest where recently we have experimenting with different ways of lying down the state.
00:02:56.934 - 00:04:17.220, Speaker A: And until recently we had a representation which currently would be where we stayed, currently would be 50gb, which was already quite a kind of good achievement. But then we found another representation which is as efficient to access as that one, but basically is 10gb. And then made me thinking that if that is possible, then a lot of the worries that we had back then in 2018, when we kick started the state rent project were actually not as urgent as we thought they were, and we can address them in much better way instead of. We do still have time to think about those things better, and whether the state rent actually does have to be coming right now, or could it come a bit later? Because the state rent does come with a very hefty price tag if you introduce it on a live system. Of course, if you design a system from scratch, maybe you can do that much easier. But trying to put a state rent on Ethereum, where there has never been a state rent, is actually quite hard and it's taxing. So, yeah, that's basically what we were talking about before.
00:04:17.220 - 00:04:25.820, Speaker A: Right. Misha, do you want to ask questions that you had?
00:04:26.690 - 00:04:33.200, Speaker C: Yes. So actually, the most relevant part of the presentation for us was the flat state.
00:04:35.250 - 00:04:36.814, Speaker A: The flat state. Okay.
00:04:36.932 - 00:05:08.282, Speaker C: Yeah. So this is a very relevant problem for us, especially with WasP execution. In our case, contract execution is very fast because of compiling intra native code, but storage access is still really expensive, so we are very interested in optimizing it.
00:05:08.336 - 00:06:34.930, Speaker A: Okay, well, to be fair, actually, I don't know if you seen that on Friday, there was another talk on Friday, at least online, but there was another one which is called EVM 384, which is basically, this is a project which also has a long story. And essentially the moral of the story is that the EVM, you can actually make EVM interpreter very efficient. I mean, the perception of people had before that EVM is sort of EVM interpreter has to be super slow, actually is not really justified, because you can write very efficient interpreters and actually there are already efficient ones. And in this particular talk, they demonstrated that for certain things, if you had certain operations added to the EVM, it actually becomes more efficient than webassembly. And simply for the fact, because EVM has the long word operations like 256. And this proposal is, for example, introducing 384 bit operations, and Eve assembly has essentially 64 bit, and it has to assemble older, more complex operations out of 64 bit operations. So it's not fair to say that webassembly is by definition faster than EVM.
00:06:34.930 - 00:07:37.066, Speaker A: It all depends on your interpreter. But coming back to the state problem. Yeah, definitely. So the flat state is exactly where, the flat representation state is exactly where the sturdy s project started. From the observation that when we do state access in the traditional layout, when we represent the state as a radix tree, essentially. So I can start drawing if you want. So some people kind of know it as the Patricia Merkel tree, but actually there is much more standard definition of this radix tree that usually if you had had a job interview, sometimes they ask you a question, if you had a dictionary, if you wanted to take a dictionary and put it into a data structure in your program, what would be the very efficient way of representing a dictionary? In the dictionary, essentially you have to look for whether the word exists or not.
00:07:37.066 - 00:08:09.022, Speaker A: And if it does exist, you need to, let's say, find either definition or translation to a different language. So usually the way. Can I. Why is it not because I don't have a pen. The correct answer to this interview question is the radix tree, where you essentially build for. So the radix tree consists of multiple levels. And at each level you can basically have, let's say that we have only six letters in the Alphabet.
00:08:09.022 - 00:08:46.874, Speaker A: And so the first level is that all the words that start with letter a, they go in this way, and all the words starting with letter b go this way and so forth, right? Then the second letter, then the next level is the second letter of the word in a dictionary. Let's say that if we have a word apple, then it of course will start with letter p and there is no letter. Let's say that there's no word starting with two letters, a. Like there's no a. There's like ab, for example. So that thing would start with the b. So we have words starting with ab, abacus.
00:08:46.874 - 00:09:47.460, Speaker A: And then we have the word starting with ac, which is like, what is the starting with something? I don't know. There definitely exists or something like that, right? And so you get the idea on the second level you have all the kind of letters here and so forth. And then on the third level you get, what is the third letter in the word? So abacus would have a again and so forth instead of Abba, for example, if Abba was in a dictionary, then it will be here and so forth. In the end, you get all through the levels, and somewhere in the end you will get either the definition of the word or the translation. So that's a leaf of the tree definition of translation. And so basically, if you're basically building a dictionary in a job interview, this is what you have to say, the radix tree. And so that's the similar idea to what the Patricia Merkel tree is or any other kind of.
00:09:47.460 - 00:11:12.414, Speaker A: So in order to map this problem to the dictionary, question is that in Ethereum, for example, imagine that instead of the words in a dictionary, you have addresses of the accounts, right? And here essentially in the first level, instead of the first letter of the word, basically you look at first four bits of the address or something. And so there's only 16 variations of the first 40 bits. So you know that in the first level there would be at most 16 elements, and then in the second level, at least at most 16 elements and so forth. And it goes all the way down to a certain level where there's no more, as you can imagine that with the dictionary, for example, at some point you don't really have a lot of branching anymore, because for example, if you take the same example of abacus, after you've done aba, Abba, there is no many words that actually start with the aB, maybe aba or whatever. So imagine that there's no other words which starts with Aba. And then, so in this case your abacus could actually be straight here. So let's sort of erase this.
00:11:12.414 - 00:12:18.014, Speaker A: And so let's imagine that there is no other words that start with Aba. So what we do in Ethereum, then we put the kus in here as like some sort of special element, and then we map it to the definition or something like that. And the reason why we could do that is because it's essentially like there is no other words with such prefix. And this is the way to kind of compress. This is how you can compress the tree in Ethereum, essentially the definition instead of definition, you will get a balanced non and something like this. And of course it's not only in Ethereum, and that's basically the structure and it's all good, but there are a couple of nuances to it. When Ethereum had one of the audits, the guy who did the audit, Andrew Miller, from least authority, or at least he was at least authority, he basically noticed.
00:12:18.014 - 00:13:36.482, Speaker A: One thing is that it is possible if you just use simply the addresses of the accounts as the words in such dictionary, then it's pretty easy to construct, very kind of deep, because we're not really like here, it's not a dictionary, it's the contract, sorry, the addresses. And you can generate the new addresses very easily. So what you can do is that you can struct the structure, which is super deep, and you can see how you can do that. What you need to do is that you need to find two addresses which have basically really long prefixes, two different addresses, but they have very long common prefix. For example, if you manage to find two addresses for which actually it's not very hard, because basically you can just say fff. And then on the very end you get basically, let's say f because the address is 20 byte long. So the depth of that thing is going to be 40 nibbles.
00:13:36.482 - 00:14:17.370, Speaker A: They call it nibbles. And then you generate another one, which is going to be exactly the same, but with the e in the end or something like this. And then you're going to have a very deep tree. And there could be other things like, oh, so he was actually suggesting to do something like the spine structure. So it's not just you do it on the very top, bottom level, but you branch it on every single level. So you're creating some kind of spine structure like this. On every single level, you do this kind of branching.
00:14:17.370 - 00:14:44.194, Speaker A: So basically by doing this, you create in a very deep structure, like 40 levels deep. And then basically the reason why usually you say on what? Like, okay, 40 levels deep. And what's the problem? Like, okay, that thing, that's fine. It becomes a problem precisely when you start thinking how you store this in the database. And this is where.
00:14:44.312 - 00:14:45.810, Speaker C: Can I interrupt you for a moment?
00:14:45.880 - 00:14:47.554, Speaker A: Yeah, sure.
00:14:47.752 - 00:15:02.470, Speaker C: So I just want to summarize kind of my high level understanding. So the goal of this tree is, the ultimate goal we have is to have a state route.
00:15:03.690 - 00:15:08.970, Speaker A: No, it's one of the goals, but it's not actually the only goal.
00:15:10.670 - 00:15:15.210, Speaker C: Okay, so what are the goals of actually this building this tree?
00:15:17.970 - 00:16:04.774, Speaker A: From my point of view, the most important goal of this tree is to be able to add things easy, easily and find things easily. It's essentially a representation of a store, sorry, sorted map. It doesn't have to be sorted. It could be kind of just a map or something like this. Yeah, but you need to do it in such a way that you can actually calculate some kind of commitment to this entire map. And it happens to be that people decided that the commitment to this entire map is going to be a miracle tree of certain construction. But the problem of computing the hash root in a specific way wasn't there in the beginning.
00:16:04.774 - 00:16:15.520, Speaker A: It has been constructed. The original problem was to be able to have a data structure where you can put stuff in and you can get stuff out. Like a mapping. Um.
00:16:20.050 - 00:16:29.010, Speaker C: Sort of. So what I mean is that if we don't have state root, it doesn't have to be part of the protocol. It could be any key value storage.
00:16:29.350 - 00:17:21.710, Speaker A: Correct? Yes. Where do we come to the state route thing? We come to the state route because when we say, okay, we need to make sure that everybody has the same mapping right in the memory or somewhere in the database. How do we ensure that? Well, first of all, if you look at what bitcoin does. Let's step back from ethereum, because in bitcoin, you can imagine they have the same kind of structure, but it is not explicitly specified in the protocol, because in bitcoin there is also a state which is set of unspent utxos, right. And you need to have a state in order to be able to verify whether a certain transaction is valid or not. So it's the same kind of thing. But the difference is that in bitcoin, they don't construct commitment to the state and they don't put them in a header.
00:17:21.710 - 00:18:15.622, Speaker A: So what they do is that they assume that everybody is having the same code, or code does the same thing without bugs, so that if they run all the blocks, they will arrive at the same state. They're assuming that, that everybody could do that. And if somebody doesn't do that means that they're incorrect, then they sometimes can do wrong things. In ethereum, essentially, for lots of reasons, the designers decided that, okay, we don't just assume that everybody has the same mapping, but we want to somehow compare these mappings. I mean, of course, merkel trees were known for a long time for such purposes. Like if you take the products like cassandra, for example, it uses Merkel trees all the time to reconcile the replicas. So if you have two replicas and data missing in one replica, they use Merkel trees to quickly find out what is missing and just reconcile it.
00:18:15.622 - 00:18:39.306, Speaker A: So they call it repair. So here, exactly the same idea. Okay, let's have a Merkel tree, and then we basically going to have a way to reconcile the replicas, because basically it's a replicated database that's in the replicated state. And somebody chose this particular form of merkle tree for the task.
00:18:39.338 - 00:18:50.706, Speaker C: Yes. So ethereum, as I understand, uses this redix tree with hashes of keys for keys to make it balanced, correct?
00:18:50.808 - 00:18:51.460, Speaker A: Yes.
00:18:53.750 - 00:19:06.630, Speaker C: I know some other projects have different ways of building the tree, such as balanced binary search, AVl, correct?
00:19:06.780 - 00:19:17.194, Speaker A: Yes. The notable example is cosmos, right? Yeah, I've looked at that structure as well. So, yeah, that works too.
00:19:17.392 - 00:19:21.900, Speaker C: Have you looked into trade offs between them at all?
00:19:22.430 - 00:19:56.390, Speaker A: Yes, I did. So the trade off is this one of the main difference. Can I create a new page here? Is it like this plus button, right? Yeah, but you can also scroll down with, you have the controls in the right panel. You can just click on this cross there and scroll it down. Or you can create a quick plus pan. You mean the pan thing. Okay, fine.
00:19:56.390 - 00:20:46.134, Speaker A: Yeah. So essentially the trade offs are quite interesting. So the trade offs are in the balanced tree. Let's say it's ivl tree, or it could be weight balance tree or whatever, red black tree. So the AVl tree are probably the best ones because they have much better bound on the depth. So what happens in balanced trees is that if you take one tree, so they basically binary, and you imagine that this is some kind of algebra, right? So in this algebra, your trees are your elements, and then you get some operators that operate on these trees. So for example, one type of operator is that when you want to add a key value pair, which is basically adding some sort of key and then value at the end.
00:20:46.134 - 00:21:16.410, Speaker A: So that's one operator. Another operator, for example, is deleting the node. So another operator is removing certain key value pair and so forth. So there's two main operators. You add something, you delete something. So the main property of the radix tree is that all the operators, they commute with each other. In this particular algebra, these operators are commutative.
00:21:16.410 - 00:21:48.890, Speaker A: So you can basically add elements in any order or remove them in any order. In the end, it doesn't matter in which order you've done it. What really matters? Sorry, one correction. The additions are commutative, but they do not commute between each other. So basically, additions commute with additions. Deletions commute with additions, with additions. But the additions and deletions obviously do not commute.
00:21:48.890 - 00:22:07.226, Speaker A: So you have to know in which order to apply them. But for the balance tree, it's not the case. In the balance tree. These operators are not commutative. So it does matter in which order you add them. So essentially, if you have two elements, you add them, let's say a and b. And take the balance tree.
00:22:07.226 - 00:22:52.078, Speaker A: You add a first and b next. You can have a different tree than if you actually add b first and then a next. So that is a bit of a challenge. And if you need this property, if you do need the property of commutativity, then you have to solve it somehow. There are solutions to that, but basically that's the main con disadvantage of the balance tree. But there is advantage, of course, in the balance tree. You don't have to do these extra hashing to produce the balance, because the tree balances itself and it simplifies quite a lot of things.
00:22:52.078 - 00:23:59.140, Speaker A: I would say, for example, because you have this hashing stuff for some implementations. Initially we also had it, but now we don't. You've got to have these pre images, which are quite annoying when you look at the tree and it's hashed and you have to always, let's say basically, if you go back to this tree, if you imagine that these letters, like these paths in the tree are not actual addresses, but the hashes of addresses, and you've somehow found one of these hashes, how do you recover what the address was? Well, it's not really possible without really knowing, without having another database which will map the hashes to the addresses, which is a bit of a trouble anyway. So I think the main thing about the balance trees, the main thing I like about balance trees is that you don't need to do this extra step. But as we say, it's a trade off.
00:24:04.010 - 00:24:23.930, Speaker C: Yeah. So then if we're implementing a client. So the client would store for this try structure, it would store a mapping from a hash of try node to the node object, essentially.
00:24:24.270 - 00:24:30.974, Speaker A: Well, this is how the yellow paper tells you to do so, but that's not how you should do it. Yeah.
00:24:31.012 - 00:24:33.306, Speaker C: Let's start with this naive implementation.
00:24:33.498 - 00:24:34.240, Speaker A: Okay.
00:24:35.010 - 00:24:40.526, Speaker C: And then for that it would be using some kind of key value storage.
00:24:40.718 - 00:25:14.250, Speaker A: Yeah, that's what everybody essentially is doing. So if you go back to that structure, or, I don't know, it doesn't matter which structure it is, it could be this one. Let's make it a bish. What is it? Getting confused. Let me erase this thing and this thing. Let me just have a little tree here. Three levels will be enough.
00:25:14.250 - 00:25:46.360, Speaker A: Okay. Right. So the way I usually think about this is that imagine that the nodes are pointing to each other. They get pointers, and each pointer is. I draw it as an arrow here. So usually when you have those structures in memory, you basically have this element on the top. Okay, so how do you actually point to something here? Is there a way to point?
00:25:49.290 - 00:25:50.950, Speaker D: We can see your cursor.
00:25:51.690 - 00:26:20.302, Speaker A: My cursor? Well, I don't have a cursor. It's an iPad. Yeah, that's why I chose iPad, because it's easier to draw and then I don't have a courser. Anyway, it doesn't matter. So I'm just going to say, even in this top element where I drew the two arrows from it, imagine if you store this structure in memory, not in a database. What it would mean is that this top element would have two pointers. Right.
00:26:20.302 - 00:27:43.260, Speaker A: And the pointers usually, I mean, if you have a 64 bit architecture, the pointer is essentially a number, which is eight byte number, which has the address, the memory address of the thing that you're pointing to so that you can go there and look it's very similar how these naive would say implementations do it. But instead of the pointer being eight byte memory address, the pointer is, in the case of Ethereum, 32 byte hash. And in order to dereference this pointer, in order to find the things it points to, in the case of our memory, we simply need to ask what is inside this memory address. In the case of the database representation, dereferencing the pointer means looking up that record using this hash as the key. So it's basically the same process. So that's why if you establish this connection, then you can basically think about this as the pointers in memory and then you can very easily reason about what happens when you access things. And you can obviously see that the depth now matters because if you always start your search from the root, which you basically have over time and you need to find something, the deeper it sits, the more hops through the pointers, the more dereferencing operations you need to perform.
00:27:43.260 - 00:28:24.494, Speaker A: And then if you remember that in a database, dereferencing means doing a lookup, then you realize, okay, if I go eight levels deep I do eight lookups, if I go 40 levels deep, I do 40 lookups. And another feature of that is that you cannot parallelize it because they are data dependent. You cannot hop, let's say in this picture you cannot hop straight from the top element to the second level. You have to first go to the first level, then to the second level. So you can't skip the levels because you need to find this. It's like a quest. You need to find the next pointer and then the next pointer and the next pointer.
00:28:24.494 - 00:28:55.220, Speaker A: And that's why the depth really matters. And that's why this security audit I was talking about. That's why it was a problem if somebody could create those spines, as I was pointing out, those. Okay, where's this thing again? Oh no, it's the wrong button. Sorry. Yeah, the spines that I was drawing here. Right, you see the spine? No, actually, anyway, so you can't see the spines because you somehow moved this thing around.
00:28:55.220 - 00:29:54.694, Speaker A: I didn't know, but the spines that I was drawing. Yeah, the spines here on the right. So the problem with those spines, if you manage to construct them, then you force the tree to be very deep and then all your queries inside the area of this tree are going to be super slow. So you can specifically, like, if you don't like certain contract, if you want that contract to become slow, you create spines around this contract by basically sending one way, a transaction with one way to those addresses. And you don't even need to do any kind of address mining, you just do that. And of course, if you now hash everything with the sha, whatever, 256 before you put in tree, it's still possible, but it's much more resource intensive. So you need to buy some sha three Asics to be able to pull that off.
00:29:54.694 - 00:30:24.190, Speaker A: I mean, you could still probably go a few levels deep, but it's not that potent anymore. And yeah, that's actually the good point, is that this pre hashing before putting it into the tree is not a perfect protection against these disbalancing attacks, whereas the self balancing trees are pretty much the best protection you can get, theoretically. So it's theoretically the best protection.
00:30:26.050 - 00:30:32.434, Speaker C: So let's start with the naive implementation and then try to optimize it.
00:30:32.632 - 00:30:34.500, Speaker A: Okay, let's do that.
00:30:35.110 - 00:30:41.140, Speaker C: In any case, we always have to compute state root for each block, correct?
00:30:42.070 - 00:31:42.498, Speaker A: Not correct, not in any case. So what you can see now happening in ethereum is that a long time ago, I don't know which year was that, 2016 or 2015. Most people basically gave up on the idea of computing the state route at each block, and that came in the form of what you know as a fast sync. Right, you're probably familiar with fast sync. So with the fast sync, essentially, instead of trying to go through blocks starting from genesis and all the way in computing, we stayed and executing everything and hashing it every block, which is super slow. What we do instead is we're choosing a certain recent block, which we call pivot, and then we download from some peers the pieces of that tree. And once we've done that, we compute the state root on that tree and then we verify.
00:31:42.498 - 00:31:58.750, Speaker A: Okay, that matches up with the head. But notice that we have skipped the computation of the root for every other block before that. From that on, of course, we start computing the state root for every block.
00:31:59.810 - 00:32:04.160, Speaker C: So when we're validating new blocks, we always have to compute the state.
00:32:04.690 - 00:33:07.330, Speaker A: Again, this is not true, again, because basically I want to open your mind to other possibilities. Basically, the whole process of discovering turbogeth is throwing away the dogmas that you might think something has to be true. And the reason why I'm saying is that because in turbo get, for example, sometimes we skip the verification of the route. For example, if we sync on hard drive, on HDD, it cannot do it for every single block, but it can, for example, do it every 15 blocks, which is, I think, still fine, because if there's some problem, you will notice it anyway after 15 blocks. So I don't see this as a completely dogmatic thing that we have to validate state root after every single block. I mean, depending on what you're doing, you might sometimes skip that. And in fact, a lot of people skip that for most of the blocks when they do fasting.
00:33:11.930 - 00:33:15.830, Speaker C: Yeah, that's great insight.
00:33:17.210 - 00:33:17.960, Speaker A: Okay.
00:33:22.810 - 00:33:45.520, Speaker C: Then my question is, now we could go now we have two different things we can do to optimize it. We can either change the way we store the state completely, or we can store a secondary version of the state. Yeah, which would be better.
00:33:46.610 - 00:34:26.986, Speaker A: Okay, so what you just said, the second option. The second option where we start building a secondary structure, which has basically kind of flattish store. This is what I believe currently go Ethereum is doing. So they also realize that the flat representation has advantages, but they don't want to kind of completely redesign the model, at least for now. So they basically started building this thing on the side, which is the snapshotter. And that's one of the approaches. But from my point of view, maybe that's the only thing they can do.
00:34:26.986 - 00:35:22.166, Speaker A: I mean, it depends what situation you're in. If you want to get the best results, obviously, you have to change the data model entirely because there are a lot of things that you simply cannot do if you still have the try, basically. So things to understand is that we're not completely throwing away the try as the concept, but we're throwing it away as the physical thing. So essentially, another dogma that you have to get rid of is that the try does not have to have a physical embodiment. It could be just a logical concept. And as long as we can produce the state root in some ways, which will match up with whatever the try would produce, that's fine. You'd never have to have a try, actually, even in memory.
00:35:22.166 - 00:36:10.666, Speaker A: And in fact, we don't. We used to have a try in memory interbriguet at some point, but now we don't even have it anymore as a physical embodiment of that thing. So, yeah, you can go as far as you want, you can go baby steps, but then it might take you a long time, or you can just rip everything apart and create a new thing. Of course, in our case, I didn't rip everything apart. I was discovering lots of things on the way. You probably plan this whiteboarding session is that we're going to start applying optimizations bit by bit, and eventually we'll come to the final design. We could definitely do that if I remember all the steps.
00:36:10.666 - 00:36:12.750, Speaker A: But there were a lot of steps.
00:36:14.050 - 00:36:39.240, Speaker C: We could try to directly go to the final design. But then let's talk about problems we have with if we would try to implement it right away. So one difficulty I see is that there is one thing that this naive way of storing the try gives us, which is accessing any version of the state.
00:36:39.690 - 00:36:40.342, Speaker A: Right.
00:36:40.476 - 00:36:51.930, Speaker C: So if we have blocks coming on top of arbitrary blocks and we have to compute state transitions, we need to access every version.
00:36:52.750 - 00:37:07.774, Speaker A: Okay, so this is what I want to question. In order to compute the state transition, you don't need access to all the versions, you only need to access to the previous. Well, basically you only need to access to the current version. That's it.
00:37:07.892 - 00:37:09.360, Speaker C: But which one is current?
00:37:09.890 - 00:37:21.506, Speaker A: Okay, so when you start your state transition, you need to make sure that you are in the correct state, and then you apply the state transition and you change your current state.
00:37:21.608 - 00:37:33.970, Speaker C: So in case of ethereum, where we have proof of work blocks, when a new block comes in, does it mean we have to roll back current state a few blocks and then apply blocks from a different chain?
00:37:34.130 - 00:38:12.340, Speaker A: No. Okay. Yeah, I think what we're kind of missing a bit here is that, again, in naive implementation, you're right, you can think that you can have multiple states at the same time. You can hold on to, because basically you can point to any version of the state by just simply one single pointer, which in your case is the root hash. Yeah, that's fine. So you don't actually have to make a choice which one is the current one. You can say, oh, I've got fewer of them around, and you can use any of them.
00:38:12.340 - 00:38:49.118, Speaker A: If you start using the flat model, you have to make a choice. You have to always know which one is my current state. Right, that's my current state, and you can only have one. So now you have to be doing things slightly differently. So if you basically receive multiple blocks which are on the same height, and you need to make a choice, or you already made a choice, but it was wrong choice, you have to come back and reapply the other thing. So you always have to maintain your chosen state. So you cannot have it in multiple ways.
00:38:49.118 - 00:39:12.840, Speaker A: So what troopergift does, it's actually, it has only one current state and it moves it back and forth, mostly only forward. Sometimes it has to move it backwards, but that's not a problem. You move it backwards and then you go forward again if you know the other path is better. And that's another realization you have to do, is that you always have to know what is your current state.
00:39:14.570 - 00:39:24.982, Speaker C: So does turbogef try to make current state the most recent blocks or kind of a few blocks back and storage separately?
00:39:25.126 - 00:39:47.860, Speaker A: The most recent block, the current state is always the state that has resulted from whatever last block we have received. I mean, whatever was the best last block received. Yeah. We never try to be a bit behind. We always try to be as far as possible. Right. In this process.
00:39:47.860 - 00:39:58.902, Speaker A: Okay. And then you probably were asking, how do we go back? Right. Is this what your question?
00:39:58.956 - 00:40:01.110, Speaker C: I assume we store diffs.
00:40:01.610 - 00:40:27.818, Speaker A: Yeah. Okay. So that was the case some time ago. We stored diffs, but actually now we store reverse diffs, which is, there's a slight difference, but it makes a big impact later on. Yes. So we store diffs as we process the blocks. Every time when we process the block, we remember which bits that we have changed.
00:40:27.818 - 00:41:05.402, Speaker A: And obviously during this execution we have information about what was the value of the thing, everything before the change and after the change. Right. And so obviously the value after the change becomes part of the new state and the value before the change, this is what we remember. Right. And then we remember that for every single block as we go, we call them change sets. And so we have them. And that means that whenever we need to go back, what we simply do is that we take that change set and we are apply it to the state because it will basically, just because it contains the things that were before the change.
00:41:05.402 - 00:41:33.700, Speaker A: So essentially it rolls everything back. You can also call them maybe rollback sets or something like that. So they basically allow you to go back and then you can also go back pretty fast because if you want to go back, let's say three blocks, you read all these three things, you combine them and you roll them, roll back. I mean, you can do it in many different ways or you can do merge sort because they already presorted. There's quite a lot of things you can do.
00:41:39.030 - 00:41:39.742, Speaker C: It.
00:41:39.896 - 00:41:43.206, Speaker E: So the flat state is a sorted map, right?
00:41:43.388 - 00:41:44.914, Speaker A: Sorry, what is what map?
00:41:44.962 - 00:41:47.126, Speaker E: Flat state is a sorted map of keys.
00:41:47.158 - 00:42:37.594, Speaker A: Oh yeah, it's a sorted map. Yes. I mean, it doesn't strictly have to be sorted, but being sorted helps a lot of things down the line because basically having them sorted allows us to do some kind of mapreduce things. You know what I mean? Basically you can set up these kind of flows of data which have to happen in a certain order. Yeah, it's sorted because basically it's in database and the database we use is the B tree and it's sorted. Oh, actually that's interesting. One of the things that haven't happened yet.
00:42:37.594 - 00:43:25.110, Speaker A: But let's say that if either scan, let's say, or whatever block explorers started using turbogift, what they could actually do, I mean, we could introduce the RPC request. So, because we actually store the state as the pre hashed version, let's say, before all this hash is applied. So we essentially store it as the mapping of addresses to the values, not the hashes of addresses. What you can also do is that I don't know how the ether scan does it. Let's say that you have the search bar where you start typing the address, and it shows you what are the addresses where this prefix exists in the state. Right. Let's say you say type in ABC, and it already shows you these are the existing accounts with the ABC.
00:43:25.110 - 00:43:42.400, Speaker A: And they probably pre made this specific table. But in our case, you don't have to have a special table because you can simply search through that state. That kind of helps to be sorted for that reason.
00:43:43.330 - 00:43:45.630, Speaker C: So you don't hash the addresses?
00:43:46.210 - 00:44:12.520, Speaker A: We do, but we don't hash them until. So basically we only hash them after we have executed the transaction. So it's all separated. So EVM works on unhashed state. Once it's finished, then we convert unhashed to hashed, and only after that we compute the state route. It's basically a separate process.
00:44:13.210 - 00:44:21.420, Speaker C: But are you worried if your back end storage that you use some kind?
00:44:22.190 - 00:44:36.750, Speaker A: That's excellent question. I know what you're going to ask, but it's an excellent question. So why do we store the state twice? Right. Once unhashed, one hashed. Right. Okay, that's kind of a waste, isn't it? Yes, it is the waste. And it was even more waste.
00:44:36.750 - 00:45:31.818, Speaker A: So when we actually introduced this extra thing, extra separation hashed, unhashed. So obviously in the beginning, we only had hash state and we executed AVM on hash state. And that means every time you access something, you have to do another sha three calculation, which is not maybe a big deal, but it actually would have become a big deal later on anyway. So we separated them, and it's obvious the state was 50 gigs or whatever, 50 60 gigs. So then instead of one table of 50 gigs, now we have two tables of 50 gigs. Bad, isn't it? But it wasn't actually bad because we also had another table which was more than that, tiny bit more than that, which is pre images. Remember I told you about pre images? So guess what? When we introduced separated the hash and plane state, we could get rid of pre images because we didn't need them anymore.
00:45:31.818 - 00:46:05.930, Speaker A: Now we have all the plane state. We can run all the queries from a plane state within net pre images. So essentially there was no net gain in the storage. Then another thing happened when we managed to squeeze the state into from 50 to ten. Then it was actually net positive we ended up because we already throw away the pre images and we wouldn't have be able to squish the pre images that way. So essentially we ended up with 20 gigs instead of 120 gigs. So as a result of these two operations.
00:46:10.850 - 00:46:26.174, Speaker C: I see. So at that point, could we maybe make kind of go back to go directly to the final implementation?
00:46:26.302 - 00:46:46.794, Speaker A: Okay. Yeah. So you probably imagine what the final. Is it scrollable or for me or not? When I scroll it, do you see it scrolled? Okay, it's a lag. Well, I'm scrolling it. I'm sorry. Are you scrolling it? You can also share your own screen in zoom if you want.
00:46:46.794 - 00:47:01.680, Speaker A: So that I don't share my screen. You want to do it? This is what. No, but thing is that. No. Okay. I basically have an iPad here on my right and I have a screen here. It will take me a while anyway.
00:47:01.680 - 00:47:24.120, Speaker A: No, just scroll it back to make sure that past that, the binary tree. Because I'm going to be drawing here. Look, I'm going to be drawing right here. Can you see that under the binary tree where I'm drawing? Okay. Yeah. Becky is like on the top of the screen or something. I'm going to start from there.
00:47:24.120 - 00:48:03.490, Speaker A: Okay, so let's say that this is address one, address two. Okay. So what basically we do is that we optimize the data layout for EVM, because EVM wants to have instructions like a balance, let's say. And the balance takes the address as an argument. Right? And it wants to have a balance. Or if you have essentially a slowed instruction, then it gives you two things. It gives you address and it gives you the location.
00:48:03.490 - 00:48:38.574, Speaker A: One, something like this. And it wants you to find a storage item. Or if it doesn't exist, it gives you zero. Right? And so this is exactly how we map this in a database. So we make it easy. So we map in our database. It's the same table, but we map addresses to the very simple structures, which is basically balance nons, something like this, maybe code hash, but we actually wanted to remove hashes from there as well.
00:48:38.574 - 00:49:27.562, Speaker A: But let's say there's some sort of hashes there doesn't matter. And the same thing here. Balance nons and some sort of hashes for these things of course we have some kind of concatenation. There is a tiny bit complexity here, which we called incarnation. Incarnation, I would say it's an advanced concept, but that's basically how we manage to get around the create two and self destructions, because that creates a bit of a trouble for a flat model. So essentially for the contract storage, the keys in our database consist of three components, not two. First component is address, 20 bytes.
00:49:27.562 - 00:49:51.780, Speaker A: Then there's incarnation, which is currently eight bytes, and then location is another 32 bytes. The location is not prehashed and the value is essentially the value, and that's basically the model. So if you want, I can go deeper into what incarnation is, if you really want to know. But do you want to know?
00:49:52.250 - 00:49:53.574, Speaker C: Yes, we do.
00:49:53.772 - 00:51:03.974, Speaker A: Okay, so the incarnation essentially is required because there is some really odd thing about self destruct and even more od things about the combination of self destruct and create two. So self destruct is this operation self destruct. So it's very unique operation because what it does for the very fixed amount of gas, it allows you to delete a lot of data, theoretically, like if you have a contract which has like million entries. Of course it doesn't really happen in practice because such contracts usually don't have self destruct instructions. But theoretically you can have a really large contract and then in one go you do self destruct and all that data is gone. It's not visible anymore. And of course if you store everything as this tree model, that's easy, right? You just remove the pointer and you expect there's some kind of garbage collection will be going on and then everything which is not referenced by the pointer will be collected.
00:51:03.974 - 00:51:47.626, Speaker A: Right? That was the still thought. But for turbogeth, because we use the flat model, it means that we can't do this thing, we have to remove it, genuinely remove data. And that wouldn't work because that would be a DOS attack vector. If you're requiring to, let's say for this fixed gas cost, remove mammalian entries from the database. It could take quite a long time. So that's why we decided to get around this problem is to, so first solution would be to just ignore it. Because we said, okay, if we do self destruct, let's not do anything with the storage, let's remove it at all.
00:51:47.626 - 00:52:45.210, Speaker A: Okay, fine. Because I thought the only way to do self destruct is first to do, create or create a contract using, let's say a usual transaction to nonexistent address or doing some sort of create. But if you remember how the address of the contract using create is calculated. It calculates from the creators. So it's basically calculated from the creator's address plus nons and some sort of hash of it. So essentially the idea was that it's basically practically not possible using create or this old way of doing things, to land a contract on the same address as the previous one. So that's why our strategy of simply not removing the storage worked.
00:52:45.210 - 00:53:13.490, Speaker A: We just simply didn't remove it. And it was fine until the create two came along. And with create two, it is possible to land the contract on exactly the same address as before. And that is a problem. Like what do we do with the self destruct right now? Okay, so that was a big issue and that's why we started to introduce incarnation. We still did not want to remove the whole storage straight away, maybe later during the pruning. So instead of deleting it, we wanted to mask it to make it invisible.
00:53:13.490 - 00:53:52.058, Speaker A: And this is why we introduced incarnation. So every time we create a new contract on the same address, we increment the incarnation. So let's say the first time a contract created on this address, it has incarnation one. Then if you sell the struct, we still remember that there was something in there. And when you started created again, it gets incarnation two and then so forth. So basically then this way we separate the storage items of the different incarnations of the contract and we don't let them to be mixed up. So that if you're, you know, we basically make them invisible.
00:53:52.058 - 00:54:39.362, Speaker A: And then the idea is that when we implement pruning, which we haven't done yet, but it should be pretty easy, we will figure out which incarnations are dead and we just clean them up. But that's how we get around the issue with create two using this incarnation thing. Just kind of weird. But it's one of these things that we have to sort of figure out. And incarnation also crucial when you do unwind. Because basically, let's say that if we didn't do the incarnations and we did delete all the things, right, imagine that you did delete everything, self destruct. But then you said, wait a second, I want to do rework.
00:54:39.362 - 00:55:40.620, Speaker A: Can I just go back and then do the block which did not do the self destruct. So that means that if you do want to do that in the rework, you have to reinsert all this million things to the database again. So that actually kind of convinced me that we should not never delete them, basically because you want to go back and you have to reinsert them. And then you might remember, maybe you haven't seen this, but basically I was saying that the self destruct was probably a bit of a design flaw in the evm, because it's this kind of operation which could potentially have very large impact, but has a fixed cost. And then reverse of this operation is even weirder because I think now, from my point of view, the inverse of any operation should also be quite cheap to execute. But the self destruct is very weird. Basically, its inverse is even worse than the operation itself.
00:55:47.130 - 00:55:49.100, Speaker E: Okay, makes sense.
00:55:51.870 - 00:55:56.380, Speaker A: So basically that's the flat model. That's it. There's not much more to it.
00:55:58.270 - 00:56:07.002, Speaker E: Just the way you compute the. Let's say we're running the actual node, and now we need to compute the state root.
00:56:07.146 - 00:56:30.680, Speaker A: Okay. State root is computed in two steps. So can we scroll it? Can I scroll it, by the way? Let me see. Does it work? No. Okay, you scroll it. Okay, so step one. Where are you going to scroll it? I wanted to write under the self destruct thing.
00:56:30.680 - 00:57:06.730, Speaker A: Okay, thank you. So first step one is that we obviously need to convert the unhashed to hashed, which is easy, right? It's pretty easy. I mean, it takes a while, but it's easy. So I'm not going to get into the details of that. You can figure that out. So at the end, after the stage, we have address hash at one mapping to something, and address hash two mapping to something to some. Values.
00:57:06.730 - 00:57:49.262, Speaker A: Values. Okay, so then what's next? And the next, you want to compute the miracle route. Okay, so what we, we do. So the problematic bit about the flat model is that initially, I didn't even bother having some extra data structures for this particular operation, because I would just say, okay, we just recompute this whole thing from scratch all the time. But obviously at that moment, the state wasn't that big. And sometimes it took about half an hour on some devices, but maybe 1 hour on other devices. And so it was okay.
00:57:49.262 - 00:58:30.270, Speaker A: But then I thought, I'm going to solve it somehow differently. So what we do currently is that it's not very difficult. So imagine that radix tree again. Let me see. Radix tree. So what we're going to do in this radix tree is that we also going to have values here at each point, which are going to be, we'd call them intermediate hashes. So with each node on this radix tree, let's imagine that we have these intermediate hashes.
00:58:30.270 - 00:59:07.290, Speaker A: So that's basically the meaning of this intermediate hash is essentially it's the hash of that subtree. Right. And that's exactly why we like, why we like these explicit representation of the smirkle tree. Because at any point of time we could. Let's say that. Let me illustrate this problem. Okay, so let's say that we wanted to modify a bit.
00:59:07.290 - 00:59:31.234, Speaker A: This bit, right? This bit. Okay. So we wanted to modify this bit. And as you know, what will happen is that will be some sort of ripple effect, right? So if we modify this, then the hash of this whole element will be modified. I'm going to circle it. Then because of this is modified, then the hash of this is modified. Right.
00:59:31.234 - 01:00:13.956, Speaker A: And then eventually the hash of this is modified. But as you can notice is that although let's say we're computing the hash of the root element, because the other hashes are not modified, we can just use them as they are. And this is exactly how we basically do the auxiliary structure. So we store the mapping of move it a little bit. It's very simple, actually. It's a key prefix to intermediate hash. And that's basically it.
01:00:13.956 - 01:00:46.290, Speaker A: That's the whole magic. So for each node in the tree, we simply store the key prefix mapped to the intermediate hash. That's it. And that structure turns out to be pretty small. I mean, I think now it's about two to 3gb for the entire state. And that taking into account that it contains all the intermediate hashes for all the basically non degenerate prefixes. I'm just going to close this.
01:00:46.290 - 01:02:03.378, Speaker A: So basically there again, the magic basically is in the way that we are. So it's not actually, again, this is not the magic. The bit that, where you need a lot of interesting engineering is that how do you update the structures and how do you essentially walk through them? Because when we are updating the state route, we start with this change set, which we're talking about, or rollback set, which basically is the enumeration of all the keys that have changed. And then we're starting with the structure and then we basically walk through the structure and we carefully, using some sort of hashtag, we just walk through this whole tree and compute the result. So it just requires the careful algorithmic work. It's not like something out of ordinary. So basically because it's like imagine that you have to do some kind of merge two structures that are already presorted.
01:02:03.378 - 01:02:29.806, Speaker A: Right? Merge sort. One way to do it. If you basically do it in memory, then it's easy. But if you, let's say do merge sort of with the two huge files which are presorted. Then of course you open two, I would say we call it corsors or whatever. One of them goes through the first file or iterators and another one goes through the second file. And then just at each point you compare them and you just move them in a lock step with each other.
01:02:29.806 - 01:02:53.400, Speaker A: So the same mechanism here. But what you do is that these two structures. The first structure is this mapping of address hashes to the values. And the second structure is the mapping of the key prefixes to intermediate hashes. Essentially we have this kind of merge iterator going on through those things.
01:02:56.010 - 01:03:13.260, Speaker E: So basically these prefixes are only if there is a node. Right. So let's say you have abba and then you have ab is the same bacchus. Right. So then you will have prefix ab.
01:03:14.430 - 01:04:01.754, Speaker A: Yeah, that's true. So basically we don't store all the possible prefixes, but only the prefixes that are non degenerate. That kind of have something in them. And I guess this kind of comes to the point is that people before us did not essentially just attempt these things because I think they should have. Because somehow they assumed that all these things will be super hard and it will take a long time. But if they tried, it would be fine. Did somebody try before us to just take the entire state and mercalize it even for this? Like we tried it, it worked.
01:04:01.754 - 01:04:27.330, Speaker A: And then other people start trying it and seeing, okay, actually that's not that bad. And then they started trying other things like, oh, what happens if you. Yeah, when you happen when we do this prefix mapping? Oh, actually it turns out be not, not that. That large. You probably thought it would be like tens of gigabytes. Actually it's a two gigabyte or 3gb.
01:04:32.090 - 01:04:50.940, Speaker C: But from the complexity point of view, when you were changing the storage, you also had to change a bunch of other stuff. Presumably in client it was relying on the fact that every version is available and now you have to do switches between.
01:04:52.530 - 01:05:33.740, Speaker A: Yes, that's why it took two and a half years to essentially retrofit all the functionality using this approach. Because basically I broke everything, I have to fix it back again. And I just kept breaking it. And until basically this year around when we did the alpha release in July, I sort of realized that, okay, we have unbroken enough, we have fixed enough stuff that has been broken. Yeah. So that's basically the thing that you do. I mean, that I did just broke everything completely and radically and then try to fix it again.
01:05:38.270 - 01:05:39.580, Speaker C: So what?
01:05:41.630 - 01:05:59.490, Speaker E: Yeah, so the question is like, let's say you need to, from RPC perspective, return some historic state. Is it even possible in Ethereum or you don't really need to worry about for this particular implementation?
01:05:59.910 - 01:06:37.582, Speaker A: Good question. Yes, it's a very good question. So there are two ways you can look at the historical state. One is, I think, more important and one is less important. So you can ask the questions about, okay, so what was the balance of such and such account at such and such block? Right. That's quite easy with the flat structure, right? Basically because if you store the state in a flat structure, you can also state the history in the flat structure. In our case, we store the history as those rollback sets and then we index them.
01:06:37.582 - 01:07:22.746, Speaker A: And that's another story. I think. Actually the data structure which stores the history is more complex than the data structure that stores the flat state. So we're basically making an index over the rowback sets, but we can pretty efficiently figure out which block the certain account has changed around this sort of. Basically, if you're asking what was the balance of such and such account and a block 1 million, then we go through our index and we find, okay, when was this account changed? First, after the block 1 million. And we use the index to find it. And once we found it, we look at the rollback set.
01:07:22.746 - 01:07:47.410, Speaker A: Let's say that it was changed in the block 1,000,015. That was the first time it has changed since 1 million. We then open up the rollback set and we look, okay, what was the value before that? And the value before that was actually the value at the block 1 million. And this is what we do for such queries. It's still much quicker than going through all the trees. Right? Right.
01:07:47.560 - 01:07:52.790, Speaker E: So how you store this index to support this type of lower bound operations?
01:07:53.130 - 01:08:44.498, Speaker A: Okay, so we have one way stored indexes, but now we're actually going to change it to a different way. So the way we change it is index is essentially a series of block numbers. So index has one record, well, logically, one record per account or per storage item. And imagine that the value is basically the string of block numbers, and those are the block numbers, all the block numbers where this account has changed its value. And you can pack it in different ways. For example, we currently store them as the actual numbers when split them into the chunks, but we're now moving most of the indices into the bitmap roaring bitmap indices. So essentially it's just going to be a bitmap with a certain representation.
01:08:44.498 - 01:09:03.980, Speaker A: And it turns out to be even more efficient than these just stored as a sequence. And I think we're going to change that soon to the bitmaps. But even without bitmaps the index works pretty well. And the index is not actually that large either.
01:09:05.310 - 01:09:08.300, Speaker E: Well, it depends on how active an account, right?
01:09:10.990 - 01:09:29.830, Speaker A: Oh yeah. No, what I mean by not that large, I mean in total for all the accounts in the system. Let me find if I can find the latest data. I mean the latest, that was in September.
01:09:33.450 - 01:09:44.458, Speaker E: Okay, so the update on it, it seems to be incremental, Constantine, because you only need to update accounts that were touched in the block, correct?
01:09:44.544 - 01:09:46.410, Speaker A: Yes. So updates are promoted.
01:09:46.830 - 01:09:52.038, Speaker E: Lookup is just a binary search on top of this right now, potentially, yeah.
01:09:52.064 - 01:10:47.790, Speaker A: Lookup is a binary search, so everything is kind of searchable. We don't do full scans. This seems to be quite, so, it seems to be 19 August, the latest chart. Okay, so basically I just have a data from 19 August, how big that index is, for example, from that date, the rollback set, or we call it the change set, we probably should remove it. For accounts, it was, for accounts it was 50gb, for contract storage it was 65, 66gb. And then index on top of that, the index for accounts was 16gb. In index for storage it was 40gb.
01:10:47.790 - 01:11:32.298, Speaker A: So that basically allows you to go back to any account or any storage item in a history reasonably quickly and figure out what the value was. And this is what we use for historical re execution. So what you can do with Trubergeth is that you can pretty efficiently re execute all the historical transactions. And my recent test was about 38 hours. I mean it takes you 38 hours to reexecute everything back using the history. I mean this is basically where you don't keep updating the current state. You don't actually do any writes, you just basically run through the history and you re executing everything.
01:11:32.298 - 01:11:56.100, Speaker A: And I need this because I want to add some extra indices like for code traces and stuff like that. And I decided instead of just rewriting this whole state calculation, I'm just going to try to do this historical thing. And that's also quite useful for other analysis that people do. It's very useful to be able to run this through in about a couple of days. You just get through all the historical stuff.
01:11:58.070 - 01:12:05.574, Speaker E: Okay, yeah, that sounds good. I think most of the stuff we understand.
01:12:05.692 - 01:12:07.446, Speaker A: Well, okay.
01:12:07.628 - 01:12:28.666, Speaker E: I would probably want to ask Michael, Michael if they have any follow up questions on top of what we do that might be unique to us. So one thing we do is like garbage collection. And that seems like might help, especially with synthesis.
01:12:28.778 - 01:13:33.522, Speaker A: So you'll just clean it, actually. So the garbage collection is going to be trivial in this case, because what happens with the rollback sets, for example, because rollback sets are keyed by the. So the keys are the block numbers. And the beauty of the rollback sets is that if you remove the older rollback sets, it does not affect the functionality of the newer ones because they never look back, they always look forward. So if you basically think of history, if you think about history as represented as the miracle trees, right, in the miracle trees, the historical references go backwards. In history, they go back. And that's why the garbage collection becomes quite tricky, because as you try to delete something old, you have to first make sure that this old thing is not referenced from something which is still current.
01:13:33.522 - 01:14:13.706, Speaker A: So you have to look for that. However, if all your references go always forward. So this is what the property of the rollback set is, that all the references going forward never back. That's why when you chop something up, it never invalidates anything in the more recent history. So that's why the rollback basically becomes deleting a bunch of records from the database. And for the history indices, it's also quite tricky. You still have to go through every record, but basically you truncate at most, you essentially delete some records, and then you truncate one of the records.
01:14:13.706 - 01:14:42.778, Speaker A: You just throw a bunch of numbers out of it. Or if it's a bitmap, you just basically truncate the bitmap. So in all these cases, I think we used to have the pruning implementation, but we just throw it away to rewrite it. Again, it's pretty trivial. And so that's one of the reasons why we have rollback sets rather than forward diff sets, because it makes the pruning trivial, because basically we don't need to run any garbage collection at all.
01:14:42.864 - 01:14:47.210, Speaker E: Right. Michael wanted to say something.
01:14:47.280 - 01:15:26.310, Speaker D: Yeah, one thing I was interested in that we haven't touched on yet, but you mentioned in your talk was the concurrency changes that you had made where you were switching from this. You had parallel streams where each stream was entirely processing the block to more like stages where in parallel you were doing all the signature verification and then all the transaction execution, et cetera. Yeah, you sort of offhand said in the talk that this made things like ten times faster. And I was wondering more details about why specifically that kind of a change made things faster and what the parallelism underneath looks like.
01:15:26.380 - 01:16:41.434, Speaker A: Okay, so there's two reasons why it made things faster. First, reason is that if we manage to basically, first of all, if you zoom out of this whole, what does the Ethereum client actually do? Right? And what it does really is that it receives some new data from the network that ingests new blocks or something. And then it basically goes through the series of data transformation. It transforms this block, I mean, transforms the current state. So it's all kind of data transformations until you get to, let's say to the data in the history in the state, and then you figure out the state route and so forth. So it's basically old data transformation. And so the first realization that first reason why the stage sync is faster is because if you now take this en masse, it's basically at a large scale if you have to do a lot of transformations, and all these transformations end up taking stuff from the database, doing something with it, and putting it back into the database.
01:16:41.434 - 01:17:46.370, Speaker A: So when you're actually putting stuff back into the database, it matters quite a lot in which order you're putting it in. So if your database is empty, if your table is completely pristine empty as what happened in the initial sync, if you presort all the data with the sorted level of keys, then inserting in this way is about ten times faster than if you're trying to insert them in some kind of random order. Essentially, instead of using database as your sorting algorithm, which is basically like it would be some kind of insertion sort, right, basically on the disk, which is actually a bad idea. Instead what we do is that we do quick sort of the memory chunks, then put them into the files and do merge sort out of the files and into the database. And then database doesn't have to do any sorting, it just puts the data on the disk. So essentially we are utilizing much more efficient sorting algorithm to put the data in. And on that large scale it matters quite a lot.
01:17:46.370 - 01:18:19.578, Speaker A: I mean, you would not think of this, but it does matter. The sorting algorithm matters. And that's the first reason, and the second reason is slightly more unexpected is that when you separate this work into these kind of much more homogeneous stages and you start looking at them, first of all, your profiling becomes much clearer. You start seeing things that you haven't seen before. You start looking, basically all your bottlenecks become clearer.
01:18:19.674 - 01:18:29.120, Speaker D: And then are you literally running like, is there literally a piece of the code that says, we are currently in stage one and we've got workers, right?
01:18:29.830 - 01:19:11.582, Speaker A: There is a directory in the code which is called east stage sync. And then it lists all the stages by the names. It stage here, stage this. I mean, we don't call them by the numbers because we sometimes insert new ones. But you can see each stage essentially has two functions. One function is move the stage forward, another function move the stage backward when we do the unwind, and at each point it receives the handle to the database transaction, which is already pre open. So it needs to do its job, take the data from what it needs to put the data where it needs to put, and then job done.
01:19:11.582 - 01:19:50.794, Speaker A: And then the same happens on the other direction. And at some point the overall kind of driver thing will decide when to commit that transaction. Because for example, one thing we do is that we don't commit every stage, we just commit in cycles. So to make sure that if you're running like a block after block, your database is always consistent up to the latest block. You never see the state of database in the middle of processing one block. You always see it at the points where the block is completely processed and committed. And if you're external reader, that's what you see.
01:19:50.794 - 01:20:32.762, Speaker A: You see the very consistent snapshots. And that's actually quite important for RPC requests and stuff like this. So you can simplify things quite a lot if you have this assumption. But yeah, the second reason is that as you can imagine, the code organization becomes different. You have these much clearer chunks of code, which does do one thing and do them well, and you can think about, can I do. So you unleash the creativity a bit more of people who work in with this code because they can. Now the problem space is much smaller, you can simplify things and you can profile them much better.
01:20:32.762 - 01:20:50.910, Speaker A: You can like, oh yeah, this is the bottleneck. And we have these moments where lots and lots of optimizations actually happen after we have separated stuff out, because it becomes basically obvious, okay, this is why it's slow, or this is why it's still slow, right?
01:20:50.980 - 01:21:02.418, Speaker D: Yeah. Okay, that makes sense. And then that actually leads us into the last thing that you talked about was the architecture, right? You wanted to do some pretty heavy architecture changes.
01:21:02.504 - 01:21:04.066, Speaker A: Oh yeah, we already do it.
01:21:04.088 - 01:21:10.226, Speaker D: Components, most of on that slide, it hadn't started yet, but do you want to just talk a little bit?
01:21:10.248 - 01:21:54.274, Speaker A: Well, if you watch the one that I did last Friday the 23rd, this is where I went more into detail about the architecture components. But I'm going to shortly explain you. What I meant is that it started with RPC Daemon when we split it up and we decided to ship turboguet as already two components in the beginning. So RPC Daemon was already separated. And at the moment it's running through some GRPC connection. But actually what we will do quite soon, and I mean, it's already working, but we don't advertise it yet. If you still want the sort of the old way of running everything on the same machine through the same database, you can still do that using shared memory, it works.
01:21:54.274 - 01:23:14.506, Speaker A: But as I say, we don't advertise it yet, because we need to make sure that migrations happen in exclusive mode and so forth. So there's some technical details. But now we have an appetite for more component separations, because actually I think this is important, and I think it would be crucial for this whole project to grow and become stronger, because I think the death of other projects, or maybe not a death, but a considerable difficulties of other projects, were that over time the development team cannot grow very fast. And so over time the team becomes overburdened by all this feature requests and all this stuff, because it's been pulled into all sorts of different directions. And the core team, which still understands the code, cannot basically satisfy this growing demand on like, oh, we need this, we need that and we need to fix that and we need to fix this. So I'm expecting that by splitting things out and actually making other people code owners, let's say one of the good example would be transaction pool. We want to split it out and make other people code owners so that they can run their own releases.
01:23:14.506 - 01:23:35.106, Speaker A: So we will have interface, we will agree on the interface, we will run the core part, and they will be developing transaction pool and it will be compatible. And at some point, I think when we fill the functionality gaps, I would like the RPC demon to also be a separate project run by different people. Right.
01:23:35.208 - 01:23:42.550, Speaker D: So the idea here is not about scaling the software itself, it's about scaling the organization around the software.
01:23:43.450 - 01:24:35.302, Speaker A: Exactly, because what you also, it's only natural that this should happen if we want the system to grow, because of course it becomes more complex for the users to say, okay, now it's got six components, it consists of, what do I do? I need to run a sentry transaction pool? Demon this, demon that here like six different things. How do they talk to each other? Yeah, that becomes more complex. But then there will be another tier of people whose job will be to package this all together, like DaP node, for example. They already made the package of trooperget, but I expect them in the future to make packages of selected bits that people want. Like do you want the component with these things or the package gives you.
01:24:35.436 - 01:24:39.254, Speaker D: More freedom to customize your node to suit whatever your needs.
01:24:39.372 - 01:25:03.440, Speaker A: It's like a different Linux distributions and stuff like this. So there are people who are specializing on providing a certain distribution with certain flavors. And I think that's kind of a sign of the healthy growth or of the system where you have different tiers of people doing different things, not necessarily like core developers doing everything. Yeah, for sure.
01:25:04.610 - 01:25:05.518, Speaker E: Okay, cool.
01:25:05.604 - 01:25:18.194, Speaker D: Yeah, I think at near, actually we do have a pretty good separation of components into different modules in the code anyways, it all ultimately gets compiled up into the same monolithic binary at the end.
01:25:18.232 - 01:25:42.038, Speaker A: But like the. Actually if you watch this presentation, I started with the kind of discussion about code ownership and why it is important for the code to be outside of your binary. Even for some people it might be just a semantic difference, like semantics, but.
01:25:42.064 - 01:25:47.710, Speaker D: Not, it feels like an implementation detail, right? Like whether or not it's running in the same process.
01:25:47.780 - 01:26:29.626, Speaker A: Yeah, but it's not implementation detail because it's much more. So. Basically I was explaining why it's very hard to get the changes in, in some of the implementations in pretty much all of them is because people feel ownership. They don't want to just, they need to personally verify a lot of things. Like most of the things to make sure that if something bad is, it's not going to affect the quality of the product because they're going to be responsible for the result in any case, not the people that you can't blame the people that you accepted contributions from for all these things. Yeah, of course you can, but it.
01:26:29.648 - 01:26:31.990, Speaker D: Doesn'T look good, it doesn't grow community very well if you do that.
01:26:32.080 - 01:26:57.922, Speaker A: No, it does not. But the proper componentization with the proper interfaces, very well defined, documented sort of stable interfaces only these things give you the ability to separate the code ownership, not some kind of cludge inside the code base that you're called a pluggable something. Sure.
01:26:57.976 - 01:27:14.540, Speaker D: Okay. Yeah, and I guess the other advantage, right, is when you have separate binaries, you can have entirely distinct release cycles, right? Like if you're all compiling up to the same binary, then you're all on the same release schedule. Ultimately at the end of the day you cut one release, who knows what features got in there.
01:27:15.150 - 01:28:03.926, Speaker A: Exactly. And then you have to be much more disciplined about the versioning of the interfaces. You can write in different languages. Of course, if your interfaces are, and that's actually what already happening. As I pointed out my presentation, we already have two implementations of, I mean, they're not production ready, but we have two implementations of p to p sentries, one in rust, one in go. We have two implementations of the core execution component, one in go, and one in c plus plus this whole thing is, we're actually probably going to have two or three implementation of transaction pool, one in go, one in rust, and one in whatever python. So I think that this is the future.
01:28:03.926 - 01:28:06.742, Speaker A: Right. Cool.
01:28:06.876 - 01:28:10.518, Speaker E: Well, thank you so much. I think we out of time.
01:28:10.684 - 01:28:18.910, Speaker A: Yeah, right. No, it was a pleasure. So I hope that it was useful.
01:28:20.290 - 01:28:22.240, Speaker D: Thank you so much. I appreciate it.
01:28:22.610 - 01:28:24.730, Speaker A: All right, thanks, guys. Bye.
