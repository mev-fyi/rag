00:00:00.250 - 00:00:24.574, Speaker A: Okay, we are live now. Hi, everyone, I'm Marcelo from near. I'm very happy to have here with me, Adrian Manning from Sigma prime. We will be discussing about Eth two network layer. So, Adrian, can you introduce yourself and give an overview of what we are going to talk today?
00:00:24.692 - 00:00:47.980, Speaker B: Sure, it. So I'm a co founder and director of Sigma prime, and we're building an Ethereum two client in rust. So over the last two years, we've been building the lighthouse. That's our implementation of the Ethereum two client and I guess. So today we'll just be talking about the networking layer of Ethereum two, seeing as we're going to mainnet hopefully pretty soon, next month or so.
00:00:50.770 - 00:01:07.460, Speaker A: Okay, so let's start with the components of Ethereum network, and we can go as you prefer. You can start in the order you want, and I will be asking questions in the way, is that correct?
00:01:07.830 - 00:01:45.086, Speaker B: Okay, so I guess I just start at a very high level for people to get like a high level overview before we kind of go down to some of the deeper layers. So in ethereum two, I guess I'll be talking about phase zero. So Ethereum two has multiple phases. I won't go into those, but phase zero is the one that we'll be launching within a month or so. So in phase zero, there's a concept of the beacon chain, and essentially what happens in a beacon chain is we get a chain of blocks. Hopefully you can see this. And the chain of blocks gets kind of grouped into an epoch, which is 32 slots long.
00:01:45.086 - 00:02:38.554, Speaker B: Each slot is 12 seconds. That's where a block kind of gets fit into. So essentially we're forming this beacon chain of blocks, a block every 12 seconds, and 32 of those form an epoch. The validators, if you're a validator on this network, you perform some extra actions than somebody that's just an observing node or somebody that's just participating on the network. So they're going to have different networking implications between somebody that's validating and somebody that's just running a node on the network. So for validators in this 32 epoch window, validators get split into committees, so they get kind of subdivided such that every validator has to pretty much do an attestation, which is kind of a vote on at least once per epoch. So once per 32 slots.
00:02:38.554 - 00:03:23.834, Speaker B: So if we have 20,000 validators, we group them all up into committees. Each slot, there'll be one or up to 64 committees, and each committee for each slot performs some attestation. So the networking challenges then are that for each node on the network? If you're a validator, you need to form these little committees. You need to be able to attest and send your votes to all the other people on the network. Of the 20,000 validators, one randomly gets selected to propose a block. So the person who's supposed to propose a block, like for example, the first one or the second one, they need to get the votes from everybody else. They need to kind of group them all up, put them into a block, and then submit the block.
00:03:23.834 - 00:04:02.810, Speaker B: The block then has to get submitted to everybody else on the network. So fundamentally, the challenges are we need everybody on the network to make a vote once per epoch, and we need the proposer, the person that's going to create the block, to receive everybody's votes that have made a vote so far, and he or she needs to propose the block and then submit that block across the network. So they're pretty much the main areas of what I'm going to talk about, how we've achieved that and what protocols we use to do it, I guess, to start. So we've got any questions so far? That's just kind of like an overview of the Ethereum two very fundamental.
00:04:04.190 - 00:04:10.518, Speaker A: Yeah, that sounds great explanation. So let's try to go deep inside each component.
00:04:10.694 - 00:04:55.798, Speaker B: Okay, so I think the easiest way to start is let's talk about somebody who's not a validator, somebody that just wants to run an Ethereum two client on phase two without any validators attached to it. So this kind of network or this kind of client or node on the network needs to pretty much keep track of which blocks are being proposed. They don't need to kind of group them or collect them all, they just need to kind of participate in the network. So let's start like a lifecycle of a node that kind of just wants to passively observe the network. So an ethereum two node, let's call it, this guy has fundamentally two kind of transports built into it. One is UDP. So it's got like kind of two output ports.
00:04:55.798 - 00:05:40.154, Speaker B: We've got UDP and TCP. It's pretty much what we're using at the moment. The tcp one might change, we might use a different transport at a different layer, but there's two fundamentally ports that are going to be exiting your node at any given time. And ideally, if you're behind a nat or a router, you want to kind of forward these two kind of UDP and TCP ports. So inside an ethereum two client, the reason that there's two of them is because the protocols that we use are kind of split up for different reasons. So inside a beacon node, there are three fundamental, I guess, protocols that we use to communicate with other nodes. So the first one is called is discovery or disk v five.
00:05:40.154 - 00:06:04.980, Speaker B: Discovery v five. Discovery B five. The other one is an f, two. RPC. RPC. And the final one is gossip sub. These bottom two are grouped under a framework called lib B to P.
00:06:04.980 - 00:06:29.318, Speaker B: And these use TCP, and discovery uses UDP. That's a d. Hopefully you can see that. Okay, so discovery itself, the discovery protocol that we use, uses the UDP transport. Everything else uses TCP. So when a node first gets on the network, the first thing. So I'm going to start, I'll go down the list from the top to the bottom.
00:06:29.318 - 00:07:02.020, Speaker B: So we'll start with discovery. So when a node first joins the network, this happens for a node that's passively observing the network or a node that's also a validator when we want to join the network, we need to find pretty much the network of peers that we have. We need to find everyone else that's proposing on the chain that already has a long chain. We need to find a discover these guys. Hence the word discovery. So the protocol that we're using is called discovery version five. It's an extension or the next generation of discovery version four, which is used in ethereum one.
00:07:02.020 - 00:08:17.270, Speaker B: So this was kind of like the brainchild of Felix, and there's a number of different reasons which as to why we've used UDP and haven't kind of combined these things, mainly because discovery v five is chatty. So the way that it roughly works is we communicate very often and very quickly with many different peers to try and find more peers, I guess, on the network, which doesn't kind of lend itself all that great to TCP, which requires kind of long live connections, so we can just quickly send UDP packets. It's lossy across UDP, so implementations should be robust against packet loss. So the UDP choice and discovery v five kind of worked well for us in a particular case. It's fast and it's quick, and we don't have to keep track of lots of file descriptors, I guess, if you're doing lots of TCP connections. So if you're familiar with Cadenlia, disk V five at the kind of application layer works very similar to cadenlia, and Cadenlia kind of builds up a distributed hash table of peers. So essentially, what happens when our node first connects to the network, we have a set of boot nodes.
00:08:17.270 - 00:08:33.780, Speaker B: So these are trusted nodes that we initially connect to. We form initial UDP connections to discovery V five has its own different kinds of wire or an encryption protocol across the wire. So everything's encrypted. Maybe I'll go.
00:08:36.550 - 00:09:06.170, Speaker A: One question regarding cadenlia. One thing, like, when we were trying, when we were implementing the network layer, we considered using cademlia. But one of the issues we see with that is that the diameter of the network, like the distance between two nodes, might be somehow larger than we expect. So when we were routing message, it may take more time than expected. So how fast are message being propagated?
00:09:10.530 - 00:10:16.106, Speaker B: So when you said there's a distance between the two nodes, you're talking about the xor cademia metric distance. So that's what defines what buckets they go into, right? Yeah. So locally, let me drop this. In cademia, every node has a local routing table, which gets split up into, depending on the metric, you use 256 different buckets. So you got like, these buckets like this go from like 1234, and these are labeled by the metric that you use between two peers, which is not geographic in the sense that it's not how far they are, it's just based on an XOR between the node identities. The way the buckets are designed in Cadenlia is that if you were to randomly sample 50% of any node on the network, 50% should go in the first, 125 percent should go in the nest, into the second one, 4.5%. Everything kind of halves.
00:10:16.106 - 00:11:51.230, Speaker B: And then as the buckets go, it's very unlikely that you get in the lower buckets, but very likely that you get in these buckets. So if you have nodes that are often greater distance than you, they're usually pretty good, because that helps you to kind of avoid eclipse attacks, in the sense that if you have nodes that are in lower distance buckets, if you were to try and fill up that bucket to replace a legitimate node, you would need to generate a lot more. It's harder to generate a random id that would fit in that bucket and replace that guy because it's a greater distance. So in terms of network propagation and speed of doing those things, the metric doesn't really, to my knowledge, play much of an effect there. The difference in the distance helps you, I guess it depends on how you're doing the node lookups, right? So if I go and ask a node and say, hey, always give me the first two buckets, and there's a whole heap of peers that are in the lower end buckets. I'm not going to find those peers if I only ask them in these. Yeah, I'm not sure if it's worthwhile going over how Codemia works and find node works, whether we've got the time to go into that, but maybe just as a high level overview for people that haven't kind of seen this stuff before, what happens in discovery, it's over three nodes on a network, so each of these nodes store their own buckets, I guess they're called buckets, which store peers that they've seen.
00:11:51.230 - 00:12:36.910, Speaker B: So you can only fit in cademia, it's 16, you can only fit 16 peers per bucket. So if you've got 100 peers, some of them are not going to be in your local routing table because this one's usually mostly full, this one is less likely to get into, and it's kind of part of these guys don't get full that often. So usually most people's local routing table kind of look like this, where the buckets are filled with peers that they know about. So you go and you ask one of your neighbors, hey, I need to find some peers. So the way that you do it is you pick some random identity, some random node. So essentially you randomize which bucket you're going to ask them to. Getting peers, there's a bit of theory as to why you do this, but this is just fundamentally how it works.
00:12:36.910 - 00:13:26.670, Speaker B: So let's say this is us here, where a is b is c, we go and we ask b, hey, what peers do you know in these three buckets, for example, and they'll come back, they'll return these lists of peers. One of them may be c, because c is in this bucket, for example. Then given C's distance from us, which is just an xor thing of their identity versus our identity, we fill it into our bucket. So it could be in a lower bucket, it could be in a higher bucket, and essentially we fill our local routing table the same as everybody else, which gives us kind of a list of peers that are on the network. That's like a high level overview of roughly how we do discovery.
00:13:27.090 - 00:13:46.370, Speaker A: One question regarding ethereum is, do you need at any point to send a message between two peers that are not directed connected directly? Like to route some message between two peers that might be several hubs? Other than broadcasting a message, is it necessary? No.
00:13:46.520 - 00:14:36.266, Speaker B: Yeah, I'll list these things here. So we have disk v five, RPC and gossip sub. These are the protocols I was going to talk about trying to live p to p. So the RPC gives direct node to node communication and gossip sub does kind of like a network wide propagation. So that handles some of the other things. So let me just talk about, instead of how discovery works, I'll just talk about Ethereum specific things about the discovery that we've implemented because, yeah, I think the other stuff you can kind of just look up quite easily on the Internet. So for discovery we have signed peer records.
00:14:36.266 - 00:15:28.614, Speaker B: So essentially we have a container which is what all peers kind of use to identify themselves. And this is what gets stored in the buckets that I was talking about in every node. So for each individual peer we have, this thing's called an ENR, an ethereum node record, and it has associated with an id which has a public key associated with it. And this thing is signed. So I'll draw a key, just this whole thing here is signed with the public key that represents the identity of this thing. So that prevents, if I'm asking somebody for a particular peer id, that prevents somebody in the middle just changing the values that's stored inside here because it's signed with a peer id. So the signature would break if some malicious actor was doing it.
00:15:28.614 - 00:16:26.802, Speaker B: So inside here you can store key value data. So for example, you can store your IP UDP port, UDP TCP port, so every peer record in discovery. So we can search for these particular things and these are the things that we get back. So when I'm looking for peers and I get a list of 16 in return, I get this kind of signed container which comes from the original peer, and it tells me what IP I can connect that peer on, whether it supports discovery over UDP and the TCP port, which means that it supports the RPC and gossip sub so that we can connect via these protocols to it. It also ethereum specific, it gives us, we have a field called f two. That field represents the fork id that the node is on. So we could have multiple testnets or different networks where the DHTs could kind of be combined.
00:16:26.802 - 00:17:07.614, Speaker B: So I could ask a peer and find 16 peers in return. But those peers could be on a different network than what I'm looking for. So I can filter those out based on their ENR. There's another field which I'll have to get to a little bit later, which is called attestation nets. Attestation nets. And essentially we subscribe to these subnets in Ethereum two, and it's useful to be able to search for peers on those particular subnets. So inside an ENR, if you want to search the Ethereum two network for peers for a particular topic, you want to put that kind of information inside the ENR.
00:17:07.614 - 00:17:43.114, Speaker B: So inside enrs we can put information that allows us to search for peers. So the two ethereum specific ones are the fork. So we can search for peers on a particular fork or network that we're looking for or chain. And the other one is the assetation net field, which I'll talk about later, but it represents what subnet they're on, so we can look for different kind of subnets. So back to just give a recap so far, what happens is when a node first starts on the network, it runs discovery. Discovery has a set of boot nodes. From those boot nodes, we ask them for a set of peers.
00:17:43.114 - 00:18:26.046, Speaker B: We get back a collection of enrs and we try and connect to those enrs on the IPTCP ports if they're on the correct fork id that we're looking for. So that should give us initially a connection to a range of initial peers that are on our network. For Ethereum, too. Cool. So that gives us discovery, and we use UDP to do that. So once we have a collection of peers on our network, we connect via TCP and we do that using lib PDP. This is the framework that supports the RPC protocol and the gossip sub protocol that we use.
00:18:26.046 - 00:19:27.486, Speaker B: So when we connect via TCP, essentially what happens is we start the negotiation framework that exists inside Lib PDP. And the way that works is there's a protocol of sorts called multistream select, which for every, I guess every protocol inside lib PDP that you support, you have an identifier name, or it's called a protocol id. So when we connect, we establish an initial connection to another peer. We start this negotiation process where I say, hey, I support this, this, and this. They go, I support this, this, and this. And amongst the protocols that they all support, we choose which ones we both support and try and set up the initial connections for those protocols. So for Ethereum two, the handy thing about this is that it doesn't just support high level application protocols, it actually can support different transport protocols.
00:19:27.486 - 00:20:29.366, Speaker B: So if somebody supports TCP and quick or websockets, you can choose the preference or the order of which transport you actually kind of connect. Sorry, I should rephrase that. You get connected on one of the transports, and lip PDP can then negotiate the high level applications from the transports, I guess the three or four fundamental parts of Lip P to P. I go from top to the bottom, this is from raw transport, connection transport. So once somebody connects you on a particular transport that you support, you then try and establish an agreed upon encryption scheme so you want to have your traffic encrypted. For ethereum two, we're using noise. So in the Lib BDP framework, in the negotiation phase, we say that we support as a protocol id, we support this protocol id which represents the noise version and everything that we support.
00:20:29.366 - 00:21:22.630, Speaker B: If the other person connecting to us or the people we're connecting to also support that, we establish a noise connection, which is our encryption layer. Once we do that, we then try and set up a multiplexer. So this allows us to set up across one kind of connection, we can multiplex a number of different protocols along the top and set up different kind of substreams. The two that we support, one, in ethereum two, we always support, all clients support mplex is the stream multiplexer that everyone needs to support. And optionally we can support YaMX. So depending on the clients and whether they support that, we negotiate the multiplexer. And then after that we start negotiating the application layer.
00:21:22.630 - 00:22:11.154, Speaker B: So for Ethereum two, as I said, there's two protocols at the application. One's an RPC and one is gossip, sir. So at this point, the LipidB framework we've got connected on a particular transport, we've negotiated an encryption layer, which is noise. We've negotiated multiplexer, either mplex or YAMX, and now we're negotiating which protocols we support at the client level. Most clients, I think, currently should support RPC and gossip sub. So this will form two connections, I guess we can now form substreams over RPC and gossip sub. Okay, so just as a quick recap of what I've just said, then when a node first joins on the network, we use discovery.
00:22:11.154 - 00:22:33.054, Speaker B: We find a set of peers that give us a chunk of enrs. From those enrs we can connect via the transport that we support, which is TCP, because that value is inside the ENR. From that LipidP allows us to negotiate an encryption layer, a multiplexer, and then we can start forming connections with the two protocols that we support, which is RPC and gossip sub.
00:22:33.172 - 00:22:41.950, Speaker A: Sorry, what is the motivation for the encryption layer? Isn't all the data public that can be passed transparently?
00:22:43.670 - 00:23:24.574, Speaker B: Yeah, so the data that we've got in the application layer isn't entirely encrypted at kind of a transport layer in general, just for privacy reasons and for other people kind of putting in the middle I think we always want encryption at the transport layer just to prevent any application layer from looking at the Ethereum two traffic. So you can see kind of TCP information, but you don't actually know exactly what Ethereum two traffic is actually going across it.
00:23:24.692 - 00:23:31.870, Speaker A: I see. That is to avoid certain type of attacks, probably. Sorry, to avoid certain type of attacks.
00:23:32.790 - 00:24:34.290, Speaker B: Yeah. So even in Discovery V five, we decided to make even the headers, all the information is encrypted. And the way that it's encrypted based on the node id that you're connecting to, so that prevents things like deep packet inspection. So for routers that decide, I'm going to stop Ethereum two by blocking its discovery protocol, I'm going to look at, I can kind of inspect the packets, figure out, oh, this is an Ethereum two header packet, but if it's encrypted at that layout, the headers, then even, it's very difficult for a firewall or something to actually block specific traffic associated with Ethereum two. Okay, so let me briefly just talk about, make sure I'm not going too much over time. Let me briefly talk about the two different protocols and their primary use. So the RPC protocol, this kind of answers your question about the connectivity, I guess, the topology between the peers.
00:24:34.290 - 00:25:20.862, Speaker B: The RPC is a direct peer to peer communication. So for a node on the network, once you've directly connected to the peers that you've discovered. So let's say this is us, try and make it bigger, and we've got four peers, 1234, we form two way direct communications with these four peers. Okay, so there's two kind of. Let me talk about what we need to do. So as a passive observer on the network, we've now connected to four peers that are on the Ethereum two network. We need to firstly make sure that we're up to sync with the current state of the chain.
00:25:20.862 - 00:26:09.182, Speaker B: So if we're a new node that just joined the network, we've just connected to four peers. We don't know whether our current view of the chain is up to date with all the other peers on the network. So we have to directly ask them. Essentially, when we first connect, what's the current state of the chain that you're looking at? And we ask each of them individually. So we use the RPC protocol. The RPC protocol has like four or five different methods, so it's essentially a request response, and we use that to essentially kind of figure out the current state of the chain, the status of these peers, and get just kind of simple information directly from them. It's primarily used for syncing.
00:26:09.182 - 00:26:53.110, Speaker B: So let me go through an event where we're kind of a structure where we're not in sync has. Let me just go through the different request responses that we have in the RPC for Ethereum two. So there's a status message. A status message is what we use when we first connect to a peer. So a status is essentially asking them what's their current state of the chain that they're looking at. It tells us in response is the current head slot that they're looking at, which is kind of where their chain is. The head of their chain.
00:26:53.110 - 00:27:49.140, Speaker B: In Ethereum two, we have a concept of finalization, so it also returns when was their last finalized slot, and also the root hashes for both of those things. So there could be two peers that have the same head slot, but they're different blocks, so they're on different forks. So with that information we can determine essentially the current state or the current head, or roughly the head view for each of the peers that we connect to of the beacon shank. The status also responds with the fork id so that we can tell if any of these peers that we've connected to are on the wrong fork. So then we instantly disconnect. I won't, given the time, I won't go into all the syncing logic. So we could say this guy is up to block ten, this guy is eleven, this guy's ten, and this guy's nine.
00:27:49.140 - 00:29:01.034, Speaker B: So when we first connect, we look at the status and each of the peers give us their current view of the chain. If the current block that we know of, if we haven't seen any of theirs, and our current block is, let's say five, then we know that we need to download the blocks that these guys have seen to be able to make an informed decision on what is the current state of the Ethereum two beacon chain. So that's when we start to invoke another RPC method, which is blocks by, it's called blocks by range. I'm just going to go BBR and a blocks by range. RPC method allows us to essentially request up to 1000 blocks in a range from one slot number to another. So in the particular case that I'm using here, we want to get blocks from slot five to ten from this peer, five to eleven from this peer, five to ten from this peer. We can do this a little bit smarter where we don't double up and duplicate the requests, but fundamentally that request will the response to that request will be that these peers give us the blocks of the beacon chain that they know.
00:29:01.034 - 00:29:11.070, Speaker B: We collect all of these, we process them, and then we decide, using some fork trace algorithms, what's the current state of the beacon chain?
00:29:15.250 - 00:29:19.680, Speaker A: This process happened only between nodes connected directly with each other.
00:29:20.370 - 00:30:11.150, Speaker B: Yes. Of the peers that I connect to, I get roughly a view of the network based on that. So the algorithms behind all this are a little bit more complicated than what I'm doing. I'm kind of simplifying it, but essentially from the peers that you're connected to, you get a view of the beacon chain. You can use the RPC to download blocks directly from these peers, from your immediate peers, you can get more heuristics or extra information from the network using the second protocol. But I haven't kind of thrown that in yet because I haven't talked about so fundamentally, we connect to all of our peers, we find them from Discovery. Given that view of using the status request, we can use the blocks by range request to collect all of the blocks.
00:30:11.150 - 00:31:06.850, Speaker B: The RPC has some quirks associated with it that allow you to stream blocks from peers, but I won't go into those technicalities just to save time and to save the interest. The other status requests that exist are there's a goodbye message. So when we disconnect from a peer, we can gracefully go, we're leaving, and give them a reason why. One of the main goodbyes that you'll probably see on the Ethereum two network is that peers have a specific kind of number of peers, like a peer limit. And usually there's a lot of peers trying to connect to everybody else. And you'll get a goodbye because your node on the network kind of has too many and we're kind of pruning you. There's a blocks by root, blocks by root, which allows you to request individual blocks.
00:31:06.850 - 00:31:44.634, Speaker B: I'll explain that guy. It kind of goes hand in hand with gossip sub. There's a ping protocol so that for all the peers that you're currently connected to, you can ping them intermittently to figure out whether they're still live. It's kind of like a liveness check. And there's a metadata RPC request. So each of these peers store a small amount of data called metadata, which contains these assessation subnets which was in the ENR. The reason we have that is that if a peer connects to us directly, we don't have access to their ENR.
00:31:44.634 - 00:32:07.814, Speaker B: So that attestation nets value that's inside the ENR we would not otherwise know. So we have this extra protocol to request it off them if we don't otherwise know it. And I'll explain that in a second with gossip sub. Okay, so if I move on to gossip sub, it's all right with the RPC, roughly that we have direct peer to peer communication. Makes sense.
00:32:07.852 - 00:32:08.440, Speaker A: Sure.
00:32:09.850 - 00:32:37.034, Speaker B: Okay, so gossip sub. So the next thing that we need now is let's imagine we've connected to peers. So I just do another recap just so that everybody's kind of still with me. When the node first connects the network, we use discovery. We connect some enrs, other people can connect to us. At the same time, we connect via a different transport, TCP. Using the lipidp framework, we negotiate these protocols, and we now have connected peers.
00:32:37.034 - 00:33:51.240, Speaker B: We use the RPC protocol to determine the current state of the chain and figure out whereabouts we stand relative to that chain. We also use the RPC to download blocks and to get up to the current state based on all of the peers that we've currently connected to, once we're in sync, let's say so that we know about all the blocks that our peers know about, we call that being in sync. Once we're in sync, if we're an observing node, every 12 seconds there's a beacon block being created, and each of these things need to propagate across the network to every peer every 12 seconds. So this is where the gossip sub protocol comes in. So the gossip sub protocol is a published subscribe protocol, pub sub for short, which means that you can subscribe to a particular topic and you can publish to a particular topic, and messages should get routed to all the peers on the network on that topic. So let me explain. There's a number of very good talks and articles by Lupita P on how gossip sub works and what it's designed to do.
00:33:51.240 - 00:35:09.482, Speaker B: So I probably recommend checking those out, but at a very quick five minute overview of how gossip sub works is if you have peers that are kind of connected to each other in some kind of network, hopefully you can see this. You got all these different kinds of connections. The very naive way, quick way, of sending a message to every peer on the network is you send a message to all your peers, and you get them to send a message to all their peers, and then they send a message to all their peers, and you get this message getting propagated really rapidly across the network, because everybody just sends the message. So if one of the peers in the network says, oh, I've proposed a block he publishes it to all of his, they publish it to all theirs, they publish it to all theirs, and it just kind of rapidly propagates across the network. The downside of that is that there's massive message amplification, because if every peer is connected to every other peer, you'll obviously find that the message gets duplicated quite rapidly, almost exponentially. So there's huge bandwidth costs associated with that. The next semi improvement to this is instead of sending to all your peers, you randomly send it to a subset of your peers.
00:35:09.482 - 00:36:25.458, Speaker B: So instead of, if I have 50 peers that I'm connected to, I don't just send my message to 50 peers, I send it to a random 20 of them, and then they send it to a random 20, and they send to a random 20. And depending on the probability that you use to randomly select peers as to how many that you select, you can kind of tweak the amplification factor that gets kind of sent. But the downside is if the probability is too low based on the network, let's say use 1% and you only send it to one peer, and they only send it to one peer, there's a chance that the message doesn't get propagated fast enough or to all the peers on the network. So gossip sub is kind of an improvement on that, where instead of sending it to a random subset of the peers that you're connected to, you create an overlay network called a mesh of a smaller subset of your connected peers. So if I'm connected to 50 peers, the configuration parameters are configurable to set up the mesh. But for Ethereum, we use roughly about eight is the ideal number of peers that we connect to. And we say we form what's called this mesh, and everyone else on that network also connects to eight of their peers, and they form this overlay mesh network.
00:36:25.458 - 00:37:33.770, Speaker B: So instead of connecting to every single peer, we form like a mesh, which is a subset of the connections. I guess then when a message gets forwarded to us, we don't forward it to every single of our peers, we only forward it to our mesh peers. So that way, if you design the mesh depending on the number of connections that you have to fit kind of the topology of your network, how many nodes and stuff are on your network, you can minimize the amplification factor, but still get quite good propagation across the network to counteract the probability that a message doesn't get sent across the entire network. There's the idea of gossip. So if a message comes to me, I send it back across all of my mesh peers, which is only a subset of peers that I had. Then I randomly select the remaining peers that are subscribed to this topic and I tell them, hey, I've seen this message, and the message is identified by an id. And I randomly tell people hey, I've seen all of these messages over the last few seconds.
00:37:33.770 - 00:37:56.166, Speaker B: Those are the peers that are in my mesh. So then if those peers get that gossip and go hey, I actually haven't seen this from my mesh, they can ask me directly to get that message and then they forward it on. So it's kind of a correcting thing for any messages that don't get propagated across the network. So it's a five minute overview of.
00:37:56.188 - 00:38:14.890, Speaker A: Gossip sub that was very interesting about gossip sub. It has its sound healing mechanism, but I'm wondering, it's fully organically the way it grows. It also has a mechanism to guarantee that it has some nice properties about fast propagation.
00:38:15.550 - 00:39:16.458, Speaker B: Yes. So there's a few papers that I probably recommend having a look at, specifically about how its performance goes and how it grows and scales with number of peers and the propagation and latency associated with that. Specifically, as I said, the mesh is very configurable. So you can configure the mesh to be the size of the number of peers that you have, in which case it's like a slider, it'll go down towards the flood sub level where you'll get very rapid propagation and very small latency, but huge message amplification. Then with the configuration parameters, you can scale it back to the other side where you only have one mesh peer or something, and then a lot of it kind of gets propagated through gossip sub, but you get an increased latency. The Lib PDP guys that invented this had a upgraded this to gossip sub 1.1. In the process, it was mainly security updates to pretty much address a number of different security issues that attackers can have when they kind of join the network.
00:39:16.458 - 00:40:02.910, Speaker B: In that process they built kind of a simulation and testing framework that allows you to simulate these networks at large scale. And based on the parameters that they have and the networks that they set up, you can see how this thing performs, what its general properties are, and how it scales. So I'd encourage to kind of have a look at that. For Ethereum two, as I said, it's a pub subsystem, so you subscribe to specific topics. And in Ethereum two, there's a number of different topics and they have different network properties. So the scoring parameters at least change a little bit based on the topics that we're subscribed to. But for our use cases, this fits the need for the block propagation.
00:40:02.910 - 00:40:50.774, Speaker B: For block propagation, fundamentally one of the nodes on this network will say I have published a block and then I'll send it across their mesh peers. It should all get propagated across the network. If it doesn't, the gossip element of gossip sub should pick that up for peers that haven't seen that message, and then they request it and propagate it across their mesh. So this has, from all of our tests has worked quite well for what we need. So that roughly makes sense so far. So I think that's all the still at a high level I was going to potentially go deeper, but at a high level, that's roughly for an observing node. An observing node, they connect to the network, they use discovery over UDP.
00:40:50.774 - 00:41:23.990, Speaker B: They find a number of peers which are enRs. The ENRs tell them how to connect to them, what fork they're on. Once we connect to them, we use the RPC protocol that allows us to get a view of the current state of the beacon chain and download any blocks that we think that we're missing from that we use gossip sub to then subscribe to the topic that publishes blocks. And then because we're subscribed on that protocol, we'll regularly see blocks that get published through this propagation mechanism.
00:41:26.410 - 00:41:28.140, Speaker A: Okay, that was great.
00:41:30.990 - 00:42:17.378, Speaker B: Yeah. So the next one is, that's for a node that's just observing on the network. For a node that is testing, they have to do a little bit more and it's a little bit more gossip sub explanation. For a node that's a validator, essentially the same thing happens. They go through and they find peers and then they get their current view of the network, but they have to now do this extra step which is voting. So they get every epoch, let's say this guy is one epoch, there should be 32 of these. So I put a little, so every epoch, a single validator, you can have many validators per node.
00:42:17.378 - 00:43:01.910, Speaker B: So if you've got 1000 validators, you need to multiply this step 1000 times. But for every epoch we get shuffled into a committee and we need to attest to a particular block and form essentially an attestation. So for every committee we get shuffled into, for each slot there can be up to 64 of these things depending on the validator count. I'll do these. These are committees that we kind of get shuffled into. So validators kind of get grouped into these little sections per slot. So every slot there could be, let's say there's ten, that would mean there's a lot of validators.
00:43:01.910 - 00:44:20.800, Speaker B: But for every slot they get shuffled into these kinds of committees. The idea is that we don't want the person who's proposing a block shouldn't have to verify if there's 20,000 attestations to verify the signatures which represent the vote of every validator on the network, because the load of that is going to be way too high. The signature schemes that we're using in Ethereum two is called BLS, and it has a very useful property that allows you to aggregate signatures so people on the network can, let's say if you get 1000 validators and you group them all up and you get them all to vote, if they all vote on the same thing, you can kind of group all of those signatures into a single signature called an aggregate signature, and then you can give that to the block proposer. And the block proposer then only needs to verify one signature, not 1000. So that's the idea of these committees, these kind of like subcommittees. We group all the validators into these subcommittees and get them to vote and then the validators get grouped into, on average, it's a probabilistic thing. On average we get 16 of the people in these committees to collect them all and aggregate them.
00:44:20.800 - 00:45:23.854, Speaker B: So they collect them all for each of the committees, 16 of these people are randomly chosen. And so everyone kind of votes, they grab all those votes, they group them all up into a single into the vote that wins or the vote that the person that's aggregating them voted on, and you group them into essentially one aggregate vote. And this happens for each of the committees. Then the person that aggregates them all sends them on a kind of like a public gossip subtopic, similar to a block. So a block proposer who needs to create a block just listens on a gossip subtopic which represents the aggregate signatures. So all these committees kind of group all their signatures and then they publish the grouped signature or the group vote on the gossip sub aggregate topic. So the proposer just watches that, gets all of these aggregate votes in, checks them all, throws them into a block, and that forms the next block, fundamentally.
00:45:23.854 - 00:46:01.440, Speaker B: So the topics that we have in gossip sub, there's a beacon block, I'll just talk about the beacon block, the main ones. So there's a beacon block. So if we subscribe to this topic in gossip sub, we'll get regular blocks that come through as I was explaining before, there's a beacon aggregate topic. It's called aggregate improve. And this is what block proposers should listen to. And they get the aggregates from all of these committees. There's a few others.
00:46:01.440 - 00:47:22.154, Speaker B: So the next important one is that from a networking layer, when you form people into these committees, you don't want each of these individual votes being sent across the network to every single peer on the network, because only these aggregators or the people in these committees really care about those individual votes to kind of group them, right? So each of these committees get split into their own gossip subtopic, which we call subnets. So I won't write out the name, it's beacon attestation, underscore subnet. But we can just have like for example, topic one, topic two, topic three. And there's up to, on main net there'll be up to 64 of these things, these kind of subnet gossip subtopics. If you're on subnet one, for example, that's for every slot, let's say this is 1234, you get shuffled up, you find out you're in slot one. So you need to subscribe to the gossip subtopic one and you publish your vote on the gossip subtopic one. All the other people in this subnet, I guess, or committee group if you will, they will also subscribe to the gossip subtopic one.
00:47:22.154 - 00:47:42.320, Speaker B: So the messages that get sent don't get sent to the entire network. They only get sent to all these people here, the aggregators then kind of group them all up, as I was saying, into a singular vote and then submit it onto the beacon aggregate topic in gossip sub. So that, as I was saying, the block proposer just listens to these two main ones and they get the grouped one.
00:47:45.910 - 00:47:53.410, Speaker A: So far, these topics like they run on top of gossip sub, like they, they are run over this mesh.
00:47:53.990 - 00:48:35.460, Speaker B: Yeah, so eat. So you form a mesh, as I was saying. So this mesh of peers, there's a mesh per topic. To simplify it, if I'm connected to ten peers, of those ten peers, two of them could be subscribed to this beacon block topic. So if they're subscribed to that beacon block topic, and I only have two peers, I'll chuck them into my mesh of beacon block topic. But if I have ten peers and they're all subscribed to Beacon aggregate, I'd probably chuck them into my mesh, but then prune them off to have some as left over and some that are in the mesh, but there is a mesh per topic. So when I subscribe to.
00:48:35.460 - 00:49:17.600, Speaker B: Okay, yeah. So you form these kind of like dynamic overlay networks for each subnet. Yeah. There's a bit of complexity in there in the sense that if peers are like, let's say this time period that you need to shift between subnets is 12 seconds. If you have all these peers shifting every 12 seconds, then the stability of whether you can find peers that are on those subnets very quickly within the kind of the twelve second shifting is difficult. So we have kind of like if you have a validator, you subscribe to a subnet for a very long period of time. So we have these kind of backbone peers that kind of sit on that network that you can form meshes with.
00:49:17.600 - 00:50:11.198, Speaker B: So this, I guess, brings me to the part that I was talking about way back at the start where I said there was an agitation nets key value thing inside the ENR and also in the metadata. So this thing is a bit field. So it contains a whole heap of up to 64 for main net, lots of zeros and ones which determine which tell you which subnet they're subscribed to long term. So you can use discovery to find peers that are subscribed to these subnets in advance. So I know one epoch in advance. So I know 32 slots in advance which one I need to subscribe to. So I can use discovery to search for peers that are already subscribed and will be subscribed when I connect to, for example, subnet three.
00:50:11.198 - 00:51:03.486, Speaker B: So I find those peers, I connect them to subnet three. That allows me to form a mesh, and then other peers do that as well in order to do this, small individual votes that can then get grouped into the larger voting system. So that covers it. Kind of like a high level the dynamics of how we're splitting up the individual votes. So instead of the proposer having to verify if we've got 20,000 validators on the network to verify 20,000 votes, we can split them up into these subnets. The subnets get propagated, the individual votes get propagated using the gossip sub protocol, and they get aggregated by a subset in that committee. The resulting aggregate gets sent to the beacon aggregate topic, which most observers and stuff will just listen on.
00:51:03.486 - 00:51:12.580, Speaker B: And you can see the aggregates kind of coming in, and the block proposer, whoever that is, can kind of chuck it into a block and then all the votes kind of get included in, in the block as we go.
00:51:14.150 - 00:51:44.926, Speaker A: Okay, this sounds great. I have a lot of follow up questions, but yeah, too little time left. That's okay. So one of the things we are struggling at near is about upgrade regarding the network, how do you handle that in Ethereum, like for example, when there is an upgrade that requires some network changes, did something that happened and how do you handle it?
00:51:45.108 - 00:52:25.260, Speaker B: Yeah. Okay, so upgradability. So I guess for consensus level things, as I mentioned through it, I kind of skimmed over it because it wasn't a huge important detail. But inside the ENR is a fork id. So we can upgrade the fork version essentially of the entire chain, and we can then find peers on a new fork, this fork, I guess number if you will. Like if the chain hard forks, for example, it's built into Ethereum too, that all the signatures become essentially invalid on an old fork. So all the new signatures and everything will kind of shift across.
00:52:25.260 - 00:53:20.314, Speaker B: For peers that agree on a new fork, the RPC status message when we connect to a peer also tells you what fork that they're on from a networking layer. I didn't actually mention this, but firstly in lib p to p, when we negotiate the protocols, actually, let me just split this up logically. So the fork id that I'm just talking about handles consensus kind of level upgrades. So the signatures change. We could change some core level parts of the specification, and nodes will be able to identify other nodes on the network and signatures kind of become invalid. The next part is we need to kind of split out all of the networking communications on a different fork as well. So we can do that based on, so discovery usually wouldn't change.
00:53:20.314 - 00:54:16.270, Speaker B: We can still talk to other peers on old forks or old outdated code, but we can discover new peers for a specific fork based on the ENR for Lib p to p. The way that we do the negotiation of protocols that I was mentioning earlier is that we have essentially a protocol id. So the protocol id is a string that says which protocols we support. If we want to upgrade one of those protocols, we can change that string inside the protocol id for our RPC protocol. And in gossip sub has a version number. So if we upgrade the version number of, for example, one of the RPC messages for the status message, for example, then we can support both versions if we want. So when we do the negotiation through Lib P to p, if one node supports status version one and another one supports status version one and two status version one will get negotiated and we will use that.
00:54:16.270 - 00:54:58.326, Speaker B: But if both nodes have upgraded, and let's say only one supports two, or one supports both, you can order which one you prefer which one has a preference. So we would negotiate based on the version and the ordering preferences in libidp, and we can do that with gossip sub as well. So if we upgrade the RPC or any of the RPC methods or gossip sub, then libidop will negotiate that for us. The final part is for each of these topics that we're subscribing to. I only gave the name of the actual topic, but the actual topic contains the fork id that we're using and the encoding that we use. So if we decide to change the encoding, then the topic itself fundamentally changes. So you would subscribe to different topics for different encodings or different versions.
00:54:58.326 - 00:55:13.780, Speaker B: So I think based on the liquidity, modularity, and the way that it negotiates protocols, we can easily kind of upgrade and change versions for old code and it kind of shifts everything across. We won't communicate with other peers on different versions that we don't support.
00:55:14.230 - 00:55:39.530, Speaker A: Yeah, okay, sounds nice. It's very good all the way. You have the versions inside the topics together. And about sharding? Well, you use the topics, for example, when you are routing and you're submitting new transactions and you need to route them to new block producers. Is it handled by the meshes?
00:55:39.870 - 00:56:19.000, Speaker B: Yeah. There's further issues, especially in the networking, which when we introduce sharding, there's more challenges that kind of come in. One thing that we're kind of working on at the moment is that there's a data availability for it. So we need to do kind of like random sampling at a network layer. But the shards should fundamentally come into kind of subnets similar to these committees. So we can still split up. I guess we're calling things subnets, but we can segregate nodes based on gossip subtopics so that network communication or propagation only happens to people that subscribe or are interested in a particular area.
00:56:19.000 - 00:56:25.990, Speaker B: So the sharding and some of the other networking challenge can kind of be subdivided based on using gossip subtopics.
00:56:28.170 - 00:56:40.780, Speaker A: Okay, nice. Okay, Aidan, I think we are on time. That was a great explanation. A great overview of the terrier nebulae. Thank you for being with us today.
00:56:41.470 - 00:56:57.886, Speaker B: Thanks for having me. Hopefully it was somewhat useful to get a general idea of kind of how these things work and whether there's some crossovers that we can have with Nia that make things applicable, or if you guys solve some problems that we're having, maybe we can borrow some of those ideas as well.
00:56:58.068 - 00:57:03.482, Speaker A: Yeah. Okay. Goodbye, Adrian.
00:57:03.626 - 00:57:04.410, Speaker B: Okay, cheers.
00:57:04.490 - 00:57:05.620, Speaker A: Thanks. Hello.
