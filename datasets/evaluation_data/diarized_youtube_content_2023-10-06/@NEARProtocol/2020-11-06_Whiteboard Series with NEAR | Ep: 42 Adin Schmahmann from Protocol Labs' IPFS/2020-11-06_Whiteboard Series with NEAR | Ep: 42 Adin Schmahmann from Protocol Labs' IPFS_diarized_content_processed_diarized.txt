00:00:04.730 - 00:00:24.030, Speaker A: Hi, everyone. This is Alex from Nier. With me today is Adin Schmaman from ipfs, and we will talk today about how ipfs works, what are the underlying components and what problems it solves. Aden, would you like to introduce yourself and give us the high level overview of ipfs?
00:00:24.610 - 00:01:05.150, Speaker B: Yeah, yeah. So, as Alex said, I'm Adine, I'm the tech lead for GoIPFs, which is the reference implementation for ipfs. And IPFS stands for the interplanetary file system. And when I tell people that I work on something called the interplanetary file system, I sometimes get some looks. All right, so interplanetary file system. So what do I mean when I say interplanetary?
00:01:05.230 - 00:01:05.522, Speaker C: Right?
00:01:05.576 - 00:01:27.110, Speaker B: So let's know. Alice is on a trip. She goes to Alpha Centauri, which know about, let's say, five light years from Earth, right? And she wants to download war in peace.
00:01:30.410 - 00:01:30.870, Speaker C: Right?
00:01:30.940 - 00:02:29.814, Speaker B: Alice wants the book War and Peace, right? And these days she might go to the. So, you know, whatever, HTPs, Gutenberg, Warren Peace. Not the correct URL, but close enough. And this is going to one, this is going to do a DNS lookup, right? That takes us to whatever the IP address, 1234. And then it's going to actually ask for the data with an HTP request. And that's sort of the download. And each of these things will take many years, I mean, even if you have DNS caching and whatnot.
00:02:29.814 - 00:03:20.262, Speaker B: And so you skip straight to the download phase. Just the simple task of going all the way to Earth and then getting your data and then coming all the way back is too long. But maybe this is the best you can do in this scenario. So let's see what happens if we try something slightly different. Let's say that instead on Alpha Centauri, we have both Alice and Bob. They both went together. Bob knew that they'd be in for a long trip, and so he brought with him sort of all these books, right? He bought with him all the Gutenberg books.
00:03:20.262 - 00:04:19.870, Speaker B: And of course, Earth is still over here, still very far away. So what we want is, instead of Alice making this request, which is bad and slow, we would like Alice to instead make this request to Bob. But there are security implications. You know, what if Bob lies? Or what if Bob gives the wrong data? What if the data is be. There are two types of the data being wrong, right? One is we'll call it like, malware or spam or whatnot. And the other is, what if it's just the wrong edition, wrong version. Now, just as an example of a wrong version thing I heard about recently, which is sort of funny.
00:04:19.870 - 00:04:41.670, Speaker B: Apparently, when Warren Peace was added to the nook, which is like a Kindle competitor, somebody seems to have done a search and replace of the word kindle and replaced it with the word nook, which meant that words like kindled turned into nook. And so it was just not the same text anymore.
00:04:42.250 - 00:04:42.998, Speaker C: Right.
00:04:43.164 - 00:04:52.058, Speaker B: And that's obviously not the same thing and not intentional. But if you wanted to make the point that these things were different, maybe you were actually looking for the wrong version so that you can make a point.
00:04:52.144 - 00:04:52.780, Speaker C: Right.
00:04:56.110 - 00:04:58.858, Speaker B: And Alice can fix.
00:04:59.024 - 00:04:59.450, Speaker C: Right.
00:04:59.520 - 00:05:05.950, Speaker B: If she has some function that will verify the Bob data.
00:05:06.100 - 00:05:06.800, Speaker C: Right.
00:05:08.210 - 00:05:09.470, Speaker B: Bob's bytes.
00:05:10.130 - 00:05:10.590, Speaker C: Right.
00:05:10.660 - 00:05:58.590, Speaker B: And then it'll be either whatever, like true or false. And there's a number of functions that we could use here, but what we're going to do is we're going to use hash functions. And the reason why hash functions are nice is that the reason why hash functions are nice is that, you know, sort of exactly what you are getting, right. They actually solve both of these of. And this will change effectively what Alice is asking for. So Alice is no longer, Alice is not going to ask, right.
00:05:58.660 - 00:05:59.006, Speaker C: Sort of.
00:05:59.028 - 00:06:39.500, Speaker B: Version one is she asks Bob for war and peace, right? But who knows what war and peace means? And how do I know? Bob sent it to me correctly? So this isn't going to work. Instead, we're going to have Alice ask for some hash of war and peace, right? Which means that she sort of knows exactly which edition she wants, or you can't hash it, or she couldn't have received a hash of what it might be. And it definitely won't be malware, because when Bob sends the bytes back, she'll be able to hash it and we'll do a match.
00:06:39.870 - 00:06:42.380, Speaker A: So it's the hash of the file that is being.
00:06:43.010 - 00:06:55.700, Speaker B: Yeah, right. So, exactly. So this thing is called, and this is sort of the IPFs thing, is content addressing, I. E. The way in which you refer to the content.
00:07:00.630 - 00:07:01.378, Speaker C: Right.
00:07:01.544 - 00:07:46.980, Speaker B: The way in which you refer to the content is by what the content is itself, right. With this digital fingerprint, this hash function of the data, this means, in a sense, there's only one of these things, right? No matter where the data is stored, no matter who has it, no matter what system it's in, there is only one piece of information that can correspond to this hash. And so now that sort of, we're on the same page of what we're talking about, which is, how do I ask for data? I ask for it by its content, then ipfs can start to work on problems of how do I find this thing now?
00:07:47.670 - 00:07:48.178, Speaker C: Right?
00:07:48.264 - 00:09:13.040, Speaker B: And so sort of the other IPFs thing is to somehow the other IPFs thing is to take a mapping of a content address and turn it into some set of people who have the data, we'll call them providers, providers of the data, of the content, right? People who have it and are willing to give it. To me, this is the function ipfs needs to provide. Once we're on the same page about content addressing, I want to back up a little bit on this interplanetary business, I'm going to sort of make up a concept I haven't seen other people refer to explicitly, and we'll see how it goes, which is we'll call it the benefits of this interplanetary property, right? So you might say, all right, well, I mean, nobody's going Alpha Centauri anytime soon, so why do I care? So we'll call this interplanetary thing.
00:09:17.330 - 00:09:17.790, Speaker C: Right?
00:09:17.860 - 00:09:34.530, Speaker B: So we have, obviously we have Alice's five light years away. Years away. We have Alice's, we'll call it Alice and Bob are infinity light years away. Or they're offline.
00:09:35.050 - 00:09:35.510, Speaker C: Right.
00:09:35.580 - 00:09:41.478, Speaker B: As far as the network is concerned, they might as well be infinite distance away. They're never being.
00:09:41.644 - 00:09:44.518, Speaker C: Right, right.
00:09:44.684 - 00:10:12.966, Speaker B: Alice and Bob are offline. You've got things. Just know here on Earth, sometimes latency, the speed of light, just isn't quite good enough. The ping from where I am in Boston to Australia is like 250 milliseconds. There are some bad ones from Africa and Australia to each other, which are like 500 milliseconds.
00:10:13.018 - 00:10:13.474, Speaker C: Right?
00:10:13.592 - 00:11:00.240, Speaker B: Satellite communications get even worse. Just like long pings, even here on earth is a big problem. And also, just like bandwidth, some connections are more expensive than others. Maybe your phone is metered, but your cellular network is metered, but your wiFi is not. And so you would prefer to talk to people over wifi than over the cellular network. And the fact that you don't really have to care where the data comes from allows you to then optimize on top of this blank canvas and say, where do I want to get the data from?
00:11:02.050 - 00:11:11.106, Speaker A: But in general, if I'm downloading one file, will it be downloading it from a single peer? Or does APFs provide something inherently that will allow me to stream it from.
00:11:11.128 - 00:12:44.098, Speaker B: Multiple people, from multiple peers? So this is why over here, and I guess, sorry, I don't even know why I did this. I write and go these days, but it's not like, not my native language, right? This is a set of providers to the content, right? So yeah, IPFs will let you get data from multiple people and this is like a thing that is working how it should. In particular, you're going to get hit as soon as you try and find multiple people, if you don't know them all in advance, if you have to do any sort of exploring and from an application layer, you might know exactly where this data is supposed to be hosted or might be hosted. But if you're going to do any sort of exploring for where the data might be, you're going to hit some latency cost, right? You're going to have to do a little bit of exploring and that's going to get some latency. But if you can make it up in throughput, then it won't be such a big deal, right? If it's a gigabyte file and it takes you an extra like 2 seconds to start downloading, doesn't really matter obviously, for things like video and stuff. Could be sensitive that first little bit, but maybe not. All right, so this is sort of this interplanetary property.
00:12:44.098 - 00:13:23.760, Speaker B: The next is ipfs is immutable, right? We have this hash function. And so like hash of war and peace isn't changing, book isn't published. But let's say we're working on some text file. There might be version one, or we're working on an academic paper, there might be a version one of the paper and there might be a revised version two of the paper. These will result in two different content addresses. They're not the same thing. And if you think about this.
00:13:27.330 - 00:13:27.598, Speaker C: If.
00:13:27.604 - 00:13:35.300, Speaker B: You go to Wikipedia, it gives us like versioning and specificity, right? If you go to Wikipedia and.
00:13:38.310 - 00:13:38.626, Speaker C: You.
00:13:38.648 - 00:14:13.920, Speaker B: Look at the references at the bottom and then you see like, the references are out of date or they don't really make any sense. That's because you're getting this mutable link to a website, which is in a sense confusing. It's not really the information you're looking for. You know, this happens all the time with, I mean like Twitter for. For sure, right? People will put links to tweets and then they just won't be there. But even for things that change, right, you go to a website, the website's not there anymore. The website looks totally different now.
00:14:14.930 - 00:14:20.560, Speaker A: But something closer to apfs would be if Wikipedia was posting links to archive, right?
00:14:23.410 - 00:15:05.594, Speaker B: I'm trying to remember, right? Archives, archive maybe on a particular date, right? Because if archive is not going to change anything, right? They're promising they've downloaded it. Once they're stamping it, then yeah, it would be more like that, which is nice. And it also means that if you start to think about the immutable, actually, I'll come back to that in a minute. Before I go too down the immutability rabbit hole, which is going to take us for a little bit, I just want to briefly address that. We have ipfs as content addressing, which is immutability.
00:15:05.722 - 00:15:06.014, Speaker C: Right.
00:15:06.052 - 00:15:55.886, Speaker B: So this is ipfs. We can come back to this later. But there is ipns, which is interplanetary and is a naming system for mutable data, which is for mutability. But in order to get the interplanetary properties, it needs to be self certified. And so one way to do this, and there are a number, is. But IPNs uses author addressing, right? Again, there's probably a more literature, generic word, but this is what it is. The author is the address.
00:15:55.886 - 00:16:28.058, Speaker B: We use the author addressing because we're using public keys for addressing, and the author owns the corresponding private keys. And these are just examples of self certified, which means that the address tells you how to verify the data you got is correct, which helps if you're letting a random person just send you information.
00:16:28.144 - 00:16:28.394, Speaker C: Right.
00:16:28.432 - 00:17:52.082, Speaker B: You need to know it's correct. All right, so back on the immutability train. Once we know we're dealing with immutable data, we can start to think a little bit about other properties that help us. So another one we'll call it is another one is deduplication, right? If I know that the data can't change and that this thing with this hash or this address is always exactly the same, I can leverage, in a sense, these slightly more clever schemes or make things more efficient. So say I have like an append only log or a blockchain, and I have whatever, 4321 and then I make a new operation, five, right? And then 54321, if I ask for the entire data structure from five, having already downloaded the whole data structure from four, I'm only going to download this because I know these two things are equivalent because they're hash based, right?
00:17:52.136 - 00:18:00.920, Speaker A: But that's assuming you don't load every block separately, right? If literally the entire data structure is a single file, the duplication wouldn't trigger, right?
00:18:03.290 - 00:20:29.350, Speaker B: Depends, yes. So you will have to think about how you structure your data to grant you some of these abilities, right? If you take a, I don't know, let's say you have some JSON file, right? And you're trying to figure out how do I make any use of it? How do I get some deduplication out of this? You're probably not going to want to just slice every 10th byte and treat it as arbitrary text, right? You're going to want to think a little bit about how you want to structure your data so that it looks a little more graph like, so that it has some of these linking properties that you might want. Which brings us to the next thing I want to talk about, which is, and for all of you out there, I'm sorry about all the acronyms, I swear. IPLD, which is, again, got this interplanetary property link data, right, which if you're familiar with JsonLd and et cetera, is basically, I want to be able to describe sort of normal things like lists and maps and strings and integers, but I also want to be able to describe a link to another piece of data instead of just having nested structs all the way down. I want to be able to actually have a thing in here that says link. Here's where the rest of the data lives. And an IPLD is at core, it's like a very generic form for how to describe this information and how sort of the generic properties or how we arrive at this, we'll go into a little bit, but high level from the linking is, let's say you wanted to do, I don't know, you have some root, all right, you have some root, and it's got number one over here on the left, and it's got number two over here on the right, these two different nodes.
00:20:29.350 - 00:20:57.486, Speaker B: You might look at the structure and say that it's something like left is number one and right is two. Right, but where one and two are these linked objects. So there's some. It's like the link to one, and like the link to two link in.
00:20:57.508 - 00:20:59.326, Speaker A: This case would be just the hash of data.
00:20:59.508 - 00:21:42.862, Speaker B: Exactly. Where the link is the hash of the one object and the hash of the two object. So in order to try and make these things, I've watched a number of the sort of great video series you guys have put together, and there's a lot of Merkel trees and a lot of different projects, a lot of different blockchains and other sorts of projects in this dweb space. And everyone's doing it a little bit differently for pretty good reasons. But the underlying primitive is the same, which is why when people say merkle tree, they tend to just sort of gloss over exactly what it is.
00:21:42.916 - 00:21:43.134, Speaker C: Right.
00:21:43.172 - 00:21:56.574, Speaker B: But it's just the hash. You have hashes you have the objects in the child nodes and then in the child nodes, and then the parent node is just the hash of the children nodes.
00:21:56.622 - 00:21:57.220, Speaker C: Right?
00:22:00.710 - 00:22:14.982, Speaker B: So we want to build something that allows you to describe these various different types of hash link data structures, no matter how you create them.
00:22:15.036 - 00:22:15.640, Speaker C: Right.
00:22:16.330 - 00:23:22.620, Speaker B: And to do this as a base, we're going to start with another project which is called multiformats. And multiformats is a really simple idea. The basic idea of multiformats is that for the low, low cost of a few bytes, you can know what the data is. So one of these is multi hash. Um, people have different hash function. There are different hash functions. People have very good reasons for preferences for, for these.
00:23:22.620 - 00:23:51.846, Speaker B: But as soon as you try and change the hash function you're using, things start to get, or you start to allow for the use of multiple different hash functions. Things start to become complicated because now, how do I figure out whether this 256 bits of data is a shot 256? Is it the first half of the digest of a shot 512? Shotsu 512? Is it a shot three? That's 256 bits. Like, what is this thing? So we'll describe it as, we'll have.
00:23:51.868 - 00:23:52.600, Speaker C: A code.
00:23:55.610 - 00:24:16.350, Speaker B: Just some variant, some variable length integer that will generally be like one byte or something. We'll have the digest size in bytes, which will also be a varint, and then we'll have the digest.
00:24:17.330 - 00:24:18.080, Speaker C: Right.
00:24:21.410 - 00:25:25.970, Speaker B: I can say that for shot 256, shot 2256, the code is zero x twelve. And I'm using the full thing, which is 20 and then some bytes next up. So this tells me I can use whatever hash function I want for the links, which is good. Now people serialize the data in different ways, so we need to account for that. Some people are going to the Ethereum blocks don't look like the bitcoin blocks, which don't look like JSON, which doesn't look like seabore. So we need some way to describe how we transform or how we abstract over sort of the core linking properties. And for this we will use a codec, a multicodec.
00:25:25.970 - 00:26:09.842, Speaker B: And this is really simple. It's just a code. It's just a code. In some table, some examples include, there's one that's called dag seabore, which is sort of include a way of using something that looks like Seabore to encode directed acyclic graphs. We have like a bitcoin. You can have a bitcoin transaction, you can have raw data. Just say, I don't know what this is.
00:26:09.842 - 00:27:14.200, Speaker B: You can have git and there's a whole bunch of these, right? And if your project is already using something that is a content addressed format, and then you want to start using any existing iPLD tooling with it or IPFs related tooling with it, you can just write a codec, reserve the code in the table and your stuff can interoperate. And then you can combine these things and you have what we call a CID or a content identifier, which people will sort of, people will refer to content identifier as hash. They'll sort of use these interchangeably. But it's a little bit more.
00:27:17.050 - 00:27:17.474, Speaker C: It.
00:27:17.532 - 00:27:23.690, Speaker B: Is sort of the version number of the CID format or the version.
00:27:30.450 - 00:27:30.938, Speaker C: Codec.
00:27:30.954 - 00:27:32.510, Speaker B: And then it's the multi codec.
00:27:35.490 - 00:27:35.854, Speaker C: And.
00:27:35.892 - 00:27:37.274, Speaker B: Then it's the multi hash.
00:27:37.402 - 00:27:37.694, Speaker C: Right.
00:27:37.732 - 00:27:50.420, Speaker B: We've basically just glued these things together with, again, this extra property where, hey, we might need to change this thing in the future. So if we just throw in another varint in front, we can make sure that we can upgrade this if we need to.
00:27:50.870 - 00:27:53.410, Speaker A: So why is it a multi hash and not just a hash?
00:27:55.990 - 00:28:04.726, Speaker B: Because if I'm using, so I can be using, say, dag seabore, and the links can use shot three or shot two.
00:28:04.828 - 00:28:05.526, Speaker C: Right?
00:28:05.708 - 00:28:24.730, Speaker B: Dag Seabore just tells me how am I going to read these bytes off of disk and turn it into a graph? I see, right. But it doesn't tell me what specific type of link is being used in order to access this graph.
00:28:26.670 - 00:28:31.710, Speaker A: And so in this case, multicodec and multi hash, that's going to be two arrays of the same length.
00:28:32.710 - 00:28:55.960, Speaker B: No, a multicodec. Okay, I'm going to move on to the next thing and then we'll let me know if I covered it. One more thing, just for completionist sake. We have something which we use in CIDs, which we call multibase, which says.
00:28:56.330 - 00:28:59.226, Speaker A: Hey, it's below the screen right now.
00:28:59.328 - 00:29:34.114, Speaker B: Oh no, thank you, I will go down. We have multibase, and that is the same concept as all the other Maltese, which is for the low cost of a character. I can figure out what base encoding this thing is. Do you like base 64? Do you like the bitcoin base 58 encoding? Do you like base 32? Do you like hex? This shouldn't be related to your data. It's really just the way you display it. And so we can just reserve some characters in a table. That prefix, the thing.
00:29:34.114 - 00:29:51.382, Speaker B: So we have like z is for base 58 or the bitcoin base 58 encoding. We have b is for base 32, and then we have say like f is base 16 or hex.
00:29:51.446 - 00:29:52.060, Speaker C: Right.
00:29:54.910 - 00:30:02.942, Speaker B: So like an example, hex encoded CID. I like hex encoding sometimes makes it a little easier to see where all the bytes are.
00:30:02.996 - 00:30:03.600, Speaker C: Right.
00:30:04.850 - 00:30:55.310, Speaker B: Would be f for hex one because that's the codec for CID version one, which is one we're using right now. There is one that is. So if I just do like IPFs added a file, what I will likely get is something like this. Say I have the CID version I can make in version one, there's a codec called Dag Pb which is 70 and that uses shot 256, which is twelve, and it uses all 256 bits of that, which is 32 bytes. And then we have the digest.
00:30:59.300 - 00:31:00.050, Speaker C: And.
00:31:00.740 - 00:31:02.160, Speaker B: This is your CID.
00:31:04.840 - 00:31:06.790, Speaker A: So what does multi stand for?
00:31:07.240 - 00:31:10.308, Speaker B: The multi is the fact that it can represent many things.
00:31:10.394 - 00:31:10.740, Speaker C: Right.
00:31:10.810 - 00:31:28.136, Speaker B: So the multi base says any base could go here and everything. And this should, you can substitute any base here and have a valid object. Right, I see, right. Multicodec says, hey, any IPL, you know, says like substitute any IPLD codec in here.
00:31:28.238 - 00:31:28.552, Speaker C: Right.
00:31:28.606 - 00:31:32.892, Speaker B: Anything that will tell me. And substitute any hash functions. That's what the multi is for.
00:31:32.946 - 00:31:36.924, Speaker A: Yeah, I misunderstood. I thought that multi hash refers to having multiple hashes there.
00:31:37.042 - 00:31:38.380, Speaker B: No, one of many.
00:31:38.450 - 00:31:39.164, Speaker C: Good.
00:31:39.362 - 00:32:50.870, Speaker B: It is a good question. But yeah, the multi is for the sort of the substitutability property. And so with our, you know, now that we are armed with cids that we can use as links everywhere now, so that we've sort of generalized this hash thing for some context, there existed Cidv zero and Cidv zero didn't have a lot of these multi things, it had multi hash, but it was basically like, oh yeah, we'll just use Shotsu 56 and we'll assume that it's always Dag PB and whatnot, and it's always going to be base 58 encoded. And these things all eventually came to bite us at some level. And so working with primitives based on multiformats, which is like super lightweight, and you can just implement the parts that you need, right. You don't need to implement base 32 in your application if you're only using base 16 and base 64. I highly recommend using some of the multiformat stuff because it's super lightweight and will likely save you down the road.
00:32:50.870 - 00:33:46.070, Speaker B: We recently ran into an issue where people like to use ipfs in web browsers, and people like to use ipfs in web browsers, and there is a finite length to DNS labels. You can have whatever, it's like 63 characters, something like that like 60 characters or something. And we wanted to use base 32 encoding because it was efficient and whatnot. But for certain pieces of information we wanted to put into the URL, it was just a little bit over the limit. It was like a couple of characters over. So we switched it to base 36 encoding instead. And that was not painful, right? Or not particularly painful, because it's just another way to represent the data.
00:33:46.070 - 00:33:57.112, Speaker B: And I guess just for somewhat completionist sake, can you give me a time check briefly? Because my timer reset over here. How are we doing?
00:33:57.166 - 00:34:00.270, Speaker A: I think we're like 40 minutes in, maybe a little less.
00:34:00.640 - 00:34:05.550, Speaker B: Okay. All right. I think we're good on time.
00:34:08.320 - 00:34:08.636, Speaker C: But.
00:34:08.658 - 00:35:11.936, Speaker B: We'Ll see how it goes. So the iPLd data model, I guess we'll say, like the base thing that you might want to know about iPLD is it's going to have all the normal JSon things like we talked about, whatever. It's sort of the iPLD data model. It's basically sort of normal JSON data model. Plus we want raw bytes because these are really nice for things. You want to be efficient and we want links, right? And then you can pass through things. So say I have some CID encoded in base 32, whatever, Baffi ABC, that can refer to some content, but I can also path into it.
00:35:11.936 - 00:35:57.148, Speaker B: So I can say like, baffi ABC, field one or whatever. Hello. Or field two. I can path into it by just treating it like it's a struct and I'm going through map elements, right. Like if you're writing this in your favorite coding language, you might be using dots instead of slashes here, right. But you're sort of pathing and traversing through. Does that make sense or seem reasonable? Yeah, okay, I'm muted.
00:35:57.164 - 00:35:59.072, Speaker A: I didn't realize. Yes, that makes sense.
00:35:59.206 - 00:37:04.356, Speaker B: Okay, cool. All right, I have to switch back. Windows double check. All right, so a little while ago you brought up this point, which I was very good, and I totally glossed over it. I was hoping my answer would be good enough to last me until now, which I think maybe it did, which is you said, well, this deduplication stuff doesn't really work so well if everything is all in one file, and you're totally right, you're going to have to figure out it is nice for other reasons relating to, call them, like, protocol level sympathies, and also this deduplication where you might want to chunk the file up, a big file up into smaller pieces. Even if you're talking about something that's just raw bytes and it's not going to have a dag structure. And even if you're not going to create some graph structure around this, which is highly optimized, and you're going to use some existing thing like a zip file, you may still want to chunk up the pieces or a video or whatever.
00:37:04.356 - 00:37:16.296, Speaker B: You may still want to chunk up the pieces smaller. So you can do things like allow multiple people to download pieces at the same time.
00:37:16.398 - 00:37:17.000, Speaker C: Right.
00:37:17.150 - 00:38:20.140, Speaker B: In order for me to download some data from an arbitrary person and be okay with the fact that I don't trust them, it needs to have some hash associated with it, right? Some way I can verify the data. If the only unit of verifiability is gigabyte blocks, then I have to download a whole gigabyte before I know if it's any good, which is one problem. And I also can't ask two different people to give me half of the file because what if one of them is lying? I don't know who lied. I can only hash the whole thing together. So I want to make these things into a smaller, more digestible unit. So we have one of these things which we call Unix fs, which is just unixfs, which basically just takes file system things, file system objects, and turns them into dags. And it's not the only way to do this.
00:38:20.140 - 00:38:52.404, Speaker B: I don't think it's not the best way to do this. We've come up with other things we'd like to do since then that are being worked on. Because even you have things like you have a file system and the file system maybe has like a directory, and the directory has another directory which has files and that one has files and so on. And you need to somehow turn it into some dag where everything is a little more broken up somehow, arbitrarily.
00:38:52.452 - 00:38:52.904, Speaker C: Right.
00:38:53.022 - 00:39:25.750, Speaker B: Your files are too big. You want to chunk them up. Your directories are too big, you need to chunk them up. And there's all sorts of ways you can do this. If we talk about files in particular, you could do something simple, which is you could just say, hey, I'm just going to make these all, like, these are all just like 256 kilobyte blocks. You could do that. This is okay if you're going to make like an append only kind of thing, right? Because then you'll just keep adding more to the end.
00:39:25.750 - 00:40:11.210, Speaker B: You'll just keep adding more to the end. But it's not great if you're going to insert something in the middle, right? Because then you're going to throw this whole chunking scheme out of whack, and your deduplication is going to go down. So for this, you can use. There are content based chunkers that take into account the data that you are hashing in order to figure out, or the data that you are processing to figure out where would be a good place to break it up. And those are a little more amenable to allowing for extra data in the middle. But this is just sort of like one area of exploration and relates to just one particular type of codec or one particular type of. Not even the codec.
00:40:11.210 - 00:40:13.374, Speaker B: This is a layout on top of it. This is.
00:40:13.412 - 00:40:13.710, Speaker C: Right.
00:40:13.780 - 00:40:21.250, Speaker B: How do I create my data on top of the existing codec? That just helps me do, effectively serialization of the DAG.
00:40:22.230 - 00:40:22.980, Speaker A: Right.
00:40:24.950 - 00:40:31.346, Speaker B: Do you see how these two things are separate? Like, the structure of the data is independent from the serialization format I happen to be using?
00:40:31.448 - 00:40:36.262, Speaker A: Yes. Though I think it brings in an interesting point that in a very simple model.
00:40:36.316 - 00:40:36.486, Speaker C: Right.
00:40:36.508 - 00:41:07.874, Speaker A: So, like, at the very beginning, we started with a model where there is a file and there's a hash of it, right. And so in that model, I think it was implied that the hash is predefined. In that case, if there is a particular file, that file has only one address. But once we have multiple hashes, multiple ways to chunk it up, suddenly the same warrant piece can exist in multiple. Like, if I'm uploading warranties, the probability that the hash will be the same as the same uploaded by someone else is very little now.
00:41:07.912 - 00:41:56.574, Speaker B: Right, right, totally. This is a really. Even if we don't really have that much time, I think we should spend a few minutes on this because it's really interesting. And there's some nuance here, which is absolutely, in the abstract sense, these two things that are chunked up differently are not the same. Right. If my machine has two megabytes of ram, and one version of this had things in 1 mb blocks and the other had five megabyte blocks, I could reproduce one of these. I'd be like, I'm having problems with this one dag, but not the other one.
00:41:56.612 - 00:41:56.766, Speaker C: Right.
00:41:56.788 - 00:42:10.326, Speaker B: So these things are not the same, but yet they are. Right. The thing that most people care about when describing this is basically what happens when I concatenate all belief nodes. What does that look like?
00:42:10.508 - 00:42:11.240, Speaker C: Right.
00:42:13.530 - 00:42:14.642, Speaker B: And there's multiple.
00:42:14.706 - 00:42:14.982, Speaker C: Right.
00:42:15.036 - 00:42:41.550, Speaker B: You can have, in a sense, there's one hat like hash goes to data. Exactly. Once. This is a one to one mapping. But data can go to many different hashes or cids.
00:42:43.010 - 00:42:43.758, Speaker C: Right.
00:42:43.924 - 00:42:54.754, Speaker B: We're talking about this. Maybe this is the backwards way to describe it. Right. But yeah, that's totally right. In figuring out whether.
00:42:54.952 - 00:42:56.420, Speaker A: Right. It's the other way around.
00:42:58.150 - 00:43:23.520, Speaker B: It depends on how you think about it, I guess. Whether data means, like, the data I've received over the wire, or data means the concatenation of the leaf nodes that you care about. As I was writing it, I was like, I don't even know what the right ways to describe this, which is like, it's a little tricky, but the question is, does this matter? And I think there are certain cases where it does matter, but in those cases, I will call them, like.
00:43:25.730 - 00:43:26.046, Speaker C: I.
00:43:26.068 - 00:43:34.986, Speaker B: Don'T want to say legacy, because legacy seems to imply, like, we are the future, they are the past. But I'll say, like, interoperability with existing systems.
00:43:35.178 - 00:43:35.920, Speaker C: Right.
00:43:38.130 - 00:44:18.670, Speaker B: What would be nice to have is if I could go. If I go to the Ubuntu website and I try and download the. And I see the ISO, and they post the Shaw 256 of the ISO. If I could search the IPFS network to say, hey, I'm looking for this Shaw 256, anyone know where it is? But someone could be storing it using the sha three. There's any number of things that could change here, even without the dag itself, even just the hash function referencing, it could change. And so it would be nice if I could do that. We can talk about that in a sec.
00:44:18.670 - 00:44:26.126, Speaker B: But for a lot of cases, no one really wants to interact with a hash.
00:44:26.238 - 00:44:26.900, Speaker C: Right?
00:44:27.350 - 00:44:36.206, Speaker B: No one really wants to look at these long strings of numbers and letters, right? These can occur behind the scenes.
00:44:36.318 - 00:44:36.980, Speaker C: So.
00:44:38.730 - 00:45:03.950, Speaker B: I don't go to most URLs by typing them in the address bar, right. This is not what happens. Certainly anything beyond the root of the website, google.com, et cetera. I don't type into the web browser. I navigate to it through some other linking. And so the linking is precise.
00:45:03.950 - 00:45:21.220, Speaker B: If I add some data set and then people start using that data set, they're not going to be re adding it and chunking it themselves. They're going to be using my graph, they're going to be downloading the graph from me and then resharing that one.
00:45:22.550 - 00:45:22.962, Speaker C: Right.
00:45:23.016 - 00:46:31.340, Speaker B: In a sense, this is like how the web is built. And so it's one of these things that it hurts, but it doesn't hurt that bad. And there are, I think, schemes that you could use to get around this. You could have schemes, some of which could be like, kind of really, you could go like full trust, or not full trust, but high trust and be like, okay, I'm just going to ask you to send me the shots of 56 of the whole file. If you have it, you'll keep a separate index that has it for files in particular, and then you could send it to me and I have to download the whole thing before I can check it. I could do that, but that would be a little unfortunate, right? We'd lose some of these other properties. You could take advantage of the fact that things like Sha one and SHA two are streaming hashes, and I could download the file backwards so that I could keep confirming that I could download it from the end going back, so that I could keep confirming that the blocks I'm downloading can hash into the final result.
00:46:31.340 - 00:46:46.960, Speaker B: I could use some sort of zero knowledge proof to prove that two dags are equivalent under some set of properties that I care about. I think those are interesting, but.
00:46:49.490 - 00:46:49.866, Speaker C: It'S.
00:46:49.898 - 00:47:44.640, Speaker B: Not brutal that they're not there. Okay, so with some time remaining, because we got a bunch of other stuff to cover here. So, talked basically about the why we care about content addressing and then what the content address data structures look like. There is also how we transport things and how we transport data around. Exchanging data. Exchanging data and finding who has it. Who has the data.
00:47:44.640 - 00:47:49.506, Speaker B: IPFs is built on lib p to.
00:47:49.528 - 00:47:50.100, Speaker C: P.
00:47:52.950 - 00:47:59.650, Speaker B: Which is a peer to peer networking stack. We're using it for all sorts of stuff.
00:47:59.800 - 00:48:00.162, Speaker C: Right.
00:48:00.216 - 00:49:09.370, Speaker B: The idea is to make it sort of pluggable to what you need. Do you need TCP or websockets or quick, or do you want tls or noise? Know, what type of multiplexer do you want? It has all these components sort of in there for you. And it also has some higher level peer to peer components, like a pub sub network and a distributed hash table. So for finding who has the data, first we'll talk about how we exchange data. So we have two protocols for exchanging data. The one, which is mostly in use now, is called bitswap, and the other one is called graphsync. Not going to talk too much about graph sync, because it's not what is commonly used with ipfs right now, although it is newer and very useful for reasons that will sort of become obvious as we look at bitswap and bitswap.
00:49:09.370 - 00:49:44.040, Speaker B: And finding who has the data is mostly the DHT, although also a little bit of bitswap, which we'll come to. So bitswap works like this. We have Alice and we have Bob and we have some graph, or maybe we have a set of people, we have whatever we have Bob, we have Charlie, we have Dan, and we have some.
00:49:51.130 - 00:49:51.880, Speaker C: G.
00:49:56.110 - 00:50:14.400, Speaker B: And Alice wants to download the entire graph rooted at a, right? So the root node of this thing is a. So she'll send, she'll ask for a, she'll get it back, she'll ask for b and C, she'll get back.
00:50:16.290 - 00:50:16.654, Speaker C: And.
00:50:16.692 - 00:51:52.480, Speaker B: Then she'll ask know Defg and she'll get them back. And these can be divided up between all of the peers, right? The more peers there are, the more I can send off the blocks, right? So obviously for the first round, a, I can't chunk that one up, I don't have enough information. I don't know what else I need. B and C I can send to two different people. DefG, I can send to three. This whole thing is sort of very simple and straightforward, right? And allows us, given this primitive, we can then start to build on some of the more smarts, right? Let me send requests to people who respond faster and send me data quicker, and let me penalize people who are problematic or our connections drop, things like that. Of course, the unfortunate thing here is obviously in the case where Bob has the data, is the only person who has the data, right? If Bob is the only person who has the data, or your data looks like a chain instead of if your data is deep instead of wide, right? If it looks like this, you're going to be really sad because this whole process is latency bound, right? Yes, latency bound, because in order to know what the next block is to ask for, I have to have already gotten the previous block so I can look at what its children are.
00:51:52.480 - 00:52:50.530, Speaker B: Graphsync, which is, I guess in many ways a successor. You can, Alice can send some selector along with a, so she could say know plus everything underneath, for example, right? Or you could ask for subsets of know. I want Ab and everything underneath. Then I want to ask someone else for AC and everything underneath. So you can do more of that. This will probably make its way into know in the future, but it requires, you might imagine this is a little more complicated to do all of this like, we call it session management. All of the smarts around, who do I ask for data when? How do I divide up all the tasks? All right, so this is transfer protocols.
00:52:51.190 - 00:53:02.146, Speaker A: Bitswap is just like, bitswap is effective. I'm traversing the graph locally. I'm just sending requests and receiving responses.
00:53:02.338 - 00:53:17.642, Speaker B: Yeah, so bitswap is basically, it swaps bits. It knows nothing about the structure of graphs. There's a thing that lives above it. That knows what graphs are. And so it tries to traverse the graph and it's like, I want a bit swap. Do I have a locally. No.
00:53:17.642 - 00:53:19.050, Speaker B: Bitswap, can you get me a.
00:53:19.120 - 00:53:19.738, Speaker C: Sure.
00:53:19.904 - 00:53:46.622, Speaker B: And then it says, okay, well, now I need b bitswap. Do you have b? And so on. And again, it uses the sessions to keep track of the various smarts of who and when it should ask for stuff. But obviously graph sync, if you are graph aware, you can make more intelligent decisions around how to download the whole graph.
00:53:46.686 - 00:53:47.300, Speaker C: Right.
00:53:49.030 - 00:54:01.670, Speaker B: Downloading a blockchain using bitswap would not be nearly as fun as using graphsync because the latency boundness of this long chain could be pretty brutal.
00:54:01.750 - 00:54:02.380, Speaker C: Right?
00:54:06.110 - 00:54:21.600, Speaker B: Yeah. Now the question is finding data. So there are two ways in which we find data. No, all right, two ways in which we find data.
00:54:26.530 - 00:54:27.280, Speaker C: Ah.
00:54:31.910 - 00:55:33.480, Speaker B: One is we just ask everyone we're connected to. We are connected to, right? I mean, those are pretty cheap, right? It's like one just like send a packet, do you have stuff? And they're like, yes or no? And if yes, they get added to the session and then we don't have to broadcast to everyone anymore. Right, because you've already found the limited subset of people who actually have some interest in this data. And the second is we ask the DHT. Ask the DHT and say, who has this? The DHT does not have the data. We ask the DHT who has the data. So this is really, instead of finding data, it's finding who has data.
00:55:33.480 - 00:56:36.006, Speaker B: So, running a little low on time, so we can go into some of this a little bit. Here is the crash course to Kademlia, which is sort of the structure of the DHT that we use. We do some things a little bit differently, but that's like the main underlying component is based on cademlia. Cademlia. The main underlying idea is that you have all these peers that are sort of server nodes in your DHT, and you've ordered them, right, based on some. These are ordered based on some metric below the skin.
00:56:36.038 - 00:56:36.522, Speaker A: Again.
00:56:36.656 - 00:57:31.610, Speaker B: Yeah, sorry. These are ordered based on some metric. These are going to be ordered by the shot 256 of the peer ids of each node. And a peer id is basically a public key. Yeah, it may be a hash of a public key. Also if the public key is too big, RSA keys, we use hashes of public keys, elliptic curve keys, we just use the public keys because they're small enough, but they all have to be in the same space no matter what key type they are. So we shot 256, all of them we ordered by shot 256 as integer, interpreted as integers.
00:57:31.610 - 00:58:14.460, Speaker B: Okay? And now they're all ordered. And what we're going to do is basically treat this thing like a skip list, right? So in a skip list you are connected to people 1248 away, right? And so you can get to the 70th element. You can do all of those in logarithmic number of hops, but you can't just use a skip list because computers go on and off the network all the time, right? So imagine you were trying to build a skip list and chunks of your memory started disappearing and new parts started popping in at random intervals. That would not be happy. So instead what we do is.
00:58:29.250 - 00:58:29.614, Speaker C: Every.
00:58:29.652 - 00:58:31.230, Speaker B: Peer has a routing table.
00:58:33.090 - 00:58:38.900, Speaker C: And, sorry.
00:58:43.110 - 01:00:16.740, Speaker B: Has a routing table where you're basically tracking the peers that are closest to you that are closest to you. In reality we're actually using the XOR metric. So what it'll be is, I'll say I want to keep track of the peers where, say zero. I want to keep track of the peers where me XOR Peer is like where we have zero bits in common, where we have our common prefix of bits is zero. So I guess the metric is what happens underneath. I guess this is really common PrEfix length is zero and then one and 256. Although realistically you really only need like 15 of these things or 20 of these things, right? Because the closer you go, the ods are, that somebody is going to be that close to you is zero, right? Nobody's going to have all their bits in common with you.
01:00:16.740 - 01:01:09.730, Speaker B: It's like breaking a hash function. I'll keep some number of these guys, maybe I'll keep 20 maximum in each bucket. And so that will allow me to sort of jump around the routing table because everyone always knows the people closest to them. Everyone knows always the 20 people closest to them because those guys, those will be down here in this end and those tables will be sparse. I'm not going to run out of slots for them. And so I can always get traversed closer and closer because I ask this node, hey, do you know anyone closer to the target I'm looking for? And they say, yeah, I know this guy. And you ask them, do you know anyone closer? And so on, until you find the target.
01:01:09.730 - 01:02:26.522, Speaker B: In order to sort of, I guess the generic form of this is that when we do a putget, we will take, we'll call it like sort of the key, and then we'll shot 256 it to put it into the same space. And then we'll say the rule is that some numbers say the 20 closest people, 20 closest peers to the key, store it, they store that record. And in the case of ipfs, these records are what we call provider records. So there's not just one record, they're additive, right? Because Alice says I have the data and Bob says I have the Data, and Charlie says I have THE Data, AnD thEy All eNd up iN the same place. They're additive on the same key. That sound good? Makes sense.
01:02:26.656 - 01:02:28.154, Speaker A: Okay, that makes sense.
01:02:28.352 - 01:02:55.554, Speaker B: Okay. And this is basically KadEmlia. Kademlia has logarithmic lookup time. Logarithms are good. They make this really small. The number of hops you need to find data should be like three at most, right? As these networks get large, maybe get up to like four. I mean, as these networks start to get really large, they go up.
01:02:55.554 - 01:03:59.958, Speaker B: But the constant number of hops is not that big. And if you start to get really concerned with some of the latency of these things, you're like, well, this 1 kb text file, I don't want to wait, or web page, I don't want to wait for three hops. Three hops is a whole bunch of hundreds of milliseconds. There are other schemes you could use to speed things up, right? There are schemes like coral which focus on how do I create latency ring based dhts, so that I sort of create smaller groups before I go up to the larger groups so that I can find the data closest to me more efficiently. So there's other sorts of schemes here. But reminder, it does not matter what this routing, right? The DHT is not intrinsic, it's like a thing that ipfs has to help you find stuff, right? But you can find content however you want to. You can plug in some external mechanism if you're using it.
01:03:59.958 - 01:06:05.390, Speaker B: Building ipfs as a library, it's like pretty easy, as long as it fulfills this contract of I give you a CID and you give me people who have the CID, you're good to go. And even if you're using it as a binary, if you have some ideas as to who might have it, you could connect to those peers and then request the data via your external system. And this whole stuff with bitswap asking everyone will guarantee that you get it too. And there are people, there are people who are building on top of ipfs in various ways who, we've improved the DHT a lot in the last year in particular, and there's still some really nice big chunks of work that we've got ahead, but for the people who've had more specialized use cases where they need to find other ways to do this, they've been able to build other routing systems, or they're in the process of doing so, because why not? Ipfs is this flat space, right? Everybody's hashes, they live in the shared space. If there's a CID of data stored in swarm, because swarm can use cids to refer to data, and we have a swarm codec, which we do, which there is, right? Then realistically I could look for swarm data in ipfs, and if anyone who was publishing it, and if I knew where to go look for them, I could set up a routing system. That way, if they were providing and saying I have this data, and telling everyone in the public DHT that they have it, I could find the swarm data in ipfs and that would just work. So I would say, I guess the key takeaway for me, at least with this stuff, is content addressing is really nice, because I do not have to care where the data comes from, not just from which IP address has it perspective, but even from a system perspective, there are blockchains that incentivize data storage.
01:06:05.390 - 01:06:50.750, Speaker B: And it doesn't matter which one of those you would use, right? Doesn't really matter if I'm storing the data on Amazon, it doesn't matter if I'm running my own, doesn't I don't have to depend on sort of the success of other systems. I don't have to be tied so deeply to them. I can diversify in the same way I don't have to be tied to the success of a particular serialization format by PLD. I can diversify. Same with all of these things. The idea is the stack has been designed to try and make it as easy to swap out things as possible, so that we don't have to guess in advance what's going to become successful. We can adapt to the ecosystem and how it is evolving.
01:06:50.750 - 01:07:35.530, Speaker B: And I think that is something I would recommend other projects to look at, because that's been, I think, very helpful for us and also helps with our engagement with other people in the web3 ecosystem is like, we just want to work with them because we want to make sure that if they have good ideas, that people who use ipfs can make use of their good ideas. This sort of like collaborative open source thing where you don't need to use everything, you just need to use the parts you need. And I think we are out of time, but we can talk about naming thing another time, because mutability is a whole other can of worms.
01:07:35.610 - 01:07:38.030, Speaker A: Yeah, I think we covered a lot of stuff today.
01:07:38.180 - 01:07:38.880, Speaker C: Yes.
01:07:39.890 - 01:07:47.802, Speaker A: Thanks a lot for giving us the overview and. Yeah, that was very exciting.
01:07:47.866 - 01:07:48.254, Speaker C: All right.
01:07:48.292 - 01:07:56.250, Speaker B: Thank you. Happy to be. Here's.
