00:00:04.410 - 00:00:21.082, Speaker A: Okay. Hello, everyone. This is Bowen from Nier. Welcome to another episode of the session. And today it's our honor to have folks from Lekisync here. Let me introduce you. We have Marson and stats from Metalabs.
00:00:21.082 - 00:00:30.438, Speaker A: And. Yeah, do you guys want to talk just like briefly introduce your often, or talk a few sentences about what you guys are working on?
00:00:30.604 - 00:00:54.560, Speaker B: Right. So we do the ZK. We're from Matterlabs. We're working on the Zk sync, which is currently the top Zk proof l two. And during this whiteboard session, we'll explain to you a little bit more. What does it do, how it works and all the cool things that are already there and even better things that are coming very soon.
00:00:58.690 - 00:01:14.690, Speaker C: I think marches has already said most of it. Zksync is an l two scaling solution. So basically, our job is to make a system which has the same security properties as Ethereum, but a lot cheaper and with battery.
00:01:15.830 - 00:01:36.182, Speaker A: Okay, sounds good, I guess. Yeah. Since you're reading whiteboard sessions, we do very deep technical dives. So maybe it'd be good for either one of you, or maybe both of you, to give us an overview on the technical level, how DK sync works. And then we will dive deep into some specific topics.
00:01:36.326 - 00:01:36.730, Speaker B: Sure.
00:01:36.800 - 00:01:38.502, Speaker A: Feel free to grab the marker.
00:01:38.646 - 00:02:29.878, Speaker B: Right. So basically, during this session, we're going to split it into roughly three pieces. So first we're going to talk about what l two s actually are, how they differ from l ones, like near Solana. Second is we're going to talk about how we utilize the zero knowledge proofs, and how do we compare with other l two s that are depending on optimistic roll ups. And at the end, we're going to talk a little bit more about scalability, how they scale, and how do we plan to scale it even more coming in the future. So, yeah, let's maybe quickly start off what is l two? How does it differ from l one? So the main goal of the l two is you want to have something that can, as touch mentioned, can share the security from Ethereum. If you think about l ones, l ones are like independent islands.
00:02:29.878 - 00:03:11.740, Speaker B: So if you imagine this is like Ethereum and this is another l one, like near, they're fully independent. If something bad happens to near, it won't. But if it did, it's independent from. From Ethereum. They don't directly interact. The goal of the l two here, like Zk sync, means that even if something bad happens to it, the security is actually on the Ethereum layer, which means, and we're going to dive deeper into this in a second. But the idea being that even if something bad happens to the company, you can still get your funds out, you can still recreate the state, even if there are malicious things going on.
00:03:16.030 - 00:03:57.986, Speaker C: So practically, it means that on l one, we would have a smart contract which will store the root hash of the l two state. And the main job of an l two is to. And its architecture in general is to ensure that its state is always updated according to rules that is typically l one. They need some consensus. They already need consensus for, say, security, to make sure that all the computations are valid. But in case of l two s, they're, by design, work correctly if the underlying layer is correct. So that's the main promise.
00:03:57.986 - 00:04:01.306, Speaker C: And that's why building l two s.
00:04:01.328 - 00:04:01.900, Speaker A: Is.
00:04:03.630 - 00:04:15.326, Speaker C: Sometimes maybe a lot easier than l ones, because consensus would be needed for censorship resistance, but for security itself, you can get it right out of the box.
00:04:15.508 - 00:04:22.366, Speaker A: Yeah. So I think you guys mentioned this very interesting part, which is this kind of interaction with l one.
00:04:22.388 - 00:04:22.574, Speaker B: Right.
00:04:22.612 - 00:04:47.938, Speaker A: I think maybe we can dive deeper into what the smart contract does and how it kind of maybe the full lifecycle of interaction between l one and l two. Maybe. Let's start with describing a bit about what that smart contract looks like. Is there anything special to it compared to other smart contract on east?
00:04:48.114 - 00:05:39.862, Speaker B: Sure. So the way, if you zoom in into this picture, basically, if we imagine this is the Ethereum and this is our smart contract in there. So the important features the smart contract need to have is on one side, it should verify, it should verify that things that are happening on l two are actually legit. And later we're going to talk about the difference between optimistic verification versus Ek Singh. But the TLDR is like this contract, if something goes wrong, this contract should react. The second thing is for the whole state recovery, l two is an l two, at least full l two, it's a roll up if you can really recover the state based only on l one data. So this contract is somehow keeping all the necessary states, all the state deltas, whatnot, inside.
00:05:39.862 - 00:05:51.190, Speaker B: And last but not least, this contract is actually also keeping the value. When you think about eth that exists on l two, from the l one perspective, it's hidden inside this contract.
00:05:51.350 - 00:06:17.538, Speaker A: Yeah, I think that the first two parts are, probably has a lot of stuff underneath that. We can have multiple hours of conversation just about them. But maybe let's start with something simple. How does the value part work? Let's say I'm a user, I store some USDT or USDC on Dk sync. How is that actually reflected on l one?
00:06:17.624 - 00:07:01.486, Speaker C: Yeah, so on l one. Okay, so right now I'll talk about the first version of the case, because it was a bit simpler, but the second version is slightly more complex, but not as much. Okay, so there, it worked like that. Users, here's a contract on Ethereum user, there is a function for depositing. And now when users deposit some funds, they transfer them to this contract, basically. And then there would be like a certain, we can say, queue of operations, which coming from l one to l two. Then this deposit, it gets appended to this queue.
00:07:01.486 - 00:08:01.670, Speaker C: And then whenever the operator of the nl two, when it tries to push a new block, create a new block, it sends his transaction here. And besides some various other data about transaction, it also says, okay, I processed some like for instance, three deposits, and then these deposits just get popped out of queue. And if the verify process is secure, as Martian said, it is achieved by different ways in different rollups. But since verify process is secure, we can be sure that if this guy indeed processed redeposit, then it was also reflected in the state route which was published here when the block is proposed. Also the new state route is published as well. So now we know that this state route fill two reflects that certain addresses for which the deposit was done now contain the balances.
00:08:03.690 - 00:08:11.042, Speaker A: When you say deposit here, does it mean, does it work for any ERC 20 token, or is there some restriction.
00:08:11.186 - 00:08:52.006, Speaker C: For any ERC 20 token? It's just that in the first version we only supported, we can call it simple ERC 20 tokens in the sense that if you deposit X tokens, you can only get x tokens back. So there is no room for tokens which you can deposit. And this time you retrieve like two x. In the second version of Zk sync, whenever a deposit is done, it's actually being done as a call to l two contract. And in the second version, this l two contract could for instance, withdraw some larger amounts. But if it implements it on its.
00:08:52.028 - 00:08:55.734, Speaker A: Own, and then for ease itself, you need to wrap it first.
00:08:55.772 - 00:09:04.970, Speaker C: And then like, yeah, for ease, it basically maintains this contract. And on l two it says that, okay, now this balance has certain amount of fees.
00:09:08.750 - 00:09:13.738, Speaker A: Basically, from the l one point of view, the contract is holding all that balance, right?
00:09:13.904 - 00:09:32.462, Speaker C: So from l one point of view, this is just a simple contract. Yeah. Whenever user interacts with it, just normal interaction layer one. And whenever a new block is pushed to l two, it's just again normal interaction with this contract just done by a different user called operator, which operates the blockchain.
00:09:32.606 - 00:09:37.700, Speaker A: And then internally the user balances are kept in the l two state.
00:09:38.390 - 00:09:43.286, Speaker C: Yeah, all the balances, they're plain text. It's only kept in l two. Right.
00:09:43.388 - 00:10:04.010, Speaker A: And the l two state is essentially the. Yeah, maybe let's briefly touch on the more complex topic, or maybe much more complex topic of state. My understanding is that l two state has to be fully compatible with l one state in the sense that you need to use the same state structure and so that you can. It doesn't have to be, okay.
00:10:04.080 - 00:10:29.954, Speaker B: No, it doesn't have to be. The way to think about it is like we can talk about the state part. So there are two cases that we need for state. So the first, the important thing is you need to be able to reconstruct the full state of l two based on l one. So this is one. And the second is state is also used for verification. So the way this stuff is being pushed is we do push, let's call it like a state route of a Merkel tree, whatnot.
00:10:29.954 - 00:10:37.682, Speaker B: But whether it's a merkel tree or a verkel tree or something else, it kind of depends on the contract itself, because this doesn't.
00:10:37.826 - 00:10:42.380, Speaker A: As long as l one, there is a way to verify in smart contract that.
00:10:43.550 - 00:11:05.134, Speaker B: And then roughly there are two approaches of what you can do, what you can put inside the state. So some people, some companies are putting all the inputs of the transactions. So if you imagine the transaction coming into l two, they just put all the inputs. In our case, in case of Ziki Singh, we are putting actually the outputs, putting the state divs off this, right?
00:11:05.172 - 00:11:24.200, Speaker A: Yeah, I think that's a very interesting part. Maybe we'll dive deeper into that after. But just talking about the state itself, you said it doesn't really matter what format you have, as long as you can do state proof and so on. So what does Vicky Singh use today for the state?
00:11:24.810 - 00:12:23.420, Speaker C: Our current state is basically a miracle tree, for which firstly, we have a few shards, which is reserved for some possible features in the future, like Ziki Porter. I'm not sure if we'll have enough time to discuss it, but we do talk about it sometimes. Okay, so the currently used shard. So firstly, there are, I think eight bits, like eight layers. Then the only use shard is the roll up one. And then we have like 256 bit tree. So what one key means, it means just that if some address under some key was written, then the key in this merkle tree, it would be hashed from address and key.
00:12:23.420 - 00:12:42.794, Speaker C: And to add to the point of marching that the state does not need to correspond to ethereum. On ethereum, all the states. And the mainly used hash function is ketchak. Yeah, and everywhere in states, ketchak is used. But in our case, we used a more Zk friendly function. Blake. No, Blake.
00:12:42.794 - 00:12:59.300, Speaker C: We use Blake. We use Blake. And that's one of the examples. So basically, on layer one, we just store this Merkel root, and with each verified step, we prove that. Okay, after this block has happened, we transferred, we correctly transitioned from this Merkle route to some other one.
00:12:59.910 - 00:13:02.542, Speaker A: Is this a procedure or is this, like, record?
00:13:02.616 - 00:13:14.570, Speaker B: So this is actually the interesting part. It has a fixed depth. It's always this deep. This is partially because of the zero knowledge. Proving this makes it far easier to do the zero knowledge proofs.
00:13:15.070 - 00:13:22.606, Speaker A: And then what is the depths? 256. The depths is 200. Okay, I saw the Wiz is 256. But then what is the.
00:13:22.628 - 00:13:25.390, Speaker C: Oh, no, it's depth. Sorry, I meant like.
00:13:25.460 - 00:13:26.080, Speaker B: Okay.
00:13:27.970 - 00:13:30.542, Speaker A: What'S the width on each level?
00:13:30.676 - 00:13:35.602, Speaker C: It's a sparse Merkel tree as needed.
00:13:35.736 - 00:14:06.794, Speaker A: I see. Okay, that's kind of interesting. Yeah, I think that's maybe the question for now about state. Yeah. Should we talk about this? What Dicky sync is doing differently from other people by. I guess this relates to the data availability of l two s, right, Vicky sync is putting state divs on l one versus, I believe, most other l two s are doing the transactions themselves.
00:14:06.992 - 00:14:52.098, Speaker B: Inputs, yes. Transaction inputs, yeah. So this is actually the interesting part, because you could not put state divs if you're using the optimistic verification. So this kind of forces us to talk about the Zk Zk saying. So roughly, when you think about how does the contract on l one verify that the l two did the right job? So there are two main approaches. Right? It can be optimistic, which is basically what's called fraud proofs, or it can be Zk. The thing is, the idea of the optimistic is like, okay, the l two is doing their job, and then it gives people amount of time to submit the fraud proof, basically to submit a challenge.
00:14:52.098 - 00:15:30.840, Speaker B: Right. And if no one submits a challenge for a given time, in case of arbitram or optimism, we're talking like seven days or something like this, then we consider that the whole thing was successful. Now, in case of ZK, we actually, and we're going to go far deeper into this, we actually do the zero knowledge proof of the execution. So now if you think about the status, having statives for optimistic would not work, because in order to create a fraud proof, you need to have inputs for a transaction. You need to rerun the transactions yourself and then say something went wrong. In case of ZK, you actually have a choice. You can do either, you can do either inputs or state divs, up to you.
00:15:30.840 - 00:16:11.042, Speaker B: But in most of the scenarios, not all, but most of the scenarios, the state diffs will be smaller. Of course, there are some use cases where you can say like okay, the call data, the inputs of transactions are tiny, but then it changes a lot of state. But again, in most cases this will be smaller. And the other very cool thing about state diffs is if you do multiple transactions, let's say next to each other, that kind of get batched before pulling to l one. If you modify the same slot multiple times, we charge you only once because we have to write it to l one only once.
00:16:11.096 - 00:16:11.618, Speaker A: Makes sense.
00:16:11.704 - 00:16:35.514, Speaker B: So this allows a very nice compression. And in theory, if we, let's say batch 1 hour worth of transactions, I'm exaggerating here, of course we don't do. But hypothetically, if we did like 1 hour, then all the writes, let's say all the DFI swaps, all the things happening to the same cells, the same slots would get nicely compressed and this would drastically lower the cost of the l one.
00:16:35.712 - 00:16:48.410, Speaker A: Speaking of compression, do you just write that data verbatim to call data, or you actually compress the state diff, like using some compression right now we just write it.
00:16:48.480 - 00:17:48.862, Speaker C: Okay, we actually use a small compression even now. So in the most naive version we could just write key and value, where key is the key in the smirkle tree and the value is just 256 bit value. But we use the following thing, is that if a key which is 32 bytes has been published once, then we remember its id. We kind of like say okay, now we assign this some incremental Id, and then if it's modified again in the future, instead of publishing the whole 32 bytes, we just publish ID and value. And this one is eight bytes. So whenever an account transfers for the first time, his nonsense of balances, et cetera, they need to be initialized. We can call it initialized in the sense that their keys should be published, and that's why it costs slightly higher than whenever they transfer afterwards.
00:17:49.006 - 00:17:50.914, Speaker A: But this is within one block, right?
00:17:50.952 - 00:17:56.462, Speaker C: Or is this no iD? It could be reused for future blocks.
00:17:56.606 - 00:17:59.406, Speaker A: Okay, so it's for all future blocks.
00:17:59.438 - 00:18:00.418, Speaker C: Yeah, for all of future blocks.
00:18:00.434 - 00:18:03.474, Speaker A: So essentially you need to keep this in the state somehow.
00:18:03.602 - 00:18:32.762, Speaker C: Yeah. So actually our key is not just value, but it's actually a pair of value and id. And for each block we know what is like the maximal id which was used. And then we assign just id to new initial rights. We call such rights initial in the order of appearance of the key. I see, yeah. And you asked about within a batch.
00:18:32.762 - 00:18:41.700, Speaker C: Within the batch, we don't even need to use any additional compression because if there are a lot of updates to one slot within one batch, we just publish one key and value.
00:18:42.310 - 00:19:04.330, Speaker B: Because this actually brings an important point that we didn't mention, is that in case of l two, we actually generate, if you imagine multiple transactions are being put together, then we create a one, what we call l one batch. And this is kind of what we prove. And this is what we actually put on ethereum. So the proofs are per whole batch. Right. We don't do like proofs for every single transaction.
00:19:04.670 - 00:19:07.018, Speaker A: Right. Yeah, that makes sense.
00:19:07.104 - 00:19:13.674, Speaker C: And to add about compression, we do plan in the future to have even better one. But that is what we have for now.
00:19:13.712 - 00:19:19.790, Speaker A: Yeah. Roughly how large is the size of the state diff for every block?
00:19:20.290 - 00:19:43.794, Speaker C: State diff for every block. I would say how we run right now is that we typically have bashes as large as possible. So we can just say 100 kb. But that's not some limit of ZK sync, it's mainly just a limit of the whole data size of ethereum. So we just run until we reach the possible limit of the budge size and then we publish the budge.
00:19:43.922 - 00:19:45.320, Speaker A: And then I assume we.
00:19:48.010 - 00:19:49.160, Speaker C: Be able to put.
00:19:50.250 - 00:19:55.820, Speaker B: Yes. So dunk sharding should drastically allow us to lower the cost of the publishing of the pub.
00:19:56.190 - 00:20:28.342, Speaker A: Okay. Yeah, actually, one question. I was thinking about this stateive approach when Marcin was talking about it. If someone wants to say, I want to prove that this specific transaction happened on Vksync, is there a way to do that? I understand the state is like, yeah, people can reconstruct the state and so on, but if someone wants to say, I want to prove a specific transaction has happened in the past, right now.
00:20:28.396 - 00:21:00.350, Speaker C: The short answer is no. So we are thinking about it, maybe we'll allow it in the future, but right now it's not possible. So yeah, the main security property of a roll up is that even if the operator goes down, it was completely malicious and responsive. Users can become new operators. They basically can continue the state and be people to process new blocks so that users can withdraw their funds. For that only state is needed.
00:21:00.500 - 00:21:46.214, Speaker A: Right. But I think I'm thinking about extremely malicious corner case, which is, let's say, or maybe not malicious, but like some faulty corner case, which is, let's say DK syncs sequencer has some bug, and also the prover has some bug as well. So you essentially accidentally did an invalid state or incorrect state transition, and the state data is posted, and then the proof happens to. Not like, happens to prove that maybe because it is under constraint or something happens to be able to check out, then yes. If, let's say, metalabs disappears, yes, people can recover the state and so on, but they would not be able to identify that some state transition in the password is incorrect.
00:21:46.262 - 00:21:53.242, Speaker B: Yes. What's worse is like, yeah. It's not only that you cannot see the given transaction is there. You don't even know how many transactions.
00:21:53.306 - 00:21:54.190, Speaker A: Yes. You don't.
00:21:55.410 - 00:22:03.330, Speaker B: In theory, if you just observe l one, you see magic, you see proofs, and then just random state changes. Right?
00:22:03.400 - 00:22:09.700, Speaker A: Yeah, but basically it's relying on the fact that, or relying on the assumption that proofs should be correct.
00:22:10.310 - 00:22:12.110, Speaker B: They better be. They better be.
00:22:12.120 - 00:22:24.342, Speaker A: Otherwise, no, but let's say if you actually store the transaction data, then even if the. Well, it's still very bad if the proofs are not correct. But at least there is a way for independent party to just rerun the whole history.
00:22:24.476 - 00:23:07.970, Speaker C: No, but you said that if the proofs are incorrect, if the proofs are incorrect, then after all these transactions, even, like if you don't publish stages, but pure transactions, even after all of them has been published, the state which is stored here could be changed to incorrect one. Because how it works is that the operator basically sends the state changes and then the proof. And the only difference with transactions is that they would suggest transactions and the proof. And so if proof is validated on layer one, the transition will be done. So yes, they would be able to, in theory, rerun them, but maybe even not, maybe he'll be able to just publish some garbage transactions if the proof is incorrect.
00:23:08.050 - 00:23:11.640, Speaker A: Sure. I think, yeah, this is a kind of corner case.
00:23:12.170 - 00:23:32.906, Speaker B: And to be fair, this is actually one of the big challenges. When you explain people how optimistic robs and fraud proofs work, it's kind of natural. Be like, yes, and I can rerun it. If something is wrong, I can be like, hey, this is wrong. And it gets reverted. Start talking about the ZK proofs. It's really the first time, and the second and the third, and the fourth time you read it looks like magic.
00:23:32.906 - 00:23:34.830, Speaker B: You're like, how?
00:23:34.980 - 00:23:49.762, Speaker A: Yeah, okay, let's talk about the magic. Maybe let's erase some of this stuff. I guess let's maybe talk a bit about how the VM or the Ziki part actually works.
00:23:49.896 - 00:24:12.794, Speaker B: Right. Okay, so a couple of important things. And here we'll also be talking not only about the ZK sync, but also about a bunch of other l two s that are in this space. So roughly what we have in ZK sync is we have ZKVM. So this is today. It's not fully EVM compatible. That's what we're working on.
00:24:12.794 - 00:24:42.600, Speaker B: There are some still like caveats in there, but it is still based on solidity. So you can write a normal solidity code, compile with our compiler and then get this deployed again. There are some competitors that are using directly EVM whatnot. It's usually a trade off between performance. Like proof performance and the compatibility thing. Right.
00:24:45.770 - 00:24:58.922, Speaker A: Sorry to interrupt. I think it's not just about compatibility. Right. Because I believe this step here today is not proved. Right. Basically it's like you have compiler that is open source. People can verify or people can look at what the code actually does.
00:24:58.922 - 00:25:07.246, Speaker A: But I don't believe this transformation is. There's some kind of verification that shows that it actually does what this part.
00:25:07.268 - 00:25:11.706, Speaker B: Is done by the user. Right. We have a system. We don't do compilation our site. We just get the bytecode.
00:25:11.738 - 00:25:27.090, Speaker A: Right. So basically you're trusting if the compiler has some bug that may not be like compared to if you just directly deploy. Or I guess there are people still trusting the solidity to even. Bytecode compiler.
00:25:27.590 - 00:25:38.760, Speaker B: This is the compiler. And yes, we are aware that this is kind of an additional risk factor. Right. The same way how soul c is a risk factor for everybody doing the something good.
00:25:40.650 - 00:25:45.434, Speaker C: Before we go deeper, I would say that's stage okay.
00:25:45.472 - 00:25:59.690, Speaker B: Right. So this actually allows us to have a little bit different assembly that is more ZK friendly. Now when we talk about. But before we go deeper, questions. Poet?
00:26:00.050 - 00:26:19.682, Speaker A: Yeah, maybe. I think you have different opcodes, right? Maybe. I don't know whether you're familiar with all the differences in opcodes. But maybe if there's some important ones that you want to highlight in terms of what are the. I think Zksync vms support fewer. Maybe there are some different opcodes that you support.
00:26:19.736 - 00:26:44.238, Speaker B: Yeah. So I don't remember the exact details. But from what I do remember is that some of the opcodes in EVM are very ZK unfriendly. Partially because we do have like in ZkVM we have one circuit that's handling most of the opcodes. I think almost all of them. And the moment, if we wanted to do like, EVM opcodes directly, the circuit would have been a lot larger because it has to handle the largest possible inputs. And that makes it in.
00:26:44.404 - 00:26:48.720, Speaker A: How do you handle catch up? Because hashing function is usually one of the most difficult.
00:26:50.770 - 00:27:12.718, Speaker C: On Ethereum, Ketchack is an opcode. But for instance, things like Sha are done by pre compiles. And basically in Ziggy sync we have both Keshek and Sha done as pre compiles. So whenever users do ketcheck, instead of writing just an opcode, there is actually a call done to the precompile. It receives the premision.
00:27:12.894 - 00:27:16.626, Speaker A: So it's like a handwritten, very optimized circuit.
00:27:16.738 - 00:27:17.400, Speaker C: Yes.
00:27:18.090 - 00:27:21.750, Speaker B: This is actually one of the benefits of the compiler. It allows you to do things like this.
00:27:21.820 - 00:27:31.660, Speaker A: Right? Yeah. And maybe talking a bit about the part. Right, like what proving system is using today.
00:27:32.270 - 00:27:32.950, Speaker B: Magic.
00:27:33.030 - 00:27:35.510, Speaker A: Okay. It's all magic.
00:27:35.590 - 00:28:02.466, Speaker B: It's all magic. It's all black magic. If you're really following this video right now, I mean, take a pause, brief in, brief out, because it's going to go crazy, honestly. There are plenty of videos out there that focus just on the proving system. So here we'll just touch like bits and pieces. But roughly up to right now, we're using a snark based system with a bunch of changes because people are calling it snarks. But there are, of course, devils in the details.
00:28:02.466 - 00:28:27.550, Speaker B: So, like, how do you do copy constraints and gates whatnot? And here we have a whole crypto team. And I would suggest a completely separate, very deep dive to explain how they differ. My perspective is the snark system with KZG commitments on l one, right? So we use the KZG. We use snark at the same time, literally today, if you're watching this, you try it out. We announced book Bojum. Bojum. How do you say it? Bojum Bojum.
00:28:27.550 - 00:28:57.986, Speaker B: I have no idea. Okay, we should edit this out. So we announced boujum, which is a new proof system that we're rolling out that is more stark based with fry verifier. The main trade offs in there is the old proof system. It's run on gpus, but it requires around 80 gigs of GPU Ram in order to run it. This means that if you really want to run it, you need to get the a whatever is the top gpu that you can get. And with the current AI, boom.
00:28:57.986 - 00:29:37.394, Speaker B: They are expensive, tricky to get. Let's say it's not even about expensive. They are not there. The new proof system with starks allows us to kind of optimize it down to, I think, 16, 1012 gigs of amp, which suddenly allows you to run it on the regular gpus. But of course, this being a stark, there are some trade offs in there, especially on the proof size and on the verification time, verification gas. So we do have some optimization that we're working on right now in order to wrap this final proof. And kind of still do the KZG, but, like, plonk KZG, but only for the last part.
00:29:37.512 - 00:29:38.514, Speaker A: Only for the last part.
00:29:38.552 - 00:29:40.662, Speaker B: So the way to think about it, you prove the whole thing.
00:29:40.716 - 00:29:48.422, Speaker A: It's kind of Nova style. It's like batch sing and then proving at the end.
00:29:48.476 - 00:29:55.500, Speaker B: Yes, you prove the proving using a different thing that allows you to stay with KZG, which is so much more.
00:29:56.270 - 00:30:08.330, Speaker A: Yeah, that's interesting, because one disadvantage with KCG is that you need to do trusted setup. I saw by moving to stark, you're trying to essentially remove that trust. Or is that not a concern for legacy?
00:30:08.410 - 00:30:37.334, Speaker C: That's an additional bonus. But, I mean, it's one of the future proof concerns. But it's not like a major concern because, for instance, dunk sharding, it's also done with Kijd and trusted mean, and also ECDSA and most of the signatures which are used in Ethereum, I'm not sure about other blockchains, but they are non quantum secure. So, like, since we derive security from layer one, it doesn't matter too much right now.
00:30:37.452 - 00:30:38.120, Speaker B: Right.
00:30:40.250 - 00:30:44.858, Speaker A: Because you have one sequencer, doesn't really matter right now.
00:30:45.024 - 00:30:47.350, Speaker B: Remember, this is like a universal process setup.
00:30:47.430 - 00:30:47.770, Speaker A: Yeah.
00:30:47.840 - 00:30:51.450, Speaker B: Universal per circuit, like in graph.
00:30:51.950 - 00:30:52.700, Speaker A: Yeah.
00:30:53.550 - 00:30:54.202, Speaker C: Okay.
00:30:54.336 - 00:31:14.770, Speaker A: That makes sense. But, yeah, I think my understanding is that snark. Yeah, as you said, one of the advantages of snark is that the proof size is quite small. Does that have any effect on decreasing by switching to a stark based model with larger proof size? Yes, maybe cost more gas.
00:31:15.110 - 00:31:46.334, Speaker B: Exactly. So this is the thing that we're doing right now. So we're still experimenting with a bunch of approaches, but on one side, starks do allow us to prove a lot more blocks. So the idea is that with Stark, we would kind of try to prove multiple blocks or like multiple l one batches, kind of doing the recursive proofs whatnot, which will allow us to amortize the cost. But again, this is still in the middle of testing and gas optimizations, whatnot. So stay tuned and we'll see how it goes.
00:31:46.452 - 00:32:03.282, Speaker A: Okay. And maybe let's talk a bit about how the proving system integrate into the l two itself, right? Basically from a protocol point of view, how does the proof generation work with the broad production and so on?
00:32:03.416 - 00:32:41.630, Speaker C: You can generally consider proof production as a black box, because there is a lot of motion in the space to create some maybe decentralized markets for proving, et cetera. Because if proving becomes centralized, actually it's just like another point of centralization. But right now how generally works is that once we have a block, we firstly publish a state on layer one, and then we, on our servers, we start generating the proof. And once the proof is generated, we just submit the proof to layer one. And layer one verifies that the proof is correct. And that's it.
00:32:41.780 - 00:33:16.774, Speaker B: If we want to go a little bit deeper with this, the way to think about it is like this. If you imagine you have like a sequencer here. So sequencer keeps receiving the l two transactions and it's trying to put them in a batch. So we actually have a bunch of criterion of when do we decide that the batch is full. One of them is some arbitrary thing, like, okay, 1000 transactions, like some constraint that we have. The other ones might be time related, the other ones are actually the pub data related. For example, we want to make sure that the amount of pub data doesn't exceed the ethereum limits, whatnot.
00:33:16.774 - 00:33:17.082, Speaker B: Right?
00:33:17.136 - 00:33:32.138, Speaker A: Yeah. Actually on that specific topic, how do you determine the size so that, for example, the proof generation doesn't take forever? Or is there like a concept of some kind of gas that you kind of use as threshold?
00:33:32.234 - 00:33:41.938, Speaker B: Yeah. So the good news about our proof system is we are not really limited by the gas, but there are some operations that we're optimizing. For example, amount of catch, right?
00:33:42.104 - 00:33:44.146, Speaker A: I have like 1000 ketchup here.
00:33:44.328 - 00:33:56.626, Speaker B: Exactly. The way you think you can think about it is like we're doing n dimensional box packing, right? And if one dimension kind of exceeds the limit, then we have to stop the, stop the batch.
00:33:56.658 - 00:33:57.126, Speaker A: I see it.
00:33:57.148 - 00:34:08.170, Speaker B: But here, in case of sequencer, we can do like try to see if it fits and if not, we can pull it out. So kind of we do a bunch of like, okay, does it fit? No, this one doesn't fit. Okay, let's put something else.
00:34:08.240 - 00:34:28.878, Speaker A: Yeah, but I understand that, but wouldn't it cost some transaction to randomly fail? From user point of view, it doesn't understand why this failed. But then transaction after it succeeded, for example, you have this transaction that got taken out, but then, I don't know, like a transaction, which I don't know how the nouns works there, but some other transaction invited that transaction got like here.
00:34:28.964 - 00:34:36.126, Speaker B: The way to think about it is when we see that something is sticking out, we're just going to close the block and we're going to put this in the next one. So it's not going to fail from user perspective.
00:34:36.158 - 00:34:38.766, Speaker A: Okay. So it's like guarantee not to fail.
00:34:38.958 - 00:34:42.854, Speaker B: There are plenty of cases where transaction would fail, but this will not be one of those.
00:34:42.892 - 00:34:43.334, Speaker A: Okay.
00:34:43.452 - 00:35:25.074, Speaker B: But the idea is that the sequencer is kind of trying to grow the block, and then it's ready and the l two block is ready. And then as Tash mentioned, we do publish it to l one as like, hey, heads up. There's like this block coming. And then what's happening separately. You can think about it as a complete separate process, is doing the proving. Now, prover is actually kind of taking these existing blocks that it kind of hopes that they will be correct and doing the whole so called witness and proof generation. Because the way to kind of have to see how deep we can go with this.
00:35:25.074 - 00:36:14.034, Speaker B: But normally, the way you think about a function is like, okay, get a function with some inputs and it produces some outputs. In case of approver, it's more like the function gets all the inputs and it kind of succeeds or fails. So like, everything is given there with the example of like, let's say in the function, you're reading, like here, you're reading some value from some memory cell. Here for the prover, what will happen as the input? It will not be just this value from the memory cell. We're going to provide like, hey, the whole Merkel path in this. So a simple operation of like read or even simpler equals free will be in normal executions. Like, okay, put this there.
00:36:14.034 - 00:36:47.194, Speaker B: In case of provert will be like, hey, can you check that? If the previous state was this and you executed this, then the next state is this. Is this whole thing like, true. So that's kind of what prover is. We can think about proverbs like doing like asserts kind of along the way. So this means that the prover is actually split into two pieces. One of them is called witness generation. And this is the heavy part because this is the part that takes each of these operations and kind of blow them up.
00:36:47.194 - 00:37:10.834, Speaker B: Mean like, okay, this is actually like this path on the Merkel tree. And we're adding this here, and this is the path of the Merkel tree after we finish the execution. And witness can get like gigabytes, many gigabytes. Inside. And then this gets passed, and that's where the magic happens with polynomial writing whatnot. And this gets passed to the actual GPU part is doing the evaluation.
00:37:10.962 - 00:37:35.838, Speaker A: Couldn't you split into two parts where one is purely proving the state, the other is proving the computation, and you somehow aggregate it at the end? Or basically the compute part assumes all the state is correct and it just runs through and just check all the computation. Then you have several things that check all the state is correct.
00:37:35.924 - 00:38:18.620, Speaker C: That's actually very similar how our proverb works. We have the main circuits which are respond for computation like we call them main, and they're responsible for ad, most of the computation, not for Keshe. And we have a bunch of different other things like memory manager, et cetera. And then all these things, like for instance, this main computation, whenever it does some memory load, it reads some unit of memory. It just blindly trusts that. I hope this value is correct. Yeah.
00:38:18.620 - 00:38:53.010, Speaker C: And then we just store, for instance, somewhere here, like some commitment to all the memory, but anyway, like commitment for all the memory. And then this memory circle is what proves that, okay, under some commitment, this memory is correct. It also remembers that there is some commitment. And then during the final aggregation phase, in the end, it's just all proven that, okay, all the commitments are right and the same for storage, the same for catch hacks and a lot of other things which we need optimize.
00:38:53.090 - 00:39:10.970, Speaker B: So if you go dead deep, there are actually like 13 of these. So there are like 13 different circuits that are responsible for different parts about like storage, writing, storage, reading, then aggregations, leaf node, scheduler, et cetera, et cetera. So there's a bunch of exciting complexity hidden in there.
00:39:11.040 - 00:39:15.914, Speaker A: And then the witness generation, is that also down in, is that also parallelized or is it?
00:39:15.952 - 00:39:29.262, Speaker B: Yeah, so this is like here, I very much simplify that. But the idea is like, yes, we are actually doing, let's say this thing is doing width generation for the main vm, for this main circuit and this, and then we do for the larger leaves, larger and larger.
00:39:29.406 - 00:39:31.774, Speaker A: Okay, it sounds very complex.
00:39:31.902 - 00:39:34.418, Speaker B: So it's like a multi step process in there. Yes.
00:39:34.504 - 00:39:48.114, Speaker A: Okay. And then I guess your question regarding maybe the post improves one. Let's say the Ethereum itself is getting contrasted, right? How that affect the Ksync?
00:39:48.242 - 00:40:30.130, Speaker C: It does affect us in the sense that users, they have to pay for the proving, they have to pay for the data published. So there are things which we do, on one hand, we try to have lower gas price for the user. So there is a small congestion, we might just wait. But on the other hand, we want the finality to be in reasonable time and we want proofs to be published in reasonable time. So in this case, for this block in particular, we'll have to take a loss. But in the next block, we'll raise the gas price for the current levels so that we get compensated. And on average we are compensated.
00:40:30.470 - 00:40:36.850, Speaker A: I see, yeah. Actually on that topic, maybe, can you talk a bit about the l two? How do you charge transactions?
00:40:37.010 - 00:41:05.600, Speaker B: Right. So basically, if you think about the l two gas price, there is a dependency on l one. We do publish data to l one, which means we have to somehow show this cost in there. The way we do this is on one side, we do have a thing that's called like. So we do have an l two gas price that's like separate from l one. But the operations that require writing to l one do have a variable cost.
00:41:06.050 - 00:41:06.606, Speaker C: Okay?
00:41:06.708 - 00:41:29.298, Speaker B: So the way you can think about it is if l one gas price is, let's say 1000, whatever, and l two gas price is five, then let's say doing a write, sorry, doing an operation that requires you to write to l one might cost you today, might cost you like 200, right? And then maybe tomorrow it will cost 400. Right, so this does vary.
00:41:29.394 - 00:41:35.418, Speaker A: Right. And that's mostly the cost of posting as call data to l one.
00:41:35.504 - 00:41:42.858, Speaker C: Yeah, most of the cost is posting the call data, but also cost of proof. Proving it also is the proof itself.
00:41:42.944 - 00:41:45.834, Speaker B: Also has to end up like on layer one.
00:41:45.872 - 00:41:46.170, Speaker A: Right.
00:41:46.240 - 00:42:04.062, Speaker B: So kind of you can think about like each l one badge that we put on layer one is going to cost us some pub data that people, that kind of depends on transactions plus these many bytes on this much l one gas to do the actual, even the actual KZG thing in the KZG verification.
00:42:04.206 - 00:42:12.834, Speaker A: Right. How expensive is that in terms of. I guess maybe that's not a meaningful question, but this is all relative.
00:42:12.962 - 00:42:51.650, Speaker C: So on one hand, it is a constant cost with KZG, with general KZG based systems, it's a constant cost. So if we had infinite amount of transactions, we could call it negligible. But right now. Exactly. Right now, in the future I think we will aggregate more blocks for one proof. But right now we have one batch and one proof. And because of that, I would say that an average cost for various overhead from block, like for verification, other things, it's like one third or maybe one fourth of a price of a budge.
00:42:51.650 - 00:42:59.118, Speaker C: So it is a lot less than state divs, but not negligible. Yet, but in the long run it is negligible.
00:42:59.214 - 00:44:27.840, Speaker B: Which actually brings us to an interesting topic, because we do publish state divs today because we are like fully an l two roll up. But the thing that we were working on is something what stush hinted at the beginning was the thing that we're calling ZK Porter, because the question might be like hey, what if I don't need to publish my state to l one? What if I'm fine with taking some risk? Because again, the risk here is the Matterlabs database, like disappearing and you're not being able to recover your state. But I'm willing to take this risk because let's say I have some less valuable things and then I can save on all the pub data costs. So this is something what we call ZK Porter or validium. And if you kind of look at the example that mentioned at the beginning with our state, there's like this shard that we published to the this is the roller and then here is this shard that would be a ZK porter that would not be published. So the idea being that if you kind of decide explicitly like okay, my contract is some low value NFT, I don't mind about this state disappearing if something goes bad, but I really want to cut my costs, then suddenly you can kind of deploy your account here. The cool thing about this model is these are still synchronous calls, so you still interact with this the way you would normally do, but all the state changes here would not be published, so you will not be able to recover this state from the old one.
00:44:28.850 - 00:44:34.122, Speaker A: Since we're talking about this viridium, or I think they get something called hyperscaling.
00:44:34.266 - 00:44:36.442, Speaker B: Are we going to talk about that too? Just in 2 seconds?
00:44:36.506 - 00:45:03.882, Speaker A: Amazing question here. Is there like a third assumption, which is that I still somewhat care about my security, of my availability of my data, but maybe I don't want to pay as high of a cost. Let's say I want to use a separate data variability solution. I don't post on etherman post like somewhere else, like near for example. Is that a thing that Ziki Singh is thinking about?
00:45:03.936 - 00:45:34.246, Speaker C: I would say there are two options here. First one is for Ziki Porter. Our end goal is to not rely only on Metrolabs. There will be obviously some decentralization in this regard. Almost in all validiums there are always not one party that stores the data, because just losing one database is too easy. At least ten parties usually store, maybe less, but depends, that's one thing. And I would say that's already good enough for a lot of people.
00:45:34.246 - 00:46:11.662, Speaker C: But if someone wants to design their own data availability solution, they can do it. But each case needs to be designed separately. So I would say in theory it's possible, because zero knowledge proofs, they give you validity of computation and how you publish data, well, it's up to the designer. So maybe someone couldn't deploy NL three on ZKsync. Maybe they can deploy NL two using ZK stack. And it's like our future tooling to help develop roll ups. That will be choice of everyone, because.
00:46:11.716 - 00:47:16.962, Speaker B: You touched on a very good point. This is what we're announcing right now is the ZK stack, the ability to take the ZK technology, this is the whole thing that's open source. We're actually making it easier for people to run and allow you to go ahead and deploy your own chain separately, your own chain. And then you can choose a different data availability layer if you want to near something else, Filecoin or whatnot. And the really cool thing now we're going to go more into scaling and hyperbridges. The really cool thing about if you're running the ZK stack and you're using the same Zke EVM, the same proofs, it allows everybody else to verify your proof without like to verify your state and computation very efficiently, very quickly. Right? So this allows the hyperscaling, the hyper bridges, because what it means is that you don't need, because in case of optimistic roll ups, you have to replay all the transactions to make sure that you're up to date whatnot.
00:47:16.962 - 00:47:54.900, Speaker B: In this case you don't because you know the circus, you can verify them very quickly and you know this is the right state. So this is another thing that we're announcing, the hyper scaling the hyper bridges whatnot. This means that multiple ZK stack deployed chains or subchains, whatever you want to call it, they can operate between each other very efficiently. Here, I'll send people to the medium post that we're posting right now for all the details. But the idea being that this allows them to kind of share multiple bridges and again, a great interoperability between them.
00:47:55.270 - 00:48:06.530, Speaker A: Actually, yeah. That reminds me of a question. Is a proof like recursive? Is it like proving everything up to this point or is only proving the past since last block?
00:48:06.610 - 00:48:18.058, Speaker B: So today our proofs are not recursive. I mean, we are just proving the state that you can think about state transition from x to y. But yes, in theory you could, especially with the stark based proofs, you could.
00:48:18.064 - 00:48:24.838, Speaker A: Do, because if you don't recurse, then you cannot. Just, like, in order to verify, you still need to verify all the proofs.
00:48:24.934 - 00:48:31.114, Speaker B: Yes, but this is exactly why I'm talking. That's why this requires the stark approach. It actually does allow you to do recursive.
00:48:31.162 - 00:48:31.566, Speaker C: Yeah.
00:48:31.668 - 00:48:36.820, Speaker A: Okay. If you only need to, like, one proof, that would be very cool.
00:48:38.150 - 00:48:38.900, Speaker C: Okay.
00:48:39.430 - 00:48:57.190, Speaker A: I think we're kind of running out of time. How much time do we have left? Okay. Yeah, roughly the time. So, yeah, anything else that Marcin of stat wants to add about awesome technology that Dicky Singh has developed?
00:48:58.410 - 00:49:06.310, Speaker C: I would say just come to our website and try out ZK sync. Read about Zkstack.
00:49:10.110 - 00:49:33.950, Speaker B: One thing I would say is that really, as I mentioned earlier, the ZK looks like magic, and it is magic. You're going to read it once, twice, five times, and then after the 20th time, we'll be like, oh, that's how it works. We actually encourage you to take a look. And again, don't give up. Really, don't give up, because at the end, this is, like, one of the most exciting technologies of the current blockchain.
00:49:34.690 - 00:49:52.530, Speaker A: It's the future. Yeah. Okay. Thanks a lot, Martin, and stats for walking me through how the Ksync works. And, yeah, thanks again for coming. And we will post the links in the show notes.
