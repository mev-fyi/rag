00:00:05.230 - 00:00:18.770, Speaker A: Hey, everybody, this is Alex and Ilya from Nier. And we're here with Anatoly from Solana to do a super deep dive and discuss really intense details about their protocol. Anatoli, you want to give a short intro about yourself?
00:00:18.840 - 00:00:34.300, Speaker B: Yeah, sure. So I'm Anatoli. I'm the founder, CEO of Solana. Solana is a very high performance blockchain that does no sharding opposite of near. So I'd love to argue with these guys about technology.
00:00:35.710 - 00:00:41.290, Speaker A: All right, let's start with kind of you describe us at high level, and then we'll go really deep.
00:00:41.370 - 00:00:41.758, Speaker C: Sure.
00:00:41.844 - 00:01:21.830, Speaker B: So at a very high level. Salana is based around this idea that you can use a clock, a global clock, to speed up a distributed system. And this idea has been around for like, a really long time since the clock is permissionless. So what's interesting about our clock is that it actually works before consensus. Like before you actually have any notion and agreement in the system, you still have a notion of time. And that's why we can actually leverage that to speed up the network. So that's kind of like the very shallow deep dive.
00:01:21.830 - 00:01:59.560, Speaker B: How does our clock work? I think if you guys have been following the space in the last year, there's been kind of this explosion of ideas around verifiable delay functions, or vdfs. I actually didn't know anything about vdfs when I came up with this. I just had too much coffee and I was up till four in the morning. And our clock is maybe the simplest way you can implement the VDF. So imagine you have a shot 256. Any cryptographic preinture system hash functions would work. And this thing just loops over itself, right? So it keeps going and going and going.
00:01:59.560 - 00:02:41.902, Speaker B: In every x amount of iterations, you just take a sample. So when I mean loops over itself, its output is. The next input just recursively, just runs. Because it's pre image resistant, you can't predict any of the values that are going to occur, like 100 million iterations from now. You actually have to run this thing in a single core, single process without stopping. So let's say this is like 1 million iterations, and the value is like zero, BIA whatever, right? This is 2 million values, like zero, C-D-F. And it just keeps going forever.
00:02:41.902 - 00:03:36.520, Speaker B: So these samples represent a data structure that tells you that somebody somewhere spent real time building it. So what's interesting about this data structure is you can encode messages into it. So I can simply take a message or a big blob of data and hash it, and then kind of stick it in here before we do this iteration. So now I have some value here, some bytes that I've inserted here. So now, at this point, all the subsequent hashes have been modified in an unpredictable way because it's premium resistant. Um, so what also is interesting is if this thing itself also contained a reference to some other value that it observed. So now this guarantees that this message was created after this point and before this point.
00:03:36.520 - 00:03:52.380, Speaker B: So now we have this data structure that encodes time, but also an order of events and the relative order between them, as well as the relative amount of time between them.
00:03:54.190 - 00:03:54.794, Speaker C: Right?
00:03:54.912 - 00:04:37.420, Speaker B: So we call this thing proof of history, because it gives you a proof of historical record. So now imagine you have this data structure that I send to you guys with a thumb drive attached to a carrier pigeon, right? And you're looking at this data structure, right? And this is like the head pointer. This is the last thing you see. And somewhere here, you see a network message. Somewhere here you see a network message. Somewhere here you see, like, network messages, right? So, given this last thing that you've observed, you can compute whatever algorithm you choose. The active set of the network, right?
00:04:39.950 - 00:04:41.690, Speaker D: What's the set of the network?
00:04:42.510 - 00:04:50.746, Speaker B: Whatever algorithm you choose. You could say that somebody had to have sent, like, a message once over the last two weeks, right?
00:04:50.928 - 00:04:52.142, Speaker D: Yeah, makes sense.
00:04:52.276 - 00:06:03.298, Speaker B: Whatever. It doesn't matter what it is, you pick some algorithm, right? But the point is that at this point in the lecture, at this point in history, which is identified by an account, right, 1 billion and some value, zero x, blah, blah, blah, you've decided that this is the state of the network. So what's interesting is that everyone else observing this data structure using the same algorithm, will decide the exact same state of the network. And if we optimistically assume that everybody's looking at this thing, then this is how we cheat the two generals problem, right? If everybody optimistically assumes that they're looking at this thing, they construct a message that references this value. That's only valid if it appears sometime later, right? And we say we attack, right? Everybody attacks. And if we're lucky and everyone did actually observe the same thing, we will see that, as evidenced here by a bunch of messages to indicate the attack, which is concluded from the exact same information that everyone observed. So that's fundamentally how our system works.
00:06:03.298 - 00:06:24.038, Speaker B: Is that based on this historical record, which is now cryptographic, it's not just timestamps that I etched in bits, right, and made up, we can actually come to agreement about the state of the network, and therefore make a decision. And that decision is consistent along with everyone else, optimistically.
00:06:24.214 - 00:06:24.794, Speaker C: Right?
00:06:24.912 - 00:06:36.270, Speaker D: And so you're saying we're cheating to generals program. You're saying that Solana would survive even if they have more than one third of malicious actors.
00:06:37.410 - 00:06:40.430, Speaker B: So there's no solution to the two generals problem.
00:06:40.500 - 00:06:40.686, Speaker C: Right.
00:06:40.708 - 00:06:50.734, Speaker D: You can only cheat. Right. But do you think that with this chit, it would be safe, even if there's like, let's say 40% of malicious.
00:06:50.782 - 00:07:14.630, Speaker B: Actors, doesn't matter how many malicious actors are, because the way you win the two generals problem is. I don't say generals problem, sorry, I'm thinking of the ungenerables problem. Doesn't matter. The way we actually solve it is when we look at this point here, we say that we solved it here, right. Whatever. 1 hour later we decide, oh, yeah, we did actually solve it. And here's the proof.
00:07:14.630 - 00:07:32.014, Speaker B: Because the system then has evidence that it did come to agreement here, and it has multiple confirmations. And our system, just like proof of work, the deeper this event occurs, the harder it is going to roll. So I can kind of dive into that.
00:07:32.052 - 00:07:37.858, Speaker A: So you may have like a bunch of forks, but presumably, like 1 hour later, they will resolve one or another.
00:07:37.944 - 00:07:38.580, Speaker C: Yeah.
00:07:39.190 - 00:07:52.120, Speaker D: So let's say, hypothetically, the person who runs the proof of history is malicious. And he has two cores, let's say. And so he has a second core where he confused the same thing, but he misses one message.
00:07:52.890 - 00:07:58.360, Speaker B: Yeah, that's fine. So we can kind of get into the consensus part.
00:07:59.370 - 00:08:01.238, Speaker D: Okay, so these parties are talking about the consensus.
00:08:01.334 - 00:08:02.506, Speaker A: It's just a way for us to.
00:08:02.528 - 00:08:04.780, Speaker D: Have evidence that the message happened.
00:08:05.470 - 00:08:19.658, Speaker B: Because once you observe the structure, you don't really care how it was generated, whether it was multiple leaders, two leaders, one doesn't matter, right. Everything about the network is derived from the data itself, so there is no weak subjectivity.
00:08:19.754 - 00:08:25.870, Speaker D: And so I, as a person who received this thumb drive, how do I verify that all the hashes were completed correctly?
00:08:26.030 - 00:08:46.630, Speaker B: So that's where you take your GPU card, you look at each slice in between here, right? And then you use one core in every slice, and about 4000 slices, you can verify it in one loop. So 1 second can be verified in about a quarter millisecond.
00:08:47.070 - 00:08:51.610, Speaker D: So with one gpu, we will be verifying 4000 times faster.
00:08:52.990 - 00:09:03.900, Speaker B: With four gpus, you're 16,000 times faster. Kind of like if you look at your entry level ethereum mining rig has like twelve gpus in there.
00:09:05.310 - 00:09:19.010, Speaker D: And then for safety, let's say I'm planning to run some sort of exchange and need to be absolutely certain there's nothing weird happening. I need to verify everything from the Genesis block.
00:09:20.950 - 00:09:25.880, Speaker B: Depends. That is a complicated thing. That's a complicated question.
00:09:26.250 - 00:09:31.702, Speaker D: If it's been four years since the Genesis block, I will verify it in one third of a day.
00:09:31.756 - 00:09:32.360, Speaker C: Right.
00:09:33.290 - 00:09:35.110, Speaker B: Depends on your gpu capacity.
00:09:35.530 - 00:09:35.990, Speaker D: Completely.
00:09:36.060 - 00:09:37.394, Speaker B: It's parallelizable.
00:09:37.442 - 00:09:37.702, Speaker C: Right.
00:09:37.756 - 00:09:41.500, Speaker D: That sounds reasonable. That's about how much time it takes to sync to Ethereum anyway.
00:09:42.430 - 00:09:47.418, Speaker B: But in 1 hour you could spin up 100 gpus on Google Cloud and verify everything.
00:09:47.504 - 00:09:48.780, Speaker D: Yeah, makes sense.
00:09:49.330 - 00:09:54.750, Speaker A: So for verifying, you still need to have the checkpoints relative frequently. So how big is that data structure.
00:09:56.610 - 00:10:10.180, Speaker B: We are generating? Maybe at most 2000. That's simply because. So our kind of target is one gigabit UDP packets are 64, can generate 2000 of them per second.
00:10:11.350 - 00:10:12.066, Speaker D: Check that one.
00:10:12.088 - 00:10:12.494, Speaker A: Hashes.
00:10:12.542 - 00:10:14.318, Speaker D: Yeah, hashes or checkpoints.
00:10:14.334 - 00:10:24.626, Speaker B: It's the same thing because every time you encode the message, which is like a data packet, you need to record the state and the counter.
00:10:24.818 - 00:10:27.320, Speaker D: How many hashes does single core compute per second?
00:10:27.770 - 00:10:28.966, Speaker B: About 3 million, I think.
00:10:28.988 - 00:10:29.842, Speaker D: 3 million hashes?
00:10:29.906 - 00:10:30.520, Speaker C: Yeah.
00:10:31.070 - 00:10:38.810, Speaker B: It might be more now we haven't measured it, but like AMD thread Ripper has chapter 56 specific instructions.
00:10:40.030 - 00:10:43.294, Speaker D: And so Justin Drake is very into.
00:10:43.332 - 00:10:44.286, Speaker A: Vdfs these days, right?
00:10:44.308 - 00:10:44.494, Speaker C: Yeah.
00:10:44.532 - 00:10:49.838, Speaker D: So they want to build this ASIC that will be computing some other VDF, not this one.
00:10:49.924 - 00:11:19.210, Speaker B: Yeah. So our VDF is extremely simple. It doesn't do anything fancy. What they're doing is something that given, like, let's say 10 seconds worth of data, the verification time is maybe constant or fast, like polynomial. The difference there is that ours grows linearly. Verification time is linear to the cpu time necessary to verify. But cpus double in cores every two years.
00:11:19.210 - 00:11:27.980, Speaker B: So I think by the time they ship v, zero of their hardware, we would be like at 8000 cores. Right, for GPU card.
00:11:29.630 - 00:11:36.542, Speaker D: But then I think, unless I remember it from Justin mentioned in one of the podcasts that Salana wants to participate with them.
00:11:36.596 - 00:11:37.150, Speaker B: Yeah.
00:11:37.300 - 00:11:39.706, Speaker D: What's your interest? If you're using a different VDF?
00:11:39.818 - 00:11:45.842, Speaker B: Why wouldn't we swap it for something that's algorithmically better? We don't care.
00:11:45.896 - 00:11:46.500, Speaker C: Right?
00:11:49.430 - 00:12:03.654, Speaker D: Yeah, but the properties you want from VDF are very different from what they want, right? You want the VDF to be producing values 2000 times per second. While their use case is like our.
00:12:03.692 - 00:12:48.978, Speaker B: Main problem with the stuff is still in research is like they're using snarks and starks and so the complicated things there is setup. So their approach maybe use a large MVC group, but it's effectively like a snark style setup. You either need a trusted group, so you need to make it huge, or you need some other mechanism to create kind of the initial seed such that you know, that this wasn't pre computed before for us because we're using shots of 56. Very dumb. It is to somebody that's been working on operating systems. It's very transparent, what it does. Their setup requires this kind of like magic ceremony.
00:12:49.154 - 00:12:49.880, Speaker C: Right.
00:12:50.890 - 00:12:52.434, Speaker A: That's as they use a snark setup.
00:12:52.482 - 00:12:53.080, Speaker D: Yeah.
00:12:53.450 - 00:12:56.326, Speaker A: Presumably starks. Now you can.
00:12:56.508 - 00:13:01.210, Speaker D: I think they use neither. They're just computing some arithmetic operation.
00:13:02.190 - 00:13:04.246, Speaker B: Their operation requires a setup.
00:13:04.278 - 00:13:04.378, Speaker C: Right.
00:13:04.384 - 00:13:05.834, Speaker D: Now they need a semi prime, right?
00:13:05.872 - 00:13:06.458, Speaker C: Yeah.
00:13:06.624 - 00:13:08.620, Speaker D: So they need a semi prime, which is.
00:13:09.070 - 00:13:29.122, Speaker A: Well, yeah, they're using a prime version of the hash function they use is like Peterson. Well, I'm speculating here, but if you're assuming Peterson commitment hash function, and you compute it many times, you can compute a snark of that really easily. That proves that you didn't done this computation this many times.
00:13:29.256 - 00:13:29.940, Speaker C: Sure.
00:13:32.710 - 00:13:58.620, Speaker B: That's the one part that's complicated. The other part is what we're doing is biasing the input. Every time you have a message in the network, we stick it into this loop. Right. The frequency of that. We can make it work with whatever function they do. But our main concern here is a security concern, is if you can bias this input, you can potentially generate a hash function.
00:13:58.620 - 00:14:00.694, Speaker B: If this is not pre image secure.
00:14:00.742 - 00:14:00.906, Speaker C: Right.
00:14:00.928 - 00:14:25.362, Speaker B: If this is not pre image resistant, when you do this message recording, we want this part to be guaranteed to be secure such that you can't generate a. Well, I've talked to Justin. He feels confident that we can get our stuff working with theirs. I trust his opinion, and I'm happy to switch. That's not a problem.
00:14:25.496 - 00:14:26.274, Speaker C: Cool.
00:14:26.472 - 00:14:30.598, Speaker D: So let's say we have the clock. What do we do next?
00:14:30.684 - 00:14:54.206, Speaker B: Okay, so the main part about why things can be fast with a clock, right, is because when you do this active set computation, you're doing this without messaging anyone, right. You got this data structure from a carrier pigeon with a thumb drive. You don't have to talk to anyone else in the network. You actually do this computation locally. And that's the speed up.
00:14:54.228 - 00:14:54.414, Speaker C: Right.
00:14:54.452 - 00:15:03.834, Speaker B: Because now there's no interactive thing that you have to do with anyone else. Simply optimistically guess that I've downloaded the right structure.
00:15:03.882 - 00:15:04.094, Speaker C: Right.
00:15:04.132 - 00:15:10.500, Speaker B: Now I'm going to send it. So that should kind of intuitively tell you why it's fast, right?
00:15:11.030 - 00:15:11.940, Speaker D: Sort of.
00:15:14.570 - 00:16:11.030, Speaker B: The second part of why we can leverage this is uses that point is because in our setup we have a single leader at a time and a bunch of validators that are receiving this data. So validators don't care how they get this data, right, because they're deriving everything from the data directly. So this leader can broadcast one over n validators worth of data to each one, and then they exchange it. So that is effectively bittorrent. And you can scale this out to a log tree structure, right, whatever your fan out is, we verified this somewhere about 200. That means that in the second layer you have about 40,000 nodes with only two hops. So our finality grows with the log of the network size.
00:16:11.030 - 00:17:02.680, Speaker B: And this fan out can be pretty huge topology like stats, or it changes all the time. How you get placed is stake based in your stake. But effectively we try to get this group to be all the top stake nodes and then everyone else. And the protocol is designed such that you receive this data partially from the network, and it's padded with erasure coding. So if x percentage of the nodes are malicious or send you bad data, or just don't do nothing, if that percentage is below the erasure coding, you should be able to recover the full data set. So kind of now this looks like a wireless network, which is like twelve years of Qualcomm actually helps here.
00:17:05.290 - 00:17:09.418, Speaker A: Go ahead. I'm assuming the leader gets rotated as well, right?
00:17:09.504 - 00:17:43.570, Speaker B: So here's the fun part. Now things get interesting. So rotation, right? And this really goes into the consensus portion. So imagine we take this timeline and we divide it up into slots, and these are just basically counters, right? Proof history counters. So this is 1 million, this is 2 million, whatever. At every point, at every slot, there's a single leader that can transmit.
00:17:45.530 - 00:17:46.374, Speaker C: Right?
00:17:46.572 - 00:18:37.060, Speaker B: So this looks like time division multiple access TDMA, one of the earliest wireless protocols. We've effectively taken the rest of time and split it up into slots where any single leader can be the only one that transmits. So what that means is me as a validator, I either receive a transmission from this leader, right? Or potentially, if I fail to reassemble the packet, I generate a virtual tick or a virtual history, because I can derive the history from the last point that I've observed by simply appending hashes with no data to it, right? So now we have kind of this protocol where the network either receives the data or fails to receive it, but it doesn't stop.
00:18:37.690 - 00:18:44.626, Speaker D: Let's think from perspective of validator, too. So I presume those things happen rather frequently.
00:18:44.818 - 00:18:45.414, Speaker C: Yeah.
00:18:45.532 - 00:18:53.270, Speaker D: So if I wait first, like, if I wait until I'm confident I didn't receive the data before I start computing hashes.
00:18:53.350 - 00:18:55.126, Speaker B: No, you always compute them constantly.
00:18:55.158 - 00:18:59.082, Speaker D: So every validator constantly computes hashes, but then we're burning trees again, right?
00:18:59.136 - 00:19:01.254, Speaker B: No, just one core per node.
00:19:01.302 - 00:19:01.770, Speaker C: I see.
00:19:01.840 - 00:19:04.418, Speaker D: Yeah, it's less of a burn.
00:19:04.454 - 00:19:05.098, Speaker A: Less trees.
00:19:05.194 - 00:19:07.722, Speaker B: Yeah, significant amount of trees.
00:19:07.786 - 00:19:08.206, Speaker C: Right.
00:19:08.308 - 00:19:16.578, Speaker D: But then once we get here, now we have all the interesting things, such as, why would I not always pretend that they never received the message?
00:19:16.744 - 00:19:40.140, Speaker B: Yeah. So this is where things get actually where you see branching is the second leader, right. Is either observed a virtual ledger or a real one, and they get to transmit. And this is derived either from here or from here. Right. So the third one now, then spans this. Now the third one, right.
00:19:40.140 - 00:19:57.678, Speaker B: Every time there's a new leader, a new slot, they have a number of options to pick from the previous thing that they've observed. So the branching kind of starts blowing up here. And how we reduce it is, I can kind of show you a different tree. Does that make sense?
00:19:57.764 - 00:19:58.880, Speaker D: Yeah, that makes sense.
00:20:00.290 - 00:20:12.820, Speaker B: Every leader either received the previous leader transmission and says, okay, I received the data, I validated it, now I'm going to transmit, or they fail to, and they've created a virtual ledger and they transmit from that one.
00:20:16.710 - 00:20:23.546, Speaker A: So there's options where they received one before, et cetera.
00:20:23.678 - 00:20:23.974, Speaker C: Right.
00:20:24.012 - 00:21:01.602, Speaker B: Because at any point, how lagging they are, right? So at any point, right? So this one sends data, then l two gets to decide, well, did I receive the data or did I receive the virtual? So this one goes t or v, right? T or v. L three is over here. So the path here, right. Can come from as if l two failed, l two failed to observe l one.
00:21:01.656 - 00:21:01.874, Speaker C: Right.
00:21:01.912 - 00:21:39.440, Speaker B: So they chose v, and l three actually saw, like, you know, also failed to observe l one, l two and chose t as well. And the only way this is chosen is if both of them fail to observe l one. Does that make sense? Because if l two transmits data, and this node observed a failure for l one, and this data depends on this one, it cannot verify that this is correct. Right. So it actually skips it and says, I'm going to branch, I'm going to prove that I've actually generated enough hashes up to now.
00:21:40.210 - 00:21:47.914, Speaker D: What could happen is that l two did transmit. L two only transmits its own portion. It doesn't transmit the portion from l one.
00:21:48.052 - 00:21:48.354, Speaker C: Correct.
00:21:48.392 - 00:21:53.874, Speaker D: And so l three, in theory, might have received l two without receiving l one and still choose to skip both.
00:21:53.912 - 00:21:55.814, Speaker B: Of them because it cannot verify them.
00:21:55.852 - 00:21:56.694, Speaker D: Makes sense.
00:21:56.892 - 00:22:09.334, Speaker B: I mean, there is repair happening and stuff like that. But the idea is that this node may actually fail to receive both. And because it failed to receive both, its only option is to assume both of it failed.
00:22:09.382 - 00:22:09.980, Speaker C: Right.
00:22:11.150 - 00:22:56.198, Speaker B: And what each node does is whenever they observe transmission data such that they vote on it, they reset their poh to start running from there. So at any point, the last thing you voted on, that's your kind of starting point, and you start producing hashes, and you could overrun basically everybody else up until it's your turn to transmit. And that could be ten slots, it could be five slots, it could be zero. And then you transmit that, plus all the virtual Ticks you've generated, and you transmit that to the network. So the network, when receiving your data, can actually derive it back to. Oh, this actually came from here. And here's the previous tree that shows that you waited x amount of time.
00:22:56.284 - 00:23:13.430, Speaker D: And what is the fork? Visual. So let's say I see l three. So l three is sending me skip, skip, transmit approve. Yes, that it's a transmit here. But I also see l two that sent me, effectively transmit v transmit.
00:23:13.590 - 00:23:13.926, Speaker C: Right.
00:23:13.968 - 00:23:22.154, Speaker D: But then obviously l two send the door in the past. Which one do I choose? Well, when I saw l two.
00:23:22.212 - 00:23:24.334, Speaker B: Now we get to the fun part. For selection.
00:23:24.382 - 00:23:24.546, Speaker C: Yeah.
00:23:24.568 - 00:23:25.342, Speaker D: For schedule.
00:23:25.486 - 00:24:03.834, Speaker B: So imagine you're a validator, right? And actually, let's go back a little bit to how consensus works to kind of give a bigger, broader picture. So you have this provistry ledger. And imagine it's just working kind of like at a very high level. It's working all the time. And I have some votes here. Every time I vote, each vote is for a particular height of the ledger. And this vote starts with a lockout.
00:24:03.882 - 00:24:04.926, Speaker C: Let's say two.
00:24:05.108 - 00:24:39.782, Speaker B: Let's say I vote again. Now, this vote is a lockout of two, and this one is a lockout of four, right? So I vote again, this becomes four, this becomes eight, this becomes two. The idea is that every time I vote, my votes exponentially stack, my lockouts exponentially grow. So this kind of gives us the same behavior as proof of work, right? We're not a PBFT or BFT solution, right? We're availability. So we're using kind of this threshold approach. But the threshold isn't based on electricity.
00:24:39.846 - 00:24:40.074, Speaker C: Right.
00:24:40.112 - 00:24:50.746, Speaker B: It's actually based on time. And fundamentally, when you look at it, it's almost like a trade off between availability and consistency. You can't cheat cap theorem.
00:24:50.778 - 00:24:51.022, Speaker C: Right?
00:24:51.076 - 00:25:00.858, Speaker B: So literally, you're trading here. What you're saying is I will drop my availability for eight units of time to break this consistency.
00:25:00.954 - 00:25:12.750, Speaker D: I see. So a couple of questions. Votes. The voting is virtual. Is virtual. When I'm as a leader, every validator.
00:25:12.830 - 00:25:17.122, Speaker B: When they observe your data, they decide to vote on it, and they're voting with a stake.
00:25:17.186 - 00:25:19.270, Speaker D: And that vote is going to be stored on the.
00:25:19.340 - 00:25:29.974, Speaker B: On the ledger sometime in the future. So this vote may have been recorded here.
00:25:30.012 - 00:25:33.138, Speaker A: You actually submit information to the next leaders that, hey.
00:25:33.244 - 00:25:34.250, Speaker B: To the network.
00:25:34.590 - 00:25:38.490, Speaker D: But doesn't it mean that per block, you need to store 40,000 votes?
00:25:39.650 - 00:25:48.640, Speaker B: You could compress them with BLS signatures if you wanted to, but we just don't. We don't care about data. We have a solution for data. So for all the data you want.
00:25:49.490 - 00:25:50.240, Speaker D: Because.
00:25:52.610 - 00:26:00.740, Speaker B: Our goal is to do 100,000 transactions per second, like steady state. Right now, it does do 100,000.
00:26:02.070 - 00:26:02.820, Speaker C: Cool.
00:26:04.070 - 00:26:11.222, Speaker D: For me to fully understand this logo means that if this boat is recorded and at some point within less.
00:26:11.276 - 00:26:13.782, Speaker B: So eight is units of time.
00:26:13.836 - 00:26:17.506, Speaker D: Units of time. So without lots of generality, let's say, those millions of hashes.
00:26:17.538 - 00:26:17.814, Speaker C: Right? Yeah.
00:26:17.852 - 00:26:26.186, Speaker D: So if someone else observes me having a competing vote within less than that period of time, there's some slashing happening.
00:26:26.288 - 00:26:26.506, Speaker C: Yeah.
00:26:26.528 - 00:26:39.678, Speaker B: So if this is a vote for this particular branch, and you voted here. Right, then you get slashed, frozen, whatever, you're effectively prevented from voting again.
00:26:39.764 - 00:26:40.640, Speaker D: Makes sense.
00:26:43.910 - 00:26:44.274, Speaker C: Yeah.
00:26:44.312 - 00:26:52.370, Speaker B: Slashing is one thing we're looking at, interestingly. All we need to do is actually just freeze you from, like, freeze your account.
00:26:52.440 - 00:26:53.010, Speaker C: Right.
00:26:53.160 - 00:26:59.810, Speaker B: From getting rewards or from voting. It's as effective as slashing in terms of keeping the network steady.
00:26:59.890 - 00:27:01.960, Speaker A: No, you can move the money to different account.
00:27:02.730 - 00:27:04.194, Speaker B: No, it's like frozen.
00:27:04.322 - 00:27:05.222, Speaker A: You cannot touch the.
00:27:05.276 - 00:27:06.246, Speaker B: For, like, six months.
00:27:06.348 - 00:27:06.854, Speaker C: Right.
00:27:06.972 - 00:27:14.614, Speaker D: But then if I voted, let's say I voted 30 times in a row. So this time my lockup is 4 billion, for all practical purposes.
00:27:14.742 - 00:27:15.420, Speaker B: Exactly.
00:27:16.190 - 00:27:21.390, Speaker D: But then let's say your work happened and actually, like, this branch was abandoned.
00:27:22.210 - 00:27:25.742, Speaker B: When was the last time bitcoin reordered 32?
00:27:25.876 - 00:27:28.480, Speaker D: I don't like bitcoin. Let's use birdcoin as an example.
00:27:29.010 - 00:27:30.302, Speaker B: That was an attack, right?
00:27:30.356 - 00:27:30.958, Speaker C: Right. Yeah.
00:27:31.044 - 00:27:43.794, Speaker B: So PPFT systems just freeze for us, two to the 32 in time units. We can define that as meaning one week. We don't care what it is.
00:27:43.832 - 00:27:44.142, Speaker C: Right.
00:27:44.216 - 00:28:01.578, Speaker B: We can effectively make a practical decision to say that if a 32 block deep reorg needs to occur, the network should basically do nothing for a week. This should never happen. But if it needs to happen, let's put a human constraint on it.
00:28:01.664 - 00:28:02.426, Speaker C: Right, I see.
00:28:02.448 - 00:28:15.934, Speaker D: And so if some validator, if it happens, that majority of validators ended up on a different four, but someone happened to have the stocky blocks, then we sacrifice this validator for a week.
00:28:15.972 - 00:28:16.474, Speaker C: Effectively.
00:28:16.522 - 00:28:25.358, Speaker B: Exactly. Something like that. That's something we can effectively decide and even make it a network parameter that the network chooses.
00:28:25.454 - 00:28:28.754, Speaker D: And you have some nice finality proofs that say that if I saw like.
00:28:28.792 - 00:28:32.420, Speaker B: 64 in simulation converges really quickly.
00:28:34.870 - 00:28:35.198, Speaker A: But.
00:28:35.224 - 00:28:36.946, Speaker D: They have like, a nice written proof.
00:28:37.058 - 00:28:44.434, Speaker B: In latex, if I saw 64 blocks in GitHub. So it's all open source.
00:28:44.482 - 00:28:44.742, Speaker C: Cool.
00:28:44.796 - 00:29:02.954, Speaker B: Even all the wards are open source, so feel free to, like, scrutinize it file issues, look at the simulation. And do you have simulation with adversaries? Yeah, so that's the really interesting part. So imagine you guys can observe this happening.
00:29:03.072 - 00:29:04.322, Speaker D: Can you use Lincolnstead?
00:29:04.406 - 00:29:45.290, Speaker B: Yeah, sure. So when I vote here, these lockouts double, right? So now this is four, this is eight, and this is 16. So that means that I can't really vote on any of these branches. And let's say this is a dead branch, right. I have to wait until this point to actually vote. But let's say a bunch of stuff happened and I actually voted here. What actually happens is, let's say this lockout actually expired here.
00:29:45.290 - 00:30:06.110, Speaker B: So this photo is expired too. So the network Aws had like a sneeze, and things stopped working, and I only ended up floating here. The actual lockouts are like a stack, and this stack is cleared. And now your lockout is two again, and this is till 16.
00:30:06.190 - 00:30:12.578, Speaker D: I see. But then by the time this is eight, on the next, the next high vote, this becomes 32.
00:30:12.744 - 00:30:13.874, Speaker B: Only when this is eight.
00:30:13.912 - 00:30:14.402, Speaker D: I see.
00:30:14.456 - 00:30:22.150, Speaker B: So the way it works is effectively you're building a tower. I initially called it lock tower because I thought it was a cute name.
00:30:22.300 - 00:30:22.710, Speaker D: Yeah.
00:30:22.780 - 00:30:29.850, Speaker B: So if this gets white, this starts at two, and then there needs to be two and four.
00:30:29.920 - 00:30:30.154, Speaker C: Right.
00:30:30.192 - 00:31:06.082, Speaker B: And until it's fully rebuilt, you don't actually get to two. 4816, 32. The idea here is that anytime you submit a vote, if this vote is at a point that is past any of the lockouts here, you clear the whole stack and you start over. And this allows you to progressively kind of undo your work. So when you vote here and you commit to this branch for 32 lockouts, you're saying that I'm going to effectively be unavailable for x amount of time to break this consistency.
00:31:06.226 - 00:31:06.966, Speaker C: Right.
00:31:07.148 - 00:31:14.120, Speaker B: So when that actually happens, you remove this vote and you start rebuilding the stack again.
00:31:14.650 - 00:31:16.614, Speaker D: Does that make sense? Yeah, perfect sense.
00:31:16.652 - 00:31:34.810, Speaker B: And this allows everybody to enroll. What also is interesting is that I can say that at 32 lockout, I want to observe 50% of the network to be at the same branch. And I can make a greedy choice. Like this is not even network enforced.
00:31:36.050 - 00:31:39.870, Speaker A: By the time you observe 50% already voted on this branch, you'll vote.
00:31:40.530 - 00:32:17.834, Speaker B: I will not double my lockup past 32 on this branch unless I observe 50% plus. And if I don't observe, this gets expired and eventually it gets totally expired. So that is like effectively how the nodes on the network can force to be in the same branch. It's because they pick some branch behind that's like at an expiration of 1 hour. And they say that they will not commit to a branch on the network unless an hour ago. We see that 50 plus percent of the stake pool has committed as well.
00:32:17.952 - 00:32:21.446, Speaker D: And the topmost vote, what is the lockout on the topmost vote?
00:32:21.558 - 00:32:23.770, Speaker B: It's like short. So one or two slots?
00:32:23.850 - 00:32:31.520, Speaker D: One or two slots. Can I not do the following? Can I not vote? Wait for it to expire. Vote again. Wait for it to expire. So never.
00:32:32.310 - 00:32:50.280, Speaker B: So the reward actually comes in 232, right. So when you double again, this is the queued and that's your reward. So you actually have to commit to.
00:32:51.370 - 00:32:52.710, Speaker A: One branch for a while.
00:32:52.780 - 00:32:53.110, Speaker C: Right.
00:32:53.180 - 00:32:58.914, Speaker B: So what this actually means is that we're kind of finalizing this in 32 blocks.
00:32:58.962 - 00:32:59.318, Speaker C: Right?
00:32:59.404 - 00:33:03.894, Speaker B: So 32 blocks ago we said that this is the final branch and this is the thing that matters.
00:33:03.942 - 00:33:04.106, Speaker C: Right.
00:33:04.128 - 00:33:08.330, Speaker B: It's the thing that the network will take a big loss.
00:33:09.950 - 00:33:17.006, Speaker D: 50% of people voted on this, but not all of them have a large tower in it.
00:33:17.028 - 00:33:17.358, Speaker C: Right.
00:33:17.444 - 00:33:20.106, Speaker D: It's just they voted. So some of them have less to lose.
00:33:20.298 - 00:33:23.966, Speaker B: But your threshold would be set somewhere here, somewhere in the middle.
00:33:24.148 - 00:33:34.770, Speaker D: I'm not going to be just saying if 50% voted, I'm going to be saying 50% in this many blocks are like this deep into this branch going back to this.
00:33:34.840 - 00:33:46.982, Speaker B: Right. What I can do is I can actually withhold my boat and observe both. L two, right? So I can observe both of these. There's no way for us to stop nodes from doing this.
00:33:47.036 - 00:33:48.810, Speaker D: But l two is not the leader here, right?
00:33:48.880 - 00:33:50.214, Speaker B: No, l two was the leader.
00:33:50.262 - 00:33:50.426, Speaker C: Right.
00:33:50.448 - 00:33:59.340, Speaker D: They transfer. I'm more interested in the case where. So let's say l two was the leader in this slot. Yeah.
00:33:59.790 - 00:34:02.166, Speaker B: And l three had a faster asic and overran.
00:34:02.198 - 00:34:02.426, Speaker C: Oh, no.
00:34:02.448 - 00:34:03.694, Speaker D: So let's say this is time, right?
00:34:03.732 - 00:34:04.814, Speaker C: Right? Yeah.
00:34:04.852 - 00:34:05.534, Speaker B: Okay, got it.
00:34:05.572 - 00:34:17.330, Speaker D: So l three published it here and nothing was published here. Yeah, but I as l four will I always choose the one that was published later because let's say l three is complete.
00:34:17.480 - 00:34:35.970, Speaker B: So the greedy choice that we want the network to make is to. If they have multiple branches, it should be unlikely because we're using this force delay to actually have l three generate x amount of data before they transmit. But they can cheat. They can use a faster ASIC. Or this one could just be slower.
00:34:36.050 - 00:34:36.680, Speaker C: Right.
00:34:37.070 - 00:34:45.046, Speaker B: So when you actually observe two branches, you should pick the one that maximizes the network reward, so maximizes the lockup.
00:34:45.158 - 00:34:45.770, Speaker D: I see.
00:34:45.840 - 00:34:55.866, Speaker B: So because you can examine both branches, you can compute all the votes and you can say that branch one is the one that actually has the most lockout.
00:34:56.058 - 00:35:01.840, Speaker D: But isn't this counterintuitive? The higher the network reward is, the more inflated the Salana token, the less.
00:35:05.810 - 00:35:14.430, Speaker B: But the more committed the more there is can be. Given these lockouts, you can actually very easily compute economic finality.
00:35:14.510 - 00:35:14.994, Speaker D: Makes sense.
00:35:15.032 - 00:35:15.234, Speaker C: Right.
00:35:15.272 - 00:35:23.714, Speaker B: And the more economic finality there is in a branch, that's the branch that you want to be a part of because that's the one where you actually get a reward. All the other ones die.
00:35:23.762 - 00:35:35.110, Speaker D: Right, I see. Well, it's not sort of humidity. Obvious to me that I would multiply economic finality by how much the token is dilution. It sort of sounds like economic finality.
00:35:35.190 - 00:35:41.354, Speaker A: Presumably vendors will be so small compared to the reward, you'll get proof of work.
00:35:41.392 - 00:35:41.546, Speaker C: Right.
00:35:41.568 - 00:36:02.382, Speaker B: Every block dilutes bitcoin, but you don't want to mine a bunch of fork that is not going to be accepted into the main tree because you're wasting electricity. Right. So similarly here, you want to commit to a fork that has the most economic finality because that's the one that's most likely to survive. Because really, for us, lockout is economic finality.
00:36:02.446 - 00:36:03.154, Speaker C: Right.
00:36:03.352 - 00:36:13.222, Speaker D: And when you compute economic finality, do you just count those guys who just dropped beyond two to the 30th second, or are you also, like, trying to predict how much people will get in the future?
00:36:13.276 - 00:36:18.742, Speaker B: The way it's written right now is it just basically adds up all the lockouts on the branches and pixel ones.
00:36:18.796 - 00:36:45.854, Speaker D: So that's kind of rough approximation, but then consider. So let's say everybody has the speed of asics is the same for everybody. This is time. Like the second zero. On the second zero, one of the leaders transmitted. Right but let's say l two actually didn't see, and then on second one, let's say it once per second, neither of them. Well, actually, let's say l two did observe something.
00:36:45.854 - 00:36:56.222, Speaker D: On second two, l two did observe something, and that continues for a while. L three has only one block it observed, and it could be produced by l three itself and never distributed.
00:36:56.286 - 00:36:56.562, Speaker C: Right.
00:36:56.616 - 00:37:02.422, Speaker D: But l two has been building for a while and then at some point alti publishes the transmit message again.
00:37:02.556 - 00:37:13.766, Speaker B: So this one would be effectively dropped by the network because everyone that saw this tease have voted. The majority of the network that observed the data has voted. And when they observe this, that doesn't include these votes.
00:37:13.798 - 00:37:15.862, Speaker D: They can't actually, because they've been voting.
00:37:16.006 - 00:37:17.386, Speaker B: And they've locked out.
00:37:17.488 - 00:37:18.042, Speaker C: Right.
00:37:18.176 - 00:37:22.814, Speaker B: Because now if I've observed three data transmissions, my lockout is somewhere here.
00:37:22.852 - 00:37:23.390, Speaker C: Right.
00:37:23.540 - 00:37:26.718, Speaker B: So this guy is effectively, like, makes sense.
00:37:26.884 - 00:37:37.780, Speaker D: This is basically packet loss. Yeah. My point was that this t is twice as important as any of this because there are many t's there, right.
00:37:38.390 - 00:37:52.450, Speaker B: So because this leader didn't observe this t, they didn't vote on it. I'm assuming that it's whatever the probability, whatever the majority of the network has observed, if the next leader is also in that majority, then it kind of continues.
00:37:52.530 - 00:38:01.980, Speaker A: But if this guy voted for this and l three included all those votes on the first t and l two.
00:38:03.870 - 00:38:08.106, Speaker B: Intuitively, think of it this way, right. I have some probability of success.
00:38:08.208 - 00:38:08.858, Speaker C: Right.
00:38:09.024 - 00:38:25.950, Speaker B: And the next leader has some other probability of success. If all of these are in the majority, then you continue kind of stacking both. But as soon as somebody's in the minority fail, this node fails.
00:38:26.030 - 00:38:26.274, Speaker C: Right.
00:38:26.312 - 00:38:30.386, Speaker B: And the network has x amount of time to kind of get back to the majority state.
00:38:30.568 - 00:38:34.660, Speaker D: And another question, does that make sense?
00:38:35.750 - 00:38:40.262, Speaker B: Every time you rotate, there's some probability that you've observed the data, right?
00:38:40.316 - 00:38:40.726, Speaker C: Right.
00:38:40.828 - 00:38:48.620, Speaker B: And there's some probability that you're in the majority group. And as long as that's true and continues to be true, we kind of keep running, keep going forward.
00:38:49.630 - 00:39:05.486, Speaker D: Let's say in the chain that I observed, the first leader transmitted the message and the second leader said, let me say, how does d even appear the.
00:39:05.508 - 00:39:10.190, Speaker B: Second leader is producing its own Poh from the last vote that they've taken.
00:39:10.260 - 00:39:11.354, Speaker D: From the last vote.
00:39:11.482 - 00:39:15.730, Speaker A: When they transmit, they also prove that they see and see five transmissions.
00:39:16.950 - 00:39:30.962, Speaker D: So tv effectively means that, let's say I'm a leader. I saw this transmission, which is. Yeah, so I'm leader three. For example, I saw transmission from leader one, and then I didn't see anything from leader two. Do people vote on this feed?
00:39:31.026 - 00:39:31.990, Speaker B: No, they don't.
00:39:32.410 - 00:39:37.400, Speaker D: Vote only accumulates when something is actually transmitted virtual. Right.
00:39:38.090 - 00:39:41.114, Speaker B: They have no data and they can be derived from any point in the state.
00:39:41.152 - 00:40:03.454, Speaker D: So in my hypothetical example, this thing has like one vote, which was never doubled because nobody voting on it. While here, this message, even though it was published later by this time, has multiple people having dates. Yeah, I see. So like, usually the chain which has more transmissions will be chosen, effective.
00:40:03.502 - 00:40:04.100, Speaker C: Exactly.
00:40:05.830 - 00:40:31.110, Speaker B: This kind of saves us in the case where somebody is built an asynchronous circuit in their basement lab, basement bath, and they're like generate a bunch of ran and then do a t here. One, because this will pop a bunch of rewards, so it'll have lower economic finality. And two. Right. That's exactly kind of how nobody can vote.
00:40:32.110 - 00:40:46.880, Speaker A: But it's possible to have, if you have some portion of the stake, you can flip the thing. Right. What is the portion of the stake that you need to vote on this.
00:40:47.250 - 00:41:13.686, Speaker B: This is like an actual censorship attack, right? So if you're l four and you're some portion of the stake, you can potentially just overrun the previous leader every time. So you do a transmission here, ignore everybody that kills whatever this node is. Right. And if you're clever, you would include all the validator messages except the ones.
00:41:13.708 - 00:41:14.722, Speaker A: You want to censor.
00:41:14.866 - 00:41:39.840, Speaker B: And this should be the majority of that work. So this is like the actual censorship attack. The only way we fight that is the VDFs will progressively approach whatever secret VDF you can build and speed. So the difference between v zero and v one might be two x, but between v one and v two is going to be 50%. Between v three and v four 5%. That's fine.
00:41:42.790 - 00:41:43.106, Speaker C: Right.
00:41:43.128 - 00:41:55.880, Speaker A: Now, for example, let's say if everybody has the same homogeneous hardware, what is the portion of the stake you need? If you, let's, let's say you have two leaders right now, right.
00:41:56.730 - 00:41:58.870, Speaker B: The rotation is stake weighted.
00:41:59.530 - 00:42:00.680, Speaker C: Yeah. So.
00:42:02.490 - 00:42:03.542, Speaker B: The active set.
00:42:03.596 - 00:42:03.766, Speaker C: Right.
00:42:03.788 - 00:42:33.210, Speaker B: Is computed from the network. So whatever it is, we compute this and then we decide this is the new schedule and you just rotate all the leaders. And even if you're a low stake node with crappy hardware, the only thing that happens is a packet loss. So our goal is to make these slots as fast as possible. In theory, it can be 100 milliseconds, because this node could have pre computed all the data right before their slot and started transmitting.
00:42:33.370 - 00:42:37.150, Speaker A: Right, 200 milliseconds to get to China.
00:42:38.150 - 00:42:50.070, Speaker B: You can pipeline it. Right. So when it goes to China, they might actually be. Right. If they're in the minority state group, they'll just get kind of censored out potentially.
00:42:51.450 - 00:42:52.774, Speaker A: We're censoring China now.
00:42:52.812 - 00:42:53.400, Speaker D: Right.
00:42:53.850 - 00:42:55.074, Speaker B: But it's permissionless.
00:42:55.122 - 00:42:55.334, Speaker C: Right.
00:42:55.372 - 00:42:56.070, Speaker B: We can't.
00:42:56.890 - 00:42:58.280, Speaker D: I think it's going to be that.
00:43:01.390 - 00:43:09.850, Speaker A: Let's imagine an example where you have one account that has 20% of stake, right?
00:43:09.920 - 00:43:12.346, Speaker B: So they want to fear one fifth.
00:43:12.378 - 00:43:30.226, Speaker A: Of the time, one fifth of the time. And then they also have 20% of all the votes. So can they pretty much do transmit, ignore, ignore, transmit, and they add like 20% of their votes in this branch. So can they censor the rest of the.
00:43:30.328 - 00:43:40.774, Speaker B: No, because if the rest of the network is observing this, which includes this, then one, they can't choose this unless, I mean, this node can choose this.
00:43:40.812 - 00:43:41.062, Speaker C: Right.
00:43:41.116 - 00:43:45.670, Speaker B: But effectively everybody else can't choose this branch.
00:43:47.610 - 00:43:50.406, Speaker A: Let's imagine before that they all own the same branch, right?
00:43:50.428 - 00:43:50.578, Speaker C: Right.
00:43:50.604 - 00:43:53.834, Speaker A: So there will be folks that have voted for this as well.
00:43:53.872 - 00:43:54.074, Speaker C: Right.
00:43:54.112 - 00:43:56.634, Speaker A: So you accumulate all that and you own 20%.
00:43:56.672 - 00:43:58.698, Speaker B: But they would have to skip, right?
00:43:58.784 - 00:43:59.130, Speaker A: Yeah.
00:43:59.200 - 00:44:06.254, Speaker B: Those folks would have to actually actively participate in this. If they voted on this. They cannot move that vote over.
00:44:06.372 - 00:44:08.286, Speaker A: Yeah, they cannot move their over.
00:44:08.388 - 00:44:09.406, Speaker B: So they're locked out.
00:44:09.428 - 00:44:09.902, Speaker C: Right.
00:44:10.036 - 00:44:21.166, Speaker B: From their perspective, this transmission is a failure because it doesn't include their previous votes. It's a purpose, it's separate for it. So they can't switch over until this expires.
00:44:21.278 - 00:44:21.940, Speaker C: Right?
00:44:24.950 - 00:44:25.700, Speaker A: Yeah.
00:44:28.650 - 00:44:33.106, Speaker B: They would have to effectively, you can bribe them and tell them, hey, don't vote.
00:44:33.218 - 00:44:33.990, Speaker A: Don't vote here.
00:44:34.060 - 00:44:36.390, Speaker B: Yeah, and I'll pay for your rewards.
00:44:37.530 - 00:44:40.346, Speaker A: Don't vote at all. Postpone it a little bit.
00:44:40.448 - 00:45:21.634, Speaker B: But you don't even have to do that. You can just tell them that. And if people agree, then the point, the main thing. So what's interesting is in our simulation, if you have all the nodes, instead of picking the branch with the most reward, pick the branch with the least reward, but still not allow, like, let's say, I think, was it 256, not allow this lockout to be under 50% of the network, right. So no, they just want to disrupt the network. They can't actually vote on branches that die because then they just lock themselves out from the network.
00:45:21.682 - 00:45:22.278, Speaker C: Right.
00:45:22.444 - 00:46:00.210, Speaker B: So the thing that they choose is to continue voting in a branch that becomes the main trunk, but always pick out of multiple branches, the worst one, the one that has the least amount of economic finality. This network still moves forward just at about like with everybody being parasitic, it moves forward at about like 10% of the rate. So that is really cool. That means that if the hardware is actually uniform, right. We can actually run this thing to be fairly censorship resistant and fairly kind of attack resistant to parasitic nodes.
00:46:00.790 - 00:46:02.798, Speaker A: That's like if everybody's parasitic.
00:46:02.974 - 00:46:03.700, Speaker C: Yeah.
00:46:04.090 - 00:46:14.680, Speaker B: It still moves forward. Right. Versus like bitcoin, if everybody's voting, if everybody's choosing their own brand, it actually doesn't move forward.
00:46:15.850 - 00:46:21.802, Speaker D: So I have a couple. One short question is you need randomness, right? What is that?
00:46:21.856 - 00:46:23.062, Speaker B: No, we don't use randomness.
00:46:23.126 - 00:46:26.374, Speaker D: Also, how leaders selected, how is the order of leaders?
00:46:26.422 - 00:47:01.810, Speaker B: Randomness is hard. It's very hard to build a random source that is actually secure unless you have a BDF. We do, but again, our VDF is biased because every time we submit a message that creates a bias. So you could, in theory, what would be easy for us to provide is you have like TT, but everybody's computing these virtual branches from the last transmission. What you can do is use this random value based on this for this block.
00:47:01.890 - 00:47:03.640, Speaker D: That sounds like a terrific idea.
00:47:04.170 - 00:47:10.182, Speaker B: It's a little weak because anyone with just an ASIC, that's too predict the random value.
00:47:10.236 - 00:47:10.840, Speaker D: Right.
00:47:11.210 - 00:47:12.718, Speaker A: You need like many vds.
00:47:12.834 - 00:47:23.760, Speaker B: Yeah, but that doesn't work because then everybody has to run like 32 cores worth of vdfs and that is problematic. So for us, we don't use randomness. What we actually use is just.
00:47:26.290 - 00:47:26.654, Speaker C: The.
00:47:26.692 - 00:47:33.390, Speaker B: Number of counts, the height as the seed to feed a random, to just scramble those active stuff.
00:47:33.460 - 00:47:33.950, Speaker D: I see.
00:47:34.020 - 00:47:37.342, Speaker B: So it's really just round robin with a little bit of reordering.
00:47:37.406 - 00:47:43.540, Speaker D: I see. But like the person, if I have ten x more stake than you, am I going to be?
00:47:44.150 - 00:47:47.426, Speaker A: You can actually buy a splitting the.
00:47:47.448 - 00:47:59.270, Speaker B: Account every time this is rotated over a long time. You will be right, because this is random enough. It's just predictable. Hopefully it's totally predictable, but unbiaseable.
00:47:59.770 - 00:48:00.758, Speaker D: I see.
00:48:00.924 - 00:48:03.414, Speaker A: Wait, but you just said the number of accounts.
00:48:03.542 - 00:48:13.558, Speaker B: No, the number, the height. So this is like height 100 million and this is height 200 million. That's the seed to some random.
00:48:13.654 - 00:48:17.550, Speaker D: But if this is the seed, can I not, you know, in advance everybody.
00:48:17.620 - 00:48:20.926, Speaker B: That'S going to be scheduled for the rest of the time, you know, in.
00:48:20.948 - 00:48:22.334, Speaker D: Advance everybody who's going to.
00:48:22.452 - 00:48:27.298, Speaker A: Everybody's participating regarding their deception. Like they don't decide if they want.
00:48:27.304 - 00:48:30.318, Speaker B: To participate or not when they stake.
00:48:30.414 - 00:48:30.770, Speaker C: Right.
00:48:30.840 - 00:48:31.794, Speaker B: There's an active site.
00:48:31.832 - 00:48:37.880, Speaker A: Yeah. So you can decide not to stake if, you know, you can pretty much decide to stake or not, just split your account.
00:48:40.490 - 00:48:46.178, Speaker B: You don't have to stake. But everybody that's staked, that wants to be in the rotation says that they're.
00:48:46.274 - 00:49:23.874, Speaker D: Let'S say that in practice, let's say in practice, 99% of people are just the same who maintain the network, right? Because it seems like if you already, let's call it mining, if you're already mining, it doesn't make sense to stop mining. And if you're not mining yet, like presumably not that many people will be on boarding, especially when the network is efficiently operational, operational. Then it feels like what I can do is I can create 1 million accounts and then start using combinations of those accounts to see how introducing those into the set of validators influence the order, right? And then once I find that good one, I just move my assets in there.
00:49:23.992 - 00:49:29.334, Speaker B: So just to make this practical, this was computed over like two weeks, right?
00:49:29.372 - 00:49:34.470, Speaker D: Okay, so I need to rely on the fact that within two weeks, nobody else, right?
00:49:34.540 - 00:49:42.410, Speaker B: And also your staking requires like two weeks warm up, two weeks cool down. We just stick human timeouts there. That's it.
00:49:42.480 - 00:49:44.326, Speaker D: This is suspiciously similar to threshold.
00:49:44.358 - 00:49:50.220, Speaker A: Proof of that is pretty much what we use for vegan chain for our.
00:49:52.050 - 00:49:59.520, Speaker B: Same, everybody's like we're all circling the same ideas, not the train.
00:50:00.530 - 00:50:15.650, Speaker D: So the second question is, so the 40,000 nodes you drew before in the tree, each of them has a gpu, right? Effectively because each of them needs to validate. Now let's say I'm a point of sale terminal. What sort of other clients?
00:50:16.250 - 00:51:15.480, Speaker B: This is the complicated part. So this is where Justin's magic VDF, that asymptotic verification would be great, would be really useful. But because that's not ready, the way we're doing this is your validation message can point to the previous validation set, right? So what use this in client. In fact, this is what we're doing for kind of our signing in play. You can observe that x amount of, you start with some set of validators, right? As you trusted validators, like you've observed the network you've done at a station, right? This is my set of validators. You can observe sets in the future of votes that simply point back. So now you know that these guys are derived from the trust of set because they're a subset of this and they voted on the branch that you care about.
00:51:16.250 - 00:51:19.734, Speaker D: So you're not validating the hashes per se or the history.
00:51:19.932 - 00:51:24.106, Speaker A: You just trust set of validators to make payment for you.
00:51:24.128 - 00:51:35.882, Speaker B: This is weaker, definitely weaker than validating the time. The problem here too is what you need to do is like actual transaction validation.
00:51:35.946 - 00:51:36.174, Speaker C: Right?
00:51:36.212 - 00:51:50.430, Speaker B: Like I sent you a transaction and you want to quickly verify it. And this appears somewhere here in the middle of all these, of all of these, all the hashes. So what we need to do is actually point this back up here through like a Merville tree.
00:51:50.510 - 00:51:59.766, Speaker D: By the way, is transaction in the history? Is it just a confirmation it was received or. That's a confirmation? This is valid as of this point.
00:51:59.868 - 00:52:00.642, Speaker B: This is valid.
00:52:00.706 - 00:52:03.298, Speaker D: This is valid. Transaction is only included if it's validated.
00:52:03.394 - 00:52:04.600, Speaker C: Yeah, makes sense.
00:52:05.050 - 00:52:07.678, Speaker A: So you have an account model or do you kick.
00:52:07.714 - 00:52:17.820, Speaker B: So we have an account model. In Theory, you can submit a transaction that pays out to two different accounts. Yeah.
00:52:19.490 - 00:52:29.630, Speaker A: Account model will do everything. But then for account model, do you include them? Include a transaction and a numerical rule. Pretty much after this transaction is applied.
00:52:31.010 - 00:52:46.014, Speaker B: We just don't allow them to be in if they fail. Yeah, because if this is just a payment, you can do that. But for a smart contract, you would need to do, you would have to print out rerun.
00:52:46.142 - 00:52:46.820, Speaker C: Right.
00:52:47.210 - 00:52:49.142, Speaker B: And that's like a much more complicated model.
00:52:49.196 - 00:52:57.340, Speaker A: But if I just join the network to start validating, I need to rerun all of the transactions from the beginning of time.
00:52:58.430 - 00:53:02.806, Speaker B: These validation messages include effectively the signature of the account stake.
00:53:02.838 - 00:53:03.754, Speaker C: I see. Okay. Right.
00:53:03.792 - 00:53:11.190, Speaker B: So you can look at the network and see everybody's stake is voting for these signatures. Go give me a checkpoint.
00:53:11.350 - 00:53:12.642, Speaker D: And that's like a merkle.
00:53:12.726 - 00:53:13.022, Speaker C: Yeah.
00:53:13.076 - 00:53:14.846, Speaker A: So voting is pretty much.
00:53:15.028 - 00:53:15.920, Speaker B: It is.
00:53:19.730 - 00:53:21.834, Speaker D: When I'm recomputing the hashes on the GPU.
00:53:21.882 - 00:53:22.046, Speaker C: Right.
00:53:22.068 - 00:53:44.466, Speaker D: So every now and then, let's say I was the leader, and I was like, this is my hash one. And then the sha of hash one is my hash two. And then the Shah of hash two is my hash three. But then for hash four, I also have some transaction. Right? And so this is my hash four. This is also running a GPU.
00:53:44.498 - 00:53:44.646, Speaker C: Right.
00:53:44.668 - 00:53:48.682, Speaker D: So GPU has this branch which says if it has a transaction, the data.
00:53:48.736 - 00:53:53.382, Speaker B: That you're submitting to the GPU is just hashes.
00:53:53.446 - 00:53:54.454, Speaker A: Hashes of transaction.
00:53:54.502 - 00:53:57.946, Speaker B: Yeah, this is like a big blob of data. And we hash that.
00:53:58.048 - 00:53:58.602, Speaker D: I see.
00:53:58.656 - 00:53:58.922, Speaker C: Right.
00:53:58.976 - 00:54:00.550, Speaker B: And then we submit the hash.
00:54:00.710 - 00:54:06.254, Speaker D: If there's no transaction, it's like a zero. Yeah. There's some sort of value there as well.
00:54:06.292 - 00:54:07.040, Speaker C: I see.
00:54:08.210 - 00:54:18.740, Speaker A: Do you do it for each one? So you have h 10, like every time for a million times? Or you just.
00:54:19.350 - 00:54:42.330, Speaker B: We don't store the zeros. We don't store the zeros unchained. The ledger data structure just contains, effectively, number of counts, number of hashes, number of DoH hashes, secured hash, and then an optional hash of the blob.
00:54:45.710 - 00:54:47.226, Speaker A: The hash of the blob adds a.
00:54:47.248 - 00:54:52.640, Speaker B: Specific at this count. So when you're replaying this, you just simply stick it in there.
00:54:54.530 - 00:55:05.186, Speaker A: But you also have the transactions attached to this data structure for this specific step. And then how many do you have? Pretty much the question is.
00:55:05.208 - 00:55:33.340, Speaker B: So blob is two k. Sorry, 64 kb. Depends on the transaction size, because they're smart contracts that are variable. Right now, our maximum is 512 bytes. But like a simple move, the smallest one we can come up with is, like 176 bytes per transaction. Per transaction for just one unique payment.
00:55:36.510 - 00:55:39.910, Speaker A: How much bandwidth do you need to GPU to verify all that?
00:55:40.000 - 00:55:43.870, Speaker B: Like, modern day gpus have 16 gigabits per second bandwidth.
00:55:44.930 - 00:55:45.294, Speaker C: Yeah.
00:55:45.332 - 00:55:49.626, Speaker B: PCI three is one gigabit per lane. Yes, 16 lane.
00:55:49.738 - 00:55:53.002, Speaker D: But you don't need to send the actual transaction, just sending the hash of it. Right.
00:55:53.156 - 00:56:02.530, Speaker B: Well, we actually do send the whole transaction data because you can't do that many signatures per second on cpus.
00:56:04.250 - 00:56:05.894, Speaker A: You verify signatures there, too, right?
00:56:05.932 - 00:56:23.978, Speaker B: Yeah, that's our defense against ddos, is just verifying many more signatures per second. And the network can actually transmit. Well, that one stops like a router attack.
00:56:24.064 - 00:56:24.314, Speaker C: Right?
00:56:24.352 - 00:56:32.686, Speaker B: Like, if everybody's flooding, like an upstream component, but at each node, we can actually verify way more signatures per second.
00:56:32.868 - 00:56:34.042, Speaker A: Than you can transmit.
00:56:34.106 - 00:56:34.720, Speaker C: Yeah.
00:56:37.490 - 00:56:41.680, Speaker A: I mean, presumably you can flood the network from many nodes into.
00:56:44.230 - 00:56:45.780, Speaker D: Shall we wrap up here?
00:56:46.470 - 00:56:51.090, Speaker A: Is there any other parts you want to mention? Like sportsmart contracts?
00:56:51.430 - 00:57:43.570, Speaker B: Yeah, that is a totally fun problem itself, because our contracts. What is the transaction? For us, transaction is. Well, first of all, the actual bottleneck in computation isn't cpu, it's memory. The problem with making things go fast is just memory pipelining through the system. If you look at just operating systems and how processes run, like, what you're dealing with is memory throughput, like caches, all this other stuff. So how do we make this fast is every transaction specifies a bunch of public keys, and these keys are pointers to accounts.
00:57:45.270 - 00:57:46.020, Speaker C: Right?
00:57:48.310 - 00:57:56.950, Speaker B: And these accounts have some data associated with them, and you can think of it as user data.
00:57:57.020 - 00:57:57.350, Speaker C: Right.
00:57:57.420 - 00:58:51.914, Speaker B: But the way I think of it is we have an operating system with a single address space. It just happens to be the size of public keys, right? So 256 foot address space, but also memory protection. So these accounts are assigned to a process, right? By process, I mean, there's some account here, whose data contains program code. And this code is just code, it has no state. So this program code is a state transition function over these accounts and the accounts that are assigned to this. This is the only code that can modify the data. So whenever you do an assignment, the state is bits are zero.
00:58:51.914 - 00:59:11.398, Speaker B: And this code is the only thing that can flip those bits, right. So every time you submit a transaction, it does some bit flipping here. So what else it can do? Because it can move tokens around however it wants and it can also pay out tokens to external, to external accounts.
00:59:11.594 - 00:59:13.762, Speaker A: This is done outside of this code, right?
00:59:13.816 - 00:59:36.354, Speaker B: No, this code does this modification and then this actual program could run on gpus, right. Our bytecode is like very simple. It's called Berkeley packet filter. It's designed for porting into any architecture you want. So we can actually port it to spear five, so we can execute all these programs on gpus and then pagens.
00:59:36.402 - 00:59:38.040, Speaker A: Data first into view.
00:59:38.570 - 00:59:49.230, Speaker B: Go fetch it, go run all the stuff in parallel, get the modifications back. And the only thing we're validating is that if the data has been modified, that it was done by this program.
00:59:49.300 - 00:59:49.822, Speaker C: Right.
00:59:49.956 - 01:00:05.226, Speaker B: And if there's tokens that have been moved around, that tokens going out were moved, this program didn't spend any external tokens and that the total sum of tokens is the same, but the tokens.
01:00:05.258 - 01:00:06.702, Speaker A: Here are meaning native tokens.
01:00:06.766 - 01:00:07.282, Speaker C: Yeah.
01:00:07.416 - 01:00:10.820, Speaker B: The actual thing that pays for cpu and storage and whatever.
01:00:12.550 - 01:00:16.040, Speaker A: So if you're for example, moving some other.
01:00:16.970 - 01:00:34.860, Speaker B: Yeah, that's just data modification. That's date, that's opaque. We don't really know what that is, but because every transaction effectively specifies all the memory regions it needs to access, we can schedule all the non relaxed ones.
01:00:37.310 - 01:00:39.654, Speaker D: And what languages compile to that Bytecode?
01:00:39.782 - 01:00:50.960, Speaker B: It's LLvM backend. So anything from LLVM, the backend is kind of weak. It's a little weak, doesn't support all the features yet, but that's just work.
01:00:56.210 - 01:01:03.730, Speaker A: So if you have any LLvM code to GPU, why are we still writing CUDA?
01:01:04.490 - 01:01:34.238, Speaker B: No, I'm pretty sure CUDA uses LVM. The problem isn't that it's that we have a bytecode that is very stable, right? This is a very stable bytecode. It's going to be unchained forever, and we can port it to whatever architecture happens to be the fastest one. It may be for some use cases that spear five is the fastest thing, but for others it's not like x 86. So we want to really keep it in this very simple to manage intermediate state.
01:01:34.324 - 01:01:40.234, Speaker A: So right now. But you execute all these programs on GPU, even though maybe some of them may be way more efficient on CPU.
01:01:40.282 - 01:01:43.360, Speaker B: Because sscs directions right now it's all CPU based.
01:01:44.610 - 01:01:45.022, Speaker C: Yeah.
01:01:45.076 - 01:01:58.150, Speaker B: So do you guys want to write some JIT to spear five? You can use the engine in your blockchain, your chain.
01:01:59.210 - 01:01:59.960, Speaker A: Yeah.
01:02:02.090 - 01:02:31.066, Speaker B: What's awesome about using a GPU for contract execution is if you have a spike like cryptokitties, the same program code can execute many different transactions at the same time on the GPU because you have this massive amount of lanes for the same code. So if you had a cryptokitties like Spike, you actually don't use a different thread per transaction. Use one thread that goes over this enormously wide lane.
01:02:31.178 - 01:02:33.940, Speaker A: Yeah. And then you parallel to everything else.
01:02:34.390 - 01:02:36.834, Speaker B: So that is like the really awesome thing.
01:02:36.952 - 01:02:41.890, Speaker A: You can do that on CPU too. You just schedule things based on data access.
01:02:41.960 - 01:02:42.674, Speaker C: Right?
01:02:42.872 - 01:03:20.590, Speaker B: Not with CPU threads because you have to use vectorized code. So like something like AVX. But the AVX pipeline is really limited for massive parallelization compared to a GPU one. So in theory, right. The cost to execute like a more popular program should actually be lower for the network. You can still precache, like you don't need to page, right. And you're effectively just reusing these lanes.
01:03:20.590 - 01:03:29.090, Speaker B: We'll see if that pays out. In practice. You have to be measured. Yeah, but it looks like it'll work in theory.
01:03:29.750 - 01:03:34.260, Speaker A: But this is done, in your case, 100 millisecond increments, right?
01:03:35.670 - 01:03:42.034, Speaker B: No, you have a bunch of transactions, they're all for cryptokitties, right? Yeah, there's one contract. That's the cryptokitties contract.
01:03:42.082 - 01:03:48.970, Speaker A: But I'm a leader for hundred milliseconds, right? So I collect some transactions, execute them, stick it in, and I don't need to do it anymore.
01:03:49.630 - 01:03:53.446, Speaker B: But if that's the popular transaction that's.
01:03:53.478 - 01:03:56.586, Speaker A: Being thrown there all the time, you.
01:03:56.608 - 01:04:04.400, Speaker B: Can leave this program in cash, right? Because you know that, hey, everybody's flooding the network with these cryptokitties transactions, right?
01:04:06.370 - 01:04:06.830, Speaker A: Cool.
01:04:06.900 - 01:04:14.466, Speaker B: Those are like levels of optimization that I don't know if we need yet, but they're fun to worry about.
01:04:14.648 - 01:04:18.180, Speaker D: So let's finish with few non technical questions. When is the main.
01:04:20.630 - 01:04:33.220, Speaker B: Oh, that's the hardest question. So our goal is to be FC, effectively kind of end of January, and then Mainnet will be driven, I think, by maybe more legal side in the US.
01:04:33.670 - 01:04:37.146, Speaker D: Awesome. Cool. Okay, let's wrap up here.
01:04:37.168 - 01:04:38.170, Speaker A: Yeah, thanks a lot.
01:04:38.240 - 01:04:44.618, Speaker D: Thanks a lot for having us asking questions. Cool, great.
01:04:44.784 - 01:04:49.320, Speaker A: Feel free to ask questions on YouTube. And from there.
