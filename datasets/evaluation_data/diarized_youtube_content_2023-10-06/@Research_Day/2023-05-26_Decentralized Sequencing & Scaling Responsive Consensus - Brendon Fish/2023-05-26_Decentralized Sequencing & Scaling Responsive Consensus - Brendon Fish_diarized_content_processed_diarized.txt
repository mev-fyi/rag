00:00:11.310 - 00:00:36.550, Speaker A: Hello everybody. So I'm Brendan. Probably a new face to a lot of people, too. I work for Espresso Systems, an engineer, mostly working on consensus. But I'm going to give an overview of how we're trying to solve the decentralized sequencing and scaling consensus problems. So, quick overview of what I'm going to go over. First, I'm going to go over the flow of a transaction through Hotshot.
00:00:36.550 - 00:01:17.740, Speaker A: Hotshot is the name of our decentralized sequencer which we're developing. And then I'm going to talk about some of the technical contributions we made to improve the scaling of our consensus. Our goal is to be able to scale to tens of thousands of nodes, because we think if there's a successful decentralized sequencer, it's going to attract a lot of people. Like a lot of people are going to want to be a validator and it's also going to be permissionless and approve of stake. So it will be open to many people. So it needs to scale. And then I'm going to go over some of open questions that we still have that we're still researching that I think might be interesting to other people and we don't have a solution for.
00:01:17.740 - 00:02:11.530, Speaker A: So I think Josh did a pretty good job explaining what a decentralized sequencer is, especially focusing on this first top left block about builders and stuff. But I'm going to focus primarily on the Hotshots. We can swerve block here, but just very briefly to start. In our system, transactions start from a user and they flow to this builder network, which we leave the details out. The builders essentially bid the proposer to include their bundle, and from there this bundle enters our system. In our system, we run a version of Hot Stuff Consensus. I'll discuss why we made that choice and a few details about it in the next slide.
00:02:11.530 - 00:02:57.426, Speaker A: But in parallel, we run the block commitment through Consensus to agree on the ordering. And we submit the block to the data availability layer. And Hotshot Consensus takes three consecutive quorum certificates. So you have to get quorum on three consecutive blocks for one of them to be committed. So the first in that chain would be committed at that point. Finally, once everything's committed, then we can send the new state to our sequencer contract. So we're like checkpointing to the L One, as well as the block data from the data availability layer to the L2 roll ups and the provers which will then send to their own roll up contracts.
00:02:57.426 - 00:03:53.100, Speaker A: There could be many L2 roll ups, in this case sequencing for all of them at once. And these roll up contracts can read the state from our sequencing contract. So now I'm just going to go over a few of the technical hurdles that we faced and sort of like the design principles of the system. So, like I mentioned, we want our system to scale, our consensus specifically to scale to many, many validators so two desirable properties that we want are optimistic responsiveness and linear communication complexity. In the best case, it's not possible to not have some quadratic. Worst case for BFT Consensus. So optimistic responsiveness, what does that mean? It means we can finalize blocks as quickly as the consensus algorithm can run.
00:03:53.100 - 00:04:41.486, Speaker A: One of our open problems we'll discuss why that's a bit of an issue and then linear communication is obvious that once you start having introducing quadratic communication with 100,000 nodes, consensus is going to run way too slowly. So to achieve this, we chose a version of Hot Stuff which we've modified. So Hot Stuff is a permissioned consensus protocol, which means it's designed around a known set of validators. But we want a permissionless proof of stake protocol. So we made some adaptations to the protocol to achieve that. But the reason we chose Hot Stuff is that it has these properties. It is optimistically responsive, optimistically responsive and it can be made to be linear in the optimum.
00:04:41.486 - 00:05:36.966, Speaker A: What is linear communication in the optimistic case? But the first challenge we had is view synchronization. So this is a problem where to run consensus, like all of the nodes need or all of the validators need to be in the same view at the same time, they all need to be voting on the same thing in the same window of time. So Hot Stuff requires some mechanism to make that happen. In the original paper for Hot Stuff, this is an N squared process that needs to run. But there's been a lot of recent research in that and we chose a protocol from Naor and Kadir. I may be saying that wrong, but we call it NK 20 for short view synchronization, which is constant latency and linear communication. On top of that, we add a special timeout certificate which allows progress to continue to be made without triggering view synchronization with just one timeout.
00:05:36.966 - 00:06:29.254, Speaker A: If there's two timeouts in a row, then we trigger this protocol. This means that we have better latency in the case where there's one validator offline that's malicious and doesn't add to the chain. The next step is that we have to adapt this permission protocol to work in a permissionless setting. So that means we need a stake table and we need to come up with a different scheme. For quorum certificates, we can't just simply use like an aggregate signature. So what we do is we have the stake table stored as a persistent merkel tree which will dynamically change as we go through views and people add themselves to it. And then on top of that, our quorum certificate has a bit vector of all of the validators which signed and an aggregate signature.
00:06:29.254 - 00:07:15.290, Speaker A: So the bit vector is all of the people that signed that signature. Yeah. So the next thing we had to do is how do we actually solve this problem of data availability without slowing down consensus? Right, the slowest part or biggest hindrance to throughput is transmitting all of the block data to all of the participants. So what we do is we separate out DA from consensus, which is a very common thing to do. And we combine two different approaches. The first is we use a small committee which we send the entire block data to. So in the optimistic case, this committee will always have the data and be very rapidly retrievable.
00:07:15.290 - 00:08:16.720, Speaker A: The problem is that we take a more pessimistic model on security in that we believe that an adversary could potentially bribe any committee, right, because a committee is much smaller than the corruptible stake in one third of the network. So we want to be resistance against this type of bribery attack. So we introduce also this Vid encoding. So Vid stands for Verifiable information Dispersal. So each Validator is sent like an erasure coded piece of the data. And if a threshold of the Validators receive this, they can reconstruct the data from those erasure coded shares. So what we do is we don't allow Validators to vote on any proposal which references a quorum certificate that does not have their data availability so that they haven't seen the Vid share or a certificate that proves that the data is available.
00:08:16.720 - 00:09:25.570, Speaker A: And by combining these two approaches we can get everybody to agree that the data has gotten out very quickly with the Vid shares. But also we have this very fast, in most cases committee to ask for the data. So we kind of maintain this very good optimistic case but we don't sacrifice in the security, which is the design goal of the system. And then finally, to tie all of this together, we need a source of randomness. Like how do we choose a committee, how do we choose a leader? And to do this, we come up with our own random beacon scheme which is we take a prior blocks signature set and we apply a delay function to it. So after a certain amount of time, validators can calculate a new random seed and we parameterize this delay function so that it takes longer than the timeout for one view. So a leader can't bias which signatures they include to make themselves a leader more often or an adversary the leader.
00:09:25.570 - 00:10:15.400, Speaker A: Yeah. And then finally, to speed everything up, we introduced a content distribution network approach. So we have some dedicated hardware that can get all of this information out very quickly. But we can't rely on that, right? That would be a very clear source of centralization which we don't want. So alongside of that, we also gossip everything in a peer to peer way so that let's say the CDN is trying to censor somebody or it just goes down like AWS is out or something like that. Well, we'll have this slow peer to peer backup and things will slow down, but progress can still be made and nobody can be censored. Yeah, so I think each of these could probably be like 510 minutes talk on its own and people on our team probably more than happy to talk about any of them if they interest you.
00:10:15.400 - 00:11:16.040, Speaker A: So now I'm going to move on to some more open questions that we have. The first is around proposal builder separation incentives and optimistic responsiveness. So the goal is we want the network to move as fast as possible. The problem here is that, well, if the proposer is running this auction or waiting for the builders to submit a really juicy or a very high fee or high mev block to them, they're incentivized to essentially use all of their allotted time as leader to wait for the best value to come in, like the best thing for them. So that kind of defeats the whole purpose of being optimistically responsive. Like, every block is going to take very near to the timeout for that round and we don't want that. So we've come up with a few ideas, but nothing is fully fleshed out to solve this.
00:11:16.040 - 00:12:03.270, Speaker A: We've talked about having a time based leader rotation where a leader can lead from multiple views in a row until a timer is hit and then we do a leader transition. The problem is this makes a lot of things in the system much more difficult. We haven't fully fleshed out how that might work. Like, for example, at the end of this time, how do you synchronize all of the validators to send their votes, right? They don't necessarily know who the next leader is going to be. Some people may think we've moved on to a new leader. Some people might think that there's still more time left in the current reign. The other idea is kind of like the flip side of this coin, which is like, okay, we'll allow the block to grow and accept many, many bundles from proposers.
00:12:03.270 - 00:12:59.066, Speaker A: This might actually achieve the same throughput. But still the latency, like, how quickly things can be ordered will be exactly the same for each given transaction. So the next open question we have is also around proposer builder separation, but this time it's with data availability. So you may notice that I said that the consensus is running in parallel with our data availability solution. Well, that can be a problem, right? Because the builder, of course, doesn't want to reveal its block before it knows that it's going to be committed, right? Because if the leader fails, then the next leader can steal the mev from that block. Or even worse, two malicious leaders can collude to do this on purpose. So we're trying to come up with a way that essentially solves this.
00:12:59.066 - 00:14:04.960, Speaker A: We call it like a fair exchange problem where the builder has this data that the consensus network needs and the sequencer networks needs, and the sequencer network has this commitment which they don't want to provide until they've seen the block or else we don't want to commit a block header that we don't have the data for. So we've come up with a few different ideas, but none that fully solved the problem, we think. So one is adding a partial extra round where nodes will agree to we'll lock on this quorum certificate that includes that block even though it's not committed. The problem with this is that a malicious leader can essentially leverage this and be like, okay, nodes already agreed to vote yes on the next round, but you haven't seen the data. So I'll only send the data to enough people that a quorum will be able to be formed, but then the data isn't actually available. Not enough people actually got the data because of these optimistic voters. So that was one idea we had that doesn't, I don't think, work.
00:14:04.960 - 00:15:17.730, Speaker A: Our other idea is to use cryptographic primitives, right? We can use secure hardware or threshold signatures so that the data is cryptographically revealed after it's committed to the sequence. There's one problem with the threshold signature approach is that we believe that there is an incentive for all of the validators to collude, right? They don't really have an incentive not to share their key to reveal the data. If they wanted to scheme to steal MAV from the proposers, there's nothing really stopping them. And other validators don't necessarily lose. I think it's debatable, but that's our view on it. And then the final solution is to use zero knowledge proofs where each transaction signature can be removed by the proposer and rolled up into a ZK proof, which proves that each transaction in the bundle was signed by the correct party. And that way nobody can reconstruct the block because they don't have the signatures even if it's revealed, they only have the CK proof.
00:15:17.730 - 00:16:11.990, Speaker A: And that way that it sort of provides a little barrier for people to try and steal. I think that's probably our most promising approach. And so the last sort of problem we have is that if you were paying attention, you probably noticed that I've been saying linear communication the whole time, but we use a bit vector of all of the nodes which signed a quorum certificate. Well, that's not actually linear because the length of this bit vector grows in N and it's included in every single proposal. So this is N squared communication. Technically we sort of hand wave around it for validator sets which are like less than ten k because the size isn't huge, it's only like a kilobyte and a quarter for 10,000 nodes. But it gets very large when we're talking about 100,000 nodes.
00:16:11.990 - 00:16:57.810, Speaker A: So one way you could solve this is by using a Snark proof to make this a constant size or a smaller size. The problem is then you have a bottleneck where the proposer the leader needs to calculate this proof. And even if we use dedicated hardware for this, we have some server that's really good at calculating the proof. It's still going to still potentially going to be the slowest part of the system, so that's not particularly desirable. Another potential solution is using threshold signatures, but this is also challenging because the network is constantly refreshing at stake. And so we have to recalculate these things. And yeah, this can also be quite expensive to calculate.
00:16:57.810 - 00:17:14.360, Speaker A: All right, so that was all that I wanted to cover. Lots of sort of different open topics of research. Be happy to connect with anybody if people are interested in this sort of thing. And thank you. Thank.
