00:00:10.290 - 00:01:07.770, Speaker A: So. Thanks, MJ. So this talk is not going to be as scary as you might think. Basically how I would frame this is that Tarun just now told us about how zero knowledge can provide better guarantees in mechanism design. But he used zero knowledge as a black box and pretty much all the talks we've heard today use ZK as a black box, whereas this talk actually opens up the black box. And the motivation for doing this is that first of all, a lot of the recent advances in the efficiency of ZK proofs come from opening the black box. And secondly, a lot of features such as collaborative proving depend on us knowing where to combine MPC with zero knowledge proofs.
00:01:07.770 - 00:03:04.058, Speaker A: Overall, I think what I want you to take away from this is some high level understanding of how zero knowledge proofs work and hopefully which parts of the zero knowledge stack are useful for your protocol. So we're going to start off basically at a high level and then we'll go down to some details of constructions and then finally we'll end up at an example which is the Lurk programming language that compiles to a zero knowledge back end. I want to start off with some motivating applications. So today I've heard quite a few, for example the L1 Zkvm and for example collaborative proving between builders and searchers. So I wanted to bring up a few that are maybe less well known because they were conceived at a time when zero knowledge proofs were not efficient and were not performant enough to actually make these applications interesting. So, for example, this paper from 2015 suggests a solution for verifiable cloud computing and specifically they focus on MapReduce, which is useful for a great variety of programs like machine learning, bioinformatics. And this solution makes use of a primitive called proof carrying data which allows us to enforce some predicate locally which allows us to enforce a local predicate on a global scale.
00:03:04.058 - 00:04:41.546, Speaker A: So at every point in this computation each cluster does not need to trust any of the other clusters, rather it only needs to verify a single proof that the whole history of computation up to that point was correct. Another use case that I think many of us are familiar with is Zkml, so I won't say too much about this. And lastly, of course, virtual machines like L1 Zkvms and risk zero. So now that I've hopefully motivated this talk, we'll go into the high level overview and this is basically talking about the proof system stack and a taxonomy of zero knowledge proofs. So the proof System stack can be divided into these components. We start off with some computation and we arithmetize it in order to compile it to a constraint satisfaction problem that we can efficiently check with a few probabilistic algebraic checks. And then to instantiate this in the real world in an efficient and secure way, we pick some kind of cryptographic compiler that introduces cryptographic assumption, for example, discrete lock hardness or for example, collision resistant hash functions.
00:04:41.546 - 00:05:44.260, Speaker A: And we finally arrive at our proof system. So some popular Arithmetizations right now are rank one constraint system, algebraic intermediate representation, as well as Planck. So on a high level, what we desire from an Arithmetization is three features. The first is support for lookup arguments. And this comes in useful when we want to offload very expensive computation to the preprocessing or offline phase. So instead of constraining like bit shifts or hash functions inside the circuit, we pre compute them outside the circuit and simply look them up during proof generation time. And this is what makes ZKE EVMs feasible today.
00:05:44.260 - 00:06:36.400, Speaker A: The second desirable feature is support for high degree constraints. So for example, Plonky Two has the fastest poseidon implementation that we know of. And this is because it squeezes the whole poseidon function into one single constraint, a very high degree constraint. And lastly, we want Arithmetizations to be compatible with folding schemes. And we'll talk more about this in the second part of this talk. So these three features that I've listed are basically what everyone's aiming for right now. But there's still a lot of unknowns in this optimization space.
00:06:36.400 - 00:07:57.952, Speaker A: For example, we can achieve lots of useful functionality with just low degree gates. And if we can get rid of one of these requirements, then basically we can optimize for a specific problem a lot better. So one of the latest advancements, I think it came out maybe a few weeks ago, is something that claims to unify all these three Arithmetizations and it's called the Customizable Constraint system. So yeah, it claims to basically be able to express any Arithmetization with no overhead. And it also claims to achieve a faster prover for Air, specifically a faster prover than the yeah, Air was made famous by Starks. And so CCS is now claiming that they have a foster prover for it. And something's interesting about CCS is that it takes a unifying approach.
00:07:57.952 - 00:09:17.588, Speaker A: So it wants to make a universal Arithmetization. And this is as opposed to another paradigm of proof composition known as commit and proof. So some of you may have heard of Legosnarc. Basically, instead of trying to unify different Arithmetizations, legosnarc says that some arithmetizations are better suited for some computations. And if we're trying to express a heterogeneous computation, we might as well use these bespoke arithmetizations and then later on combine them by committing to the same witness, by proving that we use the same witness across all the proof systems. And it's still not clear to me which approach is more efficient, but my intuition is that there is value in thinking about composing heterogeneous proof systems. There's also been a recent line of work in optimal lookup arguments and we've basically gotten to the point where the overhead of a lookup argument is independent of the lookup table size.
00:09:17.588 - 00:10:41.346, Speaker A: And it is a function only of the number of lookup inputs. And this is good for most use cases such as range constraints, where the number of possible legal values is far greater than the number of values we actually need to look up. So around the same time as CCS, this proof system called Protestar came out and they managed to introduce a lookup argument that was at the same time compatible with a folding scheme. Sorry for the alignment of the slides. So on the next level of the stack, we have sort of the information theoretic level where we express the, where we express our computation as a constraint satisfaction problem and we actually engage in a protocol to probabilistically check that it's satisfied. So the trend here of note is the return of the sumcheck. So the sumcheck protocol is very old relative in this industry.
00:10:41.346 - 00:11:30.940, Speaker A: It's considered really old. It's from 1990 and it has many desirable features such as avoiding expensive operations like Phosphoria transforms. It's also very compatible with folding schemes because it incurs very little overhead. In particular, it doesn't produce error terms in the group. And here's another tweet from Ariel. I think I have another tweet from Ariel left in this presentation. He just tweets really cutting edge research and we can get it for free.
00:11:30.940 - 00:12:55.760, Speaker A: So the next step is to instantiate this information theoretic model with a cryptographic commitment and to make it efficient and secure in the real world. So I think something to note here is I want to highlight the KZG commitment scheme. This is used in Dunk Sharding, which is Ethereum's way of providing data availability guarantees. So I highlight this as an example of how different components in the proof system stack can be separated and are interesting in and of themselves. And the point of framing proof systems in a stack is to show that they are modular and that we can mix and match different components according to our system's requirements. And finally we get to our proof systems. So I think it's less trendy now actually to present a whole full stack proof system, but basically these are the popular proof systems that are deployed today.
00:12:55.760 - 00:14:30.600, Speaker A: So now that we have a high level overview of proof systems and we can sort of classify the popular protocols of today, let's go in and see some of the recent advancements in efficiency that have been made possible. And as I said before, these advancements are possible because we went into different layers of the proof system stack and applied basically recursive techniques there. So these advancements lead us to the primitive called proof carrying data. And this is a very powerful primitive because it allows mutually distrusting parties to collaborate and to prove the validity of a whole chain of computation. And basically it enforces a local compliance predicate on a global level. And how we get to proof carrying data is by instantiating recursive proof composition at various levels of the proof system stack. So this is sort of a tech tree of techniques for recursive proof composition that lead us to the desired primitive proof carrying data.
00:14:30.600 - 00:15:47.946, Speaker A: And we're going to go through them pretty much in chronological order. So the classic instantiation of proof carrying data is actually a special case called incrementally verifiable computation. And how you can think of this is a Dag, is to a linked list as proof carrying data is to IVC. So, for example, Plonky Two uses full recursive proof composition in which each recursive step fully verifies the proof generated by the previous step. And so we embed the full verifier of the protocol in each recursive step. So Plonky Two is highly performant and the reason is that the verifier is sublinear in the circuit size. However, this sort of classic method is restrictive because there are not many systems like Plonkey Two, and there may be some other systems that we wish to use but that don't have succinct verifiers.
00:15:47.946 - 00:17:01.862, Speaker A: So this motivated us to weaken the requirements on the proof system such that we don't need a strictly succinct verifier, and instead we only need a succinct accumulation verifier. What this means is that at each recursive step we defer the expensive part of the verification to what's known as an accumulator. And instead of checking it in the recursive step, we simply accumulate all the expensive computations and check it at one go at the very end. So what atomic accumulation achieves is basically a weakening of requirements on our proof system. And this is what allowed Halo Two to achieve IVC without a trusted setup. And you can see this as an amortization of costs because when I say expensive verification, I mean linear time. So this decider at the very end performs a linear time check.
00:17:01.862 - 00:18:21.398, Speaker A: And this is only worth it if you amortize it across a huge batch of recursive steps. So the latest development in IVC is something called split accumulation, also known as folding schemes. So split accumulation further reduces the recursive threshold by making it constant size. How it does this is it removes the requirement for even a succinct accumulator. Instead, it only needs a succinct representation of the witness, of the accumulator witness. So concretely, for example, if your witness is some R One CS assignment, as long as you're able to commit to it using a constant size group element, for example, using a Patterson commitment, then this is eligible for split accumulation. So what happens is that instead of accumulating the whole proof, we now accumulate only the instance, only the commitment to the witness.
00:18:21.398 - 00:19:04.910, Speaker A: And the split accumulation verifier is tiny. It only concerns itself with the constant size commitments to the witness. Yeah, so what split accumulation has done is it's given us recursion with a highly efficient prover. And this is what's going to make things like Lurk possible. This is what's going to make things like L1 ZKE EVM possible. Any application where you need to produce proofs at almost the same rate as the computation. You want something like split accumulation.
00:19:04.910 - 00:20:29.464, Speaker A: So yeah, basically the gains in efficiency of ZK proofs have made a whole class of applications possible that were not before. So now folding schemes are spiritually very similar to split accumulation, but they differ in one pretty technical way. So instead of fold, they basically fold at an earlier stage. Instead of folding at the poly commitment claim, they fold directly at the relation. So what this means is that the prover does not need to do this processing step of committing of computing a quotient polynomial or of computing any kind of proof about their relation. Instead they can directly take the relation, be it an R, one, CS or Plonkish relation, and just accumulate instances of it. And what this lets us do is save on expensive computations like FFTs.
00:20:29.464 - 00:21:33.660, Speaker A: So in folding schemes, the proverb only needs to commit to their witness and nothing else. So the drawback of folding schemes is that first of all, it's not zero knowledge. And what I mean by that is, at each recursive step the prover basically needs to receive the whole witness. Whereas if you compare it to atomic accumulation, at each recursive step the prover only needs the proof. So therefore the folding scheme is more suitable for a single prover computing a large number of recursive steps. And the second drawback is that because we're folding so early, we need to make sure that we're always folding the exact same relation. And this removes a lot of flexibility.
00:21:33.660 - 00:22:42.638, Speaker A: Whereas again, if you compare to say, split accumulation, you're just folding evaluation claims. So anything that looks like a commitment evaluation point and a value can be folded, can be accumulated. So in summary, folding schemes are pretty much the trendiest thing in zero knowledge land right now. I think it's slightly boring by now, but basically all the papers that have come out recently are about folding schemes. So it's still worth actually understanding the features of these protocols and what they're useful for and what they're not useful for. So to summarize, folding can happen at pretty much any level of the proof system stack. So the earlier you fold, the less work your prover has to do.
00:22:42.638 - 00:24:06.636, Speaker A: But that comes with drawbacks such as losing flexibility and losing zero knowledge. Whereas if you fold later on, your proverb needs to at each step, compute maybe some FFTs. But keep in mind also that there's a parallel effort to accelerate these very common operations in hardware. So if FFTs and multiscaler multiplications become cheap enough, then we might actually prefer to progress down the proving stack a bit further before folding, if it can preserve attractive properties like zero knowledge that make it easier for collaborative proving. Sort of. The last innovation I want to mention is something called non uniform IVC. So whereas in IVC we have the same computation at each recursive step, non uniform IVC allows us to pick one out of L predefined circuits.
00:24:06.636 - 00:25:55.260, Speaker A: So this is useful, for example, in the context of virtual machines, you can imagine these L circuits as the L opcodes of the virtual machine. So the idea here is that you don't need the same huge circuit expressing the whole virtual machine logic in every step, so instead you can just pick the specific opcode at each step yeah, and save on recursive overhead. You do have to pay in the sense that you need to maintain L accumulators, but if you recall that these accumulators are constant sized and they're very succinct commitments, then that seems like a very good trade off. So now we'll end off with actually a case study that I think is a great application of proof carrying data. So this is the Lurk programming language, so they've designed it from first principles to compile to a proof carrying data backend. And how they do this is they conceive of their state machine as a chain they conceive of their state machine as a chain of recursive steps. So basically, each operation is one step.
00:25:55.260 - 00:27:58.560, Speaker A: And they don't attempt to process all N operations, let's say, in one shot. Instead, they break it up into what they call continuations. So given this tree of operations, they process basically each subtree one by one, and each time they sort of save the unprocessed part on the continuation stack, the continuation dag. So right now Lurk actually repeats the same state machine at each step. So, yeah, they're doing Uniform IVC currently, but as we saw earlier, this can be made more efficient with non Uniform IVC, where instead of paying for the whole state machine, you only need to pay for a cheap binary operation in this case. So, yeah, as we go through this tree of operations, you can see how when we end up at a self evaluating value, we can replace or we can simplify the continuation dag and basically process this step by step. And when we get to the outermost note on the continuation dag, this is when we terminate the computation and return the final proof in the PCD.
00:27:58.560 - 00:28:04.740, Speaker A: Yeah, that's all I had. Thank you.
