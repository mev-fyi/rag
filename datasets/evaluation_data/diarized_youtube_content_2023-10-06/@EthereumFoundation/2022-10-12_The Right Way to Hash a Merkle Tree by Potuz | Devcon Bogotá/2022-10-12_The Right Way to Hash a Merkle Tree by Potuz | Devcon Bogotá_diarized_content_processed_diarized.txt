00:00:12.970 - 00:00:45.318, Speaker A: Let me tell you what Shah 256 is. This shah, the hashing library, is something that generically just takes any message, any an arbitrary length message, as in a sequence of bits of any length, and it just spits out of that 32 bytes. The digest of this thing is 256 bits. The procedure to go through those consists of three parts. So I'll just go over the three parts. The first part is breaking your message into multiples of 64 bytes. So those are the chunks 512 bits.
00:00:45.318 - 00:01:22.482, Speaker A: The second part consists on scheduling words, computing a bunch of double words, which are four bytes each. And the third part consists on performing a bunch of rounds, which is just the function that actually does the hashing itself, the function that is not invertible. Here's just a brief description of this slide I just stole from somewhere in the Internet. But I want to go in some detail in each one of the three parts. So let's start with the easiest one, which is scheduling words. So, scheduling words, as you see here, you're given a message. Let's suppose that we've already broken the message in pieces.
00:01:22.482 - 00:01:58.466, Speaker A: Each one of them is 64 bytes. So the first thing you do is you compute 48 double words, four bytes each. You need 64 in total. The first 16 are your message, are the 64 bytes that you're giving. These are 16 words. With those 16 words, you can compute the next 16 words, and with those 16 words that you just computed, compute the next ones and the next one, and you computed the 64 that you needed. And the important thing that you need to remember from this slide is that to compute those scheduled words, the only thing that you need is the previous word, nothing else.
00:01:58.466 - 00:02:44.986, Speaker A: That means that when you're scheduling words, well, I think the consensus people are starting to come in anyways. I've already started, guys. So when you're scheduling words, the important thing is that you don't need to know the previous state of the hashing. You only need to know what were the previous scheduled words in particular. It only depends on the chunk that you're hashing now, and it doesn't depend on the other chunks that you're going to hash. So if your message consists of 10,000 chunks, you can compute the scheduled words for the 10,000 chunks without caring about how you would hash each one of them and without caring about the rounds part. So scheduling words only requires the previously scheduled words.
00:02:44.986 - 00:03:30.526, Speaker A: So the diagram there is taken out of an intel paper describing what were two new instructions that do this scheduling of four words at a time with only two instructions. This can be done now on modern cpus, and this is just a sketch, how diagrammatically you compute four words at a time, but it's irrelevant. The method that you use to compute those scheduled words, what I want you to remember is that to compute them, you need to know the previous words and nothing else. Rounds, we don't care about what rounds is. We only care that it's a function that is not invertible, that is hard, that is computationally hard to invert. But the important thing that we need to remember is this. We were given the message, we broke it into pieces of 64 bytes that I haven't told you how.
00:03:30.526 - 00:04:18.186, Speaker A: We computed those scheduled words that we need. And then what we do is we pass an incoming digest. If it's the very first chunk that you're hashing, we pass a constant digest that the method has. If it's the third chunk, you're going to pass the hash that the second chunk produced. The point is that you have a status, which is your current hash, and you pass it through this function that takes this status, the hash, the 32 bytes, it takes one of the scheduled words that you computed, and it takes another constant word that the protocol has. So it takes this data and it produces for you a new hash. So what do you need to remember from this is that you need to have computed at least that scheduled word before passing through the rounds.
00:04:18.186 - 00:04:48.850, Speaker A: And you cannot do this in parallel. To pass through the round, you need to have computed already the hash before. So this is something that you cannot do in parallel. Okay, so the padding block. So this is the first part of the three process, which is breaking the message into multiple 64 bytes. How do you do this? Well, you just break your message into multiple of 64 bytes. You add one bit at the very end of the message just to signal this message has ended.
00:04:48.850 - 00:05:32.070, Speaker A: So here the message is less than 64 bytes, is 24 bits. These three bytes there, a, b, and c. This one there is showing that we added that extra bit to show the message has finished. And then you pad with zero bits up to a multiple of 64 bytes minus eight, because you're going to use the last eight bytes, or 64 bits, to encode the length of the whole message, as is there in binary. That's the number 24, which is the actual length of this message. So this is the procedure to pad your message, which was arbitrary length, into a multiple of 64 bytes. Vectorization, which is the main topic of this talk, is vectorization.
00:05:32.070 - 00:06:40.326, Speaker A: So the first thing to notice is that you're not going to beat any of the hashers out there. Every implementation that I've reviewed before I got into this, and I'm not an expert at all on this, I come from a completely different subject, but every implementation I've seen is equivalent to Intel's original white paper on this, and it's equivalent to what openssl, for example. Does all of them implement the following? You can compute your scheduled words, as I told you, without knowing what the previously hashed, what the previous, what the current status of the hashing is. So all of them use vector instructions to compute several words at a time. If your cpu supports 128 bits registers like this one, and almost every cpu now does, then you're going to compute four words at a time, four double words at a time. You start with your 16 double words, which is your message, and you can put those 16 double words in only four registers. If your computer supports AVX two, it can do arithmetic and registers that are 256 bits.
00:06:40.326 - 00:07:39.546, Speaker A: Then you're going to compute eight, you're going to use only two registers, and you're going to compute eight words at a time. If your computer supports AVX 512, you're going to compute all of them, the 16 of them at a time. And, well, AVX 1024 does not exist yet, but it's already in the books. There's two options into computing this. Modern implementations either do this, this vectorization, or if your cpu implements cryptographic extensions like what was there in the picture, then they will use 128 bit registers, regardless if your computer has larger registers, because cryptographic extensions can compute four words at a time much faster than vectorized computations. But the point I want to make is that this vectorization is there in every proverbial implementation. This is since Intel's white paper also computing the words like this is useful because since you can compute the words and at the same time pass through some rounds.
00:07:39.546 - 00:08:36.942, Speaker A: So let's say that you computed the fifth word, then you can pass up to the fifth round. Then you can mix scalar operations with vector operations, and the CPU will perform this in parallel. The CPU has several ports and can take different types of operations at the same time, so you can be computing the fifth round. That is a computation that it has to be done on the scalar part of the cpu because it cannot be in parallel. And at the same time you're computing the 6th word in parallel on vector registers. Okay, so I've covered what is a typical implementation of hashing, and this is the thing to remember from the whole part, from the whole third parts of the talk is that the hasher signature? Is this in either language? It's something like this. It's something that takes an arbitrary length byte slice and it gives you back 32 bytes, which is the hash.
00:08:36.942 - 00:08:58.398, Speaker A: So it takes an arbitrary length message and it gives you the digest. And this is something that you're not going to beat. You're not going to implement this better than OpenSSFL. No one's going to do. You perhaps can do it for one cpu. You're not going to write an implementation faster than what is already there. However, we use Hashing in a very restrictive scenario.
00:08:58.398 - 00:09:28.402, Speaker A: We use hashing to hash merkel trees. And merkel trees are not arbitrary length. Merkel trees are, in the case of the consensus layer, for example, where the nodes are 32 bytes, they're something like this. Each one of these nodes represent 32 bytes, and each parent node has only two children. And these two children are hashed together. So the two children are 64 bytes in concatenated. You hash them and you get the hash of the part.
00:09:28.402 - 00:09:52.810, Speaker A: You get what goes in the part. So this is what we hash typically, and we observe two things. Well, actually the execution layer has something completely different, but still the same technique is going to apply. What we observe immediately is the following thing. First of all, we're not hashing arbitrary length. We're hashing every single time, 64 bytes. And second of all, we can hash this in parallel.
00:09:52.810 - 00:10:18.126, Speaker A: You can hash the blue one by knowing the entire left subtree and hash the yellow one completely in parallel. In the other side. Of course you can do this in parallel. If you have a cpu that, if you have two cpus, you can just use two threads to hash this. And this is exploited in the consensus layer. I don't know if any other besides lighthouse uses this, but I want to use this. I want to talk about a different kind of parallelization.
00:10:18.126 - 00:10:49.830, Speaker A: You can parallelize this in different threads, but the point I want to make is that you can do this by using vector instructions. This is the typical, this is the one that is in the specs, in the consensus layer specs. This is the implementation. I think this is Vitalik's implementation. This is a flat array approach to having Merkel tree. I highlighted the line where you're computing the hash of the parent by hashing the two children that are concatenated. The top one is different memory layout.
00:10:49.830 - 00:11:09.726, Speaker A: This is Proto Lambda's implementation on remarkable. The point is that it's the same kind of hashing. You take the two children and you hash them, and you hash two blocks at a time. This is fairly, fairly inefficient. This is a go implementation. This is also Jim's implementation. This is a production implementation.
00:11:09.726 - 00:11:34.362, Speaker A: And again, the same thing. It's slightly more complicated because it takes into account other things. But the highlighted line is the point is the point where you're hashing and you hash one node as the hash of the two children. And you do this on a loop for each pair of children. You hash one, you call the hasher and you get one hash. Okay? So I want to tell you what is the right way of hashing a miracle tree. And it's fairly, fairly simple.
00:11:34.362 - 00:12:10.998, Speaker A: So there are two things that we want to exploit. We want to exploit the fact that our hash is exactly 64 bytes. That's the thing that we want to hash. So remember, how is it that we padded our message into a multiple of 64 bytes? Well, our message is already 64 bytes, but unfortunately we need to add one bit to tell the message ascended. So that's the first bit there. The problem is that since we added that bit, now we need an entire block. And the entire block is all made of zeros except this little bit one here.
00:12:10.998 - 00:12:35.242, Speaker A: And that bit one here is saying the message was 64 bytes. So that bit there is the 512 bits that the message was. But the point is that this block is known. Is known. Before we started hashing, the whole block is known. And in order to compute the scheduled words, we only need to know the message. We don't need to know the hash of the previous block.
00:12:35.242 - 00:13:04.662, Speaker A: So whenever we hash 64 bytes, we can already compute the scheduled words for this entire block. And we can do this. Now we have them so we can hard code it in the hasher itself. This is something that the bitcoin community learned and they have it in their standard node. I don't know, how is it called? I think it's bitcoin core, the client, and it has it there. And no ethereum client used this. So I was surprised when I found this out.
00:13:04.662 - 00:13:23.360, Speaker A: So we can steal this from the bitcoin community. And indeed so prism uses this library. Lodestar also implemented this. And just by implementing this, you get at least 20% gain on your hashing speed. Just this. No changes on your code. Just include this hardcover word.
00:13:23.360 - 00:13:51.030, Speaker A: The second one is vectorization itself. And vectorization. This is also something that is implemented already. If you have several messages that you need to hash at the same time, there are libraries to do this. Intel has a very good library, and all I need is just change the signature of that library. But the point is that you can be hashing several buffers at the same time. So this is a way of encoding the words.
00:13:51.030 - 00:14:32.866, Speaker A: Again, you're going to take, this is the message that you want to hash. So these are the 16 words corresponding to this node here, the zero node. Then you're going to have 16 words corresponding to this one that you want to hash, 16 words for these two, and 16 words for these two. And what you're going to do is collect the first double word from this one, the first double word from this one, the first double word from this one, and the first double word from this one into one register. That's if you have registers that are 128 bits. If you had AVX twelve, you will collect eight of them at the same time. And again, if you're AVX 512, you can do 16 at a time.
00:14:32.866 - 00:15:13.038, Speaker A: The point is that you can hash in one path, you can hash up to 16 blocks on AVX 512, and you can do the same thing for the words themselves. So the digest consists of eight double words. If you have registers that are 128 bits, you can put four of them in one register, and then you use eight such registers for the eight words of the digest, and you can hash four blocks at a time. So again, this is a library that exists. So all I'm selling you is something that already exists. It's not that we're rolling out our own crypto. I need to brag about something.
00:15:13.038 - 00:15:36.290, Speaker A: I was told as a student that if you're giving a talk, you need to brag about something that you did yourself. And I guess this is something I did. Of course, the assembly for intel and AMD intel's libraries. I'm not going to pit it. Unfortunately, intel, or reasonably intel, does not produce assembly for arm. And there's something interesting here. Arm assembly.
00:15:36.290 - 00:16:17.814, Speaker A: I told you that every library out there uses vectorization to compute the scheduled work. That's not true on arm. On arm, OpensSl had a library using neon instructions, vector instructions to compute the hashing, and it turned out to be slower than scalar. So most libraries for arm do not use scalar instructions. However, if you're going to hash a Merkel tree, you can hash several blocks at the same time, and that's very much, much faster than hashing them on scalar. So the pipelining for arma, I think, is purely mine. Let's say that you want to use this library and you want to implement it.
00:16:17.814 - 00:16:40.438, Speaker A: So there are changes to your code. So the changes the following since let me go back a few slides. Perhaps it did had a pointer. I'm just technology at first. Okay, anyways, let's see. So as I told you, I'm going to hash with this kind of registers. We can hash.
00:16:40.438 - 00:17:31.994, Speaker A: Now I have a pointer, of course, since with this kind of register I can hash four blocks at a time. That means that I can compute this entire layer in one path of the hasher. So instead of passing on a loop and hash this two, hash this two, hash this two, and so forth, what I'll do is, and this is a requirement, you cannot use protos implementation that has pointers everywhere and the data can be anywhere. If you want to use this library, then you need to have at the very least, this entire layer consecutively in memory, and this entire layer consecutively in memory, and so forth. Vitalik's flat array would work fantastic for this thing. If I were to implement this, I would just put everything on one array. The point is that what you're going to pass to the hasher is a pointer to this or this whole slide, whatever equivalent signature for this is.
00:17:31.994 - 00:17:56.242, Speaker A: And the hasher is going to give you back all of this at the same time. So this is something that you need to change the hasher signature for. This has this form. So in go, this takes an arbitrary byte slice. So this is the layer that you want to hash, and it gives you back a slice of hashes of 32 bytes. It hashes all of them at the same time. I think I might have gotten this correctly in rust.
00:17:56.242 - 00:18:19.034, Speaker A: After several iterations. This would be in python. We have two libraries. We have one in goassembly, and we have one in user assembly with c bindings. If you are going to use that, let me know. Because crypto extensions on arm is still not implemented. It's going to take like ten minutes to add this.
00:18:19.034 - 00:18:29.680, Speaker A: But this is the signature that the library is going to use if you're using a c binding. All right, so that's all I want to tell you. And that's it.
00:18:32.130 - 00:19:01.786, Speaker B: Okay, sure, I'll just repeat that for the sake of the recording. I was just asking if the ability to process multiple different hashes in parallel was already implemented in libraries. That was the question to the answer. So there's two things that you have here, right? One is to pre compute the padding block, and then the second is the ability to in parallel, process multiple different hash values and then produce multiple different hash values, right?
00:19:01.808 - 00:19:02.186, Speaker A: Correct.
00:19:02.288 - 00:19:11.466, Speaker B: And so there are already implementations that do multiple different hash values at once, and you just modified them to deal with the pading block, is that right?
00:19:11.568 - 00:19:40.594, Speaker A: Yes. So there are two modifications here. One is the thing that you're going to use, the padding block. The padding block has these constants hard coded. And then there's other modification, which is the fact that you are expected to get a list of 64 bytes chunks, and then you pipeline this. So what this library does is it grabs all of the blocks consecutively in memory. It gets a matrix on the vector registers.
00:19:40.594 - 00:19:56.746, Speaker A: It transposes this matrix, and now you have on all of your registers the different messages. So then you can use Intel's machinery to hash those messages in parallel. You output these hashes and then you.
00:19:56.768 - 00:20:03.646, Speaker B: Just loop it have. So the result is basically it's in goassembly what you have.
00:20:03.668 - 00:20:04.718, Speaker A: Right, sorry.
00:20:04.884 - 00:20:07.390, Speaker B: Your implementation is in go assembly.
00:20:07.810 - 00:20:25.150, Speaker A: Yeah. So the original implementation was just purely assembly. This is there, it has C Bindings, but the CGO overhead is horrible. So it ended up being slower than using the go implementation, the standard library, in go. So we needed to write a go assembly library to use ourselves.
00:20:25.310 - 00:20:25.634, Speaker B: Yeah.
00:20:25.672 - 00:20:26.774, Speaker A: Okay, cool. Well done.
00:20:26.812 - 00:20:28.210, Speaker B: Very good and very impressive.
00:20:28.370 - 00:20:29.080, Speaker A: Thanks.
00:20:30.970 - 00:20:46.780, Speaker C: So I was curious. You've shown that there are always optimizations you have to do to get around the general purpose nature of these general purpose hash functions. Do you think it would be possible to design a hash function that was like a special hash function, only for like.
00:20:48.430 - 00:21:08.100, Speaker A: Oh, that's a good question. I'm not sure if you can do better than the current implementations. Like you can take the sponge type implementations, shattery and company, and you can try to adapt this for this. I don't know. I think your question is completely open. It's a very good question. I think we should think about this.
00:21:08.100 - 00:21:15.426, Speaker A: So if I understood your question, ask can there be a method that is designed for merkel trees instead of like.
00:21:15.448 - 00:21:29.942, Speaker B: A generic one in your implementation? Does it so say there's some really big Merkel trees in the beacon state, right? Yeah. Is it the job of the implementation to kind of split that merkel tree up into smaller subtrees that fit the size of your cpu?
00:21:30.086 - 00:21:53.822, Speaker A: No. So you're saying if you have this large tree, if it's the job of the implementation in splitting into smaller trees. No, this is completely orthogonal to that. So what you guys are doing in lighthouse is splitting this into smaller trees, and you send this on different threads to compute them in parallel. This is completely different. You just pass the entire slide. So big trees is the big game for this.
00:21:53.822 - 00:22:18.418, Speaker A: You're going to be at least hashing four times faster than the standard library. You just pass the entire slice like all of the slides of the bottom slice. And what this thing is going to do is not split into subtrees. What this thing is going to do is grab as many blocks as possible that it can fit in your registers. And then just go to the next chunk and the next chunk and the next chunk like this, so it doesn't split in subtrees.
00:22:18.514 - 00:22:24.250, Speaker B: Okay. So you faded all of the lathes and then it produces the intermediate layer.
00:22:25.870 - 00:22:29.658, Speaker A: And then you just feed it the next layer entirely and it produces the next one.
00:22:29.744 - 00:22:31.180, Speaker B: Oh, yeah, I got you.
00:22:31.550 - 00:23:00.870, Speaker A: So you're thinking in the beacon state for the state. This is incredibly fast. But this is not how we hash the state because we have it in cachet, typically, and you just have a few nodes that you're changing. So when you hash the dirty trees, the dirty leaves, then, well, there's two things that happen. So sometimes you have several of them that are consecutive. And then you can be smart and pass this consecutive layer to this hash. Or you can just use whatever you're using now.
00:23:00.870 - 00:23:09.510, Speaker A: Just hash two blocks at a time. You're not going to get the vectorization impact, but you're going to get at least the 20% of the hard point. The padding block.
00:23:10.010 - 00:23:16.034, Speaker B: Yeah. Okay, cool. So I guess there's maybe like an argument for maybe it's quite useful for small trees. You don't get a lot from caching.
00:23:16.082 - 00:23:33.260, Speaker A: Yeah, it's not really useful for small trees. You only get the 20% of the padding block. It's very, very useful on large trees. And here, large means more than eight blocks. So anything that has depth more than two, this is already four times faster close.
