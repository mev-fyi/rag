00:00:13.290 - 00:01:02.058, Speaker A: Okay. Yeah, hi, I'm Daniel. I'm in the compiler team for over four years now and yeah, originally the plan was for this talk to be held by Chris, who couldn't make it to Bogota, so I have to improvise a bit. I brought a lot of code snippets which will explain what we have currently in the compiler, currently the state of the language and where we are headed with it. Yeah, I guess if you are familiar with solidity, which I hope most of you are, you probably know immutables by now. It's been around for a while and is a very highly appreciated feature from our impression, which yeah, I still will quickly explain how they work. The owner variable here looks like a state variable, but it's not a really proper state variable in storage, but a mutable variable which can only be assigned to once in the constructor and then can use this many times as one once read only in the runtime code of the contract.
00:01:02.058 - 00:02:13.170, Speaker A: But there the access is very cheap, it doesn't require NS load, but it's basically inlined in the code as if it was a literal. So yeah, you probably know this, and people liked it a lot. And an apparent and very obvious extension to ask for is why do we only support that for value types? That's the restriction we have so far, that it's only integers, only addresses, but not arrays of things, for example. So this was, for example, is taken from a GitHub issue that I think opens up and opened with us to ask for an array of immutables which would then be initialized by index accessing and otherwise work just as one is used to with immutables, and we are working towards that end. But there are some issues with that. We have these assignments here where I randomly assign into elements of this array, and in general I cannot check whether this is really assigned only once. I mean, the question is, is this still an immutable thing in that sense? Also, if I start having arrays as immutables, I will want to have local references to them which I could then reassign.
00:02:13.170 - 00:03:01.842, Speaker A: So if I keep the name immutable, I just changed what this points to. So this is really, it doesn't look immutable. Immutable doesn't fit this concept anymore properly. So let me explain a bit further how immutables actually work in the constructor owner is actually a position in memory. So we store whatever you write to this variable in memory, and then when the actual runtime code of the contract you're about to deploy is copied to memory to be returned by the constructor. We fill in from this memory location into the bytecode the value that is there which in the end results in the runtime code to actually have it as a literal in the bytecode. So this variable in the constructor actually is a memory variable in runtime.
00:03:01.842 - 00:03:33.306, Speaker A: It's actually something that lives in code but filling in literal values in some. So basically the push argument into the bytecode won't work anymore for dynamic types. If we have statically sized arrays we could still do that and we probably will for efficiency reasons. But at least for dynamic types to go full wave we cannot do that anymore because we don't know the length of the thing. So we can't reserve space in bytecode for that. So instead we need to rely on code copy. And yeah, I already mentioned we will probably want to pass the immutables around by reference.
00:03:33.306 - 00:03:57.446, Speaker A: We will want to slice them, which if you think about all of that together makes immutables not an annotation for a state variable anymore, but it will become a proper whole data location. So this will probably look like this in the end. As I said, the variable in the end will live in code, so code and the opcode that will access it is code copy. So code is a natural name for these things.
00:03:57.548 - 00:03:59.574, Speaker B: So here I have a data variable.
00:03:59.622 - 00:04:58.694, Speaker A: A bytes and dynamic array in code, which I can then in the constructor just treat like any old memory variable, assign to it, modify it freely. We can drop the requirement that it's only written to once in the constructor because that was always some artificial requirement. So if we actually call it what it is, a code variable that will actually be inserted or used as in the code, in the end we can freely modify it and then in the runtime code it basically behaves like a call data reference, a read only reference, only that it doesn't come from call data but from code. So I can slice it, have local references to it and pass it around to functions, all with very little cost. This will be a bit tricky to type check in the end because yeah, in the constructor it is a memory variable. In the runtime code it bis differently. So if the constructor calls functions you need to type check everything twice.
00:04:58.694 - 00:05:47.210, Speaker A: But we have different calls for refractoring the type checker to actually do that as well. So we will do that. And there's still some considerations of gas. It would be very handy if they had a code load upcode similar to a call data opcode to actually read single words from code, but we don't, so we will, whenever possible probably still use the same mechanism method mutables use now to actually fill it into a push argument in bytecode. If not, we will probably code copy either in an entire memory region, or if you just need a snippet of a dynamic array code copy to stretch space and m load from there. This will still be significantly cheaper than anything in storage. Okay, yeah, that was the first topic I wanted to talk about.
00:05:47.210 - 00:07:31.334, Speaker A: The second thing that you may have noticed in the recent versions is our move to allow user defined value types. Goto test case for this concept was always fixed point types, which we decided to not implement as a first citizen type in the solid type system, but to build the infrastructure to define it as a user defined type. Yeah, I'm assuming that you're familiar with those, so I won't go into that much more detail in the basic concept, which what we recently added, or more recently added is this global keyword you see in the first using statement, which means that currently a using statement is only locally effective, whereas a using statement with a global at the end will only be allowed in the very same source file in which you defined the type, but will have that the effect that wherever the type is available, the functions you bound to the type will also be available. So this is what you can do then in the end, practically to write type libraries where you're in that library, define the type, equip it with some functions, and then anyone importing the type only can still use all the functions on that. Still. Of course, one major inconvenience with that for defining, for example, fixed point times that are actually nice and usable is that we have only functions that can be defined and equipped to the type, but the using statements, which is what we will soon change. So this is open PRs currently that are under review, which will probably in finite amount of time we actually merge and come through, which will allow you to define operators and literals in the first versions in a very limited setting.
00:07:31.334 - 00:08:48.630, Speaker A: So the addition, for example, can only be between two expressions of the type that you define, such that it's not getting messy and you no longer know what function will actually be executed if you have an operator expression. But it will still already be very useful, I hope, in making these user defined types more convenient to use literal. Similarly, we will introduce a way to define functions that can take up as argument a mantissa in that sense, and an exponent which is basically here, for example. For the example below, it would be 115 and the exponent is ten to the power of minus two, which will allow to then deal with, for example a fixed point setting rather gracefully and have literals without special code constructs. Eventually we will have a proper rational literal type instead of splitting it up in a mantis and an exponent here I think at least. But for now this is the fastest path to get this working. But yeah, so far these user defined types we have only are value types.
00:08:48.630 - 00:09:47.506, Speaker A: This might be equal cause to want to encapsulate and abstract dynamic types, arrays, structs and so on. And there's also of course this desire to have proper algebraic data types in general. So the more user defined data types we have, the easier it is to write a usable code. And yeah, there's questions about that, for example, where data locations would go. Currently data locations are associated with variables and not types, which makes that a hassle, which we will also probably change for independent reasons. But yeah, you may see where I'm going with this. If we have all these kinds of types and maybe also user defined containers and all that, it will no longer be feasible to write functions to cover all these types and separate functions for all these types.
00:09:47.506 - 00:10:57.582, Speaker A: So all this increases the need for having a construct of generics in the language, which has been long an idea to introduce to the language and which we're finally now making moves towards. But yeah, if we have that, the natural question is if even the built in types we have now can be user defined instead. Which leads me to another topic that we already started, which is a standard library. The idea here is to move whatever manually hard coded construct we have in the compiler now, or as many of them as possible, to actual implementation in solidity written user code, and then to ship that as a standard library that's integrated into the compiler. So yeah, this is what this is in the first instance going to look like, that you can enable the standard library version of the compiler, which in the end we will move towards and then import built in constructs from the standard library. And it will be possible to export the standard library as a set of files out of the compiler. And there you can then inspect what this function actually does.
00:10:57.582 - 00:12:23.354, Speaker A: For example, add mod is a simple example which can then actually be implemented in inline assembly with additional checks that works to reduce the footprint of the compiler and to make it easier for people to inspect what's actually happened and to extend it even. But the problem there again is this is of course very restricted in the scope in which we can move into the standard library, as long as our function facility are monomorphic, as long as we don't have generic functions that can handle multiple types. So the full potential of the standard library will only be unleashed if we have generics. Yeah, and the question is also whether we can not only move built in functions, but also built in types, which would end then, as you add earlier, as user defined types in the standard library. So the end goal with the standard library is reduce the language to a small, simple core language and have most of the current features that we have in the language defined in a standard library that is written in solidity, can be quickly iterated on, can be extended by the community, and can be audited, and so on and so forth. But to get to that point in full, we need generics. So the idea is to get inspired by a nice system of generics, which is based on a logically grounded type system.
00:12:23.354 - 00:13:31.600, Speaker A: We have product types, some types, function types, which if you're a mathematician or category theorist, the type system will probably form a cartesian closed category, which is how these things work, usually in probably designed generic languages like Haskell or like the trade system in rust, or the generic system in rust. And yeah, we will have polymorphic functions that can take arbitrary types and ad hoc polymorphism using type classes, or that's in rust, maybe if people are more familiar with that traits. And yeah, we will also, for this to work out, you kind of need to go all in on it and go the whole way. So we'll also have algebraic data types. And all of this will also be only really useful if you have compile time constant expression evaluation, because you will, in generic functions have constructed parameters that should be evaluated at compile time. So yeah, that's also something we will have, which is, as I said earlier, for the code data location, we will need to refactor the type checker to have VMAP flexible. For this, we will need that as well.
00:13:31.600 - 00:14:51.862, Speaker A: And yeah, maybe we will also have linear types, which is how rust sprout checker is constructed. But yeah, I will now have some examples with some made up syntax and some made up constructs. But I'm going to say this is all early research stage, so we have nothing fixed here yet. I'm just telling you where we're headed and what we'd want to do in the next however long it will take. So yeah, the go to, down to earth example of a use of generics is something like a resizable array, some container, which in this case is just an array that if you want to append something to it and the array is already full, you just reallocate what's twice the size, copy things over and yeah, otherwise you can just add the element to the end of the array and yeah, then one can also define an index access as a user defined operator, which will then have this thing actually work nicely. Yeah, one would need an allocation function and so on equipped with that, but one can then build a library to actually build these things without having special casing for each kind of base type. What I have here is a bit cheated though, because in reality things are more complicated, of course, because we have data locations.
00:14:51.862 - 00:15:50.490, Speaker A: Again, maybe I need to restrict that base type as by some trade mechanism to something that can live in memory, which may be a value type or another memory array, something like that. So in the end it will get complicated, but it will be worthwhile. And yeah, if one wants to take this upper notch and all the way, one can actually then think about even defining the most basic types in solidity itself. This would be the definition of the current representation of memory arrays in solidity, written as user defined type with the customly defined index access to it. So in the end, for any data type, a memory array is just one stack slot. We refer to memory arrays just as one stack slot. We index access by it, by fetching, I mean this stack slot points to some memory area where there is the size of the array and followed by the data.
00:15:50.490 - 00:16:53.690, Speaker A: That's the current layout at least. And yeah, in an index access we can just fetch the size, do some bounce checking and fetch whatever result we have at the offset that the index pointed to. And again, we could define then that index accesses work as expected. And yeah, a length field could be added to that, all defined in language, which has the advantage we had often the request to allow slicing for memory types, which we can't because the representation I'll show here doesn't allow it since we expect the size to be at the first memory offset, where the memory pointer that is the representation type of this points to is the size we can just slice away from the first element. For that to work, memory arrays would have to work different. Similarly, to call data types where you have offset and size on stack. It would be a huge effort for us currently to change the entire compiler to change the representation of memory types.
00:16:53.690 - 00:17:52.562, Speaker A: If we had things defined like this, it's minimal changes. I can just say now my memory array is defined as a tuple of stack slots. One of them is pointed to a data area, the other is the size and yeah, have a similar definition as before, slight changes, but a few source changes in our standard library would then be the same as changing the entire layout of memory types, which would be months of work while we maintained everything hard coded in the compiler. Of course it has all the disadvantages. If we actually keep this extremely generic like that, we will lose semantic information. This will actually make memory optimization harder because yeah, what the compiler sees is just a bunch of stack slots. There's no idea that that's actually memory areas we're talking about that are allocated may only be allocated temporarily and stuff like that.
00:17:52.562 - 00:19:07.802, Speaker A: So I'm not sure whether we will actually go this far. And if so, we will maybe probably do this in a compiler internal manner where we will still assume certain semantic properties about these functions without supporting similar optimizations that we can do if we know what these things do in random user code. But yeah, why did I say stack slot all the time? Maybe obvious, but just to mention the stack slot would be the one primitive type maybe apart from product types, and even the basic integer types size integer types we have right now can be defined generically just like the other cases we had. This will really reduce the footprint you can hear. Could also then distinguish between types that are checked arithmetic and unchecked arithmetic, by having very few functions that are generically written. But not to give to make you expect this to happen too soon, we are still in early design phase for generics. There's a lot of questions.
00:19:07.802 - 00:19:53.660, Speaker A: This is a very complex thing to do and a very dangerous thing to do, because all of this needs to be robotically well defined to not bite you in the back in the end. So, I mean, we will take some time to design this properly. And syntax is also a question. There's very much differing opinions on how the syntax for these kinds of things will end up being. I'm not that concerned by that now. I would first want to get the semantics right, but eventually we also need a good syntax for it. And yeah, what I just said, we need to decide what to do with this trade off between making the language really self defined in the very deepest sense, or to have some fixed functions which are fixed in the compiler, which means we can assume that semantics or there are compromises between that, but we'll see.
00:19:53.660 - 00:21:22.582, Speaker A: So yeah, to summarize what I was talking about and what I wasn't talking about, and we will still do, hopefully we will in the future try to allow more precomputation, either in the constructor by the code, data location, or in compile time by compile time constant expression evaluation, which is something a lot of people have asked for and which obviously makes it easier to write things in that you don't need. For example magic constants embedded in a contract or whatever, because you can compute them on the fly without it costing. And yeah, the huge topic for the future will be to make the language extensible and self defining by means of improving user defined data types, pushing the standard library, and making a move for generics. What we also of course can't just ignore is that we're still wasting a lot of memory. I mean, whoever has used memory in solidity will know that we basically don't free memory, which for a long time wasn't the main concern, but in the meantime is a very huge pain point for cost of contracts. There were several approaches we discussed so far for improving the situation there. A long time we wanted to deal with this on the UL level.
00:21:22.582 - 00:22:17.910, Speaker A: It turns out that may not be as simple. So maybe we will move actually to analyzing solidity, which has the properties right there. We shied away of doing that for solidity being the more complex thing to analyze, but maybe it's fine, we will see. And yeah, we will also of course try to move completely towards Bioaco generation, but we have some burdens there to overcome still, like the performance of the optimizer, better tooling support. You should still need to define good debugging data for the tooling to consume to actually make the experience as nice as with the legacy code generation, the details there would be that the tooling expects certain patterns to remain in the bytecode in the end, whereas the new optimization pipeline will mess them up by optimizing better. But tooling needs to understand that we need to output data for it, being able to understand that. But yeah, that's hopefully where we're headed.
00:22:17.910 - 00:23:20.826, Speaker A: And yeah, I'll close with that. If you want to give us any feedback, help us with designing generics or criticize what we are planning and saying we are crazy to do any of that. Reach out to us, that's the channel you will reach us at. And yeah, thank you. Yeah, it used to be the case that the compiler has two back end paths at the moment. So I mean, it used to be the case that solidity was directly translated to EVM bytecode and then the only optimizations that took place were on the bytecode level. For the past years we have moved away from that and have a different new code generation pipeline that translates solidity first into Yule, into an intermediate language, which preserves some structure and which allows for more complex optimization for inlining more analysis, and then only to translate Yule to EVM bytecode as a second step, which can reduce gas costs significantly in some cases.
00:23:20.826 - 00:23:32.830, Speaker A: In some cases it's the same as before. And yeah, the new pipeline that via IR is via the intermediate representation. So compilation via you. Thank you for the talk.
00:23:32.900 - 00:23:54.642, Speaker C: You mentioned generics. I'm wondering if you could speak to how you're planning on implementing that, whether you're going through monomorphization. Because I worry that the code contract size will balloon if you start doing the C plus plus style duplication of implementations, or if there's some uniform representation you can do la ocaml or Java.
00:23:54.786 - 00:24:28.894, Speaker A: I think there's not much you can do actually. I mean, the generics of C plus plus are different in the sense that they are analyzed differently and you only get errors on instantiations. But we will still need to instantiate and generate code for each specific case. But that's not worse than what you get now. What you get now is writing by hand for different types, different functions that would end up separately in bytecode. Nothing is worse than that. That's duplication in code and in bytecode.
00:24:28.894 - 00:24:34.606, Speaker A: If we have generics, we at least only have it in source, only in the bytecode.
00:24:34.798 - 00:24:53.610, Speaker B: So my question is related to that question regarding generics, but from a different aspect. So type system I understand, but once we get generics into solidity, wouldn't the developer have to focus on ten more things instead of focusing on writing business logic?
00:24:54.910 - 00:25:47.580, Speaker A: Good question. I would think that the down to earth go to smart contract writer will not bother with this. It's mainly something for us for defining a standard library and for people writing libraries to support smart contract developer person. So I mean, the language supporting generics and having generic types doesn't force you to use them, and it doesn't mean that anybody has to use them, but it will make the language and the evolution of the language much faster and more streamlined. We can in the future ask people if they propose a feature to just implement it in a standard library way and then standardize it in the end if it works out, which will have all the advantages, one can think about that. But for user code, for the in smart contract code, the difference is not that large. Probably.
00:25:49.710 - 00:25:51.914, Speaker B: Are you going to implement lambda functions.
00:25:51.962 - 00:26:24.870, Speaker A: Just with this equation? Definitely. Eventually I would say. I'm not sure whether, I mean, getting the basic type system going and all that will already take some time, but yeah, eventually this is of course something that will make things easier to read, easier to write, and are beautiful. As long as you don't want these things to capture variables, it's easy capturing even maybe something at some point we can also consider. But yeah, not the first thing we will do, but eventually.
00:26:25.770 - 00:26:45.120, Speaker B: Have you been thinking about integrating on chain computation and off chain computation in the one source file? This is of course very problematic, but in some moment in future we might need it. Not necessarily in solidity, but maybe you should try to think about that.
00:26:46.530 - 00:27:23.066, Speaker A: Yeah, on the solidity level, we don't yet interact that much with off chain computations, L2 stuff or whatever. But yeah, we are aware that we need to interact with that and support that where we can. I think you mentioned something about the performance of via AR. Did you mean how long it takes to actually use it? Yeah, I mean the compilation time. The compilation time is kind of like ten times or even worse in some context. Exactly. Anyone that's used via AR today has experienced that.
00:27:23.066 - 00:28:34.866, Speaker A: So how much of an improvement do you think we are going to see? I mean, so far the viar compilation pipeline has not been written with any performance considerations in mind at all. We've written it for correctness first and only now are starting to realize how bad that got and that we need to do something about performance there. So I could imagine that we can get quite a way. But yeah, it's hard to tell before actually doing it. Regarding generics, how much thought was put into the auditability for external code auditors? Will it improve the story, make it worse? More training? Can they forget stuff? I think it will actually improve things. I mean, we will be able, we have the standard library definitions of all the built in functionality which can be exported, which can be analyzed. I think at the point where we have generics going and a standard library going, we will actually at some point not promising that happening soon either, but at some point be able to define a form of semantics for the core language that remains, which can actually help formal revocation a lot and things like that.
00:28:34.866 - 00:28:41.570, Speaker A: So I think reducing the language core that is built in and hidden in the compiler is actually a good thing for formal vacation and auditing.
00:28:43.190 - 00:29:06.182, Speaker C: You mentioned that the data location is going to becoming part of the type instead of being associated with the variable. Does that mean that we are going to be able to start writing things like an in memory array of storage pointers or an in storage array of code data pointers? Because those are all well formed, but something like a storage array of call data pointers makes no sense.
00:29:06.236 - 00:29:06.502, Speaker A: Right.
00:29:06.556 - 00:29:10.190, Speaker C: So what does the well formedness look like? How does that sort of.
00:29:10.300 - 00:29:10.894, Speaker A: You see that?
00:29:10.932 - 00:29:11.950, Speaker C: Restricting that?
00:29:12.100 - 00:29:26.720, Speaker A: I mean, first part of the question. Yes, this is what this means. This is what will be allowed. Second part, of course, there are invalid combinations. There are combinations that don't make sense, which then the type is the will reject. Okay, then. Thank you again.
