00:00:01.050 - 00:01:17.166, Speaker A: Good afternoon everyone, and my name is Han Liu and I'm currently a postdoc researcher from Qinghua University in China. And I will be talking about research work on statistical linker for study smart contracts. And as normal computer programs, smart contract have smells such like unused function parameters or unprotected message cores, and maybe a delayed update which is vulnerable to reentrancy attack. So these smells are not necessarily causing any catastrophes, but these are something that you're trying to avoid in your smart contracts, because they can be stylistical errors, or maybe they are not following the best practice. Some of them are bugs and even security issues. So the most straightforward way to capture and remove this mouse would be when the developer finished their code. We can use a bunch of analyzers, for example, formal verifiers, static analyzers, to check this code and see if the bad code is there.
00:01:17.166 - 00:02:31.586, Speaker A: And if you're doing this commonly, you will need your code to be compilable in most cases. And you should be aware of the predefined rules which tell the analyzer what kind of thing they should be searching for. And what we're trying to do is that we want to make these checks earlier in the development lifecycle, and we want it to be in an interactive way, which means that the developers can check their code even if they haven't finished it, or even if they have no clue what the patterns look like. And to realize this idea, we have proposed the S gram framework, which exploits the nationalist of smart contract code. So the nationalist notion is actually coming from the software engineering community, which tells you that how natural or how irregular your code is with respect to a large collection of other code. What do we do with this nationalist notion in Esgram is that given the contract code, we will use a parser to pass it into a token sequence. And based on this token sequence, we will build this statistical language model, which captures the regularity of all the tokens.
00:02:31.586 - 00:03:27.738, Speaker A: And the language model would be able to answer the question whether the token sequence is likely to occur in a specific context. And then we can identify irregular code in the smart contract and flag potential problems. So, more specifically, the esquam framework works in a two phase manner. In the first phase, we will need a large collection of smart contracts to train the model. To do that, we will use a static analyzer to extract semantic metadata out of your contract. Basically, we're trying to do two types of things, and we can focus on the access on a storage data and also the flow sensitivity. So if we take a look at this simple smart contract, at these two lines of code, the analyzer will tell you that, oh, these two lines of code are accessing on the same storage data called user balance, and one of them is read operation and the other is write.
00:03:27.738 - 00:04:45.502, Speaker A: And these two operations are dependent on each other because they are from different public functions and are not commutative to each other. And in terms of flow sensitivity, if we look at this kind of code, the flow condition of this line of code includes constraints from the modifier and also the if statements. And the way we modeled this flow is by using the addresses and the operators involved in these flow conditions. In this case, we will be using message standard and the two operators as specified here. And then we will use a tokenizer to generate a token sequence from this contract. And the generation is basically done by traversing the abstract syntax tree in a type based manner, which means that we generate a corresponding token for a specific type of ast node and then we will be training the model using an underlying Ngram model engine and to build this statistical language model. And in the second phase we pretty much do the same thing and given the smart contract we generate the token sequences and then we can use a detector to curate the language model before, and then calculate the regularity or perplexity scores of subsequences.
00:04:45.502 - 00:06:05.580, Speaker A: And then we will highlight top candidates of these smart contracts with the highest perplexity scores. And if you are trying to use this candidate's information to help optimize existing smart contract analyzers, for example a symbolic execution engines, what you can do is to design this ranker which takes the candidate information and generates scores for all the information, all the functions in the contract. And then this scores will tell the symbolic executor which function is more buggy than the others, and then the symbolic executor can prioritize the exploration of a specific function with high scores so as to detect vulnerabilities more efficiently. And in the future we plan to work on optimizations on the language models. For example, we are trying to figure out more efficient way to encode both syntactic and semantic regularities. And also we are considering porting SRAM to more existing techniques, for example formal verification, static analysis, random fuzzing, something like that, and also to create a better developer experience. We are planning integrated SRAM with IDE so that we can capture and model developers feedback and optimize the S gram itself.
00:06:05.580 - 00:06:25.440, Speaker A: And we actually have published an academic paper about the S gram. If you guys are interested, you can look into the details. And I will be round offline around here for offline discussions, and that will conclude my talk. Thank you. Close.
