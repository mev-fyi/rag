00:00:03.850 - 00:02:13.960, Speaker A: So me and Leo are going to be talking about formal verification of solidity contracts and a little bit smart contracts in general, some of the ways in which the language and the compiler can assist in doing this process, and also talking about act, which is a language for specifying the behavior of smart contracts. And I'll try to keep this relatively brief, leaving a lot of room for discussion about what can be done to improve the workflow as it is right now. So to begin, I'll just position this question of how to specify the behavior of selected functions in context, sort of opening up for the discussion. One way in which you can do this is to use asserts directly in the function source, and if you are not using annotations, but rather referring to the variables directly in the solidity code, this is how that can look like for a transfer function. You would be asserting that the relevant variables are changing expressed in pre and post conditions, and it can look something like this. But if you don't want to do it directly in the function source, an alternative would be to have like a wrapper function. And this relates again to the discussion we just had on conventions for testing frameworks, because this looks very similar to how you would write a test if you are writing your tests in solidity at least.
00:02:13.960 - 00:03:39.682, Speaker A: And yeah, I think it's sort of straightforward what's going on here. You do some caching of variables before you execute a certain function, and then you compare those values according to some condition against the state that you get after executing that function. And then there is the alternative of specifying the behavior in a language, in a different language altogether. And so here this language is act, which I'll be going a little deeper in during the next couple of minutes. So I think this is a good example to just get a flavor of what the syntax can look like if you want to really highlight everything that's going on inside a transfer function. So it should be noted that this function makes a little more of a precise claim than the previous two. In the previous two, we're really only talking about the updates that happen to the balance of mapping, whereas here we're also saying under which precise conditions these updates are happening.
00:03:39.682 - 00:05:46.940, Speaker A: We're saying that call value should be zero, since this is a non payable function and we're doing the doing the check that is done implicitly here by the safe map. And we're also immediately forced to consider the edge case of what happens when you're actually transferring to yourself. So in aspects you are forced to split up every claim that refers to mappings, depending on whether you have a collision in those mappings or not. So I'll get more into that in the next slide, I think. But let's just talk a little bit about the different approaches and sort of the pros and cons for them. So for the first two, obviously a big advantage is that it's integrated and the developer doesn't need to learn an additional language, however obscure you may find it, but can just work directly in solidity. And if you're writing your properties of the function directly in the source, things are also pretty transparent or auditable by people reading the implementation, because they'll see your claims about what the behavior is supposed to be, and they can either test it for themselves with their own frameworks, or they can just sort of get a better understanding maybe of what this function is supposed to do if they prefer reading this functional description over the sort of implementation that is given.
00:05:46.940 - 00:08:00.780, Speaker A: An advantage of the latter two is having this wrapper spec for the specification in a language completely outside of solidity is that it's more agnostic to the particular implementation. If you were to change how the nature of transfer were implemented, you could probably still reuse the same testing function and even this spec. But if you want to change how the implementation does what it's doing according to the spec, then you'll be forced to rearrange your spec too, probably, or you'll be forced to rearrange the claims that you're making inside the implementation. Now you're editing both what the contract is actually doing and the description of what it should do in the same development process. And so another advantage of using a different language that is designed for the purpose of specifying smart contracts is that you can make decisions that are more aligned with how functions are described independent of how they are implemented. You don't really need to care much about what is the most gas efficient way to do something, but you just want to describe the results of whatever implementation you might have. And also, as I mentioned, there are certain features of the act language that are designed to lean the developer into making sure that they really think about all of the edge cases that may exist in the implementation and are forced to think about them explicitly.
00:08:00.780 - 00:09:58.608, Speaker A: Also, when one has an outside language that sort of exists beyond the scope of the EVM, one can talk about all of the variables involved as true integers and not just EVM words that wrap around to the 256. As we can see here on the last line of the if and only if block, we're able to express that the addition of value to balance sub two should not overflow. And we can express that by explicitly giving the bound of two to the 256, even though that is essentially unexpressible, at least in this form, if we are only allowed to use bitwise addition. Yeah, some additional points there that I think are obvious. One big approach that I didn't mention here is of course if you do sort of salty verify approach where we use annotations instead of using the source code directly. So to summarize the essence of act, it is designed to be a human readable specification language in which you can express function descriptions and contract invariants, get to that later, from which you can generate proof obligations to multiple back ends. So there's a large number of formal verification tools that are emerging, although there has been for a while, but they're getting very good right now.
00:09:58.608 - 00:11:29.340, Speaker A: And it's really curious to try these different tools out because they have certain strengths and weaknesses and are also operating in slightly different domains. And so act can be used to try these, test these tools against each other and compare the results, but also utilize the different tools that are targeting different domains. So when you're working with these source level specifications or development tools, you're always at risk of the compiler doing something that you're unaware of or behaving unexpectedly. And this is sort of a blind spot for the formal verification process. But then on the other hand, if you're doing bytecode verification, it takes a very long time and can be difficult to do stuff like contract environments. So act is an attempt of modularizing the process of doing formal verification for smart contracts and being interoperable with these different back ends. So our first couple of plans for different backends we'll be targeting is a cock backend for claims that are difficult to prove using S and t solvers and really require a manual proofs, s and t queries for doing contract invariants and certain checks that are easier to do in the s t setting.
00:11:29.340 - 00:13:44.830, Speaker A: And for bytecode verification, you already have a prototype of exporting proofs to KVM, and there's also an HVM backend in the works, which will be faster in many cases, but slightly less general. And so to go a little deeper into this point about the modularity of proving tools and verification setups, here is a more in depth example where we are expressing an invariant in the constructor of the token. So this also touches on a point that I'll get to a little later, where here we have this sum construct which is not native to solidity, or even something that could generate any reasonable bytecode. But it's very useful for formulating properties about what we expect our contracts to do. So, yeah, here's more of an example. And I guess one point that I'm trying to make here is that I think this language is pretty similar to how you may express the nature of what you want a token to do in a setting like you're writing an ERC. When I recently was writing an ERC for permit, I wrote the description and the specification of how this new function and token extension is supposed to behave in a language that was very similar to this, because I think that it really allows for people to make their own implementations and have freedom in how they decide to optimize their particular behavior while still being completely unambiguous in terms of what the function should do.
00:13:44.830 - 00:14:47.504, Speaker A: So as a result of this, it means that if you're writing an ERC in this language, not only will it be clear for people to read and implement, but it can also be something that you can use to test and formally verify that ERCs are actually implemented in the correct way. Yeah. So here's a little bit more about how it actually works. Is there a question or just a sigh? I'll keep going then. So I'm not going to go into too much detail here. I'm just going to say what I said before, that the design decisions of actor are made in such a way that you should be able to really think about all of the education. There was actually a question, Martin, should I go to the gitter chat? Or someone raised their hand here.
00:14:47.504 - 00:14:48.530, Speaker A: Who was it?
00:14:48.900 - 00:14:50.944, Speaker B: Justly, it was me.
00:14:51.062 - 00:14:51.730, Speaker A: Yeah.
00:14:52.980 - 00:15:09.130, Speaker B: So what are the limitations of the language? Can I, for example, describe my property over a set of function, like something should od after calling f one and f two, or something like that?
00:15:10.220 - 00:16:06.422, Speaker A: Yes. Actually the syntax that you saw here of invariant is a special case of an invariant that holds over all functions. But there's also a syntax where you can do invariant of and then function. Thanks. Set this right. So here is, I guess, a more concrete way of expressing what I mean by doing this modular approach to verification. If we take an example, smart contract, this one, this is going to be a contract that I now only express in terms of act syntax.
00:16:06.422 - 00:17:26.470, Speaker A: But you can probably imagine the solidity implementation. It should be fairly straightforward to see. So we have three variables that it's dealing with, xyz and an invariant that we expect to hold from them. If we have two functions which simply update two of our variable symbol by multiplying it with a scalar, either multiplying x and z using the function f, or updating y and z using the function g. Then this generates a cock theory which is beyond the scope and sort of agnostic about the blockchain setting that this smart contract is operating in. And so it has sort of the essence of this contract, and it allows you to get all of the relevant definitions over which you can do the proof of safety that you have expressed in the act syntax. So all of the boilerplate that comes with generating the definition and the theorems here of the smart contract involved would be generated automatically, and you simply insert the proof.
00:17:26.470 - 00:19:18.780, Speaker A: I skipped ahead a little bit. Let's see if there was something we missed. Yeah, so I guess I showed it now by example, rather than explaining the details of what I meant to say. I'm essentially saying that usually when you're doing a formal verification, this sort of end to end formal verification process where you have some smart contract business logic sometimes people refer to, or high level specification which you expect certain properties to hold of. You can make proofs on a high level which aren't referring to any bytecode specific or blockchain specific parts, and then you can also perform the bytecode level proofs with the same specification if you're using act. So this refinement that the higher level properties that you're proving also hold of the lower level bytecode that you get from the compiler is more of a property that either holds or doesn't hold of act. Well, hopefully it holds if we do everything right, but it's not something that you need to do for each project, right? So here's just a little list of some motivating properties or motivating examples for why you may sometimes require something more complex than just SMT solvers for proving correctness of your smart contracts, simply because there are certain properties of smart contracts that express the correctness of them that are too difficult for SMT solvers to do.
00:19:18.780 - 00:20:33.140, Speaker A: And this is a good example which you can actually prove using SMT theories depending on which strategy you take to doing it. But I think doing something more complex would be out of scope for this. Okay, so in conclusion, leaving more time for questions and discussion rather than presentation. In doing act, and in writing specifications of smart contracts using act, you get this language improver agnosticism. So you can compare different provers, but you can also compare different implementations of the same logic or different languages writing the same smart contract. So you can compare the bytecode generated byte per se versus the bytecode generated by solidity against each other. Actually, just as a note on that, this was the subject of a presentation that I did at last Devcon, where we had, similar to the previous slide, a specification for NIRC 20.
00:20:33.140 - 00:21:49.126, Speaker A: And then the talk was more of a hackathon where people were competing to make the most gas efficient ERC 20. And this is sort of a safe way to do these gas bulk competitions, because now you know that not only can you have these crazy optimized versions that are supposed to do something, and you sort of have some tests we have to check it against, but you can actually do formal verification against the bytecode that is generated in the end. So it's a very good measure to have if you're really keen on making crazy optimizations. Yes. So this is act. Please come to the gitter channel to discuss it if you're interested. And just to keep this sort of focused on solidity, here are just some pointers that, well, an ask really to SAl C and the developers of maybe something that was discussed already during the debug session.
00:21:49.126 - 00:22:46.262, Speaker A: I'm not sure, I wasn't able to attend that, unfortunately. But the ask is essentially this, and it's related to bytecode verification. Basically, the situation is that when you're doing bytecode level proofs, often you have a pretty performance heavy duty because you're symbolically executing EVM code. And there's a lot of things that can happen. And in order to scale this process, what one wants to do is to reuse proofs about subsets of bytecode as lemas in other proofs. So if you already know that a particular internal function can be summarized to do this, then you can reuse it whenever you get to that part of the bytecode. But this is sort of tricky to do using solidity right now, because it requires a lot of manual labor to extract the relevant pieces of bytecode.
00:22:46.262 - 00:23:36.550, Speaker A: The ast really helps and the source map really helps. But what would help even more is to have PC values directly from specifically internal functions and modifiers. So this is my path. I have a method of extracting it now which involves combining the AST and the source map, but it's very unreliable, and I think direct support for this would be beneficial for other tools as well. Are there any other things I want to say? Not really anything more than what I said here. Oh well, maybe one thing. Variable location at function heads and loops could also be valuable.
00:23:36.710 - 00:23:43.918, Speaker B: Can you be a bit more specific on that? So also on the PC values, what exactly do you mean.
00:23:44.084 - 00:24:44.760, Speaker A: So essentially what I've been doing before is when doing a proof about an internal function is that you go to the Pc value that represent the jump test where this function begins. And usually if it is a stack based function, you have the arguments organized in stack in reverse order to how they appear in the function declaration. And then you can do your proof like that. It becomes more complicated if you have a function which involves memory, and also if the function comes from a different contract that has been imported somehow or inherited from, then getting the location of how these functions relate to the bytecode is kind of tricky. Is that clear? Chris, this. Can I clarify something.
00:24:55.670 - 00:24:58.500, Speaker B: That'S so stupid, why is there even a hang up button?
00:24:58.950 - 00:25:02.420, Speaker A: Sorry, Jocelyn is raising his hand.
00:25:04.250 - 00:25:19.210, Speaker B: May I just say something to finish up on that? So you would like a mapping from probably astid of all functions to the entry point as a bytecode offset?
00:25:20.430 - 00:25:20.842, Speaker A: Yeah.
00:25:20.896 - 00:25:23.340, Speaker B: Okay, cool, thanks.
00:25:26.750 - 00:25:32.990, Speaker A: Francie. Can you direct or sort of say who? Yes, Jocelyn.
00:25:36.850 - 00:25:57.320, Speaker B: Nice talk. So could you describe when act, start and stop typically? Like is it a DSL with json output? Or does it do more stuff like checking if the variable, if the acts variable are the same as the contract variable and stuff like that.
00:26:00.250 - 00:26:59.726, Speaker A: So it is right now a generating from the specification language, adjacent output or sort of intermediate representation which can be used to plug into different back ends. And also we'll be supporting some back ends internally without going via this ast. But there's also the option of. So to answer your question, I think it's about whether the source code needs to be available for these proofs to go through or not. Or at least that seems to be part of the question. And the answer is for some back ends it will be necessary, or for some back ends you will be required to do additional work if the source code is not present. So a newly introduced feature or new, but a feature now of the solid compiler is that you can extract the location of storage variables.
00:26:59.726 - 00:27:29.410, Speaker A: And then it's very nice to write specifications like this where you want to do bytecode verification proofs, because you can simply refer to them by the name. And if you have the source code, you know what those storage locations are going to map over to in terms of EVM locations. So if you don't have the bytecode, or if you don't have the source code of a contract, and you still want to make this bycode level proof, you would need to be more explicit about where these different proficient.
00:27:29.910 - 00:27:36.050, Speaker B: And if I have the source code, the json output is going to have directly the storage location.
00:27:37.190 - 00:27:41.760, Speaker A: Yes, there's a way to include that, thanks.
