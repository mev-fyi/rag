00:00:13.690 - 00:01:01.706, Speaker A: So in my talk today, I'm going to be talking for the first time about how we've designed polygonmide and roll up, and specifically how we use this hybrid utxo and account based model to achieve some interesting properties. So just to set the context at first, the goal that we have in mind is we want to build a scalable, decentralized roll up with privacy enabling architecture. And what I mean by that is that our immediate goal is to achieve scaling, but we want to design the roll up in such a way that when we want to turn on privacy, it will not require a complete architecture overhaul. It should be very easy to achieve that. And I'm sure a lot of you here are already familiar with this. But just to set a context of what is a decentralized roll up, we have users, we have roll up operators, and we have Ethereum L one. And in this model users send transactions to the operators.
00:01:01.706 - 00:01:45.722, Speaker A: Operators aggregate those transactions into blocks and then they submit kind of the state delta in a context of a zk roll up with a zk proof to Ethereum l one. And then what we get, and this is not specific to a decentralized roll up. This is true for any roll up. We inherit security from Ethereum. What is specific for a decentralized roll up is that a roll up has its own l two chain and its own consensus mechanism, because the operators need to agree on the state of the chain. And then we want to have this operators to be the set of operators to be permissionless, meaning anybody can join and leave the set as they please. Now compared to a centralized roll up where we have only one operator, decentralized roll up has a number of challenges.
00:01:45.722 - 00:02:20.850, Speaker A: The most important ones are you need to have a separate consensus mechanism. You also have this execution bloat problem, and I'll explain what it is in a couple of seconds. And then you have a state bloat problem. So in this talk, I'm going to talk about specifically execution bloat and state bloat. And so let's get into it. So what is execution bloat? An execution bloat basically means that the network needs to execute all the transactions, and more specifically, a block producer needs to execute on the transactions in a block. But also everybody else in the network needs to reexecute the transactions to make sure that block is valid.
00:02:20.850 - 00:02:53.214, Speaker A: And that leads to a lot of reexecuting the same code over and over again. What is state bloat? I'm sure a lot of you are familiar with that. This basically means that state size grows with time. The more accounts there are, the more tokens accounts hold and all of that, the state size increases. And the reason why we can't do much about that is that nodes or operators need to hold the full state to be able to validate the blocks. And nodes need the full state to be able to produce new blocks. Now why are these things bad? Like I said, there are challenges.
00:02:53.214 - 00:03:29.626, Speaker A: So why exactly are they challenges? So the first thing is if you have state bloat and execution bloat, you need powerful machines to like let's say we have thousands of transactions per second. You need a powerful machine to process that. If you have a large terabyte sized state, you need a large machine to hold that in memory, and that leads to decentralization. And if you don't have good solutions to this problem, you might as well just build a centralized roll up the other one, because everyone sees everything and everybody needs to reexecute transactions and have the full state. There is inherently less privacy in this setup. And last one is especially specific to state bloat. This is not sustainable.
00:03:29.626 - 00:03:57.186, Speaker A: If the state grows, you can scale the roll up only as fast as the hardware scales. You can go like a hardware in a single machine or something like that. So what do we want to achieve? What is the ideal solution to this? So the first thing, we want to minimize execution bloat. And that means we want to execute transaction only once. And also we want to make sure that it doesn't have to be executed by the same party, so it's not the same block producer that needs to execute all transactions. We want to have distinct actors in the network that can execute transactions. We also want to minimize the state bloat.
00:03:57.186 - 00:04:23.870, Speaker A: And that means we don't want to enforce the condition where you need to know the full state to validate blocks. And we also don't want to enforce the condition where you need to know the full state to produce new blocks. Zkps can give us these two upper properties. If you have zkps, you can produce a proof of execution, for example, and you don't need to re execute the same transaction over and over again. But to achieve the other two properties you need something else. Zkps are not enough. You need what I call a concurrent state model.
00:04:23.870 - 00:04:48.040, Speaker A: And before I get into the concurrent state model, let's talk about the popular approaches to what the popular state model is right now. So we usually have an account based state and a Utxo based state. And if we look at pros and cons of each, let's say, for account based state. It's great for expressive smart contracts. This is what we love about Ethereum. We can write very cool applications. You have a lot of freedom, and they all interact with each other very well.
00:04:48.040 - 00:05:23.086, Speaker A: It's not great for concurrent execution. It is possible to achieve, but it is not very easy and has a lot of issues. And it is also bad for anonymity, because if you have accounts and you know which account participates in which transaction, it's very difficult to hide kind of this transaction graph, so to say, Utxo based model is kind of opposite of that. It's great for concurrent execution, because in a Utxo model, transactions are logically separate from each other. It's actually a very good tool for anonymity. Like if you want to achieve anonymity, you almost have to use a Utxo model. It's not the only thing that you have to use, but it is one of the kind of basic building blocks.
00:05:23.086 - 00:06:02.958, Speaker A: But it is not great for expressive smart contracts. You can kind of get smart contracts in the Utxo model, but it's not easy. And the more expressive they are, the more it starts to look like an account based model. So what we want to do is combine the nice properties of each of this into a single model. And I call this like basically account based model, etxo based model, and combine that with ek proofs, and we'll get something that I call the actor based model with concurrent off chain state. And I'll get into what all of those terms mean in the course of this presentation. So, first thing that I want to explain of how this works is how do transactions work in this model, and what is an actor model specifically, and how we think of transactions in that model.
00:06:02.958 - 00:06:43.230, Speaker A: So, just to take a step back and explain what is an actor model? It's a concept from distributed kind of systems, where you have actors, which are kind of state machines with inboxes. And actors communicate by sending messages to each other. And the important property is that the messages are asynchronous. So an actor can produce a message, and then a different actor can consume this message at a later point in time. The way we apply this actor model to a blockchain is that in our context, and context of MIDN actors are counts. An account holds a state and exposes an interface. An interface is just a collection of methods, which every of those methods is a MidnVM program, and MIDN VM is a fully Turing complete CK Vm.
00:06:43.230 - 00:07:28.138, Speaker A: So you can think about it as very expressive functions that you can write for the account interface accounts communicate with each other by sending nodes to each other, and nodes can carry assets. And a node also has this pen script which needs to be executed to be able to consume a node. And one important property is that in this model, it actually takes two transactions to move assets from one account to another. So in a traditional kind of Ethereum model, for example, you usually have just one transaction that moves assets from one account to another. In this model, you have to have two transactions, because the first transactions create a node and the second transactions consume a node. Now let's talk about transactions in a bit more detail. So what is a transaction? In the context of maiden, a transaction always involves only one account.
00:07:28.138 - 00:08:22.534, Speaker A: The transaction does not involve all account, and in the course of a transaction, the state of the account gets updated. Transaction can consume zero more nodes, and a transaction can produce zero more nodes. So in a previous example, for example, there was one transaction that produced one node and one transaction that consumed one node. And we can have also transactions that produce and consume notes in the same transactions. Now the execution graph of how, let's say, nodes get consumed, kind of to explain how this whole process this works is, let's say we have a transaction that wants to consume two nodes in a context of one account. So the way it would start is like we have this prologue and epilogue that do some bookkeeping to make sure that, let's say, sum of inputs is equal to sum of outputs, and nothing, kind of, no new assets get created in a process of transaction. But then we go into this execution stage where the first thing that happens is we execute a script of the node, of the first node in this transaction, and then this execute script can call any number of methods on the account interface.
00:08:22.534 - 00:08:48.158, Speaker A: So in this case, let's say there is a receive method that receives assets on the account. So a node can pass assets to the account through this receive method. And one important thing is that account methods are the only ones that have access to account state. A node cannot modify the state of the account directly. It needs to call a method on the account interface to modify an account. And then the account interface methods can create other nodes. That's how you can, for example, create new nodes in the process of a transaction.
00:08:48.158 - 00:09:28.542, Speaker A: And then if we have another node, we do the same thing, we sequentially execute the second node in the context of the same account, and that node can again call the same or different method on an account to have different effects and so forth. Now, in our context, because we can kind of execute and nodes only touch a single account, what we do is we execute a transaction and immediately produce a proof for it. So in our case we use the stark proving system. So maiden vm is a stark based vm. So whenever a transaction is executed, we immediately produce a proof of execution. And because again, I mentioned that transactions are logically distinct, they only touch each account separately. We can produce many transaction proofs in parallel.
00:09:28.542 - 00:10:03.606, Speaker A: So we actually produce all the transaction proofs in parallel. And then what we do is once we have a bunch of these transaction proofs, we recursively aggregate them into batches, and these batches then recursively get aggregated into block proofs. And then these block proofs get further aggregated into like epoch proofs. And that's what gets submitted to Ethereum. Now, it's important to know that all of this recursive aggregation can also be done in parallel. So as I mentioned, all transactions can be proved in parallel, but also all batches can be proved in parallel. The only thing that doesn't get proved in parallel is the final kind of tip of this block proof.
00:10:03.606 - 00:10:37.906, Speaker A: And then there is another interesting property is that we can prove transactions locally, and I'll get into that in a second of what exactly it means. But then these aggregation steps need to be done by the network. For example, a block producer or a block producer can delegate this kind of aggregation to someone else, some other actors. Now let's talk a little bit more about with this concept of local versus network execution. So in a traditional kind of model, when we execute a transaction, we have a step that prepares some inputs for the transaction, signs, a transaction and so forth. Then we execute it. Then in the context of the ZK system, we generate a proof for this transaction.
00:10:37.906 - 00:11:22.470, Speaker A: And finally we get this transaction proof that according to the previous slide, gets aggregated into batches, and finally end ups in the block. Now in a network model, the block producer, so the user prepares the transaction, sends it to the network, and then a block producer would execute this transaction, generate the proof, and then aggregate this proof. As I described on the previous slide, in a local context, the user can actually do all of this. So the user can both prepare the transaction, execute it, and generate a transaction proof. And then what gets sent to the network is actually just the transaction proof itself. And then the block producer doesn't actually need to execute the transaction and doesn't need to generate a proof for it. The block producer just needs to aggregate it with other transactions which it has generated the proofs.
00:11:22.470 - 00:12:26.374, Speaker A: One important thing to notice, how do we handle shared state? Because what I described works very nice when you have transactions which go and don't touch multiple accounts, or like when you have nodes that go to different accounts and so forth. But let's say we have something like a Uniswap situation where we have several accounts that want to send nodes and exchange, let's say assets for some other assets using a Uniswap account. So the way we would do it is that first we would have each account generates its own transaction to create a node that targets a Uniswap account. This would be two separate, kind of logically separate transactions. Then the block producer would generate a third transaction that would consume the first two nodes in a single transaction. And also as a result of this consumption of this node, it would generate other two nodes that would kind of target back, carried like the exchange tokens back to the original accounts. And then we would have the additional transactions that the users of accounts one and two would execute to consume kind of this nodes back into the back into their respective accounts.
00:12:26.374 - 00:13:14.650, Speaker A: So basically, in this model, we still have this ability to interact with a contract or account with a shared state. Just in this case, the transaction that interacts with the account with a shared state needs to be a network transaction. It's not a locally executed transaction, must be executed by the network or the block producer, because the block producer needs to sequence the nodes according to whatever logic they want to do, and then execute all of the nodes against the same account. Now, just to kind of summarize this pros and cons of local versus network execution. So if we want to have a shared state, kind of an account with shared state, we cannot use local transactions, but we can use network transactions. Now, if we use a local transaction, we can have privacy because nobody actually on the network needs to execute those transactions. We cannot have privacy with network transactions because obviously somebody needs to execute them.
00:13:14.650 - 00:14:04.502, Speaker A: Now, generating proofs is a fairly computationally intensive process. So the client hardware requirements might be high for local transactions, but on the flip side, because you've generated the proof locally, there is much less work than a block producer needs to do. They don't need to generate the proof for the transaction, they don't need to execute the transaction. So the fees for such transactions, for local transactions would be lower than for the ones that are requested for the network to execute. Now the next thing I want to talk about is, what kind of a state model do we need to support this type of transaction model? And this is where the Uatxo and account based model kind of comes together. So my little roll upstate is actually described by three databases. Usually you have a single database, you have usually an account database, or in HTXo context you have kind of a TXO database, but in our context actually three separate databases.
00:14:04.502 - 00:14:45.606, Speaker A: There is an account database, there is a nodes database, and there is a nullifier database, and I'll explain why all of them are needed. And then in our case, updates to all of these three databases. So like when you have a block, a block contains information that updates all of the three databases and takes the state of the network from state n plus one account database. Account database holds all of the current states of the accounts. And we use a sparse Merkel tree is a data structure that holds this information. And the sparse Merkle tree maps account ids to account hashes. But we have one kind of twist to this.
00:14:45.606 - 00:15:29.986, Speaker A: We have two different types, or two different modes of storing accounts in this database. The first one is on chain state, which is basically the same as what you would get with Ethereum, where for each hash, the nodes store also all the associated data for the account, such as like storage code, nons, and so forth. But there is also an option to do just an off chain state where what the nodes store are just the hash of the account, and the user himself or herself is responsible for storing the actual state of the account. So nodes on network do not store the actual account state. Let's go to the nodes database next. The nodes database stores all nodes that have been ever created. And for this we use a Merkel mountain range, which is an append only accumulator.
00:15:29.986 - 00:16:21.782, Speaker A: And a leaf in this Merkel mountain range is basically just a set of nodes that were created in a specific block. And one of the reasons we chose this, there are a number of reasons why we chose the Merkel mountain range, and it's very convenient for a number of purposes. But one of this is that you can extend or add new nodes to this accumulator without actually knowing most of the previous nodes. So you can discard a big part of the nodes database and still be able to add new nodes to it without problem. The other property that is very important in the ZK context, because we need to prove that we're spending a node that has been created at some point in the past, is that the witness kind of inclusion witness does not become stale. So if you have a Merkel path, it actually just needs to be extended from time to time, very infrequently but it doesn't become stale. And that means that the ZDK proof that you generate does not become obsolete very quickly.
00:16:21.782 - 00:17:01.742, Speaker A: This is very important in the ZDK context. And then lastly we have the nullifier database. And the reason why we need this nullifier database is that we have the account database which stores states of accounts. We have the nodes database that stores all the nodes ever created, but we do not remove nodes from the nodes database because we want to have this nice property of append only accumulator. Therefore we need another data structure that will tell us which nodes have been consumed. So the notify database is something that keeps track of nodes that have been consumed. And for this we also use a sparse merkel tree where we basically map a node hash to either zero 10 indicates that the node hasn't been consumed, one indicates that the node has been consumed.
00:17:01.742 - 00:18:00.214, Speaker A: So whenever we generate a proof for a block, the proof must include that this node existed in an accounts database and it did not exist in a nullifier database. We actually have a slightly more sophisticated data structure where there are multiple epochs and those are time periods. And for each epoch you have a separate nullifier tree, and then nodes are expected to keep the last two epochs, but can discard the nullifiers for the prior epochs. Now we have these different databases, and there are very different growth drivers for each of these databases. So an accounts database grows primarily with a number of public accounts or the accounts that have on chain state, because it does grow with a total number of accounts. But if you only have to store a single hash for an account that's almost negligible, like you can store a billion accounts and it's going to be only 64gb. And also we can dynamically kind of prune this for accounts, for example, that haven't been used in a while.
00:18:00.214 - 00:18:21.990, Speaker A: We can just remove all the data and store the hash for that account. Nodes can choose to do that if they wish to. The nodes database grows with the number of unconsumed nodes. So as soon as a node is consumed, it can be safely discarded. You don't need to store it anymore. So unconsumed nodes is what drives the size of the database. But also you can have this pruning where you can remove some of the nodes and just keep the hash.
00:18:21.990 - 00:18:54.514, Speaker A: And then finally we have the nullifier database. And this one is a different one because you can't easily prune nullifiers to be able to create new blocks. You actually need to keep all the nullifiers and the nullifier database depends on the throughput. So like, the more transactions per second you have, the more nullifiers you need to keep for a given epoch. We can make epoch smaller, but there are some downsides to that overall. If we look at kind of like what sizes of these databases could be, that nullifier database is going to be by far the ones that drive the size of the overall state. It's going to be larger than the nodes or counts databases combined.
00:18:54.514 - 00:19:41.814, Speaker A: Now I have a few slides to wrap up the talk to say, well, what did we achieve? First, we have this concept of different models of execution. So the network execution and local execution. And we have this concept of on chain data and off chain data. And the combination of this gives us different nice properties. So, for example, if we have on chain data and network execution, these are typical public transactions, something that happens on ethereum right now. We can also have stateless transactions if we have off chain data, but network execution, where the network doesn't store the state of the accounts, for example, but the user needs to provide the state of the account with every transaction so that network can execute the transaction. And the next thing we can do, if the data is off chain and local execution is happening, we can have private transactions where the network is not aware not only of what code was executed necessarily, but also is not aware of the data that is in the account.
00:19:41.814 - 00:20:23.902, Speaker A: And we can also hide a transaction graph using utxos. I'm not going to get into that right now, but it's a bit slightly more complicated, but we can do that as well. And then finally, for completeness, there is this local execution and on chain data. I personally don't know which use cases that would cover, but maybe people will come up with something. How did we address execution bloat with those models? So first, we achieved no reexecution, so all transactions are executed only once. Second, we have concurrent processing, where transactions can be processed in parallel on independent machines, and you can almost scale this thing horizontally by adding more and more machines to generate proofs. And finally, we have this local execution, where transactions can be executed by the users that are involved in those transactions.
00:20:23.902 - 00:21:02.590, Speaker A: And the nice property here is the more locally proven transactions you have, the less burden, computation burden the network has to encounter. Because let's say 90% of transactions are something that was proven locally. There is very little work that the block producer needs to do. They don't need to execute them, they don't need to prove them, they just aggregate them into blocks. And then regarding state bloat, we have kind of this dynamic pruning where we can collapse accounts and nodes into their hashes, we can have very light verifying nodes. If you only want to verify state transitions and you don't want to create new blocks, you actually don't need to maintain the nullifier database at all. And in that case, as I mentioned, the nullifier database is the biggest part of the state, so you can actually discard the biggest part of the state.
00:21:02.590 - 00:21:30.460, Speaker A: And we have this nice thing where because the nullifier database dominates this overall state size, the overall state size really depends on TPS. So the higher the TPS, the higher the nullifier, the bigger the state. But it doesn't vary with the number of accounts, for example as much or number of nodes in the system. And last thing that I want to leave you with is that this is what we are trying to achieve, where the more privacy there is in a network, the more scalable it is. The more scalable it is, the more private it is. And this is our goal with the maiden roll up. Thank you.
00:21:30.460 - 00:22:38.522, Speaker A: So how would the network resolve when two accounts try to spend the same Utxo, like in a text or something like that? So if two accounts are trying to spend the same Utxo, that's a conflict. You can't spend the same Utxo twice, so it's not really a problem. In that case, if you are trying to spend like if you have Utxo and I have Utxo, and we submit transactions for whatever reason that both of us can consume, the block producer will need to decide which of those transactions goes through, because you can't execute both of those transactions simultaneously, because one of them will produce a nullifier, that the second transaction will not succeed, because the nullifier for this Utxo has already been created. So like in the Uniswap example, you can send a node that says I want to swap token a for token b right at this price, and somebody else can do the same thing. And those are two different requests. But then the block producer will aggregate those requests, sequence them in a single transaction, and execute them. And there will be no conflict on that, because the state of the Uniswap contract gets updated sequentially after each consumed node.
00:22:38.522 - 00:23:06.150, Speaker A: So you're not consuming the same Utxo, you're applying the different nodes to the same account. But yes, that cannot be done locally. That needs to be done by a block producer. It's an optimization. The idea is that, let's say your node was created in a prior epoch and a nullifier was created then you will need to provide the path that proves that it hasn't been consumed yourself. The nodes are not responsible for that. So the nodes are meant to be like a short lived object.
00:23:06.150 - 00:23:22.380, Speaker A: So they are not meant to stay in a state for a long time. And if, for whatever reason, you decided to keep the state there for quite long, that's your responsibility to be able to provide this proof to the network. It's not network's responsibility to keep it for more than, let's say, six months or so. Thank you.
