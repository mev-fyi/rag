00:00:03.350 - 00:00:52.650, Speaker A: I'm homing dai or you can call me hi dai just like this. Id for short. And today I want to share about how my team, the Sol team to do several optimization on solidity on EO and Ewasan via the LBM framework. So, so here is our agenda. We have four sessions to talk about that. First is the roadmap and how Sol works. And then we will just mention just a little UO optimizer and lbn optimizer Lang we want to introduce a very experimental new mechanism to do some static analysis and feedback derivative optimization.
00:00:52.650 - 00:02:20.998, Speaker A: Okay, so let's talk about our roadmap first. Actually this whole project start from I think this year the April folds and actually we have do lots of step to create a new language extension for solidity code Liti, if you remember we have talked about that on the devcom five. After the tool we want to just more focus on some optimization research because we believe that smart contractor for blockchain will become more and more popular. So if the engineer who is writing the smart contract cannot, we want to help them. They don't need to really care about the detailed caution about the performance or the cosites. We want to leverage lots of the automation skill and to help them to generate a small size bike hole. So here is the start point, and after we decide we want to use the LVN as our infrastructure, we want to choose one front end and one back end, which is the front end is we choose the solidity language, because I believe it's the most popular smart contractor language in the world.
00:02:20.998 - 00:02:56.350, Speaker A: So we choose the solidity and then for the backend. I believe that the solidity compiler has do very great work to generate the EVM bipolar very efficiently. So we just want to create a new, or choose a new target back end. So that's why we choose the solidity. And it wasn't for first. And then we go to the audio one, the very first release. We want to create a very small demo which is just.
00:02:56.350 - 00:03:53.330, Speaker A: I believe everyone has used this library called Stepmas and we just do very little grammar. Just you can see here if else or revert require a very limited integer poll. But before the Defcom Five, we want to just join the community and we want to talk to the solid team to ask for more conversation about Ebosn or something like that. So in these two versions we focus on how to compile ERC 20 smart contract. And we want to show our Sol has the ability, we can just support the constructor and we can deploy the evasion by code to the historian. It wasn't testnet. And actually we just finished that.
00:03:53.330 - 00:04:31.600, Speaker A: And we have a very good conversation between us and solidity team. And after this event, I believe this is very key event, because after our discussion, we found that, I remember last, Alice told us there is another project called Soland. And they also do the, I think just the sensing, just compile the solidity into evolution or into the subtract. So we just changed our goal from supporting the solidity front end. Just instead of.
00:04:32.050 - 00:04:32.702, Speaker B: Sorry.
00:04:32.836 - 00:05:22.318, Speaker A: We want to support the Evo front end instead of the solidity. So Michael mentioned yesterday, actually the Sol now can pass, I think, 80% of the test suite from the liberal UO. And the Sol wants to accept lots of Evo byco, which is generated by the Solc compiler. And we apply the LVM building optimization options into the whole code gen and whole BicO generation. And also we also support evn Lvn as our evM back end. So now the Sol can support a very limited solidarity front end. And I think almost the evo front end.
00:05:22.318 - 00:06:09.150, Speaker A: And we have two backend. One is the evolution backend and another is the EvN lbm back end. Okay, so here is something we doesn't support now, just like balance or self balance, something like this. And we will just keep going to finish this unimplemented instructions just a few months later. Okay, so what is our future roadmap or future goal? We want to fully support the Eofan N. And yesterday that we found that it is a very good extension called EO plus. And I believe if there are some features, that's very interesting.
00:06:09.150 - 00:07:11.454, Speaker A: So we will also add support of the Uoplus after we finish the ufanm. And we found that there are some very interesting things. Actually, traditional optimization applies on the ewasn may not be the best choice for the real world. Because when we deploy a smart contract on Ethereum, we will very care about the cosites rather than the efficiency, because the cosites will take lots of storage. So I believe it's more expensive than the native computer. Okay, so that's the second goal we want to achieve. And the third one is actually, we have several legend paths in our itemizer.
00:07:11.454 - 00:08:04.286, Speaker A: And we will discuss that later. Okay, so that's our roadmap from the past, from the current and the future. Okay, now I want to talk about how Sol works. So the first thing is that we accept the contract, which is right in solidity language. And when you get solidity contract, you can just bypass it into the solc and with the minor minus IR and minorized IR optimized. And then you can get. And at this stage we accept this UO smart contract into our compiler.
00:08:04.286 - 00:08:35.118, Speaker A: And if you want to see how you want to get the deployer plus the runtime, you just bypass this UA contract and we will give you the cy wars. But if you want to know what is the detail of the runtime by code, you have to remove the outside part of the U object because the outside part is the constructor and something.
00:08:35.204 - 00:08:35.840, Speaker B: That.
00:08:37.650 - 00:09:38.146, Speaker A: Is just a wrapper of the outside. So you have to remove the outside URL object first and then just bypass the remaining part to our compiler and you can get the runtime was it. So what is the detailed things that the sol compiler do? The first thing is when we get the UO smart contract we will pass it and check the semantic and create our own ast in our compiler pipeline. And the second thing is we will consume this ST with our code module and then we will get the lvn module here. So at this stage now we get lots of LV and IR. Now there is no Uo anymore. So each line of the UL statement will just map into several line of the Lvnir here.
00:09:38.146 - 00:11:02.054, Speaker A: Okay, now when we get the LvM module here and the next step is to use our backend. Our backend will just apply several LVM optimization paths and we will do lots of steps like the bit swap because the eVN is big Indian integer type and the webassembly is the little Indian integer type. So we need to do lots of the conversion scenes and then after we finish lots of works. Now the last step is we will use our tucking machine and linker to generate the evasive bycode or the EVN, LV and Lvir, that's the whole internal process of Sol works. Okay, so that's my second session to just describe how the Sol works and the surprise. We want to just mention some very different things about the UO optimizer and the LV optimizer. And the first thing is, I think the rules between these two is very different because the ultimizer actually is transformed from Uo to UO.
00:11:02.054 - 00:12:25.858, Speaker A: So you can have very high level information here. And so we will have code that your optimizer can do lots of aggressive elimination to do the inline scenes to remove the function code. But when we go to the LV optimizer, there is a big limitation for us because that we just convert all of the evo statement into the lvir statement. So we will load lots of the high level invoke engine first. And the second thing is when we want to apply to generate to the back end we will need to add lowering integer because the EVM has 256 bit bigger indian integer but the web assembly only support a CIs four bit integer. So we have do lot of conversion and also just the bit swap I mentioned before. So after we add the several scenes in our lvir code it will be more hard to do some analysis and to reduce with some very heuristic rule.
00:12:25.858 - 00:13:49.362, Speaker A: So I will give you more details in the example. Okay, so the first example is we called the very common compiler optimization. There is loop rolling or unrolling, which means here you have a contract with ten element of array and we just write for loop to fill the with the index. And if you compile this code to the native computer, for example if you write this algorithm in this algorithm in C plus plus and just compare it to the native, you will find that there are no more for loop because all of the synths will be converted into array zero equals assignable to the array zero. So it will just be ten line of this statement. But in the UO optimizer you will find that actually you just keep the structure. So you will have a follow up here and each iteration will just update the storage value once.
00:13:49.362 - 00:15:32.322, Speaker A: So I think it's very great. But when we apply, for example, we apply the LV optimizer with the three frac and we will find that all of the loop is limited because in this stage the LBN and the washen backend believe that we just replace the loop with lots of the constant store is more efficiency. But actually in the historian I don't see it's a good idea because if this loop become more larger, if it's not from zero to ten, it's from zero to 100 or 1000, then we will find the cosites will glow than our expected. So when we do the rupa unrolling, we find the final washm cos size will be about three k. But if we disable the loop unloadings, we will get two point five k bytes here. So this is the first very different thing that we have to be more careful when we apply any LVN optimization, because lots of optimization is designed for the native computer, not designed for some very special cas. Okay, the second thing is I believe that's a very good example that the UO optimizer can do much better than the LB optimizer you can find.
00:15:32.322 - 00:15:51.660, Speaker A: This is Devcon Uo and it's stored here and we just call arrays arm and in the array sum we will call lots of overload. So after the evo optimizer, sorry.
00:15:54.190 - 00:15:54.554, Speaker B: I.
00:15:54.592 - 00:16:10.780, Speaker A: Missed one slide here, okay, I missed one slide here and I will give you one moment, let me check if I can get it back now.
00:16:16.630 - 00:16:22.220, Speaker B: Let it will moment.
00:16:39.840 - 00:16:40.812, Speaker A: Let me just pass.
00:16:40.866 - 00:16:59.360, Speaker B: It copy and hold on past, sorry, yes but copy.
00:17:03.140 - 00:17:04.036, Speaker A: Wait a moment, let.
00:17:04.058 - 00:17:10.120, Speaker B: Me share another screen here. Okay.
00:17:10.190 - 00:18:50.388, Speaker A: So you can find, sorry I just go back. So after the UO optimizer then we will just convert this UO to very short one. You can find all of the function code is, all of the function code is email. So there's very good things that UO optimizers do. But when we go back to the LBN optimizer three, then we will figure out that the array sum has been in light, but the array load has not been in light because in the LV architect measure we figure out that the function body and the basic block is larger than the initial search hole. There will be only the array sum has been in line and the other one will not be in line. Okay, so that's very different between the LVN optimizer and the evo optimizer because LV optimizer the inner has its own search hole and we have to do lots of the research and to figure out which search hole is good enough for the evolution cost model.
00:18:50.388 - 00:19:59.740, Speaker A: So that's the circuit, we will have to do more research here. And the third one is the storage SSC sample. So I just create a new integer storage, we will do add ten and add ten twice. Okay, so when the contraries be compiled to the UO and after the UO optimizer and you can find that there is the sensing like the function body and it just updated storage value twice and that's the sensing with the LV optimizer like you can find that. Here's the add first to add ten into this is a temporary rebel means the original story value and just store it. Oh, here is by swap, don't worry about that. And we call the storage store just to store the after value back to the storage.
00:19:59.740 - 00:21:14.980, Speaker A: But we believe if we can do a very trivial optimization which is these two line can be combined into one line which is I plus equal 20, we don't need to do just like two lie of the ad and two lie of the storage update, we can just simplify the statement into one line. This is several things that we can do more about this case. Okay, so here are three examples. We want to mention that both of these two optimizer still has lots of research and optimization technology. We can do more. So we want to introduce the final part, which is we do research to analyze the cost model. So what has the motivation? This is motivation is like, I believe the gas cost is just like the energy consumption.
00:21:14.980 - 00:22:05.056, Speaker A: If you run a very bad program on your cell phone, then your cell phone will eat more power. And if you write a very bad smart contract, then it's guess. I think it's just legal. So we apply very, we apply the same analysis and optimization just from some paper to deal with the energy consumption. So we create a project called the GPS cat, which means general purpose static coast analysis tool chain. Okay, it's just very long name. And in this tool, we have two phase tool chain.
00:22:05.056 - 00:22:54.784, Speaker A: The first one is we don't analyze the evo directly. We analyze LBIR because we can do lots of debug info. We can insert lots of debugger info to help us to evaluate the LVNIr cost equals to lots of the web assembly instruction cost. So we will start on the LVNr first. So when we get lvnir, we can just calculate it into a cost function. And then we will use up one solver to resolve this function, to figure out its score. Okay, so here is our total analysis diagram.
00:22:54.784 - 00:23:59.364, Speaker A: So, first of all, since we will define some assembly cost model, something like if we add a number, it will take, let's just say 100 or 200, something like that, and all of the LvIR bytecode will be annotated with debug info with the debugger location. Because we want to know that every LVIR will be generated to what kind of webassembly instruction. So the top idea is like this. So after we do lots of annotation, we can get the LVIR with blood course information. Then when we get this information, we need to have to apply two framework from the outside. The first one is we need to extract the cost relationship. So we use the Lbn to keto.
00:23:59.364 - 00:25:16.476, Speaker A: And the second thing is after we have this Cosmo relationship, we have to apply the upbound solver to calculate the scenes. So here is, we use two external project to help us to do this. Okay, so after we go through lots of things, now, if we apply some optimization on the LVNIR, then we can get a new score and a new formula and we can calculate. If we apply this, the static analyst will tell us, well, your cost will be increased or decreased. Then we can just adjust the theoretical or address the optimization options to help us to create more suitable optimization phase for the UAS and cost model. Okay, so I think I have time. So the following slide is just mentioned how we annotate the debug info for the LVIR.
00:25:16.476 - 00:26:47.564, Speaker A: Just like we will say each LvIR will map into which line and lowering it into the assembly, the web assembly. Then we can just get a mapping registration between the load lvnir instruction will be do nothing but the ad will be converted to three instruction of the webassembly. Then with this model we can just say we have a table of the mapping relationship and we have the cost table. So we will know each LVIR is mapping to what kind of cost and we can calculate the final basic cost and we will know the whole program, the whole cost of this smart contract when it just is queued. Okay, I think that's my whole talk about Sol and something about optimization. So if you are interesting, you can go to this link to our Sol repository to use them, or you can just, I believe Michael showed the video yesterday. And if you are interested actually, because we need to measure the Bico efficient and bico performance.
00:26:47.564 - 00:27:07.590, Speaker A: So we create our self owned Watson virtual machine to do this. And if you're interested about GPS, Cat, here is our repository and all of the examples you can find in this repository here. Okay, that's all. Thank you so much.
00:27:08.120 - 00:27:33.310, Speaker C: Great talk. Thank you. So, because we have almost 1 minute delay in the live stream, we will give people the chance to ask questions. Now either if you're watching the live stream, please make sure to comment in the GitHub chat, or if you have a question here in the room, feel free to use the raise your hand feature so that I can see you have a question. Yeah, Chris, go ahead.
00:27:36.500 - 00:27:56.230, Speaker D: Yeah, thanks for the talk. I'm wondering, so you mentioned that you need to extract the runtime code from the Yule object in order to deploy it. Is there a way you could support the deploy routine that is written in Yule? Or what is the reason for.
00:27:56.600 - 00:28:49.492, Speaker A: Oh, actually, because the generated Yule contract is just like a rectangle and deploy or constructor part. There are two parts, right? I think there are two parts because if you give the Sol with the runtime code and the constructor code, the deployment code, we will just compile the runtime code just directly into the webassembly by code. So you have no choice, you have no chance to see the lbir inside of this runtime. So if you want to do some research on the runtime by just remove the outside part.
00:28:49.626 - 00:28:56.376, Speaker D: That's not required for running it. It's just if you want to see further details on it, if you want.
00:28:56.398 - 00:29:05.012, Speaker A: To running it, of course you have to give the runtime call and the constructor all well beneath.
00:29:05.076 - 00:29:05.784, Speaker B: Yes.
00:29:05.982 - 00:29:06.970, Speaker D: Yeah, thanks.
00:29:07.500 - 00:29:18.320, Speaker C: Okay. Another question here, Mooli. Oh, you're still muted it.
00:29:20.450 - 00:29:21.646, Speaker E: Can you hear me now?
00:29:21.748 - 00:29:23.246, Speaker B: Yes, thanks.
00:29:23.348 - 00:29:31.060, Speaker E: Thank you. Very nice talk. Can you elaborate a little bit about how much you save in particular, how much life saving are you getting?
00:29:39.890 - 00:29:40.302, Speaker B: Sorry?
00:29:40.356 - 00:29:41.600, Speaker A: Can you, can you.
00:29:45.810 - 00:29:46.560, Speaker B: Oh.
00:29:48.390 - 00:30:33.470, Speaker A: I will say there are no any benchmark or we doesn't calculate the gas reduced number here because I always say it's a very experimental, it will be experimental stage. So in this moment, I believe they also don't have very strong or very complete of the webassembly cost model. So we just said lots of number layer so we don't have the real number.
00:30:33.540 - 00:30:35.920, Speaker B: Sorry about that. Yes.
00:30:37.730 - 00:30:39.200, Speaker C: You'Re muted again.
00:30:45.410 - 00:30:45.870, Speaker A: Sorry.
00:30:45.940 - 00:31:02.600, Speaker E: So on the conceptual level, if you implemented this directly on the Yule, how much more difficult would it be? What are you getting actually from the LLVM as opposed to just running it on the Yule? Is my question clear?
00:31:18.910 - 00:32:54.630, Speaker A: Actually I believe there's different scenes because why we choose the LVM framework because it's just like the GCC that there are several people and do lots of artemisian research and create lots of Artemis strategy layer. So for us it will be very difficult because we have to choose which strategy is suitable for the UO. And then the second thing, we just pulled the algorithm from the LVNIr into the ultimizer. So if we can do lots of research on these parts, yes, we can pull lots of optimization from LVN into the UO optimizer. But I believe we should just still use the LVN framework and just add lots of analysis and optimization structure layer because you will have lots of things to do because you have to analyze them and create lots of information. So with those information we can just do more aggressive optimization. But I think we take too much time to rebuild all of the themes again on your optimizer.
00:32:54.630 - 00:32:55.940, Speaker A: So.
