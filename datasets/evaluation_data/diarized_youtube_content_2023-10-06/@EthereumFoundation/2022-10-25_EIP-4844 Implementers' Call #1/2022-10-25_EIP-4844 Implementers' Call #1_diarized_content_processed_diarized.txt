00:00:00.570 - 00:01:02.126, Speaker A: Um, I think, yeah, we can, we can probably get started. People are still rolling in, but we have a good group here. So I guess the goal of this meeting is basically to make sure we work through all of the spec implementation issues as they come up, and that we're all sort of on the same page about where things are at. We're going to have them weekly for now, and we'll see, as long as they're useful, we can keep them at that cadence and we might slow them down at some point if we feel like there's not stuff to discuss every week, I guess. Yeah, the first thing I wanted to go over, so there's a bunch of open prs in the specs, and I think it's useful to just understand where they're at, if there's any blockers on them. And this way the folks working on client implementations can know kind of what to expect and what to keep an eye out as they're working on stuff. So first one, this one's been open for a while, and I think we basically just have to merge it.
00:01:02.126 - 00:01:11.810, Speaker A: But this one is yours, Ansgard a fee market. Oh, and then Micah left a bunch of comments yesterday.
00:01:13.430 - 00:02:27.354, Speaker B: So basically I think the status was that I don't think there was really any remaining open issue. My first time really actively working on the spec, so I was a little bit hesitant to just push for it being merged, but I think it should basically be ready. But then yesterday Micah left a couple comments, and I think the only one really that still has to be resolved is I think people disagree around this one constant, which is the minimum data gas per blob. So I think basically there are three different positions. The original intent of introducing the constant, I think that came out of some conversations that Vitalik and I had was just to have it basically be basically normally non binding law bound, where basically the idea would be that this would only ever be relevant in case a in the very initial phase of the EP going live, there's not yet demand. So like a one time kind of just a floor, or if there ever was some network difficulty or something, for some reason, for a while there couldn't be any blobs. So the price would fall that it doesn't fall all the way down to zero or one.
00:02:27.354 - 00:02:56.786, Speaker B: Basically, like right now, the normal 1559 basically can go all the way down to seven way, and then whenever the network would recover, it just takes a while to ramp back up to normal levels because you have subfloor that normally is irrelevant, but is hit whenever these conditions happen that's a bit higher, then your ramp back up is faster. And that is somewhat relevant because the ramp up period, of course, means that you have a sustained period of just double x network load.
00:02:56.818 - 00:02:57.014, Speaker A: Right.
00:02:57.052 - 00:03:44.526, Speaker B: Because you always hit the limit, not the target. And we're talking here like an order of an additional 30 minutes or so. I think right now, basically the ramp up with a constant would be something like 15 to 20 minutes coming from all the way to the floor to some reasonable price level. Without the floor, if you go all the way down to one, basically it would be something more like 45 to 50 minutes. Then there are some people who actually want to make this be like a really high floor that is actually binding and kind of has some opinionated approach of trying to prevent spam. I personally very strongly opposed to that, just because I think we shouldn't be opinionated what considered spam and what doesn't. And then there are people like Michael very strongly want this concept to be one because they think anything other than one is basically invalid opinion by the protocol.
00:03:44.526 - 00:03:51.766, Speaker B: So anyway, this is kind of like one niche last conflict to be resolved. I think once that is resolved, the.
00:03:51.788 - 00:04:00.810, Speaker A: PR should be right to merge, I guess. Does anyone on the call have a strong opinion about where this should resolve?
00:04:03.390 - 00:04:30.740, Speaker C: My feeling is this basically. So generally I would agree with first, I think it's very unlikely in its current form to be a binding constant, like even very soon after the whole thing is launched, just because I think there are just stupid applications that don't even have to do with blockchains that would use these. This is cheaper than my roaming data fees. So let's be honest, it's nothing.
00:04:31.910 - 00:04:32.660, Speaker A: Okay.
00:04:37.530 - 00:05:38.570, Speaker C: I think it's unlikely to be really like a problem with spam and practice. The reason why it makes a little bit of sense to be opinionate in this case. So where I would contradict Anska is that in a way, we are actually subsidizing something here, right? So we are introducing this new thing and it is, it's, we don't have a way to scale it just yet. And so, like, I think like at initially, it's very likely that essentially the rest of the Ethereum network is subsidizing this new thing that we're introducing in order to get roll ups off the ground. So that's why I think it's actually not crazy to just add a lower bound to that. And the kind of lower bounds that we're discussing are basically still very cheap.
00:05:42.270 - 00:05:43.020, Speaker A: Yeah.
00:05:43.390 - 00:06:42.910, Speaker C: Another way to see this, by the way. Is that roll ups will in practice actually have much higher costs to use this. Like if you look at the pre computation, the pre compile, it's 50,000 gas at the moment. So in practice that will be a lower bound, at least for ZK roll ups that will be much higher than this. And so you could also argue that it's kind of unfair that applications that just want to file share on the Ethereum blockchain or, I don't know, or restore the jpegs for nfts on the Ethereum blockchain will have this super cheap data because they don't actually need to connect it to the actual blockchain in any way. I don't have a super strong opinion on this. I would prefer raising the current lower limit by about 100 x, which would still make it very low in my opinion.
00:06:42.910 - 00:06:58.050, Speaker C: And it basically would remove some loud people on Twitter, which I think shouldn't be like our major concern. But I think it's a relatively trivial change and I don't see any downsides.
00:07:00.230 - 00:07:01.780, Speaker A: You have your hand up again.
00:07:02.150 - 00:07:36.494, Speaker B: Right. Two things. I don't love that approach. Personally. I would prefer, and I think there's broad support for lowering the target and maximum amount of blobs, or in the case of the updated fee market, basically the target and the maximum amount of data gas. And I think that makes a lot of sense, basically starting with even something as low as basically one two or two four as target max for blops for the first rollout of the EIP. I think that makes a lot of sense.
00:07:36.494 - 00:08:05.734, Speaker B: And that already basically very much limits any impact of potential spend, but in an unappininated way. So I would much prefer that, but as more kind of like how to move forward from this. So I'm wondering what we could do is I could update the pr to set this constant to one inside the pr, and then we could have a separate pr changing that and have the discussion over there so that we would remove it as a blocker for the fee market. PR. Would that make sense?
00:08:05.932 - 00:08:10.680, Speaker A: Yeah. And by one you mean one way, right? That's right. Yeah.
00:08:11.050 - 00:08:19.930, Speaker B: But just again, as basically something we're like, okay, now if we want to actually set it to something that would be an opinionated choice to be made in a different location.
00:08:21.630 - 00:08:34.770, Speaker A: I think that makes sense. And the thing with one, because with 1559 we have this weird integer math, which means if it gets below seven, it can't go back up. Do we have the problem with that here?
00:08:34.920 - 00:08:36.098, Speaker C: There's no problem.
00:08:36.264 - 00:08:36.834, Speaker A: Okay.
00:08:36.952 - 00:08:45.502, Speaker B: No, we don't because we have the accumulating so that they can keep accumulating and at some point it will start impacting.
00:08:45.566 - 00:09:10.640, Speaker A: Okay, yeah, I think I would move to do that just because this PR has been open for over a month. If we can just take out all the contentious bits, move them to another pr. There's like three other comments by Micah that are just like descriptions, so I think probably makes sense to just resolve those. But the one with the actual content gets decided, we can just have a different pr and discuss there.
00:09:12.850 - 00:09:14.160, Speaker B: Sounds good to me.
00:09:14.610 - 00:09:19.440, Speaker A: Plus one to that. Anything else on the femarket PR?
00:09:23.010 - 00:09:47.160, Speaker D: Just a quick question on that. It sounds like nothing. Has substances changed since the devnet was released at least, or is there something, were there changes made since? This is Roberto, by the way, I left a comment saying when I put in the changes, there's been so much back and forth, I've lost track of whether there were any substantive changes since we last implemented it.
00:09:48.670 - 00:10:03.418, Speaker B: No, I think. I think you right. When the first devnet with it came out, there were like some last minute changes before then, but that was before Defcon. Since then there have been no subset of changes and then none planned.
00:10:03.594 - 00:10:06.638, Speaker D: Okay, great. Perfect. And it sounds like we're up to date. Thank you.
00:10:06.804 - 00:10:47.340, Speaker A: Cool. All right, sweet. So yeah, let's split this one out into and merge the actual fee market bit, and we can deal with the minimum fee separately. Okay, next one. Vitalik had this comment about adding modulus node or modifying the pre compile, such as, we can take the modulus as an argument. He's not here, but I'm curious if anyone has strong opinions on that and if that's something we should be adding to the spec now. Because if so, we probably want to notice sooner rather than later.
00:10:52.430 - 00:10:58.414, Speaker D: This is probably a question that's best for the l two developers. I don't know if we have many.
00:10:58.452 - 00:11:00.720, Speaker A: Of them on the call today. Right?
00:11:01.890 - 00:11:30.380, Speaker E: Yeah, as I understand it, this is more of a user experience improvement and not something that's really necessary for at least l two s in particular. We can just keep track of what the current modulus is across upgrades of four four four in the future and it's not necessary, but it is a nice to have. I guess that's like my take on it.
00:11:30.990 - 00:11:45.390, Speaker B: Would it introduce an extra trust assumption where basically now it would have to be part of L2 governance to update the modulus they use? Because if not, they could kind of falsify proofs?
00:11:47.410 - 00:12:08.614, Speaker E: Yeah, sort of. We already do have specifications that our user rely on particularly to make challenges. So it would just be part of that. Whenever l one makes changes to this, we would make the corresponding change in our specifications and then users can.
00:12:08.812 - 00:12:09.174, Speaker A: Yeah.
00:12:09.212 - 00:12:21.540, Speaker E: So in a sense, yeah, it would be part of our governance process. I don't think that's a bad thing or it's a blocker in any case.
00:12:23.930 - 00:12:33.960, Speaker A: And is this basically used in the same way by optimistic and ZK roll ups, or is there like a difference in need or requirements on this front?
00:12:41.710 - 00:12:59.230, Speaker D: Wouldn't you need this though, if you wanted to eventually freeze updating of your contracts? Are you always going to be in this state where you'll be able to kind of make these changes whenever there's an update to say, the version hash?
00:13:03.030 - 00:13:13.360, Speaker E: Ideally we should, but it'll be much easier if L1 just handled that.
00:13:16.850 - 00:13:26.894, Speaker C: Yeah, I mean, that's a nice thing about being able to access the modules from l one. If we can do that, then we could at some point have roll ups.
00:13:26.942 - 00:13:27.540, Speaker A: That.
00:13:29.370 - 00:13:37.800, Speaker C: Don'T need any upgradability functionality, but can update to a new version hash, which would be really cool.
00:13:42.590 - 00:13:49.180, Speaker A: Is it possible to say, add this in the next hard fork? So say we implement 4844 as is.
00:13:50.990 - 00:14:02.126, Speaker C: But there would also be a very simple change to 4844, which is to simply make the point evaluation pre compile, return the modulus in addition to the.
00:14:02.148 - 00:14:07.700, Speaker B: Result, or take parameter and fail or that, yes.
00:14:09.350 - 00:14:17.380, Speaker C: Well then, yeah, I guess you could do that, but then you would also need, where does the contract that calls it get that value?
00:14:18.390 - 00:14:32.250, Speaker A: So if it returns it on like a successful evaluation, that seems like a small change now. And I assume that's not something we could do in a separate hard fork.
00:14:34.030 - 00:14:38.458, Speaker C: Basically, yes. It would be good to do it now, I guess.
00:14:38.544 - 00:14:57.250, Speaker A: Yeah. Is this literally like a one line change in the, I don't have a feeling for like, is this a value that's already being used as part of evaluating the pre compile, or is there more work to expose it as the output?
00:14:59.190 - 00:15:02.418, Speaker E: It is already being used in the pre compile, yeah.
00:15:02.504 - 00:15:09.080, Speaker A: So that is just a question of exposing that as a return value while you're making that call, right?
00:15:09.530 - 00:15:10.280, Speaker E: Yeah.
00:15:11.690 - 00:15:34.330, Speaker B: Just to briefly come back to kind of the question of whether to take it in or return it. Though conceptually it seems cleaner to me to have it in as an input value because the point evaluation pre compile already takes in an external proof that has to be provided from outside. So it basically would just, in a way, it would just extend the proof format to also include the modulus.
00:15:34.490 - 00:15:34.958, Speaker A: Correct.
00:15:35.044 - 00:15:51.780, Speaker C: But it would mean that the proof size goes up from 48 to 80 bytes. So is that a good idea? That seems like why? It just seems like some useless data that you have to pass in now every time.
00:15:52.470 - 00:15:55.870, Speaker B: But isn't it the same if it is returned?
00:15:56.030 - 00:16:21.754, Speaker C: Well, if it returns it, I mean, it's just some value in your memory, right? Like if you don't do anything with it now, I assume that 90% of contracts for now will not care. This is really for the hardcore. I want to make non upgradable contracts, which will probably come even like in one or two years at the earliest, and only they would actually read this value from memory and the others can simply ignore it.
00:16:21.792 - 00:16:22.090, Speaker A: Right?
00:16:22.160 - 00:16:33.440, Speaker C: Like you don't have to do anything just because some value is in memory. So it seems like this is less invasive as a change.
00:16:34.690 - 00:16:50.550, Speaker B: Of course there could be an argument change. There could be an argument to be made that we kind of want to encourage trustless architectures. So making it more explicit. But yeah, maybe that's adding extra cost.
00:16:50.620 - 00:16:55.720, Speaker C: Also seems like a stupid thing. I mean, what you're suggesting adds an extra cost for everyone.
00:16:56.490 - 00:17:08.778, Speaker B: No, but that's not right, right, because smart contracts that just trust that could just hard code, like they don't want to be future proof hard code the value and then just upgrade for the.
00:17:08.784 - 00:17:17.470, Speaker C: Other set adds that cost. If they return it, then they can simply use the return value and feed that to their zero noise proof verification.
00:17:18.050 - 00:17:25.200, Speaker B: No, but you still need from the outside, someone to basically say, we expected the modulus to be this. Can you check whether that.
00:17:25.510 - 00:17:32.626, Speaker E: Well, we do currently have assertions that each point provided to the precompile fits in the.
00:17:32.728 - 00:17:57.194, Speaker C: So either way, that's a different thing. That's independent. No, I'm contradicting what Anska says. No, you do not need some external oracle telling you this modulus like it's simply, yes, you do need it for the proof. But the contract would only get a proof, a proof for the pre compile, a denominator proof for the roll up.
00:17:57.232 - 00:17:58.250, Speaker A: State update.
00:18:02.110 - 00:18:32.130, Speaker C: And they would feed that modulus into the witness, sorry, not the witness, the public inputs for the roll up update proof. And so if you get it from the contract, then no, you never need to pass it in from the outside. This data does not need to get into the call data. Okay, so yes, what I'm suggesting is a real efficiency improvement. I mean, maybe tiny, I don't know what the other costs are. They are very likely to be much larger.
00:18:32.210 - 00:19:12.900, Speaker A: But anyway, so I guess say we went with that is it worth it for somebody to draft a PR to the EIP about basically what it would look like in the EIP and how also l two s would use it. And then we can discuss the pr async and make a decision in the next week or two if we want to merge it. But it seems like clearly this is at least worth considering. And I think if we had a specific pr against the EIP, we can share it not just with optimism, but with the other l two teams and just get feedback on that before we merge it.
00:19:20.710 - 00:19:22.226, Speaker E: Yes, let's do that.
00:19:22.328 - 00:19:30.780, Speaker A: Yeah. Dancrad, does either of you have the bandwidth to do. Oh, Ensgar has a thumbs up. Nice.
00:19:31.950 - 00:19:47.294, Speaker B: That was just for the idea, but I'm not so sure. I'm tuned in very much to give a lot of motivation so I can do the actual spec change, but on the motivation side, I'm not super tuned in.
00:19:47.492 - 00:19:49.360, Speaker A: Denkrad, can you help with that?
00:19:51.410 - 00:19:52.350, Speaker C: Yes, sure.
00:19:52.500 - 00:20:09.110, Speaker A: Okay, sweet. So Denkrad and Anscar, and then, yeah, we can just put it in front of the different LT teams, get some feedback on it, and if it's a small change, then we can include it in the next couple of weeks. Sweet.
00:20:10.330 - 00:20:11.686, Speaker C: Okay, next one.
00:20:11.868 - 00:21:00.850, Speaker A: Terrence had an update on the sync specs. So basically, the idea I believe we discussed in Devcon was we couple blobs and blocks for kind of gossip and quote unquote, recent sync, but then we decouple them for historical sync. Yeah, I see. There's some conversations on the pr. I think at Devcon, everyone was pretty much on the same page here. Any other thoughts, comments on this? One thing with that I was still curious about is whether we're still planning to sign the blob sidecar. I think that was a little up in the air.
00:21:00.850 - 00:21:30.140, Speaker A: Like, if we're going to gossip them together, then RPC, we don't necessarily need to do the signature verification. And if the blocks have references to blobs inside them, then the signature of the block would in theory be enough, maybe proto.
00:21:32.160 - 00:22:19.160, Speaker F: So I did discuss this change with Danny and others at. Indeed, we can do it either way. We don't need the signature. It's mostly a performance thing where verifying the signature might be cheaper than verifying the commitment. And so if we don't want to allow spammers to give us lots of different bad data, maybe it's better to have a signature to check first before then verifying the commitments. But I want to hear some numbers from people that benchmark this type of thing. Otherwise, I rather just simplify the protocol and remove the cincher.
00:22:26.080 - 00:22:28.270, Speaker A: And how should we benchmark this exactly?
00:22:31.200 - 00:23:00.790, Speaker F: I think someone here working on libraries might already have numbers on just the cost comparison of verifying the commitments that come with a blobside car versus just verifying a single Bos feature. I think it's strongly in favor of the BoS feature there. I'm just not sure if it's significant enough to be a kind of DOS factor or if we want it one way or the other.
00:23:06.200 - 00:23:30.230, Speaker A: Got it. And I guess, yeah, we want to keep this pr open until we have that. Okay. And, yeah, I see you basically have a comment, Fred, about that in the pr. And Terrence was saying he doesn't think we need the signature for the sidecars. Right.
00:23:32.280 - 00:23:43.480, Speaker F: Right. You can verify the sidecar matches the beacon block itself just by verifying the commitments. The commitment verification cost is the main concern.
00:23:45.900 - 00:23:59.230, Speaker A: On the execution client side, I think we got comfortable with doing that. Signature verification or the commitment verification. It was something like three to four milliseconds per commitment verification. Is there any reason why we'd expect it to be different on the consensus side?
00:24:05.060 - 00:24:07.216, Speaker E: Sorry, Josie, could you repeat the question?
00:24:07.398 - 00:24:26.200, Speaker A: Oh, I was just saying, on the execution layer, we got comfortable, from a DOS perspective, doing the KZG commitment verification. I'm just wondering whether there's anything different about this on the consensus side that would make us get to a different answer in terms of instituting the BLS signature as a workaround.
00:24:32.170 - 00:25:18.420, Speaker E: Basically for execution, we went with the approach of just announcing transaction hashes so the client has the. I guess the client is free to pull them whenever. And if it becomes like a DOS issue, they can start disconnecting peers in consensus. We don't have this luxury because we couple the sidecar with the beacon block, and if we think the beacon block is valid and the proposer is valid and whoever's sending them are valid, then we would always, like, in the worst case, always attempt to verify the sidecar so we don't get to choose, or at least we're not as flexible as execution to choose whether or not to verify sidecars or not.
00:25:25.470 - 00:25:40.160, Speaker F: It's still attributable to the pair. If the sidecar that comes with beacon block cannot be matched against that beacon block. So it's probably fine to remove the same chair and just rely on the commitment check.
00:25:40.610 - 00:25:41.360, Speaker E: Yeah.
00:25:48.370 - 00:26:01.940, Speaker A: Okay. Can someone just comment on the PR after this call? So Terrence knows of this discussion and can probably make the changes on it?
00:26:03.270 - 00:26:07.890, Speaker F: I think the PR already removed it. So we just continue pr ss.
00:26:09.210 - 00:26:17.190, Speaker A: Okay. Anything else we wanted to change or discuss on a PR specifically?
00:26:23.050 - 00:27:01.060, Speaker E: Okay, yeah, well, one last thing. I did have like a comment on the PR and how exactly we want. One sec. How exactly we want to sort of gossip the cupboard blocks and sidecars. Like, what are we doing with the old topic? Is it going to be deprecated from now on or is it still going to be used? It wasn't quite clear to me how we go forward from this. I left a comment on the PR, so always discuss that async as well.
00:27:02.150 - 00:27:22.520, Speaker F: I don't think we should keep the OD topic around with only the blobs or only the beacon blocks. We should just have one topic. Little reason to couple them is for consistency. By creating more topics we only create or we keep the h case that we wanted to get rid of.
00:27:24.590 - 00:27:27.500, Speaker E: Yeah, that makes sense to me.
00:27:30.990 - 00:27:55.940, Speaker A: I had another question. I'm not sure if there was a comment about this on my PR, but since we're going to couple blobs and gossip, then are we thinking of more tightly coupling the blocks and blobs in the execution engine beacon node API? Because obviously you'll need both to broadcast anything.
00:27:59.380 - 00:28:36.060, Speaker F: With the ancient API, there's an expectation that the engine always has the blobs that match the block that is being produced. So even if there's an inconsistency, it should be trivial to get the blobs. Whereas on a period pair you might have an entirely different mesh of pairs per cos of Toknik. So you don't have this guarantee that the data is there. We could couple them, but we don't really have to. And for now, I think it's nice to stay compatible with the existing API.
00:28:36.800 - 00:28:37.212, Speaker A: Okay.
00:28:37.266 - 00:28:39.980, Speaker F: Instead of introducing a new version of the method.
00:28:40.560 - 00:29:20.450, Speaker A: Yeah. Anything else on this pr? Okay. And then Terrence had another one, basically proposing 18 days the to keep the blobs. Based on some conversations we had at Devcon, it seemed like two weeks was the upper bound everyone felt comfortable with. I suspect 18 days. Maps. Yeah, it maps to a neat number of epochs.
00:29:20.450 - 00:30:04.558, Speaker A: Yeah. Does anyone have a strong opinion about this or disagree? At the very least, does anyone think it should be longer? I know some people were arguing for even shorter, but I think. I haven't heard anyone argue that it should be longer. So if not, I think we can probably just merge this change, and if we want to make it even shorter, we can do that in the future change. Okay, I will take silence as a yes. So I'll let parents know. And then the last.
00:30:04.558 - 00:30:51.440, Speaker A: Oh, sorry. Yeah, sorry. Yeah. I was just generally wondering about why 18 days, I guess. Is it supposed to just encapsulate the longest fraud proof periods we might imagine? Or is it supposed to be like a precursor to how long you need to retain data in full, sharding for proof of custody or something like that? Or is it supposed to be longer than the week subjectivity period? Right. As I understand it, it's the longest fault proof. And say you were not online, you had to sync a new node from scratch, and you wanted to participate or retrieve some data.
00:30:51.440 - 00:31:29.062, Speaker A: Two weeks felt like that. And then the other thing was as well say there was a weird consensus issue that happened on Mainnet. And for some reason, to resolve this consensus issue, we wanted to have blobs still live on the peer to peer network. Two weeks is the period where we generally think we can solve pretty much any issue on Ethereum. So it gives us some room there. Cool, thanks. But it's a very soft metric.
00:31:29.062 - 00:32:02.120, Speaker A: There's not like a hard requirement. Okay. But yeah, I think we can all agree 18 days is an upper bound. We can change the spec to that and go from there and then the storage requirement. Like Prism said, at one meg target, this would be 137 gigs. It's extremely likely that we have a target that's like, well below that. So less than 100 gigs, I would say.
00:32:02.120 - 00:32:36.582, Speaker A: And. Sweet. Okay, so those are all the actual Pec changes for 4840. Sorry, just quick going back up one on the coupling beacon box. And bobs, is there like an action item there? Just someone want to follow up with Terrence just to give him a nudge that we discussed and decided on it. Yeah, that was what I was planning to do. Yeah, I'll let him know and then I'll send him this recording as well so he can have the context of the conversation.
00:32:36.582 - 00:33:07.802, Speaker A: I think he's probably the best person to just move those prs forward. Yeah. Um, sweet. Okay, so the next one, basically, this one's, like a bit tricky where the idea of, like, how do we rebase 4844 on Capella? Tim, do we want to talk about the cryptography? Oh, sorry. Yes, actually, I didn't miss that one. Yeah, you're right. Okay.
00:33:07.802 - 00:33:34.020, Speaker A: Yeah, this is George's pr about the cryptography. So I thought. Yeah, there were some reviews on that one. I think, Roberto, you added it to the agenda and you had some questions recently on the pr. So. Yeah. Do you want to take a minute to walk us through where things are at?
00:33:34.790 - 00:33:40.962, Speaker D: Yeah, I don't know if I had any particular opinions on it. I just wanted to make sure everyone was aware of it. It was non contentious.
00:33:41.106 - 00:33:52.970, Speaker A: It looks fairly straightforward to me then, grad. I see you were part of the reviews a few days ago. Sure.
00:33:53.040 - 00:33:54.940, Speaker C: Sorry, what's the question?
00:33:56.670 - 00:34:06.910, Speaker A: I guess anything outstanding on the pr by George about the cryptography API update and the Fiat Shamir logic?
00:34:10.210 - 00:34:45.820, Speaker C: Okay, so it is ready. There's one very small question on whether we need domain separators and Dimitri will still look into that, but the change for that will be trivial. And actually there are already placeholder variables for that. So it would simply assigning value to these placeholders and even if libraries don't implement them as they are, which the change is still trivial. So I think we should merge that PR and then like Dimitri will still tell us whether he thinks we should add.
00:34:48.750 - 00:34:59.150, Speaker A: Guess. And I guess, yeah, Kev was also having a look. So opinion Kev. Making sure. I don't think Kev is here. Right? Yeah, no. So pinion Kev.
00:34:59.150 - 00:35:01.280, Speaker A: Oh, you are. Sorry.
00:35:03.090 - 00:35:07.460, Speaker G: Yeah, I agree with dying credits, just the domain separators that are the main problem.
00:35:09.510 - 00:35:17.090, Speaker A: And I guess do we expect to have an update on that in the next few days or is it something we're going to need more time to determine?
00:35:22.970 - 00:35:38.200, Speaker G: I don't know about the next few days. I'd have to ask danker about Dimitri's availability, but yeah, it's not a blocker to the actual pr and it's only a blocker if we want test vectors fixed test.
00:35:42.410 - 00:35:55.120, Speaker A: Yeah, and the reason I'm asking is just like if there's client teams that are looking into implementing it, should they just basically look at this PR, assume it to be part of the spec effectively, and it seems like we're saying yes.
00:35:56.370 - 00:36:08.130, Speaker G: Yeah, exactly. But there are CKZG and a few libraries around that sort of just implement it and client teams just need to look at the public facing API, which won't.
00:36:08.950 - 00:36:31.420, Speaker A: Okay, that's okay, perfect. So for the client teams perspective, it's abstracted by the library and then the work to implement this would be in CKZG basically. Right. Okay. Anything else on this PR? Is there a person who's going to own merging that?
00:36:35.150 - 00:36:45.040, Speaker D: Yeah. By the way, as of now I don't think all that logic is in GokGz. I think a lot of it is in the clients. We're moving pieces of it over little by little, but it's not quite there yet.
00:36:45.650 - 00:36:58.660, Speaker G: Yeah, I forked your go Ethereum branch and I was modifying the API. I just haven't pushed a PR yet. I pushed one to Prism Infi's branch, but not to yours yet.
00:37:02.790 - 00:37:06.690, Speaker A: Proto. Sorry Roberto, if you want to go and then Proto.
00:37:07.110 - 00:37:11.880, Speaker D: Okay, I'll have to take a look at that pr. I haven't seen it yet, but that's great.
00:37:14.890 - 00:37:15.426, Speaker A: Proto.
00:37:15.458 - 00:37:40.990, Speaker F: Yeah, I do think Gok has the necessary methods though not as nicely grouped together as here. We merged the PR that upstreams the federate polynomial in a federation form. So I think we're complete now. Maybe I'm missing another method.
00:37:44.530 - 00:37:45.680, Speaker A: I'll take a look.
00:37:46.050 - 00:37:47.860, Speaker D: If I'm missing anything I'll let you know.
00:37:49.670 - 00:38:07.160, Speaker G: There was an actual issue that I opened up yesterday that I think GokzG needs to basically check that, remove the, check that field elements are canonical. I think the issue is.
00:38:11.450 - 00:38:11.814, Speaker A: Free.
00:38:11.852 - 00:38:13.000, Speaker G: 00:57.
00:38:16.670 - 00:38:20.670, Speaker F: Is there any particular reason why we need to remove validation?
00:38:21.890 - 00:38:36.820, Speaker G: Yeah, so if I remember correctly, Mophie said that when a blob is canonical, it's not up to the cryptography to decide whether a blob should be canonical. It's up to the person that's encoding the data.
00:38:39.350 - 00:38:52.870, Speaker F: Specifically, if the data is out of range, the bytes have a value that does not fit in a field element. Shouldn't it just be inferient?
00:38:55.850 - 00:39:36.070, Speaker E: Yeah, that's the point I was trying to make. The data is allowed to be out of range. It's whatever the output of the encoding that should be basically fit in the field element. And Kev's point was that there are cases where you could have different data mapped to the same encoding. Well, I guess it all depends on the coding. In that scenario, I would argue that the coding is invalid and useless and incorrect. But the point was the cryptography shouldn't be able to verify that the data was encoded correctly because that's all up to the user.
00:39:36.070 - 00:39:49.020, Speaker E: We can still have the field element checks to make sure that it fits within the modulus. But the data itself, it's not something we can check because it's already encoded, if that makes sense.
00:39:49.950 - 00:40:34.760, Speaker F: So I would say that if the user wants specific data to be valid with respect to the crypto functions, then they can just apply the modulus or cut off the bytes that are the bits out of bound. There are many ways to map data to some point in a specific integer range. I'm not sure if we should create an expectation that people can just encode data however they like outside of this range and then still compute commitments over.
00:40:41.130 - 00:40:46.520, Speaker E: Was that make sense, kev? Make sure that we're in the same.
00:40:46.890 - 00:41:01.360, Speaker G: Yeah, I understood it as Proto was agreeing with what I was saying, but you agreed as well, so I'm a bit unsure, but we can take it offline. I can post the link to the issue as well here.
00:41:09.430 - 00:42:02.840, Speaker A: Anything else on the pr? Okay, think that was it for all the proper spec changes. We have this draft pr by Mophie about rebasing 4844 on Capella. And I guess the background here is that the current devnets kind of implement four four over Bellatrix. And I think the sort of rough consensus we had reached was that it makes more sense to have this rebase on top of capella instead. But then that means it might be tricky to do some of the testing while Capella is not fully implemented in all of the clients. So yeah, Mophie, do you want to take a minute and walk through what your pr proposes?
00:42:04.300 - 00:42:58.830, Speaker E: Yeah, it really doesn't propose much anymore. The original draft basically rebased four four four and capella, but added introduced a feature flag to disable capella specific state transitions based on feedback in the pr. I think we shouldn't try to enshrine such flags in the spec. So the latest revision basically removes that. It's simply rebates on Capella, but there is a section at the bottom of the spec for testing that outlines the necessary functions we want to stub out for EIP four four four specific testing. That way we retain the capella containers and concepts, but no withdrawals or withdrawal interaction actually happens when testing EIP four four.
00:43:02.080 - 00:43:20.050, Speaker A: Got it. And so that basically means that capella is a sort of like no up fork that happens and the clients run through it, but then they don't have to have withdrawals implemented in order to test the four four four changes, is that right? Yeah. Okay.
00:43:22.980 - 00:43:42.452, Speaker E: I'm still not sure if. I guess it depends on client team's implementation and how their code bases are structured. I'm not 100% sure if this would be feasible depending on how client teams implement those functions that I've outlined to be stubbed.
00:43:42.596 - 00:43:42.952, Speaker A: Right.
00:43:43.006 - 00:43:46.330, Speaker E: Also, this doesn't solve the issue of.
00:43:49.040 - 00:43:50.124, Speaker A: Yeah, I don't know.
00:43:50.242 - 00:43:52.220, Speaker E: I guess we'll find out during testing.
00:43:56.560 - 00:45:14.230, Speaker A: There's a couple of client team contributors on the call here. I think if you all can review this pr, that would be great. And I think we should definitely bring it up on the CL call next week as well, so that it gets the attention of all the different fine teams there if it hasn't in the meantime. Yeah, I'm from Lighthouse. I would generally say that we really support this because having the consensus objects in the EIP 4844 fork look like what they're actually going to look like will on net reduce a lot of work for us, because otherwise, if we have a 4844 without withdrawals fork, I guess, and just Capella fork without four eight four, and then this new set of consensus types that might be the most accurate in the future, this sort of equates to us having to support three different forks. So if we know withdrawals are going to be included, then this change reduces work for us by eliminating a fork, essentially, yeah. Okay.
00:45:14.760 - 00:45:26.810, Speaker B: I think for Techu we're not opinionated. We do all of our development on main behind feature flags, so we don't have fork issues. So either way is fine on that.
00:45:29.280 - 00:46:36.164, Speaker A: Sweet. And then Dan, you said, yeah, you've managed to implement it this way on Lodestar, and Terrence is aware of this because he's commented on a pr. So maybe what's the other client? Yeah, just maybe getting Nimbus to have a look at this would be good. And then yeah, it seems like if Nimbus doesn't have like a strong objection to this, we could, we could probably move forward. Anything else on this? And I guess, yeah, the assumption here is like any interrupt testing whatnot we would do moving forward would likely be using this format of like a stub capella, which is different than what we've used for the devnets historically. Okay, next up. Yes, one thing I just want to check in on.
00:46:36.164 - 00:47:53.330, Speaker A: So for the KGG libraries, obviously we have CKGG. There's bindings being developed in go. Is anyone aware of a client team that for whatever reason can't use any of the libraries that are being developed or that need specific bindings or something that does not exist for them yet? I don't think rust bindings exist yet, but I know there's a couple of people interested in working on them. Okay, so rust bindings for KCG library, likewise, I think for Nim. Sorry, I was saying I think the same situation for Nim. Right, so Nim and rust both need bindings for the KZG library. And there's a question for nethermine.
00:47:53.330 - 00:48:14.420, Speaker A: Okay, so there's progress as well on a net, but it's not there yet. Okay, so we have net in progress, Rust and Nim missing, but then every other client team is fine. Yeah.
00:48:14.570 - 00:48:31.450, Speaker F: On net side, we just need to update according to the new pr, which includes simplification of the API and we will integrate like that. We do not use old API version we want to use the new one.
00:48:33.340 - 00:49:30.640, Speaker A: Okay, sweet. Anything else on the KZG libraries? And I guess, yeah, we can figure out offline for the rust and Nimbus implementation how to best get those done, unless someone wants to volunteer for them here. And it seemed there is a rust library in progress. Yeah. Okay. I guess next thing I want to cover, there's a bunch of different client teams here. I'm curious to hear just generally where teams are at with their implementation and if they have any blockers or things that they think everyone else should be aware of.
00:49:30.640 - 00:49:52.242, Speaker A: Yeah, I guess I'll go from the order I see on the video. Dan, I see you've started like a lodestar pull request. That's right. Yeah, it's all going finding lodestar. There's a bit of a question about.
00:49:52.296 - 00:49:57.734, Speaker D: How we're going to use CKZG. We need to generate typescript bindings, but.
00:49:57.852 - 00:50:14.810, Speaker A: I'll figure that out when I get there. I'm basically up to the networking, so I have all the types and params and config in there. Hopeful that I can get the networking and then the blob verification done.
00:50:14.880 - 00:50:15.500, Speaker D: This.
00:50:19.390 - 00:50:24.000, Speaker A: Okay, just going through the list. Ben, any updates from the Teku side?
00:50:24.530 - 00:50:31.870, Speaker B: Yeah, we've barely started on this yet. In the next couple of weeks we'll get off the ground.
00:50:33.190 - 00:50:34.030, Speaker A: Yep.
00:50:34.190 - 00:50:37.060, Speaker B: It shouldn't be a huge lift. I think.
00:50:41.350 - 00:50:42.450, Speaker A: Mophie.
00:50:48.170 - 00:51:12.590, Speaker E: Not much work since I guess the Devnet, which is still based on Bellatrix structs. Actually, Terrence has a branch on EIP four four four. And I think we would want to start moving development to his branch because it contains the latest capella structures and basically it's more in sync with prism upstream.
00:51:13.170 - 00:51:48.474, Speaker A: Nice, Sean. Yeah, so since Devcon, we've mostly been focusing on essentially Mophie's pr to rebase 4844 on Capella. Then Poon has also been working on unifying the gossip topics. And I think we're pretty far along. So we're hoping to join the next testnet with the Cadellus trucks as well. Yeah, that's about it from us. Cool.
00:51:48.474 - 00:52:13.030, Speaker A: Jerry, we didn't start working on it yet. I'm assuming after today's planning we'll start. Sounds good. We didn't hear you. You came off mute, but we didn't hear.
00:52:17.180 - 00:52:39.230, Speaker F: Yeah, I was mostly working on guiding guys to work on that KZG library. So I would say we probably will not start much on the client side until we have a bit more work done on the KDG library. I think so we'll join a bit later.
00:52:39.920 - 00:52:42.780, Speaker A: Got it. Alexe?
00:52:45.380 - 00:53:19.690, Speaker F: Yeah. We have work in progress. It's quite important for us to have some milestone. I mean, what should be on Devnet three? Like that. And we will align to that. And the idea is to join this network and provide some functionality, but I'm not aware of what certain peers will be included. What do you want guys to see there like that?
00:53:21.500 - 00:53:57.880, Speaker A: Yeah, I think I was going to sort of finish up with the third defnet. I feel like next week, we should probably spend most of the call on that. This week. There's, I think, a bunch of open issues that we need to clean up and get merged into spec. And then I think next week's call, we should be able to say, like, hey, this is basically the scope for the next Devnet. Does that make sense? I suspect every team has at least one week of work to get at least the basic full implementation and rebase on Kipala. Okay.
00:53:57.950 - 00:54:15.612, Speaker F: This rebase is a bit disturbing for me just because it's new for me, because we have withdrawals implemented, like part of Shanghai fork, as far as I remember. And probably if we will rebase, it will cause some issues for us.
00:54:15.746 - 00:54:16.380, Speaker A: Maybe.
00:54:16.530 - 00:54:17.790, Speaker F: I need to check.
00:54:18.160 - 00:54:44.340, Speaker A: Yeah. So I think if teams should definitely look at rebasing all the four, four four stuff on top of Shanghai or capella, and then all the prs we kind of discussed today, but then I think next week we'll be in a spot where we have, like, we're able to more crisply define. These are the sets of things we want to hit, if that makes great. Yeah. Yeah. Sweet. Thanks, Roberto.
00:54:46.360 - 00:54:51.224, Speaker D: Yeah, I've just started looking into Eragon. I'm going to be working on that through the next week.
00:54:51.422 - 00:54:51.880, Speaker A: Nice.
00:54:51.950 - 00:54:53.850, Speaker D: Haven't made a lot of progress yet, though.
00:54:55.100 - 00:54:59.530, Speaker A: And I think, Marius, you're the other one.
00:55:00.060 - 00:55:01.160, Speaker D: Sorry, I didn't hear you.
00:55:01.230 - 00:55:25.010, Speaker A: Oh, sorry. I was just. I think Marius is the only other person working on a client implementation. Oh, are you still here, Marius? Connecting to audio.
00:55:25.350 - 00:55:42.460, Speaker C: Yes, hello. Yeah, we're working on a bunch of other stuff at the moment, so we don't really have a stable withdrawal branch that we could rebase on top.
00:55:45.470 - 00:55:45.882, Speaker A: Today.
00:55:45.936 - 00:56:20.600, Speaker C: I started looking into the go bindings for CKZG. That's going okay. There's a bunch of issues with it right now. So regarding withdrawals, as I said, we don't have the normal withdrawal functionality in the code yet. We have a branch, but I'm not sure how far along that one is.
00:56:21.450 - 00:57:12.912, Speaker A: Got it. Anyone else working on a client implementation that I missed. Okay, we have about a minute to go, but I wanted to make sure we also cover this. One of the big things we're working on for testing is this idea of having a large blocks that we send full of call data on the network and see how the network handles those as a way to gauge our blobs viable from a peer to peer perspective. Dan, do you want to take a minute or two and talk through where things are at there and what are the next steps? Yeah, sorry for the big block experiment.
00:57:13.056 - 00:57:28.488, Speaker D: Yes, we're just trying to figure out kind of like the logistics of actually how we would submit. How do we actually fill up these blocks with two megabytes of data. So I think we found there's some issues with mempable propagation if you try to submit like a really large transaction thinking maybe try mev boost.
00:57:28.504 - 00:57:30.012, Speaker B: But the issue is that that's extra.
00:57:30.066 - 00:57:41.008, Speaker D: Integration work and not all validators run mevbboost. So I think the latest suggestion was from Tim to try to submit to validators directly. So I don't know exactly how we.
00:57:41.014 - 00:57:44.556, Speaker A: Would do that but yeah, just literally.
00:57:44.588 - 00:57:55.952, Speaker D: Figuring out how to actually make sure we get our transactions are the ones that get picked and trying to get as close to the full two megabyte block limit as possible consistently through in a row.
00:57:56.096 - 00:58:06.120, Speaker A: Right. And then yeah, Maris, this might be a good question for you. So get limit how big the transactions can be gossiped in the transaction pool.
00:58:09.260 - 00:58:10.010, Speaker C: Yes.
00:58:13.340 - 00:58:15.692, Speaker A: Matt confirmed this morning. I think it's one hundred and twenty.
00:58:15.746 - 00:58:19.790, Speaker C: Eight k. Yeah, we can patch that.
00:58:21.680 - 00:58:28.370, Speaker A: Is there a way? No. So if we wanted to do this on main net. So we want to submit big blocks on main net. Yeah.
00:58:30.100 - 00:58:31.696, Speaker C: You want to submit big blocks on.
00:58:31.718 - 00:58:35.650, Speaker A: Mainet with call data? Yeah. No.
00:58:38.520 - 00:59:06.844, Speaker C: Okay. We can submit big blocks on our local network but we cannot do this on main net. No, this safety feature is there for a reason and removing it would be.
00:59:07.042 - 00:59:22.368, Speaker A: Yeah, I'm not saying we should remove it from get and we can take this offline, but basically starkware did it a few years ago. How did they do it would be the question that I have.
00:59:22.454 - 00:59:26.236, Speaker C: Well, you need to control the block producers.
00:59:26.428 - 00:59:34.708, Speaker A: Okay. So that's the way it's like basically send them through either flashpots or through some staking pool or something like that.
00:59:34.874 - 00:59:37.830, Speaker C: I mean you're saying one block producer, right?
00:59:39.560 - 00:59:40.820, Speaker A: Well, it depends.
00:59:42.600 - 00:59:44.070, Speaker C: How many you want to do.
00:59:44.460 - 00:59:45.210, Speaker A: Sure.
00:59:45.820 - 00:59:52.436, Speaker C: I see. Okay, so what is the guest block limit?
00:59:52.548 - 00:59:56.444, Speaker A: It's like one hundred and twenty eight k. And we would like guest does.
00:59:56.482 - 01:00:00.220, Speaker C: Not produce blocks larger than 120 transactions.
01:00:00.880 - 01:00:06.232, Speaker A: Transactions. So you could do it by sending you a bunch of 128k transactions.
01:00:06.296 - 01:00:07.228, Speaker C: Yeah, why not?
01:00:07.394 - 01:00:09.964, Speaker A: It doesn't guarantee they all get included in the same block.
01:00:10.012 - 01:00:10.610, Speaker F: Right.
01:00:10.980 - 01:00:16.144, Speaker C: It doesn't guarantee that. But if you bid a bit more for gas than they should be.
01:00:16.262 - 01:00:24.550, Speaker A: Yeah, that might be the simplest way. Yeah. And then we can probably wrap up.
01:00:25.160 - 01:00:42.840, Speaker B: Right. I just wanted to say that given that we haven't yet had any post merge load testing in general, I think we should be careful. Maybe not to immediately start with two megabyte loads, but kind of slowly ramp up to there. So just in case we notice that even at network is already struggling.
01:00:43.280 - 01:00:44.030, Speaker A: Right.
01:00:44.640 - 01:00:47.550, Speaker B: This should be a moment to stop. So I think basically.
01:00:49.440 - 01:00:55.980, Speaker C: I don't see much of a scenario where this would cause permanent damage to the network.
01:00:56.400 - 01:01:04.576, Speaker B: Well, it might. Just in case. Just because could just watch this and be like, oh, it's easy to actually bring down the ethereum network and it.
01:01:04.598 - 01:01:33.608, Speaker C: Does cause permanent damage because it increases the history. And as long as we look, we are talking about a few megabytes. We're not going to add gigabytes to the history. No, this is not a valid argument. So I think it's not. Yeah, you could flag that up to someone, sure. But like, I mean, honestly someone who's, I don't know.
01:01:33.608 - 01:01:40.990, Speaker C: Anyway, we need to be resistant to that attack, so I don't think we have to be that careful. I'm pretty sure it's fine.
01:01:41.840 - 01:01:54.240, Speaker B: Well, but I mean, even temporary network instability, I think we all kind of have pretty high reliability standards for mainet. I don't know. I personally would feel much more comfortable.
01:01:55.620 - 01:02:03.140, Speaker C: If it takes a 1 mb blocks to bring it down, then we're already not satisfying that standard, in my opinion.
01:02:05.240 - 01:02:09.670, Speaker A: At the very least we can just use vanilla guest like.
01:02:11.900 - 01:02:46.656, Speaker C: 1 mb blocks will already bring a bunch of validators down. That's just how it is. Like continued 1 mb blocks. There are a lot of validators that don't have the bandwidth required for this. What's a lot? Sorry, what's a lot? Like 1% or even 5% does not network quality. Let's just be honest. It's not a problem.
01:02:46.656 - 01:03:05.352, Speaker C: Fine, they miss some attestation, who cares? It's nothing. They lose a cent per adestation. Look, it's not a concern. Like Mainet is not going to go down because of this, just because we've designed it so that 30% can go offline without anything happening.
01:03:05.486 - 01:03:09.704, Speaker A: Sure, but if we can minimize however much we take offline, I don't get.
01:03:09.822 - 01:03:12.920, Speaker C: Why we need to start testing this on Mainet.
01:03:14.000 - 01:03:16.892, Speaker A: Yeah, we won't start on Mainnet for sure.
01:03:17.026 - 01:03:32.716, Speaker C: No, of course we can do this test in like a month on Mainet. I don't care about this. But we shouldn't. Yeah, but testnets are not going to tell you anything interesting because everyone runs their testnet nodes.
01:03:32.828 - 01:03:42.950, Speaker A: Well, you would hope. That's the thing. If this works on testnets, then you can move to Mainnet. But if we break something on a testnet, it's much better to have broken it on a test net first.
01:03:45.160 - 01:03:47.104, Speaker C: I feel people are being over cautious.
01:03:47.152 - 01:04:45.876, Speaker A: Here, but, yeah, fine, I think. Yeah, we're already over time. My feeling is just gradually ramping up the size of things we do seems to be the best way to go. And we probably don't have to deal with this being bigger than the get mempool transaction cap for now. Even when we move to Mainet, we can probably do like a first test with something like, we can probably do a first test with something like a bunch of 128K transactions with like a relatively high priority fee and hope that most of them get in the same blocks. Perry, talking about what metrics we want to track, does it make sense to just move that to next week as well? Because I don't think that go first. We're sort of out of time, but I don't know.
01:04:45.876 - 01:05:23.580, Speaker A: Yeah. Do you want to take two minutes and maybe talk about it? But I suspect to get a full list, we'll probably need to chat async. We can do it async then that's fine. Okay. Yeah, so I guess, yeah, let's in the next week discuss this in the telegram group about this. If anyone wants to be part of the telegram group talking about this experiment, reach out to me, I'll add you. And then I think next week for this call, if we can have the issues we talked about mostly resolved, a cleaner spec or target for Devnet three and then a sort of set of metrics to target for this experiment.
01:05:23.580 - 01:05:49.110, Speaker A: That'd be really good. Anything else before we wrap up? Okay, yeah. Thanks, everyone. Appreciate staying on a couple extra minutes and talk to you all soon. Thanks, everyone. Have a great day. Thanks, Tim.
01:05:49.960 - 01:05:50.960, Speaker B: Thanks, everyone. Bye.
