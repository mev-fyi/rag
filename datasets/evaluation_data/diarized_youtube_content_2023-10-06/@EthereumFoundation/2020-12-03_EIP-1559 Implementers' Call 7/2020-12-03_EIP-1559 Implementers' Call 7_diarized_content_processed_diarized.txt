00:00:00.250 - 00:00:00.800, Speaker A: You.
00:00:02.210 - 00:00:19.966, Speaker B: So, yeah. Welcome everybody, to 1559. Implementers call number seven. Now, like I said, we have a bunch of things on the agenda. I tried to list them in order to go through them, so maybe we.
00:00:19.988 - 00:00:22.160, Speaker C: Can just jump in.
00:00:22.530 - 00:00:48.060, Speaker B: First item, tim Roughgarden, who's joining us today, has put together a pretty extensive economic analysis of 1559. He published it, I think, two days ago. So hopefully people have had time to digest it since then. But maybe, Tim, do you want to take a few minutes to just give a kind of short summary of the analysis and then if people have questions or comments, we can go over those?
00:00:50.750 - 00:01:37.242, Speaker A: Sure, tim happy to. Thanks for the invitation to join the call. I don't want to go on too long because I want it to be driven more by people's questions, but maybe just sort of I'll just quickly say kind of the structure of the report. So after describing just recapping how 1559 works, so giving a fully precise, fully detailed definition of exactly how it works, the report talks about how to think about a market of Ethereum transactions. So EVM computation is a scarce resource. And so ultimately, users or creators of transactions are vying for that scarce resource. So ultimately, that's the point of the transaction fee mechanism, to figure out who gets access to that resource and what the price is.
00:01:37.242 - 00:02:21.702, Speaker A: And sort of the purpose of that discussion around Ethereum. The market for Ethereum transactions is primarily to clarify what 1559 can be expected and cannot be expected to accomplish with respect to the level of transaction fees. Because I know there's a lot of concern in the community about high transaction fees. And the main point I wanted to make there is just that when sort of demand for EVM computation fire outstrips its supply, you're going to have high transaction fees. It really doesn't matter what mechanism you use. 1559 does help with things like absorbing short term demand spikes. And so as a result, you should see lower maximum transaction fees in periods of high demand.
00:02:21.702 - 00:03:17.610, Speaker A: But again, when you have much more demand than supply, no matter what the mechanism is, you're going to see persistently high transaction fees. So then with that out of the way, then I start to analyze 1559 in two ways. So the most technical part of the report, sections five and six, that's really analyzing the incentives of 1559 at the timescale of a single block. So thinking about, say, miners who only care about the revenue that they get from that one block and are not thinking about making short term sacrifices to reap rewards later on, similar users who are just focused on getting a transaction in the current block and are just trying to figure out how to bid. And so sections five and six outline several game theoretic guarantees that you might want a mechanism to have. So miners should be incentivized to do what you would like them to do. Users should be incentivized to bid in some sort of obvious optimal way.
00:03:17.610 - 00:04:05.594, Speaker A: And then also you'd like robustness to off chain agreements so that users and miners can't easily collude, for example, to sort of basically steal money from the protocol. So those are sections five and six sort of listing those three properties. First price auctions, the status quo, has two of those three properties, but 1559 has all of them, or at least almost two in particular. Those sections include a mathematical definition of what sort of easy fee estimation might mean, or what a sort of good UX might mean. And first price auctions do not satisfy that property. And the 1559 mechanism does satisfy that property, except during periods where the base fee is much too low, which would signify that there's been a very rapid increase in demand where the base fee hasn't had a chance to catch up yet. So those are five and six.
00:04:05.594 - 00:05:19.474, Speaker A: They're the most sort of technical sections in the report. Section seven, I discuss attacks or manipulations you'd be worried about that take place over longer timescales. And so for this, you're usually thinking about a cartel of miners because anyone, or at least mining pools, because any one miner is probably mining blocks sufficiently infrequently that long term strategies aren't that useful. But if you have a well coordinated mining pool, or if you have a cartel of miners with a large amount of hash rate, all of a sudden you start sort of worrying about what they might do if they strategize over example, could they manipulate the base fee downward to reduce the fee burn? And so section seven, from what I could tell, it seemed like this was the one that's generated the most kind of discussion on, say, Twitter thus far. So maybe let me just sort of say what I was trying to say with the section. So the first goal was just to sort of revisit first price auctions, the status quo, and ask the same question. So, like, what could miners in principle do by colluding over long time scales? And what do they actually seem to do? And so there we identified collusive strategies that would in fact be in miners interest if they implemented them.
00:05:19.474 - 00:06:09.806, Speaker A: And then we observed that miners do not seem to actually do sustained long term collusion. And I'm not in a position to conclusively say why that is. I just sort of listed a whole bunch of reasons that I thought of and that people have told me about. Here are the reasons why we might not see this kind of sustained collusion with first price auctions. And then I go on to observe that that whole list of reasons that apply to first price auctions apply equally well to the 1559 mechanism. So there do seem to be impediments to collusion by miners now under first price auctions, and nothing about 1559 makes it easier for miners to collude. Now 1559 may make miners more motivated to collude because now they sort of have this additional incentive of evading the fee burn.
00:06:09.806 - 00:06:56.180, Speaker A: So the point of this section is just to say that in some sense the cost of colluding, I don't see any reason why that would go down with 1559. It's as difficult as before. However, it is true the benefit may go up to miners of pulling off the collusion. And I try to be very careful in the report of not predicting whether we'll see significant minor collusion or not. And the final section of section seven, the Caveats, explicitly discusses this point miners may be more motivated to collude than they ever have been before. And so in particular there may be types of collusion we have not seen under first price auctions which we will see not because they're easier to pull off, but just because they're more motivated to do it. Section Eight is something I thought would have generated a little more discussion than it has thus far.
00:06:56.180 - 00:07:36.062, Speaker A: The first part of section eight is just to clarify that you can't really do sort of the fee burn without the base fee or vice versa with an exception. And so this is section 83, this is one of the two alternative designs I discussed in the report. So the first alternative design is so it's really crucial for the game theory the role that the feeburn plays. What's really important is to withhold base fee revenues from the miner of the block which generates those base fee revenues. So it has to be withheld from the miner who mines the block. The simplest way to do that is with a fee burn. And of course there's lots of other reasons why people like a fee burn as well.
00:07:36.062 - 00:08:25.818, Speaker A: But section 83 points out that the game theoretic properties are really just as good as long as you pay those base fee revenues to somebody else. For example, and this is a proposal I've seen by Vitalik and possibly others. For example, you could instead pay the base fee revenues to miners of future blocks. So for example, the next thousand blocks you could spread it out equally and then there is no fee burn. Basically just each block now has kind of a bonus added to its block reward depending on the sort of base fee revenues from the previous, say 1000 blocks. So that's one of the main alternatives suggested, which I actually have not seen discussed so far. And then the other one is a version of the 1559 mechanism where instead of the tips being user specified, you hard code them into the mechanism.
00:08:25.818 - 00:09:12.778, Speaker A: And this has some problems like you would expect sort of off chain tip markets to emerge. I give no opinion on whether that's a deal breaker problem or not, but you would expect that to happen. On the other hand, it's definitely simpler to have hard coded tips and it has some nicer game theoretic properties, which just explaining that we get into the weeds, but so there are some nice aspects of that second alternative design that I call the tipless mechanism in the support. Then the last section of Section Eight is I talk about the base of the update rule. And this again, I sort of seen people coming up with very reasonable requests that this should be analyzed from a control theoretic perspective. I totally agree. I think actually it's probably a quite easy control theory problem if you found an expert.
00:09:12.778 - 00:10:05.290, Speaker A: But in any case, arguably the most sort of arbitrary feeling aspect of the 1559 proposal is the specific way that the base view evolves over time. So the functional form, all the choices are sort of natural. You can see why one would make them or why they're a natural guess, but the functional form is sort of arbitrary one plus an adjustment factor. There's two magic numbers in the rule, sort of the one Eight, which controls how rapidly the base speed can increase or decrease. And then also there's a magic number of exactly how much bigger should the maximum block size be compared to the target block size. So in that section, Section 86, I try to clarify all of the assumptions that are baked into the current update rule and what are some different dimensions that they should be experimented with over time. And it may be hard to iterate on the update rule until there's actual data from a real deployment.
00:10:05.290 - 00:10:55.486, Speaker A: Just from the armchair, it's hard to have a compelling case of why something else would be better than the current one. But I just wanted to heads up that probably this will want to be revisited over time, like the various other parameters that are revisited with every network upgrade. And then in the last section, Section Nine, I talk a little bit about the other benefits of 1559. So the report focuses just on sort of good UX easy fee estimation. But of course, there's lots of other reasons people are excited about 1559. So I just talk about what those are in Section 91, most notably the fee burn, but also kind of preventing economic abstraction, having a reliable measure of kind of the current gas price that's hard to manipulate for use in smart contracts. And then the final section discusses the escalator both kind of as a standalone proposal and also how that might be integrated into 1559.
00:10:55.486 - 00:11:05.220, Speaker A: So that's sort of the executive summary of everything discussed in the report. And obviously people have specific questions about parts of it. I'm very happy to address those.
00:11:09.430 - 00:11:09.954, Speaker D: Thank you.
00:11:09.992 - 00:11:14.450, Speaker B: Yeah, that was great. Does anyone on the call have any questions, thoughts?
00:11:16.970 - 00:11:54.180, Speaker C: I have a question which isn't necessarily something explored in the reports, but I'm quite curious about your intuition with regards to it. The question is, do you have any thoughts on what you think would happen with if you have two parallel markets running during transitionary periods because one of the suggestions has been to have both the first price auction accepting those kind of transactions with the 1559 in parallel. And I'm curious if you might have an intuition about maybe some emergence effects that might happen or I don't know, just curious about your thoughts on it.
00:11:54.630 - 00:12:26.198, Speaker A: Yeah, that's a good question. So, just to clarify, so the sort of transition plan, I've seen a few different things discussed. My understanding is that Plan A is you would have a period where legacy transactions would be converted sort of interpreted automatically in the 1559 format by taking the gas price and interpreting it as sort of both the fee cap and the tip. Is that the specific proposal that you're.
00:12:26.214 - 00:12:50.034, Speaker C: Talking about, that one as well. But there's another level to this, which is the bun wasn't intended. But when you talk about L2 systems, because many L2 systems that we see think about also having a fee market running on top of the base layers fee market. So there's the transitional periods where you do this translation, but also dual markets when you have second layer markets running on top of it.
00:12:50.152 - 00:12:56.210, Speaker A: I see. So you're saying interactions between this change at layer one versus what happens upstream.
00:12:56.790 - 00:13:02.118, Speaker C: But also inside the layer one itself. I guess these are two separate problems, but those are the two that I see.
00:13:02.204 - 00:13:46.238, Speaker A: Okay. Yeah, I agree. They're separate things. The discussion I've just seen around, I feel like some good thought has gone into and discussion has gone into how to manage the transition by the 1559 team. And I have not seen in some ways I'm not in the trenches with the implementation, so I can't comment on that. But from what I've seen, the plan seems very reasonable. And one thing that's nice about it, potentially, one would hope so first of all, people don't have to wallets don't have to change initially if you have these sort of support for legacy transactions.
00:13:46.238 - 00:14:33.860, Speaker A: And then you would hope that there would be economic pressure over time for everyone to switch over to the 1559 format because there's really basically two parameters to play with the tip and the fee cap in 1559. And if you don't bother to pay attention to that, you're kind of stuck with this much sort of more restrictive way of bidding where you just set the one gas price. So that's one thing that I think is nice about that transaction. First of all, it seems clear you don't just want sort of an immediate sort of hard stop where the legacy transactions aren't accepted. And so this seems like a really nice way to have them around for a while. But at the same time there is an economic incentive for them to hopefully go away over time. The layer one, L2 interaction I'd probably have to know more details about.
00:14:33.860 - 00:15:34.520, Speaker A: I assume that happens in various ways for various L2 s and so I needed more details to talk about it at length. I will say I mentioned this briefly that one of the side benefits of having this base fee is it should make it easier to sort of know what is like the typical gas price at any given moment, namely the base fee, unless you're in a period of rapidly increasing demand. Whereas if you just kind of looked at ether scan right now and you sort of look at a block and you're kind of like, well, if I wanted to associate a single gas price with this block, what would it be? I mean, you could use the minimum, the average, the median, et cetera. There's these statistics you could use, but there's worries about those could be manipulated if people knew what statistic you were using. Whereas the base fee is hard to manipulate and again, outside of sort of sharply increasing demand should give you a reliable measure of the sort of current gas price. So my hope would be that that would be a quite useful additional functionality or really an improvement for interactions with L2 down the line.
00:15:35.370 - 00:16:04.386, Speaker E: So I think hi, Tim. I think the easy way to think of the L2 thing is the L2 chain generates blocks at a higher frequency than the layer one chain and it's using within its own domain the exact same algorithm. So it takes a bunch of transactions, it generates blocks, it then publishes those blocks on L One in an L One transaction and that's basically all there is to it.
00:16:04.488 - 00:16:05.970, Speaker A: Okay. Thanks, Rick.
00:16:09.030 - 00:16:37.100, Speaker F: I suspect what Fred might be referring to is before the first version of 1559 we had two transaction pools at the same time. So within a single block you'd have basically two half the gas was dedicated to 1559 transactions, half the gas dedicated to legacy transactions. And so I suspect the question might have been what kind of interactions do you expect to see there? I see if we did that instead.
00:16:39.390 - 00:16:43.260, Speaker A: Interesting. So would there be perverse incentives to.
00:16:44.510 - 00:16:51.946, Speaker F: Yeah, because it gets definitely more complicated for users because now you have to decide which is better for you. Do you want to do 1559 transaction?
00:16:52.058 - 00:16:52.720, Speaker A: Right.
00:16:54.210 - 00:16:58.546, Speaker F: Do you want to try for the gamble of legacy transaction lower gas price?
00:16:58.728 - 00:17:13.510, Speaker A: Right, exactly. My recollection is this is part of why correct me if I'm wrong, but my sense was that this idea was set aside in favor of this kind of default interpretation of legacy transactions in part because that problem goes away. Is that right?
00:17:13.580 - 00:17:16.054, Speaker F: Yeah, that problem goes away and a couple of others as well.
00:17:16.172 - 00:17:26.170, Speaker A: Yeah. It's not something I've thought about deeply but the latest transition plans that I've seen, to me seem like a pretty smart approach.
00:17:28.110 - 00:17:31.950, Speaker D: Are there any drawbacks with this automatic conversion?
00:17:35.650 - 00:17:50.980, Speaker G: So I'll have some comments on this transition, but perhaps Michelle can if I'm pronouncing your name correctly. So Michelle wrote a notebook on the transition period between 1559 and legacy transactions. Maybe you can share it now.
00:17:54.070 - 00:18:23.934, Speaker B: Yeah, sure. I guess just before we go, are there just other areas of questions that people had about the report? Because I think yeah, the legacy conversion and whatnot is a whole other can of worms, I think we can cover it right after. But I just want to give this space if people have other questions that they wanted to bring up about the economic analysis first yeah, I have just.
00:18:23.972 - 00:19:35.700, Speaker F: One sorry, go ahead. So I have one question. I think it's particularly slightly outside of the report, but still very relevant. So if we look only in the transaction market as something that exists by itself, and the transaction value is always external, there's no other market to relate to. It's all fine, but what if we have a decentralized finance market where miners can hatch the cost of collusion, like of the attack, if they can actually benefit from the higher fee burning or the fees going down? In particular, we actually work on the project where miners would be able to make financial transactions, where they would benefit if the fees go higher or lower. And if they can actually make big bets on this, then they can cover the cost of attack. Did you consider this kind of coexistence of two markets, the decentralized finance market and the transaction fee market?
00:19:36.150 - 00:20:29.030, Speaker A: Yeah, not explicitly. I find it quite interesting, and in particular, I think where it ties into the report is the discussion around how to get minor buy in into the proposal. You could argue to what extent is it necessary, and then if you agree that it's necessary, you can argue about how you might want to do it. And so I think having some kind of financial instruments so that you can argue that miners are going to win either way, especially if it's something where they're particularly well positioned, maybe to make smart bets on them. You could imagine that sort of speeding up sort of adoption, sort of lowering the current sort of pushback that I believe the community is seeing from miners.
00:20:31.530 - 00:20:45.354, Speaker F: Great, thank you. When you were analyzing the sorry, which section? I had a question and I just lost it.
00:20:45.392 - 00:20:45.786, Speaker D: Damn it.
00:20:45.808 - 00:20:46.860, Speaker F: Someone else go.
00:20:48.030 - 00:21:19.270, Speaker E: Yeah, I would just like to make a brief comment. As the person who proposed the two stage having two transaction types and two transaction pools, the purpose of that was not game theoretic. It was to force the removal of the dead code path of having one transaction type be interpreted two different ways. Just to clarify.
00:21:21.850 - 00:21:48.350, Speaker C: There'S also something else that may be worth noting. I'll just brush it over quickly, which is you can see the L2 transaction fee market interact in a similar way to this first proposal of two transaction pools, because you can see it as part of the layer one gas being used and reserved for a separate transaction fee market. So I think the interactions in those might be comparable, but not with this new transition period scheme.
00:21:49.810 - 00:22:30.766, Speaker E: Yeah, that's an interesting idea. I mean, that's a really interesting idea. I think the difference is that the well, I think there's two points, and I'll say the nicer one first. Let's say you're using the operator of the L2. Whether that's a federation or an individual, whatever, they ultimately have some discretion, right? So they have within their protocol the ability to not participate in the next layer one block. It is a segmentation, but the two different pools are under different authorities. So that's a pretty big difference.
00:22:30.766 - 00:22:45.460, Speaker E: And then the sort of corollary to that is 1559 doesn't stop the L2 operators from bribing miners, which is probably what they'd end up doing, practically speaking.
00:22:47.910 - 00:22:48.900, Speaker C: Good point.
00:22:59.550 - 00:23:58.810, Speaker B: There's one more question. I think you already kind of answered this, Tim, but Nick Johnson, who's been one of the, I guess, friendliest critics of 1559 and really wanted to see your report he posted on Twitter yesterday. I'll share the actual tweet in the comments here and I'll try to just summarize his question. Basically, in section seven, four five, you explain that miners could make these cartels, but it hasn't happened before. And he says this is probably not a sound way to think about it and basically that the incentive structure is still the same under 1559 as it is now, but it fails to consider the magnitude is very different today. A cartel benefits from the difference between monopoly pricing and market curing price. But under 1559, it would benefit to the tune of the difference between the monopoly price and the cost price, which is much larger.
00:23:58.810 - 00:24:10.970, Speaker B: Yeah, I guess you mentioned earlier that the cost of collusion kind of stays the same, but the benefit goes up. I assume that would be kind of the same answer here to Nick's concern.
00:24:11.130 - 00:24:31.080, Speaker A: Right. So I tried to be careful on this point in the report. Maybe there's a way I could have written it that it would have been clearer. But I guess I would point to if you look at the very first sentence of section 74 is when I start classifying different types of minor collusion, the very first sentence of the section is I offer no prediction on whether there will be collusion under 1559.
00:24:32.330 - 00:24:32.694, Speaker F: Okay.
00:24:32.732 - 00:25:41.386, Speaker A: If I don't, so then what do I do? I just say let's let's sort of make a do an observational study of the status quo under first price auctions brainstorm possible reasons why we're not saying collusion and then assess to what extent. And then for each of these apparent impediments to collusion, do any of those impediments break down because of something specific to 1559? And I argue that. No. And if sort of in the top ten takeaways it's, number five, the assertion is not that collusion is as unlikely under 1559 than first price auctions? I didn't say that, and I very intentionally didn't say that. I just said the impediments are as strong. Meaning like, the problem is as difficult, as far as I can tell, for miners to collude under 1559 as it is now. Now, again, I'm not saying that collusion is less likely for exactly the reason that Nick mentions, which is that they might see either just because the economic reasons are more stake, or they may feel betrayed by the community and therefore sort of less altruistic.
00:25:41.386 - 00:26:18.570, Speaker A: And so that's covered in the section seven four six. So right after the caveat section, and there again, there's a sentence that says this strong negative reaction I was referring to your survey, your questionnaire by Tim. This strong negative reaction may galvanize miners to sustain collusion to a degree not yet seen under the status quo. I completely agree with Nick's point. I tried to make that explicit in the report. Perhaps it should have been positioned a little differently. So it sort of stood out more.
00:26:18.570 - 00:26:21.260, Speaker A: But I actually don't think there's any disagreement there.
00:26:22.270 - 00:26:23.020, Speaker B: Cool.
00:26:24.850 - 00:27:34.974, Speaker F: This gets into the question I was going to ask, which is and I can just talk about instead. I think the magnitude is off by a pretty large margin here, I believe. Because right now, if miners were to 51% collude, they would make double the block reward plus transaction fees with one five, 5951 percent collude can still make double the block reward, and they get a little bit more transaction fees on top of that. And while we have seen some big spikes in transaction fees periodically, the baseline is still way below the block reward. And so it's like if you conclude with 51% and you can make $100 million, or now with 1559, you include and make $101,000,000. And I feel like that order of magnitude is nowhere near enough to tip the scales just because the gains from colluding with for colluding and manipulating 1559 transactions are just so small compared to colluding. Just with any type of transaction mining.
00:27:34.974 - 00:27:51.320, Speaker F: Just by censoring 49% of miners, you double your money. It's easy money right there. So if you can collude, you can make way more money doing other things. And so that's where I feel like the real argument here should be, is that the order of magnitude is just too small.
00:28:01.070 - 00:28:01.482, Speaker C: Okay.
00:28:01.536 - 00:28:27.620, Speaker A: Yeah, so I think you might well be right. I guess in the report, I didn't want to presuppose how the base C revenues would compare to the block reward. I just felt like that's fair and reasonable. I thought any prediction I made on that point, I might just look quite foolish a couple of years from a maybe that was the main thing I wanted to say.
00:28:34.410 - 00:28:49.340, Speaker B: Any other final questions for Tim? Okay, yeah. Thanks a lot, Tim, for sharing all this. This was pretty helpful. And I'll make sure to link the report in the notes that we have for this.
00:28:51.470 - 00:29:24.794, Speaker A: So I'm going to have to sign off. But I mean, just sort of a general comment this was not like some report I envisioned just like issuing into the world and then never discussing with anybody. It's really a report I made. The point of it is to be helpful to the Ethereum community and so if there's follow up questions or anything that would make it more helpful, I'm obviously very receptive to that feedback and future discussions and what's the best way.
00:29:24.832 - 00:29:27.914, Speaker B: Maybe for people who are watching the recording to reach out to you.
00:29:28.112 - 00:29:31.420, Speaker A: So email Tim roughgarden@gmail.com.
00:29:32.030 - 00:29:33.818, Speaker B: Great, thank you very much.
00:29:33.904 - 00:29:34.780, Speaker A: Thanks everyone.
00:29:41.710 - 00:29:50.210, Speaker B: Yeah. So, Michelle I hope I'm getting your name right. Yeah do you want to go into your report around the legacy transaction simulations?
00:29:51.430 - 00:31:05.350, Speaker C: Yeah, sure. So maybe I will try to make it quick. I will give you the summary of what I did. So maybe to start with, what was the goal of this report? Of this simulation I created I wanted to answer the question how legacy transactions will be treated versus 1559 transactions by the network when 1559 is in use? I wanted to answer the question whether maybe the network will give preferential treatment to one type of transactions or other types. So I created the simulation that is based on the library ABM 1559 that was prepared by Barnabe. Sorry if I also pronounce your name wrong I introduced some changes but I use this library heavily so in my simulation I distinguish, let's say three types of transactions or maybe three types of users. So we have the Gaze users that for some reasons don't use 1559.
00:31:05.350 - 00:32:29.570, Speaker C: And when these kind of users submit transactions, these transactions have Gas Premium or Tip set to the same value as Max Fee. Then we have 1559 users that utilize 1559. But here I decided to distinguish, let's say, knife user, which always sets a Gas Premium to the same value one way. So these kind of users do not analyze transaction pool to figure out what is the optimal, the best value of gas premium. And we also have something I call clever 1559 users that look at the transaction pool and try to figure out the gas premium they should use in order to be included in the block as soon as it is possible. I forgot to say that legacy users also try to analyze transaction poll in order to figure out the best gas price. And in each iteration of the simulation, I generate the same number of legacy transactions.
00:32:29.570 - 00:33:26.674, Speaker C: Transactions from knife users and transactions from these clever users. And what is important, when we look not at the pairs but at the trio of transactions from each of these three kinds of users, they have the same value, I mean business value. The user associates with given transaction. Why? Because I want to compare, let's say apples with apples and I think that if. I have in the transaction pool one legacy transaction, one clever transaction, one knife transaction with the same business value then I can compare them in the reasonable.
00:33:26.722 - 00:33:27.320, Speaker F: Way.
00:33:29.210 - 00:33:37.482, Speaker C: And as to the conclusions, let's say the most important.
00:33:37.616 - 00:33:37.914, Speaker B: Okay?
00:33:37.952 - 00:35:10.870, Speaker C: So firstly if we look I calculate a lot of statistics and metrics so I will only tell about the basic ones. But if we look at these statistics we can distinguish phase one and phase two. By the phase one I mean situation when a base fee very quickly very dynamically grows and phase two when this base fee reaches stabilization. So in this first phase all these statistics I calculate like average gas price per block, average waiting time and many different they change very dynamically and it is quite even quite difficult to reason about this phase. Nonetheless, this phase is quite short and then we have this second phase when it is much easier to reason about the behavior of the network. So according to my simulation and I think it is good information when the base fee reaches stabilization transactions from all these three types of users will be included in the blocks. So we don't have the situation that for example only legacy transactions are in blocks or only 815 59 transactions are in the block.
00:35:10.870 - 00:36:39.700, Speaker C: However, in this first stage when a base fee grows quickly here situation is different because in this stage I observe that mainly or almost only these clever 1559 transactions are included in blocks. As to the and what it means is that it also means that almost only these clever 1559 users will take advantage from the lower values of the base fee when it comes to the gas price here, let's say concussions are not surprising. These knife 1559 users will pay the least. Why? Because they do not try to be clever. They simply always pay the same gas premium. Whereas clever 1559 users or legacy users who looks into transaction pool who wants to pay more to be included in the blocks, they pay slightly more. But if we compare legacy users and 59 users they more or less pay the same.
00:36:39.700 - 00:37:27.810, Speaker C: What else? I implemented very simple transaction pool. So I simply assume that I can have some maximum number of transactions in the transaction pool and when there is more transactions I simply remove from the transaction pool those divorced. What I mean by the worst I sort transactions based on the gas premium they offer to the miner. What is important, I observe Evictions from the transaction pool almost only in this initial phase. Then when base fee reaches stabilization.
00:37:30.730 - 00:37:31.046, Speaker A: There.
00:37:31.068 - 00:38:42.214, Speaker C: Are no Evictions and transaction pool is not full at all. What else? One more thing but this is another conclusion let's say that is quite, let's say natural, nothing surprising. I also calculated average waiting time of the transaction in the transaction pool. So of course these knife 1559 transactions which always pay the same gas premium needs to wait more than legacy or clever 1559 transactions to be included in the block. However, I spotted one interesting thing though. I cannot explain that now why it happened. Sometimes I observe that legacy transactions wait longer and sometimes I observe that these clever 1559 transactions waits longer in the transaction pool.
00:38:42.214 - 00:39:05.060, Speaker C: I need to analyze it more carefully to explain why it happened. Okay, so I think that those were the bullet points, the most important conclusions I noticed. If you have any questions, feel free to ask.
00:39:13.030 - 00:39:41.360, Speaker G: Hey. Hi. Yeah. I really enjoyed The Notebook, Michel. I think it was a really great use of a library actually. And I've been gotten to play around a bit with it since the start of the week. So with Fred we've been looking at how to, let's say, look at Oracles that give first price auction legacy users information about the current price that they should pay.
00:39:41.360 - 00:41:05.286, Speaker G: One piece of code that Fred added was this idea that users who are using so let's say we are after the transition, we have 1559 users, legacy users, and the legacy users are deciding their fees based on the Oracles, which is also kind of what you are doing in your notebook. So when you have these Oracles like the presence of a base fee, even though it's implicit for the legacy users, it has a sort of stabilizing effect on the Oracle. So let's say I have 50% of my users who are legacy and 50% of my users who are 1559. You can think of it as some of the users know the correct price that's the 1559 users. And so since they know the correct price and that's the price they're putting in their transactions, they're actually tilting the Oracles toward giving that price for the legacy users. So I think of it as almost like the first price auction is this boiling pot of water and the 1559 users are just throwing cold water, like lowering the temperature, so allowing the legacy users to almost like have a better, let's say, estimation of the current price in the market. Although it's a complete, it's very implicit, like, it's not direct, but it goes through the Oracle.
00:41:05.286 - 00:41:55.866, Speaker G: And that may explain also why by the end when base fee is stabilizing, you find that let's say legacy users and 1559 users are included in the block in almost equal proportion as they are when they join the market. I think the idea that we had in mind was that since legacy transaction users would be overpaying, they would tend to maybe have some sort of priority. But that's no longer true, let's say when base fee starts to stabilize, because when that happens, the Oracles will start to sort of align themselves with the base speed and provide to the legacy users the actual base. So you should kind of expect this convergence. I don't know if it makes sense and if it's maybe something that you.
00:41:55.888 - 00:42:58.490, Speaker C: Noted as well, yeah, I think that my simulation, the results totally confirm what you just said. Maybe just one comment. What you said is totally true, but only if we assume that these legacy users will not overpay too much. Because at least in my simulation, yeah, legacy users ask Oracle for the best price, but this Oracle returns the minimal price. However, if we have some legacy users who really want to pay much more to be included, to be included in blocks, then probably even if the base fee stabilizes, I think that we will see more legacy users in blocks.
00:43:01.600 - 00:44:12.070, Speaker G: Yeah, actually it's true, but I don't think it's true to the magnitude that we expect. So, for instance, most of your records are based on some kind of percentile of past transaction. So you look at the, let's say 95% top paying transaction like MetaMask, when it gives you the fast price, it's kind of like this very high percentile. But if you have like base fee, which is kind of stable, and most of the transactions, even some of the legacy users who are using the slow or medium who might be actually targeting the exact base fee, it might start even, let's say tilt the fast Oracle. So the one that would make you overpay towards the base fee itself, again, because it's sort of a distribution thing where because the fee variance is reduced in the block thanks to the base fee, you also have this effect that propagates to the Oracle itself. Unless the Oracle is some sort of fixed, let's say I make you overpay by five GUI. But I think most Oracles are based on this idea of looking at the distribution of transactions and setting the price like this.
00:44:14.520 - 00:44:59.360, Speaker C: Okay, so maybe one more thing, because it seems to me that we will see this stabilization effect only if we have a big enough number of 1559 users using the network. So here, of course, it's only guessing. The question is how it will look in practice if we have, let's say 80, 90% of legacy users and only 10% of 1559 users, I think that it wouldn't look so nice when we have 50% of Legas users and 50% of 1559 users.
00:45:01.060 - 00:45:36.270, Speaker E: Sorry, I would just like to comment on that. Yeah, I think that's an extremely good point and I think to me, I really appreciate all this research and I think it's really interesting, fascinating work. As a practical matter, if collectively the community can do something to ensure that 1559 gets adopted by someone like, say, MetaMask, then a lot of this simulation, we don't really have to worry about these corner cases, right? We just know that the majority of people will use 1559.
00:45:40.160 - 00:46:31.570, Speaker B: I was just going to add, I think we had a bunch of discussions about this in the past as well, but we can start with this neutral approach of reaching out to folks. There is already, I think, a lot of support for 1559 in the community. So step one is like, you reach out to folks like MetaMask, like Coinbase, whatnot, ask them to support this. And then step two is like, if that doesn't work in the next hard fork, do you want to add like a carrot or a stick right, with regards to gas prices or whatnot? But I think it's hard to predict in advance what the adoption rate will be and therefore to come up with a good plan for how do you get the people who you would have wanted to adopt it that are not adopting it to actually do so.
00:46:32.260 - 00:47:19.292, Speaker D: Also, I do think that there are self stabilizing incentives in the sense that the less stable the base fee behaves just because fewer people have adopted one five nine so far, the more incentive there is to actually adopt move to one five nine transactions as an individual user just because, again, with legacy transactions you tend to overpay or just general. It's less controllable. And so basically, the fewer people are using one five nine, the more attractive it is for individuals to move over to profit from the increased stability locally. And so I would assume that it very quickly kind of would converge to a situation where enough people move to over that the overall situation becomes relatively stable, at least under most. Yeah, but of course, that's hard to tell.
00:47:19.426 - 00:47:43.270, Speaker B: That's a really good point. And I think what's interesting is a lot of the projects we spoke to as part of the outreach that were managing transactions on the behalf of their users really care about giving their users the best price and the best UX. So if there is kind of an incentive to do so, I suspect we'll see a lot of projects wanting to differentiate by adding that.
00:47:46.060 - 00:48:35.110, Speaker G: Another consequence with this insight that the Oracles converge is that the more 1559 users you have, the easier it is for legacy users to keep using the legacy transaction. Like the less they would overpay because the better their Oracles would kind of tend to become. So, bouncing on what Rick said, if you get the 80% users by having MetaMask switch to 1559, then this long tail of users who are not switching, it's actually not that bad for them. They get a somewhat correct rate still. You have like of course it's kind of a gradient between if everybody is using first price auction versus if everybody is using 1559. But yeah, if most users are 1559 users, then I guess from a legacy user perspective, you might not be overpaying that much either.
00:48:35.560 - 00:49:24.280, Speaker B: And I think that's not the end of the world, right? The direction we're going in at the protocol right now is like if we have support for these 29, 30 transactions, these 1559 transactions, the legacy transactions, I suspect we'll have to carry a bunch of different transaction types for a while. I think there's maybe a more meta discussion about how do we deal with this long tail of older transaction versions that's kind of out of scope for 1559. And if we have some reasonable intuitions that there are good incentives for a large portion of the network to adopt it, I think that's probably sufficient given that we still have to maintain some types of legacy transactions anyways due to other reasons.
00:49:24.700 - 00:50:11.652, Speaker D: That actually leads me to a question I was having earlier. So in case that transition, like the initial transition to 1559 goes smoothly and there's a lot of adoption early on, a lot of people earlier basically talked about transition periods that would imply that there's like some end of the period where then presumably you would completely phase out legacy transactions. You're basically just saying that that might not be unnecessary, at least immediately or something. I was wondering, is there any important reason why you would ever want to fully phase out legacy transactions instead of just continuously converting them forever? Because, I mean, there are always these edge cases. Maybe someone is using some hardware wallet where they really don't have a way of generating transaction types or something.
00:50:11.786 - 00:51:16.164, Speaker B: The short answer is client code complexity. I guess the scenario under which it would be very helpful is if you have clients that don't want to sync from Genesis for a reason. So some people have talked about, like, regenesis things like that. But maybe a more possible or concrete thing is like, assume there's the E One E Two merge, right? Maybe people want to write clients to be like an E One engine for E two, but not sync everything since ETH One's Genesis just start processing stuff at the merge block. If you got to a point there where, say, I don't know, legacy transactions are not supported anymore, they just don't have to implement that, and it makes the client much easier to do that. So I think that's the main argument in favor. But when you talk with teams like Guest or other client teams that need to support clients from Genesis, it doesn't really make a big difference, say, to us on base if we deprecate 15 to nine transactions or not because we still need to validate all the blocks where there were legacy transactions.
00:51:16.164 - 00:51:26.700, Speaker B: So that means we need to keep that code in the client as well. But I think that the biggest benefit is you could build a client from the spot where you don't process those transactions anymore.
00:51:27.200 - 00:52:10.860, Speaker E: Yeah. At the time, my thinking was that there would just be potentially a lot of complex dynamics by keeping this old, by having two transaction types that are possible. And I just thought it was really difficult to reason about. I was having a very difficult time figuring out which one, what would happen. And so it's better to just close that door both from an engineering perspective. But as Tim points out, that kind of doesn't work because you have to replay from Genesis, but then also to sort of close that door in terms of economic exploitation.
00:52:16.260 - 00:52:28.710, Speaker F: One can also imagine a client that has so each time there's a fork block, the consensus rules change. One can imagine a client architecture where you have a separate engine for each.
00:52:32.360 - 00:52:33.056, Speaker B: Fork.
00:52:33.168 - 00:52:50.376, Speaker F: And so it'd be nice if your new engines don't have to speak, but you don't touch them. It's like your version one, you don't touch it. You maybe get security updates, but that's it. Whereas you don't want your V One code sitting in your V Seven code base, which may be completely isolated, again, depending on your architecture.
00:52:50.488 - 00:53:39.040, Speaker B: I suspect in practice, though, given the current clients that exist and teams working on them, nothing like that will happen before an E 22 merge. Happy to be proven wrong, but my hunch is this is the only kind of point at which it makes sense to change the architecture so much to get there. This is a bit of a tangent, though. Yeah. Did people have any other questions about the legacy transaction simulations? If not yeah. Anskar, I think you had some updates you wanted to share about the transaction pool management, which we spent a bunch of time talking on. Talking about on the last call.
00:53:39.990 - 00:54:51.666, Speaker D: Sure, yeah. Just for context, I've been following the 50 59 efforts loosely, but I haven't joined most of the previous implementers calling everything. I might not be fully up to speed, but basically, like, Tim and the quilt team, we talked like, I think two weeks ago or something, and he mentioned this kind of that there were some open implementation questions around mempool handling and so we kind of decided to look into that a little bit. And so I basically wrote up some of my thoughts around, specifically the sorting, because I think most of the Mempool related questions, like how to handle 1559 transactions differently from legacy transactions, really boil down to sorting. And so my basically initial conclusions, and again, those could be off. I would definitely not yet an expert or anything, but it appears to me that there's really basically two different types of sorting that usually happens in Mempool. The first one is just for miners that's like basically on the high end of transactions, having an efficient way of finding the currently, like, highest paying transactions.
00:54:51.666 - 00:55:44.214, Speaker D: And of course, highest paying, meaning those that basically have the highest effective tip. Currently, of course, you just use the gas price for that. And so currently, for example, in Geth, the way that's implemented is with like a max heap where you basically have like a partially sorted list by maximum gas price and you just traverse that to find the highest paying transactions. And that doesn't quite work for 1559 because unfortunately, of course, I had these little diagrams. But of course the observation, I think, is an old one that with 1559, the relative order of transaction can change when the base fee changes because of these two parameters. Basically for low base fees. Usually transactions are in the static period where they basically pay their maximum tip that they are willing to pay.
00:55:44.214 - 00:56:46.474, Speaker D: But then at some point they reach this kind of inflection point where the base fee becomes so high that it starts eating into the tip. They are still willing to pay for different transactions. That point is at a different location. And so when it can be that a transaction that was willing to pay a higher tip that now goes down and now basically all of a sudden is willing to pay less than another transaction so the relative order can switch and so you can't have like a static shorted data structure anymore. However, I think specifically for the question of mining, it seems to me that you can kind of find a somewhat more clever but not all that much more complex way of going about it. So the main observation that I had was basically that within this kind of what I'm calling static state where you are able to pay your full tip, right? And transactions that are all currently willing to able to pay their full tip. Those continue to have a static order because while they are in the static range, of course that's a static amount.
00:56:46.474 - 00:57:35.354, Speaker D: So the ordering stays constant and then within the declining phase where your tip is being eaten into by the base fee basically transactions within that also because it's like a linear one to one relationship, like basically one walkway in the base fee is one less way in your tip. And so that also basically means that they all shift in the same speed and so they never intersect. So transactions in that state also never kind of switch order. And so it's really just about transactions where they basically switch between those two states. And so I think what you can do is basically just have somewhat basically you can have like one partially sort of heap for the static transactions, one for the dynamic transactions. There are a few questions though that I don't think I have quite clear answers yet. So basically, what you'd have to do every time a new block comes in, that changes space.
00:57:35.354 - 00:58:35.310, Speaker D: Fee you have to kind of process the ones that now passed their inflection point there and now switch between the two states. And it's not quite clear how you could effectively remove them, because you don't actually want to do a lot of removal from these heaps. So there are a few intricacies but I think generally directionally this is like a really solvable problem. And then interestingly though, the other sorting problem in mempool is on the other side, right? Not the high pay transactions and that's only for a minor problem, but for Eviction, right? And there you want to find the basically bottom tier transactions to get rid of and this seems to me to be a little bit more complicated because under legacy transactions you again just use the gas price. But what you're kind of optimizing for is you want to get rid of the transactions that have the lowest chance of being included, right? Because those are the ones you want to drop. And previously with the static order and everything, that is a very simple decision to make. You just look at the gas price now with 1559.
00:58:35.310 - 00:59:42.494, Speaker D: Again with the dynamic order that can shift over time. It's not clear anymore, right? Just because a transaction right now would be willing to would have a lower effective tip than another one doesn't mean that it has a lower chance of inclusion because maybe as soon as the base fee goes a little bit higher, then the transaction all of a sudden is willing to pay more or something. So you kind of have to have implicit assumptions about the base fee behavior. So basically the metric you would want to use is the average or like the average effective tip that you expect, like the expected value of the effective tip of the transaction over a probability distribution of future base fees. And of course, you don't want to do it all that complicated. So the question is just can you find a simple heuristic that does something of that sort that is good enough? I mean, you don't for eviction, you don't really care if it's like an intellectually completely perfect solution. It really just has to be practical enough.
00:59:42.494 - 01:00:34.642, Speaker D: But it has to be practical enough under a lot of different paradigms. So slowly changing base fee, quickly increasing, quickly falling highly volatile, low volatility, all of these different paradigms. So basically the goal just is find a heuristic that is really robust and all these paradigms, but then also you can implement with some efficient data structure where what you don't want to do is every single time a new block comes in, you don't want to go through your whole Mempool. Recalculate this expected value for every single transaction and completely resort your Mempool. I think at least that is too much housekeeping effort after every single block. And so basically finding some heuristic that you can find some order that you only have to update slightly every single block or something. I don't know, I don't really have good concrete ideas around that yet.
01:00:34.642 - 01:01:18.974, Speaker D: I think it seems like that should also be kind of solvable though. But it's a little bit more of a complex issue. These are basically my thoughts on sorting. So there's maybe one more special case of transaction replacement. But I think transaction replacement really is not all that complex because you really only want it to be predictable by the user because transaction replacement where you just replace a transaction or pending transaction because you want to bump it. Basically I think you can just have very simple rules that protect you against Dos issues but also kind of keep the structure something. But yeah, basically I don't know, maybe I'm not sure if it was clear or something and again, I might have been missing things or just previous write ups or whatever on that topic, but that's my rough outline.
01:01:18.974 - 01:01:29.160, Speaker D: So like high end for miners, low end for eviction for all nodes, and high end you want an explicit solution, low end, just some heuristic. That's good enough. That's kind of where I'm at right now.
01:01:32.010 - 01:02:10.980, Speaker E: I like that analysis. Thanks a lot. I do have one question, which maybe I also, not being in every meeting, missed something. But when you're talking about Evicting transactions, isn't there a velocity? Isn't there a maximum rate of change of the base fee such that you could say it would be a week before this transaction could be included, or a day or there's some longer bound where you know that the velocity of base fee changes would certainly exclude a transaction from a reasonable amount of time.
01:02:12.010 - 01:02:52.480, Speaker F: Yes, there is. I personally advocate for using a strategy like that. The caveat we have to remember though is that in a time of rapidly increasing base fee, it is possible to see the transaction pool filled entirely with transactions that meet that criteria. So even if you say that evict any transaction that cannot be included in the next block, it is still possible to have a transaction pool that is entirely filled with transactions that meet that criteria and you still need to evict. So you still need a secondary Eviction strategy in that case to deal with that situation at the least.
01:02:53.410 - 01:03:53.300, Speaker D: Yeah, so I would agree that basically, like a simple yes no rule always runs into these edge cases where you can construct a situation where it's basically very close but still just below whatever base fee they need or something. So just some relative metric where you have like one value per transaction that you can assign and then you just compare and evict those with the lowest value. I think that is preferable. But I do think that it illustrates how while there is, again, there's some uncertainty where transaction order can flip, there's still a lot of structure in that it can only flip to a limited extent because the base fee can only change at a certain rate and all of these things. So I think you can still come up with sorting that is mostly stable. Once the base fee starts to change, it only changes a little bit. And so you basically only have to do a little bit of kind of updating of your sorting there.
01:03:53.300 - 01:04:04.280, Speaker D: The goal really should just be to be able to identify the worst transactions no matter how close they are to being includable or how far away, all of that.
01:04:05.710 - 01:05:08.960, Speaker F: So another thing that Light client brought up last week or week before is that if we change the minor bribe or tip or whatever calling it this week to be static. Not that I'm willing to pay this much base fee up to this much base fee, and I'm willing to pay this much to the minor. And those two are separate values. And so the total you pay is the sum of the base fee plus the minor. That greatly simplifies the transaction sorting problem, but it introduces a new problem that it greatly increases the complexity of upgrading legacy transactions to this new transaction type. So when people are thinking about this problem, if you can solve that problem, how do we upgrade legacy transactions? When the tip or the minor bribe or gas premium, whatever you're calling it, is static, then this whole problem of transaction sorting goes away and we're back to basically legacy style. Very simple sort.
01:05:09.970 - 01:06:15.714, Speaker D: I do have to say, though, because Lifetime and I, we talked about that quite a bit after that, and the impression that I got, and of course, feel free to correct me there, but the impression that I got there is that that is not indeed actually correct. Because it turns out for these decisions, you still want to do this kind of the chance of inclusion or something, and just because now it's a cliff. So basically you have a hard drop off that still kind of gives you the property that there's like a non static order, basically, because there can be a transaction that is more easy, that's basically higher paying for a long time. And then instead of gradually dropping off, it just immediately drops off to like an inclusion chance of zero, basically. But it still has this property that you can have intersections between the relative value of two transactions and so it's not in fact that it now all of a sudden basically it's a static order. Again, you still have the property that basically orders dynamic and flips. And so you kind of have to do this expected value thing.
01:06:15.714 - 01:06:24.440, Speaker D: So I personally don't actually think that basically gets rid of the problem.
01:06:25.850 - 01:06:29.910, Speaker C: So Micah, you were saying that the problem is the promotion.
01:06:30.570 - 01:06:32.760, Speaker F: I do think you are correct.
01:06:34.970 - 01:06:36.282, Speaker D: Can you guys hear me?
01:06:36.416 - 01:06:38.700, Speaker A: Can you guys hear me? Yeah, go ahead.
01:06:39.230 - 01:06:53.450, Speaker C: Micah, you were saying that the problem of promoting the legacy transaction types under that suggestion that you had is because there's now just this static fee that does it. There's no basically item that depends on a per gas basis.
01:06:53.530 - 01:07:28.652, Speaker F: Right. It just doesn't fit with the model we have for upgrades. So for upgrades, the model we have right now, of course, is we just say the legacy transaction gas price is both the base fee and the minor bribe. Both values are the same thing and everything kind of just works out magically. If these two values were, sorry, the fee cap and the minor bribe, if the minor bribe and feecap are now separate and so they're additive onto each other. So the thing you pay is now. Base fee plus minor bribe.
01:07:28.652 - 01:07:40.950, Speaker F: We can no longer just set the fee cap and the minor bribe to the legacy transactions gas price that doesn't work. I've forgotten. But.
01:07:44.840 - 01:08:36.660, Speaker D: I would also argue that the one other major drawback that that solution has is basically that you have this just the behavior, again, of basically your transaction is willing to pay a certain tip and then as soon as under the dynamic approach. Basically, usually you would have this inflection point and then the tip. You're willing to pay slowly, degrades, but you can still be included in the block. Whereas under the new proposal, basically at that point you could just no longer be included. And so, from a UX point of view, I think it is also a little bit problematic that now you could have transactions that are price wise perfectly able to get included, but they can't because of this rule. Sorry. So I'm personally a little bit skeptical of this approach.
01:08:36.660 - 01:08:44.230, Speaker D: Sorry. Something my throat.
01:08:50.240 - 01:08:51.708, Speaker B: So I guess just to make sure.
01:08:51.794 - 01:09:20.390, Speaker D: I'll go ahead, I only want to say but it is like a very interesting alternative approach to think about, because I think if I remember correctly, that was actually the one that kind of when Light client and I were talking about it, that was the one that kind of led us to realize that basically within these different stages, you still have the static order. So that was definitely like a very interesting kind of thought experiment. But I don't personally like it as an actual design.
01:09:25.520 - 01:09:40.950, Speaker B: And so, just so I understand, it seems like the next step here on the Eviction side is finding is there a good enough heuristic that we can use which might have some failure modes, but that should work most of the time. Is that right?
01:09:42.440 - 01:09:44.884, Speaker D: Yeah, that's how I would at least see it.
01:09:45.002 - 01:09:45.716, Speaker B: Got it.
01:09:45.818 - 01:10:21.100, Speaker F: I think the most important thing is that we do not have a failure mode that results in a Dos factor against clients. For the Eviction strategy, pretty much anything else is almost anything else is optional. That being said, the worst case Eviction strategy is you're Evicting from the most likely transactions to be included. Right. It's like the pathological failure mode. If you imagine that, then that can become a Dos vector. Because now clients are constantly dropping transactions that then they'd have to get fetch again as soon as they get included into the next block.
01:10:21.100 - 01:10:28.160, Speaker F: And so we do have to be careful about that. But that's really the core is don't allow Dos attacks.
01:10:29.780 - 01:10:52.990, Speaker C: Does any of this get easier to solve? I remember hearing, forgive me because this is my 1st 1559 call, but I remember hearing rumblings about potentially enforcing at the protocol level that blocks are filled first at EIP 1559 transactions. Does that solve any of this? Because you only have to relatively order them. Like only order 1559 transactions among themselves and then legacy among themselves.
01:10:59.020 - 01:11:00.250, Speaker F: I believe it does.
01:11:03.740 - 01:11:38.640, Speaker D: I'm not sure. The problem is that even within 1559 transactions, if you don't have any of these of these legacy converted transactions in there, I think within that block, you still have similar issues, at least. Maybe it's easier when most of the tip is somewhat in a similar range or something. Of course, for the legacy side of things, it will make things easier because then you have the same properties again. But I don't see at least why that would solve the issue on the one five nine side. Maybe it would make it a little bit easier.
01:11:38.720 - 01:11:39.670, Speaker F: I'm not sure.
01:11:43.970 - 01:11:54.210, Speaker C: So what if you add also the static minor fee instead of the per gas minor fee? And now can you deterministically sort those? The 1559 pool.
01:11:58.230 - 01:12:22.090, Speaker F: You'Re saying, have two transaction pools, one that is legacy transactions that once they're included in a block, they look like 1559 transactions, but the second pool is actual 1559 transactions, but they have the static gas price gas premium. Is that correct? My understanding?
01:12:23.070 - 01:12:45.330, Speaker C: No, I was suggesting well, maybe so. I was suggesting that we have the 50 59 transactions with the fixed tip, and then we just have lexi transactions, as they always were in a different transaction pool, except they can only be included in a block after 1559 transactions.
01:12:47.590 - 01:12:50.658, Speaker F: They can only fill up empty space, basically.
01:12:50.824 - 01:13:06.278, Speaker C: Yeah. You can evict them however you want, or you can evict all of them if there's only 1559 transactions. And then the 1559 transactions, as they are now, also would have the sorting problem, I think, because of the per gas.
01:13:06.374 - 01:13:20.380, Speaker F: So I don't know that we can have them be elastic. I don't know if we can have them be that second pool be elastic, because we don't know if we should.
01:13:23.790 - 01:13:29.646, Speaker C: Like, as long as you send out a block that has 1559 transactions first, that is valid.
01:13:29.678 - 01:13:35.810, Speaker F: Don't know if we should expand the block. So if the block is under full, like less than it's.
01:13:39.270 - 01:13:41.330, Speaker B: You'Re really breaking up, Nico.
01:13:43.130 - 01:14:33.480, Speaker E: Yeah. I think you would, in effect, just expand the block one block late, and I think having the 1559 take up all of the block and then have the original transaction type take up the remainder, and then if that was full, expand the block. That, I think is a really weird game where it makes sense to do all sorts of weird stuffing and price manipulation because now you can control the size of the block in this kind of counterintuitive way. I don't know that all those games are worth the algorithm benefit that you're aiming for.
01:14:34.330 - 01:14:35.142, Speaker C: Okay, cool.
01:14:35.196 - 01:14:35.558, Speaker B: Yeah.
01:14:35.644 - 01:14:42.680, Speaker C: I just remember hearing this as a suggestion, but I never heard kind of the counterargument to why it wouldn't work. But that makes sense.
01:14:46.170 - 01:15:12.150, Speaker B: Yeah. Just because we're running low on time and we still have a couple of other things to cover. Is there anything else regarding this that people really wanted to bring up now. Okay, if not I think the last big thing we had is Abdel has made some progress on generating testnets with a large state. Abdel, do you want to take a few minutes to kind of share that?
01:15:12.920 - 01:16:17.050, Speaker H: Yes, sure. So we want to see how the network would work with high block elasticity like can the network handle twice the block size as now? And to that, the first approach was to kind of fork mainnet but we don't really like this approach because it implies to do some tricky things in the code of the Ethereum clients and we don't want to merge that code because we don't want to introduce new attack vector. So we wanted to explore another approach which is to have basically to not touch at all Ethereum clients and to have another standalone service that interacts with clients and to see how quickly we could generate a state comparable to mainnet. So we implemented the proof of concept for this service. So I will show you.
01:16:18.860 - 01:16:20.056, Speaker G: So can.
01:16:20.078 - 01:17:46.150, Speaker H: You see my screen? Yeah yes. Okay, so basically we have a standalone service that will interact with the Terrarium client using the RPC endpoint and we have a few rest API. So basically API to handle tasks because it is all long running processes. So we need a way on the client side to see if the task is completed and the duration of the time of the task, et cetera. And then basically we only require to have two deployed smart contracts so one to create accounts and one to fill the storage, basically. So the first version to create account we were only doing basic transfers so without using a smart contract but it requires to handle a large TPS and this is more efficient to create a bunch of accounts per transaction. So this is why we create the account directly in the Smart contract and also you can monitor the number of accounts created and also yeah, we have the other contract that is responsible to fill the state storage and yeah, basically I will show you a quick demo.
01:17:46.150 - 01:18:54.104, Speaker H: So first I start one Ethereum client with a very low difficulty to quickly produce blocks. Okay. And then I start my standalone service that has the RPC endpoint of my Ethereum client and we have web application basically it connects to the Ethereum client and retrieves some configuration parameter. So for the moment I don't have anything deployed because I just deployed the network from scratch. So the first thing will be to deploy the two contracts required. Okay? The second one and now if I go to the configuration I can see the addresses of the deployed contract and some parameters directly queried from the Smart contract. So I have not created anything from the moment.
01:18:54.104 - 01:20:10.096, Speaker H: So I will start, let's say by creating 10,000 accounts and 15,000 entries in the Smart contract. Okay, so tasks are pending. Let's wait a few seconds. Okay, the concretion is done and the state storage is done as well. And if I create again my smart contract I can see that 10,000 accounts have been created and 15,000 entries have been created in the smart contract and I also have the last created address and to show you some results. So basically we tried several iterations, we started from ten k accounts and ten k entries in the smart contract and between each iteration we multiplied by ten and we have measured the time needed to build the states and so the last iteration was 100 million. So this is something comparable to mainnet and it took basically four days to build this large state.
01:20:10.096 - 01:21:11.812, Speaker H: So the two processes have been done sequentially. Next step will be to try that in parallel. And obviously we did some tests with a single node network. And if the approach sounds reasonable for you guys, one next step will be to set up a new 1559. Testnet and to kind of build a large state comparable to main net, we'll have to deploy multiple clients on each type bezoo netermind get and I think we should try to run this service on all clients directly rather. Than building the state and then sync with the other clients that will be more efficient to make sure we all deploy our clients to the infrastructure. And then we start to generate the state and hopefully within four days.
01:21:11.812 - 01:21:55.750, Speaker H: Or so we could be able to have something comparable to mainnet and then we could start to play with the high block elasticity because we did some test with the high block elasticity on the current testnet, but the state is very small, so we don't see the impact on large state. And we started to measure the evolution of the block production time versus the number of accounts. So it does have a significant impact actually. So it will be interesting to see how it will work with large block elasticity and yeah, that's pretty much it.
01:21:58.600 - 01:22:14.932, Speaker E: That's really impressive. I just have a quick question after you've generated that, spent the four days to compute that state, let's see, I'm sorry, it doesn't seem to say the size, the size of the DB.
01:22:15.076 - 01:22:27.576, Speaker H: Yeah, it's something like I will show you 237 gigs.
01:22:27.688 - 01:22:35.250, Speaker E: So does it make sense to create a backup of that for the respective clients so you can run more tests or do you just want to destroy it?
01:22:37.540 - 01:22:51.190, Speaker H: My plan was to destroy it and regenerate something from scratch using the tool because the time needed is quite reasonable, I guess less than a week, I don't know.
01:22:51.560 - 01:23:10.824, Speaker B: And these didn't use 1559 transactions, right? Yeah, exactly. So we should probably have one. I think we just did it with legacy but we should probably have I agree with you Rick that once we do it with 1559 style transactions yeah, we should keep that and not have everybody need to run a four day.
01:23:10.862 - 01:23:24.004, Speaker H: Process every time it does not really matter. I mean, to fill the network, we don't need to use 1559 transactions because most of the work is done in the smart contract anyway, so that won't affect the results.
01:23:24.072 - 01:23:44.228, Speaker B: Yeah, I guess. Yeah. What we want is we want the network. Once we have the large state, we want whatever network to be able. Okay, we could use that. Put that in a set of clients that support 1559 and then run the transaction generator tool. Right?
01:23:44.314 - 01:23:44.660, Speaker H: Yeah.
01:23:44.730 - 01:23:48.872, Speaker B: Okay, so I guess in that case, we probably should not delete it now. We probably should around.
01:23:48.926 - 01:23:57.960, Speaker H: Okay. So yeah, first I wanted to see if the approach makes sense for you and then we can see the next steps.
01:24:00.140 - 01:24:13.616, Speaker F: Yeah. So this is to check if the clients can handle the load at the level of the main net, right?
01:24:13.718 - 01:24:14.370, Speaker B: Yeah.
01:24:18.020 - 01:24:21.410, Speaker H: With twice the block size of the main net.
01:24:24.500 - 01:24:30.496, Speaker F: To generate like 100 million accounts because mainnet is 100 million accounts and then accounts.
01:24:30.608 - 01:24:31.124, Speaker H: Yeah.
01:24:31.242 - 01:24:35.700, Speaker B: And then there's also a smart contract which has 100 million storage slots.
01:24:35.780 - 01:24:38.600, Speaker H: Yeah. With 20 bytes per slot.
01:24:44.610 - 01:24:54.260, Speaker B: We're almost out of time. I know. Rye, you wanted to bring up 20 718. Do you think you can do that in like 1 minute or two?
01:24:54.710 - 01:25:29.070, Speaker C: Yeah, I think if someone has arguments against it, then we won't and it'll go somewhere else. But I'm hoping that it will just push through quickly. So essentially, since the writing is on the wall, 20 718 is going to be in Berlin and the whole point is to introduce transaction types. Is everyone good with having EIP 1559 transactions be a 20 718 transaction and we can just temporarily pick a value of like 15 for it and then pick what's it called like an incremental value once it's actually about to go into a hard fork.
01:25:35.170 - 01:25:48.820, Speaker B: I guess my question would be no good. How much time? Like, does it slow down people right now to add 20 718 support or not? Because we're already doing as part of Berlin, right?
01:25:49.830 - 01:26:02.514, Speaker C: Yeah, I was going to say that I think all the clients have it now, and so it would actually just simplify the encoding, decoding code paths to just have that be a type Ramil.
01:26:02.562 - 01:26:09.406, Speaker H: Can you say us if you have merged the Master branch? Because I'm not sure they merged the Master branch.
01:26:09.458 - 01:26:20.394, Speaker C: Yeah, so actually we almost completed, so we just need a couple of more hours. So today we are going to create pull request on the original Get repo.
01:26:20.522 - 01:26:31.140, Speaker H: And would you be confident to use 20 718 type transaction envelope for 1559 transaction? Have you looked at it?
01:26:33.750 - 01:26:36.034, Speaker F: Not yet. So we are rebate on top of.
01:26:36.072 - 01:26:40.206, Speaker C: Master, so that transaction type pull request.
01:26:40.238 - 01:26:41.506, Speaker F: Is not merged yet.
01:26:41.608 - 01:26:42.260, Speaker C: Right.
01:26:43.030 - 01:27:21.200, Speaker B: So maybe it makes sense. You have to wait until it's part of the Get code base, like it's actually merged into Get. I don't know what the status is and then set a transaction type and I assume we can kind of figure out async what we want the transaction number to be. Yeah, because I guess I wouldn't want to slow down the stuff on the large state testnet. If it'll take a while to get it merged in Geth, then we need to update the 59 implementation of Get and whatnot does that make sense?
01:27:21.650 - 01:27:26.302, Speaker C: Sure makes sense. Once Get goes in, then we can switch it to 20 718.
01:27:26.366 - 01:27:51.530, Speaker B: Yeah, and I guess we're kind of out of time, but the final thing I wanted to see is when does it make sense to have a follow up call? It feels like we have a lot of parallel threads, so should we have breakout rooms for any of them? Does it make sense to just have maybe a call in two weeks instead of a month so that we can follow up async and kind of share updates in two weeks? What do people feel will be like the most productive?
01:27:52.990 - 01:28:30.194, Speaker F: I think generally we should actually start planning the road to testnets and to release. So we should actually transition to the stage when we plan how to move it to Main Net instead of just analyzing it anymore. It's like overwhelming proof. Lots of different research cases that show that it's very solid. Probably like this few slightly risky points that were mentioned in the recent report. But apart from that, it would be great to start planning how to go to Mainnet all the way. So have the roadmap.
01:28:30.194 - 01:28:48.140, Speaker F: What's the first target date that we have for the release and how we get there when the clients join, what are the acceptance points like from our perspective from all the clients when we say, okay, we are ready and that would be great?
01:28:49.310 - 01:29:39.230, Speaker B: Yeah, I agree with you. It seems to me like from a research side, it's pretty derisked. The only two outstanding issues seem to be figuring out this transaction pool sorting, which is not rocket science, it just has to be done and then maybe looking at the update rule, but that's also pretty minor I think with regards to all core devs wailing until Berlin is out or at least kind of finalized. Probably makes sense before bringing it up there. So maybe I can definitely work on putting together a roadmap over the next two weeks. Maybe it just makes sense to follow up then to see how the work on the testnet is progressing and if we have a solution for the transaction pool stuff. And then how do we want to bring it to awkward devs basically after the holidays?
01:29:40.290 - 01:29:55.330, Speaker F: Yeah, I generally think that we should totally decouple it from the Berlin conversation. It will be much, much better for us as a working group because I still make this bet that it's like 10% chance that this will happen before Berlin.
01:29:56.950 - 01:29:58.370, Speaker B: Oh, got it.
01:29:58.520 - 01:30:34.926, Speaker D: Okay, one last maybe a little aspect that I wanted to mention. There is I think it might also make sense to start talking a little bit about general timeline for Ethereum, Mainet, because I think starting maybe a year from now or something, there'll be a lot of these big changes with the merge and maybe status and so on, and get some feeling, right. Because I would really hope that one Five Nine might be able to just go in maybe like summer, autumn or something so that then we can steer clear of all of those because otherwise it might be a delay of over a year. Additionally just because why all these higher priority things?
01:30:35.028 - 01:30:43.858, Speaker B: I agree. That was always my goal is to get 1559 ship before stateless because otherwise having the two kind of come in at the same time is pretty bad.
01:30:44.024 - 01:30:48.500, Speaker D: Yeah, but then now also with the accelerated merge timeline, that might also be similar.
01:30:49.670 - 01:31:24.876, Speaker B: Agreed. So I guess yeah, sorry, we're already a bit over time. Are people fine having another call in two weeks and doing stuff async until then and using that call maybe to do a bit more of the planning? At least I can share a first draft of the planning of what I think makes sense to bring to all core devs and we can also follow up on the various kind of transaction pool and other issues. Okay, I'll take this as a yes. Thanks a lot, everybody. This was great. I'll try to upload it to YouTube later today.
01:31:25.058 - 01:31:25.548, Speaker F: Thanks.
01:31:25.634 - 01:31:26.300, Speaker H: Bye.
01:31:27.040 - 01:31:28.316, Speaker F: Thank you. Thank you.
01:31:28.418 - 01:31:30.350, Speaker E: Thank you everyone so much.
01:31:32.080 - 01:31:32.940, Speaker F: Bye.
01:31:43.090 - 01:31:43.580, Speaker B: Thank you.
