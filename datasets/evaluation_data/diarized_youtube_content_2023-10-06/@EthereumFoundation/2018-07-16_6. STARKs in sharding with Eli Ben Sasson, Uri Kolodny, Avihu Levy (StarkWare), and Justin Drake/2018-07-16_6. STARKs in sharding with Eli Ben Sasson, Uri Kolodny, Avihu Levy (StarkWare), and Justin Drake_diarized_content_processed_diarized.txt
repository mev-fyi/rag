00:00:11.680 - 00:00:26.656, Speaker A: So, Abihu, I think you're the one who's within stockware specializing on product, which means looking at use cases specific to blockchains. What would you say is like maybe one or two number one use cases.
00:00:26.688 - 00:00:29.812, Speaker B: Of stocks to choose the number one?
00:00:29.866 - 00:00:32.456, Speaker A: Okay, this is or maybe the top few, your favorite?
00:00:32.528 - 00:00:57.680, Speaker B: Yeah. So on Friday, I presented three use cases, and two of them were mainly for L2, and another one was related to clients, and I didn't touch at all. In layer one possible use cases.
00:01:00.280 - 00:01:00.548, Speaker C: I.
00:01:00.554 - 00:01:39.730, Speaker B: Don'T know to choose the best one of them to use, but I do think that if you take the freedom to design your own layer one, then starks for scalability have a really good future. And you can think of more than one scheme where starks are using to create system, which, I don't know, ten to maybe few dozens times more scalability on top of the current solutions. So I hope it answered the questions.
00:01:42.180 - 00:03:01.080, Speaker A: Okay, so it sounds like you're more interested in scalability than privacy, which I guess is something at the firm. We're also potentially more interested at the base layer in terms of applications that I'm aware of. On the sharding side, one is making the fancy crypto that we have, like the BLS, be quantum secure, because that is one of our endgames in the design of Ethereum. But I guess another good one in terms of performance might be the idea of taking witnesses in the context of a stateless client and compressing those. I guess I picked these two examples, maybe because the underlying crypto is very easy, as in the circuits that you have to work with are relatively straightforward, maybe primarily hash based. Is that the type of scalability you were thinking of, or the other type, which is maybe related to state routes, where you have this full, very complicated EVM, a huge circuit.
00:03:03.120 - 00:03:50.970, Speaker B: Okay. Basically, I think that when it comes to payment systems, just payment systems, then it makes a lot of sense to even start with both stateless client and compressing the state at the same time. So generate a proof of the state and have the stateless client. And I would say even more. The basic idea of stateless client is to say our block will be very compressed. There will be just state root in each block, and the transactions themselves, they will be much more compressed. And why? Because now, once we don't have signatures, we can think of ways how to.
00:03:50.970 - 00:04:58.156, Speaker B: We basically stop to pay on space on the block, and we stop to pay on storage. And the only thing that we are paying now is transaction size. And then we can think of ways to tweak this, for example, in which ways we represent our accounts and in which ways we represent our value, maybe we spread it for several types of value categories. It's basic idea of sharding. And when you go to this direction, you can probably get scale because you remove the signatures and you remove the storage, you can get scale of ten to top of what ethereum currently can do. And the advantage that you will have in layer one over L2 here is that you don't have to pay the guard cost for the proof. So if you have it inherently in the protocol, then you pay just for the transactions or mainly for the transaction.
00:04:58.156 - 00:05:02.210, Speaker B: Is I given the example clear enough?
00:05:04.180 - 00:05:15.428, Speaker A: I think so. I mean, this example reminds me a little bit of the coda project where it's specifically a payments blockchain, and they use your knowledge proof to compress and provide scalability there.
00:05:15.514 - 00:05:37.100, Speaker B: Yeah, but I think that they chose to have the state on chain. So at least the last lecture I've seen from them is that they store the whole lecture and then they still pay for the storage. And I think you can get a little bit more efficient than that just going this path.
00:05:39.840 - 00:05:40.590, Speaker A: Okay.
00:05:42.800 - 00:06:06.630, Speaker B: I didn't explain myself, because when you keep the state on chain, then every time that you transfer proof from state one to state two, you have to give the delta to other miners to update their state. But if you don't store the state, then you can just go on and use the transactions, I hope.
00:06:09.160 - 00:06:24.764, Speaker A: In terms of the data, are you thinking of a model where every user is aware of his own balance, but doesn't necessarily communicate all the deltas to the whole network, to all the miners? It's kind of a more scalable in that fashion, because everyone's in charge of their own little piece of data.
00:06:24.962 - 00:06:25.710, Speaker B: Yeah.
00:06:30.400 - 00:06:55.270, Speaker A: So just moving on slightly. I think one of the big downsides of Starks is how do you produce them in the first place and how large they are? And as I understand recently, there's been a breakthrough, at least on the size of the Starks. Eddie, is that something you'd like to talk about?
00:06:57.080 - 00:08:25.868, Speaker C: Well, first of all, I'd like to say that among the proving systems that we're aware of out there, we compared Stark to all the teams that collaborated with us on this, including snarks, Liguero, bulletproofs. No, we didn't get yet collaboration from them, but all teams that basically ran on the same computers, Stark provers are ten times faster. At least they're ten times faster than the second fastest prover. And this is because they have very lean cryptography and work over very simple fields. So this all refers to the academic rate code, which means that likely with our dedicated engineering team, probably we'll see maybe more improvement in the prover running time with respect to the other technologies out there. And again, this is based on the underlying cryptography that is very lean for starks as compared with other proof systems out there. Now, regarding the argument size, which is what the verifier sort of reads from the chain or elsewhere, so indeed, in the academic paper, the length was around between like 200 and 500 kb, depending on the proof length.
00:08:25.868 - 00:09:45.630, Speaker C: And indeed we have brought this down. Our engineering and science team has brought this down. Currently we're looking at in the range of around 80 things that used to take 300 or 500 kb previously. And the improvements come from many different places, and better understanding of the soundness of the protocols and better engineering of various sub protocols in it. I think we're not yet done with the optimizations in terms of this argument length, but I'd also like to point out that in terms of verification time, even though what the verifier reads is a little bit longer than in other systems, the verification time is extremely fast, even currently. And the reason for that is that as opposed to a lot of the other constructions that use heavy number theoretic constructions with elliptic curves or things like that, basically what the stark verifier does is sequentially go through the proof computing a bunch of hashes and then a bunch of very simple operations like soars and small field multiplications. So that's where things are standing right now.
00:09:45.630 - 00:10:01.840, Speaker C: Always, as a scientist, you have to always be optimistic. I think we haven't seen the end of the improvements to this aspect, but the prover time is already fastest by at least one order of magnitude compared against all the other technologies.
00:10:03.460 - 00:10:13.990, Speaker A: Okay, so it sounds like proof of time really is one of your differentiators, and that this will be accentuated possibly by several orders of magnitude with all the engineering that will come out.
00:10:14.360 - 00:10:15.110, Speaker C: Yes.
00:10:15.960 - 00:10:21.172, Speaker A: And there's also potentially hope as well for the proof sizes to go down a little bit.
00:10:21.306 - 00:10:21.616, Speaker C: Yes.
00:10:21.658 - 00:10:29.288, Speaker A: And from a verification standpoint, it sounds like the basic operations are very simple, and so you also have an advantage there.
00:10:29.454 - 00:11:31.630, Speaker C: Yes indeed. And I guess the biggest advantage in starks compared to the other technologies is the extreme scalability, which means that a verifier, even though he's seeing a computation for the very first time, say, described by a smart contract or by a computer program, the time needed to verify claim of computational integrity about that contract or computation is exponentially smaller than the time needed to execute that contract. And this holds, even for a contract scene for the very first time, without any trusted setup, any trust assumptions of any sort. So that's, I guess, the biggest advantage in the department of scalability, which ties into what Avil was mentioning, that we believe scalability applications and solutions are probably going to be where starks will shine really the most and be used the most.
00:11:32.480 - 00:12:08.650, Speaker A: Excellent. And Yuri, a question for you. So it sounds like you've identified the blockchain space as one area where you can do business. So stockware is this company that you've set up with. As I understand, most of the people who have the core knowledge in stocks, from what I understand as well, stockware has no intention to go the traditional route, which is an ICO, or to do things like consulting. You have a different vision. What is that?
00:12:09.980 - 00:12:10.392, Speaker B: Yeah.
00:12:10.446 - 00:13:09.372, Speaker D: So we presented a few weeks ago back at consensus in New York, and Ellie was on stage and he said there's no ICO. And the volume of emails coming in asking him about participating in that ICO increased tenfold. So it seems like if you want to do an ICO and the most efficient marketing is to say you're not doing an ICO. Our intention at this point is to see if we can commercialize this technology without having our own blockchain. We were identifying a whole set of applications that are both layer one and L2. And our hope is that we can be one of the first participants in this ecosystem in the sense of sort of a layer of, call it technology providers to blockchains. And it remains to be proven that sustainable businesses can be built that way.
00:13:09.372 - 00:14:13.884, Speaker D: I think that we've identified such a path. I think, in fact, we see several different avenues to building a sustainable business. But I think that quite a few other players are concerned about that. And I think that the ecosystem at large are. Should take that concern seriously in terms of how can new participants, regardless of the current ridiculous opportunity cost, I hope that window is closing in a sense of the ease of minting your own token, having your own ICO, even once that speculative bubble goes away. This opportunity cost right now, I think, sort of pulls a lot of people in that direction. But even once that speculative bubble pops, I think the industry should consider and think very hard about how to incentivize developers and outside parties to contribute knowledge to existing infrastructure and existing blockchains.
00:14:13.884 - 00:14:18.390, Speaker D: I think that's a fundamental question that seems to me very much open.
00:14:20.040 - 00:14:45.080, Speaker A: Okay, so you're experimenting with new models. And one of your key aims is sustainability from a business perspective. And from what I understood briefly reading one of your posts, that could go in the direction with partnerships with existing blockchains, where you don't build your own one, but you partner with other ones, or potentially going down the route of specialized hardware.
00:14:46.460 - 00:14:46.968, Speaker B: Yes.
00:14:47.054 - 00:15:42.540, Speaker D: So that long term, I think that custom hardware is one interesting direction. Providing services in the context of generating proofs, I think, is another interesting model with recurring revenues there. The basic notion, I think it was alluded to, but I should just state this clearly. The basic notion is that whatever off chain computation one wants to conduct can be done off chain at, call it AWS prices. And the only thing that needs to be done on chain is this is a verification of a stark proof for that computation, which, as Ellie said, comes at an exponentially lower computational cost. So we think that's sort of an appealing concept in the sense that it allows a lot of people to consider doing meaningful off chain computations and feed that into blockchain infrastructure.
00:15:42.960 - 00:15:57.200, Speaker A: Okay, great. So, in the long term, potentially, there's this vision to have specialized provers. And I guess one of the nice things which enables that is that the provers, they spent all this computational resources, but at the end, they produce a proof which is trustless.
00:15:57.940 - 00:16:35.150, Speaker D: That's a very powerful trait of proof systems. They say the proof is in the pudding. So you don't need to go and interview the chef, inspect the kitchen. All you need to do is look at the proof and verify that. And that's indeed very powerful. And another point worth noting in this context is that to the extent the inputs to the prover are shielded, then the prover has no way of, in any way censoring the participants in the computation. That's also a very important assurance, I think, to the general public.
00:16:35.680 - 00:16:44.848, Speaker A: Okay, great. So the provers don't really have much visibility as to what they're proving. And so there is this sensor resistant that is built into the system.
00:16:44.934 - 00:16:45.568, Speaker D: Exactly.
00:16:45.734 - 00:17:16.910, Speaker A: Okay, great. And so I guess this leaves the question, how do we get from today to this end goal? And from what I understand, I think it'd be fair to say that maybe in the next year or two, there's going to be a lot of building infrastructure and building ties with the community, potentially releasing stuff, open source, making security audits to build trust, because this is very new technology. Can you talk more about how you intend to engage with the community?
00:17:17.760 - 00:18:17.916, Speaker D: So, yes, pretty much the roadmap that you described, meaning there's going to be certainly significant chunks of open source that we're going to put out there. Subject those, of course, to security audits, an open discussion with the community around the ideal cryptographic primitives that we want to work with. And in the process, in parallel to this, continue and explore layer one and L2 applications. And we seem to be identifying a fair number of these right now. I think we're far from being done in that sense. And I think there's a lot of sort of call it market development or market education in terms of both understanding what people want to do, but at the same time, explain to them what it is that Starks can do for them and sort of get them to think about their problems. With that capability in mind, and in.
00:18:17.938 - 00:18:44.252, Speaker A: Terms of kind of proving to the world that Starks can be used for scalability, would it make sense as a strategy to focus on one single building block and making it very fast, something like hash functions, making them friendly to the stock provers so that you can use this basic building block to build other things like signature aggregation?
00:18:44.416 - 00:19:02.830, Speaker D: Of course, it would seem that doing that efficiently would be of value to essentially every single application that we look at. So that seems to be a very productive move to make the entire ecosystem move forward.
00:19:03.600 - 00:19:22.870, Speaker A: And I'll just end with one technical question before opening up to the audience. So, yesterday we mentioned MIMC, which is this hash function with low multiplicative complexity. Ellie or Avihu, do you see MIMC as a promising way forward?
00:19:24.040 - 00:20:20.310, Speaker C: It's one thing that we need to explore. I'll just mention that the numbers that you get from rhino constructions are not. We need to clock and optimize all these things, but they're also very friendly. And there are other things like, well, okay, there you're going to lose quantum security, but things that work over prime fields, like Peterson and Schnor and things like that are also, as long as you're working over the same fields, also quite elegant. So MImSi is definitely one candidate. Our engineering and science teams are going to look and basically clock and measure and give parameters for all of the different candidates, hopefully also suggest a few new ones, and then there'll be, like, sort of a cost for different things, and MimSi will be included. Definitely.
00:20:21.500 - 00:20:41.768, Speaker A: Great, because I guess it would be really to stockwise advantage and to the advantage of the whole ecosystem if we could somehow standardize around a hash function, which everyone would use to build their protocols, and then that would kind of make these protocols much more approachable to stock provers.
00:20:41.864 - 00:20:43.870, Speaker C: Yes, that's our hope.
00:20:45.440 - 00:20:47.310, Speaker A: Okay, questions.
00:21:00.520 - 00:21:19.080, Speaker E: I have the most obnoxious question possible, but I'm just curious, what do you expect as a timeline for kind of seeing starks used everywhere? Because it definitely feels like one of those technologies that can be like a fundamental base for tons of other applications and systems.
00:21:22.740 - 00:22:16.230, Speaker C: Well, I mean, we'd hope for them to be used tomorrow. Realistically, as with all new technologies, it takes time. So hopefully, let's say within a year or to 18 months, we'll start seeing some industry grade stuff that people can use. I just want to mention that the academic code is already available on Libstark on GitHub under MIT license, so those who are fearless can already start using it. There are some examples there, and interfaces. There's sort of a tiny rum like interface, and you can build your own heirs. These are algebraic intermediate representations, and you can start using the system already.
00:22:16.230 - 00:22:33.290, Speaker C: But realistically, I'd say one year to 18 months. But I should maybe qualify this, that making predictions about deploying systems, as everyone here knows, is a very risky business.
00:22:34.620 - 00:22:35.370, Speaker B: Cool.
00:22:39.260 - 00:22:56.930, Speaker F: I'm going to demonstrate my ignorance with this question, but I'm going to ask anyway because I'm so excited about this technology you guys talked about the size of proofs and how that's come down quite a bit recently. Is it also the case that computing proofs takes a lot of time, a lot of resources, and if so, is that under active research? And how is that looking?
00:22:57.620 - 00:24:03.520, Speaker C: So, computing proofs takes a lot of time. Stark provers are the fastest provers out there already, by a factor of ten over the second fastest, and by many more factors over not the second fastest. And I urge you to look at the stark paper, which is also available online. There's a bunch of figures there that show the running times in single and multithread. I just want to say, from a 10,000 meters view, provers are the heaviest part in all proof systems, and they will likely remain that way in all proof systems. Specifically, the stark prover already currently is extremely lean to the point that it would require, I think, immense breakthroughs, theoretical ones, to improve on it, theoretically, not on the engineering. So the engineering, there's a huge room for improvement orders of magnitude, many of them, but theoretically.
00:24:03.520 - 00:25:27.388, Speaker C: So, without going into details to prove, you take an execution trace, you apply to it a single FFT, which takes time, t log t, and after that you have a fully parallelizable computation that takes you six times some parameter that is linear in t, arithmetic operations over a very simple field that's the fry prover, and basically then you apply hashes again, fully parallelizable. I think it's very unlikely that we will see significant asymptotic improvements. We're already looking at what's called strictly quasilinear running time o of t log t, and we don't even know. Now we'll say something as a theoretical computer scientist, we don't really know of any generic reduction from an NP language to, let's say, threesat, which is the simplest of and the most fundamental one of NP complete languages and the original NP completeness paper of Kuk. We don't know of any reduction from a generic language to three sat that takes less than n log n time. And with starks we're already at the n log n upper bounds. So I don't believe prover time asymptotic mathematical will go down anytime soon, though I hope to be pleasantly surprised.
00:25:27.388 - 00:25:47.220, Speaker C: There's a lower bound of t, and we already have an upper bound of t log t, and reducing that log t is going to be very hard. But building systems using good engineering can shave things off with several orders of magnitude. And there we're very optimistic.
00:25:48.120 - 00:26:13.996, Speaker F: We talked yesterday in the context of finality. In a blockchain. Certain types of applications do or don't make sense. So if we're making a point of sale purchase, maybe you could have a ten second limit and the certain types of transactions we might need faster finality or slower finality is okay. So based on what you just talked about, are there certain types of applications which you think will or will not work well with Starks, given the complexity and the amount of compute time required to compute the solver.
00:26:14.188 - 00:26:58.524, Speaker C: So I think that end user, and again, I urge you to download the Lipstark and play with it zcash style shielded transactions already on the academic code. Probably if you match the numbers, take very few seconds on any standard laptop that will likely go down by an order of magnitude or two. So I think it's completely reasonable that a smartphone can do a shielded transaction stark style in a second or less. That's very reasonable. The main power of Starks again is in the huge scale computations. The prover is the bottleneck. I think that there as well.
00:26:58.524 - 00:27:58.930, Speaker C: We will at some point see specifically tailored systems and possibly hardware dedicated for it that will dramatically increase, decrease latency and make these systems very attractive to everyday use. So there was a speaker earlier that showed a picture of this HPC cluster that does. What was it like? More than a PETA flop of I forgot how many. I was just thinking, wow, if we had a PETA number of the kind of operations that we need, you could probably compress the proving of the validity of all of bitcoin's blockchain. You could prove all of that, or at least half the chain or something like that. And then a smartphone could verify the validity of the Utxo in fractions of a second. And I think we'll reach that point.
00:27:58.930 - 00:28:01.520, Speaker C: There's no reason we shouldn't.
00:28:21.100 - 00:28:40.610, Speaker E: I feel like you've probably done applications that make a lot of sense, so maybe we'll give some. Like, it would be nice to have some basic intuition about, okay, this is where a stark is useful versus this is not, or something like that.
00:28:41.220 - 00:29:10.808, Speaker B: So you were not here on Friday? Yeah, no, it's okay. I only mentioned few use cases. There are many more. So in terms of Ethereum, you can think what I mentioned on Friday. Let's do it quick. It's the privacy solution for shield transaction. This is the most basic one that people already knows and familiar with.
00:29:10.808 - 00:30:24.940, Speaker B: I already explained a little bit about two approaches for scalability. The first one is from the point of the transmission size, and the other one, which is more like can save much more, is the computation perspective, meaning that you can generate any proof for off chain computation and have the contract just validate it. I also mentioned compression of the chain in the meaning that clients doesn't have to validate state by downloading all the blocks they can just validate approve. I can mention things also outside of the blockchain itself. For instance, you can validate that exchanges do hold enough assets in front of their liabilities for their users. This is something that stark specifically can be useful and good at because of the large scale of the proofs that you sometimes need to generate. And there are also some other simple use cases that still relevant for Ethereum, such as layer one solutions.
00:30:24.940 - 00:30:35.110, Speaker B: For instance, replacing BLS signatures with star based scheme that we maybe talk about later today.
00:30:39.880 - 00:31:27.344, Speaker D: I want to give one more example. We're sort of in the midst of conversations with brave, and what brave need to do is demonstrate to the general public once a month that tokens were fairly divided based on users usage and preferences, et cetera. Now this is not something that would generally require a supercomputer, but doing it on chain is prohibitively expensive from their perspective. I think it's a very elegant test case of starks, because this is something needs to be done once a month. Right now they're sort of relying on Brendan Icke's reputation, so to speak. No, in a real way and in a positive way. But he fully realizes that this needs to change and the trust needs to be established, not because he's a trusted party, but it needs to be established through the protocol.
00:31:27.344 - 00:31:41.384, Speaker D: So I think it's a very elegant case of an off chain computation, not terribly frequent one, which he wants to be able to provide the chain with, approved for on a monthly basis and.
00:31:41.422 - 00:32:58.930, Speaker C: Looking forward beyond decentralized blockchains, right? I guess we all share the belief that at some point central trusted parties, governments or various important players will start using blockchains for auditability and accountability and things like, you know, if any one of these institutions, a bank, a big exchange, not crypto, starts putting, say, merkel commitments of the daily state of the system onto a blockchain, then with starks you have basically unlimited capabilities for proving later on with zero knowledge, if the need be, and proving things to individual users that a month ago when we committed something, your state, your account, your information, your private rights were in this state. So I guess we all share this hope that we're going in that direction and that the decentralization, accountability and irreversibility will someday be adopted by the various central trusted parties that anyways run the world right now.
00:33:00.900 - 00:33:17.160, Speaker F: Apologies if this was discussed on Friday, we still have a second. I wasn't here on Friday, and maybe this is a question for the Ethereum folks in the room, but pragmatically speaking, what do we need to add sort of support for starks to Ethereum, or at least to verifying starks?
00:33:24.380 - 00:34:13.370, Speaker G: Go over some of the obstacles quickly. So first of all, a lot of the stark constructions are done over binary finite fields, and currently doing operations over binary finite fields inside of the EVM is totally not efficient. So unless we want this to be kind of gated to Ethereum 2.0 sharding exclusively, you would need a hard fork to add pre compiles for basically binary finite field operations, though it could be done pretty generically. You basically need one thing that says multiply two numbers in some finite field with some bit field there that specifies what the binary field is. And even if you don't do that, you can totally come up with stark silver prime fields as well. It has more efficiency issues.
00:34:13.370 - 00:34:40.450, Speaker G: The other one is that the size of a stark to a few hundred kilobytes, and with Ethereum's current gas limit of 8 million, the theoretical single block is kilobytes. And that start to include the cost of stark verification. And so unless you want to have a super.
00:34:52.380 - 00:34:53.130, Speaker B: It.
00:34:56.540 - 00:35:16.030, Speaker G: Ethereum, the current blockchain using up 8 million gas for a proof is legitimately very expensive. So you'd need to come up with some kind of application specific batching technique or whatever that would make that practical for the average user of a system.
00:35:18.500 - 00:35:20.160, Speaker A: If you want. For instance.
00:35:24.820 - 00:35:31.796, Speaker B: One more thing. I don't know that. For example, if you want at some point, some point in the future to.
00:35:31.818 - 00:35:32.580, Speaker C: Prove.
00:35:35.000 - 00:35:58.140, Speaker B: Things on the state or the history of Ethereum, then you want all the opcodes or all the opcodes basically to be somehow start friendly because then you can translate them efficiently to constraints. So this is one more thing that you can help start be included in Ethereum.
00:36:13.890 - 00:36:23.140, Speaker A: So I think you started off with a really great team and you're hiring. Can you tell more about the team? Because I think you really do have a world class team.
00:36:24.790 - 00:37:00.460, Speaker D: Sure. Yes, we're hiring. We think of these guys as bilingual in the sense that they're very strong engineers and very strong mathematicians. The intersection of these sets is smaller than one would hope. Probably not in this room, but out in the world. And I think that's the main characteristic of our R and D team at the moment. And the realization that in the foreseeable future we're going to run in parallel, pushing on both the engineering front and on the science front.
00:37:00.460 - 00:37:04.420, Speaker D: And that's something that we're fully prepared to do it.
00:37:07.190 - 00:37:10.100, Speaker B: And in the blockchain front as well.
00:37:11.830 - 00:37:18.880, Speaker C: And somebody in the hardware front close.
