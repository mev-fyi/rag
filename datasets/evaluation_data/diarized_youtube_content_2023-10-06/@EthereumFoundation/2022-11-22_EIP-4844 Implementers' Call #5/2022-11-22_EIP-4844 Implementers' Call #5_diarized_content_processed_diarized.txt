00:00:00.330 - 00:00:37.270, Speaker A: You okay? Good morning, everyone. Afternoon for Europeans. This is our fifth Ford four four call. Lots to cover today, as always, bunch of spec updates. Then I think it's probably worth spending some time chatting about this large block testing that we've been doing. Georgios has managed to send a few rounds of large transactions on Gordy, and so we can chat about what we want to see next there. Chat about Devnet three, how that's going.
00:00:37.270 - 00:01:22.326, Speaker A: We were supposed to be launching next week, so are we still feeling like that's possible? And then I think last, if we have time. I know we're right in the middle of discussing Shanghai inclusions and whatnot right now, so it would be good to just make sure the readiness checklist is roughly up to date. Just discuss how people feel about Shanghai. But yes, to start, I think the first thing spec wise was what to do for blocks which have no blobs. George, had. Oh, kev, sorry. Had a pr about this.
00:01:22.326 - 00:01:34.570, Speaker A: PR 30 93 in the CL specs. Kev, do you want to maybe just give us a quick recap of where things are at there? Hello.
00:01:34.720 - 00:01:55.380, Speaker B: Yeah, so currently I think we've all agreed on the strategy in the PR, and we just need to know if clients actually are going to incorporate optional sidecars. If I remember correctly, George, I think even if they don't, we might still go through with this pr.
00:01:56.950 - 00:02:59.366, Speaker C: Yeah, I think that's indeed the case. So I think, like a bit of background is that there were some bugs that appeared when an empty sidecar was given to the cryptography layer. And we know how to fix the bug, and the fixes will get incorporated regardless of whether the sidecar becomes optional or not. But I just want to understand why the sidecar is not optional when there are no blob transactions. If this is something we did, if this is a good thing, maybe because it doesn't have any special blocks like any if else conditions. And if that's the case, if the sidecar becomes optional, then it means that we can be sure that anything given to the cryptography layer will have at.
00:02:59.388 - 00:02:59.960, Speaker A: Least.
00:03:01.690 - 00:03:17.770, Speaker C: One commitment and one blob. And this might let us do a bit more defensive, have a bit more invariance in the code, but the cryptography PR will not be affected too much. I was just wondering, what's the rationale behind the sidecars not being optional?
00:03:18.430 - 00:03:55.480, Speaker D: I mean, I would ask, why make the sidecars optional otherwise? If you make the sidecars optional, you're going to have a bunch of places like in the networking spec where you're like, well, if you have a sidecar do this, whereas you have two messages, it's easy to send two messages and one message will be empty if the commitments are empty. And the other message, I guess you end up having some logic hoisted somewhere to handle emptiness. But if you're going to put that in the cryptography layer regardless, then I'd say it's actually easier to have sidecars with everything.
00:03:56.010 - 00:03:57.720, Speaker C: Okay, that makes sense.
00:03:58.170 - 00:04:21.774, Speaker A: There's another argument which is currently the data available he checked is, is there a sidecar for this block and does it validate? And in your case we would have to make that logic also something like does this block have any blobs? And if yes, is there a sidecar? And if no, don't do anything. Yeah, so also not a huge difference, but we would have.
00:04:21.812 - 00:04:34.158, Speaker D: Yeah, but it's pretty much like rather than just hey, given the commitments and given the sidecar, does it validate? It becomes you end up with preconditions to kind of even get into that logic.
00:04:34.334 - 00:04:53.740, Speaker A: Yeah, I think it makes perfect sense definitely to program it so that we can have empty ones and we can still have a discussion later on if for some reason it seems easier to make it optional. But as long as it's small, I think there's not really a strong reason for that.
00:04:54.430 - 00:04:55.500, Speaker C: Sounds good.
00:04:57.070 - 00:04:57.578, Speaker A: Sorry.
00:04:57.664 - 00:05:20.210, Speaker D: So my take is I prefer for emptiness and don't do optional unless we have to, just because when you do optional, there's also additional complexity to implement for the Marshall and Marshall layer. And yeah, it's just like one more thing that could go around there. So that's just my preference.
00:05:21.990 - 00:06:34.890, Speaker C: Okay, that makes sense. I mean, I was thinking that, I'm not sure if we're going with a decoupled block sidecar thing or not, but I was thinking that if it's decoupled, maybe it allows block verification to go through faster. But I think we have good consensus here and that we don't do optional and it's mandatory and empty and in this case can get the cryptography. Tim, there is another pr that got merged just as an update. I think it's 30 97, which basically what it does is it makes the verify KCG proof interface be a bit more high level. So like accept bytes and not field elements. This removes some burden from the client devs when they're implementing the precompile.
00:06:34.890 - 00:07:30.640, Speaker C: We did that from feedback by Alexey and there is another pr on the it side which simplifies the pre compile to use this new interface. So that's good. Roberto, suggested that maybe we do the pre compile interface of the KCG library even more high level and incorporate also the hash check. We didn't do that in this pr. But as more clients implement this thing and as we get more feedback on what's the right interface, we might want to revamp the interface if needed. So, yeah, let us know if you have feedback on the cryptography API. Other than that, I think we're at a good state right now.
00:07:32.690 - 00:07:37.040, Speaker D: 30 97. That was in the spec release on Friday, right?
00:07:39.330 - 00:07:42.410, Speaker C: It was. It maybe, I'm not sure.
00:07:42.500 - 00:07:44.462, Speaker D: Believe it was. Let me just confirm.
00:07:44.606 - 00:07:46.100, Speaker A: Yeah, I think so.
00:07:50.970 - 00:07:53.160, Speaker C: Is this a good thing or a bad thing?
00:07:55.610 - 00:08:07.820, Speaker D: It's just context for implementers. If they're targeting this release. It is in there, yes, as was the validate blob sidecar gossip condition, and a number of other things.
00:08:23.950 - 00:08:29.200, Speaker A: Okay. Anything else on either the empty blobs or updates on the KZG side?
00:08:32.050 - 00:08:58.120, Speaker E: We had a question on the KCG library. Right now, the field elements per blob is sort of hard coded in the CKCG library. And for the minimal spec on the continuous layer side, we have a different value than what is hard coded there. So maybe we want to make that like a variable rather than have it as a constant. In the CKZG library itself.
00:09:02.960 - 00:09:05.010, Speaker C: What is hard coded exactly?
00:09:06.180 - 00:09:34.410, Speaker E: The field elements per blob. It is hard coded as 40 96, and the minimal preset for the consensus set, that value is set as four. So I think it might be better to sort of have that value configurable also if we want to sort of say, benchmark different values of field elements per blop or something like that.
00:09:36.140 - 00:09:45.310, Speaker C: I see what you mean. So you want this to be configurable on the CKCG side, like make it a compile time parameter or something?
00:09:45.840 - 00:10:12.070, Speaker E: Yeah, something like that. Because right now the trusted setup parameters also checks that whatever you get from the file is equal to the hard coded parameter in the CKC. So if you pass it anything which is not 40 96, it will basically crash, sort of, because there's an assert in the load trusted setup thing.
00:10:12.840 - 00:10:40.940, Speaker C: Okay. All right. I think that makes sense. I mean, we do want this to be smaller value in the minimal preset, so the library should support will. I think Ramana is not in this call for whatever reason. So I can let him know that we want fields, whatever the constant b code to be parameterizable. Okay, that makes sense.
00:10:40.940 - 00:10:42.030, Speaker C: Thank you.
00:10:43.600 - 00:10:47.072, Speaker E: Thank you. And one more thing we were thinking.
00:10:47.126 - 00:10:47.730, Speaker A: About.
00:10:49.700 - 00:11:34.510, Speaker E: When we batch verify a range of blocks. We also sort of want to do the same thing for blocks. Like right now in Lighthouse, we batch PlS verifies 64 signatures for 64 blocks at the same time. So on the KCG verify aggregated KCG proof site, would it be as simple as just adding up the individual blob arrays from each blob site card that we get and adding all the individual KZG proofs and then just passing it to the KZG library as is? Or would that be something different? Like would we have to do something additional on top of that?
00:11:38.900 - 00:11:54.532, Speaker C: I was thinking of what proto wrote in the comments, but let's touch this second question. What are you saying exactly? You want to do more things out of the crypto layer before you pass it into the crypto layer? Or what did you say?
00:11:54.666 - 00:12:06.840, Speaker B: I think he's saying that currently we have this verify function that works for one block and he wants the verify function to sort of work for multiple blocks. So you do like a batch verification.
00:12:08.060 - 00:12:16.700, Speaker E: That would be faster because you would need to compute just one pairing, I guess. I'm not really sure though.
00:12:16.850 - 00:12:18.510, Speaker C: Where would this be used.
00:12:22.400 - 00:12:44.550, Speaker E: When we sync? Basically on the consensus layer side, we get a range of blocks right now. So presumably we would be doing the same for blocks and blobs with the blocks and blobs by range method. And we get 64 blobs at once. And instead of passing it one by one, we want to pass it batched basically.
00:12:45.240 - 00:12:49.690, Speaker C: I see. So this could be like a helper only during sync, right?
00:12:50.060 - 00:12:51.048, Speaker E: Yeah, sort of.
00:12:51.134 - 00:12:52.010, Speaker A: That's right.
00:12:57.180 - 00:13:07.420, Speaker C: Okay. And you have found that this is like the speed during sync is an issue that would benefit from such aggregate batch verification.
00:13:10.240 - 00:13:40.788, Speaker E: We haven't really tried it yet, but the thing is that we do the same for batch verifying BLS signatures that right now in the current main net, whenever we get blocks, we batch verify all the proposal signatures and the aggregate attestation signatures and stuff like that. So I thought that it would be similarly faster if we do it in a batch instead of doing it one by mean.
00:13:40.954 - 00:13:42.376, Speaker A: Just wanted to check.
00:13:42.558 - 00:14:31.456, Speaker C: It's definitely possible. I'm just wondering, like Roberto said, if it should be part of the library or it should be. Okay, maybe we can take this offline so we don't hog the meeting. But I agree that if it's taking you considerable time to sync, we could do some sort of batch verification to speed it up. I mean, we actually used to have one before we introduced the KCG proof technique a few months ago. We used to do batch verification, so it shouldn't be too hard to bring it back on. On your first comment, Proto said whether it's worth making the field thing parameterizable or just keeping it 4096 for all the presets.
00:14:31.456 - 00:14:43.800, Speaker C: I don't have an opinion on this, but if you think that's a good idea, we could also do that. Proto.
00:14:46.400 - 00:14:57.920, Speaker B: I think it's more shao way's domain, because Pawan was talking about the minimal specs having a smaller trusted setup than the main net specs.
00:15:01.780 - 00:16:14.436, Speaker A: Yes, I think we can do that, but just using the larger trusted step is also slower in python implementations. That's what I saw from the basic pytest. So we haven't generated many block verification tests so far, just some, like maybe only four or five basic tests. So I will need to try if we can do so in the CI test with the main setup for the CI usage. So I will try something locally and report it back later. Thanks.
00:16:14.538 - 00:16:56.850, Speaker C: Okay. I mean, I can see how 4096 will be quite slow in python, and especially as we add more tests, it will get slower. And I mean do the test, shall we? And if it's indeed quite much slower, we can also talk with Ramana and see. I don't expect that it would be that much hard to make it a compile time configurable thing if you're using some sort of build system. So I think we can do it that way. But I can ask Graham to see what he thinks.
00:17:03.230 - 00:17:25.630, Speaker A: Okay, anything else on this? Okay, the other open one, Terrence, you had this issue about ancestor blob availability check that you opened a while back and still has been sort of pending. Anything there do you think we should discuss now?
00:17:27.440 - 00:18:29.650, Speaker D: I think from the issue there seems like to be a good consensus on just like we should do cannot import, meaning that if we don't have the blobs ancestors, that's up to 18 days, you cannot import the subsequent blob. And I think the rationale, it is easier to rationalize because when you do optimistic thinking, you only do this for syncing part, but we don't need to sync here, so there's no point imported optimistically. So I think the next step is just to look at the spec and then further clarify it, because I'm not sure if the spec today states cannot import or states can import, but import optimistically. So that's just something to check the spec for. And I don't think this is like a blocker for net three, for example, but yes, it will be good to clarify that in a spec.
00:18:31.140 - 00:19:19.700, Speaker A: Got it. Okay. Any comments on that? Okay, sweet. Next, Angar, you had some spec updates as well. The first one on the minimum gas price for blobs. You seem to have moved to saying we should just use one away and basically go with that. Any objections or thoughts there? Okay, I'm still of the opinion that it's better to be opinionated here because there's no reason to impose the cost of the network if there's no economic benefit.
00:19:23.410 - 00:19:25.758, Speaker D: Well, it's not a sustained cost.
00:19:25.844 - 00:19:26.094, Speaker A: Right.
00:19:26.132 - 00:19:51.400, Speaker D: It's like, can the network handle this load? And it becomes kind of constant over some unit time rather than, say, building out, expanding the blockchain or the state forever. So I don't see it quite as, like, that we either should be able to handle that load or not. And I don't see it as like kind of an increased fixed cost rather than an increased sustained cost. So I'm not too worried about it.
00:19:59.020 - 00:20:55.410, Speaker A: So I guess, I don't know, just in the spirit of trying to get the spec to a spot where it's pretty much finalized, do we just leave NSgars PR open, launch the devnets, and I don't know if later we want to have this argument, we can and maybe, I don't know, once all the client teams have started implementing it, there'll be more to discuss. But I think for now, just to move quicker, I'd be inclined to leave it at one and see if there's strong objections there beyond Bankrad. I'll take this as a yes. Okay. And you had three merge pr and scars. Do you want to give a quick update on each of them? So I think the modulus one is probably both the biggest.
00:20:57.110 - 00:21:00.100, Speaker F: Do we just say to close it, to leave VP open?
00:21:03.670 - 00:21:16.118, Speaker A: Sorry. Yeah, actually, that wasn't here. I mean, I don't. So the spec right now uses the value one. Right. Right.
00:21:16.204 - 00:21:50.420, Speaker F: My preference would be to close the PR. I think we sometimes make the mistake of just trying to basically not make decisions until very late, and then that always just adds kind of uncertainty for people involved. I think given how much people disagree here and that one, leaving it at one for Mainnet launch is not an issue in any way. I would prefer to revisit this for the fork after and just have it in a place where we don't have to worry about it for now anymore. But I mean, if people really prefer to leave it open, I'm fine with that as well.
00:21:52.310 - 00:22:22.954, Speaker A: No, I think we should just close it and we can always find it. On GitHub if we want it. It's not like it's a complex pr either. So if we want to refer back to the conversation, it'll still be there. Let's close that one. Do you want to give a quick update on the three other ones? So the modulus, the transaction block broadcast, and the fork behavior. Sure.
00:22:23.012 - 00:23:31.602, Speaker F: So basically the idea was to, by this call, have all the kind of spec updates done, and so all of these prs are merged. So the one was we talked about in the past, the pre compiled return values. We ended up deciding to pad the degree value after all, so that it's also 32 bytes. The extra cost is so small and just in case there are some incompatibilities, it's just easier to do that. So that's merged and then we have the mempool behavior clarification specifically. Now the spec requires clients to not auto broadcast four transactions. There was a small question last week whether this should just be a recommendation or a mandatory requirement, and Marius pointed out that if you actually want to be able to restrict your own kind of bandwidth, just the amount of incoming void for four transactions, basically it is necessary that you can actually kick peers that flood you with void for four transactions.
00:23:31.602 - 00:24:37.318, Speaker F: So it has to be a spec violation to do that. So that's why the spec now makes it mandatory that you just don't broadcast incoming block transactions, you only announce them. And then with the upcoming E 68 version you'll actually, as part of the announcement, also announce the transaction type and the size so that clients can make a more informed choice whether or not to request the transactions from you. It'll make transaction propagation slightly slower, but that's fine. So that's merged and then 1 second, the third one is that was just like a really small one. Should be common sense, but just the spec didn't actually clarify the behavior at the fork block itself specifically because part of the base fee calculation is the parent header field with the excess gas. But that of course doesn't exist at the fork block.
00:24:37.318 - 00:24:49.360, Speaker F: So it's just now explicitly initialized at zero at the fork height. But yeah, should be common sense. I assume that's how people already implemented it. That's all. So then from my side, basically.
00:24:51.250 - 00:24:51.518, Speaker A: I.
00:24:51.524 - 00:24:55.634, Speaker F: Don'T have any kind of future spec updates that are still missing from my side.
00:24:55.672 - 00:25:10.470, Speaker A: The spec is basically in a good place. Awesome. Okay, Danielle, I just saw your comments. Do you want to take maybe a couple of minutes and walk through your doc or just like kind of brain dump?
00:25:11.690 - 00:25:12.390, Speaker D: Yeah.
00:25:12.540 - 00:25:17.754, Speaker A: What you think we want out of this basically. And then we could probably go from there.
00:25:17.792 - 00:26:13.354, Speaker D: Yeah. The big thing that we want to see is how much at different data sizes under a reasonable amount of bursts. So five or ten blocks sustained that the chain and nodes continue to function as expected. In previous minor experiments we just had the orphan rate, but we now have a lot more data on chain. So we first and foremost kind of want to look at the orphan rate. We want to look at attestation, inclusion and success rates, which become an indicator of how well validators are performing, which are nodes of quite a diverse type. And then we also want to understand if any degradation of that chain data is kind of random as to which validators are degrading depending on the slot or if it's a particular set.
00:26:13.354 - 00:26:55.202, Speaker D: So maybe we see that 10% of validators are always kind of performing poorly at some data load size, which would indicate at some bandwidth or hardware some sort of threshold. There's beginning to be issues. And then additionally we have the prism sentry nodes. It would be good to have some other node type because we might have asymmetries on how prism performs or is connected in the graph versus others. But these sentry nodes are going to dump first arrival time of various messages. So blocks, attestations and aggregates. This will give us additional network data as to how these messages are being propagated.
00:26:55.202 - 00:27:42.482, Speaker D: So you could imagine some low resource node in say Australia actually gets blocks like 9 seconds late. But most validators maybe aren't of that type. So sentry nodes kind of complement the chain data. It'd be good to have a diversity in region and good to have a diversity in the requirements that they're facilitated with. Really what we're looking for is we want to know the norm on all of this, and we want to know deviation from the norm. And then we want to understand deviation from the norm with respect to some of our key thresholds, key timing thresholds. So call it when things are deviating towards arrival times in like the three second mark, then we're kind of enter into the danger zone.
00:27:42.482 - 00:28:23.554, Speaker D: So we want to understand, we want to do this on testnets. We don't expect crazy things to break on testnets, but if they do, that's a sign. And then we want to carry forward whatever the successful data thresholds were on testnets. We want to go to Mainnet and observe this data. I think ultimately what we want to do is pick a number that functioned very happily on main net and maybe lower than that given other simulation and pen and paper analysis so that we're certainly in kind of a safe zone as we initially launched 4844. I do have to run talk to you all soon. Catch up with you all.
00:28:23.592 - 00:28:24.210, Speaker A: Bye.
00:28:26.570 - 00:29:03.040, Speaker D: Maybe picking up from where Danny left off, if these are our goals. The current status quo is that we have a very simple script which submits a bunch of 128 kilobyte transactions. No weird behavior was observed, but 128 kilobyte is not expected to do anything. I would say. So right now. I'm going to connect with flashbots today, probably on getting a builder which has a bigger limit and going to start spamming 520 and 1024 kilobyte transactions. Yeah.
00:29:05.730 - 00:29:19.158, Speaker A: Nice. Yeah, I think that's useful. And I think if we can get even the 128 transactions, I believe you managed to get eleven in a single block at the same time. So if we spam, we are going.
00:29:19.164 - 00:29:44.110, Speaker D: To be submitting full block templates via the builder. So we're going to be eating up the entire block. And so far the biggest one that we have gotten is like eleven. 128 kilobyte transactions, 2 million gas each. And that, I think probably was the biggest block for Guerley. Yeah. Happy to refine in any way that people think is relevant.
00:29:46.290 - 00:29:47.230, Speaker A: Terrence.
00:29:49.090 - 00:30:09.990, Speaker D: Yeah. So I guess I have a question for the consensus layer. Client teams here like Lighthouse techu and lowstar neem bus, do you guys capture attestation arrival latency? I guess probably not in the DB, right, but do you capture that in the matrix form, for example, like histogram?
00:30:11.290 - 00:30:12.600, Speaker F: Yes, we do.
00:30:13.930 - 00:30:24.460, Speaker D: Okay, that's perfect. So if you guys do, then Perry and the DevOps team can also launch you guys as a century node. The more the better.
00:30:25.230 - 00:30:28.220, Speaker F: I think. Lighthouse dos and nimbles dos too.
00:30:29.490 - 00:30:29.998, Speaker D: Nice.
00:30:30.084 - 00:30:58.440, Speaker A: Yeah, nice. And I guess is it realistic to expect the builder to be up and running like in the next day or two? I think it would be really neat if before all core devs Thursday we could have had a couple builder exists.
00:30:59.360 - 00:31:10.784, Speaker D: And I should have it today. That's what I've been told. I've been told that I should have it. So if I have it today, we'll get done today. If not, okay.
00:31:10.822 - 00:31:50.364, Speaker A: And I guess then the thing we need to make sure is we have some sentry nodes up and running like today or tomorrow. Yeah. So I'll follow up in the telegram chat about maybe one thing that would be helpful. So Lighthouse and lodestar, you mentioned you have all these metrics. Do you mind sharing just your docs page or say we're sending this to people running nodes like where they should look to configure the metrics correctly. If you can post it in the chat here, that would be super helpful. Sure.
00:31:50.402 - 00:31:52.140, Speaker F: I can do an issue to writers.
00:31:53.120 - 00:31:53.870, Speaker A: Sorry.
00:31:56.400 - 00:32:01.390, Speaker F: Yeah, we will send a link. It's not ready now, but I can post them.
00:32:02.580 - 00:32:04.610, Speaker D: Yeah, I can get somebody together too.
00:32:05.380 - 00:32:22.514, Speaker A: Okay, great. Sweet. Anything else on this big blog test team? Okay. And yeah, if anyone wants to be on the telegram group, just send me a message and I'll make sure to add.
00:32:22.552 - 00:33:05.280, Speaker D: Sorry, Tim, maybe one parting thought on this, because I also have the job in general, I mentioned that in the chat, but we should have a very high bar and very rigorously defined metrics. And I don't want to sound like a broken record or like breaking the party, but we should have, again, I think we have a doc already. Let's have a doc which has the checklist of the things that we really need. And let's focus on these things and work backwards towards making the benchmark successful. Just saying this as a process point to kind of minimize round trips and for us to make this kind of like, we're doing this to cover all the edge cases and we should be prepared for all the edge cases. So I'd love to see a more systematic approach, at least in the future.
00:33:06.790 - 00:33:25.640, Speaker A: Yeah, I agree. And I think Danny. So I'm just looking over the changes Danny did. So I think he's at least added since yesterday a lot of the numbers we're looking at. So these thresholds we want to make sure we're not exceeding. So I think that's a good place to start. But I agree, we can probably refine it beyond that.
00:33:25.640 - 00:33:46.680, Speaker A: Probably the best way also to frame this is like if client teams have specific numbers or thresholds or whatnot that they are concerned about, I think the reason we're doing this is sort of to convince ourselves and other client teams that this is sound.
00:33:49.370 - 00:34:24.660, Speaker D: Every stakeholder in this has a feedback loop that needs to be surfaced. And that feedback loop might be the latency, it might be the cpu, the ingress, the egress, whatever. But ideally every client team would have a list of like, here's what we need to be true for us to be okay with it. And then people work backwards. And candidly, all of this may already exist, but having it in one place as a single source of truth matters and everybody having signed on it, like ad hoc is good, but you have to have things in one place. Anyway, sorry to.
00:34:26.710 - 00:34:28.978, Speaker A: Agree, I happen to.
00:34:28.984 - 00:34:36.870, Speaker F: Have a list with all the metrics from all the consensus clients about block latency and attestation latency. I put it on the chat.
00:34:37.850 - 00:35:24.470, Speaker A: Awesome. Thanks. I'll add this in Danny's talk as well. Yeah, okay, sweet. Anything else on this? Okay, Devnet three. Yeah, I'm curious, how are different clients tracking? Do people still think launching late next week makes sense? Yeah, Roberto, I see you just put your camera on so I'll call on.
00:35:25.800 - 00:36:14.672, Speaker G: I mean, I think we need a little more momentum on the client side, to be honest. The good news is, I guess the client API libraries are now pretty solid. Within Gokzg and Ckzg there's been work making sure they interoperate nicely, and barring the few edge cases and a few of the other details from the prs that have been closed just over the past day, it's all working pretty well. But as far as clients that are fully capable of interoperating right now, within that interop repo, it's just prism and geth. Lodestar I think is close. I'd love to hear an update from the team working on that and the folks working on that when it'll get there. I myself am working on Aragon to get that up to snuff.
00:36:14.672 - 00:36:40.140, Speaker G: I haven't heard a lot about what's going on with some of the other clients though. As far as prism goes, I think we still need the version in the interop repo anyway. Needs a couple of updates. One being like I think right now it hasn't combined the beacon block and the sidecar in the same together. It's still doing the decouple thing so I'd love to hear an update on that as well, but those are kind of my updates.
00:36:42.000 - 00:36:50.896, Speaker A: Got it, thanks Alexaid and dapline yeah.
00:36:50.918 - 00:37:25.710, Speaker F: I can update on Loadstar. So as of Friday I would say optimistically I completed the full implementation of the current spec. I was able to run with the interrupt, post the ip for it, fork propose blocks with blobs and retrieve them on the PTP. But it's not passing the interrupt because there is some weird protocol issue that I have to debug. I think the implementation of the P two P to consume and assert the test have some incompatibility with us, but working on it.
00:37:29.280 - 00:37:37.600, Speaker G: Cool, great. Let me know if I can help. Is Mophie on the call? I'm wondering if we can get a prism.
00:37:38.980 - 00:37:40.130, Speaker A: Yes, yeah.
00:37:44.020 - 00:38:55.340, Speaker D: I cannot from side as for now we have three version compliant client I believe, but with some bugs and no withdrawals yet. Merged like that and the single question I have now is how we arrange Docker compose with all the containers. Do you guys see an idea to add backend nodes like for execution clients more or replace guests for followers like that. What will be in this file? We want to make a pull request, but just wanted to clarify this question before, like what will be container set of the network? Yeah.
00:38:58.270 - 00:39:00.140, Speaker G: Sorry, I'm not sure I follow the question.
00:39:01.790 - 00:40:07.790, Speaker D: Yeah, interrupt repository contains docker compose which include bacon nodes and execution site validator and so on. And we now need to update our clients, right? Yeah, Eric, never mind. I believe some site clients. So should we just add additional beacon nodes for execution clients or we can use like we can replace get as a beacon which is in the pair with one of followers like that we want to. Any ideas? Oh, it's not clear. I mean we need to be added to this network. And what would your advice?
00:40:08.610 - 00:40:30.200, Speaker G: Yeah, I think ultimately we want to be able to fire up kind of mixes and matches of execution clients and consensus clients. You're right though, the docker files that are in there do not support that yet. I think we're open to suggestions on how to best do that. I don't have any firm ideas myself.
00:40:31.690 - 00:41:02.180, Speaker D: I think this is related to that testing discussion which is also on the agenda, and in particular something that hive could support. And I will have some time next week to pick up more of the hive testing to get some diversity in the testing and also enable to swap clients more easily for testing. Now that we have more. But maybe let's discuss async. I think Mophie also likes to discuss this, but he's sick and jet works better.
00:41:06.550 - 00:41:20.150, Speaker G: Yeah, but I think in the meantime, before we sort of figure out a full general solution, just submissions that fire up specific clients independently of the other ones, just to run them through our existing NN tests and make sure they pass would be really helpful.
00:41:26.030 - 00:41:46.686, Speaker D: Okay, so we will just make pr just with additional beacon nodes of lodstar and prism connected to our client and let you judge what is best way. Maybe any suggestions appear? Yeah, thanks. No more questions.
00:41:46.868 - 00:41:47.920, Speaker A: Sounds great.
00:41:50.550 - 00:42:33.330, Speaker D: So for Lighthouse, I'd say we're maybe a day away from our full initial implementation. And then at that point until the testnet, we'll just be working on testing it out, trying to get it running, trying to get interop working. So I think we'll be ready. Do you want to test with different execution clients like that? Sorry, are you asking whether we plan to test with multiple execution clients?
00:42:34.170 - 00:42:37.110, Speaker A: Yeah, that would be the hope.
00:42:37.180 - 00:42:40.694, Speaker D: I think initially we'll just start with.
00:42:40.732 - 00:42:43.542, Speaker A: One, but probably initially get.
00:42:43.596 - 00:42:45.480, Speaker D: But then maybe another mine next.
00:42:49.080 - 00:42:49.830, Speaker A: Okay.
00:42:56.100 - 00:43:03.330, Speaker B: Just to clarify, are there any clients who have concerns about the cryptography or need stuff to be done there?
00:43:08.170 - 00:43:14.150, Speaker D: We can make it work in Lighthouse. There's just some inefficiencies, I think, for the time being, but that's.
00:43:18.570 - 00:43:19.222, Speaker A: It from.
00:43:19.276 - 00:43:32.960, Speaker D: Taco on this, because we started working on the java binding. You might be interested. Also the better team. So, yeah, from the cryptography, I think we will have something in the coming days.
00:43:35.010 - 00:43:36.240, Speaker A: Yeah, that's it.
00:43:44.470 - 00:44:02.220, Speaker D: Georgia, there have been concerns in the past around performance of the mempool during verification when many transactions arrive. And I wonder what the state of benchmarks there are, if any.
00:44:12.210 - 00:44:19.460, Speaker B: I don't think we have any there, but yeah, I'd like to hear more about what the concerns are there.
00:44:20.870 - 00:44:54.266, Speaker C: Well, I think Moffat did some benchmarks. It was like five milliseconds to verify the blobs of a transaction. I don't know the entire transaction, but I think the cryptography part was five milliseconds. I know that also, Antarct has an argument on why we don't expect to see too many transactions under normal case on the mempool. But I don't know if you're wondering about malicious.
00:44:54.298 - 00:44:56.590, Speaker D: What about in the existence of adversaries?
00:44:59.810 - 00:45:00.942, Speaker C: What's up? Say that again.
00:45:00.996 - 00:45:22.658, Speaker D: Sorry, I'm just asking a question to be clear. I don't have any opinions here, but I understand that in the happy case, yeah, there's going to be few blob transactions might be fine, but I would urge you guys to think what happens when an adversary spams, finds kind of like an optimal number of transactions and abuses the fact that there's no batch verification done on a client.
00:45:22.834 - 00:45:51.280, Speaker C: Right. I think one thing to consider here is the cryptography thing. Even if the cryptography verification took one millisecond, I feel like it doesn't make a difference. Adversarial scenarios. But if it's one millisecond or five milliseconds or three milliseconds, it's in the same order of magnitude, and hence it's still the same.
00:45:52.130 - 00:46:13.480, Speaker D: Maybe put the question, maybe putting it in anywhere, like more granular format. How does the attacker's cost profile look like when they try to abuse it? And if it's fine, it's fine. But that's the question, I guess. Can an attacker come up with a sequence of blobs that takes some time, that makes it for me, for whatever. I don't know. Just asking.
00:46:15.210 - 00:47:15.158, Speaker F: Right. So I think basically this is mostly a question about mempool implementation, mempool logic. So now that we have disabled broadcast, I don't think it is a like, this is not a problem that can bring down a node anymore, at least if the client is probably implemented and has some throttling for requesting blob transactions. But what it could do is of course if it's naively implemented, you can spam a p and then the people would have to stop processing blob transactions. So you could kind of bring down the blob transaction propagation throughout the network. And so things that can be done here, and maybe a question would be like should we have a place to kind of talk about this, to specify this or something, or should that be up to clients? But what you could do is a you can batch per peer verification, because ideally you'd want to disconnect from a peer if even a single verification fails. So you wouldn't have to do like bifurcation to actually find out which of the transaction fails.
00:47:15.158 - 00:47:50.360, Speaker F: You don't care if they send one invalid one, then that's it for them. And also ideally what you would have is you just have a per peer throttling where basically you only request five blob transactions per peer per slot or something. And if you have some rudimentary logic like that should all be fine again, because it's up for clients to implement. This is not part of the IP or the specification. So I'm just wondering, should there be some central place to discuss this, or should there be just something left for clients to make calls on?
00:47:57.260 - 00:48:23.264, Speaker D: One thought here, Ansgar, and I was rereading the AIP yesterday, would be that just use some should must rfc logic or language, I don't know, but it feels like some guidance should be given there, because given the fact that we are having this conversation now and half the room kind of like went silent when I asked the question, makes me think that we haven't thought about that enough, or maybe three people have thought.
00:48:23.302 - 00:48:24.050, Speaker A: About it.
00:48:28.490 - 00:48:33.900, Speaker D: Or maybe it's not an issue and we can just write it down somewhere, but it should be somewhere about the process.
00:48:35.870 - 00:48:36.330, Speaker A: Right?
00:48:36.400 - 00:48:53.338, Speaker F: Maybe the best place would actually be, as you were just saying, basically the kind of the rationale, kind of backwards compatibility section of the AP where it's not required specification, but it is advice for client implementers.
00:48:53.514 - 00:49:07.938, Speaker D: Yeah, and also would that be helpful? Mariju just said something about the separate mempool. Also, this is the first time I see this, so again, there's a bunch of ideas and it would be good to crystallize. Again, just in the best interest of.
00:49:07.944 - 00:49:37.178, Speaker A: The I think I think what you proposed makes sense. If you can add something to the EIP, I can also link it in the readiness checklist and when get has an implementation for this separate transaction pool, we can also reference it there. I think the EIP should have at least a mention of it, but then we can track ways people deal with that somewhere else, right?
00:49:37.264 - 00:50:09.414, Speaker F: Is there any value? I mean, what I can do is I can just, whatever thoughts I have, I can just put them into the AP. I'm always with this slightly worried because it's so much client implementation details that it might just be something that's not a realistic approach for clients to take. So ideally, of course there should be some sort of I would be kind of happy about some sort of client feedback. Is there some avenue to take here to kind of get clients to talk about this? Or should I just put something in the app to begin with and then we can talk about whether clients think that's reasonable, right?
00:50:09.452 - 00:50:17.500, Speaker D: And maybe there is a higher order bit here or conversation to be had around the feedback loops between the EIP process and client development. But I understand this is out of scope for this.
00:50:19.710 - 00:51:14.540, Speaker A: Think let's add it to the EIP. We can obviously always discuss it on awkward once once clients are a bit farther along. But I think my feeling is probably a lot of the EL teams just are not even that far yet, and so if it's in the EIP, at least we won't forget about it. It'll be there and then we can obviously discuss it on all core devs or on Discord async. But it feels like beyond get. There's basically not a team that has spent the time to consider this very deeply. Anything else on the dev net? And obviously, yeah, I guess just to be extremely clear, solving this is not in scope for the Devnet three, obviously.
00:51:14.540 - 00:51:23.840, Speaker A: Okay.
00:51:27.490 - 00:51:29.440, Speaker D: I have clear status updates on.
00:51:30.210 - 00:51:32.894, Speaker A: All the clients, which I think all.
00:51:32.932 - 00:52:13.458, Speaker D: The clients seem that previous lighthouse, Nethermind, Lodestar, guess, and Aragon all seem on track for the Devnet, but I couldn't quite tell what the status was with prism. I can give an update. Sorry, I can give an update I'm also working on as well, so I'm mostly done with the synch PDP changes. I think we are on track. Before we start a Devnet, I do really want to run through the EIP 44 changes with the consensus layer spare test and then switch away on that just to make sure that we somehow align with consensus, so therefore we don't just fail right away. We definitely do not want that. So yeah, we are on track.
00:52:13.458 - 00:52:15.010, Speaker D: That's the TLDR.
00:52:17.830 - 00:52:59.086, Speaker A: Okay. And I guess this means all the client teams that have previously committed are on track. So next week we're going to have a call like a day before we were expecting to launch the Devnet. Ideally by then, I guess. What's the thing we want? Yeah, by next week, 24 hours before launching the Devnets, we should have clear branches or prs for every client. Is it on every client to add themselves to this interrupt repo? Basically that's like the thing we would expect. Or I guess.
00:52:59.086 - 00:53:06.770, Speaker A: Yeah, I'm trying to guess what's the product that we want out of client teams before we launch a devnet?
00:53:08.390 - 00:53:25.800, Speaker D: I think we should do something fairly similar to Kinsuki or like mfura. Just have some. The client tracking sheet was really useful. And then maybe define some sort of milestone base like m zero, m one, m two. And then you keep building on top of that.
00:53:27.290 - 00:54:22.250, Speaker A: Okay, I think that makes sense. I can put this together, I can put the hackmd together. But then we're saying like the client teams, basically we just provide the Genesis file and we kind of do like m four where you check, you're interoperating, it works and whatnot. And then we can add some, I guess, some boot notes in that file as well, so people know where to connect. And this means basically it's not going to be like a single docker repo that just runs everything, but it's more like any team can connect to the devnet with the right Genesis file and peers settings. Is that correct? Yeah. Okay, so yeah, I'll put this together in the next couple of days, just so we have this definite checklist.
00:54:24.190 - 00:54:34.240, Speaker D: Kim, I did like your idea of having everything in the interoperability as well.
00:54:34.690 - 00:55:02.360, Speaker A: Is that better or worse? Because it feels like it might be better from the perspective of it's tractable and we see where it is. Is it worse in that it makes clients do all this config work that is not really realistic with how we actually run networks and also maybe ends up being kind of a crutch if you can't run prism separately. But I don't have a strong opinion there.
00:55:06.090 - 00:55:14.330, Speaker G: I mean, it provides that initial sanity check of is it going to sync with our other clients?
00:55:16.030 - 00:55:30.814, Speaker A: So say we use this milestone approach, I assume it's easier for clients to have a branch that's compatible with the devnet than to have that branch part of the interoperab repo. Correct. They sort of need the first to get the second. Yeah.
00:55:30.852 - 00:55:36.894, Speaker G: Which is fine, because the interop repo is simply a sub module which we can point at any branch.
00:55:37.022 - 00:55:37.698, Speaker A: Yeah.
00:55:37.864 - 00:55:38.482, Speaker C: Okay.
00:55:38.616 - 00:56:22.132, Speaker A: So what I'll do, I'll just separate those out in the milestone stock. So the second to last one is have a branch that people can use to run on the Devnet. And then the last one is like, have that branch tracked and part of the interoperab repo. I guess then at the minimum, what we'd want for next Tuesday is everyone has like a branch that's working that follows the spec for Devnet three. And then ideally it's all in the interop repo in like one clean place. Yeah. Okay.
00:56:22.132 - 00:56:48.232, Speaker A: And we only have two minutes to go. I guess the next two were more of just like an announcement. Heads up. But we have this on Thursday. We have an awkward abs where we want to talk about Shanghai CFI. Oh, sorry, xiaoi, I didn't see your hand. Do you want to go first? Yeah, just in case anyone didn't see the meeting chat.
00:56:48.232 - 00:57:18.724, Speaker A: So there's a bug in the test vectors that we released last Friday. But I am generating the new test vectors and we will publish it in 24 hours. That's not the spec changes, it's a bug configuration. Yeah. Okay, got it. Okay, great. Sorry.
00:57:18.724 - 00:57:43.648, Speaker A: What I was saying is there's all code devs this Thursday we're going to talk about Shanghai's CFI lists. There's been obviously a formal proposal to add four. Four as CFI proto. Just put this on the Cordev agenda. He posted an update also on ETH magicians and on GitHub about the status of the EIP. So I think that's pretty good. At least we can kind of present where things are at.
00:57:43.648 - 00:58:51.090, Speaker A: Maybe the one thing where it would be good to have an update is if there's any testing tools or things we've worked on to kind of link those on the readiness checklist. And then the other part, maybe that would be good to have a sort of written update about, is just the status of the different bindings for the KZG library. I believe every single client team is covered, but just being able to point to that, I think would be valuable. So people can kind of know that they exist and that it's supported, I guess. Is there anything else people think we should try to explain or kind of put together before awkwardevs on Thursday? Okay. If not, I guess this is a good place to end. Yeah.
00:58:51.090 - 00:59:04.794, Speaker A: Thanks a lot, everyone. And we'll chat with most of you on Aquodevs and otherwise next week on this call to launch, just Devnet. Cool.
00:59:04.992 - 00:59:06.970, Speaker D: Thanks, everyone. Awesome work.
00:59:07.120 - 00:59:08.154, Speaker A: Thank you.
00:59:08.352 - 00:59:09.178, Speaker F: Thanks, everyone.
00:59:09.264 - 00:59:10.310, Speaker A: Bye. Thank you. Bye.
