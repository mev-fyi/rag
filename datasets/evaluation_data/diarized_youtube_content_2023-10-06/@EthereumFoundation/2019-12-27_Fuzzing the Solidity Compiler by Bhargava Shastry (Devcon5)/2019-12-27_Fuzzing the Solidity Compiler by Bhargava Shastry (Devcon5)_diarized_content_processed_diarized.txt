00:00:16.010 - 00:01:26.470, Speaker A: So with that out the way, I'll get started. So like I said, usually in the information security community the, the focus of security testing are things like network switches which process external data and provide access to private infrastructure. So compiler is not, let's say, a popular target. So let me be clear and say that our threat model is not the compiler user who's a developer. So we don't assume that the person using the compiler is malicious, but rather we focus on if the code generated by the optimizer is correct. Right? So for example, if you have a function, this is use style program, essentially what it's doing is finding a function foo which returns x and essentially setting that x to two and it's storing at location zero in EDM, in the memory, the value returns by two. So essentially you expect this to store the value two in the location zero, right? And this is what you get on the right hand side after optimization, which is three times two.
00:01:26.470 - 00:02:15.320, Speaker A: So essentially we want to make sure that it's correct code, right, and the compiler behaves as expected. So for those of you who are not familiar with fast testing, fuzzing in a nutshell is essentially like this. So in a loop what you do is you generate input and feed that input to the program under test. And essentially you can do it for as long as you want, but typically you won't have any gains beyond a certain threshold. So at some point you do control c or end the whole process. It's been shown to be surprisingly effective at finding bugs because most often unit tests capture a subset of program behavior. And here we randomly generate things that are usually chronic cases somehow and break the program.
00:02:15.320 - 00:03:02.642, Speaker A: However, applying traditional fast testing is limited because our use case is essentially testing the compiler. And it's very important to generate valid programs for the compiler, which means, let's say you have the valid program on the left. So let's see, program on the left, which defines the contract, and a function poo inside the contract that does something right. And the father. Let's assume that the fuzzer applies a mutation. Mutation is essentially any operation which tweaks bytes, adds bytes, or removes bytes from the byte stream on the left. So the fuzzer sees the input as a stream of bytes on the left and just tweaks bytes and adds removes.
00:03:02.642 - 00:04:04.330, Speaker A: So it could create a mutation like the code shown on the right, which basically tweaks the keyword function and public right, because it's totally random. So you can imagine that with a very high likelihood, a random mutation at the input level is going to be simply rejected by the parser. And we don't want that because we want the program to be parsed and then optimized and then test the optimizer. So this clearly won't work, or it works, but it's not very efficient. So basically the learning is that hobling a compiler requires generating valid programs, and now generating valid programs requires some sort of structure awareness. So let me talk about what is structured awareness and how we approach this problem. So essentially we start with a high level specification.
00:04:04.330 - 00:05:04.110, Speaker A: This specification is written in the interface description language called protobox or protocol. Poppers is the whole thing. It was originally developed by Google, but it's used for various purposes, also in guessing. So essentially in the protocol language you can define units of data as messages, and each message contains one or more fields from other messages. So for example, it's useful to talk of the specification of the u programming language that we are testing in a top down fashion. At the very top you have a program. So the message program which contains a repeated sequence of a message called block, which we also define as a repeated sequence of a message called statement, and so on and so forth.
00:05:04.110 - 00:06:02.000, Speaker A: Although it's not shown on this slide deck. You could define a message called if statement, a for statement, so on and so forth, which contain other fields, and then make statement a union of all these statements. You could use the keyword one off to make the union of these statements. So essentially build this specification in a top down fashion until you have all the leaf notes, typically literals or consonants and stuff like that. And yeah, essentially try to cover as many aspects of the programming language as possible. Bear in mind that this is fully handwritten, so it's not exhaustive or complete, but for the purpose of testing, the hope is that it covers sufficient language features for us to get a sufficient assurance that things work as expected. Of course, you can find a full spec at the link below if you're interested.
00:06:02.000 - 00:06:43.998, Speaker A: So the next thing is input generation. We have a spec. How do we convert this spec into a valid input? We don't generate the input ourselves. Fortunately, there is a library called lip Rotabuff muta which is also developed with Google, which takes the specification shown in the previous slide deck and converts it into a valid input which is an instantiation of the spec. So each input is essentially a tree. For example, it can look like what is shown below here. So it defines blocks and blocks contains a statement, which is an if statement in this case.
00:06:43.998 - 00:07:30.362, Speaker A: And the if statement has a condition which contains a binary operation, which is an equality. And the first operand of that equality is a variable reference. The variable's id is zero, and then the constant that it is being compared against is zero. This is the textual form of protocol message for clarity. But of course this doesn't really make sense yet to feed this to the compiler, right, because the Yule optimizer does not recognize protobuff. So we need a program that converts an instance of the protobuff message into a valid Yule program. And this is where the converter program comes in.
00:07:30.362 - 00:08:03.638, Speaker A: So this is something that we have to write. But fortunately this is not too complex and about thousand lines of code. So essentially converter is a source to source translator. The input is the protobuff serialization format and the output is the U program. So we talked about the protobuff message in the previous slide, x. How it looks like when you convert it to Yule code is shown at the bottom. So essentially it's an if statement with a variable called x underscore zero.
00:08:03.638 - 00:09:00.486, Speaker A: If it checks that it's equal to zero. Of course this is a snippet of a larger piece of a program, so it doesn't make sense to feed this to the father yet. But this is just to give you an idea how the conversion looks like and what's the input and the output of the conversion process. But in reality we have a complete valid program which compiles and does something. So to put these two pieces together, what we have is a specification that we handwrite in the beginning and a library called lip protobuff mutator. We use that library to generate input, but that input is not ready to be fed yet because it's in protobuff language. So we write a program called Protobuff Converter which converts from this language to a valid test program that can then be fed to the compiler.
00:09:00.486 - 00:10:03.598, Speaker A: So finally we have an input that could be used to test the compiler. But then testing the compiler actually requires encoding an expectation somehow. So imagine that you randomly create a test program but you don't know what it's supposed to be doing, what side effects it has. So how do you encode an expectation? Right? So how do you check that it's doing the right thing? The approach that we use is differential fast testing. So essentially it involves tracking the side effects of a program using execution race, running the program and then running the optimized version of the program and comparing the side effects. So we use the original program as a baseline to compare against, and we compare that with the optimized program, so it's not too complicated. However, before we can do that, we need an execution phase which tracks side effects of the execution of the program.
00:10:03.598 - 00:10:36.818, Speaker A: Right. So we need to know somehow what is happening to check whether it is happening correctly. Post optimization. And this is where the Yule interpreter comes in. Yeah, Yule interpreter is essentially an interpreter for Yule programs that was written by quiz. So essentially what it does is it interprets arbitrary apart from interpretation. What it additionally does is outputs the side effect of the program as a trace.
00:10:36.818 - 00:11:39.146, Speaker A: A trace can be thought of as a string. So for example, you have the test program on the left, you feed it to the UL interpreter, it executes it step by step and then creates this execution trace shown on the right, which can look like you load something from memory, from some address x, and then store it, store some value at address y, do a data copy, so on and so forth. So we build the execution trait of the test program using the Yule interpreter. And finally we are ready to actually put all of these blocks together and test the optimizer. So we start by generating the program, feed it to the interpreter, optimize the same program, feed that again, the optimized version to the interpreter, and then we get two execution traces, which are essentially strings, and then we can simply do string equality check. Right. So if the execution traces of both these versions are equal, everything's fine, as we expect.
00:11:39.146 - 00:12:12.520, Speaker A: If it's not equal, that's a bug most likely in the optimizer. But in practice we've also had situations where the bug is somewhere else, but with high likelihood it's the optimizer. So essentially, like I said, we found about this approach being pretty effective. In practice. We found about seven bugs, two of which were in the optimization rule that was used to essentially optimize programs. Five were in the experimental Yule optimizer. But yeah, it's not supposed to be used by end users right now.
00:12:12.520 - 00:13:00.120, Speaker A: So that's the purpose of testing anyway. Two were in the EVM optimizer, so it was used in production, but fortunately it's low, very low severity. Yeah, mostly because the buggy rule in question was optimizing constants. So it was a very specific pattern that was going wrong. And this pattern could be detected visually because it's a compiler constant or. Yeah, that's essentially why it was low. The others were of course in the experimental version, so that's why we're testing it.
00:13:00.120 - 00:13:50.834, Speaker A: So that's what we've done. Challenges remain going forward. The main thing is we would like to find high severity bugs before the compiler shipped. Right, because that's what matters. The main problem with fuzzing a compiler for correctness is that it's usually a slow process. I mean, typically in fuzzing you select a small piece of code that's security critical and fuzz it, which means typically you would like execution speeds of over 100 /second but for example, if you want to test a component, we're starting to test the API V two encoder inside solidity. The problem is that compilation is slow and this is perfectly fine for the use case because developers can spend an additional second or so to save gas.
00:13:50.834 - 00:14:33.646, Speaker A: Right. The only problem is that if you apply for processing, it can become a bottleneck. So to find ways to basically make it more suitable to fuzzing is a challenge that we are currently working on. So in conclusion, we started doing continuous structure aware fuzzing to detect problems with the optimizer and alert us whenever there's a bug in the code base. It has been so far used for mainly testing the optimizer and data end and decoding. It has decent assurance. But bear in mind that testing is not formal.
00:14:33.646 - 00:15:12.366, Speaker A: It doesn't give you any formal guarantees. Yeah, take that with a grain of salt, but yeah, that's about it. Thank you. Any questions? So how much do you know about the coverage that you're able to get with, say, one day of buzing? With one day, it's hard to say. But you said you run it every day, right. But it builds a corpus over time. So we started and then it builds a corpus and the same corpus is used.
00:15:12.366 - 00:16:01.434, Speaker A: So you can think of it as a cumulative curve. It improves over time and then keeps increasing, essentially. So essentially right now, as it stands, just for the optimizer, it's pretty good. It's about 90 93% something of edge coverage. Yeah, and we keep trying to keep an eye on it and see if we can improve it somehow. Do you have an idea of what's missing? Is it syntactic features of the input language that aren't in the protocol buffer specification, or is it something else? So I believe the syntactic issues are not a problem right now. The main challenge is to keep up with language improvement.
00:16:01.434 - 00:17:00.340, Speaker A: So a new language features get added and that has to be reflected in the protocol spec. So essentially to keep up with it and making sure that when there's some change, that change is covered in the fuzzing process. Maybe what is missing is to check if all branch statements inside the optimizer steps are covered at a very low level, but at a high level it's hard to say. Yeah. How do you decide when it's issue or not? Because you are introducing solar code, so you will obtain different gas costs. So when do you decide? So regarding gas costs, the Yule interpreter is oblivious to gas. There's no notion of gas, it just runs code.
00:17:00.340 - 00:17:55.620, Speaker A: Regarding how we decide, basically, there's a bug file by the fuzzing program and it has a minimized input. We try to rerun the input and check the side effects. It's pretty simple workflow, mainly because of the interpreter. So you get an execution trace, which is sort of readable, and you compare execution traces pre and post. And it's pretty straightforward, so it can quickly tell you whether it's a bug in the optimizer or there's some other code. Of course, like I said, we introduced code and there could be a bug in the code that we introduced, but it doesn't take longer than, I don't know, a few minutes to quickly decide whether, which bucket it falls into. We.
