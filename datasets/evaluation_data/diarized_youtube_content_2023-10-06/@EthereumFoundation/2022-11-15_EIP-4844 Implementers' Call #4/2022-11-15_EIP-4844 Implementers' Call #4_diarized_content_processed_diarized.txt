00:00:00.410 - 00:00:42.300, Speaker A: You okay? Let's kick this off. We have a group of people here, and I'm sure more will join in. Welcome to the fourth of these 4844 implementers call. We have a ton of spec stuff today, as always on the agenda. And then hopefully we can get through some updates on Devnet three on the large block test that we wanted to do. And then Terrence had a couple prs that he's put up that he wanted to get feedback on, if not on this call, at least async. So we can go over those quickly.
00:00:42.300 - 00:01:06.900, Speaker A: And maybe, I guess to kick us off. Ensgar, you gave some updates on a couple of your prs in the agenda yesterday. Do you want to just take a minute quickly to sort of walk us through where those three are at? Sure. Yeah. There are always like a few of these small ones also, just to check my microphone. Works well this time around. Yeah, great.
00:01:06.900 - 00:01:45.262, Speaker A: So basically I think it's four, but one of the prs didn't have a status change. That's the one where we just discussed whether or not you have a minimum price. So the three where actually something happened. First, the kind of throughput reduction pr is merged. So now the target and max are like a quarter megabyte and a half a megabyte. The understanding, of course, is that once we collect metrics, it could be that we end up setting a slightly different throughput for bringing this domainet. But I think as a kind of default, this makes more sense.
00:01:45.262 - 00:03:00.360, Speaker A: Now then, the second one is the pre compiled return values. That one is still not merged, just because I had some concerns, last call around the formatting of the return values, and I looked a little bit more into this, and it turns out indeed that there's some precedent with existing pre compiles that handle this differently. So there's this bn 128 pairing check precompile, for example, where the return value is padded. Also, it doesn't return explicitly return a success boolean instead of failing if the pairing fails. So I've reached out to the solidity team just to basically get their take on whether or not it makes a difference and whether or not it's worth basically following precedent here. It's a small change, but I think for this, given that also for testers and whatnot, nothing depends on this, right? So I would rather want to get it right and merge the thing that we can actually bring to mainnet and not have to change it later on. So this one is still pending, waiting for clarification there.
00:03:00.360 - 00:04:06.390, Speaker A: And then the last one that's the one that I had forgotten last week was the mempool situation with not automatically broadcasting block transactions. I opened a pr that adds a dependency on Marius's EIP 57 93, which is the e 68 transaction type announcement. But I do have one question, so maybe if we could briefly discuss this here. So the way the EFP works is that it only adds the transaction type and the size of the transaction to the announcement message, but it itself does not prescribe whether or not clients still broadcast these transactions. So this change itself basically just says in your announcement message you have to include this information. But then basically whether or not clients still auto broadcast these transactions by default, or choose to no longer auto broadcast over certain threshold, that's up to clients for now. I basically also kind of word it this way and just give it as a recommendation to stop auto broadcasting.
00:04:06.390 - 00:05:21.166, Speaker A: I was just wondering if people feel strongly that we should basically have that be a requirement required behavior for clients for blob transactions. So basically not just have it as a recommendation, but an actual requirement. I don't know if anyone has opinions on this. Otherwise, I think having it just be a recommendation makes sense to me. Yeah, and I guess maybe once we do this like blob spam test, if we see something break at the mempool level or gossip level, then maybe we want to make it more than a recommendation. Yeah, Marius doesn't look like he's here, but I'm just referencing back to the execution layer workshop at Devcon. And I think the key thing we were trying to mitigate here was DOS risk of the execution layer and basically giving execution layer more optionality in whether they kind of were going to take in blobs or not, or large transactions or not.
00:05:21.166 - 00:06:02.874, Speaker A: And so I can imagine Mary is saying that it actually is really important for it to be a requirement rather than a recommendation. I think we need someone like him or Peter to really weigh in on that, of how strongly they feel. Well, I'm not sure that helps because if someone's trying to dos the network, they're not going to follow that anyway, right? They're going to broadcast blobs like mad. Yeah, but if it's a requirement, not a recommendation, I think the idea was basically you could terminate the connection. The peer connection. Yeah. You can just stop relaying so they can broadcast blobs as much as they want as long as they're connected to peers.
00:06:02.874 - 00:06:33.974, Speaker A: But if every peer drops them. Right? Yeah, but anyway, yeah, I guess we can revisit, but it seems to me like you drop them anyway. Right. Regardless of you'd notice some spamming and drop them rather than you notice otherwise, how do you tell that they're actually broadcasting something? Right? Yeah, I suppose. Though if everyone's required to request rather than. Yeah, I suppose so. Yeah, maybe that does help.
00:06:33.974 - 00:07:43.962, Speaker A: All right, sorry. Could somebody articulate the concern in concrete kind of conditions that we're looking to violate from this? I guess that has been still a bit abstract for us. What are we trying to find? What does break mean? So I guess if nodes are just unable to process new blocks or get put offline because of the increased bandwidth requirements of getting a bunch of blobs that are either invalid, are these concerns written somewhere where we can point to or can somebody own? I suspect if they're somewhere already they would be in that EiP Anzgar link to. But let me check it real quick. Well, it's basically just the existing mempool issue section in the EIP. It's a really small section, but the idea is just that it's not only bandwidth, it's also compute. Given how expensive the blob verification are, the idea is just that you want to have some mechanism to throttle.
00:07:43.962 - 00:08:45.998, Speaker A: Ideally you could also do that in an existing world with peer scoring. But the execution clients just don't have a concept of peer scoring. So by the time they would basically validate blobs and see that, for example, there are invalid blob transactions in there, they just don't keep track of which peer this came from. So that would mean a complete rearchitecture texting of the mempool to be able to handle this. So instead, if you do it with announcements, then all of a sudden you have explicit control over like, okay, I only pull block transactions from this pier once every so so and many seconds or something, and then you don't even more zooming out. I guess my point is that the stress test that we're doing is not testing blobs, it's just called data transactions. And I think I heard that the concern is not just bandwidth, it's also compute because of the KSG verification cost in the mempool.
00:08:45.998 - 00:09:16.954, Speaker A: And that's something that we'll be testing with the stress test. Is it that everybody in this group understand that this is the case or have we not? So I think the stress test actually tests something different. We have two different consensus. We have the consensus about the size of the actual assembled blocks and then we have the Mem pool. And those are completely separate. And so what we are testing with that stress test is the size of blocks, because that is consensus critical. Right.
00:09:16.954 - 00:09:55.494, Speaker A: Everyone has to download these blocks, and so they might be kicked from the network. The mem pool side of things is somewhat more forgiving, because if you are like a very constrained bandwidth situation, you don't have to actually run a mempool. You don't have to propagate all these transactions. But on the compute side, for example, it's much more problematic, because on a block, for block verification, the compute is really negligible because you do one check for the entire assembled blocks. But for the block transactions, you have to do one check per transactions. There might be hundreds of transactions coming in. So it's much more of a compute issue there.
00:09:55.494 - 00:10:38.506, Speaker A: So again, those are just like, the stresses we're doing is just for the block propagation that's separate from mempool on this one. I feel like we probably need Marius to weigh in here, because he was the person who had a strong opinion on it. And given he's not here, do we want to push this async and ask for his feedback on the pr? Yeah, I think that makes sense. And I think the test also, the test, even just on the bandwidth side, is really valuable. Right? Yeah, sounds good. We'll get it done this week. Awesome.
00:10:38.506 - 00:11:15.262, Speaker A: I'm writing this today. We'll try to run it on Gurley by end of day. Might start doing more coordinated tests over the week. Cool. Also, just in terms of philosophy, very briefly, I think it's important to point out that mempool concerns are a little bit less of an issue, because we can always launch the CRP with a mempool that is very restrictive and maybe a little bit less efficient in propagating transactions, but doesn't actually break nodes and then make propagation performance better in the long run. Whereas block propagation really has to work. Right, because the entire network breaks down if it doesn't work.
00:11:15.262 - 00:11:41.666, Speaker A: So mempool is more forgiving. Right. Okay, so those were your three ansgar, and you said there was a first one. A fourth one. Sorry. That. Yeah, you said there was a fourth one that you hadn't got in, merged, or given an update on.
00:11:41.666 - 00:11:55.962, Speaker A: Oh, the minimum. The minimum cost. Okay. So just no update there. Exactly. Okay, perfect. Okay, so next up.
00:11:55.962 - 00:13:04.110, Speaker A: What was this, Dan? Okay. Danny can't make it, unfortunately. But he was reviewing the spec yesterday and highlighted this issue where, I think two calls ago, we decided to not verify the blobs as we are propagating them. And Danny was saying that there's a way in which this means any node on the network might be able to propagate an invalid blob, which is not something we would like. I know Mophi, the two of you were chatting about this yesterday. You have a quick update on where things are at? Yeah, yeah, I think we sort of figured out that it is a concern. The options we have is to either use reliance signature verification to deter invalid cypress from being gossiped, or rely on the KCG commitments.
00:13:04.110 - 00:14:27.690, Speaker A: And based on the convoy, I'm leaning towards using signature verification as Danny proposed in the pr. Yeah, open the feedback there from anyone else that has so we were looking at the times it takes to do the KZG verification versus signature verification, and I think the difference for 256 kilobyte blobs is like two milliseconds. And I think that's probably not significant enough to warrant the signature just because the signature requires the changes to the beacon API and the validator client. And it'd be nice to avoid those changes if we don't have to. But yeah, I mean, if we end up going with bigger blob sizes, it would be okay to add later too. So sort of my two cent is like, maybe not. Maybe we shouldn't prematurely optimize for this because we also could do things like figure out KZG verification of the blob in parallel with other aspects of gossip verification.
00:14:27.690 - 00:15:18.970, Speaker A: So you're saying it's like almost because the blobs are not that big now. It's not the end of the world if you're not doing the signature check is that we do need something. And it's either we need to do the KZG verification or the signature check. And the KZG verification is more substantially slower the bigger blobs are. Right? Okay. If we're targeting relatively small blob size sizes, it might not warrant signatures yet. If things are going well and where we actually have a lot of headroom, I think it'd be okay to add signatures later.
00:15:18.970 - 00:16:14.314, Speaker A: Because signatures, it impacts the scope of work by requiring changes to the beacon API and validator client. Right. I guess I'm curious if other client teams, do any other client teams on the CL side have thoughts on that? Yeah, I agree. It feels like a premature optimization. Okay, is this worth, I guess, bringing up on the CL call Thursday, or should we just comment on this issue and try to resolve it async in the next couple of days? I mean, I think it's still worth bringing up again, but yeah, I'll reply on the issue. Awesome. Yeah.
00:16:14.314 - 00:16:52.120, Speaker A: If you want to share that on the issue. That'd be great. And then we can follow up on it on Thursday's call. Sweet. Okay, so next one after that, I just wanted to follow up, I guess, on this rebase PR. I know we've been working on this a lot and there's comments on it from yesterday and today, but yeah. Anyone have anything they want to share, bring up on this.
00:16:52.120 - 00:18:31.220, Speaker A: And just to reiterate, the point of the ruby based PR, the point of it is to make it easy for client devs that are already working on withdrawals to add four four four. On top of that, there is a clause in the spec in the PR to make it easy to disable withdrawals just for testing. And I think Danny brought up a comment of what are the other ways of doing it? And the approach we went with is to simply no op withdrawal sensitive functions such that there are no withdrawals in the beacon block and the withdrawal route is whatever the route of like an entity tree should be. So that's kind of like where we're going with testing. This lets us avoid introducing withdrawals in the El so that we can test eip four four on els that haven't quite fully implemented withdrawals as soon as possible. And also it lets us avoid any bugs in El or the Cl that could be attributed to withdrawals. Okay, so I guess, yeah, we can just keep following up on the PR there, but it doesn't seem like there's anything urgent to decide here.
00:18:31.220 - 00:19:20.660, Speaker A: Um, okay, I guess, yeah. The next thing I wanted to make sure we covered was just Devnet three. So last week all of the. Or I guess, yeah, six teams said they were trying to get this implemented so that we could launch a Devnet around November 30, which is two weeks from now. So yeah, I'm curious to hear about just the different implementations, how things are going, if people think this is still possible, or if there's any issues that seem to block this. If I can give an update on some of the work I've been doing. So I've got devnet v three, I've got Aragon and Prism updated to most of the new spec.
00:19:20.660 - 00:20:00.510, Speaker A: The pending issues are the PR. We just discussed some of the Capella rebasing as well as I think we still need a couple beacon, the sidecars with the beacon blocks and the consensus layer. So those are pretty close. Those are in great shape. I've also been spending some time pulling out some of the remaining KZG code, mostly from the execution layer into what I hope is a clean library. That's in the cryptokzg package within geth, even though it's in geth. I'm using it both for the consensus layer as well as the execution layer, KzG related or EIP 4844 related functions.
00:20:00.510 - 00:20:49.710, Speaker A: So all the go clients can hopefully share that. Likely that stuff will move into go KZG or some similar external library, but for now we're trying to get it right there. Once it's ready, then we can then think about moving it out. And I think there's some little bit of discussions like should it contain version hash related functions or should it just be KZG? So there's some discussions around the edges, but anyway, I think it's in pretty good shape for people to start using it if you're writing in go anyway, and I'll be starting to integrate that into Aragon this week. Nice. Really cool. Any updates from Nethermind? Yeah, so we are integrating everything like in master branch.
00:20:49.710 - 00:22:42.580, Speaker A: No new gas calculations still there, and probably we have no bundle v one in point being added, but everything goes quite smooth. I hope we will join with everything difficulty. Actually the main difficulty was to repeat all the binary layer of encodings, new encodings we have in the cap, I mean ssv encoding. Well, what else? Different hash algorithms and like a bit different layout for transaction encoding. This what slows a bit, but in overall it's okay. The main problem I see now is we still have the same outputs as GokZG library for CKZG library, because as far as I know, implementations a bit different in terms of like getting some internal hashes because it is based on different binary layout like that. So that's why it's hard to synchronize with guest and other go implementations now.
00:22:42.580 - 00:24:01.590, Speaker A: And we just skip verification for now. But I hope we will wait for update from go library and we will have some tests for both libraries with the same inputs and hope to get the same outputs. That's the only issue which is not resolved for now. Got it. And who is working on these harmonizing these libraries? I know, yeah, I know there's a bunch of people working on them, but is this just something that we're tracking? I don't know. I started with test vectors, but I don't think we've started integrating any of them. I can definitely speak with LXC offline to sort of interrupt the GOkZG with the CKZG wrapper that he's using.
00:24:01.590 - 00:25:02.480, Speaker A: Okay, nice. Would that library I have in geth be a solution to this? Yeah, so I'd basically just make sure that that is interoperable with whatever Alexey is using. Cool. I had a question about the CKZG library. So are we expecting all the bindings for different languages to be hosted on the CKZG repository itself? Like who would be responsible for maintaining the different language findings lit be in the CKZG repo or for Rush? Like maybe us as Sigma prime, we can have it in our own repository. Like the bindings for rush, something like that. So just wondering what other language bindings we're thinking of.
00:25:02.480 - 00:25:50.990, Speaker A: Yeah, so right now the go and the ckzJ stuff is pretty separate and even the gokzj stuff is somewhat scattered about. So I think the short answer is no, we don't have a good story for that yet. Whether we should consolidate, I'm not sure we need to consolidate it all in one. We just need to make sure that they all work together right. Work the same. But maybe people have other opinions. Yeah, we were discussing this today actually, because we are starting working on the java binding and we were asking ourselves if we want to have a separate repo for the java binding, or try to initially stick everything in the current binding directory in the library.
00:25:50.990 - 00:26:55.908, Speaker A: And I think if it is the idea, I would prefer the second. Actually the end state will be having everything in the same repo. What do you think? Yeah, it makes sense. But another question I had was in the end, maintenance of the CKZT library itself, will it be done by the EF? Whoever maintains that library would also have to maintain binding for different languages, so that might be hard or not. I'm not sure about that as well. Well, the solution will be maintained by several groups. Maybe someone are more cn crypto oriented, but maybe the bindings inside the directory will be mostly maintained by different groups of people.
00:26:55.908 - 00:27:43.056, Speaker A: I don't know if this is something that makes sense. Yeah, makes sense to me. Like I made a draft pr for the rush bindings today to the CKCG. Dancrad spoke of the CKCG libraries. That was our intention, at least from Sigma prime, that we would want to get the rush bindings merged into that repository. And anytime in terms of maintenance of the bindings itself, we could help with reviewing prs and stuff like that for rush stuff. My understanding from the latest conversation in the telegram group is that Romana is going to maintain the kind of core crypto and c libraries.
00:27:43.056 - 00:29:10.984, Speaker A: And then that, just like you guys are suggesting, we're going to have kind of separate people maintain the bindings, but all in one place. Yeah, I just wanted to clarify. What I added in the comment was I think if it's just bindings for the CKZG library, then it probably doesn't make sense in the same package. GokZG, however, is not a wrapper around CKZG, it's its own thing, so that probably belongs should be remained separate. Also, rust have completely different implementations, not simply a wrapper, am I right? I think there are multiple pure rust implementations also around, but I was talking specifically about the bindings to CKCG. Okay, and there's also a comment in the chat about working on common test vectors. Is that something anyone is working on or wants to work? Yeah, I guess we don't have to do this now, but I know in the past, like for say BLS, we would fuz all the libraries against each other pretty extensively, so that seemed like something it's probably valuable to do as well.
00:29:10.984 - 00:30:12.000, Speaker A: But yeah, it doesn't have to happen now. Okay, and I guess yeah, I'm curious to hear so this was mostly for folks on the El side with regards to participation in the devnet. On the CL side we have prism, Lighthouse, and Lodestar say they would participate. Anyone from those team want to give a quick update on where things are at? So for Lighthouse, we're still planning to participate in the next devnet. Right now I'm working on the peer to peer portion of things, and Poon is working on integrating the rust bindings for KZG that he just finished implementing. And yeah, I think we're on pace to get there. Nice for prism, Tim.
00:30:12.000 - 00:31:10.190, Speaker A: Yeah, I guess speak for the Prism team and the stuff I'm working on. So the work is basically incorporating all the four four for rebase on top of Capella. We are basically going ahead with what I have in the pr for the rebase targeting that assuming it gets merged, and also working on adding the logic for the rebase on EIP four on Capella. It's still going, just a lot of development and refactoring work going on. Nice. Lodstar. Yeah, Lodstar I think is on track to be included in this devnet.
00:31:10.190 - 00:31:59.640, Speaker A: We now have Lyon from Lodestar on this call who might want to be giving updates in the future. As I try to hand this back to chainsafe, I think it's looking good. And we now have Lodestar integrated into our interop testing repo with only the first of our test suites running against it. But it's using the updated gossip topic and the chain advances and that all works. It doesn't actually save the blobs yet, but most of the foundation is there. Sweet. And I guess anyone else think they might join the testnet or still those six teams? Well, a small update from Tegu.
00:31:59.640 - 00:32:51.660, Speaker A: We are working contemporarily with progressing with Capella and 4844. Actually, Capella is definitely far ahead with the works compared to the works for the 4844. So we are progressing in parallel strictly and pushing things directly on master. So things are progressing, but I don't think I'm going to change the idea that we are able to join the Devnet. So far, I don't think the next one will be the one for us. Sounds good. Happy to hear about the progress.
00:32:51.660 - 00:33:30.804, Speaker A: Anyone else want to give an update? This is Andrew from Ethereum js. I'm just going to comment that we're not going to be ready for Devnet three. I don't think I've got a mostly done local implementation and I'm still testing it. I'm still trying to get it to work against the local version of the Devnet just from the interop repo. So until I feel like we're able to actually trade blocks and serialize and deserialize transactions, there's no point in me trying to go to a public devnet at this point. But I am plugging away at that at some point. It would be nice to kind of understand the current status of the actual eip.
00:33:30.804 - 00:33:44.012, Speaker A: It feels pretty out of date in terms of like, the pre compile spec doesn't match up to what y'all are talking about. Right. Trying to understand. We can talk about that later offline. It's not important now. I don't want to take too much time. Just submit that I do have some questions about that at some point.
00:33:44.012 - 00:34:03.540, Speaker A: I've talked to Kev a little bit, but could you some clarity on a couple of. Yeah, the pre compiled is probably the bit where there's the like. Just like we were talking. There's the most potential changes, but aside from that, it should be pretty fixed. Okay. All right, well then I'll look at it again. There were a couple of places that I went astray.
00:34:03.540 - 00:34:23.892, Speaker A: Andrew, feel free to reach out to me. I'm trying to keep on top of all these things. To keep. No, that's cool. I appreciate it. I was still just trying to get all the code in place to even start testing it within. But maybe in December.
00:34:23.892 - 00:35:28.764, Speaker A: That's probably the earliest. I think with travel plans around Thanksgiving, I'm not going to have a lot of time to work on it and get ready for Devnet three. All good? Yeah, thanks for the update. Anyone else want to share any development updates? Okay, if not, moving on. The next thing I wanted to chat about was this large block spam test. Giorgio's posted an update on the agenda about the way they're thinking about doing this, and given that we're probably going to run a testnet run in the next week or so, I'm curious to just hear feedback from folks here about is there anything that we're missing or that we should be looking at or do you think basically George Joseph's approach is broken in some way so that once we start running them, if we can get that feedback, or before we start running the test, if we can have that feedback, that's great. And then otherwise we'll probably run this on testnets a few times and share results here before we move to yeah, maybe.
00:35:28.764 - 00:36:24.560, Speaker A: Georgia, do you want to take like a minute to just explain sort of how you're thinking about approaching this and then see if there's any feedback, thoughts, comments on it? Sure. I'm walking so it might be a bit noisy. Tldr my understanding is that we want to create sustained load on the network for some time and we're going to do it in chunks of some size which we want to parameter default base case. We're going to do 128 possible we want to try out bigger one. Don't know when that would be useful, but it's possible we want to test it up and it's generally low cost. Just make the tool generic over whatever transaction shape that we want. So my idea is that we'll just make a general load tester for call data transactions and it's going to be able to submit either to RPC and lend it on maybe to a builder to bypass the 128 kb.
00:36:24.560 - 00:37:34.770, Speaker A: Not in scope for me to do any metrics gathering and I assume that either somebody will be watching or that there could be an exposed analysis. Curious for any thoughts, reactions if there's no feedback. I guess one thing that's probably worth emphasizing is right now we have a prism branch which is specially configured to track a bit more metrics than they usually do. So we'll be obviously looking at on chain metrics and we can run some other clients as well. But if any other client team thinks it's like trivial for them to add some extra metrics and monitoring, it might be worth just looking at what prism did and that would help kind of sanity to check the data across more than one client. I don't have the branch real handy, but I'll try and find it and post it in the chat here. And we can have it in the notes.
00:37:34.770 - 00:38:20.260, Speaker A: But yeah, worst case we'll just have the on chain data and this prism that's specially configured. Yeah. Oh, I found the branch. So I'll just post this here. Yeah. And I guess once we get results from this first run, we can come back and discuss them here and see if there's anything we want to tweak before going to Mainnet. Anything else on the spam test? If not? Terrence had two prs he wanted to discuss.
00:38:20.260 - 00:39:34.122, Speaker A: He's not here to do so. The first one was adding this block insidecar retrieval by root. I think it got approved this morning, so I don't know if there's anything more to discuss on this. Well, something related that isn't quite this pr, but is by range request, like the counterpart for this request. Whether to keep it as a separate block and blob request or combine them into a cigarette request, I'd be interested in discussing that at all. I think for lighthouse generally, whether or not we have the requests together or separate, we're probably going to treat them as if they're a single request, even if they're specked out separately, because it makes handling things like attributing faults to peers a lot simpler. So we would generally have a slight preference for just combining the request.
00:39:34.122 - 00:41:11.070, Speaker A: But it's also not a huge deal if it's a pain for other teams. I was just interested about other teams to think about this. Yeah, we're having this discussion on GitHub, and I come up actually with the same reasoning about that. The only thing that we were internally discussing a couple of days ago about having a separate call is for, let's say, archive nodes that for some reason they imagine that you want to have a node that wants to suddenly start getting archive blobs and start searching for nodes that provides blobs for a deeper history. And then this node would like to start filling up the archival blobs for the very deep fast. So in these use cases where you want to have those kind of nodes, having a separate method definitely makes sense because in that case, you have to download the blocks another time. But a part of these archival nodes, I think, we think that having decoupled version is simpler.
00:41:11.070 - 00:42:30.466, Speaker A: Yeah, it's an interesting point. I hadn't thought about archival nodes, just to clarify, they are still decoupled. If you're requesting sidecars that are not recent. We do have like an API to request response to RPC to get those in a decoupled manner. Is that the concern for archival notes or something else? Yeah, I was thinking that having the coupled version actually does not assume that we will have also the dedicated sidecar method. But if you think that we will have both of them, the coupled and decoupled version, this definitely solves the issue. But I don't think it was at least intended in the first place to have them both.
00:42:30.466 - 00:43:28.530, Speaker A: Might be wrong here. Yeah, we only use both during gossip so that nodes can easily have the block and its requirements of sidecar all in one message. It makes it easy, as Sean mentioned, to attribute any problems in the message to a given peer, and also avoids weird race conditions where you get a blob but not the sidecar and vice versa, and you're waiting for one or the other. But there is a fallback to request, like a specific blob giving its root or by a range request. Similar to how for bacon blocks. It works okay. Yes.
00:43:28.530 - 00:44:40.470, Speaker A: So having them also having the option to have this dedicated sidecar method. Yeah, it definitely works. Just so I understand, this pr that Sean shared is the remove blob sidecar by range. Are you saying that we wouldn't remove blob sidecar by range? Or are you saying that there's another method that's not blob sidecar by range that would serve a similar purpose of this blob has just proposed the blob sidecar by root. We could use that instead. Unless I'm missing a use case where that wouldn't be sufficient, I was suggesting rather than having a block sidecar by range request, we have a block sidecar with signed block and sidecar by range request. Essentially.
00:44:40.470 - 00:46:35.260, Speaker A: Just saying, I feel like we're converging, uncoupling these two things everywhere. So in the one place that I don't think we have them coupled yet, which is the by range requests, should we, and I think for us, whether or not we do, we're going to handle them as if they're coupled. So it's not a huge deal if it's specked out to suggest they should be coupled, but it would be nice if all the client teams are going to handle them seriously or similarly. So was just curious about what other client teams were planning with these by range requests. Yeah, I am interested how the prism implementation works right now. Do you guys cache blobs and blobs separately and then as a buy range request for blocks or blobs? Completes like kick off processing at that point? Or are you just like making both requests at once? If either request fails, both fail, and then once both complete you just start processing. Yeah, I can't really speak to prism, but the way we had it working for the first two couple dev nuts is to do the latter request the blobs block, request the blobs sidecars, and if either of them fails then it just short circuits the other.
00:46:35.260 - 00:48:13.480, Speaker A: Yeah, so this is, I think what we would implement if the requests are separate, but it's more or less the same as just having a single request, just like a bit quirkier. And you do potentially have an advantage of parallel download, but yet to be seen. That's generally what I was curious about, because I think if all clients are going to implement something similar to that, we should think about just having a couple of requests there as well. Yeah, well at least for prism they do try to avoid parallel downloads because it makes attribution and fear scoring a bit wonky. Like for example, in the case where you make two requests, you get the box in the sidecar and one of them is invalid. Ideally you don't want to keep communicating with the other peer, especially given like if we go with 1 mb size block blobs, we don't want to keep making that request. So I imagine a lot of clients would make a request to get the block, ensure that that's valid before proceeding to get the sidecar in the decoupled case.
00:48:13.480 - 00:49:33.550, Speaker A: But if it were coupled, then we can adjust like our peer scoring parameters to take advantage of the fact and make a single request. That would just be easier. But either way, I don't think it'd be that useful, at least not without crazy workarounds to make two requests at the same time. Try to exploit parallelism there. Yeah, so we can keep talking about it offline, I guess. But that's definitely, it'd be nice to have some clarity around that relatively soon. And if we're getting too close to the, I don't know, Devnet and we're not sure we can go with separate requests, but yeah, Mophie, can we at least pick a solution for the devnet, even if it's not well thought through? Yeah, I think the current solution, which is what the current spec is right now, will work for the Devnet.
00:49:33.550 - 00:50:28.790, Speaker A: We still do need the byroute, but other than that work works half right now, which should work. Yeah, so maintaining the range separates and having the coupled by root, this is the current solution. Right? Also considering also what I was discussing before with regard of the potential blobs. Archival nodes definitely have sense to maintain them separate. And in Teco we were also thinking about in any case, having kind of simulated couple version of it. So you talk always to the same peer and you ask for block and blobs and if they match. Okay.
00:50:28.790 - 00:51:41.478, Speaker A: But if something goes wrong, you can easily peer score downscore the peer you're talking to. I just want to make sure I get this for the notes. Can someone reiterate what the decision we just made is? I think leaving a buy range as it is for the next definite right and then open to discussion in the future. We're not going to merge the 387 PR that's removed blob sidecar by range? I believe so. To be honest, I linked that more for the discussion on the PR. I haven't looked at the PR itself. There is in the PR, it's just a discussion.
00:51:41.478 - 00:52:44.064, Speaker A: It's just an issue right now. Okay, so we're not going to do anything on that. Okay. Anything else that we are going to do or are not going to do for the devnet? Let's check the devnet, doc. But by root is still something we do want for the devnet. Does that happen to me? Which is the three oh 89 add block inside car retrieval by root? Yeah. Do we have a clear next step on that? Oh, I think it's approved at this point.
00:52:44.064 - 00:53:42.830, Speaker A: So we just need to merge it. Yeah, that's pretty much done. And the other thing was Brian's pr or issue to decide if we're open up. You rely on signature verification or case of due commitments to deter invalid law of psychology and gossip. I think we kicked the decision to do this for next week. I think that's probably something we want to have for the Devnet, but hopefully by next week we'll have a decision on what we want to do. Update the doc.
00:53:42.830 - 00:54:14.954, Speaker A: Go ahead, please. I was just going to say, looking back at my notes, it looks like we had a recommendation to proceed with just the KZG commitment rather than the signature. But maybe we haven't formally decided that. Right, we do KZG, and we just confirmed this on the call Thursday with the other CL teams. But yeah, generally that's, I think, what we agreed. So end of next week, we should know for sure. End of this week, sorry.
00:54:14.954 - 00:55:00.746, Speaker A: We should know for sure. What, do we have a pr open for that one? No, there is no pr. That's actually a good point. We just have the issue from Danny, but it might be worth opening a pr before Thursday's call with the KDG and then discussing. Does anyone here want to own that? I can make a pr. Sorry, just so we're clear, the pr would be to add the KZG verification as a gossip condition, right? Correct. Yeah.
00:55:00.746 - 00:55:31.590, Speaker A: Okay, cool. And then on the Cl call, we can discuss that pr. And if for some reason people think we should do full signatures, then we can always change it. But I think at least we can come to the call with something more concrete to propose. Yeah, cool. Yeah, thanks. And I think that was everything from today.
00:55:31.590 - 00:56:14.990, Speaker A: If we have two minutes left, there were a couple action items from last time. So we discussed like nscars at the beginning, George and Kev had this one around discussing the CKZG interfaces and how they handle asserts that this happened. We have two non blocking issues for decryptor. So there's free zero 93. We're waiting for a reply. This doesn't block clients. There's free zero 97, which is the interface for the pre compile.
00:56:14.990 - 00:56:46.214, Speaker A: We agreed on a bytes array interface, but there's a bit of discussion around whether the version hash code should be in the crypto, but that's also non blocking. Okay, thanks. And then Mophie merging the PR. Yeah, we're basing 4844 on Capella. We're getting close there. Mophie open. Oh, yeah.
00:56:46.214 - 00:57:03.882, Speaker A: Open and merge PR. Shrink fluid withdrawal fields on engine API specs. And pip four. Did that happen? Oh, yeah. The execution APIs. I don't believe that's been merged yet. Are there prs for it? Yeah, let me link that in.
00:57:03.882 - 00:57:47.390, Speaker A: Nice all. Thank you. Okay. And then Terrence had three of them, but he's not here. And I think basically the two last ones. Yeah, the two last ones are the two hqs you pointed today. One is adding the block sidecar and retrieval by root.
00:57:47.390 - 00:58:48.906, Speaker A: So I believe this is this first action item he had. And then the second one was ancestor blob availability check. And I think this was the second one, if that's correct. Yeah, that seems. Yeah. This is more associated to back change, requiring to be more specific about what should do a validator in terms of start testing the head with regard of having all the blobs already downloaded. Got it.
00:58:48.906 - 00:59:49.290, Speaker A: Yeah. This has been a discussion that started on discord by me and then became an issue here, but then blended with all these things around the birange methods. But I think we were converging to the option to say, avoid block import completely if there's no blob downloaded and verified got it. Okay, is there anything we're just a bit over time. Is there anything people feel we are missing? I just linked one on that, which I think is the 390 for the beacon block. I think that's in Alex in Stokes's court. At this point, I don't know if Alex is on the call.
00:59:49.290 - 01:00:16.446, Speaker A: Yeah, he is. Yeah. This was just like deprecating the beacon block gossip. Correct. Is that good to merge? Yeah, I was going to make one edit. Danny like wanted a clarification, but I was going to do that right after this call, and then that shouldn't be merged. Okay, sweet.
01:00:16.446 - 01:02:17.980, Speaker A: Anything else anyone feel like we should have touched on but haven't? Yes. So I'd like to come back to this test vectors question, which we shortly discussed at the chat, and at least in my opinion, it looks like it would be really great to have these test vectors, something like we have now for a consensus layer that Ethereum foundation is maintaining at the proper level for the library interface. Because looking at the charts, I see that there are a lot of edge cases where people are trying to cover and discover it, and there are multiple implementations, multiple wrappers and detailed test suit for a developer level will be really useful, especially if it covers those new discovered edge cases. So I don't know if there are some resources available that somebody could take care. I saw that Kevin is working on that, but I don't know, does he have enough time to develop the full suit and follow all the details I can add in the edge cases that we discussed, it's mainly basically going to all of the wrappers and passing the JSON and adding the tests into them. I'd have to reach out to maybe Alexey and to others for stuff like c sharp. Okay, so you feel comfortable to try to take care of that? Yeah, just making the test vectors is fine.
01:02:17.980 - 01:03:28.130, Speaker A: Okay, that's cool. Thanks. By the way, for the co, we will provide basic test vectors in the spectest in the next release, and I am keen to merge the rebels pr as soon as possible. We will talk Danny to agree with it, and once it's done, then I have a test generator PR, which is based on morphe's pr, so we will provide the test vectors. Oh, and by the way, in the Cl test vectors, we use the minimal trusted setup configurations. So the format is described in this pr. So just in case, if you have time, can take a look if you agree with the format and if there are any issues, we could change it before the next release.
01:03:28.130 - 01:03:53.840, Speaker A: Yep. Cool. Okay. Anything else before we wrap up. Okay, well, thanks, everyone. See most of you on the Cl call this Thursday. Otherwise, yeah, talk to you next week on this call.
01:03:53.840 - 01:03:57.130, Speaker A: Bye.
