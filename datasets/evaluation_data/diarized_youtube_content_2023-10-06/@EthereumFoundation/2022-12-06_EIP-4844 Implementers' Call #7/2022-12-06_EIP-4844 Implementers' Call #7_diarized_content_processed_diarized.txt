00:00:00.410 - 00:00:32.534, Speaker A: You okay? So, hey, everyone, welcome to the 7th. Now, implementers call for four. Four bunch of things to cover today, as usual, testing specs, devnets to start, though, Giorgio is going to give us an update on the large block testing because he has the hop. Yeah. Georgia, do you want to recap what happened in the past week or so?
00:00:32.732 - 00:01:12.350, Speaker B: Yeah. Thank you, Tim. So the update from last week is that previously we're blocked on providing transactions that are bigger than 128 kb because of the TX pool limit. We got plugged into the flashboards builder, which itself is connected to a good amount of relays who are in turn plugged into a good amount of hash rate on Gurley, which let us submit bundles which had 1 mb transactions. So we got a few blocks with 1 mb transactions. We've got a bunch of blocks with ten or more 138 kilobyte transactions. We can do it reliably.
00:01:12.350 - 00:02:04.610, Speaker B: So now we're at the point where over the weekend we got some blocks that were bigger and some people were monitoring their networks, et cetera. So now the next step is for us to improve our bin packing for the bundle, I. E. Kind of like optimally bundle as many like 1 mb plus 1512 kilobyte plus 1256 kilobyte plus 100 whatever, to basically make the block as big as possible. So that's the one thing that we need to improve to make the benchmark better. And the second thing that we'd like to improve is how reliably we can get multiple bundles included in a row because we get outbid by others. And I just haven't figured out yet why we're getting outbid.
00:02:04.610 - 00:02:53.700, Speaker B: So if there's more hash rate that we can get, if there's more stake that we can get inside of the system to make bundle inclusion more reliable, that would be really nice. So that's the progress update. Another thing that we'd like to do is to start running this for the 4844 devnets like proper because the code paths might be different. And also, more importantly, because there is this mempool verification. There's a bunch of new verification code for KZG, which we're not touching, obviously, right now with the call Data benchmarks. So tldr the benchmark has been done at small scale. We're trying to make it at bigger scale, and we'd like to try it also out for four in the near future.
00:02:53.700 - 00:03:01.320, Speaker B: Any happy to answer any questions or brainstorm or discuss if there's anything there.
00:03:06.890 - 00:04:21.342, Speaker A: So we have a third of the network running Mev boost on Gordy, do we know another big chunk of validators who is not currently running Mev Boost? So I know client teams have like a fair amount of validators. I assume the ones who are involved in this are running it, but I don't know, does anyone have an idea of who we could reach out to to add a bunch more? So all the client teams together should be roughly 80 or 90% of curly. Definitely not all the client teams are running, as far as I know, just the EF. And if I had to guess, it would be prism running it, but I'm not sure about the other one. Pego is running five k validators on girly, but we're using Flashbot. So the bids were coming from flashbots relay on Gurley. Yeah, and I think that's what we're using for this.
00:04:21.342 - 00:04:22.958, Speaker A: So you would already be part.
00:04:23.044 - 00:04:25.026, Speaker C: So we were part of it.
00:04:25.208 - 00:04:38.870, Speaker A: But if I understand correctly, I think Teco has more than five k validators, so there should be more that aren't running it. Yeah, but not all are running under Mev boost.
00:04:39.610 - 00:05:18.180, Speaker B: Yeah, I think no need for us to go back and forth on this necessarily, but if you guys know more people that we could put on Gurley move boost, that would be fantastic. So I just messaged Tim that I've been bidding with 0.1 e. It doesn't need to be more like the bids. I looked at all the historical bids on main net, and anytime I get outbid, it's because somebody just looks at my bid and outbids me. So it's not a static system. Other people are also trying to get their bundles in because they're screwing around.
00:05:18.180 - 00:05:24.340, Speaker B: But yeah, if people want to send me a bunch of girl leads, I'd be happy to take it with.
00:05:26.730 - 00:05:42.358, Speaker A: Right now we're only getting these big blocks in through Medboost. Yeah, only Medboost generically or do you have to hit. They have to be connected to flashbots or a particular relay.
00:05:42.534 - 00:05:48.330, Speaker B: I'm submitting to anything that conforms to the east send bundle API.
00:05:49.310 - 00:06:04.382, Speaker A: Okay, so if we ran an analog of this experiment on main net, we would likely be able to hit a fairly large amount due to something like greater than 60% being connected to medboost.
00:06:04.526 - 00:06:13.490, Speaker B: Of course, this is designed so that it's kind of like one click change for Mainet.
00:06:15.670 - 00:06:16.386, Speaker A: Got it.
00:06:16.488 - 00:06:23.458, Speaker B: And that's why to me, it's important that I get the biding algorithm proper and the bin piking algorithm proper, so that I don't want to be overpaying.
00:06:23.474 - 00:06:43.660, Speaker A: For all my blocks on main network. Okay. Yeah. And we can look offline if there are clusters of gordy validators that we're aware of that we can reach out to.
00:06:46.270 - 00:07:11.960, Speaker B: So the main ask from the group is to just make sure that people's metrics APIs are up. And if people are going on this or if there is somebody that's blocked or whatever, let us know, because I think this week, by the end of this week, we'll have productionized this, or whatever you want to call it, and then we want to start hammering reliably all the time.
00:07:15.000 - 00:07:16.084, Speaker A: Got it.
00:07:16.282 - 00:07:21.080, Speaker B: I would like to run this for, like, ten minutes, for example. I don't want to run this for, like, five blocks.
00:07:24.290 - 00:07:54.310, Speaker A: Perry and others, do we have initial metrics insights, like, things seem like we're gathering the data that we need. Yeah, that's actually one of the asks. Andrew shared a couple of dashboards earlier today. If you guys could have a look and let us know if something more needs to be added, then we can work on that over the next week. But right now, based on what we've seen, there's no cause for concern. Nothing changed, nothing was unexpected.
00:07:55.770 - 00:08:03.100, Speaker B: To be clear, I don't think that we should be using the data that we got so far as signal, because we haven't ran really anything serious. Right.
00:08:03.470 - 00:08:05.786, Speaker A: Yeah. We just need to know that we're getting the data.
00:08:05.888 - 00:08:16.506, Speaker B: Yeah, of course. Just making explicit that the fact that we got three 1 mb blocks or two megabyte blocks, actually, like, with 1 mb transactions and over the weekend, doesn't.
00:08:16.538 - 00:08:20.814, Speaker A: Really mean anything yet. Yeah.
00:08:20.852 - 00:08:23.346, Speaker B: And I think I got some of these metric dashboards. Thank you.
00:08:23.368 - 00:08:43.480, Speaker A: Barry Yannick's attestation analysis that he just put out using some of those tools, this is all on chain data that might be really valuable as well. I just shared the blog post.
00:08:52.660 - 00:08:53.810, Speaker B: Thank you all.
00:08:55.080 - 00:09:51.040, Speaker A: Sweet. Anything else on this? Okay, we had a couple spec PRS issues from last time and new ones that we wanted to cover real quick. First one, Daplion had this PR 31 41. I don't think he's on the call. And Danny, I think you literally just commented on it 30 minutes ago. Anything we should discuss here about it? From my understanding, I think there's generally agreement to not allow this to grow unbounded in times of non finality. So I think then one, to reduce complexity, and two, to kind of know the load that's happening here.
00:09:51.040 - 00:11:09.552, Speaker A: I think the question really becomes, in non finalized period, if you're trying to reorg to something that you don't know is available of more than an 18 day or whatever the prune depth is. Do you say is data available is true or do you say is data available is false? I think there's certainly some edge case attack scenarios to kind of consider here. Also some ux around if your client was offline and what the recovery modes look like. So it's kind of some trade offs I think. Still to discuss, happy to discuss a bit more here if people have questions about the state of the conversation, but also it seems like we're pretty active on the thread. Anyone have any other thoughts, comments on this? Okay, so yeah, we can just continue that async and then Terrence, we had your issue also from last time, 31 25. It seems like this is just ready to merge.
00:11:09.552 - 00:11:36.690, Speaker A: Is that correct? Yes, ready to merge. Nice. Yeah, I think I'm good. Other than if you can just change the header comment to say what this does now just for posterity because it's a bit confusing if you read that. Yeah, got it. Okay. And then the last one I think is another one that lion asked for, 3113.
00:11:36.690 - 00:12:40.756, Speaker A: So. Oh wait, is this the exact same thing? No, this is not the same thing as the previous one. And yeah, I'm sorry, I don't remember what we discussed about this last time. Is Sean or anyone that's been on this give us a oh, enrico as oh yeah, both of them are. So the issue is about how we want to handle cases where we need to make a get block and blob by root request for a missing parent block. Because when we do this, we don't know at what slot the parent is. So that means we could be making a query for before the 4844 epoch, for example.
00:12:40.756 - 00:13:31.830, Speaker A: So we don't know whether to make a request for a block and blob or request for a block based on the current spec. So the appline is suggesting this might be a reason to uncouple the block and blob request because then you sort of have just an optional sidecar in that scenario. And then the other solution we were considering was making the response to the block and blob by root request be in enum, like a union type in SSD. That's either a block or a block and blob. So yeah, it's the TLDR. I'm not sure where we're at with that. For us at Mighthouse, we've sort of just kept the coupled request and not bothered to resolve this edge case for now.
00:13:31.830 - 00:14:11.250, Speaker A: So from prism side, I don't think it's that hard to resolve? To basically resolve this case because you can just call the block and blob and check the error code. If the error code is something like unavailable, you can try the block by root after just like a fallback case. I mean, it's pretty ugly, but I think it's okay. Workaround. Yeah. This is also something that will fade into oblivion once we're finalized and kind of firmly past this range. Right.
00:14:11.250 - 00:15:02.370, Speaker A: It depends by, I think, the error code. If there is an error code specific for this situation, then at some point you will not get any of this situation anymore. But if you got, I don't have the details of the error code, but if we can exactly catch this situation by error code and just retry in these very edge cases that go away, it's okay. To me, one thing is when you refer to a union, you're referring to SSD union. Yeah. Right. I would want to avoid something kind of dirty in the spec for what ends up being like handling the transition cases here if possible.
00:15:02.370 - 00:15:34.364, Speaker A: Yeah. So an error code would also be fine. I think we can resolve this in a few different ways. It's more about just coming to consensus on a solution. Okay. Can we move that to the thread or is that something people want to and Daplayan can jump in there? Okay. Yeah, we can come back to it on next week.
00:15:34.364 - 00:16:12.890, Speaker A: Call if it's still being discussed. And there was one more cl spec PR that I had missed before, but shall we point it out? 3145, which updates the max blobs per block to four, which I guess matches what's on the El side. Any comments or thoughts of that? I guess the question with that is should we target this for the deaf net three or should we follow up after the definite three?
00:16:15.260 - 00:16:18.810, Speaker D: It's just a simple constant change, right. So why not include it?
00:16:19.520 - 00:16:43.440, Speaker A: Yeah, I don't have a preference. Yeah, that's fine with me. People are probably targeting master or whatever on the EIP, right? For configuration values? Yes, they are. Yeah. So we should align it. Okay. And I'll make sure to add it to the hack MD just in case it doesn't get merged like today.
00:16:43.440 - 00:16:54.600, Speaker A: So we can at least know that it's there. Okay. Anything else on the specs themselves?
00:17:01.240 - 00:17:38.450, Speaker E: One thing is that Ramana, I think, merged the pr that makes field elements per block configurable so that we can have minimal presets also on CKCG. Since that was causing problems for some testing situations. I'm not sure if it has been used or the clients know it. I guess the bindings also need to be updated, but this is something that is client relevant and happened this week.
00:17:41.300 - 00:17:49.010, Speaker A: Yeah, we were waiting for this to be merged, and we are ready for the binding to use it.
00:17:57.460 - 00:17:58.850, Speaker D: Can you link the pr?
00:18:01.380 - 00:18:03.890, Speaker E: Yes, I'll find it and link it.
00:18:10.600 - 00:18:49.182, Speaker A: Sweet. Anything else on the specs? Okay, the next up, Devnet three. Yeah. I'm curious where the client teams at and how are we feeling about getting this up? I know on the last week we were talking about potentially getting a single El Cl combo. I'm not sure we quite got there. Yeah. Does anyone want to give a quick update from their client side? Yeah, I can give a quick update.
00:18:49.182 - 00:19:13.674, Speaker A: So we're passing four. A four spat test as of last week. So thank you, Shawi and all the people that's working on the spat test, I'm working on sync. That's close to done. One thing I like to finish before trying the Devnet three is that I do want to do some sort of local interrog test. And I'm targeting Roboto's branch for that. So thank you for that as well.
00:19:13.674 - 00:19:43.270, Speaker A: I haven't tried. I believe there is time based or slot based fork now. So that should be compatible as what we're doing with the capella as well. I guess one thing I do need is that last time I checked, so the engine API for four four four is still using v two. So I do need those to be v three to try. And I'm wondering if there's a status for that. Oh, Roberto just said that.
00:19:43.270 - 00:19:47.638, Speaker A: He just added it, so. Yeah, sounds good. Thank you. I will try it today.
00:19:47.804 - 00:19:57.570, Speaker D: That comment was actually with respect to time based forks, but V three APIs were also added last night. Thank you, Mophie, for sending a pr for that. And that's now merged.
00:19:57.730 - 00:20:18.820, Speaker A: Okay, sounds good. So no more block on my end. I will try local interrupt today and. And I will give you guys an update. Could you also share your configuration for Cl site, please? It would be interesting to test.
00:20:20.550 - 00:20:20.914, Speaker B: With.
00:20:20.952 - 00:20:49.962, Speaker A: You, too, as another mind. Okay, sounds good. I will prepare and interrupt Doug for this. Nice. So for Lighthouse, we're in a similar boat where we're now just trying to test locally against the latest geth updates. We can test against Nethermind too. And last week, I'd say the major outstanding work was in sync, but we've made a lot of progress there.
00:20:49.962 - 00:21:47.320, Speaker A: So now we have an implementation, but it's untested. So after we get, like, lighthouse execution layer interop working, we'll probably start trying to make a local lighthouse network working and see what sync looks like and then we'll try to hopefully work with Prism or lodestar to see if we can get sync working there. That's it for us. Nice. Sorry, apologies. We started working on it more or less only this week, so I'm now working on the SSD serialization and more people will join me working on it this week. So let's see what the updates will be next week.
00:21:47.320 - 00:22:32.714, Speaker A: Sweet. Lexi yeah, just want to remind rop site has some tricks there too. I mean you will need it for transaction hash like that and it's quite tricky. Never mind. So we tried to synchronize with Geth to run Geth. At least it looks fine and we need a sideline to make network. We will try to run such network next couple of days and hope we will synchronize soon.
00:22:32.714 - 00:22:49.110, Speaker A: And we are working on benchmarks for pre compile too. That's just any other client team.
00:22:53.980 - 00:23:31.780, Speaker D: Hey, this is Andrew from Ethereum Js. I think as I noted last week we're, like I said, joining late, so we're still behind. I have over the last week I've kind of honestly gave up trying to get Ethereum Js to cooperate with Prism. Not sure why it's not working on the version on the interop, but I'm not that experienced with operating with the CL client. So I've been working with Lodestar just because I'm more familiar with that one and we have got it up and sync. We can sync past the sharding block and using the current kind of sharding block based hard fork switches. So we've got that working.
00:23:31.780 - 00:24:15.976, Speaker D: I'm working through basically finding all the bugs that I wrote in my initial implementation of 4844. So still working through at this point, just kind of getting the blobs to actually get transmitted to Lodestar so it can validate them. I haven't actually successfully transmitted a blob from El to CL yet, but that's currently where I'm at. So slow not sometimes the steady progress on that. And we are hoping to also implement kind of related the timestamp based hard fork management within our client over the next week or two. I'm working with Gagender, who kind of does work with us, and also with Lodestar. So hopefully we'll have some of those other building blocks in place for when we're ready to join the Devnet.
00:24:15.976 - 00:24:20.070, Speaker D: So hopefully by the end of the year, but we'll just see how much progress we make.
00:24:21.800 - 00:24:23.110, Speaker A: Nice. Thank you.
00:24:26.950 - 00:24:34.280, Speaker D: I continue chipping away at the Aragon. Client still has a bit of a ways to go. I didn't have a lot of time last week to spend on it, but coming along.
00:24:36.810 - 00:25:48.260, Speaker A: Sweet. Any other ones? I think we covered most of the ones that had said they're going to be part of the three. Okay, I guess then for the next week, does it make sense? So it seems like a lot of the clients are just trying to get things up and running and fix some issues on their own. Is there a client pair that we think might be still more ready to start on the devnet so that others can try to pair with that when their implementations are done? I mean, after we do some local testing, it might work. So maybe lighthouse. I think it sounds like probably the same for prism too. Okay, so let's try to get lighthouse or prism up and running with guests, potentially as an El and yeah, if we can get those two, that'd be a good start.
00:25:48.260 - 00:26:43.460, Speaker A: Sweet. Okay, then next thing I wanted to cover real quick is last week we covered Martin's benchmarks for guest, and basically it seemed like the pre compiles were maybe a bit underpriced. I know there's been some work done in the past week. Looking at that in more detail, and potentially the benchmarks were a little bit pessimistic. So I guess I was curious to hear from people who've looked a bit more into the benchmarks. I don't know if Kev is on a call, what your data is thinking is there, and then how we should approach doing this generally maybe across more clients to make sure that we get the right pricing for the pre compile.
00:26:47.920 - 00:27:27.304, Speaker C: Yeah. So I was looking into switching out the Gokzg for a more native library, and it reduced the allocations by around 80%. So it seems like Gokzg might not be as optimal. Even after switching it out, I was still getting some fluctuations, but I don't know if Martin is on the call. I think this is from the GC. So if you test it with CKZG, for example, you'll probably get more consistent results. So I think that was the main problem.
00:27:27.502 - 00:27:38.510, Speaker A: But on the same order. Right. In terms of timing it, call it 50 instead of 67 or something, but it's not changing the order magnitude, right?
00:27:39.120 - 00:27:49.404, Speaker C: Yeah, it's not going to immediately do a two x, but it might be the difference between what Martin was saying with 67k gas and 50k gas.
00:27:49.532 - 00:27:50.530, Speaker A: Right. Okay.
00:27:51.540 - 00:28:14.420, Speaker C: I just ran it on my computer. I did an EC recover. I think there's one more optimization to add. The EC recover was 42.2 m gas per second. And the pre compile, I got it at around 17.9. But there's optimization in ganrock that needs to be applied.
00:28:14.420 - 00:28:19.690, Speaker C: So I think it can get closer, but I need to just re benchmark it.
00:28:20.540 - 00:28:28.590, Speaker A: And then the fail case, there was an issue there and that actually the fail case should be the same as the succeed case.
00:28:29.040 - 00:28:55.750, Speaker C: Right. I think this was because Gokazg was basically doing all these allocations and the GC was kept kicking in. Right. Now when I test against, I'm only testing against the fell cases, they're roughly the same. There are sometimes when the GC kicks in and then it goes to like 15 m gas per second. But I don't know how to sort of solve that because you can't control when the GC kicks in.
00:28:57.720 - 00:29:16.260, Speaker A: Okay, but we're probably more in the even 100k is probably very pessimistic. And the 60k would be if we end up going with not fully optimized go KCG, but in that 50 to 60 range is probably very realistic.
00:29:16.640 - 00:29:59.512, Speaker C: I think with GokZG it's kicking more towards at least 60. I don't know whether the allocations are quite a lot that it's doing. We haven't tested with CKZG through go. I'm using the ganark sort of bindings instead, which is where I'm getting closer to 50. Like 50 to 60. There's some low hanging fruit to optimize go KZG. Yeah, I think because all we're benchmarking is the pre compile, which is pretty simple.
00:29:59.512 - 00:30:30.700, Speaker C: It's not anything to do with the aggregation. So yeah, if there's low hanging fruit, it's going to be sort of on the BLS side, because you're just deserializing points and scalars and then just doing a pairing. Once this last optimization goes through, then I'd like to benchmark it again and see if it goes to 20, which would be closer to the 50 that we talked about.
00:30:36.440 - 00:30:55.710, Speaker A: So then Tim, you wanted to consider how we need to play this in relation to other clients and languages as well. Other clients and languages going to be utilizing the native CKZG. Because in that case, I think a lot of this can and should translate. But if they're not, then maybe more benchmarks should be done.
00:30:57.680 - 00:31:04.720, Speaker C: Yeah, but I guess my question is, what happens if there's a discrepancy between the GEF code and the other clients.
00:31:06.900 - 00:31:09.760, Speaker A: In terms of the EC recover comparison.
00:31:10.260 - 00:31:11.010, Speaker C: Yeah.
00:31:13.480 - 00:32:11.110, Speaker A: I think it's worth at least knowing what it is. Right. Because either a, the clients get, like, if there's a large gap in, like, one client, you know, that client can probably try and improve their implementation. If get is significantly quicker than all the other clients, for some weird reason, then it might make sense to use something more conservative in terms of pricing. But yeah, this is kind of why knowing that they're all within the same ballpark would be useful. But if they all use basically the same library. The thing that's unclear to me is what overheads do other clients have from the bindings to their specific language? And how big is that relative to the overall execution time?
00:32:13.480 - 00:32:14.230, Speaker C: Right?
00:32:14.600 - 00:32:54.214, Speaker A: Yeah, I guess we know generally what the cost of pairing can and should be. And if overheads are more than two x that, then I think that's like a sign for optimization rather than changing the price. But nonetheless, it'd be good to know that so that the optimizations can occur regardless. Yeah, and I think, never mind. Last week you were kind of the other team that was sort of ready to look into this. Is that right, Alexey? Yeah, we're trying to make some benchmarks. Okay, nice.
00:32:54.214 - 00:33:18.080, Speaker A: And we have Yasek on the call, I hope I'm saying your name correctly, who can probably help look into KZ. I assume you're not in the KZG chat on telegram. I'm not, no. Okay. But yeah, maybe I'll add you to that. If you want to just send me your telegram handle. I'll add you to that.
00:33:18.080 - 00:34:03.550, Speaker A: And then, yeah, it probably makes sense to just get started on Nethermind and see. Did the numbers roughly line up to what we thought I would get? Sure. Thank you. Anything else? I'm just testing benchmark anya. If not, the last quick thing I just wanted to cover is when do we want to have these calls? In the next few weeks. So I think it makes sense for us to have it next week. And then you're sort of moving into the holidays.
00:34:03.550 - 00:34:49.588, Speaker A: Do people want next week to be like our last call this year? Do we want to do one more after that? Yeah, how do people feel about that? Okay, Roberto is around for both weeks. Okay, so we'll do next week. I was going to say I'll be around for both too. Okay, so let's do that then. Let's do the 13th and the 20th. Then we can take at least the 27th off and decide if we want. Yeah, it might make sense to do the third as well.
00:34:49.588 - 00:35:15.790, Speaker A: So if some people are around, then we can do that. Next two are okay for us. Okay, awesome. So let's do the next two. Take the 27th off and I'll be back on the third. So if people show up, we can have that and then go from there. I have one more quick point.
00:35:15.790 - 00:36:04.492, Speaker A: If people give in consideration how to handle fork identifier and the time based forks, essentially an EIP 21 24 extension or modification. I think naively looking at this, if we no longer would do forks by block number and only timestamp. Timestamp is strictly much larger than our latest block number. And thus I think you can kind of like layer an extension on here where you use the UN 64 fork next as a timestamp instead of a block number. But I think we would just need to get one. That's my very cursory look at this. So if somebody has some other ideas and then two, I think we just need to agree.
00:36:04.492 - 00:36:54.040, Speaker A: It's probably something that we want to certainly by the end of January be agreed upon. It's minor, but would be annoying if it was being a blocker. Yeah, I agree. And generally we're going to need this for Shanghai regardless, right? Yeah. I can knock on a couple doors of people that maybe the co authors of this and see if they have quick ideas. Yeah. And I know on all core devs a few weeks ago, I think the teams are saying we want to get some prototype implementation that we're kind of happy with and then write a new Eip to specify it.
00:36:54.040 - 00:37:24.710, Speaker A: But it'd be good to just follow up on where that's at and make sure we do have something in the next month or so. Anything else? Okay, well, thanks everyone. See you all next week. Thank you. Bye, everyone. Bye.
