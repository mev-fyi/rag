00:00:00.170 - 00:00:38.178, Speaker A: You okay? Thank you. Ethereum 20, randomness using a verifiable delay function. So I'm going to speak about how to build a randomness beacon using a new primitive called a VDF. And then I'm going to explain how to build the VDF, the cryptography that goes behind it, and the hardware, the supporting hardware. There should be some time for questions at the end. The mics are here. Okay, so let's get started with the randomness beacon.
00:00:38.178 - 00:01:20.290, Speaker A: So we use the randomness in two different places. We use it at the consensus layer in the beacon chain. And basically we're doing secure sampling of validators. We have this huge pool of validators, each with 32 e, that could be 100,000 or even millions of validators. And we're basically sampling what we call leaders and committees, and that's part of the process of Ethereum 2.0. In addition to using it at the consensus layer, we can also expose it in the shard at the application layer. So through an opcode in Ethereum 20, you should be able to have totally unbiaseable randomness as a core primitive in the virtual machine.
00:01:20.290 - 00:01:56.550, Speaker A: And so that should be useful for lotteries and gambling and gaming and all sorts of other applications. So what are the goals for the randomness beacon? We want it to be unpredictable, of course. We want it to be unbiaseable, and it turns out that's much more difficult to do. And we also want it to be unstoppable. And I'll talk more about that later. So there's basically two classical families of randomness beacons. There's beacons based on commit reveal, for example, Randall.
00:01:56.550 - 00:02:46.226, Speaker A: And these randomness beacons have an attack, which is called the last revealer attack. So you have an ordered list of participants, and then when you're about to use the randomness, the last participant can either reveal or not reveal, and therefore bias the randomness. And then you have approaches such as the definity construction, which is based on threshold cryptography. And these basically require a certain threshold of online participants to create the next random number. And so if you don't have enough online participants, the randomness beacon stalls. And in the case of definity, that just stops the whole blockchain. So just to give a bit more context on this last condition, one of the design goals of Ethereum 2.0
00:02:46.226 - 00:03:25.040, Speaker A: is to survive World War three. So we're assuming that 80% of the nodes could go offline, and we still want the system to run. So let me briefly explain how Randall works because we're going to be building upon it. So in Randall, you have a Randall epoch, about 17 minutes, and that's 128 slots. Each slot is 8 seconds, basically in the beacon chain. And in each slot you have a beacon proposer. The beacon proposer is invited to create a block and basically reveal secret that they've committed to in the past.
00:03:25.040 - 00:04:19.550, Speaker A: So the Ferris beacon proposer reveals his secret, which is like local entropy, and it contributes to this pool of entropy through the Randall mix. So we have the next revealer that's mixed in using the XOR sign. It's okay if some proposers don't show up and don't reveal, we just move on. And then once we want to use the randomness, for example, at the very end of the epoch, at slot 128, this is where the problems start coming up, because the last revealer already knows all the previous actions of the previous proposers. And so they have a choice, they can either stay put and not reveal, or reveal their secret. And effectively they can choose between two random numbers and they'll choose whichever is most favorable to them. And that opens the door to various attacks.
00:04:19.550 - 00:05:01.114, Speaker A: Okay, so we have this new cryptographic primitive, which is very recent, just a few months old, and it's VDF. So the function part of it is very simple. It just means that you have an input, and for every input you have a unique output. And if you want, you can think of it as a hash function. There's a second parameter which is the difficulty. So the difficulty specifies the amount of sequential work that needs to be done in order to compute the output. So we're talking about inherently sequential computation, which takes time.
00:05:01.114 - 00:05:44.422, Speaker A: And this is where the delay part of VDF comes in. And then we have a second output which is the proof. And this is where the verifiable part comes in. Basically, once you've done the computation and you have the output, you can also produce a proof and give the proof to others and convince them that the output corresponds to the input immediately without having to do all this sequential work. Okay, so this is the gist of the construction. We have two parts. We have the Randall mixing period, which is one epoch, and that produces a biasable Randall mix.
00:05:44.422 - 00:06:34.134, Speaker A: And then you feed in the Randall mix into your VDF. The VDF is going to take time to compute at least one epoch of guaranteed delay. And then on the other side the output is going to be your unbiasable randomness. Okay, so this is briefly the safety argument. Why does this produce unbiaseable randomness? So if you look at one given epoch, you will have at least one honest proposal. And the reason is because we have 128 blocks, and we have an honesty assumption and a liveness assumption which says that with very high probability we'll have at least one. And if you look at the last honest proposal, that will be the point after which everything is predictable by the attacker.
00:06:34.134 - 00:07:39.342, Speaker A: So the attacker can try and build various Randell mixes, given his local entropy, and he can start feeding it into the VDF as soon as possible. But the VDF is going to give you a guaranteed delay of one epoch, which means that all the outputs will be produced after the end of the Randall epoch. And it will be too late to try and bias the randomness, because every action that has been made is now binding, the blockchain has moved forward. Okay, so in order to get this guaranteed delay, we are making a safety assumption, which is rooted in hardware. And specifically we want to prevent an attacker, even an attacker with a huge budget, to be able to build specialized hardware which is significantly faster than what can be done on the commodity hardware. So the speed at which you compute the VDF, the function is going to depend on the hardware you have. And basically we want the good guys to be not too bad relative to the bad guys.
00:07:39.342 - 00:08:43.890, Speaker A: And in particular, we have this Amax protocol parameter, the maximum speed advantage that an attacker can have. And for example, we can set it to ten. And the strategy that the FM foundation is taking is actually to go ahead and build the best ASiC that we can and give it away to the world, so that the baseline for the commodity hardware is actually pretty good. So that we can simultaneously have a very conservative Amax, but at the same time have a reasonably small Amax. Okay, we also have a liveness assumption. So we need at least one person in the whole world to be running the commodity hardware. And the strategy of the Ethereum foundation here is basically to build thousands of rigs and give them away to the community for free, give them away to the film community, but also beyond that, for example to third parties.
00:08:43.890 - 00:09:27.566, Speaker A: And if at least one of these pieces of hardware stays online, then we're good. Okay, so we have the commodity hardware and we have this Amax assumption. Now it's very easy to have a guaranteed delay of one epoch. All you need to do is target an evaluation period of Amax epochs and an attacker will only be able to shrink that down to nothing less than one epoch. And this is the whole scheme, basically. So we have part one. The random mixing process produces biasable entropy this is taken by the VDF evaluators.
00:09:27.566 - 00:10:09.310, Speaker A: We need at least one in the world to do that. They will start the number crunching. That was going to take a bunch of time, about 3 hours, and then after 3 hours pops out the unbiaseable randomness. And then we need a one epoch inclusion buffer for the randomness to come back on chain. And again, within one epoch, we're assuming that there's at least one honest participant, and that participant will make sure that it's on chain. And what we do is we basically have a recursive construction. So we use the strong randomness, the unbiasable randomness to recede the next 128 proposes.
00:10:09.310 - 00:10:50.890, Speaker A: And also another thing that we want is we want a new random number at every epoch. We want a reasonably fast generation of these things. And so we're going to have parallel randomness beacons, and they're going to be staggered in this fashion. Okay, so that's it for the randomness beacon. Now let's have a look. How do we actually go about instantiating a VDF? So the vdfs, they tend to be built as a basic building block around, and you keep on iterating this round many, many times. And so the basic building block that we have is the squaring function.
00:10:50.890 - 00:11:24.040, Speaker A: So you take a number and you multiply it by itself. That's it. And then you reduce modulo n, where n is an unfactorizable RSA modulus. So no one knows the factorization of n. And basically you do multiple squarings, you do t squarings where t is going to be your time parameter and the output is going to be x to the two to the t. Okay, so let's go through the whole VDF scheme in one slide. Super simple.
00:11:24.040 - 00:12:09.910, Speaker A: So the output, as I said, is going to be x to the two to the t mod n. And then we want to build the proof. So the proof is going to be based on a challenge response scheme where a random challenge will be given to the person wanting to build the proof, and then they will build this proof as shown. And you can make it non interactive with the fiat shamir scheme. And then the verification is just checking this equality. So this equality is very fast to check. It takes about one millisecond on a single core, and it's basically two small exponentations and a multiplication.
00:12:09.910 - 00:13:12.570, Speaker A: So this is the scheme by Benjamin Wasolowski from June 2018. Extremely nice. But there is one important detail, which is how do you generate the modulus? We need to have an RSA modulus which no one can factor. And so there's various ways to get such a modulus, but the preferred approach that we're looking into is having an RSA ceremony. So this is similar to what Zcash did with the powers of tau. So you have a bunch of participants, for example, 1000 participants, and they're going to participate in what's called a multiparty computation, and you need just one of them, to be honest, in order for the output, which is a 2000 bit RSA modulus, to be unfactorizable by everyone. And just to speed things up, we could have a trustless coordinator in the middle, as shown.
00:13:12.570 - 00:14:01.574, Speaker A: So the team that is working on the multiparty computation is ligero. They're experts in npcs, and they're from a couple of universities. And these are like the parameters of the ideal multiparty computation that they're building. So we're looking to have 1000 participants, which is much bigger than the Z cache that they only had 88 participants. I believe we're looking to produce a modulus of size, 2000 bits. It's n minus one, maliciously secure, which means that you only need one honest participant to be there. One of the things which might be a bit tricky is it's a synchronous thing, so everyone needs to be online at the same time to participate.
00:14:01.574 - 00:14:47.910, Speaker A: The good news is that it's only a one time thing, a one time setup, and it shouldn't last too long. It should last about ten minutes. And part of the reason why it's so short is because they got it down to just 20 rounds of communication. Okay, so now the last piece of the puzzle, the VDF hardware. So right now we're working with universities around the world that specialize in hardware implementation of modular multiplication. And they have various candidate circuits, and some of them are extremely fast. And based on the circuits that they've presented, this is what we believe we can achieve.
00:14:47.910 - 00:15:21.810, Speaker A: So we can achieve a latency of two nanoseconds per 2000 bit. Modular squaring. This is extremely, extremely fast, much faster than what a cpu or an FPJ could do. We're targeting a fairly advanced process node, 16 nanometer from a TSMC. And the size of the chip, the die area and the power are very reasonable. 20 watts is pretty good. And so we'll be taking these asics and putting them in a rig.
00:15:21.810 - 00:16:27.606, Speaker A: Each rig would have a max, the number of asics, so maybe about ten asics, and that will lead to each machine consuming about 100 watts. And the machine hopefully should look something like a Mac mini that you just plug in the wall and it just works. Building hardware is expensive. We're talking tens of millions of dollars, especially that we want to give away the rigs completely for free. But I'm very proud to announce that we're making a partnership with Falcoin. So we've agreed on a 50 50 split on the current ongoing research. And if we are going to go through with the whole project, then I think that would be the largest cross blockchain collaboration ever, inviting more other blockchain projects to come in.
00:16:27.606 - 00:16:54.260, Speaker A: So Chia is working on vdfs. I know that Tesos is looking to upgrade their randomness. They're more than welcome to come in. Cardano could use vdfs, Algorand could use vdfs. The more the merrier. We'll have a better ASIC at the end, and every participant will have to pay less for the ASIC. So it's a win win, and we really encourage collaboration here.
00:16:54.260 - 00:17:56.866, Speaker A: One of the exciting things that we're looking to do to get the fastest possible circuit that we can is to organize an open source hardware competition. So anyone who knows how to design a hardware circuits will be invited to design latency optimized modular multiplication circuits. And there will be very large cash prizes for the participants. We're also doing research between now and the competition, so the competition maybe will happen mid 2019. And so right now we're looking at all the possible ways in which we could squeeze latency to get a really good commodity ASIC. So if you have expertise in any of these things on the right modular multiplication, reduction trees, compressors, finfets, please email us. And today we just released this website, vdfresearch.org,
00:17:56.866 - 00:19:00.698, Speaker A: where you'll find tens of links, maybe 30, 40 links, and you'll be able to dig in more into the content. So just to give a little bit of perspective on what we're building here, what we're looking to build, let's compare the vdfs with a traditional proof of work. So vdfs offer something rather unique, which is unbiaseable leader election, but they're also much less costly than proof of work. So in terms of the energy expenditure, it's about 10,000 times less energy than proof of work. And in terms of hardware, proof of work right now requires about 10 million gpus for Ethereum, whereas we'd need only a few thousand for the VDF. And also in terms of protocol subsidies, it's very expensive. All the hodlers need to pay a billion dollars of inflation per year to support the proof of work.
00:19:00.698 - 00:19:45.510, Speaker A: Whereas for VDF the incentive would be fairly small, about 1000 times less. Okay, so this is my conclusion. Slide and then we can take questions. So if we are going to go through with this project, and I really hope we do, then we'll be basically breaking several world records. So we'd have the first World War three proof unbiasable randomness. The only construction that we know that has both the unbiasable aspect and the strong liveness uses vdfs. We would be organizing the largest multiparty computation ever.
00:19:45.510 - 00:20:27.670, Speaker A: The previous record was ZCash. We would be building the first open source ASIC. Open source Asics haven't really been done before, so this is very exciting for me. And also, as I mentioned, we are looking to have the largest cross blockchain collaboration to actually build this thing as an industry wide project. Okay, thank you. So we have about ten minutes of questions. There's microphones on both sides.
00:20:32.090 - 00:20:53.070, Speaker B: Thank you for the talk. So, question about the RSA number generation ceremony. Can you talk a bit more about this? And what is the input from each participant and how is the resulting number going to be bound to 2048 bits?
00:20:53.970 - 00:21:36.714, Speaker A: Okay, so you want to know about the details of the MPC. So this is still kind of a bit of open research and somewhat beyond my field of expertise. But basically every participant has a random number, and then you take the random numbers from every participant and you add them in such a way that no one knows what the addition is. And then you do bi, primarily testing on the results. So basically, in a way that no one knows the details of the secret, you check that. You're basically looking for a number that is the product of two primes. And if it's not the product of two primes, you do that again and again and again.
00:21:36.714 - 00:21:40.910, Speaker A: So you do many, many rounds until you find a number that is suitable.
00:21:43.490 - 00:21:52.750, Speaker C: Hello, thank you for your talk. Am I right in saying that the problem with definity that you defined was that it fails if nodes go offline?
00:21:53.190 - 00:22:14.390, Speaker A: Right? So definity has a two thirds honest and online assumption. So I made the calculations. If roughly ten to 15% of the honest nodes go offline in some sort of catastrophic situation, or even not so catastrophic situation, then the beacon would stall and the whole blockchain would stall.
00:22:15.130 - 00:22:39.310, Speaker C: Okay, so I just want to challenge part of that assumption. I'm not linked to definitive or anything like that. You're asking us to trust three aspects here. One, that the signing ceremony won't generate toxic waste. Two, that this centralized hardware will be trustable, and three, that this brand new set of cryptography from this year is the right thing to use, rather than just trust that 10% of people won't go offline simultaneously.
00:22:40.850 - 00:22:46.014, Speaker A: Right. I mean, you can pick whatever trade off you want. It's true that a pure software.
00:22:46.062 - 00:22:48.258, Speaker C: We don't get to pick it, you're picking it.
00:22:48.424 - 00:23:26.106, Speaker A: No, at the end of the day, this is a community decision, and we're just making a suggestion here. There is a trade off. You can either have strong liveness and hardware, or you can have a pure software solution and no strong liveness. A lot of the infrastructure that we're building for Ethereum 2.0, actually, all the infrastructure is designed around strong liveness. So that is not something that we want to compromise on. What is totally possible is actually to just stick with Randal.
00:23:26.106 - 00:24:21.150, Speaker A: Randal is a pure software solution with no hardware, but there's actually no loss in having hardware that will upgrade Randal. And the reason is that there's two ways that the hardware can fail or the cryptography can fail. Number one, the RSA modulus is factored, for example, by a quantum computer, and in which case, it would take no time for nataka to compute the VDF. In that case, we fall back on the safety of randal. In the case where all the hardware suddenly goes offline or is all hacked at the same time, then, instead of having a liveless failure, we also fall back on randal for liveness. So the VDF is a strict upgrade over Randal.
00:24:26.140 - 00:24:37.580, Speaker D: So you mentioned about synchronicity for multiparty computation. Can you expand? Why you would need a synchronous ceremony.
00:24:38.800 - 00:25:16.010, Speaker A: I didn't hear the whole question, but I think you're asking, why do we need to have a synchronous? Yeah, simply because the state of the art of RSA mpcs, we just don't know how to make them asynchronous. The zcash powers of tau was asynchronous, and I believe this is more the exception than the rule. So the current state of the art is synchronous. So we're stuck with that. The best we can do is make sure that the duration of the NPC is as small as possible so that we don't waste people's time.
00:25:17.260 - 00:25:23.512, Speaker D: What's the fundamental reason at a high level? Can you expand on that?
00:25:23.646 - 00:25:43.696, Speaker A: Yeah, again, I mean, the MPC is going beyond my domain of expertise. There will be a paper published soon, I believe, by the Ligero team. And actually their work is based on a paper from crypto 2018. So I can point to you, if you email me, I'll point to you to the paper.
00:25:43.878 - 00:25:44.770, Speaker D: Thank you.
00:25:45.460 - 00:26:12.680, Speaker E: Thank you for your presentation. I have a question about the VDF. So you are using the VDF, which is easy to speed up using asics, right? So your VDF is easily speeded up. Have you considered doing a competition for a VDF which would be ASIC resistant? If you like spending $20 million on the ASIC, maybe you can take a million dollars and try to look for different vdfs.
00:26:13.340 - 00:27:23.580, Speaker A: Right? So there are different VDF constructions, and there's some VDF which are known as Protovdfs, where instead of having an exponential gap between the prover and the verifier, you only have a constant gap. And one of the teams, blockchain teams is called Solana, they're actually going that way. So they're using repeated SHA 256, I believe, as the VDF. And in order to allow for parallel verification, they have these checkpoints and then they use gpus for the massively parallel verification. And the assumption there is that intel is very good at designing SHA 256 instructions. What I can tell you is that from the little that I know about the hardware from studying for the last few months, I'd be actually very surprised if intel has an optimal implementation. Initially I was thinking that modular multiplication would, for us, 2000 bit modular multiplication would take maybe ten or 20 nanoseconds.
00:27:23.580 - 00:27:44.440, Speaker A: Now we got it down to two nanoseconds, and there's these pretty fancy optimizations, which I don't expect intel to do necessarily. You have a trade off between latency power area, and intel is trying to find something reasonable. We only want to optimize latency.
00:27:46.700 - 00:28:03.148, Speaker F: Hey Justin, just a quick question about the. So one of the inputs is a difficulty setting. Can you talk just a little bit about how that's calculated and maybe, possibly what the implications are? Is there an attack vector there?
00:28:03.234 - 00:28:03.624, Speaker A: Possibly.
00:28:03.672 - 00:28:05.100, Speaker F: Could it be manipulated?
00:28:06.480 - 00:29:06.144, Speaker A: Right. So the Amax assumption that I've been talking about, we believe we can have it hold for at least five years. So for at least five years we won't need any more asics and we won't need a difficulty adjustment scheme. And once we have the length of the Randall epoch, which is probably going to be something like 17 minutes, and we know Amax. So for example, Amax equals ten, then we just set difficulty to take 170 minutes on the commodity hardware. So that's all it is. If we want to move to a more long term solution where we want to have a dynamic difficulty adjustment, where for example, new hardware enters the market and we want the difficulty to go up organically, we would need to have a difficulty adjustment mechanism there.
00:29:06.144 - 00:29:20.950, Speaker A: It does introduce some complexity, so there is some trade offs there. I mean, I wrote an e free search post on mitigating the main attack, so I'm happy to link it to you.
00:29:21.320 - 00:29:38.140, Speaker F: Just one other question. Say the VDF somehow goes offline, all of them. Does that change the assumptions? Do you have a way to account for the fact that now the randomness is coming from Randall?
00:29:39.760 - 00:30:25.800, Speaker A: Yeah, in this slide, we're basically recursively using the randomness to select the next randall. And if the randomness doesn't come on chain soon enough, which should not happen, but let's say there's some sort of exceptional condition, then we just use the blue dye as opposed to the red dye. So we fall back on Randel. At the application layer, things are actually better. So in the opcode you will specify the EPBoC and it will return either no randomness like, or it will give you the randomness. And so you can design your application in such a way that will just retry until it eventually gets the randomness.
00:30:26.640 - 00:30:28.030, Speaker F: Okay, thank you.
00:30:32.720 - 00:30:46.290, Speaker G: Hello. Is there any sort of incentive for the ones that you are entrusting with the VDF asics to maintain that, to ensure that they are honest other than goodwill and that they're probably highly involved in the scene as well?
00:30:48.180 - 00:31:23.900, Speaker A: Right. So I guess we'll make sure to widely distribute the Asics. And one way to do that is to just give it away for free. There will be in protocol incentives. So the easiest incentive to implement is to provide a reward for the block proposer who includes the randomness and the proof. We could also directly incentivize the evaluator by giving them a reward. And we do have schemes for that.
00:31:23.900 - 00:32:07.180, Speaker A: But there's a trade off between basically introduces complexity and more burden on the beacon chain. So I think it's reasonable. If we have thousands of nodes of VDF rigs distributed around the world, the foundation will run rigs, exchanges will run rigs, investors will run rigs, enthusiasts will run rigs, and we just need one of them to be online. I think that's not too bad. And the incentivization for the block proposals, that actually incentivizes sophisticated block proposers to run a VDF rig themselves and maybe to overclock it just a little bit so that they'll be slightly before everyone else and they'll get the reward.
00:32:08.240 - 00:32:35.860, Speaker G: Thank you. One more question. So you suggested that the VDF rigs would upload or would have their output inside of the block that they propose, correct? That the proposers would have their VDF output inside of what they propose and get rewarded for that because they submit that. Would that imply that these VDF ASICs or rigs are running concurrently with the validators?
00:32:38.440 - 00:33:08.430, Speaker A: So anyone can be a VDF evaluator. You don't have to be collateralized, you don't have to be a validator, but in the special case where you are a validator and you want to earn a little bit more money and you are sophisticated, you can run the hardware in parallel and you can try and make it run slightly faster, maybe cool it a bit better. One of the things that would be cool on this question is if we could have one of the rigs in a satellite around the world, at least we have this one node that's online.
00:33:08.960 - 00:33:09.950, Speaker G: Thank you.
00:33:11.120 - 00:33:23.490, Speaker A: I'm out of time. I'm so sorry. I'm more than happy to speak about this all day long, so please come to me after the talk. Thank you. Thank you.
