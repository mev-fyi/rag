00:00:13.530 - 00:00:37.030, Speaker A: So hello everyone, my name is Ye. I'm the co founder of scroll. Today I'd like to introduce Scro's architecture and our pre alpha testnet upgrade before diving into more detail. For those who are not familiar with hue, you are. So scroll is a general purpose scaling solution for Ethereum. So in short, it's just making ethereum cheaper, faster with a higher throughput. And more specifically, we are building an EVM equivalent Zikiro app.
00:00:37.030 - 00:01:27.778, Speaker A: So technically speaking it's a Zikurop solution which is considered to be the most secure scaling solution with shortage finality based on mass. And we are also EVM equivalent by saying that EMR Zikirap is bico level equivalent, which means developer can reuse everything that they use on ECM layer one toolings, including like hard hat and all those development toolings. And we can achieve native bico level compatible which means you can migrate the code from layer one to layer two seamlessly. And so in the rest of the talk it will be divided into two parts. In the first half I will talk about the architecture of scroll and how your transaction is being processed on scroll. And in the second half I will talk about our important upgrade for our testnet and the roadmap like in the future. So now let's take a look at the architecture of scroll.
00:01:27.778 - 00:02:09.058, Speaker A: So before diving into more detail to give you a better sense of how scroll work, let's take at the traditional architecture for zikirop. So the idea of zikirap is that instead of sending all the transactions to layer one, you send all your transactions to a layer two node and then layer two node will run some zero node proof algorithm and a generator proof. So the proof will be verified on smart contracts layer one. And so verifying the proof is mathematically equivalent to executing all the transactions. So that's how you get the scalability. Because for example ethereum only get ten tps, but each transaction is verifying some proof which is equivalent to executing 100 transactions. Then you can scale your network massively.
00:02:09.058 - 00:03:05.330, Speaker A: So intuitively the architecture of scroll look like this. So you need some sequencer which is sequencing the transaction after receiving that and generate layer two blocks. And then you also need some relayer to relay message between layer one and layer two. For example, there are some deposits from layer one directly through the bridge and your relayer need to relay this message from layer one to layer two. And also there are some deposits where sequencer need to send this message to the relayer. And after sequencer sequencing the transaction, the getting layer to blocks it will send to the prover and the proof will run some algorithm like zero proof algorithm and generate the proof and the relayer will submit proof, necessary data and a unique feature of scroll is that we are not running this prover in a centralized way, but instead we have a decentralized proof network for generating the proof. So in our architecture we have a coordinator which will receive blocks from the sequencer and generate execution trace.
00:03:05.330 - 00:03:45.662, Speaker A: It will dispatch the execution trace for different blocks to different provers in our network. And the provers, we call them rulers in our network to distinguish from miners. They will run thekevm and generate proof and then we'll send back proof to the coordinator. The coordinator will then send to the relayer and relate up to the layer one. So the magic thing actually happens on the ruler side where you are running some ZKE EVM and generating proof for the validity of all the transaction inside the block. So now let's take a look at what's happening inside the roller. So after receiving this execution trace from the coordinator of a certain block, the roller will run the KVM.
00:03:45.662 - 00:04:26.046, Speaker A: So what is the EVM? So the EVM is composed of several circuits. So the circuit means one circuit can verify certain functionalities for certain parts. For example, EVM circuit can verify that your EVM state machine moves correctly from for example like push to pop and to the next upcode you are executing. And then Ram circuit is useful to prove that your read and write for this virtual machine is consistent. For example you previously write to some place and then you read. So this Ram circuit can prove that those are consistent. And there is also a storage circuit which means when you are updating the storage you are doing things correctly.
00:04:26.046 - 00:05:06.446, Speaker A: And there are some other circuits to prove some other functionalities for EVM, including like ECDS circuit for signature and some bico circuits and some catch up circuits for other functionalities. And you need a circuit input builder in between to translate your execution trace directly fetch from gas to the circuit specific witness. And then intuitively the KVM should have multiple proofs, right? Because it need to have a proof for EVM circuit have a proof for RaM circuits. So all those proofs need to be verified on layer one. Efficient. So what we do here is that we build another aggregation circuit. So this aggregation circuit is used for proving that the proof is correct.
00:05:06.446 - 00:06:07.374, Speaker A: So for example like the aggregation circuit is saying that EVM proof is correct, ram proof is correct and other circuit proofs are also correct. So this is your aggregation circuit and in the end you will only have one block proof for the whole block to prove that your execution trace is correct. And moreover, what's to notice that our coordinator will dispatch block to different provers, so those rollers will generate proof in parallel for different block. They are not competing for the same block, which will have a better utility for the poorer network in our system, because all the poolers are doing something useful, they are not doing something redundant. And now let's take a look at how your transaction is being processed on scroll and the workflow of scroll from a timeline perspective. Let's start with the workflow of liquor op. So on ethereum layer one, because you need a consensus, so you generate block very slowly, and on layer two you can generate block much faster and with a higher throughput.
00:06:07.374 - 00:06:51.962, Speaker A: So you generate multiple blocks. And then after a period of time you roll up your transaction data and generate a validity proof to prove that all the transactions are correct and send that to Ethereum layer one. But worth to notice that this block data doesn't really rely on validity proof, it's used for data availability. So what you can do here is that also part of Scrooge's design is that we separate this block data with validity proof. So you will submit the block data first on chain to get some committed version, which for example, users can see their transaction on chain even without the proof. And then you wait for some proof generation to finally finalize your transaction. So accordingly, you have three different status for your layer two transaction.
00:06:51.962 - 00:07:38.618, Speaker A: One is called pre committed, which means your transaction is sent to the sequencer and the sequencer has already included your transaction in a layer two block. So it was sent back a pre confirmation, which is just maybe 3 seconds and something like that. So you get this pre confirmation from our sequencer and the next state is called committed, which means we already roll up your data on chain and which usually takes minutes. And so users can, this is a much stronger confirmation because users can see their data and even replay their data by themselves. And finally is finalized, which indicates that you already generated proof and the proof got verified on layer one. So that's the final state where you get the final confirmation on layer one because your proof is generated and verified. Let's take a look at from a timeline perspective.
00:07:38.618 - 00:08:58.034, Speaker A: So you send your transaction to a sequencer and the sequencer have included your transaction in the block. So the orange one, it means the block is pre confirmed and then the sequencer will upload your data and with some proof to layer one row up contract, and then your block gets committed and then the sequencer will dispatch this block to the coordinator and the coordinator will find one prover inside our network for proof generation like to generate proof for this block and similarly for the next block. Sequencer will also, after committing this block also coordinator to find a roller in the whole system. And similarly you can do the same thing for block three and block four. And after those proof generation the prover will send back the proof to the coordinator and the coordinator receive multiple proofs. And then we do another dispatch to dispatch those proofs to another prover and let the prover do some aggregation to further reduce the verification cost because you can actually aggregate multiple block proofs inside one using one proof. And then after this proof aggregation you finally get one proof which can prove that p one, p two, p three are correct, which means the block one, block two, block three, the transaction inside are valid.
00:08:58.034 - 00:09:41.202, Speaker A: And then you submit this proof on chain clarification and the rough contract will use the previously input as some public input and this proof to verify that it's correct. And then finally your block get finalized. So that's the final state for your transaction. And we have built a special roll up throughout to show the block status. So for example, you have like a few seconds ago you have pre committed block, which is the orange one. And then a minute ago you have multiple committed block and there is a commit transaction hash where you can find which transaction is committing your data and you can find your data on chain. And there is also finalized transaction hash which means for example your proof get verified.
00:09:41.202 - 00:10:26.126, Speaker A: And there is a finalized transaction hash there for showing which transaction contains this proof and when you get verified. So this is a special explorer built by us for letting user to know that what's happening inside. And now after some talking about the technical background, I will introduce our scrolls. Pre alpha testnet and where we are. So three months ago we have released our testnet, our pre alpha testnet. That version is mostly for the community users where we can get user feedback like they can play with our pre deployed applications, for example fork of Uniswap, and also through their familiar wallet like Metamask. So it's all for users.
00:10:26.126 - 00:10:59.710, Speaker A: And users can also bridge their assets between layer one and layer two. Like for example they can experience the deposit and withdraw. They can also see their transaction status through this ruak explorer. So that's where we are. It's all for collecting feedback from the community to improve our UI and UX and also fix some bug ahead of time. And we'd like to thank our community for their helpful feedback so that we fix a lot of bugs on the UI side and improved our front end a lot. And we have onboarded over 10,000 users to test our bridge and depth.
00:10:59.710 - 00:11:55.118, Speaker A: At the meantime, we are still scaling up our pooling infrastructure to support 100,000 users on our waitlist. So the reason for that is that we don't open enough provers for this testnet. So once we open this decentralized pooler network for everyone, we can scale out the users or the transactions throughput like massively. A few days ago we make a very big announcement which is the upgrade version for our pre alpha testnet. So it's a very important milestone for us. So the most important upgrade for our testnet is that we are not only a testnet only for users and for pre deployed contracts, but it's a testnet for the developers where developers can deploy arbitrary smart contracts on us. So it's very important because it's not only interaction between users, but you can actually deploy things on us and you can experience seamless migration without any need to change any line of your code.
00:11:55.118 - 00:12:56.190, Speaker A: You can just directly copy paste your code from layer one and directly deploy on layer two. And we also support all the toolings around because we are natively EVM compatible and even EVM equivalency on the back of level, we can support remix, hardhat and unfundry and all the toolings around. And days ago we have a hackathon at East Global for letting hackers to register to our testnet and deploy things on us. We have also done some live demo at East Global and also yesterday at VK community session where we led the community to deploy smart contract on us and we have opened this register to all the developers. So if you want to become an early tester or the contributor SAP at Scroll IO early dev and you can experience how easy that is to deploy things on us. Now just a quick summary for user and developers. So the developer experience will be exactly the same as ECM layer one for the concrete performance.
00:12:56.190 - 00:13:47.794, Speaker A: So layer two block generation takes less than 3 seconds, which means for example for users you can get your pre confirmation like within 3 seconds it can be even further as we move to multi block aggregation it can be even bring down to 1 second and your experience will be pretty good. And the deposit usually takes two minutes because you need to wait for six layer one blocks. So it's not because ask, but you need to wait for layer one block confirmation and withdraw takes around like six minutes or more depending on your concrete, like how many poolers you have in your network and what's the throughput. So usually this takes like two minutes to 1 hour. But the fastest pool generation already like for one block is six minutes, so it's very fast. That's for our pre alpha testnet. And now let's talk a little bit about our roadmap and where we are and what we plan to do.
00:13:47.794 - 00:14:37.702, Speaker A: From a high level. Our roadmap look like this. So in phase one, we have a pre alpha testnet for user and developers, so users can interact and developer can deploy arbitrary contracts through they registered. And in phase two, we will move to Alpha Testnet, which we will move to that very very soon, which is a permissionless version, and anyone can directly use that without any permission, and developer can deploy any contract without register. And so that's for our alpha testnet, we are moving to that very soon. And in phase three, we open this layer two proof outsourcing to the poorer community, or eulates had a large overlap with the minor community. So which means in phase three we will open proof generation for anyone to be the proverb, and they can run their pooler machine and be one of our proving nodes to generate proof for us.
00:14:37.702 - 00:15:32.530, Speaker A: So that's in phase three. And then we will move to phase four, which is our main net. So the distance between before magnet is that one is that because ZkevM contains many line of code, which as Vitalik indicates that it won't be bug free for quite a long time. So we need very rigid secure auditing, for our Zikkevm to be really confident that we can reach the state of Mainnet. And also we need to wrap up some of the rest Ziki circuit to make that more sound, and also improve our performance massively, like through poor optimization and circular optimization. And in the phase five, we will apply some research result, which we are doing in parallel with the development, is that for example a decentralized sequencer to make the sequencer more censorship resistant. And also we are doing some survey for some neurological virtual machine, and see if there are some interesting part to improve our lick EVMC efficiency.
00:15:32.530 - 00:16:12.878, Speaker A: And so that's our high level roadmap. And one thing which we hear really like a lot of things from the community, that people usually ask about our decentralized approver and what's the requirement for running such a prover node and what's our plan for hardware. So I will tell a little bit more about our plan for this hardware acceleration. So we have three stages, so in stage one, we will build a private, the GPU cluster for running this prover. So we have already built a very fast GPU solution to generate proof for our thekivm circuits. So the current performance is really good. Like for example, 1 million gas only takes six minutes to generate proof.
00:16:12.878 - 00:16:59.986, Speaker A: People usually think generating they keep proof has such overhead and it's unaffordable, but it's actually very fast on our GPU prover. And beside that, besides the GPU solution we have built, we have also built a private GPU cluster to provide the very stable computation power for our testnet at this stage. And so it's already there and it's already live there. And meanwhile we are collaborating with several large companies which are aiming at making the proof faster. They are decay hardware companies and they build more customized solutions for making proofware faster. For example, they are building some IPG solution, ASIC solution, and the GPU solution. So that's in stage one, we started this collaboration, we already built a cluster there.
00:16:59.986 - 00:17:50.222, Speaker A: And then in stage two we will give access to our hardware partners to run our approver, so they can test their approvers and generate proof for us. But at stage two, it's still for large partners which they are committed to generate proof for us and something like that. And we believe that using even more customized proverb can shorten the finality time and massively improve the user experience, because you get cheaper prover and with even faster finality. And so that's day two, and in stage three we will finally move to this permissionless prover, where I call that layer two proof of outsourcing, where you are letting the external parties to run the prover. And we will open source our GPU prover with a permissionless license for everyone to use. So even now our cpu prover is totally open source. You can already run the cpu prover if you want, but just the GPU prover.
00:17:50.222 - 00:18:32.858, Speaker A: We are still improving the performance and will be open source later. And anyone can run our prover, and the prover access will be permissionless, and anyone can generate proof at home for us. And they can also buy some customized hardware from those companies or even stick to use, because there are some companies are providing some proof as a service, so you can stick there and use their service to generate proof for us. So that's basically our plan for this hardware acceleration. And one last thing is that we have a very solid and decentralized tech team. So we have four directions. One is infrastructure team, which is building out the whole infrastructure, making that more robust and to support the permissionless testnet.
00:18:32.858 - 00:19:22.474, Speaker A: And it's ULA based in Asia and Europe. And we have ZK team which building the ZK circuits and some critical parts and for example optimizing the pool performance. So those two are like engineer teams and we are across like six or seven time zones. It's totally decentralized. And also besides the engineering team, we also have an in house security team which makes things really special because the security team because we really care about user security, right? There are so many bridges or platform get hacked. So we have this security team which composed of several experts like expertise in blockchain security, smart contract auditing and cryptography. So they will be in charge of our security of the whole system and also collaborate with external hackers and auditors to make our system more secure.
00:19:22.474 - 00:19:59.178, Speaker A: And finally we have a research team exploring very multiple research directions. For example how to decentralize a sequencer and how to upgrade the next generation's proof system. And doing a lot of interesting research like that. And also around Ethereum we are actually contributing to a lot of eips. So that's part of the research team. And our vision is that we want to onboard the next billion of users for Ethereum because we think making the transactions really cheap and your confirmation really fast will make more users go into Ethereum ecosystem. And everything we build is totally open.
00:19:59.178 - 00:20:27.320, Speaker A: And especially for the Zikiwan part, we are co built with a large community, for example the privacy and scaling exploration team from XM foundation and several other community members. And we want to find for decentralization across different levels like starting from decentralization of the approver. So if you are a vision alliance and you really like what we are building and we are still hiring and check out our hiring page and I think yes, that's it. And thank you for.
00:20:28.810 - 00:20:40.390, Speaker B: Yeah, hi. So obviously you have this kind of cool infrastructure with the prover and the sequencer. Could you talk about how gas fees work in scroll? How you price transactions?
00:20:40.730 - 00:21:06.242, Speaker A: Yes, so the gas fee. Currently we hard code that to be exactly the same as ECM layer one. But it might subject to change if it doesn't match the proving cost. But it will be minor, mostly targeting as some pre compiled, very expensive pre compiled which are not liquid friendly. But most opcode will be the same. And right now it's exactly the same. Hi, can I know the data availability strategy for scroll? Yes, so that's a good question.
00:21:06.242 - 00:21:29.880, Speaker A: So currently we are directly submitting the raw transaction data on chain as part of the data availability. And we do believe that dunk, sharding and other cheaper data solutions on Ethereum is coming very soon. And also by submitting the role transaction data, users can replay the transaction when you are in the committing stage. So you don't need to wait for the proof generation time to get a stronger confirmation ahead of time.
00:21:30.750 - 00:21:39.610, Speaker C: Thanks for the talk. What's the impact of Reorgs on Ethereum, on the components like the sequencer coordinator and the prover?
00:21:40.130 - 00:21:43.194, Speaker A: So you're asking, how do you handle.
00:21:43.242 - 00:21:45.038, Speaker C: Reorgs on layer one?
00:21:45.204 - 00:21:46.174, Speaker A: Handle what?
00:21:46.292 - 00:21:49.390, Speaker C: Reorgs, reorganizations of blocks.
00:21:50.530 - 00:21:54.098, Speaker A: You mean like layer ones? Blocks are not confirmed or.
00:21:54.184 - 00:21:57.586, Speaker C: Yeah, if blocks get reorganized on litigation, yeah.
00:21:57.768 - 00:22:16.146, Speaker A: So basically, when your transaction is within layer two, it can be confirmed really fast, so it will only influence your deposit. So for now we just wait for six blocks. But in the future it might change if we think it's not so safe enough. But for now we just wait for six blocks.
00:22:16.338 - 00:23:08.006, Speaker D: Thank you for the presentation. I have two questions. One of them is about the hardware component. How do you make sure that there's a decentralized network of the people who are provers? If you're working with specific companies, how do you make sure that the provers are a decentralized network versus being centralized to one or two specific FPGA companies or GPU companies that become very large stakers? And then my second question is, so this process for decentralizing the prover, can you talk about some of the differences for challenges in decentralizing the sequencer? How does those two processes differ? And what are some of the different considerations for decentralizing a sequencer versus the prover? Thank you.
00:23:08.108 - 00:23:43.342, Speaker A: Yeah, that's a very good question. So, for the first one, as I mentioned, we will have two versions. Firstly, that as we are collaborating with the external companies, we will also open source, permissionless, licensed GPU prover. So anyone can directly use the GPU prover if they don't want to use FPGA or some other companies. And we are not incentivizing the fast prover because, for example, even if someone has ASIC prover or someone has IPJ proofer, they don't necessarily can beat you. So the strategy is that we will have a time period for submitting the proof. As far as you can submit a proof in time, you can be incentivized.
00:23:43.342 - 00:24:36.914, Speaker A: So it doesn't necessarily, you have to generate one minutes. You can always beat the other proof. So it's more like for parallelization and how you are making use of the computation power across the whole network in parallel. So even if you have those hardware partners like companies, you can still choose whether you just want run independently using GPU prover or using their service. And so that's for question one. And for question two, when we are thinking of this, is that the proverb is easier to be decentralized because for example, for now at this stage, we are having a centralized coordinator, so you can still have some, for example, verifying the proofs and doing something like that. And so when we are thinking of decentralized approver and sequencer, because it's actually two communities, because the proverb community requires specialized hardware, but the sequencer might be just some level of decentralization.
00:24:36.914 - 00:25:13.900, Speaker A: And when you are making the sequencer decentralized, there are some problems. Like for example, if you want to do some force withdrawal and order interaction, it's much harder there than using a centralized sequencer. So that's part of the problems. And also how you incentivize between sequencer and approver and how to balance those incentivized. That's also part of the challenging problem we face and how to make the whole system more efficient, because you still need some consensus there among those sequences. And yeah, close.
