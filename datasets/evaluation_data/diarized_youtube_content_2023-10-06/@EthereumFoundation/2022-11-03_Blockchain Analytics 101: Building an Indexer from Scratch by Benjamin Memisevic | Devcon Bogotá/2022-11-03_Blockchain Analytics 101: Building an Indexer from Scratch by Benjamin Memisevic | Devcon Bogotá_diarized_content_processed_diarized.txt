00:00:14.010 - 00:00:49.990, Speaker A: I'm a software and smart contract engineer. Been in the space since about 2018. Today I'm basically going to do a tutorial where we'll build through a simple blockchain indexer and just sort of explain how indexes work, different pipelines you can design and make and in general just interacting with geth on a sort of RPC level. Okay, just some background quickly about me and how I would love to run the presentation in general. Yeah. So as I mentioned before, I've been in the space since 2018. I used to work at a consultant in Labyrinth, which is an Australian smart contract consultancy.
00:00:49.990 - 00:01:38.700, Speaker A: And then I worked as an engineer at Tracer, Dow and Mycelium, which is a perpetual swaps protocol on Arbitrum. Yeah. I was the team lead at Reputation Dao, which was a project at Mycelium where we focused on indexing and monitoring Oracle systems on chain to make sure Oracle is essentially held accountable and also that they're functioning properly and accurately. So sort of just detecting errors sort of in that pipeline. So in terms of my experiencing in the indexing space, I've been working in indexing for about two years. I like to know what I'm talking about, but I guess we'll see. In terms of questions, please just shoot up your hand if you have anything as I go.
00:01:38.700 - 00:02:02.340, Speaker A: Okay. Etherscan. Ether Scan is probably the most popular indexing tool all around. I've spent a lot of time on Ether Scan. It's basically a hobby of mine. I'll click through random blocks and see if there's something fun. If you've ever accidentally sent ETH to a contract instead of calling the function and you think nobody saw it, I saw it.
00:02:02.340 - 00:02:32.910, Speaker A: So this is one trend. Oh, there's a screen here. This is just one address that I found this morning in A block, and it's essentially an EOA that seems to be fanning out ETH to a bunch of addresses extremely quickly. So it's sending out four or five transactions, trying to spread out, I think, about 0.8 E. Yeah. But it's really worthwhile, if you haven't done that, to sort of explore what's on Ether Scan and see how interactions are happening on chain.
00:02:32.910 - 00:03:11.990, Speaker A: Okay, so observability and transparency, I know at the moment, ZK and privacy and transactions is highly important, and that's absolutely true. And if you want privacy in your transactions, go for it. But on the other side, observability is also highly important. Just as a question, does anybody have any pros that they think having transparent transactions have over ZK? Yeah, no corruption. Exactly. Nice. And I'm not trying to just shill transparency.
00:03:11.990 - 00:03:47.350, Speaker A: Don't worry, I don't work for the SEC or anything like that. I'm not after the taxes. But it's still highly important in certain systems. Oracles, which I work for, don't really function on zero knowledge. You have to know that your Oracles are operating properly and you have to know what each Oracle is submitting. Having anonymity there reduces the accountability of those Oracles and it can cause your systems to not function the way you hope. One bad Oracle transaction can completely crash a market, especially derivative markets.
00:03:47.350 - 00:04:56.378, Speaker A: This is actually one example here also from this morning. So Mongo markets lost $100 million last night because of Oracle manipulation. So making sure that you can observe what's happening and also the exposure that those Oracles have onto each market is very important and something where transparency and building an indexer is highly useful. Okay, what actually is an indexer? The way I define an indexer and the way I like to think about it is it's basically an ETL program where your back end is communicating directly with a node. You're requesting data from the blockchain, and you are converting that data or storing it in a unique way that makes it more accessible or useful to you. Yeah, sorry, let me just run it here. So why would I actually want to store it differently than on chain? Because the blockchain is a database in and of itself.
00:04:56.378 - 00:05:53.322, Speaker A: All the data is always there. The problem becomes, though, if I want to retrieve specific data or a range of data or God forbid, create some sort of average across it. So if I wanted to do that, I might have to make hundreds of ETH calls to process one transaction or one event that I define on chain. So one really popular reason to make an indexer, which a lot of now RPC sort of services like Alchemy and Coinbase support, is to fetch the transaction history of an address that is not possible directly. When you're directly interacting with the node, you would have to pass through every single block that that address has made a transaction on and then manually look at those transactions. Another really good reason is non ephemeral data on the chain. So mempool data, anything that you run through as a test, as an ETH pool, doesn't stay on the blockchain forever.
00:05:53.322 - 00:06:51.230, Speaker A: Mempool data in particular is highly useful for arbitrage strategy, but there's actually no consistent history of the mempool. Obviously, mempool also differs from node to node, but having a history from a range of nodes is highly useful if you want to sort of back run strategies that you could have done if you're an arbitrature. Yeah, also Gas estimation as well. If you're making ETH calls that don't actually go through chain, you want to have a history of how your contracts will look when they're finally deployed. Creating a mini indexer that runs off your local host smart contract node is absolutely a valid option. Now I just want to quickly break down a log that happens on chain quite frequently. So this is what happens when a chain link Oracle submits, well, lots of chainlink Oracles actually submit an answer for a price feed on chain.
00:06:51.230 - 00:07:31.680, Speaker A: So this is the decoded version. I'll show you the encoded version a little bit later on. But even here not everything is super obvious to the eye, right? So you have the answer at the top, which is obviously the new value that price feed will have. For those curious, this is the price of one inch. But there's other points that are really confusing, right? If you look, there's observations where you have what seems to appear as an interray, right? And you have Observers, which is just this confusing byte array with a weird name. You have raw report context. Let's sort of just look at what's going on.
00:07:31.680 - 00:08:10.266, Speaker A: Okay, so step one to creating any indexer. Creating any indexer, sorry. Or sort of observation solution is first you have to understand the contracts that you're working with. What your ETL really does and what it should aim to do is to take this raw, like hex data on chain and give it new meaning or new accessibility. And to do that, you have to know what the contracts are doing already. Just copying the data across isn't really enough. So sort of on that talk, on that point sorry, I'll just quickly explain how OCR works.
00:08:10.266 - 00:08:56.770, Speaker A: That was the chainlink log that we just looked at previously. So how chainlink operates nowadays is that also I don't work for chainlink. I just want to say how chainlink operates now is that most of the aggregation and collection of data happens off chain. So a price deviation will happen. All these nodes will look at their different APIs and they'll send all of their answers to a single node. And then this node will submit all of these answers and the final aggregated answer on chain on behalf of everybody else who's supporting that price feed. So, sort of knowing that, I'll also just go through a simple timeline.
00:08:56.770 - 00:09:40.950, Speaker A: So, as an example, say I have a chain link feed that's monitoring the price of ethereum. When the price of ethereum dips by a certain percentage, all the chainlink nodes will know, and they'll start a new round. Every single chainlink node will then send an answer that they've collected from their APIs to a leader. That leader will aggregate that answer, normally taking a medium of some sort. He'll pass this along to the elected transmitter for that round. And then that transmitter will send off a message on chain containing all the information that everybody submitted individually, as well as the final answer. And why I sort of went through this is by knowing this information, knowing how the system works, we can now make sense of the log that we looked at earlier.
00:09:40.950 - 00:10:23.886, Speaker A: Okay, so believe it or not, this observers array right here, byte array, is actually a list of the addresses that each of these observations come from. So all those numbers, there's a corresponding oracle address of the oracle that submitted that answer. And that answer is the number that they thought is true. Another thing to note is that number is very large. Yeah. If that number was exactly how it is, I would be retired as holding like 51 inch tokens, but here I am. So this is sort of another step we have to overcome.
00:10:23.886 - 00:11:22.470, Speaker A: So we're sort of missing more information, but we know it's available somewhere within the contract. So this information is available through a view function in the chainlink aggregator contract called Transmitters, which returns a list of the addresses of each Oracle that is supplying that contract with data. And there's another function called decimals, which tells us by how much I should divide those numbers, those observations, to get the accurate, like, human readable price of one inch in this case. The problem, though is that these variables can change block by block. These are adjustable variables. Transmitters change all the time, not only for security reasons, but sometimes certain Oracles are better at performing on certain feeds than others. And decimals can change when different markets require more precision out of a price feed.
00:11:22.470 - 00:12:15.480, Speaker A: Okay, so this is sort of how we break it down. So I had my observations arrayed there and the way to break it down and you can find this out by sort of reading the contract and then doing a bit of guesswork, is that if I look at that byte array and I split it apart into pairs, each of those pairs can be decoded from hex into a number. This number represents the index of which that observation in order corresponds to which item in that Transmitters array the observation was from. So in this case, the second observation here was from the second Oracle. So we're indexing from counting from zero. So zero x three, one, two. EA responded with 439399.
00:12:15.480 - 00:13:13.786, Speaker A: So already we're sort of getting kind of complex in how we're just deciphering this one single log. So we'll go through the steps in code as well, how we would do all the different calls. But essentially at the minimum, what we have to do to fetch this data is first we have to encounter this chain link log. Then we have to pass the data back into regular data types, take our Transmitters byte array, break it up into pairs, convert that into hex to get a number, call the Transmitters function with an ETH call, and then pass out which addresses with which address. Now, doing this for one log, not too bad. I probably wouldn't want to do it by hand, but imagine if you wanted some sort of aggregate data. Imagine if I told you, okay, tell me how accurate this Oracle was on every single Wednesday for the past six months.
00:13:13.786 - 00:13:50.118, Speaker A: Now, you're passing hundreds of logs, you're making thousands of calls. And if you want this to be done on demand, it's going to be incredibly slow. It might take minutes. On the other hand, sorry, I could make like an SQL statement, like select average where Oracle equals this, where time equals this, and this. I'm converting hundreds of lines of code, thousands of http requests to one line in SQL. That's sort of the value that indexing can bring. Before we actually get into the indexer, I also just want to go through the data types Ethereum has.
00:13:50.118 - 00:14:28.814, Speaker A: So these are sort of our avenues into getting different data. So there's actually more than this, but this is just a quick summary. So does anybody know by heart what things that you can search on Etherscan or any like, blocks, contract addresses? Any others? Okay, yeah, transaction hashes. Yeah, so transaction hashes, addresses, block number, block hash, sorry, ENS names.
00:14:28.862 - 00:14:29.266, Speaker B: Yes.
00:14:29.368 - 00:14:59.258, Speaker A: Nice. No, you cannot. Not within the top search bar. Yes. So these things are the things that Ethereum will naturally index. So you can already select these from the node relatively quickly and they each have a relationship with each other. If I have the log, I can retrieve the contract address that that log was emitted from, and I can also retrieve the transaction that log came from.
00:14:59.258 - 00:15:38.150, Speaker A: And then through that I can also retrieve the block number that that transaction happened on and so on. And receipts as well. So transaction receipts. So I can see how much gas was paid for that receipt. But the point here is that you're sort of migrating across these different data types to collect all the information you need for your index. So dissecting a log, I'm just going to go over the structure of logs. And for those curious, the reason why I'm concentrating on logs is that there's quite a lot of functionality within the Geth client and with blockchain clients in general for creating these indexers based on logs.
00:15:38.150 - 00:16:22.630, Speaker A: Creating them based on transactions is also possible, but a lot more manual, you have less sort of filtering power on that first step. So how a log works. So logs are constructed out of topics and data essentially. So topics are also data. Topics one to three are data types that you can emit in your event and mark them as indexed. And why this exists is that instead of building an indexer, I could potentially just use the blockchain client as well to look for cases where the data is equal to a particular amount. So I could tell Geth, tell me when topic zero is equal to X and topic one is equal to this, and retrieve all those cases for me within some block range.
00:16:22.630 - 00:16:53.940, Speaker A: Topic zero, I'll go over in a moment. The data, sorry. Something to also note on topics is that you can only index up to three fields data, though you can throw as much stuff as you want in there. And this is like highly valuable. And a good way to think of logs and data in general is sort of like a print statement for your smart contracts. So if you've ever like debugging, and we're all programmers here, and we hate using the actual debug tool, so we just print. Hello.
00:16:53.940 - 00:17:39.630, Speaker A: This is exactly what a log is yeah, so we also have the transaction index, which is at what point our transaction appeared within that block. And also log index which is separate, which is where this log appeared within that block, which is normally completely different. There's also removed, which is like a ball. That just signifies if our log was removed from the caninegal chain due to a reorganization topic zero, topic zero is highly useful. Topic zero defines the log, I would say. So what topic zero is, is I take my event definition. So say I have an event called transfer and it takes a UN as an argument.
00:17:39.630 - 00:18:46.534, Speaker A: I want to look at every single instance on the blockchain where this transfer event happened and to do that I have to calculate topic zero, and to do that I take the caca of my event name, so transfer and then each data type that is within that event definition. So here it would be UN 256 or whatever. Some important things to note you do not include, and I learned this painfully by experience, do not put spaces between the types because your hash will be wrong. And also don't put the name of the variables within the type because the EVM doesn't care about that. So something to note as well is that this topic zero is unique for every single contract. That is to say that you can only have one event with one exact hash definition per contract, but this is not stipulated across the whole chain. So I could potentially say look for every single log with topic zero, but I might actually collect data for four or five contracts that are doing a completely separate thing.
00:18:46.534 - 00:19:35.810, Speaker A: That's just something to be wary of. Storage another really cool thing that you can do when you're building an indexer is accessing the actual contract storage. So private variables now become completely accessible to you. This is really good. One of the main use cases that I've certainly used this for is when a contract is implementing EIP 1967, which is the proxy pattern. So you have a contract that's implementing all the calls from another contract and that contract's address that's actually got the implementation is always in a specific storage slot. I can go into this example later as well, but this is just one hugely powerful example of stuff you can access if you're building your own indexer.
00:19:35.810 - 00:20:17.422, Speaker A: Okay? Infrastructure Design so this is a bit of a Web Two thing, but it's important to think about how you want your indexing application to run. You can make it as complex or as simple as you want. So if it's just a regular ETL, you can have your program connected to a node and then you're inserting into a database. But you can do so much more. You can create a hackathon submission that is looking for events on Lens for example. And then I'm sending out alerts through EPNS every single time someone's profile gets know I could scale my application. I could have several nodes, have a load balancer between it, so I'm not overwhelming any single node.
00:20:17.422 - 00:21:18.294, Speaker A: I could be sending off my messages after I've dealt with them to an AI analysis, and I could be storing them across several database nodes. Stuff I've worked with personally that I recommend is a sort of microservice architecture. You can have Kafka sort of in the middle as a message broker and pass information between all your services, all your different storage points. Maybe you're an arbitrager and you want to do sort of an analysis, throw that analysis inside in memory database like readers for quick access, while simultaneously throwing any archive data into your own sort of postgres database database options. So indexing is really powerful because you can choose how quickly you want to access what subset of data. So these are just like a few I threw up. So Timescale DB is extremely interesting, but any time series database sort of proves this point.
00:21:18.294 - 00:22:11.318, Speaker A: The blockchain doesn't give you an option directly to look through transactions within a time range. You can specify a block range, but that's not really always ideal. Blocks take a different amount of time to create over time, and you have to do sort of extra legwork there if you want to convert from block to time. Timescale offers something really awesome, especially when you're looking at a large data set like a blockchain, and that's continuous aggregates. And these are materialized views that you can create for aggregate data. So, for example, I can calculate the average every single week of how much gas is spent on Ethereum, and I can permanently store that data in a view and access it instantly, as opposed to having to recalculate that data. KDB is very popular within sort of the Mev community.
00:22:11.318 - 00:22:55.540, Speaker A: From what I've seen, it's a completely in memory database. It's extremely fast, so similar along the lines of redis, but far more performant. It's used a lot by quant firms Arbitrage and also formula one like racing, so they use it for in race analysis and stuff like that. Postgres absolute classic, completely free, perfect RDS system. Why would you use anything else? Don't talk to me about MySQL I hate it. Okay, now I just want to go through a quick code walkthrough of a simple application so we can actually build these indexers in about 150 lines of Go code. Easy.
00:22:55.540 - 00:23:46.680, Speaker A: Okay, so first I want to explain my language choice because I've been contested on it in the past. I really like using Go for the backend and interacting directly with Geth and with the Geth library. First of all, Geth is go is fast. Geth is probably the most well maintained library. It obviously doesn't have features of certain other blockchain Ethereum clients, but it is definitely the most well maintained and it always fits through the specs, the yellow paper specifications. Go has a huge bonus, and that parallelization is directly built into the language. So we're going to be doing a lot of calls through web circuits or through Http and we want to paralyze these and we don't have to worry about race conditions when we're using Go.
00:23:46.680 - 00:24:44.534, Speaker A: Yeah, another huge bonus is that all of these functions that we're going to call to extract data, our program is going to be completely portable to every single EVM chain except from memory XDI. So we can port our application, make it as generic as possible, and we can start looking at events on polygon, ethereum, optimism, Arbitrum, whatever you want without having to do any extra legwork. It's like deploying a smart contract to multiple chains. And the reason this is, is all these RPC calls that we're making are specified within the yellow paper. So any EVM client that is being created is going to have to fit through these specs. Okay, creating a client, this is like pretty basic code, so I'm just creating a Http client. I'm feeding in my Alchemy sort of key.
00:24:44.534 - 00:25:00.460, Speaker A: That's what the RPC thing is there. It's just a string. I'm doing a bit of error handling here, making sure that I am in fact connecting and that's really it for this. There's really not too much to worry about here. Yes.
00:25:03.470 - 00:25:04.570, Speaker B: Like remote.
00:25:07.330 - 00:25:42.730, Speaker A: Okay, that's a good question. I'll just repeat it. So the question was, should you run your own node infrastructure or use an outsourced node like Alchemy? So there's several advantages to running your own and a few disadvantages. Running your own node beyond you just have direct access and you can manage all your load yourself. You can also put other sort of add ons on top of that node. You can put add ons on top of that node to make the indexing even faster. So when you're doing retrieval, you're sort of indexing already on an index, which is fantastic.
00:25:42.730 - 00:26:22.390, Speaker A: Me personally, if I'm running in a production environment and I have an application that's fully running as an indexer and my system is depending on it, then I would run my own node. I see some node operators here and I feel bad, but for hackathon projects, absolutely use Alchemy. Fantastic solution. Not just Alchemy, any infura, whatever. I'm not sponsored guys, I'm sorry. I keep using the same names, WebSockets versus Http. So most providers will allow you the option of accessing their service through a WebSocket or Http.
00:26:22.390 - 00:26:58.100, Speaker A: I'm just going to go over this quickly because it's more of a web two thing. Http is normally how your wallet connects to the chain. So whenever I'm making a request, my wallet is creating that TLC connection and it's shooting back a message and that connection is then closed. When you're creating an indexer, you're going to want to make lots and lots of requests and that extra latency between creating a new handshake every single time is actually quite cumbersome. I would just say straight up that if you can use a WebSocket connection. Always use a WebSocket connection. No reason not to really.
00:26:58.100 - 00:27:46.194, Speaker A: Okay, now we're going to look how to create a query. And this is an SQL. By query I mean we're going to query data directly from the node. So there's this ethereum geth object called filter query and it does exactly what you think it does. It takes an array of addresses from which I want to collect logs from and it takes a list of topics. So before we talked about topics and how you can index up to three and you have topic zero. So you can specify here I want to collect data from which topic zero and which other topics I want to be equal to some hex value and I sort of just declare the subject and that's all you really need from here.
00:27:46.194 - 00:28:16.742, Speaker A: We haven't actually made the call yet. This is just constructing the query. This also takes a block range, which I didn't include for some reason. Next. Okay, so now we're actually making the query and this is where block range becomes very important. So there's two ways in which I can request data from a blockchain node. I can do subscribe filter logs, which is I'm establishing a connection to the node and I'm telling the node, okay, here are my parameters.
00:28:16.742 - 00:29:00.460, Speaker A: Whenever this happens on chain, send me a message. So this is really useful because now I can sort of distribute my load from the node because I get these messages one at a time, I can process them and as long as my application is at least decently efficient, I'll process everything without any lag. Now this always isn't an option because one thing about subscribe filter logs is that I can't request historical information. I will only be receiving data from the point I have subscribed. So from the next block that's coming. Now I can also do filter logs in which I can request a bunch of logs that have already happened. So that's essentially what that does.
00:29:00.460 - 00:29:43.154, Speaker A: It's really useful, especially if you want to do a historical analysis. The thing to be wary here is that it is quite hard on the node if you're asking a retrieval of megabytes or gigabytes of data. And also your application is going to have to keep time as well. Definitely super expensive if you want to collect the entire history for a particular log throughout the whole chain. Okay, channels. This is a Go thing, but it's also extremely important to know and it's one of the reasons that I suggest making these index applications in Go. So what a channel is in Go is essentially like a pipe.
00:29:43.154 - 00:30:14.710, Speaker A: So if I'm sending data from a process to B process, I use a pipe. But there's certain issues associated with doing this, right? So you have to deal with race conditions for one, which is a huge headache. Also, piping can be quite messy. I don't know how you're doing it, but you can do it like through a bash script. It can get pretty weird. What Go offers is channels. And channels are naturally blocking so I don't have to worry about message one, getting there before message three and screwing up all my analysis.
00:30:14.710 - 00:31:12.530, Speaker A: The channel will only send a message across to my next process when that process is ready to receive the message. And this for loop here, we're sort of declaring an infinite for loop and using this select statement. And the select statement is just saying, I've created this channel, logs one, and I'm waiting continuously until something is sent to that channel and the blockchain node is sending data to the channel and I will pull out that message as soon as it comes. And select is just saying whichever one comes first. So if my channel comes back with an error, I'll deal with that and crash my program. Or if a log comes first, I'll go and do some processing. Okay, how do I actually process data? So when I request data from the blockchain and I get logs back, I actually get back a pretty messy data structure.
00:31:12.530 - 00:32:01.140, Speaker A: Not messy, but not human readable. So what I'm going to get back is logs data structure, which we went over earlier. I'm going to have these topics and I'm going to have the data field inside the log. And if I'm pulling the transaction similarly, I will have the data field within the transaction field which is identical to the sort of log data. Now passing this across is pretty complex. So there's natural part padding that happens from RLP encoded values and I also have convert from hex back into sort of regular values, at least for Go to interpret, but also so humans can interpret it. I don't really know what's happening when somebody says, oh, the value came back is like zero x 60, zeros and then a three.
00:32:01.140 - 00:32:40.762, Speaker A: So what we can do instead is we can generate an Abi. And this is exactly how Ether scan does it. And for those not familiar, an Abi is a specification of all the functions and all the events that happen on a particular contract. And I can generate this by going to a contract and using Solsi. So that's the command at the top, solsi Abi and the name of the file or the path, sorry. And then I'll get a huge spit out of a JSON file and we'll use this and define it as a string and we'll use it further along. But the Abi is highly useful.
00:32:40.762 - 00:33:25.920, Speaker A: But something to note is that it does not directly appear on chain. On chain is only sort of the compiled like bytecode. So the Abi you have to get by having access to the source code or sometimes people upload it to Etherscamp. And what the Abi allows us to do is the Abi knows, as I described before with the Kak, it knows all. The input types for all the functions and all the events that are defined within that contract. So now I can use the EVM to sorry, it's plugged in. Should come back up in a moment.
00:33:25.920 - 00:34:01.594, Speaker A: Okay. Oh, it is back. Fantastic. So I can use this Abi and use the EVM to decompile all of this hex data that I'm going to get blurted out back into regular data types. So creating an API object similar to the JavaScript concept, but I'm going to call this Abi function, which is also abi library, sorry, that's also part of the Geth module. And I'm going to pass in the Abi string that I defined previously. I'm going to check that it is in fact a valid string, which is important.
00:34:01.594 - 00:34:30.910, Speaker A: And then I can start unpacking data. And that's that second sort of code block in there I call my object, I tell it to unpack. I pass it in a string, which is the name of my function, and then I pass in my data. And what it's going to spit out is a huge array of interfaces. And interfaces are just generics in Go. So it's data that I don't know the type of, and I have to just tell Go which data type that is. And I can find that out by looking at the smart contract code.
00:34:30.910 - 00:35:09.870, Speaker A: I can find that out by looking at the Abi. And even if I make a mistake here, go will tell me that I made a mistake. If I try to assert wrongly that it enters a string, go will tell me, oh, actually you want to type assert to a string. I don't know why I can't just do it automatically, but life can't be that easy. Okay, working with a database. Okay, so now essentially what we've constructed is we have a system that's requesting data from the node either live or through a historical query. I've collected that data.
00:35:09.870 - 00:35:54.640, Speaker A: I now have a method to unpack that data into regular Go data types, and I can convert between them as I wish. Now I want to insert that data somewhere. So this is a pretty standard way of just interacting with a database in Go, completely fault proof, and it's using a library called Gorm. So Gorm, it used to be sponsored by Chainlink actually, as well. Pretty fantastic library, highly recommended. So it's very easy for what I have to do, I just define a struct, which has what I want my table to look like. I throw on some strings where I want my primary keys or foreign keys or indexes, and I'll just leave that in the module by itself.
00:35:54.640 - 00:36:32.454, Speaker A: And then what I'll do is I'll initiate a connection to my database. And it's also just this one line code, so I just tell it to open. I have postgres listed here, but it has support for most popular database management systems and Gorm config, which you can specify different things in but we can just leave it blank. If you're not doing something fancy, you can pretty much leave it alone. I do some error tracking and then I've got migrations for those unfamiliar. Migrations are fantastic. If you ever worked for a production system and you've wanted changes in your database and you basically it's like a nuclear reactor.
00:36:32.454 - 00:37:30.794, Speaker A: Like two people have to turn their keys at the same time to edit a table. Migration sort of sidestep that. What Gorm will do is I can pass in my structs that I defined before and it'll automatically create or edit the tables that I already have. So that way the code that I have is going to be exactly represented in my database inserting. So we're pretty much like done here guys. All I have to do now is use the structs that I previously defined, pass in my decoded values, and then I just have this one line insert or update on conflict. So what this is essentially doing is I'm sending a message and it'll be acid depending on your database of choice, I guess out to the database and I'm telling it okay, if there's no entry, insert it, if there is, update it and updating actually absolutely can happen.
00:37:30.794 - 00:38:12.070, Speaker A: Although the blockchain is immutable, remember that the node is getting new data all the time. reorgs do happen. So something you want to watch out for is your block hash changing and also your block timestamp profile IDB. Sorry, we just have like type assertion here so you can sort of see how these are getting passed back into the struct. So I have my profile ID and this is an example that I did based off Lens. So profile ID and Lens is an integer which represents your user on Lens and then also NFT, which is an NFT that's generated on Lens whenever you follow somebody. And I just pass these into that struct.
00:38:12.070 - 00:38:36.660, Speaker A: I'm calling my database clauses update function and I'm passing in a pointer to the struct. Okay, so that's basically it. I can go through some actual examples well, but I just want to do a Q and A actually. How much time do I have left? Got plenty of time, yeah.
00:38:40.870 - 00:38:42.050, Speaker B: Five variable.
00:38:44.950 - 00:39:37.480, Speaker A: Okay, so EIP 1967 basically says it's a proxy pattern. So say I want to make changes to a smart contract. You obviously can't really do that. So what I can do is I can define EIP 1967 contract and tell it do all the functions of another smart contract and I can change which smart contract logic is actually being changed by just changing that variable. But the problem is that there's no view function for the contract address that's on that 1967 contract. But the storage slot where that address is stored is always the same across any EIP 1967 contract. So I can retrieve this address and then I can query that contract for any information that I want.
00:39:37.480 - 00:39:59.450, Speaker A: Does that help okay, yes, I wanted to clean up a bit, but it's submitted within the ETH hackathon. I can send a link out, I'll send it out on my Twitter, which is at the end, how many block confirmations.
00:40:01.950 - 00:40:04.090, Speaker B: Or how you update?
00:40:06.210 - 00:40:39.420, Speaker A: Yeah, I'll just repeat the question. But the question is how many block confirmations do you wait before inserting the data into the database or how do you deal with updates? So the great thing about this system is that you don't have to wait at all. Every time there's a new reorganization, the logs will be re put through the node and then the indexer will automatically update that column for you. So the data you have in your database will always be the most accurate that it can be and most up to date. Sorry, yes.
00:40:40.830 - 00:40:43.610, Speaker B: Voluntary, not standardized.
00:40:44.030 - 00:40:45.740, Speaker A: Yeah, but it's like the most.
00:40:48.750 - 00:40:51.602, Speaker B: Why is it like not standard? Why is it voluntary?
00:40:51.766 - 00:41:19.560, Speaker A: Why is it voluntary? I mean, it costs gas to emit events. Why isn't it standardized? I mean, it is standardized in development. It's not like you have to deploy a contract with events. And if there is contracts deployed without events, you can also create an indexer based on that. I think that's what you're getting at. Yeah. Okay, for sure.
00:41:19.560 - 00:41:22.760, Speaker A: Sorry, can you repeat the question?
00:41:26.090 - 00:41:27.750, Speaker B: Why some logs?
00:41:31.550 - 00:42:30.060, Speaker A: Okay, why indexes mostly use logs that's also, I want to say more my opinion than true fact, but also most popular index platforms. So the graph and stuff focus on logs quite a bit and sort of why there's so many logs and you have to find the useful ones. That information is hard to come by. And if you want to create your indexer, as I said, you should know the contracts and the system. But also you can make a general solution like the graph or like, oh, I forgot the name, can someone help me? Their logo is like Dune Analytics thank you. Like June Analytics where I can do an SQL query on absolutely anything. But in terms of why there's so many events happening, a lot of these events are just shot out for debugging purposes and they're left in the contract and you sort of have to decipher as you go.
00:42:30.060 - 00:42:34.240, Speaker A: Any other questions? Yes.
00:42:36.130 - 00:42:37.360, Speaker B: Strategies for.
00:42:41.730 - 00:43:18.054, Speaker A: Contract state. Yeah, so as before, as I went through storage, it's getting annoying. If you want to get contract state, there's three main things that you can do. So there's access through variables, so every single public variable on a contract automatically has a getter assigned to it. So that's one way where you can access state. The second one is through storage, as I mentioned before. So it's kind of tricky to find out exactly where a variable is in storage.
00:43:18.054 - 00:44:09.654, Speaker A: But if you go on the solidity documentation page, they will actually walk you through how storage is structured based on the contract. And you can sort of moul your way through and find the variable. The third is traces. So traces are the individual opcodes that a contract is pushing, which you can also request from the node. And with combination of those three, you can see absolutely everything that a contract does in terms of sort of creating strategies based on collecting that state. I think the best indexers are ones that you make yourself and they're purpose built and you're traversing across these different data types to create sort of your perfect arrangement of data. So I built one on top of Ave a while ago and that can be was like relatively tricky.
00:44:09.654 - 00:44:32.194, Speaker A: So they have an EIP 1967 pattern. We're accessing storage to find that contract. That contract is a pointer to other contracts. I'm doing an ETH call to access the variables to find those other contracts. And then I'm calling balance to find ETH balance of those contracts. I'm also utilizing storage. Yeah, stuff like that.
00:44:32.194 - 00:44:56.506, Speaker A: And then you just have a really neat one row item in your database for what you want. Sorry. Yes. Do you mean deploying the same contract on two different chains or changing the.
00:44:56.528 - 00:45:00.170, Speaker B: Code redeploying or deploying?
00:45:04.610 - 00:45:43.094, Speaker A: Okay, so yeah, that is actually important to note. So if somebody destructs a contract yes. Okay. If somebody self destructs a contract, I'll just repeat the question. If somebody self destructs a contract, I believe it's possible for another contract to appear on that identical address and that can have different code, so it can potentially break your indexer. What you can do there to solve that system is you can have a microservice looking for destruction transactions and matching them up, doing a query in your database to see if that's affecting one of your indexers, and then making changes to that indexer live. So I'll normally do that through a microservice architecture.
00:45:43.094 - 00:46:13.620, Speaker A: So I'll have one looking for destructs. If a destructs happen, send a message, adjust or cancel my indexer for a period. The great thing about the blockchain is the data is always there. So even if you miss a few logs, you can do that historical query and rebuild your data set. Yes. When you write all the code by yourself. Yeah, sure.
00:46:13.620 - 00:47:08.726, Speaker A: The advantages are sort of okay, so specifically to the graph, I would say is that there isn't a lot of functionality, I believe, but please do correct me if I'm wrong to do queries across different subgraphs, at least currently. Okay, that was definitely one use case when I built them. Okay. So number one, I would say in general is latency. You don't have to wait on any graph node operators to fill up the graph for you. Second of all, I can create a graph that is a data set that's infinitely more complex than what I could do. I mean, I can theoretically do anything on the graph, but here I have very close knit control of exactly what my code is doing and what I'm putting through sort of.
00:47:08.726 - 00:47:40.170, Speaker A: Third, I have control over my own infrastructure, so I'm not dependent. The graph is decentralized and a fantastic protocol, but having your own data set with your own database that you know exactly what's happening on is extremely useful. Fourth, you don't have to pay it's free. Yeah, I guess that's like some of them. But really my main pitch for it is that this isn't that much code. It's not expensive to run. It's mainly really just doing Http calls.
00:47:40.170 - 00:48:02.600, Speaker A: You can run this for free on any cloud provider, on the Et micro or whatever the AWS equivalent is. You can run it on the free tier. Superbase has free postgres databases. You're running your own infrastructure, creating your own DAP, and it has no cost to you whatsoever. Yeah. Anyone else? Yes.
00:48:06.490 - 00:48:20.750, Speaker B: Transaction do you handle? Yeah. Statement.
00:48:21.890 - 00:49:22.602, Speaker A: So, yes, there is a way to handle this, and you can subscribe new. So there's a function called subscribe New Head inside the Geth client, and every single time there's a new block header or an uncle, you'll get a message there. So you can make a service where you're looking for uncles. It'll give you the information you need, and then you can look at what that uncle did and it should produce new events and it should tell you like, the removed tag for the event, for the log. So when your block hash gets uncled, like, let's just assume that you detect it properly through subscribe new head, you can re request that block through the block hash, and then it'll give you, inside the log field, a removed field, which is a ball to tell you if that log has been deleted or not. And if it has been deleted, then you can mark that in your database, you can delete the row, or you could it depends on your private key. Sorry, not private key, your primary key setup.
00:49:22.602 - 00:49:38.200, Speaker A: You can delete it or change that variable to true that it has been removed to mark that it's not valuable data. Okay, looks like we might throw Ravi here instead. Yeah. Anyone else? Yes.
00:49:40.730 - 00:49:42.390, Speaker B: Large number of events.
00:49:44.890 - 00:50:23.854, Speaker A: Yeah. Sort of hardware that you need. So I'll say like for a couple of different components from a database point of view. Absolutely. It depends what sort of service you're running, how quickly you want to retrieve that data. But in terms of storage, I don't know, I would say maybe 600, 700 gigs for the database. The more difficult component is if you want to do that, historically, for every single block number, you're going to run into hiccups with having nodes requesting that data from nodes.
00:50:23.854 - 00:50:59.166, Speaker A: So that's going to be very expensive if you're doing that through Alchemy or Infura or one of those node subscribers where you're paying per call. And if you're running your own node, I would say one isn't enough if you want it to finish like this century. So what I would do is run, whatever, 20 nodes, hook them all up through historical data so you can use snapshots now. So it's nice and easy to set that up and then have a load balancer and distribute out your calls into the different nodes. And that's probably the quickest way to do it. In terms of the actual indexing, pretty lightweight. I wouldn't really worry about it.
00:50:59.166 - 00:51:06.740, Speaker A: The way I normally run it is have a Kubernetes cluster and then just auto scale as I need it. Yes.
00:51:10.790 - 00:51:12.930, Speaker B: For long term performance.
00:51:16.090 - 00:52:01.134, Speaker A: Okay. It really depends what your application is doing, but what I would say is that for long term use, I would recommend a relational database system. So SQL, just because you can get a lot more use out of it. And if you're storing the data long term, you probably are imagining some use cases that you don't have yet. NoSQL, I would suggest for when you need really quick retrieval of that data. But in terms of sort of data strategy, a pretty common one that I've been successful with is time partitioning my database. For those who are not familiar partitioning is sort of imagine you have a CSV and you have a huge Excel file.
00:52:01.134 - 00:52:56.130, Speaker A: Now instead of having this huge Excel file that I have to search through, I break out the Excel file into like five different files and I define them by a range. And now I can just search through that range for a particular transaction. So there's lots of time partition databases, but I would time partition your data if that's relevant, and then create indexes on everything the regular blockchain does and also anything of direct interest to you. So just like a congregate index, so whatever you want, like block number plus the value of some variable plus like topic zero hash, for example. Also another good thing to do is to break apart into different tables your events. Yeah, I think that sort of goes for best retrieval, obviously. Also, if your database is getting huge, also separated out into different nodes, so you're not paying for vertical scaling.
00:52:56.130 - 00:53:41.154, Speaker A: The other one I didn't mention is graph databases. I haven't seen that much use for them. I haven't used them myself, sorry. But that's also an extremely powerful way to store your blockchain data. So I can see the relationships between different addresses or I can see the relationship between, for example, like Oracles and the RV market. Yes, sync for indexing. So the different channels, I mean it's paralyzation, but essentially I don't have to wait for one request to get through before I do the other one.
00:53:41.154 - 00:54:22.654, Speaker A: So it's very useful. So I'd say if you're doing the subscribe filter logs method when you're getting one event at a time, you're probably absolutely fine without using a routine. But if I'm doing the filter log method and I'm requesting a huge amount of logs at the same time, and I have additional calls to do on each of those. That's where paralyzation will really help you out. You'll be able to make thousands, millions of calls at the same time instead of one at a time. You're going to be cutting down the amount of time you need to index from whatever weeks to hours. Yeah, you can insert more than one row at the same time.
00:54:22.654 - 00:54:40.760, Speaker A: I could run multiple index programs, for example, from like, block range zero to 100, 100 to 200, 300 to 400, and they can all insert separately. So I'm finishing that index far quicker than if I had to go from zero to 400 on one routine or one program. Yes.
00:54:45.850 - 00:54:48.150, Speaker B: Just because of how the structure.
00:54:53.070 - 00:55:22.020, Speaker A: Yeah, I mean, look, node providers are fantastic and highly professional, so you're going to get amazing service and you're not going to really experience much delays there. But you can set up your node in GCP and you should not really have too much more of a delay. The bigger risk with running your own node is if you make a mistake or you're missing a geth update. A lot of the manual stuff you have to do around managing your node is automatically done by node providers, while you have to take care of those operations yourself.
00:55:26.070 - 00:55:31.910, Speaker B: I have my own node running and I see some especially around the mempool data.
00:55:32.060 - 00:56:05.920, Speaker A: Okay. Mempool data? Yes. Are you trying to do like, Arbitrage or something similar? Okay. One thing that you can do is that you can run multiple nodes in different regions because the mem pool is not necessarily synced between all nodes at the same time. So a popular setup is to run three or four nodes in different regions and they'll have different peers. And then you can congregate that data yourself through another process, and that way you'll get a much more complete view of the mem pool at like, block execution time. Yes.
00:56:11.410 - 00:56:12.400, Speaker B: This is.
00:56:20.490 - 00:56:34.850, Speaker A: Sorry, I have like five more minutes. Anyone else? No? Okay, fantastic. Yeah, thank you for listening. Here are my details. Thank you. Bye.
