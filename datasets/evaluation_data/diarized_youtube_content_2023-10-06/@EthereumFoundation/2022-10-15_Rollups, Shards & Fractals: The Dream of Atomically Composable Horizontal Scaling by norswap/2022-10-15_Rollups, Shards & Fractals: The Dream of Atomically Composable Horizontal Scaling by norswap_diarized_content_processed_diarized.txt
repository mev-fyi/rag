00:00:14.330 - 00:00:44.618, Speaker A: The titles are doozy. It's very long. But what we're going to be talking about essentially is scaling. And there's some lag, but basically there's two kinds of scale vertical scaling, which is when you have a huge machine, and horizontal scaling when you spread the load between multiple machines. So yeah, scaling. So basically, I mean, I can wing it without the slides by the time they come by. So in this talk, well, goals, great.
00:00:44.618 - 00:01:15.266, Speaker A: I want to contextualize vertical and horizontal scaling. I want to show you a cool new idea I have for a Sharding scheme which has atomic cross shard transactions. And we're going to explain exactly what that means. But more importantly, what I want to do is demonstrate some research process. So like, hey, can we do this? There's a problem, there's a solution. Maybe it's at the Dan that we need to go in our direction. So can I restrict that process? And generally what I want is to foster interest in this kind of research.
00:01:15.266 - 00:01:35.466, Speaker A: And I want people to collaborate with and to work with me and with us at Optimism on this stuff because this is the kind of stuff we care about. This is not something that's really on the roadmap. It's a complete side quest. And so, like I said, this is a nerd snipe. Like this is supposed to be brain candy. Super interesting stuff. But if you want to work on it, you should talk to me.
00:01:35.466 - 00:02:36.510, Speaker A: So like I said, vertical scaling, big machines, horizontal scaling, many machines spread the load. And in terms of ethereum, vertical scaling has been done by roll ups, right? Roll up, enable vertical scaling. Because if you're a validator, you can validate the roll up by validating the ethereum main chain. There's only one extra assumption, is that there is one honest L two validators that can also supply data for the roll up. But the question is, okay, we can scale vertically and we can probably scale ethereum ten times, maybe 100 times if we start pushing some buttons like scaled database and things like that. What if we need extra scalability and there's multiple reasons we might want that. So one is that sure, we could require a data center for L two sequencers, but we want to have a healthy validator network and not everybody can get data center, not even every company or every Dow.
00:02:36.510 - 00:03:24.878, Speaker A: So we might want to spread the load so that you can validate a subset really of the chain. Our thing is that we might want many different roll ups. This might be roll ups with different security assumptions and different parameters, like different fee, different pay tokens in a different pay fees in a different token, increase the trow output, change the vitality rules, maybe use a different VM, all that stuff you might want to do. But you might still want to have multiple roll ups talking to one another. Okay, so there's two main approaches to horizontal cyclobility. One is parallelization and in that approach you're going to have a big blockchain, lots of state and you're going to spread the road between machines. Two options to do that optimistic, you just try to do things in parallel and you hope they don't touch the same state.
00:03:24.878 - 00:04:14.450, Speaker A: If they do, you just throw away one of them and you start again. If you do this, the pitfall is that you can't increase the throughput because basically you can't charge more for transactions that are not parallelizable. That's because you don't have a clear criteria of what is paralyzable and what is not. Our option is to use Strict access list. Some fancy people call this UTXO, but I think Strict Access list is clearer. And so basically that's shipping every transaction with a list of either all the contract that it touches so you can have a lock on the contract or all the storage slots that it touches so you can lock these specific storage slots. The paratization approach stills assume that you have one entity that validates everything just now it's spreading things across cores and across machines.
00:04:14.450 - 00:04:53.270, Speaker A: So it's still a high cost on Validators. If you want to reduce the load on Validators you may want to do sharding and then Validators can validate a single shard. So a shard is basically a small sub blockchain. The annoying thing with that is that apps now need to think about on which shard they want to deploy. Which might not be great, but the great thing is that we can do heterogeneous roll ups. So like I said, you can customize roll ups to have different VMs, different fee models and things like that and they can all talk to each other. The problem with sharding is that if there is no way for shards to talk to one another in a way that is effective, this is a bad solution.
00:04:53.270 - 00:05:27.898, Speaker A: So I'll explain what an effective way means. But as you guessed it from title, this is about atomic composability. So this is an example I would use for a cross shard transaction. It's a transaction that swaps bitcoin for ether on shard A, then bridges the ether to a shard B and on shard B will buy NFT for ether. So this is specified in a single transaction that you send either to a system or to shard A depending on your architecture. So one very desirable property is atomicity. If any part of this reverts, everything reverts.
00:05:27.898 - 00:06:35.050, Speaker A: And in this case in particular, if you can't buy your NFT for either, then you also don't want to swap your bitcoin for either. Sometimes you can achieve this via application level atomicity, but in this case it doesn't work because sure, if you can't buy your NFT for either, you could swap back your ether for bitcoin but now you've paid two swap fees and you've been exposed to bitcoin either volatility. But there are sometimes cases where you can revert the action on chart A and if you can do that then you can do okay, I was standing in the wrong area. So in some case you can do application level automaticity if you can roll back the thing on chart A. So what do we have today? Today we don't have atomic composability but between pretty much any pair of chains you can do eventual delivery of messages. Okay? So imagine you have a chain A, chain B you can use systems like layer zero. I think wormhole does this, nomad to exchange messages between chains.
00:06:35.050 - 00:07:15.254, Speaker A: And what you have is that the guarantee that if you do the action on chain A, eventually you'll do the action on chain B. The action on chain B can fail and there's nothing you can do about reversing the action on chain A. That's what we have today. That's implemented ideally with late clans and ZK proofs, but maybe also a good old multi SIG that happens. So getting to the fractal part of the talk, we can do a little bit better with roll ups and especially with ZK rollups we can at least ensure a bound on this, right? This was eventual delivery. Could mean if I go back to previous slide, could mean many blocks. Sorry, didn't mean to do that.
00:07:15.254 - 00:07:46.740, Speaker A: You could go back like many blocks in the future go forward many blocks. What you could do is say, well, we're going to do this on the next block. Okay? One scheme to do this uses ZK roll ups. And so basically you have this architecture where you have a parent roll up here ZK roll up one, it has child roll ups. ZK roll up eleven and twelve. And basically it's a fractal structure because the roll up one rolls up to the layer one. But the layer eleven and twelve, they roll up to the roll up one.
00:07:46.740 - 00:08:48.174, Speaker A: And if you assume that there is a ZK proof posted every single block then now what you can do is do instant delivery transaction between say this 111 and one. So for instance you could do your swap here and then buy your NFT here. It's still not atomic because you can't revert but now at least you know it's going to be done like in the next block or one block apart. That's good, but that's still not what we want. So what about cross shard atomicity? So I'm going to propose a solution for this and we're going to assume that we are slicing our system into one blockchain block which is going to comprise one block for every chart. So every shard advanced at the same rate and we want to have some form of atomic crosshard transactions to enable this shard need to be able to send messages to other shards. So let's just take the most naive possible idea which is eager interchard blocking.
00:08:48.174 - 00:09:32.530, Speaker A: So you just process all your transaction. At some point you hit onto a transaction I want to send a message to another chart. You just literally call that chart, you wait for the answer and then you continue. So this is less crazy than it sounds and it's somewhat equivalent to what strict access lists do because in the worst case you have the same throughput as a synchronous blockchain in theory, but you can charge fees because you know exactly which transactions are crossing the shards. The great thing is you don't need to specify the access list. But the really, really bad thing in practice, not in theory, is that you're assuming that all these shards are being validated by different entities. And so the latency for one shard to talk to another is really high.
00:09:32.530 - 00:10:05.054, Speaker A: And so in reality this is not feasible because the latency will just kill any throughput that you might have. Let's try to instead reduce the number of exchanges between shards. Let's try to bound this very tightly. And so the idea would be to divide the block time into multiple slots. In the first slot every shard executes its own transactions and also collects messages to send to other shards. In the second slot all the shards exchange the messages and then they run the messages that they receive. Optionally you can keep this going.
00:10:05.054 - 00:10:31.842, Speaker A: So you can have many, many slots. You could say, well, messages can send other messages, et cetera, et cetera. I think in practice you don't really need this. You can have a lot of expressivity with just like one hop, just one shard talking to another shard. So this is still bounded message delivery though you can't revert, you can't revert part A if part B fails. And that's because other transactions rely on the result of the A part. Okay, so this is very unclear.
00:10:31.842 - 00:11:00.800, Speaker A: So there's a graphic here. So say I'm chain A, I'm doing my local transactions, I'm doing the A part of a cross shard transaction and let's say that's a swap. So both of these will be swapping and then on B they will be buying an NFT. So this transaction here, the A part depends on the A part of the first one because the first one swapped save bitcoin for either. So it moved the price. So this one will use the price that was moved here. So there is a dependency link.
00:11:00.800 - 00:11:57.598, Speaker A: And because of this dependency link you can just revert this transaction if this one fails because this one uses the result. So this is the problem that we're dealing with. And the general property is that atomicity requires synchronicity. Basically local transaction can still be processed separately but we would like all the crosschain transaction to be executed as though it was a single person executing them. Right? So we have a problem which is we don't want chart to talk to each other too much. Solution to that is to create a special shard and we're going to call it the atomic shard that will execute all the cross shard transactions. And we have a second problem, is that to execute all the cross shard transaction, you need the state from all the shards, right? And this removes one of the big points of sharding, which is that you can separate the state between all the shards.
00:11:57.598 - 00:12:38.314, Speaker A: So the solution to that is to make the execution in the atomic shard stateless. And that mean you will supply all the state that's being accessed by the transactions for the atomic shard. So this sounds too good. So where's the poop? The Poop is that you need to simulate transactions on each shard individually. So in general, in the VM, given that you're not sure the state, it's impossible to collect all the storage slots accessed by a transaction. You can approximate. And the easiest way to approximate is just to run a transaction against the current state and just assume that the transaction before it will not revert.
00:12:38.314 - 00:13:05.830, Speaker A: So here it's easy. You just run the state of the first transaction against the state after all the local transactions for this one, you just assume this one will revert and execute and simulate. But maybe it will revert. So maybe your simulation will be incorrect. Everything you can do is ending. So you just tell the shard, well, this transaction is going to touch this slot. Or you can give him some logic to find all the slots that are being touched.
00:13:05.830 - 00:13:37.410, Speaker A: And there's many ways to do this that can be explored. It could just be some EVM code that just touches the slots. It could be some kind of in protocol information or even out of protocol information that is not being validated, but that's being sent to the sequencers. So I want to make things a little bit more clear. So here is a cross shard transaction, very abstract. It computes things on some state and this state might change, right? We're not exactly sure what it is. Then it's going to send a message to Shard B and this message is going to depend on the result of the computation.
00:13:37.410 - 00:14:09.820, Speaker A: That message will have a result. We're going to compute something based on that result and we're also going to send the result to Shard C and then do some more computation. So this shows you sort of the dependency we have, right? We can depend on the results from one shard. We can send to a shard things that have been computed. And so this will be important to understand later. So to rephrase what I'm proposing before we dig deeper into the problems, like I've showed you, only a small part of the Poop. A lot more Poop coming up.
00:14:09.820 - 00:15:19.250, Speaker A: But the scheme is this. Phase one, the shards execute the local transaction and they simulate the cross shard transaction. When they do this, they will collect all the cross shard messages and all the access storage slots. In phase two, we do the exchange and the shards simulate the messages and similarly they collect all the storage slots in phase three, all this in phase three, all this stuff that we collected here and there, we ship it to the atomic shard and this will automatically execute everything. And normally it should have all the state it needs and if it doesn't, it will just basically fail to execute transactions. So the big problem with this scheme is that the simulation restrict the expressivity, so we can only safely express transaction where the simulation will be deterministic in some ways because otherwise the state might change and you might not be able to execute things. So you might sometimes take the risk that a transaction won't work, say, well I'm going to try, if this says doesn't change, it's going to succeed, but if the state changes it's not going to succeed.
00:15:19.250 - 00:15:56.286, Speaker A: We'll see that sometimes that doesn't work. By the way, this is exactly the same problem as building strict access list or UTXO for slots. Same thing you need to collect all the storage slots that have been accessed. So this problem is pretty similar. So the problem when it gets worse is that crossroad messages may depend on uncertain state and we need to derive all the messages during simulation. And that's why we want to be deterministic. We can also want an answer from our Shard and so we absolutely need to have a hint for the answer.
00:15:56.286 - 00:16:35.478, Speaker A: So let me illustrate because this is very abstract. So what I was saying was that this parameter here, x, depends on the result of computation. So we need this to be always the same, otherwise we're just asking the B sharp to compute something that might not be what we want. So we absolutely need x to be known and you know, that the message sends an answer and we're going to send this answer actually directly to C. So we absolutely need to know the answer already. Also I've actually illustrated this constraint. So here we need the computation to be deterministic in a storage class that accesses here, we actually need the computation to be deterministic.
00:16:35.478 - 00:17:53.970, Speaker A: So to always return the same result here, if we use Y just in a computation, we need to hint the result approximately just to get the correct rational. But if we send it to another Shard, then we need to know exactly what Y is like. No guessing, otherwise we're just asking C to do some bullshit and we don't want that. So open questions, is this reasonable? Is this a good idea? Are these restriction feasible? Do they give us a powerful model that's useful in practice? Another question is can we statically guard against some cases of the nondeterministic execution that we want to avoid? Or do we just say, well, it's a user responsibility and maybe some people will build some tools to detect some of these cases. This is for sure a footgun, right? Like if you're going to make a crosshair transaction and you don't expect it. You expect it to be deterministic. But isn't there's? Definitely there are a lot of possibility to make bugs, but is it worth the cost? Our question is, what is the correct abstraction level for all this? If we don't enshrine static checks, we can simply add, like, a cross short call upcode to the EVM and then just on the back end, it will do what I explained, but from the point of view of the EVM, there's just a new upcode.
00:17:53.970 - 00:18:05.110, Speaker A: This has been my presentation. We have five minutes for questions. I'm sure some of you have burning questions. You should have a lot of questions because this is very questionable.
00:18:06.090 - 00:18:19.100, Speaker B: Yeah. Do you think Sharding is maybe just fundamentally kind of impossible and kind of like when you're trying to do you have all these different systems, it's almost like they shouldn't be able to interact in this way. It's a really dumb question.
00:18:21.070 - 00:18:40.498, Speaker A: In a certain way, you're right. The ideal idea is, like, you can paralyze what you can, but then sometimes you don't want to paralyze, and you're just trying to build this ebrid, like asynchronous synchronous system. So yes and no. Someone else someone comes with some wacky idea. There's no dumb question. Yeah.
00:18:40.584 - 00:19:11.550, Speaker C: In your model regarding the new Opcode that is aware of sending message cross Sharding, does that mean, like, smart contracts need to be aware of calling another smart contract that is in another Shard? How do we envision those things? And also in Sharding, without the new Hope code, are smart contracts being aware of calling something that is in another chart? Because that brings friction to smart contract developers.
00:19:12.450 - 00:19:41.218, Speaker A: Yeah. The model I've been assuming is the model of Sharding as, like, small sub blockchains. There are some proposals of Sharding that I would classify under parallelizations. You just pretend you have a big blockchain, and then on the back end, you just put things in Shard and you move things around. But for the user, they don't see that. Here the model is explicitly you have multiple sub blockchains, and they talk to each other. This, I think, simplifies the implementation a lot, but not everybody agrees.
00:19:41.218 - 00:19:58.010, Speaker A: Actually, my good colleague Car does not entirely agree with that. But that's at least my point of view. And there are other advantages, which is that you can have multiple roll ups that do things slightly differently, as I mentioned. So that's an advantage.
00:19:59.230 - 00:20:20.580, Speaker D: Okay. So regarding the Sharding in itself, is it possible, for instance, to imagine a system where the accounts would dynamically move across the Shard to try to reduce as much as can be the inter Shard exchanges and so to simplify the whole process, or is it, like, completely delayed use?
00:20:21.190 - 00:20:53.686, Speaker A: It is possible. I know there's research on this. I don't really know anything about it, but yeah, I think this was more related indeed to the models where you would just rebalance the Shard and you would necessarily be aware of the shards. Yeah, it exists. It's definitely feasible. It wasn't really thought through here in this simple model. Yeah, I think this sort of needs some balance chain where you have all your coins that will probably or at least have your balances be accessible by every shard.
00:20:53.686 - 00:21:06.100, Speaker A: That would support a simplified design a lot. And in practice, when we see things like Avalanche, they have a special chain for payments because it's so special and so important. So that might be one part of the answer.
00:21:08.150 - 00:21:26.790, Speaker E: Hey, my other question was, could we use things such as, like, AI or sort of predictive behavior in combination with this to sort of just automatically optimize? Or is somebody working on stuff like that to sort of predict the interactions between the Smart Contracts and how to kind of optimize across charts?
00:21:28.490 - 00:21:46.958, Speaker A: I'm sure some people are working on it. I know there's a bunch of research with that, even on payment chain. So things like Bitcoin and stuff. I don't know why. It was very hot academic. Like, academics are, like, lagging behind the stuff we do a lot. And there's been I've been to a workshop this year, and they were very interested in exactly this problem.
00:21:46.958 - 00:22:01.920, Speaker A: And I'm like, does this change the smart contract? And we're like, no. I'm like, what's even the point? Like, just put it on a big server and be done with it. But, yeah, you could do it. I sort of didn't assume that model here, but it's doable. Okay, thank you very much.
