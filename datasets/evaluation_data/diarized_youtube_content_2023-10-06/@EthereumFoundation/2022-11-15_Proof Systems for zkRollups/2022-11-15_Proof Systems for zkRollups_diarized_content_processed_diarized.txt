00:00:12.170 - 00:01:02.234, Speaker A: So today there are three parts. In the first part I will talk about some concepts for modularized proof system and in the second part I will talk about how we are choosing proof system for our decade RoPS. And finally I will conclude with some your practical considerations when you are choosing this proof system. So now let's start with the modularized proof system. So when you want to generate proof for some computation, the first thing you need to do is that you need to do some optimization. So basically normally people call that circuit, but more formal word is called constraint system and there are many different ways for this circuit organization. To give you some example, for example in R one CS, so assume that you have a huge circuit and you put all the wires as variables and you lay out that as a vector like from should be w one to wn and then you lay out all the witness as a vector.
00:01:02.234 - 00:01:39.898, Speaker A: So the form of RNCs is that you can define the linear combination of the vectors of your witness times a linear combination equal to a linear combination. So that's a form for one constraints in RNCs. So for example, you can access any of those witness cells like using this linear combination. But the limitation is that you can only have one multiplication. So it's very good for some programs. For example if you have multiple large linear combinations but with a few modification. And so this is one example if you want to access cell like omega one, omega two and omega m minus two.
00:01:39.898 - 00:02:19.234, Speaker A: And another example is that you can basically access any weakness cells using this RNCs. So this is like the form of RNCs. Another commonly used optimization is called punctuation. Basically what you can do is that you are not layouting all your witness as a vector but instead you lay out your witness in a table. Like for example, you have three rows and then what? You can define that you can define something which is more flexible. Like for example, you can define degree three constraints. Like for example, you access some sales and then you define those as a very specialized gates or specialized custom constraints.
00:02:19.234 - 00:02:56.590, Speaker A: And another thing you can define is that you can define some membership relationship. For example, this tuple belongs to two columns in the table. So using this you can do like range proof really efficiently. For example, you can have a fifth column and you believe that you prove that this element belong to that table. So that's another thing you can define. And the third thing you can define is some permutation like you define like some cells are equal and so why they say it's useful? Because that in the first custom gate when you are defining after defining multiple gates, you need to connect them together. So you need this permutation to for example define the first gates output equal to the second base gates input.
00:02:56.590 - 00:03:31.022, Speaker A: So those are three constraints you can define in a plunkish optimization. So it's custom gate lookup and permutation. So from the modulus perspective, there are three commonly used front end, one is RNCs as I mentioned, which relies on linear combination and second is Plunkish which relies on custom gates, lookup and permutation. And third is Air which is usually in Stark. It basically contains translation constraint which is very similar to custom gates but only access two rows. And also some boundary constraints for defining your starting point. So that's the front end for a proof system.
00:03:31.022 - 00:04:26.042, Speaker A: And after you get this constraint system, when you need to get a really implementable proving system like in practice, you also need to pass it through an informational theoretical compiler. So what does this mean? So this compiler will define some interaction model between a provera and a WireFire. So in this information theoretical compiler you only define how the provera and the WiFi are interacting and assuming that there is some oracle where you can for example provera send a message to some oracle and then verifier can later query from this oracle. So there is nothing like concrete for example what kind of commitment you are using. So you just send some message to the oracle oracle and the verifier can query. So that's what happened in this information theoretical compiler. And so this is an example for interactive oracle proof and the short form is IOP and another example which is commonly used is called polynomial IOP which the message is in the form of a polynomial.
00:04:26.042 - 00:05:28.990, Speaker A: Like for example you use a p one x to represent your message and then what the verifier can do is that during the query phase stage it can query at a random point to get this evaluation and you also do some interaction between the approver and the verifier. You can do multiple rounds. For example, there is constant round poly IOP which for example Planck is using that like Marlene is using that a lot of proof system with verification are using this constant round polynomial IOP. And then after you get this IOP to make that really practical, you also need some cryptographic compiler to compile down to an argument which can be implemented in practice. So I will give you an example. In this polynomial IOP like model, one precryptographic compiler you can do with that because it's interactive and you need verifier to send a challenge every time to the approver. So what you can do is that you can initiate this challenge model using some fierce xamia and you just hash the transcript at the next round challenge.
00:05:28.990 - 00:06:07.302, Speaker A: So that's one thing you can do. And another thing is that because there is still like magical oracle there where you are sending this polynomial to and verify can query from, right? So you need to initiate what's happening here. So you initiate the concrete polynomial commit scheme to replace this oracle. So for example, there is Kiliji, there is Frybase, there is Dory, there is dark, there is many polynomial committee scheme. So you just use those schemes to commit to a polynomial and later open at a random point. So after those stages you finally get your protocol. So just as a summary for commonly used the proof system, you have front end and you have backend.
00:06:07.302 - 00:07:16.246, Speaker A: So on the front end you have RNCs, Plunkish and Air and on the back end you usually use polynomial IOP plus a polynomial communication scheme. And a quick summary for the advantage between those different front end. So RNCs is really good for linear combination like why I'm saying this, I'm specifically talking about growth 16 because for example in a Martini or Spartan or some other back end which also supports RNCs, it's totally different because the complexity actually lies on the sparsity for your matrix. So which is a different for evaluating the efficiency and it's also more general because each constraint can access any witness cell like you do linear combination. For that you don't need permutation because all the witness have already been layout to a vector and the plankish and air stuff is more useful for some uniform circuits. So uniform circuits, basically you have repeated structure and you can define one custom case for those repeated structure you just need to increase the length of your execution trace and to really efficiently prove that and it's also more customized. For example, you can have lookup, you can have variable components, so that's a front end difference.
00:07:16.246 - 00:08:46.030, Speaker A: And on the back end, what really influence your concrete property is that the polynomial commission scheme you are using for example it will influence your trusty setup whether you will have that and some security assumption and I will give some example later and also influence your concrete proof efficiency because like for example if you are operating over a group operation it's less efficient than some scale operation and also proof size and verifier efficiency and some commonly used polynomial commencement game is KDG Fry based and some inner product argument which is derived from bulletproof and another called multilinear PCs. So for KDG, if you initiate your proof system with KDG, then you have universal setup, you have DELOG security assumption and the prowr is relatively slow but it's easy to parallel because it's mostly doing some multiscalar multiplication and verifier only need to do pairing and the proof size is really small. And for Frybase you don't need trusted setup and it's based on Hush. You're using Merkle tree and the resolvement code to commit to a polynomial. But the tricky thing here is that when you are really using Fry in practice you also have some other parameter choice which will influence your practical security assumption and Prora is mostly doing Hashes and FFTs and Verifier is doing Hashes and it has really large proof. But there are some improvements from for example plunky tool which can make the proof really easy to do recursion. And for the inner product argument, it doesn't have setup because it's derived from blur proof and it has DELOG assumption and proof of verify are both doing scalar modifications.
00:08:46.030 - 00:09:35.906, Speaker A: And you can use some technique like passac curve where you have two cycles to make that easy to recurse and you have like a middle size proof. And those are three polynomial commitments which are commonly used for committing to a univiral polynomial. Like it only have one variable, but the degree might be high. And another interesting direction which at least blockchain people or like industry people haven't really looked into is called multilinear polynomial commitment scheme. So it's usually commonly used in some check based constructions. So some check is basically so this model linear polynomial commitment scheme is basically you have a polynomial but you have unvariables and you need to commit to this invariable polynomial and then open at unramployment points and you can do some interaction to really reduce that to just one opening. So this is very useful for many schemes which I will introduce later.
00:09:35.906 - 00:10:41.382, Speaker A: And so just to decompose some commonly used protocols, for example, people who use Plunkish optimization, Plunk IOP and use IPI as polynomial commission scheme or the community version is using KDG. And so basically when you are describing a concrete protocol, you can divide that into three parts what kind of optimization you are using, what kind of polynomial commission scheme you are using, what kind of LP or the information theoretical model you are underlined and Stark is UNR and Stark LP and some fry as this Ponoma commission scheme. And unfortunately, growth 16 is not falling to this gate, not falling to this category because it's basically based on some linear PCP which you encode some trapdoor in your trusty setup and do the query there. So it's a very special case, it doesn't fall into this PCs modular diagram. And there are some new protocols as I mentioned, which is based on multilinear panomic committee scheme which is showing more and more potential. One is Spartan which use R one CS as the front end. It has its own IOP to handle this R one CS and the polynomial committee is IPA.
00:10:41.382 - 00:11:28.274, Speaker A: So the good thing for Spartan is that the proverb is only doing one large motor exponentiation. So you can use GPU to really make that faster because it doesn't need FFT, it doesn't need some other operations, so it's highly pilotable. And there is another new scheme called Breakdown, it's also Rncf based and it's derived from linear time encodeable code which makes your poor time complexity becomes linear. So it's linear to the scalar operation, it's not to group operation. And also another advantage of breakdown is that you can use arbitrary field for your witness. So one thing like even if you are using Fry based one, you are only based on merkle tree and hash because you use resolvement code because you need to do FFTs a lot. So you have to choose a field which has a large tool audicity to doing those FFTs.
00:11:28.274 - 00:11:54.202, Speaker A: But breakdown can using breakdown you don't even need to limit your field to be FFT friendly. And there is hyperplunk, which is from Espresso. And they do this. Plunkage optimization and hyperplunk LP. And you can use KDG or pride derived, like, multilinear polynomial communication scheme, which has some potential. And there is also Nova. And so for Nova, you can't actually really fit into this IOP diagram.
00:11:54.202 - 00:12:23.314, Speaker A: It's RNCs based. It's really good for doing recursion. And when you are doing repeated computation, you can use Nova to do recursive really easily. Has some nice property there. But unfortunately, as you say, those multilinear Panama commission scheme most only support RNCs frontend. That's why Hyperplant brings you this interesting property where you can define something more customized. Now after talking about the Proof system let's take a look at what we are using in Zikirap.
00:12:23.314 - 00:13:19.094, Speaker A: So the idea behind the Kirap is that you send all the transactions to one L2 prover and this L2 prover will generate proof and submit the proof on chain with some necessary data for verifying this proof. And so the proof system really matters in Ezekiel, right? Because your prover time, your prover cost, your proof size all influence the money you are spending on each transaction and also verification cost which influence your guess you are spending. So now let's think about what proof system we should use for such general purpose vikirap so as I mentioned you first need to know what you are really proving so what computation you are executing so for general purpose like Vikiroap you are actually executing the EVM execution logic. So you need to think of EVM as some type of computation, and you need to prove that. So to constrain this EVM virtual machine, you have multiple limitations. For example, your EVM word size is 256. Unless you use some ring based structure.
00:13:19.094 - 00:13:52.742, Speaker A: It will always be the non native field, so you need some efficient ring proof. And there are some ZK unfriendly opcodes, which means you need some specialized circuits, because if you put everything in the same circuit. It will be huge with a large requirement for the memory and for the machine you are using. So you need to decompose the circuit into different types and you need to efficiently connect different components and thirdly, that is a virtual machine circuit. So you need read and write efficiency. So you need some kind of efficient mapping from read and write. And also one last thing is that EVM is very different from static circuit you are using for some fixed program.
00:13:52.742 - 00:14:41.298, Speaker A: Because the Execution trace is dynamic. Because different transactions have different trace, right? It fill up this table even in the same position it might have different opcodes you are proving so that's why you might need some efficient offselectors. So the first three drive us to your circuit animation have to support lookup because you have efficient range proof there you can connect circuit there and also you can do this efficient mapping. And the last one drive you to some more uniform representation for a circuit. You are defining some IR in between for instructions and also select at the point you want. So that's the reason why we have to use plankish customization for Stark based because they can also support something more customized and now let's take a look at the Vic EVM we are actually using. So in Vic EVM we have two layers.
00:14:41.298 - 00:15:11.770, Speaker A: One layer is proving the EVM logic directly. For example, it contains EVM circuit to prove the state transition. It contains Ram circuit to prove the read and write consistency story circuit for state update and other circuits. For example Ecdss circuit for signature and some other circuits and they are directly used for proving the EVM itself. And then because you result in so many proofs, so you have an aggregation layer which can aggregate multiple proofs and into one proof. So when we are thinking of how we are choosing the proof system, it's actually two layer. So think about the requirements for the first layer.
00:15:11.770 - 00:15:50.070, Speaker A: The first layer really need to be expressive because you need to express really large circuit and so that's why you have to support custom gate and lookup lookup tables and also you have to use some hardware friendly prover to lower your prover cost because your input computation will be the largest one. You are directly handling EVM, not the verification. So that's why you need some hardware friendly prover. But I think hardware friendly there are two things. One is parallel ball and second is low peak memory. Because if you have low peak memory, then you can run very cheap machines. And also the verification circuit is relatively small because you get so many proofs and you need to aggregate that in the second layer.
00:15:50.070 - 00:16:29.942, Speaker A: And ideally, there should be transparent or universal setup because you might add some. New pre compiles or add some new circuits to this existing diagram, which makes the whole thing like if you are seeing, you have to do setup again. Again. And so there are some promising candidates. For example, planky two. Circuit and they are using Fry to do this really efficiently and there is a halo tool but for the Kilogram version it's not that promising because although the verification size is very small but it's on another field. So one potential is that you can still use the passacro for halo two, doing all the recursion for your lick EVM and contain a lot of computation.
00:16:29.942 - 00:17:20.742, Speaker A: And then use something like a bridge to bridge this halo two proverifier to your final verification. And there are some new IOPS without FFTs, because FFT, you need large memory and it's less parallel. For example, there is Hyperplunk, there are some new constructions which remove FFT in your PCs and so that's two promising candidates and also there are some check based protocol and by design it doesn't have FFT, it only involves some group operations. So for example Spartan, Virgo and all those constructions or Nova but unfortunately they only support RNCs. So if one day they can support punky stuff then it will be more easy to use. And in the second layer as I mentioned because the requirement is actually efficiently verification on EVM. So even if some WiFi is efficient, if it's not on EVM, then it doesn't make too much sense for the KWAP.
00:17:20.742 - 00:18:08.962, Speaker A: And also the secondly that you need to prove the verification circuit from the former layer efficiently. For example like if your former layer involves some non native stuff then your second layer is better to support some plunky stuff because you need some customized stuff to handle your verification circuit and ideally it's hardware friendly and ideally it's transparent or universal. I said ideally here, just because it's not a very harsh requirement because the largest computation had already been done by the first layer and then for the aggregation circuit you need to do smaller amount of computation. So that's why maybe not the big stuff. And so some promising candidates is that one is Grow 16, which I think Harm is already using in practice. And second choice is actually plank with very few columns. For example, we can configure that to be just one advice column, one lookup and one fixed column.
00:18:08.962 - 00:19:14.186, Speaker A: It can be even, sometimes even more efficient than Growth 16. Like you use KDG or you use F flunk proposed by the Aztec team, which the trade off is that using F flunk you might have more efficient verifier than Growth 16, but the trade off is that you triple your Pruning time. And also there is a catch up fry which with a large code rate and which you have a smaller verification circuit and you can basically do verification evenly. So this is like our construction, we choose the first layer to be hello two and second layer also hello two but the hash function uses is different because it has those nest properties. Since we are running a little bit out of time I will just skip this slide and so it's just basically good performance with GPU prover and the verification circuit is more since it's succinct and the second layer is that because you need to prove for the first layer like verification you need custom gate. There are some future work we are actually thinking of and want to have some spec on is that we want to generalize this framework a bit. So we believe that the front end, the end out goal of the front end will be Halo Two because it provides really flexibility for writing circuit.
00:19:14.186 - 00:19:53.222, Speaker A: Like you have different rotations, you have different layout configurations. So we want to generalize this framework to support using the same hello two front end but support different polynomial IOPS, for example, like harbor pollunk IOP. And for example, you can add Fry to the Halo Two. And also we need to significantly improve the hello two tooling support because I heard a lot of complaints from developers who are really using hello tool, there might be some DSL you need and the bug reporting is really poor and other any feedback from developers who are really using that. And one last slide, I will talk about some other considerations even besides the efficiency for proofread and WireFire. So there are some concrete considerations here. Firstly, your ecosystem.
00:19:53.222 - 00:20:30.838, Speaker A: So if you are using zero proof to develop your application, one thing you need to think about is that whether it's compatible with existing libraries. So if you for example, whether there are so many projects and Gadgets implemented there. Because for example, if you want to just build a simple application, if there are so many implement Gadgets, you can directly use that which can simplify your development process a lot. So that's what I mean by ecosystem. And second is implementation. So even if some new paper coming out with very nice complexity, you still need to think about what's the implementation and how long does that take. For example, like industry implementation is usually more solid and with better performance and better security considerations than the academic one.
00:20:30.838 - 00:21:01.180, Speaker A: And you also need to consider the best practice. Like for example, if you are running a benchmark, it shows really slow but if you know from the algorithm side that it's very easy to parallel using GPU kernel, then it's not a big deal. And also there is license and audit and so if we can really standardize the framework for the proof system we're using and we will have a very large community and even have ASIC specifically for making this kind of proof system faster and making that really great. So I think that's pretty much all I want to cover and thanks for having me here.
