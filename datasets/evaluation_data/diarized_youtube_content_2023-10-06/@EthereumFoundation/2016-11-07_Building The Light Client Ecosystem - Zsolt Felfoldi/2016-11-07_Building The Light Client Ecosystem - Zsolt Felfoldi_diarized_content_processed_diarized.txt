00:00:20.090 - 00:00:49.980, Speaker A: Hello everyone. I hope you're all really excited to hear the latest news about the light client. It. Well, the good news is that it's already in public beta testing stage. It's being prepared for the official release. If you would like to try it, I will show the necessary links at the end of my presentation. Yeah, if you haven't tried it yet, the first thing you will notice is that it syncs up really fast.
00:00:49.980 - 00:02:16.194, Speaker A: This is of course mainly because it doesn't have to process state transitions, just download and check headers processing headers is a lot faster than processing entire blocks. Our current implementation can process up to 5000 headers per second on a good desktop computer. To improve syncing even further, it is possible to start syncing from a trusted checkpoint, which is represented by the root hash of a merker tree containing all previous block hashes. It's called a canonical hash tree. This structure also allows to access old headers that hasn't been downloaded during initial sync, which could be useful for searching for old logs or accessing old transactions. Currently there is such a checkpoint hardcode into the client, but in the future if we can make these checkpoints or some equivalent information a part of the consensus, then it will be possible to obtain these checkpoints from the servers in a safe and trustless way. In addition to fast syncing, another important feature of the light client is that it has generally low resource requirements since everything can be fetched on demand.
00:02:16.194 - 00:02:51.170, Speaker A: The database basically acts like a cache. It can be kept really small. Memory requirements are also significantly lower than with the full node, mainly because we don't have to process entire state. This aspect of the get implementation can be improved even further still. It provides an RPC interface that is compatible with the existing full node interface. It's not perfect yet, but it can already work with mist. Hopefully tomorrow you will also see a nice mist demo using the light client by Alex.
00:02:51.170 - 00:04:16.590, Speaker A: Several people have already successfully tested it on smaller devices too. This picture is from Martin Brooks syncing with an intelladeson and this screenshot is from a raspberry PI running mist using the lite client, courtesy of John Garris, who by the way also donated a lite server to help the public testing and like other community members, provided a lot of useful feedback during testing. In addition to basic protocol functionality. Another important question is whether all of this can work in a large scale with good performance. The basic client strategy is simple. It always tries to have a few active server connections selected randomly from a suitable peer set whenever one of them seems slow or unresponsive, it drops it and looks for another one. Servers take care of themselves by limiting the bandwidth of clients and also dropping them if necessary, so they limit the time and resources spent on serving clients.
00:04:16.590 - 00:05:49.622, Speaker A: For limiting client bandwidth though, we needed some smarter mechanism than simply delaying request replies because that would ruin the user experience of the client, which depends heavily on quick server responses. This is why we created client side folk control, which is a simple feedback mechanism that can tell clients when they can send their next request, so that clients can better distribute their requests among the few server connections they have. If they send a request too early, they would get immediately disconnected. They shouldn't do that, but these trick tools have the advantage that requests never get queued up on the server side and therefore they can be answered immediately. This mechanism can ensure a good distribution of server load throughout the entire network, but we also need some market for assist to incentivize the running of good servers. In theory, micro payment is the ideal way to incentivize high quality service and responsible use of resources. But on the other hand, we should also take into consideration that requiring payment for all Les requests would seriously hinder the adaption of the protocol and also limit its usefulness.
00:05:49.622 - 00:06:52.698, Speaker A: You couldn't even sync up to make your first payment using a light scient. So another important question is whether it is possible to create an ecosystem where both free and paid services have their place and purpose. Fortunately, I believe the answer is yes. With a service like this, demand changes very rapidly, while the available server capacity changes relatively slowly. So if you want to provide high quality service, you have to have a lot of resource capacity and you usually get a low utilization ratio, which means of course that servers can sell the remaining capacity at a lower priority and a lower price. So basically our model is that clients are buying priority from the servers and on the lowest possible priority level. If the servers still have some free capacity, they can basically give it away for free, which of course still wouldn't ensure that they actually do this.
00:06:52.698 - 00:07:57.590, Speaker A: But we can create a service model when they will have an actual incentive to do so. Free service is a good indicator of reserve capacities, which are necessary for providing a high quality service that is actually worth paying for. So in our model, free service can act as an advertisement and also as a protection against the scam for clients. It can protect them from paying for and then getting no service in return. So the basic client strategy should be that even if you are willing to pay for services, if you find a new server through peer discovery, first you always evaluate it for free. Collect some statistics about availability and average delays, and then if the statistics are acceptable, then you can start paying for it. I wanted to talk about the new peer Discovery protocol we are working on.
00:07:57.590 - 00:09:11.440, Speaker A: Unfortunately, there's no time for many details. It's a new feature is an advertisement feature where nodes can advertise their capabilities. They can pick multiple category identifiers or so called topics and advertise them under these categories. And of course they can also look for nodes who advertise themselves under certain topics. Of course one of such topics will be light server. And finally, I would quickly like to talk about one of my future development plans, which could greatly enhance the performance and flexibility of the light protocol by allowing clients to run complex operations on the server side. In theory, basically srequest can provide any information a client needs, but if they want to evaluate something more complex, like a contract accessor function that accesses 8000 state entries, that would also mean a thousand consecutive alias requests, which would take a very long time.
00:09:11.440 - 00:10:59.386, Speaker A: Usually, evaluating complex data structures on server side could be orders of magnitude faster. And I don't only want to evaluate contract functions. I would like to create a universal virtual machine that can access anything, anything from the blockchain, including block headers, transactions, receipts, logs, everything, and allow clients to run any code in such a virtual machine on server side so that basically they can ask any question about the blockchain that a full node can possibly answer. Of course, if we are running code on the server side, we have to make sure that the clients can somehow know that they are getting the correct answer. And there are two possible approaches to achieve this, and I would like to make both of these options available for clients to choose according to their priorities. One of these approaches is that when the server runs a virtual machine code, it collects all the data it accesses, creates Merkel proofs for all of them, and returns this proof to the client so that client can, with one request and one reply, the client can rerun the entire function and have other data available. Another approach might be useful when processing larger amounts of data, and it's a more generalized off chain computing approach, very much like what Christophe was talking about yesterday.
00:10:59.386 - 00:12:46.872, Speaker A: Basically it's about server signing statements, saying that I guarantee that running this function with this blockchain as an input returns in this many clock cycles. With these results, the client can then ask multiple randomly selected servers to answer the same question, evaluate the same function. Hopefully all of them will return the same, and then the client can believe it. In the unlikely case when they return different results. Of course at least one of them will be false and the client should post the statements to a judge contract which will then request intermediate states of this VM execution from the signing parties until it finds the one single inspection that has been executed differently, and punish that one which has been lying by taking away a security deposit. Both of these approaches have their advantages and disadvantages, but whichever one the clients are choosing, this remote virtual machine execution will basically be the ultimate flexible Les request which can minimize any gap between the capabilities of full and light clients, which I believe will bring us closer to realizing our original vision we had with Ethereum. So we are at the end.
00:12:46.872 - 00:14:00.650, Speaker A: Thank you for your attention and as I promised, here are some links. There's a GitHub GitHub im ethereumliteclient and this is the main forum where you can follow the developments. You can ask questions, and whatever news I have, I always post it there. And there's also a wikipage with the instructions to try the current beta version, so please stay tuned for much development in the near future. Also, a lot of documentation is coming soon because another good news is that now both parity and C Plus plus wants to implement the light protocol, so of course we have to improve specifications better because so para I have concentrated mostly on code. Thank you. That was close.
