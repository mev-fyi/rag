00:00:03.750 - 00:01:21.650, Speaker A: Okay, so thanks everyone for joining this post London infrastructure call. Yeah, so I guess the goal here is mostly just to discuss, kind know what different people have seen since London has gone live, how we can adjust. I know that there's been a lot of conversations happening around how do we handle the new fee mechanism and how do we make sure that we're providing a good user experience to people. I guess the first hand agenda was like an overview of the upgrade. I mean, to just do this quickly, at least on the consensus level side, everything worked kind of as expected, so we don't expect to do any changes to kind of the core protocol shortly. Regarding the fee market, we didn't see anything kind of go wrong or go different than what we would have expected from the testnets and from just the simulations that we've done in the past. Yeah, I guess I'm curious to hear, just like from others on this call, what they've seen.
00:01:21.650 - 00:01:49.900, Speaker A: And it can be useful if you share something to just kind of share what product, or at the very least, what type of product you're working on. I think wallets have had a lot of interesting experiences, but yeah, just curious to hear kind of everybody what they've seen and if there's any issues that they think are important that we should bring up, I guess I'll go first.
00:01:51.150 - 00:02:22.870, Speaker B: My name is Austin Bunsen. I'm the co founder of Quicknode. We provide blockchain infrastructure to companies. We're running open Ethereum, and this is probably very unrelated to the London hard fork, but maybe, just in case I'm going to mention it, we are noticing that we're seeing a lot of dropped peers after upgrading to the version of open Ethereum that supports London. Again, probably unrelated, but just throwing it out there in case it is pertinent.
00:02:23.610 - 00:02:46.906, Speaker A: Okay, thanks. That's good to know. I'm not sure. What's this? So, I know open Ethereum was being deprecated basically after London. I assume they still have people looking at the repo help. Is there a specific issue actually that you've opened or anything yet, or that you've seen on the repo?
00:02:47.018 - 00:02:47.486, Speaker C: Not yet.
00:02:47.508 - 00:02:51.840, Speaker B: We've been toying with configs and trying to figure it out.
00:02:52.370 - 00:03:12.758, Speaker A: Okay, yeah. If you do not figure it out, I think if you can share just the issue in the all core devs chat, once you hide, that's really helpful. I know basically a lot of the open Ethereum team has migrated to Aragon, so the devs are still kind of working in the ecosystem, and that's probably something they should look at if it.
00:03:12.764 - 00:03:13.910, Speaker B: Needs to be fixed.
00:03:15.050 - 00:03:15.558, Speaker A: Awesome.
00:03:15.644 - 00:03:16.440, Speaker B: Thank you.
00:03:29.110 - 00:03:41.900, Speaker A: If no one has anything burning, I saw Barnaby, you were on the agenda as having some data to present about the upgrade so far. Is that right?
00:03:43.150 - 00:03:45.926, Speaker C: Yes. I've got a couple of slides.
00:03:46.118 - 00:03:49.260, Speaker A: Sure. Yeah. Do you want to go ahead and share that? Sure.
00:03:50.030 - 00:03:51.660, Speaker D: Let me share the screen.
00:04:00.290 - 00:04:01.600, Speaker C: Can you see it?
00:04:02.850 - 00:04:03.600, Speaker A: Yes.
00:04:04.370 - 00:04:47.310, Speaker C: All right, cool. Yeah. Disclaimer. It's been only a week, so it's really early impressions and I did not get as much time also as I wanted to dig into much more data. But yeah, I hope these impressions kind of maybe help frame some of the discussions after. So as a kind of look back to a previous conversation with Hai on the podcast, I said I would be looking at three things. The gas used kind of a dynamic to see when are we in full blocks? And maybe first price auctions kind of come back on the table.
00:04:47.310 - 00:05:43.440, Speaker C: The second thing is base fee. What does the, let's say, trace of the base fee look like? Is it smooth? Is it more like oscillatory? And then the last part is the oracles. And are they doing the job well enough? Like, are they tuned properly? So I'll try to say a couple of things on each point. On the gas used, I focused here on the longest trick that I found as of yesterday. I don't know if another NFT drop happened in the meantime, but the longest streak of full ish blocks I found was around 35 blocks. So it took the base fee from low thirty s to something like over 1500 guid. And I'm plotting below here the priority fees of 1559 transactions that were included during that ramp up.
00:05:43.440 - 00:06:25.710, Speaker C: Namely I'm plotting the inter quantile range. So like the 25th percentile and the 75th percentile of priority fees. I guess if we expected to see these first price auctions on the priority fee, we would expect some kind of ramp up that would be a bit smoother than it is like here. The priority fees really get super high very quickly. And my impression is that what happened is legacy transactions are still very much dominating on the network. And at this point they were at the time. So these transactions are sent with a high gas price because we cast them into these 1559 formats with full priority fees.
00:06:25.710 - 00:06:41.860, Speaker C: Their priority fees are high. And so in turn, the 1559 transactions, they kind of copy via the oracle and they also send high priority fees. I see that Mika has the hand raised. Should I. Yeah, you want to go ahead.
00:06:42.790 - 00:06:51.320, Speaker B: Just wondering. So you have a couple of dips on that top graph there. Are those artifacts, or was there actually block space available?
00:06:52.730 - 00:07:19.680, Speaker C: Yeah, at some point there were, like, less than full block. So I kind of took the streak to be. I can't really take 100% because there's always a little bit of space. So I took it as, like, the moving average was above something like 95%. So there was indeed, like, maybe one block where the gas use was a bit lower than others? Yes.
00:07:20.130 - 00:07:30.740, Speaker B: Okay. So it wasn't like an empty block or something that could have been an artifact of just how mining works. Like, it was partially full. Meaning the miner was including transactions and everything.
00:07:31.190 - 00:07:40.360, Speaker C: Correct? Yeah, definitely not empty because this is, like, the proper size. So something like 21 million gas, I guess.
00:07:41.130 - 00:07:41.880, Speaker B: Okay.
00:07:42.250 - 00:08:21.906, Speaker C: Yeah. I've published this dashboard on the dune analytics, so if you want to take a closer look, I can send the link after. Right. Yeah. So maybe we do have FPS, but at the moment, I guess it's a bit early to conclude that, okay, this is what the system will look like when you have a series of full blocks. I think a lot of what we are seeing might be artifacts of having more legacy transactions in your system about base fees. So people have noticed that it seems to oscillate quite a bit.
00:08:21.906 - 00:09:08.578, Speaker C: Like, it's not really smooth. It goes up, down, up, down. Sometimes you have a full block, then followed by another empty block. Is that normal? So we know that there's definitely like, let's say, a region of the system where things can happen that make base fee look like this. My take on this at the moment, and this is more of a reasoned intuition than actual data analysis, is that legacy transactions that are sent by users, they would set the gas price to something that's maybe higher than base fee, but which remains kind of close to base fee because the oracles they use, they give them kind of. Okay, this is the ambient current gas price. And so this is what you should set your parameter to.
00:09:08.578 - 00:10:25.866, Speaker C: So you send a transaction which is close to base fee, which means that includable transactions, they actually tend to clump around the current base fee. Small upward deviations of base fee, then price out a lot of transactions. And if you have then base fee decreasing a little, you have suddenly, like, a lot of transactions which are again includable. So you could observe these sorts of hiccups due to the fact that these legacy transactions, they don't have a lot of margin to get into the system or not as much margin as the 1559 transactions have with the max fee. Another point is comparing legacy and 1559 transactions. So this was a simulation that we've done maybe a year ago, trying to simulate a mixed system where you have both legacy users and 1559 users exactly the same, let's say dynamics. But what we observed is actually it's not really that legacy users overpay, especially when they send their transaction with a gas price, which is kind of close to the base fee, but it's more that they are easily non includable and in that case.
00:10:25.866 - 00:11:10.418, Speaker C: So these red, purple and brown line, they all represent legacy users, while the green, I think green and yellow line, well, the flat line here is 1559 users. So what happens is anytime base fee kind of rises, suddenly legacy users, they get priced out and they have to wait quite a bit. I see kind of two scenarios. Either they are included quickly if the base fee is stable, these legacy users, or they are priced out and they need to wait until the base fee comes back down. So my take on this is that they seem to be paying more with their time than with their money. And actually. Thanks block native.
00:11:10.418 - 00:12:01.094, Speaker C: I've seen this graph. Parama gave me a heads up that this exists. It was posted in the gap discord. It does seem like these blue spikes is kind of a pending time to inclusion for legacy transactions, and it does seem to be considerably longer than most of 1559 transactions. So I think these dynamics are quite interesting, especially still as we have quite a bit of legacy transaction in the system. All right, so last point, the oracles. This is more of an anecdotal observation, but I was able to send transaction with one way priority fee and they got in really quickly, even though metamask was at the time recommending me to use minimally four to five way.
00:12:01.094 - 00:13:03.870, Speaker C: And this is not to put down metamask. I think it was quite smooth experience, but as Tim noticed also in the notes for this call, I'm guessing that the priority fee oracle might be biased a bit upward. And most likely this is due again to the legacy transactions who, when they got in, they might be. Even if I said they pay more with their time than with their money, there's still more range for them to overpay on the priority fee. And so perhaps this is biasing the fee history oracle a little. I think if we want to be sure of that, we could kind of take a look at the inclusion delay as a function of a priority fee that was sent. Yeah, that would give us maybe a clearer picture of what's happening, but it'd be interesting to know how low can we go with a priority fee and what are the kinds of guarantees that we need on this fee to be included? Yeah, go ahead, Nika.
00:13:05.010 - 00:13:13.178, Speaker B: Do we know what formula metamask is using to determine that? Four to five base fee or four to five priority fee?
00:13:13.274 - 00:13:14.240, Speaker A: Ella. No.
00:13:16.630 - 00:13:23.220, Speaker C: My understanding was that they were using the fee history oracle, but perhaps they are on the call.
00:13:26.150 - 00:13:53.820, Speaker A: Yeah, I don't know for sure, but I think the four to five is some sort of minimum, depending on the setting. If I remember correctly, the last call where I think we had discussed sort of like starting minimums and four to five was in the range, I think like two, three and four or something. So it pulls from the fee history. But if it's like one way or something, we might be hitting a minimum. But I'm not positive. That's just my guess.
00:13:55.070 - 00:14:06.350, Speaker B: I don't think it's a minimum. I used it earlier today and it was recommending to me like 4.73 two nine six or something. It didn't seem like a hard coded number unless someone had fun with primes.
00:14:07.270 - 00:14:11.700, Speaker A: And it's possible we may have updated at this point, too. I'm not sure when this data was pulled from.
00:14:14.470 - 00:14:38.700, Speaker C: Yes. So there might be a minimum, but I still believe there is some kind of dynamics here where the priority fee. So fee story is looking at the quantiles. The quantiles might be quite high because a lot of people are a little overpaying with their priority fee. So that could also be both. A combination of having a hard coded minimum plus the oracle itself.
00:14:39.390 - 00:15:08.100, Speaker A: Yeah. And if you had a lot of legacy transactions. Right, like these might push. You could have a case where, say, only 5% of the block is 1559 transactions. Those transactions can get in with a one great priority fee. But then if you're looking at, say, the first decile, there's not even 10% of the block, that's 1559 transactions. So you're pulling the priority fee from a legacy transaction, and that might explain why it's lower.
00:15:08.970 - 00:15:19.318, Speaker B: I thought the fee history endpoint was giving you a percentile of the lowest priority fee included in the last n blocks. Is that not correct?
00:15:19.404 - 00:15:21.900, Speaker A: In that case, I'm wrong. I'm not sure.
00:15:23.790 - 00:15:27.420, Speaker B: I'm not super confident on that statement. So if someone knows better, please speak up.
00:15:31.120 - 00:16:13.194, Speaker C: Yeah, I'm not sure as well. It'd be interesting to dig into this. Right, so this was for the priority fee. So the second parameter that is kind of relying on Oracle a bit more crudely at the moment is the max fee. So the early guideline that we set was to say, okay, look at the current base fee, multiply it by two, add whatever you were going to propose as priority fees, and this should be good enough. And if we have more data, we will make it better. And this is some data courtesy of Perama.
00:16:13.194 - 00:17:04.614, Speaker C: Thank you for the data, which seems to be saying that what is happening here, what he's been looking at is how long does a transaction stay viable, given that you are multiplying? You're setting the max fee as a multiple of the current base fee, and two x definitely remains viable for 30 seconds because there's no way the base fee can go that high. 99% plus of the time. It remains viable for a couple of minutes even. But it seems the numbers are fairly close even for lower multipliers. So 1.71.5, even 1.3. So in a sense, what this seems to say is maybe the market isn't so unstable that we need to have two x.
00:17:04.614 - 00:17:33.470, Speaker C: Maybe we can use less aggressive max fees than two x. A default of 1.5 seems fairly reasonable. This is, of course maybe more of a static analysis of if everybody changes their max v to something else, the dynamics could be different. But I do think this is early evidence that two x might be a little high. Go ahead, Michael.
00:17:35.250 - 00:17:40.820, Speaker B: Sorry, I might have missed it. What was the time frame this data was gathered over was a since launch till now.
00:17:42.710 - 00:17:50.898, Speaker C: Yes, I think he posted that two days ago. So pretty much most of the blocks until that time. Yeah.
00:17:51.064 - 00:17:56.310, Speaker B: And does that include that big event you mentioned earlier where we had a bunch of blocks in a row?
00:17:56.650 - 00:18:19.554, Speaker C: I think so, yes, I would check, but I believe it does, even if that spike is not included. Well, these spikes are not so long, right. So you could still have a spike for it could represent 0.1% of your sample and you would still be viable.
00:18:19.602 - 00:18:20.200, Speaker B: Right.
00:18:23.870 - 00:18:51.454, Speaker C: All right, that's about it. Yeah. One thing I wanted to highlight is that I thought was really interesting and super cool was the many dune dashboards. There was definitely a lot of community engagement around looking at the data. I actually learned a lot from conversations on Ethernd, a lot of inputs from wallets, implementers, infrastructure providers. That was super nice. Hopefully this continues.
00:18:51.454 - 00:18:55.106, Speaker C: I think we are all really keen to dig more into this.
00:18:55.208 - 00:18:55.474, Speaker A: So.
00:18:55.512 - 00:18:56.820, Speaker C: Yeah, thank you.
00:19:01.370 - 00:19:55.234, Speaker A: Thanks for sharing. Yeah, this was great. I guess the other just kind of thing I wanted to make sure we mentioned on the call is we talked about the fee history API. I know there was some issue where the return type for the oldest block before was in decimal rather than hex, and that caused some problems. So get released version 110 seven yesterday, where the return type of the oldest block in P history is now a hex string. So I think, yeah, a few people had mentioned that this was causing issues and that should be fixed if you use the latest release by guest. And I guess that's pretty much what we had planned for the agenda.
00:19:55.234 - 00:20:02.680, Speaker A: I'm happy to leave the rest of the call for just people's concerns or comments or anything you all want to discuss.
00:20:07.530 - 00:20:41.880, Speaker E: If I jump in here, the only thing that I've noticed is the very low priority fees are getting included. So for example, 0.3 Gwai. And I'm just wondering if anybody had any thoughts around why that is happening or how that might change. Because previously we'd spoke about the minimum being, for example, one or two, or as Jake said before, possibly three, four gwe. So yeah. Any ideas around why that is happening?
00:20:42.570 - 00:20:56.746, Speaker A: That's really interesting. How frequently have you seen like 0.3 not zero would happen if you send a transaction directly to a miner, but I haven't seen a ton of between zero and one.
00:20:56.928 - 00:21:03.282, Speaker E: Yeah, people from outside have been testing, I don't know if. Roman?
00:21:03.446 - 00:21:04.800, Speaker C: Yeah, I'm here.
00:21:06.450 - 00:21:06.910, Speaker B: Yes.
00:21:06.980 - 00:21:45.290, Speaker C: So I sent like probably 1015 introductions with 0.20.3 gray. And also sometimes I set max priority fee higher, but conductions would be still included with like 0.30.4 tip. Well, because base fee would be kind of higher than I expected, but those transactions still would be included.
00:21:46.270 - 00:21:51.660, Speaker A: Do you have like a rough sense for how long they would sit in the transaction pool? Well.
00:21:54.290 - 00:22:04.560, Speaker C: Sometimes it took like hours, but I had multiple times when it was like four or ten minutes.
00:22:05.170 - 00:23:15.430, Speaker A: That's pretty cool. I guess the reason we mentioned one or two as an initial base fee is because that's the price that offsets the uncle risk for miners. And it's kind of like the economically fair price if you want. Whereas where if they include transactions on average for one way, and then they get on cold every so often, then they should end up net ahead. I wouldn't want to see default go below one way just because I think then we end up in a spot where if we're sending transactions which are on average not profitable for miners to include, that might not be great. But yeah, I guess some miners might be willing to pick up those transactions if there's nothing else in the transaction pool. I guess what we're seeing is kind of like the equivalent of before, if the gas price was 20 and you sent like a transaction with one way or something.
00:23:15.430 - 00:23:20.534, Speaker A: And some minor just decided to pick it up. Yeah.
00:23:20.732 - 00:24:06.680, Speaker C: So basically the best way for me to reproduce this was to set max fee per gas as base fee, which is ten percentile for past 100 blocks, ten percentile of base fee plus the value which is returned by Max Hippergas method. And in this case, usually transactions would be included like in ten minutes or so, and tip would be reduced to some value below one way. Yeah, if anyone is interested in.
00:24:09.290 - 00:24:14.698, Speaker B: Are you saying the effective premium was below one, or the premium you've set on.
00:24:14.704 - 00:24:20.460, Speaker C: The transaction was below one, usually higher than one day.
00:24:26.470 - 00:25:05.780, Speaker B: So, okay, so that suggests there might actually be a bug in guess then, since, and I would not be surprised if one of them screwed it up. My guess is they're sorting transactions by the premium on the transaction, not the effective premium, and they're not correctly excluding when that doesn't get met. I would be curious if someone can get through a transaction separately. So this should be looked into, see if it's a bug and geth. If not, it's probably some minor running a custom fork where they screwed something up. Either way, I would be curious if someone can get through a transaction with less than one as the configured premium on the transaction. That would indicate a different situation.
00:25:06.150 - 00:25:08.402, Speaker A: I can try sending one now that happened.
00:25:08.456 - 00:25:17.800, Speaker C: Also, I used like 0.3 set as max fee per gas and it worked.
00:25:20.330 - 00:25:26.246, Speaker B: Max premium per gas, max player, max tip. Okay, yeah.
00:25:26.428 - 00:25:30.680, Speaker C: Find those transactions if you need. Not a big deal.
00:25:32.430 - 00:26:23.930, Speaker B: So in the case of when you set that, I think there's probably two situations here. One, either a bug in geth or a bug in miners with regards to internal transaction sorting and causing them to not correctly do the rational thing. The other one for when the priority fee on the transaction is actually set to lower than one. We saw this long time ago, back before there was congestion in Ethereum when you could just throw a transaction out and it would get included eventually because there was always space. There were some miners that mined transactions that were below what was profitable for them, despite everyone knowing that it was unprofitable. We always assumed they were just like altruistic miners who just didn't really care about the money, just wanted to make Ethereum great and include everybody who wanted to get included. And so you could do like a 0.5
00:26:23.930 - 00:26:42.880, Speaker B: or 0.1 even transaction and just wait several hours until this one random altruistic miner would show up and include your transaction. It might be something similar, or it could just be one of the miners configured something wrong, or they're testing things. Yeah, it's interesting.
00:26:50.410 - 00:26:57.270, Speaker A: Yeah, there's a good comment in the chat. Like different miners have also different profitability threshold.
00:27:01.450 - 00:27:18.590, Speaker B: Maybe. I don't think any miner that we know of has uncle risk that's lower than, significantly lower than one. Like 0.8. I think Vitala ran the numbers a while back and 0.8 was like for the good miners, the ones who were really well connected and had very low uncle rates, like 0.8 was their threshold.
00:27:25.120 - 00:27:49.030, Speaker A: Yeah, I'll definitely share this with the get team and make sure that they look into it. Thanks for sharing the two transactions there. Yeah, was there? Oh, yeah, go ahead.
00:27:49.480 - 00:28:54.890, Speaker C: Sorry, I was going to bring up something else and just thinking out loud here. Based on some conversations with users of etoscan, it feels like there's two different groups of users when it comes to gas prices. There are the people who want to get their transaction true in a short or reasonable amount of time for them, and that's the ones who want to have the current or previous kind of grass oracle experience. And then kind of the comments that I've seen in the agenda and even for myself, there is another kind of user who would be okay with spending like one or two priority max fee and then with a max fee of a few way higher than the current base fee. And it gives them like say a 90 plus percent probability of getting their transaction true within the next couple of minutes. And yeah, I don't know if others have a similar kind of perception that there's two different groups and whatever guest oracle that you want to show, you kind of have to choose which group you're showing for.
00:28:57.420 - 00:29:26.870, Speaker A: I'd say I've heard people also mention there's like a third group, which I think is maybe the group that's having the toughest time is people who want to send a transaction with a low fee, who don't mind it waiting for hours in the transaction pool. So it's like you want the person, just like the person who wants to be in the next block, the person who wants to be in the next five blocks, and the person who wants to be in the next like 24 hours. Yeah. I'm curious if wallet teams or others.
00:29:31.880 - 00:30:36.856, Speaker B: The core problem here is that when you try to, every user has different time preference. And so some users have a high time preference, some users have low time preference, and when you try to factor that in, you end up with a far more complicated problem. And the UX becomes insane very rapidly. Basically it turns it from being able to ask user a question, yes, or no, versus asking a user to look at a 3d chart and say, where are you on this three dimensional graph in this three dimensional curve? Where are you in terms of both time preference and financial preference? And these are multiple variables in this problem. And so the reason I have always lobbied for the default for wallets to be what I have, which is low priority fee, high max fee, is because it simplifies the problem to just a boolean question. So just a yes or no for users, which is very easy. And it just kind of assumes that everybody has a time preference of I want in right now or not at all.
00:30:36.856 - 00:31:14.584, Speaker B: And the only reason, not because we think that is the dominant user, but because that is the easiest user to solve a problem for. And it gives a very comfortable user experience even when they fail. So even it gracefully degrades to okay, I'll just come back later. Which you can express that to user by just saying, hey, by the way, transaction fees change throughout the day, you might want to try again. It's something easy to communicate to users where it's very difficult to communicate to users. Hey, here's a three dimensional curve of all the possible things you need to consider if you want to include your transaction. So you're definitely right.
00:31:14.584 - 00:31:56.160, Speaker B: There are definitely users across the spectrum that have different time preferences, different price preferences, and I definitely encourage wallets to try to think of how you can cater to those different users. I just want to exercise caution of building super complicated uis that users see first. As long as the first ui they see is the easy one, I think it can work out pretty well. And then for more advanced users they can use things like we've seen these eth gas station and I don't remember what the other one was. I think by block native has one as well. They have all sorts of different pieces of information you can see. And then you can also look at like base fee over time.
00:31:56.160 - 00:32:21.950, Speaker B: And so you can say, okay, I've noticed the base fee usually drops on Sunday at 04:00 p.m. The base fee is usually at its lowest. So I'm going to wait till Sunday at 04:00 p.m. And then what the base fee is and then try it then. But if there's a big token sale going on or an NFT sale going on at that time, then I'm going to wait until 05:00 p.m. Things get really complex really fast. So just a warning that if you go down this path of trying to build a UI for that, it gets really complicated incredibly quickly.
00:32:43.420 - 00:32:45.050, Speaker A: Any comments on that?
00:32:45.660 - 00:32:49.004, Speaker B: I did not mean to kill the conversation, maybe.
00:32:49.042 - 00:33:13.164, Speaker F: I'm sorry. Hi, it's Don Vincenzo here. I'm a developer at Mistex for the Alchemist community. We are utilizing a technology that's called flashboards. For those who don't know about it. It's directly sending to miners the transactions. And we are having a hard time building the UX around the base fee due to the fact that the flashboards, when you send a transaction, the transaction will be sent and signed.
00:33:13.164 - 00:33:46.776, Speaker F: Sorry. Will be signed by the user immediately, including the max gas fee. But then this transaction is sent to flashboards and retried until it's included. And this is what we do. This is the service that we propose. The issue that we are having is that on the UI and as a UX, we need to estimate the transaction fee to show on the screen to the user. And this estimation is to include the base fee due to the fact that we are submitting at every single block.
00:33:46.776 - 00:34:29.790, Speaker F: Let's say that this transaction is not included in block one or block two or block first three, but maybe block plus 20. That means that we need to increase on display the on display potential estimated base sheet that the user may be paying. So we basically need to show the max fee that they may be paying to remain honest about it and to remain transparent. But this is a big problem that we have, because this on display only shows a really potential, really high fee when the user is really not going to pay that max fee. This is going to be a very rare case where the increase is going to be at every block for the next 20 blocks, for example.
00:34:31.200 - 00:35:00.612, Speaker A: Yeah, I think metamask was. I saw a thread by Dan from Metamask yesterday kind of covering this. I think they tried to use the average. How many blocks, on average, does it take for your transaction to be included? I don't know. Jake, if you have any thoughts on. I can't. I can't speak to exactly what the estimated fee is, but it's taking our best guess and then highlighting the estimated number.
00:35:00.612 - 00:35:27.710, Speaker A: And then we also show the max fee too, as like an mean. It's one of the things we struggled with the most in the UI is trying to show that. Right, because you don't want to show a super high fee that they're not actually going to pay, most likely, but then you also don't want them to be surprised by a high fee if you never expose the max fee. So, yeah, we do our best to guess or estimate what we think the user will pay and then highlight that number and then show the max fee as kind of a secondary number.
00:35:29.780 - 00:36:28.896, Speaker F: Yeah, I guess the issue that we are having is that whenever you use Uniswap, for example, when you click on the swap button, you will be redirected to the metamask window opening and showing clearly what gas and fees you're going to pay. So users clearly and naturally understand that this is extra fees and network fees. Since we are sending that to flashboards, we can't have that metamask window opening, and we are showing everything in one go, in one place. And where we are being hurt today is that users, they will compare our prices and our fees with uniswap, for example. And on Uniswap, that will not include any base fee or anything that will be shown after, in the next display. So that's where we are being hurt at the moment. And this is really something that we're trying to solve by displaying better, expanding better, but also by finding the right way to show a base key.
00:36:28.896 - 00:36:31.890, Speaker F: That's not scary, that's not driving our users away.
00:36:33.060 - 00:36:33.536, Speaker C: I see.
00:36:33.558 - 00:37:17.680, Speaker B: So this is the problem of the person who is submitting the transaction is not the same person who is paying for the transaction. And currently, the transaction types that we support do not support secondary payers like we used to. So it used to be that you could have a different person paying for the transaction versus who signed the transaction, particularly with flashbots, because you could submit a bundle where one person pays, one person doesn't. We have talked about this before on having new transaction type that would. It's a couple of options. One is we can make it so the miners can choose to cover the base fee. And this is something we talked about, and it almost got included, but we withdrew it because we wanted to keep the initial 1559 simpler.
00:37:17.680 - 00:38:09.410, Speaker B: I don't think there are any strong arguments against that one. So, if people have real use cases like it sounds like you do, for making it so miners can pay the base fee, then what that effectively means is that from the flashbot's perspective, you would submit a bundle where the user's transaction had a base fee of zero, or, sorry, had a max base fee of whatever, but that would be covered by the miner. I think that use case would work. The other option is a new transaction type where you have two signatures, basically one signature of the person who wants to do something on ethereum, and another signature for the person who's going to be paying for gas. And that also, there's no strong arguments against it, theoretically, it just needs a champion to kind of push it through the process and work out all the details. Both of these things are on the table. So I think your situation, we can do better in the future.
00:38:09.410 - 00:38:13.510, Speaker B: It's just this initial launch didn't have either of those.
00:38:15.000 - 00:38:15.956, Speaker A: Yeah, I agree.
00:38:16.058 - 00:38:42.430, Speaker F: That will help. Definitely both of those proposals I don't think it will solve if you want the users to pay for the fees at the end of the day, I don't think that would be solved by those, but definitely that will give us room for extending that kind of options and start thinking of another way to make profit and pay for the user's base fee and not carry them away like it does today.
00:38:56.070 - 00:40:10.450, Speaker A: Anyone else had anything they wanted to share or bring up? If not, I guess one question I had for all the folks on here is I understand that 1559 was the first time in a long time we've had such a broadly impacting change to ethereum that rippled across a whole lot of different areas. We do have another one of those changes coming in the next, I don't know, six to nine months depending on how things go with the merge. I'm curious if people here have anything that they would like to see or they think could help them as we're working on the merge to make the transition smoother and to offer kind of the best experience to their users. Yeah. Either things that we didn't do in London that you would have wished or things that you thought were actually quite good and that we should definitely do again. Yeah, that would be really useful as we're starting to work on this.
00:40:15.770 - 00:41:11.862, Speaker G: Personally, I think these calls are great and trying to bring people from different levels of the stack helps a lot in terms of what to improve. I would say try to stagger more to give a little bit more time for each layer to implement whatever they have to implement so the layers on top have enough time to adapt. For instance, having guest ship fee history or required changes just a few weeks before the merge made it really difficult to get to it and focusing a lot on making sure that testnets are really representative of what's coming up on Mainnet. For instance, something that beat us after the merge was that we have tested everything thoroughly on testnets, but the base fee on Testnet was ridiculously low since blocks were not full usually. So when we actually got a higher base fee on main net, some things.
00:41:11.996 - 00:41:14.726, Speaker B: Started fading due to poorly set up.
00:41:14.748 - 00:41:48.100, Speaker A: Gas. Got it. Yeah, that's useful. So you mentioned having more time for different layers of the stack to adapt and whatnot. What do you think is like the right amount of time from when we have kind of a release of JSON RPC that people can actually use the features to going live on Mainnet? Is it like one month, two months, three months? Hopefully not six months.
00:41:49.270 - 00:42:14.234, Speaker G: I would say it depends on the complexity of what we are looking at. For something like 1559, I would say yeah, one, two months. Probably two months sounds reasonable. Of course I'm going to push for as much time as I can have as possible, and that means putting more pressure on core devs and node developers. So I know that there is a tension between the time frames for each.
00:42:14.272 - 00:42:15.370, Speaker B: Part of the stack.
00:42:16.430 - 00:42:47.910, Speaker A: Yeah. But yeah, it's helpful to know the rough kind of estimate of time that you need. Thanks. Yeah, this is really valuable. Anyone else? Yeah, Micah, go ahead. What's your question?
00:42:49.180 - 00:43:21.280, Speaker B: So I'm just curious, we have a particular group of people here like wallet developers and whatnot. I'm curious what you all think you need to do for the merge. I suspect that it differs greatly from what you actually need to do, and I think now sooner rather than later, is a good time to start clearing that up. But I'm curious, what do people think is necessary from wallet developers related to the merch or non wall developers? Any third party integrators?
00:43:29.700 - 00:43:35.840, Speaker E: Well, on the design side, I haven't even thought about it, so I don't know if that helped.
00:43:37.860 - 00:44:06.420, Speaker B: You're more correct than a lot of people I've spoken to. In theory, the merge should have relatively little impact on integrations, but I want to start those conversations now to make sure we're not forgetting something. Is everybody in the same boat basically completely haven't thought about it? You know, it's a thing that's sometime in the future.
00:44:10.610 - 00:44:38.540, Speaker D: Yeah, this is Jen from Rainbow. Yeah, I guess when I think about the merge, at least from a wallet perspective, that kind of relying on the tools underneath me to maybe have to shift a little bit, but kind of relying on not too much, kind of changing from a UX perspective, but yeah, also thinking of it as like, oh, sometime in the future and once gets more real, then we'll have these calls again at the different layers of who needs to change what.
00:44:39.550 - 00:44:46.362, Speaker A: That's a great point. Maybe what's the point where it starts to feel real for you?
00:44:46.416 - 00:44:47.420, Speaker D: All right.
00:44:48.350 - 00:45:15.958, Speaker A: And I understand it's kind of farther in the future than when the clients start looking at it because right now it's not even implemented in places like guest and Whatnot. But yeah, what are the signs that will make it feel real for you? That's helpful. Hey, this is Bruno from Rainbow. Yeah, I just want to say that I think it's like a gradual kind.
00:45:15.964 - 00:45:16.520, Speaker B: Of.
00:45:18.650 - 00:45:27.642, Speaker A: Implementation on first client level, then having a testnet, then wallets can start playing with it, and then dapps and other people.
00:45:27.696 - 00:45:28.154, Speaker C: Right.
00:45:28.272 - 00:46:39.378, Speaker A: In that order, without having testnets starting to think about stuff. Not starting to think, but actually doing any work. It's just like theory, right? Yeah, when you say testnets. So one challenge that we've had in the past is it's easy to spin up new testnets, like a merged testnet, for example. But because a lot of people actually rely on Gordy, Robson and Rinkabee, it's a bit harder to fork them until we're pretty far in the process. How useful is it for you all when we have new testnets? Is it something that's easy for you to integrate and you can kind of start prototyping? Or is it something that's basically useless? Because if it's not Gordy, Rinkibi or Robson, then you just can't really do much because of how your infrastructure is set up for us, it doesn't make a difference. I think it depends.
00:46:39.378 - 00:47:17.680, Speaker A: Aside from being a tester or a new tester or not, it depends on how many raking changes in code or like JSON RPC. That's what actually breaks or complicates things, not the testnet itself. You know what I mean? Yeah, that's helpful. Cool. That's pretty much all I had. Anything else anybody wanted to bring up?
00:47:21.680 - 00:48:01.888, Speaker E: This is Michigan from Anchorage Digital. I know I kind of missed the boat on this, but I just wanted to voice support on. I think it was don from flashbots that was mentioning the difficulty of predicting the fees that we're displaying for our customers and perhaps including the new kind of transaction types or whatever the ideas are that we're kind of coming up with for kind of solving the issue of a long time where base fee may be changing drastically between when transaction is initiated to submitted.
00:48:01.924 - 00:48:02.510, Speaker A: So.
00:48:04.640 - 00:48:07.390, Speaker E: Just wanted to kind of reiterate support for that.
00:48:08.000 - 00:48:09.070, Speaker A: Got it.
00:48:11.120 - 00:49:21.876, Speaker D: I had just a few. I mean, I haven't think about it a little bit more, but. Bartebay, thank you for your notes and presentation. And I think I'd like to digest kind of what your findings were and especially kind of the last slide that you had with this two x is equal to, or it covers like 100% of the time, or 99 point whatever percent of the time. Maybe it'll be easier if we follow up afterwards with some people just to see if we could break down even further. Because I know that your numbers were for all time, and so I'd like to kind of distinguish between certain peaks and norms when things are flat versus when things are spiking. And then micah, you were talking about, it's funny because from a wallet perspective, we were kind of trapped because we were trying to take into consideration the user's intent, which is distinguishing between when a user wants now or never versus whenever versus like, I really want to get this in.
00:49:21.876 - 00:50:13.700, Speaker D: It's got to be ASAP. And I want to keep trying until it gets that, that urgent but also extended timeline versus just now or never. And of course now or never is much easier. We do have plans in our UI to kind of be like, hey, things are going crazy right now, might be better just to try again later, sort of, sort of thing, when things are spiking. But we were kind of hoping that, I don't know if ether scan or some other APIs are here, but we were kind of hoping that this could be more of like a math problem that is solved by someone who could just give us the numbers that we want for these different scenarios. Like, so that the user doesn't have to do the math, but the API can do the math, and we can just give them an appropriate suggestion based off of the intent that we read off of the user. I assume that's probably very hopeful.
00:50:15.640 - 00:51:05.076, Speaker B: So it is possible to do the math. Like I mentioned, it's really like a three dimensional curve, and if you know the inputs for that curve, you can do the math and tell the user, okay, this is what you should do. The hard part is getting those inputs from the user in a digital form. So a user who just kind of vaguely says, I kind of in a hurry, that's not super helpful for the math side. I'm kind of in a hurry into like, this is the digitization of my time preference. That's the real hard part. So if you guys can figure out, or someone can figure out, how to distill a user's time preference and price preference relative to each other into quantifiable numbers, then we can definitely put together a formula that will tell them, okay, this is what you should do.
00:51:05.076 - 00:51:38.850, Speaker B: Based on all of history and what we know about the ecosystem, all these things, I'm not sure if that's reasonable or realistic at all, because I think even when I ask myself, what is my time preference, I can't put that into a number. I don't know what my time preference number is. I just know that I kind of want it to go in today maybe, or like, I'm going to go to bed soon and I want to be sure it gets in before I fall asleep. So in the next couple of hours, they're very vague numbers for me. I don't know if other people have more solid things.
00:51:43.300 - 00:52:19.150, Speaker D: So I can understand trying to extrapolate users intent into actual inputs to a mathematical function. I understand that, but I'm wondering if we can kind of chop it up into maybe three or four different categories or boxes in some way. Yeah, I'd have to think about a bit more, but that would be if that math function is there, and then maybe we can give a little bit more thought of, like, how do we translate user intent? Or how do we even get a signal about user intent? I think we have a few ideas about how to get the signal for users intent. Then maybe we can follow up on that.
00:52:20.080 - 00:52:56.490, Speaker B: You might be able to kind of craft some straw man users where you just kind of describe a particular person, and then you give that fake person some actual numbers to plug into these formulas. And then you say, are you this person? Or are you this person? Are you this person? That might be possible. It's still going to be vague. Like, those people won't map exactly to actual real humans, but that might get us closer. So instead of just mapping to the now or never person as our only straw man, we now have three or four straw man. And you can map to users can then pick one from like a nice little picture book.
00:52:58.720 - 00:54:09.440, Speaker D: Yeah, I think that would be helpful, I guess from our perspective, I'm glad that Barnaby had that slide about how two x seems generally, on average, too high. Because intuitively we also think that we should tighten the multiple that's placed on the base fee and if anything, kind of like play around with a priority fee instead. Because we are kind of like, even if you're urgent, then, if you're urgent, now or never, you also want a tighter multiple because you don't want to be potentially waiting around forever. But if you don't care that much, then it's kind of unfair to show you a huge range of prices that you could use. So it's better just to wait around and maybe you might get dropped if it's really busy, but it's better to give you a tightened bound. We don't want to have a huge range that we show to a user where on average, it's like a very small subset of that range that you're actually going to be spending. That's what we'd like to avoid.
00:54:09.440 - 00:54:23.620, Speaker D: See, I guess I'm just asking for magical estimations that'll work.
00:54:24.230 - 00:54:27.250, Speaker B: I nominate Barnaby to do magic.
00:54:27.750 - 00:54:28.500, Speaker D: Great.
00:54:30.810 - 00:55:51.550, Speaker E: I don't know if we have enough time, but on that, in Barnabay's slides, there was one example of kind of a normal scenario where there was some variability in the base fee, but I think if you were to create a moving average, you would see oscillations in the base fee across that. And then there was the other scenario with multiple consecutive full blocks. And the issue on the design side is. Right, okay, which of these two scenarios are we currently in? So how do we reasonably estimate whether somebody will be included in the next few blocks based on which curve everything is currently on? And it's almost impossible to tell. You don't know what is going to happen. You don't know if the trend is going to continue upwards and it will take a long time for that period of time to pass with the multiple consecutive full blocks, or whether it's just kind of the normal state where things oscillate up and down. And with that on the design side, it's really, really difficult to make a call and say, okay, actually we're in this situation and it's going to take x amount of time for you to be included.
00:55:51.550 - 00:56:42.430, Speaker E: And it's also quite difficult to flag, okay, we're on an upward trend here, and we don't know when this is going to end. It doesn't instill confidence and it's quite difficult to communicate. And if somebody figures out how to communicate these potential scenarios, then great, perfect. But I'm not sure whether it's. Whether just having a way to determine which of these two trends you're on is going to be helpful to lots of people. It may be helpful to some, definitely, but I think, yeah, there's lots of communications that need to be done here above actually determining those trends.
00:56:44.370 - 00:57:39.954, Speaker B: So the core problem here is anyone who can answer the question of which trend are we on can make far more money by going into finance than we can offer to pay them to help us. Because essentially it's the exact same problem as predicting the future price of a stock or commodity. It's an attempt to predict future demand for an asset, which comes down to keeping an eye on all the things that are happening in Ethereum, keeping an eye on the news, sentiment analysis. When we get these bursts, they're not bursts because of something that's predictable. They're always bursts of something that happened, like an event occurred in the world that resulted in all of a sudden everybody wants to use Ethereum. Now, that being said, some of these events air quotes around that are seasonal. And so there really is at a certain time of day every day, the fees generally are lower than other times of day.
00:57:39.954 - 00:58:27.246, Speaker B: We do see strong seasonality in gas prices. And so those ones we can predict and we can show to users potentially, and they're actually pretty easy to graph, especially now we have the base fee that become really easy to graph. So you look at the base fee over time and then plot by day, plot by time of day, plot by day and time, and we should see some very strong seasonality. But the ones like we saw earlier, which was like an NFT sale or something, those ones are just effectively random for anyone who's not an NFT buyer, and it's NFTs today, it was ICOs. Before that it was cryptokitties. Before that it was some event occurs in the world, Elon Musk tweets something and it triggers it. And so I just want to make sure everybody's aware that it is very unlikely we'll ever fully solve that problem.
00:58:27.246 - 00:58:48.150, Speaker B: The best we can do is capture the seasonal stuff and try to present that and let people know, hey, we're on the morning uptrend. Every morning it starts to go up, so we're going to estimate a little higher, or we're on the evening downtrend, so we're going to estimate a little lower. We can maybe do that, but I think that's probably about the best we're going to get realistically.
00:58:56.660 - 00:59:27.820, Speaker F: One thing I've noticed is that it was especially difficult. I mean, it was really like EAP 1559 went live and we discovered the effects, or some of the effects were discovered after the go live is there already, and I will not be aware of that yet. Or is it possible to have a testnet which will have all transactions replayed, maybe with a delay of 24 hours or something that will allow us to see the impact of such upgrades?
00:59:32.320 - 01:00:10.170, Speaker A: Yeah. The two challenges with that is one, basically the tech to replay transactions is hard. That's solvable. But yeah, we don't have a team that can help with that. The second part is the money, basically. So because transactions on main net are worth something, we get different patterns and different incentives to send than on testnets. So it's hard to get a perfect replay, basically.
01:00:11.420 - 01:00:41.110, Speaker B: And to expand on that a little bit, one of the issues with replay testnets is that they very quickly fall out of sync because if you're replaying under a different rule set, some transactions will fail. That on the main net succeeded. And so I guess in this case. Sorry, when we talked about this before, we were talking about testnets for future changes. Are you talking about testnets for future changes or do you want just a test net that just replays history so you can do testing like back testing? Basically.
01:00:43.800 - 01:01:32.340, Speaker F: What I'm thinking is for example, for this go live. If a week before EIP 59 was deployment on the testnet, that every day was getting the main net transactions with a delay. We will have seen some of those issues that we're having now a bit earlier with our new UI, new UX, and the impact it has. I understand it's difficult, right? I understand it's not something easy to do. But I was thinking you don't really need to reproduce some of those data. Like you don't need to reproduce the from the addresses or anything. I think what matters here is the amount, the tokens in play and all the transactions.
01:01:32.340 - 01:01:43.620, Speaker F: We got the same flow of transactions, the same number of transactions in the system. So we can really see the impact that those will have on the gas and the usage of the network eventually.
01:01:45.980 - 01:02:32.840, Speaker B: Yeah, so the core devs have talked about this in the past and I think we generally got agreement this would be generally useful for the core devs as well, just because it's nice, like you said, it's nice to see real world stuff. The issue that we run into is that when the rules are different between the two chains, they very quickly fall to sync with each other. And so transactions, you start with one transaction that fails on the testnet but doesn't fail on main net and then that leads to the world state being slightly different. And then now another transaction fails because the world state is different and then another one. And this kind of balloons out pretty rapidly. We don't know how rapidly that happens. It probably depends on specifics of the rule changes, but so it means we can't just spin one up and leave it up forever because eventually the world state will differ so much that we just simply doesn't make any sense to replay anymore because everything's failing.
01:02:32.840 - 01:03:13.508, Speaker B: That being said, we talked about doing things like having a daily or weekly reset or something. So we constantly do have real world transactions being replayed on a test network using new rule set, but we just reset it periodically to make sure the world states stay in sync. And that is an option, like I said, the core devs generally were favorable to this idea. It's just a matter of core devs are overworked and we have to choose. And at least for London we decided not to do this. But we did talk about maybe for the next big feature fork or something, we will want to do this. Like I said, there's general interest.
01:03:13.508 - 01:03:15.620, Speaker B: It's just a matter of prioritization.
01:03:16.120 - 01:03:25.290, Speaker A: Yeah. Thank you. And yeah, ligy, you had your hand up a while back and we never got to you.
01:03:28.300 - 01:03:28.776, Speaker C: Oh yeah.
01:03:28.798 - 01:04:11.216, Speaker H: That was to the other thing. Basically I wanted to say, it's not only personas, basically, where you need to make preferences, it's also the transaction type. For example, I'm the Persona that usually doesn't care about the timing. But then when it comes to uniswap transactions that has a timeout, then it's a problem. Then you want it fast. I once made a post on magicians about that, that we should make a way to signal that. So either via nutspec, so that contract authors specify that on nutspec, or we add it to the RPC so that we as wallets, that's also good for the user experience because then we have less cognitive load on the user.
01:04:11.216 - 01:04:31.660, Speaker H: Right. So if they don't need to decide, it's better. But it's also an important signal because then also it's happening for a lot of users. They don't really know about the timeout or the expiry of the transaction. So basically the transactions can also signal that they want in very fast. It's not only Persona.
01:04:34.880 - 01:04:36.750, Speaker B: Yeah, I would love to see that.
01:04:39.600 - 01:04:51.960, Speaker A: Agreed. We're already a bit over time, but we're still here. So any final questions or comments?
01:04:57.180 - 01:05:02.616, Speaker D: If I wanted to follow up with Bonner Bay or Micah, should I just do that on the discord?
01:05:02.808 - 01:05:03.164, Speaker C: Yeah.
01:05:03.202 - 01:05:36.796, Speaker A: So we have a 1559 dev channel, which probably makes sense to use. Yeah. Cool. And then relatedly, does it make sense to have another one of these calls? Probably not like two weeks, but I don't know, does it help people in a month or when people have had more time to dig into this? We don't have to schedule it now, but. Yeah. Do people generally want another one of these calls? About 1559? And if so, when would be like the right timing? There's at least one. Yes.
01:05:36.978 - 01:05:45.410, Speaker D: Yeah. I would like at least one more call after you do a little bit more back and forth first. So probably a month makes sense.
01:05:46.980 - 01:06:03.590, Speaker A: Cool. Yeah. Okay, so let's aim for a month from now, roughly. It's probably easiest to have it be literally four weeks from now when it's not all core devs or something. Yeah. So I'll make sure to set that up.
01:06:05.480 - 01:06:10.920, Speaker B: To your last answer. Tim, aren't we combining the 1559 channels into just bound fee market?
01:06:10.990 - 01:06:56.956, Speaker A: Yeah, we might rename the channels on discord to have it be just fee market. So if you can't find a channel with 1559, fee market is basically the same thing. It might make sense, I guess, just given this and that, we're having another call in a month. I'm finally just holding to change that. Yeah, I don't think it has to happen right now, but yeah, if there is no more 1559 channel, just search for fee market and if you're not sure, just ask anywhere in the discord and somebody will share the link. Yeah, let me just share the link to the discord in the chat here. Cool.
01:06:57.058 - 01:07:02.908, Speaker B: Anything else? I recommend anyone who joins that mute the channels you're not interested in. There's a lot of them.
01:07:03.074 - 01:07:11.900, Speaker A: Yes, this is all of the cordev channels across all of the research. So, yeah, definitely we're muting aggressively.
01:07:15.090 - 01:07:17.742, Speaker B: You cool?
01:07:17.796 - 01:07:31.246, Speaker A: Well, yeah, thanks a lot, everybody, for coming on. And yeah, I'll share the information for the next call when it's all set up. See you.
01:07:31.268 - 01:07:34.460, Speaker D: Thank you. Thank.
