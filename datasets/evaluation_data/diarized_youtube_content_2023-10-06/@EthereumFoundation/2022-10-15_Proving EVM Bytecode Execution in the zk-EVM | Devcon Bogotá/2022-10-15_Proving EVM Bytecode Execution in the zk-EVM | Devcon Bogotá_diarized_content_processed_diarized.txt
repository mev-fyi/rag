00:00:11.690 - 00:00:12.046, Speaker A: Okay.
00:00:12.148 - 00:00:36.120, Speaker B: Good morning, everybody. I'm morelian security and protocol dev at optimism. And this morning, I'm happy to introduce you to. Oh, hang on. We have Franklin and Olivier from consensus, who will be speaking to you about proving EVM bytecode execution in the ZKe. Evm. Okay, take it away, guys.
00:00:37.130 - 00:01:24.370, Speaker C: Hello, everybody. Thank you for coming. So, yeah, we are both from consensus r d. We've been working on a ZKVM pretty much for about a year, and in this talk, we want to talk to you about its arithmetization, how we implemented it, and we'll have an announcement at the end. All right, so, first of all, why Zk rollups? The screen is not working for me. Well, in terms of scaling ethereum, one of the bottlenecks that is addressed by a Zk rollup is that of the state. So in order to validate a state transition, which is, you need to basically execute the transactions within a block.
00:01:24.370 - 00:01:57.230, Speaker C: And the state is a big object. And this validation is a resource intensive operation. So the promise of a Zk rollup is to basically alleviate that workload from most of the nodes in the network. So when you have a Zk rollup, you have basically this powerful node, which is an operator, which provides proofs of state transitions. And these proofs typically verify the following. The validity of the transactions in the batch. The fact that the internal logic of the roll up is respected.
00:01:57.230 - 00:02:38.558, Speaker C: Roll ups are usually application specific, and the fact that they induce the proposed state transition. And here and throughout, when we talk about proof, we talk about proofs of computational integrity, which in practice are implemented as zero knowledge proofs. So a ZkEVM is a particular kind of a Zk rollup. The big distinction, or the thing that makes it a ZKVM, is the logic part. The logic that is being executed or improven in the proof is the execution of the ethereum virtual machine. It also means that the transactions that roll in pretty much obey the same format as standard l one transactions. Sorry.
00:02:38.558 - 00:03:32.526, Speaker C: Okay, so how does that work in concrete or semiconcrete terms? Well, you have your l two, which has a state, and a bunch of transactions roll in, and they induce, or executing them induces a transition of state. And the ZKVM basically plugs itself at this point. It extracts the required data from the previous state, it takes into account the transactions and basically the diff of the new state. And it does its magic, and it produces traces, which it passes on to approver. And the prover then produces a proof which ends up on main net and in this way you bundle all of these l two transactions into a single l one transaction. So why would one want to build a Zk EVM? There's two parts in this. There's the Zk and the EVM part.
00:03:32.526 - 00:04:32.366, Speaker C: So the EVM part, basically the advantages or the interesting parts, is that it allows you to reuse or to use all the existing tooling which has been developed for l one. You can basically write in a ZKVM, write and deploy smart contracts on l two just as though you were doing it on the l one. Furthermore, you can redeploy already deployed bytecode onto l two. And the ZK part in the ZKVM, well, it gives you basically the scalability boost and also faster finality, just because of the fact that there's a proof associated to it. So that makes it interesting as well. Okay, when we set out on this project, we had a few goals, so we wanted to be able to, in our ZKVM, prove the execution of unadulterated native bytecode and respect the logic which is specified in the Ethereum yellow paper. We wanted full coverage of all the opcodes.
00:04:32.366 - 00:05:13.338, Speaker C: Where we allowed ourselves to deviate is in terms of the representation of the state. And so for instance, we will not be using ketchup. We are building a type two ZKVM in the sense of the classification put forward by Vitalik. So this project, I'm sure you're aware, presents a lot of challenges. There's a lot of complexity that comes from the EVM itself, which is composed of many parts which are tightly coupled and have complex interactions. There's a lot of intricacies that are really specificities of the EVM. You have families of opcodes that have slight variations in their execution, slight non uniformity.
00:05:13.338 - 00:05:52.138, Speaker C: There's a completely different kind of challenge, which is that of auditability. So Franklang will touch upon that in his portion of the talk. And the main challenge which everybody faces today is that of performance efficient proving schemes. Yes, and this is something we will communicate on at a further point. Today is really about the arithmetization. Okay, so here is the basic setup of how it's going to work. You will have a modified Ethereum node and execution client which receives transactions sent by users or by dapps.
00:05:52.138 - 00:06:40.870, Speaker C: We plug ourselves into this execution client and extract some data which we use to fill some traces. I'll be talking more about traces later. And these preliminary traces are fed into this tool, which we call corset, which does many things. Among them it is responsible for producing the constraint system, and it also expands and computes the remaining parts of the trace. All of this constraints and expanded traces are then fed into our inner proof system. The inappropriate system we use is not compatible with Ethereum per se, so we have to feed it into a verifier, which is a circuit over bn 254. This is where we plug it into with gnark, and Gnark then produces the outer proof, which is then posted on main net.
00:06:40.870 - 00:07:51.598, Speaker C: Okay, let's talk about a bit about the arithmetization. So first of all, on Monday we published an updated and expanded version of the spec, which is now a pretty hefty document, and its contents are basically the arithmetization whoops of the EVM. So when we talk about arithmetization, in this talk at least, we mean to basically construct or write down a system of polynomial equations, the simultaneous satisfaction of which perfectly encapsulates or captures a particular computation performed on particular set of inputs. For us, the computations of interest are valid executions of the Ethereum virtual machine given a set of transactions and initial state. Okay, so since the EVM is a beast of complexity, it's a big thing we basically need, or it pays off to try to decouple as many of these components as we can to work in a modular fashion, to sort of concentrate the complexity in different places. So this is the general architecture we have. We have the central piece, which we call the hub, which is basically our stack and our call stack.
00:07:51.598 - 00:08:40.754, Speaker C: And then we have plenty of smaller modules that are tasked with doing specific kinds of operation, such as arithmetic or binary operations or storage. And I don't know if you can see it, but there's also the memory part, okay? It doesn't work. The memory, which is this MMU and MmIo modules. Okay? And when you run the ZKVM, what do you get? You get these traces that I was telling you about, which are these large matrices which contain data represented as field elements. There's one such trace per module, and each trace obeys its own internal constraint system. So on the previous slide you had these arrows which pointed from one module to another. And this is basically connections, plocup connections, which allow us to transport data from one place to another.
00:08:40.754 - 00:09:20.750, Speaker C: The other kinds of constraints are basically the internal constraints. So for instance, when you update the program counter, you expect something particular to happen, but you also have another kind of constraint, which is sort of global constraints, global consistency constraints, which range over the entirety of the block rather than two or three consecutive rows. And that for instance, may express properties such as that. Well, when you reenter an execution context and you load something from Ram, you get what you last put there. So let's zoom in a bit on this central piece, the hub, which is, as I said, our stack and our call stack. It gets its instruction from the ROM. And what it does once it has an instruction is to basically dispatch this instruction wherever it makes sense.
00:09:20.750 - 00:10:01.270, Speaker C: Once it has an instruction that it loads from ROM, it first does some preliminary decomposition of that instruction. It extracts some parameters which are hard coded, and it decides on certain things, such as how to interact with the stack, how much data to excavate from the stack, and where to put it in the trace, the layout, basically, of the data. It also raises some module flags, et cetera. And the next step is then to dispatch the instruction. But before we can go there, we actually have to deal with potential exceptions. And the hub also deals with the exceptions. It's basically some of them it can detect, others it imports from other modules.
00:10:01.270 - 00:10:35.494, Speaker C: And if an opcode makes it past this hurdle, then you have the instruction dispatching per se, which kicks in, and you have these flags, these activation bits that light up, and you have some stamps that are updated because you need to keep track of temporality. Yeah. At this point, these activation flags, well, they tell you what will be active. So for instance, when you do a create, you will be touching RAM. So you'll be touching those two modules, MMU and MMIo. You will also be touching Rom because you'll be deploying initialization code, and you will touch gas and memory expansion. And same thing for create two.
00:10:35.494 - 00:11:30.034, Speaker C: But since there's a larger hash involved for the initialization code, you tap into some hash modules. Okay, so let's talk about the next big piece, which is RAM. So RAM is probably the most complex piece in our arithmetization, by the way, all the figures that I put here are available in the document. And some of the. Well, one reason why it is so complex is that it has all these data stores with which it can interact, and the different instructions which interact with RAM actually have different sources and targets. So there's already some first complexity. The next source of complexity comes from the fact that operations which are atomic from the point of view of the EVM, such as returns or creates, have to be broken down into smaller elementary operations in the ZKE EVM.
00:11:30.034 - 00:12:16.770, Speaker C: And so the first task is to basically do a lot of offset computation and deciding when some padding has to be done, this sort of stuff. And once all of this has been decided. Well, you can start writing instructions, and this is still just writing them without executing anything. You just have this sort of workflow that tells you in this case I need to do this and that some exoram, slight chunk or something. And once you're at this point, you are at the phase where you can actually start doing something. So this is the work of the MMIO, which is the actual ram in a sense. This is the component that touches data and that actually does the sort of byte decompositions, recombinations, slicing, dicing, surgeries, et cetera.
00:12:16.770 - 00:12:29.974, Speaker C: And then you have these consistency checks that I told you earlier about, which is basically finishing the memory part. Basically what you've written last is what you should retrieve next time. So I'll stop here for the arithmetization. I'll hand it off to Franklan.
00:12:30.102 - 00:12:57.550, Speaker A: So thank you for this very extensive description of what is the constraint system on the arithmetization. And now I'm going to talk to you about. Please come back. Thank you. I'm going to talk to you about how to go from this, well, let's say conceptual data to how we actually implement the whole thing. There is a lot of challenges when we want to go from the specification to the implementation. And the most one is that there are three moving parts.
00:12:57.550 - 00:13:38.478, Speaker A: On the one hand we have the actual specification, 250 pages, then we have the implementation of the specification, and then finally we have the proof system of the verification of the traces. So all of this stuff is developed by different people working on different teams. We still need to maintain 100% conformity between all of these pieces. So of course if the prover is proving something else, that's what the spec is describing, which is itself something else. That's what is actually implemented, nothing will work. And finally it's how to audit three diverging code bases or three diverging sources of data. So what kind of solution did we find? We developed a formalized single source of truth, which is then exported to multiple targets.
00:13:38.478 - 00:14:31.726, Speaker A: So what happened is that we have in some format that I will show you in a few seconds, a description of all of this constraint system. And from this single source of data we are able to produce first go library, defining the constraints on the data that will then be used by the prover to actually prove stuff. Then we have another goal, libraries that is used by the Ziki VM implementation to ensure conformity with the specification. And finally, we can generate latex data for integration within the final specification document on the 215 pages of PDF. So here is how it looks so you can see that this is very clearly lisp inspired languages with a lot of parentheses or other kind of stuff. And this is a very simplified example of what you could find, for example, in all of the MMU. So what do we have in this? First, we have the column definition.
00:14:31.726 - 00:15:19.506, Speaker A: Those are like the columns of the matrices that Olivier showed you a few minutes ago. And you can see that they can either be normal, like the one at the top, or we have a very rough typing system, type systems that is used for some optimization. And finally, for the pure ease of implementation and ease of use, we have some kind of very simple array, that kind of stuff. Afterward you have helper functions, which are functions that can be well defined, like any other list function, to act on this data. And for instance, here you have two functions. The first one is checking that two arrays of length eight are actually, element wise, equal one to the other. On the other one is computing checking that an array of eight elements is actually a by decomposition of a given value.
00:15:19.506 - 00:16:01.242, Speaker A: So these do not really exist per se, but they are just like small syntactic sugar for the ease of implementation. And finally, we have the meat of the data, which is the constraints themselves. So here we have an example of two, three constraints. So the first one is doing a party composition of some data, the other one is checking that memory is aligned, and so on and so on. And from this we will run it through courses that Olivier evoked a few minutes ago. On course a will do quite a few things, and among others, it will, for instance, generate a lot of go code for the prover that you can see here. And you can imagine how writing this by hand would be a living nightmare.
00:16:01.242 - 00:16:46.762, Speaker A: Or we can also generate latex. So here you have, for instance, a piece of latte code on the PDF renderings that is ready to be incorporated in the spec. So this whole stuff that we call corset is really a cornerstone in our workflow on our implementation of the ZKV. Now I will talk to you about some results that we have reached for now, from the specification to the implementation to the actual results. So the first thing we want to benchmark our implementation against is the EVM test suite. Of course, the EVM test suite is a golden standard for ethereum clients, including, but not restricted to the EVM itself. So we have tested our current implementation of the Ziki EVM, which is well advanced but not yet finished.
00:16:46.762 - 00:17:32.186, Speaker A: So for now you see here the list of modules that are ready. So we have the hub, the MMUs, the ROM Dlus, the binary and some comparator functions. And on over 17,000 tests we run our EVM on this and 16,000 are a success. So it means that it runs and the traces are validated, zero are failing. So it means that we do not have any problem handling this test. And finally we have 1300 on three, which are hitting functions that are not implemented in the ZKVM, namely in this case the precompact contract and the self district operation. So for now we have a 92.6%
00:17:32.186 - 00:18:18.342, Speaker A: success rate on the EVM test suite, which is, we believe a good start. Another way to test onto benchmark or implementation is of course to work on real data. Right? So we have quite a few real award examples, but the most striking one we have is I believe the successful execution, validation of proof of exemptions using the Unisoft contract, and the successful execution and validation of random mainnet block. So what we do is that we run our Ethereum client on the main net and we generate traces for some block there on there, and then we validate all of the constraints. So I will show you a little bit how this work. We are going to check the verifications of the constraint on the traces we generated for this block. Zero x 35 b seven en it, blah blah blah.
00:18:18.342 - 00:18:51.270, Speaker A: This block was created yesterday in the afternoon. So let us start, and while it is working I will show you a little bit of what is in this block. So here is the data on a tor scan of this block. As you can see it is like 23 house all. We have quite a lot of different transaction in it. So we have a basic transfer transaction, we have some wrapped stuff, we have some multicol, we have failing transaction, we have successful transaction, we have Uniswap, we have Tezer. So it's quite a nice example.
00:18:51.270 - 00:19:39.254, Speaker A: It's not a very big block, it has only 52 transactions, if I remember correctly, and it's only using 2 million gas on the half, but it is still quite inclusive in what it provides. If we take a quick look at the traces that we generate for this block. So here you have a decomposition of all the data that we generate for this block. So on the very big red stuff at the top of the screen, you can see that in the end we generate, let's say 13 and a half million of cells, which is like the actual content of the traces. You can see that the biggest one are the binary with 2.5 million of cells. Then you have the ROM, well sorry, the ROM first with 5 million, then the binary with 2 million and a half.
00:19:39.254 - 00:20:22.302, Speaker A: The hub, which is also before the binary, sorry. With 3 million on all of that is culminating into 13 millions of cells that have to be proved unchecked and then cryptographically checked. So here we can see that Corsay is actually doing a very naive check of all of this, because it will basically just run all of the numeric constraints that we define on all of the lines and rows of the matrix one by one, which is obviously very suboptimal. Absolutely not cryptographic. But this is only debugging tools that I show you to prove you that our stuff is still working. And as you can see, the validation is successful. On the block zero x 35 B is actually validated.
00:20:22.302 - 00:20:47.946, Speaker A: With all our constraints on our EVM, we'll be able to give all of this data to the proverb. On the proverb we'll be able to generate the proof and put it on the main chain. Now going back to the presentation, so sorry, I can't play because no Wi Fi. So you will have the development version. So in conclusion, the complexity of the EVM implementation has been partially solved with modules. Well, completely solved with modules. The intricacies of the EVM, maybe.
00:20:47.946 - 00:20:49.980, Speaker A: Olivia, you want to say something about this?
00:20:50.670 - 00:21:03.950, Speaker C: Yeah. So in terms of basically finalizing the arithmetization, we have two big chunks that are still left to be done, but we are quite confident that will be done basically for the testnet.
00:21:04.290 - 00:21:46.334, Speaker A: Regarding the auditability, we don't have yet an audit or a formal verification of what we have done. But thanks to our single thought of truth mechanism, we have laid the groundwork to actually be able to work on that and prove in a single strike all of the three components with a single audit. And finally, regarding performances, we are now connected to the prover system, and there will be more information regarding that soon in a coming paper. So thank you for attention. We will be launching a testnet soon, so if you want to join, please just scan the stuff or just go to the URL. And if you have any questions then we will be happy to take them.
00:21:46.532 - 00:22:36.586, Speaker C: Can you just give a high level overview of the differences between your implementation and the current other ZKE EVms, such as from scroll and Polygon, et cetera? I don't know exactly the inner workings of Hemez and Skrull. I know that we share some design principles with both. I know that our prover will be different, our arithmetization is also going to be very different, and I think this probably represents actually a strength overall. Not for us in particular. But for the ZKe EVM ecosystem as a whole. Keeping in line with this multi ZkeVM prover that Vitalik has been talking about. But in terms of concrete details, I'm.
00:22:36.618 - 00:23:15.580, Speaker A: Not quite aware regarding concrete details. The big difference with scroll is, let's say the arithmetician method. The both of us are using different methods. We will see which one will sustain the test of time. Regarding the difference with polygon, Hermes, the main difference is that polygon, contrary to us, doesn't directly works on EVM bytecode, but they first translate the EVM bytecode into another bytecode that is running not on the EVM, but on AdoC register machine, which is then approved. So there is this supplementary step that neither scroll nor we do have.
00:23:16.990 - 00:24:13.920, Speaker C: What are the opcodes that have been the most difficult to implement in the GKE EVM? You may be surprised, but call data load has been horrible. You would expect if you can do a m load, you can do a call data load. But in the way that we arithmetize things, it's actually quite difficult because of the provenance of the data and the fact that there's some padding involved. But in terms of the real complexity, the real complexity is actually for anything that involves writing a whole lot of data with padding, potentially. So anything such as code copy, Xcode copy, I don't know. Yeah, basically these kinds of opcodes have been the most complex. And if you look in the arithmetization about MMIO, there's literally pages upon pages of we're defining nibble on bit eight, and they have to interact in some complex way.
00:24:13.920 - 00:24:15.850, Speaker C: Memory has been the worst.
