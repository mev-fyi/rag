00:00:00.410 - 00:00:50.720, Speaker A: You okay? We are recording. This is the third .44 breakout room. Share the agenda again in the chat at a high level, there's been some updates in terms of the implementation on the devnet, and we'll start by kind of sharing those and going over it. There's a bunch of things left to do in terms of work on the devnet, and I know over the past couple of weeks a bunch of engineers have reached out that they wanted to help. So we'll make sure I take time to cover those and see if there's some folks here who can help with any of these tasks. And then I want to make sure we kind of spend a good chunk of the call as well, talking about the higher level kind of design questions.
00:00:50.720 - 00:01:34.198, Speaker A: So there's three main ones. This idea of how the base fee and the whole fee market works for this, the sync design, which Terrence wrote a really useful doc for, and then updates on KZG verification optimizations. If we get all that, we're happy if we have some more time. I think some stuff to chat about is just know if we have a second devnet. What's the feature set we want if there's any updates on the KZG ceremony side. But yeah, the KZG stuff has called every two weeks, so it's fine to leave that to there. Yeah, I guess.
00:01:34.198 - 00:01:40.960, Speaker A: Mophie, do you want to give us a quick update and maybe like demo of the devnet? You?
00:01:42.370 - 00:02:36.880, Speaker B: Hey Tim. Yeah, sure. So we have a devnet out. This is going to be the first of hopefully only two Devnets we're going to have rolled out for the EIP 48 44 this devnet. Basically there are a few things in the spec that we still need to, I guess, discuss and finalize. So hopefully the goal of this destination is to let us, I guess, test what we have, what we've already come into consensus, and have something that a community can start playing around with and hacking with before we later decide on what we need for the spec. At that point we would have like a second devnet and then we can go from there.
00:02:36.880 - 00:03:37.860, Speaker B: So Devnet we have running available contains basically four validator nodes and four beacon nodes. And I wrote up like a pretty handy guide that's linked in the GitHub issue to help onboard folks into it. Let me post it here on Zoom. So to connect to the devnet, all you need is basically the latest geth and prism implementations of the spec. And I posted some configs you could use. It's not really geared towards, I guess folks that are not very familiar with running Geth or prism. So the guide is a little bit baroque, but it should be able to get you started.
00:03:37.860 - 00:04:00.186, Speaker B: Let's see here. Let me see if I can maybe demo how this will work. I don't think I've done that yet. Just to show you how it work, let me start my video, if that works. Is there where I could share my screen?
00:04:00.368 - 00:04:02.458, Speaker A: You should be able to share screen. Yeah.
00:04:02.624 - 00:04:07.194, Speaker B: Okay, let's see.
00:04:07.232 - 00:04:07.820, Speaker A: Here.
00:04:21.330 - 00:04:39.030, Speaker B: We go. Share screen. All right. It. I hope everyone can see my screen now.
00:04:39.180 - 00:04:39.880, Speaker A: Yes.
00:04:40.570 - 00:05:37.720, Speaker B: Cool. So we actually have like. So Michael from Coinbase has this really cool repo that sets up the docker compose, making it very easy for you to connect to the devnet. Let me post that in the chat. I think that'll be like the easiest way to onboard users. Basically takes the guy that I wrote and solidifies that into a couple scripts. And I think, michael, we still need to update the genesis in that repo, right?
00:05:38.670 - 00:05:40.460, Speaker A: Yeah, I can do that right now.
00:05:40.910 - 00:06:50.414, Speaker B: Cool. So I have that repo pulled in. In my instance, I have the Genesis running and pretty much if you want to start the devnet, my containers here, you can pull the repo like I posted in the chat. And once Michael has updated the Genesis files, I think you also need to update the geth image to point to the latest one because it has the embedded geth Genesis as well. Once you have that set up, all you have to do is just run, dock and compose up, and it should start. Both Geth and Prism connect to the boot nodes we have set up for both execution and consensus and should be good to go. This would take a while because I think we started the devnet over a day ago.
00:06:50.414 - 00:07:53.570, Speaker B: So it would take like a couple minutes, not hours, to sync it. And then once that's synced in, I created this really handy script called blob utils. It basically lets you upload and download blobs pretty easily. So let me post that in the chat as well. You can kind of use that to interact with blobs in the network pretty easily. So for example, if you want to upload a blob file, or rather to send a blob transaction, kind of use this command, give it like your geth URL, a blob file private key. Hopefully everyone forgets that one and you can easily send blob transactions to the network.
00:07:53.570 - 00:08:27.840, Speaker B: Same thing you can do it for. Like to download a blob that was sent to the network, use the same tool. It's really self explanatory if you take a look at the help page. Yeah, can't really do that right now because my node is still syncing, but hopefully you get the idea. Yeah. So right now one thing that's kind of missing. We'll get into it later.
00:08:27.840 - 00:08:59.180, Speaker B: While you can interact with the beacon chain network, you can't send any voucher's actions without eth. Right. So feel free to ping me on discord or telegram if you need some eth to get started with. And then I can just send that to you if you want to start sending blob transactions. Yeah, that's pretty much it, yeah. Any questions?
00:09:08.300 - 00:09:46.230, Speaker A: Yes, no questions. But yeah, that's very cool. Thanks for demoing Mophie. Yeah, and I guess you kind of hinted at this, there's a couple of things missing right now. So one of them is obviously there was like a faucet or some less manual way to get eat on the Devnet. I think that would be pretty useful. And then beyond that, sorry, I had a little list of other things.
00:09:46.230 - 00:10:39.188, Speaker A: Yeah, beyond that, I guess if there was a way to automatically use your blob transaction sending tool and kind of span the network or create like a high load of blob transactions, I feel like that would be good to see the nodes processing that to be able to also just verify that when the blobs expire, they're actually taken out of the network and removed kind of from the beacon chain. And then I don't know if it would make sense to have an RPC endpoint as well, so that people who may want to interact with the devnet but not run the whole Docker setup might be able to do that. Yeah. Is there anything else you think?
00:10:39.274 - 00:10:53.008, Speaker B: Yeah, the RPC endpoint. I think that's something we can definitely add in in a day or two. We just need to harden it a little bit on our optimism side to make sure that users don't spam the network.
00:10:53.204 - 00:11:20.980, Speaker A: Awesome. Yeah, that makes sense. And then I'm curious. I know, yeah, there's a bunch of newer folks on the call. Does anyone feel like one of these things, like around either the faucet, kind of a blob spamming transaction tool, just like testing the expiry of blobs. That's something that someone's interested in working on and we can follow up after the call to kind of get that sorted.
00:11:23.960 - 00:11:38.760, Speaker B: Yeah, it's probably something I could look at. Obviously there's a new tool, so I'll have to try it out and I'll have to set up a vm to put Geth and prism.
00:11:41.420 - 00:11:46.940, Speaker A: Was there a thing in particular in that that you were interested in working on or just generally?
00:11:48.160 - 00:12:07.250, Speaker B: No, I just want to help with anything and I'm sort of learning at the same time. So I run a main. Net validator node, but it'd be good to set up a test. Net and while I'm playing with it, I can help out with whatever testing you need.
00:12:07.640 - 00:12:26.676, Speaker A: Cool. Yeah, that sounds great. And we can chat in the discord room as well. I could try my hand with the faucet if that's helpful. Yeah, that would be okay. And I think Georgios is on the call. I think paradigm has a faucet repo that might be open source.
00:12:26.676 - 00:12:42.060, Speaker A: I don't know if that's 100% correct. Yeah, we do not support for networks to the extent that you can give us the e. Okay, cool. Okay, I'll take a look at that. Sweet.
00:12:42.640 - 00:13:18.650, Speaker C: Yeah, I can probably get some nodes running as well. And I could take a look into writing like a utility to just kind of spam transactions or whatever. You also mentioned it might be useful to have an RPC endpoint. So I know Mophi just said that you want to harden it to avoid spam on the network, but is it useful to kind of have an open one that we designate for spam? Maybe that can be useful to just kind of look at node usage and things like that.
00:13:22.140 - 00:13:46.720, Speaker A: I think it's probably best to just, if you're spamming the network, actually run your own instance of the testnet and propagate the transactions that. Yeah, unless Mofi disagrees. That would be my gut feeling, but yeah, rather than rallying it through external RPC.
00:13:47.940 - 00:14:16.968, Speaker B: Yeah, I guess by spam I mean not quite the right word meant to dos. Like our endpoints, it's generally fine. The endpoint, once we have it set up, you can spam it. It's just you will have some DOS protections and if you really want to go that far, then like what Tim said, it's best to just run your own node and send transactions to. Yeah, yeah.
00:14:17.054 - 00:14:17.592, Speaker A: Okay.
00:14:17.726 - 00:14:18.650, Speaker C: Makes sense.
00:14:19.580 - 00:14:35.410, Speaker A: For that blog spammer, should it basically simulate, kind of like a realistic scenario where it's like a roll up, posting blobs to the same contract every time, or it's the same address rather every time? Or should it try to balance between addresses and try to just throw things everywhere to see if it breaks something?
00:14:37.460 - 00:14:38.556, Speaker B: What's the worst case scenario?
00:14:38.588 - 00:14:40.710, Speaker A: I guess on the node side.
00:14:43.000 - 00:14:43.316, Speaker D: I.
00:14:43.338 - 00:15:14.750, Speaker B: Would say try to trigger the so we have limits in place. Like, for example, the number of blobs in the block that can be included in the block, or the size of the blobs that can be included in the block. It would be really useful to be able to push these limits or trigger them. And that's one way we could see to make sure that the network is still working even as those limits are being hit.
00:15:17.600 - 00:16:03.642, Speaker A: And I feel in terms of, yeah, that's probably the main thing. If you want to make it like a tad more realistic, it's like there's probably going to be a handful of different contracts that mostly interact with the blobs. If you think of l one, right, there's like, call it maybe five to ten roll ups. There's not 100, but there's also not just one. So that's maybe the order to aim for. But I think it's also fine for a v one to just hit the limit on one contract, make sure that works, and then if it does, you can kind of scale up from there. I'll post this here.
00:16:03.642 - 00:17:18.050, Speaker A: I'm not sure how helpful this is for the spamming stuff, but Marius from the get team has a bunch of fuzzing repos and transaction sending repos. I know that they don't work with blob transactions. I don't know if it's easiest to extend something like that, or just to write something from scratch, in case that's helpful. Sweet. Yeah. And I think, yeah, I think at a high level, if we could get the faucet up just like some spam on the network, an RPC endpoint, and then a couple of people running the devnet, and also trying to look at the kind of blobs expiring and making sure that works, that's really valuable. I think the other part that's maybe not as urgent, but if we have a second devnet of that working towards documenting things better, and that can be as simple as if you're playing around with this and you're in Mophie's hackmd and something is wrong or could be better documented, just like gradually adding to it.
00:17:18.050 - 00:17:36.760, Speaker A: That's always helpful because you hit a bunch of edge cases when you have different people running this stuff. And hopefully the Devnet V two is a bit more accessible, where you don't need to be like a protocol level developer, but maybe you're just like an app developer and you're able to use it smoothly.
00:17:40.380 - 00:17:46.780, Speaker D: Maybe I missed this, but is the blob expiry set to something pretty low? On the devnet, so it's easily testable.
00:17:47.520 - 00:17:50.056, Speaker A: I think it's a day. Is that right, Mofia?
00:17:50.168 - 00:17:51.132, Speaker B: Yeah, that's right.
00:17:51.266 - 00:18:07.220, Speaker A: Okay, cool. Yeah, sweet, I guess. Any other questions, thoughts, comments?
00:18:09.980 - 00:18:31.710, Speaker C: Yeah, I have a bit of a new question about getting the devnet running. Should I just follow the same sort of hardware guides for eth mainnet for disk size, number of cpus, Ram, things like that? Or can I go lower?
00:18:35.380 - 00:18:48.980, Speaker B: You can go lower. I think particularly the disk requirements will be much lower. Yeah, but I imagine though, as people start spamming things, things can get quite gnarly.
00:18:50.920 - 00:18:59.160, Speaker C: Okay, cool. I've got a few servers on Hetzner that are bare metal and have a fair amount of resources.
00:18:59.500 - 00:19:14.380, Speaker A: Okay, cool, thanks. Yeah, that's actually a really good question. Maybe we can just add something in the hack MD as well for just like a recommended minimum processing and storage.
00:19:17.280 - 00:19:36.784, Speaker C: I can take on another task, which is just to sort of monitor the node and try to get like a baseline of resource usage. And we can maybe publish that as like a first step for how much you actually need for ERP 4844?
00:19:36.982 - 00:20:13.470, Speaker A: Yeah, that would be great. Actually, Tim, it seems like it'd be useful to get some of the l two s to be running on this so we can see actually have some real roll ups. Yes. I think Frodo is not on this call. He said he was planning to look into doing this. On the optimism side, I believe they still needed some changes to bedrock to make it work. I can also send it around to the other LT teams to see if some of them have the bandwidth to deploy on it quickly.
00:20:13.470 - 00:20:45.290, Speaker A: No, that's a good point, I guess. Especially if we have an RPC set up, then it's much easier for them to don't even have to run the devnets. Basically they just have to deploy their smart contracts and. Yeah. Anything else on the devnet itself?
00:20:47.980 - 00:20:50.200, Speaker B: Are there plans for an RPC setup?
00:20:50.860 - 00:21:05.170, Speaker A: Yeah, Mophie said he was going to work on one. Yeah. Okay, great. Is that a typo for the 20 gigs, Mophie, or is that actually 20 gigs?
00:21:05.330 - 00:21:07.042, Speaker B: It's actually 20 gigs.
00:21:07.186 - 00:21:07.880, Speaker A: Okay.
00:21:10.510 - 00:21:13.260, Speaker B: I imagine we'll probably bump that up pretty soon.
00:21:19.470 - 00:21:21.130, Speaker A: Anything else on the devnet?
00:21:26.520 - 00:21:27.830, Speaker B: I think that's it.
00:21:29.800 - 00:22:05.970, Speaker A: Okay, sounds good. Okay, next up, I guess the thing I wanted to chat about was light client, your pr about the fee market. I know Proto had left a bunch of comments on it. I'm curious. You have a feeling basically of where we've landed and whether we can go ahead and merge this or is there still some work to be done.
00:22:07.460 - 00:23:11.780, Speaker E: So my understanding with the disagreement on the fee market is it's a question of are we targeting more of a long term number of blobs? Or I guess they're both going to target a long term number of blobs, but one is going to target the long term number of blobs in a much slower way, whereas the 1559 mechanism is going to very quickly change the price of the blobs to reach that amount. And I think that the original idea was that we didn't want to use the 1559 mechanism here again, because of how quickly it was moving within just a handful of blocks. So on that front, I don't know if anyone has any other thoughts related to it. I don't think that this PR is changing the fee mechanism from what it was. It's really only removing the fee mechanism from the state contract.
00:23:12.200 - 00:23:12.660, Speaker A: Right.
00:23:12.730 - 00:23:17.640, Speaker E: That was the original goal of the PR, is to not modify anything but just take it out of the state contract.
00:23:17.980 - 00:23:33.340, Speaker A: Yeah, my recollection is like we were all on the same page about that. But then it probably makes sense to not merge this now if we want to make further changes.
00:23:33.490 - 00:23:52.770, Speaker E: Is that mean I'm on board with making this change because I think the change is only removing out the state contract, which everybody except for Vitalik is ok with. And we should separately modify the fee mechanism if we.
00:23:54.100 - 00:24:00.150, Speaker A: I think. Yeah, I think that makes sense. Does anyone else have thoughts on this?
00:24:00.760 - 00:24:19.150, Speaker F: Yeah, also just briefly wanted to give my plus one to that as well. I think it makes a lot of sense just because kind of having it in the state is just this extra source of complexity. And even if we going to change it in the future, that mechanism itself, I don't see why it makes sense to wait into bundle. So I think having them separate is the right way to.
00:24:21.840 - 00:25:09.980, Speaker A: Then. Okay, so let's do that. And then we were going to discuss this on the CL call last week, but we got busy with the merge and maybe we can bump it to next week's call and see if there's some more insights there. Because I think, yeah, the core of proto themes was like, is it better for the nodes to receive short bursts of blobs or just like a more constant stream of them? And we wanted CL teams feedback for that. Question for Matt, in that you mentioned that the reason you'd choose one over the other is that one results in a slower adjustment. Can't we get that from the 1559 mechanism by just tuning the constants so we can adjust how fast it reacts just by tuning constants.
00:25:12.000 - 00:25:17.660, Speaker E: I think that we should be able to also achieve it by tuning the constants.
00:25:22.560 - 00:25:51.316, Speaker F: I mean, I think for now, this is unfortunately kind of the distinction between the kind of this long run average and the 59 mechanism is always on these calls, a little hand wavy, because I don't think we've fully kind of looked into, rigorously looked into it enough yet. So I think that's definitely the next thing to do. Yeah. I'm also not 100% certain that it's in a place yet where necessarily it's already very helpful to get CL teams feedback on just because again, it's a little bit hand wavy.
00:25:51.348 - 00:25:51.640, Speaker A: Right.
00:25:51.710 - 00:25:56.920, Speaker F: Kind of describing the distinction for now, but yeah, we can talk about offline.
00:25:59.580 - 00:26:34.852, Speaker A: And I think the reason why we did want to reach out to CL teams is there was an argument that maybe actually getting burst of blobs is a bit easier to process because they're easier to process in chunks than in small increments. And I think if there is something there with how the clients work that can at least help cut the design space or narrow the design space a little bit, I don't know. Terrence, do you have any thoughts on. Actually, yeah, Enrico is here as. Yeah, yeah.
00:26:34.906 - 00:26:44.890, Speaker D: I don't have a strong opinion. I need to see some benchmark data or even just play with myself first before I form an opinion for this.
00:26:45.820 - 00:26:48.730, Speaker A: What type of benchmark data would you like to see?
00:26:49.680 - 00:26:57.790, Speaker D: Just the basic, just the compression data and how fast to verify and to validate. Yeah.
00:27:02.530 - 00:27:04.718, Speaker A: Enrico, any thoughts from you?
00:27:04.804 - 00:27:18.880, Speaker G: Yeah, I just jumped into the topic and I need to wrap my head around this. So I need some time to form this kind of question before asking.
00:27:20.210 - 00:27:22.260, Speaker A: Okay, sounds good.
00:27:23.830 - 00:27:24.434, Speaker G: By the way.
00:27:24.472 - 00:27:24.818, Speaker A: Sorry.
00:27:24.904 - 00:27:40.966, Speaker G: Just to add one thing, you mentioned that there is some syncing discussion taking place somewhere in GitHub or somewhere. Do we know where this is taking place so I can catch up?
00:27:41.068 - 00:28:20.502, Speaker A: I don't know that there's a discussion and that was going to be the next thing, but basically we did discuss it in various places and like Terrence wrote a doc summarizing kind of the approaches. I guess we can move to that next. If there's nothing else on the fee market. Just on the fee market. It seems like merging this pr just about moving things to the header instead of the state. We should do that. Unclear about what the right design mechanism is and we probably want to get some more data and potentially some more opinions from client teams to inform that.
00:28:20.502 - 00:28:52.880, Speaker A: But it doesn't seem like the most urgent thing, and getting the verification optimizations in before is probably better. Does that seem roughly right? I'll take this as a yes. Okay. So, yeah, next up, blobsync. Terrence, do you want to actually take a minute? Oh, sorry.
00:28:53.490 - 00:29:15.106, Speaker E: I just wanted to say one last thing on that. If somebody could just go through and give a thumbs up on it so that we feel confident because there was a change in how we're calculating the gas cost for the blob, it should have the same result, but it would be nice to have someone else thumbs.
00:29:15.138 - 00:29:20.022, Speaker A: It up just to make sure I handle that.
00:29:20.156 - 00:29:21.190, Speaker F: That helps.
00:29:22.350 - 00:29:26.458, Speaker E: Okay, thanks. So I'll go ahead and merge it.
00:29:26.464 - 00:29:45.270, Speaker A: Once we get the thumbs up. Sounds good. Sweet. Okay. On the blob sync, Terrence, do you want to take a minute or two and walk us through your doc, either sharing your screen or we can pull it up. I've posted it in the.
00:29:46.920 - 00:29:49.860, Speaker D: So, like, if you just change to open the doc.
00:29:50.300 - 00:29:50.760, Speaker F: Sorry.
00:29:50.830 - 00:29:51.064, Speaker D: Yeah.
00:29:51.102 - 00:29:51.304, Speaker A: Okay.
00:29:51.342 - 00:29:52.244, Speaker D: I am unmuted.
00:29:52.292 - 00:29:52.456, Speaker A: Yeah.
00:29:52.478 - 00:30:18.476, Speaker D: So just open the dog. And I don't think we need to keep this loan so I can quickly go through it. So there's just two approaches we are considering right now. One is you essentially decouple the sidecar and the block. And that's how the spec is right now. So there are two different objects, and one is just tightly coupled.
00:30:18.508 - 00:30:18.656, Speaker A: Right.
00:30:18.678 - 00:30:20.492, Speaker D: So you put a sidecar within the block.
00:30:20.556 - 00:30:20.928, Speaker A: Right.
00:30:21.014 - 00:30:25.824, Speaker D: And there are just essentially pros and cons between each trade off for the coupling.
00:30:25.952 - 00:30:26.630, Speaker A: Right.
00:30:27.080 - 00:30:51.876, Speaker D: If it's not coupled, then it's likely more optimized and more extensible for the spec because for den sharding, we can essentially reuse the same function. And then if it's coupled, then it's more better for the client. I would say so. But honestly, I went through two different approaches here and there. I thought about this sometime, and I don't think the difference is that drastic.
00:30:51.988 - 00:30:52.456, Speaker A: Right.
00:30:52.558 - 00:31:17.936, Speaker D: So if you don't couple it, there's more code on the client side. You have to handle the queue. You basically have to wait until you receive the block before you can process the sidecar and before you can run for choice. And then the changes are not that bad because we kind of do this for attestations already today. Just say today you receive attestation on the beacon chain.
00:31:17.968 - 00:31:18.260, Speaker A: Right.
00:31:18.330 - 00:31:49.310, Speaker D: You can't really process the attestation until you get the block that the attestation is voted for. So it's kind of the similar concept. So I don't foresee that bad of a pushback from the client team. But with that said, right, I do want to give more inputs on the client team because if just my input, it's not enough. There's like four other awesome teams out there. They definitely should voice their opinion. I would say so.
00:31:49.310 - 00:32:06.550, Speaker D: Tldr I think it's okay to not having them together, but it just require more client work. And then I love to give more feedback on the client team side. And that's it.
00:32:11.320 - 00:32:17.270, Speaker A: Got it. Enrico, I assume you haven't had time to form an opinion on this.
00:32:17.820 - 00:32:38.040, Speaker G: Yeah, just to go through it, and I was more thinking about syncing, but I see at the very end of the document you mentioned the blob sidecars by range. So I mean, this one is the change to actually be able to sync.
00:32:38.200 - 00:32:48.092, Speaker D: Yeah. So right now there is request respond, gossip topic. Basically allow your peers to request the sidecars by range.
00:32:48.156 - 00:32:48.576, Speaker A: Right.
00:32:48.678 - 00:33:04.928, Speaker D: So one scenario we can think of is that a node joins, say you've seen the checkpoint sync, whether it's finalized epoch, finalized checkpoint, or with subjectivity checkpoint when he joins.
00:33:05.024 - 00:33:05.668, Speaker A: Right.
00:33:05.834 - 00:33:12.744, Speaker D: And that's usually going to be like t minus a few days or t minus one week, something like that.
00:33:12.782 - 00:33:13.128, Speaker A: Right.
00:33:13.214 - 00:33:18.456, Speaker D: So he needs to basically backtrack and get a blob data.
00:33:18.558 - 00:33:18.968, Speaker A: Right.
00:33:19.054 - 00:33:47.430, Speaker D: But usually it doesn't mean that he has to backtrack and get a block. So that's kind of the asymmetry right there. So if they're coupled together, you can just say, hey, we can just backtrack and just get a block for the last month, easy. But now since they're not coupled, you have to backtrack and get a block without a block. So that's kind of like the little OD part. I mean, it is workable, but that's just something to think about.
00:33:47.880 - 00:33:51.110, Speaker A: Yeah. Right.
00:33:56.340 - 00:34:14.330, Speaker D: So, yeah, on my end, I'm going to also forward this to Paul from Lighthouse and then everyone else just to try to get more feedback on the client side. But I really don't think since everyone's really busy working on the merge, I don't think we'll form like an opinion or decision until maybe shortly after the.
00:34:15.340 - 00:34:21.288, Speaker A: Yeah. Yeah. Fair. Yeah. Edgar? Yeah.
00:34:21.374 - 00:34:53.350, Speaker F: I was just wondering, this could be a stupid question, but would there be any sense in keeping them loosely coupled, basically separate, but creating some sort of new wrapper to just kind of during, kind of while you are at the head of the network, while you're not kind of doing histories or something? They usually come in together, but then for basically further back data, they're still in their separate form. So it's kind of like kind of combining the properties of the two.
00:34:53.880 - 00:35:18.540, Speaker D: Yeah, no, that's what I'm actually doing on the code, but that's kind of, in my opinion, like implementation detail depends on other languages may handle differently. For example, for go, I'm just using interface for it, which is quite nice. So on the code level, they're pretty much treated as the same object. But the point is that you cannot do one thing without the other for the four choice, so you always have to wait.
00:35:18.610 - 00:35:18.892, Speaker A: Right.
00:35:18.946 - 00:35:25.790, Speaker D: So I think that's kind of the debate here, but from the language perspective, people can make it look the same, basically.
00:35:26.240 - 00:35:43.670, Speaker F: I know what I meant was just like though, on the networking level, to basically have some news kind of official kind of structure, something that's kind of like block with blob or something. With blops or something. And then basically you usually request that entire thing, so they just come in together.
00:35:47.880 - 00:35:54.136, Speaker D: Basically a new network object. No, I think that's definitely one option as well that we should probably consider.
00:35:54.238 - 00:37:06.710, Speaker A: Yeah. Anything else on this sync? Okay. And then I guess the last kind of spec level issue was around the verification optimizations. George, I know you've been spending some time looking at that and talking to supernational team. Oh, you have a. We got you. We got you.
00:37:07.160 - 00:37:25.572, Speaker H: Okay. So I talked with the supranational team like two weeks ago or something, and we gave them a list of tasks that we need from. I mean, that's not exactly related to the verification thing. That's more about the KCG library situation.
00:37:25.726 - 00:37:26.284, Speaker A: Right.
00:37:26.402 - 00:38:17.020, Speaker H: But we gave them a list of functionality we need from the library. They came back with us with some timelines and stuff like that. Then we discussed it further and kind of scrutinized the stuff that we already sent them and kind of tried to minimize the work to make it come out as soon as possible so that ideally client teams have a library to work with as soon as possible. So now they're supposed to get back to us this week with a new deliverable list, but I haven't heard from them this week, so I don't have any more precise updates.
00:38:18.240 - 00:38:52.450, Speaker A: Got it. I assume no one else has updates on this. Okay. And then I guess on the actual optimization side. So I think the gist of the issue there was like Mophie, you implemented some of the original optimizations that were added to the spec, and then they were not actually as performant as expected. And I know there was some back and forth about that in the discord, but I'm curious, what was the status?
00:38:55.830 - 00:39:47.762, Speaker B: Yeah, basically, I think George pointed to. Basically. So the crux of the, I guess, performance problem was there were two major operations that I think we can optimize when computing aggregate proofs, and one of them is the modular inverse. We do a lot of them when evaluating the polynomials. And George pointed out that we should be batch running modular inverses and helpfully pointing out to a resource in kilch where this could be done. I haven't taken a look at this, but this week, that'll be my main thing to get back into that. Related to that.
00:39:47.762 - 00:40:14.650, Speaker B: This though, one other thing that kind of like want to bring out is that we also noticed that running the SSE roots on the fiat premiere challenges is expensive. Would it make sense to use a different method of computing those challenges for the polynomial evaluation?
00:40:15.150 - 00:40:55.370, Speaker H: Yeah, I think we really don't need the entire Merkle tree situation that SSZ has to get security here. So ideally, we would just switch that entire hashing thing to just use like straight up a basic hash function instead of computing like, crazy Merkel trees. I guess that also has. We have to change the spec, I guess, to be able to do that. I haven't done it yet, but you're right that this is probably also useless time drain.
00:40:56.270 - 00:41:20.740, Speaker B: Right. Okay. So, yeah, I will also look into that. I guess we can discuss the details offline, because I think before we should change the spec, we should take a look at a couple of hash functions and see what works performance wise and without sacrificing security, and then we can go ahead.
00:41:23.750 - 00:41:43.720, Speaker H: Yeah, that makes sense. I mean, my basic intuition would be to just use the underlying hash function that the miracle tree uses. But instead of doing the crazy tree thing, just like straight up hash. The value, however. Yeah, we should talk about it offline and we can figure it out. That's a good point. I actually forgot about this.
00:41:44.090 - 00:42:05.600, Speaker A: Yeah, because that seems like the main blocker. It's like, if we can get that, or I mean, for a next iteration, if we can get that, then we can get some benchmarks. It helps figure out the throughput with regards to blobs, and that might shape the fee market design as well. So, yeah, that seems like a really valuable next step.
00:42:11.910 - 00:42:14.290, Speaker H: Will you be at SBC, morphe?
00:42:20.150 - 00:42:22.210, Speaker B: Wait, SBC what'sBC?
00:42:23.610 - 00:42:30.040, Speaker H: Okay, some conference in Stanford or something in two weeks.
00:42:30.490 - 00:42:39.020, Speaker B: Oh, that. I hadn't had plans to, but I could take a look. Yeah, why not?
00:42:39.790 - 00:42:40.540, Speaker H: Okay.
00:42:47.610 - 00:42:48.214, Speaker A: Oh, great.
00:42:48.252 - 00:42:58.250, Speaker B: It's in the states, so, yeah, I should be able to make this one, just anything international. I'm still trying to get my passport.
00:43:02.750 - 00:43:14.320, Speaker A: If you do plan to go, I think there's a bunch of side events as well. And we can probably. Oh yeah, actually they're listed literally on the top of it, so that's good.
00:43:15.890 - 00:43:16.640, Speaker B: Nice.
00:43:21.590 - 00:43:30.820, Speaker H: I just want to tell you that if you come, we can do some hands on together and figure out more precisely performance stuff.
00:43:32.250 - 00:43:36.120, Speaker B: Yes, I think that'll be really useful. Yeah, I'll try to make it.
00:43:48.120 - 00:44:41.186, Speaker A: Sweet. Okay. Yeah, I think that was like the last kind of big, I guess, design level issue in the spec. Is there anything else that people feel is really important to make progress on this that we haven't discussed so far? Okay, that's a good sign. On the KSG side, like I said, they have biweekly calls now, so we don't have to kind of rehash all of them. But it seems like they're getting audits started for the ceremony, both for the spec and the implementation. That's good.
00:44:41.186 - 00:45:16.170, Speaker A: So I don't think we'll be blocked on think. Yeah, just in terms of next steps. Generally we had all these tasks about the existing. So yeah, if folks can help out with those, that would be valuable. And then that means Mophie can start focusing on this verification optimization. That seems like the main thing we want to unblock now and then, based on how complicated it is to get the fee market. We might want to have a second devnet.
00:45:16.170 - 00:45:35.026, Speaker A: Either we just combine those two into the second devnet, or maybe we launch them separately if the fee market is a big other separate discussion. Just getting the optimizations right seems like the core thing. Anything else? Sorry, yeah, go ahead.
00:45:35.128 - 00:46:26.980, Speaker B: I just wanted to add on to that, like an interest of progress. Really appreciate if we could get more feedback on light clients pr so that we can merge that since possible. And I know there's some things in the market that probably can still hash out a bit, but I'm just thinking like from an implementer's point of view, the bulk of the work is getting consensus on, I guess, where the fees are being tracked, whether it's in the system address or in the block header. And that's like the most important aspect to me as an implementer. And if we could get that merged, then we can iterate on the actual fee mechanism later. On the fee market later.
00:46:27.430 - 00:46:36.580, Speaker A: Yeah, I don't think there's any pushback on the headers, but I think if ANzgar can review that, give a thumbs up, then we can merge it probably sometime next week.
00:46:39.130 - 00:46:57.580, Speaker F: By the way, that's one of the nice things about the fee market, at least that it's purely a theoretical kind of question. And so once we have a kind of a final decision there, it should be really lightweight in either way on the implementers. This should not be at all one of the challenges for implementers. Whatever we land on.
00:47:08.630 - 00:47:11.220, Speaker A: Anything else anyone wants to talk about?
00:47:17.240 - 00:47:32.410, Speaker C: In the last 4844 meeting we had brought up that test coverage on both Geth and prism needed some work. Is that something that's still either in progress or active or needs help with?
00:47:32.860 - 00:48:31.576, Speaker D: Yeah, I can give an update on that since the last meeting. A few days after the last meeting I managed to sync Mophie's awesome repo with our latest develop branch. So it took me like a while actually to sync that, just because there's so many conflicts. So I finally finished that and as of our update, so this Friday will release our v three will release our v three release. So that means that there will not be any code changes unless critical bug is found post merge. So I think after this Friday our code should be in a very stable place. And then so after this Friday, maybe over the weekend I will resync again just to make sure the code is in the latest state, and after I'm finished that I will ping you or anyone else that's interested to contribute.
00:48:31.576 - 00:48:43.410, Speaker D: So therefore then we can start adding more unit tests. So sorry it's hard right now or the last few weeks just because there's so many moving pieces. And yeah, I think we should be in a much better state coming soon.
00:48:44.500 - 00:49:04.120, Speaker C: Okay, awesome, thanks. I had one other comment about some of the perf stuff. I don't have a ton of context, but just a quick suggestion or question I guess. Are folks using flame graphs to measure performance? Or how is that process working? Feel free to say talk offline or whatever.
00:49:04.190 - 00:49:27.730, Speaker D: We typically do that on running process. So yeah, we definitely do that. But I would say we'll do unit tests, we'll make sure production performance. So we'll run the node and we'll run flame graphs just to see the latency, just for the traces as well. We do everything, so definitely anything you can help, feel free to take it.
00:49:29.140 - 00:49:30.290, Speaker C: Cool, thanks.
00:49:41.350 - 00:50:48.970, Speaker A: Anything else? Okay, I guess last thing before we wrap up, does it make sense to already schedule another call or should we wait a bit? And the reason for waiting is like about a month from now the merge is scheduled to happen on main net, so I feel like we can maybe schedule something today, but there's a world where we just cancel it if it's really close to the merge. So I don't know, what do people prefer? And is it also worth waiting till we've had time to actually talk with the CL teams, or should we just schedule something optimistically and if the merge happens on that day, we scrap it? The people have a preference. Okay. Yeah. I like that. Yeah. Let's wait after the next Cl call, see if we've had time to discuss see if we get time to discuss any of those issues on the Cl call, and then we can potentially schedule something after that.
00:50:48.970 - 00:51:10.636, Speaker A: Cool. Anything else before we wrap up? Okay. Well, yeah, thanks for everyone, and, yeah, talk to you all on the discord. Have a good one. Thanks. Bye.
00:51:10.668 - 00:51:12.310, Speaker C: Thanks, everyone. See ya.
