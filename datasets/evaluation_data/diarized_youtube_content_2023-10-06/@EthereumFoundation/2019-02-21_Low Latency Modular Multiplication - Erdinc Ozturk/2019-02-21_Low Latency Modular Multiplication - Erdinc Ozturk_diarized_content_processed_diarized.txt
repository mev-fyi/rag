00:00:01.450 - 00:00:02.000, Speaker A: Okay.
00:00:03.490 - 00:00:07.018, Speaker B: Hello everybody. Simon, please stop when it's disconnected.
00:00:07.114 - 00:00:07.790, Speaker A: Yep.
00:00:08.610 - 00:00:45.020, Speaker B: So I'm going to sorry for not being able to show up. I had family issues. But I'll try to present from here and I'll try to talk slowly. So I'm going to talk about our low latency modular multiplication design. It which is something we came up with specially for VDF. So my presentation, first I'm going to talk about motivation and then I'm going to talk about large integer arithmetics, the implementation of large integer arithmetics. And then I'm going to talk about the overall architecture of our design.
00:00:45.020 - 00:01:41.920, Speaker B: So, challenge is design a modern multiplication architecture with the lowest possible latency. And it wasn't feasible for most applications before as time area increases significantly before it was, for example, for RSA and for ECC, maybe for other applications it was a throughput game. Throughput was more important than latency. Before the lowest latency was not that important. And as I stated here, time area increases significantly for the lowest possible latency for a single large modular multiplication. It wasn't done before, but now we have VDF. The aim is to get the fastest squaring, let's say fastest modular squaring possible.
00:01:41.920 - 00:02:17.190, Speaker B: And the area does not matter. So that was the motivation for this design. And the advantage of this design is that our architecture will be used in exponentiation with a very high degree. So single squaring is not getting the full reduction on the single squaring is not important. The exponentiation is important. So we can use laser reduction and full intermediate reduction is not required throughout the process. It's going to be done trillions of times.
00:02:17.190 - 00:02:47.854, Speaker B: So we can implement laser reduction. Without laser reduction, our design would not work as fast. So I'm going to try to walk you through the design. I'm going to try to walk you through the complications of the large integer arithmetic in hardware. I will try to keep everything very simple. So that's why I have this algorithm in my slide, the schoolbook multiplication algorithm. It's very basic.
00:02:47.854 - 00:03:33.120, Speaker B: It's a classical multiplication. What we do is we represent each number that's constructed using digits. So here in this example, a number is represented using k bit digits. And when we multiply this number without using karasuba or anything complicated, just classic school multiplication, this is our algorithm. And as I said before, the reason I'm showing this algorithm is show you the complication of constructing a circuit that has very low latency that's as parallel as possible. And this is the picture of the schoolbook multiplication example. We're just multiplying two digit numbers.
00:03:33.120 - 00:04:04.742, Speaker B: So we have A and B. We're multiplying A with B. We're representing A as a one and a zero and B as b one and B zero. And when we multiply them, for example, a zero and B zero are k bit numbers. And when we multiply a zero with B zero, the result is a two k bit number. And what we do is in the end, when we're done with those four multiplications, four inner multiplications of the digits, we just accumulate them with respect to their weights. And their weights are shown on the picture.
00:04:04.742 - 00:05:01.070, Speaker B: So in the end, when we multiply two k bit numbers, we just get a 4K bit number. And again, as I said, this is just trying to show the complexity of large degree multiplications in hardware without any optimization, without carousel or anything. If we did multiplication of large numbers, we had to do it this way. And now modular reduction, we multiplied two k bit numbers. We have a 4K bit number, but now we have to take modular induction. Now we have two major modular reduction algorithms, barrett modular reduction and Montgomery reduction. Here in this picture, without getting into the details of the algorithm, I just want to show you how Barrett modular reduction works in hardware.
00:05:01.070 - 00:05:58.174, Speaker B: So we have the number C, and we're representing that number C with three different sections, high section, low section and the middle section. This is again an example. This can be built different ways, but since in this hardware we're representing, we are using the sorry, we're reducing k bits at a time. This is why I showed you this picture. Sorry, something wrong? No, you can go ahead. So what we do is we reduce k bits from the top, we multiply ch with the Barrett constant and then we divide divided by two to the k. And then we multiply with the modulus and add it to its corresponding place.
00:05:58.174 - 00:07:00.578, Speaker B: And in the end, the top cable sorry, we subtract it from this corresponding place and in the end the top cables become zero. And if you see from this picture, we don't even touch the lowest cables Cr. So we do the reduction of the high part independent, we do the reduction of the high Cabid independent from the low cabins, the lowest cable Cr, C middle, we just accumulate the result to C middle. And if you look at Montgomery reduction, it's doing the reduction from bottom to top. So we multiply the lowest k bit with the Montgomery constant and we take modulo two k of that number and multiply that number, which is t three with the modulus and we add that to those corresponding place. And when we do this addition, in the end the bottom cHBIs becomes zero. So this is how Montgomery reduction works.
00:07:00.578 - 00:07:41.438, Speaker B: And as you can see from this picture, what happens is that the sub k bit ch is not touched in this picture. So when we do Montgomery, we're not even touching the highest KB. And when we're doing Barrett reduction, as seen here, we're not touching the lowest k bit. So these two reductions can happen in parallel, as shown here. What's happening here. Again, so we multiply th with the Barrett constant and then we get the result. We divide the result by two to the k multiply with the modulus and subtract it from this corresponding place.
00:07:41.438 - 00:08:39.650, Speaker B: And at the same time in parallel, we multiply the lowest k bits with one primary constant, take modulo two to the k and then multiply it with M and add it to its corresponding place. And in the end we get a two k bit number in the middle. And of course it's in Montgomery space. The result is actually T times R to the minus one module M, where R is equal to two to the k and which needs to be compensated at the very end of the exponentiation operation. So this is a high level explanation of our design. So this is a high level, let's say optimization. Now I want to talk about the low level optimization, which is more important, which gives opportunity for more and more parallel architecture.
00:08:39.650 - 00:09:30.660, Speaker B: So the idea is if you do, you can do school math multiplication, schoolbook multiplication, Montgomery reduction, bear reduction algorithms on polynomials. So can be applied to polynomial arithmetic. When we do polynomial arithmetic, there is no carry propagation between coefficients, so everything is added to its corresponding place. But in our architecture, with our representation, we only propagate the carry to immediate neighbors. And this is the algorithm for polynomial arithmetic, schoolbook multiplication algorithm for polynomials. It's very similar to a classical multiplication. Instead of see, what we do is we have two numbers, a and b.
00:09:30.660 - 00:10:13.230, Speaker B: We represent a using as a polynomial and we represent integer b as a polynomial. And it's eight AI from I is equal to zero to k minus one. It's AI times x to the I and same thing for b. And here you can think of x as any. For example, the coefficient length of the polynomial is 16 bit. X here can be two to the 16, basically. But we represent each digit as a coefficient of a polynomial and we don't intermix between the coefficients, just like in polynomial multiplication.
00:10:13.230 - 00:11:10.606, Speaker B: So all we do is we do inner product of the coefficients of a and b and we sum them up. And in the end, when we multiply two k minus k minus one degree polynomials, we have two k minus two degree polynomial. So how and why we use this idea, let's say we use this representation well to reduce critical path. If you look at let's go back to the school book multiplication. If you look at this picture, when we accumulate these numbers together, we have 4K bits in the critical path. In the end, at the very end we can do the additions in carry save arithmetic, but at the very end we have to have a rip of carry addition because we need to have a single number. So we have here a 4K bit critical path.
00:11:10.606 - 00:12:18.162, Speaker B: And when we're doing a two k bit by two k bit multiplication, and when we do the final final addition at the end, our critical path can grow up to 4K bit which is very huge for hardware, it's not desirable for a parallel architecture. So what we do is, in polynomial representation, we split up the addition, the accumulation part, the compression part into length. So now this is the picture, what I want to talk about the most. So we do the same thing. We divide the numbers A and B into digits, a one, A zero, B one, B zero. Now they are coefficients of polynomials, okay? Now when we do a zero times b zero, it should have been added just to the lane zero, to the C zero and a one b zero should have been added to C one. A zero b one should have been added to C one and a one b one should have been added to c zero in polynomial arithmetic.
00:12:18.162 - 00:12:49.940, Speaker B: But the thing is this approach, with this approach, the numbers grow a lot. So next time when I get this number, let's say I did the reduction on this next time the coefficients are going to become close to 36, 37 bits. In this approach, we don't want something like this. So coefficients have to stay. So let's say I pick my coefficient length to be 16 bit. The coefficient has to stay around 16. It cannot grow any longer than that.
00:12:49.940 - 00:13:52.006, Speaker B: So what we do here is when we do a zero times b zero, let's say a zero is a 16 bit number and B zero is a 16 bit number, a zero times b zero is a 32 bit number. And instead of adding that entire accumulating, that a zero times b zero to c zero, immediately what we do is we split that up into two portions and we represent a zero times b zero as a polynomial also. So we get the lowest 16 bits and accumulate that to C zero and we get the highest whatever remaining bits. In this case, 16 bits. But it can grow to 18 bits and accumulate to C one. And same thing for A one times B zero and same thing for a zero times B one and same thing for a one b one. So in the end, instead of getting a degree two polynomial, we have a degree three polynomial in this picture, but it's a zero b zero.
00:13:52.006 - 00:14:42.840, Speaker B: The multiplication itself can be done in parallel the accumulation, so the compression of the result can be done in parallel. But as I said, the result of a zero, b zero, which is a 32 bit number, is split up between coefficient c zero and C one. But the thing is, there is no carry change across the blue line. Everything is added. You can think of those blue lines as lanes and everything is accumulated inside those lanes. At first there is no carry chain carry propagation across those blue lines. So what happens here is that for this picture, c one becomes an 18 bit number.
00:14:42.840 - 00:16:05.122, Speaker B: It grows from 16 bits to 18 bits and when we have more, for example, if this was a four x four multiplication, a four degree polynomial times a four degree polynomial, then c one was going to grow, sorry, maybe c two or c three, they were going to grow to maybe 1720 bits. And for a two k by two k multiplication, if we use 16 bit coefficients, they're going to grow to 26 27 bit. So now what happens is that when we have these results, c zero, C one, C two, c three, the compression is done in parallel. We do one step carry propagation, just a single one, just single step correction and the correction happens to its immediate neighbor. So the c zero is 16 bit. Okay? We don't touch C zero, c one becomes in this picture, 18 bits. What we do is we propagate the top two bits of c one to c two, the lowest 16 bits of C two and same thing, the top two bits of C two, lowest 16 bits of C three, et cetera, just a single cherry propagation at a single level.
00:16:05.122 - 00:17:08.230, Speaker B: And in the end the coefficients become 17 bits each. So our intermediate result can stay at 17 bits, not 16 bits. We have redundant representation, we allow our coefficients to grow to 17 bits, but nothing further than that. And everything happens in parallel. Now in the critical path, the ripple cherry adder has a bit length of if you're using a 16 bit coefficients, the ripple cherry addition has a critical path of 16 bits instead of two k bits. So now this is the critical path of optimization and this allows us construct a highly parallel architecture. So now FPGA specifics, well, we started implementing this on an FPGA, and on the FPGA there are dilinks FPGA, there are DSP slices.
00:17:08.230 - 00:18:03.400, Speaker B: And as you can see in this picture, there is a 25 x 18 multiplication inside these DSP slices. This is the reason we started constructing our hardware using 16 bit coefficients. 16 bit coefficients mean that we have to do a 17 x 17 core multiplication each time. And this 25 x 18 multiplication is a sign multiplication. So we can represent our numbers. Our coefficients can stay as signed numbers also, which also actually works well, we need them to be signed numbers because here for Barrett reduction we have to do a subtraction. So the numbers, the intermediate coefficients, they have to be signed numbers.
00:18:03.400 - 00:18:53.160, Speaker B: But this multiplier, this 25 x 18 multiplier is a signed multiplier. So picking a 16 bit coefficient length works perfect for us, for FPGA implementation, not for ASIC implementation. This is strictly speaking, FPGA implementation from now on. So what's in the critical path if we have a two k bit integer and if we represent it as a 128 degree polynomial with 16 bit coefficients in the critical path, we have 317 by 17 core multipliers and two of them are constant multipliers. Because we're working with the known modulus. So the second core multiplier is multiplying with the Barrett and Montgomery constant. Well, they are known numbers.
00:18:53.160 - 00:19:51.618, Speaker B: And the third core multiplier constant, multiplier is multiply with the modulus again, which is known numbers. And we have compressors, 192 to one compressor and 26 bit compressor, which means in the end, we have a ripple carry addition of sorry, not 2017, but ripple carry addition. But the entire compression happens from 192 to two and then a single ripple carry addition. So, yeah, this is the critical path of an example design. So the level of redundancy, if we use 16 bit coefficients, the intermediate result can grow to two k plus one to an eight bit with a 17 bit triple carry addition. For example, take eight bit coefficients. Now, we need to have nine by nine for multipliers, and the intermediate result can grow to two k plus 256 bits.
00:19:51.618 - 00:20:47.260, Speaker B: But the critical path the ripple carry for the ripple carry addition goes from 17 to nine bit. And for four bit coefficients, the intermediate result grows to two k plus 512 and five bit RCA. So we have to find the most valid circuit, while the lowest number is not always the best number, because there is a lot of wiring, a lot of routing, a lot of lacing of these architectures. So we kind of have to do a design space exploration and find the best possible candidates. Just to give you an idea of what this is, I implemented a 192 to one compressor, 26 bit compressor on Vertex. Seven FPGA Vivado tools are given a 4.2 nanosecond critical path, which is pretty fast.
00:20:47.260 - 00:20:55.280, Speaker B: So, yeah, I guess this is it. Any questions?
00:21:01.060 - 00:21:01.424, Speaker A: Thanks.
00:21:01.462 - 00:21:16.930, Speaker B: Erdis, okay, my question is, does he feel that there's a lot of fat in his protocol and how low can he go?
00:21:17.300 - 00:21:21.620, Speaker A: So Ernie, do you feel like there's fat in your protocol? And how much lower do you think it could get?
00:21:21.690 - 00:21:24.416, Speaker B: Potentially fat in my protocol?
00:21:24.528 - 00:21:25.910, Speaker A: Yeah, let's say.
00:21:28.440 - 00:21:35.032, Speaker B: Sorry, slack in my protocol. Okay, sorry.
00:21:35.166 - 00:21:36.680, Speaker A: How much room for improvement?
00:21:38.060 - 00:22:34.840, Speaker B: See, the room for improvement is here in this picture. I mean, one thing is we need to get the best possible 192 to one compressor here, so the room for improvement is there. So instead of saying sorry, 190 to one, we need to do a six to four to one, maybe compressor, and then use it in the overall architecture. And another thing is get the best possible 17 x 17 core multiplier. And the thing is, yes, on the FPGA, we're using the DSP slices. But the thing is, the DSP slices are giving us a single output, it's not giving us the output from the Tierc arithmetic, it's doing the ripple theory addition at the output itself in ASIC. What we can do is we can build a 17 x 17, for example, core multiplier, but it can spit out two redundant results instead of one, which saves a lot from the critical path.
00:22:34.840 - 00:23:14.500, Speaker B: So there is a lot of room for improvement on aging. The high level algorithm, we're talking about classical. We're saying that we're going to use classical schoolbook algorithms. Yes, there are other algorithms that can be tried. Karasuba and other things. But the thing is, for example, Karasuba, the idea of Karasuba reduce the number of multiplications and increase the number of additions. But here in the critical path, we have a single core multiplier and we have like, maybe thousands of these multipliers.
00:23:14.500 - 00:24:06.340, Speaker B: So all we're going to do with Karasuba is increase the level of compassion here. So instead of 190 to one, maybe it's going to increase to 300 something to one compression. So it's going to increase the critical path. It's not going to help us. So maybe what we can do is we can increase the number of multipliers. We can have redundant multipliers to reduce complexity that we get from wiring so that we get this compression part as fast as possible. And another thing is in ASIC we need to try these three and see which one is the best, which one gives us the best routing, best critical path in the end, best latency.
00:24:10.490 - 00:24:10.998, Speaker A: Okay.
00:24:11.084 - 00:24:12.374, Speaker B: Does that answer your question?
00:24:12.492 - 00:24:23.720, Speaker A: Yep, I think so. Other questions. Okay, I think that's it. Thanks, Erin. It.
00:24:29.610 - 00:24:34.720, Speaker B: Enjoy, guys. Speaking.
