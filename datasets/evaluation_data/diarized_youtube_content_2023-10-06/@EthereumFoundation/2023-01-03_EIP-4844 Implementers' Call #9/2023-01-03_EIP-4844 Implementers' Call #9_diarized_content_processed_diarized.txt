00:00:28.390 - 00:00:54.194, Speaker A: Okay. We're close to holidays. I don't know how much bigger of a crew we're going to have. So I think we should go ahead and get started. Okay. This is issue six eight nine on the pm repo. We'll talk about some lingering spec items, which I think are very near.
00:00:54.194 - 00:01:48.100, Speaker A: Just clicking the merge button. Devnet three updates. I'm not sure who's going to give us the update, but someone can step up. George is going to give you a quick on the large block span test, quickly revisit the pre compiled benchmarking and then if there's anything to hit on the readiness checklist as we move into the new year. Okay, so we had discussed this many times on how to handle unavailable data outside of the prune window. There is a PR up 3169 that will be merged today, which I believe reflects the general consensus on this both on this call and on the consensus layer call. If anyone has any final comments, please say so now or jump into the issue really in the next hour or so because it's time.
00:01:48.100 - 00:02:39.300, Speaker A: Anything on this one? Okay, great. The other one also is kind of a last call in that how to handle this edge case where in certain contexts you might not be able to get a sidecar, whether that's past the prune window or outside of the fork depth or whatever it may be. And the general agreement here was to have a particular error code for the resource not being available. And then you can try on the non unified beacon block and blobsidecar. This was by coupling. This was a known edge case that we're going to have to work through. And there's a PR 3154 with this.
00:02:39.300 - 00:02:55.960, Speaker A: This is also in the state where it's going to be merged very soon today. So are there any comments on this or otherwise? If you don't have them here, jump in. Really in the next hour or two. Any comment?
00:02:57.660 - 00:03:00.590, Speaker B: Sorry, what was the PR number again?
00:03:01.840 - 00:03:07.390, Speaker A: This is 3154. I'm going to drop it right here.
00:03:16.100 - 00:03:16.850, Speaker B: Thanks.
00:03:17.620 - 00:03:57.378, Speaker A: Yeah, great. Okay. And then I did open up an issue that I shared. I wish I had noticed this earlier, but just where we're doing this data availability check in the spec is a bit strange with how the spec has been designed thus far. It kind of brings this like hidden and should be cached input to the state transition function. Or hidden. Or hidden is maybe not the right word, but an additional input to the state transition function other than the block and even a data availability sampling.
00:03:57.378 - 00:04:51.602, Speaker A: It would be kind of like this weird dynamic call to the network. How things have been designed, it's more appropriate to go and kind of fork choice and be a blocker on getting the tree in place. From an engineering standpoint, things are done probably in various different places and results are cached. For example, you do parts of the state transition function when you're checking gossip, like the proposer signature, you probably cache that, then you go into your fork choice. So the actual implicational engineering, I think is pretty low here. It's more on getting the spec in a slightly more standard place and where this would be tested in the spec. I believe on everyone that works on the spec regularly.
00:04:51.602 - 00:05:49.120, Speaker A: There's general agreement here. Seems like Mikhail kind of agrees here too. But I'll leave this up for discussion. I'm happy to take questions or discuss right here, send you input, but I just want to kind of draw your attention to this, that likely this is going to be shifted around a little bit. Okay, please take a look at this if you are curious or want to weigh in. This is 31 70 on the consensus specs repo are there any other spec updates or spec discussions? Any pressing items on consensus specs? The EIP or engine API?
00:05:54.260 - 00:06:12.840, Speaker C: So I'd be interested in talking a little about an idea Yasik brought up, which was rather than gossiping blobs, could we possibly gossip just blocks and then directly request blobs via RPC once we see the block?
00:06:16.700 - 00:07:43.700, Speaker A: So you can there's a bit of a chicken and egg problem here. If you're only doing the requests on the RPC, then you don't necessarily have the blobs well seeded in the network and you end up kind of just hammering exactly who just sent you the block to get what you just asked for. There are like mixed strategies on gossip networks where you kind of, and we have this in gossip sub kind of implicitly where you, you push to some and you announce to others. So, you know, it might make sense to find a more explicit hybrid strategy here where the push announce ratios may be different, but I think that what you end up with if you have purely only announce on that is that you end up with a much slower and stunted gossip. And if those blocks are getting far ahead of where the sidecars are, you'd be asking people that don't even have what you want and then be searching around for it. Does that make sense in that you kind of need some amount of push to get the network seated here?
00:07:44.390 - 00:08:01.160, Speaker C: Yeah, the way I would think it would have to work is you wouldn't be able to propagate blocks until you had a blob. And then you would always ask whoever just gave you the block for their blob. But I could definitely see that being really slow. But on the other hand.
00:08:03.850 - 00:08:06.440, Speaker A: Sorry, what? Please?
00:08:08.190 - 00:08:25.440, Speaker C: I was going to say, on the other hand, not gossiping blobs would reduce the bandwidth concerns. And then it also generally structurally looks more like what it sounds like full data availability sampling might look like. So I thought it was an interesting idea.
00:08:27.330 - 00:09:05.880, Speaker A: So I guess generally what we're trying to do here is potentially increase the rounds of communication, but reduce or eliminate the amplification factor. Because I know, at least if you're honest, if you've given me the block that you do have the blobs, then I can ask for them. Now, if three other people give me the block because of the amplification factor, I don't ask for them. So then I kind of take some round trip hit communication hit on each step to eliminate the gossip amplification. Is that kind of the strategy here?
00:09:08.090 - 00:09:21.470, Speaker C: Yeah. Mean, that sounds like it could work. I don't really know. I just thought this line of thinking was something that was interesting to explore, I guess. It sounds like it's been explored to some extent.
00:09:23.990 - 00:09:32.846, Speaker D: This is already in part of the spec, right, that we're not supposed to announce the blobs. It's not implemented in Devnet, but it's.
00:09:32.878 - 00:09:34.354, Speaker A: Part of the specification.
00:09:34.402 - 00:09:35.590, Speaker D: As far as I recall.
00:09:39.290 - 00:09:46.550, Speaker C: Right now, we send the blocks with the blobs together. So it's like, I guess announce a line of blocks.
00:09:47.070 - 00:09:53.766, Speaker D: Oh, so this might just be just for the transaction gossiping then, rather than the block gossip.
00:09:53.798 - 00:11:00.634, Speaker A: Yes, there was a trade off on that one, but transaction gossip is arguably less timely. And so the kind of like announcement and the round trip is more, okay, at least can be handled, I think, with a little bit less care than with the block here, I guess. Sean, the other, I think that could work. I think it complicates our gossip rules a little bit, although I think we can kind of shove it in there. I think it's going to make each hop take longer, but it's going to greatly reduce the amplification factor on gossip. I think the other thing to consider here is the episode, because episode attempts to try to reduce the amplification factor of all gossip, but in a more kind of generic way. But I'm certainly willing to kind of put this on the table as we discuss things in January.
00:11:00.634 - 00:11:02.770, Speaker A: It might be worthwhile.
00:11:04.230 - 00:11:08.500, Speaker C: Yeah, I'd be interested in, I don't know, testing it out experimenting a little bit.
00:11:12.130 - 00:11:21.860, Speaker A: Has he or you or anyone opened up an issue? I know this is in more of like a DM chat. No, I don't think so.
00:11:23.510 - 00:11:26.466, Speaker C: Okay, I can open one after the call.
00:11:26.648 - 00:11:56.970, Speaker A: Yeah, that'd be good. Thank you. Any other initial thoughts on that? Not for me. Very cool. Let's take it to an issue and discuss. I think certainly there are probably a few different strategies to reduce the bandwidth here. That would be one episode, would be one, erasure encoding be another.
00:11:56.970 - 00:12:25.000, Speaker A: But let's keep that conversation going in January. Thank you. Any other spec items? Great. Devnet three updates I've not been following this and I'm not sure who on this call has the info, but if you do, please speak up.
00:12:29.150 - 00:13:21.450, Speaker B: Yeah, I guess I can structure this by requesting updates from people that have been working on the various clients. Sounds good. I can start with prism. Terrence already posted his update on the implementers call PM, but basically prism is pretty much up to date with the Devnet. There are a couple issues like what Terrence highlighted blobs by root isn't quite implemented, but I think that's something we can, I guess, update eventually. And it's not necessarily for the Devnet to get it operational because it covers one edge case for the Devnet. But yeah, prism is ready.
00:13:21.450 - 00:13:29.980, Speaker B: And Roberto, how is the I think Aragon you were working on, how is that going?
00:13:30.610 - 00:13:47.540, Speaker D: Yeah, I've made considerable progress. It's not ready yet though. Maybe by the end of the week I'm hitting some pieces of codo that are quite different than geth to set me back a little bit.
00:13:49.350 - 00:14:05.590, Speaker B: Okay. And speaking of Geth, Geth is also pretty much implemented and is Alexi or for liquidly in the call, maybe we can get us an update on Nethermind.
00:14:07.530 - 00:14:31.070, Speaker A: Yeah, we lined our implementation with the latest geth and prism changes and looks like it's working. There are some known issues but not very critical. It still be able to be run in the network.
00:14:33.190 - 00:14:36.930, Speaker B: Awesome. And Sean for Lighthouse.
00:14:37.910 - 00:14:51.400, Speaker C: Yeah, I'm still sorting through a couple sync bugs but I think I identified the problem as of this morning, so going to try to fix that and test it today. Other than that we should be there.
00:14:53.370 - 00:14:58.230, Speaker B: Excellent. Any other client does on the call want to share the updates?
00:15:07.210 - 00:15:22.990, Speaker E: Hi Lord star. So we have been successfully able to interrupt with Gat and as well as generating a transaction and sort of including it in the block mostly we are ready for the interrupt.
00:15:24.610 - 00:15:25.550, Speaker B: Excellent.
00:15:27.810 - 00:15:50.870, Speaker E: And since I think I'm here representing Ethereum js as well. Since Andrew is not here for Ethereum Js as well. We have made considerable progress and we are also able to interrupt for Ethereum Js with Loadstar, and I think Ethereum Js would also be able to join the interrupt.
00:15:53.630 - 00:15:57.450, Speaker B: Great, that's good news. Any other client devs.
00:15:59.310 - 00:15:59.914, Speaker F: Here?
00:16:00.032 - 00:16:03.310, Speaker A: Taco. We are progressing.
00:16:05.410 - 00:16:42.060, Speaker F: On the storage for blob sidecars, and we are progressing on the networking for the RPC methods. So currently what is still missing as important is the sync logic, but everything is kind of progressing and we expect to be able to join some test nets hopefully by the end of January to be ready.
00:16:44.030 - 00:16:49.178, Speaker A: For the meeting. Cool.
00:16:49.264 - 00:17:41.738, Speaker B: Thank you for the update. I think that covers all the client devs. So some extra news about the Devnet. We actually do have a tentative, and I'm very particular with the word tentative because we only have one client combination working, which is geth and prism. We have a Devnet deployed adhering to Devnet V three spec. With those two nodes. I would like to add more clients, particularly Nethermind lighthouse, since they're the closest and I guess lodestar into the Devnet, so that we can be able to test the behaviors of client interop.
00:17:41.738 - 00:17:55.570, Speaker B: I'll post the details of the Devnet configuration and parameters on the EIP 48 44 testing channel, but yeah, hoping to get some more contributions here in the Devnet.
00:17:57.670 - 00:17:59.940, Speaker E: Is the Devnet already running?
00:18:00.950 - 00:18:02.340, Speaker B: Yes, it is.
00:18:02.790 - 00:18:09.000, Speaker E: Okay, so maybe we can try sync up the Devnet locally and that would be a good idea.
00:18:09.690 - 00:18:18.390, Speaker A: Yes. Thank you. Nice. Are you sending blob transactions on this Devnet?
00:18:19.530 - 00:18:40.720, Speaker B: Yes, I am. And yeah, there's like a hack MD user guide similar to the style of Devnet V one, whereby you can interact with the Devnet. We have a bunch of public endpoints exposed so developers can start building some tooling similar to the previous Devnet on top of this one. Yeah, it all works out.
00:18:43.030 - 00:19:13.906, Speaker A: Fantastic. Great work. I have a quick question. I'm looking at this Devnet three doc and looking at the milestones. And then at M three, how do the El plus Cl interrupt test vectors work? I apologize, I'm not familiar with this. This is your repo Mophie. So I guess I'm asking you.
00:19:14.088 - 00:19:56.450, Speaker B: Okay, so that basically we have a suite of tests. It's styled like hive tests, but they're more succinct. For every client implementation, we add that to that repo and execute those tests, and if they're passing, then I guess they're good, at least for the most part. We've already gotten some contributions to get those tests passing for various clients. But we've been running into a couple of issues. Integrating the clients with the interop repo.
00:19:57.910 - 00:20:09.780, Speaker A: Got you. The hard problems, just for my edification, are Gath and prism passing m three.
00:20:12.230 - 00:20:21.910, Speaker B: Gath and prism is not passing all of it. There is one particular test in prism that is not passing, which is historical sync.
00:20:22.570 - 00:20:23.330, Speaker A: Okay.
00:20:23.500 - 00:20:40.490, Speaker B: I suspect the issue is mostly on the client side. It's more of a technical issue than a consensus critical issue, which is why, but for all purposes, prism and geth, they should be working fine for the devnet.
00:20:40.650 - 00:21:11.830, Speaker A: Got you. Okay, thank you. Any other discussion points for Devnet three? Great. Thank you, Muffy. Thank you, everyone. Large block spam test. Do we have a status update here? Maybe Georgios.
00:21:11.830 - 00:22:16.280, Speaker A: Or maybe Georgios is not here. Does anybody have any visibility on this? I know they're going to be running another wave of tests with the additional monitoring up, but I do not know the status of that. Okay, well, we can circle back outside of this call and see how the monitoring worked here, I guess to contextualize the bandwidth reduction proposals, whether it be episode, whether it be some different push pull strategy, I guess these types of tests and simulations will hopefully help inform us as to whether we want to add additional complexity by following one of these bandwidth reduction proposals. Okay, pre compiled benchmarking. Kev, are you still here? I saw that you maybe had to drop out.
00:22:19.210 - 00:22:32.010, Speaker G: Hello? Yeah, I'm still here. Yeah, I wrote in the chat, nevermind. Gave some numbers last week for. It's a lot closer to the original estimates.
00:22:33.550 - 00:22:33.962, Speaker A: And.
00:22:34.016 - 00:22:39.740, Speaker G: Yeah, Nim and the Java client, I think we're still waiting for estimates from them.
00:22:42.210 - 00:22:54.020, Speaker A: Okay. But we're increasingly honing in on that 50 to 60k number. And nothing unexpected on those benches have shown up after we resolved the negative case.
00:22:54.630 - 00:23:00.260, Speaker G: Right. It seems that we just need to optimize the go KZG client a bit more.
00:23:02.230 - 00:23:04.180, Speaker A: Okay. Because that's like 67.
00:23:05.770 - 00:23:14.280, Speaker G: In the best case, it's 67, but the worst case it was going to more than 100.
00:23:14.650 - 00:23:16.390, Speaker A: This is a garbage collection.
00:23:16.890 - 00:23:19.050, Speaker G: Yeah, it was just doing a lot of allocations.
00:23:21.070 - 00:23:25.530, Speaker A: And what's the average for Gokzg?
00:23:29.230 - 00:23:34.400, Speaker G: Yeah, I can't remember what Martin said.
00:23:37.250 - 00:23:44.210, Speaker A: And this is with which library? That's Kilich.
00:23:45.510 - 00:23:49.406, Speaker G: I don't remember what the default one it was using. It might be Killich.
00:23:49.598 - 00:23:50.290, Speaker A: Yeah.
00:23:50.440 - 00:23:51.234, Speaker B: Okay.
00:23:51.432 - 00:23:53.540, Speaker E: But with gnark it would be lower.
00:23:54.070 - 00:23:54.386, Speaker A: Yeah.
00:23:54.408 - 00:24:03.640, Speaker G: With Ganark, I think the worst case was better than Gokazg's best case. So there's like a big difference.
00:24:05.770 - 00:24:19.930, Speaker A: Help me understand. Are there lots of allocations in one call and thus the garbage collection can be kicked in. Or is it because of the benchmarking and repeated calls there kind of ends up being an allocation blow up. That hurts some of these calls.
00:24:20.370 - 00:24:22.560, Speaker G: It seemed like it was in one call.
00:24:22.930 - 00:25:24.910, Speaker A: Okay, but I guess other libraries coming in at the numbers that we expect the signal to fix the library that doesn't rather than to tune to that library. But we can continue this in January. Cool, thank you. Anything else on pre compiled benches? Excellent. The last link that Tim had in here was just linking out to the readiness checklist. I don't have particular items on here. It looks like testing is certainly pretty important thing with respect to hive transaction pool, especially on the execution layer.
00:25:24.910 - 00:25:28.720, Speaker A: Any updates on testing, just in general?
00:25:34.530 - 00:26:47.538, Speaker H: Okay, so I did update the nodes with the progress of Hive testing. So there are some good prs from Mario and I put up some prs. What we did is we added support for go workspaces in Hive. So now we can deduplicate the testnet related code of these hive simulators, meaning that the testnet that basically runs in these simulators, that spawns these clients we can reuse for four and for other future eips. Now aside from the workspaces and the code duplication, we've also been working on some extra features like metric support in Hive. My hope is that eventually we'll have some benchmarking in Hive where we can automate metrics which might be really useful for blobs benchmarking. And I believe Mario is working on the withdrawals testing.
00:26:47.538 - 00:27:13.310, Speaker H: And with that in place, I think we can basically on top of that also implement for testing. There's this sequentiality here where if we're going to test the ancient API of post Shanghai, then we might depend on the withdrawals testing.
00:27:18.420 - 00:27:57.290, Speaker A: Got it. Okay, any questions for proto or any further comments on testing? Okay, anything else on the readiness checklist? I'm going to do a pass on this. For example, the setting, the min gas price is still in there, but that pr is closed. A few things like that.
00:27:57.660 - 00:28:16.284, Speaker I: Hey Danny, can I circle back to testing real quick with a quick question? Do we have an ETA on updated retest cases? I'm specifically working on the KZG pre compile contract and I was wondering if anybody had or if there were plans to start working on ref tests for.
00:28:16.322 - 00:28:16.910, Speaker A: That.
00:28:19.920 - 00:28:23.970, Speaker I: Same question basically extends to all the different types of functionality we need to add.
00:28:25.940 - 00:28:56.720, Speaker A: Yeah, I can circle back on this outside of the call. I know the intention is there. I do not know if it's at the immediate top of anybody's list. Mars, do you have any visibility on that? If you're speaking, we cannot hear you. I'm going to make a note on that and follow up.
00:28:56.870 - 00:28:58.290, Speaker I: No problem. Thank you.
00:29:10.720 - 00:29:45.244, Speaker A: Okay. Anything else on the readiness checklist? Really? Any other items we want to discuss today? Excellent. We will close. Thank you, everyone. Happy holidays. We will reconvene this call, I believe, the first week of January. Yeah, that January 3.
00:29:45.244 - 00:29:56.390, Speaker A: And Tim will again be leading the call. Oh, yes. Thank you for writing notes. Okay, cool. Happy holidays, everyone. Talk to you soon. Take care.
00:29:59.480 - 00:30:00.900, Speaker C: Happy holidays.
00:30:01.560 - 00:30:02.130, Speaker E: Have a real.
