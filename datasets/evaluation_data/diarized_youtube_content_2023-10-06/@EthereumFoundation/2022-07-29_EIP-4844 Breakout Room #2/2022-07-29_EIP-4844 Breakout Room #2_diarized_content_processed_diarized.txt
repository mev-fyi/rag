00:00:00.500 - 00:01:09.310, Speaker A: You okay? We are live. Welcome, everyone, to the second 4844 breakout. I guess the goal for this call is just to kind of get everyone on the same page about the progress on the implementation on the KCG ceremony, and then take some time to chat about what we see as the biggest blockers or issues that we need to address on EIP and also kind of try and list out what are the types of skills that would be helpful to have people contribute to the EIP. So, like yesterday, there were a bunch of people talking on Twitter about how important this is. And I think, yeah, a few people already reached out, but if there's a way we can just better articulate what's needed and what's helpful, I think it'll help filter the different people who'd like to help out. Yeah, that should be it, I guess, to kick it off. Mophie and I don't know if Michael is on the call.
00:01:09.310 - 00:01:18.290, Speaker A: I don't see him, but, yeah, Mophie, do you want to start and give us an update on the implementation and where things are at there?
00:01:25.690 - 00:01:26.920, Speaker B: I can't hear you.
00:01:27.290 - 00:01:29.590, Speaker C: Yeah, you're muted.
00:01:33.410 - 00:01:34.640, Speaker A: Yeah, no worries.
00:01:40.770 - 00:01:42.000, Speaker D: Can you hear me?
00:01:42.610 - 00:01:46.720, Speaker A: Yeah, it's a bit quiet, but we can hear you.
00:01:47.170 - 00:02:40.930, Speaker D: Okay, I'll try to speak louder. Yeah, so I've been working on the implementation. So where are we at? We basically have been working through the spec and implementing. We have like, implementations of the spec for PRIsm and Geth, and we have most of the spec, no doubt. But there are a couple of issues that I guess we'll discuss later during the meeting. Right now, we have an implementation for the law of verification optimization. And this is something that George worked on a couple of weeks ago where we are at NKC groups to speed up the verification of laws.
00:02:45.750 - 00:02:47.860, Speaker A: Yeah, it's a bit.
00:02:50.150 - 00:02:52.214, Speaker D: All right, let me try disableing that.
00:02:52.332 - 00:02:53.590, Speaker A: This is actually perfect.
00:02:53.740 - 00:02:55.222, Speaker E: Whatever you just did worked.
00:02:55.356 - 00:02:56.040, Speaker A: Yeah.
00:02:58.090 - 00:02:59.254, Speaker D: Is it better now?
00:02:59.372 - 00:03:00.780, Speaker A: Yeah, much better.
00:03:01.150 - 00:03:38.774, Speaker D: Okay, I'll take it. All right, we have most of the spec now. Now. And other than the couple open issues that we need to resolve right now, we're working on just optimizing the implementation, making it as fast as possible. And the point of contention there is the KCG blob verification. There's like an open issue where we want to ensure that verifying blobs is in a DOS vector. So there's been some work that's been put into the spec and the implementation to speed that up.
00:03:38.774 - 00:04:03.040, Speaker D: And, yeah, that's mostly where we are right now. And also like a quick prelude to announcement. We are working on a devnet that will be publicly available pretty soon, so looking forward to having external contributors joining the network and testing things out, because that's going to be really needed.
00:04:07.250 - 00:04:08.000, Speaker F: Nice.
00:04:09.170 - 00:04:26.470, Speaker A: Anyone have questions, comments, thoughts on that? I guess. Do you have a link to the repo repos that you and Michael are working off to share here with?
00:04:29.160 - 00:04:32.708, Speaker D: Yeah, yeah, sure. Post that in the zoom.
00:04:32.884 - 00:04:33.610, Speaker A: Awesome.
00:04:37.740 - 00:04:59.330, Speaker G: By the way, Mophie, you asked some questions about the verification code and why it's so slow and all that stuff I tried to answer last week. I hope my answers made sense, but if they didn't, just ask again. Or we can do a call, the two of us, to figure out in more details how to optimize the code.
00:05:00.820 - 00:05:09.030, Speaker D: Yeah, thanks George. I did skim through them, but I haven't looked into it in detail, but should have more time next week to take a closer look at that.
00:05:14.940 - 00:05:15.800, Speaker A: Sweet.
00:05:17.740 - 00:05:22.830, Speaker E: Were those responses like a public doc or anyone can look at?
00:05:26.640 - 00:05:29.260, Speaker A: I think it was in the sharded data chat.
00:05:29.920 - 00:05:31.310, Speaker F: Okay, thanks.
00:05:32.420 - 00:05:33.170, Speaker A: Yeah.
00:05:37.780 - 00:06:34.130, Speaker F: Maybe it's worth briefly saying, because I also put it in chat just now, but because we have this kind of 1559 light mechanism for blobs as well, we have a reasonably good understanding of the kind of frequency with which transactions will come in because the Mem pool can be very small for them as well. So you'd only expect to see one legitimate blob transaction legitimate, meaning that kind of the commitment actually matches the blob sent with it coming in every few seconds. So the verification of the legitimate ones is not a problem at all. Like performance wise, it's really about handling if people spam you with transactions where the blobs just don't match the commitments because then you can't even charge them for it. So it's similar to an invalid signature. So it's really mostly about peer scoring and making sure that you just don't allow one peer to send you multiple of those. So that's the core of the DOS issue.
00:06:38.280 - 00:07:06.348, Speaker A: Got it. And as I understand it though, there's just no peer scoring on the execution layer. Right. There's no easy way. You need to be able to verify them quickly because there's no granularity in the scoring. Either you stay with the peer or you disconnect them. So you could disconnect the peer, but basically after they've tossed you.
00:07:06.348 - 00:07:09.150, Speaker A: And ideally you haven't gone down because of that.
00:07:12.800 - 00:07:13.550, Speaker F: Right.
00:07:15.460 - 00:07:16.370, Speaker A: Go ahead.
00:07:19.460 - 00:08:22.436, Speaker D: Okay, so just related to that, we do have peer scoring in the consensus layer but there's like a weird issue where you sort of have to defer verification of block kcgs in consensus whenever the block headers are not available. Right. And in that point, in that case, if you are deferring blob verification, it's much harder to penalize peers if they do send invalid blobs, because you'd have to keep track of what peer is associated with this blob. And I imagine that complicates the implementation of various consensus clients. At least that's what my experience has been implementing this in prism. If the execution layer is not synced, like you're in an optimistic sync mode or something.
00:08:22.538 - 00:08:22.852, Speaker G: Yeah.
00:08:22.906 - 00:09:11.140, Speaker D: Which header did you mean? I'm referring to the blob sitecar in consensus client. So as that's being gossiped, it is possible that you receive a sidecar that's associated with a beacon block that hasn't been observed yet. And in that case, you want to defer processing of that blob sitecar rather than just simply rejecting it because it might be incorrectly labeled as valid. So if you do defer that processing, then you need to keep track of what peers sent that sidecar in order to penalize it. And that's just one complexity with the implementation.
00:09:12.120 - 00:09:33.550, Speaker A: Got you. Yeah, that makes sense. So obviously we want to make sure that verifying blobs is ideally just like not a DOS vector because it's very impractical on the El and somewhat impractical on the CL to deal with peers based on that.
00:09:34.960 - 00:09:46.720, Speaker D: Well, we do do this on the Cl. If you get an attestation ahead of time, it's the same problem. So it's not impossible. It just probably is more complicated than like an MVP.
00:09:47.620 - 00:10:22.430, Speaker A: Got it. Okay, that's good to know. Okay, I see you have a comment about the Cl sync and the side cars. Does it make sense to discuss this now? Yeah, I guess so, actually, if we're in the implementation stuff. Yeah. Do you want to take a minute, Anskar, and kind of share your thoughts on that?
00:10:23.440 - 00:11:16.700, Speaker F: Right. So I think this is basically just a question that a couple people had when we were discussing this in Paris. So basically, I think for now, the plan is to, as Mofius was saying just now as well, to have this site architecture where basically blobs are more or less gossiped independently from the beacon blocks between CR clients. And that can lead to all these differences where sometimes you get a blob and you haven't actually received, observed the beacon block yet, and the other way around and everything. And there was some concern, I think I only heard that secondhand. I think Proto was saying that some CL client teams had kind of, I'm not sure, maybe could say something to that. But there were people that basically raised some concerns that this might introduce sync complexity.
00:11:16.700 - 00:11:49.336, Speaker F: I'm not sure proto, if you wanted to say like something to that. Basically the conversation that we were having is whether or not it's actually worth introducing this extra complexity. Now, of course the reason people did it in the first place, or came up with this architecture in the first place, is that it's more cleanly forward compatible with full sharding, because basically then we could just drop that whole kind.
00:11:49.358 - 00:11:49.930, Speaker A: Of.
00:11:52.140 - 00:12:33.830, Speaker F: Because in the future we'll have to have blobs and blocks be separated anyway because clients will no longer download all the blobs. So it's cleaner to already have that separation today, but it does front load some of the extra complexity. So if we want to really follow the strict minimum complexity approach for four eight four, there is a case to be made to return to something where basically you bundle the blobs and the blocks after all, so that whenever you receive a blob it comes with a block and the other way around, so there's no extra complexity around having one but not the other and things like that.
00:12:40.920 - 00:12:55.610, Speaker H: I think that's fair. Long term, if thanks, sharding, you may need the separation short term maybe. I think it's up to implementers to make the right call. Mophie, what do you think?
00:12:59.900 - 00:14:23.852, Speaker D: Yeah, I think that bundling them will simplify this implementation, at least for them. One concern I do kind of have is so the advantage of kind of keeping them separate is it makes it easy and quickly to drop invalid sidecards before we even observe them. And what I mean by this is basically if you observe like a beacon block that is invalid and you immediately later receive the associated sidecar, you don't have to do expensive valid, you can just drop it immediately. And if we start bundling things, there is a network cost of transmitting the whole gamut. And therefore I guess you're shifting. There's like a cost involved with always transmitting the entire beacon block and making sure that if it's invalid, then you've already incurred that cost of storing that beacon block momentarily, which includes a sidecar. And yeah, I guess this issue can be solved with appropriate peer scoring.
00:14:23.852 - 00:14:29.110, Speaker D: And maybe this is not a non issue, but that's basically my only concern here with doing this.
00:14:30.920 - 00:14:39.290, Speaker H: And Mophie, the current implementation does already separate the sidecars from the regular blocks, correct?
00:14:40.620 - 00:14:41.370, Speaker D: Yeah.
00:14:41.980 - 00:15:01.390, Speaker H: Okay, so maybe we should not change it for the stability of this definite then, and then take more time to consider whether or not we should merge the two things. I don't see a short term gain into merging them at least.
00:15:05.140 - 00:15:45.710, Speaker A: Yeah, I would tend to agree with that. And I feel like once we have maybe a devnet working and kind of these other spec issues resolved, we can also bring this up on the Cl call. In the meantime, also get Cl devs to look into it, assuming they have time, which is a very generous assumption. But yeah, I agree that if the current version works right now it's not worth refactoring the entire sync. But it's worth noting that there might be a simpler approach and discussing this with CL teams, does that make sense to people?
00:15:47.040 - 00:15:48.590, Speaker D: Yeah, that sounds good.
00:15:59.200 - 00:16:04.820, Speaker A: Any other thoughts, comments on the implementations generally or on sync?
00:16:07.950 - 00:16:57.130, Speaker H: I have one thing to add right now. We have the prism prototype. There's this fork by Mophie in one separate repo, and then there's this other GAF prototype with a fork from Michael Hauch. So we have these two forks of consensus and excretion clients that may have this distance in terms of git differences from the latest merge work. And so if people from these clients are listening, I'd like to hear feedback about incorporating more of the latest merge work and whether or not it's the right time to start rebasing.
00:17:02.470 - 00:17:24.810, Speaker A: Okay. Yeah, I don't think there's anyone from Geth here, and Terrence from Prism told me he could join probably for the second half of the call. So, yeah, when he joins we can maybe ask him about that directly as well. Anything else on implementations?
00:17:35.170 - 00:17:45.070, Speaker D: I don't know if we'll get to this later, but there's the issue of the KCG libraries we have, we are using for the implementation.
00:17:46.710 - 00:17:47.506, Speaker A: You're using what?
00:17:47.528 - 00:18:18.810, Speaker D: Sorry, the KCG libraries. Yeah, we are using for the implementation. Up until the blog verification, we've basically just been using one library in Geth, and now that we also need some of that functionality in prism, the consensus we sort of have to decide what's the best approach to reusing the same functionalities across both implementations.
00:18:22.110 - 00:19:55.820, Speaker G: So on that front, we are in contact with the last national people. I would say that there is progress, but this progress is kind of slow. So we sent them over a month ago, we sent them the requirements for what is needed. They got back to us this week and they told us they're going to send us back some sort of report on what they gathered from what we sent them and that we should do a call next week to figure out next steps. So things are moving and things will probably happen next week. I will report back with what I learned, but also another thing I want to raise on this topic is that it might be a good idea to have some of the more people, more involved in the implementations of these things involved in such future calls with the supranational people to give a better idea of what is needed in term of interface, because I think I know what is needed, but maybe someone who is more involved with the actual stuff can give more insight. So I'm going to let you know next week of what happens, but I might ask for some volunteers to join in future such calls with them to build a better API basically.
00:19:57.870 - 00:20:43.420, Speaker A: Cool. Yeah, that's really useful. I know Amarius from get had mentioned he had some thoughts on that, so he's probably a good person to reach out to to join those, you know, Mophie and Michael here as well. But yeah, on the last one of these calls he seemed to have some pretty strong opinions about it. Yeah. And I guess generally do people feel like blast is like basically the best option is to adapt blast and make that better? Because I believe that's like what all the Cl teams use already. Correct.
00:20:43.420 - 00:20:48.300, Speaker A: But there's no kind of other option really on the table right now.
00:20:52.110 - 00:21:07.874, Speaker F: The interfaces we need are a very thin wrapper around functionality that blast already has. I mean, around functionality that any BLS library will implement already. Since we're all using blast, it makes a lot of sense just to put those in.
00:21:08.072 - 00:21:30.210, Speaker A: Got it. Okay. Anything else on the implementation?
00:21:33.590 - 00:21:34.820, Speaker D: That's all I got.
00:21:36.150 - 00:21:56.830, Speaker A: Cool. Any other questions from folks here? Okay, I guess. Next up, Trent, I see you're here. Do you want to give a quick update on the BLS side of things? Sorry, not BLS, the KZG side of things.
00:21:58.080 - 00:22:29.770, Speaker I: Yeah, I was going to say I could barely cover the KZG set. I definitely can't cover BLS, but yeah, so similar to, since we started this, we're just doing kind of the same stuff. We have an audit coming up for the ceremony, implement or not the implementation, but the design of the ceremony with secbit coming up soon, so we're preparing for that in a few weeks. I just shared a link to a bunch of resources which has.
00:22:31.660 - 00:22:32.168, Speaker A: Linked to.
00:22:32.174 - 00:22:49.820, Speaker I: The implementation or one of the implementations, but specifically the calls. If anybody wants to catch up or is curious how far along we are, that's the main thing that we're preparing for the audit. And we have the next call next week on Thursday, 1130 utc.
00:22:50.880 - 00:22:51.724, Speaker A: Awesome. Yeah.
00:22:51.762 - 00:23:29.128, Speaker I: There's also a timeline doc in there if anybody's curious about when we plan to start this. Hopefully around Devcon, and then we'll have a period of closed contributions before that to test it. And then at Devcon, hopefully we'll have some live contributions from the audience, and then it'll run for a few months. We also have some people starting to work on a couple of test sites. Jeff Lampard's been working on that to make sure all this stuff works. And we've started working on an interface that users will actually interact with in the browser. That should be everything.
00:23:29.128 - 00:23:32.990, Speaker I: Any general questions that I can maybe answer?
00:23:41.420 - 00:23:43.336, Speaker A: Okay, that's it.
00:23:43.518 - 00:23:48.490, Speaker D: Just a quick question. I'm just curious, what is the size of the ceremony that you have in mind?
00:23:49.680 - 00:24:02.960, Speaker I: Number of participants? Yeah, we're hoping for 10,000, which would, depending on who you ask, it would make it the largest trusted setup ceremony.
00:24:09.360 - 00:24:22.640, Speaker A: Nice. Any other questions on the ceremony? Okay, I see Terrence has.
00:24:24.690 - 00:24:25.150, Speaker D: You.
00:24:25.220 - 00:24:26.942, Speaker A: Can you hear us? Terrence, do you have a mic?
00:24:26.996 - 00:24:32.260, Speaker C: Yeah, sorry, I had another meeting, but I am here, so feel free to.
00:24:34.150 - 00:25:36.454, Speaker A: I think there's two things that we discussed that we're curious to get your input on. The first is around the Cl sync. Earlier, we were having a conversation that we've decoupled blobsync from block sync to have it be kind of forward compatible with the full sharding approach, but that might introduce more complexity at the Cl side. And we were thinking that there might be value in potentially just recoupling blobs and blocks at the syncing level for the first version of 4844 and then eventually making the sync more decoupled. But I'm curious generally, do you have any thoughts on that and how much of a simplification it would be to couple them now? And is it valuable to do it, or should we try and front load as much of the sync design as possible? Right.
00:25:36.572 - 00:26:11.598, Speaker C: We definitely had this conversation at ECC, which I remember, and I am in favor of the coupling approach. I'm not too worried about trying to be the same as sharding in day zero. I think with real dense sharding, we need to have a hard fork anyway, so we can change it then. It's not that big of a change, but it will be nice to just like, I think we can definitely ship four four slightly faster, just couple them together. It's less engineering challenge that way. It's less implementation. It's also better ux.
00:26:11.598 - 00:26:15.810, Speaker C: So I am 100% in favor of the coupling.
00:26:16.230 - 00:26:38.440, Speaker A: Okay, awesome. And yes, second question we had for you is the diffs between the current prototype are starting to diverge from master with the merge work and prism and Gif. When do you think it's like the right time to rebase this?
00:26:39.370 - 00:26:53.726, Speaker C: Yeah, Porto asked me to help, so I think I should be free after in a few days, just trying to finish last minute Mac boost related issues. So yeah, I should be free in a few days and then I'm more.
00:26:53.748 - 00:26:54.654, Speaker A: Than happy to help.
00:26:54.692 - 00:26:59.280, Speaker C: Just send it over the branch, I can rebase it for you, it shouldn't take me more than a few.
00:26:59.810 - 00:28:18.100, Speaker A: Oh, okay. Well, nice. Sweet. I think those were the two things for Terrence Sweet and I guess yeah, the other thing I want to make sure we chat about is we have folks now obviously on the optimism side on Coinbase kind of working on this, but this is like a pretty big eip and there's obviously a bunch of folks, sorry. So by this I mean the implementations, there's a bunch of other work as well. But yeah, there's obviously a lot of work to do to get this implemented and tested in clients, and it seems like there's some interest by the community to help. And I guess I'm curious from Coinbase and optimism, what skill sets or tasks do you think would be most helpful to have people help out with that are maybe a bit independent from the work that you're doing or that can be parallelized if there's engineers who have some time and experience that can help here.
00:28:24.910 - 00:29:48.300, Speaker D: I guess one that will be really useful once we have the net running is just having users in the network testing all sorts of scenarios, sending blobs, downloading blobs, ensuring that the current gas fee calculation sort of works in a dev environment. And yeah, we just like to have more participants in the IP 40 44 testnet. That would be super useful. Another thing would be people should just take a look at the code, the various repos that I posted, zoom, maybe we can make these available somewhere like in the community call agenda. But take a look at the repo, see if we can improve test coverage, particularly in Prism, because a lot of the testing we're doing here is based on another repo that basically interrupts both Geth and prism for testing. But it would be nicer to have more test coverage in prison to target specific scenarios that ensure that the EIP is as robust as possible.
00:29:53.390 - 00:30:22.260, Speaker A: Got it. And I guess in terms of actually implementing things, I guess we have kind of the Coinbase folks working on the Geth implementation. You working on the Prism side, I see there's like a bunch of pine devs on the call. Sorry. Yeah. Mixed up at Geth and Prism and Coinbase and optimism. Yeah.
00:30:22.260 - 00:30:37.420, Speaker A: Do you think it makes sense to have other implementations sooner rather than later? Or should our focus be like let's get these two kind of as far as possible and then add some more?
00:30:40.430 - 00:31:17.910, Speaker D: Yeah, I think it makes sense to get as far as possible because we are still making changes to spec, particularly the gas price update rule. We're probably going to have a discussion later right now how we're going to do that. Also, if we do decide to go ahead with bundling the beacon block and sidecars, then that's like another change that other implementers will have to do. So it just makes sense to consolidate all the changes in one and once we get to a point where the spec is sort of stable, then we can start introducing more implementations.
00:31:20.250 - 00:31:43.440, Speaker A: Okay, that makes sense. Okay. Basically, I guess the two main things now is just like testing on the devnet as soon as that's out and then basically seeing if there's test coverage that can be approved in the current prism and get implementations. Those would be like the two most useful things, right?
00:31:45.650 - 00:31:53.140, Speaker E: Yeah, but that said, I think if someone came along, was an expert in a particular client we're not working on, wanted to get started, we certainly wouldn't stop them.
00:31:53.510 - 00:32:15.880, Speaker A: Yes, obviously. And I guess would it be helpful like if someone comes along and they're an expert in prism or guest, is that also helpful to have more people working on those specific implementations? Or is it just like too much people on the same kind of parts of the code?
00:32:17.710 - 00:32:42.740, Speaker D: No, I think that would also be helpful. There are like two or three major items I foresee in the next couple of weeks where two or three people can work on differently without stepping each other's toes. So yeah, I think that will be helpful, having experienced prism or deaf devs contributing into the.
00:32:44.390 - 00:33:14.362, Speaker A: Ok, great. So I guess if you're an experienced guest or Prism dev listening, you can reach out to me. Or I guess Liam, you also posted about this yesterday. I'll put you on the spot here. Yeah. If you're interested in contributing and if you're not sure where to start, I have notes for this call. So we linked a bunch of stuff there and then the very first place is probably either the devnet or looking at the specs and kind of diving deeper from there.
00:33:14.362 - 00:34:22.242, Speaker A: Does that make sense? Cool. Sweet. Okay. So I guess, yeah, the last thing I wanted to cover today, and I think it should bring us right to time, is just basically like our list of issues from the last time, and we touched on some of these already, but not all. So on the KZG library section, we're still working on improving this. We discussed the sync a fair bit, and I guess the last one is like the fee, and I guess just to put some context here, so right now the current devnet implementations use kind of the naive fee market with like a hard coded gas price per blob all the time. This is not going to work.
00:34:22.242 - 00:34:58.480, Speaker A: There was a proposal in the EIP for just a more complex one that basically uses EIP 1559 style pricing for the fee market. On the last call we kind of discussed moving this from a special contract in the state to the block header. Yeah. And I guess I was curious to hear a from people like does this general fee market just make sense? Do we think it's good enough to move forward? And b, does everyone agree that just having this in the block header is the way to.
00:35:02.690 - 00:36:07.970, Speaker F: Yeah, sure. So I think kind of, with regards to the header, I think basically everyone agreed that it might just be the more practical way to go. For now. The only person disagreeing was Vitalik, incidentally, but I think he's not on the call, so forfeiting his voice here, I think on the mechanism itself, generally, kind of the mechanism proposed by the IP more or less works. The only reason why for a while now it's been a somewhat open research topic is just that there are things we would like to get that are not fully provided by the fee mechanism, but they are more like nice to have. So basically for one, it's that while this works really well for something like blops, where demand is relatively slow moving, it wouldn't quite perfectly be generalizable, because basically. Sorry, step back.
00:36:07.970 - 00:37:06.278, Speaker F: This would be the first time that we introduced like a two dimensional pricing mechanism, one dimension for bops, one dimension for normal execution. Roller projects for a while now have been saying that they would really like to have a standard for doing two dimensional pricing because they have to do that anyway, because they have to price layer two gas and layer one gas inside one transaction. Basically, for now, all roll ups basically hand roll their own mechanism for two dimensional pricing. We would like for the four eight four mechanism basically to be generalizable. The Kant version is not ideally generalizable, just because in that context, kind of the two dimensions would be much more fluctuating, and because they share the same gas limit, that might become a problem. Again, not a problem for blobs, just a problem for generalizing the mechanism and then also kind of similarly. We would also ideally want this to be maximally forward compatible with full on multidimensional pricing further down the road.
00:37:06.278 - 00:37:57.174, Speaker F: But I think on both of these counts it's a somewhat similar situation like when we're talking earlier about bundling blobs and blocks on the CL side, where we might just want to be practical and say we move forward with a minimum working version for now and then we can always iterate on it later. So I think there's still some effort to try and maybe look into this whole kind of compatibility with layer twos just because they would really like that. I think so maybe if we come up with a slightly alternative design within next month or so that would include that. I think all the better. But for now we can just work on the basis that we have a mechanism that is good enough, basically. Sorry, that was a bit long, but I hope that made sense.
00:37:57.292 - 00:38:04.380, Speaker A: Yeah, no thanks, that's quite useful. And yeah, proto I was going to because you have a bunch of comments on.
00:38:05.470 - 00:39:22.930, Speaker H: Yeah, right. Sister mentioned that light client does have a pr open in the eips repository to update the old mechanism to a new mechanism that uses a header field instead of state, but then otherwise does not change anything about the previously proposed fee update mechanism. And I want to note here that this is not exactly the same as ERP one five nine. The adjustment works a little bit different. And I think there are some subtle issues with this mechanism, and I'm not entirely sure what the right direction is to correct them. With this blob pricing problem we have this balance we can make, or this incentive, whether or not we want to prefer a burst of blob data or a repeated smaller burst. So if we go over the target, the gas price or the fee rises, and this is incrementally more and more costly.
00:39:22.930 - 00:40:05.070, Speaker H: And so small burst right now are more expensive than grouping all the blobs together, even though the total amount of throughput after the end of the example is the same. And so I have this question. Are we more concerned about bandwidth on the network and about the stability of the bandwidth, or are we more concerned about the processing? Because if we have processing, I think it might actually be favorable to create this incentive for a large burst on blobs rather than this more stable amount of blobs.
00:40:06.610 - 00:40:14.418, Speaker B: Don't think we care much about either of those. I thought what we care about is long term storage costs. Isn't that the dominant factor here?
00:40:14.504 - 00:41:00.850, Speaker H: By a pretty large margin we have pruning. So long term is really just a month worth of data. There's this other issue with the current design of the fees. I'll give an example. If you exceed the target, then the price will go up. And then if for say a month or whatever the period is that blobs are retained, if you perfectly match the target, then you will eventually prune the excess, but the gas price will still be sticky and will still be high. So even after pruning, after correcting it for a long period of, and stabilizing it for a long period of time, the gas price is still high due to the old excellence.
00:41:00.850 - 00:41:19.990, Speaker H: So I think the gas price update should consider pruning perhaps, and we should consider the kind of characteristic that we want with the blob throughput if we want like repeated small additions or infrequent large additions.
00:41:24.570 - 00:42:21.580, Speaker F: Sorry, just to briefly mention, I think one of the concerns on the pruning side was just that it might be not ideal to basically enshrine specific retention, like specific assumptions about retention periods in the pricing mechanism itself, because otherwise this is basically just a client parameter where of course, I don't know, we like to give some defaults and some recommendations, but basically if you want to run a cl and just drop blobs after a week, you can do that. Or if you want to keep them for a year, you can do that. But the moment we kind of have some sort of finite memory set in the block pricing mechanism, then of course we're starting to enshrine that. Other than that, I think it's perfectly reasonable. And it's also not too complicated, I think, to do that.
00:42:22.430 - 00:42:23.820, Speaker B: I think no matter what.
00:42:24.350 - 00:43:00.780, Speaker A: Sorry, I was just going to say I agree we probably shouldn't enshrine some specific value, but we should price the fact that they are temporary to some extent. Right. And it's almost like you don't want to enshrine like a week versus a month, but you also don't want the mechanism to even implicitly assume they're going to be stored for a year, if that makes sense, because that kind of nudges clients to not store them for a year, which is what we want. But I agree, you don't want to have a hard coded cut off of this many epochs or something.
00:43:01.950 - 00:43:37.240, Speaker H: One approach could be to bias the pricing towards more recent throughput so that older throughput is dampened. I think there are some balance here because otherwise we basically end up pricing blobs based on very old throughput details which might already be pruned, and it just makes pricing less accurate, in my opinion. I think we can do better than that.
00:43:37.770 - 00:44:23.460, Speaker B: That's exactly the same situation, 1559. However, I believe that the counterargument there is that kind of latent memory of historic pricing is completely lost in noise in the real world. In your theoretical scenario, you had perfectly even throughput except for that one little spike. And that one little spike causes that to kind of remember the spike forever. But in the real world, you are never going to get that perfect. And as soon as you have any kind of variance, that little tiny spike gets lost in the noise right away, I believe. I'd be very surprised to see that memory matter at all in any even kind of worst case scenario, real world situation.
00:44:25.270 - 00:44:40.826, Speaker A: This is maybe like a dumb question, but can you just walk us through actually how the repricing occurs and how it differs from 1559? Yeah, so 1559 is like, you look at the gas in each block and you go up or down by like 12%.
00:44:40.928 - 00:45:59.940, Speaker H: I'll try to give my best interpretation. I do think there is, like, a small inconsistency in the explanation of the gas pricing in the EIP currently, so it might not be 100% correct about this. So the basic interpretation is that we track the amount of blobs that have been confirmed since the start of the EIP, and we track, or we can compute the target, the expected amount of blops that we would want. Now, we take the minimum of those so we know whether or not we are under or below the target, and say if we're over the target, we're going to adjust the prices upwards. If we're under the target, then if we're under the target, I think the current EIP makes blobs very cheap. I'm not exactly sure if the EIP is correct in this case, but let's just take the case where we are over the target. In the case that we're over the target, we use this exponential thing where the more we are over the target, the more the blobs will cost.
00:46:00.390 - 00:46:12.230, Speaker E: I think there's a cap where if it goes below target, it'll take the maximum of target versus where we currently are. So it never actually goes below target. That was my reading.
00:46:17.370 - 00:47:18.042, Speaker F: I think the difference between the pricing of the proposed pricing for four, eight, four and 1559 is that 1559 basically always does relative adjustments, so it doesn't care about the absolute value of the base it just says, okay, the block was underfall, go down. The block was overfall, go up. It only looks at one last block, whereas four eight four does the exact opposite. It has, like, this infinite time horizon where it just says, I want to always have half of the blob space filled. And I just keep track of historically, like, accumulating over all history, what was the percentage? And as long as the percentage was under 50%, then basically blobs are free. And the moment we are over 50%, then blobs basically cost something. And that price keeps going up the further we are above 50% until we, at some point, get pushed back down to 50%.
00:47:18.042 - 00:48:03.674, Speaker F: Or there could be some equilibrium where we know we are 51% or something. But now, just very briefly saying, why does it not really matter that it has this long term memory? And I think that's kind of also what Micah was alluding to. Because of this mechanism, we will always end up in a scenario where we are close to 50%. We could be below 50% in the very early days when no one uses blobs, but besides that, we'll always be, like, in the 50% to 55% range or something like that. Right? And so just because blobs might have been more in demand in the past or something doesn't really matter, because it just means that this value will be at 50. Between 50 and 55%. So the worst case is that now the demand is only 50% or 51%, and it used to be 55%.
00:48:03.674 - 00:48:29.618, Speaker F: So there's, like, a 4% difference or something, but that really doesn't make a big difference, and it washes out over time. I agree that maybe it's still preferable to make that more explicit, but there can't be a scenario in which the historic accumulator is at, like, 90% or something, because that's the entire thing that the kind of targeting was supposed to help against.
00:48:29.784 - 00:48:30.546, Speaker A: Is that right?
00:48:30.568 - 00:48:55.878, Speaker H: That makes sense. With EP one five nine, though, as we are adjusting downwards, there's more precision in adjusting downwards, whereas in EIP, four eight four, as soon as we're under the target, even by a little bit, things start to become, I think, a little bit chaotic as the pricing is not accurate anymore.
00:48:56.054 - 00:49:12.960, Speaker A: Yeah, that seems weird, because you could imagine, I don't know, there's no blobs for a week. That doesn't mean that we can then process infinite or a ton of blobs the week after. Right.
00:49:14.470 - 00:49:25.090, Speaker F: But it kind of does. Basically, the idea is that because we have this maximum, that's only two x the average. Anyway, we would be okay with a sustained.
00:49:29.450 - 00:49:41.180, Speaker A: So the assumption we're making here is we're okay with a sustained full blobs for long periods of time, which is not an assumption 1559 makes.
00:49:41.950 - 00:49:43.226, Speaker F: That is correct, yes.
00:49:43.328 - 00:50:24.978, Speaker H: I think even give a counterargument against this. In your example, when there is a week of no data and then a week of double the amount of data, then on average there's no excess. But there's a bias towards recent data. So assuming there's pruning or no pruning, we might end up holding a lot more data due to this imbalance over time. Right. Because pruning time was a week, now we're holding twice as much as otherwise with normal pruning and normal throughput.
00:50:25.154 - 00:51:06.962, Speaker F: Right. I think basically the assessment was just that, basically this inefficiency is there. You could basically just. Because in the long run we don't expect this to really be the case much, because you'd never be like for a sustained period of time, be below 50%. Because at least in our assumption, there would always be some demand for blobs so that it would be used like before we get dipped down below 50%. But in the early days it could definitely happen. And so we have this slight inefficiency that we basically have to be able to handle storing two x, the amount, the average amount for say a month or so, because there would have been an empty month and then a double month.
00:51:06.962 - 00:51:24.730, Speaker F: And so we basically have to store two x. For that we gain simplicity in the algorithm. So this is a trade off. We could try and make the trade off algorithm more complex and more sensitive and then we don't have this two x storage overhead in the worst case. Yeah, that's a choice.
00:51:25.230 - 00:51:59.160, Speaker H: I think we are starting to basically converge on the other problem with this choice between prioritizing many smaller amounts of blobs versus a few larger amount of blobs. If we have clarity about this point, like what kind of throughput maximum in a sustained manner is that we want to favor, then maybe we just also solve for the other problem.
00:52:01.930 - 00:52:15.100, Speaker B: Michael, what was the reason behind choosing this mechanism instead of the 1559 mechanism? What is the perceived advantage? They seem like they'd result in basically the same thing, but this one requires an extra header field.
00:52:17.870 - 00:52:19.386, Speaker F: Wait, how does it require an I.
00:52:19.408 - 00:52:21.226, Speaker H: Can give some because you have to.
00:52:21.248 - 00:52:24.730, Speaker B: Keep track of how many blobs have since genesis.
00:52:25.950 - 00:54:20.130, Speaker H: Give some details about the header fields and how it would look like if we emulated ERP one five nine so uses the parent information, the parent block base fee, and then has this lag to update towards the new base fee, validating that the base fee update is correct and it uses the total amount of gas that was used to do so. So this is the second header field that is already available for regular gas to be able to do this update with two header fields from the parent block to get and compute the new base V of the next block. With this EIP, we don't have such information that captures how many blobs were included in the previous block without having to make the full block available. Like the header data itself is not enough to get the right information to update a base fee in the same way that EIP 1559 would do. So instead, this mechanism tracks just that information, the amount of blobs that have been included. And then instead of introducing this base fee that needs to be updated, it computes it just from the total amount of data that has been included by keeping track not just of the last parent block, but of all of the total included blobs, and then comparing it against a theoretical target based on the block height difference and the number of blocks, the number of blobs that should go into each block.
00:54:20.950 - 00:54:35.618, Speaker B: So would the short version of that be that 1559 requires the transactions from the parent block? This does not require the equivalent of that which would be the blob.
00:54:35.794 - 00:54:50.250, Speaker H: So if we are going to exactly emulate EFP one five nine, we would need to add two fields to the header. One to count the number of blobs and one to count the or to represent the base fee for the blobs.
00:54:54.930 - 00:54:59.870, Speaker E: Isn't number of blobs already in there though effectively.
00:55:04.370 - 00:55:05.746, Speaker H: Can you repeat that?
00:55:05.928 - 00:55:09.634, Speaker E: Isn't the number of blobs already effectively in the block header we keep track.
00:55:09.672 - 00:55:10.420, Speaker A: Of that.
00:55:12.870 - 00:55:16.130, Speaker E: Not total number, but number of blobs in the previous block.
00:55:16.630 - 00:55:32.140, Speaker H: No, as blobs right now they are referenced by Deeplob transactions and Deeplob transactions are just part of the transaction list. So we only really have a hash of the transaction list which doesn't really tell how many blobs there are.
00:55:34.510 - 00:55:38.620, Speaker B: Is this formula written down in the EIP at the moment?
00:55:41.410 - 00:55:46.240, Speaker H: The one based on total number of blobs for all time?
00:55:46.690 - 00:55:47.534, Speaker B: Yeah.
00:55:47.732 - 00:55:59.170, Speaker H: Yes, that is in the EIP. And then in the pr of math you can find the header based version of that as opposed to reading it from the stat. I'll link it in the chat.
00:55:59.750 - 00:56:02.580, Speaker B: The gas price update rule in the AP, is that correct?
00:56:03.990 - 00:56:59.814, Speaker A: It's in there. Yeah. Just because we're kind of basically hitting on time here, I feel like this idea around short burst versus long term history is something that we probably should get client teams feedbacks on, and especially on the cl side, along with the sync design. That feels like the main probably thing here. I guess the other part, like Ensgar, you mentioned around having l two s, being able to use this as well as a pricing mechanism. It feels to me like once we kind of have the preference from the CL teams, that's maybe like the second thing to look at. And basically those are like, the two most important things to figure out for the free market.
00:56:59.814 - 00:57:01.974, Speaker A: Does that make sense? Right.
00:57:02.012 - 00:58:00.390, Speaker F: Although, just to clarify, this would not be on this question of what specific kind of. Well, I guess it would also be relevant, like, whether it would be short term or long term kind of stabilization mechanism. I guess they would favor, of course, the short term stabilization mechanism, but it's much more about kind of how does the two dimensional pricing actually work. So the way the EIP right now works is just, basically, just translates the variable price into, like, a variable amount of gas consumption, but then the gases within the transaction is accounted as normal. That has some disadvantages that aren't really that relevant for four eight four, but they would be more relevant for roll up. Basically, if we wanted to make this kind of more roll up compatible, that might mean we would have to slightly change the way the accounting works as well, not just this design choice.
00:58:03.210 - 00:58:05.740, Speaker A: I do feel like. Yeah, go ahead.
00:58:06.670 - 00:58:19.434, Speaker B: So am I correcting that this is not adjusting the gas price, it's adjusting the gas cost, like the amount of gas that's used for blob. You just said escar.
00:58:19.562 - 00:58:20.286, Speaker F: Yes.
00:58:20.468 - 00:58:21.360, Speaker I: I see.
00:58:21.810 - 00:58:26.178, Speaker B: Yeah, I'm not a fan of that, but we're out of time, so I won't complain too much right now.
00:58:26.344 - 00:59:26.610, Speaker A: Yeah, okay. Yeah, that makes sense. I guess it's like, yeah, if you think of it as, like, the interloping constraints or something, I just want to make sure that what we present as the trade off space for l two s is kind of what Cl teams want to optimize for, because it's kind of crucial that CL teams are happy with this if we wanted to implement it on l one. And then beyond that, I guess getting the BLST editions, that'd be really helpful, launching the Devnet and having people kind of look into that. And then finally, does it make sense to already schedule another one of these calls, or do people prefer to do this? Async Terrence.
00:59:27.110 - 00:59:29.198, Speaker C: Yeah, I wonder sorry I came late.
00:59:29.214 - 00:59:30.466, Speaker D: So I wonder if this was discussed.
00:59:30.498 - 00:59:39.066, Speaker C: Has there been any thoughts about having some sort of meta spec? Because for me I'm like looking at all the specs and it's hard to know which one is the version that.
00:59:39.088 - 00:59:41.900, Speaker I: We'Re aiming for, so something like that would be nice.
00:59:42.750 - 00:59:48.250, Speaker A: I think Proto has one, but I'm not sure how changed 30 minutes ago.
00:59:48.320 - 01:00:03.230, Speaker H: So that must be yes, I'm adding links as they pop up to keep track of everything, but we do not have a firstening scheme for the EIP, so all these different resources are at varying stages of programs.
01:00:05.830 - 01:00:17.970, Speaker A: And we'll be discussing the executable spec for the execution layer on core devs next week, if that's the type of stuff people are interested in. Sorry George.
01:00:19.670 - 01:00:59.940, Speaker G: No, I was just going to say that this thing between the two specs right now is an actual issue, because with Shawi we did the consensus specs for eight four four thing to be executable and that brought a bunch of edits and differences. And right now the two specs are pretty desynchronized in terms of the KZG stuff, and I've been waiting to make an EiPPR to bring it in sync, but I'm not sure when to do that. So that was another topic I want to raise in this call, but maybe we can do it on the next one. Like what's the best way to keep the two?
01:01:05.110 - 01:01:16.034, Speaker B: Is the reason for not updating the EAP regularly, just because too much hassle and you wait until things are kind of hammered out and then update the AP? Or is there some other reason that the IP is lagging?
01:01:16.082 - 01:01:45.680, Speaker G: Yeah, that's the reason. It's code duplication in a code base, but to change the second code duplicate I need to go through the whole PR process, and so I was waiting to batch a bunch of stuff inside before I do so, but this is all related to the execution executable spec thing, so maybe after the ACD we can have a more productive discussion about this stuff.
01:01:47.510 - 01:03:02.070, Speaker A: I think this is one of the best examples of how our process is broken, because anyways, and I know we're already over time, but I think if you want to come and proto as well on awkwardevs to kind of highlight that next week, I think it would be good, because I don't think this is the last time we have a feature that touches both layers. That'd be really helpful. Yeah, I guess. Do people want to set up an x call right now, or do we want to do that outside of this and the time, I guess looking just like roughly at the next couple of weeks, I think the time I would propose would be like Wednesday, August 17 at 14 UTC. So if everyone here is happy with that, we can just put that now. Otherwise we can just chat about it on the discord. So any objections to the 17th 1400 utc? Okay, no objections.
01:03:02.070 - 01:03:19.120, Speaker A: Cool. So I will see you all then and yeah, let me share the notes in the chat here. I'll post them in the GitHub agenda as well. Yeah. Thanks everyone. This was really good. Thank you.
01:03:19.490 - 01:03:20.286, Speaker I: Thank you.
01:03:20.388 - 01:03:21.310, Speaker F: Thanks everyone.
01:03:21.460 - 01:03:26.090, Speaker D: Bye charcoal. Bye.
