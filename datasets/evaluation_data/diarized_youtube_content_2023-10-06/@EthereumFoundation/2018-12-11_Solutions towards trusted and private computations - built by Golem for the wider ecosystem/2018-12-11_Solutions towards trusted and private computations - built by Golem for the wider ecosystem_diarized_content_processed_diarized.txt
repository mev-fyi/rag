00:00:01.530 - 00:00:59.114, Speaker A: So in the decentralized environment, and taking under consideration that malicious software can get somehow to your computer, trustworthy computation goes without saying. It's a need, it's a must. But different parties may need different aspects of trustworthiness in terms of computation, and this is an important question of why we really need computations. So, for example, we may need to deploy a peer to peer network of decentralized computers with a non consensus algorithms, or we may need to deploy a smaller consensus between two computers to compute something. And the second use case requires other aspects of trustworthiness than the first one. In the first one, you need bounds on the malicious actors, the number of them, and some availability. In the second case, you probably need confidentiality.
00:00:59.114 - 00:02:02.286, Speaker A: If you would like to decentralize the centralized service that hosts a general purpose computation, then it is quite a different deal, and there are more. So we had to ask ourselves this question, what is important from our perspective at Golem? So just a short recap in context of this presentation. Golem is a network of heterogeneous resources ranging from individual machines to subnetworks, exposing their compute power to the network, and two kinds of participants that interact with both the resources and with each other. The first one is the requester. The participant would like to do some computations using the resources, and the second was one is the provider. Provider rents out the resources and perhaps would like to do it in a secure manner. So this is the basics, and this is the fundamental layer, on top of which of course we need to build another layer, for example, economy.
00:02:02.286 - 00:03:17.834, Speaker A: But in context of this presentation, I focus only on this infrastructural part. So the problem statement is pretty easy. The requester wants to carry out a computation in a trustworthy manner on the resources provided by the provider, and the provider should be safe in this setting, should be protected from the malicious software, malicious binaries, and this simple problem statement maps pretty well to high level requirements. So yeah, requester wants to be able to run any binary in this network, perhaps efficiently, and this binary shouldn't need any additional changes just to be run within the network. Task input used by the requester should be exactly by the provider, should be exactly the same as requester provided, the binary that is run on the provider's machine should be exactly the same that requester wanted, and the environment should be exactly the same as requester wanted. This is different from the first requirement, which states that requester should be able to choose any binary, absolutely any binary. Right here.
00:03:17.834 - 00:04:03.866, Speaker A: We need to make sure that no one can interfere with the binary before it is run, the execution should be carried out the valid way, no tampering, no interference, which means that the requester can expect their valid results, provided that the task was encoded the right way. The output data shouldn't. Well, it cannot be in fact altered by anyone in a way that's undetected to the requester. And only the requester should be able to show the data to the external world. Meaning that perhaps the application that's hosted on provider's machine can look at the data. But other than that, no one should be able to leak this data to the azure world, only the requester. If Hugh is to do so.
00:04:03.866 - 00:04:53.914, Speaker A: And let's not forget about the provider, he has to be protected from the code as well. So how can we meet these requirements? There are a few approaches, or quite a few. The point is that there is no single bullet, some silver bullet, for these all requirements and different approaches result in meeting some of them, or all of them in some way, but not all of them with all the required features. So for example, we can constrain ourselves to deterministic algorithms for which proof of work exists. And the trade off is that we only can do deterministic work. We may sacrifice confidentiality and get into integrity, but yeah, we lose confidentiality. On the other hand, we may, for example, use third party sources of trust.
00:04:53.914 - 00:05:41.414, Speaker A: This is either a third party service that is going to provide trust to the actors, or we may simply use some checkpointing to make the cheating less feasible. At the same time, we can use infrastructure such as trusted execution environments or mix of thereof. So as I said, there's trade off for every approach, and it's either inherent to the method, such as using proof of work or task specific. For example in rendering. You can be interested in confidentiality if you render a movie, but if you render open source animation, it may not be so important. And you also care about the integrity. It's only also runtime dependent.
00:05:41.414 - 00:06:53.362, Speaker A: For example, for trusted execution environments, memory access patterns may have profound impact on the efficiency, so this has to be taken into consideration. From my perspective, you wanted a solution that is generic, easy to use, has well specified security considerations known to us and to others. L allows for remote computation because it's at gollen we do so. As you know, our trusted execution of choice is SGX. Or more importantly, it's not only SGX, it's the technology stack that can be built on top of the SGX, and we achieve it by means of graphene and graphene G. And this is quite interesting stack and quite fine, because it allows us to provide generic computations meeting most of the requirements with more or less known efficiency considerations and known security considerations. We know that the communities around building those, well, evolving in terms of, for example, security and efficiency, and that's why we moved toward this question.
00:06:53.362 - 00:07:34.618, Speaker A: So just a short recap, what SGX is. SGX is an intel technology. It's an architecture enhancement to the processors, allowing protection of application and the data from the processes on the same machine, even the privileged ones. This happens in so called enclave compute model, and additionally there is a way of making sure that the computation takes place in the enclave. So it's called the remoted station. So this is quite powerful and quite good. The problem is that, well, not the problem, the issue that from developer's perspective it's kind of limited.
00:07:34.618 - 00:08:20.166, Speaker A: So we can only run application if you code them from scratch, preferably using intel SDK. You cannot run arbitrarily binaries, so they have to be modified. And by default you have to specify static interface of interaction between the application in the enclave and the untrusted part of the application outside of the enclave, of the host. And this is, well, something kind of limiting. So we get this powerful feature of running in secure enclave, but at the same time we cannot run arbitrary binaries, which is important to us. But this is a good starting point, good building block for something more generic. And the next step from our perspective is graphene.
00:08:20.166 - 00:09:14.590, Speaker A: Graphene is a Libos based framework which allows you to run arbitrary Linux binaries in enclaves using all the features of SgX. And from the application point of view, this is just as interacting with the regular OS. So this is important. This is completely unmodified Linux binary. And why we preferred graphene over other approaches, well, there are a few nice properties that we're interested in, and the first one is that the Libos approach clearly states the distinction between the trusted compute base and the attack surface. So as you see, Libos is bigger than the alternative approach. It contains more code, it increases the size of the TCB.
00:09:14.590 - 00:10:23.074, Speaker A: But this is the part we and invisible thing, labs can work on, mostly invisible think labs, so they can put lots of resources to making it better, not only them, the community. So we can improve on that, but we have absolutely no control over how the user is going to abuse the interface. So the smaller and the better self contained it is, it's simply more secure. Playboy S is good for sandboxing. We still want to have our provider secured, even if you add this additional layer and it's pretty cool because libos can be used in a way where it's easy to replace host or guest OS. So what's the difference between the regular way you run the app computation and the graphene? Well, in vanilla sgx? Well, using the Intel SDK you have to get the application source code, tyler it to the enclave compute model, specify the interface, compile it and run. Okay, so this is quite different to what you have to do here.
00:10:23.074 - 00:11:27.398, Speaker A: You get the arbitrary binary, right now it's Linux binary, Ubuntu and Debian, but other than that it's an arbitrary binary. You pack it with the graphene and good to go almost because this process still requires some manual work to configure, deploy and run the application. That's why we took the next step. This is called graphene ng, which is graphene plus a bunch of features that are interesting and should result in better UX regarding both the enclave lifecycle and deploying the application. So the protected files. The protected files is a library which allows secure or encrypted communication between the owner of the enclave or any party that initiated the computation and the enclave itself and the host which hosts the graphene. And the enclave cannot interfere with the files in inducted way.
00:11:27.398 - 00:12:23.082, Speaker A: Docker support is important because as I said, we want to make sure that the computing side, the provider side, is secured, and at the same time it makes it easier to configure the environments, tools and scripts are there for the UX and bug fixes. It's very small point, but it's very important. Making graphing stable took a lot of work, so it's a lot of work put by invisible things lab. And although there were no, or almost no important security or vulnerabilities, there were quite a lot of bug fixes related to the stability of the solution. So for us it is important that those arbitrary binaries can really be run, and this requires stable graphene. So the features were there. I think the easiest way to showcase how it works is just to show an example.
00:12:23.082 - 00:12:53.890, Speaker A: And by accident we have one such example. It's Golem integration with brass, it's graphene ng with blender. It's proof of concept for now due to the stability issues, for example. But other than that, it is a working example. So let's take a look at a few points of views of how to use the graphing. The first one is the provider's point of view, so the providers just have to prepare an image. With arbitrary application, preparing this image is mostly automatic.
00:12:53.890 - 00:13:36.260, Speaker A: The manual part is single action when SGX have to be enabled. This may require some BIOs tweaking. Well, not tweaking, just switching something in BIOs and configuring protected files which require senclave manifest and specifying the docker parameters during the run, because enclave has nothing to do with Docker by default. So those protected files have to be configured on both sides in the enclave and with Docker. But other than that it's almost automatic. So the user prepares the container, deriving it from the provided one. The one with Golem AsGx template is the one that's ready.
00:13:36.260 - 00:14:15.840, Speaker A: The user app is whatever the user chooses to be. The key is something that user can either generate using a script, or it can be used by, well, from whatever source. And the key can be is then used to sign the container. It happens with script as well, so it is mostly automatic. Exactly the same process was used to prepare Golem integration and the blender integration. So there's nothing different. The only thing is that we have a specified application, this arbitrary app, and the arbitrary means blender here.
00:14:15.840 - 00:15:22.434, Speaker A: Okay, so the point of view of the handshake process, I'm not going to describe it in detail, but the point here is that this process allows the requester to connect with the enclave on the provider machine and the hosted binary through encrypted channel. And what's important here is that it's mostly automatic, and if it's not, then there are scripts to assist here. So it was important for golem integration because the user interacts with the golem by UI and is not really interested in seeing all this. So this happens automatically, and there's script to verifying quote for example. And yeah, the more interesting point of view of the application. So the application point of view is what you see on the left, simply nothing. Application sees it as a regular OS or regular I O, and that's it, no changes.
00:15:22.434 - 00:16:10.130, Speaker A: If you take a step back, then you see that the framework is there, the logic is there, and it's doing its job under the hood. For example, the IO is encrypted and decrypted and fly. So it's there to be transparent to the application and in fact to users. Okay, one more important and interesting point of view is requester point of view. So requester still has to interact with the nodes in the network and connect to them, choose the nodes which should compute something to him. But it can be envisioned as if the requester was using more locally available resources. Of course these resources are not available all the time, but other than that it can be treated well.
00:16:10.130 - 00:17:03.330, Speaker A: Requester can treat them just as maybe not all the time available local resources, making his computer more powerful. So yeah, as I said, we have a working example, working integration. You can get more information about this at our booth and see the demo. What we achieved with this integration is, well, we proved that the graphene ng approach is the valid one and that the features that it offers can be used to make such an integration. It offered what we wanted, confidential remote computation with arbitrary binary. Well, the blender is binary of choice, but it could be any binary, well, Linux binary at all. And what's more important is that it's a clear decoupling of the binary and the application that is hosted on the infrastructure, in the infrastructure itself.
00:17:03.330 - 00:18:27.850, Speaker A: So the infrastructure is SGX and the application is any application which can be seen as SGX, as an infrastructure from Golem perspective, which is important because we can, providers can rent out this platform. Okay, so it is pretty cool, but there is still some more work to do, both on our side and general SGX intel site, you name it. So what would be fine to have is liberated intel attestation service so that you don't need this central point to attach your enclaves. Flexible launch control, meaning that for now, intel controls the way enclaves are launched and can potentially disallow some enclaves to be launched. So you should be able to write your own launch enclave and launch any enclave you wish. Another thing is that there are some known attacks to enclaves and mitigation steps are required for these attacks, and we need to increase the efficiency of the solution. So this is from the SGX side, we want stable and efficient graphene, and this requires bug fixing proof of computation for economy and Windows support, because right now we have only Linux binaries.
00:18:27.850 - 00:18:54.726, Speaker A: So this is proof of concept. This requires additional work, but we can talk about use cases that potentially can benefit from this stack. So the first one is Golem specific is local verification. Hey Peter, you only have about two minutes. Okay. So with local verification you just make sure that the verification takes place on provider machine. Usually it takes place on requester machine and you don't lose any trustworthiness this way.
00:18:54.726 - 00:19:36.642, Speaker A: Other one is Golem unlimited. In Golem unlimited you have a LAN like setting. With trusted computers. You can equip them with SGX, making a more powerful SGX node and expose it, for example to encode transcode movies. Identity management inside the Golem unlimited network. This makes sense because it's easier to seal the identities well keys inside the enclaves provider nodes and work with them automatically, or other use cases that people can come up with should be able to benefit from this technology. Wider audience.
00:19:36.642 - 00:19:58.266, Speaker A: For example, decentralizing server services. We can do it using the sgx. Of course it is not an easy task because decentralizing may require additional algorithms. But the building block is there and it may be potentially used by secure multiparty computation algorithm. It may be used to decentralize constant service. Atomic swap well, we can do it. Well, we can do it.
00:19:58.266 - 00:20:40.018, Speaker A: It can be done. It has some problems because in atomic swap you need to make sure that the applications are not only not forged, but they reach the blockchains. But if you use multiple enclaves, then it's easy to reach the consensus and push the transactions. This can be used, for example as a building block. In distributed exchanges we can also seal keys either locally or remotely. It's just the question of whether you trust the technology enough to seal your keys remotely. And this way you can, for example, store your identity in a distributed manner, both geographically and by making k of n signatures required.
00:20:40.018 - 00:21:20.866, Speaker A: You can express the notion of uncertainty towards how do you trust the TE. If you trust it, then there's one. If you don't trust it that much, then there is more. And in existing projects it can be used as well. So for example, we have a manual viable plasma, with the central point being the plasma chain operator. We would like it to be reliable and not to cheat. So let's just enclose it into enclave, and at the same time we would like to make it reliable in terms of block withholding, which means that we can decentralize it and make sure that it's harder to withhold blocks by an operator.
00:21:20.866 - 00:22:06.782, Speaker A: Another one is Horde. Horde is a platform for governing assets in games and binding it to players so that players have the true ownership of items. And this requires keeping the notion of state on the server side. So game server is in the Internet, player plays the game, and the game server keeps track of state and makes sure that player doesn't cheat well. In fact, most of the logic can be run locally in the enclave, and we only care about the integrity, not the confidentiality. Understep is decentralizing the whole system and making this local game server connect with others and build a bigger server. And the last one is data streaming.
00:22:06.782 - 00:22:58.806, Speaker A: So we have this intermediate layer of SGX enabled machines, and content creator creator uploads the content. And by virtue of what SGX offers, only authorized clients can download the content. Video streaming is the most obvious example here but we can also envision processing the data, aggregating the data, and hosting some custom logic for this data management so users can benefit from it not only by downloading the data, but getting some results resulting from the data. And this may be a building block for wider, bigger network of data processing. Okay, so I know it was dense, I know it was quite a lot of information. I hope that you got a glimpse of why the technology stack might be interesting to you. It is definitely interesting to us.
00:22:58.806 - 00:23:01.540, Speaker A: We are going to work on it well and thank you.
