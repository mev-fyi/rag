00:00:00.410 - 00:00:39.560, Speaker A: You. This meeting is being recorded. Okay, we are now recording hi to the couple more people who just joined. Okay, so this is our fourth 4844 breakout. We have a lot on the agenda today. Hopefully we can get through it well, but at a high level I want to discuss kind of where we're at with the implementations get and prism being the two main prototypes working on. And I see some folks here who signaled they wanted to potentially work on other implementations as well.
00:00:39.560 - 00:01:37.290, Speaker A: So talk about where the current ones are, what are the potential blockers and what should we do next to go beyond the current Devnet we have? I know there was a lot of conversations also happening about the libraries to use for KZG and clients, so if you could get a quick update on that, that'd be great. And then Danny, you put out a doc about sync yesterday that was quite good. So we can probably discuss that. And then the last thing I really want us to get to, everything else just kind of a bonus. But there's a bunch of people from the community who want to contribute to this. So if we can take a few minutes to kind of walk through what are some tasks and where people can be helpful, I think that'll be then. Yeah, if we have time to do updates on the ceremony, that'll be great, but we may not get there, I guess to kick it off.
00:01:37.290 - 00:01:45.470, Speaker A: Roberto or Mophie, do either of you want to give a quick update on where we're at with the current prism and guest implementations?
00:01:47.330 - 00:01:50.240, Speaker B: I'll let Mophie take that since I've been out a few days.
00:01:50.690 - 00:02:31.920, Speaker C: Of course, yeah, sure. So where are we at? Yeah, so there have been a couple of spec changes, both in consensus and execution. For one, the fee market updates. We have a fully fleshed out specification for how the fee market and gas pricing should work, and that is being implemented in our execution client geth. In this case, I have a pr open. I had like a pr open that was merged, but there were some bugs in it. I have another pr to fully flesh out and iron out the chunks there.
00:02:31.920 - 00:02:47.700, Speaker C: This change does not include Anscar's most recent updates. This change is like targeting what my client already merged into the spec repo, that is to move the state.
00:02:50.630 - 00:02:50.898, Speaker D: From.
00:02:50.904 - 00:03:36.420, Speaker C: The EVM to the block header and have like a simple gas price targeting rule for the blobs. So that's currently in progress. We're also currently working on the corresponding change in consensus in this case would be the prism client. This work has been going on for some time now. It's taking a little bit longer because we kind of like to be having a framework in the Discord channel. There's some compatibility issues we need to be mindful of when implementing this change and integrating it with Geth. So those are the two main things we're currently working on.
00:03:37.590 - 00:03:57.618, Speaker A: Got it. And I guess on the point of that second pr by Esgar, I know it's been open for a while now. There's been like a lot of back and forth on it. You're on the call. Do you want to give us a quick update of where things are at there? Sure.
00:03:57.724 - 00:04:45.142, Speaker B: So I'm not sure basically how many people have had a look. Basically it's a, I don't know, moderately size change to the fee market. Just basically introducing in the EIP as is right now, we used to want to basically charge an e directly. Basically we had this floating gas price for blops, and now we want to move to a system where basically we introduce a second type of gas that after some back and forth we want to call a data gas. And for now, blobs are the only thing that is being charged in data gas. Basically. I think the pr is kind of mostly ready.
00:04:45.142 - 00:05:43.734, Speaker B: There are some small questions still remaining around. Well, one big one is just that. If people remember, there was this idea that we also wanted to bring 1559 over to a time based targeting system away from a block based targeting system so that we have constant throughput over time, even if there are missed slots. I had an eap there in the past and it turns out it's easier to do if we do this excess accounting that we want to do with four eight four. But there's still some open questions around. Do we want this to be per slot or do we want this to be per second? Because what if slot times change in the future? There are a few kind of attached questions around there just because ideally, once we lock a design in here, we also would want to move the main base fee 1559 mechanism over to this design as well in later fork. So we have to make sure that it also works for the main 1559 mechanism.
00:05:43.734 - 00:06:20.040, Speaker B: And because the gas limit there is currently voted on every block, it's a little bit more complicated. Basically, this whole kind of sub question around time based targeting is still a little bit open. But besides that, I think it's mostly ready. And again, I don't think it's a big change from a conceptual point of view. Of course, implementation wise, there are some tricky issues. I would expect this to be merged, say, next week.
00:06:20.570 - 00:06:41.658, Speaker A: Got it. I think right now this is probably the main blocker for launching your next version of the devnet. What is the. If we can try and get. Go ahead. Right.
00:06:41.744 - 00:06:49.466, Speaker B: Just from my understanding, like in terms of implementation, I assume you'd only want to start kind of implementing the changes once they emerged.
00:06:49.498 - 00:06:49.646, Speaker A: Right.
00:06:49.668 - 00:06:51.978, Speaker B: Because before then it's kind of tricky.
00:06:51.994 - 00:07:04.420, Speaker A: To rely on them, especially if it's unclear whether you're going to go with a time or slot based approach. Right. I can imagine that changes the implementation quite a bit.
00:07:04.790 - 00:07:20.860, Speaker B: Okay, makes sense. Would it be preferable then to maybe try and just merge this pr and then later on have a separate one for moving to time based? Or would it be better to just resolve this first so that we then wouldn't have to change the implementation again.
00:07:23.790 - 00:07:35.840, Speaker E: There is a stub in the implementation for testing. We could merge and say, use the stub for this devnet and have a warning on the other part.
00:07:40.850 - 00:07:43.886, Speaker C: What do you mean by a stub in the implementation? How would that work?
00:07:43.908 - 00:07:51.602, Speaker E: Nonscar? Isn't there like a section on quote for early implementations? Just do so.
00:07:51.656 - 00:07:54.274, Speaker B: Yeah, but that's in the existing API ready.
00:07:54.312 - 00:07:58.420, Speaker A: That's not a new thing and that's a constant price, right?
00:07:58.950 - 00:08:22.430, Speaker C: Yeah, that's kind of like what relied on for the first devnet. But I think one of the goals we want for the second Devnet is to have a more concrete representation of what the spec is so we can start building tooling on top of it and start collecting meaningful metrics and having, using stubs or something like that wouldn't be very useful for the second Devnet.
00:08:28.930 - 00:08:45.282, Speaker E: Right. I guess if we're just balancing trying to ship in the next couple of weeks some sort of Devnet, then it seems like there's going to be uncertainty, at least for the next five days on this gas market. So we just have to find the right trade off there.
00:08:45.416 - 00:09:20.160, Speaker A: Yeah, I guess from an implementation perspective, Mofi, I'm curious, if we were to do it all block based, the current PR kind of points to, and then switch it in a few weeks to time based, is that better because we can move forward and at least have a second version of the Devnet? Or is it going to be so much work to then rip out all the block based fee market that it's not worth it?
00:09:24.310 - 00:10:01.740, Speaker C: It would certainly be way less work than the initial fee market update where we had to change the payload and update both consensus and execution. So yeah, I think we could merge this or implement this now in the devnet for the upcoming devnet. And then if we do move to like a slot based gas pricing time, then that could easily be integrated with the updated devnet and we can iterate from there. It will be easier, I think.
00:10:02.350 - 00:10:39.738, Speaker B: Sounds good. And I think there are some partial changes that are uncontroversial, like changing instead of charging one data gas per blob, we wanted to go to something like basically one data gas per blob byte or something, just so that it's easier later on with the time based changes. But I think that part is uncontroversial. So I think I could get the PR into a form where hopefully can be merged pretty soon and then maybe have like a very small separate second pr a little bit later. If that's. That seems like the most practical way.
00:10:39.744 - 00:11:16.150, Speaker A: To go, then yeah, I think that makes sense. And I think the time based versus slot based might be something that requires broader discussions. I want to make sure that we don't move to time based and then there's some pushback by client teams for a reason or another. And we've implemented all of that already across the different prototypes, whereas like slot based seems pretty uncontroversial.
00:11:18.810 - 00:11:25.510, Speaker B: Right, that's my thinking as well. Basically split up into the uncontroversial PR and then one that we have somewhat time to debate.
00:11:25.930 - 00:11:33.450, Speaker A: Okay, yeah, I think that sounds great. Any other thoughts, comments on that?
00:11:33.600 - 00:11:43.550, Speaker E: When do you think we can iterate on the last couple of little changes in this pr? Anskar?
00:11:46.850 - 00:12:06.950, Speaker B: Yeah, I was basically holding off a little bit to get the timevest resolved, but then if we want to just basically fast trick this pier first, then I'll go through whatever remaining open comments there are later today, and then we can hopefully get this merged by, I don't know, early mid next week at the latest.
00:12:08.410 - 00:13:08.780, Speaker A: Cool. So I guess we'll get this reggae right before Devcon, and then either during or right after Defcon, we can launch another devnet using. Yeah, if anyone's looking into starting an implementation or continuing one of the existing prototypes, they can also just reference that pr. Okay, anything else on that? Okay, then there was another PR that got merged on the consensus spec by George about the reverse bit ordering. And I think the question was whether we want to include. Oh, thanks. Yeah, whether we want to include this in the next version of the devnet as well.
00:13:08.780 - 00:13:18.540, Speaker A: Yeah, I forget what's the reason? We were thinking why we might not include it. Mophie, do you remember?
00:13:22.450 - 00:13:49.240, Speaker C: I think it was more of not exactly cosmetic change, but it's a change that we're making to make protodank sharding fully compatible or more compatible with full dank sharding. And this change tweaks the KCG crypto a little bit to make that happen. But it's not critical for prototype sharding itself.
00:13:51.130 - 00:14:17.440, Speaker D: Yeah, that's correct, but the changes are also extremely trivial in that I think you can implement everything by just reordering two constant arrays like the Lagrange setup and the roots of unity. So in a way it is very easy to implement. But I agree, you can test everything on 4844 only without implementing this.
00:14:20.990 - 00:14:22.410, Speaker C: Thanks for that, caller.
00:14:25.450 - 00:15:16.334, Speaker A: Okay, so does it make sense to hold off on implementing this in the next version of the Devnet? Okay, no strong objections. Sweet. And then on last bit on the implementation, I know there's a couple people who wanted to start looking at different client implementations. So we have geth and prism have prototypes now, but as we get more, that'll be super valuable because we can do some cross client testing. And Terrence and I believe with the help of Danny, you've put together a really good Cl implementer's guide. I'll link this in the chat here. But Terrence, do you want to take a minute or two to kind of walk through that? Sure.
00:15:16.334 - 00:15:16.662, Speaker A: Yeah.
00:15:16.716 - 00:15:17.766, Speaker D: Hello everyone.
00:15:17.868 - 00:16:26.940, Speaker F: So high level wise, I haven't just been thinking about what does it mean for a client team to implement for a four four at a very high level. So I basically break down the documentations into several portions, such as storage requirement. What is the storage increase? Because when we first started a beacon chain, right, we were advertising people to get 1 tb SSD, and that's probably not going to fly anymore with the merge and not with 4844. So that's something to consider and also the networking requirement as well. Just currently we advertise like ten megabytesymmetrical and recommended 25 megabits per second. So how does that affect that area as well? And as of syncing, which I think Danny will cover that a little bit later, what type of validations which we do for syncing, especially just right now, we can do forward syncing, we can do backward syncing, and then you have these edge cases that you can have a block without a blob or you can have a blob without a blog. So how does that work basically? And then last but not least.
00:16:28.910 - 00:16:29.226, Speaker A: How.
00:16:29.248 - 00:16:58.390, Speaker F: Do we treat the fortress mode without a blob? That's definitely something very interesting, because right now we have this notion of optimism mode, which means that the Cl client can still pin the El client to syntu head, but this doesn't make much sense when a block doesn't have a blob, so that's something worth considering. Yeah, so that's pretty much it. Take a look at the documentation and feel free to give feedback.
00:17:00.170 - 00:17:10.730, Speaker A: Yeah, that's a really good overview of all the bits. And if someone's looking into another Cl implementation, hopefully it's useful.
00:17:12.510 - 00:17:29.390, Speaker D: Regarding the bandwidth concerns, is there anything we can do to not have every node consume root of the number of Ps bandwidth for the blob distribution?
00:17:30.930 - 00:17:33.438, Speaker E: Are you talking about the gossip amplification factor?
00:17:33.534 - 00:17:34.530, Speaker D: Yeah, exactly.
00:17:34.680 - 00:17:44.114, Speaker E: It's not root on the consensus layer like six to eight target rather than failing totally, but it still seems very.
00:17:44.152 - 00:17:47.430, Speaker D: Wasteful given the amount of data that we're now considering.
00:17:50.650 - 00:17:57.474, Speaker E: Certainly I would say 1 maybe untenable given the gossip factor.
00:17:57.522 - 00:17:58.218, Speaker A: Yeah, exactly.
00:17:58.304 - 00:18:02.620, Speaker D: So do we have any ideas on how we could reduce this?
00:18:09.390 - 00:18:18.320, Speaker A: So I guess basically the two options are either you gossip to less people or you gossip smaller things, right?
00:18:19.650 - 00:18:32.020, Speaker D: Yeah, smaller things doesn't help if they have the same amplification factor, like whether you make it like one megabytes or like 128 kb times eight.
00:18:33.110 - 00:18:54.678, Speaker E: That doesn't change the actually just having less of payload max. But yeah, I mean, you can reduce the fan out or distribution factor, but then that potentially one increases gossip times and two, I think, begins to reduce the results.
00:18:54.854 - 00:19:03.680, Speaker D: Are nodes aware of their own bandwidth, like do clients or does p two p somehow know this?
00:19:04.210 - 00:19:06.480, Speaker E: I do not believe so.
00:19:10.400 - 00:19:30.470, Speaker D: Because one way, if nodes knew this information, right, then we could simply say nodes that know that they are on a low bandwidth connection, say less than 25 megabits per second, just reduce their outgoing amplification. And maybe they can also set a flag to their peers saying, hey.
00:19:32.520 - 00:19:32.896, Speaker A: Don'T.
00:19:32.928 - 00:19:40.250, Speaker D: Just send me the payload, just give me a notification that is available and I'll ask one peer for it.
00:19:41.740 - 00:19:42.490, Speaker E: Right.
00:19:43.340 - 00:20:26.970, Speaker D: My suspicion is that we have a very dense network of high bandwidth nodes, right? I would say most nodes probably easily have 100 megabit or more, and quite a few will have gigabits. And so if we simply make it so that these nodes just distribute among themselves very quickly, then the other nodes can easily get it from them. I mean, that may not be like the perfect solution for the ultimate sort of sharding implementation, because it certainly introduces some centralization vectors among nodes. But I think for 4844, which is kind of a temporary thing, it may just be good enough.
00:20:31.810 - 00:20:43.054, Speaker E: Yeah. Implied in your statement was also push versus pull, essentially like can some of these nodes pull it down rather than getting it gossip.
00:20:43.102 - 00:21:02.490, Speaker D: Exactly. And these should be the lower bandwidth nodes. And then we can still get extremely fast distribution peer to peer network because all the high bandwidth nodes just gossip it as usual. And the others with exceeding probability will have one of those high bandwidth nodes among their peers.
00:21:03.150 - 00:21:54.460, Speaker E: So the easiest way to do so without doing deep changes to gossip sub is essentially have the blob. Assuming that these are two separate network payloads, having the blob sidecar be an optional topic. And that if you get a header, if you're not on the blob sidecar and you get headers or beacon blocks, then you then go and ask your peers for request and you actually know which peers to ask for the request because you know which peers are advertising that they're on that topic. It could potentially work. There's trade offs here and it's kind of a hack. How do users configure that value? And then you're kind of like shifting the honesty of the healthy mesh and that kind of stuff.
00:21:58.910 - 00:22:19.620, Speaker C: So one thing to note is that blob validity is tied to the blobs now. And I'm curious to know what were the arguments, if there were any, for the beacon blocks to always gossip them rather than do like a pull based model as we have now.
00:22:22.630 - 00:22:47.130, Speaker E: The beacon blocks, yeah, I guess in general it's very important. This is a very important message that literally every node gets. There is a poll based backup with the gossip sub. Chatter I want I have. But it's also just a product of kind of using the gossip sub stuff off the shelf. It's primarily a gossip protocol.
00:22:47.950 - 00:22:48.940, Speaker C: Got you.
00:22:51.310 - 00:23:17.622, Speaker E: In terms of designing push versus pull, there are strategies where you're kind of like gossiping to some amount of peers and you're chattering to others. And that's kind of what's happening here. But because it's a high value distribution message, you have to gossip to some amount. Otherwise it becomes very slow. You're pretty much gossiping, but just in a slow method. Right.
00:23:17.676 - 00:23:29.900, Speaker C: So I asked this because doesn't that rationale extend to blobs given that we cannot validate beacon blocks until the associated sidecar is available?
00:23:30.910 - 00:23:31.660, Speaker E: Right.
00:23:35.230 - 00:23:56.686, Speaker D: The argument here that I bring is not that they shouldn't be gossiped. My argument is that it's enough to gossip it to some of the nodes. I think if we wanted to, we could do the same thing for the beacon blocks. It's just less urgent because they tend to be smaller.
00:23:56.798 - 00:23:58.898, Speaker A: Right. So there's likely maybe I think the.
00:23:58.904 - 00:24:10.530, Speaker D: Current system in my opinion, is very wasteful. Given more engineering work, we could build a much better system for this. I think that has much less overhead.
00:24:10.610 - 00:24:20.074, Speaker E: But some of the overhead here is also in resilience to attack. That's part of the gospel. Redundancy is not always.
00:24:20.192 - 00:24:40.238, Speaker D: Yeah, no, but there are better ways to achieve redundancy than just sending copies, right? So basically erasure coding. So I think there are much better systems than what we have now. The system we have now is just very simple. The better system would require more engineering work for sure. Yeah.
00:24:40.404 - 00:25:27.680, Speaker E: Setting that aside, Mophie, I think it is very important that both these messages are widely gossiped to the network. The data availability check is get all of that data. I think the intuition here is if the strategy becomes partial push versus pull rather than just full push or full pull, then maybe there's still a distribution time in that trade off there. That still is reasonable. It's like if 50% of the network is pushing and have waste and 50% is pulling and thus is a little bit slower, we might still be able to get a distribution model that's like sub that 4 seconds that we're good with. But that's just a hope on that trade off space.
00:25:29.010 - 00:25:30.640, Speaker C: Got you. Thanks.
00:25:31.810 - 00:26:01.514, Speaker D: Is there a way to only tell one of your peers that you're subscribed to a subnet? So say I know that I'm a low bandwidth node, I still want to get the blobs, but I don't want to get a lot of copies from them. So I only tell one of my peers or maybe five of my peers, so that the expectation is I get one blob that I'm subscribed to this. I mean it's a bit hacky, but.
00:26:01.632 - 00:26:05.340, Speaker E: That might also, I mean off the shelf probably no.
00:26:05.710 - 00:26:06.460, Speaker D: Yeah.
00:26:08.750 - 00:26:25.940, Speaker E: You could imagine some additional configurable parameter that wouldn't be too difficult to get into there. But then the kind of analysis of gossip sub all the attacks that it's presumably resilient against, I think you begin to degrade something there.
00:26:32.640 - 00:26:48.960, Speaker A: Is this something we need to test somehow? Like obviously we know we're introducing a bunch more requirements. The gossiping level. It seems unclear how much more we can introduce and what the effects of that are on the network.
00:26:51.380 - 00:27:26.524, Speaker E: We definitely need to test. If we we night don't anything we need to figure out what is a safe gossip distribution number. What is a safe number for this or we're not happy with that safe number. Then we need to be making engineering changes. Pop on our end is opening up and beginning to do some simulation analysis and hopefully we'll have something, at least on the bare bones. If we assume x distribution of bandwidth and 1 blocks, then this is what it looks like before Devcon. But there's definitely some additional work to do here.
00:27:26.524 - 00:28:02.330, Speaker E: This is like one of the things I'm most concerned about on current 4844 is that we don't know what the network can handle in terms of pushing this data around. The 1 safety assumption comes from Starkware's I believe Starkware's big red dot analysis from 2019 or 2020, where they pushed around large blocks and paid for them large, relatively uncompressible blocks on main net and showed that the uncle rate was not greatly affected. That's the best we have right now. I'll share that link.
00:28:04.780 - 00:28:05.240, Speaker A: Yes.
00:28:05.310 - 00:29:02.956, Speaker G: By the way, I wanted to bring something, so maybe I will bring it now because it is somehow related. I'm not really comfortable because of the fact that we don't know those numbers, and I feel that it has impacted the choice of KZG. And for example, when I read the argumentation that says that we cannot use an alternative to KZG, the main argument is that either it won't be compatible with data availability sampling or it will consume more data. But I'm saying that the impact of KZG, the fact that it require a trustee setup, is huge. So I think we should spend some time to do those analysis and see how much we can handle in terms of bandwidth and storage and stuff like that, and then decide if we can use another polynomial commitment scheme. I don't know if that's clear, but.
00:29:03.138 - 00:29:25.270, Speaker D: Yeah, can you name a concrete commitment scheme? Because we have done this analysis that's been done for years. Right. This is not a new idea. We have for a long time thought about stocking mocker roots. We have sort of using fry directly. They're all very far from being practical, like very far.
00:29:25.640 - 00:29:29.716, Speaker G: Okay, very far. So it is very true that we can choose them.
00:29:29.898 - 00:29:34.440, Speaker E: And Doctor, the IPAs are in order of magnitude larger.
00:29:35.180 - 00:30:16.660, Speaker D: Yeah, IPAs are definitely quite a bit larger. And they also only give you. So proofs would be like several kilobytes, I think, like around five to ten or so, depending on which exact scheme you use. And there's also a big problem in that there's no efficient algorithm to compute the proofs. So that's a major downside as well, which we have for KCG. Yeah, so IPAs would also only solve the trusted setup problem and not be post quantum so to me personally, the trusted setup is a much smaller problem than it not being post quantum.
00:30:21.160 - 00:31:13.770, Speaker H: Somewhat related to bandwidth concerns. I'm working on setting up sort of like a community cluster for observability around the 4844 testnet. So I'm essentially only running it on my nodes, but I am measuring a bunch of infrastructure metrics. I can also add network metrics to that, and then hopefully some point next week I can give broader access to some dashboards and things like that. So at the very least we have some baseline for what the current testnet is using. The only concern, and I guess question I have right now is how good of a signal are these metrics from a testnet, considering the testnet is fairly small right now.
00:31:15.580 - 00:31:19.000, Speaker E: Sorry, if you were to run like large blocks on a testnet.
00:31:20.060 - 00:31:20.810, Speaker H: Yeah.
00:31:23.340 - 00:32:13.688, Speaker E: I think one, they're small, and two, the distribution of nodes does not necessarily reflect that of Mainnet. Mainnet might have one way more highly powered nodes and way more home nodes. Even people that are running on testnets that are home stakers might be using cloud instance because one, they don't care about the security of that, and two, they don't want to overload their local bandwidth. I don't think it's super representative. And then when we get into simulation, it doesn't mean that it's not worth doing the experiments, but we just have to try to contextualize. And then we go into simulations or anything like that, and then we're going to be guessing the distribution of what nodes look like. We can certainly do some worst case, say there's 10,000 nodes and they're ten megabits per second and see what happens.
00:32:13.688 - 00:32:18.030, Speaker E: And you can also do some pen and paper analysis, but it's hard.
00:32:18.960 - 00:32:30.944, Speaker D: Can you remotely analyze the upstream of nodes somehow? I'm not 100% sure.
00:32:30.982 - 00:32:36.210, Speaker E: I know you can try to measure round trip latency, but I don't think that really.
00:32:39.620 - 00:32:50.150, Speaker D: Yeah, I just was just wondering if you basically just dos each node for like a second and see how much you can get through or something.
00:32:51.720 - 00:33:35.604, Speaker H: Yeah, so one of the related things that was being discussed in sharded data was writing sort of like a spammer tool to just spam blob data to a node. And the primary reason I'm setting up some of this observability stuff is so that I can spam my own node and measure performance that way and some other analysis later on. But just for this, I don't know, we could set up a spammer to do like a certain size of blob at a certain frequency on just a single node and see if we can extrapolate anything from the metrics we see from that. But I'm not sure, again, how accurate that would be.
00:33:35.802 - 00:34:33.830, Speaker A: I think that'll be helpful for a different reason, though, because you want the throughput on the network to be well below what a node can handle in the worst case. So when you think about, I think spamming a single node, if you know that your node can process, I don't know, like 20 megabyte blobs per slot, and we're considering going with two, then at least we know we're safe on that front. But if at three megabyte blobs per slot, your node has issues like staying in sync or anything else, I think that's really helpful to know. So it's different from the gossiping, but it tells you, can your node actually process the amount of stuff that it's receiving on main net with like a really large error bar or margin of safety? Basically, yeah. Okay.
00:34:34.280 - 00:35:16.390, Speaker E: Another potential idea that Donker and some other guys were discussing was assuming we stay in a low gas paradigm, maybe even on the weekends doing some sort of analysis, you could potentially abuse call data in a way like Starquare did a couple of years ago, and send large blocks on main net, but potentially do a better analysis. So maybe have some sentry nodes see the different timing, maybe go to various client teams and other operators and get logs of when things were received and try better data on main net.
00:35:16.840 - 00:35:44.008, Speaker D: I mean, the best possible metric you can get is actually just seeing, looking at attestations. So I would argue that setting up a few nodes that just watch all the attestation subnets and see the delay for each validator of their attestations can give you a lot of information already, because it basically tells you, at least for all the staking nodes, how well they are doing at processing those blocks.
00:35:44.184 - 00:35:55.200, Speaker E: So we'd probably want to know when random nodes around the network get the blocks, when random networks around the network get attestations and watch chain data for blocks and attestations.
00:35:58.420 - 00:36:06.230, Speaker A: Yeah. Would 1559 make this harder to do?
00:36:06.840 - 00:36:10.292, Speaker D: I mean, you only need to do it for like ten blocks or something, right.
00:36:10.346 - 00:36:12.644, Speaker A: Okay. So your gas price will just go.
00:36:12.682 - 00:36:16.704, Speaker D: Up like a little bit. What does ten blocks do?
00:36:16.842 - 00:36:18.324, Speaker A: I think it's more than ten blocks.
00:36:18.372 - 00:36:22.170, Speaker D: Maybe four x. Yeah, about a bit more than two x.
00:36:24.540 - 00:36:25.316, Speaker E: But it's reasonable.
00:36:25.348 - 00:36:28.692, Speaker A: It's not 40 x. Yeah, it'd be a good experiment.
00:36:28.756 - 00:36:37.410, Speaker E: The thing is, if the experiment doesn't do well. It's hard to iterate on that experiment, but it could at least give us some information and know kind of which direction to go in.
00:36:38.980 - 00:36:52.610, Speaker A: Yeah, I think that would be actually a really cool thing to do. And do we know how much this abdel, if you could find out how much it cost.
00:36:54.840 - 00:36:59.572, Speaker D: From the gas cost, we know exactly how much it's going to cost. Yeah, it's very easy.
00:36:59.706 - 00:37:00.790, Speaker A: Just need to find.
00:37:01.660 - 00:37:05.880, Speaker E: You can have the experiment wait until some minimum gas cost is started.
00:37:05.950 - 00:37:06.664, Speaker D: Exactly.
00:37:06.862 - 00:37:48.950, Speaker G: So I have a follow up question. Doncal, can we leverage the efficiency of KZG to improve the data availability guarantees on the protocol? I mean like for example Edo from Stackware, which is on the call. Thought about a system where you could do some random queries based on random data, like Rhonda for example, and to do some random queries that will be included in the block to enforce, because for the moment we trust and we rely on honest validators implementation. But can't we leverage the efficiency of KZG? I don't know if that's clear. Maybe, ido, you want to jump in and explain what you had in mind?
00:37:50.280 - 00:37:56.932, Speaker D: Sorry, this is completely off topic. It feels at the moment, but I'm happy to answer that question. Private messages.
00:37:57.076 - 00:38:21.820, Speaker A: Yeah, just because we have only 20 minutes, I would agree. If we can move this to the discord, let's do that. Yeah, but yeah, I think it would be worth it to try and back to the starquare thing. If someone wants to look into how we could replicate and adapt it, I think that would be really valuable.
00:38:22.720 - 00:38:49.770, Speaker D: And also getting, creating those blocks is trivia. Right. I can create a script that will do this. The only thing that needs to happen is that someone needs to set up the instrumentation so that we actually get good data from it. I mean, we will get some on chain data. Just on chain attestations would already be pretty interesting, but it would be a bit wasteful not to have some nodes that simply record all the attestations and give us much more data.
00:38:50.460 - 00:38:58.220, Speaker E: Yeah, so using sentry nodes or using the diversity of nodes that maybe client teams are already operating.
00:38:59.520 - 00:39:15.440, Speaker D: I think there are also operators who already do this, who basically watch the whole peer to peer network. So just getting in contact with one of them, if they could simply be part of the experiment and give us that data and then have to do it ourselves.
00:39:16.340 - 00:39:23.830, Speaker A: Yeah. If somebody wants to do this, what's the list of metrics they should be asking.
00:39:27.720 - 00:39:47.740, Speaker E: We want for a given node, when did they first see a block. When did they first see every single individual attestation that they got off the wire? And I think that's probably it because then everything else is chain data because then you want to see were blocks orphaned and did you have high attestation inclusion rates?
00:39:48.880 - 00:39:54.460, Speaker A: Okay, so just first time to a block, first time to every attestation for each block.
00:40:02.500 - 00:40:25.896, Speaker E: It's best if these are maybe fully connected nodes that see all attestations because they're on all attestation subnets. But then all of a sudden that's potentially a biased node with respect to where they sit in the mesh because they're so well connected. But I don't know how much that data coupled with many of those nodes across the world I think would still be very good.
00:40:26.078 - 00:40:38.696, Speaker A: And should we try and replicate lowish bandwidth? Like you could imagine doing this with like a ten megabyte node, 25 100 meg and then a gig.
00:40:38.888 - 00:40:40.056, Speaker D: Yeah, I mean if we were willing.
00:40:40.088 - 00:40:46.700, Speaker E: To not just use other people's nodes but provide our own sentry nodes then I would do a distribution of sentry nodes.
00:40:47.440 - 00:40:57.890, Speaker F: We can do that pretty easily from our main net nodes and then I also have a few at home nodes, I can set those up as well. I think we have the infrastructure to monitor those data already.
00:40:59.640 - 00:41:00.436, Speaker A: Nice.
00:41:00.618 - 00:41:03.670, Speaker D: But do you have a way to record it.
00:41:06.360 - 00:41:06.820, Speaker A: Today?
00:41:06.890 - 00:41:09.510, Speaker F: We capture the arrival time for everything.
00:41:10.200 - 00:41:10.996, Speaker A: Okay.
00:41:11.178 - 00:41:17.204, Speaker D: And you have nodes that would be powerful enough to just subscribe to all gossip sub channels?
00:41:17.332 - 00:41:23.850, Speaker F: Yeah, that's not hard either. We just have to basically upgrade the instance and then just add more peer and stuff.
00:41:25.180 - 00:41:34.876, Speaker D: If you think that's easy for you to do then I think yeah, that would be great. We should just do it and who knows how long gas will be cheap. So let's do it soon.
00:41:35.058 - 00:42:00.390, Speaker A: Yeah. And I guess for a gas price to pay for the gas. Okay. I was skimming this starquare one, the Starquare post, and they said they did this over a range of like 6000 blocks. Is that roughly the duration we'd want?
00:42:01.320 - 00:42:07.272, Speaker E: I think we'd want like a burst or a handful of bursts. I don't think we're going to be doing a 6000 block test.
00:42:07.326 - 00:42:11.764, Speaker A: 6000 blocks is like 20 hours or it's like a day basically on proof of work blocks.
00:42:11.812 - 00:42:12.410, Speaker E: Yeah.
00:42:14.240 - 00:42:22.300, Speaker D: I think the shortest and most intensive burst possible is what gives us the most information.
00:42:22.450 - 00:42:23.260, Speaker A: Right. Okay.
00:42:23.330 - 00:42:28.700, Speaker D: Like we'd rather have ten two megabytes blocks than 20 1 mb blocks.
00:42:28.860 - 00:42:29.890, Speaker A: Got it.
00:42:32.180 - 00:42:43.284, Speaker E: And it's something that we can do this for a minute or two and then see what's going on? And then if we want to do additional analysis, assuming gas prices stay low, we can.
00:42:43.482 - 00:42:47.700, Speaker D: I guess we should also ask ourselves before, is there any chance that we.
00:42:47.770 - 00:42:49.270, Speaker A: Might actually break it?
00:42:49.960 - 00:42:54.692, Speaker D: Should we do, like a one or two block test first and to see, like.
00:42:54.826 - 00:43:02.040, Speaker E: Yes, I guess if I break it, I have no worry that it would recover.
00:43:02.780 - 00:43:03.530, Speaker D: Right?
00:43:05.180 - 00:43:05.988, Speaker A: Yeah.
00:43:06.174 - 00:43:11.528, Speaker E: Those validators might want compensation for their missed attestations.
00:43:11.704 - 00:43:42.852, Speaker A: Okay, we're going downhill quickly here. Okay. I guess, Terrence, on the prism side, is there anything, do you need help from anyone else, or is this something like your team can just set up and then we can get somebody else to work on just building the blocks and scheduling when the actual kind of test would happen. Right.
00:43:42.906 - 00:43:55.950, Speaker F: I think a good place to start is just like a one pager, like a requirement so we can put those on paper and then share with the necessary party. And I think that's a good place to start. And once we have the one pager, I think relatively, it's pretty easy.
00:44:00.880 - 00:44:05.950, Speaker E: Terrence, is it easy for others that are running prism infrastructure to get similar data?
00:44:07.040 - 00:44:13.440, Speaker F: I will probably publish, like, a branch with style modification. So as long as they update to that branch, it should be fine.
00:44:13.510 - 00:44:35.540, Speaker A: Yeah. And then one thing that would be neat is, yeah. If we ask across other client teams, it would be good to sanity check that at least another. If there's another client team for whom it's easy to get this data, getting just two that roughly aligned would make at least me feel much more comfortable.
00:44:35.620 - 00:44:44.410, Speaker E: Yeah, prism might be really well connected or really poorly connected due to something that we're not aware of. So getting another one would be nice.
00:44:45.020 - 00:45:19.350, Speaker A: Yeah. And I'm sure we can find some other node operator somewhere like who's not a client team who wants to do this or already basically records all of this, whether that's like a staking provider or like a team like block native or someone that has highly connected node. Sweet. Yeah, no, this was really good. Is there someone who feels like they want to take on writing this one pager of requirements and sharing this with the group? Here.
00:45:24.450 - 00:45:28.990, Speaker E: I can help. I'll make a document right now and start filling in some notes and share it here.
00:45:29.140 - 00:45:46.094, Speaker A: Okay, awesome. Thanks. Um, sweet. I think. Yeah. So we spent, we have only like ten minutes left, but I think this was quite valuable. Oh, yeah, migaslab.
00:45:46.094 - 00:46:07.420, Speaker A: That's a good one. Thanks, Terrence. Okay, so, yeah, we had a couple more things, but quickly, on the KZG library side, are there any notable updates there that people wanted to share I know there's the CKZG effort that's going on. Denkrad, you maybe want to quickly give a quick update on that.
00:46:08.590 - 00:47:09.680, Speaker D: Yeah, so Ramana and I used built on top of Ben Etchington's work, CKCG, a library that has low level implementations of all the functions necessary for eight, four, four. It's built on BLSC, so it's all pretty fast and everything's a NC. And. Yeah, I mean, right now we're just basically looking for a client or clients that actually want to use these functions so that we can build an API together. Basically, people were, I think, a little bit unhappy with the blast API, so I think it would be worth making something that actually works for clients and makes their life. So, like, if there's anyone on this call who says right now we need a library for this, or we need a faster library than what we have now, then would be great to connect.
00:47:12.050 - 00:47:19.460, Speaker A: Nice. And I guess, yeah. Mophie on the get side. Do you think that's needed right now?
00:47:24.070 - 00:47:58.010, Speaker C: Possibly. So, one thing I just recently realized from Terrence's write up of the implementation notes is the current implementation to computing the proof from the blobs is not quite as efficient as I would hope. And I was going to look into what we could do to optimize that once I'm done with the Devnet, but I'm not sure if there are any further.
00:47:58.090 - 00:48:17.140, Speaker D: So we have the proof computation implemented in CKCG, and it's fairly well optimized. So you can use that. Yeah, the only thing is not yet, it's not parallelized, but we could do that. There's a simple way to make it parallelized as well, but it wasn't a priority so far.
00:48:18.630 - 00:48:20.642, Speaker C: Okay, yeah, I'll definitely.
00:48:20.696 - 00:48:26.280, Speaker D: But it uses like, pippenger for doing the multiscalar, so it's quite fast.
00:48:27.530 - 00:48:28.278, Speaker C: Cool.
00:48:28.444 - 00:48:48.720, Speaker A: Yeah. Alexa said in the chat they're from nethermind and they're looking for an implementation, so it might be neat for them to use that to start and so we can get some feedback on it. And it might be easier to use it from the scratch than to swap whatever's in guest already.
00:48:50.210 - 00:48:50.960, Speaker D: Cool.
00:48:51.410 - 00:48:54.730, Speaker A: Yeah. Alexa, I assume you're in the discord.
00:48:54.890 - 00:49:00.210, Speaker D: And the chat telegram is better for contacting you than discord.
00:49:01.670 - 00:49:22.946, Speaker A: Cool. Sweet. Okay, and then next up, Danny, you had a document about sync coupling, which is something we've talked about for many weeks. Do you want to quickly recap?
00:49:22.978 - 00:49:47.678, Speaker E: Yeah, I can go over that. I was just thinking about it the other day and wanted to jot down some notes, pretty much. There's two things. It's gossip and historic sync gossip we currently have. The sidecar approach rather than coupling. Historic sync, I think is still to be defined or still being debated more. And there's two things that I think we want to minimize.
00:49:47.678 - 00:51:08.314, Speaker E: It's the complexity of the change going into 4844, and then the potential complexity and the change going into full dank sharding. I make an argument that on the kind of gossip approach with the sidecar, this does mimic some of the potential problem that we're going to see given kind of like the race condition between these two type of message types on gossip, because we will see it in full sharding, because we're distributing rows and columns rather than the full blocks. But you still have the kind of the same thing. But I also argue that the signature approach that we're using today doesn't actually really mirror what will happen in full dank sharding unless builders are bonded and they have a signature and they could potentially be slashed. And so because of that, I do question on the gossip, and I think originally was arguing for decoupling as they are today. But I do question the value of the decouple now, given that, I don't think it maps directly to what gossip looks like in full dank sharding. That said, the decoupling does allow us for alternative push versus pull methods that we've been discussing today.
00:51:08.314 - 00:52:03.994, Speaker E: So in that context, my argument, I'm more than just kind of laying off the trade offs. And if we are going to be engineering some sort of push verse pull, we certainly would want them to be decoupled. Additionally with the historic sync, we have these like blocks by route, blocks by range request where you can request one or many blocks by a range or by a specific route. I think coupling here is bad because I think it kind of messes up a relatively robust mechanism by putting in a bunch of additional conditional logic, especially because the pruning depth of blobs is going to be different than the pruning depth of beacon blocks. And so now you have kind of this stuff you have to handle. Maybe there's not a route, maybe there's not a blob, maybe there's. Do you want the blob, like different kind of stuff.
00:52:03.994 - 00:52:44.406, Speaker E: So I think it's easier to put it as an adjunct protocol with kind of these parallel methods rather than coupling. And if we coupled today, we would add that complexity and then would have to remove it in full dank sharding so I think it's much more clear to me that the coupling on the historic sync requests adds more complexity today and adds more complexity in the future. The coupling on the gossip. I could potentially go either way. I think that if we weren't doing any sort of sophisticated push pull. Great, we probably should just have them coupled. I think it's much simpler and doesn't buy us too much in the future by doing the decoupling.
00:52:44.406 - 00:52:48.860, Speaker E: But if we do want the push pull, we should keep them decoupled. Thank you for coming to my TED talk.
00:52:51.230 - 00:52:53.500, Speaker A: Any thoughts on that?
00:52:58.290 - 00:53:12.962, Speaker C: I guess it makes sense, but I also think if we do couple either distribution or peer to peer, it has to be consistent. Otherwise you end up with a case.
00:53:13.016 - 00:53:13.620, Speaker A: Where.
00:53:15.750 - 00:53:34.230, Speaker C: You gossip like the block, and for some reason the sidecar doesn't appear, doesn't observe the sidecar, and then the node has to make a request for it. Would you rather have the full coupled payload or just keep them separate?
00:53:34.390 - 00:54:27.114, Speaker E: That's an argument. I think, again, I'd call that quote historic. That's not quite historic retrieval, but that's also another argument for keeping them decoupled on historic retrieval. Just because you're more likely to get a beacon block than maybe this blob, and if you have one and you don't have the other, you're going to want to make a direct request, and you don't necessarily want the thing you already have. So that's a blobs by root request or beacon by root request. But I don't necessarily think that if gossip is coupled, that you don't want to decouple the historic requests, because decoupling the historic request kind of seems independent, even if you receive these things in tandem on gossip, no problem. If you're getting the historic request, you're saying specifically what I want.
00:54:27.114 - 00:54:35.594, Speaker E: And so it doesn't have the same issue of information being missing. I don't think I was very clear.
00:54:35.632 - 00:54:36.426, Speaker C: In that response, though.
00:54:36.448 - 00:54:37.234, Speaker D: I apologize.
00:54:37.382 - 00:54:44.400, Speaker C: No, you said, makes sense. I'm just thinking it through. Yeah, that makes sense.
00:54:47.830 - 00:55:06.534, Speaker A: And I guess just for clarity, for the next version of the devnet, we probably don't need to change what we're doing for sync, but then probably the one after that we would want to. Does that make sense?
00:55:06.652 - 00:55:14.200, Speaker E: Apologize for my ignorance. Do we have these signed blobs by range requests in the PDP spec right now?
00:55:15.770 - 00:55:18.118, Speaker F: I don't think they're signed. I think they're just blobs.
00:55:18.214 - 00:55:19.210, Speaker D: Okay, blobs.
00:55:19.870 - 00:55:21.500, Speaker E: I do put a note on.
00:55:23.390 - 00:55:23.766, Speaker A: There'S.
00:55:23.798 - 00:55:51.566, Speaker E: Probably no hurt in making it a signed variant, but I don't know if it's actually that valuable to have them signed for the historics, because you don't necessarily even know the proposer id when you're getting these historic blocks, so you can't necessarily validate them independently, whereas when you're in the head within a certain slot range, you do know the proposer id, so you can pre validate before you get the beacon block, I guess.
00:55:51.608 - 00:56:17.920, Speaker F: Another thing I was thinking of the other day, which is interesting, because right now the blobs are not chain, right? They don't have the parent field. So for example, for blob today, when you backtrack, thinking you can just get the children, if the children is valid, then you can ensure all the assessors are valid. But blobs don't have this property. And I wonder if it's useful to add that property in there.
00:56:22.850 - 00:56:26.880, Speaker E: In the signed variant or just in blobs in general.
00:56:27.970 - 00:56:36.740, Speaker F: Just in general, where you can just say if the children is valid, then the parents must be valid, but for now you have to verify them one by one by one by one.
00:56:37.190 - 00:56:43.698, Speaker E: Right. I don't know if you could actually shoehorn that into the commitment scheme.
00:56:43.874 - 00:56:48.886, Speaker F: I see, I was thinking like hashing all the commitment and just made that as a parent route or something, but.
00:56:48.908 - 00:57:00.154, Speaker E: That'S probably a bad idea right now. I'm sorry. Talk to us.
00:57:00.352 - 00:57:49.690, Speaker A: Yeah, thanks, Danny. Yeah. If some folks can stay on another five or so minutes, I think the last thing that would be important to cover is there's a bunch of people who aren't part of client teams and want to contribute to this. And it's kind of a first to have this many folks wanting to contribute. The hard bit is, I think, finding what are useful tasks that are pretty well defined or that no one is already on where they can have an impact. I guess I'll just open the floor here. Does anyone have something they feel like would be really important that nobody's looking at and if somebody else could take it, it would make their life easier.
00:57:49.690 - 00:58:04.640, Speaker A: Okay. Please.
00:58:05.330 - 00:58:40.390, Speaker I: So in terms of implementation, there are the obvious candidates around already. We have Prism GAF prototypes and then a lighthouse prototype that was started during Berlin. In terms of tooling, there's a lot to build, so if you want to start smaller, I would recommend starting there. One of the things is more tooling to create blobs and insert transactions or integrate it into existing tooling like foundry, and just having some kind of expo to view the blobs that are being confirmed on the Devnet would be really useful.
00:58:43.550 - 00:59:18.422, Speaker A: Yeah. Explorer to visualize the blobs, that would be great. And then in terms of the implementations themselves. So as I understand, Prism and guess are obviously the most advanced lighthouse, if we can get a link to the prototype, I'll keep track of that. The Nethermine Alexa said they're starting to look into it. I believe Trang, who's on the call here, is going to start looking at an Aragon implementation as well. So I'll try and keep track of all of those.
00:59:18.422 - 01:00:06.326, Speaker A: And I've put together sort of checklist like we had for EIP 1559. If you start working on an implementation, you can just open a pr and link it there. And then if you have issues on your implementation that you need help with, I think that's helpful because people can kind of go through that. The other bit, I think that would still be really good, is just testing. So we have a little bit of consensus tests that I think Danny put together. I don't think we have anything on the execution layer yet, unless I've missed it. So if somebody's keen to look at basically the hive or the state test and.
01:00:06.326 - 01:00:38.670, Speaker A: And dive into that, that would be quite valuable. Anything else? And then we talked also about basically the spamming of like a single node. Booklearner, I believe you were talking about that. I think that would still be quite valuable. Like kind of getting performance metrics on one node that's being spammed by blobs and see if it stays in sync and whatnot.
01:00:39.890 - 01:00:47.700, Speaker H: Yeah, I'm working on that, so I can continue working on it. If anyone else is interested, please reach out. I'd be happy to give you access to my node and stuff.
01:00:51.850 - 01:00:53.414, Speaker A: Someone was saying something.
01:00:53.612 - 01:00:55.126, Speaker H: Yeah, I was just going to say.
01:00:55.148 - 01:01:48.360, Speaker A: That I think something that's more generic, that's generally always useful in these kinds of scenarios, is writing either summaries or comparisons of open issues. So if somebody was interested in contributing, I think it'd be cool to write just an overview of the different ideas, maybe for sync, like comparing two different things and just like laying out the conversation points that have been had in calls on the various different threads that we've been discussing. And I think, yeah, Danny sort of done that for sync. I think the bit where it might be helpful is like for the libraries, just like looking at what's there and what the trade offs are. I don't know if there's any other areas like that.
01:01:51.070 - 01:02:44.860, Speaker C: Yeah, that's good. I just wanted to add something that as we get more client implementations, alternative tooling, it would help to have test vectors against what we have in the spec so that everyone is on the same page of what certain outputs should look like. So for example, I think who was it? Marius brought up like a bug in our implementation of the SSE route for the newly updated beacon block. And I think there was a mismatch between theirs and ours. And it turns out there was a bug in our fork of prism and with the test vector it would have been easy to cross check what we're doing was correct.
01:02:48.430 - 01:02:53.450, Speaker A: Got it. And what's the right format for those test vectors? Just like JSON tests.
01:02:54.610 - 01:02:57.280, Speaker C: Yeah, I think JSON works fine.
01:03:00.930 - 01:03:50.320, Speaker A: Yeah, I know Marius had like the ones for Empora. I can't find the link now, but yeah, I think I'll try and find the link to those and kind of share them as an example of what it looked like for the very early merge ones. Okay. And then Proto added some thoughts on the explorer. I'll copy all of your comment proto in the notes for this call. Anything else in terms of tooling mofi, you have like this blob utils, Repo. Is there anything there that you've been meaning to do but just never got the chance?
01:03:54.690 - 01:04:26.620, Speaker C: Well, sort of related is Proto has this PR in prism. So right now the only way to download blobs is if you're the peer to peer network. But ideally this should be done using the Beacon API, even if it's like an internal API. So you can just talk to your beacon node directly and just download the blob that it already has. I would like that PR merged. I haven't had the time to take a look at that. It would be helpful if someone could.
01:04:26.620 - 01:04:29.146, Speaker C: I can link it in, but we.
01:04:29.168 - 01:04:41.470, Speaker I: Could like, I can rebase and polish it. I just need a target to test against. So it needs to be clear which brands to use and which testnet.
01:04:42.530 - 01:04:45.150, Speaker C: Okay, I'll stick with you offline then. Proto.
01:04:50.550 - 01:05:50.070, Speaker A: And I think the one other bit that would be valuable is like Mophie. We have your Devnet guide for Devnet one. I think if someone wants to polish that and make it, if somebody's going through that basically and stuff is not obvious or anything, kind of extending that, I think over time, making it easier and easier for people to join the devnets and not have to run a bunch of custom commands or if they do, knowing what the failure modes are is really valuable. I'll link that as well. But just like documenting if you're playing around with this stuff and finding some edge cases or issues like documenting what you did to make it work so that the next person, it's slightly easier for them is really valuable.
01:05:50.890 - 01:05:52.194, Speaker C: Yes, totally.
01:05:52.322 - 01:05:52.854, Speaker A: I think. Yeah.
01:05:52.892 - 01:06:02.810, Speaker C: A couple of people have had issues connecting to the Devnet and like a troubleshooting section should. Easy way to figure out your problem should be really helpful.
01:06:09.680 - 01:06:35.682, Speaker A: Cool. Yeah, I think that was worth having as a conversation. Anything else people think we need help with? Okay. If not. Yeah. Just as we're closing, I've put together this sort of checklist like we had for 1559. I mentioned it before, but I've just entered the chat here.
01:06:35.682 - 01:07:09.630, Speaker A: If you are working on the client implementation or start working on test vectors or whatnot, please add your stuff there and so other people will be able to see. I'll try and add all of the stuff we discussed and mentioned on this call to it today. So it's pretty up to date. And I think that'll be just like an easy place where we can track all the different things that are going on and then with like less than a minute to go. Trent, where is the best place for people who want updates on the whole KZG ceremony?
01:07:12.530 - 01:07:26.050, Speaker E: There's a timeline document, but generally there's a best place. Is the repo in Ethereum KZG ceremony. Repo has a bunch of resources and a link to the timeline document.
01:07:26.950 - 01:07:27.362, Speaker A: Cool.
01:07:27.416 - 01:07:29.202, Speaker D: But the TLDR is that it will.
01:07:29.256 - 01:07:35.926, Speaker E: Launch post Devcon run for two months, and then there's a special contribution period. And we have grants available, whole bunch of stuff.
01:07:36.108 - 01:07:37.046, Speaker A: If you want to make your own.
01:07:37.068 - 01:07:45.080, Speaker E: Implementation or create a unique randomness generation, please reach out.
01:07:47.450 - 01:08:05.000, Speaker A: Sounds good. Anything else before we close? Okay, yeah. Thanks everyone for joining. Yeah. Talk to you all on discord. Bye bye. Thank you.
01:08:05.000 - 01:08:10.420, Speaker A: Thank you. Bye, everyone.
