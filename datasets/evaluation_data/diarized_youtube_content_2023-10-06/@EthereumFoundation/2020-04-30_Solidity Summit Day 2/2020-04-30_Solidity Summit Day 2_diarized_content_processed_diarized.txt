00:00:00.570 - 00:00:05.440, Speaker A: And then we have a 62nd delay, so I will.
00:00:13.280 - 00:00:14.430, Speaker B: Doesn'T matter.
00:00:24.330 - 00:00:38.030, Speaker A: Cool. Solidity summit loading on the live stream. Excellent. It 1 minute to go people. This is so exciting.
00:00:42.050 - 00:00:43.710, Speaker B: This is so exciting.
00:00:45.330 - 00:00:48.740, Speaker A: It is. It's really exciting for me to watch this. All working.
00:00:59.850 - 00:01:02.230, Speaker B: People seem to be tired from yesterday.
00:01:07.370 - 00:01:08.790, Speaker A: The after party.
00:01:09.370 - 00:01:25.246, Speaker B: Yeah, I missed the after party. I'm not sure if there was an online after party, but yesterday it went until 930 ish or like 940 because one session was canceled due to sickness. Today it's going to go a little longer.
00:01:25.428 - 00:01:27.760, Speaker A: Okay, it's 130.
00:01:28.610 - 00:01:51.060, Speaker B: Yes. All right, then I guess we can get started. Welcome everybody. I will wait a couple of more seconds. All right, cool. Welcome to day two of the solidity summit. Happy to have you all back here.
00:01:51.060 - 00:02:51.750, Speaker B: Before we dive into the content, I will give you a quick outlook on what we will discuss today and which kind of talks we will have on the agenda today. So let me quickly share this with you. Obviously you can all access the agenda. This is the Google sheet agenda, which I also just shared again on Twitter as well as in the GitHub chat. So whenever you want to interact with the speakers today and you don't happen to be in the jitsu room itself, don't worry, you can ask all of your questions in the Ethereum slash solidity and moderator. So either myself or somebody else will relay your question into the chat room. So today, day number two of the solidity summit, we will kick it off in four minutes with Anne, who will give us an overview of creativity and solidity development.
00:02:51.750 - 00:04:09.620, Speaker B: After that we will have Hong Ying Tai who will give a more detailed overview of the Sol compiler, which we already had a lightning talk about yesterday from Michael, and today we will learn about optimizing solidity yules in bytecode via the LLVM framework. After that we will get an introduction to the remix analyzer from Aniket from the remix team, and then we will hear from Sean about the overview of the Solung solidity compiler. After that, Sebastian from Quantstamp will present how to detect DOS vulnerabilities caused by gas limits with fuzzing. And then we have Gonzalo and Martin, who will maybe show us the visualization of large code bases with the solidity visualizer extension for vs code or something else. I'm not sure. Maybe it will be a surprise, but they will for sure present something, and I'm super curious to see what. Then we will have a short break, and after that, in the afternoon we will start with the discussion sessions kicking it off with the first discussion moderated by Alex about gas unrestricted alternatives for send and transfer.
00:04:09.620 - 00:04:58.050, Speaker B: Then we will get a nice introduction into LSP and possible applications of LSP to solidity. That will also be an open discussion. After that we will have an open discussion and lightning talk combined. First a lightning talk from remix tests and then an in language testing Zuntax discussion. Then we will listen from Johan about mutation testing with vertigo, a tool. Then we will have a quite long discussion around formal verification called acting formal where we will get both an introduction to act as well as discussion on formal verification. And I'm sure we will also hear about the SMT checker moderated by Martin and Leo.
00:04:58.050 - 00:05:45.198, Speaker B: Then we will have another open discussion on immutable and explicit copies which will be moderated by Alex and Leo. And then another short break followed by another open discussion. Wow, a lot of open discussions today about modifier areas moderated by Chris. Then we will have a talk from league introducing us to source, verify or verify all the sources. And then we will have another open discussion, this one moderated by Nick Georgita about ethPM equals metadata. And lastly we will have an open discussion on functional solidity. And then you see this open slot over here at the end of the conference where right now you cannot see any specified content.
00:05:45.198 - 00:07:00.950, Speaker B: This is because we kept an open slot here in case there's anything during the day that we realize we still want to discuss or that you guys maybe still think you have questions about. So during the day in the GitHub chat, we can collect whenever something pops up that you guys still think is missing, then we can basically use this open slot to discuss it at the end of the conference. And if we think that at 930 german time, we have discussed everything we needed to discuss, we might also skip the session. So yeah, keep stay tuned on the last session and without further ado, I am stopping my screen share with the last announcement that in case you guys only joined for the second day, there's the option to get a proof of attendance protocol batch for proving your attendance here at the solidity summit. If you want to claim such a badge, just write me a message, a direct message on GitHub and you can get such a redeem link if you provide me with some feedback on the solidity summit, what you liked, what you didn't like in exchange. Okay, now I'm handing over to Anne who will speak to us about solidity and creativity.
00:07:03.930 - 00:07:23.990, Speaker C: Thank you, Francisca. All right, hopefully everyone can see me. Great.
00:07:35.480 - 00:07:36.548, Speaker B: Looking good.
00:07:36.714 - 00:07:52.952, Speaker C: Okay, great. Let me begin. This is creativity and solidity development. My name is Anne Kilzer. A little bit about me. I'm a software engineer and visual artist. I grew up in Montana, in the United States, and moved Tokyo two years ago.
00:07:52.952 - 00:08:27.268, Speaker C: And I'm now the lead software engineer at Curvegrid. And that's me with a real yak curvegrid. We are nine engineers in Tokyo. We are trying to make blockchain easier to use by building a middleware platform for Ethereum and other blockchains. So first I'd like to talk about creative constraints. Now, when I was in college, I studied fine art and computer science and math, and I found that they often related. So here's an example of a woodblock I was working on in Japan, and I'm trying to make a picture of a train.
00:08:27.268 - 00:09:15.708, Speaker C: But in order to make this visual picture, I have to think about color separations and I have to think about technical issues. So I always found the two that science and art really went well together. And here's the finished image. There's several different layers, but again, there's this problem solving that goes into making the end result. So sometimes in coding and in life, we encounter different obstacles. Maybe our hero here can't get through this door, but sometimes if we take a step back and get some perspective, we might find another way through. So how does this have to do with solidity? Well, solidity is one of the more unique languages I've worked with, and I'm no expert in the language, but I find it really fun that.
00:09:15.708 - 00:09:54.050, Speaker C: Well, I found it challenging at first, but then I thought, hey, these challenges are actually jumping off points for creative thinking. So I wanted to highlight some things that I think are interesting about solidity, from language features to things people have built on top of solidity. And these are just inspiration points. So very popular lately is ERC 721 tokens, or nfts, non fungible tokens, which are great for collectibles. A real world analogy would be the vinyl figurines that you can see in Japan in stores like Mandarake. And these can be very valuable. Some of these go for $500 us.
00:09:54.050 - 00:10:46.252, Speaker C: So people have built all sorts of creative things using ERC 721, from Axio Infinity, little creatures to block cities, to even real world mashups like this austrian postal service stamp that's issued on Ethereum and crypto Kaiju, which combines NFC and vinyl figurines with blockchain collectibles. One of my favorite projects is by larva Labs. It's called autoglyphs. And these are unique artworks that are ERC 721 tokens. Each artwork is generated, there's an addition of 512 pieces, and unfortunately they're all sold out. But you can still check out the code. And one of the really novel things about this project is all the code to make the image is written in solidity.
00:10:46.252 - 00:11:32.380, Speaker C: So I've got a little snippet of some of the code here. You can see that we're drawing and placing different characters to make these kind of neat, symmetric images. One of the first personal dapps that I wrote was kind of, well, it was completely a joke, but I wanted to do something to make my coworkers laugh. And I took something common in Japan, which is overpriced fruit. Yes, they can go up to $27,000, but it's more likely at a grocery store you'd see melons from twenty dollars to one hundred dollars. And I thought it would be funny to make a token backed by fruit. And in order to do that, I used the timestamp feature of solidity language.
00:11:32.380 - 00:12:21.388, Speaker C: As far as I knew when I did this in 2008, I hadn't seen any other projects that were doing expiring ERC 720 or ERC 20 tokens. But, yeah, so basically there's a date when you plant your fruit, and the contract records that, and then there's a certain window which the fruit should be ready. The now keyword is useful in solidity, but it's a word of caution. Miners can make small manipulations, so it's good to have the window not too small in case there's cheating. And I have the source code for this on my GitHub page. If you're curious, here's an example of the internal transfer function for the ERC 20 token. Notice that we require that it's not expired.
00:12:21.388 - 00:13:00.644, Speaker C: That internal check is expired is checking against the timestamp. And so basically, when the melon rots, we can't transfer it. Hash functions or one way functions can be used to kind of mask information or condense it. Another game and dap that I built was a blockchain stamp rally. What's a stamp rally? It's a treasure hunt that's popular in Japan. Often they're connected with train stations. People will travel on a rail network to different train stations and get different physical stamps, and you can kind of collect these in a book and get a prize.
00:13:00.644 - 00:13:49.460, Speaker C: I thought it would be fun to do this virtual as a virtual thing. And it's even more relevant today when many of us are trying to stay at home instead of going to a place, maybe you could enter a passphrase or enter the lat lawn in some sort of virtual world like decentraland. So here's an example of the web3 front end that I built that has the circle where the stamp would go. And then if you enter the correct passphrase, you get a cute little image. The next feature I want to show an example, I was playing Axie Infinity's Blood moon rising the other day and noticing that they've updated their dap a lot. It's pretty cute. But one thing, the DaP is really interactive and it's fast paced, which is great for games.
00:13:49.460 - 00:14:47.628, Speaker C: So for a user experience for a game, you wouldn't want to constantly be signing transactions. Can you imagine if you just had pop up pop up and you have to spend gas? That would kind of interrupt with the flow of the game. So some developers use summary transactions where you periodically will sync things to the Ethereum blockchain. But you don't have to always be doing that because it might interrupt with the flow. Now, it's a little tricky to tell what's going on under the hood with Axie Infinity because not all of their source is public, but from what I've read, they're using a side chain built on the loom SDK, and I believe the side chain is called Zombienet or zombie chain, and then they sync it to the Ethereum network. So I was able to look at this on ether scan and you can decompile the bytecode, but it's a bit. Well, there's a little more sleuthing to be done.
00:14:47.628 - 00:15:09.750, Speaker C: Anyway, there's clearly these points where you're checking into and syncing with the Ethereum blockchain, but the actual desktop app is really snappy. And unless you really knew, you wouldn't necessarily notice that it's a blockchain game. It just feels like a game. And here's another picture of it.
00:15:11.640 - 00:15:11.956, Speaker A: We.
00:15:11.978 - 00:15:55.920, Speaker C: May be familiar with burning in solidity as far as sending a lot of ERc 20 contracts will have a burn function that you can send your tokens to the zero address and relinquish control of them. And that could certainly be done with any other crypto asset. For example, some of the older Axio Infinity code on their GitHub page is calling a burn function inside of when they're retiring an axie. I guess taking it out of gameplay, they're burning it. Now here's the feature where I'm going to use the catchphrase that this is going to change gaming forever. So yeah, here it is. I think this is one of the most exciting things is layering and interoperability.
00:15:55.920 - 00:16:40.620, Speaker C: Because blockchain is public and because it's shared, we can interact with it and it's kind of out there for anyone to use. And this can really result in some wonderful mashups. For example, cryptozombies is this solidity tutorial from the loom network I'm sure most of you have heard of. They actually feed on cryptokitty DNA, and that's just kind of fun and whimsical. Another example using cryptokitties is a project called Kitty Hats. Unfortunately, the Google Chrome extension no longer works, but you can still see the source code and some fun pictures. And this is an ERC 20 token that's layered on the ERC 721 collectibles.
00:16:40.620 - 00:17:23.650, Speaker C: And you spend the token to apply the hat, bow tie, apple Watch or monocle to your kitty. And then interoperability is the idea that an item can be used in more than one context. So a lot of NFTs work in different realms. So here's a crypto kitty that is combined with mytherium playing card. And then over on the right hand side, I'm showing a scene from Decentraland, and it's featuring Axie Infinity characters, and I believe they plug in with a lot of NFTs. So yeah, basically your smart contracts can work together. It's super cool.
00:17:23.650 - 00:18:04.252, Speaker C: The final part of my talk is a brief exercise. We did a longer form of this talk at Devcon five in Asaka last year, and we had 20 minutes and we did this breakout session with cards and people got to brainstorm ideas and that was really fun. Now, this is a virtual conference, so the next best thing I can do is show you a software version of the game. So I wrote it quickly in solidity, the brainstorming activity, and I'd like to show it to you. But first let me explain how it works. There's three characteristics of games. I'm making this up, but there's a type of game.
00:18:04.252 - 00:18:46.970, Speaker C: Is it a board game, is it a role playing game, is it a God game, where there's some personality controlling the universe? What are the themes of the game? Like what is the world it's set in, or the time period? And who are the characters? And then what's the novel feature of solidity? That's our starting point for creativity. So here's an example. Let's say the game is a racing game. The themes are post apocalyptic, and our characters are ducks, and the feature of solidity we're going to use is burning. So if a duck loses the race, maybe it gets burned. And cool name for that might be mad quacks too. The pond warrior where ducks are just fighting in some gritty desert future.
00:18:46.970 - 00:19:20.480, Speaker C: Another game could be a role playing game where we use baking and japanese mascots, and then we use layering or interoperability. Let's say it's like a Nintendo switch type japanese mascot bake off and then the underlying tokens. Each cookie is a different NFT that can be traded in another game. Why not? So I'm going to use the front end. I'm going to show a demo of this. This is the product that we built at Curvegrid. It's called multibask.
00:19:20.480 - 00:19:47.500, Speaker C: Basically it's an API and UI over your smart contract. So I wrote this smart contract, the dap idea generator, and we're going to interact with it. So here we have the name, the address here. I've deployed it on Rinkabee. We've got some of the read only functions of our solidity code. We've got events that have been emitted, and then we can interact with this contract.
00:19:48.640 - 00:19:49.630, Speaker B: Let's see.
00:19:51.540 - 00:20:25.280, Speaker C: I'm going to just click send method. Okay, great. So Metamask is going to ask me to sign the transaction and we'll let it go for a moment and see what our new idea is. Okay. Apparently it would be a role playing game with snake people, fashion and burning. So that's the idea. Can generate another one and see what we get.
00:20:33.040 - 00:20:33.790, Speaker B: And.
00:20:37.460 - 00:21:17.980, Speaker C: Give it a moment, wait for it to be mined. You see some of the earlier ideas I generated today, which include a addictive click to win scheme using romance pizza and multisig wallets. So I think you win if you and your lover buy a bunch of pizzas. Science fiction Wild west using voting and a role playing game. Yeah, so that's the very short version, the lightning talk version of my game. If you'd like to try it out. The contract is uploaded on Rinkabee and I'll share a link to these slides in the gitter chat.
00:21:17.980 - 00:21:38.090, Speaker C: You can try multibass, our project, for free. It's free to use on Rinkabee and Robson networks, and that's at ww curvegrid.com. And the source code is on my personal GitHub page for the dap idea generator. Thanks for listening to my talk and let me know if you have any questions.
00:21:42.220 - 00:22:20.710, Speaker B: Thanks so much, Anne. That was a fun talk and introduction and I learned so many new cool projects that I didn't know before, especially autoglyphs. I don't know how I couldn't know this before. So beautiful. All right, if you have a question for Anne, we still have a couple of minutes left before the next talk, so either shoot it in the GitHub chat or raise your hand here. Yeah, today, I didn't mention yet there's a raise your hand feature in jitsi. If you are in the video conference, you can raise your hand, and then I see that you would like to speak.
00:22:20.710 - 00:22:43.410, Speaker B: No questions so far in the room, and nothing here. Yeah. Then I have a question. What is your favorite creativity solidity project of all of those?
00:22:46.100 - 00:23:15.230, Speaker C: I mean, I liked larva labs a lot just because it was completely on chain. And with all of them, there's always a risk of the ones that are sort of hybrids of like, oh, well, what if they lose? How do we make the artwork for this cool collectible? What if that server gets lost? So I like that it was like completely blockchain. I'm a little sad that they're sold out and I didn't get one, but that's, I guess, what makes it special. And then I think, second favorite, I like the axes because I think they're really.
00:23:18.000 - 00:23:31.612, Speaker B: Yeah, absolutely agree. Very nice. Cool. Anybody else? Any question? We have one comment on GitHub. A good way to start day two with Ankilsa's fun talk. Smiley.
00:23:31.756 - 00:23:32.912, Speaker C: Oh, thank you.
00:23:33.046 - 00:23:53.160, Speaker B: From Alex. Cool. Then we can. Thank you so much. If you have questions later on, feel free to put them also in the GitHub chat. And as Anne said, the slides will be also available in the GitHub chat. Chris, do you want to say something? No, just show your face.
00:23:53.160 - 00:24:12.030, Speaker B: Nice. All right, then, next up we have Hong Ying Tai. And I hope that today the technology will be with us better than the time that we tried this. So maybe it's good that we have a couple of minutes of buffer. Hi. Yes.
00:24:12.400 - 00:24:15.330, Speaker D: Okay, let me share my screen with a moment.
00:24:19.380 - 00:24:43.870, Speaker B: Oh, we have one more question, so maybe you can try sharing your screen and I can relay another question to Anne. We have a question from Fox, who's asking, how far can you go withdrawing crypto assets in a contract? If you, for example, could draw a cryptokitty in a blockchain like a pixel mp?
00:24:44.210 - 00:24:44.960, Speaker A: Sorry.
00:24:48.210 - 00:25:11.720, Speaker C: Yeah, that's a really good question. I have not tested the limits on that. I mean, I think there's some. How much are your players or users willing to pay in gas fees? And then I guess there might be a block limit too. That's a really good question. I don't know the answer to it, but I think that's a pretty interesting thing to dig into.
00:25:13.850 - 00:25:14.598, Speaker B: Yeah, cool.
00:25:14.684 - 00:25:31.230, Speaker C: All right, I guess spitballing too. You could use a side chain. You could also use a side chain and condense some data onto the contract too. And then it would still be blockchain, but you could pick like a side chain that's optimized for image rendering or different things.
00:25:31.380 - 00:25:31.838, Speaker A: So.
00:25:31.924 - 00:25:34.320, Speaker C: Yeah, thanks for asking that.
00:25:35.330 - 00:25:40.030, Speaker B: Nice. All right, cool. Then over to you now, Hong Ying.
00:25:49.990 - 00:25:51.918, Speaker D: Can you see my screen sharing?
00:25:52.014 - 00:25:54.020, Speaker B: Yes, it's working really nice now.
00:25:54.330 - 00:26:23.114, Speaker D: Okay, great. So let's start. Hi everyone, I'm hominidai. Or you can call me hi Dai. Just like this, id for short. And today I want to share about how my team, the Sol team, to do several optimization on solidity on EO and igbozen via the LBM framework. So here is our agenda.
00:26:23.114 - 00:27:07.930, Speaker D: We have four sessions to talk about that. First is the roadmap and how Sol works. And Lang, we will just mention just a little UO optimizer and LBN optimizer. Len, we want to introduce a very experimental new mechanism to do some static analysis and feedback derivative optimization. Okay, so let's talk about our roadmap first. Actually this whole project start from, I think this year the April folds. And actually we have do lots of steps to create a new language extension for solidity called Liti.
00:27:07.930 - 00:28:16.018, Speaker D: If you remember, we have talked about that on the Defcon five. After the tool, we want to just more focus on some optimization research because we believe that smart contractor for blockchain will become more and more popular. So if the engineer who is writing the smart contract cannot, we want to help them. They don't need to really care about the detailed caution about the performance or the CoS size. We want to leverage lots of the automation skill and to help them to generate a small size bike hole. So here is the start point. And after we decide we want to use the LBN as our infrastructure, we want to choose one front end and one back end, which is the front end is we choose the solidity language, because I believe it's the most popular smart contract language in the world.
00:28:16.018 - 00:28:55.214, Speaker D: So we choose the solidity and then for the backend, I believe that the solidity compiler has do very great work to generate the EVM bipolar very efficiently. So we just want to create a new target backend. So that's why we choose the solidity. And it wasn't for first, and then we go to the audio one. The very first release. We want to create a very small demo, which is just. I believe everyone has used this library called Step Mass.
00:28:55.214 - 00:29:00.170, Speaker D: And we just do very little grammar.
00:29:00.250 - 00:29:00.494, Speaker A: Just.
00:29:00.532 - 00:29:38.966, Speaker D: You can see here if else or revert require a very limited integer support. But before the Defcon five, we want to just join the community. And we want to talk to the solid team. To ask for more conversation about Ebolson or something like that. So in this two version, we focus on how to compile ERC 20 smart contract. And we want to show our sol has the ability. We can just support the constructor.
00:29:38.966 - 00:30:13.622, Speaker D: And we can deploy the evosant bycode to the historian ewasant testnet. And actually we just finished that. And we have a very good conversation between us and solidity team. And after this event, I believe this is very key event. Because after our discussion we found that. I remember last, Alice told us there is another project called Solan. And they also do the same thing.
00:30:13.622 - 00:30:49.222, Speaker D: Just compile the solidity into evolution or into the subtract. So we just changed our goal from supporting the solidity front end. Just instead of. Sorry. We want to support the Evo front end instead of the solidity. So Michael mentioned yesterday, actually the Sol now can pass, I think, 80% of the test suite from the library. And the Sol wants to accept lots of evo byco.
00:30:49.222 - 00:31:23.770, Speaker D: Which is generated by the SolC compiler. And we apply the LBM building optimization options into the whole code gen and whole bicol generation. And also we also support evn LBn as our EVM backend. So now the Sol can support a very limited solidarity front end. And I think almost the evo front end. And we have two backend. One is the evolution backend and another is the EvN LVM backend.
00:31:23.770 - 00:31:58.614, Speaker D: So here is something we doesn't support now. Just like balance or self balance. Something like this. And we will just keep going to finish these unimpreven instructions. Just a few months later. Okay, so what is our future roadmap or future goal? We want to fully support Eofan N. And yesterday we found that it is a very good extension called uoplus.
00:31:58.614 - 00:32:37.830, Speaker D: And I believe if there are some features, that's very interesting. So we will also add support of the EO. Plus after we finish the Ufanm. And we found that there are some very interesting things. Actually traditional optimization applies on the ewasn. May not be the best choice for the real world. Because when we deploy a smart contract on Ethereum.
00:32:37.830 - 00:33:32.850, Speaker D: We will very care about the cosites rather than the efficiency because the cosites will take lots of storage. So I believe it's more expensive than the native computer. Okay, so that's the second goal we want to achieve. And the third one is actually we have several legend pass in our itemizer and we will discuss that later. Okay, so that's our roadmap from the past, from the current and the future. Okay, now I want to talk about how Sol works. So the first thing is that we accept the contract, which is right in solidity language.
00:33:32.850 - 00:35:17.962, Speaker D: And when you get solidity contract you can just bypass it into the SolC and with the minor minus IR and minorized IR optimized. And then you can get, and at this stage we accept this Uo smart contract into our compiler. And if you want to see how you want to get the deployer plus the runtime, you just bypass this ucontra and we will give you the cy wars. But if we want to know what is the detail of the runtime by code, you have to remove the outside part of the U object because the outside part is the constructor and something that is just a wrapper of the outside. So you have to remove the outside URL object first and then just bypass the remaining part to our compiler and you can get the runtime was it. So what is the detailed things that the sol compiler do? The first thing is when we get the UO smart contract we will pass it and check the semantic and create our own ast in our compiler pipeline. And the second thing is we will consume this st with our Cojay module and then we will get the LVN module here.
00:35:17.962 - 00:36:10.554, Speaker D: So at this stage now we get lots of Lvnir. Now there is no ul anymore. So each line of the UO statement will just map into several line of the LvIr here. Okay, now when we get the LVM module here and the next step is to use our backend. Our backend will just apply several lv optimization tests and we will do lots of steps like the bit swap because the evn is big indian integer type and the webassembly is the little indian integer type. So we need to do lots of the conversion scenes. And then after we finish lots of works.
00:36:10.554 - 00:37:16.446, Speaker D: Now the last step is we will use our tucking machine and linker to generate the evasive bycode or the evN. LV and Lvnir, that's the whole internal process of Sol works. Okay, so that's my second session to just describe how the Sol works. And the third part we want to just mention some very different things about the urbanizer and the LV optimizer. And the first thing is, I think the rules between these two is very different because the ultimizer actually is transformed from UO to Uo. So you can have very high level information here. And so we will have urchimizer can do lots of aggressive elimination to do the inline scenes to remove the function code.
00:37:16.446 - 00:38:53.270, Speaker D: But when we go to the LV optimizer, there is a big limitation for us because that we just convert all of the Evo statement into the LVIR statement. So we will lost lots of the high level improvisation first. And the second thing is when we want to apply to generate to the ewasm back end we will need to add lowering integer because the EVM has 256 bit bigger integer, but the Webex only support a 64 bit integer. So we have do lot of conversion and also just the bit swap I mentioned before. So after we add the several scenes in our LVIR code, it will be more hard to do some analysis and to reduce with some very heuristic rule. So I will give you more details in the example. Okay, so the first example is we code the very common compiler optimization that is loop rolling or unrolling, which means here you have a contract with ten element of array and we just write for loop to fill the ML with the index.
00:38:53.270 - 00:40:14.450, Speaker D: And if you compile this code to the native computer, for example, if you write this algorithm in this algorithm in c plus plus and just compare it to the native, you will find that there are no more for loop because all of the synths will be converged into array zero equals assign to the array zero. So it will just be ten line of this statement. But in the UO optimizer you will find that actually you just keep the structure. So you will have a follow up here and each iteration will just update the storage value once. So I think it's very great. But when we apply, for example, we apply the LB optimizer with the o three frag and we will find that all of the loop is eliminated. Because in this stage the LBN and the wasam back end believe that we just replace the loop with lots of the constant store is more efficiency.
00:40:14.450 - 00:41:46.680, Speaker D: But actually in the historian I don't see it's a good idea because if this loop become more larger, if it's not from zero to ten, it's from zero to 100 or 1000, then we will find the cosites will glow than our expected so when we do the rupee we find the final Wasam cosite will be about three k. But if we disable the loop unrollings we will get two point five k bytes here. So this is the first very different thing that we have to be more careful when we apply any LVN optimization because lots of optimization is designed for the native computer, not designed for some very special cas. Okay, the second thing is, I believe that's a very good example that the UO optimizer can do much better than the LB optimizer you can find. This is Devcon Uo and it's store here and we just call arrays and in the array sum we will call lots of overload. So after the evo optimizer, sorry.
00:41:49.290 - 00:41:49.606, Speaker A: I.
00:41:49.628 - 00:42:05.800, Speaker D: Missed one slide here, okay, I missed one slide here and I will give you one moment, let me check if I can get it back now.
00:42:10.020 - 00:42:17.290, Speaker A: Let it will moment.
00:42:34.910 - 00:42:36.220, Speaker D: Let me just pass.
00:42:37.330 - 00:42:54.430, Speaker A: It copy and hold on past, sorry, yes but copy.
00:42:58.130 - 00:42:59.106, Speaker D: Wait a moment, let.
00:42:59.128 - 00:43:02.840, Speaker A: Me share another screen here.
00:43:03.210 - 00:45:05.814, Speaker D: Okay, so you can find, sorry I just go back. So after the UO optimizer then we will just convert this UO to very short one. You can find all of the function code is, all of the function code is email. So there's very good things that UO optimizers do, but when we go back to the LBN optimizer three, then we will figure out that the array sum has been in light but the array load has not been in light because in the LB architect measure we figure out that the function body and the basic block is larger than igniter search hole, there will be only the array sum has been in line and the other one will not be in line. Okay, so that's very different between the LVN optimizer and evo optimizer because LV optimizer, the inliner has its own search hole and we have to do lots of research and to figure out which search hole is good enough for the evolution cost model. So that's the circuit, we will have to do more research here and the third one is the storage SSC sample. So I just create a new integer storage, we will do add ten and add ten twice.
00:45:05.814 - 00:46:21.940, Speaker D: Okay so when the countries be compiled to the UO and after the ultimizer and you can find that there is the sensing like the function body and it just updated storage value twice and that's the sensing with the LV optimizer like you can find that, here is the add first to add ten into this is a temporary rebel means the original story value and just store it. Oh, here is by swap. Don't worry about that. And we call the storage store just to store the FTA value back to the storage. But we believe if we can do a very trivial optimization, which is these two line can be combined into one line, which is I plus equal 20. We don't need to do just like two lie of the ad and two lie of the storage update. We can just simplify the statement into one line.
00:46:21.940 - 00:47:22.822, Speaker D: This is several things that we can do more about this case. Okay, so here are three examples. We want to mention that both of these two optimizer still has lots of research and optimization technology. We can do more. So we want to introduce the final part, which is we do research to analyze the cost model. So what has the motivation? This is motivation is like, I believe the gas cost is just like the energy consumption. If you run a very bad program on your cell phone, then your cell phone will eat more power.
00:47:22.822 - 00:48:06.062, Speaker D: And if you write a very bad smart contract, then it's gas. I think it's just legal. So we apply very, we apply the same analysis and optimization just from some paper to deal with the energy consumption. So we create a project called the GPS cat, which means general purpose static coast analysis tool chain. Okay, it's just very long name. And in this tool, we have two phase tool chain. The first one is we don't analyze the evo directly.
00:48:06.062 - 00:49:04.322, Speaker D: We analyze LvIr because we can do lots of debug info. We can insert lots of debugger info to help us to evaluate the LVNIR cost equals to loss of the web assembly instruction cost. So we will start on the LVNIR first. So when we get LVNIR, we can just calculate it into a cost function. And then we will use up one solver to resolve this function to figure out its score. Okay, so here is our total analysis diagram. So, first of all, since we will define some assembly cost model, something like, if we add a number, it will take, let's just say 100 or 200, something like that.
00:49:04.322 - 00:49:54.398, Speaker D: And all of the LvIR bycode will be annotated with debug info with the debugger location. Because we want to know that every LVIR will be generated to what kind of webassembly instruction. So the top idea is like this. So after we do lots of annotation, we can get the LVIR with block course information. Then when we get this information, we need to have to apply two framework from the outside. The first one is we need to extract the cost of relationship. So we use the LBN to keto.
00:49:54.398 - 00:51:21.678, Speaker D: And the second thing is after we have this cost of relationship we have to apply the upon solver to calculate the scenes. So here we use two external project to help us to do this. Okay, so after we go through lots of things now if we apply some optimization on the LVNIR, then we can get a new score and a new formula and we can calculate if we apply this, the static analyst will tell us well your cost will be increased or decreased. Then we can just adjust the theoretical or address the optimization options to help us to create more suitable item management phase for the US and cost model. Okay, so I think I have time. So the following slide is just mentioned how we annotate the debug info for the LVIR. Just like we will say each LVNIR will mapping to which line and lowering it into the assembly, the wafer assembly.
00:51:21.678 - 00:52:48.142, Speaker D: Then we can just get a mapping resolution between the load LVNIr instruction will be do nothing but the ad will be converted to three instruction of the webassembly. Then with this model we can just say we have a table of the mapping relationship and we have the cost table. So we will know each LVIR is mapping to what kind of cost and we can calculate the final basic cost and we will know the whole program, the whole cost of this smart contract when it just is queued. Okay, I think that's my whole talk about Sol and something about optimization. So if you are interesting, you can go to this link to our Sol repository to use them or you can just, I believe Michael showed the video yesterday. And if you are interested actually, because we need to measure the bico efficient and bico performance. So we create our self owned wasan virtual machine to do this.
00:52:48.142 - 00:53:02.760, Speaker D: And if you are interesting about GPS, Cat, here is our repository and all of the examples you can find in this repository here. Okay, let's all thank you so much.
00:53:03.210 - 00:53:28.320, Speaker B: Great talk. Thank you. So because we have almost 1 minute delay in the live stream, we will give people the chance to ask questions. Now either if you're watching the live stream, please make sure to comment in the GitHub chat. Or if you have a question here in the room, feel free to use the raise your hand feature so that I can see you have a question. Yeah, Chris, go ahead.
00:53:31.570 - 00:53:51.300, Speaker A: Yeah, thanks for the talk. I'm wondering, so you mentioned that you need to extract the runtime code from the Yule object in order to deploy it. Is there a way you could support the deploy routine that is written in Yule or what is the reason for.
00:53:51.670 - 00:54:44.582, Speaker D: Oh, actually because the generated Ul contract is just like a rectangle and deploy or constructor part. There are two parts, right? I think there are two parts because if you give the Sol with the runtime code and the constructor code, the deployment code, we will just compile the runtime code just directly into the webassembly by code. So you have no choice, you have no chance to see the LBIR inside of this runtime. So if you want to do some research on the runtime by LBX, just remove the outside part.
00:54:44.716 - 00:54:51.398, Speaker A: That's not required for running it. It's just if you want to see further details on it, if you want.
00:54:51.404 - 00:55:00.102, Speaker D: To running it, of course you have to give the runtime call and the constructor all well beneath.
00:55:00.166 - 00:55:02.060, Speaker A: Yes. Yeah, thanks.
00:55:02.590 - 00:55:12.470, Speaker B: Okay, another question here, Mooli. Oh, you're still muted.
00:55:15.530 - 00:55:24.266, Speaker A: Can you hear me? Thanks. Thank you. Very, very nice talk. Can you elaborate a little bit about how much you save in particular, how.
00:55:24.288 - 00:55:26.080, Speaker E: Much life saving are you getting?
00:55:34.910 - 00:55:35.322, Speaker A: Sorry?
00:55:35.376 - 00:55:36.620, Speaker D: Can you, can you.
00:55:40.910 - 00:55:41.660, Speaker A: Oh.
00:55:43.410 - 00:56:28.510, Speaker D: I will say there are no any benchmark or we doesn't calculate the gas reduced number here because I always say it's a very experimental, it will be experimental stage. So in this moment, I believe they also don't have very strong or very complete of the webassembly cost model. So we just save lots of number layer so we don't have the real number.
00:56:28.580 - 00:56:30.960, Speaker A: Sorry about that. Yes.
00:56:32.770 - 00:56:33.950, Speaker B: You'Re muted.
00:56:39.630 - 00:56:48.542, Speaker A: Oh, sorry. So on the conceptual level, if you implemented this directly on the Yule, how much more difficult would it be?
00:56:48.676 - 00:56:50.206, Speaker E: What are you getting actually from the.
00:56:50.228 - 00:57:00.740, Speaker A: LLVM as opposed to just running it on the Yule? Is my question clear as well?
00:57:14.010 - 00:58:49.650, Speaker D: Actually, I believe there's different scenes because why we choose the LVM framework because it's just like the GCC that there are several people and do lots of artemisian research and create lots of the artemisian strategy layer. So for us it will be very difficult because we have to choose which strategy is suitable for the UO. And then the second thing, we just pulled the algorithm from the LPNIr into the ultimizer. So if we can do lots of research on these parts, yes, we can put lots of optimization from LVN into the UO optimizer. But I believe we should just still use the LVN framework and just add lots of analysis and optimization structure layer because you will have lots of things to do because you have to analyze and create lots of information. So with those information, we can just do more aggressive optimization. But I think we have too much time to rebuild all of the scenes again on ewtmizer.
00:58:50.730 - 00:58:52.120, Speaker A: Thank you very much.
00:58:52.730 - 00:59:31.090, Speaker B: Okay, unfortunately we are running out of time. I will just relay two more comments that we received via the chat, which are more comments than questions. Sean Young says, very interesting. Thank you Hong Ying Tai, great work. And Alex says, really excited about the GP scat tool and the possibility to introduce gas aware optimization into LLVM would help with ewasm and e two phase two a lot. So thanks again for your talk and we will move over now to the next talk, which is from Aniket who will introduce us to the remix analyzer.
00:59:34.950 - 00:59:37.010, Speaker A: Am I audible, Francis?
00:59:37.610 - 00:59:39.846, Speaker B: Yes, we can hear you and see you. Hi.
00:59:39.948 - 00:59:41.334, Speaker A: Okay, cool. Hello.
00:59:41.532 - 01:00:26.174, Speaker E: So hello everyone. This is Aniket from remix team and I'm going to talk about the remix analyzer. I would like to share my screen. So basically just in the short introduction, remix analyzer is a tool which is used to perform code analysis on solidity smart contract. So like usually code analysis tool, it basically examines the solidity smart contract and tells about security vulnerabilities and bad development practices. So remix analyzer works underneath this remix id. Solidity static analysis plugin.
01:00:26.174 - 01:00:32.178, Speaker E: So I'm going to elaborate the process using this plugin.
01:00:32.274 - 01:00:37.960, Speaker A: So I'm sharing my screen here. Okay.
01:00:47.790 - 01:00:52.894, Speaker E: So I think Remix ID is visible now. Freddie, can you please.
01:00:52.932 - 01:00:55.280, Speaker B: Yes, it is. Yeah, we can see.
01:00:55.910 - 01:01:35.786, Speaker E: So this is the remix browser when you load it very first time on your browser. So solidity static analysis plugin comes with the solidity environment. So we have two environment here, solidity and wiper. So we can just select the solidity environment. And here on this analysis icon, you can see this is the solidity static analysis plugin. So we can just quickly move there and talk about its interface. So basically, solidity static analysis tool performs its analysis under some of the modules.
01:01:35.786 - 01:02:20.342, Speaker E: Those modules are listed some category according to their uses, and those categories are here. So you can see there are currently four categories. So one is the security, another one is the gas and economy, third one is the ERC, and last one is some of the miscellaneous modules. So there are around 21 modules currently. And using this, we perform the analysis in this plugin. So I can quickly explain some of the plugins. So basically if you are using the TX origin, we know that this is used to know the origin of the transaction.
01:02:20.342 - 01:03:04.682, Speaker E: So basically originated address of a transaction. So this basically should be used in the rare cases. So there is a module to track that in your code. So if you are using RTX origin in your code. So this analysis will throw a warning for that and same for the check effect. So there is a check effects pattern in the solidity that suggests that before making external function calls we should make the state changes before of that. So it is about that then in the gas on economy it is mostly concentrated on the modules which affects on your gas cost.
01:03:04.682 - 01:03:52.966, Speaker E: So if there is a function for which our gas estimator is saying that there is infinite gas required so it will throw warnings for that. And basically we should avoid using for loop on dynamic arrays and transferring ethers in a loop. So basically you can get the complete information of each module on our documentation. So clicking this icon you can just move the documentation and read about all the modules. So I will quickly just go to the compiler contract. So this static analysis tool works with the solidity compiler plugin. And whenever you compile a contract, so this analysis just run automatically.
01:03:52.966 - 01:04:23.458, Speaker E: And that is the courtesy of this auto run checkbox. So let's keep it checked for now and try to compile a contract. So these are the initial contracts that are provided by the remix itself. So let's select the first one and try to compile. This is very basic contract. And if we compile so we get a green checkbox here, it means there are no warnings for this file. So we don't have much to explain.
01:04:23.458 - 01:05:04.350, Speaker E: So let's go to the second file, let's compile this one. And we see one warning here. So if we will go through the warning. So this warning basically explain the uses of assert and require. So this basically if you are using require or assert in your code as we are using here in the line number 21. So this analysis suggests that you should be aware about the uses of assert and require and you should be using them accordingly. Okay, let's further go to our very familiar ballot contract.
01:05:04.350 - 01:05:59.646, Speaker E: So let's compile this one. Okay, so we see nine warnings here. Let's discuss them. So basically this warning says that gas requirement of a function is infinite and there are almost four or five warning about that. So this is like our gas estimator, sometimes that infinite gas usage is required to execute the function that may be right or maybe incorrect. So this is a good to know thing that if you are having this, you should manually examine your code and you should avoid the loops. So this is basically mentioned in the warning also that please avoid loops in your function or actions that modify that large areas of storage.
01:05:59.646 - 01:06:50.558, Speaker E: So we know very well that gas estimates depends on the storage updates. So this warning basically tells about that. Then if we scroll down, so there is one warning which is referring to us, this line number 37, which is warning us using the loops. So it is saying loops that do not have a fixed number of iterations. For example, loops that depend on storage values has to be used carefully. So due to the block gas limit, transactions can only consume a certain amount of gas. So because of some of our previous conversations, I think we are very well known about this block gas limit, that we can spend a certain amount of gas in a contract in a block.
01:06:50.558 - 01:07:05.990, Speaker E: So this is warning about that. So if you will move to line number 37. So we see a for loop here and which is running on the dynamic array. So this basically suggests.
01:07:08.330 - 01:07:08.934, Speaker A: If you are.
01:07:08.972 - 01:07:46.990, Speaker E: Using a loop on an array. So you should ensure that the fixed number of iterations are getting done for that loop. So same warning is here for this line number 1119. So if we will move there and we will see that there is again one loop here and another warning, we will go through about it. So this is saying that in this method give right to vote on line number 52. So it is here. So there are a similar variable names, which are voter and voters.
01:07:46.990 - 01:08:24.414, Speaker E: So this analysis basically suggests to avoid similar names which can further be responsible for some of the typos. And that can be a big blunder in the deployment part. So that is the one suggestion, and if we will see another warning there. So that warning is also about the similar variable names. So that is on the line number 98. So this is the method. So we show the method name here, vote and then the line number here.
01:08:24.414 - 01:09:18.418, Speaker E: So user can directly go to that code part and check if something needs to be updated. So in this method we are using a proposal and proposals both. So it is basically warning us about that. And then there is that same similar warning of assert and require. So this basically, if in your complete contract you are using require or assert. So this is kind of a combined warning to ensure the uses of require and assert is getting done properly. So now, if you are developing a contract and you see these infinite things, and you are very experienced in the solidity development, so you can select the modules about which you want to see these warnings here.
01:09:18.418 - 01:10:27.190, Speaker E: So if, for instance, we don't want to see this infinite gas users, and we are sure that we can take care in testing or in some other part. So we can go to this gas and economic category and uncheck this gas cost section. So this basically throws the warning for that and we can just run the analysis directly from here for our last compilation result. So this analysis tool works on the last compilation result. So if we will run here this analysis, so it will work for the last compilation of this contract. So I'm running it here and you will see that warnings number are decreased to five and now those infinite gas uses warnings are gone. Similarly, if you are thinking that you can take care about the loops you are using and you are very sure that they will be working fine, so you can basically uncheck this box for loop over dynamic array and then if we will run this so warnings will be decreased.
01:10:27.190 - 01:11:11.446, Speaker E: So you can directly, also select all the modules of all the categories directly from this checkbox. You can select and deselect directly them from here. So if you want to run the analysis again. So you can run directly here. So as now all the modules are selected. So we are getting all those nine warnings again here. So this is the thing then if there may be chances that you are developing a contract using the id and you are trying to compile it to check the compilation errors, don't want to run the static analysis for that code.
01:11:11.446 - 01:11:19.282, Speaker E: So you can just uncheck this auto run box and you will go to the contract and you can try to compile another contract.
01:11:19.346 - 01:11:19.960, Speaker A: Even.
01:11:22.590 - 01:12:07.830, Speaker E: So this contract is compiled, but you will see that the result of last compilation is stored in this analysis. So this will basically keep showing you that last result until you compile any another contract. And then if you are not compiling it, if you are not running the analysis along with the compilation. So you can just go when you are done with the development, you can go to this plugin and then directly run the analysis from here. So this will be run for the last contract that we have compiled here. So this is the warning for two underscore owner Sol here. That is the same warning.
01:12:07.830 - 01:12:49.990, Speaker E: This is the functionality of solidity static analysis tool. And the remix analyzer tool is also an NPM package. So you can just use that also. So a detail about that is there in our documentation. I think you can see it on your screens. So you will see in the bottom that there are some links referring to the uses of remix analyzer. You can use it as a library and can perform the analysis according to the currently available modules.
01:12:49.990 - 01:13:25.758, Speaker E: So this is basically the explanation and uses of remix analyzer and for the future aspect of it. So we are in discussion to add more relevant modules for here for the users so that more security vulnerabilities can be cached at the time of development itself. And yes, probably that's it. Thanks for your time, guys. And if you have any queries so we can take that.
01:13:25.844 - 01:13:26.830, Speaker A: Fredrick.
01:13:28.470 - 01:14:13.470, Speaker B: Yes, thank you so much for your presentation. Are there any questions in the room? While we may wait at least 1 minute to see whether there are questions on the live stream, if you have a question, please either put it in the GitHub channel or raise your hand if you have a question here in the room so that I can see. Anikat will also be back later in the discussion session on in language testing syntax, where we will see another demo from remix on remix tests. Yes, it does not seem like there are any questions so far, but let's wait one more minute.
01:14:18.050 - 01:14:30.546, Speaker A: Short question. You worked on going from legacy ast to the new ast. Is that transition finished? Yes, exactly.
01:14:30.648 - 01:14:41.650, Speaker E: So currently this analysis plugin supports the latest ast. So we have migrated that part. So we have moved from legacy Ast to the current ast.
01:14:42.470 - 01:14:45.670, Speaker A: Nice. And do you already support immutables?
01:14:47.450 - 01:14:59.660, Speaker E: Not exactly. So that will be probably coming in another release. I think immutables are just introduced in some of the previous versions, am I right?
01:15:00.750 - 01:15:13.760, Speaker B: Okay, we have questions coming in on the chat Mirko is asking remix. Could remix static analyzer be used in some way externally to remix ide? Or is it possible to save results in some way?
01:15:14.930 - 01:15:42.658, Speaker E: Okay, so currently it is not possible to save the results. So that is a good suggestion that we can consider, but using it out of the. So this is the remix analyzer is an NPM package, so it can be used as a library in the. Yeah, so that is an open source NPM package and one can use that in the relevant codes.
01:15:42.834 - 01:15:57.310, Speaker B: Okay, and another question in the chat from Akosh, who's asking what kind of analysis does remix analyzer run in the background? Looking for certain patterns in the ast, performing symbolic execution, doing some formal methods.
01:15:57.730 - 01:16:32.840, Speaker E: Yeah, exactly. So we use the ast provided by a solidity compiler and basically we check these asts to analyze the code and to reach our results. So in the future aspect we are also looking to categorize it more so that user can be sure that which warnings are concerning and which can be kind of ignored or user can take about that, take care about that, so that there is a process about that and we can discuss that in some hackMd or in some issues.
01:16:33.710 - 01:16:51.390, Speaker B: Okay, awesome. That's it with the questions. And we're perfectly on time. Thank you so much, Anikat. And we are perfectly on time for our next talk, which is from Sean, who will present us the Solung solidity compiler.
01:16:53.650 - 01:16:54.206, Speaker F: Hello.
01:16:54.308 - 01:16:55.310, Speaker B: Hi Sean.
01:16:58.550 - 01:16:59.780, Speaker F: Let me just see.
01:17:01.190 - 01:17:09.140, Speaker B: You can also click on your camera again in the video chat so that we can see you and your. Yes, exactly. Perfect.
01:17:16.460 - 01:17:17.160, Speaker F: Okay.
01:17:17.310 - 01:17:18.120, Speaker A: Oops.
01:17:18.800 - 01:18:16.280, Speaker F: Hello, my name is Sean Young and I'm going to present the Solan solidity compiler. So I have about 20 slides to go through, so I'll go through these fairly quickly. So what is Solang? It's solidity compiler, which compiles solidity into webassembly. It targets different blockchains, so currently targets substrate ewasm and sawtooth and other ledgers, if they're interested, could add themselves. Now that the LLVM EVM backend is getting more stabilizing, that would be a very interesting target to add. So Solang is written with different tooling than source c, so it's written in rust. When you write in a compiler, you have to deal with asds and control flow graphs.
01:18:16.280 - 01:18:58.460, Speaker F: And these are represented actually quite well in rust enums. So I think rust is a quite suitable language. It uses LLVM as a library, so it uses LLVM for its optimization passes and to write out the WASM file itself. There are also some other advantages to using LLVM, which I'll talk about later. It has a solidity grammar, and from that the parser is generated. So recently I've been adding try catch and adding this to the parser, which is about ten lines of code. So it's fairly straightforward with a generated grammar.
01:18:58.460 - 01:19:45.176, Speaker F: So on different blockchains, there are some underlying differences which might be visible in the language. On substrate, the address type is 256 bits. By default. It can actually be different from that depending on how substrate is compiled. So the address type ncellidity is different. Also, constructors can be overloaded in substrate, and substrate uses different ABI encoding than Ethereum. Lastly, there's a print function which is just useful for debugging.
01:19:45.176 - 01:20:19.024, Speaker F: This is only available in development chains, but it does really help. So here's a funny little example of a solidity contract on substrate. So we have an overloaded constructor. The Abi encodes a function selector for the constructor. The Abi encoding for in 64 is simply eight bytes, little indian. So all the types are fairly simply encoded. So we have a function hello, which takes a string.
01:20:19.024 - 01:20:52.320, Speaker F: A string simply has a single field with length followed by the bytes of the string in the Abi encoding. And then we have a print function call which is built in, which calls the substrate print. And here we also can do string concatenation. I'll talk a little bit more about that later. So, a brief history of Solang. Is my screen very blurry?
01:20:53.300 - 01:20:55.344, Speaker B: No, it is crystal clear for.
01:20:55.462 - 01:20:58.290, Speaker F: Okay, okay. It's blurry for me.
01:20:58.680 - 01:20:59.092, Speaker A: Okay.
01:20:59.146 - 01:21:43.628, Speaker F: So in March I wrote a prototype. So this was a simple grammar for a very limited set of solidity, some LLVM, and within a weekend of hacking I could get a very primitive contract to run. So late in December I was awarded a grant from the web3 foundation. And this was specifically to complete solidity language support for substrate. This was divided into ten milestones. Five of those have already completed. So these are the remaining milestones for the web free grant.
01:21:43.628 - 01:22:43.540, Speaker F: And in September this will end. This should mean that Solang has feature language support complete doesn't mean that the output is optimal. The first aim is to get language correct. Also, there will be differences between Solc and Solang language. For example, on wasm it would be quite difficult with the current scheme to support assembly statements. Also there are some additions, like the print thing I just talked about and the different Abi encoding on different chains. So a little bit about how when I set out to write Solang, I wanted to build a traditional compiler.
01:22:43.540 - 01:23:29.860, Speaker F: I didn't want to do anything revolutionary. I also wanted to be simple. So there are some fairly simple stages to the compiler. So the first thing compiler always does is parsing. So we have grammar and we have a custom Lexa. And the Lexa is really needed because of the pragma statements. So if a Lexa tokenizes pragmatility with the simver, then it will produce a lot of tokens when it should just take the value as everything up to the first semicolon.
01:23:29.860 - 01:24:13.760, Speaker F: So the next stage in the compiler is the resolver. So the parser outputs the abstract syntax tree and resolver, well, resolves all the symbols in, that generates all the warnings and errors, et cetera. And in Solang it also makes it simple. It goes straight from the ast to a control flow graph. This is actually where the bulk of the code is in the project. This is where all the language support really is. And this is also a front end compiler because it uses LLVM.
01:24:13.760 - 01:24:40.760, Speaker F: So the next part is there's also a standard library. So this is some C code which gets compiled into llvm irr by clang. And this gives us a heap. So we have Malloc and realloc, et cetera. We can implement things like string compare and stream concatenate. We have a Kellogg hash for changes that don't provide it. That's what we have at the moment.
01:24:40.760 - 01:25:41.900, Speaker F: But this can be expanded to have much more things you might want in a language. So in the print statement earlier, it would be useful to be able to print ints or addresses or so. So string formatting would be a very useful thing. And that would be implemented in C, added to the standard library, and then compiled into LVMir. And then this will be linked into one big LLVM code and then using global dead code elimination, any unused functionality will be removed. So having a large summit library doesn't mean that the resulting wasn't file would be any bigger. So the last stage of the compilation is the emisser.
01:25:41.900 - 01:26:25.450, Speaker F: The control flow graph generated by the resolver is specifically geared towards LLVM. So there's mostly one to one mapping. We have to do some tricks for phi nodes, we also have to generate some specific things for WASM. So because WASM doesn't support two five, six bit arithmetic, we have to have some arithmetic functions. We have to have an ABI encoder decoder. So we have an ABI encoder decoder for Ethereum abis, and we have one for parity scale abis. It also does the linking midsommar library, and it has to do some touch ups to the final WaSm in order to make it correct.
01:26:25.450 - 01:26:44.110, Speaker F: And there's custom code for each particular target to generate target specific calls to the externals, for example. Lastly, we also need so.
01:26:47.600 - 01:26:48.044, Speaker A: In this.
01:26:48.082 - 01:27:41.420, Speaker F: Directory in source ABI we have some codes to generate abis. In substrate these are called metadata files. This is because in substrate the file contains more than just the function constructors and events. It contains names, comments, what compiler version generated the file in the future may contain the hash of the wasm. So you can check the ABI against the Wasm, make sure it's the correct one. So Solang is a Hyperledger project, and Hyperledger have the mentorship program, which is a bit like Google Summer of code. And through this, this year there's a mentorship program for Solang language server.
01:27:41.420 - 01:28:44.160, Speaker F: So this is for ides. So in an IDE when you write solidity, it can tell you where errors and warnings are. It can do syntax highlighting, give you information about identifiers, et cetera, things that make life easier when you're writing code. So that's mentorship happening this year, hopefully be more next spring after wait and see, of course. So here's some future ideas of things I would like to work on, but nothing has been done so far. So one of the advantages of using LLVM is that we can use the LLvM linker, so anything that can compile to LLVM IR can be linked. So if solidity had a foreign function interface, then any C code or rust or whatever could be called from solidity and then linked into the wasm.
01:28:44.160 - 01:29:33.750, Speaker F: People want to run all sorts of crazy stuff on chain, so this would help them do that. Also, this could be helpful to add crypto, which is written in c to smart contracts. Another thing is we would like push and pop on memory arrays because we have a heap in our standard library and a real lock. This actually isn't that hard to implement. This is just a question of wiring things up. The other thing I've had many people ask about is improved data structure sensitivity. So we want hash maps, linked lists, sets, trees, all those sorts of things, either in memory or in contract storage, and it'd be great to have those available.
01:29:33.750 - 01:30:40.536, Speaker F: If you have hash maps, et cetera, you might want generic types to use those. So something like, something in typescript would be great, though this is all hypothetical really, and I really would like to collaborate with the source team and see what they think. I do not necessarily want to take solidity in its own direction and away from the official solidity, kind of just ideas of what would be good. So the other thing with standard library would have much better string processing, and that just makes life easier for when you're debugging code. As Linus Torvald said, all you need is printf in order to debug any problem. So this would be just make life so much easier and. Well, that's it actually, yeah.
01:30:40.536 - 01:30:52.350, Speaker F: So it's just me working on a project now. There are people interested, but if you want to get in touch or ask any questions, please do. And thank you very much for your time.
01:30:53.200 - 01:31:19.270, Speaker B: Great, awesome. Thank you, Sean. All right, same procedure as every talk. If you have a question in the room, please do raise your hand or shout so that I know that you would like to speak. And for the people on the live stream, we will give you a minute now to think about. If you have questions, then please put them in the GitHub chat. We know you have a delay, so we are waiting for you.
01:31:19.270 - 01:31:28.940, Speaker B: Anybody here in the room has any questions with regards to the Solang solidity compiler? Yes, Chris.
01:31:30.660 - 01:32:13.870, Speaker A: Thanks for your talk. So I agree that we should come together more and talk about potential features. The main problem I see is that the language you're working on has substrate as the main, and maybe only target. And this is why your language and solidity sometimes have to make different trade offs. Right. So, I don't know. I just wanted to say that, for example, memory push and pop.
01:32:13.870 - 01:32:40.520, Speaker A: Right? If you have a heap and memory is cheap, and then this is certainly a good idea. But as a feature facility, we try to not add features that lure people into thinking that the operation might be cheap, but it is not in the end. So this is a certain trade off we have to make here. I think you're muted.
01:32:44.080 - 01:33:15.990, Speaker F: Thank you. I totally agree. So when, when as and when ewasm happens, then some of these concerns might somewhat go away around memory usage, et cetera. Also, now that the LLVM EVM backend is quite stable, I think it would be quite interesting to add that as a target to Solang as well.
01:33:17.720 - 01:33:22.600, Speaker A: Yeah, I would be very much interested in some benchmarks there for the LRVM EVM backend.
01:33:23.020 - 01:33:23.770, Speaker F: Yes.
01:33:25.980 - 01:33:37.740, Speaker B: Okay, we also have another question on the chat from Alex, who's asking. Some of those features you've mentioned are planned. At least we have some issues on them. Could you open issues for the other feature requests?
01:33:38.320 - 01:33:44.750, Speaker F: Okay, yes, definitely. That will be a great place to have a discussion on that.
01:33:47.220 - 01:33:47.632, Speaker A: Actually.
01:33:47.686 - 01:33:52.290, Speaker F: Is there a tag I can use on issues for language design?
01:33:54.260 - 01:33:56.128, Speaker A: Yes, it's called language design.
01:33:56.294 - 01:33:58.450, Speaker F: Okay, excellent. Thank you.
01:34:04.850 - 01:34:32.694, Speaker B: Okay. I'm not sure if actually, as the creator of the issue, you can tag it, but we can for sure tag it later on. Nice. Thanks so much. It seems as if there are no other questions for you on the chat so far, but I'm sure Sean will stick around for a couple of more minutes. So if you want to ask any question, just put it in the GitHub chat, and then you can take it from there. Next up in two minutes.
01:34:32.694 - 01:35:20.880, Speaker B: So let's also give it the time. So in case people just want to join for the talk, we have Sebastian from Quantstep, who will present detecting dos vulnerabilities caused by gas limits with fuzzing. Since we have a little time left, let's not rush it. Sebastian, I can see you here already. Are you speaking already?
01:35:24.050 - 01:35:24.462, Speaker A: If.
01:35:24.516 - 01:35:30.580, Speaker B: Yes, I cannot hear you. Maybe we can use the time to figure this out, then.
01:35:39.050 - 01:35:40.166, Speaker G: Cannot hear you.
01:35:40.268 - 01:35:43.750, Speaker B: Yes, now I can hear you in a very low volume.
01:35:45.210 - 01:35:46.326, Speaker G: Can you hear me now?
01:35:46.428 - 01:35:48.200, Speaker B: Yes, it's getting better. Perfect.
01:35:49.770 - 01:35:51.578, Speaker G: Is it still low or.
01:35:51.744 - 01:35:53.100, Speaker B: No, now it's good.
01:35:53.470 - 01:35:55.180, Speaker G: It's good. All right.
01:35:56.590 - 01:35:57.340, Speaker B: Okay.
01:35:59.470 - 01:36:02.860, Speaker G: Share my camera. Share my screen.
01:36:18.330 - 01:36:22.540, Speaker B: All right, great. Feel free to get started whenever you're set up.
01:36:23.870 - 01:36:35.358, Speaker G: Okay. I'm set up. Let me know if we should still wait. Or we can just start. Should we wait? I think it's five past.
01:36:35.444 - 01:36:38.574, Speaker B: I think it's five past. Yeah, you can get started now. Perfect.
01:36:38.772 - 01:37:49.480, Speaker G: Awesome. All right, so, hello everyone. My name is Sebastian, and today I'm going to be presenting this topic about detecting denial of service vulnerabilities caused by gas limits using fuzzing and other techniques. But we're not just going to stop at detecting, we're also going to be looking into how to generate exploits and fixes for these vulnerabilities. This is joint work together with Professor Vijay Ganesh from the University of Waterloo, as well as the group of his PhD students working on smart contract security. So probably most of you already know this, but I created the presentation for general public, so not assuming that everyone knows about smart contracts, they're just programs executed on a virtual machine. The Ethereum virtual machine, and calling a function in the smart contract changes the state of the EVM, and these state changes often involve transfers of funds and so on.
01:37:49.480 - 01:38:39.074, Speaker G: Programs, of course, might contain bugs, any type of program. So therefore smart contracts also contain probably bugs. And exploiting these bugs in smart contract can lead to stolen or frozen funds, as we've seen many times in the past. On the other hand, there's the notion of gas. The EVM has this gas mechanism which charges the function caller. So the transaction senders and execution fee which is computed using gas price times gas consumed. And there is a block gas limit for each block that is mined, and the gas consumed by any function call in transaction cannot surpass this block gas limit.
01:38:39.074 - 01:39:57.280, Speaker G: If it does, the transaction is reverted. And this is meant to prevent resource abuse and denial of service attacks on the Ethereum network. However, it can cause denial of service attacks at the smart contract level by not allowing a user to call the function and fully execute it. So this could lead to frozen funds, and frozen funds are basically lost funds. Here's a toy example, and this is something which you could naively implement if you're not familiar with solidity or gas issues in Ethereum. Basically, you want to reward all the users of a certain bank, let's say, and you want to push out interest payments every month or every year. And to do that, you basically iterate over all users and you do some computation based on how much balance or how much deposit they have and how much time they have kept their deposit there, and you basically send them this interest.
01:39:57.280 - 01:40:59.860, Speaker G: Now, you notice that this users length is controlled or influenced by users joining this bank right. So the more users you have, the more iterations this for loop is going to have. And if it gets out of, like, if it at some point passes the block gas limit, it's going to cause the transaction to revert. And if there's no other way to push out interests, it basically is going to lead to a lot of unhappy users of this bank and other problems. So that's the basic idea. More famous example. Are you still with me?
01:41:00.390 - 01:41:01.940, Speaker B: Oh, you broke up.
01:41:02.790 - 01:41:03.298, Speaker G: Okay.
01:41:03.384 - 01:41:07.554, Speaker B: And you seem to be frozen, so I can still see your screen.
01:41:07.752 - 01:41:10.660, Speaker G: Okay, where did you lose me?
01:41:13.130 - 01:41:21.400, Speaker B: Yes, exactly. Now I can see the screen again. Perfect. Okay, so, yeah, that was the slide. And you said a more famous example.
01:41:22.250 - 01:42:28.718, Speaker G: Okay, so more famous example is this project called governmental, and it's from 2016, and they suffered for some time due to this kind of blockcast limit. So there was a denial of service for the payout of the jackpot, which was 1100 e, because the payout mechanism was using too much gas. And as part of this payout mechanism, the contract was clearing internal storage using these instructions. And this was compiling to something that iterates over storage locations and deletes them one by one. And because the list was too long, it reached the block s limit at that point in time, and that led to frozen funds. So this is the source where I took this information from this Reddit post. And of course, like, back in 2016, the Block S limit was quite lower than it is today.
01:42:28.718 - 01:43:29.486, Speaker G: So it was under 5 million, and today it's 10 million. As we see, it keeps evolving, it keeps growing. With some exceptions, most of the time, the block gas limit is increased at a certain Hard fork. And the motivation for this work that we're doing and trying to detect and exploit this automatically is because there's simply too many accounts and also smart contracts on the EthereuM network to try to do this manually and find this out. So, basically, during our audits, even Today, we're seeing a lot of gas usage issues in the smart contracts. And using state of the art tools like slither or MyFril, you can detect these issues. It's pretty easy to detect.
01:43:29.486 - 01:44:36.680, Speaker G: You basically look for loops and some computations or function calls in those loops, and there's many tools freely available that find this issue. However, what do you do once you detect them? Right, one thing you can do is try to remove the loops and redesign your code such that you completely avoid loops. And you just try to accumulate values, as other functions are called. However, if that's not an option, you can just maybe do a gas analysis, determine when exactly the error, the out of gas error occurs. And you can add something like a required statement or an assert statement to basically prevent the revert from happening and prevent the waste of gas. And this is currently a manual, potentially lengthy and tedious process. So the solution which we propose in this work is to automatically generate these kind of denial of service exploits that lead to out of gas at smart contract level.
01:44:36.680 - 01:45:51.994, Speaker G: There's several challenges. The first one is like how do you determine the exact gas usage during execution? Second one is how do you search through the large search base of possible inputs to functions? There could be functions that have several parameters, or even like there could be multiple functions that need to be called in order to reach a state where this kind of dos or out of gas error is reached. So the first challenge is actually easy to solve due to web3 and solidity features. In our approach, we use the gas left function from solidity and we basically simulate everything on top of the Ganachi network. I mean the Ganachi tool. And the second challenge is more, I'm going to talk more about that one. So fuzzing a large number of inputs is more tricky.
01:45:51.994 - 01:46:40.510, Speaker G: There's several possible fuzzing heuristics. For instance, you can brute force every possible input, and that's very slow. You can do a divide and conquer approach, which is faster, but it's not always applicable if you don't have certain rules like integer intervals and so on, that you can easily divide into partitions. Basically, for our approach we're using reinforcement learning, which is also fast and is more generally applicable. And we'll see in a second why there's also possible, other possible heuristics. Not saying this is the best one, but this is the one we chose for our project. So the reinforcement learning approach looks like this.
01:46:40.510 - 01:47:47.250, Speaker G: We basically model the problem as a markup decision process where we say that the set of states s is all of the states of the EVM. Basically a state is a state of the EVM. The possible set of actions is calling smart contract function with some randomly chosen inputs, or also more carefully chosen inputs, increasing those inputs, decreasing them, and so on. So these are like the actions that the agents, the reinforcement learning agents can take. The probability of transitioning from one state to another, basically when executing a given action is always 100% because the EVM is deterministic. And the interesting part is the reward function. Basically the reward that the agent gets when he transitions to a state s is one minus the division between the gas left and the block gas limit.
01:47:47.250 - 01:48:53.190, Speaker G: And this is because we're rewarding actions that are going to consume more gas. So if the transaction that led to the state s used more gas, we're going to give a higher reward because we want to reach an out of gas error. So this is pretty intuitive. Here's a simple example of a pure function that just receives value as input, an integral value as an input, and iterates over all integral values from zero to that number, sums them up and returns the sum. The goal of the reinforcement learning agent would be here to find the right value for n, which leads to this kind of out of gas error. And we're going to see later how this code is fixed. Another example is maybe a slightly different function, still pure function that has two parameters.
01:48:53.190 - 01:49:50.182, Speaker G: You can even think about more parameters, but basically you do some computations with these parameters and they don't always influence the result or like the gas usage in the same way. So here you can see that we're dividing n by m. So the goal of the reinforcement learning agent is to find a large value for n and a small value for m. But m should not be zero because otherwise it leads to a division by zero. And what are the right values for this? Or maybe something else we're going to see later. What's the right way to fix this? Here's another example where there's a small contract that is vulnerable. It has several entries, integral entries in them in it, and it has several functions.
01:49:50.182 - 01:50:45.210, Speaker G: And the first function just adds an entry in the list. I saw typo there. The second function gets the entry at a certain location. And the third function sums up the list of entries, basically returning the sum. So here you can see that the goal is to find a trace of function calls like this, basically adding several entries up to n and summing them up. And the question is, what's the value of n such that when you call some entries, it leads to an out of gas error. So the challenge is how to determine.
01:50:45.210 - 01:51:51.060, Speaker G: First you need to determine which functions affect basically the loop bounds. Because as we saw before, there was also a function called getentry. And the reinforcement agent should not be calling that. It would be just wasting sort of time calling that one because it's not going to affect the loop bounds inside of some entries. And the solution to detect which functions affect the loop bounds that we are taking is to do reverse taint analysis and then forward taint analysis. So for those of you who are not familiar, taint analysis is a form of information flow analysis where you first, taint or tag a memory location, for example, a variable x. Then you trace the flow of that tainted value through the execution of your smart contract functions, and you determine which instructions or which other memory values are affected by that tainted part.
01:51:51.060 - 01:52:46.100, Speaker G: The information flow may be explicit where you have a direct assignment memory transfer, and it could also be implicit where different values in memory depend indirectly on your tainted value. For instance, if you have a branch condition like this, if x greater than zero, and inside of the if and else branches, you have other values like other variables like a and b, those variables will be implicitly tainted by x. So then we do the reverse taint analysis on that function. We slightly modified the function some entries to also include an implicit taint example here. Can you still hear me?
01:52:46.870 - 01:52:49.746, Speaker B: You were breaking up, but now you're back.
01:52:49.928 - 01:53:49.158, Speaker G: Okay, so here we modified some entries function a bit in order to show that. Can you still hear me? Okay, to show that there is a possibility of an implicit taint, we start from the loop at the close to the bottom and we taint the bound variable. And we go up, we go in the reverse and we see that first the variable n is tainted by bound because there's an explicit assignment. And also the length of the entries list is also tainted explicitly. And we also have an implicit tainting of those variables, but since they're already explicitly tainted, then basically they're tainted. We also have a taint of the constant zero. So that instruction is also tainted where bound is assigned a zero value.
01:53:49.158 - 01:54:57.930, Speaker G: And based on this analysis, we can say that, okay, the input of the function sum entries is tainted and also the state variable entries is tainted. That is, the length of this state variable is tainted. So once we determine this, we can do a forward taint analysis where we just taint the entries length and we start executing each function to see which instructions in which function may affect the entries length. And we see that only the add entry function affects the length of the entries list. The get entries functions does not. Therefore, the reinforcement learning agent can just try to call this function before it tries to see if it ran out of gas using that sum entries function, which is not shown here due to lack of space. So the question here towards the end is like, okay, we took this approach, we did all this stuff.
01:54:57.930 - 01:56:09.940, Speaker G: We ran the reinforcement learning agent. So what do we do once we know where the out of gas error occurs? I already hinted towards the answer. We basically fix the code, and fixing could look something like this. If removing the loop is not an option, you could have a require statement close to the beginning of the code, which basically signals that the parameter or the parameters that you provided will lead to an out of gas error. The second example which I showed you could also go for fixing, actually after you've done all the computations, you can add this require statement which is easier than checking values for different inputs. And the third example is also interesting because we're not placing the require statement inside of the function which has the loop like not inside of some entries, but we're placing it inside of the function that affects the length of the loop. So inside of the add entry function.
01:56:09.940 - 01:57:28.730, Speaker G: And of course these values are just like preliminary values, they're just mocks. You can also not just hard code the values in there, but you can also let it be settable by the contract owner, such that if there's a fork or the gas limit is increased, they can adapt this. Or if the cost of the opcodes changes, these values can be changed as well. In conclusion, just want to say that probably know loops cause out of gas errors in smart contracts, and these can lead to frozen and hence lost funds. Detecting such problems is quite easy with state of the art tools, however, determining exactly when they would occur with which inputs is harder. And we're taking the approach of fuzzing with reinforcement learning and taint analysis to generate the inputs needed for an out of gas error faster and in a more general way. And yeah, we're using taint analysis to guide fuzzing.
01:57:28.730 - 01:57:34.400, Speaker G: That's it from my side. Thank you very much. Any questions?
01:57:35.570 - 01:58:04.760, Speaker B: Thank you so much. Yeah, so now first to the people in the room, if you have any questions for Sebastian, use the raise your hand feature so that I can see that you would like to ask something. And to the people in the live stream, we know you're lagging behind, so we will wait for you for 1 minute and you can put your question in the GitHub chat. Yes, Jocelyn.
01:58:06.540 - 01:58:08.488, Speaker A: Hi Sebastian. Nice talk.
01:58:08.654 - 01:58:09.850, Speaker G: Hi. Thank you.
01:58:10.540 - 01:58:12.090, Speaker A: What tool are you?
01:58:16.060 - 01:58:19.596, Speaker G: This? You mean for reinforcement learning or which part?
01:58:19.698 - 01:58:21.100, Speaker A: For the wall approach.
01:58:24.080 - 01:58:26.670, Speaker G: We're building a custom tool for this one.
01:58:26.980 - 01:58:29.730, Speaker A: Okay. So it's everything built into us.
01:58:30.660 - 01:58:41.540, Speaker G: Well, it's going to be published, but it's basically the joint effort between the university and quantstamp, and we're going to release the code once the paper is accepted.
01:58:47.530 - 01:59:00.730, Speaker B: Cool. Anybody else here on the room? Any question? If not, I would give a little bit of more time to the live stream viewers to also post their questions into the chat. We have some time left. Anyways.
01:59:05.110 - 01:59:07.140, Speaker G: I wanted to say thank you for your question.
01:59:17.060 - 01:59:30.370, Speaker B: In the meantime, while we are waiting for more questions for Zebi. Yep. Are Gonzalo and Martin already there?
01:59:32.100 - 01:59:33.656, Speaker A: Yep, we are.
01:59:33.758 - 01:59:34.216, Speaker B: Okay.
01:59:34.318 - 01:59:35.156, Speaker A: Francisco.
01:59:35.268 - 02:00:08.880, Speaker B: Hi, guys. Okay, I'd suggest that we wait until the scheduled time for you so that in case people are only joining for your session, they have enough time to join the live session. So if you like, we can already try whether the setup is working for you guys because we didn't do a trial session. And thank you to Sebastian again. If there should be any questions coming up, I will relay them.
02:00:09.030 - 02:00:10.464, Speaker G: All right, thank you very much.
02:00:10.582 - 02:00:11.360, Speaker C: Thank you.
02:00:11.510 - 02:00:12.276, Speaker G: Have a nice day.
02:00:12.298 - 02:00:23.450, Speaker B: Bye bye. So, Gonzalo and Martin, did you try the share your screen feature? Are you sure that this is going to work or should we just try it now?
02:00:24.140 - 02:00:31.800, Speaker A: We tried it before. Not sure it's going to work, but we tried it before for surprises.
02:00:34.620 - 02:00:40.350, Speaker B: You didn't want to try it with me first, so, no, there's nothing I can do for you guys.
02:00:42.000 - 02:00:48.910, Speaker A: We're very sorry. Francisco. Like, Corona has everybody on the tip of their toes. Like, these are crazy.
02:00:49.440 - 02:01:15.052, Speaker B: Know, I know. No worries. Now, I think we have, like, five minutes left. Let me check the agenda. Yes, we have five minutes left before your talk, so let's just give some people more time to join in case they want to join for your talk. And, yeah, for all of you guys who are watching, feel free to get a drink, a coffee, a tea, a water. Stretch your legs.
02:01:15.052 - 02:01:21.450, Speaker B: I will go and stretch my legs really quickly, and I'll be back in a couple of minutes.
02:01:22.220 - 02:01:37.790, Speaker A: I'm going to ask a question for all the people out there. While we are not presenting yet, if you guys want to comment on roughly how many lines of code your systems have, like just on gitter or something while we wait, that'd be amazing.
02:01:40.180 - 02:02:01.312, Speaker B: Okay, cool. So, guys, you have something to do for the next five minutes. Let us know how many lines of code your code base has. I'm back in just a second. Michael. Streamer. We don't need to pause, it's just a really tiny pause, so no need to do the break mode.
02:02:01.312 - 02:07:08.460, Speaker B: Thanks, guys. Nobody else here has a code base, guys. I can't believe it. Only maker. I think gnosis also should have some pretty big code bases, but has been mentioned yesterday, but Rihad is not around to comment. Okay, anyways, it's now 35, so I think let's start with the next talk, which is visualization of large code bases from Gonzalo and Martin. Go ahead, guys.
02:07:10.850 - 02:07:23.882, Speaker A: Okay, let's see if this works. Chrome tab share. It's working. Is it working?
02:07:24.036 - 02:07:31.314, Speaker B: Yes, it's working. But you can now also click on your camera again so that people would see your face in addition to the screen.
02:07:31.432 - 02:07:37.640, Speaker A: Okay, cool. So if I click here? Yes. Made it work.
02:07:38.570 - 02:07:39.526, Speaker G: First try.
02:07:39.628 - 02:08:16.754, Speaker A: There you go. So I think introductions are done. Me and Martin, from consensus diligence, will be talking for 20 minutes today about visualization of large code bases, but also framing it as to why we've built the tools that we did and how they play a role in securing large code bases. Right. This is not only about visualization. This is about making your code secure when your code needs to be big. Right.
02:08:16.754 - 02:09:00.670, Speaker A: Because security, and oftentimes as the security team to many products, we advocate for small code, but sometimes it's just not possible. Right. Some things need to be complex and need to be sizable. So that's why we're talking about this today. Let us start, if I may add to that, we have this unique not problem, but challenge sometimes that new hires in your companies might have as well. So they get to see your code base and it's fairly large. And how do they start understanding the whole code base? And for us auditors, this is kind of like regular business.
02:09:00.670 - 02:10:18.554, Speaker A: So with every new client, we get like a new, more or less complex copace. And we want to make sure that we spend as little time as possible in understanding the complete system and finding the red lines so that we can start auditing and finding any security issues in there. So that's why it's also an interesting take on what tools are available or how can we aid new hires that we want to have in this ecosystem to be able to quickly write up code and quickly understand existing code bases and kind of also in a secure manner. And that's what we are going to talk about. So for the history of the tools, we'll give you a little bit of background on why things happen and what happened and at what time. So this whole thing started. Well, this whole thing, obviously, we have always been thinking about security, but this specific branch of our tooling started because of Aragon Os, right? Up until then, all the code bases that we had seen, or me specifically, had been related to tokens, simple implementation of vips, stuff like that.
02:10:18.554 - 02:11:44.390, Speaker A: Right. And then came Aragon Os. And it was a beast of a code base, right? And I'm not saying that it was not well written, which it was, and very well documented, too. Right? Probably better than most of the projects that we have audited. But still, the sheer size of the system made it so that it's really hard for you to create mental models, right? Even with threat modeling exercises, you always end up to do some sort of visualization, right? Be UML diagrams or something that helps your brain come to terms with such a high level of modularity and composability, right? So size is not the only factor that plays a big role in making a system complicated. Modularity too, right? And this is why Surya was born. Surya is purely another abstraction on top of the ast that actually maps it out, right? It's a call graph printer, nothing fancy, that was worn out of necessity.
02:11:44.390 - 02:14:13.874, Speaker A: The main drivers for Syria to be built the way it was were portability and low dependency tree, right? We wanted to make these tools so they could be run everywhere and anywhere and also have a small footprint so that we could run them everywhere and also be cross platform. We just want to make it easy to run these things, right? We don't want to deal with versioning, we don't want to do any of those things that end up complicating a tool like this. Because the tool's purpose to begin with was to make it easy for people to visualize and mental map something. So if we make it hard to run, then it's going backwards, right? So we went with Federico's awesome tools and the ANTLR programmer and parser that he had built for a long time already, and then came along the best part of the stack of this tooling branch that Martin wrote, which is the solidity visual auditor extension. I'm now looking at the name. In doing the presentation, I see how we could probably make it more receptive towards developers, because even though there's auditor in the name, it's only target audience for sure. And again, going back to what I was saying before, if our goal is to make it as easy as possible for people to visualize and map out systems, then using cleat tools is not really the way to go, right? While you're developing, you don't want to interrupt flow by having to run a system on your terminal or in another terminal session if you use Vim or just like even going through the trouble of visualizing those things in another window is already too much.
02:14:13.874 - 02:16:11.900, Speaker A: And so Martin brought us this beauty. Basically, how this was born was I was new to solidity, but I didn't want to always have to keep everything in my mind about all the language specific things like what parts of the language are actually considered insecure, or not as secure as they should be. Or if you write your code for example, in visual studio code, which is the IDE that I'm using, it wouldn't alert me on certain things that I would want to see from a security perspective, which is passive information about what functions are actually publicly accessible, what are public interfaces, what are only internal functions, or when I'm pasting some piece of code from stack exchange because I want to do like a delegate called proxy and I'm totally unaware what this is actually doing. It's pretty easy to mess it up, right? And if you paste it into the ide and you have this extension enabled, it will throw a whole lot of red flags at you by highlighting the delegate call itself in plain bold red, so that you know, like, oh, I should probably read up on that thing, what is it doing? They can even hover over that delegate call and we give you extra information about what the call is about and also what security concerns there might be around it. So it was initially geared towards allowing new developers to develop code by getting passive security information while they are writing code. So it was starting out as a syntax highlighting, as an extended syntax highlighting extension, but it got into that big beast of a lot of other nice tools that we kind of put into that at some point because they were always useful for us when we had to engage with new code bases. Hence the name for this talk.
02:16:11.900 - 02:17:23.762, Speaker A: Yeah, so you can see a little bit of it. We'll show the tools in a minute, but you can see a little bit of it, like the syntax highlighting under the pragma and under the state variables. Also, this is like an accompanying thing that Martin built, which is basically an SVG interactive graph for VS code, which is like the frosting on top of the cake. Honestly, what happened a little bit after, so probably when you're into development, is that we realized that we needed to own all of the dependencies for speed of development and support, right? So we took them up recently and thank you so much. I thought, I wink at the team and thank you guys for doing that. We don't need to worry about the grammar anymore because the solidity team actually took it under their wings. So we're free of that now also with the whole corona thing and our workload has been these past few weeks.
02:17:23.762 - 02:18:31.400, Speaker A: And so I know that there's another on GitHub solidity parser that started a new fork of the solidity parser that is more up to date than ours. We are very excited about contributing there, but thinking that we should probably still maintain our own in case in the future we need it again. Yeah, this is also like a recommendation from our side. When you're developing a new language, always make sure that people have grammars easily available to their tools. The NTLR grammar is nice because we can use it in a fussy way. It doesn't need to be as strict as the compiler will be afterwards, because we are mainly using it for visualizations, even though there can be some unsharpness in that case, but you have to be aware of that. But for most of the cases it's fine if we find all the functions, all the state variables and stuff, and we don't need to find like 100% of it all the time.
02:18:31.400 - 02:19:44.480, Speaker A: But it's utterly important that if parsers are readily available, like generic ones, as the MTLR parser, then it's very easy to build a whole lot of tools or IDE integrations for all kinds of platforms, and it makes it just easier for people to adapt the language. Thanks for taking it under your wings as well. Yeah, and yeah, since then a lot more tools have been built by Martin, all again with the same goal of building passive security, as Martin has said. Right? This might mean syntax highlighting. This might mean more information about the code that you're writing and keeping yourself up to date with it, including, as we just said, giving you tools that allow you to find that red line that is kind of like through a new code base quickly. And that's kind of what we're also showing in a few seconds. Then in a live example.
02:19:44.480 - 02:21:07.610, Speaker A: Again, I have been talking about this throughout the whole presentation, but the reason why we're building these tools is because of these three main factors, right? We understand that everybody has deadlines, and obviously this comes very much from an auditor's perspective. So this may be biased, but we try to put ourselves in the developer's shoes, right? And think what is important for them and what constraints they have and what challenges they're facing. And we know that everybody has deadlines, right? Time passes. You don't have 200 years to write a code base. At most, you have your whole lifetime, right? So good external support is important even when you have strict timelines. And you also should understand your code before anyone else, right? Then other people will need to understand it too, hopefully. But you should be the first person to understand your code, right? I can't remember who said this, but code is made to be read by humans and then incidentally by machines.
02:21:07.610 - 02:21:19.754, Speaker A: So you should make sure that you understand your code even before someone else does. And when you're new to a language, you learn by making mistakes.
02:21:19.802 - 02:21:20.398, Speaker G: Right.
02:21:20.564 - 02:21:53.334, Speaker A: And our tools can help flatten that curve and hopefully the mistakes associated with it. Right again, passive security. Okay. Yeah. And just again, borrowing very much from the preparedness mindset for an audit. And this is actually very specific to audits. Right.
02:21:53.334 - 02:22:41.186, Speaker A: But we can also generalize it towards writing safecode. You should document your code, you should make it easy for everybody else to run. Right. So understand it first, make everybody understand it and. And run it after that, clean it up, run preliminary tools like linker, run proper analysis tools, and then release it to the world. As you might or might not know, we have a branch of toolings. The diligence team has also started a branch of toolings that is now an entity in its own right, Methex, that does dynamic analysis.
02:22:41.186 - 02:23:29.190, Speaker A: Right. Like it's a concolic execution engine, static and fuzzer. But this part that we're talking about today is made to tackle the second and third sections that you see here, right. So it basically helps you earlier in the development lifecycle so that when you get to steps four and five, you already have both. Again, a good mental map of what's happening in your whole system. You have a good knowledge of where the danger areas lay in your code and hopefully just make everybody's job easier. Right.
02:23:29.190 - 02:24:12.370, Speaker A: Be developers, quality assurance, auditors, I don't know, whatever is there, what stakeholders are there in the middle. We just want to attack this problem super early in the development cycle so that everybody can have like a merrier life. So let's see the lead tools in action. Do you want to take on the screen, Martin, and share the. Let's do it. We have a couple of minutes left, right? So let's quickly do that. So you should be able to see my wisher studio code instance.
02:24:12.370 - 02:25:05.426, Speaker A: And notice we're doing Aragon, we're doing Aragon, because it all started out with Aragon and Gonzalo trying to make sense of the code base, like years ago when he first had a glance on it. So this is visual studio code, basically. I tried a whole lot of ides, and this is the last thing that actually basically worked quite well, and it is quite extensible. So that's why most of the things I do are nowadays with vs code. So that's like in the middle, you see the code editor. There's no window open right now, and on the left side there's the code three or the file three with all the files. I'd like to show you, like the metrics plugin first you install it from the extension marketplace.
02:25:05.426 - 02:25:40.542, Speaker A: It's all free, it's all open source, you can audit it and then just go ahead, click here. On any folder where you want to, like where you expect solidity files, click right, click solidity metrics. And it will take a few seconds because it's now parsing, it's finding all the solidity files in there, and it's parsing all of them with the antilab parser in the end. And then we do some nice number crunching on it. So you see like this is the workspace name, table of contents. Let's skip to the scope. You see what is in scope, what is out of scope, what code base we were using, and then you get that view.
02:25:40.542 - 02:26:37.470, Speaker A: And this is quite nice. Like the first time I see a code base, I usually run this tool because it shows me how many solidity files are in the code base, how many contracts are in total in the code base. These are all the logic contracts, and there's nine interfaces in the contract as well. Then you will see, for each file you see what type of contract it is, whether it's a library or just an ordinary contract, or even like an interface. From this icon you see the number of lines, normalized source code lines, which is like normalizing the function, signatures, comments that are in the file, which is a good indicator of whether you need to spend a little bit more time on documenting stuff. You get even a complexity score, which is not cyclomatic complexity, but something is much more simpler. Basically we find anything that is risky and add a score to it, even if it's a branch.
02:26:37.470 - 02:27:12.398, Speaker A: Anything that adds complexity or security concern just adds something to this core. And the higher it is, the more likely the higher should be. Probably your priority to look at things in here. And we also watch out for any patterns or calls that we know that should be investigated. For example, this file is using hash functions, basically catch up calculations. And this file is doing any assembly stuff. And then we have this file, which is, to be honest, I prepared this so that it does show up with a lot of things.
02:27:12.398 - 02:28:02.018, Speaker A: It's not really the Aragon code base that has all this stuff inside, but you see like there is payable function in there, the construct is destroyable, it's initiating e for transfers and stuff like that, and it's even creating stuff. So you get a nice overview on where might be some risky points in that 60 files of code bases. And it helps you to dissect the interface and stuff that you don't need to look at in the beginning. From the stuff that could be very critical in a code base. You get a summary review, you see everything that was excluded from it. We even have some experimental stuff in there, which is kind of like our take on a bit on the risk chart. This is something that we feel like should be presented to solidity developers.
02:28:02.018 - 02:28:45.580, Speaker A: So they know the more often they use assembly code in there, the more complexity add to the system and the more time it will take to actually also review that and make sure it's secure. You get another overview on what type of contracts are instantiated and what versions were observed, and also anything you might want to know from the inner workings of this extension. We just print it out just for fun because we have the data. And you also get like for everything, inheritance graphs and stuff. This is basically serial output. So this is the metrics plugin I think. Are we already overtime or do we have like two minutes or three minutes to also show up the auditor extension for a second?
02:28:47.550 - 02:28:53.934, Speaker B: You guys are overtime, but we are running into the break so it's okay. Okay cool, go for it.
02:28:54.132 - 02:29:37.740, Speaker A: So I tried to be fast. So you can also install the solidity visual auditor extension, which is basically this one. Once you install it, it will show up a new icon in the panel on the left side. You can click on it to get like a quick overview. You can also hide it if you don't want to have that because I know I don't want to see all the icons on the left side. So what you can do to explore the code base, for example, is we can click on this view and it should update the workspace Explorer view. Unless my machine is too slow today.
02:29:37.740 - 02:30:17.426, Speaker A: Let me just quickly try that again. It okay, so it's now searching for all these solidity files in the code base. The idea of this explorer is it should only show you anything related to solidity files. You don't want to see any JavaScript files or any other things so that you can specifically focus on solidity code. So you can browse around whatever you want to see in there. You can right click and even find top level contracts. This is kind of something that I usually do in the beginning when I am exploring new code bases.
02:30:17.426 - 02:31:11.614, Speaker A: I want to see what is the main interfaces to that contract system. So this list that is now populated here will show all the contracts that are the most derived, basically that are not inherited by any other contracts in the system anymore, and they are very likely to be deployed at the end for your smart contract system. You can also from this view on just flatten all the files if you want to flatten stuff. But you can also create graphs with basically sura for certain folders or selected files or anything. So let's just try that. By the way, we always give you the source code so that you can modify stuff because you might want to use it in a different way. So this is basically what's going on in the whole Aragon OS system from Asuria's call graph view.
02:31:11.614 - 02:32:09.500, Speaker A: And it's kind of a lot of things. If we just boil it down to generating the graph for a couple of files, it might even be easier. All right, so that's the view just for the APM part. What you can now do, for example, is some simple taint analysis. If you want to see any calls that reach to root node, then you can even visually trace that path or do any other thing in the code base, however you want to use that. Also you can show the inheritance graph just for this part. Or what's also interesting, a lot of time is you want to see, for example the UML chart for that file, and you can also just generate it.
02:32:09.500 - 02:33:00.780, Speaker A: You can even do that for a flattened version of that file. So if you click here, flatten that file, it will flatten it down and you can then just visualize that flattened file. This will take a little bit more time, but it's worth the wait. So this is everything around the ACL flattened and how the contracts are derived from each other. And we even parse out, which is kind of like an experimental feature a bit, we parse out potential actors in the system. So whenever we see any addresses in the system, we denote them as possible actors to the system. Yeah, so that's basically the graphing functionality that we have in here.
02:33:00.780 - 02:33:08.622, Speaker A: One thing that might also be interesting is let's just go back to for example, ACL for a second, just looking.
02:33:08.676 - 02:33:27.960, Speaker B: At the time, Martin, because we want to allow the people to have a little break at least. What I would like to do is already remind the people on the live stream, if you have questions, please put them in the chat now. And if you have questions in the room, please already raise your hand so that I can estimate whether we need to discuss stuff or whether Martin can go on.
02:33:28.410 - 02:34:06.434, Speaker A: All right, I will come to an end. So one interesting feature we were working on recently is context sensitive information. That's why we call that view a cockpit view. So if you click into a method here, you get like context specific information in these views on the side. This view just shows all the public state changing methods plus their modifiers so you can see whether create permission has an authentication modifier on it. Just quickly check if any of these are like missing modifiers and stuff. And the last feature I'm talking about is basically also from Syria, which is the call tracing function.
02:34:06.434 - 02:34:34.860, Speaker A: So when you click into this method, for example, it will update the function call trace, which basically shows all the downstream calls that are happening within that method or even like somewhere in the layer below. And something that we might also integrate in the future is this into the other direction so that you can find all the callers to that function and other things. All right, so that's basically it from the demo side. If there are any questions, please go ahead.
02:34:35.230 - 02:35:22.700, Speaker B: Great. Thanks so much, guys. All right, any questions in the room? If not, then I would recommend that we move the Q A to the chat. You guys are also there, right? And then we can have at least seven minutes break where everybody can just go get a drink, stretch your legs, relax your eyes, do whatever. Thank you so much and see you back in seven minutes for our first open discussion of the day, which will be on gas. Unrestricted alternatives for send and transfer moderated by Alex Michael, can you go into break mode with the stream until 04:10? So seven minutes. Thank you.
02:35:22.700 - 02:41:42.680, Speaker B: Alex. Test, test. Can you hear me?
02:41:45.450 - 02:41:46.550, Speaker A: Yeah, I can.
02:41:46.700 - 02:42:21.250, Speaker B: Perfect. All right, welcome back everybody. We can start the stream again. It, thanks. Cool. So that was a very short break. I hope you are all excited and energized for our first open discussion of the day.
02:42:21.250 - 02:42:28.610, Speaker B: Gas unrestricted alternative for send and transfer. Handing it over to Alex.
02:42:33.260 - 02:42:35.230, Speaker A: Is the screen sharing working?
02:42:36.480 - 02:42:37.230, Speaker B: Yes.
02:42:39.920 - 02:43:17.448, Speaker A: Okay. Yeah, the stream mode presentation is still kind of weird. All right, so this is a discussion session regarding the functionality for transferring eater. And currently we have send and transfer. So I think in the end we're going to have some discussion on some alternatives. But first I want to go through some background and some problems why we may want to change this in the first place. So.
02:43:17.448 - 02:44:03.210, Speaker A: Just one sec. Um, all right, so some quick background. Whenever an account has code and there's any kind of incoming ether transferred to the account, the code is executed. But there are two exceptions here. So if it's a target of a coinbase transaction or target of a self destruct, the code is not executed. And the reason for this functionality is because we want contracts to be able to reject incoming eater. But we also want to have this code executed at the time of incoming eater to not do too much.
02:44:03.210 - 02:45:27.810, Speaker A: It should only be able to maybe load something from storage to check the state and it could emit events, but it shouldn't modify the state. And here you can see that this functionality is kind of complicated. So here I have just four different cases. What happens when such an account is called? So this is kind of like the call of code, but I removed some of the other inputs to the call upcode, and I'm only just showing the gas and the value inputs. So if we are setting the gas property to zero, and we're not sending any balance, then zero gas will be available for the target, which essentially means it cannot do anything. If we're still setting the gas to zero, but we are sending any ether value, then suddenly there will be a stipend of 2300 gas at the time of executing the contract. And then you can follow the logic that if you do pass gas and there's a balance sent, then that extra gas is passed on top of the 2300.
02:45:27.810 - 02:46:31.148, Speaker A: And because this call is kind of complicated, a while ago a function called send was introduced in solidity, which tries to hide this difference. And this function tries to enforce that the recipient code will always have 2300, not less and not more. And send seemed not to be good enough at some point. So we had a transfer, and the only difference between the two is that send returns a boolean to state the outcome, while transfer would revert in case the transfer failed or the execution failed. And we also have this low level feature, the call, where you can manually set the gas and the value. And you can see the old syntax below, which was quite confusing. But luckily, lately we were able to come up with a much nicer syntax.
02:46:31.148 - 02:47:27.896, Speaker A: So these are like the three main ways one can send ether to a different account. And this actually means we have quite a few problems. I think I listed five problems here. So the first problem is that the stipend is not fixed, or maybe not fixed. Maybe it won't ever stay at 2300. There was a proposal thing maybe half a year ago or so to raise this stipend to 3500. Now, if we assume that maybe this proposal goes live, that means that any solidity code which was deployed prior to this proposal being accepted would have two different behaviors when they use send or transfer, because the compiler inserts this little piece of code which ensures that 2300 gas is sent.
02:47:27.896 - 02:48:27.080, Speaker A: If no value was passed after this change, the send would behave differently in case a value was passed or not. So that's already a difference. I mean, we could say that the send with zero value is kind of not useful, but we still have that feature. Now, a second problem is that gas prices can decrease. And this was proposed last year in Constantinople with net gas metering. And the outcome was that basically an S store was possible within the stipend, and this was the reason why this function was eventually dropped from Constantinople. Here's another proposal which was suggesting a workaround, and eventually these two proposals merged is what was launched in Istanbul.
02:48:27.080 - 02:49:29.810, Speaker A: So there's like yet another edge case in making sure that s store is not possible within the stipend. But gas prices can also increase. So with Istanbul, there was a couple of new changes, including two which raised the cost of s load and balance. And as suddenly people were running into the issue that this fallback function may not be possible to be executed after Istanbul if it did too many s loads. State reads so it is like the third problem we have here. The next problem is basically, I guess, the reason why. One of the reasons this stipend exists as a limit is because we don't want any kind of reintrancy as an issue.
02:49:29.810 - 02:50:22.880, Speaker A: And that's why we don't want s store to be possible within the fallback. Because if you remember, that call itself, I believe, is still 700 gas. It is possible to make a call out of the fallback function within the stipend, but it's not possible to do an s store and a call. But I think it's generally a bad idea to rely on these gas costs, especially because of the previous problems listed. And I put here an example of another proposed vip which would introduce transient storage. So those are new opcodes which I believe wouldn't really have an effect here. But I'm sure whoever looks deep enough could find some further issues with new proposals.
02:50:22.880 - 02:51:25.664, Speaker A: And I believe the last problem here is, at least in test presentation, is proxies saving. Though I mentioned that it's possible to do a call because a call is just 700. It's not only the 700 which is needed to prepare a call, one also needs to prepare the memory, the call data which is passed. So there's a lot more going on than just the 700 itself. For many cases, I think proxy calls out of the fallback today is possible, but if you think about those possible gas changes, it's likely that this will break at some point in the future. So here's like two further reading links this blog post is quite interesting. It explains some of these problems, maybe in a more lengthy way, and the issue itself is where we discuss the problem on the repo.
02:51:25.664 - 02:52:52.730, Speaker A: And this is quite a long issue, and we have listed quite a few different solutions to the problem. So basically this issue is, I guess, which triggered a call, triggered this session, because I think it was the first one where somebody spoke out that the send and transfer as it is is just not good enough. So I collected some of these proposals from the issue itself and the first one, and I guess this would be a good time to maybe, because we're quite good on time to maybe take a break and get some feedback from anyone, if anybody has any questions before we go into the discussion part. In its current form, it's not obvious that it stops execution. So this is something. Yeah, maybe we should find a better name for that. Yeah, I would be really interested to maybe hear some feedback from auditors.
02:52:52.730 - 02:54:17.418, Speaker A: I know that Jocelyn is still here. I wonder if Gonzalo left. But it would be nice to get some feedback from people who are really familiar with auditing contracts. I'm here. I was just not paying enough attention, I guess. Sorry, Alex, is this on the send transfer proposals? Yeah. So I guess the question is, before we go into the actual proposals, the question is, is distend and transfer a problem, maybe for auditing and for proxies? Yeah, I guess for any of these? Or would anyone say that it's not worth changing? So to give you a perspective on what is happening right now with us, I think even the tools themselves are a little bit tweaked to worry less about sending transfer because of their gas stipend.
02:54:17.418 - 02:54:58.106, Speaker A: Right. So I think by now that is very much ingrained into everybody's mind. Right. Developers and auditors alike. Changing it to pass on all guests just breaks that assumption in everybody's mind. So I would be wary of that. But we have also advocated since that conundrum with the opcode gas repricings that people do use call with the value parameter.
02:54:58.106 - 02:56:20.002, Speaker A: Right. I'd say that number one would be more sensible towards be respectful of everybody's just, again, mindset and assumptions about the EVM and solidity specifically. But I don't dislike three and four. I think just number two is a little bit more dangerous. Would number four suffice for all use cases? I mean, of course there's always the call, but if we just remove send and transfer and keep call and add a kind of withdrawal function, would that work for most cases? I believe so, yeah. Obviously we would still see call being used in a lot of places. But I'd say that if I had to give a very rough estimation of everything that I've been seeing the past few months and years, I'd say it would suffice for at least half of the instances where a transient send is being used.
02:56:20.002 - 02:56:24.940, Speaker A: I think half is like a good rough estimate issue.
02:56:26.830 - 02:56:39.700, Speaker B: We also have Jocelyn and Rijad wanting to say something. You're muted, Jocelyn.
02:56:42.380 - 02:57:40.360, Speaker A: Sorry for that. Okay. Yeah, so I was saying that is a really good comment about the fact that we should not rely on gas to mitigate reintroducing. So the first solution makes sense to remove send and transfer for me in particular, because right now, for a lot of developer, it's kind of confusing. What are the good practices, which one they should use and stuff like that. If someone start to use solidity, like for now, it started like a couple of months ago, he's going to look at transfer, he's going to look at some blog post, and he's going to see how transfer is safe from transy because he's going to read a blog post from like two years ago. While if they are decre, it's going to be a bit more kind of clear for him that you should not use them for the last solution.
02:57:40.360 - 02:58:36.764, Speaker A: When you say stop execution, is it something that you are going to do, like to run at compile time? Or is it something that's just going to stop the execution? Because one of my concern is that someone is going to have a visible call within an internal function and you might not expect. Yeah, I just realized that's bullshit. If you have a function that returns a value and you can't really stop execution. I was thinking about self destruct, but that's a totally different thing. But maybe we could have compile time checks that. So we could have the compiler check that there are no state changing things after the withdrawal. But yeah, as you mentioned, if there is like a one tone statement or something like that, it won't be possible.
02:58:36.764 - 02:59:14.090, Speaker A: It sounds a bit like dangerous. The second option is also definitely dangerous. You don't want this to pass all the guides because just people are not going to expect this. So what I gather from both of your feedback is that you wouldn't advise changing the semantics of send and transfer. Yeah, I would advise to remove them. Also. There are two more people wanting to speak, so let's give them.
02:59:14.090 - 02:59:52.770, Speaker A: I think Lucas was the first one and Richard was the second. Okay, so the general problem is the hard coded gas values in these functions. And with gas repricings, this can have a lot of effects. Even if you introduce a new function, what would the gas stipend be there? What's the point? Why not just remove all the hard coded gas values? That would pretty much boil down to call value.
02:59:54.020 - 02:59:54.770, Speaker E: Yeah.
02:59:55.640 - 03:00:40.260, Speaker A: The proposal would be to pass on all gas, but the EVM would also add the stipend. In some cases, we cannot control that. That's something what I still don't really get. I thought it's in the yellow paper, the stipend. But what is the EVM doing with the stipend? Why is this value in there? So the EVM, when you make a call and there's a value, then the call itself will cost more. And out of the extra gas it takes, the stipend is a gas passed to the receiver side. Okay, so that's an EVM feature.
03:00:40.260 - 03:01:14.990, Speaker A: But isn't this pretty much outdated because with the gas repricings, you cannot do what you used to do with this stipend. Yeah, but I guess it's outside of the scope here to change. Yes. Okay, I agree. Just one comment. What I can see in the wild is that people are just using call value for most of the stuff they used send and transfer for. And that's also in the eth security community.
03:01:14.990 - 03:02:33.752, Speaker A: The best practice is to just call value and make your code reentry safe by other means, because it's never a real reentry safety to have this gas restrictions. So what I would be interested in, do they make it reentrancy safe by using the withdrawal pattern so no state changes after the call value, or by using mutexes or other mechanisms that require state changes after the call. I mean, there are several mechanisms, like there is reentrancy guards, and in general, just the order of calculating stuff so that nothing happens in the function after your transfer. If you would enforce that on the compiler side for a specific new function, would that work for most people? I cannot speak for most people, but that sounds reasonable. Yes, I think we're kind of running out of time, but I would like to give time for Richard to say something because he raised his hand multiple times. Thanks. Sorry.
03:02:33.752 - 03:03:20.580, Speaker A: No, actually it's fine. For me, it was mainly asking more about the withdrawal pattern and how it would look like for you under the hood. I think I agree with what most people said. I would probably go for one and maybe add four as four will not be able to be used everywhere where send and transfer was used, but it probably would also enforce some good practice because it would throw errors if you do not follow this pattern, that you should not do state changes after you have done your withdrawal. Right? Like it would enable you to enforce a little bit more of this thinking about security. And I think even yesterday we saw that this is in general a good approach to get people to aware of things that they should not do and this would allow it. For me it was more interesting.
03:03:20.580 - 03:03:47.730, Speaker A: Also one of the cases why for me this is important is because proxy patterns become more apparent and proxies obviously use one s load always and then it becomes really tricky to emit an event if you already did an s load. Yeah, I think we ran out of time, but there's the link for the issue. I would really appreciate for all of you to participate in this and we can make a decision there soon, I hope.
03:03:49.620 - 03:03:55.970, Speaker B: Awesome, thank you. I also put the issue link in the GitHub chat. It's issue 7455, right?
03:03:58.040 - 03:03:58.692, Speaker A: Yes.
03:03:58.826 - 03:04:34.380, Speaker B: Okay, perfect. That was a pretty short discussion. There's also some discussion going on on GitHub at the same time, so maybe you can keep on discussing there or directly in the issue, which would be appreciated. Next up we have LSP, and Christian Papat, who is also a team member of the solidity team will give us intro into LSP in general and possible applications to solidity. And then we are also opening up the round after the introduction to some discussion.
03:04:38.260 - 03:04:40.464, Speaker A: Yes, hello, can you hear me?
03:04:40.662 - 03:04:41.552, Speaker B: Yes, we can.
03:04:41.606 - 03:04:45.330, Speaker A: Okay, perfect. Then let me switch to the thing.
03:04:46.820 - 03:04:57.030, Speaker B: How did you manage to get your camera on the left? That's impressive. Yeah, that's a secret crazy jitsi hack. Okay, I will ask.
03:04:57.480 - 03:04:59.716, Speaker A: It is not a jitsu hack. It's not?
03:04:59.818 - 03:05:00.084, Speaker B: Okay.
03:05:00.122 - 03:05:03.476, Speaker A: I was preparing something. It's a twitch hack.
03:05:03.508 - 03:05:06.824, Speaker B: Exactly. All right then. There you go.
03:05:06.942 - 03:06:06.744, Speaker A: Okay. Nevertheless, yeah, my name is Christian and I would like to at least just introduce a bit into the language server protocol and the potential idea of having a language server for solidity. So what is the language server? What is the LSP? This protocol basically is a standardized communication protocol between the client, which is the IDE usually, and the server, which is typically some kind of compiler inside. And it does provide some facilities not just for diagnostics but also code completion. I mean we all know it. And code refactoring, semantic highlighting is something I also really appreciate that you see, whatever symbols are also the same symbols as the one you are currently hoovering. Go to definition and find all references as well as code documentation and signature when you hoover a symbol.
03:06:06.744 - 03:06:39.028, Speaker A: This is also something that I really appreciate. Another thing. Yeah. I just wanted to give a very short overview of what an LSP message lifecycle looks like. So it's JSON RPC, and basically the client just sends a message over the JSON. It's a JSON protocol in form of JSON to the server and it is then going to be decoded into a LSP high level object. This is how I call it.
03:06:39.028 - 03:07:52.110, Speaker A: So I don't want to deal with native JSON objects, but with native c structs in my case, and it's then going to be handled. And optionally we're going to encode a response LSP object into JSON RPC and then just send it back. That sounds trivial. And that's also later. I can show you some live demo by the way, but I didn't decide to go for some JSON RPC library. Okay, so motivation, why do we actually want to have that? I mean, definitely real time compilation validation is really nice, and I believe Vmix is also having it, right? Real time code completion is something I would also like to have in my vim, but probably something not every editor is having. For solidity is assisted code factorization so that you can rename symbols and that it is going to rename wherever it is also mentioned quick code navigation such as go to definition and code introspection with just one click is actually something that I really appreciated now with the life of intellisense, also in Vim for example.
03:07:52.110 - 03:08:48.000, Speaker A: And at least from my point of view, that's my motivation to have it for solidity, to make some things easier. Weird time auto correction for common developer mistakes. That's a more sophisticated goal that I would think could also fit into a language server, since the language server can propose code changes. So usually only the editors of course editing the code, but the server itself can say look here, you typed if and what about NS? I mean, that's a trivial one, but just imagine the guy forgets to specify the visibility of the function he's currently writing. Then this can be proposed like auto fixing code. There's actually more about LSP and that's called LSIf. So usually the LSP works.
03:08:48.000 - 03:09:44.770, Speaker A: Let me go back one slide. The LSP works in a way that the LSP has to be running, your language server has to be running. But what if, for example, you have a web page such as GitHub or other tools? And lately GitHub for example introduced a way to also navigate through the code, and that is basically done with some kind of language server indexing format that I particularly find very interesting because maybe we could have something like this also for solidity inside of GitHub. And I also believe why this is the reason why Microsoft behind has been introducing this feature. But that's kind of future talk. So as I said, analysis tools as well as websites such as GitHub can benefit from this. And I don't see any other users here, but I guess only time will say this is kind of new still.
03:09:44.770 - 03:10:43.750, Speaker A: Okay, so where are we now with Wegard's facility? I would say we are in a research and development phase. So I was bootstrapping tiny bits already, and I call it sols like solidity language service. It's currently a branch inside of the solidity GitHub repo. And what can you do? I can show you if you want to in a second, but let me first go into the challenges I had while trying to implement something like this. First, our internal kernel API was our internal compiler API was a little bit not really made for LSP in mind, for tooling in mind and also error reporting. We report precise error numbers instead of ranges. These are things that I would like to refactor maybe in case we want to work on this one, like continue to work on this one.
03:10:43.750 - 03:12:06.400, Speaker A: So far so good. But I also experienced some, or we experienced some kind of open questions. That's the reason why we are now here. And that basically is like what clients do we actually want to support? Like Vim is what I'm actually using and for me it's working in VIm. But what about vs code? What about Vmix? Actually that's one thing I find interesting. Would Wemix be interested in using that? What kind of features do you think must have or are least important? Like not so important such as compiler diagnostics, that's a prerequisite, but cone completion code navigation, introspection, do you think they're all nice to have? But what would you feel like is most important to have in a initial release that we from then on also support? Or even more sophisticated features such as a real time gas cost information like when you hoover a symbol, or not a symbol, but some code fragment, it is going to tell you how much gas this one would cost. One type like while you are typing you could also get your code being reformatted so that you obey some, how do you call it, coding standards.
03:12:06.400 - 03:12:53.724, Speaker A: And since a compiler is inside anyway, we could also respond with the compilation. Beside if there's interest in this, that would be then fairly easy to add. And also one interesting question is like would we want to add a userboard? I believe Chris, you mentioned this one once and yeah, would we have any use case for also implementing LSIF, this index file format for example for remix or. Yeah, even for GitHub they could use it. Then that basically is our open questions or my open questions. That's a software set in the terminal. So yeah, that's basically what I think what the stake currently is.
03:12:53.724 - 03:13:15.620, Speaker A: And are you actually interested in a very small demo? Yes, of course. I prepared something for you. Okay, you see my screen now, right? You should, yes. Perfect. Okay, here's the code. So as you see here, the language server is. I already configured it in the.
03:13:15.620 - 03:13:55.020, Speaker A: Let me just show this one really quick. Nvim cox settings syntax off because it looks weird otherwise. So this is the solidity part here. And literally it just means you specify your subsection here and pass in the command and then you're actually done with it. I specified a little bit more here because I actually actively debug. But those two lines, which I'm currently highlighting, I believe you see the front right is what is really required. Everything else is nice for if you want to do something more, but that's not required.
03:13:55.020 - 03:14:19.284, Speaker A: So let's see our test story and let's open this one. I copy and pasted this one from, I believe, so many unit tests or so I needed one. We already see something. So it's not just fake. This morning it has been fake error messages like on Fixme and to do. But now we actually see weird error messages and warnings. So on the very first line we see this is pre release.
03:14:19.284 - 03:14:43.330, Speaker A: Let me jump to the next one. Now it's saying, hello, fix me. I hope you can read that. Fixed me should be fixed. This is an artificially generated message that I was actually adding there. That's a weird one because here, apparently I've been done something that does not belong here. I can fix this one really quick.
03:14:43.330 - 03:15:00.884, Speaker A: Now it's working. Okay, so let's have a look at the next one here. No visibility. I mean, in the end you get the point. It's full of syntax errors or like symmetric errors. And so far it is working since like 2 hours or so.
03:15:01.082 - 03:15:01.830, Speaker B: Nice.
03:15:02.760 - 03:15:17.470, Speaker A: The question is, do we want to continue? How much do we want to continue? What would you like to see in the first place? What are the priorities? What would you like or what would you not like to have? Maybe I didn't mention something and you can tell me then.
03:15:18.480 - 03:16:04.010, Speaker B: Awesome, thanks. Maybe you can go back to the slide with the questions. And in the meantime, does anybody in the room already have any thoughts or feedback on this? If yes, please raise your hand. Or maybe also put your comments in the chat. We're still discussing about gas alternatives for transfer here in the chat. So people are lagging a bit behind. Chris, I'm sure you have an opinion on the LSP stuff.
03:16:04.460 - 03:16:28.224, Speaker A: So just a question. You were just typing and the error was gone, and this did a full rerun of the compiler, right? Yes. Or maybe not a full rerun. Did you? Okay, yeah, you did have to do the part. It is a proof of concept. So it means basically that I'm basically using the compiler stick and try to make it running. It's not production ready.
03:16:28.224 - 03:16:45.460, Speaker A: I mean, a few things are hard coded, like EVM version and such, so these things still need to be configurable. But yeah, given enough time, yeah. I mean, one question is, how do we do all the compiler shopping?
03:16:47.900 - 03:16:54.920, Speaker B: Metal band of Las Vegas, Nevada. De bent Namibet of Denmatia Latvah.
03:16:57.260 - 03:16:59.710, Speaker A: That was interesting. And from a piece of information.
03:17:03.200 - 03:17:03.950, Speaker B: Okay.
03:17:06.320 - 03:17:40.000, Speaker A: What did you say, Chris? Yeah. I would like to see your slides. Yeah, they are blurry. They're blurry. Oh damn. Can we just have to wait a little? Maybe disappear? It's not this video. It's not the video from can you disappear video or not? Oh, wait a second, there it is.
03:17:42.530 - 03:17:57.970, Speaker B: I don't think this will influence it, but the open discussion questions are probably also in the live stream already. Or maybe just take a screenshot and post it into the GitHub chat of this slide.
03:18:00.890 - 03:18:57.618, Speaker A: I can click a share, right? So maybe one comment on the autocomplete. As much as I would love that it requires a recovering parser, so it requires the compiler to work with broken code. And we had an external contributor that did a push in that direction, but it resulted in, I don't know, a big inflow of bugs. And I'm not sure that we have a good cost benefit ratio here. Yeah, I noticed that also, and I thought I had it on the slides that this is one of the things that would need improvement. I think you can still do code completion. The only thing that is gone then is that every error below is not implicitly fixed because you don't see it anymore.
03:18:57.618 - 03:19:26.240, Speaker A: But you can't do confusion. I mean, you need type information, and type information is only done after parsing. Yeah, right. Yes. Yeah, good question. So is that something auditors would like? Are there, let me auditors around? So things like jump to definition. That is rather easy to do.
03:19:26.240 - 03:20:05.480, Speaker A: I'm still around. Yes, very much so, actually. Of course it does need some rework, but at least I think it is worth looking into code completion or into incomplete asts and see how much we can get out of there.
03:20:08.410 - 03:20:14.760, Speaker B: I put the questions also in the GitHub, in case it's still too blurry for you guys.
03:20:17.710 - 03:21:03.922, Speaker A: The renaming, that should also be not too difficult, right? That should definitely be possible. It does not require recovery. I mean, in this case renaming only works if the source code is actually functioning. So no source code errors. How much? So if you rename, would you get one call to the language server per character? No. So currently if you're editing, then for every character you type, I'm getting one method call from the client to the server. If you're pasting in like five lines, I'm getting one change request of those five lines.
03:21:03.922 - 03:21:35.056, Speaker A: But if someone is requesting a rename, I'm getting this rename request once and then I can send a proposal out and then the guy can actually accept. The client can accept. So it's a little bit more interactive. But it's only one operation. Basically it is one operation. So the IDE asks the compiler for the places that need to be changed. Yeah, exactly.
03:21:35.056 - 03:22:50.316, Speaker A: And then you send a new name. Yeah. Can you say a bit more about this lsIf? Yeah, basically one just needs to ask how GitHub works with go to definition, which is also possible for typescript. What basically is happening is that the language server is capable of dumping all of its knowledge into a database file in LSIF syntax, which is, if I remember correctly, also JSon. So it's just the dumped knowledge of the language server, like literally going through all those simple ranges and dumping the knowledge about it like go to definition and signature information and such. Would that help? I mean, is that something that is useful in general for debuggers or whatever, or auditors? Again, actually, I believe debug go to definition feature. And would it be auto activated once we have that, or we probably still have to apply somewhere.
03:22:50.316 - 03:24:07.088, Speaker A: Right. Is that known how this process works, to add the feature for a new language to GitHub? No, I don't know this process. I would love to investigate, but I only know about LSIF since yesterday evening, so I'm still in my own research phase in finding out how all works. Just curiosity, what is the memory footprint of keeping the language server running right now? Well, I'm only having a debug version here, right? Yes, it's the same as running the compiler once, right? Yeah. Okay. It 41 megabytes, it's very manageable. But it's only one text file in there, right? Currently, so yeah.
03:24:07.088 - 03:25:05.950, Speaker A: What do you think about these more sophisticated feature ideas, such as giving more information such as gas cost, because we have discussed gas estimation functionality. Anyway, I think it might be interesting for people who want to optimize for gas to know about how much gas a fixed version is spending. The problem with gas is that many of the interesting parts there. Yeah, gas just very much depends on the state and the actual inputs. Yeah. That's why it's only an estimate. What about Vmix? Is there actually any possible way to integrate such a potential tool into Vmix via webassembly, for example? Yeah, that's another question.
03:25:05.950 - 03:25:37.188, Speaker A: How can we make that work without JSOn RPC via TCP? Oh, that's fairly easy. You can make an HTP API on top of it. No, but without TCP. Without TCP. Okay, so how do you want to. Because we can't run a TCP session inside the browser. Right? Yeah.
03:25:37.188 - 03:26:22.740, Speaker A: But we can move JSON RPC payload back and forth via function calls. Yeah. Then this one. Do you know something about that? So what Vim is currently doing, or the LSP plugin for VIm, it is using STD in and STD out as communication pipeline. And in the end, when I just talk about Vmix here, or these kind of web based applications, what I also believe is possible is that in webassembly you can export functions, and I can just export functions for accepting new messages.
03:26:24.040 - 03:26:31.080, Speaker B: Okay. Liana from remix is just posting in the chat. We could run it in plugin in an iframe.
03:26:35.660 - 03:26:37.480, Speaker A: What was the first word? Plug.
03:26:38.060 - 03:26:41.160, Speaker B: We could run it in plugin in an iframe.
03:26:41.500 - 03:27:32.672, Speaker A: Yeah, that's the default. Oh yeah, it could be a plugin. Yeah, that has been the idea. My motivation was also to integrate it potentially into remix. Then at least it would have one strong user. So what do you think? Is it more effort on the ide side or more effort on the compiler side? You mean for the language server? Yeah. I think from my experience in learning all LSP stuff, as of right now, I think the most effort we would spend in potentially making sure that code completion works because of error recovery.
03:27:32.672 - 03:27:35.310, Speaker A: So refactoring our own code base.
03:27:39.240 - 03:27:52.120, Speaker B: A few more comments coming in over GitHub. Mr. Chico says could be integrated into solidity. Probably. Carmel says maybe it could communicate over websockets.
03:27:53.020 - 03:28:13.136, Speaker A: Well, websockets for sure. That's definitely possible. Emac solidity, I don't know. But Emacs definitely. Emac solidity is probably the Emacs plugin, right? For solidity. Yeah, that is definitely possible. No need for integration there, right? Yeah.
03:28:13.136 - 03:29:03.868, Speaker A: Since it's a language server or client LSp can make use of it. Yeah, I mean websockets and stuff. I really think we should, I mean if it's not standardized, just have our own thing and just pass JSOn RPC payloader around. It functions and then yeah, remix can use it like that. Yeah. So there's no mode where you have a, I don't know, one shot thing, right. You always start it up, send some messages back and forth and then you started and you're getting terminated whenever the editor tells you to terminate.
03:29:03.868 - 03:29:08.730, Speaker A: Yeah, and it's a long running process basically.
03:29:25.490 - 03:29:35.642, Speaker B: Then I would say we can wrap the session up now. If you have any closing thoughts please tell them now.
03:29:35.796 - 03:30:28.820, Speaker A: One closing thought, or at least one thought I'm having because Patricio has been just writing code completion is especially due to the complexity with vigilsover current API I would say this feature could be know later implemented and everything else can be done before. Like there are clearly low hanging fruits. Once the groundwork is done then let's just do the low hanging fruits such as compiler diagnostics, code navigation, introspection and symbolic weaning and especially hoover and then we can. This hover thing is also something that's probably very easy to do, right. We already have our doc string parser. We could print summaries of functions when they are called, including nutspec documentation. So signature plus nutspec, that's my idea here.
03:30:28.820 - 03:31:25.500, Speaker A: So the only stopper for me currently is code completion, which I still would really love to have. I feel like it's not a language server if you don't provide code completion. But this due to the complexity do it as the last big milestone, not in the first release. So there are other people who have requested some more. So currently the compiler usually either the compiler is built for code, that is correct. And it doesn't care how much broken it is. But I think we could try to implement some more checkpoints in the computation pipeline where we can exit early and still provide some useful feature, some useful information.
03:31:25.500 - 03:32:12.362, Speaker A: Sorry. It has one really interesting technique, how C sharp compiler is doing that. In case anybody knows they are parsing only the ast basically and doing everything else on demand. Like when you're doing code innovation then of course on demand you request everything. So their idea is to only resolve type information whenever you need it. And they did this kind of design especially because they didn't want to code duplicate and create a stable intellisense engine like an LSP for C sharp. Maybe that one would be possible for us too.
03:32:12.362 - 03:32:26.960, Speaker A: I don't know yet. Yeah. Is it easy to determine when you need type information when you don't? I mean, what we already do is we only generate code if code generation is requested. Yeah, but then we generate along the way.
03:32:32.370 - 03:33:25.890, Speaker B: Okay, so looking at the time, we unfortunately need to wrap this up now and here. Thanks so much for doing the research and also doing the demo and already putting so much time into it. Christian Daniel has linked in the GitHub chat the issue. So please, all of the people that just commented on GitHub or posted their needs, make sure to visit the GitHub issue 7763, which is the LSP issue where you can voice whatever you need so that we can get this tackled. Next up we have another open discussion session. This one is on in language testing syntax with a lightning talk from remix tests, and Alex will be moderating this one. So I hand over to Alex without further ado.
03:33:46.520 - 03:33:51.572, Speaker A: All right, I guess everybody can see the slides in me, or at least hear me.
03:33:51.706 - 03:33:53.110, Speaker B: Yes, all good.
03:33:53.720 - 03:34:58.810, Speaker A: Okay, so this session I don't think we can reach any kind of decisions on this session, but rather we would like to maybe open up discussion on a new functionality. And the new functionality would be some kind of a testing syntax, maybe supported by the compiler, but at least defined in the language. So here's a teaser what this could look like, and there's an explanation later on, much later on. I just wanted to put it out. So this would be a testing syntax resembling solidity and yeah, let's see what's out there today. I have also in this issue link here. I have also explained, I guess the same topic in a much more lengthy manner, and I would invite everyone to keep the discussion there.
03:34:58.810 - 03:36:07.036, Speaker A: It was only created last week, so it doesn't really have any conversation yet. Okay, now different languages take different approaches on how they do unit testing. So there are some languages like C sharp, Rust and Python who have like a unit testing syntax either in the language or as part of the compiler and the compiler distribution, while other languages decided not to include anything. So one big example is Javascript or typescript. There is no built in unit testing for it, but there are a lot of different libraries and solutions to accomplish unit testing, and not just unit testing, but just testing in general. I did like a short look at the different frameworks and what exists out there for solidity, and I found that most of them do support some kind of test written in solidity itself. But obviously these tests don't use any special syntax, they're still just like regular solidity contracts.
03:36:07.036 - 03:36:54.520, Speaker A: So out of these frameworks, truffle, remix and DS test, all of them have tests written in solidity, but unfortunately they're not compatible with each other. So how do actually these tests work? So you write them as contracts. They have a library for assertions. So you would use assert true or equals. And behind the scenes, what this assertion is doing, or this function is doing. It emits special events. And then the unit testing tool, the runner, can execute these contracts and listen for these events and display the results.
03:36:54.520 - 03:37:13.204, Speaker A: So these events would also have strengths attached to them in some cases, and those would be the messages in case of a certain failures. Maybe this would be a good time if Anikad is already connected. I hope he is.
03:37:13.322 - 03:37:24.410, Speaker B: Yep, he just joined. He is having some technical difficulties to hear anyone. Let me quickly troubleshoot with him. 1 second.
03:37:26.700 - 03:37:58.630, Speaker A: It's just one. Yeah, I don't want to go to other slides now, but maybe while you're waiting. So what Aniket would be doing here is just showing the syntax and remix. But maybe if anybody has any opinion in general, whether what this would be a bad idea or a good idea, maybe you could speak up now.
03:38:21.810 - 03:38:31.060, Speaker B: Okay. Two, three. People would like to say something. Are you handling this, Alex, while I troubleshoot or can't you see the screen?
03:38:32.150 - 03:38:38.134, Speaker A: Okay, I can now, but yeah, I cannot unmute anyone or anything like.
03:38:38.172 - 03:38:42.760, Speaker B: No, no. But Patricio, Mr. Chico and Jocelyn are raising their hands.
03:38:43.150 - 03:39:50.622, Speaker A: Yeah, whoever was the first, maybe Patricio, let's go with you first. Okay, well, first of all, I want to say that I do like the proposal, but I don't think that this would be enough for testing real life, smart context. Because most of the time, most of the tests also need to control things like the time where when a transaction is mined. So you would have to extend solidity to add functionality to control the blockchain where the tests are going to be run. And I don't know if that's the intention. Yes, that's part of the proposal. But I guess that was one of the questions, whether we should restrict ourselves to unit tests or have some kind of an interaction with the blockchain state as well.
03:39:50.622 - 03:40:14.726, Speaker A: But the later slides do have some examples how that could be accomplished. Okay, great. So maybe Mr. Chico next. The problem with controlling the blockchain state is that if we do that, it is not enough to. I mean, we have to implement an interpreter or whatever, a runtime in addition to that. Right.
03:40:14.726 - 03:41:34.914, Speaker A: Or make some assumptions about how to control it. Yeah, I mean, my idea was that we could just use APM one because we already use it for testing. Yeah, I read the proposal on this and there are things that I like about it, but I'm also skeptical that the compiler should run these tests, not only because there are things that you often want to mock, like the timestamp, but often you can also, using the same tests, just run them against, for example, the state of the mainnet, as is the case for DS tests at least, or debug and execution. So step through it with backstepping and so on. And I think it would grow to a very ambitious project if the compiler tries to accommodate for all of those things that individual testing frameworks might do. But I think there is something that can be done in terms of just making sure that maybe the common commonalities of the different frameworks are sort of unified, maybe with some function modifiers, that this is the testing function or something like this. Yeah.
03:41:34.914 - 03:42:46.114, Speaker A: So that's one of the potential outcomes I think discussed on the issue as well, that even if it's not implemented in a compiler, we could consider having either just a recommendation of what these search and libraries should be, so that tests could be exchanged between different frameworks, or we could consider introducing some syntax for them as well. Maybe. Jocelyn, if you wanted to say something as well. Can you hear me? Yeah, my comments were mostly the same as the previous one. I do like the idea. I have some concern about how far you can go if you do everything in solidity. If you want to give some ethereum to an account, or if you want to increase the time or stuff like that, you might have some difficulty with the syntax, maybe as limited unit test framework, just to test some specific functionality.
03:42:46.114 - 03:43:07.598, Speaker A: It might make sense, but I'm expecting that you are going to have some limitation or this is going to require a huge amount of work. Yeah, I think that's what we're here to discuss. Maybe Francie, is Aniket joining or should we move on?
03:43:07.764 - 03:43:41.340, Speaker B: I think somebody just joined. Maybe this is Aniket. If you can hear us, please shout now. He was just trying with a different browser. No, does not seem like it. Then I just recommend you skip over that part for now.
03:43:41.950 - 03:44:48.000, Speaker A: Yeah, so if there's any time left at the end, we can move it mean whenever Anika is online. Okay, so as we discussed briefly, I only found two main reasons why it would be really beneficial to maybe have it in a compiler or have a specific notation for it. And one of the benefits is close coupling of tests together with these sources. Which I find really useful in languages with supported such as rust. But in all these solidity frameworks, the code and the tests were quite separated, and maybe there are different kinds of tests. So for unit tests, it makes a lot of sense to have it closely coupled with the functions, at least on the same file or in the same namespace or module. And maybe for more complex integration tests, maybe it doesn't make that much sense to have it there.
03:44:48.000 - 03:45:44.016, Speaker A: But I did found like closed coupling useful, at least to me. And the second, which might be a more interesting feature, is that access to internal and private members might be problematic. Today, I think I've seen that in some cases people would export those functions to be able to make tests for them. So maybe this would be a good place to get some feedback from you guys, whether I guess, closed coupling is useful or not. And access to internal and private members, what's the story with that today? So I expect Patricio, Mr. Chico and Jocelyn to give feedback.
03:45:44.148 - 03:45:46.940, Speaker B: Yeah, Martin already commented on the chat.
03:45:48.800 - 03:46:44.990, Speaker A: So about the private members, I think that could and maybe should be handled by external tooling. Some time ago, Francisco from Openseepling suggested to create a pre processor that would expose private and protected members by creating a new contract, and I was working on that. The reason I didn't is that has lots of edge cases, and it wasn't a big priority for me, but I think that could be solvable outside of the compiler. The main benefit I see from having standard testing syntax or functions is portability updated. I think that's a huge one.
03:46:50.020 - 03:46:55.760, Speaker B: Okay, just to let you know, Alex Anikat is now also there, so whenever fits.
03:46:59.940 - 03:48:24.380, Speaker A: Right, maybe. Mr. Chica, do you have any feedback on any of these? Yeah, I mean, I can also repeat the point that I made in the gitter, which is that even though we might unify tests, I think there will still be a lot of other aspects of these frameworks that will remain incompatible. So you mentioned rust, an example of built in tests to the language, and I think they also seem to have done a good job of building in dependency management and really the whole dependent development pipeline pretty closely together with a compiler. And I think that's another reason why these frameworks have developed different ways of doing tests, because they've also had to solve the problem of dependency management with respect to the internal private members. Yeah, this is a little bit of an issue, but not a very. I mean, all you often really need to do is to make functions public while testing, and then make them into private again once you want to deploy.
03:48:24.380 - 03:49:04.060, Speaker A: Yeah Jocelyn, you wanted to also say something? All right then I suggest maybe Anika, you can do your demo and then we can come back to some of these proposed features. Okay, thanks Alex.
03:49:09.780 - 03:49:54.540, Speaker E: So as we are talking here about the in language testing, so I will first introduce myself. So this is Aniket from remix team. And regarding the in language testing, basically remix provide a module for this purpose that is named remix tests. So with remix tests you can use the solidity to write the unit tests for the solidity contact itself. So basically remix tests is a core of remix id solidity unit testing plugin. So I will quickly demonstrate this plugin and so that we can understand this.
03:49:54.610 - 03:49:55.500, Speaker A: It's working.
03:49:55.650 - 03:49:58.510, Speaker E: So I'm sharing my screen here.
03:50:01.360 - 03:50:07.188, Speaker A: I think I am visible now. Fancy.
03:50:07.224 - 03:50:09.840, Speaker E: Can you confirm that my screen is shared?
03:50:10.360 - 03:50:11.510, Speaker B: Yes it is.
03:50:13.000 - 03:50:54.720, Speaker E: So this solidity unit testing plugin comes with the solidity environment. So you can just quickly select this environment and get this double checkbox here in the side panel. So this is the solidity unit testing plugin. So as it says you can test your smart contract in solidity. And to get started you should just select a file, a solidity file from the file explorer and click on the generate to generate a test file. So let's quickly divide. So I am selecting a very basic storage solve file.
03:50:54.720 - 03:51:31.288, Speaker E: So now you will see that this generate button is now enabled. So it also says generate sample test file. Now if I will click on this, so a new test file will be generated in the same folder where the file to be tested is. And I will quickly talk about this file. So you will see that we are importing this remix underscore tests solve. And here in the comments it is mentioned very well. I would like to zoom that this import is automatically injected by remix.
03:51:31.288 - 03:52:37.216, Speaker E: So this is the library which is responsible to perform the assert actions which are used below in this file. So with remix test, your test file should have this suffix that underscore test solve using which we detect that this is a test file. And also yes, like an usual solidity file, this file can also contain more than one test suit. So like a testing framework, remix tests also provide some special functions that are before each, before all, after each and after all. So as their names are before each, any code inside before each will be executed before running each test, and any code inside before all function will be running before all the tests and similar for the after each and after all. So there are some initial tests here. So we use this assert library here.
03:52:37.216 - 03:53:32.224, Speaker E: And regarding this assert library, if you want to see that what all things are available with this library? You can go to the documentation here and there is one dedicated section for this assert library. So we have these methods available currently and then here are some methods to run the test. So there are very basic words, so we are just comparing the integers with them and using more methods. And there is another way if you are not looking to use any assert library. So you can also use this returns property in which you can use the return value to test the contract. And then at the end there is one failure test. So if I will quickly run this file.
03:53:32.224 - 03:53:59.550, Speaker E: So here there will be all a test file loaded here you can select a file here to run the test and I will quickly run it here. So this progress bar shows the progress of the file tests and here you will see the results of it. For a passing test it will be in the green, and for the failing ones it will be in red and also will be showing the regions of failure which you will mention here.
03:54:01.280 - 03:54:01.596, Speaker A: In.
03:54:01.618 - 03:54:38.984, Speaker E: This line in this equal test. And this test overall will show a fail if at least one contact test for this file is failed. Here you can see the total time that was consumed to run these tests. So I would like to also explain this before each and before all functions functionality. So this can be understood using this existing ballot test file. So we have this ballot contract. I think people using the solidity should be very familiar to it.
03:54:38.984 - 03:55:45.944, Speaker E: So regarding that we have tests in this file. So you can use this before all to create an instance of ballot contract before running any test. So you can just create an array of names and you can instantiate that ballot contract here and then you can further use that instance to run multiple tests on the methods. Okay, so I will quickly also like to summarize that if you want to provide the compiler configuration for running this test file. So we use the solidity compiler plugin for that. So while compiling this test file, we will pick the compiler version from this tab and this evm version from here. And also you can enable or disable optimization here for the long contract and for the custom transaction context.
03:55:45.944 - 03:56:45.872, Speaker E: If we talk about that, we can configure the transaction for custom message sender or message value. So we basically provide that inbuilt functionality here. I would like to quickly run a test, so we use solidity nat spec for that. So there is a fixed syntax for that. So you can just mention this sender with a hash and a colon and a space like this just before a method in which you want to set the sender. So to use this account functionality you need to import one more file that is remixaccounts solve. So this library provides the accounts to set as a sender and there are currently ten accounts available so they move from zero to nine.
03:56:45.872 - 03:56:50.770, Speaker E: So you can provide the index zero to nine here. And if I can quickly.
03:56:52.600 - 03:56:53.910, Speaker A: Check it here.
03:56:57.000 - 03:57:12.250, Speaker E: We can directly check the message of sender here and the method to access these accounts is this so we can use here.
03:57:14.140 - 03:57:31.250, Speaker A: Invalid sender check system config okay so I can remove these things from you.
03:57:32.100 - 03:57:35.490, Speaker E: And now if we will run this contract again.
03:57:38.980 - 03:57:41.780, Speaker A: Okay, semicolon is missing.
03:57:45.400 - 03:58:22.320, Speaker E: So this will be passing. This is our check custom config. So this ensures that the message sender which is getting passed inside this method is the account nine. So this can be used in many contexts. Another custom configuration which remix tests provide is for the value which is very useful in the payable methods. So you can define a value using the similar syntax here, but this is a value in v which will be passed along with this method execution.
03:58:22.660 - 03:58:27.010, Speaker A: So I can just quickly write one more test here.
03:58:35.320 - 03:58:41.764, Speaker B: Anika, just looking at timing it would be good if we could wrap it up soon because we have other pipeline.
03:58:41.812 - 03:59:02.370, Speaker E: Yeah we are done in just 1 minute. So this is another way of massive value so we can provide the custom transaction configuration like that. And remix test can also be used as a CLI and CI module. So that is well explained in the documentation part so we can continue the discussion now.
03:59:03.460 - 03:59:15.670, Speaker B: Okay great, I saw that already. Some of you were discussing meanwhile on the GitHub chat. Alex, do you want to continue or did you finish your styles already?
03:59:18.760 - 03:59:24.070, Speaker A: No, I haven't finished them, but we did discuss some of it. Is it visible now?
03:59:25.800 - 03:59:26.752, Speaker B: Yes it is.
03:59:26.826 - 04:00:10.070, Speaker A: Okay, we discussed some of it and I shared the link. So these were just some of the ideas. I don't want to go through them one by one. I guess we did mention them like the state interface could be used to change the change state, but rather I would like to go through this example because it accomplishes the same. So this example here has its own syntax for a few things. So one, it doesn't start with a contract, it starts with a test. And two, it doesn't have functions here, but rather it has something called a test which has the case name.
04:00:10.070 - 04:01:09.050, Speaker A: And in this proposal functions are still possible and functions wouldn't be executed as tests, but you could use the functions within the actual test cases. The second thing, which is not a change to anything, the modifier could be used here. So this would be a way to accomplish like before each, before all, et cetera. And then lastly, this more interesting example, because questions came up regarding how to change the change state. So it would be potentially possible within the current language syntax to have like a sender for transfer. Obviously bear with me that transfer we discussed separately previously. But anyway, the important part here is that we could have like a sender to change the sender itself, and we could also make use of the try catch functionality we have.
04:01:09.050 - 04:02:26.370, Speaker A: And lastly, this built in expect object would be the assertion library. Now, I guess we run out of time, and I don't really expect that we can reach any decisions here, but based on the gitter chat, it seemed like that people were interested to maybe reconcile the testing libraries so that tests could be moved around between the different frameworks. And I guess the question there is an easy way to do that is just have like a single assertion library, which can be accomplished in two different ways. One, we as a community define an interface for the testing library, and then each framework can implement the internals and what kind of events they use, but from the outside assert that is equal, et cetera. All these would be the same across all the tools. I think this would be already quite a good move, but we could also consider further integration, maybe having some kind of a syntactical sugar as in this example, or maybe some kind of comments annotations. I think there's a lot to discuss here, and I'm looking forward to discuss this in the future.
04:02:26.370 - 04:02:36.550, Speaker A: Mostly on I don't have the link there. Where is it? Yeah, so I'd really invite all of you guys to discuss it on the issue.
04:02:37.800 - 04:03:23.590, Speaker B: Yeah, thanks so much. The issue link is also already in the GitHub chat, so feel free to comment there and to take the discussion forward over there. Unfortunately, we again have not too much time on air here to discuss this further, but there were already quite some good comments in the GitHub chat. And yeah, if you have any thoughts on that, please feel always free to put them directly into the issues that would for our team, this is the easiest way how to deal with all of the incoming thoughts and discussions. All right, next up we have another talk, this time on mutation testing with the tool vertigo by Johan, who I can also see is there already.
04:03:28.560 - 04:03:34.748, Speaker A: Yes, thank you. You guys can see the presentation looks perfect.
04:03:34.834 - 04:03:35.480, Speaker B: Yes.
04:03:35.650 - 04:04:54.940, Speaker A: Awesome. Thanks. All right, so I'm also going to be talking about testing, but specifically mutation testing, which is kind of a meta subject. So a little bit of background. I wrote a tool called Vertigo which implements mutation testing for smart contracts, which I released about a year ago. And in this presentation, I'll first give a little bit of background on mutation testing, a crash course of sorts, and then I'd like to talk about the features that are currently in the mutation testing tool and the features that I would like to add, and how other parts of the technology stack can enable these features, because the mutation testing tools kind of build on top of other tools like the solidity compiler. When we talk about mutation testing, you almost always first have to talk about code coverage, and code coverage is this ubiquitous method which is used to evaluate test feeds.
04:04:54.940 - 04:05:54.904, Speaker A: At the moment, I think almost all people will use this metric to see whether their test suite is adequate. But there's a few issues with code coverage. Specifically, it doesn't really tell you anything about the code that you do cover, but it does tell you something, namely the code that you don't cover yet. So it is actually a very useful metric because it helps you improve. It tells you where to change your tests, to have a better test feed, to have better guarantees. Mutation testing, on the other hand, tries to improve on this metric by telling you exactly how efficient your test suite is at detecting bugs, rather than just telling you which part of the code are covered. Which is great because it allows you to improve your test suite even more efficiently.
04:05:54.904 - 04:07:07.760, Speaker A: It also gives you a nice measure of the guarantees given by your test suite. So kind of to give an overview of mutation testing, this is the general strategy. We start out by creating a lot of bugs which could be introduced in a solidity project. Then we run the test feed for each bug and see if the bug is detected, and then we know something about which were these bugs that didn't get detected lid and we can improve the test week. So generating these bugs, how does this happen? So, in mutation testing, we call these bugs mutants because they are mutations of the original program. And we use so called mutation operators to take the original project and modify it to get this bug in the program. And mutation operators are basically rules.
04:07:07.760 - 04:08:43.970, Speaker A: They can be substitution rules which describe how to inject bugs, basically. And for the tool vertical, what it is is I look at existing tools for the mutation operators used there also in previous research, and I implemented some that are somewhat specific to the weaknesses that happen in solidity smart contracts. To give you an example of what mutation operators do is, I have made a little table. So for example, we swap the addition sign with a subtraction. Another one which I really like is the modifier removal mutation operator, where we take a look at the source code, and to generate a bug, we remove a modifier. And what I like about this is that it kind of emulates a case where a developer forgets to introduce some authorization and authentication logic, which generally is implemented using modifiers such as the only owner modifier. After having generated a bunch of bugs, we get to a part where we need to evaluate them to know whether the test detects them or not, which is the next step.
04:08:43.970 - 04:10:04.010, Speaker A: So kind of the two basic things that can happen are the test suite succeeds or it fails. So if the test suite succeeds, that means that we were not actually able to find a bug or the introduced bug, in which case the mutant survived and we call the mutant alive. The other cases where the test suite fails, in which case the mutant is detected and we say it's killed or dead, then there's two additional categories to deal with some edge cases. So, for example, what could happen is a mutation operator modifies a piece of code that results in interlip solidity, in which case the compiler will not successfully compile and we cannot even run the test suite. So in this case, we say the mutant is error. The fourth case is timed out, which we have included, because mutants can also introduce infinite loops. So what we do, we take a timeout based on the original time that a test suite took.
04:10:04.010 - 04:11:22.310, Speaker A: And after this timeout has expired, we kill the execution of the test suite and we categorize the mutant as timed out. So then there's this fifth category, which I put in brackets, because it's not really a category on its own, but rather a specific instance of a live mutant. So what mutation operators also can do is modify code so that there is a syntactic change while not changing the underlying meaning of the code. So to give you an example, I have the implementation of the max function by Zeppelin. And as you can see, there's a slight difference here in the comparison operator, however. So the difference lies at a point where a and b are equal. But for the evaluation of the max function, it doesn't really matter which branch you take when a and b are equal, because both will be the max number.
04:11:22.310 - 04:12:43.976, Speaker A: So this is an equivalent mutant, and we don't really want to account for this as an alive mutant because the test feed was not insufficient, it was doing okay, because this is actually also correct. So unfortunately, this process is somewhat manual, and I implemented some feature to automate this a little bit, but it's not possible to automate that in general. So there will always be some categorization manual categorization required when you're performing mutation testing. Once we have categorized all the mutants, we can compute the mutation score, which is kind of similar to the code coverage metric. But in this case, we're talking about the efficiency of the second bugs of the test suite. So the mutation score is computed using this formula, and it's basically the rate at which you detect mutants. So you take total number of mutants that you killed or that were detected and divided by the total number of valid mutants.
04:12:43.976 - 04:13:56.816, Speaker A: So these are the non equivalent alive mutants and the killed mutants. We disregard the other ones for the mutation score. So this gives you a general metric of the quality of your test suite, and then the specific surviving mutations tell you something, give you really detailed information about which part of the code you could improve, or you could improve the test speed for rather. Okay, so that was kind of a bird's eye view of mutation testing theory. Right now, I'd like to go over a few of the features that are in Vertigo, the tool right now. Then what's kind of on my wish list, followed by how kind of the foundational technologies, such as the compiler or test frameworks, can enable or support the development of these features. So, first, what do we have today? There's a few features listed here which I think are kind of the main ones, and the first one is parallel evaluation, which is pretty straightforward.
04:13:56.816 - 04:15:10.940, Speaker A: Instead of running the test reads sequentially for each mutant, we run them in parallel. Most of the times, there's going to be a lot of tests, so we won't run all of them in parallel, but we'll be able to use whatever computational resources your machine might have. The second optimization is mutant sampling, where, instead of taking the entire set of mutants we've generated in the first phase, we take a random sample of those. This will give you an estimation of what the original mutation score would have been while drastically reducing the time you need to compute it. And there's a balance here because there's, of course, inaccuracy because you're taking a random sample. The third really nice feature, what I think is a really nice feature, is the support of universal mutator style rules. So, this is a project for general mutation testing, where the authors have designed a method of formulating mutation rules using brackex patterns.
04:15:10.940 - 04:16:12.864, Speaker A: And this will allow you to easily develop and demo different mutation rules. But what it will also do is enable you to write mutation rules specific to a certain project. For example, you might be able to write mutation rules for a specific safe math library. And here I have a little example from the universal mutator repository itself, which describes some mutation rules for solidity, specifically the time keywords. So this should give you some idea of what this is capable of. So lastly, we have compiler equivalence, which is enabled by the solidity compiler, and this has to do with these equivalent mutations. So what verticode does is it takes the original program and the mutated program, and it will compile both.
04:16:12.864 - 04:16:58.130, Speaker A: It will then compare the generated bytecodes, and if they are equal, conclude that therefore also the source code must have been equal in meaning, and it will disregard the mutants entirely. So it won't even start the testing process. This assumes, of course, that the compiler is correct, and let's hope it is. Okay, so what's next? There's kind of two categories. First is optimizations. The other is usability. There's no time to really do a demo, but these would really help.
04:16:58.130 - 04:18:05.136, Speaker A: But first, optimization first is incremental evaluation. And this would be really nice to have, because in a regular scenario, you would be running mutation testing every once in a while, maybe, and between those runs you could reuse a lot of information. So, for example, what you can do is look at the previous analysis results and look at which test killed which mutant. And if you see the same mutant again, try to find that test and run specifically that test first, instead of running the entire test suite. That can save a lot of time. So maybe 80% of the mutants will be killed by the same test, and that will save a ton and execution time, because you won't have to execute the entire test suite every time. Then there's mutant clustering.
04:18:05.136 - 04:19:14.364, Speaker A: And mutant clustering is kind of similar to mutant sampling, but instead of taking a sample from the entire set of mutants, we group them first and take samples from the groups, and this gives a more accurate estimation of the mutation score. The third optimization, which I really would like to have, is test selection based on code coverage. So what we do is given detailed information on the coverage. Specifically, you'd want to know which tests cover which lines of code you'd select, just those tests that cover a mutant to evaluate, rather than the entire test suite. So assume, for example, that each mutant will only be covered by 10% of the test suite. Then this optimization will give you a performance improvement of ten times, which is huge. It takes like, well, it's used anyway.
04:19:14.364 - 04:20:20.930, Speaker A: So then there's a usability aspect. So I didn't get to show this, but if you set up vertigo to run parallel evaluation, then you have to instantiate a bunch of Ganesh networks. This setup process shouldn't really be necessary, I think. And I think removing that will make setting up mutation testing a lot easier, because you won't have to set up like for example, twelve development networks. The other part of this is that I believe Ganesh is not made to run a test feed like 500 times. What happened on my machine is that it would create like a few million files which would use up the inodes on my machine, and it would break almost everything. So with Ganesh network creation, you could clean out after a test suite run and prevent this problem from happening.
04:20:20.930 - 04:21:53.500, Speaker A: And then we also have framework expansion, which basically means that I would extend further go to work with other frameworks which also are commonly used, because currently we only really support truffle. So then there's kind of three areas of foundational technology which support the development of mutation testing framework in general, but specifically vertical, and that's the compiler test framework. And then I made a little category of others. So first the compiler and the solidity compiler actually already does a bunch of things right for mutation testing. So first, where vertigo really uses the solidity compiler is in the ast it generates. So instead of doing manual parsing and analysis of the source code, we kind of use the solidity generated AST to find specific the locations that we want to mutate. Based on the information in the aST, we determine how we should modify the original file to get to the mutant, since a recent version of solidity, it's also, I think, 6.20
04:21:53.500 - 04:23:13.880, Speaker A: 62. It's also possible to recompile from a modified Dsp, so that would make the process even easier. But on the other hand, it would also likely require some more tight coupling between the mutation testing tool and a test framework, because you need to be part of the compilation process. The second part, where the compiler really helps out, is in the compiler equivalents feature, which I mentioned previously, so I won't really go into it. Then we have the test framework, which kind of enables almost everything of the mutation testing process, because it's our interface to the project. So the first part is the interaction with the unit test, which is not optimal at the moment for mutation testing, because it's not, as far as I know, not easy at least to directly execute single tests, or maybe a list of tests which the mutation testing framework would want to do. So, for example, the test selection optimization and the incremental mutation testing optimization would require this functionality.
04:23:13.880 - 04:24:40.870, Speaker A: I do think it's possible to sync out specific files to run, so that's already some improvement over the generic run the entire test suite, but I think a more detailed interface would be super beneficial for mutation testing tools. Then we have the other part, which is automation of test network creation. And I think this is handled by some of the test frameworks or IDE frameworks, but to a limited extent, because I guess it's not a common use case to run your test suite in parallel five times. So that's probably why it hasn't been implemented yet, to have multiple test networks being created and cleaned up all the time. But this is also a feature that could be handled outside of the test network. And lastly, there's this item of test evaluation speed, where really any improvement to test evaluation speed. So this could be like a compiler optimization, a VM optimization, or some other optimization also reflects in the performance of a mutation testing tool.
04:24:40.870 - 04:25:52.650, Speaker A: And then the last category, miscellaneous. I put detailed code coverage here because code coverage is currently handled by, I think by a separate tool called solidity coverage. And it's on the issue tracker for this project, but I'm not sure when it's going to be implemented. But this is one thing that would enable the test case selection optimization and I believe some other features outside of mutation testing. Okay, so that was kind of the overview of mutation testing theory, kind of what the tool does right now, and what could be improved and how other people in other parts of the tool stack might help with the implementation of these optimizations. And I think there's some time for questions. I would also love to hear any suggestions, maybe improvements on usability or other questions.
04:25:54.140 - 04:27:10.200, Speaker B: Yes, thank you for your talk. As Johan already outlined, we still have five minutes left for questions, so feel free to raise your hand if you have a question and you are attending here in the chat room, in the video conference. If you are watching the live stream, please put your question in the GitHub chat and we will be patient and wait a bit because we know there's a delay. Any feedback here from the roommate does not seem like it so far, but let's give it a few more minutes to also give the people on the live stream the chance to react to this. In the meantime, just checking in, Leo and Martin, you're both there already, right? Yes, I can see you. And Nicholas wants to say something. Yes, go ahead.
04:27:11.290 - 04:28:28.980, Speaker A: Hi John. We've tried using a couple of mutation testings tools in the open sepling contracts library a couple of times, and the issue always is that the test rate is so large and the number of contracts so large that running the entire test for a single mutation doesn't make any sense. Do you think instead of having the automated only run what tests need to be run based on coverage? If you could have a way to very simply say, only do mutations on this contract, and then only run these tests, which would be as simple as provide a way for the user of the library to write the test command where they could say, I'll only run this test, that would make it much more usable. So that's like for improved usability, I think you could certainly do that. So protocol already supports the invertion of this by allowing you to ignore certain directories. So I guess what you could do right now is ignore everything except the thing you'd like to test. There's no selection of tests implemented yet, but I think I could.
04:28:28.980 - 04:29:45.340, Speaker A: But your question is kind of valid about the duration on a project like yours, and I ran vertigo on open Zeppelin and Ergon Os as well, and it took, well, using 16 parallel processes took like 2 hours or between two and three. And that's, as you said, a really long time. But I think that given these optimizations, for example, the test selection and incremental evaluation optimizations that could be cut down a lot, for example, the test case selection at ten times speed up would decrease the time to maybe 30 minutes or so. Of course, it would have to be evaluated to see if that's actually the speed up that you would get. But I think with some optimizations, it should certainly become a lot more usable and practical to run in a CI setup, or at least frequently. Yeah, that sounds great. Thanks.
04:29:49.550 - 04:30:11.490, Speaker B: All right, thank you, Johan. And I'm sure you will stick around for a couple of more minutes in case somebody asks a question on the chat. Then you can take the discussion in the chat directly, moving over to the next session, which will be moderated by Dio and Martin on act and all things formal verification for solidity.
04:30:14.010 - 04:30:34.790, Speaker A: Thank you. Fancy. Let's do the classical check of whether you can see my screen and my slides. Oh, and we also can do this so you can see my slides and the slides.
04:30:34.950 - 04:30:36.490, Speaker B: Yes, that looks good.
04:30:36.640 - 04:31:23.820, Speaker A: Amazing. Okay, cool. Let me also just start a timer for myself so that I know how much time I'm taking. So thanks, everyone. And to the conference organizers, this is my first time doing a virtual conference. Sort of feels like I'm premiering streaming, which is always something I've been interested in doing. So me and Leo are going to be talking about formal verification of solidity contracts and a little bit smart contracts in general.
04:31:23.820 - 04:32:54.680, Speaker A: Some of the ways in which the language and the compiler can assist in doing this process and also talking about act, which is a language for specifying the behavior of smart contracts. And I'll try to keep this relatively brief, leaving a lot of room for discussion about what can be done to improve the workflow as it is right now. So to begin, I'll just position this question of how to specify the behavior of selected functions in context, sort of opening up for the discussion. One way in which you can do this is to use asserts directly in the function source. And if you are not using annotations, but rather referring to the variables directly in the solidity code, this is how that can look like. For a transfer function. You would be asserting that the relevant variables are changing expressed in pre and post conditions, and it can look something like this.
04:32:54.680 - 04:34:25.366, Speaker A: But if you don't want to do it directly in the function source, an alternative would be to have like a wrapper function. And this relates again to the discussion we just had on conventions for testing frameworks, because this looks very similar to how you would write a test if you are writing your tests in solidity at least. And yeah, I think it's sort of straightforward what's going on here. You do some caching of variables before you execute a certain function, and then you compare those values according to some condition against the state that you get after executing that function. And then there is the alternative of specifying the behavior in a language, in a different language altogether. And so here this language is act, which I'll be going a little deeper in during the next couple of minutes. So I think this is a good example to just get a flavor of what the syntax can look like if you want to really highlight everything that's going on inside a transfer function.
04:34:25.366 - 04:35:54.100, Speaker A: So it should be noted that this function makes a little more of a precise claim than the previous two. In the previous two, we're really only talking about the updates that happen to the balance of mapping, whereas here we're also saying under which precise conditions these updates are happening. We're saying that call value should be zero, since this is a non payable function and we're doing the doing the check that is done implicitly here by the safe map. And we're also immediately forced to consider the edge case of what happens when you're actually transferring to yourself. So in aspects you are forced to split up every claim that refers to mappings depending on whether you have a collision in those mappings or not. So I'll get more into that in the next slide, I think. But let's just talk a little bit about the different approaches and sort of the pros and cons for them.
04:35:54.100 - 04:38:35.960, Speaker A: So for the first two, obviously a big advantage is that it's integrated and the developer doesn't need to learn an additional language, however obscure you may find it, but can just work directly in solidity. And if you're writing your properties of the function directly in the source, things are also pretty transparent or auditable by people reading the implementation, because they'll see your claims about what the behavior is supposed to be, and they can either test it for themselves with their own frameworks, or they can just sort of get a better understanding maybe of what this function is supposed to do if they prefer reading this functional description over the sort of implementation that is given. An advantage of the latter twos, or having this wrapper spec for the specification in a language completely outside of solidity, is that it's more agnostic to the particular implementation. If you were to change how the nature of transfer were implemented, you could probably still reuse the same testing function and even this spec. But if you want to change how the implementation does what it's doing according to the spec, then you'll be forced to rearrange your spec too, probably, or you'll be forced to rearrange the claims that you're making inside the implementation. Now you're editing both what the contract is actually doing and the description of what it should do in the same development process. And so another advantage of using a different language that is designed for the purpose of specifying smart contracts is that you can make decisions that are more aligned with how functions are described independent of how they are implemented.
04:38:35.960 - 04:40:56.484, Speaker A: You don't really need to care much about what is the most gas efficient way to do something, but you just want to describe the results of whatever implementation you might have. And also, as I mentioned, there are certain features of the act language that are designed to lean the developer into making sure that they really think about all of the edge cases that may exist in the implementation and are forced to think about them explicitly. Also, when one has an outside language that sort of exists beyond the scope of the EBM, one can talk about all of the variables involved as true integers and not just EVM words that wrap around to the 256. As we can see here on the last line of the phenomena block, we're able to express that the addition of value to balance of two should not overflow, and we can express that by explicitly giving the bound of two to the 256, even though that is essentially unexpressible at least in this form, if we are only allowed to use bitwise addition. Yeah, some additional points there that I think are obvious. One big approach that I didn't mention here is of course if you do sort of salty verify approach where we use annotations instead of using the source code directly. So to summarize the essence of act, it is designed to be a human readable specification language in which you can express function descriptions and contract invariants, get to that later, from which you can generate proof obligations to multiple back ends.
04:40:56.484 - 04:42:12.988, Speaker A: So there's a large number of formal verification tools that are emerging, although there has been for a while, but they're getting very good right now. And it's really curious to try these different tools out because they have certain strengths and weaknesses and are also operating in slightly different domains. And so act can be used to try these, test these tools against each other and compare the results, but also utilize the different tools that are targeting different domains. So when you're working with these source level specifications or development tools, you're always at risk of the compiler doing something that you're unaware of or behaving unexpectedly. And this is sort of a blind spot for the formal verification process. But then on the other hand, if you're doing bytecode verification, it takes a very long time and can be difficult to do stuff like contract environments. So act is an attempt of modularizing the process of doing formal verification for smart contracts and being interoperable with these different backends.
04:42:12.988 - 04:44:55.180, Speaker A: So our first couple of plans for different backends we'll be targeting is a cock backend for claims that are difficult to prove using SMT solvers and really require a manual proofs, SMT queries for doing contract invariance and certain checks that are easier to do in the s t setting. And for bytecode verification, you already have a prototype of exporting proofs to KVM, and there's also an HVM backend in the works, which will be faster in many cases, but slightly less general. And so, to go a little deeper into this point about the modularity of proving tools and verification setups, here is a more in depth example where we are expressing an invariant in the constructor of the token. So this also touches on a point that I'll get to a little later, where here we have this sum construct, which is not native to solidity or even something that could generate any reasonable bytecode, but it's very useful for formulating properties about what we expect our contract to do. So yeah, here's more of an example. And I guess one point that I'm trying to make here is that I think this language is pretty similar to how you may express the nature of what you want a token to do in a setting like you're writing an ERC. When I recently was writing an ERC for permit, I wrote the description and the specification of how this new function and token extension is supposed to behave in a language that was very similar to this, because I think that it really allows for people to make their own implementations and have freedom in how they decide to optimize their particular behavior while still being completely unambiguous in terms of what the function should do.
04:44:55.180 - 04:45:50.202, Speaker A: So as a result of this, it means that if you're writing an ERC in this language, not only will it be clear for people to read and implement, but it can also be something that you can use to test and formally verify that ErCs are actually implemented in the correct way. Yeah. So here's a little bit more about how it actually works. Is there a question or just a sigh? I'll keep going then. So I'm not going to go into too much detail here. I'm just going to say what I said before, that the design decisions of actor are made in such a way that you should be able to really think about all of the education.
04:45:50.336 - 04:45:53.866, Speaker H: There was actually a question, Martin, should.
04:45:53.888 - 04:45:55.126, Speaker A: I go to the gitter chat?
04:45:55.158 - 04:45:59.034, Speaker H: Or like someone raised their hand here? Who was it?
04:45:59.232 - 04:46:41.900, Speaker A: Justlyn? It was me. Yeah. So what are the limitation of the language? Can I, for example, describe my property over a set of function like something should od after calling f one and f two, or something like that? Yes. Actually the syntax that you saw here of invariant is a special case of an invariant that holds over all functions. But there's also a syntax where you can do invariant of and then function. So, yeah, it's a good point. Thanks.
04:46:41.900 - 04:47:31.040, Speaker A: Set this right. So here is, I guess, a more concrete way of expressing what I mean by doing this modular approach to verification. If we take an example, smart contract this one. This is going to be a contract that I now only express in terms of act syntax. But you can probably imagine the solidity implementation. It should be fairly straightforward to see. So we have three variables that it's dealing with, xyz and an invariant that we expect to hold from them.
04:47:31.040 - 04:49:16.160, Speaker A: If we have two functions which simply update two of our variable symbol by multiplying it with a scalar, either multiplying x and z using a function f, or updating y and z using the function g, then this generates a cock theory, which is beyond the scope and sort of agnostic about the blockchain setting that this smart contract is operating in. And so it has sort of the essence of this contract, and it allows you to get all of the relevant definitions over which you can do the proof of safety that you have expressed in the app syntax. So all of the boilerplate that comes with generating the definition and the theorems here of the smart contract involved would be generated automatically, and you simply insert the proof. I skipped ahead a little bit. Let's see if there was something we missed. Yes, so I guess I showed it now by example, rather than explaining the details of what I meant to say. I'm essentially saying that usually when you're doing a formal verification, this sort of end to end formal verification process where you have some smart contract business logic, sometimes people refer to or high level specification which you expect certain properties to hold of.
04:49:16.160 - 04:51:07.166, Speaker A: You can make proofs on a high level which aren't referring to any bytecode specific or blockchain specific parts, and then you can also perform the bytecode level proofs with the same specification if you're using act. So this refinement that the higher level properties that you're proving also hold of the lower level bytecode that you get from the compiler is more of a property that either holds or doesn't hold the back. Well, hopefully it holds if we do everything right, but it's not something that you need to do for each project, right? So here's just a little list of some motivating properties or motivating examples for why you may sometimes require something more complex than just SMT solvers for proving correctness of your smart contracts, simply because there are certain properties of smart contracts that express the correctness of them that are too difficult for SMT solvers to do. And this is a good example which you can actually prove using SMT theories depending on which strategy you take to doing it. But I think doing something more complex would be out of scope for this. Okay, so in conclusion, leaving more time for questions and discussion rather than just presentation. In doing act and in writing specifications of smart contract using act, you get this language improver agnosticism.
04:51:07.166 - 04:52:29.150, Speaker A: So you can compare different provers, but you can also compare different implementations with the same logic or different languages writing the same smart contract. So you can compare the bytecode generated byte per se versus the bytecode generated by solidity against each other. Actually, just as a note on that, this was the subject of a presentation that I did at last Debcon, where we had, similar to the previous slide, a specification for an ERC 20. And then the talk was more of a hackathon where people were competing to make the most gas efficient ERC 20. And this is sort of a safe way to do these gas called competitions, because now you know that not only can you have these crazy optimized versions that are supposed to do something, and you sort of have some test suite yet to check it against, but you can actually do formal verification against the bytecode that is generated in the end. So it's a very good measure to have, if you're really keen on making crazy optimizations. Yes.
04:52:29.150 - 04:53:21.614, Speaker A: So this is act. Please come to the gitter channel to discuss it if you're interested. And just to keep this sort of focused on solidity, here are just some pointers that, well, an ask really it to Solsi and the developers of maybe something that was discussed already during the debug session. I'm not sure, I wasn't able to attend that, unfortunately. But the ask is essentially this, and it's related to bytecode verification. Basically the situation is that when you're doing bytecode level proofs, often you have a pretty performance heavy duty because you're symbolically executing EVM code. And there's a lot of things that can happen.
04:53:21.614 - 04:54:23.022, Speaker A: And in order to scale this process, what one wants to do is to reuse proofs about subsets of bytecode as LEM as another proofs. So if you already know that a particular internal function can be summarized to do this, then you can reuse it whenever you get to that part of the bytecode. But this is sort of tricky to do using solidity right now, because it requires a lot of manual labor to extract the relevant pieces of bytecode. The AC really helps and the source map really helps. But what would help even more is to have pc values directly from specifically internal functions and modifiers. So this is my path. I have a method of extracting it now, which involves combining the ast and the source map, but it's very unreliable.
04:54:23.022 - 04:55:46.990, Speaker A: And I think direct support for this would be beneficial for other tools as well. Are there any other things I want to say? Not really anything more than what I said here? Oh well, maybe one thing. Variable location at function heads and loops could also be valuable. Can you be a bit more specific on that? So also on the PC values, what exactly do you mean? So essentially what I've been doing before is when doing a proof about an internal function is that you go to the Pc value that represent the jump test where this function begins. And usually if it is a stack based function, you have the arguments organized in stack in reverse order to how they appear in the function declaration. And then you can do your proof like that. It becomes more complicated if you have a function which involves memory, and also if the function comes from a different contract that has been imported somehow or inherited from, then getting the location of how these functions relate to the bytecode is kind of tricky.
04:55:46.990 - 04:56:08.920, Speaker A: Is that clear, Chris? Or can I clarify something that's so stupid, why is there even a hang up button?
04:56:10.330 - 04:56:12.760, Speaker B: Jocelyn is raising his hand.
04:56:14.570 - 04:56:41.246, Speaker A: May I just say something to finish up on that? So you would like a mapping from probably astid of all functions to the entry point as a bytecode offset? Yeah. Okay, cool. Thanks, Francis. Can you direct or sort of say who?
04:56:41.348 - 04:56:43.390, Speaker B: Yes, Jocelyn.
04:56:46.850 - 04:57:59.080, Speaker A: Yeah, nice talk. So, could you describe when act, start and stop typically, is it a DSL with JSOn output? Or does it do more stuff like checking if the variable, if the axe variable are the same as the contract variable and stuff like that? So it is right now generating from the specification language a JSON output or sort of intermediate representation, which can be used to plug into different back ends. And also we'll be supporting some back ends internally without going this ast. But there's also the option of. So to answer your question, I think it's about whether the source code needs to be available for these proofs to go through or not. Or at least that seems to be part of the question. And the answer is, for some back ends it will be necessary, or for some back ends you will be required to do additional work if the source code is not present.
04:57:59.080 - 04:58:52.620, Speaker A: So, a newly introduced feature or new, but a feature now of the solid compiler is that you can extract the location of storage variables. And then it's very nice to write specifications like this where you want to do by click verification proofs, because you can simply refer to them by the name. And if you have the source code, you know what those storage locations are going to map over to in terms of EVM locations. So if you don't have the bytecode, or if you don't have the source code of a contract, and you still want to make this bytecode local proof, you would need to be more explicit about where these different locations go and so on. And if I have the source code, the JSON output is going to have directly the storage location? Yes, there's a way to. Thanks.
04:58:55.150 - 04:59:11.950, Speaker B: Cool. Do we have any more questions directly related to Martin's talk, let's also have a look at the chat, if not Leo, did you also prepare something, I'm assuming?
04:59:17.030 - 04:59:20.660, Speaker H: Yeah, I guess. Then I'll share my screen.
04:59:21.750 - 04:59:23.378, Speaker A: Let me just find everything.
04:59:23.464 - 04:59:32.200, Speaker H: Here's does the sharing work?
04:59:32.890 - 04:59:33.734, Speaker B: Yes.
04:59:33.932 - 04:59:35.160, Speaker A: Okay, cool.
04:59:36.090 - 04:59:37.960, Speaker H: Does my camera also work?
04:59:42.830 - 04:59:43.900, Speaker B: Not really.
04:59:44.830 - 04:59:50.460, Speaker H: Oh really? Because on my own screen it shows actually my slides and myself.
04:59:51.250 - 04:59:53.390, Speaker B: Oh yeah, it was just delayed.
04:59:54.450 - 04:59:58.298, Speaker H: Yeah, somehow jitsu is using like 300% cpu.
04:59:58.474 - 04:59:59.360, Speaker C: Oh no.
04:59:59.810 - 05:00:02.720, Speaker H: But hopefully it's going to survive half an hour.
05:00:04.610 - 05:00:05.360, Speaker B: Final.
05:00:08.790 - 05:01:04.690, Speaker H: So what I want to talk about is a part of what Martin mentioned, which was specification inside the source code. And this is basically a list of things I would like to go through. We'll for sure not have time to go through all of them, but I basically want to start a discussion and wrap up some issues that we have open and decide on some new syntax and behavior of the compiler. So the first thing is right now we have these two constructs to write some sort of spac. Inside is loaded code. We have require and assert functions, right? And from a logical perspective requires are preconditions and asserts or post conditions that is required to filter the behavior of a certain function. And we use assert to state things that should never break.
05:01:04.690 - 05:02:27.950, Speaker H: And if they do break, it means that there's a bug somewhere. The thing is, in the current version of the compiler these two functions also generate code. So the first question that I already want to ask everyone is so we have the wish to have a sort of static assert c plus plus style static assert construct in solidity, which would work only from a logical point of view, allowing verification tools and static analyzers to know the intent of the programmer without generating bytecode for it. And this is already done kind of manually by a lot of people. When you see commented out assertions in the code, or people add assertions for testing or verification, then they remove the assertion before country deployment to save some gas. So does it make sense to add something called static assert that would for example only allow side effect free expressions or even const expressions? And just another question on top of that that christian parpart actually raised. And the issue was we don't necessarily need to add a new keyword static assert if we basically add a compiler debug flag for example to generate code or not given an assertion.
05:02:27.950 - 05:02:58.200, Speaker H: So that's my first question. So I think it would be really useful to have some sort of static assert. And I actually agree with Christian parpar at this point that it would be nice to just give a flag and say, if I am a debug mode, don't remove the assert. And if you're release mode, for example, similar to how it is done in c and C plus plus, for example, you can just give a flag and remove the effect of the assertion. What do people think? I saw someone raise their hand. Please go on.
05:03:01.870 - 05:03:06.540, Speaker A: The static assert. That wouldn't result in any code anyway.
05:03:08.670 - 05:03:09.706, Speaker H: It wouldn't what?
05:03:09.808 - 05:03:12.942, Speaker A: It wouldn't result in any bytecode in the output anyway.
05:03:13.076 - 05:03:22.110, Speaker H: Right. But the debug flag would simply be there so we don't have to add a new name, a new keyword. Static assert.
05:03:23.830 - 05:03:31.490, Speaker A: So you mean that assert would be the keyword and it would be removed from the code in release mode?
05:03:32.150 - 05:03:39.620, Speaker H: Yeah, something like that. Chris.
05:03:41.880 - 05:03:54.920, Speaker A: I think any compiler flag that changes the semantics of code is rather dangerous, because usually you only take a look at the source code and know that the flags used to deploy a contract.
05:03:58.910 - 05:04:16.348, Speaker H: Yeah. My argument, not necessarily against that, but around that, was that the flag is not more dangerous than people already manually removing assertions from their code. But I don't necessarily disagree with what.
05:04:16.354 - 05:04:20.770, Speaker A: You said, but if you remove from the code, then it's not in the code.
05:04:21.620 - 05:04:51.270, Speaker H: Yeah, but then you got a compromise, you get something in between, because if you remove from the code, then you basically lose the assertion and you lose all the analyses that would be done on top of the bytecode. Does any person not from the compiler team have an opinion on this?
05:04:54.970 - 05:05:32.786, Speaker A: Yeah, I think it's definitely going to be useful to have a static asset. When we do some testing of contract, we end up adding a lot of assertion within the code base that are going to be removed later. So if you can have something like this, it's definitely useful. So maybe to clarify, static assert is a non code generating assert, right? Because static assert in C is something else. Yeah, the compiler does not verify that the assert holds.
05:05:32.898 - 05:05:34.422, Speaker H: Well, it depends if it's a const extra.
05:05:34.476 - 05:06:14.690, Speaker A: It does, yeah. Justly, just to be like, the way I see it is that it might generate code if you want it. Like you want to have some static asset and you are not sure that it can be proof at compile time. You can have this asset that can be used for unit test or for fuzzing. Basically like any assertion you don't want in the deployment, but you want for testing or for compilation.
05:06:15.830 - 05:06:34.360, Speaker H: That's exactly where I see the value of some compiler flag, either to disable or to enable code generation. Because if you want both, if you want it as an analysis point, but you also want code from it, then if there is no flag, then you would basically have to write both.
05:06:38.190 - 05:06:39.850, Speaker A: After with proper lag.
05:06:41.090 - 05:06:50.770, Speaker H: Or rather you would write assert if you want code generation and analysis. And you write static assert if you only want analysis. Is that a feasible solution?
05:07:00.700 - 05:07:08.440, Speaker A: Maybe I'm still misunderstanding. Would the flag influence the code generated for assert?
05:07:10.320 - 05:07:23.340, Speaker H: Well, I guess there could be two options there, right? So you could have a flag that enables code generation for static assert, or a flag that disables code generation for assert.
05:07:35.510 - 05:07:37.400, Speaker B: Benhardt wants to say something.
05:07:40.330 - 05:07:44.060, Speaker A: Yes, I just wanted to say, if I understand it correctly, that.
05:07:57.190 - 05:07:57.666, Speaker H: I'm not.
05:07:57.688 - 05:08:01.534, Speaker B: Sure if I. Yeah, I also think we lost connection.
05:08:01.582 - 05:08:02.180, Speaker A: Okay.
05:08:02.550 - 05:08:11.830, Speaker B: Yeah, it seems a spanner discount. He rejoined apparently.
05:08:12.810 - 05:08:14.390, Speaker A: Could you hear what I just.
05:08:14.540 - 05:08:16.946, Speaker B: No, no, you were breaking up. Just repeat it.
05:08:16.988 - 05:08:38.874, Speaker A: All right, so if I understood it correctly, Leonardo, the static assert would not emit any bytecode by default, but you could activate it, right. And then this would allow you to instrument the bytecode to add additional asserts for checking. Do I understand that correctly?
05:08:39.002 - 05:08:41.534, Speaker H: Yeah, that could be an option that.
05:08:41.572 - 05:08:59.590, Speaker A: Would be super useful. So that would solve a problem that we have all the time, that it's very hard to instrument bytecode somehow in a way that doesn't modify the original code that we are testing. So yeah, please do it immediately.
05:09:00.570 - 05:09:11.850, Speaker H: So what do you think about Chris's argument that if you have a compiler flag that changes code generation, then what you see might not be what's actually deployed?
05:09:12.670 - 05:09:25.610, Speaker A: Well, I think that's a certain reasonable trade off that you want to make, right. The only thing that is added is conditional branch and an exception.
05:09:25.690 - 05:09:26.320, Speaker G: Right.
05:09:29.090 - 05:09:57.740, Speaker H: But what would you all think of having both a cert and static assert, and no compiler flags and assert generates code? Static assert doesn't generate code. So if you only want the assertion as a specification, then you add a static assert, and if you want it both as a specification and as code, then you use assert. Would that work as well?
05:09:59.230 - 05:10:04.378, Speaker A: Yes. I was not disconnected for a bit, but the last part sounded great.
05:10:04.544 - 05:10:14.782, Speaker H: Okay, then it sounds like no compiler flag and having both with a single behavior. I think someone else raised their hand.
05:10:14.836 - 05:10:16.826, Speaker B: Please go, it's Joan.
05:10:17.018 - 05:10:48.570, Speaker A: Yeah, that was so specifically the case that Bernard was discussing, I think is where we use some automated testing tools, which is for example fuzzing or similar execution. And if you have a lot of assertions you want to check when you're doing this, but you don't want to have them in the deployed bytecode, then it wouldn't work. I think without a compiler flag.
05:10:50.830 - 05:10:56.380, Speaker H: Okay, sounds good. So that went rather well.
05:10:57.150 - 05:10:58.860, Speaker B: Alex also wants to say.
05:11:02.530 - 05:11:49.830, Speaker A: Not, I'm not clear what kind of assertions are added for testing only which can be removed on a release binary. Does anybody have any example? Yeah, lots. Like for example, some contract invariants that you want to check on every function, but you only want to have these checks when you run symbolic execution, for example. And you don't necessarily need to run on the actual deployment because it would be just like you want to add a lot of asserts that would be a waste of gas to run, right? But you want to check those properties when you run your fusser or your symbolic or SMT solver.
05:11:55.020 - 05:12:01.230, Speaker B: We also have du crisis, cool crisis raising their hand.
05:12:02.800 - 05:12:37.864, Speaker A: So it seems that the trade offs from a security standpoint, it's very risky from a security standpoint actually to have, in part of the semantic doesn't match the bytecode itself. Right? And in my personal opinion, you have from one side, one developer who actually will put that assert, thinking that it actually is going to have some impact on their final code. That ambiguity adds to the risk of the smart contract itself, right?
05:12:37.902 - 05:12:47.100, Speaker H: So we have one more person in favor of both assert and static assert, each one with one behavior, if I understand correctly.
05:12:49.040 - 05:12:51.950, Speaker A: So what is the behavior of static assert now?
05:12:52.400 - 05:12:55.820, Speaker H: No code generation at all and no compiler flag.
05:12:58.580 - 05:13:02.864, Speaker A: But it does cause checks being run by the SMT checker, right?
05:13:02.902 - 05:13:09.170, Speaker H: That's the, yeah, and the SMT checker and any other tool that might look for this kind of stuff.
05:13:10.040 - 05:13:14.740, Speaker A: How could it look for that kind of stuff when it doesn't generate code from the ast?
05:13:15.720 - 05:13:18.032, Speaker H: I guess. Yeah. Oracle, the target solidity.
05:13:18.096 - 05:13:27.782, Speaker A: Yeah. So it's still only for constant expressions or anything more?
05:13:27.916 - 05:13:32.630, Speaker H: No, I would say not only constant expressions, but side effect free expressions.
05:13:36.030 - 05:14:05.670, Speaker A: Last question maybe on that. So if it's all the tools who would use this would have to look at the source, then why do we need static assert and not maybe some annotations in comments or any other syntax? Because the comments are not parsed by solidity so that reference resolution doesn't happen. Well, not spec is.
05:14:06.440 - 05:14:12.230, Speaker H: Well, any tool that target source this would already work. So what you mean is what a tool that target eVM bytecode, right?
05:14:15.980 - 05:14:33.980, Speaker A: I mean that's actually a point, right? We could use a special comment that has to contain valid, valid solidity expression that is side effect free, and then it's obvious that it will not result in bytecode being generated.
05:14:36.960 - 05:14:49.200, Speaker H: But couldn't that be mixed with normal comments? I think that could be more confusing than having a function, say, called static cert that has that expression.
05:14:50.020 - 05:15:01.350, Speaker A: But we already have triple slash comments that have a special meaning. But static assert is confusing for anyone who is not familiar with c plus plus. Anyway.
05:15:03.000 - 05:15:08.490, Speaker H: To me it doesn't have to be called static assert, it's just a keyword. Could be anything.
05:15:11.870 - 05:15:19.450, Speaker A: Yeah, I think I personally have a slight preference for some kind of an annotation and not function which is not part of the bytecode.
05:15:20.130 - 05:15:34.290, Speaker H: I would be fine with that. But I personally don't really like the comments thing because I think it can easily be confused with comments and not necessarily ignored, but in many cases ignored.
05:15:35.190 - 05:15:38.594, Speaker A: What would be the problem if it's ignored? Ignored by who?
05:15:38.712 - 05:15:40.820, Speaker H: By people that should read it.
05:15:42.070 - 05:15:45.140, Speaker A: But if it's true anyway, then you can also ignore it.
05:15:45.910 - 05:15:49.270, Speaker H: Yeah, but then you misanalysis or misinformation.
05:15:51.530 - 05:15:59.754, Speaker A: But it's not enforced by anything. So people reading it, it doesn't mean anything to them. It's only meaningful to tools which enforce it.
05:15:59.872 - 05:16:01.180, Speaker H: Yeah, that is true.
05:16:04.310 - 05:16:52.900, Speaker A: Jocelyn, yeah, if you add this feature either by annotation or by this static assert or any function, it might be worse to have a special flag to generate the code so that tools that are not working at the solidity level can also check for it, like testing or something like that during the code generation just to be compatible with tools that are not looking at solidity. I would be fine with such a flag if it's a comment where it's obvious that the test is not there, but okay, I mean, yeah, and we have a check that it doesn't generate bytecode if it cannot be verified, and we have a flag that we might add the test to the code.
05:16:54.550 - 05:16:57.906, Speaker H: Yeah, wait, it's getting confusing now.
05:16:58.088 - 05:17:00.020, Speaker A: Yeah, maybe I should have said anything.
05:17:02.170 - 05:17:06.390, Speaker H: But what would this look like in a bytecode?
05:17:16.770 - 05:17:25.970, Speaker A: What, the comment you mean or. Yeah, the comment is yeah, maybe. I would suggest to you we have to take this offline.
05:17:28.710 - 05:17:49.590, Speaker B: Just propose that we can write down the proposals, which I think is a good idea to get a few more bit more structure in there. We can use the hackmd if you like. I just have to make it editable and I will show the link in a sec. And also Akosh is raising his hand, so please go ahead Akosh.
05:17:49.750 - 05:18:27.000, Speaker A: So what about some completely new construct which is clearly distinguishable from both code and comments, like some kind of annotations that for example Java has? Like you can write at test or at immutable, or like in C sharp you put these so called attributes in square brackets. What about a solution like this. Yeah, I think we mentioned that. Annotations or comments, but we should write down all these options. I think.
05:18:30.810 - 05:18:36.120, Speaker H: Jits is kind of freezing for me. Can you still hear me?
05:18:37.710 - 05:18:39.146, Speaker B: Yes, we can hear you fine.
05:18:39.248 - 05:18:39.562, Speaker A: Okay.
05:18:39.616 - 05:19:34.618, Speaker H: Yeah, it just has a delay like when I click unmute. But anyway. Okay, then maybe we'll try to get one more discussion, which is contract invariance. This was an issue that I opened a long time ago, but then it got revived in another issue recently, and it's basically a construct invariant that you can do a similar thing and give side effect free expressions that represent contact invariance. And you can kind of do that if you want code generation with. This would kind of be a sugar. If you want code generation for you're adding an assertion after the constructor, adding requires of that property before each externally called function, and add an assertion at the end of each externally called function.
05:19:34.618 - 05:20:09.010, Speaker H: So this would basically create an inductive invariant in your code, because you assume the property and you run the function and the property has to hold over the new state. Many people seemed in favor of this. I think the feature itself is wanted. But then I guess the same question arises, do we want code generation for this? Any thoughts?
05:20:13.830 - 05:20:29.590, Speaker A: Yeah, some of before. This is definitely really good. And if we can have code generation with a specific flag, like testing or verification mode or whatever, it's going to be also useful.
05:20:31.130 - 05:20:52.110, Speaker H: Yes, we need some mode like that. I guess there's two general questions here. One, do we add this flag or not? And two, how would this look like in the bytecode if you don't want code generation on it, so that tools that target bytecode can still verify those properties?
05:20:53.250 - 05:21:18.454, Speaker A: Yeah. So why not consider it the same way as the static assert and have an annotation for both and have the compiler flag and maybe some other tools like other mapping. I would put even if there is no bytecode, but yeah, I think it.
05:21:18.572 - 05:21:33.446, Speaker H: Sounds good to treat all of these in the same way. Okay, that sounds good. So if no one disagrees, I'm going to move on to the next.
05:21:33.568 - 05:22:25.066, Speaker A: Maybe one thing require before externally called functions. I think that is not a clear thing. What is not? You might have a function you call externally that is still part of the same contract system. Part of the same transaction, you mean? No, the same of the. If you have a contract system that consists of multiple contracts that call each other and also know each other's behavior, then it might be that you have an invariant that only holds at the outer part of that contract system. But is violated inside these internal calls.
05:22:25.258 - 05:22:31.210, Speaker H: Yeah, that's what I mean. Like the invariant would hold only around the outermost transaction.
05:22:31.290 - 05:22:31.920, Speaker A: Yeah.
05:22:33.730 - 05:23:00.130, Speaker H: I think that's actually what I meant here. Maybe I just phrased it in a confusing way, but that's what I meant here, because these invariants, they are, they might be broken during the transaction, considering also internal transactions also jumps to internal functions, these kind of things. So, yeah, the invariant holds around the outermost transaction. I guess that's what you meant as well, right?
05:23:03.720 - 05:23:04.470, Speaker A: Yes.
05:23:07.080 - 05:23:50.760, Speaker H: Okay. Is it fine to move on the next topic? Okay, so quickly jump to number four, because it's also related. And I think we're going to have a quick decision. Number four is loop invariance. My thing we want to write invariance for loops. Same question, do we want to generate code or not? And I'm tempted to assume the same answer here, that this is wanted and it should work in the same way as the other things, maybe with annotation or any of these special features.
05:23:55.180 - 05:24:04.620, Speaker A: Yeah, definitely, if we have them, should be working at the same as the other invariant. Could it be like scope invariant instead of loop invariant?
05:24:05.920 - 05:24:07.840, Speaker H: Sorry, can you repeat that last part?
05:24:07.990 - 05:24:12.480, Speaker A: Could it be like scope invariant instead of loop invariant?
05:24:12.900 - 05:24:15.472, Speaker H: Scope invariant, yeah, like you have a.
05:24:15.526 - 05:24:23.684, Speaker A: Scope within a function, which is basically what loop does, but it would be like this, but more general. That makes sense.
05:24:23.882 - 05:25:16.132, Speaker H: Can you comment that on the issue so we can discuss that there later? The thing with loop invariant, it's a very common thing with formal specs and languages that target verification. So I think that's more of a wanted feature. But of course we could consider that as well. Okay, so now I want to quickly try to get this discussion done. So I think many people will agree that modifiers are very commonly used as preconditions. So you often have this shape in modifiers. So you require a certain side effect free condition, and then you have the body of the function where the function looks like this.
05:25:16.132 - 05:25:46.024, Speaker H: So you have the function, you apply the modifiers, and then you apply the modifiers as guards, basically as preconditions, and then you execute the function. Then the first question that I have is, should they then be restricted to side effect free preconditions? If the answer is yes here, then they could look like something like this, where you don't have the body placeholder anymore. You basically have only the condition, and you apply these preconditions to the functions.
05:25:46.072 - 05:25:55.280, Speaker A: Yeah. Is there a case this might not work? Typically, people are using modifier to have reentrance.
05:25:56.580 - 05:26:02.180, Speaker H: Yeah, that's actually the only case I could come up with that doesn't fit this shape.
05:26:03.400 - 05:26:15.636, Speaker A: Why not call it precondition instead of modifier like this? Like what? Really slow.
05:26:15.668 - 05:26:19.050, Speaker H: I'm trying to get there. Do you see it now?
05:26:21.740 - 05:26:23.450, Speaker A: Yes. Okay.
05:26:24.380 - 05:27:03.080, Speaker H: That's where I was getting to. So you could have a construct called precondition that basically saves a property, and then you can tell over which functions it applies to. The problem that I see with this approach is that you apply over the function in the declaration of the precondition, but when you look at the function itself, you don't really know what's going on. You don't know which preconditions are applied to that one. So this is why I think this is dangerous. And my final proposal would be something that looks like this. So you have a property, and at the function level, you declare or annotate.
05:27:03.080 - 05:27:16.424, Speaker H: If we go with the same solution for all these things, which properties apply as preconditions where the syntax could be in many different things. Yeah.
05:27:16.462 - 05:27:24.220, Speaker A: Just why not calling the precondition or property directly as a modifier?
05:27:26.340 - 05:27:31.330, Speaker H: Because you mean using a modifier this way.
05:27:33.700 - 05:27:44.916, Speaker A: You call it, like precondition magic, let's say. But when you want to apply it to function f, you just call it similarly as you would call a modifier by just calling the name, because I.
05:27:44.938 - 05:27:48.310, Speaker H: Find the modifier body placeholder misleading that way.
05:27:49.240 - 05:27:49.990, Speaker A: Okay.
05:27:50.920 - 05:28:09.070, Speaker H: It sounds like, to me, it doesn't sound like a precondition anymore. If you basically place the body of a function inside the modifier, and then if you do that, then the modifier doesn't really modify anything anymore. Yeah.
05:28:11.680 - 05:28:17.020, Speaker B: Chris, just a heads up. You have four more minutes for the session.
05:28:17.940 - 05:28:20.528, Speaker H: This is the last thing that I'm discussing anyway.
05:28:20.694 - 05:28:21.410, Speaker B: Cool.
05:28:22.580 - 05:28:46.722, Speaker A: I really like this a lot. I see one downside, though, and this is you can't specify error messages anymore. So we would need a mechanism to specify an error that is thrown on violation, maybe, possibly also in line with the new error types, but that only matters.
05:28:46.786 - 05:28:50.040, Speaker H: But then the next question is, does this generate code?
05:28:50.650 - 05:28:54.470, Speaker A: Yeah, sure. It has to, doesn't it? Yeah. With the precondition.
05:28:56.490 - 05:29:09.100, Speaker H: I guess we could put the message somewhere in there, could have, like, a last parameter of the property, could be like the message.
05:29:09.870 - 05:29:14.890, Speaker A: And the reason you call it property is because you also want to use it for invariants and post conditions.
05:29:14.970 - 05:29:48.870, Speaker H: Yeah, I would like to somehow put those things together, because an invariant is basically a property that holds both as pre and post conditions of every function and as a post condition of the constructor, but I don't see a way. Would you basically name the properties and say invariant p one so that such that property p one also holds as an invariant? Yeah, I don't have a concrete proposal on how to mix the property and invariant together, but yeah, I would like to get those two together. Justin?
05:29:49.610 - 05:30:00.810, Speaker A: Yeah, it's not become confusing for me. Like this property or precondition, are they meant only for invariant, or are they meant to do like just code validation over your contract?
05:30:01.390 - 05:30:35.218, Speaker H: I think the difference is that the property can be anything, right? So property can be a precondition, it can be a post condition, can be invariant. So the property declaration doesn't really generate any code. It's only how you use it that gives those semantics. Right. So in this case, we're using those properties as preconditions of f. But we could symmetrically have a post constructor where you would say function f pre these and post those where they're not necessarily invariants. But you have certain pre and post conditions.
05:30:35.218 - 05:31:02.640, Speaker H: And this could actually also go together with the scope invariance that you just mentioned. You have a certain scope, and you say locally, in this scope, this certain property holds as a post condition, where you might have this other property as a precondition for this scope. So these are just general properties that might hold in different ways in different locations. So the same property can also hold as pre and post, but in different locations of the code.
05:31:03.730 - 05:31:21.350, Speaker A: My question is more like, as a developer, if I use this property, and if the property failed, does that mean that I have a bug, or does that mean that function is going to revert and it's like the correct and expected data validation behavior?
05:31:21.850 - 05:31:37.530, Speaker H: I would relate that to pre and post, also kind of relating to requires and asserts the way it does now. So if a precondition fails, I would say the same as a require failing, and if a post condition fails, the same as an assert failing. But that's my personal feeling.
05:31:38.990 - 05:32:04.260, Speaker A: Okay. Yeah, if you go in that direction, I think it's important to have a good documentation about how it's supposed to be used, because I can see people using the post condition just as like a filtering of the result, and they will not want the SMT checker to say there is a bug while it's just filtering the result of a transaction. Does that make sense?
05:32:05.510 - 05:32:55.810, Speaker H: Yeah, I feel like a lot of this is mostly sugar for things you could already do with requires and asserts, but it just makes the intent a lot more clear. Like, you can just take a look at the code and you get a visual spec right away of what people mean with the pre and post, which maybe would also incentivize people to use these a lot more than developers currently do with requiring asserts. Like, I don't see a lot of asserts in the wild. And maybe if you give this whole set of features, maybe, just maybe, we would see more of that if you give this structured bag of features.
05:32:57.110 - 05:33:10.662, Speaker A: I think it's just important to make a good difference between stuff that are meant to check, like invariant over the code, and stuff that are meant to be used just to do input validation or output validation.
05:33:10.806 - 05:33:30.990, Speaker H: Yeah. When you say output validation, do you mean that as validation of your logic of your code? Because if it is, then it would say an assertion is the correct thing, which is the same as a post, which means that if it fails, there's a bug.
05:33:31.570 - 05:33:44.866, Speaker A: Not necessarily because you can want to use the post condition just to check that. For example, the balance did increase, right? Let's say, yeah, but if this is.
05:33:44.888 - 05:33:49.960, Speaker H: A post condition, and if it fails, then there's a bug in the code that came before that.
05:33:50.730 - 05:34:03.980, Speaker A: No, it depends on your definition of pre and post, because it can just be like validation of execution and you're just reverting if the execution is not correct.
05:34:04.750 - 05:34:08.714, Speaker H: But that's exactly the cert, isn't it?
05:34:08.832 - 05:34:12.970, Speaker A: It's not necessarily a bug. You can use the post condition.
05:34:15.470 - 05:34:15.882, Speaker E: As.
05:34:15.936 - 05:34:21.200, Speaker A: A mean to check the result. It did do the job.
05:34:25.190 - 05:34:41.160, Speaker H: I think we're talking about the same thing. But to me, validating the execution is basically an assertion, because if the assertion fails, it means that your code does not have the property that you asserted it had, and there's something wrong with it.
05:34:44.010 - 05:34:58.220, Speaker A: Yeah, I just meant like that people might choose the print post condition as validation rather than invariant checking. Right.
05:35:01.870 - 05:35:04.646, Speaker H: I guess we're a little bit over time.
05:35:04.768 - 05:35:06.382, Speaker B: Yeah. Can we wrap this up?
05:35:06.516 - 05:35:10.782, Speaker H: Yeah, I'm the next speaker anyway, so I can just till my own.
05:35:10.916 - 05:35:23.780, Speaker B: This does not mean that you decide over the agenda, but sure. Cool. Do you want to say some closing words or where should. Do you have some issues?
05:35:24.230 - 05:35:39.560, Speaker H: Yeah, I'm going to put the link to a link to the talk in the gitter channel. Then people please. So you can go through the slides and then you'll have GitHub links to the issues. And then please go and comment so we can continue the discussion over there.
05:35:43.610 - 05:35:55.550, Speaker B: Awesome, thanks. And then I guess next up is again Leo. With or without Alex? I'm not sure. Introducing the discussion on immutable and explicit copies.
05:35:57.410 - 05:35:59.594, Speaker H: I'm just trying to change my slide.
05:35:59.642 - 05:36:00.960, Speaker A: Just give me a second.
05:36:02.450 - 05:36:03.246, Speaker B: Yeah, sure.
05:36:03.348 - 05:37:22.926, Speaker H: I'm just going to post the link quickly on Twitter. And am I back? Yes, my browser crashed. So jitsi finally did it. Oh no, but it was enough for the first talk. Maybe it's going to be enough for this one too. I'll quickly share my screen. Okay, do you see my slides and me?
05:37:23.048 - 05:37:24.520, Speaker B: Yes. Looking good.
05:37:25.450 - 05:37:29.400, Speaker A: Okay, looks good.
05:37:36.010 - 05:38:32.118, Speaker H: It okay, so the discussion now is about immutable variables, mutable, copyable, and trying to make some things that are really expensive more explicit in the language. So just a reminder. So today what happens solidity are all variables are mutable by default, and you can have immutable state variables if you declare. So in the latest release, all the non value types are references, and that's based on the storage on the location. It can be a storage reference, a call data reference, or a memory reference. And assignment between different locations will result in copies of the values. And if you copy an entire array, for example, this basically means a loop which can be unbounded, and sorry, it cannot be unbounded.
05:38:32.118 - 05:39:24.170, Speaker H: But unbounded loops in smart contracts are bad because they are really expensive, potentially unbounded ones. And if you make a copy from an assignment, this could be a very expensive operation. So if you do something like this, you don't necessarily see that this is making a copy of the entire array from storage to memory. So many people might not be aware of this, but this is a very expensive operation. So do you really want to copy everything in this case? So this issue listed here is about trying to make that more explicit. And the first, or maybe the current suggestion we had for that is an operator called copy off, which basically has to be used when you're changing.
05:39:26.030 - 05:39:26.346, Speaker A: When.
05:39:26.368 - 05:40:22.302, Speaker H: You'Re making assignment between references of different locations. And the current proposal would be so that you would have to say copy off the thing you're copying from, and it would perform a lazy copy, so the copy would not be performed at the assignment point. But later on, when you change the assigned version, the assigned variable, and this is not set in stone. Some of us actually don't really like the name copy off or the way this is proposed. Some alternate syntax options would be there's a typo here. So copy off as it is in the example, copyable copy instead of copy off, both without and with parentheses, or with just different ways. And then let's start discussion then with the question.
05:40:22.302 - 05:40:35.810, Speaker H: So should copy perform a copy at the declaration site, meaning at the assignment point, the first one where you see the copy off at the example or in the first use meaning a lazy copy.
05:40:37.430 - 05:40:42.098, Speaker B: Opinions only see black for some reason. I'm not sure if this is only me though.
05:40:42.264 - 05:40:43.074, Speaker H: Really?
05:40:43.272 - 05:40:49.862, Speaker B: Yeah, maybe Chris or somebody else. Can you see Leo on the screen?
05:40:49.916 - 05:40:52.166, Speaker A: No, it's black for me too.
05:40:52.268 - 05:40:52.920, Speaker H: Really?
05:40:54.010 - 05:40:57.350, Speaker A: It says connect lost, but we hear you. That's weird.
05:40:58.190 - 05:41:00.278, Speaker H: Let me try to reshare.
05:41:00.454 - 05:41:51.210, Speaker A: I also want to say something, so I don't think it is feasible to do the copy only at the first modifying operation. So like lazy copying, like it is done for memory pages on Linux. But what we have to do is we cannot have a copy keyword that performs a copy itself, at least not without specifying a destination and especially for storage. It is only possible to do the copy together with the assignment because otherwise we don't know where we copy into. So I think the original proposal was that the copy of keyword itself does not perform a copy, but anything you do with that expression will do the copy.
05:41:55.650 - 05:42:00.974, Speaker H: Can you repeat the first part of the sentence? My browser crashed again. Can you hopefully see my slides now?
05:42:01.092 - 05:42:06.878, Speaker A: Yes, and it's a very wide screen. You shared your whole desktop, not just the browser window.
05:42:06.974 - 05:42:20.900, Speaker H: Yeah, I know because I've had issues before with Gc that if I share only application, then it crashes. If I share the whole screen it works fine. I think we can connect the monitor. Is it better now?
05:42:21.270 - 05:42:23.160, Speaker B: Yeah, that's perfect.
05:42:24.090 - 05:42:26.950, Speaker A: Something like copy on modification. That's very hard to implement.
05:42:29.070 - 05:42:34.700, Speaker H: Copy on modification. But what was your problem with the first version? Copy at assignment point.
05:42:35.870 - 05:43:18.262, Speaker A: That's what we can do. Yeah, I don't know what I personally would suggest or it's not actually copy at assignment point, but copy as soon as you do anything with an expression that is a copy of expression. So the expression whose source code starts with copy off. Yeah, and you would have to use copy of also if you call f here for it. If you call f of a and or. No, if you call x. No, if you call othercontract g of a, you would also have to write othercontract g of copy of a.
05:43:18.262 - 05:43:18.886, Speaker A: Yeah.
05:43:18.988 - 05:43:23.690, Speaker H: Because it translates to call data and it will also perform a copy.
05:43:26.270 - 05:43:29.626, Speaker A: Okay, sorry. So there were some people who wanted to speak.
05:43:29.808 - 05:43:36.240, Speaker H: Yeah, I can't see the jitsu screen anymore, so please Chris or Francis, let me know when people want to speak.
05:43:37.890 - 05:43:38.314, Speaker C: Yep.
05:43:38.362 - 05:43:39.120, Speaker B: All right.
05:43:43.110 - 05:43:52.290, Speaker H: Does anyone else have an opinion on, first of all, having to make a copy explicit? If an assignment is performed between different locations?
05:43:57.770 - 05:44:36.820, Speaker A: I'm not sure to say what the lazy copy brings here, because you are going to do the copy anyway, so it's going to be the same cost. So we're not doing the copy at the first time. Yeah, so the idea for the lazy copy was really just because you need a place to copy into, and there cannot be just a copy operator that performs an independent copy that would work for memory, but that's actually not. We want to have, because we want to copy into storage, or from storage into memory, or from memory into call data and things like that. That's the lazy part. Okay.
05:44:44.720 - 05:44:45.470, Speaker H: Sorry.
05:44:48.660 - 05:44:55.170, Speaker A: Think that this would be too verbal. Is it annoying to have to write copy of all the time.
05:45:06.120 - 05:45:20.570, Speaker B: We have? Harry wanting to say something. Harry, we can't hear you so far.
05:45:20.940 - 05:45:40.572, Speaker A: I'm sorry, I forgot to unmute myself. Yes, it's slightly annoying, but I think it's probably worth it for the gain in clarity. Just like explicit, the various explicit type requirements that were added in 50. Because right now the semantics of assignment.
05:45:40.636 - 05:45:44.044, Speaker D: Are one of the most confusing parts of solidity.
05:45:44.172 - 05:45:52.580, Speaker A: And so just making it explicit when you are and are not doing it, making a copy would make things so much clearer.
05:45:55.640 - 05:46:31.330, Speaker H: Yeah, I think I myself used to have to go a lot in the documentation to remind myself which assignments will make a copy, which ones won't. So this would have saved me some time, even though it's more verbose. I completely agree with that. Is anyone opposed to this kind of change? I'm not saying the change would be necessarily this, what's written here in the example, but making the copy more explicit and having to write something extra whenever you have a location change.
05:46:36.900 - 05:46:38.240, Speaker B: Yeah, Francisco.
05:46:40.840 - 05:46:47.910, Speaker A: I'm not sure I understand. Can you describe the scenarios where a copy is not made?
05:46:48.360 - 05:47:01.448, Speaker H: If you assign storage to storage, you would only assign reference. If you have storage instead of memory in this line, then app would be a local pointer to a.
05:47:01.614 - 05:47:05.240, Speaker A: Is it possible to not copy something that is memory to memory?
05:47:05.580 - 05:47:08.360, Speaker H: Memory to memory is also a reference.
05:47:09.900 - 05:47:18.524, Speaker A: So if I say here, I say un array memory, x equals a, but a is storage.
05:47:18.572 - 05:47:30.390, Speaker H: Okay, if a was also a memory array that you will allocate inside the function, then x would be a reference, right?
05:47:34.120 - 05:48:05.310, Speaker A: So basically the idea is that currently the solidity compiler generates performs implicit copies in several places, and the idea is to always require the copy of operator for such places, and that would be the initial implementation. So currently there's no implicit copy for memory to memory copies. So you wouldn't be able to write un memory x equals copy off and then some other memory variable. But we will probably also have that at some point.
05:48:06.960 - 05:48:29.460, Speaker H: Yes, as Chris mentioned before, the copy off operator wouldn't perform the copy itself. So if you only have the expression copy of a, this is not a copy. This does not perform a copy yet, because they don't have a this copy. You need this expression copy of a to be assigned to something or used in order to actually perform the copy.
05:48:31.240 - 05:48:35.864, Speaker A: So I think the only places is assignment is something and external function call.
05:48:36.062 - 05:48:36.810, Speaker H: Yeah.
05:48:42.200 - 05:49:09.320, Speaker A: And then the kind of weird edge cases are, would ABI encode need a copy of, or would catch need a copy of? Because they are not really external function calls, but similar. Yeah. Maybe you want to check gitter. I extended the proposal.
05:49:18.000 - 05:49:21.180, Speaker H: Can you Chris go through it, or.
05:49:21.250 - 05:49:33.840, Speaker B: Alex, and can you also maybe put the proposal, if you like it in the hackmd? Because in the chat, like everything right in the chat will eventually be lost. So either put it in issues afterwards or in the hack mds.
05:49:37.690 - 05:49:50.234, Speaker A: We definitely need to move everything into issues. Hack MD is going to be lost. Can I mention something?
05:49:50.432 - 05:49:51.500, Speaker B: Yes, sure.
05:49:52.290 - 05:50:32.650, Speaker A: I think as you said, the semantics of assignments are very confusing from what I hear now, the semantics of this operator would also be quite confusing. So I can't say that as it was presented, it would be a great idea. I think making copies explicit is a good idea. We should find a way to do that. But I am worried about this accumulation of very confusing and unintuitive semantics of operators in the language.
05:50:34.030 - 05:50:39.340, Speaker H: Okay, cool. Then I think it's time to move on, because that relates to the next thing.
05:50:40.190 - 05:50:42.250, Speaker B: I also wanted to say something just to.
05:50:42.320 - 05:50:43.562, Speaker H: Okay, sorry, go on.
05:50:43.696 - 05:51:16.660, Speaker A: Yeah, I just wanted to point out that the weird semantic of the copy of operator might not be that much of a problem because we can just disallow it in any case where it might be confusing. So it will just be allowed to actually assign such an expression to something or use it in place of an argument or something, and then there's absolutely clear what it does. So we've just disallowed stray occurrences of copy of something because that doesn't make sense to have, and it's easy to detect those. Yeah, that's it.
05:51:19.030 - 05:51:20.580, Speaker H: Any comments on that?
05:51:21.930 - 05:51:27.160, Speaker A: Yeah, that could work. Cool.
05:51:28.410 - 05:51:55.070, Speaker H: So from this point, okay, it's good. Seems everyone likes the idea of explicitly having to explicitly point that out. So from this point, Alex himself had the idea to put this together with immutable and mutable, the immutable or mutable property for variables.
05:51:56.770 - 05:52:08.260, Speaker A: Maybe just to not add some more confusion. Again, please forget whatever you know about the new immutable feature. Yeah, this has not completely unrelated or might be completely unrelated. Please just ignore it. Yeah.
05:52:10.070 - 05:52:20.070, Speaker H: So just one example where a pure function acts weird because you have a mutable reference, because everything in solidity is mutable by default.
05:52:20.970 - 05:52:22.440, Speaker A: Are you doing this now?
05:52:23.770 - 05:52:25.740, Speaker H: Oh, you wanted to get from this one.
05:52:26.990 - 05:52:55.490, Speaker A: I think I'm just confused. But anyway, just one comment on the new immutable feature. So this one predates that. And I actually argued we shouldn't call it immutable because this, what we're going to discuss here is going to confuse people a lot if we have those immutable constants. So it's kind of a shame. Yeah.
05:52:55.560 - 05:53:01.540, Speaker H: Then I think Alex might then take over from here.
05:53:04.570 - 05:53:08.278, Speaker A: You can, if you want nothing, you.
05:53:08.284 - 05:53:11.560, Speaker H: Can go on if they mutable. It was your idea in the first place.
05:53:19.790 - 05:54:23.854, Speaker A: All right then this starts with this question. Are pure functions really pure? And they actually aren't, because memory references mean that you can change any memory. Even in pure functions we do enforce that storage cannot be changed and any other state modifying things cannot change, but memory can. And if you look at this issue, seven one five, do you want to go to the next page? In that issue we described an immutable or mutable keyword. And the issue might be confusing because we went through different versions over time. But here one option on the top is we could make, and maybe let's just focus on these arguments to pure functions. Let's only consider this use case for now.
05:54:23.854 - 05:55:23.600, Speaker A: So let's say that these non value types are mutable by default. So then we could say, and there's no keyword for mutable. So then we could say we introduced an immutable keyword, turning those arguments immutable. And then the compiler could check and say that a change of that memory would be a compiler error. And then we have the inverse, where we consider that these arguments are immutable by default. But we introduce a mutable keyword, and then if it's set to mutable, then the functions can change those memory references. So the question is, is this a problem to anyone? Is this a wanted feature? Is this an unwanted feature? Jocelyn, please go ahead.
05:55:23.600 - 05:56:15.798, Speaker A: Organing the initial comment. In this situation, should pure not be able to compile with mutable power methods? The issue was to have pure functions that cannot change any memory. Yeah, I guess the pure function, we weren't sure what should be the default. I mean, just the time to discuss it. Okay. Nick? Yeah? Can you currently modify storage in pure functions through assembly? I believe not, but unless there's an issue in the compiler. I was going to check it, but I figured I wouldn't waste time.
05:56:15.798 - 05:56:50.270, Speaker A: My question is, it's not really pure if you have to mark specific variables as immutable or mutable, because you could always just have an assembly block inside the pure function that modifies another part of memory, and I don't know if that's worth consideration. Is pure actually something we're discussing here? Because maybe. Well, maybe just a discussion. Yeah, I mean, if that's not the discussion, then I'll pipe down, but it seems related.
05:56:56.710 - 05:56:59.060, Speaker B: So Nicholas also wants to say something.
05:57:00.150 - 05:58:02.400, Speaker A: Yeah. So are you thinking of the immutable attribute just for reference types, but also VAR types, like integrates? Yeah, value types is the next slide. Okay, so the example says pure functions, but basically this would apply to any kind of function. It would be possible to ensure that the function doesn't mutate memory references or storage references. I guess this could turn into that there are functions which cannot modify the state, but directly, but they can make calls. So maybe, does that make any sense to have such a function? I guess this is to Chris, because you said it's related or not related to pure. Maybe it is actually related to pure.
05:58:02.400 - 05:59:34.000, Speaker A: I mean, pure only refers to contract state currently, and that's why pure function are actually pure. Just you have to say what you're talking about. Yeah, it's pure regarding state. So, did anybody run into issues with guess mutable memory and functions? No, I don't think so. Would anybody against making these immutable by default? So reference types to pure functions? Someone recently said that your solidity is way too verbose. Yeah, I'm not sure to understand the issue that this is trying to solve. And if you make the default immutable, might become reverb for a lot of cases.
05:59:34.000 - 06:01:01.544, Speaker A: Why would it be cumbersome? You can always mark it mutable. Yeah. I'm not sure to understand what is the issue here, which would require more verbality for the developer. Adding the mutable immutable feature itself would add a new keyword that you have to add to all the reference types. So the problem on this slide itself is that pure functions are only pure in terms of the state, but they're not pure in terms of memory. Yeah. Would it make sense to say mutable only applies to memory and then have immutable memory and mutable memory, and maybe some shorter keyword that combines that, but.
06:01:01.582 - 06:01:05.850, Speaker H: That kind of sounds like it's going toward complicated semantics again.
06:01:06.940 - 06:01:07.396, Speaker E: Martin.
06:01:07.428 - 06:01:08.120, Speaker B: Martin.
06:01:13.850 - 06:02:10.230, Speaker A: Oh, I'm muted there we go. Maybe this can also be done on the calling side of a pure function. So if one has the copy of syntax, for example, then the one can do call by value instead of call by reference whenever the function is pure. And then one has to do the marking of them, marking of the argument by the copy of keyword. Because I also find it kind of confusing that you would even have a pure function that doesn't have a return argument, because that's sort of a know up function. Unless. Yeah, I mean, maybe the question is more about how to understand the pure marker.
06:02:10.230 - 06:02:39.030, Speaker A: You're right. You're correct. The example is kind of incomplete. It should have had a return statement doing something, because, of course, this example is kind of unusable. In the case it's immutable. I think if there are pure functions, they should really be pure. So, intuitively, memory should not be mutable.
06:02:39.030 - 06:03:39.580, Speaker A: I don't understand the need to introduce keywords. How can they be pure if they mutate memory? Could we decouple the pure and mutable discussions? I mean, we can introduce a pure state and a pure, really pure also on memory thing, and split it even more. But this is not only about function parameters, right? It's also about local variables being mutable or immutable and so on. Yeah, local variables can be muted, as far as I can imagine. Yes. And we want to introduce a new feature to allow specifying that local variables cannot be modified, especially arrays and structs. So the values of them, I would say we can.
06:03:39.580 - 06:04:23.220, Speaker A: I liked Mr. Chico's suggestion that this could be done at the caller side. Maybe. I want to explore that idea a bit more, but maybe we could move to the next slide with the variables, and then we can come back. Yeah. So this is basically just inspired by Rust, because today every variable is mutable by default. And if you do have, like, an immutable or mutable keyword, we could consider applying them here as well.
06:04:23.220 - 06:04:43.980, Speaker A: So in this example, it's immutable by default, and we can mark them mutable. I guess the question is whether this is useful at all for anyone. And then we have a few other questions there.
06:04:51.800 - 06:05:13.770, Speaker B: Okay, so, Nick, comments, plus one for taking a page out of Rust's book and doing immutable by default. Any other thoughts?
06:05:24.740 - 06:05:43.780, Speaker A: So, combining this with the other thing, if we have an immutable array and call a function that takes a mutable array, we could use copy off to make it still work, right? Yeah, exactly.
06:05:43.850 - 06:05:52.630, Speaker H: I think that would be one of the effects that doing this, we could also make it very clear when there's a copy, when there's not.
06:05:54.360 - 06:06:17.102, Speaker A: Do, is.
06:06:17.156 - 06:06:22.320, Speaker H: Anyone against immutable by default for everything?
06:06:27.110 - 06:07:09.646, Speaker A: I think the bigger arguments against essentially most of these things are adding more keywords, more complexity to the language. I am not sure that it's necessary, and I can't say to be for or against this without knowing how this would affect the majority of programs and the majority of variables. I don't think this decision can be made without looking at the way that programmers write and how much this would affect them.
06:07:09.748 - 06:07:44.360, Speaker H: Yeah, it would require a little bit of a change in the way people write because I kind of disagree with the things you said in the sense of, of course, if you take all the code that's written right now and you do that, then of course you would just have to add mutable to everything, and then it would become strictly more verbose. But if you also change the way you write, then certain things might become less. Yeah, I guess it's my argument on it. I don't want to be too long on it.
06:07:46.350 - 06:07:47.210, Speaker B: Nick.
06:07:48.430 - 06:08:28.120, Speaker A: Yeah. The argument of looking at how code is written today doesn't seem to hold, because look at the way people wrote code before the Dow hack. Right. We're trying to build a language that is good for financial applications that do not change. We should consider safety. Yes, of course. What I meant was more, if there is an intention to introduce the mutable keyword, how do people use local variable? I guess you're right.
06:08:28.120 - 06:09:04.450, Speaker A: But one also has to do these changes gradually. I guess I'm a little shocked by the many different new keywords that are being proposed. Maybe it's just that. Yeah. But I certainly agree that we have to see how that, I mean, it doesn't necessarily mean that we just add mutable everywhere, but we have try out how contracts would look like with these features.
06:09:05.350 - 06:09:48.100, Speaker H: Yeah, I think the main point is that, sure, you would add a new keyword, mutable, which is supposed to be an exception. Right. It's not like we want people to write code the same way and then just have mutable everywhere, which is, of course, what would happen if you just translate code, adding this feature. Ideally, mutable would be an exception that you add sometimes, but the way you'd write code would be a certain way that you wouldn't need to write mutable, hopefully not making it more verbose by then adding a bunch of new variables. But again, I agree with both that we need to see how code would look like.
06:09:49.590 - 06:10:07.458, Speaker A: Yeah, exactly what you said. So if this keyword is introduced, what other features would be in place so that people don't essentially declare all variables mutable? Right. I guess that's what needs to be analyzed.
06:10:07.634 - 06:10:09.270, Speaker H: Yeah, completely agreed.
06:10:14.610 - 06:12:26.660, Speaker A: I'm not that big of a fan of the mutable keyword. I think simply because of verbosity, and it feels like you're already thinking in an imperative language, would just add extra keywords. So is the problem the verbosity and the number of keywords, or the length of the keyword, or all of these at the same time? Yeah, and I guess also I don't feel like there would be a high chance of this catching any critical issues. Would it be a sensible next step, maybe looking at some bigger contract and trying to apply these changes and see how the new version would look like and whether it could help in maybe identifying some issues or I guess we have to do that as the next step. I think before that it would be good to know what issues this would tackle more in abstract before going into a concrete contract, because I don't think that's clear. Like someone said just a minute ago, I think the problem that sparked this whole discussion was the fact that when you call a function internally and pass a memory array, then you have no guarantee at all about what happens to that memory array. And if you at least know that the function can modify the memory array, then yeah, it's a big win for thinking about matrix of the program.
06:12:43.800 - 06:12:44.164, Speaker G: Maybe.
06:12:44.202 - 06:14:35.510, Speaker A: Martin, a question to you regarding act. Would immutable inputs to pure functions, would that be any use to act or any kind of verification? Did you say immutable functions? No, pure functions or any functions which have immutable inputs, so they never going to modify any other state. Yeah, I think that's quite valuable. I mean, essentially they could be written as like helper functions that simplify the way that the post state looks like without requiring a separate behavior description because they're essentially a function. So that's one of the use cases we wanted to convey, but probably really badly. However, anyone who is looking at gitter, I posted a long example trying to take in a lot of these points from this discussion, including the copy, and maybe it's easier to understand some of these things using that example. Yeah, to be clear, I really like the keeping pure functions pure suggestion, but I think it has mainly to do with the calling semantics of call by value or call by reference.
06:14:35.510 - 06:15:24.540, Speaker A: But then how would you know, just looking at the pure function itself, that it's going to be pure? You have to look at the calling sites as well, right? So I would maybe enforce that. For pure functions, you need to call them by value, but we really want to have steady call as much as possible. Right. So we certainly need functions that can modify memory, but promise to not modify state. But those of you. Okay, you have a point there.
06:15:27.070 - 06:15:29.098, Speaker B: Daniel. You also wanted to say something.
06:15:29.184 - 06:15:30.300, Speaker A: I just did.
06:15:31.790 - 06:15:42.160, Speaker B: Okay, looking at the time, I guess it would be good if we could wrap this up in the next minute or so. Oh, Jocelyn also has a point. Still. Go for it.
06:15:43.410 - 06:16:37.668, Speaker A: Yeah, I think I'm just a bit lost around the, so like, the main issue here is that the pair function are not pair. Because of that, we want to introduce the immutable keyword, is that correct? No, we want to introduce the immutable keyword to have a promise that for internal calls, memory arrays are not modified. Okay, I would really like to decouple this from the pure thing. Okay, maybe one final comment, unless someone else. So I think we should be careful not to just blindly copy stuff from languages like rust. And one thing is, I mean, we already said we have to try that in code. And one thing that is different between rust and solidity is that solidity cannot do arbitrary internal function calls.
06:16:37.668 - 06:16:58.364, Speaker A: So recursion is a bit difficult, and it doesn't have, I don't know, fold or whatever. And because of that, some more value types need to be mutable than in rust. So, for example, I don't know, a regular for loop with an index variable, I that always needs mutable.
06:16:58.492 - 06:17:31.340, Speaker H: Yeah, that's exactly the example that I was thinking when I don't remember, sorry, who it was that said that maybe this also implies that some new features are also needed so that you don't just basically make everything mutable. And I think the set of functions that you just mentioned are probably part of this, because if you don't have fold map, all this kind of function, then you have to write a bunch of for loops, then you already need a mutable for the index or whatever variables you're using to iterate over the loop.
06:17:38.470 - 06:17:55.238, Speaker A: I guess the index would look quite ugly and making it, if you have immutable by default, marking it mutable might just look. Yeah, yeah, I think there's a lot to think about.
06:17:55.404 - 06:18:00.570, Speaker H: Maybe there's a motivation for Chris's discussion later on. Functional solidity.
06:18:06.030 - 06:18:13.120, Speaker A: Yeah, I think that's a good point to maybe just wrap up and continue it later on.
06:18:14.290 - 06:18:48.202, Speaker B: Okay, thanks, guys. Then I'd say let's wrap it up. And I see that you're still commenting on the GitHub so you can just keep on discussing it there. But I would also highly recommend you to take the time that we have allocated for the break to actually take a break. We have three or no, actually four interesting sessions coming up still after the break. And then the very first solidity summit is already closely or slowly coming to an end. We will reconvene here in 20 minutes.
06:18:48.202 - 06:35:45.618, Speaker B: That is at 08:05 german time. And until then, feel free to relax your eyes, get a drink, get something to eat, stretch your legs, do whatever. See you back in 20 minutes for the next discussion on modifier areas. And Michael, we can go into break mode with the stream. Uncle. Hello, everyone. I think we can slowly.
06:35:45.618 - 06:36:15.240, Speaker B: Mikael, we can slowly turn on the live stream again. Switch on Coolio. Yeah, I think we still have like a couple of seconds left, but I just wanted to be on time. Let me update this.
06:36:17.690 - 06:36:19.080, Speaker A: Now we're here.
06:36:20.670 - 06:36:31.280, Speaker B: Okay, so next up is actually Chris with an open discussion session on modifier areas. Chris, are you back from the break?
06:36:32.530 - 06:37:14.380, Speaker A: Yes, I'm back. I'm a bit distracted because I finally found someone who knows stuff about fixed point numbers but wasn't able to go to the session. So modify areas. Okay, let me find the presentation. Shall we start or wait a little bit?
06:37:14.390 - 06:37:19.130, Speaker B: Oh, yes, please. I thought you'd be getting started already. Go ahead.
06:37:26.500 - 06:37:49.940, Speaker A: Okay, good. This is a discussion session about something called modifiers. And the main idea is to change the way modifiers are applied to functions. And this kind of ties a little bit to what Leo was talking about earlier.
06:37:51.960 - 06:37:55.670, Speaker B: Just FYI. Chris, your camera seems to be black.
06:37:57.000 - 06:37:59.408, Speaker A: Might be just too dark. Let me turn on the lights.
06:37:59.504 - 06:38:00.150, Speaker B: No.
06:38:04.440 - 06:38:21.020, Speaker A: It'S just the privacy thingy, but it was also rather dark. Let me actually. Sorry. What can you see on that camera?
06:38:26.060 - 06:38:28.168, Speaker H: A lot of private keys.
06:38:28.344 - 06:39:16.200, Speaker A: Yeah, there are people protesting outside, but that shouldn't stop us. Okay. Yes. And here, see an example of how this could work. You have a function called regular, a modifier called only owner, just the regular only owner modifier. And then there's a directive using modifier only owner. And then inside that block, a bunch of function definitions.
06:39:16.200 - 06:40:32.080, Speaker A: And it would mean that the modifier or the comma separate list of modifiers, given there, are all applied to all these functions. And the benefit would be that you have a syntactic area, a scope of functions, where you are sure that all of these functions have the modifiers. So you could, for example, group all the admin functions into a modifier area that is the only owner area. And you could also implement something like a state machine could also be implemented in a way that is much more visible. What it happens because you have one modifier area for each state of the state machine, and all the functions that are available in that state are defined in that area. These are the benefits. Now onto the drawbacks.
06:40:32.080 - 06:41:16.156, Speaker A: The definition of the function and the modifier that is applied. Those might be rather far away from each other, so it's not directly visible anymore. Which modifier is applied to which function. You still have the indentation usually. So you at least see that some modifiers are applied to that function. And then if you have kind of crossing so overlapping concerns, for example, this is a bit difficult to do. So if you have two modifier areas that are not, that share some functions.
06:41:16.156 - 06:42:17.456, Speaker A: So admin functions that are available in state one, for example, and other admin functions that are available in state two, then this can't really be used because the modifier errors cannot overlap. You would have to either mention the functions twice and then modify errors could also lead to nesting. If you have again the case of admin functions and then states inside them, you could do that, but you would have to add indentation and it again could get more complicated. Yeah. Further ideas to make it even more complicated. The visibilities and state, how do we call it, state mutabilities. They could be seen as modifiers.
06:42:17.456 - 06:43:06.944, Speaker A: So you could have an area that has all public functions, for example. And then even completely different idea would be to use a statement like apply modifier. So apply modifier name to and then a list of functions. So this here you could say apply only owner two and then list of functions. So you would see all the admin functions that again has the drawback that it's easy to forget a function and that you have to search for the function to see which modifier are applied there. Yeah, and then the last idea, just leave everything as it is and try to use auditing tools that can do the grouping for you. So you can ask a tool.
06:43:06.944 - 06:43:16.870, Speaker A: Okay, what are all the functions that have the only owner modifier, and what are the functions that are missing the only owner modifier. And then you can inspect that.
06:43:18.120 - 06:43:20.500, Speaker B: So Francisco already has a comment.
06:43:20.660 - 06:43:56.864, Speaker A: Yep, I'm actually finished with the presentation. Cool. Thank you. So I'm not sure about modifier areas in particular, but I wanted to share one motivating use case, which is that today it's not possible to implement a full complete re entrance guard. And there may be similar cases like that. So you can have this mutex, but you need to apply the modifier that grabs locks that mutex to every function. And in some cases, like public state getters, it's not even possible to apply them.
06:43:56.864 - 06:45:05.598, Speaker A: So if one wanted to do that properly, you would need some way to apply a modifier to the entire contract, essentially like a piece of code that would run at the beginning. So that is something related or maybe extending on that. You would actually even like to apply the modifier to all functions in all derived contracts, right? Yeah, that was something I thought about. So that would kind of. So the mutex use case would hint towards something where we have apply modifier to a star, and this star means all functions, even those in derived contracts. Yeah, that would work. But it's interesting how this mutex use case always pops up for one example, and it's the only example where.
06:45:05.684 - 06:45:20.770, Speaker H: Yeah, that's what I was wondering. How many other cases are there that you need both sides of the modifier, so to say, like the pre and the post of the placeholder.
06:45:30.140 - 06:45:32.780, Speaker A: Do you see that related to modifier areas?
06:45:37.600 - 06:45:53.810, Speaker H: Not necessarily, but I guess modifier area could also be, as Chris said, like a visibility area, mutability area, preconditioned area, or these kind of things. But I don't want to derail the discussion. So.
06:45:58.740 - 06:46:04.180, Speaker A: I mean, would it make sense to have an invariant only apply to a set of functions and not to the whole contract?
06:46:05.000 - 06:46:23.070, Speaker H: I mean, I had a slide for that, but. Because it's going kind of fast. But the syntax would be like invariant something over and then the set of functions. This would be the invariant area. Just instead of scoping it inside, like grouping it inside, you would just give a list.
06:46:27.740 - 06:47:19.080, Speaker A: The mutex is also the only example that does something. The only modifier that does something after the function. Right? Not the only one, but it is one. What is the other example? Not off the top of my mind. Do you have other examples for which modifier areas, not to the whole contract, but to a set of functions is desirable. Like what motivated people to ask for that? Is it maybe because one presumes it would be harder to forget? Yeah, that's the idea. So you have a modifier area with all the owner functions, and then a modifier area or an area without a modifier for all the other functions.
06:47:19.080 - 06:47:56.576, Speaker A: These are the examples I could come up with. Not sure about which syntax are you proposing? The apply modifier or the area from like two slides ago? Me? Yeah, both. I think they're rather different. Yeah, sure they are. And I think this apply modifier. I don't think calling it like modifier area is like a good way because it's not an area anymore. Yeah.
06:47:56.576 - 06:48:58.170, Speaker A: Okay, then let's change the title of the talk to applying modifier in different ways. It was just confusing to me. I personally like this one a bit more when it's next to the modifier declaration. I suppose the areas, because with the areas we always ran into the issues. How do you have multiple areas? It will just become a mess of different groupings of functions which need different areas or different modifiers. Yeah, I mean, both of them have benefits and drawbacks. I kind of like the idea of having basically a danger zone of admin only functions just clearly marked.
06:48:58.170 - 06:49:10.210, Speaker A: Maybe that's like the main use case of it. Yeah.
06:49:20.860 - 06:49:35.070, Speaker B: There'S a comment on the chat. Another somewhat hacky example of a modifier that does something after the body is a gas profiling modifier, calls gas left before and after the body and emits an event with the difference.
06:49:40.020 - 06:49:41.410, Speaker H: That's a cool case.
06:49:45.300 - 06:50:22.770, Speaker A: I mean, also the invariants done manually are such modifiers, right? Yeah, it so this session was not something like, yeah, this is totally what we're going to do, but more like, hey, we had this idea, is it total crap or might it be useful for some people?
06:50:25.700 - 06:51:09.810, Speaker H: I think similar to some of the other things we talked about. To me, actually, the main concern is the overlapping areas. If it's actually a problem, if it's not a problem at all, and it's just something that I don't have enough data to have an opinion, it's just a point of concern to me. You would basically have to choose the modifier you care the most, and then the other ones. And then if you have overlapping ones, then put the least ranked ones like it is right now, and then choose one or two to be the bigger ones.
06:51:16.290 - 06:51:17.840, Speaker A: So I didn't get the end.
06:51:21.490 - 06:51:37.430, Speaker H: You would basically have to choose one or two to be the bigger ones, and if you have overlapping smaller ones, smaller in the sense of the size of the set of functions they apply to, for those, you'd have to place them the way it is now on the function definition.
06:51:38.010 - 06:51:38.760, Speaker A: Yeah.
06:51:40.650 - 06:51:53.690, Speaker H: But then again, I don't know if it is common that it would have such an overlapping situation. Maybe it is well separated in a way that it could group them.
06:52:00.630 - 06:53:00.946, Speaker A: This is kind of tangential, but this apply modifier syntax made me think of the issue with applying a modifier. I'm sorry. It used to be a very common pattern to inherit a contract to only add modifiers to a few functions from the parent contract. So, for example, if you have the possible contract you can inherit, which only provides the possible modifier, you can inherit ERC 20 and then apply that to a certain set of functions. With the introduction of overwrite in 0.6, that became really burdensome because you can have multiple paths and you need to specify all of the possible paths in the overwrite parentheses. So maybe the apply modifier syntax is a way to sidestep that issue, because you wouldn't have to write overwrite in there.
06:53:00.946 - 06:54:08.120, Speaker A: Sorry, I didn't get that. So you inherit from a contract that defines the modifier. Yes, and one of them defines the modifier. Or you can define it in the child contract, and you inherit from another contract that provides a set of functions, and you want to apply a modifier to the functions from the parent, but as long as the function is only defined in the one parent, then you don't need. There can be situations where there are inheritance issues. I mean, sorry, this is a tangent, but it is just something that would also be potentially solved by the apply modifier syntax. So maybe that, especially the version where you can apply modifiers to derived contracts, right? Yeah, but would you then apply the modifiers to all functions or just to a set? But then it's hard to reference them by name, right? Because in that scope they are probably not known or not visible.
06:54:08.120 - 06:55:02.640, Speaker A: There may be something there if they are linearized, though. Okay, this is too complex. We can leave this for later. By the way, this is also maybe in the other talk, but since we're talking about modifiers, is there a reason why modifiers have to be defined inside contracts? Or should we relax that to allow modifiers being defined at file level? They often use state variables, sure, and if they don't, yeah, maybe most of them do. But I mean, is there an argument against allowing them at file level?
06:55:16.090 - 06:55:30.250, Speaker B: There's another comment in the chat related to inheritance modifiers. One pain point I've encountered in using modifiers is that they can't easily be reused between libraries and regular contracts.
06:55:34.430 - 06:55:55.820, Speaker A: This seems to be related to the modifiers at file level, is it or not? Yeah, I would be interested to know which modifiers are reused between libraries and regular contracts. Or is it that.
06:55:59.010 - 06:56:05.550, Speaker B: Now he's asking, or would it be possible to allow modifiers to be exported from libraries?
06:56:08.990 - 06:57:26.426, Speaker A: So libraries cannot inherit, and that's why library functions cannot use inherited modifiers, is that the problem? But then these modifiers apply to library functions would not have access to state variables. Allowing modifiers at file level would be a solution to that, wouldn't it? You never know whether those people are in the YouTube stream or not. So how long do you have to wait for an answer? I wouldn't say it's a solution because there is no problem. I mean, if the problem is you can't use modifiers from libraries, then putting modifiers at file level is not really a solution. Why not? Well, because of the way the problem is stated, which is modifiers and libraries can't be used. I would tackle that as a problem in itself. Like why can't that be done? Are modifiers on library functions disallowed at some point? They weren't allowed.
06:57:26.426 - 06:58:14.600, Speaker A: I don't know if they are now. I don't think they are. Then I don't know what problem modifiers at file level would be solving. It would solve the problem that the same modifier can be used, can be shared in a contract and in a library. And currently you can share modifiers through inheritance, and inheritance doesn't work for libraries, so you can't share modifiers in a library. I think that's the problem. In that case you can define it in the library and then use it in a contract.
06:58:14.600 - 06:58:34.560, Speaker A: I don't think you can reference modifiers through a path. That's what I was referring. That doesn't work. Okay, maybe we can go back to modifier areas. Yep.
06:58:37.540 - 06:58:44.070, Speaker B: Nikki, can you stop sharing your screen? Because one screen is overpowering the other. Thanks.
06:58:52.610 - 06:59:21.350, Speaker A: So, any more comments on modifying areas? I think it would be nice to have a resolution to some of the proposals from Leo regarding properties or preconditions and relation to modifiers. I think it would be a better idea to do one without considering the other.
06:59:22.200 - 06:59:23.872, Speaker H: But that's also a bit orthogonal.
06:59:23.936 - 06:59:24.164, Speaker A: Right?
06:59:24.202 - 06:59:35.240, Speaker H: Because even if you decide to do some stuff using properties or preconditions that are currently done by modifiers, you could still try to do the same via areas.
06:59:35.580 - 06:59:43.848, Speaker A: Right, so are you saying that because initially you wanted to retire modifiers and only have the new options?
06:59:44.014 - 07:00:19.670, Speaker H: Yeah, that was my extreme goal. And I actually wanted to ask in the end of this discussion how much people would hate it if modifiers didn't exist. Modifiers, in the sense of being able to write things first with effect and second before and after the placeholder. So you would be able to only write preconditions or post conditions. How much would people hate that?
07:00:21.180 - 07:00:31.160, Speaker A: I mean, we don't have to remove modifiers, right? But I think it would be good to add this feature of condition.
07:00:32.060 - 07:00:36.430, Speaker H: Yeah, but then you have a bunch of different syntax to do the same thing.
07:00:38.720 - 07:00:50.050, Speaker A: So I would only introduce properties on top of modifiers if the goal is to see which one gets more use and we would remove the other.
07:00:50.900 - 07:00:53.650, Speaker H: Yeah, I'd be down for that.
07:00:59.350 - 07:01:04.310, Speaker B: Okay, nice. Anything else you want to discuss, Chris? In this session.
07:01:06.490 - 07:01:09.640, Speaker A: If there are no other comments? No.
07:01:13.770 - 07:01:18.680, Speaker B: I think we still have five minutes. Yeah.
07:01:24.330 - 07:01:57.254, Speaker A: And maybe in the next session there are some things we wanted to discuss. I didn't have time in the end. How is the session structured? Is it just a talk? Right? It's just a talk, yes, it's just a talk. Yeah. Lightning talk. I didn't think it's so much, but in the end it was quite crowded. But perhaps we can do it in the session also of Nick, because that's an open discussion and basically tackles similar things.
07:01:57.254 - 07:01:59.160, Speaker A: Right, right.
07:02:02.720 - 07:02:11.680, Speaker B: You can also, if you like, get started a little earlier and then we have a bit more time if we don't have anything else on modifier anymore.
07:02:12.500 - 07:02:29.700, Speaker A: If people only joined to see the talk, then it's perhaps not a good idea to start early. Drew, we could discuss license pragma quickly or just get some comments.
07:02:31.420 - 07:02:32.730, Speaker H: Yeah, do it.
07:02:35.100 - 07:03:18.390, Speaker A: Isn't that related to the package manager? Kind of. Kind of, but not really, because it's not about the package, it's about the source code. Yeah, we can also discuss other things. No, I'm fine discussing it. And we can come back on the package manager. I guess a five minutes is not enough to discuss the other preconditioned stuff from Leo. You wanted to come to a resolution here, and now I see probably that's impossible.
07:03:18.390 - 07:04:40.584, Speaker A: I think what we need to do is get more specific proposals and then ask yes or no that we could do for the licensed pragma right now. Yeah. Okay, so let's introduce a pragma called pragma license, and then an identifier that is taken from a list of open source licenses, or can be the special word closed. And this defines the license for the source file the pragma is given in. Of course, the compiler has no way to check whether that is the correct license, but it gives an indication and would help source code repositories getting onto a bit more firm legal ground. For example, etherscan asks you to select a license whenever you run source verify, run the source verification. And yeah, the compiler would warn if no license is specified and would assume it's closed source and at a later time.
07:04:40.584 - 07:05:36.926, Speaker A: Maybe we could also add a feature to the compiler that checks whether all licenses of all files that are sent to the compiler are compatible with each other, how would you determine compatibility? I don't know. At a later point in time, maybe. There are some websites which claim to explain compatibility between them, but you would need to be a lawyer to do that. Then we don't add that feature. I mean, one feedback from me, I would have said it's kind of a bad idea because it belongs more to the package. But I think you do have a point with the verification of single files. And also I don't think it belongs to the package.
07:05:36.926 - 07:06:21.730, Speaker A: I mean, it applies to a source file usually, and not to a whole package. Of course the package could have source file specific licenses, but why not just put them in? I mean, usually a license requires you to put a special comment at the top of the source file anyway, right? And why not just with a pragma or an addition to a pragm? But this won't replace that. Yeah, yeah, sure, but it would be machine readable. That's the thing, Nicholas. Actually, one more comment on that. The SPDX, which we discussed for the license names, that also specifies a way of a special comment which can be parsed by tools. So maybe that's another option instead of the pragma.
07:06:21.730 - 07:06:48.650, Speaker A: Yeah. To me, the compiler refusing to compile source code because it thinks licenses that much sounds frankly insane. I don't see why this could be done. It would just be a warning, of course. Okay. I don't see why this couldn't be done with a comment. Yeah, I don't see whether we need to change any code in the compiler at all, given that there's much more pressing needs on the language.
07:06:48.650 - 07:07:00.240, Speaker A: Comments have served this purpose just fine for years and years. It's a ten minute change, I would say, but yeah.
07:07:02.370 - 07:07:22.210, Speaker B: Okay, now we managed to spend the time, and it's 835. So next session will be by leaky, who will give a talk on verify all the sources. And you are muted.
07:07:25.190 - 07:08:03.300, Speaker A: Hello everyone. Now and unmuted, right? Wonderful. Hello everyone. I want to talk about source verification first thing, the motivation, like why do we need to do that? Life is hard enough in this times with COVID and stuff, so why now? This whole source verification thing, it adds important context to make a signing decision. The thing is, currently what we often see is basically yolo signing. Because what you see here is basically not meant for humans, that's meant for machines. But often we just have to deal with that, because currently there's not a lot of options to avoid that.
07:08:03.300 - 07:08:36.774, Speaker A: But bytecode metadata is already a feature of solidity since 0.4.7 which is like ages ago 2016 released at that point. And it's basically Seabore included metadata hash and a solidity compiler version. What we are most interested in is the metadata hash. The problem with that is that existed for a long time, but often it was not accessible. Basically it's either an IPFs URL or a swarm URL. Swarm URL is in the beginning.
07:08:36.774 - 07:09:04.750, Speaker A: Now there's also ipfs. But the problem is people didn't really publish it or had the option to pin it basically. So things were lost. So when I wanted to access it from a wallet, for example, I couldn't resolve that anymore. That's basically what we want to fix. So how can we publish metadata? One wonderful thing now is that remix edit is wonderful. Published to ipfs when you did deploy.
07:09:04.750 - 07:09:39.520, Speaker A: And I just want to do that in between and I want to do that that we later see. It's like fresh data. I take a random word from the Internet and hope that it's nothing bad. That seems to be okay. We don't get censored by YouTube. So I added here I compile this contract, compile it and deploy it and publish to IPFS metamask, confirm everything's wonderful. And now we want to see basically what happens under the hood when we do that.
07:09:39.520 - 07:10:07.494, Speaker A: So that was the simple life we had before. Basically we had a deployer that's often like you. And we have this blockchain that's where we deploy to. And now things get a bit more complex. But with most of the complexity you don't have to deal, basically that's what we deal with basically. Now the deployer doesn't only deploy to the blockchain, but also publishes to swarm or ipfs. And if that is done, the monitor is picking it up.
07:10:07.494 - 07:10:38.126, Speaker A: Basically a monitor monitors if there's new contract creations on the chains, then gets the data, if it's available from swarm ipfs, verifies it, index it and pins it. That is important. Basically that solves the availability problem that we had before. And then there is one more thing. It's basically a manual input for the verifier that's mainly for old contracts, basically for historical ones that you can do them. And that's looking like that. Basically you find that currently on verification computing.org
07:10:38.126 - 07:11:04.234, Speaker A: and can verify it there. You have to specify the network and specify the address. Drag the files in there and you're done. Or even easier now also with the remix plugin, basically there's now the source verify plugin and then in the source verify. Basically you see also specified network and then the files already selected. So it's a bit easier. And then you can also publish that.
07:11:04.234 - 07:11:40.934, Speaker A: But you don't need to do that usually because when you just click deploy to ffs that's already done. So how can we now access this metadata? One thing is you can access it via ipns. We publish it via ipns. A problem with that is ipns is currently very slow. There's a bug open for that for a long time. Happily, like with go ipfs which was just released this week, resolving ipfs records should simply get faster. I tried it out, it's not yet.
07:11:40.934 - 07:12:13.694, Speaker A: But the ipfs maintainers say more people need to migrate to IPFS 50 to have that change basically take effect. And there are tests that they did basically that they are also in the real world. So all of you who run ipfs nodes please update. Then that gets faster. Another version is basically to access it via ens. The only problem with that is basically it's slower in updating because you have to pay transaction fees to do that. But resolution is much faster.
07:12:13.694 - 07:12:47.934, Speaker A: And we can get more decentralization that way because with ipns basically you just have one key. With ens you can have multiple keys and you can also access it via normal htps. But we want to have it decentralized. So that's basically the fallback option. So how can we use this metadata? First of all, now there is a warning, like involved. I do a warning if there is no metadata to force people to do that. And I hope other wallets also do that in the future.
07:12:47.934 - 07:13:20.790, Speaker A: So that's basically the negative thing. But the positive thing is if we do that we can use for example nutspec for example here you see now votes for proposal 23 with a score of five. And I want to show that basically with the contract we deployed before and here you see that it's confirmed on ether scan and the metadata is deployed. So now our verification server should have picked it up. So I have to copy basically the URL, then start the emulator.
07:13:24.830 - 07:13:25.386, Speaker G: And then.
07:13:25.408 - 07:13:48.586, Speaker A: We make a transaction to this address. Did I copy it's. And now we see we can add an action. Basically we just have one function here. So we will just have one action. And we get this warning because we don't have a fallback function. Basically we get the warning.
07:13:48.586 - 07:14:25.260, Speaker A: We cannot estimate the gas. So now we have this basically score and proposal. And let's say we give it a score of eight because it's 08:00 and 42 is always a good number. We do that and now you see vote for proposal 42 with a score of eight. And the other cool thing you can then get is basically it's just enabled for developers. So you have in the settings to set the urdeveloper, you can also directly access the source code, which I find like I want to see more source code of the contracts I'm interacting with. So that's basically that.
07:14:25.260 - 07:15:38.322, Speaker A: Another cool thing that you then can do with the source verifier plugin basically is to load the contracts of an address into remix directly and play with them. And also what is really cool new feature that basically you can debug directly a transaction and it downloads the source code. I also want to show that. So basically this is the doesn't it? And you have to use another profile for that because otherwise it would be quite pointless profile that's here. And then we have to select Yan wants to fix that because then we can also do that basically from like inside wallet directly debug from there. But currently that's not possible because we need a provider. That's why we need to basically switch to injected web3 here.
07:15:38.322 - 07:16:13.482, Speaker A: Then we can debug and then enter the transaction hash. Then it's downloading the sources here and recompiling and you have the sources here. Wonderful, that's working fine. So the next thing basically we can do, basically that's more the run part of the presentation or what we need to really improve. Currently there exists nut spec and there exists rut spec. Both have spec in their name, but both are not really specked. Like there is no good specification.
07:16:13.482 - 07:17:19.362, Speaker A: Like nutspec basically says use javascript but really lacks also for the information, for example like how to access things in the contract. With that there's not much specification and rutspec only is a JavaScript implementation currently and there's a third party documentation, but it's basically abandoned. And I did collect some matrix, basically about what is currently used from nutspec and rutspec. You can find that on contract matrix at computing. The good thing is because it's not much used currently, people also don't use all the features and that might be a thing we could discuss in the session also afterwards if Nick wants, perhaps that could also then be part the specification that we basically make a simple specification first and then extend it. And also because it could be nice that I have part of the package manager to specify what is currently actually used because I have to guess if it is rutspec or nutspec with rutspec I can see if certain keywords are used. I know that it's ratspeck, but it's currently a guessing game.
07:17:19.362 - 07:17:58.986, Speaker A: Should be part of the metadata in the end I think. But that could be part of East PM Nick will talk directly afterwards about it. It's a nice project which basically standardizes metadata. And now that will come together with the source verification and be a standard together. There are some open tasks still to do. What is quite important is basically that we add the whole thing not only to remix as the IDe, but also basically to all the CI pipelines that depublishing is done automatically or automatically. So basically in truffle please do the plugins.
07:17:58.986 - 07:18:17.830, Speaker A: That would be really nice. Basically that should be all automatic adapt node integration would be really nice. And get more wallets supported. And I hope also more wallets. Then do basically this warning and warn their users. And I think it's their duty to warn the users that they interact with an unverified contract. Because that is fucking dangerous.
07:18:17.830 - 07:18:39.662, Speaker A: And we need to decentralize more aspects. We tried to decentralize a little bit more before there was ether scan. I think we went a step further than ether scan now. But there is still a little bit of work to do. We currently little bit of centralization is still left and we need to get rid of that. We have ideas for that, but we need to go step by step. That's the thing how you could really help.
07:18:39.662 - 07:19:25.898, Speaker A: It's basically for example if you have a DAP node install this IPFS pinner and pin verification that basically we solve this availability problem that when we go down that we are not the only ones pinning this content. Please pin this content like for example with the APFS Pinner or with your own scripts. That would also work. Then what we really should do with the ENS thing. Currently we do basically call content hash directly on the resolver. But it would be much nicer if we generate a DAO. Basically that just we propose the new content and then have some strategy like some people in an organization that need to propose it and agree on the new content and they all verify that's important.
07:19:25.898 - 07:20:02.306, Speaker A: Basically that they all verify the content, recompile the contracts and see that it's valid and then publish it to Ans that the trust is less. Because currently we could publish badly verified contracts but then you could see it in the end, but not on all, for example mobile wallets. You don't want to recompile all of them. There are also some open challenges that not only targets is a bit more harder. Basically branding and awareness. We need to raise more awareness for that. And also currently we just call it verification computing.
07:20:02.306 - 07:20:27.722, Speaker A: But that's more like there was no name in between. If you have a nice name or have branding ideas, that would be really nice. Hardware wallet. You have a logo now that's really good since last week logo, very good. Hardware wallet support would be really nice. But it's challenging because there is the air gap between the hardware wallet. There are some ideas basically that you load and sign the content to the hardware wallet regularly.
07:20:27.722 - 07:21:00.300, Speaker A: For example, the tracer Model T has an SD card. There are ways, but that's a bit harder thing. Localization is a big issue and that basically also ties into mutability. Basically the metadata you want to publish is immutable. But what we could do is basically links to where you could get your translations or where you could get updates. Because like interfaces, sometimes you just want to update. It's complex and we need to find strategies to do that without losing the guarantees we create here.
07:21:00.300 - 07:21:45.842, Speaker A: Credits like who are basically we like. Christian Reisener is the main driving force on that. Thanks for that, Christopher and Eddie, wonderful work. Mark also and the remix team big up because they integrated that already very fast and wonderfully to test that out. You can also join like we have a source verify repo. Just join there and grab some issues, create some new ones, try it out or close issues and submit pull requests. As always, we are happy to see that and join the conversation like we have this, you know, now through the summit basically about the GitHub.
07:21:45.842 - 07:22:00.222, Speaker A: Join us on GitHub. We are on ethereum source verify. We also have a video conferencing each Monday. So perhaps ask us to also join that if you want to contribute something. And that's basically it for me. Do we have time for questions or five minutes?
07:22:00.276 - 07:22:08.160, Speaker B: I think yes. Thank you, Ligy. I would be very interested to learn more about the logo. I didn't know you have a logo yet.
07:22:08.790 - 07:22:17.010, Speaker A: Yeah, I had to do all the screenshots newly, basically because the logo just appeared there. It says source verify.
07:22:17.350 - 07:22:18.100, Speaker B: Okay.
07:22:18.790 - 07:22:40.634, Speaker A: And I think as nasty as it sounds, but it's quite important basically that people know about that and that there is a little bit of branding and a little bit people are aware that we basically overcome this issue, that nobody really cares for that currently because you basically need the stamp of this thing is verified, that you basically get a bad feeling if you interact with an unverified contract. And you really should.
07:22:40.672 - 07:22:43.740, Speaker B: Yeah, I like the term yolo signing a.
07:22:46.430 - 07:22:47.870, Speaker A: It's brilliant.
07:22:49.250 - 07:22:55.982, Speaker B: All right, so please, if you have any questions in the room, raise your hand now and use the raise your hand feature. Yeah.
07:22:56.036 - 07:23:24.698, Speaker A: Mr. Chico, I'm wondering if you have attempted to crawl Etherscan and get all of the sources from there. For example, the contract matrix is doing that. But there comes this legal problem that Christian mentioned. It's not really like we could import. It would be really easy. Like I have it locally here because I look up some things but I just want to import it into that.
07:23:24.698 - 07:23:59.490, Speaker A: That has basically two reasons. A, this legal thing and b, etherscan also injects things in the metadata and then we cannot really recompile things, but also maybe legal thing. I want to stay legal. I tried recompiling stuff from ether scan but I failed for all the contracts. And the interesting thing is ether scan says fully 100% match. Actually I think the website says 100% match, but it's never a 100% match because they ignore the metadata hash. They never compare them with metadata hash.
07:23:59.490 - 07:24:28.140, Speaker A: The cool thing about the metadata hash is it's an hash anchor that basically anchors the actual source that was used to compile the binary that was then deployed. And for etherscan you can add arbitrary comments, you can rename variables and so on and will still say 100% match. This is not the case here.
07:24:28.910 - 07:24:33.120, Speaker B: Okay, we have a couple of more questions. Leo was next.
07:24:33.650 - 07:24:38.480, Speaker H: I just wanted to say that the logo looks like the solidity and the Viper logos together.
07:24:40.710 - 07:24:46.820, Speaker A: Might be. That is the case. I don't know. But it's also Sv. So it also.
07:24:49.190 - 07:24:53.606, Speaker B: Then we have. Richard, just wanted to ask.
07:24:53.708 - 07:25:20.326, Speaker A: The last time I checked this metadata file also contained to some extent my local pass or sometimes. Actually it was not always consistent. Sometimes it was like an absolute pass. Sometimes was a relative pass. But this kind of made it for me impossible to verify the contract that I uploaded before from a computer, that I reset it because I don't have the same pass anymore. I don't have the setup from the time anymore. But it actually requires me to.
07:25:20.326 - 07:25:45.870, Speaker A: That's why for me to do it for contracts in the past is really hard. Like if I now upload a contract, I can do it now. This is not possible. If you have the metadata file, then the paths are all there. You just need the metadata file and then, so the compiler doesn't care about the path of the file. The compiler just takes the path, you send it and uses that. So it's always about how you call the compiler.
07:25:45.870 - 07:26:21.680, Speaker A: Yeah, but I need to have the metadata file to create the same hash again. Right. Like if I don't know what was my, I don't have the metadata file. I have the exact source from this point, but I don't have the metadata file which has some nuances that I cannot reproduce and therefore I cannot get it to verify from the old way. I know that's an issue, but I mean, this is not just about the paths. It's about any settings or whatever, right? No, it's true. I think the path was, for me the one that was the most obvious one that you see the first something that happens there.
07:26:21.680 - 07:26:39.506, Speaker A: That's why we really need to get it in all the CI pipelines that you don't have to do it retrospectively. I think it's just important for really often used contracts that we might be able to verify them again. But ideally it's automatically then. It's even easier than with esoscan. You don't have to checkboxes.
07:26:39.538 - 07:26:39.990, Speaker G: You just.
07:26:40.060 - 07:27:00.910, Speaker A: It's part of your pipeline. You just agree to somewhere. The only problem that, for example, truffle had, they didn't know. They didn't want to do it by default because some might not want to publish their source code. I don't know. And big thumbs up to truffle. They store the metadata file, but they also include the full path, which is kind of a little downside.
07:27:01.490 - 07:27:04.670, Speaker B: Okay, we have more questions. Next up is Alex.
07:27:07.010 - 07:28:07.090, Speaker A: It was more like a comment on when Chris said with the ether scan they have been removing the metadata for quite a bit, I think ever since it was introduced because we also had like the experimental flag in it and it was just causing confusion. But I've been recently at least I verified one contract on Etherscan, I think after ECC. And I just grabbed the sources from the repo and verified it myself. It wasn't my contract and realized just after doing it, I mean, I talked to the author, but I realized after just after doing it that I should have added a comment into the source because it wouldn't have detected it and I could have said anything I wanted there and this wouldn't be the case if the metadata is considered. So it wouldn't be the case for source verify.
07:28:09.430 - 07:28:12.550, Speaker B: Okay. And another question from Lucas.
07:28:13.050 - 07:28:53.730, Speaker A: Okay, my question is, isn't having a verified source without the metadata better than having no verified source at all? So isn't there some value in this? Because at least I know that this source code generates the exact same bytecode. Sure. The hope is that just publishing the metadata and storing it is just part of the deploy workflow. No, I totally get that, but I see some value in making for most.
07:28:53.800 - 07:28:55.250, Speaker G: Of the legacy code.
07:28:55.400 - 07:30:00.070, Speaker A: We're actually also thinking about adding source code to this repository that does not match the metadata but everything else. But then the problem is, what is the correct thing? What if someone else uploads another code that also compiles to the same bytecode? Do you store both of them or just the first one? Or what do you do? Right, yeah, but at least one source code that compiles to the same bytecode is definitely helpful. Yeah, sure, because all the old contracts are still interesting to look at and having them not in there just because we don't have the metadata doesn't make sense to me. Yeah, this is the exact problem Etherscan has, because whoever verifies it first is able to introduce comments as they wish. And then they do have the option to reach out to Etherscan because it's an authority on this to replace the code. But if you decide to only store the first one, then it's up to whoever does it first to make the source look like as they wish, as long as it compiles to the same binary.
07:30:02.970 - 07:30:25.120, Speaker B: Okay, there's one more comment from Cairo on GitHub and then let's move on to the next session after that. Cairo says, the problem is I do not want the source code of a project in testing to become public while we do testing on, E. G. Robson, the problem is once we go to Mainnet, we may be fine with the source being public right away.
07:30:26.210 - 07:30:54.450, Speaker A: Yeah, don't be afraid to publish software like that. It's the same problem on GitHub. Often people are just too afraid to publish something like just look at others and it's no shame to publish that. Why not? I don't see a reason why not publish it. Publish early, publish often, because otherwise often you have the problem. People say, yeah, we will publish it later, but then people can look at it later and tell you later that there's a problem. Yeah, I don't think that's a problem.
07:30:54.450 - 07:30:59.530, Speaker A: Publish as soon as the first line is there. But that's my personal opinion.
07:31:00.270 - 07:31:03.500, Speaker B: Okay. Nick from Truffle also wants to say something.
07:31:03.870 - 07:31:22.260, Speaker A: Yeah, I don't mean to keep this conversation going too long, but we see a lot of people that use truffle that are working with closed source projects and so they don't want to publish ever, let alone not right away. Go to hell. Sorry, I mean, tell me about it.
07:31:23.110 - 07:31:38.120, Speaker B: Okay. With that being said, let's move on to the next discussion round, which is connected anyways, it's ESPM equals metadata moderated or introduced by Nick. Nick, are you there?
07:31:42.330 - 07:31:43.514, Speaker A: Yeah, hello.
07:31:43.632 - 07:31:44.202, Speaker B: Hi.
07:31:44.336 - 07:31:45.740, Speaker A: Hey, how's it going?
07:31:46.270 - 07:31:47.642, Speaker C: Good. How are you?
07:31:47.776 - 07:32:01.680, Speaker A: I am great. Share my screenplay. Okay. Is it good?
07:32:02.370 - 07:32:05.120, Speaker B: What happened to you? Something happened.
07:32:05.490 - 07:32:55.342, Speaker A: I cut off all my hair, locked inside for too long. My beard was huge. Cool. So I'm here to talk about etHPM and a lot of this piggybacks off what Ligy was talking about with the source verify project, how we kind of are attempt to find like a mutually compatible data structure for both of our projects. I'll probably start by giving just a little history of ethPm, how it works, where we are, where we're going, and try to open up for discussion as soon as possible. EtHPM is a decentralized package manager. We're just trying to do what package managers in other languages do.
07:32:55.342 - 07:33:28.650, Speaker A: For Ethereum, there are two main components. One is a JSON schema. So essentially every EtHPM package is just a JSON object, and the schema defines how you arrange your contract assets and standardizes it. And the second component is an on chain registry specification. This is just a read and write API for your onchain registry. So this is a pretty simplified version. But just to give you the full workflow, you start with a smart contract.
07:33:28.650 - 07:34:31.470, Speaker A: Then you'd create a JSON object that represents those contract assets. You publish that JSON file to any content addressable file system, mostly ipfs, and then you would write these three pieces of data to an on chain registry, the package name, the package version, and the content address URI, and that constitutes a package release. And inside a package you can also include any number of contracts and any number of on chain deployments. It's really up to the package author to choose what they think is important to include in the package. And so we also have a defined URI scheme. So with this one string you can plug that into any tooling or framework that supports ethpm. And now that tooling will framework will go to the blockchain, look up the URI, pull down the JSON object, and expose the contract assets in whatever way is useful.
07:34:31.470 - 07:35:10.118, Speaker A: EtHPM started in 2016 ish. It was a joint effort between Piper Merriam from the Ethereum foundation and Tim Coulter from Truffle. One important thing to keep in mind is v. One had a single on chain package registry. This was decided not to be great. It's a lot of maintenance overhead and introduces some security assumptions that weren't ideal, but also it made the developer experience a little better. And so far, to date, EPM V one has had much more usage than v two, and this is mainly because of truffle.
07:35:10.118 - 07:36:11.230, Speaker A: Truffle has integrated it very nicely into its workflow, so it's easy to use. EthPM V two, which is the current version of today, broke the single onchain registry model, and now we have federated registries. So if you're a package author and you want to publish your packages, you're responsible for deploying your own on chain registry where only authorized parties are allowed to release packages. So far, to date we've got a pretty wide range of tooling. We have a CLI, we have a remix plugin, native support in brownie, JavaScript, Python libraries, a web explorer. But Truffle is still on v one and we haven't seen great usage or like a large amount of v two packages floating around. Although V two support for Truffle is in the pipeline and should be available in the next major bump version bump.
07:36:11.230 - 07:37:28.790, Speaker A: But yeah, so this is just to say that developer experience has been really important in adoption for ethPm. Without the nice integrated truffle workflow, we kind of had this chicken egg problem. Protocols and companies package author or smart contract writers don't hear demand to publish EtHPM packages, and smart contract devs are happy to copy and paste code from GitHub to build off of other people's code. And so on the right is just some of the top level keys in the JSON schema. And so etpMV three, the conversation started in the source verify gitter channel, trying to find a data structure, a standardized JSON schema that can work for both of our uses. And this is just really nice because it was very important as like it's proven very important for package management for it to be as simple as possible for developers. Otherwise we'll just copy and paste our code now with native support in the compiler, we'll just automatically seed the ecosystem with a ton of packages.
07:37:28.790 - 07:38:33.990, Speaker A: And hopefully this spurs adoption and better software development practices rather than copying and pasting code. We've had some productive workshops over the past month. We're quite close, I'd say, to finalizing the v three spec. There's still some technical edge cases to iron out, but yeah, so once we iron those out, the next step would be to create an EIP and then work on implementation. And yeah, this is essentially the workflow that I envision, or like one instance of it where a protocol or auditor would have this widget in their GitHub and you just click it to copy and paste the URi and then you can plug it into any framework or tooling that you want. It's a really simple spec to implement, it's just json. So NPM is used a lot, but that's only for Javascript tooling, which isn't ideal.
07:38:33.990 - 07:39:15.820, Speaker A: And NPM also comes with some security mean. That's the background I would like to open up for discussion. Here's some important links. I mean, you can find everything from ethpm.com. The Ethereum magicians working group is where most of the discussion is happening. And so yeah, if anyone has any questions it seems like we have a shortage of packages in Ethereum. Openzeppelin has done a phenomenal job of building the standard library, but standard libraries only get you so far.
07:39:15.820 - 07:39:37.550, Speaker A: And so if anyone has any thoughts on the developer workflow, it'd be interesting to hear those why, is this something you want to use? Would you find this useful? And then if not, we can move on into the technical details of the implementation.
07:39:45.270 - 07:39:49.860, Speaker B: Do we have any questions or like comments on that in the room already?
07:39:52.090 - 07:41:14.560, Speaker A: Yeah, I have a question here. I want to know, how coupled are you building the workflow for consuming EFPM packages with ipfs as a backend? Like for example, would it be possible to have the user install an EFPM json manifest from GitHub using the tooling that is being built by truffle? Yeah, sure. So the only requirement is, well, I guess the three main backends that we're supporting are ipfs, swarm and GitHub Blob URis. The only requirement is that there's a content hash in the URI that can't verify the authenticity of whatever contents are pulled from GitHub. So the GitHub blob Uris have that content hash, and there are some packages out there that use it. It's up to truffle whether or not they want to support it, although the ethPM JavaScript library supports it, so it should be pretty trivial for truffle to support it, I think. So you're not restricted to ipfs, but it's definitely the most popular one.
07:41:14.560 - 07:41:35.750, Speaker A: And we also have kind of piggybacking off of the data availability problem that source verify has. EthPM has a pinner running, it listens to events. Whenever a package is released, it scrapes it for any ipfs files and it'll pin those to try and help with the data availability.
07:41:41.130 - 07:41:53.210, Speaker B: Okay, then there's a little discussion on GitHub between Alex and Chris. We discussed years ago with ESPM to consider the metadata. Why does it take so long? Wink from Alex.
07:41:55.490 - 07:42:21.720, Speaker A: I don't know. Let's see. V two happened in 2018. My first knowledge of this discussion was when I was talking with Chris about it maybe two months ago. So maybe I dropped it. I'm not sure where the ball ended up, but it's happening now.
07:42:22.570 - 07:42:25.990, Speaker B: Okay, then. Nick from truffle.
07:42:28.650 - 07:42:52.734, Speaker A: Hi. Yeah, I have a question. It seems to me that maybe this isn't going to end with the question mark. It seems to me what is like, the metadata is different from a package description. How do we reconcile that? Maybe we should go over the spec. Yeah, that sounds like a good idea. So we kind of found a way to.
07:42:52.734 - 07:43:22.440, Speaker A: Yeah, it is different. And I think we found a good way to find basically two different views on the same format. And I think it would still work fine for both use cases. Sure. Yeah. So the specification is here. So, yeah, the metadata object is a subset of a package object.
07:43:22.440 - 07:43:45.440, Speaker A: There are still some edge cases that we need to iron out, but I don't think this is the best time. Maybe that'll happen in our weekly call next time. I hope that this would be the time. Okay. The example. And then I think that's easy to see at the very end. I think there's an.
07:43:45.440 - 07:44:20.010, Speaker A: Yeah, so, okay, so, yeah, Chris, this is what I was talking with you about on our last call. The idea is that a source identifier is unrestricted. It can contain any character. However, we use colons to parse. So here we go. Source has a URL as its source identifier, which is fine. And then for a contract type to identify the source file it originates from, we use another colon.
07:44:20.010 - 07:45:05.066, Speaker A: So you have a prefix of the source identifier and a suffix of the contract type name. And this is fine because you can parse it and pull out the last colon and use that as the divider. However, in ethPM, I didn't bring this up in our last chat, but you can also identify contract types from dependencies. So this would be a different package. Say this is package like XYZ. It has a dependency on package ABC. So to identify a contract type from ABC, you start with the dependency name.
07:45:05.066 - 07:45:31.890, Speaker A: Then you would have to put in the source identifier and then the contract name. And now it becomes trickier. What is a contract type? A contract type is a data object. We have defined the difference between deployed contract and contract. Sure. Instance and contract. Yeah, a contract type contains the ABI.
07:45:31.890 - 07:47:09.760, Speaker A: It can contain the bytecode and the natural specification. Devdoc user doc so about the dependencies, I'm not sure. How do you send these dependencies to compiler? Isn't the source identifier a globally unique identifier? Okay, we could enforce that. We could enforce binary dependencies that are not part of the source code. Yeah, I think this comes back to what Nick is talking about is like there's a lot of edge cases in trying to find a nice compatibility between a package specification that is tailored towards package management and this metadata compiler motivation. One way I see around this would be just adding a field to the contract type object that contains the source id rather than having this prefix. Then there's no parsing issues because currently in eTHPM, so this contract type identifies a contract type from a dependency.
07:47:09.760 - 07:48:04.304, Speaker A: But you could imagine this gets nested. Or you can reference one in a couple of levels deep and it would become quite messy at that point. What are contract dependencies or what are build dependencies? These are two separate packages. This is package. On top is package ABC, and on the bottom is package XYZ. For this deployment, which would be an on chain instance, it refers to a contract type, and that contract type is not required to be present in package XYZ. So it just can be built on top of package ABC and it can just refer to a contract type in its dependency.
07:48:04.304 - 07:49:18.812, Speaker A: So if you wanted to pluck the ABI for the contract type, you'd have to dig down into the dependency and be able to identify the representative contract type. We can use different separators, like, I mean, JSOn structured separators, like make it an array of strings or whatever. Ah, I think that is solvable. Yeah, that would actually. So are we trying to come up with an agreement in regards to what changes in the metadata or what changes in both formats will mean, at least for the metadata? We have a pretty solid proposal already. Yeah, and there's also like, Nick, I'm not sure if Ben from Brownie and Viper is here, but we also need to keep in mind the frameworks and try and find a nice suitable data structure for everybody. Sorry, go ahead.
07:49:18.812 - 07:50:04.520, Speaker A: Do you have a link to this proposal? Yeah. So ethpm.com is the easiest place to find it all. Our GitHub repo has a machine readable schema of version two and version three. It also has example packages for both versions. We have documentation which you can find from the repo, but it can all be found from ethpm. Can you show the documentation on the very bottom there was this example metadata, right? Wasn't it? Not sure.
07:50:04.520 - 07:50:52.490, Speaker A: So I had this repo open and I had read the docs or anyway the spec open, but you made it sound like that there's like a proposal for changing the metadata in solidity. And is that written down somewhere? Not from the solidity side, although I'm not sure if they're everything. Right? Yeah. This is the combination of the boat of changes to both ethPM and solidity. Solidity. Off the top of my head. Oh, I think let's try.
07:50:52.490 - 07:51:32.388, Speaker A: This might have been what you're talking about. Yes. Okay. Yeah. So this would be formatted or pretty printed what the solidity metadata would look like in v three. Basically the internal fields will stay the same, sources will stay the same. It will just be a little bit reorganized at the top level.
07:51:32.388 - 07:52:04.208, Speaker A: So this contract types is introduced, version changes. I think we said it will be manifest, underscore version or something like that, right? Yeah, this might be a little outdated. And also the ethPM spec is very flexible. So even if we don't define like a remappings field in the EPM spec, as long as there's no name collisions, then you can include any extra fields as you want and it'll be fine.
07:52:04.294 - 07:52:08.210, Speaker B: We have a couple of comments. Nick from Truffle was first.
07:52:08.980 - 07:53:09.700, Speaker A: Yeah, I just want to know how do we represent packages that have solidity? And this is, I think Chris proposed in our last chat and this would be a good thing to hash out now is inside sources. Inside the source object, which would be here we'd also introduce a field called something like type. And then this can be like JSON Viper or solidity. JSON is important for allowing imports of JSON ABI files as interfaces. A major breaking change between version two and version three. In version two a compiler was defined inside contract types. In version three we pulled compilers out to a top level array.
07:53:09.700 - 07:54:03.780, Speaker A: In EPM we also have a compiler information object which defines like compiler name version settings. So inside this compilers array you can include as many compiler information objects as you want and those will map to sources. So now you can feed an ethPM package into a Viper compiler and it'll know which source files to compile. And same with solidity. Does that link to the contract types as well? Yes, a compiler's information object. Let's see. Three, this has a field called contract types, which is an array of contract types.
07:54:05.400 - 07:54:07.590, Speaker B: And then leaguey has a common too.
07:54:11.800 - 07:54:44.656, Speaker A: Yes, sorry, lost the window. Yeah. Can we basically make the contract license? Basically what was discussed before with the pragmat and also part of the metadata. Sure. Right now or in version two, I guess in version three also we have a metadata. This is a top level metadata. So it represents an entire package.
07:54:44.656 - 07:55:15.870, Speaker A: And inside the meta object is a license field which is just a string. Should conform to SPDX, but it's not a requirement. And so I think, Chris, the idea would be to move this into a contract type object so it maps if source has a license. Yes. Okay. Yeah, so we'd add something like.
07:55:20.500 - 07:55:20.864, Speaker H: Or.
07:55:20.902 - 07:56:02.730, Speaker A: Whatever the SPDX would be. So yeah, it seems like these two fields, they aren't in the specification here, but I'll add them as soon as possible. If everyone seems okay with these for type, it's probably better to do something like JSOn. Abi, not just JSOn. Should type be from a predefined list of strings or can it be any string? I would say it should be from a list of strings, but can be any.
07:56:04.480 - 07:56:07.150, Speaker B: Okay. Alex also wanted to comment for a.
07:56:09.440 - 07:56:26.084, Speaker A: Mean, just an epic. Why is it version three and not version two? But this is not the real question. Is the main motivation that you want to support multiple, I mean, I guess you want to support JSON APIs and input as well as maybe other languages or. What is the main motivation for the change?
07:56:26.202 - 07:56:28.070, Speaker E: Because it doesn't look all that different.
07:56:29.240 - 07:57:34.410, Speaker A: Yeah, there aren't significant changes, just minor rearrangements. The motivation is just to find a nice data structure that works with all the EPPM tooling and also solidity's metadata. Now solidity automatically spits out EPM packages, which helps grow the ecosystem of packages. Now really from truffle or brownie you can just use the contract address and if the tooling supports it, it'll go look up the metadata and it can automatically just expose the source code. Maybe one more question on this just briefly. A single solidified will always have a single entry, a single contract type because it always contains a single contract. But I guess the reason for this metadata is that you could combine multiple of these sources into a single one.
07:57:34.410 - 07:58:13.780, Speaker A: Sorry, can you say that one more time? So a single bytecode will always contain a single contract. So metadata for included in a single contract will always have a single contract type. I suppose, yes. But this change suggests that you can have multiple contract types. So is the goal that you can combine multiple metadatas into a single JSON structure? Is that the main motivation? Sure. Yeah. The metadata would be just like a base object that can be extendable in any way that people find useful.
07:58:13.780 - 07:58:41.040, Speaker A: You can also ETM spec has this deployments field, so you can include addresses for on chain deployments. And now that's useful for a lot of tooling or developer use cases, but the metadata would be just this kind of base etHPM compatible object that can be extended in any way that people find useful or imported into tooling.
07:58:42.420 - 07:58:47.616, Speaker B: Okay, Nick from Truffle also wants to say something, and then let's keep in mind that we have to wrap this.
07:58:47.638 - 07:59:17.530, Speaker A: Up soon to keep the schedule for JSON Abi. Where do you get the solidity identifier for the interface name? We are thinking about adding a syntax I don't remember fully, but it's using interface name from and then the file name. Oh, nice. Cool.
07:59:22.030 - 07:59:38.080, Speaker B: Okay, so Nick, if you don't have anything else to discuss in this session, I would recommend to wrap this up. Let's have a look at the chat. If there are any more questions over there. No, just a discussion about.
07:59:44.050 - 07:59:44.846, Speaker A: Cool.
07:59:45.028 - 07:59:47.634, Speaker B: No, or go ahead, what did you.
07:59:47.672 - 08:00:08.680, Speaker A: No, NPM is fine, but I just think we can do a bit better. And NPM is also restricted to just Javascript tooling, which is dominant but might not always remain dominant. If we still want to discuss stuff here, I think I don't have too much content for the functional part, so we can spill it a bit over.
08:00:11.850 - 08:00:18.460, Speaker B: Okay, so Alex says it would be nice to have some session discussing this with the team. With the team? You mean with the solidity team?
08:00:20.910 - 08:00:33.898, Speaker A: Yeah, and the 08:00 p.m. Team together. Let's just meet on our Monday meeting at. I think it's 05:00 p.m.. CST. No, sorry, we had different meeting. That was the source verify meeting.
08:00:33.898 - 08:00:55.014, Speaker A: Oh, I'm confusing. Yeah. 08:00 p.m. Is usually Thursday at some time. Let's set a date now then it will happen. Should we do next Thursday? What was the time? I think it was 09:00 a.m. Central American.
08:00:55.132 - 08:01:02.570, Speaker B: Okay, sorry guys, to bump into here, but let's not do meeting appointments here because we have people on live stream.
08:01:02.990 - 08:01:03.642, Speaker A: That's it.
08:01:03.696 - 08:01:26.930, Speaker B: Not relevant for everybody, but yeah, thanks so much for this introduction and discussion on ESPM. And I guess let's move on to our very last official point for the solidity summit 2020, which will be functional solidity by Chris. An open discussion.
08:01:32.370 - 08:02:31.198, Speaker A: Good. Yeah, so this session is about adding some more functional programming features to the solidity language, whatever that means. And I just want to throw some proposals out there and get some feedback. There is certain functional ness in solidity by the way of internal function pointers here. For example, we have a library that defines a reduced function. And yeah, you can supply an internal function, underscore f, and it runs the usual reduce semantics on the array with f. Yeah, of course, the problem here is that we have types.
08:02:31.198 - 08:03:13.310, Speaker A: Solidity is statically and strictly typed, so you can only run this reduce on un memory arrays with functions that have uns as output. And of course the obvious idea is to add some kind of templates or generics feature to solidity. This has always been on the roadmap, but it's easy to get wrong. Oh yeah, plus one for generics. Yeah, thanks. I also want to have them, but I don't want to implement them. So this would allow you to have different output values for the function, different types in the memory array.
08:03:13.310 - 08:04:06.030, Speaker A: And it gets even more complicated because of the difference between value types and reference types and solidity. I think that makes it really hard to even write source code for that. That's actually a bug in the code here. It shouldn't be v here. And then how would it work for libraries? You probably wouldn't be able to write a template library with public functions, because that library at deployment turn doesn't know which types it would be applied to. Of course you can generate a new library for each template value, but that could also get messy quickly. And then again, we don't really know what to do with libraries as we've seen in the previous sessions.
08:04:06.030 - 08:04:24.790, Speaker A: So. Yeah. Are there any comments on this? I think the demand for that is high. Right.
08:04:25.400 - 08:04:27.428, Speaker B: Nick wants to say something.
08:04:27.594 - 08:05:03.160, Speaker A: I just want to know where the symbols t and v come from, because they look like free variables. These are the template parameters? Yeah. Where are they defined? Oh yeah. Maybe I should have added them to the library, like typescript, whatever. Cool. We actually have back in, I don't know, 2015 maybe. I started preparing that by a certain abstraction in the compiler code, so it could be possible.
08:05:06.750 - 08:05:19.600, Speaker H: But yeah, just a general comment. Do you see any of the functional stuff without generics? Because I don't really.
08:05:21.010 - 08:05:22.970, Speaker A: Do I see what without generics?
08:05:23.130 - 08:05:24.766, Speaker H: Any of the functional stuff.
08:05:24.948 - 08:06:10.110, Speaker A: What is functional stuff? What the function, whatever. Maybe. Unless there are comments on something. Do you have an example of how that would look at a call site? It would just call. I'm asking because of the template parameters mainly you would need the lambda to make it nice. Whatever. I mean, you probably write l, angle, brackets, u, int, comma u and angle brackets and then reduce.
08:06:10.110 - 08:07:23.626, Speaker A: So you specify for the library. So the library is parameterized on the templates, or maybe the reduce function is parameterized and then you have to specify, explicitly specify the template parameter values and just call the function. Is the problem with templates or the internal function here the function pointer? No, I was just wondering syntax. I think Leo was making a point in that there's like two things being conflated here. One, which is the usage of function pointers and then generics. So I'm not quite sure what it is we're discussing now. I'm all for generics, and this looks like precisely what I want, but yeah, I'm wondering what you think about this discussion, and we could try to implement generics for internal functions for libraries.
08:07:23.626 - 08:08:14.510, Speaker A: Just give it a go and see what happens. In my particular use case, we've thought of many custom data structures which are quite interesting and which are also very tied into solidity because they take advantage of specific aspects of the EVM and how that's different from other computing environments. So some things are harder to do, but some things are easier. And one of the key difficulties that we find is whenever you want to store any data type, then you need to duplicate a code for every single problem. Yeah, sure. And those are often supported anyway with libraries, with internal functions on some struct which holds the data. So they seem like another natural fit for internal functions and libraries.
08:08:16.230 - 08:08:19.614, Speaker H: But you still need to repeat the code for the different types.
08:08:19.662 - 08:08:20.260, Speaker A: Right.
08:08:21.830 - 08:08:29.650, Speaker H: If you want this for an array of integers, an array of something else, then you'd need to implement two functions.
08:08:31.430 - 08:08:32.580, Speaker A: What are you saying?
08:08:32.890 - 08:08:34.946, Speaker H: What, without generics?
08:08:34.978 - 08:08:36.360, Speaker A: Yeah, exactly.
08:08:37.450 - 08:08:46.570, Speaker H: But I also wanted to ask, besides saving the gas for deployment code, do you also see other potential to save gas?
08:08:47.870 - 08:09:05.070, Speaker A: Why saving gas? It wasn't more about saving keystrokes in the source. Saving what? Saving keystrokes in the source code. True generics, it's just implement one algorithm once and then reusing it for different types.
08:09:05.410 - 08:09:15.614, Speaker H: Well, you do save the gas. If you implement it for several types, then you have the deployed code of the different functions for the different types of the arrays you want to reduce.
08:09:15.662 - 08:10:06.340, Speaker A: On a question that might be related to that. Chris, regarding implementation, do you see this closer to C templates where the code is just like copy paste pasted with types replaced? Or do you see this dynamic thing where the runtime is able to do the right thing based on the type? I don't see this possible at runtime. Right, so there wouldn't be any gas savings at deployment right, because the only functions that would be used are the ones that are actually in the code, which is the same case as today. It's saving keystrokes in the end. Is that correct? Yeah, I mean saving keystrokes and just doing a correctness analysis, at least a manual one or maybe an automatic one.
08:10:07.210 - 08:10:09.640, Speaker H: Yeah, right. You're right. Okay.
08:10:16.200 - 08:10:33.290, Speaker A: Let'S move on. So I see there's a certain need for templates. What about lambda functions? So is there a need to define functions at expression level or at statement level?
08:10:35.600 - 08:10:41.470, Speaker H: If we allow generics and all that, I don't see why not. Maybe capturing the state as view.
08:10:43.200 - 08:11:05.796, Speaker A: At least it would be nice to have a shorter notation, right? At some point we were even thinking about a shorter notation, right? Yeah, I think we were. I think with this long notation nobody would use it or they would be crying to use it.
08:11:05.978 - 08:11:17.210, Speaker H: Yeah, there could be an alias for a lambda function that is by default pure and I don't know, some other stuff maybe.
08:11:17.820 - 08:11:26.620, Speaker B: Okay, from the chat we get the feedback. Nick from tropical plus one for lambda functions and Corey Dixon plus one for currying.
08:11:27.600 - 08:11:29.500, Speaker A: Yeah, currying is on the next slide.
08:11:31.440 - 08:11:34.030, Speaker B: Somebody's already sneak peeking here.
08:11:35.040 - 08:12:44.710, Speaker A: Can you explain if there is any performance gas overhead to using lambda functions as compared to calling an internal function? For example, I think lambda functions with external calling semantics, that's probably not that useful. And with internal calling semantics, I mean it might, right? In the new compilation path via Yule you will probably always end up in the dispatch routine, which might be a bit bad. But with proper inlining maybe you won't. It won't be as gas efficient. I guess the reason I ask is that I think very few people use this today. I don't think it's because the syntax is bad. It's probably another reason.
08:12:44.710 - 08:13:57.708, Speaker A: Maybe just a theory. There is a feeling that this would incur too high gas costs. I mean, people usually don't associate functional programming to efficient programming, even though it can be, of course. So maybe until those routines that you mentioned can guarantee a decent gas performance, maybe I would wait until that before implementing anything like this so that people can have the assurance when they are going to use them. The thing is, these features, lambda functions and reduce or whatever, these are usually applied to arrays and also functional programming has a lot of recursion and both arrays and recursions are not really popular with smart contracts because of their gas costs. So maybe even with a good optimizer they still won't be used. Yeah, that's a really good point.
08:13:57.708 - 08:14:41.080, Speaker A: I would add functions can also be used for creating higher level abstractions, not necessarily only for arrays. So maybe structs that encapsulate some behavior, but I don't think it makes a lot of sense to go in that direction at this moment. Things like that. That would also require persistence, which is also difficult, I guess. So storing a closure in storage, I'm not sure if that works. Yeah, okay, any more comments?
08:14:41.900 - 08:14:52.264, Speaker H: I have one proposal for reducing the length of the lambda line. Maybe not only for lambda, but for this case only for lambda.
08:14:52.312 - 08:14:52.910, Speaker A: First.
08:14:54.960 - 08:15:01.330, Speaker H: Having the last statement of the function body being the return. So you could skip the return word.
08:15:03.220 - 08:15:11.060, Speaker A: Yeah, that's another rust feature you're trying to sneak in. But then we have to apply that everywhere.
08:15:12.840 - 08:15:14.052, Speaker H: Then I have to what?
08:15:14.186 - 08:15:27.156, Speaker A: Apply that everywhere. Actually, the properties you mentioned, I think a property should just be a function that returns a boolean and doesn't need explicit mention of the return type or the return keyword.
08:15:27.268 - 08:15:33.450, Speaker H: Yeah, you can just implement a property with the boolean function view boolean function.
08:15:36.640 - 08:16:39.398, Speaker A: More comments what else do you have? Yeah, this is something that is rather easy to add. Free functions. So functions defined at file level, which are then always have internal calling semantics and could be reused. And I think this is a good replacement for what we currently have with internal library functions. Internal library functions are very confusing because they cause the code being inlined or incorporated in the contract code. And I think it would be much more intuitive if these internal library functions were just functions defined at the file level. And of course they do not have access do not have explicit access to a contract because they are not defined, or explicit access to a contract's state because they are not defined inside the contract.
08:16:39.398 - 08:18:01.760, Speaker A: But if they get past a storage pointer, then you can still do the same thing as internal library functions. Yeah, and a weird thing, or maybe a little less obvious thing we could also do, is defining an interface of a contract by assignment. So we have just have a list of functions at the file level, and then we define the public interface of a contract by specifying the interface, and then get the actual implementation via assignment. I think this is an interesting idea, but I don't think it has to do with functional programming. So it has to be discussed in the context of modularization and structuring and the abstractions of the language. I think what you mentioned about internal library functions being so weird, it's kind of a bad design, and this is better in that sense for the same purpose. I would avoid conflating this with the assignment syntax that you show here.
08:18:01.760 - 08:19:19.316, Speaker A: And it would need also a good story for structs, because we feel that structs in libraries with attached functions are very nice and useful. I don't know what that would look like here. You mean the using for syntax, the dot method syntax? Yeah, that's actually something I wanted to discuss in the libraries session, but we got carried away by the other topics. There could be a way to group or to directly apply functions, maybe defined in this way to structs without the using library name for struct name, but instead, I don't know, like Rust's impulse feature. Yeah. I also disagree with the statement that internal library functions are strange. I actually think the opposite, that public library functions are strange.
08:19:19.316 - 08:20:14.836, Speaker A: And the whole delegate call thing, that's very weird. The only change that I see between this and functions is that you do the namespace of the library itself, which I don't think that's something, I mean, it helps with the idea of importing a library, et cetera. So yeah, if anything, if you want to help, is the weirdness between internal library functions and public functions. I will just try to find a way to perhaps find a new name for one of those and just call the other one library and the other one something else. But this just removes the namespace and gives it no name. I mean, you would still want to import files with functions, right? You can use imports to create namespace. So basically a file is a namespace.
08:20:14.836 - 08:20:37.984, Speaker A: You can have a file that only has functions and nothing else, and then use the import to create the namespace. I mean, this is how modules are defined in other languages. Yeah. You cannot have standing structs, right? They have inside of library block. So it's like part of the same issue. Structs can be at the file level since solution. I've never ever seen that already.
08:20:37.984 - 08:20:56.250, Speaker A: That's actually one reason why, I don't know, we started, yeah, maybe structure, data structures, they are not really related to contracts. Why shouldn't we be able to define them at file level? So we did that, and now I thought, yeah, I mean, functions, why should they have to be inside contracts? We can also define them at file level.
08:20:58.220 - 08:21:00.890, Speaker B: Alex, you've been raising your hand for a while now.
08:21:03.420 - 08:22:28.330, Speaker A: Yeah, I think what I wanted to say was actually for the previous topic, because I raised it before, but I do have a few comments for this one as well. We introduced interfaces for a reason, and here with this example with contract c, you're trying to kind of use it as an interface and move out the implementation. So why do we have interfaces then? I mean, moving out the, I mean, these are basically two different proposals, right? One is adding free functions to do something like internal library functions did before, and this defining a function's behavior by assigning a free function. But I'm not really too sure about the second one. So this was just a random idea. Yeah, I have a comment for the previous slide. I was wondering if I'm a big proponent of functional features, but I'm also concerned about the gas usage and whether that would be a deterrent to people.
08:22:28.330 - 08:23:10.790, Speaker A: But I'm also wondering if any of these features are at all used by anyone or would be wanted to be used, and maybe an alternative for some of these which is not functional but we don't have is range based for loops. Maybe that would be a more pressing issue considering some recent defi things. Yeah, that sounds very good. Do you want to go over your other slides, Chris, so we don't run? Yeah, that was it.
08:23:21.950 - 08:23:23.020, Speaker B: Okay, great.
08:23:24.670 - 08:24:13.546, Speaker A: But maybe just final kind of poll. Does anyone think that these free functions are a bad idea? I'm concerned about the naming space. So how would it look if you import a file that checks as free functions and you want to use the namespace? You can use import, how is it called? So the import statement either can pull in, not the import file. Sorry, can you say that again please? Yeah. Then you need to use the other flavors of the import syntax as opposed to just import file. That's it. You don't need to, right.
08:24:13.546 - 08:24:45.234, Speaker A: I mean, if you just use import file, then all the functions defined at file level will just be available in that file too. Yeah, but not under any space. That's my concern. The same thing, other identifiers. My comment is you just have the add function and you have no idea where it comes from, and it can come from any of the files you've imported. But there's no way to know, right. So that can make reading code harder, because the only way to know would be to scan all of those files and then figure that out.
08:24:45.234 - 08:26:02.480, Speaker A: Yeah, I mean, importing files without adding something that is discouraged. You can also explicitly import it. You can write import add from file name. I think this touches on an interesting issue that I think there's an open issue hasn't been resolved, which is that all of the symbols in the namespace are exported. So having free functions would, I mean, I understand that people can use the import star as something statement, but it would probably result in a very polluted global namespace and the potential for clashes. So it would be interesting to pair free functions with explicit exports. So if you do not pick specific items, then there will be some kind of default stuff that is imported, or are you even disallowed to pick other stuff that is? No, I mean, a file needs to explicitly export something for it to show up on the other side of an import.
08:26:02.480 - 08:26:27.670, Speaker A: It's impossible to access anything that was not explicitly exported. Right. And that would avoid polluting export anything. Maybe then it's just the old behavior. Yeah, that's also something that could be interesting. Yeah. That is something people do nowadays when they have a library with internal functions and then helper functions that are private and therefore not export.
08:26:27.670 - 08:27:15.140, Speaker A: So yeah, there's definitely a need for either choosing what to export or just somehow marking stuff internal. So this could be the export keyword, right? Just start anything with export and then export it. I think that's how it is in Javascript, right. In the modules thing. And of course we could also introduce namespace feature. The free functions are implemented, though I would look into internal library functions. Maybe they don't, shouldn't exist.
08:27:15.140 - 08:28:01.240, Speaker A: I think it's a bit of a mess. Yeah. So there's some stuff in the chat. Daniel is again proposing pure functions and he's mean we would have to check that it doesn't access, I mean, yeah, pure function. Right. So it can't use this. Yeah, that could work.
08:28:01.240 - 08:29:10.362, Speaker A: One more minor comment on the topic of structs attached to functions and exporting. One issue that is, one aspect of the language that it doesn't exist today is the ability to have a struct that is opaque. Basically what I mean is that its members cannot be accessed, which is a very nice thing to have when you're trying to have some sort of extraction that is behind a struct and you can only interact with it via functions. Being able to hide access to the internal members is very useful. So perhaps the ideas around exporting a namespace can be used to solve that problem also. So this is similar to, I don't know, object orientation, where you have certain members that are only accessible inside the same class and also visible only there. Yeah.
08:29:10.362 - 08:29:59.990, Speaker A: I wouldn't say object orientation, rather encapsulation in general, but yeah, when you have that sort of thing. Yes. So the struct would be fully accessible inside the source file it was defined in, and only accessible by name outside, for example. Yes. That will let you create simple abstractions on top of existing data types that make for useful things. We have a number of those, and there's the danger of somebody inadvertently using the inner fields and not realizing that's an issue is like a concern. Basically provide a way to make it harder to misuse code is what I'm saying.
08:29:59.990 - 08:31:12.670, Speaker A: Anything else? So yeah, I mean, this is also with the suggestion by Daniel, this is kind of touching on what is a contract? Is it just a namespace? Is it just the external interface with regards to external calls and abi? Or what is it? Right? Is it just a storage layout? Maybe? What do you mean? What is a contract? I'm not sure I get the question. No. If we can call pure functions of other contracts internally, and if we do something like I had on this slide here, um.
08:31:16.630 - 08:31:20.100, Speaker H: I'm actually surprised it's not allowed. I really thought we had that.
08:31:22.390 - 08:32:11.280, Speaker A: No, it's only for libraries. Then if you define all the functionality with free functions, then the contract reduces to just a specification of its public interface. I mean, yeah, a contract is public interface and storage, and the implementation is clearly the thing that's most decoupled and the thing that's most reusable. But then yeah. Cycling back to Alex's question, what then is the difference between an interface and a contract? And then the answer would be just the storage layout, which is, yeah, can be deployed, if you will. The storage is an implementation data. I don't want to derail this.
08:32:11.280 - 08:32:29.910, Speaker A: I'm not really sure if I'm ready to allow calling pure functions of other contracts, because this also ties into virtual functions and stuff.
08:32:31.880 - 08:32:33.860, Speaker H: It must get rid of inheritance.
08:32:38.220 - 08:33:27.300, Speaker A: I think calling functions of a different contract strange, because it seems like that function should be abstracted away somewhere outside of the original contract if it's so general that it can be reused in that way. So I don't see why there would be a need. Allowing for more stuff isn't necessarily bad, but it can also create a situation where it lets you write code. That is great. Yeah. Was it you who said that public functions of libraries are weird? Maybe we could reduce the concept of a library just to a namespace of functions with internal calling semantics, and that's it. And then disallow calling internal functions of other contracts.
08:33:27.300 - 08:35:18.918, Speaker A: That has been my mental model for library for the past few years, and I don't ever use public functions in libraries, and it works just fine. So if that helps. Okay, I guess the main point of the public functions was the idea that it would be reusable, and maybe the reusability was limited by multiple factors, including the lack of package management. Will the presence of maybe a better EPM? Would that change anything or are we still more concerned about gas? I mean, this also ties into the other discussion of splitting up large contracts. So maybe the question is, what is the reason people are afraid using public functions and libraries, gas cost associated with calling into, with that call and just calling into a separate or address? I would say by far. But the gas cost issue also is present when people split up contracts. Is that a more controlled effort if it's done by some kind of a splitting as opposed to relying on libraries? Or.
08:35:18.918 - 08:36:44.520, Speaker A: Why is it any different? In the cases that I've seen when you do that sort of thing, it's because you have a sort of multi contract system that is, by the very nature of it being large enough that it doesn't fit into a contract, sort of complex and composed of many parts. And you can often modularize it in a way where you can have separate entities and they all make sense by themselves. On the other hand, when you have a library with public functions and you just call those, those are like very low level, like average or some, or the examples that Chris was showing, and that can be scattered across your code base. So at many different points you're making calls into delegate calls, whereas if you have a sort of multi contract system, it's much more clear and understandable, at which points you're calling into some other entity. So I think at least it's much more predictable in many ways what the cost will be and which actions may have concerns, whereas the other one is more harder to track. So that's more like a syntax issue then, I guess. And also libraries have this other weird thing that it support more features than the ABI and the calling convention.
08:36:44.520 - 08:37:59.450, Speaker A: Can you expand on that? Passing storage pointers via delegate call? So some data types were supported earlier for library calls via delegate call, and we chose a different encoding for them. And these are, I mean, storage pointers. That's not part of the Abi. And the other thing was, was it enums? No. So yeah, I guess conceptually what you're saying makes sense, and it is true. I think it's just that in practical terms, at least in the cases that I've encountered personally, I've never seen a situation where the code is so large because of inherent complexity, as opposed to being composed of multiple parts that can be neatly slippery apart. But yeah, eventually, if one single component is large enough, you have to do that sort of thing and rely on delegate call semantics for libraries.
08:37:59.450 - 08:38:10.510, Speaker A: Perhaps it's just the fact that haven't gotten to that point, or at least I haven't seen it, but yeah, at the end of the day that is like the last resort.
08:38:17.090 - 08:38:36.210, Speaker B: Just quickly jumping in here to check whether the discussion is still focused and energetic and necessary to happen in this round. Chris, is this still part of your open discussion on what were we even discussing on modify a functional solidity?
08:38:36.710 - 08:39:40.390, Speaker A: People were complaining that functional means immutable, so we could have called the previous discussion functional, but yeah, I think it's still useful, but I think okay, it's still on topic, but I'm not sure if we're making any progress here. I think the suggestion by zoo 25 is quite good to maybe decouple the main usages of libraries as collection of pure utility functions functions related to a specific struct and reusable function that access state. So maybe we could replace the using x for y by something like something like implementation for struct, name opening braces, and then a list of function. But yeah, I'm fine with closing this if no one else has any comments.
08:39:44.300 - 08:40:12.710, Speaker B: I mean, as with all of the other discussions, the conversation doesn't have to stop here. It's just another conversation starter into all discussions that have been probably going on in issues already. And we will certainly also do this more often going forward. So we will end this forever. I'm just saying maybe we should end it for today. If nobody objects, that is.
08:40:14.680 - 08:40:16.500, Speaker A: No discussion ever ends.
08:40:18.520 - 08:41:29.260, Speaker B: Okay, I guess that's it on our very last discussion round for today then. As you can see in the agenda, we had this very last agenda point still open there, which was called community voted placeholder discussion or open Q A with the solidity team, and we decided that we want to use this slot for two things, mainly before I say some final words from my side. And those two things are that on the one hand, we would love to hear some feedback from you. How did the conference go? What did you like? What didn't you like? What would you like to have more of or less of? And on the other hand, if you have any questions to the solidity team that have not been answered in the last two weeks, you can also shoot now. So with regards to the feedback, I will also circulate a survey after the solidity summit, probably next week, where you can also put everything you want to tell me in writing or you want to tell us. But if you already want to say anything today, feel free to shoot now or write it on GitHub.
08:41:39.320 - 08:42:41.464, Speaker A: Richard, first of all, I wanted to say thank you for organizing. It was very nice. I think it was also for me very interesting to have it very condensed, a lot of information about solidity, and was a very good mix. I really liked the open discussion rounds, even though I didn't participate in all of them, but I found a lot of stuff where I could participate, which was very nice, I think. I wouldn't say negative thing, because it's also positive thing is, this was actually quite long over the day, right? I actually stayed always from start to end and we had very little breaks compared to that. And it ended up, for me, always grabing food and eating in front of my computer, which it's nice, but it's not bad. Let's say this, but isn't it true? But actually there was nice weather, at least on Wednesday was the first day, I think, there, but I didn't have a big issue with that.
08:42:41.464 - 08:42:49.310, Speaker A: I'm totally fine with that. That was the worst thing of it, and that was not really bad. So I really enjoyed it. So thanks for organizing it.
08:42:50.560 - 08:43:24.484, Speaker B: Thank you. I also agree giving some feedback from my side. I mean, this is the first conference solidity summit we have ever done, and it's also the first online conference I ever organized. So I'm pretty happy that from a technology standpoint, everything worked fairly smoothly. Every speaker managed to present whatever he or she wanted to present. Screen sharing worked, the website didn't break down, we didn't have big delays. All of the talks and open discussions were great from a content perspective.
08:43:24.484 - 08:44:01.570, Speaker B: So thank you to everybody who's hearing that right now. Thank you all for your contributions. And yeah, anybody else? Any comments here? What would you like to have better the next time? Or what did you enjoy especially? Thank you guys for the nice comments on GitHub. Alex.
08:44:05.990 - 08:45:20.470, Speaker A: I have two comments based on, I guess, just the learning or takeaways for me. The first one is I think it would be really nice to have every few months some kind of a one day event just to be a platform for others to showcase their projects relevant to solidity. I really enjoyed those talks over the past two days. I mean, these are outside of the discussion talks and I think it would be nice to have such a platform in the future. And maybe the second is that regarding the discussions, we really underestimated the time allocated to them and we had a lot of topics and maybe not fully explored questions for the discussion. So we may not have been the most efficient, but it would be nice to maybe have some kind of more focused groups regarding these different topics in the future. So maybe anybody who is interested to participate in language discussions, not in all of them just certain parts.
08:45:20.470 - 08:45:29.760, Speaker A: It would be nice for you guys to step up and just message someone from the team in any way so we can put you on the list.
08:45:31.010 - 08:46:04.090, Speaker H: I think, related to that. One thing that would be nice is if we make specific calls to decide on specific features, and we decide on that call maybe like a week in advance, and then we make that call public, and then more people who are interested in that piece of language design can join that specific call. Like, I was thinking of doing that to try to get more decisions on the intra source spec stuff. So I think I'm going to try to do that as an experiment.
08:46:10.120 - 08:46:12.420, Speaker B: Okay, nice, Francisco.
08:46:13.880 - 08:47:15.210, Speaker A: Yeah, regarding that last point, I think it's important to note that this isn't only decisions, it's also analysis. And at the beginning of this conference, it was mentioned that the language is still not there to have something like rust. Actually, many of the talks were features inspired by Rust, but I feel that we should rather take inspiration from the process. Maybe such a full blown process is not needed, but I think it would be valuable to have some more structure for each feature that is being discussed. What are the motivations, what are the alternatives, and what is the, I don't know, other effects that it could have in the perception and the experience of the programmers. I think some more structure like that would be really important to introduce, to guide these discussions to good decisions eventually. Yeah, I think you're absolutely correct.
08:47:15.210 - 08:47:35.760, Speaker A: We need to select a topic and properly prepare on the topic and do some kind of an analysis before maybe an analysis inside a team. And when we are clear that it's like a useful discussion, then bring everybody on board so that nobody's time is wasted.
08:47:45.190 - 08:47:46.450, Speaker B: Yes, Michael.
08:47:49.770 - 08:47:50.518, Speaker D: Hi.
08:47:50.684 - 08:48:20.240, Speaker A: I have a personal question, as one of the producers of interspace. Curious how you all experience this as a platform with the integrated features. Is it something that you would like to continue using to do these type of events together, or would you prefer to use the traditional tools that have been used in the past to the group?
08:48:25.510 - 08:48:26.530, Speaker B: Richard.
08:48:27.190 - 08:49:21.800, Speaker A: So for me, I think the raise hand features work quite well. To be fair, we were like a normal size group, like not the smallest, but also not the biggest. But I think that worked decently well for me. The only thing that sometimes was weird is that I actually had a better image quality on YouTube than on directly being in the chat, even though it's the same video stream and it should not be a problem of my downlink at least. But besides that, I think having these tools that facilitate that being like in a big room being able to talk with each other made it very easy here. And for me, this was very nice experience. This was one of the things that made it actually very nice experience also in the open discussions because it made it very easy to not everybody talk at the same time and ignoring each other.
08:49:21.800 - 08:49:28.360, Speaker A: Anybody else?
08:49:30.810 - 08:49:47.726, Speaker B: Michael, I can also relay some feedback to you later on, which I received via the Poap token collection because I always asked in exchange for some feedback and actually, people were really happy with the technological setup. I can send those points to you later.
08:49:47.908 - 08:50:24.886, Speaker A: Cool. I'd like to work together with you in maintaining this portal for your community and doing these type of things. Hopefully it's not a one off solution for you guys to be able to collaborate with each other. I really enjoyed everything. I sat here all day for two days and listened to a bunch of brilliant people talking so politely and casually and intelligently with each other. Comments to all of you for your knowledge and the way that you interacted with each other. It was a great experience for me as a spectator.
08:50:24.886 - 08:50:26.010, Speaker A: Respect.
08:50:28.110 - 08:51:05.062, Speaker B: Thank you. All right, let's wrap this up now, once and for all. The one thing that I also really didn't expect is this to be that exhausting as it was. I actually thought, I can easily sit eight to 9 hours in front of a computer. That's not a problem. But I did not expect it to be that challenging content wise, to be moderating most of the times. I'm super happy how everything turned out in terms of participation as well as content and external contributions throughout the two days.
08:51:05.062 - 08:52:14.300, Speaker B: That was really great. We had more than 200 sign ups, and out of those, we had, at the highest times, more than 100 viewers combined over the two platforms. And at the lowest times, we never had really less than 50. So I think now we have reached our lowest point in the two day conference history, where we are at roughly 30 people, but it is super late, and we've been doing this for two days, so that's fine, especially for the open discussion sessions and this really niche content. I think this is a very good turnout. So thank you guys very much. And going forward, how many of you already indicated this will not be the end? So also moving forward, my role in the team will be definitely to do some more of this outreach and gaining feedback from the community, bringing it back inside to solidity and organizing everything that needs to be organized around that, so that the solidity team can focus on what they do best, which is coding, I guess.
08:52:14.300 - 08:52:20.926, Speaker B: Yeah, that's it. From my side, I would also like.
08:52:20.948 - 08:52:59.434, Speaker A: To say some final words. So even though he's probably not listening, I would like to thank Maurice because he did the initial scouting for the venues to have the in person event. And, of course, that turned out to be a little bit different. And then, of course, very big thank you to Francis, who, she came on board and more or less at the same time, we had to turn around the event 180 degrees to make it all virtual. And I think it really turned out flawless and was perfectly organized. So, yeah, really big thank you on that. That was really great.
08:52:59.434 - 08:53:33.000, Speaker A: And also another reason why this was really flawless was, I think, due to the interspace team. So I think the tech setup was really great. I was very much surprised and, yeah, I don't know, it was really great experience. And then, of course, thank you to everyone who participated. Thank you for everyone who gave their thoughts and suggestions. And as Francis said, I hope we'll be able to continue that.
08:53:34.330 - 08:53:46.380, Speaker B: Yes, let's not have this breaking up. So when I reach out to all of you via email, please do respond and give me feedback on whatever measures we come up with on how to stay in touch in future.
08:53:48.830 - 08:53:49.578, Speaker C: Cool.
08:53:49.744 - 08:54:21.010, Speaker B: I guess if nobody wants to say anything else, that's it. Oh, yeah. One last note. The videos of the talks will be edited and will be uploaded in a better quality and basically cut into chunks some when, I guess next week. So we will start working on this next week. And then I will distribute links of the talks as soon as we have it, the open discussion session. So the part where we were really discussing will not be uploaded, but only the introductory parts of some open discussions as well as the talks.
08:54:21.010 - 08:54:55.738, Speaker B: I guess that's it. So long. And as long as the talks aren't uploaded yet, you can of course distribute the live stream links which will work in order to show people your talk, your discussion, whatever. Okay, then I wish you all a good night, good evening, a good noon, whatever time of the day it is for you, and see you around on the Internet very soon. Bye bye.
08:54:55.834 - 08:54:56.590, Speaker A: Bye.
08:54:58.630 - 08:54:59.410, Speaker H: Bye.
08:54:59.830 - 08:55:02.130, Speaker A: Thank you. Bye bye bye.
08:55:13.710 - 08:55:14.460, Speaker C: Cool.
08:55:19.090 - 08:55:28.310, Speaker A: Yeah, I target pro blame a savian conference normal.
08:55:38.190 - 08:55:44.570, Speaker B: This is definitely manzipti Gans aside from computer mechanic and for Mal room Laufan and urged.
08:55:48.670 - 08:56:21.974, Speaker A: Yeah, I'm sure you've gained all the lessons that I gained over the last month as well. We really need to powell and build out how it is that these things can be more streamlined but still hold their sort of like bald, you know. There's definitely a great know to be found, I think. But again, once more, Francie, it's been such a pleasure working with you on this project, and I'm so proud of what you accomplished and was so cool.
08:56:22.012 - 08:56:23.030, Speaker H: To be a part of it, man.
08:56:23.100 - 08:56:28.760, Speaker A: Thank you so much for choosing us to assist you in manifesting this event.
08:56:29.130 - 08:56:39.540, Speaker B: Thank you. I mean, seriously, it couldn't have gone any better. We had no issues. Not from the technology point of view, not from a speaker point of view, not anything.
