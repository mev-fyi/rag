00:00:22.810 - 00:01:12.500, Speaker A: So the official top title is optimization techniques for EVM implementation. And you can see from this is the proof the title is too long because didn't fit into the box there. The better one would be how to build a fast EVM. Yeah, here, like high pitch, cover it for a while. Okay, thanks. But to be precise, I think we have to mention interpreter in the title. So this is not talk about how to do fancy stuff with just in time compilers, head of time compilers, or anything like that.
00:01:12.500 - 00:02:17.190, Speaker A: The all thing I'm going to talk about here will be related to pretty much portable code. Nothing of that would use advanced techniques from generic interpreter optimization space. My name is Pablo Bolita, and for many years I do EVM. So let me go briefly through my history of working with EVM. I started quite early, even before the Ethereum had launched, and the first project I was working on with my colleague was Evmjet. So it was a project that was supposed to prove that EVM can be repeated fast. And it was working by translating EVM bytecode to Llvmir and then using LlvM tooling.
00:02:17.190 - 00:03:28.160, Speaker A: It was compiling that to machine code loaded to memory, and then it was executed as an idea of code. So the product died around 2017. I think there was mostly not the interest in working on it and using it, although it proved a long time ago, and that's doable. But I think the big controversy about can we actually use compilation pipeline on common sense's critical platforms. But what actually come out of the EV engine project was EVMC, which was attempt to, and still is attempt to define the low level API for EVM. And it's actually for both sides for evms and the clients that use these. And if one of the sites implemented, it can use different implementations of evms, more like a way as plugins work.
00:03:28.160 - 00:05:05.440, Speaker A: And the first point here is this RF project, which originally was the CPP Ethereum, and it has like big C code base. It was one of the free implementations that started before the original Ethereum implementation, but there actually, we also used the EVMC API and packed the EVM implementations from there into this API. So it's available as a plugin nowadays. And the last project is EVM one, which I started last year. And most of the things I'm going to present here are coming from discoveries in this project. So EVM one is interpreter implementing EVM, and the key differences are it has pretty good 256 bit integer implementation, and it has also experimental, efficient, different way of counting gas and checking other constraints, and has many other optimizations, kind of like micro optimizations, which are related either to C plus plus or maybe not. But I'm going to talk about the first two points here, and both of these were actually released in version 0.1
00:05:05.440 - 00:06:25.790, Speaker A: some time ago, so they were done on the same time. So when we start with the, when we want to start with the two, five, six integer implementation. So we can say it's critical from EVM point of view, it's like the arithmetics EVM uses. So actually I was kind of like not sure that this is really case. Maybe it's important, but maybe it's not so much. That was my expectation. But yeah, my comment actually recently you had option to actually place the general implementation of integer in Aleph interpreter, which previously was using boost multi precision.
00:06:25.790 - 00:07:45.288, Speaker A: So boost is commonly known as c library, some kind of standard library extension, and it's commonly used in many c projects. So I think it's nothing to be worried about intentional quality and performance. But we had option to replace that implementation with int x implementation, which is the implementation actually did for EVM one product. And at this point, Dick, thanks to some Internet guy called vstak who actually contributed all these changes, and the results look more or less like that. So this is like zoom of the benchmark, small set of benchmarks I did, and we'll going back to that in the end. But simply by replacing big integer implementation, we can get around three times speed improvement here. This is summary of some benchmarks I will show later.
00:07:45.288 - 00:08:52.060, Speaker A: So it's like geometric mean of a small set of days. Tony has some kind of idea how the average usage of the EVM performs here. But if we go back to talking about this two, five, six integer implementation, and if you can refer probably to very overused 80 20 row. So if you want to implement integer, it actually works mostly like that. 80% of your struggle is around division, which is pretty much complex to do. And anything else you want to implement there, it's pretty much straightforward. So by division here I also mean reminder of the division of modular operations and so on, because the results of these actually coming from the same algorithm.
00:08:52.060 - 00:10:16.290, Speaker A: But considering short time we have here, if I can give quick, let's say hints how, how like how the, I think the performing two, five, six bit precision integer implementation should look like. So these are some guidelines I have. So first of all, you can forget about any kind of fancy algorithm that show up somewhere in the space. So anyone else like multiplication algorithm that can some multiplications of the words, any names? Okay, good, because they are not needed actually. So I mean, they work at point when you actually have big numbers to multiply and so on. For the precision, it's actually quite small precision, actually comparing big numbers in general, so none of them are needed. And I'm pretty sure that's correct statement.
00:10:16.290 - 00:12:04.022, Speaker A: But what I also did in x is I actually ignored all these easy cases to handle up first, like when many implementations do. So even if you have option to figure out that the numbers you are multiplying are small, like they within 64 bits, I think it's not worth it to actually handle this case separately because it distorts all the other algorithm and you have additional conditions and branches to check pretty much. It works great if you just only focus on two, five, six bit. And the first thing I want to mention here is first optimization we can do is try to avoid dynamic memorial location. So if you use any kind of generic multi big number libraries that actually can handle arbitrary precision, which are not fixed on this compile time and effects like that, they will definitely use dynamic memory allocation because it's unknown during computation, like what the size would be. So if there's option to use fixed like fixed precision, but at this point I think it's perfect because that stays with the memorial location. Some libraries offer that, but I think we should go further here.
00:12:04.022 - 00:14:06.250, Speaker A: And also like, I'm sure I can easily explain that with visualization, but many of the big number implementations actually have kids. The number of words they will later have in the number representation, the kind of additional information stored in the type, which makes the type bigger by this additional item, and actually in our case makes it more or less 25% bigger. And I think from even the memory access patterns, it's kind of important to not have it, because if you have type, which is like the minimal possible size, which is 32 bytes, it nicely fits into cache lines and so on. And when you access stack items of this size, it performs much faster. And the fifth suggestion is try to understand that CPU has some kind of special instructions, let's say. I mean, they are not obviously available from the high level languages, so they can actually speed up addition and other operations and have either high precision multiplication or can actually give you high part of the multiplication and so on. And I think this should be aware that when you build a code in high level language, it's good to know if either compiler can give you access to these by using well known patterns, how to access them, or you might need to use some intrinsics.
00:14:06.250 - 00:14:59.854, Speaker A: For example, get access to high precision multiplication that your cpu probably has. And last thing in terms of division, I mean I was actually struggling with the vision for a long time because I mostly had to learn most of these. But this paper really helped me here. So this is a paper coming from GMP library, which is like well known big number implementation. And actually it gives you like straight away. Maybe it's not straight away because the form is described in this mathematical form, so you still have to translate it into code. But I think it gives you almost full recipe how to actually implement that.
00:14:59.854 - 00:15:52.160, Speaker A: And I did it still GP is faster here, but not by some margin because maybe they definitely use assembly directly as a way of speeding up. So maybe just somehow compiler fold that it cannot, it cannot optimize the same way like some people did some time ago. But yeah, it was very hard to do. But if you still a bit lost. So my suggestion is if you don't have other options, try to actually port the my code to other codes. What's the license on it? It's Apache two. Awesome.
00:15:52.160 - 00:16:47.520, Speaker A: So I mean by porting I don't mean use it as a specific in this case, with all this accessing c code by cap, I think it won't work very well. But we can try that by coding. I mean you can try to just copy it to other languages and make it build and try to actually use the same sympathy if you don't know where to start. Yeah, so that was mostly about this individual stuff. If any quick questions now, I can handle them. Probably we will continue. How big is the library? I'm not sure exactly, but it can be, I think between 2002 thousand, I think something like that.
00:16:47.520 - 00:17:36.318, Speaker A: But also design is like this is done by using generic and kind of how to explain recursive design. So you build two, five, six operations out of 128 operations. I think lot of c plus plus craft around templates make it difficult, it increases line code, but I think it's not really complicated. Except for the vision. Yeah. So let's go to the. It's about small innovation here and experimental way of counting gas and doing other things.
00:17:36.318 - 00:18:36.418, Speaker A: I actually wanted to make it in written form and there is a link to a draft or article I'm writing about that and explain it maybe two or three years later. So I just wanted to leave a pointer to it. But if we consider it how EVM execution works, so it actually needs to do it in two phases. So first one at least needs to check where are the various jump destinations. And in APM we actually put a bit more work into this pre processing phase before the execution starts. So I will try to explain that by example. So we have a code EVM bytecode assembly here, but you don't actually need to understand what it does.
00:18:36.418 - 00:19:46.890, Speaker A: Actually, I've tried to do it a bit like do something, but in the end there's one mistake here, so it's actually pretty stupid now. But anyway, what we want to do, and we will try to do what the preprocessing step is doing for EVM one. So we want to identify basic blocks, and basic blocks are the sequence of code that are kind of executed uninterrupted, and this is coming from the compiler construction. And basic blocks are the sequence of instructions that are for sure executed in order. And then the basic blocks are kind of linked by jumps between them. Yes, that's a good question, but yeah, it will be seen in a moment. Okay, I think you're asking like where does basic block start? This is also the start of basic block.
00:19:46.890 - 00:21:06.770, Speaker A: So we're actually looking here for the terminator instructions that actually end the basic block. So either by being a jump or it actually terminates the EVM execution entirely. But it actually doesn't matter in this way. And the second group of interesting instructions actually have one, only one in it. It's a jump deck which starts the basic block, and by using that we can split it into basic blocks. This is like visualization of that with some gaps here, because this one instruction is after the terminator instruction, and here actually both conditions are applied in the same place, but we can also find examples where JAmda is not preceded by anything in nature, but still it creates place. Okay, so can I have like 1 minute? And what we do here we actually can.
00:21:06.770 - 00:22:37.210, Speaker A: Then having basic log, we can pre compute the base cat scope of this instruction, but we can also pre compute some stock requirements for that. And having all of these three values during execution, we can check this condition only for basic block, and then it doesn't have to be checked anymore during execution. So this saves us a lot of time during interpretation, because we've pre computed some information and we don't have to check it for basic instructions as such, addition, multiplication and so on. So all these three checks, the base code, check underflow, stock overflow can be done only at the beginning of the baby block, and then we can safely assume they are checked and happily execute federal instructions. And this mostly is based on this fact that EVM exceptions are indistinguishable between the types. What actually the error happened. I mean this information, they revert all the changes and consume the gap.
00:22:37.210 - 00:22:37.900, Speaker A: Yeah. Thank you.
