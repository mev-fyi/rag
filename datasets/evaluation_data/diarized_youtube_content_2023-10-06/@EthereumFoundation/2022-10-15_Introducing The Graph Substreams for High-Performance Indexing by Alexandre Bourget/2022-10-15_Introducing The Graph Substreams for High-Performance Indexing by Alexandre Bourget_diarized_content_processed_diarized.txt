00:00:14.010 - 00:00:49.850, Speaker A: Hi, my name is Alexander. I'm CTO at Streaming fast. And I'm also a pianist, a data scientist, whatever that means. I'm a father of eight beautiful children, two of which are there love designing and crafting software, which I've done since I was twelve. And I'm here today because one day in 2013 I read the bitcoin white paper and that changed the trajectory of my life. And fast forward to today streaming Fast, a company based in Montreal, Canada is now one of the core developers of the graph. And we joined the graph a bit more than a year ago in a kind of bizarre MNA 2.0
00:00:49.850 - 00:01:22.434, Speaker A: fashion. Our lawyers still don't understand what happened, but anyway, we said thanks and goodbye to our VCs and shifted our focus to make the graph the greatest data platform on Earth. So today I'm here to introduce substreams, which is a powerful new parallelized engine to process blockchain data. And before I can do that, I just want to set a bit of context. Perhaps you can raise your hand if you know what subgraphs are. Raise your hand if you know oh, wow, you're good. Okay, so subgraphs can be thought of an ETL process, right? Extract, transform and load.
00:01:22.434 - 00:02:12.914, Speaker A: And subgraphs add that little queue there, the GraphQL layer to it. And subgraph today provide that sort of simple approachable, end to end solution to blockchain indexing. And the graph node is responsible for all of these components, right? The extraction is done through hitting JSON RPC nodes and then transformation you provide some assembly script you guys know, that compiled WASM running in a distributed environment. And then you have the load aspect, which graph node does, puts that into postgres and offers you a rich and beautiful GraphQL interface on top. And one of the reasons we were brought in was that so we could push the graph to new height in terms of performances. So to do that, we brought first thing the fire hose, something at the extraction layer to archaic to boosting performance by one, two, three orders of magnitude. The first layer extraction.
00:02:12.914 - 00:03:08.742, Speaker A: It's a method of extracting data from blockchain nodes. Imagine prying an egg open where the data is exfiltrated as fast as possible and all the juicy data gets then thrown in a gRPC stream as well as into flat files. And you can think of that as sort of the bin log replication stream for blockchains, where you'd find in a master slave replication engine like in databases. So we'll get back to firehose in a minute. Then substreams is sort of rethinking of the second box, the transformation layer. Here, instead of the traditional subgraph handlers and assembly script, you will write substreams modules in rust and those can be executed in real time as well as in parallel with unprecedented performance. So let me give you first a primer on firehose because there's a lot of benefits of substreams that come directly from the firehose so a streaming fast.
00:03:08.742 - 00:04:06.154, Speaker A: For many years we've been thinking hard about all these indexing problems from first principles and we needed at first a robust extraction layer. We wanted something that was extremely low latency, something that would push data out the moment the transaction was executed within a block, within a blockchain node. JSON RPC was not going to cut it and we didn't want to have to deal with those large bulky nodes, right, hanging on a thread occupied with managing high write throughput, kept everything in a key value store behind a JSON RPC request and it was really heavy in Ram and CPU and you needed super optimized SSDs. It's really annoying. And all these things are much costlier than what's needed when our goal was to get to the data inside. So we also wanted proper decoupling between the processes producing the data. So the blockchain nodes and its intricacies and its request response model and they're all different.
00:04:06.154 - 00:05:06.862, Speaker A: And the data itself, we wanted the data to be the interface and we wanted something also extremely reliable in a sense that we could avoid hitting load balanced nodes that had all sorts of different views of the world and that we need to have client code to latency inducing code to resolve what's happening there. If there's a fork, you need to query nodes again for reorganization, heuristics, for example. But also we wanted something better than even the WebSocket streams that pretend to be sort of linear that the nodes have implemented because when they would send you a signal that let's say this block was removed, it could leave you hanging. If you happened to be disconnected for just half a second, you'd reconnect, you'd miss the signal. So the reliability was not built in. So we wanted something to address that. And above all, we wanted something that is able to process networks in 20 minutes, well, okay, an hour or two, but never three weeks.
00:05:06.862 - 00:05:38.098, Speaker A: Or things where we're waiting linearly. And that's still our goal today. And when we say network history, I mean executing guests and extracting data executed into flat files. That's the extraction layer, but also any sort of indexing after the fact. We wanted to be able to have massive parallelization, like there was no other way to have reliable and durable in performance without paralyzation. So our solution was the fire hose. And the fire hose solved all of these issues in a radical way.
00:05:38.098 - 00:06:26.834, Speaker A: We took a radical approach because we wanted to solve those problems definitively, meaning that there would be no further optimization possible except attempting to bend sort of spacetime continuum itself, right? So with streaming, with even multiple nodes pushing out data, multiple nodes are actually racing to push the data. The first sort of consuming process gets the first to get out. Like you can't really remove more latency there and there can be nothing faster than immediately when the transaction has just executed from your node and then regarding staple processes and costs, flat files. Flat files for the wind. We have a hashtag for that, right? Flat files are the cheapest, much cheaper than processes. They're easier to work with. There's nothing simpler nor cheaper in terms of computing resources.
00:06:26.834 - 00:06:56.094, Speaker A: These storage facilities have been optimized like crazy. And it's also where data science is headed these days. And there's one common thing to every blockchain protocol, and that it processes data. Data is also the right abstraction for this technology, not an API. That's common to all chain data. So firehose clearly delineates responsibilities. And the contract between the extraction and transformation layers is, again, the data model Firehose creates.
00:06:56.094 - 00:07:32.506, Speaker A: And for every chain you can imagine, the best data model the most complete. And that's what we've done for Ethereum, for example, the data model for Ethereum within Firehose is the richest there is. Like you have in there the full call tree internal transactions. You have the inputs and outputs as raw bytes. You have the logs, obviously, you have the state changes like you see on Ether scan down to the internal transaction level. You have balance changes the same way with the prior value and the next value. So when you're doing like navigation backwards or forward, you have the data you need.
00:07:32.506 - 00:08:17.002, Speaker A: You have also gas costs at different places. And there's that important notion of total ordering between things happening within the logs, state changes and calls, all of these things happening during execution are totally ordered. So you get in there, everything parity traces would give you and more and everything you would need to rebuild a full archive node from flat files. And everything there is scoped to the transaction level, not rounded at the block level, which is crucial if you want to index with precision. Rounding of blockchain. Information at the block level was meant for helping in consensus, but it doesn't mean that what happens mid block is of less value than what happens at the boundaries. Okay, so that's very interesting.
00:08:17.002 - 00:09:06.534, Speaker A: And now, regarding reliability whoops, no, not so fast. Regarding reliability, the Firehose ERPC stream provides reorg messages like new block or undo. This block or this block is now final, accompanied by a precious cursor. I think that's really key here with each message. So if you get disconnected and upon reconnection you give back that cursor, you'll continue exactly where you left off and potentially receiving that undo signal that you would not have seen where you disconnected, right? So you will get it so with the guarantees or linearity of the stream. So no WebSocket implementation would do that because it doesn't make sense for a single node to track all the forks possible, even two days after the fact. And undo messages come with full payloads, so you get all the delta.
00:09:06.534 - 00:09:51.014, Speaker A: So you can just turn around to your database and apply the reverse or pluck again in the full payload of what happened in the block and decide what to do. So it doesn't impose on the reader to store what happened, like at that previous block, if the signal was just removed, block 7000. Right, okay. And when you commit that cursor to your transfer database, well, you get sort of that. Finally some consistency guarantees within your back end. So some of our users told us they could cut 90%, 90 of their code reading the chain because they were relying on that reliable stream. Okay? And it also lays down the foundation for massively paralyzed operation files, plus a stream.
00:09:51.014 - 00:10:26.854, Speaker A: And so this is the future of the graph's unbeatable performance and is core to our MultiChain strategy, because any blockchain can have that data model. Now let's dig into substreams. substreams is a powerful clustered engine to process blockchain data. It's a streaming first engine, and it's powered by the firehose underneath and its data models of the chain. So let's dig in. Here are a few quick facts. It's invoked as a single Jace gRPC call, and within the request, we provide all the transformation code like you'll have in there.
00:10:26.854 - 00:11:22.194, Speaker A: Oh, it's too low. You'll have in there the code, some WASM modules, relationships between the modules and all the transformations within the request. It's not a long running process, except if you run it for long, it's not a service you spin up, right? And the backing nodes are stateless, which provide nice scalability properties. Modules for transformations are written in rust. They compile to WASM and they're run in a secure sandbox on the infrastructure there, similar to a subgrass. And the ultimate data source being the blockchain, data being deterministic, all the transformation outputs are also deterministic. And if the request you send involves process prior history, even if it's 15 million blocks, well, the substream's runtime will then turn around and orchestrate the execution of a multitude of smaller jobs in parallel, fuse the results on the fly for you and aggregate the results to simulate a linear execution.
00:11:22.194 - 00:11:57.490, Speaker A: So you would see a dime a difference. And all results are streamed back to you as fast as possible with the same guarantees provided by the firehose, with a block per block cursor and a transparent handoff from batch and historical processing to the real time, low latency rewards aware stream of the head of the chain. So, let me show you, if you're interested, how we create one of these things. Raise your hand if you're curious. Okay, you're good? Okay, so let's start, we start with a manifest like that. Do you see that down here? Can move the podium, I can't. So there's package information, some metadata there.
00:11:57.490 - 00:12:25.290, Speaker A: You have pointers to the protobuff that you'll use. Again, contracts between modules are about data. So there are protobuff models similar to the protobuff models of the root chain, of the chains, the layer ones. And you have pointers to the binary that you're working on your drive and all that. And you have imports. And imports are actually very interesting because you can import third party substreams packages and these YAML can be packaged and so you can import from someone else's package. You can write your own or combine both.
00:12:25.290 - 00:13:03.074, Speaker A: That means substreams enables composition at transformation time, which I think is pretty unique and a pretty game changer. And then follow up and there you have the module sections which defines the relation between the different modules. And you see it defines a directed Acyclic graph. You have modules that slowly refine the data. And so there's two types of modules. One the mapper, the first up there map pools and this one takes inputs, does transformation and outputs it's parallelizable down to its core block wide, so massively parallelizable. And then there's the store input.
00:13:03.074 - 00:13:44.178, Speaker A: I think it's awesome. This one takes any inputs and outputs a key value store that are sort of an accumulated in a stateful way. And stores can then be queried by downstream modules. Okay, so we'll see a bit more after then the name corresponds to the function in the WASM code and the inputs can be of a few things. Either the raw firehose feed, so for example, the source here, that means the block with all transactions for that particular block and it can be the output of another module like you see down here, the input of map, map pools. So you'll get the data as bytes. And it can also be a store, which would be a reference we'll see in the next slide there.
00:13:44.178 - 00:14:14.974, Speaker A: And on the store pools here, you see there's an update policy which sets constraints on what you can do with the store. And it defines a merge strategy for when you're running parallelized operation. I'll get to that also a little later. And the value type field will help anyone decoding understand what bytes there is in that store. So you're going to UI, you can Jsonify them and your code consuming can automatically decode them with protobuff, all languages supported. Otherwise the key value is just keys, strings, bytes, values. Very simple.
00:14:14.974 - 00:15:10.718, Speaker A: And one thing to note here is that because it has deterministic input, it's possible to hash a module like the Kind and all of its inputs and the pointers to its parent and including the initial block. So you have a fully determined and hashable, let's say, cache location for all of the history similar to Git, right, all of the history of data produced, you'd hash also the WASM code, right? So it makes it for an extremely cachable system and highly shareable and cross verifiable output of modules which opens really interesting possibilities for collaboration within the graph ecosystem. And imagine that one has large disk and no one has large CPUs or sleeping CPUs. They could pull resources together to build something bigger than themselves. Okay? And see the relation there, so this gets piped to that. And if we add another module here, you see how the graph comes together. This one computes the price.
00:15:10.718 - 00:15:41.274, Speaker A: This is uniswap v three thing. It computes the price. But you want to get them for certain pools because maybe you want to use the decimal placements in the pool. We'll see a little bit more there. And when you're running, let's say you're running that at block 15 million. Well, you're guaranteed the runtime guarantees that the store you'll have to execute code at block 50 million will have been synced linearly or in parallel, but you wouldn't know. But it'll give you a full in memory store eventually backed by some diff, but whatever.
00:15:41.274 - 00:16:17.922, Speaker A: And you can query the key value store at each block. It's guaranteed to be synced for you. That's exciting, no? Okay, so you see the Dag fully being built, right? The dependency. So now this leads us to composability. See, each color here means a different author and modules written by different people, ideally the most competent for each, right? Like we would hope they would corrupt analyze what's on chain and refine the data and abstract it to new heights. And the contract between the handoff is always data. It's a model of data.
00:16:17.922 - 00:17:08.766, Speaker A: So you take a module, it's bytes in, bytes out. And so you see here we can get the prices from uniswap v two and prices for uniswap v three and sushi and chainlink and whatever and have someone write a module that takes these input at transformation time and then averages them out and whatnot, right? And then that you'd have sort of one beautiful universal price module that you can then hook on top and feed to some who knows, maybe someone feeds that back onto the chain for some reason and then soon enough all of that. Well, someone wants to build on top of it, something like that. Someone wants to compute the USD denominated volumes aggregation of NFT sales on OpenSea. You'll take some sales, you'll merge it with a price. And we see here that little trader inc. Maybe he wants to feed that into his trading bot.
00:17:08.766 - 00:17:30.742, Speaker A: Because this is streaming engine. We're not storing that in a database yet. Right, but this begs the question where does all that beautiful data land? Where does it get piped? That's where syncs head up like substreams being limited to the transformation stage of the ETL analogy. Remember, it doesn't really care where you load it. And that could be anywhere. These are just a few examples. You can load that in databases.
00:17:30.742 - 00:18:22.378, Speaker A: We already have a sync for postgres and mongo. You hook to substreams and it just loads it into postgres with a data model that we've agreed upon, right? If you write it in a certain way, it just syncs over there or message queues or whatever data lakes or some bots or some trading algorithm, I don't know, some whale detector you want to hook directly on the stream or also something I think big for doing some ad hoc data science. Because now you have a really fast engine, allows you to process the whole history in like it can take a few minutes to process the whole of Ethereum to go and pluck some new insights. So you can write your code, send it to the network and then stream out the results similar to for those who know BigQuery, the big clustered service by Google. That's what they do, send the request, it just shot at everything. They send you back the request. Well, all of a sudden substreams engine can allow you to do some things like that ad hoc.
00:18:22.378 - 00:19:00.390, Speaker A: And you can write any program that supports gRPC and protobuff, which are many. And the last one here, not the least subgraphs through graph node. So we're working to make substreams feed directly into graph node to then provide the same loading experience and then querying experience that you've come to know and love. And you'll be able to deploy a subgrass this time not containing assembly script, but a substreams package with an entry point and would process the history in parallel and load that in your database in crazy speeds. So stay tuned for that. That's not out yet, but soon. Okay? And so this is a simple example.
00:19:00.390 - 00:19:38.722, Speaker A: In Python, it's not really longer than that. You have one or two dependencies like gRPC so that you can use a query. So we're leveraging a lot there and you can see that spkg there. We can use that to code gen Python classes and helpers and all of that because it turns out that the manifest sort of the spkg there is for those who know protobuff, is a file descriptor set. It contains all of the things, all the protobuff definitions. So the spkg also contains all the WASM code, the module graph information, the dependencies, the inputs and all that and even some documentation. Everything is needed is in there.
00:19:38.722 - 00:20:13.978, Speaker A: So you can pass it down to the modules, you take it from the disk and boom, you send the request to the server and it's running. So you can deploy packages also very easily and consume them very simply this way. There's a few imports we've omitted there, but it's simple just to show. Okay, and let's look at the simplified data model for uniswap V three and I'll show some code making use of it. Okay, so this here, the pool is a list of pool. This is actually what gets handed off from our mapper, which finds the pool that were created down to the store pool, which we're going to look also. And so it has a list of pools.
00:20:13.978 - 00:20:38.214, Speaker A: And the pools you can imagine an address and the two tokens that are concerned here. And we have a reference to the token also, which is going to be very useful to enrich the data downstream. We'll have the decimals right at hand. Like we won't need to do much loading. It's going to be very close so we can enrich all these uint 250 79,000 and put the comma where it belongs. So let's see what happens in the mappers. So this is sample Rust code.
00:20:38.214 - 00:20:54.050, Speaker A: Raise your hand if you love Rust. Raise your hand if you know Rust. Okay, it's very simple. Here I'm going to go through. It has a map Pools function corresponding to the manifest. There has one input, the block. This is the Firehose block with all transactions, all logs, all state chains.
00:20:54.050 - 00:21:19.858, Speaker A: You can craft your own triggers in there as you wish. But we have a simple Voyager. See that line there blocks events. You have a thing that goes through transactions and it's going to trigger on pool created. And that pool created object in Rust was actually code Gen from the JSON Abi. So you can just give the instruction and we're going to filter it for only the V three swap factory. And then that beautiful filter map will give us the log, okay? And then we'll output.
00:21:19.858 - 00:21:50.782, Speaker A: We're going to collect some of these things into one list of pools. And it's assigned to the Pools object there. And notice that little thing here? This is the RPC create uniswap token thing. This actually hits an ETH call on a node behind, similar to what we have in subgraphs. And that's actually very important. It means that once we've processed this layer once and we've done it for the history, it can be cached very efficiently. So anyone relying on that thing will never need to reprocess it again.
00:21:50.782 - 00:22:16.194, Speaker A: You can give the package to someone and they can access the stores that's been cached by other people immediately. So you could go to the block 15 million and you'll have the list of all pool created that you can query super fast. You can depend on it also. So I think that's pretty cool. And here, that's the store module. The store module is pretty simple. Here it receives C, the pools from the output of the prior module.
00:22:16.194 - 00:22:49.230, Speaker A: And it does, it loops through the little pools there and calls output set. And see, the key there is Pool colon Dalala, the address. It's going to store the proto buff encoded stuff of the pool with the token decimals for both, right? Did I say that to be constrained? So the store here is constrained in two ways. In order to preserve parallelizability, the stores are write only. You cannot read your writes, otherwise that would make them potentially cyclic. And they expose only the function defined by the update policy. In this case, it's sift.
00:22:49.230 - 00:23:28.602, Speaker A: So let's see what happens if we run things in parallel. So here we have two jobs covering two segments of the chain, 1 million block each. And you see those ugly arrows there? They correspond to a pool created event. And so in our code you've seen we would write a key for each of them in the first partial run we'd have what we call a partial store with four keys. And the next one would have two keys. And so when we'd run the merge operation, we would apply the set merge policy which says basically take one store, take the other store, cycle through keys and the last key wins. If you do that, you can paralyze endlessly.
00:23:28.602 - 00:23:56.594, Speaker A: So we'd have here a complete store with six keys. And now at that place we have a snapshot. We can have periodic snapshots. And so if you want to go and explore the chain at any point in time, you have a snapshot plus a little partial. You can have the state synced at any block height. This one has the last keywind policy, but you have a few others like Min, Max, Add, and another one like first keywind. So if you merge them, you would have set if not exists.
00:23:56.594 - 00:24:21.854, Speaker A: And then that allows us to build different aggregations. Right? You'd like to see that running live a few minutes now I need to bring that other window up. So do you see that? Okay, that's good enough. Okay, so let's imagine we want to see that pools created thing. Okay, do you see that? I want to see the output. I'm going to run that. I hope everything is good.
00:24:21.854 - 00:24:37.026, Speaker A: The demo gods are connecting. Okay. Whoa, not too fast. So this is going through starting at the beginning. And we have there a pool created event. And see that we have everything decoded because it's a protobuff thing. We have the thing to decode it.
00:24:37.026 - 00:25:01.914, Speaker A: We could feed that it arrives on the wire as bytes and properly serialized bytes. And then we see that the token address is there. We have the decimal, we have the address. And so what that means is pretty crazy already. Oh, do I see that. Okay, what that means is that you can inspect the chain with your code at any place in time for a mapper especially you can go there and I could run it again here and say I want to run the mapper. Let's say a block.
00:25:01.914 - 00:25:24.770, Speaker A: I don't know, something more recent. Give me a recent block. What's the block yesterday? 15 7 million? I don't know, something like that. Okay. And see, is there anything recent? So there's some stuff, right? Some things are recent. Can I see that? Someone still created a new pool and I can inspect my code to make sure it works. Where is that? Come on.
00:25:24.770 - 00:25:43.254, Speaker A: Right. This one was wrapped ether in infinity. They just created that address as a new pool. So you can go and test your code everywhere. Once that's done, well, you're set, right? You can go then to the next data dependency. So this really changes dynamics for Debugging. And this can also work for stores because you can ask for a store and it's going to process it in parallel.
00:25:43.254 - 00:26:08.558, Speaker A: And then you can inspect all the keys that exist there or see the deltas coming through. Okay. And let me show you something running in parallel, graph out. Now this is very interesting because let's start it at 15 million. Let's say I want Graph out at 15 million. I didn't run it again before and I want so let's run it. So this starts a whole bunch of parallel processes.
00:26:08.558 - 00:26:25.906, Speaker A: And you see up there the number of blocks per second. Yesterday I had that 8000 on salana blocks, I had 16,000. Depends on the power you put behind there. But this all like you have the pool count is a dependency on the pools. The pools is further down. So we're able to schedule things and all that. Just massively parallel.
00:26:25.906 - 00:27:00.914, Speaker A: And once that's ready, let's say everything was done, I would start streaming and get all the content. And so let me show you the Graph Out. It's very interesting because Graph Out has refined the data up to entities. Now we're talking about database tables and fields. And you get out of that. Do you not see it? Okay, that wait a second here. So let's imagine, see we have token and an update and you have the field derived east and you have the old value and the new value.
00:27:00.914 - 00:27:30.358, Speaker A: I don't know if you've seen this thing in the data science world, this looks very much like Change Data Capture CDCs that can power a lot of large scale systems. And you have the prior and after. So you can feed that to your, let's say postgres, apply the changes. And when you have an undo signal, it gives you back that payload. You then just flip everything and you have guaranteed linearity. So you're stored with a cursor and it's flawless. It's just extremely simple to keep your things in sync.
00:27:30.358 - 00:27:52.366, Speaker A: You also want to have a slack body, can have an undo message. Remove the message if you have a thing coming through. Right? So I think that's pretty cool. What do you think? Okay, that's cool. Can I shut that down here? Do I have another window I'm close to? Done. Prepare your question, please. Ask them succinctly we have a half a minute.
00:27:52.366 - 00:28:43.506, Speaker A: I just wanted to have a final note there. So as a final note, I wanted to share with you a little bit of my vision for the graph. Okay. I don't know where the window is, so whatever it just says fine. I'm imagining the Graph becoming that sort of huge worldwide cluster of processing and storage capabilities and something like Google's BigQuery, but where people join because it's better together instead of running it alone, where you need to have all the resources alone. And I see also a new era of composability, which means more collaboration and in a tighter community working together more intimately with those data contracts. And I see also a new mix of collaboration between indexers like exchanging data or sharing resources in terms of compute and storage and.
00:28:43.506 - 00:29:16.750, Speaker A: Whatnot therefore introducing new value flows. And also I'm seeing new products, new services being offered directly on the network to satisfy some needs that perhaps couldn't be addressed before. And I mean, there's a place for you in there as a developer, as an indexer, as someone who realizes the radical benefits of such a platform and who builds on it and promotes it. But my ask to you is you go to go try substreams. Put pressure on your favorite layer one, so that they do integrate the Firehose natively, that's pristine's. Aptos has done that recently. Some other sarkware, I think.
00:29:16.750 - 00:29:32.530, Speaker A: So that makes it everything we've seen today becomes immediately available to them. Sell them on the goodies. So also join our discord. I would love follow up questions and come see me afterward. I love feedback on these sort of things. All of this is open source. So let's dig and build something together.
00:29:32.530 - 00:29:45.926, Speaker A: The biggest blockchain platform on Earth. Thank you for your time today. Do we have time for two, three questions? We're the last one. So if you have a question hey.
00:29:46.028 - 00:30:05.070, Speaker B: So one question. Modularity and composability of these substreams is super, super powerful. But still, if I look at this compared to SQL and DBT models, it's a lot more complex. So how can we enable people to really kind of learn this and build these kind of hypermodula data streams?
00:30:05.810 - 00:30:32.934, Speaker A: It's a good question, but the transformation layer is not the SQL layer. Like this is Powering going through history. It's an ADOC transform with stateful storage. But you would pipeline into SQL store to do other things, right? You would have refinement, you have knowledge from the community as to how to analyze this and that protocol ever increasing refinements. But then you might store that in your store with off chain data and maybe that's best fit for you. Maybe you feed it into a subgraph. That's what you need.
00:30:32.934 - 00:30:54.798, Speaker A: You have a total decentralized solution and you don't need to host anything. So this is enabler at a lower level. It doesn't seek to replace SQL, but it puts itself at a place where we can feed all the systems on Earth with enriched data, which you would need to do in SQL. And it's really not fun. So you leave that to the community, right? Gotcha. Thank you.
00:30:54.964 - 00:31:06.402, Speaker B: We have an old subgraph which is pretty slow and we would like to transform it to the new type of subgraph. Should I only read some code on Rust and that's it or something else?
00:31:06.536 - 00:31:26.694, Speaker A: So it is not the same paradigm. To enable parallelization, you need to distinguish the data dependencies and that infers a number of stage of parallelization that is needed. It's not easy at all. Actually. It's pretty crazy to try to parallelize the subgraphs. We try that. That's what yield us to design substreams by cutting uniswap stuff.
00:31:26.694 - 00:31:39.754, Speaker A: So you will want to go and write in Rust modules and it's a different paradigm. So it's not just an easy switch, I admit, but it brings us to the next stage in evolution of blockchain indexing.
00:31:39.922 - 00:31:41.214, Speaker B: I see. Thanks.
00:31:41.412 - 00:31:44.730, Speaker A: Thank you so much, Alexandro. My pleasure.
