00:00:14.970 - 00:00:38.434, Speaker A: Set. Hello, everyone. Thank you for coming or for staying. Those of you who stayed past Patrick's talk understand I don't have his celebrity status. But yeah, I will be talking about sequencers, L2, sequencers and more generally, the principles of ordering and execution. It'll be sort of an survey of the design space. Turn the buttons and forgot.
00:00:38.434 - 00:01:12.194, Speaker A: There we go. Cool. So the stuff that we'll be covering stuff we'll be covering. I'll start with sort of an introduction into sort of the motivation and design rationale of kind of what sequencers are and why we even have them to begin with. On L2. We'll talk about the current state of sequencers in the L2 world, which is basically that they're centralized, but there'll be a little more to say than just that. And then focus on ways we can sort of improve this state of affairs, trust minimizing them, decentralizing them, sort of limiting sequencers power in various other ways.
00:01:12.194 - 00:01:35.580, Speaker A: And then if there's time, we'll sort of wax theoretical about how these concepts could apply to layer one, to what extent they do, to what extent they don't. But we'll see how things go. So oh, yeah, hi. That's me. My name is Daniel. I do engineering and tech research at Offchain Labs. We are the team behind Arbitrum, the L2 that hopefully, you know.
00:01:35.580 - 00:02:15.880, Speaker A: And yeah, I won't be focusing on Arbitrum, but there'll be a bit of Arbitrum perspective, arbitram bias. Okay, to get into the subject of sequencers, I think sort of the fastest way to understand why we have them is to think about why we even have L2s to begin with. Sort of what we want from L2s. Right? So the baseline starting point for what a L2 is, is we're trying to scale ethereum while not introducing new trust assumptions. So inheriting security from ethereum. That's our starting point. The scaling part is we want to improve the status quo of what ethereum is like to use in some other ways.
00:02:15.880 - 00:02:48.174, Speaker A: There's all sorts of ways it could potentially be improved at L2. But notably, we want it to be cheaper because when layer one gets congested, it gets expensive. And we would also like it to be faster because when layer one gets congested, it becomes slow. So an ideal L2 would just sort of do all of those things. What we've sort of seen in the L2 space, I pace around, so I have to get used to not leaving the mic. Forgive me. What we've seen in the L2 space is that basically when we're talking about L2, in the context of ethereum, we're talking about roll ups.
00:02:48.174 - 00:03:30.490, Speaker A: That's the design that's dominated. And that's in large part, I think, because of the UX, the sort of transition in UX, if you're sort of used to using ethereum, used to layer one, it's actually just very similar. You can use a lot of the same tools. It feels very similar. But the kind of key trick that roll ups use as L2s is they require that you publish transaction data on layer one itself. And by doing so in a roundabout way that I won't get into, that's how we're able to claim that these things are trustless, that they inherit l one security. So via L2 magic, we can sort of enforce the safety of the L2 train of the L2 chain after data is published.
00:03:30.490 - 00:03:50.654, Speaker A: But essentially all we're doing is publishing data as far as layer one is concerned. So all of the other work that goes into processing a transaction, validating it updating state, that happens in a separate environment. We're not using layer one resources, so we can make things cheaper. So, long story short, by publishing data on layer one, we get trustlessness. We needed that. Yeah. We get cheaper transactions.
00:03:50.654 - 00:04:34.606, Speaker A: We needed that. The other thing we said we wanted, or top of the list of things we wanted, was fast transactions, right? So do roll ups give you fast transactions, it turns out, is kind of a loaded question, right? So if you've used any roll ups, you've probably noticed it's faster than using layer one. But it sort of depends what we mean by fast transactions. And even right here, with what's on this slide, we can see we have a bit of a contradiction here, right? By fast we mean faster than layer one. But what I said is that the requirement of roll ups is that we post data on layer one. You can't go faster than layer one by posting data on layer one because that's a circular problem or perhaps a triangular problem. I'll be honest, this slide probably isn't necessary, but I really wanted to include a trilemma somehow.
00:04:34.606 - 00:04:59.642, Speaker A: So this is a new trilemma. I don't expect it to catch on, but this is another way of thinking about the sort of situation we're in with roll ups, right? We have this nice feature of open participation that's kind of directly downstream from the fact that we publish data on layer one. So we can have trustlessness. But that means we can't quite have fast finality. We can only have two of these things. There's other l Two designs called channels which give us fast finality and trustlessness. But the UX is very different.
00:04:59.642 - 00:05:44.970, Speaker A: They don't have the flexibility of roll ups and so on. So we kind of have to decide where in the design space we're going to live. And in fact, by the way, those of us the arbitrary mogs in the crowd who followed our first testnet release, it had no notion of fast transactions. It was entirely like that diagonal line, the one that says roll ups, right? And as we put that out, one of the questions we kept getting was, okay, it's L2. Aren't we going to get fast transactions? And we kind of just said, no, that's not really possible, but there was clearly demand for it, right? So we kind of reached this point, this kind of middle ground settlement where we said, okay, what if we can provide a fast pass? That's trusted, but it's optional. And we were not the first L2 team to come up with this or do this by any means. Pretty much all l two s do this.
00:05:44.970 - 00:06:15.854, Speaker A: In fact, we actually came around to this idea later. And the idea here is, you can imagine. So you have a situation where there is some party, a user can trust them. The trusted party says, I promise to include your transaction later, and then hopefully later it does. That's basically what it comes down to, the sort of version of this that doesn't work. The naive solution is you let a user trust whoever they want, whomever they want. So if a user just decides, I trust this random party, that party promises to include their transaction.
00:06:15.854 - 00:06:49.150, Speaker A: Even if the party is trustworthy. This doesn't quite work because that party can't really predict the future. And even if it thinks it will include your transaction in a given order, someone else kind of might get there first. So even to have a trusted solution like this, you need to sort of enshrine you have to enshrine a party within the protocol, give it this special permissioned privilege, and that's kind of what the sequencer is. So the sequencer, in other words, is literally this party that we give the ability to directly post transactions into L2. Everyone else kind of has to wait. So the sequencer has this narrow, short term view of what will happen on L2.
00:06:49.150 - 00:07:27.094, Speaker A: Therefore it can give trusted kind of soft confirmation transactions, we call them. So to sort of recap that, for those who just saw Patrick's talk, this is a bit redundant, so I'll go quickly. But when we introduce a sequencer, the sort of lifecycle of a transaction looks something like this. In the sort of normal state of affairs, the user kind of gives their transaction off to the sequencer. The sequencer immediately gives this promise, totally trusted, that says, I will impute your transaction later. At some point later, in the case of Arbitrum, usually every few minutes, it'll post a batch of transactions on chain. And at that point, the sequencer is kind of out of the picture.
00:07:27.094 - 00:07:53.522, Speaker A: Once it's on chain, we're in full roll up mode, full trusted mode. It's committed to this particular ordering. And as far as a user is concerned, your transaction is as finalized as a layer one transaction, right? Because all of the data is available there. Anybody can execute it. Anybody can see what the final state will be. The actual explicit claim or commitment to what that final state actually is happens later, and the sequencer is not involved. And that's that third step there.
00:07:53.522 - 00:08:33.354, Speaker A: The validator asserts the state, but there's no rush to do that. That's really just so that we can communicate back to layer one and process withdrawals, but from the layer, if you're just interacting on L2, once your data is on Chain, you're done. So that gives us this way that we can get this nice fast, low latency fast transaction path thrusted if you want to. But it's optional and that's very important. So just to drill that point home, because you should be suspicious when I start saying it's optional. What does it mean that using the sequencer is optional? Well, in the sort of normal case, the system's working well, if the sequencer gives you this fast promise, you can just ignore it and say, screw you, I don't care what you say, I'm going to wait for you to post on chain. So it takes another few minutes and then you get the full sort of trustless finality.
00:08:33.354 - 00:09:16.122, Speaker A: In the unhappy case where the sequencer just goes rogue and isn't even answering anything, there is still a way that you can do anything you want on Arbitrum. There's this sort of alternative path, basically. Long story short, it's a lot slower. I think Patrick talked about this, well, I'm going to stop talking about him, but it's on my mind. But yeah, the point is, the system can work in a sort of slower way, in a slightly more inconvenient way. It can work without the sequencer entirely, so it's entirely optional and that's why we can still call this thing a L2 and sleep at night. So I've just been talking about the sequencer like it's my friend or something, but what actually is it? And basically we just define the sequencer as this entity that can give these fast promises.
00:09:16.122 - 00:10:03.770, Speaker A: That has all the properties that I had. But you might ask, how does it decide which transactions to include, who controls it, in what order these transactions go and so on. And basically the design space for what the sequencer is, is pretty open. It could be anything, right? It's going to be controlled by some smart contract. You could swap in whatever sort of mechanism you can think of with the one caveat that because the whole point is to give fast transactions, whatever mechanism we use can't involve interacting with layer one, because that just defeats the whole purpose, at least the way I see it. The way we see it, other people would describe it differently, but yeah, it could be all sorts of things. So the current state of affairs of what sequencers actually are in L2? As far as I can tell, I think at least predominantly, most of the L2 s, most of the major ones is that they're centralized.
00:10:03.770 - 00:10:34.882, Speaker A: Somebody did something fancy on a L2 that I don't know about, you can yell at me afterwards, I apologize, but generally speaking, sequencers are centralized. That's the status quo. And we get asked about this a lot. It's probably the most frequent question that has to do with progressive decentralization and decentralization roadmap is like when are you decentralizing the sequence? And it's not a bad question, it's a good question. Here's a good things to ask about. But I think often it comes from a bit of a misunderstanding. So there's kind of two things to say to this issue of centralized sequencers.
00:10:34.882 - 00:11:02.670, Speaker A: And the first thing is it might not be as bad as it initially appears. Specifically, centralized sequencers. I'm suddenly nervous someone's going to screenshot this slide. If you're going to screenshot this slide, screenshot the next one too. That's all I ask. Okay, because the next one's very important. I'm not claiming centralized sequences are fine, but the first half of the answer is the power that a sequencer had is very limited and circumscribed, right? It can't, for example, simply steal money from the system and it can't lock up users funds forever.
00:11:02.670 - 00:11:38.282, Speaker A: Other parts of the system are sort of more important, even a centralized sequencer. Arguably you could argue that an L2 could just have a centralized sequencer for good. That wouldn't be the end of the world. And the other thing that's worth saying here is that again, on just about every L2, certainly arbitram included in the current state of affairs, they all sort of have more fundamentally centralized parts. So even if we said, hey, we decentralize the sequencer, if we didn't take care of those other things, it sort of doesn't really give you anything. And you can read more about that on our docs or L2 beat. But we don't want to mislead by emphasizing one thing and not another.
00:11:38.282 - 00:12:15.400, Speaker A: So those other things contract upgradability the power, validators have, those are the things that you if you want to spam our discord about something, those are the more important things. Don't spam our discord, you know what I mean? Okay, important follow up. Obviously, having a centralized sequencer is not ideal. Looking at the time, I'm going to try to speed up a little. What can a centralized sequencer do that's bad? Well, even if it's honest, honest mistakes happen, right? So even an honest sequencer could have downtime because of infrastructure failures and server issues. And that's bad because it sort of slows the whole system down. It's very inconvenient going to skip over one thing in there.
00:12:15.400 - 00:12:44.622, Speaker A: The juicier stuff is what if the sequencer is actually malicious? It just turns evil. What can it do? Well, it can equivocate. In other words, it can make one of these promises and then not make good on it later. It can make inconsistent promises to different users, right? These fast transactions are trusted. It can violate that trust. It can't censor you technically, but it can temporarily censor you, right? So if it stops processing your transaction, you'll be able to get it through eventually. And we like emphasizing that fact, but the also important fact is there is a short window of time that you'll have to just wait.
00:12:44.622 - 00:13:08.438, Speaker A: And that also could suck, right? Being unable to transact for some number of hours might be a real problem. The sequencer can do that to you. And then finally, the kind of elephant in the room here is mev. Mev. Okay? So all I want to say here, there's plenty of other talks about mev, people who have more and more interesting things to say about mev than I do. I'm not an expert. I don't even know what the M stands for.
00:13:08.438 - 00:13:55.078, Speaker A: It's unclear. At this point it was minor, now it's maximum. I learned when I was procrastinating this, this is called an orphaned initialism if it stops meaning anything tangent. But when we talk about mev, we're talking about the power you get when you have the power of ordering transactions, particularly the power to order them in such a way that benefits you, the orderer to extract value from them. And what I would say is this whole architecture of introducing a sequencer has this side effect that the sequencer has control over transaction ordering, which means if it's economically interested, it might use this to extract value. Whether this is a bad thing we need to fix or an opportunity to take advantage of is sort of a philosophical debate. But we can say this is the case with sequencers, right? We have this power, we need to at least think about it.
00:13:55.078 - 00:14:36.782, Speaker A: And yeah, the way we think about it is in terms of sort of minimizing it. So I'm going to sort of run through some of the strategies that we can be used to sort of improve this bad situation of centralized sequencers. And they'll all involve improving kind of one of these four things mev, equivocation, blah, blah, blah. Okay? So the simplest thing that we can add to sequencers to make them better a little bit is the sort of crypto economic penalties. We can require sequencers to be staked and we can say now if they particularly equivocate, basically when they give these off chain promises, they'll be signed. So they give a signed promise and then users can use that to prove that they equivocated. And if they equivocate, we can just slash the sequencer.
00:14:36.782 - 00:15:03.194, Speaker A: So that's cool. This helps with the equivocation problem, not the other problems. And in fact, this could be applied even to centralized sequencers just as easily. Really, any sequencer mechanism this could and probably should be applied. A thing to note here is all we can really do in this case is punish the sequencer. We can't sort of rectify the situation for users and that's because if the sequencer is equivocating, each of these claims are sort of internally consistent. They're just inconsistent with each other.
00:15:03.194 - 00:15:47.578, Speaker A: So it's not really clear which one is the more valid one to take either way. If we simply took one, we'd be screwing over the other user unfairly. But all we really can prove is that the sequencer did something wrong. So, okay, we have this provable cost to equivocation for the sequencer, something in terms of particularly the mev problem. Shout out to shutter here who's working on this strategy. I probably should have put their logo bigger or something, but there's this idea of threshold encryption and sort of using threshold encryption to minimize the mev that a sequencer can extract. And the idea here is when a user, instead of simply passing their transactions onto the sequencer directly, there'll be this step.
00:15:47.578 - 00:16:12.818, Speaker A: They pass it in this encrypted form and they encrypt it. There's sort of this network, they call them keepers, which is kind of cute. They do this distributed key generation. So you encrypt your transactions, give it to the sequencer, the sequencer commits to an ordering blindly. It doesn't know the contents of the transactions, commits to it off chain. Right? And then only after it commits to them do we sort of reveal the contents of it. So now the sequencer can't easily extract value because it doesn't know what it's looking at.
00:16:12.818 - 00:16:41.654, Speaker A: So this is cool. And there's some added levels, some potential concerns. Let's say the parties that are in charge of this distributed key generation. The idea is you distribute it so that they can't easily collude. But if they do collude, they could, for example, reveal the contents of the transactions to the sequencer before you want to. Also, if they sort of don't reveal keys in time, things like that, there's certain ways that they can delay things further. And remember, this was kind of all about improving latency.
00:16:41.654 - 00:17:09.560, Speaker A: So we do want to take latency concerns seriously. Even in the sort of normal case, when you're doing this threshold encryption stuff, there's rounds of communication that are required, so it's inevitably going to add some latency. So that's one concern in our eyes. But, yeah, cool stuff. So, I mean, these two techniques so far, again, you could apply these to a centralized sequencer. We haven't really talked about decentralizing, it just limiting its power. Now, let's get to some ideas for more properly decentralizing it.
00:17:09.560 - 00:17:49.138, Speaker A: So one design space, let's say, is that of mev auctions, as it's known, proposed by some of the folks at Optimism, along with a few others some years ago. I'm going to describe it sort of in the abstract. I'm not claiming this is their plan or anything like that, just sort of talking about the design space. So the idea here is you can imagine at any given time we can point to who the sequencer is. It's still one specific party, even a centralized party, but every so often over time, we hold an auction and you can buy the right to become the sequencer. Okay? In that sense, it becomes, in the big picture, permissionless. Now, why would you want to be a sequencer? Seems like a thankless job.
00:17:49.138 - 00:18:36.900, Speaker A: The answer is, by being a sequencer, you have the power and the ability to profit by extracting mev. So this design kind of leans into the mev thing and kind of says, yeah, let's take advantage of this and use mev as a revenue source. Again, that's sort of a philosophical distinction or an ideological one even in terms of how we want to handle it. I think important in the mental model for these is that these auctions kind of have to be infrequent. It's not as though these potential sequencers are sitting, looking at transactions, seeing ways to extract value from particular transactions and then bidding on the rights to order them. It's more like they're bidding on their future potential for ordering. They're taking a bet on their own mev powers in the future.
00:18:36.900 - 00:19:23.390, Speaker A: They can't really be frequent for a few reasons. These auctions, you can't have frequent sequencer turnover. The main reason being that we sort of at any given point have to know who the sequencer is so that it can give these fast transactions, right? Otherwise, again, it sort of defeats the whole purpose. So imagine on the order of maybe hours, maybe days, I don't know, we hold these auctions. So I'll talk about why myself, I'll just speak for myself. But generally us at off chain labs are sort of resistant to this design space because it depends on the ability to order transactions and extract mev. This is sort of inherently at ODS with the other thing that we want Sequencers to do, which is give low latency simply because you need some time to figure out the optimal order of transactions to extract value from them.
00:19:23.390 - 00:20:00.038, Speaker A: So there's some tension there in practice. Maybe that won't matter, maybe they only need a few extra seconds. But it's not nothing, right? And again, the way I kind of see it, latency is the name of the game with sequencers. That's one thing because these auctions sort of have to be infrequent. Like I said, you have this temporary centralization. There's some concerns about some random party coming in and censoring transactions, griefing them. It's sort of the double edged sword of decentralization in this case, right? You open it up to any party, but that means any party can come in and sort of not do what they're supposed to and things like not including transactions.
00:20:00.038 - 00:21:10.834, Speaker A: It's a hard thing to prove, it's a hard thing to punish because of data availability problems and stuff. The other practical, the thing that we would probably expect is that there'll be a single party that just keeps winning the auction again and again, simply because whoever sort of best optimizes for this, there will be some party who does that, right? So it's open, but you might get a sort of practical centralization. But again, the bigger thing is this question of are we really comfortable with leaning into the idea of extracting mev or introducing new parties that have this mev power versus minimizing? It so brill. I'll try to just get this in before I stop because I know we're almost done, but I want to talk a bit about sort of where our head's at at offchain labs in terms of decentralizing the sequencer. So the model here, this sort of fair ordering model, is we replace the sort of single party sequencer with a fixed federation or committee of sequencers. And these, in order to give a receipt, they have to come to consensus, sort of like a BFT style consensus. But it has this special property which is the ordering of transactions is sort of enforced within the consensus itself.
00:21:10.834 - 00:21:57.726, Speaker A: So unlike a lot of BFT leader selection algorithms, where there's a leader is chosen and that party kind of has full control over what to propose. In this case that control is distributed and we enforce fair ordering. What we mean by fair ordering, it's a little hard to formalize, but it's something like you have transaction A and B and if a supermajority of sequencers kind of witness transaction A coming before transaction B, then as long as the supermajority is honest, that will also be the result. Some nice research has been done, some nice progress has been made into sort of improving the style of algorithm. The initial ones were just practically required a lot of network level communication, so they weren't usable. But the latest thing called themists is kind of a nice breakthrough which makes it viable. So this does introduce an honesty assumption.
00:21:57.726 - 00:22:49.362, Speaker A: We have a fixed set of parties. We're assuming an honest supermajority. Again, any sequencer solution is going to have some centralization in. It can't truly be open, but that is a definite downside. The more interesting downside here, and this is some of the pushback that we got, has to do with, okay, we've taken power away from sequencers, but now if you're a power user who's trying to extract mev, what world does this create for you and what are you capable of doing? So just because okay, so we have fair ordering. There's still this question of what exactly is the ordering that we're enforcing? And initially we were thinking FIFO, first in, first out, right? The order that the sequencer sees them is the order that they sort of give the receipts and publish transactions, which is what the centralized sequencer does. Now, if you trust me, the issue here is now you can imagine a power user who sees an arbitrage opportunity or something if they want to get there first.
00:22:49.362 - 00:23:37.726, Speaker A: Now they're incentivized to have direct network level access to the sequencers, like literally set up servers geographically close to the sequencers and have fancy hardware so they can communicate clearly, which is sort of all exogenous to the system. It's not really something we want to incentivize. It's sort of benefiting users in this weird way and probably a pull towards centralization because someone will optimize that best so we can do better. So this idea, the top of the slide is cut off but very recently we got this proposal from some of the folks at Flashbots, particularly Shin Aka. I guess Sexy Sun is that you pronounce it. But the idea here is we have a sort of hybrid ordering policy so we can keep our fair ordering algorithm, but we don't enforce the fairness at the transaction level. We enforce it in these sort of chunks of time.
00:23:37.726 - 00:23:57.890, Speaker A: We can imagine discrete intervals of like half a second. I think that's the wrap it up music. Anyway, this is a really interesting topic. It's very new and fresh, so I recommend if you're a researcher or interested person, this is a good place to start in terms of fair sequencing. I have more to say about l one, but you can find me later. Thank you all for listening.
