00:00:00.160 - 00:00:03.914, Speaker A: Novich. I hope I pronounced your name correctly, Ms. Love.
00:00:04.694 - 00:00:05.894, Speaker B: Yeah, it's correct.
00:00:06.014 - 00:00:25.594, Speaker A: Okay. And Ms. Love is a PhD student at ETH with Martin Vichev, and he has done some very nice work in applying machine learning to SMT solvers and solvers in general. So without further ado, Ms. Love, the floor is yours. Please go ahead.
00:00:26.804 - 00:00:59.880, Speaker B: Thanks for the introduction. Also, thanks for the invitation to speak at this workshop. I hope you can see my slides today. I will tell you about the work that we did on learning how to synthesize strategies for SMT solvers. So this was work that we presented at Newreps 2018, and it was joint work with my colleagues at ETh, Pavel and Martin. So I guess everybody here is familiar with SmT solvers. But let me just give you an example.
00:00:59.880 - 00:01:57.028, Speaker B: Anyway. So there is a formula, Phi, which consists of three variables here and some set of constraints. And then our question is to find an assignment to all the free variables such that this formula evaluates to true. And as we know, the standard tool to solve such a problem is an SMT solver. So once we give this logic formula phi to the SMT solver as an input, then the solver will produce either the assignment to this variable so that formula is true, which is called a model, or it will provide a proof that the formula is unsatisfiable. So of course, we also know that solving this problem in full generality is very hard. For example, just looking at the quantifier free boolean subset of the SMT, we know that it's mp complete.
00:01:57.028 - 00:03:10.546, Speaker B: So we cannot hope to actually solve this in full generality. However, what we actually care about in practice is that users have some specific applications that they want to use, SMT four, such as software verification or program synthesis. And then for these applications, users convert them to the SMT formulas and run SMT solvers on those. And now, the developers of SMT solvers, they crafted some very smart heuristics, which tend to perform very well in these practical settings that arise from such applications. And then once we have some formula, then the key question is which heuristic to apply. So given a formula phi, then what SMT solar does is that it goes through the list of possible heuristics and then examines the formula. So it checks whether the formula is in some theory, and it also checks properties like how many variables there are in the formula.
00:03:10.546 - 00:04:01.744, Speaker B: And then based on these properties, it picks one of the heuristics. And if this is chosen well, then the solar will terminate quickly with an answer set or unset. So this is what happens in practice. However, one drawback with this is that if you are a user and you have some new benchmark that you want to run, so you want to solve your benchmark, then it may easily perform badly on your own problem. And the reason is that these heuristics in SMT solvers, they are designed quite well to cover wide range of scenarios. So they work on wide range of benchmarks. But if you really want to solve just your benchmark and solve it well, then you maybe want something tailored to your own benchmark.
00:04:01.744 - 00:05:14.674, Speaker B: And another issue is that if you actually run this on your benchmark and it doesn't work that well, then you may need to go deep into the heuristics of the solver and look into the implementation and try to fix it. But this may require expert knowledge and these two problems can actually limit wider adoption of these tools. And here in this work, we want to address this. So one paper that motivated our approach is called the strategy challenge in SMT solving, where the authors asked SMT community to try to develop some methods through which users can control these heuristic aspects of SMT solvers. And then this motivated us to look actually into the machine learning approach to this problem, where we would fully automate this. So user would supply some formulas and then we would learn strategies that actually perform well on these formulas. So that is the work fast SMT which we presented at neurips.
00:05:14.674 - 00:06:01.970, Speaker B: So let me just give brief overview of the setup. So we assume that user has some set of formulas that they want to solve, and then they would run fast SMT on some of these formulas. And what fast SMT would do is it would learn some strategies that solve many of these formulas and solve them as quickly as possible. And then once we have this, then we can just plug it in the existing solver and use it instead of the default strategy. And now if this is done well, then it would work nicely for the problem. And one important thing is here, that we will assume no prior knowledge of the internal strategies used by the existing solvers. So we want to be fully general.
00:06:01.970 - 00:07:04.274, Speaker B: But of course, we will assume knowledge of the building blocks of the strategies, which are called tactics, which I will explain later. So this is the overall, this is the goal we will try to achieve. So how does SMT solver actually apply these tactics? So what SMT solver does is that it starts with some formula, phi, and then it decides to apply some transformation which rewrites the formula or partially solves it. So we can see here that the solver applies tactic t one and then it arrives at a new formula, phi one, and then it is done repeatedly. So solver chooses some tactic and then transforms the formula using this tactic, until in the end we obtain formula that is trivially satisfiable or trivially unsatisfiable. And in that case we are done. So let me give some examples of heuristics in the solvers.
00:07:04.274 - 00:07:55.874, Speaker B: For example, constant folding heuristic exploits the fact that addition of zero in integer theory can be omitted because it has no effect. Normalization of the bounds can introduce a variable substitution so that left hand sign of the inequality is equal to zero, which can simplify solving. Then bit blasting can change the representation from integer to bit vector. And there are many more of these such tactics. There are overall around 100 of such tactics in z three solver. In this work, we will treat all of these actions as black boxes. So these tactics will be just some actions for us, and we will not assume any knowledge of the internal workings of these strategies.
00:07:55.874 - 00:09:00.704, Speaker B: Good thing about this is that then if somebody implements a new tactic, then it can be easily integrated into our framework. Another key ingredient that SMT solvers enable are using predicates. So what we can do is we can given a formula phy, we can evaluate some predicate p on that formula, and then based on whether the predicate is true or false, we choose either tactic t one or tactic t two. For example, we can check whether the number of atoms in the formula is less than ten, and in that case we choose tactic t one and otherwise we choose t two. And this allows SMT solver to have great flexibility, because otherwise it would just be able to apply same sequence of tactics to all of the formulas. And there are many of these formula measures that we can use to build up the predicate, such as number of atoms or the formula size. And we can also have conjunctions and disjunctions of these rules.
00:09:00.704 - 00:10:15.476, Speaker B: So here is an example of a real strategy implemented in solar ices, where the predicate checks whether the formula is not in a difference fragment, and whether the number of inequalities divided by number of uninterpreted constants is below some threshold k. And if it is, then we choose simplex algorithm and otherwise we choose floyd Warshall algorithm. So this is one of the heuristics that was handcrafted by the developers of this SMT solver. And in this work our goal will be to automatically learn strategies like this. And of course we will not learn this strategy, but we will learn strategy that works best for the set of formulas that user is interested in. So once we want to do that, we can look at this bigger picture where what we want to do is that for each formula we want to choose some tactic to apply. But if you look at this search tree, then we can see that since it is exponential, then there are too many paths which we can take.
00:10:15.476 - 00:10:58.834, Speaker B: So for example here we can take the green path and solve the formula in 12 seconds, or we can take yellow path and solve the formula in 500 seconds. Or we can take red pet and not solve the formula at all. So, handcrafted strategy inside the solver, it just determines one of these paths. But this may not necessarily be the optimal one. And we want to learn a policy which will actually select the optimal path. So in this case, we want to learn a policy that will select the green path. So we know that the SMT solver now is instantiated by a formula and the strategy which is a sequence of tactics.
00:10:58.834 - 00:11:56.276, Speaker B: So one simple idea that we may do is we may just have a model which receives formulas and input, and then given this formula, it outputs the entire strategy. And once we have the strategy, we just instantiate it with the solver. So one problem with this approach is that it may actually be hard to learn this, because this strategy can be quite long. And then if the model makes one mistake, so makes one wrong tactic, then the solution may fail, solving of the formula may fail, and the model will not get any feedback at all. So this approach would be quite tricky to make it work. But let's now go back to this example and let's again look at the search tree. So what we can observe is that this is very similar to reinforcement learning setting.
00:11:56.276 - 00:12:47.830, Speaker B: So in reinforcement learning, we have states, actions and rewards. So naturally our states would be corresponding to the current formula that we have. Our actions would correspond to the tactics that we can apply to the formula. And our rewards would be dependent on how long it takes for us to solve the formula. And once we define these three ingredients, states, actions and rewards, then we can actually more easily leverage concepts from machine learning to learn the best strategy. So this is how it would look after we, after we apply this abstraction from two states, actions and rewards. And our goal is to learn the optimal one and difference with the other approach.
00:12:47.830 - 00:13:55.554, Speaker B: Now we start with the formula Phi, and then we query our policy to give us the next tactic. So it would give us tactic t one, we would pass it to the SMT solver and SMT solver would modify our formula to Phi one and this would be done repeatedly. So each time policy produces a new tactic and SMT solver applies this tactic to the formula and until in the end the obtain formula, which is trivially satisfiable or unsatisfiable, and in that case we are done. So this is of course much easier to learn because now the policy just has to output next tactic and not all strategy at once. However, there are also some drawbacks. For example, we lose the internal SMT state. For example, if you started with the integer representation x equals to five, and we applied bit vector bit blasting to turn into bit vector representation, then SMT solver will not be able to remember this because of this decoupling between the policy and SMT, and we will not be able to get a model in the end.
00:13:55.554 - 00:14:41.784, Speaker B: And another problem is that we will have some runtime overhead, because at every step we need to query the policy. And policy can be actually some quite expensive model such as a neural network. And so we can see that there are still some issues with even if we can learn a very good policy, there is still this issue that we have to address. But we can actually use this learning process that we have as a first step. So given some set of formulas, we can perform learning. And I will describe in the next slide what we do for learning. But imagine that we have some procedure to learn the policy.
00:14:41.784 - 00:15:32.540, Speaker B: So once we have this policy, we will use something we call policy extraction. So we will extract strategy from this policy. And strategy will be written in the language of SMT strategies. So recall that this language of SMT strategies uses some specific predicates and conditionals, like it checks for the number of atoms and so on. So this means that you cannot rely on neural network anymore. So we need to convert this kind of uninterpretable model, such as neural network into a strategy that is written in this language of strategies that solver has. But if we manage to do this, then we can just plug in this strategy in the solver instead of the default one.
00:15:32.540 - 00:16:04.064, Speaker B: So we just replace it. And now this is very tight integration with the existing solver. Now if we want to get the model or onset core, this can be easily done. We can essentially use all of the features of the solver and there is no runtime overhead. So now we do not need to query the policy at every step. So these are the two components that I will describe next, learning and policy extraction. And after we have this, then we can actually apply our approach in practice.
00:16:04.064 - 00:16:59.474, Speaker B: So the key component of learning procedure is the policy. And policy essentially can be any function that receives an input formula and it outputs next tactic to apply. And we tried several different models and what we found works best for us is the following. So it consists of three parts. First, we give it as an input the list of tactics that were applied to get the current formula. So this is useful because then we can learn that some tactic is only efficient after some other tactic is applied before then. Another feature is formula measure, such as we can evaluate number of constants or whether the formula belongs to some theory, and we extract these features.
00:16:59.474 - 00:17:39.135, Speaker B: And we can also compute some formula representations such as peg of words or abstract syntax tree. This helps if you want to count, for example, how many additions occur in the formula. So we can embed all of these features. And these are all features that will be helpful to decide what next tactic to choose. And after this is embedded, it can be passed through some model which is some neural network for us. And this model now has kind of two heads as outputs. First head gives probability distribution over tactics.
00:17:39.135 - 00:18:37.268, Speaker B: So in this example it would predict that with 70% chance, the best tactic to apply now is PB to BV, and it also outputs parameters for these tactics. So each tactic can have some number of parameters, such as number of steps in some algorithm. So after I describe the policy, how do we actually train this end to end? That's the next question. So we start with the set of formulas that the user has, and now we can initialize policy randomly. Even just this random policy can be used to sample some strategies, and most of them are going to fail, but some of them will manage to solve the formula, although they will take quite a long time. But we can still use those strategies to build up a training data set. And this training data set essentially is we look at the search tree that we explore so far, and then we look at each node.
00:18:37.268 - 00:19:31.824, Speaker B: In hindsight, what would have been the best action to take in this node? And I will give you more details in the next slide. But once we build up this data set, we can actually retrain the policy so that it takes this best action in hindsight. And after we train the policy, then it will perform much better. And this training can be done using combination of cross entropy loss to predict the next tactic and mean squared loss to predict the parameters. So after we retrain the policy, then we can actually use it to sample again. So we sample new strategies which will hopefully be better than the ones from the random policy, and we improve our data set and then retrain the policy. So this continues again until we are satisfied with our policy or we run out of time.
00:19:31.824 - 00:20:15.866, Speaker B: So this is the end to end training procedure. So here is an example of a dataset. So in this data set, essentially, we have, on the left side, we have a formula that we explored so far. And for each formula, we know the best tactic to apply. So for example, for the middle row formula Phi three, we know that with 80% probability, the next best tactic is t one. So when we build this data set, then we can fit policy PI, so we can fit neural networks to this data set. And now this policy will generalize to the new states.
00:20:15.866 - 00:21:27.174, Speaker B: And this really is the key reason why we use machine learning, because this model will be able to pick up some patterns from this data set that we explored so far. And then once it ends up in some part of the search tree that it has not explored yet, it will be able to generalize these patterns and it will be able to guide itself to the optimal strategy in this part of this part of the tree that he has not seen before. So that is the basic promise of machine learning that we are building on. Okay, so, so far I told you how we get, how we learn this policy, which produces an action given a state, and we can use it to run it on a bunch of formulas. And for each formula, we get, at least we get some strategy, some sequential strategy. But remember that we still need to do the next step, which looks at the search tree. And then when there is this tree of strategies, and then for each node which has multiple outgoing edges, we need to make a decision which action to choose.
00:21:27.174 - 00:22:11.154, Speaker B: So this is something that we still need to do. And let me just give a sketch of what we can do. For example, here we are at some node, and then we have four formulas, phi one to four and four strategies, s one to s four. And each strategy solves some of the formulas, but no strategy solves all of the formulas. And we want to choose some predicate in the allowed language of the SMT solver, so that it splits the formulas into two sets. And then each of the subsets can be solved by the same strategy. So here we can observe that formulas phi, one and four are quite similar.
00:22:11.154 - 00:23:07.516, Speaker B: They can be solved by the same strategies, and similarly, phi two and three can be solved by the same strategies. So ideally, we want to find a predicate which values to true for formulas phi one and four, and to false for formulas phi two and three. And this is, of course, perfect scenario. But in practice, we just need some metric that given a predicate evaluates how good is this split obtained by the predicate. And this will be some heuristic and we will use something similar to decision trees for the people who are familiar with this. Essentially we will define entropy of the set of formulas where we sum up over all strategies. And then we look essentially for how many formulas that this strategy solve and build up this entropy formula based on this.
00:23:07.516 - 00:24:07.350, Speaker B: So here we would split based on this predicate c, which would have perfect entropy of zero by splitting into the blue set and the yellow set of formulas. So far I described this. So now I described also the second part where we extract a strategy from already learned strategies. And now that I described both components of our algorithm, then let's look at the evaluation. So in the evaluation we are going to compare to the state of the art z three SMT solver and we will compare on five benchmarks from SMT competitions. So for each of the competitions we will first run learning procedure. So the first part of our algorithm to learn a set of strategies, and then we will extract a policy, will extract some specific strategy from this policy.
00:24:07.350 - 00:24:46.464, Speaker B: So that's the second part I was describing. And once we have this strategy, we can plug it into the z three and measure the speed up and measure how many more formulas we managed to solve. So we are comparing z three now with two versions. One version is default, one with the default strategy, and another one is with our strategy. And we can show this in this kind of graph where we evaluate on 100 formulas. And for each formula we measure speedup which is shown on the y axis in log scale. And so this horizontal line at one is our baseline.
00:24:46.464 - 00:25:28.948, Speaker B: So that's the z three default strategy. And everything above one means that we are faster than it, and everything below one means that we are slower. So when we evaluate this sage two benchmark, we get quite good results and we can observe two main things. So first, we are always above one. So this means that for all of the formulas we are actually faster. And another thing is that if you look at the median, so the median speed up at 50, this means that for at least half of the formulas we are at least ten x faster than the default strategy. And for some benchmarks we get even higher improvements.
00:25:28.948 - 00:26:20.056, Speaker B: And there are of course some benchmarks where there are formulas where default strategy does quite well and it does better than us. But even there we still improve on lots of formulas. And another thing I want to emphasize is that our approach starts from scratch. So we essentially don't start at this horizontal line with one, but we start at the line which says zero, and we still manage to improve over the default strategy. So this is quite nice. And another thing that we can measure here is the number of formulas that we solve. So once we combine our strategies into one, using the second part of our approach and plug it into the z three solver, then we can compare with the default strategy.
00:26:20.056 - 00:27:36.944, Speaker B: And on these five benchmarks that we consider, then we can see that we can solve many more formulas in some cases. For example, on the stage two benchmark we can solve 500 more formulas than the default strategy. So we can still let, we can see that here we also get quite nice improvements. And I also want to briefly remark that this approach can be generalized also to improve some other tools and not just SMT solvers. So we also in our CCS 19 paper use similar approach to learn how to test ethereum programs with fuzzync. So there essentially you want to generate some test cases for the Ethereum programs, and a tactic corresponds to tactic here corresponds to function call in that program, and strategy corresponds to some sequence of function calls, and then you want to learn a strategy that achieves highest code coverage and discovers as many bugs as possible. So this was quite nice example of how we could apply similar learning strategy in two different settings.
00:27:36.944 - 00:28:24.954, Speaker B: And so I told you about our approach to learn SMT strategies, which consisted of two components, a learning component which I described first, which gives a policy to select the next tactic in the sequence, and the policy extraction component which produces a strategy that can be plugged into the solver. And so all of our materials are open source and can be found on our website. Our code is on GitHub. And with this I would like to conclude my talk. Thanks for the attention and I'm here for any questions that you may have.
00:28:25.854 - 00:28:33.114, Speaker A: Thank you Misla, for a very nice and clear talk. Very good. Boshe, you have some questions.
00:28:34.614 - 00:29:20.764, Speaker C: It's more of a comment that again, very nice, very nice work. I think this idea that we need to take going from deep reinforcement learning to or say some kind of code, extract code that we can use other than just a deep model. This has been explored also in other contexts. And I'll send you some references. There was a PhD student at Tristen Qt, Abhinav Varma. There's a look at different application, but also did kind of the same thing, use deep learning, but then transform into code because it gives you many properties. For example, explainability is much better with code than it is with some deep, deep model.
00:29:21.104 - 00:29:30.804, Speaker B: Yeah, exactly. That's right. So there are also several other settings where this is quite important to do. Thanks.
00:29:32.024 - 00:29:45.744, Speaker A: So I have a couple of questions. So I'm assuming that the strategy that you extract at the end, the overhead due to it is quite minimal. It's the same as the default strategy, right?
00:29:46.844 - 00:30:02.024, Speaker B: Exactly. So default strategy may be somewhat shorter because it's crafted by humans, but we observe that there is no overhead, essentially by plugging in our strategy.
00:30:04.284 - 00:30:38.984, Speaker A: And if I were to, you know, kind of extend this idea to. So if a user were to want to use this, they could use the default strat or the, your strategy that you trained using your tool, the fast SMT tool. But then. But if that doesn't work well, they will probably have to go through the same series of steps. So they just use your tools to do the learning and the policy extraction and then augment the strategy program.
00:30:40.684 - 00:30:48.584, Speaker B: So you mean that they would essentially first try our strategy, and then if it doesn't work, then they can try the default one.
00:30:50.444 - 00:31:01.424, Speaker A: Right. But it's possible that their set of formulas are quite different from the formulas that you trained on. They're kind of out of distribution, if you will. So therefore.
00:31:01.724 - 00:31:27.652, Speaker B: Yeah, exactly. So the idea is that kind of user, when they have some set of formulas, let's say they have 10,000 formulas that they want to solve, and then they train on some small subset of these formulas and learn strategies that works best for them. So the strategies that I described in our experiments, they will not work probably for specific applications that users can have. So they need to retrain on their data set.
00:31:27.748 - 00:31:28.464, Speaker C: All right.
00:31:30.164 - 00:31:48.324, Speaker A: Any other questions? If not, I thank Mislav again for a very nice and clear talk. Thank you. And I hope it's not too late. Wherever you are, Mislav, the next speaker. It's my great pleasure to introduce Professor Kevin Layton Brown.
