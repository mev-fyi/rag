00:00:00.120 - 00:00:17.714, Speaker A: Anyway, it's a pleasure to introduce Simon, who's going to tell us about quantum speedup for graph sparsification, cut approximation, and Laplacian solving. He's a postdoc at NREA and CWI, and he's going to tell us about this joint work with Ronald de Wolf. So please, Simon.
00:00:18.454 - 00:00:53.744, Speaker B: Thank you, Andrew. So, hi, everyone. I hope everybody's doing well, doing good. So, yeah, I'm going to talk about this joint work with Ronald de Wolf, which I'm very happy about. It's about quantum speed that we prove for the problem of graph sparsification and cut approximation and Laplacian solving. All right, feel free, like Andrew mentioned, to interrupt at any time to ask a question or just put it in the chat, and Andrew will forward that to me. So I'll be very happy to, you know, be interrupted essentially.
00:00:53.744 - 00:01:29.536, Speaker B: So, unsurprisingly, this talk is about graphs, and I'll just make two claims. One is that graphs are nice, at least I think so. Graphs show up all over computer science, discrete math, biology, and so on. They can be used to describe abstract things like relations, networks, groups, and again, many more. Now, the second claim that I want to make is that sparse graphs are nicer. What do you mean by sparse graphs? By sparse graphs, I mean graphs that have fewer edges, roughly. So.
00:01:29.536 - 00:01:58.674, Speaker B: I'll go into details of that. Now, why is that? Because they take less space to store. They require less time to process. Solve problems on these graphs typically takes less time. And an example of this phenomenon is that expander graphs, which are used to model or to approximate complete graphs, are completely, much more interesting than complete graphs. Expanded graphs are sparse approximations of complete graphs. All right, so this raises a very natural question.
00:01:58.674 - 00:02:51.272, Speaker B: We can compress general graphs to sparse graphs. Okay, it's basically the topic of a subject of a line of study called graph sparsification. And just to put something straight, I'll be talking about undirected weighted graphs throughout the talk. With a node set v, edge set e, and non negative weights w, our graphs will have n nodes and m edges where m is at most, and choose two. Regarding the edge weights, I just want to fix one thing. An edge which is not present can be seen as an edge with weight zero, which means that basically an edge with a very large weight is kind of a low cost edge in some sense. Okay, now, how do we access this graph? We assume adjacency list access.
00:02:51.272 - 00:03:47.114, Speaker B: What does that mean? It means that we can query with a pair ik, and we get from this query what the graph returns is the k neighbor of node I. So in this case, this would be node j. So there's some arbitrary but fixed ordering on the neighbors of every node. And according to this ordering, we received a k neighbor. All right, so graph specification asks the following question somewhat more specifically, given an input graph g, try to reduce the number of edges of this graph while preserving certain interesting quantities. Okay, so we want to output a graph h, which is one, a sub graph of g, and two potentially re weighed it. So it means the degree of freedom that we have is we can take a subset of the edges of g and we can re weigh certain edges.
00:03:47.114 - 00:05:06.864, Speaker B: And now the goal is to sparsify g while preserving certain interesting quantities. So what are interesting quantities? What do we deem interesting quantities? Some examples are extrema cuts of the graph, such as the min cut or max cut eigenvalues associated to the graph, properties of random walks on the graph, and so on. Now, many of these properties are captured by what we call the graph Laplacian Lg, which is a linear algebraic characterization of the graph. So specifically, we define the graph laplacian of a graph g to be equal to d minus a, where d is a diagonal matrix containing the weights, the sum of the weights of the edges, leaving node I. So basically, if the graph would be unweighted, this would just be the degree, and a is the weighted adjacency matrix. So at the entry ij, we get wij equivalently, and this equivalent formulation will be somewhat useful for later purposes. In this talk, we can define a graph Laplacian as a weighted sum of edge Laplacians, where an edge Laplacian Lij is defined as the outer product of ei minus ej.
00:05:06.864 - 00:05:38.604, Speaker B: So basically this is a very sparse matrix. There's only four nonzero entries, namely two one s on the diagonal and a minus one minus ones on the I j of diagonals. And so any Laplacian can be expressed as a sum of these kind of very sparse terms. So it's a weighted sum of dispersed terms. And specifically we'll be interested in quadratic forms in the Laplacian. So quadratic form can be expressed as follows, where x is a n dimensional vector. So n is the number of nodes.
00:05:38.604 - 00:06:50.654, Speaker B: We basically take the inner product of x transpose with lx. This equals bilinearity is equal to the weighted sum of x transpose, and it's quite easily follows that is equal to the, basically the weighted weighted sum of the squared differences of this vector input vector x. To get some feeling of this, consider the following situation where the input vector x is the indicator vector on some subset s, so it equal, it's equal to one for all indices which are in s, and zero for all indices which are the complement of s. Then we get that this quadratic form reduced to something which is very nice. Namely, this quadratic difference will be equal to one when I j crosses the cut, whereas it will be equal to zero when it does not. Right? So in the picture we get that only the red edges contribute with value one to the sum and the others do not contribute. So basically what we get is that this quadratic form expresses the value of the cut s versus s complement in the graph g.
00:06:50.654 - 00:07:41.974, Speaker B: All right. Now as it turns out, this is just one example of the many things that quadratic forms of the Laplacian express. So, we'll be interested in quadratic forms of the Laplacian like we just mentioned, but also in quadratic forms of the pseudo inverse of the Laplacian, which we'll be be denoting by l plus g. So the Laplacian has a trivial so if the graph is connected, then the Laplacian has a trivial kernel which is just the all ones vector. And so l plus g just means the inverse of the Laplacian, so orthogonal on this, on this kernel. Now, as it turns out, these quadratic forms basically describe not only cut values, but also eigenvalues associated to the graph. So eigenvalues of the Laplacian, specifically effective resistances hitting times, and again many more things.
00:07:41.974 - 00:08:35.457, Speaker B: So it seems interesting that we would try to preserve these quadratic forms. So we wanted to preserve certain interesting quantities. And it seems that these quadratic forms are kind of a nice notion to try and preserve. And this is exactly what is done in spectral sparsification. So in spectral sparsification again we start from some input graph g. We want to construct a sparser graph h by taking a subset of the edges and re weighing them, suggest approximately all quadratic forms are preserved, so the entire set of quadratic forms indexed by all potential n dimensional vectors x. More specifically, we define h to be an epsilon spectral sparsifier of g if and only if the following holds, namely the quadratic form in the Laplacian of H.
00:08:35.457 - 00:09:30.894, Speaker B: Epsilon approximates the quadratic form in the g for all potential x. Right? So we want an epsilon multiplicative error in the approximation here. Equivalently, we can express this condition using the pseudo inverse. So this kind of demonstrates that if we preserve all quadratic forms in the Laplacian, we'll also preserve all quadratic forms in the pseudo inverse of the Laplacian. And again equivalently, without much details, we can formulate this as a PSD ordering, so that lh approximates l approximates lg in this PSD order. This is a very nice notion and it seems kind of useful. But of course, the main question is how sparse can we go? Can we even drop a single edge and still preserve all these quadratic forms? Because you're asking for quite a bit here.
00:09:30.894 - 00:10:25.964, Speaker B: And through a long line of work which started by Karger in 94. Then there was Benzer Karger in 96, Spielman Tang in zero four, and Batz on Spielman Srivastava in zero eight. We learned the following first very non trivial statement. Every graph has an epsilon spectral sparsifier h with a number of edges, o tilde, n over epsilon squared. So what do, what do I mean by o tilde? O tilde is basically hiding poly log vectors in the argument. Okay, so every graph has an epsilon spectral sparsifier with a number of edges scaling approximately as n over epsilon squared. So this is, this is very strong because, because if you only care about, say, constant factor approximations, then basically we can construct a sparsifier which only has a near linear number of edges, so, near linear in the number of nodes which can be quadratically less than the number of edge in the original graph.
00:10:25.964 - 00:11:05.724, Speaker B: And then, second fairy on trigger statement is that the sparse pair can effectively be found in time, near linear in the size of the graph. So throughout the talk I'll be talking about, when I talk about classical algorithms, near linear in the input size means near linear in the number of edges of the input graph. Just a small side note this last work by Bacchus, Spielman and Srivastava, they actually showed that the tilde and the size of the sparsifier is actually not needed. So actually you only need oxygen squared edges. So it was a very strong and surprising result.
00:11:07.504 - 00:11:16.862, Speaker C: Can I ask a question? So, yes, and this is just, you don't introduce new edges in this specification, you just like delete some edges and you maybe rebate them.
00:11:16.958 - 00:11:53.836, Speaker B: Yes, exactly. So what Bachelor and Spielman Srivastava showed is that we actually don't need a tilde here. But I'm putting the tilde because if you wanna find the sparsifier in a tilde on time, then basically at this point we can only do it if there's a tilde. Okay, a small remark. This entire concept sparsification, given this result is only relevant when epsilon is at most square root n over m. Otherwise, we're basically ending up with a graph which has more edges than, or would have more edge than the original graph, which, of course, wouldn't make much sense.
00:11:53.980 - 00:11:57.064, Speaker D: Simon, I think that inequality needs to be reversed now.
00:11:57.604 - 00:12:30.912, Speaker B: Yes, yes, so sorry about that. So epsilon should be at least as large as n square root n over n. Yes. Okay, so we have these results. Are they useful? They turn out to be quite useful. They form an important building stone of many near linear, classical near linear cut approximation algorithms. For instance, they form an important cornerstone of the near linear Maxcut algorithm by Rohan Kail from 16.
00:12:30.912 - 00:13:05.896, Speaker B: And there's a min cut, min st cut, sparse cut, and so on. And all of these algorithms kind of build on this sparsification motion. Similarly, spectral sparsification was introduced by Spiel and Tang. Basically for their work on laplacian solving. They proved in zero four the following famous resulting graph with m edges. The corresponding laplacian system, LGx equals b, can be approximately solved in time near linear in the number of edges. This award and the goedel price in 2015.
00:13:05.896 - 00:13:39.974, Speaker B: And here's a picture of Spielman on the left side and Chang hwa tang on the right side. This again had a slew of applications. Nearly. This gave new linear approximation algorithms for things such as electrical flows, max flows, spectral clustering, and again, many more things. Now, regarding our contribution, we first know that classically, this near linear runtime is optimal for most graph algorithms. It's not all too hard to prove that. Basically by search argument, you usually need to examine all edges in order to solve a certain problem.
00:13:39.974 - 00:14:22.734, Speaker B: Now, the question that we addressed is, can we do better using a quantum computer? So that seemed like an interesting question, and that's exactly what we studied. So our work can be split up in three windows. Basically, first we prove that there exists a quantum algorithm that speeds up classical sparsification algorithms. Namely, we can find an epsilon spectral sparsifier in time square root mn over epsilon. Then we proved that we did our job well and that effectively there's no better quantum algorithm. And finally, we prove number of applications. So we find quantum speedups for a range of problems.
00:14:22.734 - 00:15:10.706, Speaker B: Okay, so I'll start off by our upper bound, namely this quantum algorithm. To share some details about quantum algorithm, I'll first sketch how classical sparsification algorithms typically work. This is a very common blueprint of most classical sparsification algorithms. Most of them, they work through sampling. So basically, probability is associated to every individual edge. And these probabilities have to be interpreted as sort of individual Bernoulli probabilities. What do I mean by that? In the second step, we're gonna keep every edge independently with probability PE and rescale its weight by one over PE, right? So what we could trivially do is we could trivially put all PE is equal to one.
00:15:10.706 - 00:15:56.244, Speaker B: Just keep all edges and everything would be fine. But of course, we want to sparsify the graph. So we want that the sum of pes, which is the expected number of edges in the sparsifier, is small. Now, why do we do this weight rescaling? Basically, it ensures that the expected weight of a single edge in the sparsifier h, equals the expected weight in g. And by linearity, this implies that the Laplacian, which I mentioned is just a weighted sum of edge Laplacians equals the original. So that's fine, we fixed that. But of course, in order to ensure that lh effectively approximates lg, not just an expectation, we need to prove some sort of concentration.
00:15:56.244 - 00:16:45.730, Speaker B: What's a way to ensure this concentration? This was shown in a very nice paper by Spielmann and Srivastava in zero eight. So you'll see that Spielman is figuring all throughout this talk, basically showed that it suffices. So basically, we can ensure concentration by giving a high sampling probability to edges with a high effective resistance. So in some sense, the effective resistance of an edge is a measure for how important this edge is. But of course, I mean, what is the effective resistance? So, the effective resistance of an hijack is defined in a sort of electric networks analogy. So what we do is we basically replace all edges of the graph by unit resistors. So from time to time, I'm going to assume that the graph is unweighted.
00:16:45.730 - 00:17:45.423, Speaker B: Like in this case, for an unweighted graph, we just replaced every edge by a unit resistor, and then we measure what the resistance is between nodes I and j. So somewhat more intuitively, by Ohm's law, this equals the voltage difference that we have to impose between node I and node j, so as to flow a unit current from node I to j. So if in g are in some sense badly connected, then we're going to have to put a lot more voltage to make a unit current flow from I to j. So the following rule of fifth applies. If there are many short and parallel paths from I to j, so basically many short and this joint paths from I to j, then the effective distance will be small. And you kind of understand that because this edge will be less important if there are many parallel paths in some sense. So in this example, the red edge would have effective distance equal to one, whereas the black edges would have effective resistance scaling as one over n.
00:17:45.423 - 00:18:56.534, Speaker B: For a black edge, you can simply see that by taking, let's say, two nodes in the left side, and then you can see that apart from this one edge which connects these two nodes, you can easily find approximately n disjoint paths of length two connecting these endpoints. So this shows that this black edge has an effective, which is small. And this indeed justifies our intuition that these edges on the left or on the right within these cliques are somewhat superfluous or redundant, whereas central edge will be very important. But of course, we're interested in algorithms, not just statements about graphs and edges. So we have to ask the following question. Is it possible to sort of efficiently identify which edges have high resistance and hence are important? And this example is kind of easy to see, but in general, this might not be the case. And then there's this very nice result by Kutzen su, which we use, namely kuzen su in 14, proved that a graph spanner must contain all high resistance edges.
00:18:56.534 - 00:19:52.832, Speaker B: So I'm just bringing a new concept which is kind of easy, but I promise, at least for a bit, this will be the last concept that I introduce. So, grass spanner, what is that? A grass spanner is again a subgraph of g, a sparse subgraph, now with o tilde and edges. This time it's not re weighted, so it's just strictly a subset of the edges, such that all distances in f, where by distance we mean shortest path distances, are stretched by at most log n. So note that if we take a sub graph and look at the shortest path distance, then this can only increase, right? So if we drop edges, then the shortest path might only increase. And so what we want to ensure is that the distances don't get stretched by too much. So, distance, a certain distance in g can be log n times this distance in f, but no more. So we want to go from graph g to sparse graph f.
00:19:52.832 - 00:20:56.174, Speaker B: So, for instance, this red edge in g should have an alternative path, and f, which has length at most log n in this case, and this is just four. And so what they proved is that a grass pattern must necessarily contain all high resistance adjacent. So just to give some intuition about why this is the case, and the proof is not too hard, it's kind of elegant, but I'll just give the idea for r equals to one. So if the graph is unweighted then if an edge has effective resistance equal to one, then that means that there are no alternative paths between the endpoints. Basically this is like we saw in this one graph where the red edge had effective distance equal to one. Now since there are no alternative paths between, let's say node I and node j apart from this edge, we have to keep this edge in the spanner because there's no alternative path. And so the length, if we drop this path between I and j, the shortest path would be infinity, right? So we necessarily have to keep this edge present in the spanner.
00:20:56.174 - 00:21:45.570, Speaker B: And basically a similar argument applies to show that all high resistance edges should be contained inside the graph spanner. Okay, so given this stuff, given this information, we can now propose somewhat more applied sparsification algorithm, which is an iterative sparsification algorithm. So what we can do is we first construct o tilde one over epsilon squared spanners. We keep these edges. So we just construct enough spanners basically to cover all the edges which are important, all the edges of high effective resistance, and all edges which are not inside these spanners, we can feel confident that they should be less important. And so we can kind of downsample them. This is what we do in step two.
00:21:45.570 - 00:22:37.224, Speaker B: Like all the remaining edges, we keep each one individually with probability one half and then double its weight, basically to ensure the expected value equals the weight in the original graph. So thinking back about the former scheme where we associate pes to the individual edges, this corresponds to setting pe equal to one for the spanner edges and pe equal to one, two for the other edges. And then basically using results from Sriman and Srivastava and Kutuzensu, it's not too hard to prove that with high probability the output of this scheme will be in epsilon spectral spot with m over two plus o tilde n over epsilon squared edges. Right? So m over two, because in step two we're basically down sampling by a factor two. And then there's this extra overhead which corresponds from step one. Right, we construct o tilde one over epsilon squared spanners. And each spanner has size tilde n.
00:22:37.224 - 00:23:17.584, Speaker B: So if we repeat this process a logarithmic number of times, we get an epsilon spectral sparsifier of the original graph with o tilde n over epsilon squared edges. So this was a classical algorithm, and this is basically what we used to find a quantum speedup. So our quantum algorithm consists of three components. One is a faster quantum spanner algorithm. Then we use something we call a k independent oracle. And finally we owe you a sort of magic trick. First is quantum spanner algorithm.
00:23:17.584 - 00:24:04.450, Speaker B: What's not too hard to prove is that there is a quantum spanner algorithm. So quantum algorithm, which explicitly constructs a spanner inside the graph which has query complexity, scaling with square root mn. So by query complexity I mean the number of quantum queries that we have to do to the adjacency list of the input graph. All right, so it's not too hard to prove that the query complexity of a quantum spanner algorithm is at most square root mn, basically by starting off with the classical greedy spanner algorithm. So just show how this works. So the classical greedy spanner algorithm starts with an input graph and an empty spanner. And we want, this graph is the spanner of the original graph.
00:24:04.450 - 00:24:26.574, Speaker B: So initially it's empty. And what we do is we iterate over all edges of the original graph. And for every edge we examine reader. So the distance of this edge in the original graph is one in the spanner. The corresponding distance can be at most log n. And so we check this, and if this is not the case, then this violates this condition. We have to add this edge.
00:24:26.574 - 00:25:13.088, Speaker B: And by just iterating over all edges, no one can prove that one, f will be a spanner and two, the number of edges of f will be o tilde n. Then given this, we can propose a quantum greedy spanner algorithm, which again starts from adjacency list access to some input graph and an empty spanner. An empty graph at first. And now we're gonna use grover search to search over the full set of edges of the input graph and search for an edge that violates this condition. Right? So we're going to search for an edge so that in the current graph, which is supposed to be a spanner, the distance is violated, the stretch condition is violated. And then when we find such an edge, we have to add it to f. And we just keep on doing that.
00:25:13.088 - 00:26:18.624, Speaker B: And it's not too hard to prove that we'll find the Ottila and edges of the spanner in approximately square root dynamic queries. Now we're of course interested not in query complexity, but in time complexity, right? So to make a distinction, when I talk about time complexity, I mean the number of, let's say gates that have to be applied in sort of standard circuit model, quantum circuit model. So we not only want the number of queries to be small, but we also want the time complexity to be small. And we have to be to do a bit more work to basically prove this theorem. So we'll call it a less easy theorem, but I'm going to spare you of most of the details. Roughly, what we do is, we start from a classical algorithm by Thorop and Zwick from zero one, who use, instead of this greedy algorithm, they propose a more efficient algorithm, which uses shortest path trees. And then we use a quantum algorithm by Durer, Heiligman, Heuer and Mahler from zero four, who prove that one can find a quantum speedup in the time complexity of constructing shortest path trees.
00:26:18.624 - 00:27:01.544, Speaker B: So, that's all I'm gonna say about this. This proves that quantumly we can actually speed up the construction of spanners. And so we thought like, okay, I mean, that's not too hard. So we thought we were done, because now in step one, we can just use this quantum algorithm to construct this bunch of spanners, which we can now do in Times Square root mn over epsilon squared, keep these edges and down sample the remaining set of edges. But of course, the problem is that this scheme has an iterative nature. So what does that mean? It means that after one iteration, we're still left with most of the original edges. Namely, we'll have approximately m over two edges.
00:27:01.544 - 00:27:35.586, Speaker B: And since we're aiming for an algorithm which has a sub linear scaling, we cannot afford to explicitly write down this intermediate graph. Whereas classically, this of course is not a problem. And so it's not completely clear how to resolve this issue. Right. How do we keep track of this intermediate graph in small o time? So, in sublinear time? So, here's a small picture. The original graph we accessed through the adjacency list. And then after one iteration, we want to kind of throw away half of the edges and keep the other half.
00:27:35.586 - 00:28:15.888, Speaker B: But this graph can no longer describe. So what do we do instead? The idea here is kind of straightforward. Idea is to maintain offline a random string of nchoosetobits. So this is an offline random string, which you can just generate in advance, oblivious to the graph. So, without knowing what the graph is, we just keep this offline random string, which has nchoose two bits. It's a uniformly random bit string, and it indicates whether a certain edge ij is discarded or whether we keep it. Now note that there's going to be a bunch of edges in there which are not actually present in the graph, but we don't care about that.
00:28:15.888 - 00:28:41.252, Speaker B: It's just like an oblivious thing that we keep offline. And so we can just generate that. And then whenever we do a query again of the form ik we receive enabled j. What we do is we check with this offline random string whether this edge should still be present or not. And as it turns out, this suffice to implement our quantum algorithms. So now we get the following picture. The initial graph is described by the JCCclist.
00:28:41.252 - 00:29:41.494, Speaker B: After single iteration, we're going to query both the JClist and this random string, and it's going to tell us which edges are still present and which one are the random present. But of course, there's a very clear problem, which is that it takes time, omega n squared, to generate this random bit string. So how do we deal with that? Classically, it's not too hard to deal with that, because you can always assume you have access to very long random bits without effectively having it by just, whenever you query for a bit, generating uniformly random bit. This is called lazy sampling, and that's not too hard. But quantumly we cannot do that because we can have this very long random bit string. What we can do is you can do queries to it in superposition, so we can kind of adjust all bits in superposition, but we cannot be that lazy like we can be classically. But as it turns out, we can still outsmart this sort of all powerful quantum daemon.
00:29:41.494 - 00:30:41.604, Speaker B: Namely, we have the following fact. And the following fact is a nice consequence of the polynomial method, which was proven by Beals, Berman, Cleave, Mosca and the wolf in 98. This is a a method used to prove a lower bounds on quantum query complexities. A nice consequence of this method is the following fact, namely, a quantum algorithm that makes only k over two queries to a bit string cannot distinguish where this bit string is uniformly random, or it's only k wise independent. So what do I mean by ky's independent? Kyz independent is a weaker notion than a uniformly random string. So ky is independent string x behaves only uniformly random on subsets of k bits, whereas a uniformly random string should behave uniformly random on any subset. But here, this is only the case for a subsets of k bits, and this is significantly weaker notion.
00:30:41.604 - 00:31:45.744, Speaker B: To show why this is a weaker notion, we basically, we can use some sort of results here. So, to rephrase, we aim for a quantum algorithm which makes a sub linear number of queries, namely square root mn, something like that. And by this result, by this polynomial method and this fact, we know that it suffices to use a ky as independent random bit string, where k only scales as square root mn. But of course, we still cannot effectively explicitly generate a string, right? Because still it's a string with n choose two bits, so approximately n squared, but we only need to be able to query it. And so as it turns out, it's possible to simulate queries to such a string without explicitly generating it. And that's only possible because the string is not uniformly random, but only kwise independent. Maybe we can use the following result, and we're kind of lucky to have this result because it's not been around for that long.
00:31:45.744 - 00:32:41.666, Speaker B: This result on efficient k independent hash functions by Christiani, Pag and Thorb from 15 what they proved is that in preprocessing time Ottila k so we so at the start we just do ottila k work. We can build a sort of database, this oracle, which allows to simulate queries to a kwise independent string of length n squared and time polylog per query. So otala one per query. So we can act as if we have this long kwas independent bit string, whereas we don't. So kind of elementary corollary of that is that any k query quantum algorithm that uses a uniformly random string, so you can see that as a resource, can actually be simulated in time with tilde k. So with that most poly log overhead without this random string. So in our case we get the following new algorithm.
00:32:41.666 - 00:33:25.372, Speaker B: In the first, we use this quantum algorithm to construct these spanners. This is just util squared edges, and so we can just explicitly describe these edges and keep all of them so we can explicitly write them down. That's fine. Then step two, instead of flipping coins for every of the remaining edges, we just construct this k independent oracle where k will be scaling as square root mn. That implicitly marks the remaining edges, right. So we won't explicitly do that, but we implicitly mark them so that whenever we do a quantum query to the adjacency list at a later point, we just simultaneously query this oracle, which tells us whether an edge is still present or not. Super iterations will have complexity scaling as square root mn over epsilon squared.
00:33:25.372 - 00:34:12.456, Speaker B: And this gives us the following theorem. So there is a quantum algorithm that explicitly constructs an epsilon spectral sparsifier with utility n over epsilon squared edges in time square root m n over epsilon squared. So there's one thing that we still owe you. Namely there's a squared there and there shouldn't be squared there, right? So we promise a squared mn over epsilon algorithm. And now we have the square root mn over epsilon squared. So we still owe you one magic trick, which somehow remind us a bit of a Baron von Munchausen shown in this picture had to you know, save himself by pulling him up by his own hair. So what we do is to improve the epsilon dependency, we do something roughly which looks as follows.
00:34:12.456 - 00:34:41.174, Speaker B: First we create a rough spectral sparsifier, right, for constant epsilon. So let's say epsilon equal to 110. So epsilon is constant. So the runtime of this is just square root mn using our quantum algorithm. Then we now we have the sparse graph h. And what we can do is we can estimate the effective resistances of edges and of node pairs inside h. And since h is so sparse, we can actually do that using the Spielmont tank classical solver.
00:34:41.174 - 00:35:29.704, Speaker B: So we have access to these effective resistances. And since h is a spectral sparsifier of g, these effective distances will approximate the effective resistances of g. So we get estimates of the effective resistances in g using this Ross spectral sparse fire. Now, these are only constant factor approximations, but as it turns out, this is sufficient to mark which edges we should keep and which edges we should not keep. Basically, this constant factor approximation just results in a constant factor overhead in our sampling process. So we don't see that. And so what we can do is we can use Grover search to find among these m edges the n over epsilon squared edges, which we implicitly mark using the effective resistances approximations.
00:35:29.704 - 00:36:18.494, Speaker B: So this we can do in square root mn over epsilon squared, giving us this final and main theorem, which is that there is a quantum algorithm that explicitly constructs an epsilon spectral sparsifier with n of epsilon squared edges approximately in time, giving us square root mn over epsilon. Again, a small side note. As I mentioned before, spectral sparsification is only useful or meaningful when epsilon is at least square root n over m. And so it basically proves that this here is at most o tilde m. So it's always faster than the classical algorithm. And for constant epsilon and dense graphs, we get a speed up from n squared to n to the three two space. So I think that this is a good point to maybe take a brief second and see if there's any questions so far.
00:36:22.874 - 00:36:41.362, Speaker C: Maybe just how do you use this like rough effective resistance information? Do you somehow prepare a state which is like approximately proportional to this resistance and then somehow fine tune it? Or what's the idea?
00:36:41.498 - 00:37:27.012, Speaker B: Not quite. So, basically, we know that the pes individual assignment probabilities have to be at least something like scaling as re the effective resistance. And so what we can do is we can simulate an extra register, a coin toss, which is basically Bernoulli random variable. So basically we generate a superposition over the set of edges. For every edge, we have a second register which contains the approximation of the effective resistance corresponding to that edge. So this indicates whether this edge is important or not. And then we just add a last register in which we kind of flip a coin, which will be one with large probability, if this is large, and otherwise will be small.
00:37:27.012 - 00:37:34.524, Speaker B: And then we just do a grover search over this last register. Does that make sense?
00:37:40.344 - 00:37:43.484, Speaker C: Why is it important that you know these effective resistances here?
00:37:43.944 - 00:38:04.384, Speaker B: Because. Because first you want to. So we kind of just sample. Right. We need to know what we need appropriate sampling probabilities of the individual edges. And these effective resistances tell us which edges are more important, and so which edges should get a higher sampling probability and which ones should get a lower sampling probability.
00:38:08.164 - 00:38:09.184, Speaker C: Okay, thanks.
00:38:09.724 - 00:38:17.172, Speaker A: There's another question from Tong Yang. Lee Tong Yang. Do you want to just unmute and ask your question or do you want me to relay it? Sure, I can ask.
00:38:17.268 - 00:38:50.444, Speaker B: I have a question about epsilon dependence here. I think in your slide you mentioned that a Spearman tung, the classical paper, can do like a t o of n, which has polylon error dependence. So how do you, like, explain the quantum epsilon dependence here? Okay, so that's a very good question. So one thing is that this epsilon dependence here is necessary, and it's also present classically. So, classically, constructing an epsilon spectral sparsifier takes time, m plus n over epsilon squared. And so this epsilon dependency is polynomial. Epsilon dependency is necessary.
00:38:50.444 - 00:39:21.524, Speaker B: But what I mentioned before is their result on Laplacian solving. Right. And so if you want to solve a Laplacian system, then you can get a log epsilon dependency on the error. And that's basically by being a bit smarter. So they use this rough epsilon spectral sparsifier, and then they do sort of iterative refinement using this rough, rough sparsifier. I see. Can you observe the same phenomena if you use a quantum Laplacian solver? That's not at all clear.
00:39:21.524 - 00:39:30.624, Speaker B: So, but I will go into some details of quantum Laplacian solving a bit later, so maybe that'll answer your question. I see. Thanks. All right.
00:39:31.204 - 00:39:37.504, Speaker A: Any other questions right now? Okay, maybe we can save more for later.
00:39:37.804 - 00:40:15.724, Speaker B: Okay, so let me just go to my. Yeah, again. So assuming epsilon at least square root n over m, this is at most util m. So what we do is in the second part of our work, we prove a matching lower bound. And so, in the interest of time, I'm not going to go through all the details, but I'll first give some intuition of this lower boundary proof, which doesn't do as much justice, however. So the intuition for the lower bound is that quantumly finding k marked elements among capital m elements takes time. Omega square root mk.
00:40:15.724 - 00:40:46.444, Speaker B: So this is basically by using rover search, and this is optimal. Hence I'm going to put this between quotes. Finding the n over epsilon squared edges of a sparsifier among the m edges of an input graph should take time. Square root mn over Epsilon, right? But of course I'm putting the hands between quotes because it turns out to be not quite as easy to prove. Then we need a couple of results. But it was a fun thing. So one of the things that we need is.
00:40:46.444 - 00:41:35.184, Speaker B: So the idea here below the hands is that we want to construct a graph which is a sparsifier and hide it in a much larger graph. But of course, what is a sparsifier? Which graph is a sparsifier. So to kind of consolidate what we mean by that, we construct what is called a unsparsifiable graph. So we start from a small random bipartite graph and just one over epsilon squared nodes, where every node on the left side is connected to a random half of the nodes on the right side. Now we take epsilon squared n copies of that right independent copies. So we get a larger random graph which has n nodes and n over epsilon squared edges. And then we can use the following result, which was proven in 16 by Andoni Chen, Krautgammer, Ken Woodruff and Jan.
00:41:35.184 - 00:42:02.446, Speaker B: They proved that any epsilon spectral sparsifier of this random graph must contain a constant fraction of its edges. So basically the graph cannot be further sparsified. So they proved that. So the reason why they proved that is they wanted to prove a space lower bound on spectral sparsification. This result basically shows that an epsilon spectral sparsifier in the worst case requires an over epsilon squared edges. So they did. They proved this result.
00:42:02.446 - 00:42:51.494, Speaker B: To prove a lower bound on space complexity, we use it in a slightly different way. What we do is we start from this graph which we know is a sparsify in itself, and we hide it in a much larger graph. So given parameters n, m and epsilon, we construct this random graph which has n of epsilon squared, n nodes and n of represent squared edges. And we hide it in a larger graph which has m edges. But all the edges that we add are just kind of obfuscating edges. So we give them weight zero and the original edges in the sparsifier h, we give weight one. This basically ensures that any epsilon spectral sparsifier of the larger graph g should necessarily contain an epsilon spectral sparsifier of the embedded graph h.
00:42:51.494 - 00:43:44.514, Speaker B: And by this formal result by Andonia et al. A sparse h necessarily contain a constant fraction of the edges of H. Hence a spectral sparsification algorithm on this graph should find a constant fraction of the edges of h, which we hid among a large number of edges. So how do we prove a lower bound on that? So basically the original graph is described by some bipartite adjacency matrix like this, and the larger graph is described by an adjacency matrix which looks like that. So basically every entry of the original graph is hidden among a number of non significant edges. And if you want to check whether one edge is present, we basically have to go through all these input bits. So you can use quantum to speed that up a bit, but not too much.
00:43:44.514 - 00:44:39.306, Speaker B: Maybe we can forget about graph and just think about the following more elementary problems. We got an input graph a, which is a Boolean matrix, and this a is not given to us directly, but is given to us as the blockwise or function of much larger matrix, where this would be the JCC matrix of the bigger graph. And the task is to output a constant fraction of the one bits given access to this larger capital n times little n square matrix. So this is what we call a relational problem. It's a relational problem because we can output, there's a number of outputs which are fine, we just want to find a constant fraction of the one bits of a, but it's composed with the or function. And composing problems often allows for a nice quantum lower bounds. But it's not always that clear priority.
00:44:39.306 - 00:45:26.692, Speaker B: Namely, what is the quantum lower bound for? In this case, the composition of a relational problem with the or function. So it turned out that this result was not out there. And so we asked some experts, namely we asked Alexander Belov and Troy Lee, and upon our question they agreed that this was not currently around, but were so kind to prove it for us. So this is in the future and to be published manuscript. They proved the following thing. The quantum query complexity of an efficiently verifiable relational problem were efficiently verifiable. Just a technical condition which has a lower bound l composed with the or function, which quantumly has a lower bound square root n is a product of both, right? So this is basically what we would expect, but that doesn't make it easy to prove it.
00:45:26.692 - 00:46:23.744, Speaker B: But that's exactly what they did. And in our case, we can prove a lower bound of omega Tilde n on the relational problem and capital n in our case is m over n epsilon squared. And combining these things and turning everything together proves us that the quantum query complexity of explicitly outputting an epsilon spectral sparsifier of a graph which is n nodes and m edges is omega tilde square root mn over epsilon. That basically proves that our algorithm is optimal up to log up to polylog factors. Okay, so given that we have this algorithm, we now wanna kind of demonstrate, you know, what it's capable of and what we can do with this algorithm. So, not so surprisingly, similarly to how classically spectral sparsification really allows for so many kind of algorithms and speedups and so on. Quantum in the quantum case, we find the same thing.
00:46:23.744 - 00:47:09.900, Speaker B: We have this general blueprint of the quantum speedups that we show, and there's somewhat vanilla mainly consider an input graph g and we're interested in. So you're approximating a certain quantity associated to this graph p. This could be a min cut. This could be solving a laplacian system associated this graph, something like that, and assume that it's approximately preserved under sparsification. Then also assume that you have a classical near linear time algorithm for approximating or exactly calculating this quantity. Then we can do the following thing. Namely, we can use our quantum algorithm to sparsify the graph g to sparse for h n times square root mn over epsilon, where h is now a much sparser graph.
00:47:09.900 - 00:47:57.052, Speaker B: Namely h just has n over epsilon squared edges. And so we can just use the classical algorithm on h and solve the problem on this parser graph. Right? And since this one only has n over epsilon squared edges, the classical algorithm will only take time with tilde n over epsilon squared. This gives us a approximate o tilde squared m n over epsilon quantum algorithm for estimating this graph quantity p. As demonstration, consider for instance the problem of finding the min cut. So if you're given an input graph g which can be weighted, we want to find a bipartition ss complement that minimizes the cut value, right? So the number, the sum of the weights of the edges that we have to cut classically. There is a min cut algorithm by Karger, which is near linear from 2000.
00:47:57.052 - 00:48:35.810, Speaker B: So basically this already shows that we have the two properties that we need. Namely the main cut is approximately preserved by sparsification, and we have a classical near linear time algorithm. So that proves that. Okay, sorry. So the approximation part is on this slide. So if we sparsify g to sparser graph h, then as we showed, the cut value is just a specific is described by a quadratic form in the Laplacian hence it's approximately preserved by sparsification. So the min cut of h will be an absolute approximation of the min cut of the original graph.
00:48:35.810 - 00:49:36.904, Speaker B: And hence we can just apply this blueprint scheme. Namely we quantum sparsify g two h, which takes time square root mn over epsilon. Now we use the classical min cut algorithm on h, which takes time n over epsilon squared, which gives us a square root mn over epsilon quantum algorithm for finding the epsilon min cut. Similarly, we can prove quantum speedups for finding the min st cut, finding the approximate sparsest cut or balanced separator, and finding the max cut, all following the same blueprint. Also similarly, you can think about laplacian solving. So like I promised, I was going to say something about this, just some background. If you consider a general linear system, ax equals b given a and b, and let's see that a has at most m non zero entries, then the complexity of approximately, approximately solving this linear system scales as the min of m times n, which is nice for sparse graphs, or n to the power omega where omega is a matrix multiplication coefficient.
00:49:36.904 - 00:50:36.992, Speaker B: But like I mentioned before, for laplacian systems we can do better. Namely Spielman and Tang showed that for Laplacian L, which has m non zero entries. So basically any Laplacian L can be associated to a graph and this graph will have at most m edges the complexity of approximately solving this linear system. This Laplacian system is near linear in the size of the system basically or tilde m. Then we use the fact that, and this is not entirely trivial to prove, but it's well known by these people, is that if h is a an epsilon spectral sparsifier of g, then the solution of the linear Laplacian system LH x equals b epsilon approximates in a certain norm the solution of the original applausion system. So that means that we can just solve the linear system in the sparsifier and find an approximate solution of the original system. Since the sparsifier is sparser, we can do this faster using a classical solver.
00:50:36.992 - 00:51:35.644, Speaker B: So basically we can use a quantum algorithm to sparsify g to h, taking times square root mn over epsilon. And then we solve the sparser linear system lh x equals b classically. And since lH only has n over epsilon squared non zero entries, we can use classically in time or tilde and of epsilon squared. This gives us a quantum algorithm for a passion solving in time or tilde square root mn over epsilon. And by a small reduction of the standard reduction there is a by small variation on the standard reduction there is a quantum reduction, which shows that, and this is the same in the classical case, that we can at the same time solve the more general class of symmetric and diagonally dominant systems. So Laplacian is always symmetric and diagonally dominant. But it turns out that the much larger class of general symmetric and diagonally dominant systems can be reduced to Laplacian solar using some quantum routine, as we can also solve this larger class, and until a square root mn over epsilon.
00:51:35.644 - 00:52:18.524, Speaker B: So this gives us an epsilon approximate solution in Times square root mn over epsilon. Applaudion solving and friends. I mean, there's many friends. Having this quantum applaud in solver allows us to approximate effective resistances, things like the COVID time of random walks, estimating eigenvalues of the graph, and many more. So, this is just a couple of demonstrations, and then I'll try to round up by first giving a summary of exactly what we did. So, we proved that there is a quantum algorithm for spectral sparsification, which takes time o tilde square root mn over epsilon. Then we show that this is tight up to polylog factors.
00:52:18.524 - 00:53:05.194, Speaker B: And then we showed how this gives a speed up for things like cut approximation of applaudion solving. And I'll end by some open questions, which we think are very interesting, and we're kind of excited to hear any thoughts on them. The first question is, we proved a matching lower bound for sparsification. So we showed that you cannot sparsify a graph faster than square root amount of epsilon, essentially. But this is not something that people are truly interested in. What people want to do is they want to solve Mincut, our sort of laplacian system. And sparsification is just a tool for that, right? So we prove a lower value for sparsification, but it doesn't give a lower bound for the real interesting applications, such as approximating the Mincut or solving a laplacian system.
00:53:05.194 - 00:54:01.804, Speaker B: So we're very interested in trying to prove a matching lower bound for these applications, right? So this would show that, for instance, our algorithms are tight. So this kind of following this easy blueprint, there's not much more to be done. And it would also basically make our lower bound obsolete, because getting a better bound on sparsification would give a better bound for solving these problems. And the second question, which I also think is very interesting, is that so? Again, we proved that our sparsification algorithm is tight, cannot be improved, but it's for weighted graphs. So, recall that we very explicitly and crucially relied on the fact that we can give certain edges weight one, and then just add a whole bunch of non significant edges. Right. You can just add so many edges which have weight zero and just obfuscate the axis so you can just not see the real edges, which are important.
00:54:01.804 - 00:54:40.194, Speaker B: And we are very strongly convinced that this lower bound does not apply to unweighted graphs. So it seems that for unweighted graphs, if there's many edges, then typically the problem also becomes easier. Like, I mean, that would mean that things are well connected and so on. So hence the question, can we do better for unweighted graphs, what's the complexity of spectral sparsification? Or solving any of the aforementioned problems for unweighted graphs? So that was it. I'll thank you all for listening, and I hope that you can all stay safe in these sort of peculiar types. Thank you.
00:54:41.694 - 00:55:09.138, Speaker A: Okay, thanks very much. So, well, maybe I can thank you on behalf of the audience. And if you can find this button, you can signal your applause like this. And so now we have time for questions. So if anyone has questions, I think you can just feel free to unmute and ask. Or if you like, you can enter questions in the chat box and I can mute them. And maybe.
00:55:09.138 - 00:55:31.414, Speaker A: So not hearing a question right away, maybe I can just start off with one to kind of break the ice. So, I guess I'm not sure that I understood completely. Sort of like the model in which your algorithms are operating. So, like, to what extent are you doing this just in the plain circuit model? Or do you need to assume something like Quantum Ram, where you have coherent access to data that you store and maybe even modify along the way?
00:55:32.214 - 00:56:02.242, Speaker B: We need coherent access to data that we store, but we don't need qram in the sense of being able to generate quantum states efficiently, which would put data into a position or something like that. What we basically need to do is we keep this offline database, which marks certain edges, whether they're still present or not. We need to be able to coherently ask questions to this offline thing. Yes, I see.
00:56:02.298 - 00:56:02.690, Speaker A: Got it.
00:56:02.722 - 00:56:03.334, Speaker B: Thanks.
00:56:03.834 - 00:56:13.826, Speaker A: Are there other questions? Oh, I think someone has their hand up.
00:56:13.930 - 00:56:16.506, Speaker E: Eth, can you hear me okay?
00:56:16.570 - 00:56:17.938, Speaker A: Yes, please go ahead.
00:56:18.026 - 00:56:42.974, Speaker E: So, for the applications for the laplacian solving, does the open question that you have here at the end, does that actually depend at all on the quantum work? Or can you just approach that question by just saying, given an epsilon, sparsified graph, Laplacian or whatever, can you work on a lower bound, just given that information independent of any kind of quantum assumptions?
00:56:45.434 - 00:57:18.124, Speaker B: So, given a spectral sparsifier I would say most of the work is done. Um, so next, epsilon spectral sparsifier has size n over epsilon squared. And you could, you could solve this. This. This guy in time, n over epsilon squared. But I think the main question is, I mean, how to get to the spectral sparsifier, right? Does that answer your question?
00:57:19.024 - 00:57:29.604, Speaker E: Oh, you. I forget now. Some of this is a little over my head. So you got, do you use a classical algorithm originally to get the spectral sparsifier in the first place? And then.
00:57:30.264 - 00:57:44.324, Speaker B: No. So we use a quantum algorithm to construct a spectral sparsifier, and now we use a classical solver, which cannot be improved on the, I mean, as a classical algorithm cannot be improved on the sparsifier.
00:57:46.404 - 00:58:06.204, Speaker E: Okay, so for the, for the application for the, for the laplacian solving, does that, is that just given, given a sparsified Laplacian, does that give you a better, lower bound, or do you need to specifically worry about how you found, how you found the.
00:58:06.324 - 00:58:33.684, Speaker B: Yes. Yes. So the question sparsified why? I think this is an interesting question is because spectral sparsification might have nothing to do with solving laplacian system. So it might be that there's an entirely different way of solving a laplacian system using a quantum computer, which might be more efficient than first constructing the spectral sparsifier and then solving it on the spectral sparsifier. So I guess that's where the work lies.
00:58:34.944 - 00:58:58.642, Speaker E: Okay, thank you. Yes. Sorry. I think my question was a little confused. I just wanted to know, basically if other people could be working on that question without all of your quantum knowledge, if you could just kind of hand, if it's possible that that question just could be handed off to other people and say, I can give you this, this sparsified Laplacian, and then prove that this gives you a better, lower bound for solving the Laplacian system. But thank you. I think you answered my question.
00:58:58.778 - 00:59:00.374, Speaker B: Yes, I guess not. Yeah.
00:59:02.794 - 00:59:04.894, Speaker A: Great. Are there other questions?
00:59:07.374 - 00:59:08.234, Speaker B: I see.
00:59:08.574 - 00:59:09.678, Speaker A: Did you have a question?
00:59:09.766 - 00:59:34.254, Speaker F: Yeah, actually. So I just wanted to say that was a great talk. I really enjoyed both the content and the presentation. I just wanted to follow up on what Andrew asked you about this offline storage. I didn't quite understand what you mean by the, you know, why this storage is offline and why you can't just consider it part of your. So maybe.
00:59:35.994 - 01:00:18.932, Speaker B: Wait, so let me, so what I mean by offline is we're assuming Oracle access to some graph, right? So this could be a graph which is logically described by some function, but it could also just be like a black box. So this is one thing that I would say, these are online queries, but then offline, without oblivious to the graph, we keep this entries two sides, bit string. And what we need to be able to do is like, we can do quantum queries to this graph, we should be able to do quantum queries to this offline register. And so we need coherent access to this register, which can have something like n squared edges and squared bits.
01:00:19.068 - 01:00:46.474, Speaker D: Okay, can I add something to this, please? I think the online versus offline distinction is a little bit confusing. It's better to think of the, the algorithm is having two different phases, a pre processing phase and a real algorithm phase. And the time complexity of both of these phases is upper bounded by order square root of mn. So instead of talking about offline things, I think it's better to think of a preprocessing phase, which is also not too expensive.
01:00:47.494 - 01:00:52.838, Speaker B: Yes, yes, but we need to be able to do quantum queries to this offline register.
01:00:52.926 - 01:01:02.814, Speaker D: Yeah. So the preprocessing phase sort of produces a classical memory of about square root of mn bits. And we need to have coherent access to that memory in the second phase of the algorithm.
01:01:02.974 - 01:01:03.714, Speaker B: Yes.
01:01:09.414 - 01:01:29.810, Speaker F: Sorry, I thought I understood that you have these n squared bits. So how do you pre process them and reduce them to square root nm? And then for the. So I understood you have n squared random bits that you, that you're now need coherent access to, I thought through the algorithm. So how do you pre process them?
01:01:29.842 - 01:01:30.058, Speaker B: And.
01:01:30.106 - 01:01:32.378, Speaker F: Okay, so we have so many, so.
01:01:32.386 - 01:01:48.474, Speaker B: It'S a good question because it's kind of a subtle point. So naively, we would create this n squared bit, you know, range of n squared random bits beforehand, but I would require omega n squared pre processing time.
01:01:48.594 - 01:01:51.218, Speaker F: I see. So now you have this data structure is what you.
01:01:51.266 - 01:02:03.920, Speaker B: Exactly. Exactly. Which only requires pre processing time, k, where k would be the independence that we need. And k is just a runtime of the algorithm. So necessarily this wouldn't give an overhead and.
01:02:03.992 - 01:02:14.684, Speaker F: Sorry. So what about the graph itself? So you did need quantum access to that. So how do you reduce that as well?
01:02:15.784 - 01:02:38.664, Speaker B: Yeah, so we're working in a oracle model, right, in a black box model, where, and we're assuming that we can do quantum queries to some graph. But of course it might well be that this graph just has a concise description, right? Like in a sort of physical system, then this graph would have some logical structure, and then we just have to evaluate some function to see whether an edge is present or not. Something like that.
01:02:41.244 - 01:02:42.184, Speaker F: Thank you.
01:02:46.124 - 01:02:56.264, Speaker A: Okay. Are there other questions? You have a question? Please feel free to just go ahead and ask.
01:03:02.204 - 01:03:02.572, Speaker B: Okay.
01:03:02.588 - 01:03:24.104, Speaker A: Well, I guess if there aren't any more questions, maybe we can thank Simon again. I just want to mention that next week, so in this series, there's going to be a talk, I think, again, at the same time, there'll be an announcement about it with the link, if it's not the same as this week. But the talk next week is going to be by Anuraganshu, and he's going to talk about his improved 2d area law work with David Gossett.
01:03:24.684 - 01:03:25.076, Speaker E: Okay.
01:03:25.100 - 01:03:29.348, Speaker A: So let's thank Simon again, and thank you all for attending the talk.
01:03:29.436 - 01:03:32.684, Speaker B: Simon, thanks for listening and thanks for organizing. Thank you.
