00:00:00.280 - 00:00:03.230, Speaker A: So our next speaker for the day is Siki Liu, and she'll tell us.
00:00:03.262 - 00:00:07.554, Speaker B: About hyper connectivity in high dimensional expanders. So take it away.
00:00:11.934 - 00:00:21.494, Speaker C: So, I actually, while preparing for this talk, I realized in bootcamp, you guys already heard about hyper contractivity and epsilon product space.
00:00:21.654 - 00:00:31.722, Speaker D: So today I'm just trying to give a more cohesive view of it and try to fill some of the gaps maybe left by, like, due to, like, having different talks covering this topic.
00:00:31.898 - 00:00:33.290, Speaker C: So the first half of the talk.
00:00:33.322 - 00:00:40.986, Speaker D: I would try to cover some backgrounds on hypercontractivity. Like what? Like, what really is it about? And the second part, I would try.
00:00:41.010 - 00:00:51.934, Speaker C: To set up a framework for the proof of epsilon products based global hyper contractivity inequality. So this talk is based on joint work with Tom Gore and Noam, who's here.
00:00:53.294 - 00:00:53.790, Speaker B: Yeah.
00:00:53.862 - 00:01:00.278, Speaker C: Okay, so let's jump in. So, maybe I should start slower when.
00:01:00.326 - 00:01:11.510, Speaker D: People come in still. So this is the first slide is about the setup. So, okay, when we consider hypercontractivity, what is the object we really care about when we're studying?
00:01:11.542 - 00:01:14.318, Speaker C: Right? So, to me, it feels like, really.
00:01:14.366 - 00:01:20.358, Speaker D: The object we care about is a probability space, which I denote by omega mu, where omega, we should think about.
00:01:20.406 - 00:01:25.334, Speaker C: Just a sample space, and mu is a distribution. And probably, like, I should also mention.
00:01:25.374 - 00:01:32.790, Speaker D: That here, another object that we should always keep in mind when talking about this kind of properties is the function space.
00:01:32.942 - 00:01:36.142, Speaker C: Like the space of functions over this probability space.
00:01:36.278 - 00:01:40.982, Speaker D: And also, we really require this space. So we need a norm to be.
00:01:40.998 - 00:01:42.674, Speaker C: Defined for all these functions.
00:01:43.534 - 00:01:48.014, Speaker D: I'm not really writing it out, explaining, but that's what we should think about.
00:01:48.354 - 00:01:49.794, Speaker C: So, okay, now we have the space.
00:01:49.874 - 00:01:51.370, Speaker D: We have the function, and now we.
00:01:51.402 - 00:01:54.810, Speaker C: Consider a linear operator, t rho, that's.
00:01:54.842 - 00:01:58.874, Speaker D: Over l infinity mu. So l infinity mu is just the.
00:01:59.034 - 00:02:01.762, Speaker C: Space of functions, real value functions.
00:02:01.818 - 00:02:03.466, Speaker D: Let's say real value functions over the.
00:02:03.490 - 00:02:12.414, Speaker C: Space that have bounded infinity norm. And we say that this operator over the space is hypercontractive if it satisfies the following two requirements.
00:02:14.004 - 00:02:18.468, Speaker D: So, number one is if for every.
00:02:18.516 - 00:02:22.716, Speaker C: F was bounded l one norm, we require that t rho when rho is.
00:02:22.740 - 00:02:26.188, Speaker D: In the range zero between zero and one.
00:02:26.356 - 00:02:30.132, Speaker C: Okay, this t rho, we should think about it just as the noise operator.
00:02:30.268 - 00:02:32.116, Speaker D: You guys probably have already seen in.
00:02:32.140 - 00:02:38.108, Speaker C: The context of boolean function analysis. Okay? And when rho is larger, it's just.
00:02:38.196 - 00:02:55.344, Speaker D: Closer to being identity operator. And when it's zero, it's closer to being really a uniform, a uniform distribution. So the first requirement is that the.
00:02:55.424 - 00:02:56.904, Speaker C: L1 norm of t rho should.
00:02:56.944 - 00:03:03.592, Speaker D: Be t rho of f should be bounded by l, one norm of f. So this is just saying literally, like.
00:03:03.648 - 00:03:06.764, Speaker C: T rho should be an averaging operator should be contracted.
00:03:07.584 - 00:03:10.672, Speaker D: The second requirement is saying that for.
00:03:10.768 - 00:03:14.776, Speaker C: There exists a specific row zero such that if I take my noise to.
00:03:14.800 - 00:03:18.064, Speaker D: Be large, or I take my averaging.
00:03:18.104 - 00:03:19.800, Speaker C: Operator rho, so that rho is smaller.
00:03:19.832 - 00:03:22.024, Speaker D: Than rho zero, then I have that.
00:03:22.064 - 00:03:24.120, Speaker C: T rho of f's four norm is.
00:03:24.152 - 00:03:26.704, Speaker D: Bounded by some constant c times its.
00:03:26.824 - 00:03:28.804, Speaker C: Two norm raised to the power four.
00:03:29.384 - 00:03:31.284, Speaker B: Okay. So.
00:03:34.564 - 00:03:40.588, Speaker A: Yes, I'm talking about it in the next slides. But yeah, usually you want this to form a semicolon.
00:03:40.716 - 00:03:54.144, Speaker C: And here by p norm, I really mean like f p to the p really is the piece moment of the function f over this probability space. So.
00:03:57.004 - 00:03:57.396, Speaker B: Okay.
00:03:57.420 - 00:03:58.636, Speaker D: Also, by the way, if you have.
00:03:58.660 - 00:04:00.544, Speaker C: Any questions, feel free to interrupt.
00:04:10.354 - 00:04:12.874, Speaker E: It's the expectation of absolute value of.
00:04:12.914 - 00:04:14.730, Speaker D: F to the t. All right.
00:04:14.802 - 00:04:16.214, Speaker B: Okay. Yeah, that's right.
00:04:16.634 - 00:04:22.650, Speaker E: So why do we care about l one in the first? I mean, usually it's contracted for all nodes.
00:04:22.802 - 00:04:29.728, Speaker A: I think if you have l one contractor, you can derive l infinity contractor and in between all of them, contracted.
00:04:29.816 - 00:04:31.456, Speaker C: Contracted, yeah.
00:04:31.480 - 00:04:32.240, Speaker A: So it's a quote.
00:04:32.272 - 00:04:33.048, Speaker C: Yeah, but, right.
00:04:33.096 - 00:04:35.056, Speaker A: It's literally saying like, you should, it.
00:04:35.080 - 00:04:38.844, Speaker B: Should be contracted for all the zero.
00:04:40.144 - 00:04:42.484, Speaker A: Okay, so, um.
00:04:43.584 - 00:04:48.000, Speaker D: Okay, now, as Yvel was mentioning, we.
00:04:48.032 - 00:04:51.084, Speaker C: Actually usually have this trial being a semigroup.
00:04:52.024 - 00:04:57.620, Speaker D: And even more specifically, tro is usually this operator, which is e to the.
00:04:57.652 - 00:05:00.796, Speaker C: Negative log rho times l here, l.
00:05:00.860 - 00:05:02.624, Speaker D: Is the laplacian operator.
00:05:04.084 - 00:05:04.452, Speaker B: Okay.
00:05:04.468 - 00:05:05.980, Speaker D: If we are on the hypercube, Boolean.
00:05:06.012 - 00:05:10.652, Speaker C: Hypercube, it's literally Laplacian of the, of the graph itself.
00:05:10.788 - 00:05:17.828, Speaker D: If we think about the space as a graph. But if it's a distribution like standard gaussian or something, this will be the.
00:05:17.876 - 00:05:21.592, Speaker C: Distribution of Laplacian, which will have this definition here.
00:05:21.788 - 00:05:24.104, Speaker D: We won't really use it too much.
00:05:24.144 - 00:05:28.124, Speaker C: So just showing it here, and no worries if it's confusing a little bit.
00:05:30.384 - 00:05:30.984, Speaker A: So.
00:05:31.104 - 00:05:43.232, Speaker D: Okay, yeah, as I mentioned, like this t row is the Einstein Ullenbach semigroup operator over the gaussian space. And it's the noisy operator, noise operator.
00:05:43.288 - 00:05:55.308, Speaker C: On that boolean hypercube, let's talk about in the bootcamp, I guess. So, following that, I want to say.
00:05:55.356 - 00:06:09.304, Speaker D: There'S a couple variants or a couple different ways of expressing the hypercontructivity inequality. So I just want to write them out here. So maybe in some setting you will see, like. Okay, in the previous setting, I've shown.
00:06:09.804 - 00:06:14.828, Speaker C: We only have this like four norm is bounded by two norm expression.
00:06:14.876 - 00:06:20.884, Speaker D: But in general, once you have this four norm is bounded by two norm expression. You can actually also in the black.
00:06:20.924 - 00:06:24.244, Speaker C: Box way deduce like a t rho of f q norm is bounded by.
00:06:24.284 - 00:06:30.484, Speaker D: P norm for every p and q for every like p greater than one.
00:06:30.524 - 00:06:41.884, Speaker C: Smaller than q. Yeah, this works for, yeah, you. Yeah. For the same rule as the one in the 42.
00:06:45.544 - 00:06:46.884, Speaker E: Puncture.
00:06:47.984 - 00:06:48.720, Speaker C: Yeah, yeah.
00:06:48.752 - 00:06:51.104, Speaker D: C. Yeah. Right. C actually changes.
00:06:51.144 - 00:06:53.440, Speaker A: C is no longer the c in the 42.
00:06:53.592 - 00:06:55.524, Speaker C: It's depending on p and q.
00:07:00.424 - 00:07:16.658, Speaker D: Okay, so, and also I want to say like sometimes people would drop the t rho when and instead having the c depending on some degree parameter d. Especially. This is the case as we've seen in the discrete space like a boolean.
00:07:16.746 - 00:07:19.194, Speaker C: Boolean q, because this t row actually.
00:07:19.274 - 00:07:22.442, Speaker D: Its eigenvalue decomposition nicely corresponds to the.
00:07:22.578 - 00:07:25.174, Speaker C: Degree decomposition of functions.
00:07:26.314 - 00:07:34.276, Speaker D: So there's ways to really write t rho instead as like some constant times.
00:07:34.340 - 00:07:38.828, Speaker C: The different degree parts of f, and therefore being able to extract ACD on.
00:07:38.836 - 00:07:40.220, Speaker D: The right hand side and dropping t.
00:07:40.252 - 00:07:48.104, Speaker C: Rho in general in this expression. Okay.
00:07:50.684 - 00:07:53.820, Speaker D: Okay, so uh, then in the.
00:07:53.852 - 00:07:55.044, Speaker C: Rest of the talk I will mostly.
00:07:55.084 - 00:07:57.492, Speaker D: Just adapt this notation where we don't.
00:07:57.508 - 00:07:59.444, Speaker C: Have t row, like we just have.
00:07:59.484 - 00:08:02.324, Speaker D: The four norm itself and hiding the.
00:08:02.404 - 00:08:06.268, Speaker C: Row dependency really in this c constant.
00:08:06.436 - 00:08:13.916, Speaker D: Which is also happen to be a, usually you have a degree degree parameter hidden there, but you sometimes you could.
00:08:13.940 - 00:08:19.236, Speaker C: Have more like more information about the space also could influence the value of.
00:08:19.260 - 00:08:19.824, Speaker B: C.
00:08:22.124 - 00:08:38.772, Speaker D: So why do we care about hyper contractivity quality? I guess this has also been already talked, so I will just summarize some of them. So first, is this really in some sense is a bound on the higher moments of a random variable, of a.
00:08:38.788 - 00:08:40.308, Speaker C: Function of a random variable.
00:08:40.476 - 00:08:43.188, Speaker D: And therefore, because one could get this.
00:08:43.276 - 00:08:44.244, Speaker C: Bound a higher moment.
00:08:44.284 - 00:08:46.108, Speaker D: So one thing one could immediately get.
00:08:46.156 - 00:08:53.494, Speaker C: Is the concentration for the random variable fucking probability space.
00:08:53.794 - 00:09:06.986, Speaker D: Okay, so the first one, like I'm dropping the process of deduction, but really this is through Markov by, by using a higher higher moments than two. And on the right hand side, this.
00:09:07.010 - 00:09:11.734, Speaker C: Is also anti concentration result. So just by Zigman.
00:09:14.154 - 00:09:17.234, Speaker D: So this is kind of the more.
00:09:17.274 - 00:09:22.034, Speaker C: Straightforward corollary, almost like, and also there.
00:09:22.074 - 00:09:28.162, Speaker D: Is this so called degree d inequality. This is something one can deduce from.
00:09:28.218 - 00:09:31.290, Speaker C: Hypercontractivity for functions defined on the space.
00:09:31.442 - 00:09:32.994, Speaker D: And are Boolean valued.
00:09:33.034 - 00:09:38.146, Speaker C: So zero one functions basically the, okay.
00:09:38.170 - 00:09:48.688, Speaker D: You can see the expression here. It's saying like the less than degree d part of f has weight bounded by this c to the quarter times.
00:09:48.856 - 00:09:52.672, Speaker C: F two norm raised to the three halves power.
00:09:52.848 - 00:09:54.120, Speaker D: So what does that mean?
00:09:54.192 - 00:09:56.872, Speaker C: That's generally, intuitively just saying.
00:09:57.008 - 00:09:59.864, Speaker D: If your space is hypercontructive, then this.
00:09:59.904 - 00:10:02.924, Speaker C: Boolean value functions all have their weights.
00:10:03.704 - 00:10:07.136, Speaker D: Have their weights really concentrated on like.
00:10:07.320 - 00:10:28.264, Speaker C: High degrees, especially those Boolean functions with low density. So low density coming from really c to the one fourth should be, should be much, really should cancel out the f two norm to the one half on the right hand side.
00:10:32.564 - 00:10:33.188, Speaker B: Okay.
00:10:33.276 - 00:10:37.036, Speaker D: Oh, okay. Maybe one thing I should remark here is that the c really should not.
00:10:37.060 - 00:10:39.802, Speaker C: Depend on, say the size of this a domain.
00:10:39.858 - 00:10:43.802, Speaker D: Otherwise, this kind of implications just not.
00:10:43.898 - 00:10:46.194, Speaker C: Like trivial for us.
00:10:46.234 - 00:10:50.674, Speaker D: So we wouldn't want, we wanted this still to be like independent, say of.
00:10:50.714 - 00:10:56.014, Speaker C: Like the size of the entire domain, but could depend on some of the parameters and we'll mention them later.
00:11:00.394 - 00:11:08.254, Speaker D: And another implication, which is more like implication of that level D inequality we just saw, is the small set expansion.
00:11:10.494 - 00:11:15.278, Speaker C: For this, specifically for this Markup chain that's defined by t row.
00:11:15.406 - 00:11:16.774, Speaker D: So, okay, what do I mean by.
00:11:16.814 - 00:11:21.710, Speaker C: A Markov chain defined by T row? So over the, especially over the, okay.
00:11:21.742 - 00:11:23.366, Speaker D: Even over the continuous space, but especially.
00:11:23.430 - 00:11:24.910, Speaker C: Over the discrete space.
00:11:25.062 - 00:11:27.910, Speaker D: True, really depends, really defines the Markov.
00:11:27.942 - 00:11:32.354, Speaker C: Chain over the space, over your sample space.
00:11:32.774 - 00:11:37.074, Speaker D: And the, specifically, the Markov chain have.
00:11:37.114 - 00:11:43.002, Speaker C: Stationary distribution being mu, and then the Boolean hypercube case.
00:11:43.138 - 00:11:44.354, Speaker D: This t row will be the noise.
00:11:44.394 - 00:11:49.602, Speaker C: Operator, and this would be the noisy hypercube. That is the Markov chain that's mentioned here.
00:11:49.738 - 00:11:56.586, Speaker D: So quantitatively, a Markov chain or a graph is a small set expander.
00:11:56.730 - 00:11:59.250, Speaker C: If for every small set, most of.
00:11:59.282 - 00:12:12.506, Speaker D: Its edges that touches it points towards to goes to some other vertex that's outside the set. And this is interesting, more related to.
00:12:12.530 - 00:12:14.602, Speaker C: Like a harness of approximation application.
00:12:14.658 - 00:12:16.578, Speaker D: And I believe Dora have talked about.
00:12:16.626 - 00:12:21.774, Speaker C: This in connection with PCP and the hardness of approximation.
00:12:22.354 - 00:12:25.986, Speaker D: So I honestly don't know as much about it.
00:12:26.010 - 00:12:30.494, Speaker C: So probably referring to the previous talk would be the best way to see this application.
00:12:32.824 - 00:12:37.616, Speaker D: So, okay, so here I did have.
00:12:37.800 - 00:12:43.920, Speaker C: Short proof of it, but I think maybe let's not care too much about what's written here.
00:12:43.952 - 00:12:50.144, Speaker D: It's just a short proof. This is just to show that from level D inequality to small set expansion.
00:12:50.184 - 00:13:04.404, Speaker C: It'S relatively straightforward, if you already kind of know the eigenspace of t row. And also there are a couple more applications. It's very interesting, but I dropped here.
00:13:04.444 - 00:13:07.984, Speaker D: Because they're slightly more complicated.
00:13:08.684 - 00:13:10.204, Speaker C: I mean, one of them is from.
00:13:10.244 - 00:13:12.972, Speaker D: Hypercontractivity quality, one can usually deduce a.
00:13:12.988 - 00:13:19.104, Speaker C: Luxable living quality which will help us to prove like fast mixing over Markov chains.
00:13:19.964 - 00:13:21.236, Speaker A: That's one application.
00:13:21.340 - 00:13:31.860, Speaker C: And there are also other applications to combat KKL theorem. Those type of results can also be deduced from hypercontractive inequalities.
00:13:31.932 - 00:13:34.236, Speaker D: So this is kind of a result.
00:13:34.300 - 00:13:36.064, Speaker C: That do have many implications.
00:13:37.604 - 00:13:46.104, Speaker D: There are probably more that I'm not fully aware of, but yeah, in general, the idea is that this is a very useful result.
00:13:46.964 - 00:13:52.404, Speaker C: Okay, next, I want to give some examples of hypercontractivity.
00:13:52.444 - 00:13:53.144, Speaker B: Of course.
00:13:55.024 - 00:14:01.008, Speaker C: So one of the first known, maybe this is, I'm not 100% sure, but.
00:14:01.056 - 00:14:15.768, Speaker D: This is as far as know, like one of the earlier like results in hypercontractivity, equality is in the standard gaussian space. And the result is saying like for rho between zero and one over square root three, we have like t row of f four norm to the force.
00:14:15.816 - 00:14:23.184, Speaker C: Is bounded by the two norm to the force. Okay, so the seal here literally is just one.
00:14:26.164 - 00:14:28.940, Speaker D: And also, of course, there's the bonami.
00:14:28.972 - 00:14:31.384, Speaker C: Lemma, which we've seen before.
00:14:31.684 - 00:14:39.228, Speaker D: And actually that's literally the same. Like the top expression here and the bottom expression here is literally the same.
00:14:39.276 - 00:14:40.956, Speaker C: Except for we move from the standard.
00:14:40.980 - 00:14:52.430, Speaker D: Gaussian to 0112 Boolean hypercube, and we take out the t rho by really looking at its eigenspace and extracted these.
00:14:52.502 - 00:14:57.194, Speaker C: Nine to the degree of f to the right hand side. So this is our c here.
00:15:07.934 - 00:15:08.674, Speaker A: So.
00:15:15.274 - 00:15:17.090, Speaker D: So here I want to give.
00:15:17.122 - 00:15:19.330, Speaker C: A couple more examples of some known.
00:15:19.402 - 00:15:23.114, Speaker D: Results in known hypercontractivity quality.
00:15:23.274 - 00:15:24.602, Speaker C: And I want to say that they.
00:15:24.618 - 00:15:28.570, Speaker D: Are by no means complete. And I actually didn't include some of.
00:15:28.602 - 00:15:33.842, Speaker C: The more recent results on very interesting like spaces, but it's to more illustrate.
00:15:33.898 - 00:15:36.654, Speaker D: What kind of is known in general.
00:15:37.554 - 00:15:40.134, Speaker C: So, here are a couple results.
00:15:40.514 - 00:15:43.082, Speaker D: And the first column shows this probability.
00:15:43.178 - 00:15:53.426, Speaker C: Space, and the second column shows what is this constant c. Here the bound and last shows like for what type.
00:15:53.450 - 00:16:03.290, Speaker D: Of function f this kind of inequality hold. So previously in examples we've been talking about, it's like this hyper contractivity inequality.
00:16:03.362 - 00:16:05.698, Speaker C: Generally holds for things with bounden norm.
00:16:05.826 - 00:16:20.074, Speaker D: But as we see actually for a couple of domains, we know results. We know that actually it doesn't hold general for all functions with bounded p norm. So we instead have results that shows.
00:16:20.114 - 00:16:28.954, Speaker C: That they hold for specific class of functions called the global functions, which I will talk more about later, but okay.
00:16:28.994 - 00:16:45.896, Speaker D: Let'S take a look at the domains. So the domains, the first one is the uniform distribution over Boolean hypercube. And we know like nine to the degree of f. And also in the Kiewish lan lift shades Menzer paper, they.
00:16:45.920 - 00:16:47.744, Speaker C: Show it for general product space.
00:16:47.904 - 00:16:52.352, Speaker D: And here the c is 100 to.
00:16:52.368 - 00:16:53.044, Speaker C: The d.
00:16:54.944 - 00:17:02.368, Speaker D: Where delta is a parameter that's related to the globalness. So I want to find here, but roughly like exponential.
00:17:02.416 - 00:17:03.164, Speaker B: Indeed.
00:17:04.764 - 00:17:08.348, Speaker C: And later we also have results about.
00:17:08.436 - 00:17:11.460, Speaker D: Symmetric groups and that's also give exponential.
00:17:11.532 - 00:17:17.944, Speaker C: And polyd parameter here. And also we have.
00:17:20.444 - 00:17:22.628, Speaker D: Uniform distribution Odana.
00:17:22.676 - 00:17:26.428, Speaker C: Wu, which is uniform distribution over like case slices.
00:17:26.556 - 00:17:30.228, Speaker D: So it's just a Boolean hypercube. But you only restrict your attention to.
00:17:30.276 - 00:17:32.146, Speaker C: All the points that have k one.
00:17:32.170 - 00:17:44.194, Speaker D: S. Exactly k one s. And finally is filmma. So Donna Wu have the multi slice. So instead of Alphabet size being only plus minus one, you can have like.
00:17:44.234 - 00:17:53.954, Speaker C: A constant size Alphabet size. Okay, so some, oh, I need to say a disclaimer.
00:17:53.994 - 00:17:57.922, Speaker D: Some of these I computed myself because as I said, like, they are not.
00:17:57.978 - 00:18:14.506, Speaker C: All like written in the standard form. So, but I hope I didn't make any mistake. But if I did, please, yeah, let me know if something is wrong here. Okay, these are the couple results.
00:18:14.570 - 00:18:17.914, Speaker D: But okay, as I said, there are a couple results I didn't include, which.
00:18:17.954 - 00:18:21.546, Speaker C: Include very interesting grassman graphs.
00:18:21.610 - 00:18:23.514, Speaker D: And also like the more recent like.
00:18:23.594 - 00:18:31.544, Speaker C: Shortcut graphs that that's by Ellis Candler and lift shapes. Those are all very cool results, but.
00:18:32.564 - 00:18:34.324, Speaker D: They belong to a slightly more complicated.
00:18:34.364 - 00:18:42.028, Speaker C: Domain, so I'm not mentioning them here. So one thing I want to note.
00:18:42.076 - 00:18:44.064, Speaker D: Is that if we take a look.
00:18:44.924 - 00:18:50.900, Speaker C: The first two probability space are product space, which means like, we kind of.
00:18:50.932 - 00:18:54.926, Speaker D: Have like n random variables here, and each of them take value plus and.
00:18:54.950 - 00:19:04.062, Speaker C: Minus one with equal probability. And this gives us a product space over n random variables and also for general product space.
00:19:04.118 - 00:19:05.046, Speaker D: Okay, it's more clear.
00:19:05.110 - 00:19:10.966, Speaker C: This is actually product space. These three, though, on the other hand.
00:19:10.990 - 00:19:12.766, Speaker D: Are not really products based, because for.
00:19:12.790 - 00:19:31.786, Speaker C: Symmetric group, we know that if my first coordinate value I, then my second coordinate cannot take value I because it's permutation group. So they're not really independent, but they're pretty close if you only look at like very low degree components of these functions, because literally, like, your second coordinate.
00:19:31.810 - 00:19:34.562, Speaker D: Couldn'T be I, but you could technically be everything else.
00:19:34.618 - 00:19:47.104, Speaker C: So their correlation is pretty low. Similar things for like, similar things for the slices. So for a Boolean slice, we have n random variables.
00:19:47.274 - 00:19:53.196, Speaker D: And we required that essentially, if there are already k random variables that we.
00:19:53.220 - 00:19:55.228, Speaker C: Know takes a value one, then the rest cannot.
00:19:55.356 - 00:19:57.580, Speaker D: But if there are much fewer than.
00:19:57.612 - 00:20:00.172, Speaker C: K random variables taking the value y.
00:20:00.348 - 00:20:03.892, Speaker D: One, we technically still feel it's almost.
00:20:03.948 - 00:20:16.124, Speaker C: Uniform that the rest of the coordinates still take one and negative one with independent probability. Okay, similar things hold for multi slice.
00:20:17.504 - 00:20:21.528, Speaker D: So the point is that they're not really, that they're not exactly product space.
00:20:21.576 - 00:20:26.084, Speaker C: But they have low correlation if you only restrict your attention to a couple coordinates.
00:20:28.904 - 00:20:29.644, Speaker A: So.
00:20:33.984 - 00:20:43.182, Speaker D: Something to say about the techniques being used here. So we know that the bonamile uses like induction, basically induction and also probability.
00:20:43.238 - 00:20:50.678, Speaker C: Of the Bernoulli random variables. And also general, for the general product.
00:20:50.726 - 00:20:54.474, Speaker D: Space, there's also this technique of coupling. So.
00:20:56.614 - 00:21:05.134, Speaker C: We could do coupling, okay, sorry, product space. Actually we'll later see a proof of product space, but which is not exactly.
00:21:05.174 - 00:21:11.342, Speaker D: The proof that's used here, because here we get pretty good parameter like exponential indeed.
00:21:11.358 - 00:21:13.646, Speaker C: But later we'll see a parameter, a.
00:21:13.670 - 00:21:15.430, Speaker D: Proof with worse parameter, but gives us.
00:21:15.462 - 00:21:17.314, Speaker C: Intuition for more general cases.
00:21:18.694 - 00:21:24.934, Speaker D: And for those spaces that's not really product space, there is generally used technique.
00:21:24.974 - 00:21:28.758, Speaker C: Which is coupling, try to couple the space with a product space.
00:21:28.886 - 00:21:30.134, Speaker D: Of course there will be some loose.
00:21:30.174 - 00:21:31.914, Speaker C: Due to the correlation, but.
00:21:34.054 - 00:21:34.486, Speaker D: If your.
00:21:34.510 - 00:21:37.452, Speaker C: Distribution is close enough to product space.
00:21:37.508 - 00:21:41.940, Speaker D: Then the lose is not too much. And also these two results are derived.
00:21:41.972 - 00:21:43.940, Speaker C: From like luxable living equality.
00:21:44.012 - 00:21:52.556, Speaker D: So there's, for many domain, there's this almost equivalence between luxable inequality and hyper contractivity. And I won't really talk about how.
00:21:52.580 - 00:21:55.144, Speaker B: Do you prove this, it's just a.
00:21:55.684 - 00:22:01.144, Speaker C: More general thing than the hypercontructivity.
00:22:02.044 - 00:22:11.188, Speaker D: Okay, so basically the message here is that we were able to, we've seen results for hyper contractivity quality hold for.
00:22:11.316 - 00:22:15.676, Speaker C: General product space, and also hold for like these almost product spaces.
00:22:15.780 - 00:22:17.756, Speaker D: So the question really here is, does.
00:22:17.780 - 00:22:19.428, Speaker C: It hope for like can we more.
00:22:19.476 - 00:22:22.628, Speaker D: Generally show that for every space that's.
00:22:22.676 - 00:22:37.294, Speaker C: Close to product space in some way that these spaces all have, I'll have hypercontractivity. So that's a question we are going to try to answer in this talk.
00:22:37.714 - 00:22:38.454, Speaker D: And.
00:22:40.754 - 00:22:42.578, Speaker C: First, to answer this question, we.
00:22:42.626 - 00:22:44.402, Speaker D: Need to really answer the question like.
00:22:44.458 - 00:22:51.050, Speaker C: What is almost product space mean? It turns out that if we define.
00:22:51.082 - 00:22:56.670, Speaker D: It in this specific way, then we're able to show that for these kind of almost product space that we call.
00:22:56.702 - 00:23:00.206, Speaker C: Epsilon product space, we are able to show hyper contractivity.
00:23:00.350 - 00:23:01.966, Speaker D: And it captures some of the older.
00:23:01.990 - 00:23:03.790, Speaker C: Results, although with worse parameters.
00:23:03.822 - 00:23:05.634, Speaker D: But it also captures some new.
00:23:07.734 - 00:23:08.094, Speaker C: Product.
00:23:08.134 - 00:23:10.158, Speaker D: Space that was not known before, including.
00:23:10.246 - 00:23:22.938, Speaker C: The high dimensional expander, which people have been talking a lot about in this program. What is a product space. So consider a product space.
00:23:23.106 - 00:23:24.514, Speaker D: I will write a product space over.
00:23:24.554 - 00:23:27.346, Speaker C: K random variables as this here the.
00:23:27.370 - 00:23:30.250, Speaker D: Omega, the sample space is really v.
00:23:30.282 - 00:23:33.962, Speaker C: To the k. Okay, so you have.
00:23:34.058 - 00:23:45.146, Speaker D: Just a product of set b, and the distribution is a tensor of a marginal distribution on one random variable, which.
00:23:45.170 - 00:23:46.334, Speaker C: We call mu zero.
00:23:48.054 - 00:24:06.422, Speaker D: So epsilon product space, on the other hand, still have the same domain, but the distribution mu is not really exactly the product. But it needs to satisfy the following condition. Condition on coordinates, a small set of coordinate s. Within this k coordinates, we.
00:24:06.438 - 00:24:17.642, Speaker C: Have that the conditional distribution over the rest of the, over the rest of the variables need to be almost pairwise independent. So almost pairwise independent is just saying.
00:24:17.698 - 00:24:21.122, Speaker D: Like if I take two random variables.
00:24:21.218 - 00:24:25.186, Speaker C: From this conditional distribution, which is obtained.
00:24:25.210 - 00:24:28.506, Speaker D: From conditioning, assigning the value, some arbitrary.
00:24:28.530 - 00:24:35.586, Speaker C: Value x to the coordinates in s, then this two random variables have functions.
00:24:35.690 - 00:24:37.858, Speaker D: Any functions f, g defined on them.
00:24:37.986 - 00:24:49.434, Speaker C: Such that f and g have correlation bounded by epsilon. So this is just saying there the conditional distribution is almost pairwise independent.
00:25:01.614 - 00:25:10.812, Speaker D: So as I said, this captures some interesting instance of probability space. The one I will mention here is.
00:25:10.828 - 00:25:12.412, Speaker C: The epsilon high dimensional expander.
00:25:12.508 - 00:25:15.348, Speaker D: So I know that we've covered multiple.
00:25:15.436 - 00:25:17.836, Speaker C: Definitions of epsilon high dimensional expanders, so.
00:25:17.860 - 00:25:22.436, Speaker D: I will reiterate which one this is. But just to be more precise, this.
00:25:22.460 - 00:25:32.412, Speaker C: Is like the epsilon two sided, epsilon two sided high dimensional expander. So how do we view a high.
00:25:32.428 - 00:25:39.152, Speaker D: Dimensional expander as a probability space? Well, actually, epsilon high dimensional expander here.
00:25:39.168 - 00:25:43.896, Speaker C: I'm trying to use non simplicial complex language. So let's just say they're hypergraphs with.
00:25:43.960 - 00:25:46.808, Speaker D: Associated edge distribution vk to the mu.
00:25:46.936 - 00:25:52.032, Speaker C: So over size k hyper edges. So consider the top level edges of.
00:25:52.088 - 00:25:56.964, Speaker D: These height of a to be k hepar edges.
00:25:57.264 - 00:26:00.416, Speaker C: And for example, I have an example here.
00:26:00.480 - 00:26:03.060, Speaker D: This is a, this is a size.
00:26:03.132 - 00:26:06.660, Speaker C: Three hyper edge is their top level edge.
00:26:06.692 - 00:26:12.444, Speaker D: So actually if we translate them back into high dimensional expander, it will be two dimensional expander.
00:26:12.484 - 00:26:17.300, Speaker C: But let's not worry about that. So literally, this graph is defined by.
00:26:17.372 - 00:26:22.452, Speaker D: Having a, having, having the top level.
00:26:22.508 - 00:26:27.464, Speaker C: Edges being the set of, okay, sorry.
00:26:28.044 - 00:26:31.204, Speaker D: Having the, having the set v exactly.
00:26:31.244 - 00:26:34.420, Speaker C: Being the set of vertices in this hypergraph.
00:26:34.612 - 00:26:36.064, Speaker D: And the domain.
00:26:38.004 - 00:26:45.116, Speaker C: The domain is v to the k, where k here we take to be three in this example. And this mu is a distribution which.
00:26:45.140 - 00:26:48.428, Speaker D: Is usually, sometimes it could vary, but.
00:26:48.516 - 00:26:55.420, Speaker C: Usually the high dimensional expander is just a uniform distribution over its top level hyper edges. So in this case, it will just.
00:26:55.452 - 00:26:57.816, Speaker D: Be a uniform distribution over, uh, the.
00:26:57.840 - 00:26:59.804, Speaker C: Set one, two, three and one, two, four.
00:27:00.944 - 00:27:01.336, Speaker B: Okay.
00:27:01.360 - 00:27:03.416, Speaker C: A small caveat here though is that.
00:27:03.560 - 00:27:05.536, Speaker D: Usually, you know, when we're defining a.
00:27:05.560 - 00:27:09.360, Speaker C: Probability space and we have multiple variables, the order does matter, right?
00:27:09.472 - 00:27:14.736, Speaker D: Um, but usually in high dimensional expanders we have these edge being set.
00:27:14.800 - 00:27:21.344, Speaker C: So kind of the order doesn't matter. So what should be considered here is.
00:27:21.384 - 00:27:30.140, Speaker D: That, um, instead of writing one, two, three, we can write like all different orders of them and give like split.
00:27:30.172 - 00:27:38.628, Speaker C: The distribution, split the weight on the hyper edge, one, two, three evenly to all of these order in different orderings of this hyper edge.
00:27:38.756 - 00:27:40.292, Speaker D: And that will give us a order.
00:27:40.348 - 00:27:49.724, Speaker C: Domain that's actually still the same as this one here. If we remove the order, is that clear?
00:27:54.904 - 00:28:03.032, Speaker D: So this is how we view this hyper, this high dimensional expander, or hypergraph rather as a probability space.
00:28:03.168 - 00:28:08.440, Speaker C: So what's so special about the probabilist space defined by the epsilon high dimensional expander?
00:28:08.472 - 00:28:19.536, Speaker D: Well, it's the link property that make them special. We know that for a epsilon two sided high dimensional expander, spectral high dimensional.
00:28:19.560 - 00:28:23.164, Speaker C: Expander, we have that the second eigenvalue of every link.
00:28:23.584 - 00:28:31.424, Speaker D: So let me define what the link is first. So if we consider any hyper edge.
00:28:31.544 - 00:28:34.984, Speaker C: In which in this case, in this.
00:28:35.024 - 00:28:39.096, Speaker D: Case I take the edge, singleton edge two as an example.
00:28:39.200 - 00:28:44.864, Speaker C: So if we take two two, then its link is defined by first taking all the hyper edges that contain two.
00:28:45.924 - 00:28:49.464, Speaker D: And then these sets of edges actually.
00:28:49.964 - 00:28:57.468, Speaker C: Become a new hypergraph. And after we get this new hypergraph, we remove two from all of them.
00:28:57.596 - 00:29:00.612, Speaker D: So it becomes this image on the.
00:29:00.628 - 00:29:04.852, Speaker C: Right hand side, this graph on the right hand side. And we specifically look at the layer.
00:29:04.908 - 00:29:10.776, Speaker D: Which has singleton vertices and size two edges.
00:29:10.960 - 00:29:14.648, Speaker C: Okay, we look at the bipartite graph.
00:29:14.736 - 00:29:19.120, Speaker D: Basically defined by these two size hyper.
00:29:19.152 - 00:29:22.224, Speaker C: Edges and the link.
00:29:22.384 - 00:29:29.896, Speaker D: When we say lambda two of the link, it's really like lambda two. The second eigenvalue of this second largest.
00:29:29.920 - 00:29:33.232, Speaker C: Eigenvalue of this bipartite graph, or rather.
00:29:33.288 - 00:29:37.870, Speaker D: Sometimes you can think about it as, you know, if you want to think.
00:29:37.902 - 00:29:39.994, Speaker C: About it just as walks on the.
00:29:40.534 - 00:29:49.766, Speaker D: Vertices, you can think about instead a graph purely on the vertices and two vertices are connected if and only if.
00:29:49.870 - 00:29:57.126, Speaker C: There'S an edge, contain both of them. Well, in either case, the lambda two.
00:29:57.190 - 00:30:00.910, Speaker D: Of the link is really the link. Lambda two of the link is really.
00:30:00.942 - 00:30:06.506, Speaker C: The second greatest eigenvalue of this bipartite graph.
00:30:06.650 - 00:30:11.946, Speaker D: And epsilon two sided spectral expander ensures.
00:30:11.970 - 00:30:14.334, Speaker C: That this lambda is upper bounded by epsilon.
00:30:16.114 - 00:30:21.530, Speaker D: So intuitively that's just saying like if I take a random walk on this bipartite graph.
00:30:21.562 - 00:30:22.734, Speaker C: It mixes fast.
00:30:33.734 - 00:30:34.954, Speaker D: Okay, so.
00:30:37.334 - 00:30:38.726, Speaker C: Now I want to just.
00:30:38.830 - 00:30:43.990, Speaker D: Convince you why these epsilon, these distributions given by the epsilon high dimensional expanders.
00:30:44.022 - 00:30:55.606, Speaker C: Are actually epsilon products based. So let's consider what's, what's the definition of epsilon product space? Epsilon product space need you to have.
00:30:55.670 - 00:31:05.904, Speaker D: A sample space and the distribution. And similarly, like we are able to define a sample space and distribution on the high dimensional expenditures.
00:31:07.364 - 00:31:09.740, Speaker C: And the requirement, though the second requirement.
00:31:09.772 - 00:31:11.828, Speaker D: Is more interesting in the Epson product.
00:31:11.876 - 00:31:14.504, Speaker C: Space where we require is that conditional distribution.
00:31:15.884 - 00:31:16.824, Speaker D: Need to.
00:31:18.604 - 00:31:32.382, Speaker C: Need to have that. After we consider conditional distribution, any two random variables and the conditional distribution have bounded covariance, sorry, bounded correlation, like written here.
00:31:32.558 - 00:31:40.662, Speaker D: And in epsilon high dimensional expander, we require that really, when we fix a, when we can, when we fix a.
00:31:40.678 - 00:31:42.526, Speaker C: Hyper edge and consider its link, this.
00:31:42.550 - 00:31:47.174, Speaker D: Is equivalent to, you know, here in product epsilon product space, we fix assignment.
00:31:47.214 - 00:31:51.974, Speaker C: To a subset of coordinates and consider the conditional distribution. So that's the same act.
00:31:52.354 - 00:31:56.090, Speaker D: And once we, okay, in high dimensional.
00:31:56.122 - 00:31:59.494, Speaker C: Expander, once we fix the set of.
00:32:00.394 - 00:32:06.226, Speaker D: The hyper edges and consider the link, then what we consider is this following.
00:32:06.410 - 00:32:07.454, Speaker C: Random walk.
00:32:09.914 - 00:32:11.586, Speaker D: That really walks from a.
00:32:11.610 - 00:32:33.730, Speaker C: Vertex up to a etch inside the link and back down to the vertex. So essentially the two, like the marginal distribution of two random variables in this link is really just the uniform distribution over all the edges in the link. Therefore, we're able to write the lambda k, sorry, we're able to write the.
00:32:33.762 - 00:32:36.122, Speaker D: Second eigenvalue of a link exactly in.
00:32:36.138 - 00:32:39.174, Speaker C: This form here, the inner product.
00:32:41.194 - 00:32:41.506, Speaker B: To.
00:32:41.530 - 00:32:42.986, Speaker C: Make sure the inner product is taking.
00:32:43.050 - 00:32:47.336, Speaker D: Product between like, you know, two connected.
00:32:47.400 - 00:32:51.844, Speaker C: Vertices in this link graph that we described.
00:32:53.664 - 00:32:57.464, Speaker D: So essentially we can really just write.
00:32:57.504 - 00:33:06.120, Speaker C: This requirement of link lambda two of the link being bounded, just equivalently actually.
00:33:06.192 - 00:33:12.696, Speaker D: Give us that really the correlation between any two coordinates in this conditional distribution.
00:33:12.800 - 00:33:29.464, Speaker C: Or in this link is bounded. So this is kind of a sketch of why it's true that epsilon products, sorry, epsilon high dimensional expander are epsilon products based. Any questions about this?
00:33:38.244 - 00:33:39.784, Speaker A: When you say coordinates.
00:33:44.454 - 00:33:46.334, Speaker B: So like select.
00:33:46.374 - 00:33:49.754, Speaker A: Coordinates and s being assigned x, or like all of them.
00:33:52.854 - 00:33:54.142, Speaker C: Inside x, sorry.
00:33:54.198 - 00:34:08.954, Speaker A: S is a subset of coordinates. So subset of random variables, x is an assignment to it. So you can select any assignment x for s and this equal. So this, this value should hold.
00:34:16.734 - 00:34:29.154, Speaker E: So up there we have the requirement for all I and j, right? But down there, what's the correspondence for that? Is it because your flux space given by FDX is symmetric so we don't have to add?
00:34:30.494 - 00:34:57.028, Speaker A: Yeah, that's right. So yeah, so epsilon product space is more general because. Yeah, if we restrict in this conditional distribution, marginal distribution on variable I and j could be different, and also their joint distribution could be different. But due to the symmetric nature of how we define high dimensional expander, remember we said it's naturally an order, but we just give it symmetric ordering because of that. Actually, all the IJs, all the IJs.
00:34:57.116 - 00:34:59.220, Speaker D: Are the same, and all the joint.
00:34:59.252 - 00:35:04.660, Speaker A: Distributions between two different coordinates, IJ, are all the same.
00:35:04.732 - 00:35:05.988, Speaker B: So. That's right.
00:35:06.036 - 00:35:07.224, Speaker A: That's a good point.
00:35:07.794 - 00:35:09.858, Speaker D: So we don't need to have the IG requirement.
00:35:09.906 - 00:35:17.334, Speaker B: Here's.
00:35:20.354 - 00:35:21.094, Speaker E: Zero.
00:35:21.474 - 00:35:24.386, Speaker A: They're all allowed to substitute the zeros.
00:35:24.490 - 00:35:25.774, Speaker D: So actually.
00:35:28.034 - 00:35:31.658, Speaker A: What do you mean by zero? You mean v being null or.
00:35:31.826 - 00:35:36.654, Speaker E: No, the x. So things correspond to substituting ones, but you can also substitute zero.
00:35:38.964 - 00:35:40.784, Speaker C: Oh, actually, sorry.
00:35:44.164 - 00:35:47.104, Speaker A: Correspond to substituting one.
00:35:47.564 - 00:35:49.932, Speaker B: You should also substitute zero.
00:35:50.108 - 00:35:54.704, Speaker A: So, okay, do you mean this zero or actually domain.
00:35:57.924 - 00:36:04.184, Speaker E: She'S embedded into apartheid complex with a different order that say this variable, is this not the same?
00:36:04.924 - 00:36:08.180, Speaker A: Right. So actually the domain here is v, it's not 10.
00:36:08.252 - 00:36:16.828, Speaker B: So, yeah, okay.
00:36:16.876 - 00:36:22.772, Speaker A: So, yeah, this is just to say that epsilon high dimensional expander is really.
00:36:22.828 - 00:36:24.144, Speaker B: A special case of.
00:36:26.964 - 00:36:29.196, Speaker D: And finally, in this talk, I'll be.
00:36:29.220 - 00:36:34.970, Speaker C: Able to state the result. Um, our result says that for all.
00:36:35.002 - 00:36:41.682, Speaker D: Epsilon products, space, um, omega, mu, and all functions f with bounded l two norm.
00:36:41.818 - 00:36:45.418, Speaker C: If f is degree d, um, which.
00:36:45.506 - 00:36:52.722, Speaker D: I'll later define what really degree d means. Um, and also, uh, if f is, uh, the so called d delta global.
00:36:52.778 - 00:36:56.122, Speaker C: Functions, which I will also later uh.
00:36:56.178 - 00:36:58.704, Speaker D: Define, then we have that the four.
00:36:58.744 - 00:37:04.144, Speaker C: Norm is bounded by this number, this expression on the right. So you see that there is part.
00:37:04.184 - 00:37:09.640, Speaker D: That depends on this delta and this degree d. And there's also this error.
00:37:09.672 - 00:37:21.000, Speaker C: Term that depends on epsilon. And I should say this k really is, this is maybe the part I wasn't explicitly saying, but this k is.
00:37:21.032 - 00:37:25.132, Speaker D: The dimension is the level of faces.
00:37:25.188 - 00:37:27.932, Speaker C: That our f is defined over in.
00:37:27.948 - 00:37:39.664, Speaker D: High dimension in this space. So, okay, and I want to say that, well, as Max mentioned already, like.
00:37:40.484 - 00:37:42.460, Speaker C: The same result is shown by Buffner.
00:37:42.492 - 00:37:56.804, Speaker D: Hopkins, Kaufman and Levitt. And they're actually able to obtain a better expression here. I think they're able to get exponential in D. And their results specialize for.
00:37:56.884 - 00:37:58.504, Speaker C: Epsilon high dimensional expander.
00:38:02.284 - 00:38:11.904, Speaker D: And the techniques are different. So, yeah, so in the rest of this talk, I will talk a little bit about the technique in this paper.
00:38:15.584 - 00:38:17.832, Speaker B: Dependence on k is exponential.
00:38:18.008 - 00:38:19.536, Speaker E: What is the dependence on k?
00:38:19.680 - 00:38:38.648, Speaker C: What's the dependency on k? Okay. Oh, dependence on k. Exponential in k? Yeah, I don't exactly remember, but I think it's exponential. Okay, so, okay, so it means like.
00:38:38.696 - 00:38:49.424, Speaker D: Really you want your epsilon to be inverse exponentially smaller than k. For this bound to really work. So you really need really, really good high dimensional expanders.
00:38:50.804 - 00:38:52.544, Speaker C: Actual high dimensional expanders.
00:38:56.284 - 00:39:02.364, Speaker D: Okay, so before talking about the proof framework, then I feel I should really.
00:39:02.404 - 00:39:06.884, Speaker C: Explain why do we have this global function restriction?
00:39:07.044 - 00:39:08.940, Speaker D: So recall that actually when we were.
00:39:08.972 - 00:39:16.714, Speaker C: Talking about some implications of high dimension, sorry, of hypercontructivity, we mentioned one of them is the level d equality.
00:39:17.134 - 00:39:19.714, Speaker D: So the level d inequality.
00:39:21.694 - 00:39:22.262, Speaker A: In general.
00:39:22.318 - 00:39:24.254, Speaker D: Says something like, oh, if you have.
00:39:24.294 - 00:39:28.430, Speaker C: Like hypercontractivity for all functions s, then.
00:39:28.502 - 00:39:31.550, Speaker D: You should have that, those boolean valued.
00:39:31.582 - 00:39:48.644, Speaker C: Function with low weights. So here, weights means low expectation. With low weights should have their, have their weights concentrated on high degrees. So the high degree part of the function should have large norm. However, this is really not the case.
00:39:49.184 - 00:39:55.480, Speaker D: In like Epson product space, and not also not the case in things like.
00:39:55.632 - 00:40:01.084, Speaker C: Very biased hypercube or slice with, or very biased slices.
00:40:03.744 - 00:40:09.944, Speaker D: And the reason, and the reason is that, okay, if we consider like in the very, in the Epson product space.
00:40:10.064 - 00:40:27.944, Speaker C: Maybe my function can be only of degree one. My function can just be the indicator function. Okay, it's not exactly, yeah, but my function could be an indicator function of, of whether my first coordinate is equal to one. If I have that, then this is a really low weight function.
00:40:27.984 - 00:40:31.224, Speaker D: This could be a really low weight function if your Alphabet size is really large.
00:40:31.264 - 00:40:39.214, Speaker C: Right, this is could like, in our case, it's like order of one over n, because our Alphabet is from one through n, it's all the vertices.
00:40:40.234 - 00:40:45.210, Speaker D: However, this function literally have no weight.
00:40:45.242 - 00:40:47.202, Speaker C: On the high degree part, because this.
00:40:47.218 - 00:40:49.906, Speaker D: Is just indicator function that depends only.
00:40:49.970 - 00:40:53.534, Speaker C: On the first coordinate. So it couldn't possibly be of high degree.
00:40:53.954 - 00:40:59.858, Speaker D: Therefore, this level d inequality doesn't really hold. And that tells us basically this general.
00:40:59.906 - 00:41:09.904, Speaker C: Hypercontractivity couldn't hold, especially couldn't hold for the function type of function I just talked about, which is like indicator function of a small set of coordinates taking a certain value.
00:41:11.684 - 00:41:13.604, Speaker D: So because of that, we need to.
00:41:13.644 - 00:41:32.724, Speaker C: Exclude these kind of functions that we couldn't hope to get hyper contractivity for. But it turns out that actually excluding these type of function is enough. I think that's a more interesting result. Like all functions that's far from these kind of functions are going to have hypercontractivity inequality.
00:41:33.224 - 00:41:39.696, Speaker D: Okay, so specifically, a function we call function f is d delta global.
00:41:39.880 - 00:41:54.604, Speaker C: If for any set of coordinates that's of size and most d, and any assignment x over these set of random variables, we have that if we restrict our domain to just these.
00:41:56.304 - 00:42:01.986, Speaker D: To just these space, this subspace where the coordinates.
00:42:02.010 - 00:42:11.650, Speaker C: In s are assigned to x, and we take the l two norm of this new restrictive function. It's upper bounded by delta, where square root delta.
00:42:11.682 - 00:42:13.294, Speaker B: So there's a square here.
00:42:14.234 - 00:42:24.162, Speaker D: So it is saying like you couldn't have a subpart. That's too, no, that's too salient. For example, if actually our function is.
00:42:24.178 - 00:42:29.594, Speaker C: An indicator function, then when I assign that the first coordinate, for example, to.
00:42:29.634 - 00:42:38.154, Speaker D: Be one, then this will be like one. So that wouldn't be bounded, but so.
00:42:38.194 - 00:42:41.334, Speaker C: That'S not a global function, the example we just mentioned.
00:42:42.394 - 00:42:46.418, Speaker D: Okay, so these exclude, this requirement really.
00:42:46.466 - 00:42:57.374, Speaker C: Excludes all the functions that are close to being a dictator function, as I just mentioned.
00:43:01.834 - 00:43:07.842, Speaker D: So, to talk about the final way to talk about actually the proof, we.
00:43:07.938 - 00:43:11.098, Speaker C: Need to have a couple of useful notions to discuss.
00:43:11.186 - 00:43:13.874, Speaker D: So literally, there are two things you really need.
00:43:13.914 - 00:43:15.410, Speaker C: The first type of thing you need.
00:43:15.442 - 00:43:18.244, Speaker D: For this kind of proviso or sagnal.
00:43:18.354 - 00:43:25.404, Speaker C: Or an orthogonal decomposition for the function f in the space.
00:43:25.944 - 00:43:33.360, Speaker D: So here is an example for the decomposition in the Boolean hypercube case. In the Boolean hypercube, we have this.
00:43:33.392 - 00:43:37.016, Speaker C: Decomposition being like the Fourier characters, and.
00:43:37.080 - 00:43:38.800, Speaker D: The coefficient here are just the Fourier.
00:43:38.872 - 00:43:41.512, Speaker C: Coefficient, and we know that these are.
00:43:41.568 - 00:43:48.050, Speaker D: Orthogonal decomposition and therefore they're unique also. And another thing we really need is.
00:43:48.202 - 00:43:51.254, Speaker C: Like a derivative operator for these functions.
00:43:51.594 - 00:43:56.362, Speaker D: So we can define these functions, these like d sub s of x to.
00:43:56.378 - 00:43:59.474, Speaker C: Be derivative with respect to like variables.
00:43:59.514 - 00:44:07.914, Speaker D: In the set s and evaluated at the point x. So x is assignment to any arbitrary.
00:44:07.954 - 00:44:15.254, Speaker C: Assignment of s. And we really need, so that this derivative operator decrease the.
00:44:15.294 - 00:44:18.614, Speaker D: Degree of the function from degree of.
00:44:18.654 - 00:44:21.874, Speaker C: F to a most degree of f minus size of s.
00:44:30.614 - 00:44:31.542, Speaker D: Mostly what I.
00:44:31.558 - 00:44:35.710, Speaker C: Will do is I will show the proof actually first for the product space.
00:44:35.862 - 00:44:38.414, Speaker D: Because our proof for the epsilon product.
00:44:38.454 - 00:44:51.236, Speaker C: Space really mostly follows the proof of the product space, but with some relaxation, because we couldn't possibly, couldn't really get a nice orthogonal basis that base the degree decomposition.
00:44:51.420 - 00:44:54.964, Speaker D: So I will introduce, what are the.
00:44:55.004 - 00:44:58.804, Speaker C: Two notions we mentioned before? What are they in product space.
00:44:58.964 - 00:45:05.628, Speaker D: So, in product space there is this general decomposition called efron Stein decomposition that.
00:45:05.676 - 00:45:07.784, Speaker C: Actually is orthogonal and unique.
00:45:08.114 - 00:45:11.882, Speaker D: So we can decompose f into a.
00:45:11.898 - 00:45:19.306, Speaker C: Subset of like f equal to s, where s are subsets of k and such that when s and t are.
00:45:19.330 - 00:45:22.362, Speaker D: Not equal, then f equal to s and f equal to t. They really.
00:45:22.418 - 00:45:38.644, Speaker C: Have zero inner product under this distribution. And, okay, they're defined really by this projection. They're defined by this projection operator. A sub s. A sub s is.
00:45:38.684 - 00:45:41.012, Speaker D: Literally a projection operator projecting f on.
00:45:41.068 - 00:45:42.344, Speaker C: All the coordinates.
00:45:44.204 - 00:45:46.316, Speaker D: Onto only those coordinates.
00:45:46.340 - 00:45:57.068, Speaker C: In s. So it's taking expectation over all the rest of the coordinates. That's the projection. Probably the only thing that need to.
00:45:57.076 - 00:46:00.260, Speaker D: Be remembered is that these are orthogonal.
00:46:00.332 - 00:46:03.504, Speaker C: And unique decomposition over product spaces.
00:46:04.034 - 00:46:11.778, Speaker D: And then we can, um, equivalently define like the derivative. It's really analogous to the derivative we see in calculus.
00:46:11.906 - 00:46:19.738, Speaker C: It's literally summation over all t that contains s. F equal to s. Also derivative operator.
00:46:19.866 - 00:46:24.778, Speaker D: Because we take the derivative and evaluate it at x, it's a function really.
00:46:24.826 - 00:46:29.324, Speaker C: On the rest of the coordinates that it's not including those in size.
00:46:30.054 - 00:46:32.214, Speaker D: So because of that, we have its.
00:46:32.254 - 00:46:37.670, Speaker C: Degree as a most df minus s. Okay.
00:46:37.702 - 00:46:44.294, Speaker D: And I want to say like really the degree of a component is really, you can a degree of function.
00:46:44.334 - 00:46:49.470, Speaker C: You just look at the largest and nonzero component in this summation.
00:46:49.662 - 00:46:53.830, Speaker D: So for example, f equal to s, this part would have degree size of.
00:46:53.862 - 00:47:01.734, Speaker C: S, and the component that have the largest, the largest nonzero component would give you the degree.
00:47:05.794 - 00:47:11.314, Speaker D: So now we, with this two, with this two notion, we're able to talk.
00:47:11.354 - 00:47:14.858, Speaker C: About how to show hyper contractivity over the product space.
00:47:15.026 - 00:47:19.014, Speaker D: So specifically in this talk, we'll quickly.
00:47:19.934 - 00:47:22.982, Speaker C: Yeah, we'll quickly go over this, how.
00:47:22.998 - 00:47:28.982, Speaker D: To prove this in general, as you can see here, is 400 d to the power of d. So it's d.
00:47:28.998 - 00:47:34.154, Speaker C: To the d coefficient in this hypercontractivity inequality. So it's not optimal.
00:47:35.414 - 00:47:51.794, Speaker D: Yeah, and maybe address this a little bit more later. And this is really done also by induction, just like bonhomie lemma. But however, bonhomie, we do induction by the number of coordinates.
00:47:51.914 - 00:47:55.442, Speaker C: Here, we do induction by the degree of the function f. If we're able.
00:47:55.498 - 00:47:59.378, Speaker D: To decompose this four norm into some.
00:47:59.426 - 00:48:04.738, Speaker C: Component involved two norm, and some other component still having four norm.
00:48:04.826 - 00:48:06.458, Speaker D: However, it's on the four norm on.
00:48:06.466 - 00:48:08.850, Speaker C: The derivative of f, which we know.
00:48:08.882 - 00:48:12.870, Speaker D: Just by the discussion previously, have lower.
00:48:12.902 - 00:48:36.942, Speaker C: Degree than the function f. Okay, so this is a decomposition. Some part involves two norm and some part involves four norm of a lower degree function. Yeah, I'm thinking, okay, yeah, maybe I should still cover parts of it, but.
00:48:36.998 - 00:48:40.602, Speaker D: I think I will probably skip some of the things that's a little bit.
00:48:40.658 - 00:48:43.374, Speaker C: Too much to cover here, given the time.
00:48:43.754 - 00:48:49.842, Speaker D: Uh, we, we can still follow this, this line though. So this line is really rewriting the.
00:48:49.858 - 00:48:57.562, Speaker C: Four norm, um, as like a two norm of the function f squared. Okay, so this is like nothing.
00:48:57.698 - 00:49:00.134, Speaker D: This is like this just by definition.
00:49:01.074 - 00:49:02.770, Speaker C: And next, what one can do is.
00:49:02.802 - 00:49:07.246, Speaker D: Really to uh, write the recall that.
00:49:07.270 - 00:49:08.318, Speaker C: We'Re still on product space.
00:49:08.366 - 00:49:11.542, Speaker D: So we have all the nice things about unique decomposition.
00:49:11.598 - 00:49:14.142, Speaker C: We're still going to decomposition, okay, so.
00:49:14.238 - 00:49:19.246, Speaker D: Because of that we can write this l two norm into the summation of.
00:49:19.270 - 00:49:24.394, Speaker C: A bunch of l two norm square, okay? Because they are orthogonal decomposition.
00:49:24.974 - 00:49:27.110, Speaker D: And finally, we can open up the.
00:49:27.222 - 00:49:30.362, Speaker C: F square inside by also writing f.
00:49:30.478 - 00:49:32.178, Speaker D: Each of the f a summation over.
00:49:32.266 - 00:49:38.294, Speaker C: F equal to t. So we have this large term inside here. So we open up twice.
00:49:40.514 - 00:49:42.922, Speaker D: Okay, now at this point we're going.
00:49:42.938 - 00:49:46.014, Speaker C: To address each of these kind of components.
00:49:46.514 - 00:49:53.574, Speaker D: So we divide this term into, we treat them separately and divide them into three cases.
00:49:53.914 - 00:49:57.692, Speaker C: First cases, okay, let's see, we have this diagram here.
00:49:57.778 - 00:49:59.744, Speaker D: First case is when they actually have.
00:49:59.784 - 00:50:03.404, Speaker C: Non zero intersection, all three of them. So something here in the middle.
00:50:04.184 - 00:50:11.576, Speaker D: And second case is when like when some, some, like some element, it's either.
00:50:11.640 - 00:50:14.204, Speaker C: In this part, this part or this part.
00:50:14.504 - 00:50:17.024, Speaker D: So there only some element that's only.
00:50:17.064 - 00:50:20.320, Speaker C: In one of the three, okay, if.
00:50:20.352 - 00:50:22.168, Speaker D: None of the first two cases happen.
00:50:22.296 - 00:50:25.332, Speaker C: Actually one can convince oneself that it.
00:50:25.348 - 00:50:28.188, Speaker D: Must be then that t one symmetric.
00:50:28.236 - 00:50:34.584, Speaker C: Difference, t two is equal to s because everything needs to be in one of these three small petals.
00:50:35.884 - 00:50:36.744, Speaker B: Okay.
00:50:38.964 - 00:50:40.864, Speaker D: I will just quickly say that.
00:50:41.324 - 00:50:51.170, Speaker C: In the first component, when there is something in the middle of the three, then what one can do is really like, because there are some, some coordinates they share.
00:50:51.292 - 00:50:53.174, Speaker D: What one can do is one can.
00:50:53.214 - 00:51:02.314, Speaker C: Condition on that coordinate. Once one condition on that coordinate, this component become a one degree, at least one degree less function.
00:51:03.214 - 00:51:10.694, Speaker D: So we kind of what happened is this become a l two norm of.
00:51:10.814 - 00:51:20.994, Speaker C: Some new function that have degree lower than our original f. So this case, this case really become this component here.
00:51:23.854 - 00:51:25.954, Speaker D: I won't comment too much on it.
00:51:26.854 - 00:51:39.806, Speaker C: I would just say like this is more mechanical. So just to kind of believe it at this point, maybe the more interesting parts are the second and the third case.
00:51:39.990 - 00:51:41.630, Speaker D: So it turns out that second case.
00:51:41.702 - 00:51:47.344, Speaker C: In the second case, this type of term here in the summation completely vanish. They're just zero.
00:51:47.684 - 00:51:54.564, Speaker D: And the reason being when there is some I uniquely, in just one of.
00:51:54.644 - 00:51:57.092, Speaker C: These three components, there are two kind.
00:51:57.108 - 00:52:01.580, Speaker D: Of cases, either I as in s or I as in either t one or t two.
00:52:01.772 - 00:52:07.784, Speaker C: When I is in s, what happens is that this component become, can be written as.
00:52:08.564 - 00:52:19.266, Speaker D: Okay, a is equal to f one. Oh, sorry. F equal to t one. F equal to t two. Literally, this component only depend on coordinates in t one union t two.
00:52:19.410 - 00:52:21.290, Speaker C: And because this is product space, so.
00:52:21.322 - 00:52:28.530, Speaker D: We can actually write this equality that it's equal to a t one union t two.
00:52:28.642 - 00:52:31.594, Speaker C: So we project this product onto just.
00:52:31.674 - 00:52:34.106, Speaker D: The coordinates t one, t two and.
00:52:34.130 - 00:52:37.654, Speaker C: Then take its equal to s component.
00:52:39.434 - 00:52:43.668, Speaker D: Because, because the I is in s but not in t one nor t two.
00:52:43.716 - 00:52:59.036, Speaker C: So s is not a subset of t one and t two. And because of that, actually this component, this inside function has zero component on.
00:52:59.060 - 00:53:02.744, Speaker D: The subset s because, because s really.
00:53:03.164 - 00:53:13.224, Speaker C: Is not contained inside these set of coordinates and therefore this is actually zero. This is by orthogonal decomposition.
00:53:22.644 - 00:53:23.316, Speaker B: Questions.
00:53:23.380 - 00:53:23.932, Speaker D: Yeah.
00:53:24.068 - 00:53:27.196, Speaker A: How is this case mutually exclusive than the first one?
00:53:27.260 - 00:53:28.984, Speaker C: Is this supposed to be that like.
00:53:31.924 - 00:53:32.954, Speaker B: This trigger?
00:53:33.044 - 00:53:33.430, Speaker D: Oh yeah.
00:53:33.462 - 00:53:35.914, Speaker A: This three cases are non mutually exclusive.
00:53:36.694 - 00:53:38.594, Speaker D: Because we're trying to upper bound so.
00:53:39.374 - 00:53:43.790, Speaker C: We can double count some cases. Yeah. The first two cases are not mutually exclusive.
00:53:43.902 - 00:53:48.710, Speaker D: The last one is just capturing the things that's neither one or two.
00:53:48.862 - 00:53:53.114, Speaker B: That's right. Okay.
00:53:56.014 - 00:54:00.838, Speaker D: Similarly, when I is in either t one and t two, in which case we actually just assume they're in t.
00:54:00.886 - 00:54:03.306, Speaker C: One, what we can do is we.
00:54:03.330 - 00:54:11.442, Speaker D: Can, we can the inside ft one, ft two, we can write it as we project f equal to t one.
00:54:11.578 - 00:54:14.934, Speaker C: Onto the space of all coordinates. Except for I.
00:54:17.754 - 00:54:33.504, Speaker D: Or the reason being that because later we're going to take the projection onto equal to s. So the part depending on I will vanish anyway. So because of, uh, actually because of or sagna decomposition, we're able to, we're able to do this. Um.
00:54:34.564 - 00:54:35.304, Speaker B: Right.
00:54:36.564 - 00:54:51.824, Speaker D: And then we, uh. Because of that, uh, we have that uh. A, uh, this, this whole component is really depending.
00:54:56.764 - 00:54:58.156, Speaker C: Missing something here.
00:54:58.300 - 00:54:59.224, Speaker D: Okay, so.
00:55:25.624 - 00:55:50.212, Speaker A: Sorry, I'm confusing myself a little bit. Okay. You're saying that it's because if we. Okay, this is self adjoin, self adjoint operator. So you're saying, moving it to, applying this, a projection to this f equal.
00:55:50.228 - 00:55:57.540, Speaker C: To t two, we'll still give you.
00:55:57.572 - 00:55:58.820, Speaker D: F equal to t two.
00:55:58.892 - 00:56:10.012, Speaker C: I think. Sorry, let me this.
00:56:10.068 - 00:56:21.732, Speaker E: A operator kills everything that contains coordinates outside of t one unit two.
00:56:21.908 - 00:56:22.604, Speaker B: Okay.
00:56:22.724 - 00:56:29.464, Speaker E: And s in that case actually does contain a coordinate outside of t one union t two.
00:56:30.324 - 00:56:31.772, Speaker B: I see.
00:56:31.908 - 00:56:36.184, Speaker E: And so the function inside has nothing on the s component.
00:56:36.904 - 00:56:43.124, Speaker B: Let's see. Okay. I see.
00:56:43.584 - 00:56:46.528, Speaker A: I thought though, I thought though, the.
00:56:46.576 - 00:56:48.480, Speaker C: Actual thing happening here is that because.
00:56:48.552 - 00:56:52.232, Speaker A: T one is not containing k exclude I.
00:56:52.408 - 00:56:56.564, Speaker C: So what happens is that this part, this first part here will become a zero.
00:57:01.824 - 00:57:03.164, Speaker E: The previous one.
00:57:03.584 - 00:57:04.760, Speaker C: Okay, cool. Cool.
00:57:04.832 - 00:57:06.528, Speaker A: Yeah, sorry. That confuses myself.
00:57:06.656 - 00:57:07.128, Speaker C: Sorry.
00:57:07.216 - 00:57:17.320, Speaker A: So, yeah, so the actual thing, what, what happens is that this first component, when a k squared I is applied to f equal to t one, it's.
00:57:17.352 - 00:57:18.792, Speaker C: Going to give a zero function.
00:57:18.848 - 00:57:45.396, Speaker A: And because of that, this whole thing will evaluate to zero. Oh, so let me just quickly say the last one. The last one, well, last one is even slightly more complicated, but the last one is saying like, okay, if we have f equal to t one, if.
00:57:45.420 - 00:57:57.304, Speaker D: You put the t two equal to s such that the symmetric difference of t one, t two is equal to s, then we can write the first equality. That is because, uh.
00:57:57.764 - 00:57:58.612, Speaker C: Okay, a.
00:57:58.668 - 00:58:05.236, Speaker D: A s projecting on this space actually should have included many more things than just the equal to s part. It should also include all the equal.
00:58:05.260 - 00:58:10.380, Speaker C: To s prime, where s prime are strictly subsets of s. However, that actually.
00:58:10.452 - 00:58:13.596, Speaker D: Reduced to our case two, because then.
00:58:13.740 - 00:58:16.556, Speaker C: That implies like there is some component.
00:58:16.620 - 00:58:17.980, Speaker D: In t one or t two that's.
00:58:18.012 - 00:58:24.294, Speaker C: Not in s. So those terms actually would vanish. Um, because of that, we can write it this equality.
00:58:25.354 - 00:58:28.614, Speaker D: And, uh. And also because like t.
00:58:30.514 - 00:58:31.066, Speaker B: Yeah, and.
00:58:31.090 - 00:58:33.810, Speaker D: The, and later we can also write.
00:58:33.842 - 00:58:35.734, Speaker C: This because this is a product distribution.
00:58:37.434 - 00:58:43.034, Speaker D: As a result, we know that this is a product, this two, uh, these two functions.
00:58:43.154 - 00:58:52.726, Speaker C: And the first function is really a function only over coordinates in s intersect t one. And the second function is really a function over the coordinates s intersect t two.
00:58:52.910 - 00:58:55.142, Speaker D: So, and also this is a product distribution.
00:58:55.198 - 00:59:02.550, Speaker C: So they are completely disjoint and independent functions. As a result, one can write this.
00:59:02.582 - 00:59:04.350, Speaker D: L two norm is just l two.
00:59:04.382 - 00:59:08.054, Speaker C: Norm of each of them product together.
00:59:08.174 - 00:59:10.190, Speaker D: Times together, multiply together.
00:59:10.262 - 00:59:10.914, Speaker B: Sorry.
00:59:11.414 - 00:59:14.432, Speaker D: And therefore, it's finally bounded by this.
00:59:14.518 - 00:59:20.604, Speaker C: L two norm of each of them multiplied together. And this part is actually going to.
00:59:20.644 - 00:59:22.340, Speaker D: Be the part that contributes to this.
00:59:22.372 - 00:59:26.624, Speaker C: First term, which have l two norm.
00:59:27.204 - 00:59:32.572, Speaker D: There is additionally a step I'm going to skip about. Once you're here, you can actually apply.
00:59:32.628 - 00:59:38.756, Speaker C: The standard hypercontractivity over the boolean hypercube to get this thing here, because we.
00:59:38.780 - 00:59:40.204, Speaker D: See that nine to the d here.
00:59:40.244 - 00:59:41.984, Speaker C: Actually exactly come from that.
00:59:42.284 - 00:59:46.884, Speaker D: Um. All right, so this is roughly how.
00:59:46.924 - 00:59:50.756, Speaker C: This proof of this key lemma goes.
00:59:50.900 - 00:59:52.904, Speaker D: And once you have this, you really.
00:59:53.204 - 00:59:55.624, Speaker C: Have the original product distribution.
00:59:59.124 - 01:00:00.396, Speaker D: Hypercontructivity.
01:00:00.540 - 01:00:04.804, Speaker C: And, okay, I really should have covered this earlier, but you know, like once.
01:00:04.844 - 01:00:06.116, Speaker D: We want to move away and move.
01:00:06.140 - 01:00:12.682, Speaker C: To the Epson product space, the only thing that happens is like, we still define the projection operator as before.
01:00:12.858 - 01:00:14.730, Speaker D: We still kind of define the f.
01:00:14.762 - 01:00:18.610, Speaker C: Equal to s component as before, except for, because it's not a product space.
01:00:18.682 - 01:00:20.530, Speaker D: We end up not having them being.
01:00:20.562 - 01:00:27.354, Speaker C: Orthogonal, they're just epsilon close to orthogonal, so we don't have unique decomposition. Many discussion we've seen just before about.
01:00:27.394 - 01:00:32.594, Speaker D: Case two and three only hold approximately. So when before we can write equality.
01:00:32.674 - 01:00:34.114, Speaker C: Or before we can write equal to.
01:00:34.154 - 01:00:36.344, Speaker D: Zero, now we can really only write.
01:00:36.514 - 01:00:53.388, Speaker C: Uh, it was a error term of o of k epsilon f two. And with really, with the change of that, we're able to get a, get through the entire proof with a error term here.
01:00:53.516 - 01:00:54.224, Speaker B: Yeah.
01:00:56.844 - 01:00:58.452, Speaker C: This really is three.
01:00:58.508 - 01:01:00.812, Speaker D: So I will just talk about two.
01:01:00.868 - 01:01:03.264, Speaker C: Open directions I think might be interesting.
01:01:04.544 - 01:01:06.920, Speaker D: So one interesting one thing is we've.
01:01:06.952 - 01:01:08.848, Speaker C: Seen that in this proof, although I.
01:01:08.856 - 01:01:11.712, Speaker D: Didn'T really get into it, but very crucially, we rely on the fact that.
01:01:11.728 - 01:01:14.448, Speaker C: The link expansion, okay, in specifically, in.
01:01:14.456 - 01:01:16.312, Speaker D: The case that we have epsilon high.
01:01:16.328 - 01:01:18.472, Speaker C: Dimensional expander, we really require that domain.
01:01:18.528 - 01:01:24.524, Speaker D: Have like epsilon bounded like lambda two links.
01:01:24.824 - 01:01:27.376, Speaker C: So that's really fit the definition of.
01:01:27.520 - 01:01:29.704, Speaker D: Epsilon high dimensional expander.
01:01:29.784 - 01:01:54.324, Speaker C: However, for other type of high dimensional expander that also are probably more appropriate for applications like testing or like harness reduction, for example, all those like co systolic expander or robust testability. Robust tester or like co boundary expanders, how are this kind of property going.
01:01:54.364 - 01:02:02.744, Speaker D: To be shown on over such domains? As another, it's a question, I think it's interesting. And also, as I said, there are.
01:02:02.824 - 01:02:10.112, Speaker C: Already results on Grassman graphs and also shortcode graphs, which are literally non commutative spaces.
01:02:10.168 - 01:02:23.672, Speaker D: So how to in general show things on those spaces is also interesting direction. And finally, I want to say we see that in the induction there is this loss of 400 d to the.
01:02:23.688 - 01:02:25.564, Speaker C: D, which is not really optimal.
01:02:26.304 - 01:02:36.864, Speaker D: Another interesting thing is really we didn't make use of the noise operator at all, really for things like the continuous space. The proof of hypercontractivity really heavily relies.
01:02:36.904 - 01:02:50.048, Speaker C: On using those noise operators. But here in the proof, we never seen it even occurring. Possibly. Could there be ways to incorporate stochastic process and all those noise operators in.
01:02:50.056 - 01:02:51.824, Speaker D: The proof so that we can save.
01:02:51.864 - 01:02:54.444, Speaker C: That 400 d to the d factor in the front?
01:02:56.314 - 01:02:58.034, Speaker D: That will be the things I think.
01:02:58.074 - 01:03:00.814, Speaker C: Are probably interesting to some of you guys.
01:03:01.314 - 01:03:03.414, Speaker D: Okay, so, yeah, that's it.
01:03:11.474 - 01:03:16.414, Speaker E: So we're a bit over time, but if there are any quick questions, now is the time.
01:03:23.734 - 01:03:36.206, Speaker A: I have a question about like Yc depends on like degree of the function in some of the cases, and then on like n which is the dimension of this space.
01:03:36.310 - 01:03:37.114, Speaker B: In other.
01:03:40.654 - 01:04:00.040, Speaker A: Okay, I see. So in the case that it depends on n, it's usually when this marginal distribution on one coordinate are really biased. So that's why it shows up. Like, you see all the un over k, they are over the slices. And when they're depending on. In general they should depend on degree.
01:04:00.112 - 01:04:02.644, Speaker C: Actually, because our noise operator t rho.
01:04:02.944 - 01:04:04.684, Speaker D: Follows the degree decomposition.
01:04:05.144 - 01:04:08.688, Speaker A: So yeah, in general it should depend.
01:04:08.736 - 01:04:10.976, Speaker C: On the, if it's written in the.
01:04:11.000 - 01:04:17.554, Speaker A: Form like a four norm of f by two norm of f without the t row inside.
01:04:21.094 - 01:04:25.714, Speaker E: I was just wondering, is it well understood where the global requirement is necessary?
01:04:26.734 - 01:04:34.910, Speaker A: I think we kind of entered, at least we know how to predict it because we know specific functions that cause them to be bad, which are those.
01:04:34.942 - 01:04:35.514, Speaker D: Like.
01:04:37.574 - 01:04:40.480, Speaker A: Indicators of subcubes or like, those.
01:04:40.512 - 01:04:45.880, Speaker D: Are the, literally, I think those are the bad cases. So we kind of do know.
01:04:45.912 - 01:04:46.880, Speaker A: What explains that?
01:04:46.912 - 01:04:48.084, Speaker B: We couldn't get local.
01:04:49.424 - 01:04:51.084, Speaker A: Sorry, we couldn't get general.
01:04:54.024 - 01:05:02.304, Speaker E: Okay, let's say thanks again. We come back at 330.
