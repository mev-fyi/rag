00:00:00.960 - 00:00:06.914, Speaker A: So last speaker this morning is Cameron Masco from Microsoft New England, who's talking about matrix approximation.
00:00:08.734 - 00:00:58.444, Speaker B: Yeah, so I'm going to be talking about basically some recent advances in approximating specifically positive semi definite matrices, and just some work we've been trying to do in understanding what sort of structure can we exploit in positive semi definite matrices to get low rank approximations much faster than we can get for just general matrices. So everybody here probably knows what a PST matrix is. It's basically just a symmetric matrix a that is all non negative eigenvalues. Another way to look at this is it's just a matrix that if I take the dot product of a vector x over it, this is always non negative. So x transpose ax is always non negative. And these sorts of matrices show up all the time in applications. They include things like graph laplacians, gram matrices, kernel matrices, which are sort of nonlinear gram matrices, covariance matrices, hessians of convex functions.
00:00:58.444 - 00:01:47.166, Speaker B: They just appear in all sorts of applications. So understanding basically how to operate on PSD matrices is sort of a fundamental thing in linear algebra. Another thing about PSD matrices is that often we get really big ones and we get ones that are really big and really dense. So often we generate really large PSD matrices from much smaller data sets. So an example of this is in kernel methods, things like kernel Ridge regression, which is very closely related to sort of equivalent to gaussian process regression, where it's called kriging. In the geosciences, kernel distance measures like maximum mean discrepancy, which are sort of ways of testing distances between distribution using kernels. In all of these methods, what I do is I start with some data, some set of data points with D features.
00:01:47.166 - 00:02:29.584, Speaker B: I have n data points with D features, and I want to sort of work with these data points in a non linear space. So what I do is I sort of map this original data set to a kernel matrix. What this is, is basically the IJTh entry of this kernel matrix is equal to the dot product between data point I and data point j. But this dot product is on some under some nonlinear inner product. So like the gaussian kernel or some people use sync kernels. So basically every entry of k is determined by these nonlinear distances between the rows in x. Once I have done this transformation, a lot of these problems, like kernel Ridge regression, gaussian process regression, reduce to basic linear algebraic computations on k.
00:02:29.584 - 00:03:02.996, Speaker B: The problem is that k can be really huge. So if I even want to write down k or for example, perform a single iteration of a linear system solve. And if I just want to multiply by a vector, I'm suddenly paying roughly n squared time where n was my original number of data points. So if I have large data sets, if I have even like a mildly sized data set, say I have 100,000 data points, suddenly k is 10 billion entries. Suddenly it's a matrix that takes a ton of space to store. It's a big linear algebraic problem, and typically these things are going to be dense. So typically k, this kernel function, is going to map to non zero numbers.
00:03:02.996 - 00:03:25.014, Speaker B: So typically it's going to be a really large density. So I need to basically do something about this if I want to sort of apply these kernel methods to reasonably sized data sets. So there's sort of two options. One is I could just apply parallelization mass of computation. I could sort of form k in parallel, I can multiply it by it in parallel. I could do everything in a distributed way. So this is one approach we could take.
00:03:25.014 - 00:04:31.142, Speaker B: The approach that I'm going to be looking at today is basically how can we develop effective approximations for these large bs matrices. So instead of working with k, I can work with sort of a much more compressed representation of it. And specifically I'm going to be talking about a specific type of compressed representation that a lot of people here are going to be talking about, which is low rank approximation. Basically like as Ming was showing, you often see spectral k in matrices, and you can exploit this by basically approximating a matrix by a low rank matrix and showing that it's close to it. So what I want to look at is if I have a PSD matrix a, how can I really efficiently compute a low rank approximation and then transpose that's as close to as good as I could get for a, and do this without paying this whole n squared runtime cost. So traditional low rank approximation methods, things like say, computing a full SVD or doing Krilov subspace methods, rankling QR random projection, all of these methods at least require, usually more, but at least require multiplying one vector by your matrix. So in the case of a dense n by n matrix, they at least require n squared time.
00:04:31.142 - 00:05:05.796, Speaker B: And when n is large, we want to totally avoid this. So there's a lot of methods that have been proposed that require less time, that basically require little o of n squared. So this is like think of as being sub linear time. You're going to try to get an approximation without reading your full matrix k, things like incomplete Chileski entry y, sampling nice drum approximation. This random Fourier features method, which is used a lot in machine learning, can be viewed as basically a randomized low rank approximation method. So lots of these methods exist. And I'll just point out that this has been a really success area actually of like randomized linear algebraic methods.
00:05:05.796 - 00:05:27.766, Speaker B: So like nice drum approximation, random Fourier features are essentially randomized low rank approximations. And they're sort of what's used in practice to approximate these large kernel matrices. They're sort of what we have these different bound, these different methods. They have various bounds. Lots of different bounds exists. Some are stronger, some are weaker. Some use assumptions, some don't use assumptions.
00:05:27.766 - 00:05:58.454, Speaker B: There's nothing as strong as what we typically see for like our fast algorithms for general matrices. So generally sort of like in this community, one thing that we often try to get is low rank approximation of the form a. The distance between a and n transpose, say in the Frabinius norm, is within one plus epsilon of the best low rank approximation. This is between a and Ak. Ak is the projection of Aon to its top k eigenvectors. So I want to get these sorts of bounds for PSD matrices, but I want to do it without paying n squared runtime. I want to basically do it in little o n squared runtime.
00:05:58.454 - 00:06:38.454, Speaker B: So that's the goal. We want strong worst case approximation bounds, no assumptions on the matrix, but sublinear time methods to get these sorts of results. Another way of looking at this is in many contexts, as I explained, what we're doing is we're taking a moderately sized data set, we're blowing it up to this big PSD matrix, this kernel matrix, and then what we want is some sort of compressed representation. And this is inherently an expensive pipeline. What I want to do is sort of cut out this middle step. I want to say I take an initial input matrix. I skip like forming this full PSD matrix, this kernel matrix, or wherever it's coming from, and I go right to a compressed low rank approximation that's close to this matrix.
00:06:38.454 - 00:07:22.244, Speaker B: So first of all, let me talk about why we can't hope to do this without this positive semi definite structure. So I'm telling you I want to get a good alluring approximation to a, and I don't want to read all of its entries. I want to run a little o of n squared time. Even when a is dense. This is clearly an impossible goal if a is a general matrix. So for any general matrix, if I want any sort of approximation bound to it, I better read the whole thing. So specifically, if I just place a random, really large entry in the matrix, say the matrix is all really small entries, and I just take one diagonal matrix and make it really large, basically any good approximation to a is going to have to know that that entry is large.
00:07:22.244 - 00:08:14.536, Speaker B: And so if I even want to identify that entry with constant probability, it's in a random location, I can't really do anything but randomly guess where it is. I basically need to read a constant fraction of the nonzero entries in a in order to find this large entry. So we can basically see that if I have a general matrix a, if I want any non trivial approximation bound, say a minus, the lowering approximation is less than anything, less than one times a forbeenius norm. So anything that I can't just return zero, I better read basically the whole matrix. So the first observation is that this lower bound does not hold for PST matrices. So for PSD matrices, we actually have that for any off diagonal entry, it's bounded by the, basically the maximum of its two on diagonal entries. So that means that if I try to hide the really big off diagonal entry in the matrix, I'm going to go an algorithm can actually find this.
00:08:14.536 - 00:09:05.688, Speaker B: It can read the diagonal of the matrix, it can look at all the really large on diagonal entries, and then it can check those columns to make sure there's not a big off diagonal hiding in them. So basically, if we have a PSD matrix, we can find these heavy off diagonals. We don't have this lower bound, we obviously don't have an algorithm. So the question now becomes, what do we actually do? How do we actually get an algorithm using this sort of intuition? Basically, how can we use additional structure arising from positive semi definiteness to actually get like some non trivial approximation? Day and the additional structure we're going to use is just a really simple fact, which is just that any positive semi definite matrix has a square root. So basically a can be written as b transpose b. For some b, use whatever your favorite decomposition is. I'm just going to think of say we have the eigen decomposition of a.
00:09:05.688 - 00:09:34.940, Speaker B: B is going to be basically square root of the eigenvalues times v transpose. And note that b is not something I'm computing algorithmically. B is just sort of a tool I'm going to use in my analysis in my proofs. But every p SD matrix has a square root. What does that mean? Well, if I look at the columns of b, then the entries of a are just the dot products between these columns. Basically ij at the entry of b is the dot product between bi and bj. And this means that basically a is a grand matrix.
00:09:34.940 - 00:10:10.302, Speaker B: It contains the dot products between a bunch of n dimensional points. This puts a lot of geometric constraints on its entries, and it's sort of these geometric constraints that we're gonna be able to use to get faster algorithms for approximation. So this heavy diagonal observation I mentioned a couple of slides ago, basically the thing that ruled out my lower bound is just a simple corollary of Cauchy Schwarz. So basically a is the dot product between bi and bj. So by Cauchy Schwarz, it's bounded by the root of their norms. Their norms multiplied together, these norms are just the diagonal entries of a. So basically you get a is less than root of aii times aj.
00:10:10.438 - 00:10:17.684, Speaker A: So consequence of the fact that, that the determinants, the determinants of the principal submatrices are non negative.
00:10:17.984 - 00:10:54.804, Speaker B: Yep. Yes. Yeah, I guess probably, yeah. You can also see it that way. Yeah, so yeah, you can see this, you can see this sort of fact in a number of different ways, but it arises out of this geometric structure, sort of another view that we're going to take on it and that's going to help us develop algorithms, is that we're going to think of a as basically containing a lot of information about the column span of b in some very compressed representation. Basically, we already have all the dot products between the columns of b precomputed for us, and we can just access them by like reading an entry of a at order one cost, when typically, in order to compute these.com dot products, if we were given b itself, we'd have to pay like order n cost.
00:10:54.804 - 00:11:38.316, Speaker B: Okay, so the good news is that if I wanted a low rank approximation of this factor matrix b, we can show how to do this using less than n squared column dot products. And this corresponds to using less than n squared axis to a, and therefore a sub linear time algorithm. If I wanted to compute a Lauric approximation of b, I'll explain how to do this in a second. It's not very hard, and we can do it using a lot of methods developed in this community for basically like cur type algorithms. But what does this bias? Well, B has the sort of same, right, singular vectors as a. Its singular values are closely related, they're just the root of the values of a. So basically its top case singular vectors are related.
00:11:38.316 - 00:11:43.380, Speaker B: And finding a low rank approximation of b is really closely related to finding a low rank approximation of a.
00:11:43.412 - 00:11:46.184, Speaker A: So you're not assuming that b is a symmetric square root.
00:11:47.284 - 00:12:30.996, Speaker B: No, I'm not, but if you want, you can make it a symmetric square root two. So, for example, if I could get an optimal low rank approximation for b, I could immediately use this to get an optimal low rank approximately approximation for a. Of course, in all of these algorithms that we have, there's some error and things are going to get complicated when we introduce error. But this is sort of the intuition. We're going to focus on getting a low rank approximation for this matrix b, which we don't actually have in hand. It's just a proof tool, and then we're going to show that this is, we can actually boost this to a good low rank approximation for a itself. So how do you do, how do you answer this first question? So how do you actually find a low rank approximation of b using less than n squared? This can be done in a number of ways.
00:12:30.996 - 00:13:08.178, Speaker B: It can be done with leverage score sampling, these volume sampling methods, a variety of like cur type methods. One way to do it that I'll just use today for exposition, because it's really simple, is something called adaptive sampling. So this was a theorem showed by Deshpande Pala. Basically there exists a subset of just k squared over epsilon columns of any matrix that spans basically a good low rank. Approximately. It spans some z, such that b minus zz transpose b is within one plus epsilon of the best low rank approximation. Once I found these columns, I just, in order to compute this low rank approximation, I just need to know the dot products with those columns.
00:13:08.178 - 00:13:54.280, Speaker B: So I just need to know roughly n times k squared over epsilon column dot products. And further, I can find this low rank span, this span of a few columns really efficiently. Basically, I'm going to use a very simple approach. I'm trying to find columns that span my matrix. Basically what I'm going to do is I'm going to start with no columns, and I'm going to do, in every step, I'm going to basically look at, for all my remaining columns, project off what I've already covered, project off the columns that I've already selected, and then just select by the remaining norm, select by the norm of basically column bi minus its projection off the subspace that I've already selected. So I basically keep trying to collect columns that I haven't sampled yet that are not covered by my subset. Yeah.
00:13:54.280 - 00:14:26.836, Speaker B: So I sample these probabilities. This can be implemented with very few axes to a. So how do I actually implement this algorithm? Well, in the first step, p's is zero. I haven't added anything to my subset, so all I'm doing is selecting columns by their norm. So what I do is if I want to select the column of b by its norm, the norms of the columns of b are just written down on the diagonals as I read the diagonals of a, and I sample one by its size. So in the first step, what I can do is I read the diagonal of a. I get a column, that's the first column in my column subset, I'll reorder it, I'll just apply a permutation and put it at the beginning because it doesn't matter.
00:14:26.836 - 00:14:54.936, Speaker B: So, note that the first step of the algorithm reads the diagonal of a. This is sort of critical. Breaking my lower bound required reading the entire diagonal of a. So you sort of knew that was going to have to come into my algorithm at some point. Okay, in the next step, I need to compute this probability again. Well, what is this probability? I need to basically, for every column I need to know one its norm and I need to know its component in the direction of b one. So I just need to know all the dot products between that, the columns and b one.
00:14:54.936 - 00:15:37.996, Speaker B: These are written down in the first column of the matrix a. So I sample a second column, I reorder it again, and then I keep iterating like this. To compute the projection off of b one and b two, I basically need to know all the dot products with those columns. That requires reading the first two columns in my matrix a so I can perform this whole algorithm. In the end, I read the diagonal of a, which is n entries, and then I read k squared over epsilon columns. So I haven't read the full matrix. And so what comes out of this is that basically there's an algorithm, it uses roughly nk squared axes to a this PSD matrix, and it computes some low rank span that satisfies with good probability.
00:15:37.996 - 00:16:21.724, Speaker B: Basically, if I projected b onto it, this would give me a near optimal low rank approximation of b. And you can improve this by the way, like in the actual paper, we do this, we don't use this adaptive sampling technique. We use leverage score, specifically rigid leverage score based techniques. This basically gives you sort of the right dependence on all the parameters. So I have this algorithm for finding a low rank span for b. What is this good for? Well, first of all, it immediately gives me at least something for a. So how does this translate to a low rank approximation for a? If I just convert these norms from Fribinius norm to trace norm a has the squared, the trace norm is just the sum of singular values in a matrix instead of sum of squared singular values.
00:16:21.724 - 00:17:04.656, Speaker B: So I get this sort of relative error trace norm approximation approximation bound. So already we have something for getting a lowering approximation of a, which takes sublinear time and gives us some sort of non trivial bound. If you don't have intuition for this trace norm bound, this nicetrum algorithm that we give actually gives a sort of stronger bound that this trace norm bounds, a corollary of which I think is a sort of nice way of viewing it. Basically, we output a low rank approximation nn transpose that spectrally approximates a. It approximates it to some additive factor lambda I. So basically it gets all the singular values correct up to some additive factor lambda. And we essentially get the optimal trade off between the rank of n, the number of columns nn and this parameter lambda.
00:17:04.656 - 00:17:43.779, Speaker B: And this is an especially useful way of looking at these algorithms, because in like applications, what you often care about is computing dot products over the matrix a. So for kernel distance metrics, what I'm doing is exactly computing a single dot product over over this PSD matrix a. For kernel Ridge regression, these sorts of guarantees are exactly the right what you want in order to get approximate solutions. So this is another way to look at what the approximation we get is. And we've done this without assumptions on a, and we've done it without reading all the entries of a. I'll just mention that this nice term algorithm, if you haven't seen it before, is basically like things studied in this community. It's like a column subset selection method.
00:17:43.779 - 00:18:35.638, Speaker B: You basically select k landmark indices which correspond to rows and columns of your matrix, and you then basically project a onto these, onto this subset of rows and columns. Another way of looking at this algorithm is that if we think of a as a kernel matrix as the dot products, every entry is the dot products between two things. This basically corresponds to doing column subset selection in kernel space on your data points. So it's actually a very natural thing to do. But what about this? What about the Forbenius norm guarantee? Like I want to actually give you sort of a sub linear time algorithm that actually achieves this sort of guarantee. I get within one plus epsilon of the best for benius norm error of my matrix. How do I actually get this? The first thing to notice is that our nicer method, and also, for example, that adaptive sampling method that I described to you, is sort of limited in that it actually only accesses the diagonal of a and some subset of the columns of a.
00:18:35.638 - 00:19:15.714, Speaker B: And if we want to get sort of this stronger guarantee, we're actually going to have to do something a little bit more advanced than that. This is basically because I can think of a as being an identity matrix with this rank one block on it, rank one block of just all ones, and this is a root n by root n block. So the eigenvalues of a are basically all ones and then has an eigenvalue of root n. And basically a constant fraction of a's for venius norm is dominated by this block. So if I want to get a good Frobenius norm rank one approximation to a, I have to mostly find the entries in this block. If I take anything less than root n columns, there's no way that I'm actually going to find most of this block. So I'm not going to get a good approximation to a.
00:19:15.714 - 00:19:49.992, Speaker B: So this tells me that column sampling is not going to give me anything better than say n to the three two runtime. For getting a good forbeneous normal rank approximation, you can actually get this sort of runtime. It's still interesting, it's still sub linear. You don't, we don't need to read the whole matrix, but we want to get something that's ideally linear in n. And our solution is basically going to be that we sample both rows and columns, and what we're going to do is sample these columns and rows. I'm not going to go into the details at all on this. I'm just going to sort of give you a sketch of what the algorithm looks like using the ridge leverage scores for the factor matrix a of the one half, or this is what I was calling b earlier.
00:19:49.992 - 00:20:59.102, Speaker B: These we can compute efficiently. And what we can show is that if we sample rows and columns by these scores, I basically can get down to a sort of core of my matrix, a root nk by root nk submatrix that contains enough information to actually identify in your optimal low rank approximation for a. The technical tool we're going to use is something we've been using a lot in recent work, which we call a projection cost preserving sketch. This is basically I take a I sample some set of its columns, and if I sample them in the right way, for example by these ridge leverage scores, I get a matrix that preserves the distance to any rank k subspace. So I've written that here as basically for any rank k projection matrix p, I preserve the distance from a, I look at the distance from as to pas, and this is within a close approximation factor of the distance from a to pa. What this means is, for example, if I compute an optimal low rank approximation for as, I get a near optimal one for a, and so I can use it as basically a state stand in for a and lorentic approximation. What we show is that we get a projection loss preserving sketch first by sampling rows and then by sampling columns.
00:20:59.102 - 00:21:35.982, Speaker B: So what our algorithm looks like is I start with this PSD matrix a. I sample its columns by these efficiently computable ridge leverage scores of the factor matrix. This gives me a projection cost preserving sketch of a. I then sample the rows by the same leverage scores, although it's a different set of rows. So note that this doubly sampled matrix, it's not a principal sub matrix, it's not necessarily PSD, but it is a projection cost preserving sketch of as one. And then I can sort of recover a good lowerc approximation to a first use, say an input sparsity time method to get a good low rank approximation of this submatrix. Note that the submatrix has at most n times k entries.
00:21:35.982 - 00:22:22.880, Speaker B: So if I apply an input sparsity time method, my runtime is roughly n times k or n times k with some poly epsilon factors. And then using sort of standard technology, I can use this low rank approximation along with that projection cost. Really sketch guarantees to find a low rank span that well reconstructs as one, and then in turn find a low rank span that well reconstructs a. So the upshot of the whole algorithm is that we get an algorithm that given a PSD matrix, a axis is n k over some factor epsilon, epsilon to the 2.5 entries of the matrix. The runtime is something similar. The runtime is roughly nk squared over poly epsilon, and it outputs a low rank factorization, n times m transpose that gets within a one plus epsilon of sort of the best.
00:22:22.880 - 00:23:03.468, Speaker B: And this is without any assumptions on the matrix a. Just using the fact that it's PSD. And if you compare this to, for example, like sort of state of the art runtime for, for regular matrices like Clarkson Woodruff, 2013, your runtime looks like something NNZA the time to read the matrix plus n poly k over epsilon time. Basically, our runtime looks just like the second term of this. We're basically totally wiping out this whole need to read the matrix. Okay, so yeah, so in this bound, there is no gap notion. Right? So how did you get this gap here?
00:23:03.516 - 00:23:04.980, Speaker A: You don't need a gap for lower approximation.
00:23:05.012 - 00:23:38.724, Speaker B: Yeah, you don't. In general, for long approximation you don't need a gap, because I don't care about actually approximating the singular values or the vectors. I really just care about finding if there's not a gap, then basically the two singular vectors that are close to each other, singular value wise, are just as good for getting a low rank approximation. So this is sort of how you get rid of it. One thing to note is that if my singular values don't decay right, then in general this norm is just not going to be that small. If my singular values don't decay, this is a pretty large error. So in this sense it does worse than if you say, for example, use like a Krilov subspace algorithm.
00:23:38.724 - 00:24:40.352, Speaker B: Okay, any other questions? Result. And otherwise I'm going to talk about sort of lower. Can I quickly talk about future work? Okay, so I'll quickly talk about some future work. There's a lot of open questions. This is sort of a first pass at what can we do if we start trying to exploit this PSD structure? Obviously, PSD matrix are super well studied, but to us, it was surprising that we could get this sort of result using pretty, I would say, relatively simple techniques. So one question is what can we, what else can we do for PSD matrices? We give some applications in our paper to approximate ridge regression. Are there other linear algebraic problems that require a second look that we could do a lot better when we, we have PST structure? Another question is, are there other natural classes of matrices that emit sublinear time low rank approximation? So I saw, like in the upcoming nips, that Bakshi and Woodruff have something that does sub linear time lowering approximation of distance matrices, which as you can imagine, are really closely related to gram matrices, really closely related to PSD matrices.
00:24:40.352 - 00:25:37.548, Speaker B: So there's a lot of other matrices we can maybe be looking at and trying to see can we get these types of sub linear runtimes? And then the biggest open question that I'm thinking about, and that I think is sort of an interesting problem, is can we do even better for PST matrices that have additional structure, like kernel matrices where we know exactly how this matrix was generated? So say I have an n by n, p s d matrix a. It's the kernel matrix corresponding to some input data x, and some non linear kernel function that I have r methods basically access. If I ignore all the epsilon factors, they access n times k entries of a. In order to compute these entries, this requires n and z x times k runtime. I need to compute the kernel distance between roughly k points. I mean, I need to compute basically the kernel distance between n k pairs of points. And computing this kernel distance requires reading a row to two rows of the matrix x.
00:25:37.548 - 00:26:15.934, Speaker B: So the runtime is something like n and zx times k. David and I showed that this is sort of the best you can do. So for a huge variety of kernel functions, including the gaussian kernel, just the standard linear kernel, polynomial kernels. If I want to get any better approximation for any approximation of this form where I have some delta relative error to the best reg k approximation, then unless I can significantly speed up fast rectangle rectangular matrix multiplication, I'm not going to do better than this bound. So we have sort of a computational lower bound via reduction to matrix multiplication. You can try to do it, but you shouldn't sell it as a low rank approximation paper. You would sell it as speeding up matrix multiplication.
00:26:15.934 - 00:27:02.602, Speaker B: But getting Nnz x time is totally open when we look at this sort of trace norm approximation result, or how I like to think about it, is computing a spectral approximation to this PSD matrix a. In this case, we don't know if we're really basic kernels like the gaussian kernel, the linear, well, not the linear kernel, but the gaussian kernel, the polynomial kernel, whether or not we can get this n and z x time. And I think that developing tools to try to get this should lead us to some interesting results in basically understanding how to do oblivious embedding for kernel matrices. So let me just say really quickly what that looks like. Kernel matrices. One way you can think about it is I have these data points x one through xn. I apply some mapping that maps them to really high dimensional space.
00:27:02.602 - 00:27:25.166, Speaker B: So it maps each data point xn to basically phi xn, which is some high dimensional vector. And oftentimes, by high dimensional, I mean it's an infinite dimensional vector. My kernel matrix a is just the product. It's the grand matrix of these points. In this high dimensional space. It's equal to PI PI transpose. So one thing I could imagine doing to approximate this kernel matrix is I could apply random projection.
00:27:25.166 - 00:28:14.774, Speaker B: This is what we do all the time here. I could just put this random matrix of, say, Gaussians in here. This would compress this matrix PI, and I'd get a low rank approximation to a. Now, of course, this is impossible to do computationally. If my kernel has, for example, infinite dimensions, then I can't just multiply it by an infinite dimensional gaussian matrix. But it's something that hopefully we can get some sort of understanding of what the actual distribution of the output would be and maybe just directly compute PI g without actually doing this kernel expansion. So the hope would be replace this sort of algorithm that I'm just using as a sort of template with basically multiplying PI by g, approximating that by multiplying our original input x before expansion by a gaussian, and then applying some nonlinear function.
00:28:14.774 - 00:28:55.492, Speaker B: And this is actually exactly what things like the random forward features method, which is common in machine learning, do. They exactly do this type of thing. It's basically a nonlinear random projection. And getting better bounds on approximating these kernel matrices basically boils down to getting better bounds in these nonlinear random projections. So, in NIPS 2017 paper, David and I have some initial progress on this. We combine sort of techniques for Faust gaussian approximate gaussian matrix multiplication with these leverage score based random Fourier features results which I won't talk about, but further progress is definitely possible. Chris's talk will sort of relate to this, and I think there's a lot of interesting directions here.
00:28:55.492 - 00:28:57.100, Speaker B: So, yeah, that's it, actually.
00:28:57.212 - 00:29:05.180, Speaker A: So, in the interest of lunchtime, can we just defer questions? And let's thank Cameron, Cameron, Cameron for.
00:29:05.212 - 00:29:06.284, Speaker B: Great talk and great words.
