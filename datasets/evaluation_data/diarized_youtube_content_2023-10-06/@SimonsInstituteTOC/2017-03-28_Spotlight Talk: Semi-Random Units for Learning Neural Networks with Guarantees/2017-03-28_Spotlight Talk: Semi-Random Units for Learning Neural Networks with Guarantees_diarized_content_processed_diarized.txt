00:00:00.200 - 00:00:49.610, Speaker A: And today I'm going to talk about semi rendered units for learning neural networks with guarantees. And this is joint work with EU, Kenji and Lee. So as we know, neural networks are very successful in learning many nonlinear functions and has wide applications in for example, computer vision, natural language processing. And most of the neural networks are trained with simple stochastic gradient descent. And however, the objective function is highly non linear, highly non convex, with possibly numerous local optimal and setup points. So the intriguing problem we ask is why in practice SGD works so well to train neural networks. So in this talk I'm going to give some first attempt to solve this question.
00:00:49.610 - 00:01:35.100, Speaker A: So we look at one particular model, which is one hidden layer neural net with relu activation functions. And the first layer ways are called wks and the second layer ways are the vks. And this function is parameterized as follows. Fx and the sigma is the relu activation function. We optimize this list squares loss over a bunch of m data points. So the main without is that for some nice configurations of the neural weights, the w's, and with high probability we can guarantee that any stationary point will be a global optimal. So we start by analyzing the structure of the gradient.
00:01:35.100 - 00:02:48.044, Speaker A: So the gradient with respect to the first layer weight wk is as follows. And we can so this is for one particular wk. If you stack all the gradient together into a long vector and we can rewrite them in a matrix form, then we have this special matrix where each column corresponds to one data point and there are n times d rows where m is hidden units and d is the input dimension and this one is the residual vector where each entry is the difference between the function prediction and the the target. So for simpler notation, this is the concatenated long gradient vector is equal to the special matrix we call d times the residual vector. So in the stationary point the gradient will be zero. And somehow if the d matrix is nonsingular, then immediately we will have this r, this vector will be zero, which means it's a global optimal. So in this talk I'm going to give some conditions under which this d will be nonsingular.
00:02:48.044 - 00:04:01.784, Speaker A: And this is the intuition that this key inequality, where this norm of the r is upper bounded by the norm of the gradient and also related to the inverse of the minimum singular value of d. That is, if you have small gradient norm and we have a lower bound on the smallest single value d, then the training arrow is upper bounded and gradient descent will minimize this gradient norm. So we can directly try to analyze the singular value of d matrix, which is we can look at the GM matrix which is d transpose d, and it is note that it is the function of the weight. So it is kind of difficult to analyze directly. So that's why we introduce an intermediate variable g, where assume the weights are uniform random and then we take expectation of them. So this intermediate variable g is no longer a function of the weight of the actual weight. Then the spectrum that we care about can be decomposed in two parts.
00:04:01.784 - 00:04:08.284, Speaker A: Is the spectrum of g, which does not depend on the actual weights, and the difference between g and g n.
00:04:10.024 - 00:04:11.884, Speaker B: What's the distribution of the weights?
00:04:12.304 - 00:04:55.516, Speaker A: So it's the uniform of the sphere. So to bound the first term. So know that g is an m by m matrix and look at the Igs entry which consists of interpret between xi and xj and the expectation of these nonlinearities. So for relu activation function, this nonlinearity will be an indicator function and we can calculate this expectation analytically, which only depends on the angle between xi and xj. And note that this function also only depends on the inner product between xi and xj. So it's an inner product kernel. And we can suppose assume the data point also normalized on a unit sphere.
00:04:55.516 - 00:06:20.098, Speaker A: Then we'll write this spherical harmonic decomposition where we have this gamma gamma u is the spectrum of this kernel function. So basically the idea is that this gn depends on data set, and if the data set the number of data points increases, you will concentrate on the spectrum of the kernel function. So the bound for the first term is that the spectrum of the g is related to the spectrum of the kernel gamma m. And in practice the spectrum of the rel function is actually in between one over m, or one over one over square over m. Now let's bound the second term which is the difference between g and gn, and it's upper bounded by some function of this l two w, which is to call the weight discrepancy. So the weight discrepancy is defined as follows is the pairwise similarity between the weights wi and wj measured by this kernel, where this kernel is related to this rectified linear function, and also minus this expected quantity. So the reason the intuition behind this is that g is about expected quantity and gn is about the actual weights.
00:06:20.098 - 00:07:10.284, Speaker A: So the difference is upper bounded by the quantity which depends on expected weights and the actual weight. So to put the two terms together, we have a lower bound on the singular value of d. And for simplified result, suppose n, which is the number of hidden units, and d, which is the input dimension are large enough and the weight discrepancy, l two. W is small enough. In particular, if n is one over gamma m and d is one over gamma n and l is in this order, then we have a simplified bound. Then with high probability. This smallest single value is low, bounded by linear in number of data points.
00:07:11.624 - 00:07:17.524, Speaker C: You have some kind of generative model weights that generate your weight.
00:07:18.424 - 00:07:41.784, Speaker A: So here we don't talk about the true model yet. This particular w does not, is not related to the true generating model cell. So, so here does not concern anything about the true generative ways.
00:07:42.164 - 00:07:44.172, Speaker C: You say this hyperbaric, I'm trying to understand.
00:07:44.228 - 00:08:10.912, Speaker A: Yeah, the hyperpopitude is over the data points. So the hyperpop is relative to all the xi's data points. Xi's is w random? W now is a particular weight. Is there any particular weight? Is that random or not? It's not random. Is that the solution of your optimization problem? Doesn't the fact that you take the.
00:08:10.928 - 00:08:14.264, Speaker B: Radiant become zero, doesn't it couple the weights and the random points?
00:08:14.344 - 00:08:37.114, Speaker A: Yeah, that's right. That's a point I'll address later. So for now, I just assume any particular w weight. And then if this as a solution, if this w has a low discrepancy, then this holes in the solution. The key is that we want to have this small weight discrepancy.
00:08:38.574 - 00:08:42.878, Speaker D: So it's like a certificate of global optimality is that's what, that's where you're headed, right?
00:08:43.046 - 00:08:44.222, Speaker A: Yes, something like that. Yeah.
00:08:44.278 - 00:08:46.610, Speaker D: So you can compute it and it's a certificate.
00:08:46.742 - 00:09:26.760, Speaker A: Right. But, yeah. So let me talk about more about this. So, to get a final arrow, if using the same assumptions, we can have a bound on the training arrow, which is on the square root norm of the gradient. So that means if you have a small gradient, then we have a small error. So, to talk a bit more about this w business. So in most w, we actually satisfy this load discrepancy, but we are not really, there's one point that we are not sure yet if the solution obtained by gradient descent or any other optimization algorithm, we actually have a small weight discrepancy.
00:09:26.760 - 00:09:30.880, Speaker A: So that's still a gap. That's for future work.
00:09:31.032 - 00:09:35.844, Speaker B: But you still need to soup over all those w's in those class of small weight discrepancies, right?
00:09:36.384 - 00:09:37.320, Speaker A: Excuse me?
00:09:37.472 - 00:09:48.826, Speaker B: I mean here the w for which you proved it, you proved it for each individual set set of weights in this class. This will depend on your xl and yl, right?
00:09:48.930 - 00:09:57.614, Speaker A: Yeah. No, no. So w is so I define a set which has small weight discrepancy. So for any w that has small weight discrepancy.
00:10:06.354 - 00:10:11.130, Speaker C: The capacity, so don't see variance coming in to use none.
00:10:11.202 - 00:10:34.364, Speaker A: Yeah, yeah. But we have, when I see that most w, we satisfy small light discrepancy, but we are not sure if those particular points arise by gradient descent, will actually have small discrepancy. So there's still further research to download on that part. Yeah. And typically it is, yeah, so you have more weights than people. Examples. Right.
00:10:34.364 - 00:11:01.154, Speaker A: More waste than examples. Yeah. So I will talk about it. So the n is the number of hidden units and d is the input dimension. So the typically should have a requirement, should be in order, square root, n and m. So we're actually in this over parameterized region where n times d is the total number of parameters, is slightly bigger than the total number of data points. Okay, let's take it.
00:11:01.154 - 00:12:03.854, Speaker A: And yeah, to recap that, we analyze the one hidden layer, the optimization as of one hidden layer in neural network, and then show that with high probability, if the configuration of the weights are nice, in particular have a low weight discrepancy, then any stationary point with high probability would be a global optimal. But the technical difficulty is that we don't, currently, we don't have any guarantee for any, for the points arrived by the optimization algorithm, you actually have a small weight difference. Then next, we describe the semi random units to address this issue. So, as we discussed before, the difficulty comes from the nonlinearity part. But you can think about a rel function as two parts where it has nonlinear part and this linear part. So for semi random units, we replace the w in the nonlinearity part by random projection by r. That is.
00:12:06.354 - 00:12:06.642, Speaker B: The.
00:12:06.658 - 00:12:26.696, Speaker A: Nonlinearity only depend on random parameter, and we don't optimize that, but we only optimize the linear function, so that in this case, the D matrix will only consist of functions of r's but not d. So you will not change as the during the process progress of optimization.
00:12:26.840 - 00:12:32.552, Speaker D: So why not just raise random coin toss? It's just a random coin toss, right?
00:12:32.608 - 00:12:39.888, Speaker A: Yeah. It is just a gaussian. Yeah. Random coin with probability one, two, right?
00:12:39.976 - 00:12:40.644, Speaker D: Yeah.
00:12:41.624 - 00:12:56.564, Speaker A: So is r shared by different indicators? Yeah. So this r is for any unit? For any, so for any hidden unit I have a one r. So if there are n unit, I have nrs.
00:12:56.724 - 00:12:58.424, Speaker D: So it's like a random coin toss.
00:12:59.764 - 00:13:03.984, Speaker A: It's fixed. Yeah. Correlated. Because it depends on the input x.
00:13:04.524 - 00:13:06.504, Speaker E: It depends on the input x.
00:13:08.444 - 00:14:19.104, Speaker A: No, no, it's a fix r, you fix it at the beginning and you don't optimize it, you only optimize w initially, right? Right. So on this setting then this d matrix will be high probability satisfy the low weight discrepancies condition. And this is a figure of this semi random unit on the 2d case where this threshold, this direction of the threshold is determined by r and you can adjust this part freely. And here are some property of the semi random unit. It sits between the fully random features which is random projection and then nonlinearity which are used a lot in kernel methods, and between these fully adjustable units used in neural networks. And so we have compromise the trade off between these two kind of units. It's also linear in the number of parameters but nonlinear in the input and according to previous analysis guaranteed to converge to a global optimum with high probability and also has universal approximation ability.
00:14:19.104 - 00:15:03.974, Speaker A: Next I'm going to show some experiment result and we see these semi random features are not as flexible as value, but we can using slightly more unit to match the performance of Relu. So these are experiment result on two data sets and the blue dots are the relu and the red dots are the semi random features. And these green ones are the fully random features. As we can see is using only slightly more units, it can match the performance of red loop, but compare with random feature. Random feature requires many more units. And then another interesting property is the between width and depth. So given the same number of fixed number parameter, we can distribute it using more layers or using more units per layer.
00:15:03.974 - 00:16:16.648, Speaker A: And this experiment shows that if you use four layer architecture it actually achieves lower lower error compared with the two layer and also one layer architecture. So it shows that using for semi random features this deeper, they also have this tradeoff between width and depth and usually depth helps. And then some more experiments on real world data sets for image classification. So basically we run convolution on the Internet and we replace the matrix multiplication with the random units, with semi random units and fully rendered units. And we can again see with slightly more units we can match the performance of Relu. Whereas in render feature, even using many many more units, we still have a huge gap between the performance of Relu. And in conclusion we have analyzed a one hidden layer neural network and this small weight discrepancy condition any critical point with high probability with a global optimal.
00:16:16.648 - 00:16:50.404, Speaker A: And the result depends on the spectral decay of this kernel associated with the value activation function. And then we also proposed semi random features. And these features are guaranteed to converge to the global optimal resize probability, and then also show that using slightly more units we can match the performance of relu. But then for fully rendered features, it has many, many more units. Thanks. Any questions?
00:16:53.104 - 00:16:59.528, Speaker F: So relu is non smooth. So what happens in points when the guidance is not well defined?
00:16:59.656 - 00:17:29.264, Speaker A: Yeah, that's a good question. So this kind of points are very small subset compared with the whole point. So in particular, we only use the sub gradient to define the. Yeah, at the .0 we use the define each gradient to be zero, but this point are very small compared with all the points. And if you use stochastic gradient descent, then you mostly likely will not run into those points.
00:17:32.104 - 00:17:46.804, Speaker F: Depending on how exactly you define the gradient, there might be a critical point, like a local minimum, where however you define the gradient, it was non zero, but it's still a critical point.
00:17:47.424 - 00:18:01.864, Speaker A: But also, although our result is not actually about a particular point, it's about a neighborhood where the arrow is small. So for many points around the neighborhood we can control this error. So I don't think it would be very sensitive to that point.
00:18:02.324 - 00:18:18.224, Speaker E: To be more precise, you can just define the gradient value at that zero point or some other value, and then the whole argument still holds. Basically the conclusion.
00:18:24.964 - 00:18:26.044, Speaker D: Okay, thanks very much.
