00:00:01.320 - 00:01:06.240, Speaker A: Thanks for giving me an opportunity to talk a little more about this stuff. Let's just for a second have a show of hands. Who here has ever seen a proof of Shiro's lemma? Not everyone. Among those voting who raised their hands was the proof. If you remember the proof, is it the proof where you take a and b and replace them by a union b and a intersection b? Or was it a different proof? So who seems to recall seeing a proof where there's this step where you do some log sub modularity or something? Okay, so that was the proof of Schurz Lemma that I was first taught 20 something years ago. And then Giacomar came and showed me the correct proof. And really, I don't know if this was Shiro's original proof.
00:01:06.240 - 00:01:54.824, Speaker A: I couldn't find it online. I found, I couldn't find the paper online. But his original proof was by induction on the number of times each point is covered totally the wrong way to look at it. And if you know the right way, then it's easier to prove other things. So I wanted to show you a proof of the fractional version of Shearer's lemma, which uses the, which is just expanding on the, on the book proof of Shearer's lemma. But instead, I'll just do a special case, which is what we, what we needed at this point in the talk. So let me, this is going to be a special case of the fractional shearer, and we'll see how easy it is to prove it, if you know how.
00:01:54.824 - 00:02:46.696, Speaker A: So we had a ground set, and we had a family of sets, Ijkl. And I wanted to impose some consistency, so I took my ground set and I covered it. I partitioned it into seven parts, and I called these four parts script I. And these four parts I called script J. And these four parts I called script k. And these four parts are script l. And so now I'm only going to look, there's a sum here where you and you have sets I, j, k and l, and each one of them gets a weight.
00:02:46.696 - 00:03:27.516, Speaker A: The weight is the Fourier coefficient. And we can assume that all the Fourier coefficients are positive because on this side of the inequality, they're squared. And we can approximate them by rationals. And then we can clear common denominators so we can assume everything. All the weights here are integers. And so this is supposed to be on sets I, j, k, l, such that there's symmetric differences, the empty set, meaning four sets, such that if you look at their union, every point there is covered an even number of times. But I'm going to restrict only to sets which are consistent with this partition.
00:03:27.516 - 00:04:10.256, Speaker A: So if you tell me that I union j union k union l is this, then I already know what I is. I is this and j is this. And so I'm only looking at sets such that if you look at the Venn diagram of those sets, each part of the Venn diagram of those sets falls in a corresponding part of the big Venn diagram. So I'm going to restrict only to those sums. And then I have to show this strange inequality. If you restrict to those quadruples, take the square, it's less than the product of these four sums. Sum of f I squared, f j squared, f k squared, f l squared.
00:04:10.256 - 00:05:07.344, Speaker A: And here's how we're going to do it. Numbers, they're just positive integers. And what we're going to, we'll take the log of both sides and I'll show you the log of the left hand side is twice the entropy of something. So we're going to define that something and then twice the entropy of that something is going to be less than the sum of four other entropies, the entropy of something coming from here plus the entropy something coming and so on. And those four entropies will, they won't be necessarily equal to the log of the right hand side, but they'll bound it from below. And that will show us that the right hand side is greater than the left hand side. So what are our random variables? And let's just say for the sake of this talk that I, if you go over the elements of I, you have one and then you have two and then you have 17 and then you have other.
00:05:07.344 - 00:06:18.552, Speaker A: So script I is 1217 and some other things. So here's my random variable. The random variable I'm going to look at is of the form x, CI, CJ, ck, cl what are these? And what are these? Exactly. So first of all, I have x one up to xn. X is a binary vector of length n, and this is just the characteristic vector of some consistent I union j union k union l. So what values can this random variable take for every I, jk and l which are consistent? I look at their union and I take the characteristic vector of their union. So that's they're n coordinates.
00:06:18.552 - 00:07:28.816, Speaker A: They tell us for each element in the ground set whether it's in the union or not. And then CI is just going to be belongs to so I have this and then CI is just going to belong to the set one up to f hat of I, of which I, I of x. So once I know x, I know the I union, j, union, k union L. I also know what I is. Once I know what I is, I let CI be any integer between one and f hat of I, and the same with c of j, c of k, and c of L. Okay, so now it's clear how large is the support of this random variable. It can go over all quadruples, and for each quadruple I have, the freedom I have for Cicjck, cl is precisely this.
00:07:28.816 - 00:08:15.964, Speaker A: So that's the size, what's written here is the size of the support. So if I take this uniformly over the support, then what I promised you is true. The log of the left hand side is precisely twice the entropy of this random variable. Okay, now what about the, because you calculate squared, right? Yeah. I take the log, I get twice the log of this, twice the log of the size of the support. And the CI is determining the x's? No, the x's determine the CI's. Once I know the x x's, I know I union J union, K union, and L.
00:08:15.964 - 00:08:45.760, Speaker A: And I should have used colored marker for this. And once I know the union, I also know the projections. So once I know x, I know I, I know J. Okay. I know l. Okay, it's important that uniform on the product of those things, right? Yeah, yeah, I said here, I'm just telling you what values it can take. And then I'm saying I'm taking uniform on all values that it can take.
00:08:45.760 - 00:09:37.236, Speaker A: So then the entropy is just a log of the size of the support. And what you said is true, the size is, well, this is the size, what's inside the brackets here. Okay, what do I have on the right hand side? What random variables am I going to use here? I'm going to use the following random variables. So I'm writing x restricted to script I. So this is just, I take this vector x one up to xn, but I only look at its projection on script I. So I only look at the coordinates in script I. Okay, so I have, this is the definition of x script I.
00:09:37.236 - 00:10:40.870, Speaker A: I take this long vector and look at the window corresponding to the elements in script I. And then I have something I call CI superscript one and CI superscript two. And this is my random variable. And what are these two random variables? They're just this copies of this CI. So when I know the value of x restricted to I, then I know what I'm talking about. And then I let then I take two random variables that conditioned on, on, on the fact that I know I already they're conditional distribution is just uniform on one up to f I. Each one of them is uniform and one up to f I.
00:10:40.870 - 00:11:12.162, Speaker A: Or the pair is uniform and one up to f I squared. And now I have to say two things. First of all, and I do the same thing for Jk and L. So why is this true? Why is the entropy of these four things bigger than twice the entropy of this and y? So this is y one. No, this is y two and this is y one. So this is question one and question two. So question one.
00:11:12.162 - 00:11:53.274, Speaker A: I made it one because it's very easy, you see. Oh, I told you what values this random variable takes, but I didn't tell you its distribution. The distribution is as follows. I take x uniform from here and then I look at the projection on these I coordinates. So of course the size of the support is this. So it goes over all possible eyes. And for each one you still have this if hat of I squared choices to make.
00:11:53.274 - 00:12:33.944, Speaker A: But why is it not equal to, why is the log of this not? Why is the entropy of this not equal to the log? It's just bounded by it because it's not necessarily uniform. If it was uniform, then so the largest it can be is the log of this if it's uniform, but perhaps it's smaller. So that answers question number one. And now we reach the crux of the whole proof. Question number two. And what I haven't used here is the part of the hypothesis of Shear's lemma, or the fractional scherer's lemma, and that every point should be covered t times. And here t is equal to two.
00:12:33.944 - 00:13:49.494, Speaker A: So when we take, when we look at our ground set, every point in our ground set is covered by at least two of the sets. Script I, script j script k. Script l. And now I'm going to use that. How am I going to use that? If I write the entropy of the entropy of x CI, cj, ck, cl, it's equal to h of x one, plus h of x two given x one, plus dot, dot, dot, plus h of x 17 given x one up to x 16, plus dot, dot, dot all the way to h of xn given all x's with index smaller than n plus. Yeah, plus the entropy of CI given x, plus the entropy of cj given x plus the. Okay, so this is, and let's put a two here twice.
00:13:49.494 - 00:14:34.748, Speaker A: So everything has a two. Two two. Okay, that's twice the entropy of this random variable. Now what happens when I expand this random variable, when I take h of x restricted to script I comma CI one, CI two, that's equal to, well, let me use the chain rule for X script I. This is h of X one. Ah, okay, so that cancels one of the two copies of h of X one that I had plus h of x two given x one. Great, that cancels another copy here.
00:14:34.748 - 00:15:18.576, Speaker A: Plus now comes a subtle point, h of x 17 given x one and x two, because I'm only looking at the coordinates of x which are in script I. Here I have x of 17 conditioned on all the coordinates up to 16. Here I have it conditioned only on some of them. The more I condition on, the less entropy is lift. So h of x 17 given x one, and x two is greater than h of x 17 given x one up to x 16. So this cancels this easily. So it brings it down from two to one, and then I continue.
00:15:18.576 - 00:16:12.330, Speaker A: And then I have h of CI copy one given X restricted to I, plus the entropy of CI two restricted to a conditioned on X restricted to I. And I say okay, here I have the entropy of CI conditioned on X. But the only thing that in X which affects the entropy of CI is the coordinates in I. So here I'm conditioning on less, and here it's actually equal. So I have two of these copies and they cancel out these two. So it seems I'm off to a good start. But then I say, oh, I've only canceled out one copy of H of X one.
00:16:12.330 - 00:17:01.304, Speaker A: How am I going to cancel the other one out? I say ok, I was only doing a I, but one also belongs to l. So when I do the random variable that corresponds to l, then I'll get to, it'll dominate the second copy of h of x one, and so on and so forth. Every element that appears there, I'll get it treated twice, if not four times. So this answers this question. And that's, so that's, that's the proof of the fractional shear's lemma. That's how the proof goes. And this is just small generalization of the what is the correct proof of Shiro's lemma? Okay, so that's what I wanted to say about this.
00:17:01.304 - 00:17:53.098, Speaker A: Just, I'll remind you, the context in which we were talking about this was that when you take, I wanted the sum not only with respect to one partition, but I wanted to drop the condition, it's consistent with this partition. And then I said okay, if you take a random partition, then you're getting some. The ratio between what you see here and what you really want to evaluate is bounded, but it turns out the bound is exponential in the size of the set. So the sets are size m. You get some bound which is exponential in M. And if you remember what we were proving. So this is what we were proving.
00:17:53.098 - 00:18:24.114, Speaker A: We were comparing the four norm to the two norm. And if the sets in question, the support of the Fourier transform was of size m, you pay something which is exponential in M. Okay, so that, that was the first thing I wanted to talk about. And the second thing I wanted. Are there any questions about this? Yeah. So you had this inequality for a fixed partition. Right.
00:18:24.114 - 00:19:08.954, Speaker A: You wanted to turn that into an inequality without fixed partition. Yes. So I have to count how many times each if I have a, the more times it appears, right? No, all the sets are I, j, k and l are of size m. But then you give me four fixed sets ijk and l, I have to count how many partitions they participated in, and I forget the exact calculation. But the answer is, this is where it's not a constant. No, it's something that depends on M. It's something exponential in M.
00:19:08.954 - 00:19:57.434, Speaker A: That's what I'm saying. But what should appear here is so square root three to them, but this is somehow a wasteful procedure and gives you a non optimal constant. Okay, any other questions about this? Good. Okay, then. Second thing I wanted to talk about was the slides I rushed through yesterday. Okay, so this is, this is, these are the two. Okay, what's on the bottom is the true version of the bonhomie gross Beckner inequality that I want to prove.
00:19:57.434 - 00:20:38.804, Speaker A: You have two vectors x and y, which are epsilon correlated. So y is a noisy version of x. And you look at the expectation of f of x times g of y. F and g are non negative functions. And the claim is it's bounded by the one plus epsilon norm of f times the one plus epsilon norm of G. And maybe it's easier to understand it intuitively if you look at the Boolean case. So if f and g are just the indicator functions of some set.
00:20:38.804 - 00:21:31.914, Speaker A: So x is the indicator of f is the indicator of script X, and g is the indicator of script Y. Then, because there is some correlation between x and y, you expect that perhaps if you can choose x and y such that the probability of x falling in script x and y falling in script Y is slightly bigger than the measure of x times the measure of Y. So it's slightly bigger than what you'd get if they were independent. This tells you that it's not much bigger. So if you take a number less than one and you raise it to a power less than one, you're increasing that number. But it tells you if you increase this by this much, then it already dominates this. So it gives you some measure of how much you can make these two things correlate.
00:21:31.914 - 00:22:23.902, Speaker A: Okay, so let's see, more or less what we have to do to prove this and how going from this to this is just one small more, one more small step for men. Okay, so what we're going to do is something similar to here. We're going to have some inequality and have entropy of something on the left hand side, entropy of something on the right hand side. But unfortunately, this time, to show this, you have to solve some calculus problem and show that indeed this is greater than this. So it comes, it's more ugly. It's not that elegant. So let's see what, so here's what we want to prove.
00:22:23.902 - 00:23:03.264, Speaker A: So I just, I write this out, the expectation of this. So I go over every x, give it weight, two to the nice. Then for every x, I go over every y and I give it some weight depending on how many coordinates it agrees with x and how many it disagrees. So and then, you know, I take the log of both sides and I cancel terms, and I get something like this. So here's some expression. A of X and Y is how many coordinates they agree, b is how many coordinates they disagree. And you want to give a prize if they agree, and a fine if they disagree.
00:23:03.264 - 00:24:08.680, Speaker A: And this is what you get. And then once again, I want to approximate everything by rationals and then clear common denominators. I want to have integers, so I'll have support of some random variables. So after you, so this is what you get after you clear the, the common denominators where r and s depend on epsilon somehow s over r plus s is one minus epsilon over two, and r over r plus s is one plus epsilon over two. So you get something like this. And then, okay, now we can start defining our random variables, and it's sort of clear what random variable you have to choose for the left hand side. So I'm going to choose a random variable, x, y z, and I'll tell you what the support is.
00:24:08.680 - 00:24:48.166, Speaker A: And it's going to be, I'll take it uniform on the support, and then its entropy will be the log of the size of the support. So what I need is that x is in script x, y is in script Y. And now I'm going to define four sets. A zero, zero is a set of size. Ra is a set of size R. A ten is a set of size S, and a zero. One is a set of size, size, and z.
00:24:48.166 - 00:25:36.306, Speaker A: So x and y, everything here belongs to zero, one to the n and z. I always belongs to a of xi, yi. So if xi is one and yi is zero, then zi has to belong to a ten. So given the fact that xi is one and yi is zero, z is distributed on some set of size s and s is smaller than r. So whenever they agree, you get something, you get more entropy. And whenever they disagree, you get less entropy. And that defines what the support of this random variable is.
00:25:36.306 - 00:26:11.514, Speaker A: And this is exactly the size of the support. Okay, so the entropy of that random variable is the log of, is the log of this. Now, on this side, I'm going to have. I'm going to look at the same x and y. I'll take them from this distribution. So I'm taking x and y. Note that if this is uniform, if x, y, z is uniform, it forces x and y to be epsilon correlated.
00:26:11.514 - 00:26:53.386, Speaker A: It means that it's more likely to get xi equal to y I than xi different from yi, because r is greater than S. So I'm going to take the conditional distribution of x and y, and those are going to be, this is going to be greater than h of x, and this is going to be greater than h of Y. And then I'm going to use the chain rule. So let's see what it. Okay, so h of z is going to be equal to the left hand side. And on the right hand, this is supposed to be less or equal to the right hand side. And I'm going to use the chain rule.
00:26:53.386 - 00:27:39.764, Speaker A: I'm going to look at it bit by bit, and I'll hope that every bit that I expose here. Well, when I do the chain rule, I have to condition of what I, what I've seen so far. We're hoping that the conditional distribution is still such that the entropy of the next bit of zi is less or equal to. Well, we have n steps here. In every step, we get this factor plus something times the entropy of the next bit of xi and the next bit of yi. And so, using the chain rule. Okay, so using the chain rule, this is what we have to prove.
00:27:39.764 - 00:28:18.184, Speaker A: I mean, at this point, as I told you yesterday, I knew I had to be lucky because it doesn't have to be true. Bit by bit, it could be that on average, it's true. If it was true on average, I think this proof would be even worse. But luckily it's true regardless of the past. And then, okay, I want to get rid of this z. So I write h of z as h of xy plus h of z given xy, and that makes z go away. And then I get some expression in x and y.
00:28:18.184 - 00:29:05.424, Speaker A: And also, there's another thing, just believe me, that if I multiply r and s by any, it only depends on the ratio between r and s. If I multiply r and s by a constant, this expression that I want to show that is non negative stays the same. So here's the calculus problem. You get, you have a distribution, this is the joint distribution of xi and y I. And you have to prove this, that entropy of x plus the entropy of y times something minus the joint entropy. And then you get some additional factor that comes from the case when x is different than y. The case when x is equal to y was multiplied by log one, so it disappeared.
00:29:05.424 - 00:29:54.484, Speaker A: And, okay, now, this is a calculus problem. And, okay, so you do some. You have to dirty your hands a bit, but not too much, because everything is very nicely symmetric. So you use the Lagrange multipliers, you get some expressions, and then you start, you start looking. What's the best way to utilize them? And so, one last thing. I have two more minutes that I'll say is that it's sort of, it's slightly intimidating to solve this, unless you already knew what you're looking for. I know that in Beckner's inequality, it would only be equal when script x equals scripty equals everything.
00:29:54.484 - 00:30:52.676, Speaker A: In that case, Xi would be distributed half, half between zero and one Y. I would also be uniform on zero and one, but they'd be epsilon correlated, and then you'd get this distribution. So if you know what the minimum is, it's easier to try to show that these constraints imply that the minimum has to look like that. So, okay, you play around with it a bit. I'm going to skip those details. And last thing I want to say is, what about non boolean functions? So when you have non boolean functions, you have to add on another two coordinates to xyz instead of x y z. It's sort of similar to what we did here you have xyz a and b, where a is uniform on one up to f of x.
00:30:52.676 - 00:31:51.624, Speaker A: B is uniform on one up to g of y. And you have to see how that changes the entropy on the left hand side and on the right hand side. And luckily, because you have some kind of homogeneity, that this power is one over this, somehow it works out that what you add to the left hand side is exactly what you get on the right hand side. You use this nice fact. If you have the entropy of X and you had the expectation of log of any function of x that's less or equal to the sum of the log of that function on all of x. So that's a fact that's easy to prove. And using that fact, you see that the contributions on the left hand side and the right hand side by adding these two extra coordinates are the same.
00:31:51.624 - 00:31:59.484, Speaker A: So you go from Boolean to non Boolean, just in two lines, and that's it. Okay, that's all I have to say.
