00:00:00.600 - 00:00:23.074, Speaker A: Hope everyone is nice and awake. So our first speaker is Yasmin Bari from DeepMind. She's done a lot of cool work at the intersection of physics and learning theory and deep learning, trying to understand kind of, you know, what makes deep learning tick. And today she's going to be talking to us about neural scaling laws.
00:00:24.614 - 00:01:46.822, Speaker B: Can you all hear me? Great, thank you for the kind introduction. Also the invitation to speak at this workshop with really an amazing kind of list of speakers. So the topic I'd like to talk about today is some of our now older work trying to kind of understand scaling laws, sort of from first principles, but in a way that's also trying to kind of match with experiments. So that's something that I care a lot about. And so just to start, kind of jump directly into the material, there are a lot of basic scaling knobs that we can use in machine learning. So things like the amount of training data, the size of the model, the amount of compute that we have, or the time. And so the question we're going to be tackling at the beginning, kind of in the first parts of this talk, is to ask what is the average performance of a machine learning model as a function of some of these basic scaling variables? And certainly some aspects of this problem fit into classic learning theory, particularly when you focus on kind of how an ML model improves as you scale the amount of training data.
00:01:46.822 - 00:03:10.586, Speaker B: But maybe one of the new ingredients is we'd like to kind of understand a few of these variables altogether and in a way kind of bridge the gap with experiments. So why study this now? It's really motivated by empirical observations and large models, both in the kind of vision setting and in language. And so, you know, this paper now a few years from a few years ago by Kaplan McCanlish kind of point really kind of pointed this out pretty starkly at an empirical level. They looked at the scaling of test loss in large language models as a function of these basic variables, saw that they tend to follow kind of nice smooth trends as a function of these three basic variables with these exponents for the power laws that I've listed here. So that's the problem we're going to be talking about mostly for this talk, I won't get to talk about other very important knobs, which are also important to understand both in theory and practice, like data quality and the diversity of the data set. So more examples of some scaling laws. So this is now changing the metric.
00:03:10.586 - 00:04:38.184, Speaker B: It's looking more at accuracy. This is from the GPT-3 paper also really highlighted kind of this behavior and the way in which performance changes when you drastically change the model size for a problem, like in context learning. And here you can see kind of the aggregate behavior performance for GPT-3 across kind of a wide range of benchmarks as a function of the model size. So there's a lot of empirical work now on kind of trying to find the best way to scale, where best kind of depends on the setting in practice that you want to work in, but it often might mean the compute optimal way to scale. So you have access to a certain amount of compute, and you would like to figure out how to divide that compute up amongst the amount of data that you have and the model size and so on. And so there are often these in the literature from the empirics researchers have used these kind of functional forms that are often of the following form. If I just think about the two variables d and p, the amount of training data and the model size, there might be a constant shift plus some leading kind of power law terms, and there might be more complicated forms out there, fits to the data as well.
00:04:38.184 - 00:04:45.000, Speaker B: But we kind of would like to understand, you know, see if we can say some things about this.
00:04:45.112 - 00:04:48.264, Speaker A: Sorry, does the amount of compute also enter into the formula?
00:04:48.384 - 00:05:49.460, Speaker B: So it could oftentimes, well, the computers might be like, depending on the model that they train, it's related to the amount of data and the number of parameters. And so that you could plug that in as a constraint and then like solve and. Yeah, solve for the kind of optimal values. So it's true that compute is often one of the main variables that we're going to look at in practice. So what we're going to be looking at, I'll be talking about is mostly training data and model size, but it's easy to incorporate compute in the setting that I'm going to talk about. So let me now break this down into maybe some specific set of questions that I think are interesting. So can we say anything more about these empirical trends? So what are some types of questions we would like to ask? For example, what aspect of the data and the models determine the functional forms, if we can say anything about that at all.
00:05:49.460 - 00:07:50.274, Speaker B: For example, are the exponents for data and model scaling ever the same, or are they always different? So, you know, is, are some, is the problem more sample efficient compared to parameter efficient? Or is there some symmetry between these things? Are all scaling regimes, quote unquote, the same, or are some different? So something kind of approaching a taxonomy or a classification so by same, I mean, kind of the thing that drives the improvement in performance is the same sort of, you know, has the same underlying cause which you could see in the theory, or like more mechanistically, it's more obviously the same. Or are there kind of different things, different kind of more mechanistic reasons behind this? And I'll try to say that I think different scaling regimes do have different origins. And then is there, for example, any empirically observable universal behavior? Or is the whole problem just too dependent on microscopic, microscopic, in quotes, microscopic details, meaning that for your particular setup, the problem depends so much on the type of training data that you have in the model architecture and so on, that it doesn't make sense to try to tackle this problem. Theoretically, you would just go out and measure the exponents and that would be it. Or measure the functional forms. And usually you do theory when you believe that some, not every aspect of the problem is highly dependent on the microscopic details you would like to understand maybe if there are more general principles behind this. So, one caveat is that a lot of this talk, since this is a little bit of older work and uses a bit of theory, focuses on images as the domain, but some is going to be applicable to llms, and I'll try to remember to point that out as we go along.
00:07:50.274 - 00:09:16.282, Speaker B: Before I start, I just want to acknowledge my collaborators with whom this work is done, many of them at two of them at DeepMind, Ethan Dyer, Jehoon Lee and Jared Kaplan, Udkharsh Sharma were at Johns Hopkins. So all of us have backgrounds in theoretical physics. And so the kind of tools and ideas we use are kind of either limited or inspired by that. So the approach that you'll see, I kind of take in this talk, and I like more generally, is to start with simple theory in a place where you have kind of a nice handle on the problem, and then in places where you don't have theory, you can try to test out some things empirically, and so start simple and work your way up in complexity. And we did this by basically studying a simpler setting and then figuring out what we could generalize, even to other regimes. So I kind of like this back and forth between doing some theory, doing some experiments to kind of confirm that, and feedback in into theory. So, just a few slides, if you'll bear with me, of what the problem setup is, and then we won't have so much of the formality.
00:09:16.282 - 00:10:04.344, Speaker B: So the theory problem is the kind of classic one that some folks in the audience might be familiar with. We would like to learn a model f sub theta with learnable parameters theta for the data distribution p. It's classic supervised learning. In this setting we're thinking about images, for example, and we have a training data set d we have some empirical loss. It could be, I've written down MSc, but it could also be cross entropy, and then we minimize it to obtain parameters theta, hat and the learned model. We would like to evaluate the population loss. So evaluate this error on the true distribution that we don't actually know.
00:10:04.344 - 00:11:02.384, Speaker B: And there's a further dependence on other aspects of the problem, like the initial condition theta, not for the parameters that you might use in when you initialize for optimization, for example, as well as the draw of the training data set. So we'll also average over those quantities, unlike so some work, oftentimes we'll just focus on a fixed draw of the training set and maybe get kind of worst case bounds. We kind of would like to consider fully averaged quantities. So we get this smooth thing, that is the loss as a function of the amount of data and the number of model parameters. So now to take a look at talk about different architectures. You can keep in the back of your mind, for example, a vanilla fully connected network, but it doesn't have to be fully connected. We can also discuss other architectures.
00:11:02.384 - 00:12:23.964, Speaker B: And it has a depth l and in terms of the most basic kind of features of the model, there's a depth to it and there's a hidden layer width, so l and n that I'll use and a fixed kind of fixed size input. And again, theta refers to the set of learnable parameters and there are p total parameters. So I'm going to be relying on some results, kind of from deep learning theory that are applicable to different model types. So not just fully connected layers, but could be convolutional, residual and even attention layers. I'll come back to some caveats with models that have attention in their architecture in a bit, but in terms of the kind of the most cartoon basic way of thinking about the model, you know, the number of parameters p could be increasing because the depth is increasing or the width is increasing, or there's some joint scaling. And so those we would like to kind of incorporate, ideally all of those into this, into this way of scaling, into understanding scaling. So to just reiterate, we're going to consider this fully average quantity.
00:12:23.964 - 00:13:35.762, Speaker B: The number of parameters, as I mentioned, can grow in two different ways or perhaps multiple ways with this joint scaling. And because of some of our limitations on the theory side and in terms of what's understood really well in deep learning theory, we're actually going to study the loss as a function of the data set size and the width. So the over, you know, the over under parameterization is going to come from changing just the size of the hidden layers in the network, but keeping the depth fixed. And what we'll be able to extract are power law terms that or the form of the power laws in that look like this. So we might just focus on scaling with the amount of training data and look at the exponent for that scaling, or look at the scaling with the width or the model size and extract that exponent. So I'm mostly going to focus on these exponents, alpha D and alpha W. Sometimes I'll use alpha W interchangeably with alpha P because it's labeling width or parameters.
00:13:35.762 - 00:13:44.804, Speaker B: But these are the two exponents for which we're going to be able to say some things about different regimes and values that they might take.
00:13:45.104 - 00:13:49.896, Speaker A: Are you also testing the assumption that there are no interactions between DNN beyond this?
00:13:50.000 - 00:14:16.920, Speaker B: So unfortunately that is, we can't access from the theory side a joint scaling. So that is a limitation. Yeah, the ideal treatment would kind of focus on both DNN scaling together. But in the background, I'll say this in terms of what's known on the deep learning theory side, those limits can be hard. Not a lot is known about them.
00:14:16.992 - 00:14:37.594, Speaker A: Can I also ask on only focusing, can I also ask on only focusing on width? I mean, so from a theory side, width is easy because we know you get universal function approximators with big width. But it sort of seems like for a deep learning and practice side, only talk about width and not depth is kind of uninteresting.
00:14:38.334 - 00:15:31.918, Speaker B: So that's a good question. I think that. So I think it's maybe a bit sharper than talk, than universal approximation because we're going to, in the background you're going to be relying on results about what is learned. And so what is learned in these large width limits, a lot more is known about that and it's more specific than just, you know, we know things about the functional form that's learned. There are some relationships to kernels and other things that I'll talk about. So I think it's more, it certainly does capture some aspects of depth that I'll talk about, but I think it's more specific than just universal approximation in terms of the kind of constraints. And also we'll, I guess the question.
00:15:31.966 - 00:15:34.354, Speaker A: Is still, is it reasonable to ignore depth?
00:15:35.174 - 00:15:41.566, Speaker B: It's not gonna. From the theory side, we. I think there will be depth shows.
00:15:41.590 - 00:15:43.154, Speaker A: Up in the analysis. Right.
00:15:44.574 - 00:15:47.634, Speaker B: So depth will be in there, but it just won't be scaled.
00:15:47.934 - 00:15:51.454, Speaker A: Yeah, I don't think depth is being ignored. I think it's just not being taken to infinity.
00:15:51.534 - 00:16:41.512, Speaker B: That is, it is important that, I mean, there are other interesting limits where depth and width would be scaled together, for example, and maybe training data would either be much less or scaled with that. And those are just challenging to access theoretically. So then we do some experiments at the end to try to understand a bit of that effect, but we just don't have a theoretical handle. Okay. So just to highlight what the kind of results are going to be about, they're going to be either looking at descaling or with scaling, not both together. So this is the sort of kind of empirical result that I'd like to work up to. So let me explain what's being plotted in these different plots.
00:16:41.512 - 00:17:58.768, Speaker B: So this is kind of the taxonomy that I referred to earlier. So if I. And these two by two kinds of, kind of blocks resemble, are supposed to be kind of experiments that correspond to this two by two grid over here. So the taxonomy is that if you are looking at the scaling of one variable, let's say amount of training data, or it could be the model size, there's a regime where there are basically two regimes we have access to on the theory side, or in our setup, either the amount of training data is the smaller of the two variables, it's sort of the bottleneck, or the amount of training data is the larger of the two variables. So as you scale d, you move from this regime, d much, much smaller than n, to this other regime. And there's some crossover in between which we can't access on the theory side. But we gave these different regimes names, which I'll kind of justify in a bit, depending on basically which variable is the bottleneck and which variable is not the bottleneck.
00:17:58.768 - 00:18:37.704, Speaker B: So when you're studying the scaling of a variable, that is the bottleneck, we term that a resolution limited regime, because there's a connection to some data manifolds and properties of the kind of some spectral properties I'll talk about in a bit. And when you're scaling kind of the larger variable, you're controlled just by a different limit. And so you're actually not gonna. The exponents that you observe are actually just kind of integer quantities. They're not as affected by the microscopics of the problem.
00:18:42.594 - 00:18:55.418, Speaker A: I'm assuming that when you talk about empirical results, it's got some practically interesting learning tasks. What's the theoretical counterpart? What learning task are you looking at?
00:18:55.586 - 00:19:41.074, Speaker B: So these experiments are all image classification, they're like the benchmark tasks. And actually they're all kind of part of this was to kind of aggregate different tasks and show that some of them have a universal exponent in some regimes, some don't for the learning problem. So part of what we do is we don't pause it. So we generalize it in part. We study a student teacher setting. So there's teacher data that's generated by a different network. But apart from that, there are no other strong assumptions other than power laws in certain quantities in the data.
00:19:41.074 - 00:20:10.584, Speaker B: So it's sort of, it's not specified by a very like specific task. Let's say that's like it's, but it's maybe the approaches that you pause it. The distributional assumption kind of comes in on the decay or the properties of certain tensors, matrices, quantities. That's where like the data assumption is, is, is hidden in the background, basically.
00:20:10.744 - 00:20:25.444, Speaker A: So it's still. Sorry to follow up, but somehow you're avoiding the fact that there's certain problems which are just intractable to learn. Yes. So there must be some proxy for learning that you have. And what's that proxy look like?
00:20:25.954 - 00:20:50.814, Speaker B: Maybe the, maybe it'll be clear in a couple steps when I define the problem, but it is in a student teacher setting, so that there is a teacher network. So it's realizable, that generates the data and that has kind of the same form as the model that learns. And then on top of that, there are going to be some assumptions on the kind of properties of that data which are related to power laws.
00:20:52.894 - 00:21:09.694, Speaker A: For these regimes. Are you literally comparing the number of training examples and the width of the model? Or, and if so, how do you define a training example? Is it just any input target pair that you compute a gradient with respect to?
00:21:09.814 - 00:21:41.584, Speaker B: Yeah, yeah, that's right. That's a good question because this is going to come back in this setting, you know, d on the order of n, where that's basically like a small number or one is the kind of comparison you can make. But then to transfer that to other, you know, like in the wild for a more complicated architecture where this transition happens, you know, how small is small and how large is large is hard to say. So that's going to be something I'll come back to.
00:21:42.324 - 00:22:02.724, Speaker A: So I think the point umesh was making was like if you had tried to learn a pseudo random function, for example, then these loss curves would just be flat. They would never go down. Right. So then there's the claim that like, there is a large class of practical learning problems where this is, you know, where the loss curves will look like this.
00:22:03.584 - 00:22:22.544, Speaker B: Yeah. And it's not, it's not as formally stated, but the fact that this setting where we have a full handle is a student teacher setting. And so the data itself is generated by a teacher model. And so it's realizable, you know, that there is.
00:22:36.644 - 00:22:44.436, Speaker A: Still hardener. Maybe we can let her get to the theorem statement and then if it's still mysterious, we can.
00:22:44.580 - 00:23:27.632, Speaker B: Yeah, I think it will have an answer in a couple sites. But it's a, it's not a random function. B, it has a structural property that will come up. But to go back to what we're, the results we're going to try to work up to, theoretically, to match with what we observe in the empirics, is basically this two by two kind of classification. So there's some regime where exponents actually vary and depend quite a lot on the problem. You can see here, these are different data sets. There are also different kind of architectures being plotted here, as well as different losses, and they have different exponents.
00:23:27.632 - 00:24:04.116, Speaker B: And then some regimes where the exponents are all across these different settings are all pretty close to one. And similarly, if you look at the width scaling, if you start from the regime where width is kind of the smaller of the two variables, it's the bottleneck, and you transition to this other regime, which we termed variance limited, at some point where the exponents are close to one. And so these are governed by different factors, or just driving these different, these two different regimes, which we tried to give names to. Any questions about this plot?
00:24:04.260 - 00:24:09.544, Speaker A: You explain what you mean by resolution limited and variance limited.
00:24:09.964 - 00:24:47.800, Speaker B: Yeah, those terms came from also. Maybe I'll say a bit more about that. It was just motivated by kind of the idea that in this regime, it's kind of like you're resolving a data manifold better and better with higher resolution. So when you're adding, you know, if D is the bottleneck variable and you're adding, you know, you're increasing DD helps you resolve this data manifold, but at some point you kind of saturate what you get out of that. And you, you are controlled by, you approach a different limit, and that's controlled kind of by fluctuations around that limit. How you.
00:24:47.872 - 00:24:51.444, Speaker A: Is that more of an overfitting kind of phenomenon?
00:24:53.004 - 00:25:57.718, Speaker B: I don't know. That it doesn't necessarily, I think it doesn't have to be connected to overfitting. It's more about what just drives the kind of approach to a limit. And these were kind of not being aware of a classification or if this was, you know, if this sort of taxonomy exists. We gave these names kind of based off of our understanding from problem. Any other questions about this? Okay, so let me now say just a bit more about the theory and what can we even rely on from deep learning theory? So, because this is a pretty challenging problem. So I will just kind of mention for folks who might not be as familiar with this area of work, there's been a lot of recent progress in the past few years in deep learning theory has been on studying these large width limits of neural networks.
00:25:57.718 - 00:26:55.230, Speaker B: So n going to infinity or n pretty large. And one of the findings has been that this is really a dynamical phenomenon that's connected to the way that we initialize and parameterize neural networks, that you get a connection. Basically, as you approach this limit, the model becomes more and more like a linear model, linear with respect to parameters, with random features that are derived from kind of the neural network at initialization. So there's still kind of a nice nonlinear set of random features, but nonetheless, you have a model that's linear in parameters. And also another way to state that is that there are some, there's a relationship to kernel regression if you're doing, if you're using square loss, for example. And these are kind of rich compositional kernels that are derived from the neural network architecture. So because of this connection I mentioned, it's very much dynamic.
00:26:55.230 - 00:28:12.292, Speaker B: It's a dynamic phenomenon, and it's related to the way that we parameterize or initialize neural networks with a particular scaling that's been common in practice. So that this limit kind of kicks in at some point. This connection means that basically, if you, for example, have square loss, you can just solve the problem analytically and rather generally. So the solution, you know, the predictive function that you get, for example, this is the continuous time version of it, is just solving this linear ode, and you can do that in closed form. This ode here has this quantity k, which is a kernel, and I turned it k, not because it's derived from the random neural network at initialization. And so anyway, this is kind of a summary for a whole body of work where you can drive and study these different kernels, where there are recursion relations governing what they are that depend on the architecture. And this limit exists basically for kind of a lot of different network types where there's a natural notion of width.
00:28:12.292 - 00:29:10.970, Speaker B: So fully connected layers, convolutional layers, when that refers to the number of convolutional filters, residual networks, as well as attention layers. Although the case of attention, I think is sort of interesting because I think the limit is not as natural for attention. To kind of get a similar limit, you have to study the limit of a large number of different attention heads. And so maybe it's a bit, it stands out as being a little bit different from some of these other architecture types, I think, just like. So it's for our, with our theoretical tools. It's nice because you have an exact solution to the predictive function and then you can calculate things with that. And also it connects to, I mean, certainly a lot of work in learning theory kind of studies kernel regression, for example.
00:29:10.970 - 00:29:38.854, Speaker B: So this is basically kernel regression when you're using square loss. And, and you also know what k is and it depends on the architecture. You can write down kind of, of recursion relationships. So this is not, this is a particular solution, let me say the more general problem is actually a hierarchy of differential equations that you would have to solve for a deep neural network with no obvious truncation for them.
00:29:40.714 - 00:29:54.964, Speaker A: So can I ask you a sort of a funny question? So how does this relate to what Ilya was saying yesterday? You know, let's assume that you could actually do these minimizations, etcetera. Is that limit related to this limit in any way?
00:29:55.984 - 00:29:58.816, Speaker B: Do which minimization, you know, so we're.
00:29:58.840 - 00:30:13.284, Speaker A: Computing this, the shortest program, etc. You know, computing the Kolmogorov complexity. So, you know, here you're able to do a, you know, give a closed form solution. So is that, does that in any way relate to the.
00:30:17.514 - 00:30:20.254, Speaker B: I think, I think it might be a bit different.
00:30:23.394 - 00:30:40.894, Speaker A: We know these models are very suboptimal. Also, these results, these results exploit the learning dynamics. So it's like important that we're doing SGD with a certain size. So it's not really an argument over everything, but. Yeah, and we know it's very sublime.
00:30:43.794 - 00:32:00.834, Speaker B: Maybe one thing to say is I think that maybe like perhaps the sub optimality depends on the amount of data that you have access to. For example, I'll try to say a bit more about this, but so a model which doesn't, you know, in general models have feature learning and feature learning, I'm going to define it by just meaning that this quantity here, k, is really a dynamical variable. It depends on t and there's also some equations governing the dynamics that you would have to solve to really solve the full set of equations. Now, different regimes of learning might have different amounts of in different regimes, you might have different amounts of future learning depending on how deep the network is, how wide it is, how much training you have. So these are all maybe also, again, different regimes of learning. This is just what I'm trying to summarize here is that this is basically an exact solution that kicks in as n when n is pretty large relative to other aspects of the problem, like amount of data. There's also a constraint on the learning rate that I think Jacob was mentioning as well.
00:32:00.834 - 00:33:02.186, Speaker B: But it's one of the best known. I mean, in the sense that it's been used in a lot of places. I'll talk about other places where I think it's also relevant. But the nice feature in part is that it's a pretty general solution. So then you can kind of separate out maybe the properties of the data with the properties of the kernel or see how they're tied together basically by studying this one. Any, any other questions? Actually, I'll show, I'll show some plots that kind of compare how, at the very end, from empirical work, kind of comparing the performance of, you know, a model where k is fixed versus a model where k is allowed to change dynamically. And you can see how, if you're perhaps in an ultra low data regime, these models are pretty good and so on.
00:33:02.186 - 00:34:10.936, Speaker B: But this is the point of this is to see what can we learn from just studying this model that we know is exact in large width. And it's not to get necessarily precision predictions out of it, but to kind of understand this taxonomy. Okay, so let me maybe go more quickly through this next few slides. So, just to precisely, say, the regime that we study a bit more kind of have a full handle on, is this student teacher regime. It's a classic kind of setup within learning theory and also kind of statistical physics approaches to studying machine learning. So in our setup, we have a teacher model that generates some data and generates the underlying data distribution with d samples. And the student model learns from this training data set.
00:34:10.936 - 00:35:12.664, Speaker B: We will assume student and teacher models that are both linear in their parameters, and they use arbitrary random features. So this is, again, this class of models that I mentioned on the last slide. So there are some features, phi, which are fixed and random, derived from a random neural network, and you can construct a kernel out of that from those random features. And so more specifically, the teacher is a model of this form, these f's are the random features. Omega, Ms are teacher weights that are just drawn once randomly and then fixed, potentially an infinite set of features. And the student model is a particular class of is also constructed by using some subset of the teacher features and learning a linear model on top of that. So it's all within the class of linear models with random features.
00:35:14.604 - 00:36:08.754, Speaker A: So maybe if I can sort of follow up on Shafi and Scott's earlier question to see if I understand how one way functions were ruled out, is it. So let me tell me if this is kind of correct. So, we're sort of implicitly assuming that the data was generated by some infinite width neural network that started, you know, was somehow obtained by starting with random weights and then doing some small learning rate limit. And in that limit, you can't really construct one way functions because everything is sort of asymptotically linear in these random features. And so this is like, if we had, like, worst case overall infinite with networks, then we would sort of be screwed. But because they're kind of, the teacher model is like implicitly something with random weights, then, then you can get these one way functions. Is this kind of how we're ruling out the bad case?
00:36:09.094 - 00:36:35.896, Speaker B: It could be. So the weights certainly are kind of. They are drawn randomly. They're not fine tuned to any specific problem. The teacher network could have infinitely many features or finitely many features, but it is a linear model with random features, and there's going to be some power law assumptions on the kernels associated with these random features. So that's a different, that's another, I think, reason.
00:36:36.080 - 00:36:44.324, Speaker A: I see that. Thanks. That still leaves the question of what functions could we express? If we could tune the weights, could we express anything that way?
00:36:45.224 - 00:36:57.292, Speaker B: So, in, I think when it's infinitely large, I think you can resort to universal approximation, if I understand correctly. But I think there are folks in the audience who maybe have.
00:36:57.408 - 00:37:00.224, Speaker A: Yeah, okay, thanks.
00:37:01.084 - 00:37:26.238, Speaker B: There is, there is going to be this important assumption of the power laws on, on the spectrum. So what class of functions kind of are related to having that power law assumption? I think. I don't know. They're, they're basically. Yeah, they're basically. Well, they're supposed to be a complete basis when, when you have infinitely.
00:37:26.286 - 00:37:37.950, Speaker A: So, for example, if you have the worst case, then even one of these would be a worst case function. I think FM is the thing that is like the solution to that differential equation on the left side.
00:37:38.022 - 00:37:38.302, Speaker B: No.
00:37:38.358 - 00:37:39.434, Speaker A: Oh, no, no.
00:37:42.494 - 00:39:12.044, Speaker B: That was just the predictor for a neural network. These f's are, let's say they're a set of basis functions. And maybe the one, here's maybe one constraint on what they are, which is essentially like a distributional assumption on the data, which is that when you construct a kernel from these random, from these feature functions, this is some kernel function, you can construct a, if you looked at the spectrum of this eigenvalue, spectrum of this, of the kernel matrix, you would construct from that on the data that it just follows this power law form. So, you know, this is true for certain classes of f's and not for others. Does this maybe help with the background assumption? So this is the, this is one of the features of realistic data that we wanted to work into the model, which is that when this is a power law and this is kind of a generic property, you know, sometimes in the theory setting, you would study kind of a bit more pathological versions of this. For example, there might be symmetries in these eigenvalue degeneracies and so on, but these are not. One of the points we'd like to make is that this is kind of not generic.
00:39:12.044 - 00:39:47.754, Speaker B: Generically you have, these kinds of quantities have nice power loss spectra. And just in anticipation of the next slide, I've labeled the exponent that you have asymptotically as one plus alpha k. Any questions about this? So if you changing, for example, this is an experiment that changes the resolution of the images. And when you coarse screen the problem, it becomes simpler and it changes this.
00:39:47.794 - 00:39:56.906, Speaker A: Kernel exponent, these eigenvalues, the spectrum, these are for different data. Like what are the different codes?
00:39:56.970 - 00:40:15.930, Speaker B: So it's just, it's a fixed architecture or fixed set of, I think it's like a fully connected, it's a fully connected network, but it's just core screening. So like kind of downsampling, averaging, you know, over images. So that tunes the kind of difficulty of the problem. I see.
00:40:15.962 - 00:40:17.434, Speaker A: And that's seen in the spectrum.
00:40:17.554 - 00:40:56.324, Speaker B: Yeah, that's. And that's reflected in the spectrum. So the more difficult problems have. Yeah. The pool size means a smaller image or kind of more coarse grained image and that has a steeper slope and with kind of greater resolution, you have a shallow slope. Okay, so just to kind of move through this section. So given that on zots, on the data, which is motivated by experiments.
00:40:56.324 - 00:41:41.184, Speaker B: So this is trying to kind of model realistically these some aspect of the data, then you can derive that the data scaling exponent is alpha k, kind of to leading order. And the width scaling is also alpha K. And so in this setting, you find that the exponents alpha D and alpha W are actually the same. They're kind of just controlled by this one quantity that appears. We call this a duality. And I'll come back to this in an experiment in a couple slides. But so by setting this resolution limited regime, you can kind of at least just better understand that this scaling originates from a power law that exists in the data and the model together.
00:41:41.184 - 00:41:49.112, Speaker B: So it's really not talking about data independently or model independently, but just this one quantity that appears.
00:41:49.208 - 00:41:56.180, Speaker A: There's a typo. I think the d is, the alpha D is in the data field alpha.
00:41:56.212 - 00:42:01.700, Speaker B: D. Oh, this, no, this. So it's saying that alpha D is.
00:42:01.732 - 00:42:04.876, Speaker A: Alpha K. Oh, that's what you're saying.
00:42:04.980 - 00:42:10.544, Speaker B: Yeah, so that's the result. And alpha W, this exponent is also equal to alpha K.
00:42:12.844 - 00:42:18.376, Speaker A: Just to remember what Lambda I is, this is an assumption about the student or the teacher.
00:42:18.540 - 00:42:59.044, Speaker B: They use the same set of features. So the student is like a projection, potentially smaller set of features, but the whole collection of features has this form. So this basis f. Okay, let me skip over kind of in the interest of time, some of these other sections. So that was what we called this resolution limited regime. We also studied this variance limited regime where you're studying the scaling of the variable that is not the bottleneck. And kind of by looking at the ways in which covariance, finite size covariance matrix approaches its limit.
00:42:59.044 - 00:43:35.180, Speaker B: The leading order kind of expansions are things that have exponent one. And so if you follow that through into how it affects the loss, you get exponent integer exponent one. So the finding is, one of the points is that in these different regimes, you're controlled by different things. One is very data and model dependent. The other is controlled by just a nice kind of, basically one over n, one over n squared, and so on. Expansion about a limit. So this is, let me skip over this slide as well.
00:43:35.180 - 00:44:20.222, Speaker B: This is just kind of a sanity check on this specific kind of set up where now we're looking at these student teacher models kernels. It's a special case of the experiments I showed at the beginning, so I won't dwell on them further. But again, kind of the results we're working up to are that these exponents are one on the diagonal. And in this student teacher setting, we found that they were related to alpha K directly and that they're equal, which is what I call this duality. Okay, so a more general setting. So there are two aspects that can be generalized a. We could go beyond the student teacher setting and have more of a generic kind of function that generates the data.
00:44:20.222 - 00:44:56.374, Speaker B: The other is to go beyond linear models to include this important aspect of feature learning. And I'll comment on that at the very end. So in terms of these two regimes, the variance limited regime, and I won't go into the details of it, is something that we can show under more general conditions. So this exponent's equal to one. It doesn't rely on being in the kernel limit. So you can, because it's unrelated basically to feature learning. And it originates from, as I mentioned, these leading order terms about a nice limit which where.
00:44:56.374 - 00:45:29.734, Speaker B: And to show this, one of the things we rely on is these leading kind of finite with corrections to infinite net, with networks. That's also understood. So this is kind of a more robust kind of result for the resolution limited regime. For neural networks. In the wild, where there is feature learning, we lack a general form for the predictor. So it's not just a linear model. And instead I'm going to try to, we're going to try to rely on hypotheses that we can test in experiments.
00:45:29.734 - 00:46:21.342, Speaker B: Before I move on to that, let me just mention one kind of pretty realistic, or I think kind of comes up a lot, a setting where you actually can find that these results carry over. And that's the one where you have pre training and fine tuning. So let's suppose we have pre trained embeddings, for example, embeddings that are trained on imagenet and then fine tuned on cifar ten. And this is kind of an experiment that's outside our theory, because it's not the student teacher setup. There's no teacher generating the correct labels. And it also uses learned embeddings. And we basically find that some of these, that the results I just talked about for the exponents are robust.
00:46:21.342 - 00:47:30.322, Speaker B: So here, of course, you see these alphas equal to one, close to one. But what's surprising is that in this regime, when you look at data set scaling or model size scaling, the exponents are the same. And so this duality that I mentioned didn't have to hold in this pre training fine tuning setup. It's not a regime we actually have control over, but you find that, as you see here, alpha P and alpha D are pretty close. So just to kind of drive home the point about what's happening in these plots, as you change data set size, you're on one side of this curve. That's why we're kind of, this is the regime where data set size is the bottleneck variable, and at some point, if the problem is not regular, optimally regularized, you hit double descent, and then later you cross over to this regime where the exponents are controlled by the variance limited scaling. So we're describing two different kind of limit sides of a double descent curve, if you like.
00:47:30.498 - 00:47:45.122, Speaker A: I just have a kind of historical question here. So if I understand correctly, like this implies that if you kind of have a fixed amount of compute, you should always scale n and d at the same rate. Is that right?
00:47:45.298 - 00:47:50.274, Speaker B: So that's a good. Are you, are you thinking about like optimal scaling?
00:47:50.394 - 00:48:03.086, Speaker A: Yeah, because this paper came quite a bit before, like the Chinchilla paper and OpenAI had a co author on this paper, but they were still using the wrong scaling law. So I'm just trying to understand how those three facts are reconciled level.
00:48:03.190 - 00:48:52.758, Speaker B: So it is kind of tantalizingly close to the Chinchilla law. I think we can't say be as definitive about that scaling law because we're not studying. Our results are for scaling of one variable at a time rather than the joint scaling. And so I think you need to have access to both, you know, the full form l, p to be able to solve for, you know, given a fixed compute, what is the optimal way to scale DNP? But it is interesting that the exponents are kind of, this are the same in this setting. Thanks for that question. So let me keep going. So, I find it surprising that this is a setting, as I mentioned, it's outside the theory.
00:48:52.758 - 00:49:05.154, Speaker B: We have learned a learned model. We have real embeddings. They're not teacher generated, but you can still observe the duality between these exponents in the resolution limited regime.
00:49:05.974 - 00:49:23.794, Speaker A: I have a question. So, in the student is a model, the loss goes to zero. Exactly right. But in the real world, cases that you are showing, the loss cannot go to zero. It will bottom out at some point because of the variance, right? Is that right?
00:49:24.584 - 00:49:26.724, Speaker B: Which, which of these losses.
00:49:31.184 - 00:49:34.124, Speaker A: Go to zero? Or does it bottom out at some point?
00:49:35.624 - 00:50:09.864, Speaker B: If one. So, because one of the variables is usually like finite. So when you take the bigger, if you study the scaling, for example, this variance limited regime, you see, I've subtracted off the asymptotic value in one of these plots, and that's because one of the variables is still held fixed. So you're kind of limited by that. I have maybe five minutes. Ten minutes?
00:50:10.724 - 00:50:13.264, Speaker A: Yeah, maybe to leave time for questions.
00:50:17.444 - 00:51:07.766, Speaker B: Let me then skip over. Just give it a high level, and I can talk about it more offline, the way in which you would try to generalize some of these results. So we tried to then, you know, in the wild, when we don't have, when models don't behave as linear models, you can try to relate things, some of these exponents, to numerical measures of dimensionality of the data manifold. So we did some things in that direction. But to kind of wrap up, let me go back to kind of the experiments we wanted to work up to. So we did actually explain, you know, where these alpha equal to one exponents come from. And in the resolution limited setting, we have some control, some understanding of why these exponents, you know, take on the values that they do.
00:51:07.766 - 00:52:19.822, Speaker B: And there's some relationship between alpha W and alpha D that you can observe, for example, in pre training and fine tuning. Maybe I'll skip over a few more experiments. So we've looked at experiments trying to change, just empirically changing the task, for example, superclassing a task when there's kind of a fine to coarse hierarchy and labels, how that changes the exponents, it keeps, for example, the exponents invariant and just shifts the plots up and down, whereas when you connect inputs, you see a much stronger effect on the exponents. So maybe one hypothesis is that neural networks primarily model the input data manifold less so the classification task. If you also look at changing architecture, for example, you change the width of the models, you see a larger change in the exponent compared to, for example, changing the depth. And this sort of thing we know is understandable. It's connected to models approaching kind of this kernel limit that I described where there's no feature learning.
00:52:19.822 - 00:53:24.214, Speaker B: And when there is feature learning, they become more sample or parameter efficient. Okay, so what are we missing from deep learning theory? I mentioned that a lot of, you know, the reason we could do analytics using our tools was because we could, we can solve this problem, which is exact in a certain limit, but it lacks this aspect of that. I mentioned where k is really a dynamical variable that changes from its initial value. So it's difficult to tackle this problem in full generality. And I'm not sure that it's going to be tractable to study kind of, you know, generalization in a very general way when you have feature learning. I'll just mention in physics language, since there are a few folks with physics backgrounds in the audience, that this kernel limit is sort of like a non interacting problem or exactly solvable problem, and it's related to gaussian field theories and linear problems. So these are all things that we can solve, and these other limits are strongly interacting.
00:53:24.214 - 00:54:20.204, Speaker B: And, you know, so people have tried to use techniques from physics to study these other kind of limits where there might be future learning. So techniques from correlated physics where you have, say, you define some partition function and try to kind of have some mean field theory that is self consistent to be able to solve this, but you can't. It's difficult to solve in full generality. And so when you make approximations for a particular architecture or a particular type of data, it might not be clear how to generalize that to other problems. So I hope, I kind of convinced you that we can gain some things from studying this simple setting. We can understand some, you know, a regime where there's some universal behavior. We can understand some aspects of duality that affect pre training and fine tuning, but we can't capture all the things, for example, fully understand feature learning.
00:54:20.204 - 00:55:30.294, Speaker B: And here's just a plot that shows the effect of future learning, going back to the question of is it optimal or not? So, this plot shows kind of the performance difference between one of these kernels versus a finite size neural network that's trained. That's the blue curve versus the green. And you see that there's a certain data set size, critical data set size, where the performance of the neural network is better than the kernel, but it's equivalent kernel, but in a very kind of small training dataset, training data size regime, the kernel can be better. So there's some crossover point where one kind of overtakes the other. Okay, this is sort of a summary slide, and to revisit the questions I asked at the beginning. So I hope you kind of walk away maybe with the thinking that different scaling regimes are governed by different driving factors or driven by different factors. And so you can kind of have a taxonomy of different scaling regimes.
00:55:30.294 - 00:55:45.250, Speaker B: A number of open questions, which I'll also skip through. I actually really wanted to talk about emergence and phase transitions, but I'm almost.
00:55:45.322 - 00:55:57.764, Speaker A: Probably on a great niche. I think maybe let's take questions and. Oh, no, we asked a lot of questions. Okay, so go, go ahead. Sorry.
00:55:59.464 - 00:56:47.172, Speaker B: But I'm happy to talk more about this. Well, I wanted to, you know, a lot of what I covered now was it was not directly studying llms. It was trying to do more first principles theory and simple setups. I wanted to kind of pivot into some of the discussions that were happening related to conversations from yesterday, but also kind of tie it back to some things from so my training is in condensed matter physics, and so I kind of like to look at some of these things from that perspective. And I wanted to make just a few remarks about it. Okay. So to kind of pivot, you know, I talked about loss, and that's.
00:56:47.172 - 00:57:25.582, Speaker B: And it's really like an in distribution problem and not out of distribution generalization and all of this. But now, you know, in this era of llms, we're also looking at just abilities. So different metrics, accuracy as well. I didn't talk about accuracy on a problem, for example. So, you know, you might observe very nice improvements in loss as a function of the scaling variables, but the performance is a function of accuracy. When you study, it might be quite a bit sharper. I just wanted to highlight some of the results from this big effort across a number of different institutions.
00:57:25.582 - 00:58:40.644, Speaker B: That was called big Bench. How many folks are familiar with big bench? Okay, many people. So then maybe I don't need to talk about it, but I wanted to show a few slides about the way, looking at some of the results, I think that this is like a great effort to turn claims about ability into more scientifically measurable quantities, something that you can evaluate. I think it brings up an interesting question of how to generate interesting and diverse tasks in a scalable way. So not through just human annotation of different tasks, but, for example, a lot of these tasks are programmatically generated by individuals. And so if you want to test something like, you know, this might be doing human annotation for images and so on, might be quite natural. But if you want to, for example, test scientific reasoning in a more complex way, then, you know, you could do that by looking at high school textbooks and kind of curating question answers from textbooks.
00:58:40.644 - 01:00:05.564, Speaker B: But it's difficult, it's challenging to generate a diverse set of benchmarks that really probes more deeper scientific reasoning without doing it by hand. So I think that brings up kind of an interesting thing to think about. So this paper kind of highlighted this fact that tasks can have different behaviors. Some might behave linearly as a function of something on the x axis, so it could be model size in some of these, or some might have breakthrough behavior in quotes, and some might even just get worse with scale. Here's another example, actually also related some big bench tasks, but this is from a different paper also showing this kind of, you know, what's being called emergent behavior for different tasks as a function of some way of scaling on the x axis, which could be model size, but it could also be other ways of scaling. So, I think one thing to note, kind of a remark from the big bench paper, but also studied in more depth in this paper by Schaeffer et al. Is kind of the dependence of these emergent abilities on just simply changing the metric.
01:00:05.564 - 01:01:19.050, Speaker B: So if you measure it kind of with something very sharp that is nonlinear or discontinuous, like getting an exact string match, it can be quite sharp if you change the metric. They found that many of these kind of sharp, sharp behaviors that were observed in big bench could be reduced if you simply change the metric. So I think it brings up interesting questions about when is the behavior actually sharp and what is the source of non analyticity. So I just wanted to maybe say a few comments about that. So, there are two kinds of notions, I think, that are naturally connected to ones that maybe come up in physics. Sometimes there are claims about relationships to phase transitions, for example, in this literature. Rather, at an informal level, I wanted to mention that the reason and kind of give say a little bit about the fact that in physics, the origin of the non analyticity that you observe is because there's a thermodynamic limit that you, that you deal with.
01:01:19.050 - 01:02:25.940, Speaker B: So, for example, in physics and statistical mechanics, you have a partition function that is a sum, a finite sum, over analytic functions. And you always work in this thermodynamic limit where the number of the system size, the number of particles, number of states, goes to infinity. First, before you study kind of the thing that tunes you through a transition, for example, tuning through a critical temperature data. So that's the source of non analyticity in these sorts of transitions. And to establish it carefully in numerics or in experiments, usually you do finite size scaling. So you study how the system, how with a changing system size, the observed variables, the measured variables, become sharper and sharper. So I think to put some of these observations on a more kind of rigorous footing, you might want to do things like things of that form and basically just try to identify the origin of non analyticity.
01:02:25.940 - 01:03:55.484, Speaker B: If it does, indeed, it exists. So it could be due to the metric, perhaps it's not due to the metric, and there are some other variables that are indeed non analytic. But I think this is a place where theory can do interesting work, because by taking the right limits, you can kind of get non analytic behavior. So that's maybe a comment about sharpness and whether there is indeed kind of sharp behavior that's setting in, or just gradual behavior that we simply don't observe in experiments or aren't measuring kind of the right quantities for. And I'll maybe just close by, kind of contrasting the approach I took this, in this talk with, you know, other approaches that you might try to take. There's a lot of references these days in the LLM literature to kind of this defining paper for that kind of defines the spirit of condensed matter physics. And just the idea of emergence more generally across different sciences, where, you know, at every level, as you go up in scales, at every stage you need, you might have qualitatively new behavior set in that also requires new theoretical language or new theoretical starting point that is not easy to derive from more microscopic kind of behavior.
01:03:55.484 - 01:04:39.150, Speaker B: So, for example, solid state or many body physics kind of emerges from particle physics, chemistry, from many body physics, and so on. So I think it's. And I know other speakers will talk about this later. It's an interesting time to think about what are appropriate, useful, or insightful formalized notions of emergence. And the approach that I took in this talk was really some combination of trying to be first principles about things, using theory that we know, but also trying to do induction in some stages, when to fill in the gaps. So making a guess about the answer and then trying to test it in experiments. But it might be that there's.
01:04:39.150 - 01:04:53.884, Speaker B: It would be more appropriate to start with different conceptual or theoretical primitives, to say useful predictive things kind of in this, in this current era for llms. And so maybe I'll just. Lm there.
01:05:07.864 - 01:05:22.684, Speaker A: Maybe if there's one burning question while Sasha sets up. Yeah, so, I mean, I. I worked on NTB a few years ago, kept up too much.
