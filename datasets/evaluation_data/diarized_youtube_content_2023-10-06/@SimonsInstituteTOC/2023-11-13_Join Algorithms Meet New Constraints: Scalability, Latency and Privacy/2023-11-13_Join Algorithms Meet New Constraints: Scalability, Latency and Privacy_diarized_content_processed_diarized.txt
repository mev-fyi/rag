00:00:34.994 - 00:01:05.214, Speaker A: Hello everyone, my name is Xiao Wu. I'm a research fellow in the logic program. I'm also the incoming assistant professor in the University of Waterloo. Today, I'm very happy to show some interesting direction in database database theory and algorithms, which is the join? Algorithms meet new constraints, scalability, latency and privacy.
00:01:08.834 - 00:01:09.694, Speaker B: Okay.
00:01:10.034 - 00:02:02.720, Speaker A: In the relational database, the join is a fundamental operator that can link the information stored in different tables together. So given two tables with the common attributes, the join operator will find all pairs of tuples with the same value on their common attribute. And if we have more relations, we can use the joint operator to express more complex analytical queries. In this relational knowledge graph, each node is an entity and each edge is labeled with the interaction between these two entities. And we can store the edges with the same label in one relation and ask join queries. For example, find all people who like something in the Louvre museum, okay? Use this three way line join. This problem in general is np hard, if not assuming the query size as a constant.
00:02:02.720 - 00:03:06.714, Speaker A: So people focus on the data complexity of this problem and the traditional goal of the research for the joint processing is to find the most time efficient way to report all the joint results. In the last 20 years, the lead to process and analyze big data has invigorated this traditional research area with a lot of new challenges. For example, large volume data cannot be stored and processed by a single machine. People have developed a massive parallel system such as Mapreduce and spark for data processing. Then how to design joint algorithms that can skew to more than hundreds or thousands of machines. We can also use theoretical model to capture the architecture of those systems. So given input, join, query and the database distributed over the machines, the computation process in rounds and in each round the servers can communicate with each other and then perform some local computation.
00:03:06.714 - 00:04:24.164, Speaker A: The complexity of the algorithm in the message parallel computation model are first measured by the number of runs and then the load, which is the maximum size of message received by any server in any round. So here the target is to design join algorithm that achieve the optimal trade off between the run complexity and the load complexity. The second challenge is that data is generated at a very high speed, how to deliver tiny answers to the users. Similarly, we can have a theoretical model to capture the capture the settings. So given a joint query and the database, the target is to design a data structure, okay, that can be updated efficiently with the updates to the data and also whenever the query answers is required from this user, we can enumerate the query answers from the data structure with delay guarantee. So the delay here can bound the waiting time between consecutive answers. And here the theoretical target is just study the trade off between the pre processing time, the update cost and also the delay in an enumeration.
00:04:24.164 - 00:04:49.782, Speaker A: The third challenge is that data usually comes from multiple resources and contain sensitive information. So people have pay a lot of attention to the privacy issue. Then how to evaluate a joint query without leaking the information of the underlying database? We can see the scenario that even both the input data and the join answers are encrypted.
00:04:49.888 - 00:04:50.186, Speaker B: Okay.
00:04:50.210 - 00:05:49.104, Speaker A: The adversary can still learn something by observing the behaviors of the join algorithm. So what does the behavior mean? So in a sequential ram model it could be the sequence of read and the write operators to the main memory. Or in the massive parallel systems it could be the send and the receive communication pattern of the servers. Here the goal is to design join algorithm whose behavior does not lead to the distinguishability of the input databases. And if you want different level of protection of the privacy, you can also assume differentially oblivious algorithm by the labor for the labor input database. Okay, so beyond the natural join, there are many interesting join related problems I have worked on. For example the seminary join join aggregate queries, sampling over join, a temporal join difference of join queries and generalizing synthetic data site for the join queries.
00:05:49.194 - 00:05:49.988, Speaker B: Okay.
00:05:50.156 - 00:05:55.824, Speaker A: If you catch any interesting keyword, feel free to talk to me. Thank you.
00:06:06.884 - 00:06:17.496, Speaker B: Okay, we have one question. Yes. How about the join sync parsing data like streaming data joints? Is that something that you are?
00:06:17.580 - 00:06:35.364, Speaker A: Yeah. For example, we have some counting query on top of the join results and we want to release a synthetic data set for example, which is differentially private and then still provide some accuracy guarantee for answering counting queries over the joint result over the synthetic data.
00:06:35.864 - 00:06:36.304, Speaker B: Thank you.
