00:00:00.200 - 00:00:40.120, Speaker A: Thank you. So I'd like to thank the organizers for inviting me to this very interesting conference. And I actually changed my title, if you noticed. The reason I changed my title is I was going back and forth between talking about what was the original title, which is about reverse hypercontructivity, and talking about the second topic. The second topic is a bunch of observations, which are nice, and I wish that somebody have told me these observations five or six years ago. So I'm going to share these observations with you. And what's nice about these observations is that they were actually observed here at the institute, like in the last couple of weeks.
00:00:40.120 - 00:01:35.892, Speaker A: So there are going to be two parts. In the first part, I'm going to talk a little bit about reverse hypercontectivity. I apologize to those of you who've seen a version of this at Cambridge, maybe a year or two ago. And then in the second part, I'm going to talk about something that's really new, that's from the last record. And one of the reasons I changed the title is that in the age of the Internet, there is a rumor that actually Krister Borrell may be watching this talk as we're speaking, and they will send emails to Jeff if you have any questions or comments. Ok, so the first part of the talk I'm going to talk about, the joint work with both Arnab and Christopher are both sitting here, and it's going to be about reverse hypercontectivity. And again, I'm going to go even maybe a little faster than I'm usually going, because we've seen this kind of stuff quite a bit during the workshop.
00:01:35.892 - 00:02:15.484, Speaker A: I'm going to talk about noise stability or noise sensitivity in the discrete hypercube. The fact that there's a half here means that I'm looking at the uniform measure on the hypercube, and tt is the noise Markov operator, where the noise, what in the previous talk, Ronan, called rho or rho square, now is going to be e to the minus t. So given x, I'm going to pick a y. How am I going to pick a y for each coordinate independently? With probability e to the minus t is going to stay the same, and with the remaining probability, I'm going to de randomize it. So that's the noise that we're going to look at. So that's one definition and equivalent. Second definition is the functional definition.
00:02:15.484 - 00:02:37.334, Speaker A: We act on functions. What do we do to a function? Well, it's probability e to the minus t. If we look at the function on the two point space it's probability e to the minus t. We stay at the function and otherwise we average the function. So this is the operator on the two point space. And then we look at minus one, one to the n. We are looking at the n's n wides tensor of this operator.
00:02:37.334 - 00:03:17.098, Speaker A: And I'm going to do the usual crime of choosing a subset of the names of the names that should be here. This is a random subset of the names that should be, but with some weights corresponding to something. Okay, not really. Say again? Oh, sorry bonhomie. Okay, right, so there's some semi group noises acting here, I apologize for that. And that's, and that's the, that's the hypercontactive inequality which we, which we all know. So you look at tt at f, at a higher known p, it's less than f at the smaller norm q, as long as the semi group run for long enough.
00:03:17.098 - 00:04:11.162, Speaker A: And we know the tight constant for this. So t has to be at least one half log p minus one over q minus one. Okay, so I'm going to go quickly, but please interrupt me if something looks unfamiliar or you want me to go slower. So, okay, the experts in the next topic I want to comment about briefly, because this is going to be related to what I'm actually going to talk about, is what happens if you look at other discrete spaces. So instead of looking at the space zero one to the n with the uniform measure, you can imagine you want to look at some other probability distribution. So for instance, you can look at this space zero one with alpha to the n, which means that alpha is taken with probability one in each coordinate independently. You can ask the same question, what can we say about hypercontactivity? Or you can ask about some general product states omega to the n may be discrete, maybe not discrete.
00:04:11.162 - 00:05:11.636, Speaker A: The question is, again, what can you say about hypercontactivity in this setup? So again, the semigloup is going to be the same semigorp that we've been talking about before. So again, two interpretation. One interpretation is that if you think about the random walk, the random walk, each coordinated, keeps the semis probability e to the minus t, and otherwise it picks it according to the stationary distribution on the one dimensional space that we're looking at. And the functional version is that in one dimension what we are doing when we act on functions, we take the average probability one minus e to the minus t, and otherwise we take this probably e to the minus t, we take the function itself. Okay, so again, the points I'm putting right now is since the exports are on the room, I'm not going to go into too much detail, but a lot is known. And this result by Kishta Foleskiewicz and Pavel Wolff. And there are similar results in the setup of log sublevel of inequalities, which we know.
00:05:11.636 - 00:06:06.402, Speaker A: And I don't want to go over all of these inequalities, but let me just highlight, maybe I should have highlighted in the slides, what are, what is the key points I want you to look at. So somehow, in this hypercontactive inequality, rates look exactly the same way that we've seen before. So there's a higher norm over here, there's a lower norm over here. What we see is that there's some dependence on the norm, but there's also some dependence on the space. So there's this quantity log of one over alpha, where alpha in the case of zero one, is the probability of the smallest atom among zero or one. And alpha in the case of a general discrete probability space, is the probability of the smallest atom in the space. So what does it mean? It means that suppose we all like to apply hypercontractive inequalities, and we are used to doing it in the discrete cube zero one to the n with the uniform measure.
00:06:06.402 - 00:06:43.818, Speaker A: If we want to apply it in a different product structure, we can do it. But we lose something. What do we lose? What we lose in Des scaling, which we talk about the time, what we lose is we lose a factor. The time has to be bigger by a factor, which is log of one over the smallest probability of any atom in the space, which is something that some of us are well aware of. Because if you remember, if we looked at examples, say, where the probability measure was gave one probability one over n and zero, it gives probability one minus one over n. And we know that something like kKl doesn't hold in the setup. Why doesn't it hold in the setup? It doesn't hold because the hyper contractive inequalities as we know them do not hold.
00:06:43.818 - 00:07:16.604, Speaker A: What do we lose? We lose log of one over alpha when alpha is n, which means that we log a factor of log n. So it's exactly the log n that we gain, we lost. And therefore, kkl doesn't tell you anything. In this setup, the space is very, is very best, and this is necessary. So these inequalities are sharp and you know, there's nothing you can do about it. These are the type results. Any questions so far? Okay, good so now we're going to talk about Borrell's reverse bound.
00:07:16.604 - 00:07:45.130, Speaker A: So, this is a. In a paper of crystal Borrell from 82. This notion of reverse hypercontactivity was introduced earlier in the gaussian context in a paper of Borrelian and Svante Janssen, also that was published in 82. And the setup is the following. The statement is the following statement. So again, we are looking at the uniform measure on the discrete cube. Now, we are looking at the positive functions, and the inequalities are going to be somewhat reversed.
00:07:45.130 - 00:08:12.842, Speaker A: So everything is going to be reversed. And it will only make sense if the functions are positive. So, the function here takes positive or non negative value. And the statement is the following. If you look at the q's norm of t, t greater than the p's norm of f. Okay, so averaging increases the norm for norm q that less than norm p. And actually, both of the norms are not norms because they don't satisfy the triangular inequality.
00:08:12.842 - 00:08:44.886, Speaker A: They're semi norms because P and Q are both less than one, and they can actually be negative. This is the reverse inequality. And everything about it looks reversed, right? This inequality goes in the other direction. The norms are actually not norms. And, you know, everything looks very, very stretchy about this inequality. And the Borel paper is a wonderful paper. And in fact, this fact appears in about the statement and the proof.
00:08:44.886 - 00:09:05.166, Speaker A: Take about ten lines in the paper. Okay? You know, it's mostly about hyperconductivity. And then Crystal Borrell observed, he said, well, in fact, you know, if you do the opposite, you know, you see that all the opposites are preserved. And, you know, you repeat the same proof, then you have to check the following inequality. And you check it, and it's fine. Okay? So that's everything that is in the paper. It takes a little bit of deciphering to do.
00:09:05.166 - 00:09:44.794, Speaker A: But Christopher Borel did it very, very nicely in the paper. And maybe I'll comment a little bit about how it. Okay, so let me say a little bit more about the statement. So, that's the statement that we're going to talk about. The first question I'm going to ask, for those of you who haven't seen it before, does this statement make any sense? And in order to talk about what senses makes. So, let me tell you a little bit about norms that, or seminoles for these norms for p, less than one. So the first thing that you have to notice that even without hypercontactivity, if you look at the positive quantity or a positive function, averaging actually increases P's norm for p less than one.
00:09:44.794 - 00:10:29.334, Speaker A: Okay, so indeed, even before anything sophisticated, just if you take a Markov operator, you average a non negative function, the PIs norm is going to increase. Similarly, if you look at something, if you want to apply something like kosher Schwarz or Helder inequality, you can do it, but it goes in the opposite direction. So if you look at two norms, p and p prime, that are dual, the sum of the reciprocals is one, but the norms are less than one. And in f and g, again, everything has to be positive or non negative. If f and g are both non negative functions, then the expected value of f time g. We are used to the fact that it's less or equal than something, but it's going to be greater or equal than something. So it's going to be greater or equal than the p nodes of f and the conjugate norm of g.
00:10:29.334 - 00:11:01.434, Speaker A: One of them is going to be negative. Yeah, but we okay with negative? No, I mean, once you allow yourself the flexibility of the mind to talk about norms less than one, negative norms are not such a big deal. Okay, so it's like, so the nose can be. Yes, the nouns can be negative too. Okay. And if you think about these two basic results, which were definitely known to hardy or probably even before, if you think about these two results, then maybe it's not so surprising that things go in the other direction. Right? So you expect averaging to increase, but not just in the p norm.
00:11:01.434 - 00:11:58.962, Speaker A: Maybe you can even gain more by, by getting something in the, in the smaller norm, in the q norm that you get in the q norm, that, that the average according to the semi group of f is greater than the norm or according to the p norm, as long as they're at the semi group, run for long enough. Okay, so this was just to explain why this inequality might make sense. Okay, so I'm going to explain briefly why is this inequality? I started at 1020, right? Yes. Okay, so I'm going to explain briefly why is it true? So I'm going to explain in a similar way to Borrell. I'm going to show you very quickly a short proof and try to convince you that it actually works. But I won't actually go over the proof. And then I'm going to tell you what is it good for? So this is all very, you know, older news, and then I'm going to talk about what is it true for other spaces? Okay, so here is.
00:11:58.962 - 00:12:28.964, Speaker A: And you know, usually you do the opposite, right? Usually when you present a part of a paper, the information on your slides is less, is less than what's in the paper. What I'm going to do now, I'm actually going to present something that's small in. What's Borrell's proof of Borrell's result? Right, so the reverse result. So it's going the reverse direction. I'm actually going to expand, I'm going to devote two slides to something that Borel did in like ten lines. But let me still do that because I think it can, can be useful. So here's Borrel's argument.
00:12:28.964 - 00:13:19.158, Speaker A: So Borrell says, okay, how do you prove things like that? He said, at the time we proved, he said, we already know that inquiries like this tensorized. And what do we need to tensor eyes in order to tensor? As inquiries like the hyper contractive inquiries, we need Minkowski inequality. And in p less than one, we have reverse Minkowski, since the inequality goes in the other direction, since reverse Minkowski was, it's enough to prove it for the case of one dimension. So he says, it's in one line, I did it in two. Then he says, you know, actually you don't have to consider all of the range of the parameters, because various arguments about continuity and more important, duality shows you that it's enough to show it for specific values of p and q, because the other ones are dual of these parameters. So it's enough to show it for parameters p and q as before, but positive, right? The negative ones even. So, I said that we are flexible enough to accommodate.
00:13:19.158 - 00:13:50.010, Speaker A: Actually working with them is a pain. So duality lets you actually work just with p and q that are strictly positive. And then what's the core of the proof that I'm going to show you in the next slide? So the core of the proof, we are going to use something that's not necessarily completely standard. In hyper conductivity, we are just talking about positive functions. So up to scaling, we can write our positive functions in the form one times x, where is some number between minus one, one. Again, we are on the two point space. So every function is a linear function.
00:13:50.010 - 00:14:19.468, Speaker A: It's a positive linear function. So after multiplying by a constant, the function that we're looking at is just one parameter. It's one plus ax, where a is between minus one and one. Okay, so now all, you know, after applying sort of this general machinery translated to the reverse setup, all we have to do is to prove this inequality for this specific function, for this specific family of functions. So this is what we need to do. And then you do what you have to do. And, you know, again, I don't think that Borel actually writes these formulas in this paper.
00:14:19.468 - 00:14:37.284, Speaker A: I think he only writes this. He says, you have to check this. And this is true. Okay, so this is what he said. But what do you do? What do you do? You say, okay, I'm going to expand the p's node to the p, and I'm going to. What I should compare it to is the norm of the Q's norm to the p. But let me first write the q's norm to the q.
00:14:37.284 - 00:14:55.284, Speaker A: So I write it in the analytical sum that I can write it. This is the p is known to the p. This is the Q's known to the q. And then what I'm interested, actually, is the piece normal of this quantity, not the skews norm. So I have to take the power p over q. Now, p is greater than q, so I have convexity. So I can just lower bound what I'm after just by the.
00:14:55.284 - 00:15:17.034, Speaker A: By the first order correction. So I get this expression. Then all I have to do is I have to compare this expression to this expression, and I'm just going to do it term by term. So I have to come to show that this expression is always greater or equal to this expression. Then you simplify, and you get that you have to show this inequality. And then, you know, you do the calculus, and you see that this inequality always holds. And this is how you prove the reverse hyperconce.
00:15:17.034 - 00:15:43.584, Speaker A: Very elementary. It's really. You can give it to first year calculus students, and, you know, you will. You will be able say again of the classical hydrocontactic. Right? So a proof like that also works for the hypercontract. You can also prove the hypercontractive inequality in a similar way. And I think, actually, that's one of the things that Borel talks about in the same paper.
00:15:43.584 - 00:16:06.780, Speaker A: Okay. So you can, you know, things are reversed, the expansions are different, and so on, so forth. But you can prove the usual hyper contractive inequality the same. Good. So, yes, so this is. This is different inequality. And, you know, as we will see, this inequality.
00:16:06.780 - 00:16:41.788, Speaker A: Okay, so this is related to the sort of the general direction which I want to go for in the next ten minutes is to ask, you know, both the question that Michel was asking and the question that Ron was asking, the same question. You know, are you doing anything different, or is it actually the same? Right. So that's when I read this paper. That's the question that I was asking. Is it really different than the usual hyper conductivity that we know, or is this reverse really different? So, all the arguments I presented so far, and the answer that I, you know, my answer to Michel indicates that it's, in fact, the same thing. So let's keep. No, this is.
00:16:41.788 - 00:16:51.580, Speaker A: Right. So this is a philosophical question. Let's keep it in our head for the next five minutes. I'll give you some applications, and then we'll start answering this question. Okay, so that's. That's what I want. It's a good question, but that's.
00:16:51.580 - 00:17:18.744, Speaker A: That's what I'm going to talk about. Okay, so let me. Let me just give you a couple of applications. So, this is, this is where I got to learn about borel result. There's a joint work with, again, at least two of the people are in the audience. So John paper with Ryan O'Donnell, Odette Regev, Jeff Stife, and Benny Sudokov. Well, we use this inequality, and we use it in the setup that I think might be interesting to people who are interested in probability in general.
00:17:18.744 - 00:17:58.804, Speaker A: So we looked exactly at the setup of points discrete cubed that are all correlated. So rho is going to be e to the minus t, the expected value of x I, y is e to the minus t. And for the application that we had in mind, we needed statements of the following form. We needed to say that if a and b are two subsets of the discrete cube that are not too small. So let's say each of them is of measure, at least epsilon, then the probability that x is in a and y is in b is not too small either. Okay. And, in fact, what you can tell, you rediscovered this, right? So, among other things, we rediscovered it even though our proof wasn't.
00:17:58.804 - 00:18:33.918, Speaker A: I mean, I think there was an evolution of the proof, which ended by the fact that we realized that Borel proof is the nicest. Okay. But, you know, it's like we realized, how nice is the proof? You know, we were a little disappointed that it was written in so few lines right, at some point. But, you know, if the proof is very nice. Okay, so what do we get in the setup? So we get in the setup that, you know, if you have. If for any two sets that are not too small, the probability that x is in a and y is in b is going to be at least epsilon. Well, it's at least the product of the distribution, the probability.
00:18:33.918 - 00:18:54.394, Speaker A: This is what you would want. Epsilon squared. But you lose a little bit. What do you lose? You lose a factor of one minus e to the minus t. Right. But at least if the sets are relatively big and the time t or the correlation row are not, the correlation row is not too close to one. You get that the probability of this intersection events of correlated x and y is pretty large.
00:18:54.394 - 00:19:19.378, Speaker A: Are there sets a and b which meet this bound? Say again, are there sets a and b which meet this bound? Yeah, like. Yes. So essentially this bound is tight for opposing hummingbirds. Second in the exponent. Second, the constant in exponent is not Tight. Right. I think the const, actually, I think the correction is actually of second order, but okay, I don't REMEMBER.
00:19:19.378 - 00:19:44.178, Speaker A: It's been, you know, dad is younger than me, remembers better. It's pretty tight. So of course if you get t, t goes to infinity, you know, you expect to recover the product and you recover the product. But the question here is about a fixed. Very good. Okay, let me just mention that the proof of this inequality for this result is very, very short. You take Borrell's result, you apply reverse order inequality.
00:19:44.178 - 00:20:33.024, Speaker A: So reverse order inequality gives you this inequality again, the inner product of g with t t of f is at least f norm p and gnome q. And now it holds for a symmetric condition in p and q. So the condition is t has to be greater than essentially minus one half log, one minus q, one minus p. And the point about this inequality is that if you think about q and p, how this probability x belongs to a, y belong to b, how this depends on t at all. So t, the t depends on t is here, x and y are y dependent, x and y are rho correlated, where rho is e to the minus t. So x is equal to y is probability one plus e to the minus t over two xi equal to y I. Okay, so when t goes to infinity, x and y are becoming independent.
00:20:33.024 - 00:21:07.876, Speaker A: And then you get the trivial bound epsilon squared, which is the comment of low end that, you know, you get epsilon squared. And you know, the closer t is to zero, the higher the power of epsilon is. So the more correlated x and y are, the higher the power you get. But still you get some, some result stimuli hyper tube expansion. Right, right. So it's some statement about expansion. And one of the things, it's clear that I'm going to run out of time, but one of the things I wanted to do, maybe I'll do in a few studies, I'll talk about comparison of this of say, with the expander mixing level.
00:21:07.876 - 00:21:34.928, Speaker A: Okay, so how does this compare with the expander mixing level? So let me get to that. So let me just say, the proof of this is pretty, pretty straightforward. The main thing that you have to notice is what the hyperconductivity gives you. It lets you choose Bosnam Q and P to be positive. Once the norms are positive, you get the result that lets you actually talk about probabilities of the set. If it's negative, you don't really get, okay, let me skip this. That was the original application, why we cared about this problem in that paper.
00:21:34.928 - 00:22:02.410, Speaker A: But let me skip it. And again, this is something that I was planning to skip. But you know, one of my favorite area of applications of this result is in what's called quantitative social choice. This is an area that was essentially created by Gill. And in most of the results in this area, somehow when you want to prove this strong result without neutrality, you have to use reverse hypercontectivity. And these are the results that are highlighted by red. That's the result we actually need.
00:22:02.410 - 00:22:26.534, Speaker A: So that's a very nice area of applications of this result. Okay, so let me go back to the philosophical question. This was to convince you that it's useful for something. It's a probabilistic tool that's useful for something. So let me go back to Juan's question and to Michel's comment, even though Michel at least knows the answer. So you look at these two inequalities. Sorry again for bonhomie, for the swap, I should do swap again.
00:22:26.534 - 00:23:02.858, Speaker A: So you look at these two inequalities, and, you know, you look at the proofs, and what is the conclusion? The conclusion that I drew for a number of years is that hyperconductivity and reverse hyperconductivity have to be exactly the same thing. You know, the inequalities are of course different, but maybe there's some duality. And once you prove one, you prove the other. And, you know, I was just looking for duality proof, such as, you know, once you prove hypercontructivity, you get reverse hyperconcticity and vice versa. In particular, one thing that I didn't want to do is I didn't want to recover the result of pavel. And for other probability spaces, I said, well, you know, we know the results there. There's going to be some duality.
00:23:02.858 - 00:23:18.904, Speaker A: I'm just going to smart and find this duality and then I'm going to recover all of them. Resource. Okay, so that was my plan for various years. And the surprising, I think, for both of the results of Borrell that I'm going to talk about. One of the nice things is that the results are nice. But there's also a surprise. The surprise is that this is false.
00:23:18.904 - 00:24:02.078, Speaker A: So it turns out that for reverse hyper conductivity, the inequality actually doesn't depend at all on the underlying space. So you get a uniform reverse hypercontractive inequality, no matter what probability space space you're looking at. So here's the result that we got with Schistofen armad. So you look at any product specs, omega to the n, any positive function, then you have a reverse hypercontractive inequality of the form that we want, with no log one over alpha. Here, there's no dependence on the smallest atom on the space. This just holds completely uniformly. So this is indication, I mean, so this was counterintuitive.
00:24:02.078 - 00:24:41.764, Speaker A: And the first indication of that was saying Arnab was doing some simulation. We were trying to actually find the constant, and the constant didn't go to, you know, it didn't have any dependency on alpha. And this was, of course, an indication that we have bugs in the program and so on, until, you know, we suddenly realized there's something different going on here. So, reverse hyperconductivity is really different than hypercontactivity in the sense that it doesn't depend on all on that. Let me try to explain. If f is a constant function, then there is a quality, right? Right. So for all of this, all of this inequality, whenever you have a constant, in this case constant positive function, the two sides are just going to be this constant, right? So that's just even this non norm, satisfy this property.
00:24:41.764 - 00:25:27.558, Speaker A: Okay, so the result is clear. Okay, so, okay, so I already said that. So I'll try to tell you a little bit about why this is true. But let me tell you, let me relate it more generally to things that we know. So, this doesn't just hold for product spaces, it holds for general Markov summing groups. And for instance, we know the following two results. Whenever you have a log sub inequality with constant c, or one log subleve constant, which is also called modified log suble of constant with constant c, then you have reverse, reverse hypercontractivity in the sense that if the time is greater than log one minus p over one minus p with this constant over four, then you have the reverse inequality for every positive function.
00:25:27.558 - 00:26:02.842, Speaker A: So this is a diff. This is. Okay, this result actually implies the previous result I told you, because it turns out that the one log sublevel of inequality, or the modified log symbol of inequality, holds for every simple probability space for every simple operator. So this result implies the previous result. But it, some output puts it in context, and it shows that the reverse hypercontactive inequalities are weaker than the standard log sub eleven inequalities, and they are also weaker than the weaker inequalities that are called modified or one log sub eleven. That's another part of the picture. And here's again the application.
00:26:02.842 - 00:26:39.290, Speaker A: So now you have applications like that. Whenever you have one log sub or two log sub inequalities, you have exactly the same kind of applications that we had before. There's a missing two factor compared to Borrell result, which correspond to having this factor two here that we lose. But this now holds for general probability spaces. And just for comparison sake, let's compare it to what's called the expander mixing lena, which is another way of saying that if you have two sets, the probability that x is in a and y is in B is allowed. So this is one bound for two sets of size. Epsilon you get this bound is another bound.
00:26:39.290 - 00:27:10.894, Speaker A: That's the expand. The next thing I might tell you, the probability that x is in a and y is in B is at least the product minus an error term, or it's equal to the product up to the error term. What is the error term? The error term is square root of the product of the probabilities of the set times e to the minus t times the spectral quantity. So this what you get in the Planck array inequality. So when you look at these two inequalities, which one is better? Depends. Very good. That's a good answer.
00:27:10.894 - 00:27:24.854, Speaker A: I wanted you not to answer at all, because it's not clear which one is mixing. Lemma applies in much greater generality for graphs and. Right. So what I'm talking about also holds whenever you have. So, okay, so let's, no, so let's do, let's do it. Let's do it. Let's do this.
00:27:24.854 - 00:27:42.532, Speaker A: Let's compare the setups again. So the setups are completely general. But in one, one case, I'm going to measure things according to the spectral gap. In the other case, I'm going to measure it either according to the log sublev constant or according to the modified log sublev constant. Right? So we know that these are worse than the spectral gap. But I can do it in any setup that we want. Right.
00:27:42.532 - 00:28:28.450, Speaker A: I can talk about a Markov chain, I can talk about some graphs. I can do it in any setup that I want. And I can compare these two inequalities. This inequality is a spectre, is the spectral inequality, and this inequality is the inequality that you get from this log's overlap constant. So the difference between them, sometimes this is better, this bottom one is better than top one, and sometimes the top one is better than that, the lower one. What is the relationship? So essentially what is going on is that if the set a and b are small, where small is compared to what we, the spectral information that we have on the gap, then this, this error term is going to be much bigger than Pa Pb and you're not going to get anything. And then this inequality gives you something that even works if the sets are tiny, even if they're very, very small, you get the, this quantity is positive and, you know, and you get quantitative bound.
00:28:28.450 - 00:29:11.716, Speaker A: However, if the sets are large in the sense that the square root times e to the minus t over d is much smaller than the product of the measure of the sets, this typically is going to be a much tighter result about the probability of x in a and y in b. Okay, log Sobolev is enough to imply this, right? So that's one of the things that I quickly said in the previous slide. Log sublev is enough to imply it, but in fact, even a weaker population, which is called modify loxobelv, is enough to imply this. So Loxoblev is in some sense equivalent to hyperconductivity. No? Right? So log sublabel is equivalent to hyper conductivity. But one of the messages of this talk is that reverse hyperconductive is implied by something weaker. So it's implied by Luxor level implied by something weaker.
00:29:11.716 - 00:29:41.606, Speaker A: That's called modified log sub inequality. In particular, again, the example of just a space of two points where you have a big measure on one point and a small measure on another point. Here the log sovereign constant is deteriorating as the space is becoming more and more unbalanced, while the modified log server level, the reverse hypercontuctivity, remain the same. Or, you know, there's a uniform lower bound. Okay, so that's exactly, exactly what I'm trying to conclude. Good. Other questions? So just add one comment.
00:29:41.606 - 00:30:21.770, Speaker A: The thing is that in the, in expanding mixing level, you usually have an expander and an adjacency matrix. And you're comparing, you're proving a lower bound for this. In the above case, you're actually changing the chain and you're looking at the continuous chain, right? So, so you need to, if you really, so I already, the translation is done here. This is the translation of the expanded mixing limit to the continuous time Markov chain. So the comparison is a fair comparison. Right? So it's like I'm due to the continuous time outcome chain, and this is the continuous time outcome. The expander mixing lemma doesn't really care if you're in continuous time or in discrete time.
00:30:21.770 - 00:30:45.550, Speaker A: The proof is the same proof, the statement that is, you know, very closely related. So in order to make the comparison, since this talk is in continuous time, I made the comparison in continuous time. But that's a good point. Right? So usually when people think about the expander mixing lemma, they're used to, you know, you read about it, discrete setup. So, you know, but it's the same, right? But it's the same, it's the same in continuous setup. That's a, that's a very good comment. Yes.
00:30:45.550 - 00:31:30.550, Speaker A: So a few minutes ago, you were saying that you were hoping at one point in your life that the hypercontivity and reverse hypercontactivity would turn out to be equivalent. Right? From what you said up to now, is it still feasible that there's an easy way to deduce the reverse from. From the non reverse? Right, so this. Okay, so if I. Okay, so, so this is what the next, you know, okay, maybe I'll make a vote at some point, you know, so the next slides are supposed to tell you why can you deduce one from the other? And what is the relationship between the usual hypercontactive inequality or the usual oxygen inequality into the inequality that we're talking about right now? Okay, so this slide, skip, it talks about situation where you look at Markov chains. This is just for your valve, global dynamics in high temperatures. For other people, you talk about crowd shuffling.
00:31:30.550 - 00:32:36.236, Speaker A: So this inequality holds in this setup, which means, again, if you have one big set, you running some markov chain in stationarity, you ask, what's the probability that at some time, in some big, but of measured epsilon set, in some other time, I mean, some other measure epsilon set, you get uniform bounds that just depend on the log suble of the modified log sub constant. So this really holds in the more general setup, that type of production. Right? So these are two examples that are mentioned in our paper. And in fact, Kshystov also came up with a very beautiful queuing example. Bucky. Okay, so, in fact, the main thing that we're proving, and this answers to some extent, the question that they would ask, is asking, what is going on here? So, so if you want to talk about. To think about it at the high level, one way to think about it at the high level is the following way we already know, and, you know, we had to do a little bit of work because I don't think anybody thought about it too seriously in the context of reverse hypercontactivity.
00:32:36.236 - 00:33:28.600, Speaker A: But we already know that log survival of inequality and hypercon contractive, or in our case, reverse hypercontractive inequalities, are the same. So this kind of analogy, or the fact that you can take, take the derivative of the semigroup and relate it to holding the more general setup that we're talking about. And the main, the main thing that we actually prove in our paper is to prove that these inequalities are monotone, right? So there's going to be log sub wave inequalities for every norm p. And what we're proving is that this is monotone in the interval between zero and two. So, if you have log sub relative inequality for p and you have log sub relative inequality for q, then if p is greater than q, then the p one implies the q one. So, this is one of the reason why reverse hypercontectivity is easier than hyper conductivity. Hyper conductivity.
00:33:28.600 - 00:34:08.474, Speaker A: Hyper conductivity is two log sub eleven quality. And what we're showing is that what we will show is that, for instance, that log one sub eleven equality, implies reverse hyper contractibility. So that's the main idea of the proof. So, okay, let me, I always like to have a vote in the, in the talk, right? So I have two, there are two options. I can tell you a little bit about how we prove this stuff, or I can tell you about plurality, stablest and the standard y. I will prove this stuff. Okay, polarity is tables in the standard y.
00:34:08.474 - 00:34:23.846, Speaker A: Okay. It's about equal. So I can decide. Very useful. I won't tell you how we prove this stuff. I'll just, okay, maybe I'll just make two points. So, one point is that you have to come up with the right definition of p log server level of inequality.
00:34:23.846 - 00:35:32.058, Speaker A: And there is, the definition is standard in the literature is enthalpy less than a constant times the duration of form of f to the piece minus one f the main. Since there's something really nice, if you choose the right normalization, and the right normalization is that you multiply the constant, there's an absolute constant, and then you take some expression that depends on the norm. This expression is p squared over four, p minus one. And only if you take this parameterization, you get this monotonicity that I talked about. So you have to take the right definition, and in the right definition, the constant in the log sub level of inequality relating the entropy to the Dirichlet form needs to be some function of the norm p that you're looking at. Maybe it's worth saying it's a product of p and the dual of p. Right? So one of the things that you'll see here is that this definition is one of the reasons how you can derive it is you can see that this definition has the property that you satisfied p logger level if and only if you satisfy the dual one, which is also a very aesthetic form and also a reason why it's enough to look at the interval zero to two.
00:35:32.058 - 00:35:56.924, Speaker A: You don't care about the negatives. The negatives are just the duals of zero to one. You don't care about nodes greater than the duals between one and two. And this shows you that this may be the right parameterization. And then what you have to prove is you have to prove a statement like that. So this is, this is a statement stronger than what's called the structural estimate. So you have to prove something about the Dirichlet energy.
00:35:56.924 - 00:36:37.034, Speaker A: But when you look at p and q norms, we have to prove that if p is greater than q, then for any number between zero and two, you have to prove that this Dirichlet form is less than that Dirichlet form. And this is, of course not quadratic, because there are all these powers. But the Dirichlet forms act in a quadratic way. So this reduces to actually proving a statement about every two numbers a and b. So you have to prove that for every two numbers a and b, you have this beautiful inequality, and then you prove it. And then you get this inequality, and then you plug it into the usual machinery, and the usual machinery will tell you this is the basic two points inequality that you have to prove. This is just an overview of what's going on in the setup.
00:36:37.034 - 00:37:08.562, Speaker A: So let me tell you a little bit about the other thing that I want to talk about. So this was Borrell result. I want to talk about another Borel result. So, half spaces minimize the gaussian surface area. Borrell's theorem says that if you have two row correlated random variables, then if you look at any two functions f one and f two, the correlation between f one on n and f two of n is less or equal than the correlation between the corresponding half spaces. So this will do functions f one and f two. This is two function h one and h two.
00:37:08.562 - 00:37:33.008, Speaker A: The expected value of h, this is a vector, is the same as the expected value of f. So f one has the same expected value of h one. F two has the same expected value of h two. And that's the Borrell reserve that Ron was mentioning in the previous talk, when we talk about two sets. So that's Borrelia's oil, and it implies modulity. Stables, which I want to formulate now for a number of years. I'm obsessed with the following question.
00:37:33.008 - 00:38:07.942, Speaker A: What happens if you divide the space into three parts? So two parts we all understand very well, but what happens if you partition the space into three parts? And the only result that we know, which sort of gave us hope that something can be done for three parts, is what's called the double bubble theorem in gaussian space. So this is from about ten years ago. There are many authors on the paper. This is why I didn't put it next to, to colonel. I call Adams. Also, you notice it's not alphabetically. It's clear that something interesting was happening in this paper and so on and so forth.
00:38:07.942 - 00:38:40.218, Speaker A: All of these people, authors on this paper that show that the double bubble is optimal in gaussian space. Except in that case, it's called the standard y. So what is a standard y? A standard y is a partition of rn into three parts using three hyperplanes, half hyperplanes, all meeting at 120 degrees. Okay, so let me just draw. I should have done it before the talk. This is the standard y. Where the center is will determine the measure of the three sets.
00:38:40.218 - 00:39:14.544, Speaker A: And what did this guy prove? They addressed the isotherimetric problem. So you want to find a partition like that which minimizes the surface area. And they said the standard y minimizes the surface area. Well, as long as all of the pieces are essentially of the same measure. So it shouldn't be that the difference between any pieces in measure is more than 0.04. What is this the question again? What is e one, e two, e three? Okay, so this is how, I'm sorry, this is how I define partitions, so it's easier to do it in vector notation. So I'm going to have three parts of rn.
00:39:14.544 - 00:39:52.710, Speaker A: One of them is going to map to the unit vector one, one of them is going to be mapped to the unit vector e two, and one of them is going to be mapped to the unit vector e three. What I'm going to do is I'm going to measure the gaussian surface area, which I'm denoting by gamma plus. So gamma plus, this is the surface area of this partition. And I'm saying that this surface area is bigger than the surface area of the standard y, where y is one of these y's. But this y can be shifted in various ways when I choose the. Yeah, that satisfies that the measure of each part is the same as the measure of the corresponding part. In f, the surface area is the area of the boundary, the area of the boundary, the area of the boundary between the three parts.
00:39:52.710 - 00:40:26.924, Speaker A: Okay, so in this case, just going to be this one dimension, co dimension, one area, this co dimension, one area, and this co dimension, one area, the sum of these three areas. That's what it's going to be. Okay, so this is related to the resolution of the double bubble theorem, which was a very big, big deal. And in fact, they use a lot of the technology that's been, that's being used there. They couldn't quite prove that the standard y, which in other words, you would call a plurality, minimizes the surface area, but it does minimize the surface area if the parts differ by its most focus. Okay, so that's what they proved. That's all we know.
00:40:26.924 - 00:41:16.764, Speaker A: And of course, you know, given this, there are various natural questions. First of all, just for the Gaussian as a parametric problem, what happens if the partition is not on its balance? Is the similar statement too, for no stability? And is what we call plurality stable? Is it true for discrete probability spaces? Okay, so this is a question that I'm asking myself for a lot of time. And in the last two weeks, we realized something. So we realized that the kind of statement that we have in the case of Borel can, cannot hold. So this is, again, joint discussion with Stephen and Joe. We realize that these kind of claims that we can make in the case of the theorem of borel do not hold in the setup that we are talking about now. So now we're not going to talk about one partition.
00:41:16.764 - 00:41:44.456, Speaker A: I'm going to talk about two partitions. So I'm going to consider a pair of partitions. The pair of partition is going to be given by f one and f two, and one partition. I want to be a partition to three parts each of measure one third, one third, one third, one third. And the other partition, I want to be partitioned into two parts, are going to be of size one half, and one part is going to be of size zero. Okay, so we're trying to do something like borrow resolution. What did we do in borreliasult? We said, ok, I'm going to take two partition of the space to two parts.
00:41:44.456 - 00:42:28.834, Speaker A: In one partition, the measure may be 50 50, in the other partition, maybe the measures are going to be 75% and 25%. What are the best partition? And Borrell said well, the best partition is to take hyperplane here and a parallel hyperplane here. One measure is going to be one half of the space, the other measure is going to be three quarters of the space. And this is the optimal partition. Now we're asking a similar question in partition in three parts. So if we believed something like Borrell resort would be true in the y, we'd conjecture that, you know, the partition here would be the standard y, the y are the origin of this form, and the other partition would be a y, which is partitioning into two, which is just a partition into two half, half spaces. That would be the corresponding result to Borrell results in the case of two different partitions.
00:42:28.834 - 00:43:05.430, Speaker A: Unfortunately, we can prove that this is not the optimal partition. Isn't it supposed to be like one partition? You want the first point in the noisy partition. So that's, I mean there are various versions, but all of the proofs of Borrell result that we know also also holds for two different positions and also wait for five more minutes. I'm also going to address this question in another way, but just for now, what all the proofs that we know, we know for two partition partition is actually a very easy proof that this is not the optimal partition. So why this not the optimal partition? So this is very clear. I mean, this is the optimal partition. I can just draw what the optimal partition is.
00:43:05.430 - 00:43:27.274, Speaker A: So here's the optimal partition. This is going to be the three parts, part one, part two and part three. Why is this the optimal partition? Well, we can prove it using borrell results. We can prove that this is the optimal partition. But it's also clear, I'm telling you, I'm running election between candidates one, two and three. And at the end of the election I'm just going to ignore candidate number three. I'm going to throw them to the garbage.
00:43:27.274 - 00:44:00.024, Speaker A: I'm going to say that they were not born in the US and actually they cannot be elected. Okay, that's what I'm going to do. And I'm asking you what's the most, the best way to elect the two candidates? I have to assign probability one third to each of the candidates. So if there's a great majority for one, I'm going to say that one wins. If there's a great majority for two, I'm going to say that two wins. And in the cases that are undetermined, this I'm going to assign to three because anyway, I know that three is not going to win, and this is the most stable partition and it's more stable than these two y's. So you can prove it.
00:44:00.024 - 00:44:36.930, Speaker A: And, you know, the proof shows that this is, you know, whatever the extension of Borrell resort would be to two parts doesn't hold for three parts in the setup. But an India, as a professional complaint, he said, oh, but usually when we talk about stable, we want the same partition twice. And in many, hardness of approximation sets up. That's sometimes what you want, not always what you want. So this is also not true. But we only know it if the partitions are almost balanced and always go to zero. So now I'm going to partition the space once, and I'm going to take two correlated Gaussians, and I'm going to ask the two correlated Gaussians to land in the same part of the space.
00:44:36.930 - 00:45:27.704, Speaker A: And the claim is that among all partitions of rn to three parts, such that the first part has a measure, one third plus two eta, the second measure, the second, and the third part has measured one third minus eta. One third minus eta. The partition maximizing the probability that you fall into two parts is not given by a standard y, assuming that the correlation law is sufficiently smart. Unfortunately, some smart and not so smart people conjectured that plurality is stable. So I'm involved in this group, so you can guess who are the smart and not smart. This still doesn't contradict the conjecture that plurality stables, because the smart people in the plurality stables group made sure that we only talk about balance partition, where all the measures are exactly one third one. So this doesn't contradict the plurality of stables conjecture, but it contradicts something very, very close to that.
00:45:27.704 - 00:46:05.864, Speaker A: What does it mean in election terms? In election terms, it means, for instance, suppose you're running an election and you know the bias between the two candidates. One of them has a little bit more chance than winning than the two others. And you're asking what's the best way of electing among the three candidates? It's not going to be a plurality election. It's going to be something else. And in the binary case, majority, majority is always if the biases are moved in which direction majorities are distributed here, something much more delicate is going on. Okay, so let me tell you why this is true, and then I think I'll run out of time and finish the talk. So again, this is true.
00:46:05.864 - 00:46:47.494, Speaker A: This is true only when the quantity rho is sufficiently small and the correlation is sufficiently small. And basically what you do is you do an expansion when row goes to zero. So when you do the expansion of when rho goes to zero, as many of you know, the correlation between the f of x and f of y. Well, the first thing is just coming from the zero level Fourier coefficient, which, you know, I'm writing it this way. This is vector notation, right? So this is really the zero level Fourier coefficients. And then there are going to be the first order of Fourier coefficients. What are the first order Fourier coefficients? Well, the first order of Fourier coefficient coefficients are just the center, the distance of the center of masses of these partitions from the origin squared.
00:46:47.494 - 00:47:09.518, Speaker A: So this is the standard y. And it turned out that if you shift the standard y a little bit, then the center of mass is shift in a way that doesn't satisfy the property. So the standard y, we have the property that all of these angles are 90 degrees. I don't know if I see. So this is 90 degrees. This is 90 degree. And this is 90 degree.
00:47:09.518 - 00:47:33.690, Speaker A: And this is, this is 90 degree. So all of these are 90 degrees. And it turns out that if you shift the standard y a little bit and you look at the center of masses, it's not 90 degrees. A little more. And if it's not 90 degrees, it turns out that you can make a little bit of modification of the sets that will increase this quadratic quantity while preserving the measure of the set. So it's not optimal. Okay.
00:47:33.690 - 00:48:06.602, Speaker A: So what is the conclusion from that? The conclusion of that is that if plurality established is true. It's true in very, very special cases. And all the proofs that we are used to think about proving inequalities like that, do not we, you know, really, we don't know a proof that is so specialized. Probably, if it's true, there are some very interesting phase transitions that relates to row. And, you know, you know, some of the various parameters come into interplay in a much more interesting way. This project looks even more ambitious than it looked ten years ago or five years ago. And I started thinking about it.
00:48:06.602 - 00:48:59.792, Speaker A: Okay, so I stop here. Thank you. So is there some argument to say that at least the boundary boundaries are flat or the sets are convex or something like that? No, I don't know. Yeah, I don't know any argument. I think maybe that's the way to start thinking about it, really, in a more basic form about what can you characterize? And one of the things that one can try to do is to follow the literature of the double bubble theorem proof, which is ten or 15 papers to try to see claim by claim, which claim you can prove, because this is how they start. A lot of the work is just connectedness and proving that the faces are, you know, faces are nice and so on and so forth. But, you know, it's something that one can try to do.
00:48:59.792 - 00:49:54.280, Speaker A: But I don't know any of the statements currently. So the classical hypercontructivity gives you various tail estimates on chaos. If you reverse the hyper contractivity, do you get some information about the estimate? Right. So it's something I thought about, and I don't know any, I don't know any application, all applications that you should. So, one thing, it's related to maybe some question that was asked after Hamid talks. One thing that you have to take into account is that in many of these tail estimates on chaos, we know that they become worse and worse depending on the space that you're looking at. Like if from gaussian you move to south other space, right? No, but what I'm saying is that whatever you do with reverse hypercontivity, the result will not depend on the space if you just use reverse hyperconductivity on its own.
00:49:54.280 - 00:50:25.174, Speaker A: Right. So the kind of estimates that you're looking for different than the usual estimates that we know that do depend on the space. And one candidate I was actually looking after the talk was this bourgeois statement about comparing the q norm of this. I mean, so this is a kind of statement that doesn't depend on the space. So statements like that, which we are usually not. So this estimate, this kind of estimate, we may try to prove this way, but I don't know any, any proof that actually goes along this line. It's a very natural question, an idea.
00:50:25.174 - 00:51:09.746, Speaker A: Maybe there should be application of the reverse to eco ellipticity because it seems that you have something like a density, since you have amino ac. I don't know. I don't know. I was wondering if you move, you have your definition of the boundary, which is a total, but remember that in the top it's not measured like that. It's more a max of the mean, right? So maybe there is something going if you change the definition, not taking the sum you mean here for this problem. Right. But for this problem, I mean, the question about the stability is actually coming.
00:51:09.746 - 00:51:39.814, Speaker A: I mean the application is very, you know, the reason that people conjecture that plurality is stabless is that it's a very natural extension of the fact that majority stables. Right. If you think about voting, majority is the best. You know, you think what is the extension of majority to other voting setup? It's plurality. Right? So, I mean, this conjecture, I mean, it's true that one can think of alternative questions, but, you know, there was a reason for the question being asked the way it was asked, so. But it's, you know, still, one can explore other directions. But that was the reason the question was asked this way.
00:51:39.814 - 00:51:44.554, Speaker A: Okay, thank you.
