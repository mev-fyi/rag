00:00:29.654 - 00:00:52.074, Speaker A: Hi, everyone. I'm Shafi Goldwater from the director of the Simon Institute for Theory of Computing. I'm delighted to have distinguished guests with me today. And we're going to talk about a fascinating subject, which is communication in non humans and non mammal. And I'd like to start with introducing the speakers and the panel. Start with Lilac. Lilac Adani is from Tel Aviv University.
00:00:52.074 - 00:01:31.980, Speaker A: She's a professor and a pioneer in the area of planned hearing and speaking, which is going to be the topic of our little discussion today. Welcome, Lilac, and thanks for joining in on Friday night in Israel. And our next speaker is David Gruber, who is professor of biology from the City University of New York. And he's also an emerging explorer at the National Geographic Society. And our last introduction is Michael, Michael Bronstein, who is now a professor of machine learning and pattern recognition at the Imperial College of London. He's also the head of the graph learning research at Twitter. And along with David and me, he was also a Radcliffe fellow.
00:01:31.980 - 00:02:34.666, Speaker A: So we all met at Radcliffe a few years back. And Michael is a researcher on many, many things having to do with machine learning. Computer vision and fascinating techniques apply to all problems, and in particular, the problem that is in the topic today, which is how can we detect and understand non humans communicating? So I guess we met the first time, all of us on screen, at a workshop that we organized in the Simons Institute on this topic. And all of you gave talks, and also we had talks about people talking about elephant communication and how birds flock together through some mechanism of communication and many other fascinating topics. And I guess maybe I'll start with, with David, who has kind of been pushing us along, at least Michael and I, and tell us a story about how we got together and story behind the workshop and the project that we're doing together.
00:02:34.850 - 00:02:52.744, Speaker B: Yeah, sure, sure. Thanks, Shafi. And thanks for the Simons Institute for hosting such a, such a fun symposium. Well, you're really the one who started this. And in one sense, when. When I was. When we were Radcliffe Fellows in 2017, 2018.
00:02:52.744 - 00:03:56.304, Speaker B: It's a really beautiful program where it brings together 50 different scientists or people working in the humanities, and we all get an office there, and we work on our different projects. And one of the projects that I had been working on at the time was designing animal eye cameras, so cameras that can see from the perspective of marine creatures. And I had done this for a shark, and I was working on this with a turtle. And it was during the Radcliffe year that I had read a book called deep about sperm whales and about this click communication and about free divers that were interested in holding their breath to be able to study whale communication. And I was just starting to think about sperm whales and had gotten a bunch of sounds. And that was when one day when you would come into my office and I was playing you these sperm whale sounds, and you started asking a lot of questions, and at the same time, you had organized a seminar at Radcliffe. It was like a machine learning.
00:03:56.304 - 00:04:49.316, Speaker B: I call it machine learning for dummies, because it was people that didn't know about machine learning. But you brought amazing people, Michael, and people from MIT that were working at groups to kind of explain it to the non specialists, and including Martha Minow was there with us and thinking of how to use machine learning to go through all the law books and make it possible to offer low cost, high end legal advice. But we all came with our different ideas of how we could use machine learning. And it was so beautiful to have some of the best people in the world explain it to us and kind of think about our applications and then open up the hood of what's really underneath the hood of machine learning. And I think that was really fascinating. And at one of the talks, you would ask me to play some whale sounds to the group. And I had my computer out, and I took the computer out and played sperm whale clicks.
00:04:49.316 - 00:05:27.366, Speaker B: But I just remember that I was a little bit embarrassed. I almost felt like I didn't even belong in the room. And I played the clicks, and everyone kind of listened intently. And I think it was Michael and a few other people that were like, ooh, those are very interesting. Do both the males and females make those clicks? It's almost like a morse code. How many of these clicks do you have, and how much data is available? Is there annotations? Could we use supervised or non supervised? And that set me on a journey to go back within my marine biology community and find out those answers. And that led to one of our first studies where we applied advanced machine learning on sperm whale clicks.
00:05:27.366 - 00:06:48.734, Speaker B: And Michael could talk about that. That's really the genesis of this workshop and how we all came together. And then that led to submitting a proposal to do a large scale project, and that we're just super thankful that that got TeD audacious funding, which is just starting. So this is a brand new project that's at its like most baby stages right now, but it'll be five years, and there's about 20 scientists involved, and we are going to even take two years to be establishing our core whale listening station that will cover 20, allow us to hear which whale is talking, to which. Twenty four, seven, and then start building up the data sets using a combination of this core system and tags that go on the whales to build up a rich data set in the millions or billions that we could then apply some of these advanced tools. And that's where I guess it comes in, of thinking of how we would pipeline all this information efficiently into something like Twitter or a Google and allow it to apply these industrial processes to an animal like a whale or a plant.
00:06:49.534 - 00:07:11.834, Speaker A: Yeah. In fact, Michael, maybe you can tell us more about the SETI project and your role in it. I mean, maybe it was machine learning for dummies, but you certainly were an expert and. Yeah, me too. But Michael was actually sitting there and nodding when everybody was explaining, and he could explain to us further. So could you tell us a little bit about the project?
00:07:14.014 - 00:07:29.306, Speaker C: Sure. So I think. Well, yeah, so I think we are. Well, as David mentioned, it was really him bringing these whale sounds and sharky organizing the machine learning seminar, maybe not.
00:07:29.330 - 00:07:32.546, Speaker D: Necessarily for dummies, but somehow it brought.
00:07:32.570 - 00:07:40.162, Speaker C: Us all together and trying to look from a different perspective on something that has been attempted in the past.
00:07:40.338 - 00:07:41.770, Speaker D: So I would say that, in brief.
00:07:41.882 - 00:08:09.024, Speaker C: Project SETI is now a multidisciplinary collaboration that attempts to exploit modern machine learning technology to understand the communication of sperm whales. And I would not use the term language because it's maybe a little bit controversial, especially among the linguists. And actually, recently I was reading a paper that Kevin, our linguist, the project SETI team, suggested.
00:08:09.184 - 00:08:10.936, Speaker D: It's a seminal paper on the faculty.
00:08:10.960 - 00:08:24.370, Speaker C: Of language by Hauser, Chomsky and Fietz. It starts with this analogy, or this illustration that if immersion landed on our planet, it would be struck by that.
00:08:24.402 - 00:08:25.194, Speaker D: We all look the same.
00:08:25.234 - 00:08:41.621, Speaker C: The remarkable similarity among all Earth's living creatures, essentially, all life we know is based on proteins that are encoded in DNA that can create practically limitless variety of species, individual organisms.
00:08:41.757 - 00:08:44.733, Speaker D: And yet this kind of common pattern.
00:08:44.773 - 00:09:13.986, Speaker C: Or common blueprint is completely absent when talking about communication. So if you look at different animals, they communicate in completely different ways. And actually, this Martian would immediately see that humans are really set apart in the way that they communicate. The complexity, the hierarchy, generative, recursive, practically unbounded expressiveness of our language and whether.
00:09:14.050 - 00:10:07.294, Speaker D: Animals have anything similar to human language. Basically, whether animal communication can be called a language is more or less disputed. So some people say that, yes, animals have communication that can be called a language. Some people say that only humans have language. And we are obviously not the first to attempt to try to understand animal communication. And basically you can wonder why would be the first ones maybe to succeed, really to understand and hopefully talk back to the whales. And this is what I tried to articulate in the talk in the Simons workshop, is basically doing some decisions, some choices, and this is the choice of the species, the technology we will use, and also these multiceporities and the special team that that David has assembled.
00:10:07.294 - 00:11:08.260, Speaker D: So this pieces, we chose the sperm whales. Obviously, there are many reasons besides just having listened to the sounds of these animals in the seminar. And then I also remember one cool evening when David forced us to watch the Star Trek movie where there is this episode where the starship Enterprise travels back in the past to save whales. So these are, seriously speaking, these are mammals with the largest brain in the world. They are several times bigger than the human brain, and they have a lot of similarities in, let's say, social organization to humans. And probably most importantly, and this is really what David mentioned, is they communicate in something that sounds very much like Morse code. So it's a sequence of clicks that travel long distance underwater, and they are organized into very highly structured patterns that biologists call codas.
00:11:08.260 - 00:11:48.260, Speaker D: So it's apparently a very complex communication system with rather simple encoding, which is very appealing for the analysis. Right, because it's just clicks with intervals between them, at least in some first order approximation. The problem is, of course, that we know very little about whales because they live underwater. They dive very deeply where humans can hardly descend. And even observing these animals has been rather challenging. What plays to our favor is that their communication is primarily acoustic. So one of the goals of project SETI is building technology and devices for recording the communication and behavioral information.
00:11:48.260 - 00:12:44.888, Speaker D: And we want to do it over a large area in Dominica and also over long time span, because most of these observations have been pretty short and localized. And I would say that even this is rather unprecedented, because, well, likely to end up with probably the largest civil underwater recording installation in the world, maybe that is only dwarfed by some military submarine tracking devices. So the second thing is that now, probably for the first time, we have the technology, the capability to process and analyze the massive amounts of bioacoustic data that project setting will generate from these sensors. And deep learning is one such technology. So one of the prominent topics in the workshop was the use of modern machine learning technologies. So it is the kind of technology that has revamped the way that we approach natural language processing. It also created a multibillion industry in the way.
00:12:44.888 - 00:13:52.742, Speaker D: So if you think of Google Translate or Siri that you use on your iPhone, it is all powered by deep learning. So for example, modern NLP methods allow to learn language models from scratch using unannotated data. And even if you think of machine translation, which has traditionally required a parallel corpus of text in different languages, kind of rosetta stone can now, since couple of years, be done in totally unsupervised way. And finally, the third component is really the brilliant multidisciplinary team. And you can think that building the hardware that will allow to record the whales, to process their bioacoustic data, to train machine learning models and interpret these results, requires synthesis of skills and a team of biologists, computer scientists, linguists, experts in marine operation, because it's completely non trivial to deploy and operate such devices underwater. And this is probably what is so special about this project. And I should say that, frankly, I feel really humbled to work with such an incredible brainpower that David has managed to assemble together.
00:13:52.742 - 00:15:08.016, Speaker D: And for me, this is really once in a lifetime project, a moonshot that probably doesn't happen in a career of a scientist. So I'm really very proud to be part of it. And maybe taking a step back and thinking of the longer impact of project SETI that goes beyond what probably we can even imagine now in the wildest of our fantasies, and excuse me for maybe a little bit arrogant analogy, if you think of other groundbreaking large scale scientific endeavors, such as, let's say the Human Genome project, it eventually costs more than $2 billion to sequence the DNA of a human being for the first time. So if you think of the length of the DNA, it's more than $1 per nucleotide. And you may wonder, why was it such a big deal? Because it didn't really answer a specific question. But the groundbreaking impact was really creating the technological capability and laying the foundations to enable investigation of new questions that continuously arise from these data rich sources. And thanks to this technology, two decades later, now we have previously unbelievable capabilities, whether in medicine and biology, and really the sky is the limit.
00:15:08.016 - 00:15:34.404, Speaker D: So I think the long term impact of project SETI is to try to deliver these technological foundations that will enable the study of non human species communication. We will start with sperm whales, but maybe it will eventually diverge into other marine mammals and animals in general. And maybe it's not only us, but will encourage the academic research community to do it.
00:15:34.784 - 00:16:12.498, Speaker A: I just want to pick up on something you said. Obviously, people, when they talk about machine learning, they talk about application to medicine, to advertising. Quite a difference. But in any case, they talk always about applications that have some sort of, besides global, important topics. They have profit. And what's beautiful to me about this project is that this is really about fundamental science. Understanding how animals communicate, how this other form of life manages to accomplish the goals that they accomplish is beyond sort of, you know, an interest group of one or another.
00:16:12.498 - 00:17:09.654, Speaker A: There's some fundamental thing that we all want us to stand. I think even as children, you have such incredible curiosity about these animals around you. And what are they saying? Are they saying anything? So, to me, you said about moonshot. It's like this one project that it would really advance the world, in my opinion, in this most basic form, the knowledge of the world. And it's mostly under the assumption that we're successful. We're not the first to try, but we do have a host of new techniques to our disposal, because there are always questions for some people, is, what's new? Why are you guys going to break this enigma that has fascinated people for centuries? And I think, between what all of you have said, what's new is that there are these tools out there, and there's ways to get data, to collect data, and possibly by collecting vast amounts of data and applying the tools that have been developed which are data driven, we will find out something new. We will be able to crack this code.
00:17:10.834 - 00:18:10.356, Speaker B: Yeah, I think it's about organizing us, right? We're organizing from the beginning. So, a lot of the times I work on projects where you're just given data sets, and you try to make sense of the data sets and three different people collected them, and you'll find some bias in between the different data sets. So, with this is like thinking, having people like you and Michael from the really ground, before we even start collecting the data, to even think about how we will collect this data, because you could collect. You could collect infinite amounts of data, but it's. What are the where and how do we collect it, and then how do we funnel that together? And I think that's one of the really exciting things of. Of piecing this big puzzle together, but thinking from scratch and then watching as we go from year one to. We have five years of funding to see where we get, which I want.
00:18:10.380 - 00:18:41.474, Speaker A: To bring discussion over to Lilac, who I also met sort of accidentally, not in Radcliffe, but in Tel Aviv with another friend. And she was telling me about this research that she was doing about plants and the fact that the plants can actually hear us. And I was telling her about the whales. And it turns out that we have a lot more in common than we originally thought. So, Rilaf, this is, you know, like, wow, yet another dimension. Can you tell us a little bit about your work on plants and hearing talking?
00:18:44.054 - 00:20:03.564, Speaker E: So, I think the question of animal communication is already quite controversial. And then we have an even more surprising question of plant communication. And in particular, we work on plant acoustic communication. So this is work that is still in the first stages. But the first things that we know is that plants can hear in the sense that they can respond to sounds, they can respond rapidly in a relevant way, like if we record the sound of one flying bee and play this sound to a flower, in this case, the evening primrose, then within three minutes, the flower can produce nectar that is significantly sweeter. That means that there is communication, or at least response of the flower to the sound emitted by the bee. And we definitely expect this to occur out of the lab.
00:20:03.564 - 00:22:23.234, Speaker E: And we also, if it works for this flower, it also probably works for other flowers. And if plants can respond to sound, then there is the potential that they can respond to all sorts of sounds that are relevant to them. That was the first step that we discussed long, long ago. And then the question that we are still wondering about, can plants respond to the sounds of other plants? And do plants emit sounds that other organisms can hear and respond to? And here, this was really more similar to the whale example than we expect, because we discovered that plants actually emit cells clicks that we can record, ultrasonic clicks that they do so particularly in times of stress. And then we wanted to say something about these clicks, and to see if we can separate them both from other noises and between different sounds emitted by different species of plants, different conditions of the plants. And here we found ourselves with deep learning techniques and constructing classifiers that happen to be quite similar to the whale case. And we found that, indeed, we can separate very effectively from the sounds of the plant, the sounds of the plants, from other noises in the greenhouse, and also between sounds of different plants, different plant species, and sounds of the same plant species under different conditions.
00:22:23.234 - 00:23:33.414, Speaker E: And now the most exciting part is still open. So, in contrast with the animal case, where we know that there is communication, and we want to understand what exactly they are saying, but the difficulty is recording them effectively and following them, etcetera, with the plants, they stay in the same place. They make a life very easy in that sense. But we are not yet sure if there is grindication. Do other plants respond to these sounds? And another option that we think might be exciting is if other organisms respond to these sounds. Once we know that the sounds are out in the world, and they do carry information, because we can identify, like a drying tomato plant, according to its sounds, only then other animals that are capable of hearing these sounds might do something about it.
00:23:36.154 - 00:24:10.674, Speaker A: So, Rilach, I remember from your talk, also during the workshop that you mentioned, I think it's in connection with your work on evolution earlier, that you felt that the way the land has developed, in some cases was possibly because they want to emit certain sounds, which is something that's probably completely new as a reason for. Why would a plant take a particular shape that might look like a broadcast instrument or a receiver? I thought that was fascinating. Different line of trying to explain why a plant looks like it does. Is there anything you want to say about it?
00:24:13.094 - 00:26:01.484, Speaker E: So I think this is one of my favorite parts, because when we think of flowers, of flowers that are large and colorful, and we think that they invest in catching the eye of the pollinator, and that's definitely part of the business. But then, as the size of the flower, and especially the structure of the flower, also affect its ability to be an effective ear in that sense, it is not exactly like an ear in any other sense, but this is the part of the plant where airborne sounds turn into vibration of the tissue. And then now when we see a flower that is large and has a roundish shape, it might have been under two regimes of selection. One selection acting on how it would look to the pollinators, and another acting on how good it is as an ear. If it is a more effective ear, then it can respond more effectively to the pollinators when they arrive and offer them improved reward. If the pollinator is happier, it would look for a similar plant flowers afterwards. And so it might be that when we walk in the field, it is all full of ears in different sizes and shapes, and this should still be explored.
00:26:01.484 - 00:26:14.824, Speaker E: How well do different flowers hear? But it seems that some selection on hearing ability was definitely affecting shapes of some of the flowers.
00:26:15.824 - 00:26:37.524, Speaker A: Fascinating. Another thing, I guess I remember, you just actually touched on it when you were speaking, is this issue of stress that possibly plants emit stress sounds when they're stressed before you can actually see that visually. So possibly listening to them, we could have earlier indicators if the plants are being stressed or not watered enough, and so forth. Yeah.
00:26:38.984 - 00:27:31.218, Speaker E: So we tested two types of stress. We did most of our work on tomato and tobacco plants. And these plants, when they are happy in the lab, when they are watered enough, and they don't make almost any sounds. They start emitting sounds when things go wrong. And we tested one case when they experience dehydration, and another case where we cut the stem with scissors. So in Boke's case, they emitted sounds. The first case was more interesting for us, first, because the abrasion is always a problem.
00:27:31.218 - 00:28:18.994, Speaker E: How much water do we need to use to water the fields and not to use too much? And it turns out that when we stop watering the tomatoes already, in the second day, they start emitting sounds when they still look healthy and green. And so this suggests, but this needs further exploration, that we might, in some cases, have indications that plants need watering even before damage had occurred. It might be that some of the sounds are emitted already at the first stages of dehydration.
00:28:22.934 - 00:29:12.550, Speaker A: So it's fascinating also, and I was wondering if anybody wants to chime in on that, is that it seems like we're using similar tools when we are analyzing sounds between whales, sperm whales, sounds between plants, which is sort of the basic machine learning, neural nets and deep nets and so forth. So there could be two explanations to this. One is that maybe there is a, you know, a kit or a toolkit which can be applied more generally to any form of communication, or that we're just scratching the surface where it's beginning, and we only have very primitive means, and that's what we're applying. But there needs to be much more specialized tools, or can be more specialized tools for each one of this species. So anybody wants to weigh in on this, I would welcome to hear some.
00:29:12.582 - 00:30:27.584, Speaker B: Thoughts, I guess, you know, thinking about, in my career, I've studied many different animals all the way now, from bacteria to sperm whales. And I guess the one thing that's constant with anything that I study is that I'm never underwhelmed at any form of life on this planet. It's always interesting, when we dig deeper into something, there's always really amazing secrets that we can find. So what's really fascinating, I think, to this project is that first I was kind of trying to crossing. It's really about crossing disciplines, because by Shafi, you having that machine learning workshop was. It was opening up a new set of tools that, as someone who studies biology and looking at communication that you're on, the work was so advanced that it hadn't really trickled into many different fields of animal behavior. And I think that by putting together this workshop, we went and we found Joyce Poole and elephants and plants and yosi Yovo on bats and highlighted some people that were doing really excellent work.
00:30:27.584 - 00:31:05.250, Speaker B: And it turns out that we're using similar tools, but maybe in different ways. And I think Michael could also probably talk about too. And in the sperm whale community, some of the techniques might be things that had been done 1015 years ago in other fields. It's not like everybody's moving at the same pace. And I think one of the beauties of this seminar was really to bring together people that were working in all different kinds of animals and put them in the same room, because we don't often go in the same room. We probably don't come across the whale crowd as much, or the bat crowd or the elephant crowd. But there are all these similarities.
00:31:05.250 - 00:31:55.014, Speaker B: And I think the moment that we're at with machine learning, it may not have been here for that long, where there's the really possibility to apply these advanced tools and really advance these fields. So I think that's kind of one of the beautiful things of like putting this conference together, putting these people together, realizing that we're all addressing similar problems of where's the sound coming from, how to identify that, and then how to kind of walk through the measuring the communication of the animals into making sense. And I think that's where the beauty of the unsupervised learning is. It comes in and offers so much power to find patterns that we might not have seen before.
00:31:56.954 - 00:32:18.384, Speaker A: Michael, what do you think about this idea that to a hammer everything looks like a nail? So in other words, we've got this hammer of neural nets and we're just going to applied everywhere and we will get results just because the results are there to be had. Or do we, would you say that we need to develop some more specialized tools?
00:32:19.604 - 00:32:34.966, Speaker C: Well, I very much agree to this maxim that as you're saying, once we have a camera, then indeed everything looks like a nail. And indeed neural network sleep learning methods have been maybe a little bit overhyped.
00:32:35.060 - 00:32:38.602, Speaker D: But I think what is common in.
00:32:38.738 - 00:32:55.938, Speaker C: These problems, or in general applications where neural networks are applied or machine learning methods are applied, is that there is large quantities of data. And this is really a fundamental difference that happened probably over the past decade.
00:32:56.026 - 00:32:58.954, Speaker D: Or two that we started in many.
00:32:58.994 - 00:33:52.884, Speaker C: Fields of science, which previously were mostly observational type of science, including biology, we started adopting data driven approaches. And this, I think this is a game changer, because once we have data, we can extract patterns, and it's not surprising that machine learning is used. Talking about machine learning, it's also like biology, it's a zoo, different methods, it's like whales and elephants, and they might be used in different ways and different methods can be used. So, for example, for whales, we are mostly looking at unsupervised methods that try to somehow fill in the blanks to try to predict, for example, the next clicks from the previous ones. For plants from Lilacsoc, I understand that they are using mainly supervised methods which try to associate certain sounds to certain behaviors.
00:33:53.044 - 00:33:54.756, Speaker D: But I think the common denominator here.
00:33:54.780 - 00:33:55.824, Speaker C: Is really the data.
00:33:57.504 - 00:34:56.238, Speaker A: There was, one of the talks in the workshop was, I think, by Michali Rani, where she was actually talking about how can you get away with a little bit of data in the area of vision, where you have pictures which have a lot of repetitions, so you sort of harvest data from one image. And that brought up, I think, some interesting questions also that the communications in whales and plants, I guess there are also some natural communication. I don't know if there's really patterns there. I think it's so fascinating. All I can say is that I'm in awe of this exploration. I want to touch back on this issue that Michael alluded to about communication versus language. Though some of my friends early on, who are not on this project, where I mentioned theoretical computer scientists, mentioned the idea that we're looking for, I incorrectly use the word language.
00:34:56.238 - 00:35:25.434, Speaker A: And they were really up in arms. They're saying, well, what is language? And whales don't speak to each other, and brought up a very interesting question indeed. We have to be so careful in our language not to use this term. What is language? I don't know that any one of us can really weigh on that, because it's not our specialty. But it's an interesting question. And if anybody wants to say anything about the language versus communication issue.
00:35:28.454 - 00:36:13.274, Speaker E: We still have to justify the use of the communication term all the time because we don't know yet about the sounds emitted by the plants. They might be emitted passively, it might be physical response to other processes. But once the sounds are out there, then someone might respond. So when exactly do you need to wish to emit the sound, to actively emit the sound for this to be considered communication? So it's a question that we ran into again and again.
00:36:15.214 - 00:36:46.074, Speaker A: So I guess it brings us to a question. Is there like an experiment one could run to be able to distinguish whether something is closer to a language rather than a communication? I think, Michael, you mentioned to me the idea of, can we tell whether whales are talking about an event that happened in the past as a sort of test for whether they are abstracting and speaking in language rather than just communication. I think it's.
00:36:47.014 - 00:37:53.534, Speaker D: Well, I'm not an expert, so take anything that I'm telling with a grain of salt. But I think even this is disputed, what you are mentioning, the ability to talk about events in the past, this is what linguists call displacement. So if you think of communication system as a set of features, and you can argue which features are present in animals and which features are uniquely human. So people have been arguing based on certain experimental evidence, whether, for example, you have displacement in animals, and there are certain examples that could lead to this belief. So what Chomsky and Kosser has argued that the unique, really the unique feature of human language is recursion, the ability to embed a phrase in a phrase, in a phrase in a phrase, making it infinitely rich and varied. And this is something that we would probably be able to discover by analyzing the large datasets of whale bioacoustic data. I would say that whether it is language or not, it is probably not that important.
00:37:53.534 - 00:38:42.506, Speaker D: Maybe for in some disciplines it is extremely important. I think it's more a semantic notion. How do you call it? I think to find what are the complexity of communication tries, associated to behaviors, find maybe some pattern, some structure, maybe some of it is utilitarian, because language has probably evolved to converge certain information, and maybe in humans it has evolved beyond this. And we can use language for abstract things that probably have no immediate, immediate utility. So I don't know what will we discover? To me, the biggest disappointment of this project will really be if the whales are dull and non interesting and they just click and don't really talk about anything. But I am very doubtful that that.
00:38:42.530 - 00:38:43.146, Speaker C: Will be the case.
00:38:43.210 - 00:38:45.254, Speaker D: I think we'll discover a lot of beautiful things.
00:38:46.854 - 00:39:07.526, Speaker A: Yeah. In fact, communication is not necessarily just acoustic. I mean, that is something that also came up in the workshop. We had people talking about other forms of communication between different, you know, between animals from different species. And as Rilach mentioned, communication in terms of color and so forth, even in.
00:39:07.550 - 00:39:26.254, Speaker B: Pheromones and scent and. Yeah, yeah, it's such a. It's a broad thing. It's interesting. I, as the different science disciplines talk about this, there are certain trigger words that I've just been learning to avoid. And if you just say language and like, you make, you know, people get happier. But I think that's one of the.
00:39:26.254 - 00:39:47.454, Speaker B: Even scientists, we speak different languages among disciplines, and that's one of the. I guess one of the beautiful things about this project is we're trying to speak a common language and understand each other. So then we could understand these other things that are growing in the forest or swimming in the ocean.
00:39:48.354 - 00:40:35.174, Speaker A: So, you know, the Simons Institute is an institute for theory of computation. Obviously, this is not a theoretical project, in fact, very much. But what we do is develop algorithmic tools. We analyze them. And I am extremely also excited about the idea that all this method, all that we are developing the new algorithms, the new analysis methods which are applied to whales or to plants or to elephants or to bats, you know, will have as a side effect, essentially coming up with new tools, new algorithmic methods. And that's always been the case. And in a sense, you're looking for one thing, yet you're developing an entire field of apparatus, machinery, algorithms.
00:40:35.594 - 00:41:24.054, Speaker B: Well, it's also interesting, I think, that you brought in babies with Celeste Kidd. You know, to in one sense, it's like humans, we as babies, we have this natural ability of learning and start babbling and then speaking. And that just the types of people that you brought together for this to really think among all these different areas, because the tools that are applied in humans or in plants or in bats or in elephants, they could all possibly, in the years ahead, start funneling together and like you said, coming up with more pipelines that could better understand outside life. And then I guess the hope would be to write, then better understand ourselves.
00:41:26.074 - 00:41:44.854, Speaker A: Always. Thank you, guys, and thank you, Lila, very much for coming to the workshop, for all the work you're doing and also coming to this polylog interview. I'm really humbled to be part of this group. Thank you.
00:41:45.834 - 00:41:46.194, Speaker D: Thank you.
