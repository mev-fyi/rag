00:00:00.160 - 00:00:05.074, Speaker A: So let's start the next part of part two, Nika's tutorial.
00:00:07.374 - 00:00:25.954, Speaker B: Okay. Thanks for coming back. So, hopefully I didn't scare you or bore you, or maybe now you're sitting here regretting, like, maybe you skipped the last session you're regretting. Oh, no, why did I come in? That seems like I shouldn't have been here. Well, you're stuck here. Sorry. But thanks for coming back.
00:00:25.954 - 00:01:21.354, Speaker B: Okay, so I feel like I'm seeing mostly faces that were here if you weren't here. We talked about different paradigms of learning, and now we just started talking about sort of the min max theorem in zero sum games. So let me run through this slide. So this is probably a good place to start again, kind of. I also got a question during the break. So I think for those of you who haven't seen this before, maybe another place to pay attention to is that what is it that each player is doing? So, we talked about zero sum games, and each player is picking a distribution over actions. So if I go back to the model of a game, I have this game matrix roleplayer column players.
00:01:21.354 - 00:02:16.938, Speaker B: Each row and each column are these. We call them actions or pure strategies. And essentially each player has to now pick a distribution over its rows or columns. So that's an important aspect of talking about this min max and Max min, and especially if you want them to be equivalent, as we said in a celebrated von Neumann's Min Max theorem. So we ended the last talk by just looking at this min max theorem and asking, first of all, what is it saying and when is it true? And I said that at the very least, we don't. Okay, this slide doesn't tell you anything about infinite matrices, or whether or not for infinite matrices, this min max holds. That's something we are going to talk about briefly in a little bit.
00:02:16.938 - 00:03:33.720, Speaker B: But before we do that, it's good to really think about why should even the min max theorem hold, even for finite games? So, not for these really weird corner cases, but for the most natural, small, plain games that you can think about, why should it hold? And I'm going to try to tell you a little bit about this, although you have probably seen something like this, even maybe in the previous tutorials or in the courses that you've taken. So there's two directions in the min max theorem, and we were just talking. It's always takes a second to realize which one is the easy direction. One is this one, which essentially says that whoever goes second is better at doing their job than if they had to go first. Kind of like the way to think about it is that the person going second knows what their can curate their response to whatever the other player had done. So now their response is very, very well curated, and because of that, they do a better job. So if the maximize maximizer goes second, he can make the value larger than if he were to go first.
00:03:33.720 - 00:04:33.424, Speaker B: And the same if the minimizer goes second, he can make the value smaller than if he had to go first. So this part of the min max theorem is true just by understanding what you're giving up, curating your response to something very specific, rather than trying to cover all of your bases if you are going to go first. Now, the difficult part is realizing that the reverse direction holds. The reverse direction, if you want to think about it, is that you're giving up the chance of going second. And essentially you're almost like as if you're committing to something, not knowing what would have been the responses. So in itself has something very robust built into it that you don't know what kind of responses you would have seen, but you still want to do well. And this is where the connection sort of, to online learning comes through.
00:04:33.424 - 00:04:36.364, Speaker B: And a beautiful paper and observation.
00:04:41.824 - 00:04:42.472, Speaker C: But.
00:04:42.608 - 00:05:39.942, Speaker B: It'S been shown that essentially online learnability and Min Max are both about interactions with an adversary and at least for finite games. One way that you could have proved the reverse direction of this claim is to use a no regret algorithm, or two no regret algorithms. Let me talk about the version where essentially what you're doing is you take one player, player one. We are telling them that they need to be a no regret algorithm. What that means is that the probability distributions are producing these pts at every time step, they're producing something new. And if I look at the end of time, and I want to know if they regretted not having committed to some p from the beginning, that p they had very little regret. This regret would have gone to zero, this vanished regret.
00:05:39.942 - 00:06:30.674, Speaker B: So this is what I'm asking player one to do. And then for pL2, there are many different versions of this. You can also ask them to be no regret, but I'd actually rather ask them to be something that's much simpler, which is the best responder. So they can even see the PT at this current time step, and they're always best responding to it. Now, I'm not saying that I'm changing the min max theorem to now hold in a repeated way. No, I'm saying that I can get this repeated interaction to prove the Min max theorem and how I would do it is that I'm not going to show you the proof is to say that actually, literally, this kind of the regret here is not having known what to commit to. And it's very similar to saying that if you switch your min and Max, your regret there would have come here.
00:06:30.674 - 00:07:21.204, Speaker B: And as long as you have a no regret algorithm, you're going to get this inequality to hold at tighter and tighter inequality until your regret is exactly zero, and then this is going to hold the way you want it. What this is saying is that essentially the average, if I run this game infinitely long, the average strategies that will come out of this are in fact, min max strategies. They are the type of strategies that you could have committed to not having cared about what people are going to do. Okay, this is a high level idea course, has talked a lot more about actually what happens to, I think, PT and QT itself. But we are ignoring all of that. We're just proving them in max theorem.
00:07:22.304 - 00:07:29.244, Speaker A: So, yeah, so is this something proof of duality?
00:07:30.784 - 00:07:34.244, Speaker B: They're very connected. So it's definitely a duality proof.
00:07:35.564 - 00:07:53.264, Speaker C: Min max theorem, duality, they're all highly, highly connected to each other, and no regret. Learning black will approach ability. A bunch of others also are different ways of sort of addressing the same concepts. So they're all highly related.
00:07:55.004 - 00:07:55.864, Speaker B: Okay.
00:07:59.084 - 00:08:01.228, Speaker C: So if I write such a.
00:08:01.236 - 00:09:17.464, Speaker B: Thing for you, the next thing you're going to ask is, well, it seems like online learning, or online learnability has some role to play when it comes to the definition of an equilibrium. What is that role? Is it that anytime you can have an online algorithm, you prove that a min max theorem exists just because you can ask one of the players to play an online algorithm? That's the question we're going to answer briefly by talking about the role of littlestone dimension and role of online learning in particular here, because remember, we want to talk about adversaries. So the very first question is, is online learnability a sufficient condition for Min max theorem to hold? And the reason you might believe this is because essentially the only thing here I did was to say that let player one play an online algorithm, and especially an online algorithm for deciding these pts. So it feels like what I told you essentially is saying that any online algorithm would have been good enough. Anything that makes an online algorithm work is good enough.
00:09:17.924 - 00:09:42.872, Speaker A: So our question is, in the previous slide, using online ability to show the min max you are using, you do Jensen somewhere like to like to, you know, for what you have on the bottom left there, you have a sum of losses. Eventually you want to put the sum inside the loss. You will use Jensen. So, this proof uses convex concavity of.
00:09:42.928 - 00:09:51.224, Speaker C: L. L. I mean, l is the expectation of.
00:09:51.344 - 00:09:58.004, Speaker A: For your more general question, though, the question is, yeah, do you need this multilinearity or convexity or you?
00:09:58.804 - 00:10:50.190, Speaker C: Okay, so, I mean, the way I define min max theorem for finite, for a game, for finite games, is that the expectation itself is linearizing this problem. But there's another way I would have, I could have talked about min max theorem, which would have been about sort of pure, maybe, I don't want to call it pure strategy, but about points like x and y. These are not necessarily representative of appointment like probability distributions. And in that case, then what that l of those would have been, would have been important. And there is an assumption that is convex in one and concave in another. That's usually the general form of min max theorem written for these arbitrary spaces. But sort of, I simplified this already for you guys.
00:10:50.190 - 00:11:01.784, Speaker C: So we are in the linear world with the expectations and sort of. So this is already kind of structured. So I'm talking about the min max in games.
00:11:02.564 - 00:11:05.944, Speaker D: So you want to talk about the min max in general games.
00:11:06.844 - 00:11:11.224, Speaker C: Zero sum games. Zero sum games, but, but potentially infinity size.
00:11:14.844 - 00:11:31.358, Speaker D: Just understand that means that if it has infinite size in every round, I submit the infinite enumerable vector. That is the probability vector.
00:11:31.446 - 00:11:32.902, Speaker B: I think this is your question, which.
00:11:32.918 - 00:11:49.670, Speaker C: Is one of the subtleties I was going to mention. I'm asking you to give me a probability distribution over infinitely many rules. And how would you represent that? That's, that's itself a little bit problematic. Let me not answer this yet, because.
00:11:49.702 - 00:11:54.074, Speaker B: There are some other subtleties that I need to tell you. And then I'll give you one solution.
00:11:54.534 - 00:12:15.646, Speaker C: Statement of the solution that fixes both your issue and other issues. But that is an issue. Let me give one other comment about this, which is that I could have, I'm interested in this value rather than the actual strategies. So I could have replaced this max with a soup and then with an.
00:12:15.670 - 00:12:17.502, Speaker B: F. In fact, a lot of the.
00:12:17.518 - 00:12:57.294, Speaker C: Things I'm going to talk about from now on, especially when we start entering the infinite games, are about the soup n rather than maximin. And that by itself should tell you something, because maybe you have a sequence of probability distributions that have a certain property, and they're sort of easier to talk about than just probably distribution over infinitesis. Okay, so there are already really good questions coming then. But I want to put my own question forward as well, which is that, is online learning sufficient? And there are a couple of reasons that it's not.
00:12:58.234 - 00:13:46.454, Speaker B: One is that games and game theory in general require the mixed strategies to be probability distributions over the actions. So it's important that there's a probability distribution over the rows. If I'm playing rock, paper, scissors with you, it's important that what you do is also rock, paper, scissors. I enter a game and I think that I'm playing rock, paper, scissors, and you bring a tank, maybe you'll win. But that wasn't the contract we made, so that's actually not true for the way I define learning. For online learning, I actually allowed you to do whatever you want. Okay? If you thought that a tank was necessary, you could have used it, and there was nothing to stop you from that.
00:13:46.454 - 00:13:55.130, Speaker B: This is a solo point. Let's go back. This is the exact slide that I had for online learning before.
00:13:55.322 - 00:14:06.704, Speaker D: Wait, I'm confused. Sorry, what do we mean that we do not know the actions that we need play? How then I slay.
00:14:08.564 - 00:14:24.740, Speaker C: Let me actually give you an example here. For games, it's important you know the action set and you play only on that. No, you cannot invent another action for learning. You can invent hypotheses which refer to actions.
00:14:24.932 - 00:14:26.372, Speaker B: And here is. So this.
00:14:26.428 - 00:14:44.144, Speaker C: This is the slide. And I want you to focus on three aspects of this. And hopefully it's, here was the pr. I feel like when this light, the colors are a little bit different. Okay. So you have to focus here, here and here. Okay.
00:14:44.144 - 00:14:48.240, Speaker C: The first two places you're focusing, I.
00:14:48.272 - 00:14:50.664, Speaker B: Defined a function, class h. That was.
00:14:50.704 - 00:14:57.284, Speaker C: Going to be what you competed, but I never forced you to play any function from that class.
00:14:57.624 - 00:15:00.656, Speaker B: Specifically, I use ft rather than Hg.
00:15:00.720 - 00:15:53.614, Speaker C: To emphasize the fact that this doesn't have to come from your class. This is what I mean, that you could have invented any function here you want. It wasn't one of the rows in what I offered you. Now, there are these proper learning algorithms, proper online learning, and we call them proper if FD had to belong to the same page. So really, the question that you have to answer if you want online learning to be sufficient for min max theorem is whether there are online learning algorithms that, first, are proper and second, are characterized. The regret is characterized by or upper bounded just by little sum dimension. Again, not by size of the class, not by some other combinatorial dimension, but by the little sum dimension.
00:15:53.614 - 00:16:18.944, Speaker C: Why do we care about proper nouns? There was a question earlier, last talk, which said, what is this a random thing? We were talking about how your ft could be at random. It could have been chosen at random. So now if you have a proper algorithm and you chose ft at random, that's exactly the same as having chosen.
00:16:18.984 - 00:16:22.768, Speaker B: A row or distribution over your rows.
00:16:22.816 - 00:16:31.994, Speaker C: Where the rows refer to those hypotheses. So now an online algorithm, at every step, is literally taking a probability distribution over its rows.
00:16:33.334 - 00:16:34.194, Speaker B: Okay.
00:16:34.774 - 00:16:36.678, Speaker C: The way that I define the.
00:16:36.846 - 00:16:43.310, Speaker B: And now I've changed the simple optional algorithm. I still, I didn't have time to check the simpler standard. If one of you knows, tell me.
00:16:43.382 - 00:16:49.594, Speaker C: Otherwise, I have to check it later. The algorithm that I told you kind of inductively does a little solution calculation.
00:16:51.814 - 00:16:53.550, Speaker A: From the audience says standard.
00:16:53.622 - 00:16:53.998, Speaker B: Standard.
00:16:54.046 - 00:16:54.848, Speaker C: Okay, that's what I thought.
00:16:54.886 - 00:16:55.704, Speaker B: Okay, thanks.
00:16:57.684 - 00:17:10.652, Speaker C: I think that's what I had in the last one. And just like hallucinated simple, and it's simple, it's more standard. Okay. The simple optimal algorithm, the way we.
00:17:10.668 - 00:17:56.964, Speaker B: Defined it, at least it wasn't obvious that it was proper. Why wasn't obvious that it was proper? First of all, it didn't really choose a hypothesis. If I go back here, what it was choosing was how to label something. This y hat, this was the label that I was suggesting. And there is no guarantee that there is a hypothesis that would have matched this for every x. At least it doesn't come with that guarantee. And because of that, maybe it's not obvious here, but because of that is written in different language, at least there is no obvious reason that SoA should be proper algorithm.
00:17:56.964 - 00:19:25.254, Speaker B: So, but the nice thing about this, and I had promised you that I will have some quite recent results, is that that's been resolved. The standard optimal algorithm can be implemented as a proper learning algorithm with finite support. And in fact, we kind of know what its support is as a function of its vc dimension and the little sum dimension of the class. So what that is saying is that, in fact, a simple optimal algorithm, you could think about it at every time, it's using majority vote of only a few of these rows, majority votes, meaning that it's going to have it in support. And that allows us to now say that because that simple optimal algorithm was proper, and its regret was still defined by just like something like almost littlestone dimension divided by t square root of it. Now, finiteness of littlestone dimension is going to be sufficient for my min max theorem to hold. I still have no proof of it being necessary, but it's sufficient because what I can do is to take this algorithm and plug it into the first player's algorithm and then the second player is going to best respond, and that's going to give me the min max theorem holder.
00:19:25.794 - 00:19:28.014, Speaker A: Is it possible that it's necessary, or.
00:19:29.354 - 00:19:38.564, Speaker B: Is it possible that it's necessary? We actually know the characterization. It's the next slide, so I will talk about that in a few minutes. Any questions about this before I talk about necessity?
00:19:38.674 - 00:19:39.324, Speaker C: What.
00:19:41.384 - 00:19:45.416, Speaker D: Does this finite support mean here? Like this is an algorithm, and what.
00:19:45.440 - 00:19:57.764, Speaker B: Is the finite support here? So what I mean is that the probability distribution that is using at every time step, so the game has infinitely many rows, but actually it only the probability distribution has support only on finitely many of it.
00:20:00.544 - 00:20:18.354, Speaker D: Maybe I will repeat what you said, but that would be verified that I understood correctly. Algorithm. What we will do is that we will have all the hypotheses, that is, the distributions that we would play. We will follow the simple optimal algorithm just to play for this round. And we know that just in the.
00:20:18.394 - 00:20:19.134, Speaker C: Limit.
00:20:20.834 - 00:20:28.894, Speaker D: We will get the min max theorem, because they regret the, did they get correctly?
00:20:30.474 - 00:20:39.376, Speaker B: It's important that, I'm not sure if you mentioned the plugging in. So this is, this is important aspect of it.
00:20:39.400 - 00:20:45.016, Speaker C: So you can't just plug it in without also making sure that the adversary is best responding.
00:20:45.080 - 00:20:45.924, Speaker D: Yeah, right.
00:20:47.344 - 00:20:48.632, Speaker C: All that you need to do is.
00:20:48.688 - 00:20:52.720, Speaker B: You plug in that algorithm that is proper, any algorithm that's proper and only.
00:20:52.752 - 00:20:58.524, Speaker C: Depends on the Luftwanne dimension. It's regret here. And that SOA is one example.
00:20:59.184 - 00:21:04.504, Speaker D: Very. And he will respond with a hypothetical.
00:21:04.544 - 00:21:07.318, Speaker C: Or is the who, the adversary?
00:21:07.366 - 00:21:11.874, Speaker D: Yeah, yeah, the adversary, the tool, the pL2. It's not adversary, it's the pL2.
00:21:12.214 - 00:21:17.114, Speaker C: PL2 never needs to actually respond with a, because it's pL2.
00:21:17.414 - 00:21:27.678, Speaker B: You can come up with deterministic actions as a response. So the pL2 is not very important, just the fact that it's not passive. It has to actually do something here.
00:21:27.766 - 00:21:31.282, Speaker C: The PT is a distribution and has finite support.
00:21:31.358 - 00:21:31.978, Speaker D: Very interesting.
00:21:32.026 - 00:21:38.054, Speaker C: That's the only clue. Okay, other questions.
00:21:44.394 - 00:22:26.924, Speaker B: I think you are here. Okay, so Samina asked, and I mean, this is the next question you should ask, is those in the mention, is finiteness necessary? I think this is really interesting, which is that, surprisingly, it's not. Here I have a class of infinite little sum dimension. It's the same class we talked before. This example is actually from communication with Steve Henecke, where you have these intervals from an integer to twice the integer. And we talked about the fact that this has an infinite velocity dimension. Actually, the min max and maximum value here are both zero.
00:22:26.924 - 00:23:24.966, Speaker B: I didn't have time to draw this as much, but kind of like at a high level, what you would have done here is, let's say, as the learner, what you need to do is to decide a distribution over these funky intervals. And what you want is, let's say that you're trying to minimize that. Uh, one. So one way to think about it is that doesn't matter, and the adversary has some distribution over the instances. Uh, and instances are here. Uh, you can always sort of cut this somewhere and put your interval here so that there is very little, um, probability distribution in this corner. And you can move this farther and farther and farther, or doesn't matter, have to be farther.
00:23:24.966 - 00:24:03.114, Speaker B: You could just move it anywhere to make sure that it's capturing a small amount of the distribution. That's what you would do if you were the learner. What you do if you were the adversary is also try to have. I didn't work this out exactly, but you would sort of pick a distribution that any, um, that in any interval of a to two a, it has, uh.
00:24:05.734 - 00:24:06.086, Speaker C: It'S.
00:24:06.110 - 00:24:53.508, Speaker B: It's sort of a. Yeah. Okay. So its density is kind of like falling in these intervals a to two a, so that you're not going to be giving a lot of room to the learner to actually capture this. I didn't work this out, so I have to work this out. But I think this is kind of the overall idea of why you could show that both the min max and Max min are going in the limit, the sup inf and in soup, it's actually going to be zero, because you can make sure that you do not essentially capture any of the distribution in the active part of your interval, um, either way. So that's an example of why Internet little stone dimension is not necessary, and you can work this out at home.
00:24:53.508 - 00:26:10.444, Speaker B: But I wanted to highlight an interesting aspect of this, which I promised last. I actually gave you a pointer in the last, last talk, which is, how come it's not sufficient. Like, if I think about that infinite tree that I made, it should be quite easy to show that anytime I have this infinite tree, like, I grew a tree down to infinity, I could have continued growing it. I shouldn't have min Mac theorem, because of very similar idea that we had, which was kind of, you know, what the adversary has to do to make sure that there is a gap between the average regret, there's a gap between max and maximum, then intuition here is that there's a difference between growing a tree to infinity and having infinitely large trees. So to capture that, actually this paper comes up with a different notion of complexity. And that notion is going to characterize, it's definitely related to the Olson dimension, but not the same. It's going to characterize when Min Max and Max min hold for zero one valued games.
00:26:10.444 - 00:26:56.784, Speaker B: So what is this? This claim is that, let me take any game matrix. This is infinitely large game matrix. If I can have a subgame of it, where the subgame, the rows, can be sort of rearranged so that it's triangular. If I can do that and I can create this for an infinitely large submatrix that has this property, then the min max theorem is not going to hold. And that's the only condition that will make min Max theorem not holding. So existence of an infinite subgame that is triangular. Here's the interesting part.
00:26:56.784 - 00:27:47.914, Speaker B: I mean, actually, all of it is interesting, but here's the subtle part. As we discussed, there's two ways to make little sun dimension infinite. One was to take a single tree and grow it to infinity, and the other was to keep coming up with bigger and bigger and bigger trees. Note that the bigger and bigger and bigger trees don't exactly follow this type of sub game, because this is a subgame that you have to. The fact that it's a subgame, like every subgame of it, has this triangular behavior, is something that means that you should have been able to grow it to infinity. But there are classes that little sun dimension is infinite, not because of the sub game, but because there are larger and larger and larger and larger little sun dimension trees. So this notion of.
00:27:52.214 - 00:27:54.114, Speaker C: The infinite subgame.
00:27:54.474 - 00:28:03.694, Speaker B: Is exactly trying to differentiate these two from each other. In one, we have the min max theorem holding. In the other, we don't have the min max theorem holder.
00:28:04.674 - 00:28:26.364, Speaker C: Can you think of it as saying that in one, there is always a sub game with equal triangular for all d, and the other there's actually an infinite d. I think, that can be thought about as the same. But let me just say that this is not going to be the same. As Los Angele mentioned.
00:28:28.464 - 00:28:29.432, Speaker B: There'S this other.
00:28:29.488 - 00:28:36.284, Speaker C: Dimension called the threshold dimension, which is essentially this, but it doesn't have, it can again be infinite in two different ways.
00:28:37.504 - 00:28:39.408, Speaker B: It doesn't have to be like a subgame.
00:28:39.576 - 00:29:03.024, Speaker C: And what you're saying, I think, would be the equivalent, the analogous to that. And what we know is that that dimension dimension are within double exponentially within each other. So the important thing is that if one is finite, the other is minus. But the actual numerical thing doesn't translate directly. Yeah.
00:29:04.404 - 00:29:35.466, Speaker D: If I look about random matrices, it's a question. If I have a random matrix where it's entry would be with probably one half plus one and the other minus one, do we know if zero or one, it doesn't matter. Do we know if a random infinite matrix will have malicious triangular matrix?
00:29:35.650 - 00:29:40.314, Speaker C: Okay, I have to think about this, but this seems like a question you can answer with stochastic.
00:29:40.434 - 00:29:43.732, Speaker B: It's just like a process. It feels like it should.
00:29:43.788 - 00:30:06.524, Speaker C: If you are infinitely growing it, you should be able to have it in a second. But I might be wrong. It's not something I can answer. But this is the question you need to answer like the better or not. Some constant. I don't know if it has to be. I guess when you're talking about random matrices, I guess you want to say with some probability that holding or not, right?
00:30:06.604 - 00:30:08.364, Speaker B: Yeah, it feels like it should, because.
00:30:08.404 - 00:30:16.014, Speaker C: You'Re allowing it to grow, like you're allowing the matrix to grow infinitely. So you should be able to, you should be able to show that it's infinite.
00:30:18.034 - 00:30:20.970, Speaker B: Yes, counting should allow you to do that.
00:30:21.122 - 00:30:22.402, Speaker C: But again, I'm not going to claim.
00:30:22.458 - 00:30:24.374, Speaker B: That, but I think you can do it.
00:30:25.394 - 00:30:43.014, Speaker C: Okay, questions about this. Is there any hope to extend this? Again, there are no 0150. Maybe it's next slide. No, it's not the next slide. I will mention that actually corset has done some work on that.
00:30:43.324 - 00:30:44.396, Speaker B: We don't know a characterization.
00:30:44.460 - 00:30:57.124, Speaker C: We know one side. But before I get to that, we saw a lot of different results, and I want to take an opportunity to actually think about them. Beyond the mathematical thing, what we have.
00:30:57.164 - 00:32:20.994, Speaker B: Noticed is that at the very minimum we've noticed is that learnability is very sensitive to adversarial assumptions and to the particular notion of adversarial assumption, because we know that zero sum and online learning are very similar to each other, but there's still a difference in what is characterizing them. And of course, offline and online are quite different from each other, and there is a huge gap between the two. So at the very least, what we know is that this is, we need to really think deeply about our adversarial assumptions. But I think one other thing that I would take from the last two rows especially, is that it feels like the particular definition of adversary is really important in the sense that some of these results, especially the lowest one dimension, appears to be brittle, because even if you don't change the adversary, it's still adversary, but you're changing the context of what you're thinking about, you change the characterization. In the next part of this talk, I'm going to dig deeper here and try to open this online learning and really look at how brittle is this, and are we sort of closer if we try to avoid those brittle cases? Are we closer to offline learning, or are we somewhere else? So that's a quick takeaway. The other thing is that there are a bunch of things I did not mention. This is a very interesting area.
00:32:20.994 - 00:32:50.252, Speaker B: And that's real valued functions, and real valued like games for the first two, for adversarial, for stochastic and adversarial. They actually have a pretty good understanding of all of that. There are several different ways of characterizing them. So there is radomacher complexity and sequential radio marker complexity. There is pseudo dimension as a counterpart to BC dimension. There are a bunch of other things. There's fat shattering, there's a whole lot of different things.
00:32:50.252 - 00:33:30.874, Speaker B: But these two essentially are what characterize those processes so we at least understand them really well. For min Max, there are sufficient conditions, as I said, that Kosis has worked on with one of his students, Noah. I don't know, she's coming later in the semester. But it's not a necessary type of. It's not necessary. So characterization is much harder to think about characterization for real valued functions, especially because the margin conditions start mattering, but is through something that looks like sequential cat shattering. Um, that's it for this part.
00:33:30.874 - 00:34:42.156, Speaker B: Other questions before I move on to this kind of looking at these brittle examples a little bit more. Okay, so here is, uh, as I told you, this kind of the huge gap between offline and online, and then also the differences between online admin maximum. Really, I encourage us to really think more deeply about the differences between these two. Are we actually in one world or the other? Both of them seem sort of, maybe one is very idealistic, the other is like super pessimistic way of looking at reality. So, can we have a slightly more realist point of view? And this is something that algorithm design has done for decades now, in the sense that we love talking about worst case adversaries, we love talking about sort of average case, and then everything is beautiful. But we also want to talk about what happens in between those. Do these models actually nicely transition to one another or not? And that's what's called smooth analysis.
00:34:42.156 - 00:35:14.883, Speaker B: One of the ways to do this is at least smooth analysis framework. Um, a little bit of an overview. Um, beautiful, very impactful work. An idea from, uh, tang about that started with sort of simplex and kind of understanding. Oh, we know simplex doesn't do well for worst case adversaries. What does it do? Well, if I were to perturb my adversary a little bit. So, smooth analysis, the idea is that the adversary chooses an instance, and then the nature perturbs it slightly like, adds a gaussian noise to it.
00:35:14.883 - 00:35:37.574, Speaker B: And then the goal is, for instance, that either an expectation or with high probability over these perturbations, you want to give a good performance guarantee. And this has been repeated in many different applications. The modern perspective on these gaussian perturbations are that maybe it's not gaussian necessarily. Actually, the adversary is choosing.
00:35:42.694 - 00:35:43.526, Speaker C: Diversity, is.
00:35:43.550 - 00:36:42.580, Speaker B: Choosing a distribution over instances, and this distribution has to be sufficiently anti concentrated. There was a question earlier about thinking about distributions in the worst case, and we were saying that they could be sort of focused on one thing, and here we are saying that they had to be sort of dispersed and anti concentrated. And this is a really useful framework when we suspect that there is something, the worst case instances are brittle. And ideally, what we want to be able to get is that essentially, we're going to get the same performance guarantees as an average case. As in then things came from a distribution, but for the case of adversaries that have been smoothed. So what does this look like? Let me skip this, since we are a bit short of time. What does a smooth analysis perspective on online learning actually look like? Well, we're going to start with the model we had, which was, I have a world, I have a learner, and I have an adversary.
00:36:42.580 - 00:37:26.254, Speaker B: But now my adversary is not an adversary, and it's not the world. It's a combination of the two. What it's doing is that at every time it's picking a distribution. And that distribution, he could have picked it, knowing everything about the algorithm and the history. Again, if I allow the distribution to be super focused on one point, this is just the adversarial view of the world. But I'm going to change this by saying that the smooth, the Sigma smooth distribution is a distribution that cannot focus on any one point. In fact, its max density has to be something like one over sigma times the uniform density over the domain, something like this, a uniform upper bound on the max density that you can provide.
00:37:26.254 - 00:38:16.324, Speaker B: And we are doing it with comparison to a uniform distribution. You could do it many different ways, actually. And then what you're asking is, again, the same thing. You know, what happens to the regret? In fact, this is the more modern perspective of what I was saying is the gaussian perspective, which is that the adversary would have chosen an instance, and then the instance would have some gaussian noise, would have perturbed it. Okay? And in this language, this was both of them were actually introduced by Shidaran and Rockland Tiwari in 2011. So I like this formulation because it's not about a specific noise model, it's much more general. And what we are hoping is that we are going to get vanishing regret and sort of get it, but not something that depends on little sum dimension anymore.
00:38:16.624 - 00:38:16.960, Speaker C: Right.
00:38:16.992 - 00:38:55.384, Speaker B: We want to avoid the dependence on little sun dimension, which was really large. Even as soon as I put a threshold in, it would have jumped to infinity point. So, to recall, I can actually write the worst case and sort of the best case in terms of two different languages of amount of smoothness. So the worst case is if I have no smoothness at all, and a very good case, it's not the offline, it's actually a special type of offline. It's the uniform case, where the distribution itself was uniform, refers to sigma being one. And we know that this is the higher characterization with the little solar mentioned. This is an upper bound.
00:38:55.384 - 00:39:40.764, Speaker B: Actually, the only claim here is that it's an upper bound. And we know that sort of the way that we look at the first row is to interpret it as an impossibility result. Because thresholds are so common in machine learning. That's how we distinguish between good and bad. Zero one allowed, not allowed, and essentially, littlestone dimension jumps to infinity whenever we do that. So what we want is to avoid this situation. And a nice result here with collaboration, in collaboration with Tim Rafkarna and my student Abhishek Shetty is looking at what the regret bounds here would look like for smooth analysis.
00:39:40.764 - 00:40:19.404, Speaker B: So what we've shown is that even in presence of an adaptive adversary that picks the sigma, you know, smooth distribution based on what's happened in the past, you can still get regret. That looks so much more like the offline learning setting. It's characterized only by vc dimension and not by little zone dimension. Of course it's going to have a limit, a very tiny dependence logarithmic on the size of the noise. This is something that you cannot avoid, because when the noise is zero, you should go back to the motion. So this is the overview of what I'm going to talk about. Are there any questions about the setup?
00:40:28.104 - 00:40:32.644, Speaker C: Yeah. So if the sigma is one. So there's a bunch of things that I haven't covered here.
00:40:34.624 - 00:40:36.552, Speaker B: Sigma one over t is probably fine.
00:40:36.608 - 00:40:39.204, Speaker C: You don't want it to be one over two to the t for sure.
00:40:41.084 - 00:40:42.276, Speaker B: I don't want it to be tiny.
00:40:42.300 - 00:40:57.596, Speaker C: I think sigma one over two might be actually okay. Two to the t is definitely essentially made, not that, no assumption. I'm not sure if one over t.
00:40:57.620 - 00:40:58.260, Speaker B: Is okay or not.
00:40:58.292 - 00:41:46.086, Speaker C: So there's some log of things that I haven't put here, so. But I think one over t is five. So since I'm talking about pseudo dimension, it's binary value, but everything I say falls in the real value with a pseudo dimension instead of pseudo dimension. So it's actually, it is. The neat thing here is that we actually really do a reduction to the offline setting. So whatever would have worked out for that class in the offline setting on the uniform distribution is actually the thing that comes in. Okay, so I'm going to tell you a little bit about this, and I.
00:41:46.110 - 00:41:56.046, Speaker B: Want to just build some intuition here as to what are the challenges, because this is going to build more intuition about the challenges of the adversarial setting and also why smoothness helps.
00:41:56.230 - 00:41:56.742, Speaker C: Question.
00:41:56.838 - 00:42:28.080, Speaker D: Yeah, yeah, but there is a bump. Just as a mathematical curiosity, could someone prove that when I have the limit that sigma goes to zero, then the visit dimension will become the regression dimension?
00:42:28.192 - 00:42:28.584, Speaker C: No.
00:42:28.664 - 00:42:53.800, Speaker B: So it's. Okay, so you're asking a good question, but the VC dimension and Lewis one dimension is not the right way to frame it. BC dimension and the lungstone dimension are only about hypothesis classes. They have nothing built into them. So we do have, this characterization actually tells us something about little stone dimension in terms of VC dimension and the size of the domain.
00:42:53.952 - 00:42:54.768, Speaker C: Okay.
00:42:54.936 - 00:43:20.764, Speaker B: And this, I mean, it comes out of this characterization, it wasn't known before this log of one over stigma. You can think about it as the effective size of the domain. Another way to say this is that you could have asked this question about something like Radamarka complexity, and if you want to talk about that, we can talk about it offline as some ongoing work that I have in my group.
00:43:24.064 - 00:43:24.960, Speaker C: Okay.
00:43:25.152 - 00:44:06.950, Speaker B: So I wanted to start kind of thinking about why did the stochastic case work here? And then what's failing? So the reason this, like, one reason that the stochastic setting is working is that I have something defined as a net. A net is an approximation of my hypothesis class, and it's a finite approximation of it. So I replaced my hypothesis class, which is potentially infinite, with a finite hypothesis class, where each hypothesis has a proxy. So, like, this grid is the proxies of my hypothesis. And I want something about this net. It has to be a good net. And goodness is one that it doesn't have a lot of weight between.
00:44:06.950 - 00:45:02.924, Speaker B: And like not many points fall between any hypothesis and its proxy. So that sort of measured over the distribution, the function is close to its proxy. That's what you want from it, and that's something you can easily get. And it's kind of, for this distribution looks like this, a hypothesis here, its proxy is going to be there, and if there is less than epsilon in that total thing, the function is going to be close to its hypothesis measured over a distribution. And the reason sort of the offline setting works is that not only in expectation, this is small. What that means is that because I have IID assumption on the adversary, the world is throwing things at random into this distribution. I have this anti concentration property about each of these little bits in my net.
00:45:02.924 - 00:45:50.356, Speaker B: What this means is that iidness is not allowing the adversary or the world to put too many points here. So they are close to each other both in expectation and in reality, after things have been realized. So the thing that makes the adversary to be really powerful is literally not having this anti concentration. The fact that an adversary can always focus arbitrarily in one area, because it can correlate the future non trivially with the past. Okay, so that's what we are trying to avoid when we don't have Iid assumption. A more powerful adversary would definitely be able to correlate. The question we are wondering here is that of course I have this sigma smooth assumption.
00:45:50.356 - 00:46:59.202, Speaker B: None of my distributions are focused in one place. So I can make a net that is reasonable. But because the adversary is adapted, because this is a long term strategy, maybe the adversary in long term can still focus in one area. So is it that correlations across time are actually messing up my anti concentration properties? That's literally the question we have to answer, which is that how do we preserve anti concentration from one step to the next, to the next, to the next, over t times steps, even when those distributions themselves were adapting each other. And here the challenge is that the literal correlations across time, the fact that a new distribution was chosen after I drew some instances from the previous, so they are highly correlated. So we actually introduced a toolbox here that can be used not just in online learning, but can be used in talking about any adaptive adversary in smooth analysis. And I'm going to just talk a little bit about that toolbox.
00:46:59.202 - 00:49:02.598, Speaker B: The most important tool in there is a type of probability coupling a probability coupling is just a fancy way of talking about the joint distribution, but a joint distribution that has, sorry, but a joint distribution that has some nice property embedded into it between the sort of each draw of x and z. Um, the idea that we are going to use is that I have an adversary that's in smooth analysis that's generating these instances. Can I couple this adversary with a fully stochastic distribution? Because if I could couple it and have some nice sort of behavior between these, this coupling, essentially, maybe I can replace an adversary with, with a fully stochastic world, and we show that that's doable. All you have to do is to actually increase your set of points that you're going to generate. So for any adaptive sequence of t distributions that are sigma smooth, there is a coupling between the adaptive adversary adversary sequence and the uniform distribution in the sense that I can, with high probability t draws from my adaptive adversary is going to be included in the set of t times k draws from a uniform distribution where k is about one over sigma. So the nice property of a coupling here is really, this monotonicity is the fact that by, I'm producing a little bit more random variables, but I'm making sure that this set of random variables includes all the ones that would have been really produced. Why is this a good property? It's good property is because first of all, I'm coupling it with uniform distribution.
00:49:02.598 - 00:50:04.804, Speaker B: Uniform distribution is like the model distribution for anti concentration. It's not concentrated. It's like the least concentrated that you can find out. If the uniform distribution is not concentrated and my real set of variables is a subset of it, then this couldn't have been too concentrated either at a slightly different time scale. But by coupling into the uniform distribution, I'm borrowing those properties of a uniform distribution. So this allows me to say that if that area between H and H proxy didn't include too many x's from the adversary, it doesn't because it doesn't include too many z's from the uniform distribution. And essentially all I've done is that I have replaced an adaptive smooth adversary in my analysis, not an actual algorithm and actual life, but in my analysis with a fully stochastic adversary on a slightly different time scale.
00:50:04.804 - 00:50:42.794, Speaker B: How does this work together? Just, we're going to see this at very high level. This is going to allow us to do the following. I choose my net. The net is usually defined over a distribution, and I don't know what distribution. So I'm going to just define it based on uniform, which was sort of my basis of defining what a smooth distribution looks like. Then what I have to use algorithmically is I'm just going to run an algorithm for the worst case regret patterns, like for the worst case instances on the net only. Okay, I'm going to ignore all the other hypotheses.
00:50:42.794 - 00:51:30.504, Speaker B: So this is the algorithm. Now, as I said, we chose this from the uniform distribution, because if you choose it from the uniform distribution, the fact that you're working with Sigma smooth distribution actually makes it nice. In expectation, you don't have a lot of weight there. And in expectation you can show that for any, for a fixed sort of cell here, for a fixed h and h proxy, you have a pretty low expectation of having points in there. That's just a stigma and smoothness working with Adnet. The thing you want, however, is something not just about the expectation of a single cell, but an expectation of everything. In fact, this includes a switch between a min and max.
00:51:30.504 - 00:52:07.244, Speaker B: But another way to think about it is that I want to talk about what is my approximation error, which is kind of like the max number of points in any of those little holes. And I can think about this, I can just ignore those random variables and introduce the uniform random variables using my coupling, because that set inclusion is something that works nicely with this summation here. And once I do this, I am back in the world with uniform distribution. In the uniform distribution, things work out nicely with just vc dimension alone, and not the little.
00:52:08.624 - 00:52:09.564, Speaker C: That's it.
00:52:09.984 - 00:53:12.514, Speaker B: So here what we really see is that there is a toolbox that we can replace t interactions with an adaptive smooth adversary, with that of a uniform, with slightly longer time scale, but of a uniform world. And the nice thing is that it allows us to borrow not just analysis, but also algorithms from the stochastic world if we choose to. This is a very general model. I gave you an example from online learning to highlight especially the brittleness of online learning characterization. But it works in general for many other settings. And I think the way you want to think about it is that we are getting the ideal results, which is to get essentially the same performance as in the offline setting as you would in the online setting with just a little bit of smoothing. Or essentially this coupling that I talked about can get rid of the worst part of an adversary and bring you back to something that looks a lot more like an offline work, or a lot more like it's fully stochastic work.
00:53:12.514 - 00:53:36.528, Speaker B: This gives us a recipe. You can actually apply this recipe to generally many different problems. You could think about it as what is the recipe? You just take your problem that you want. It has an offline version and online version. You solve the problem for the uniform distribution. It's usually the easiest thing you can do. Once you do that, you isolate where you use anti concentration.
00:53:36.528 - 00:54:33.884, Speaker B: Those are usually things that respond well to set inclusion. Once you identify them in all of those steps, you apply a coupling and replace your sort of adaptively generated random variables with uniformly generated random variables slightly more often, and then you bring it back. Once you do that, you are done. You'll come back and continue the rest of the analysis. And you continue the rest of the analysis by sort of having accounted for the uniform distribution on a slightly different time scale. And as an example, not only this works in online learning, we've also applied it to a bunch of other online problems, or problems that aren't necessarily online, but use the same min max idea to solve the problem statement. So, online problems like, for example, online learning, online discrepancy, data driven algorithm design, but there are problems like differential privacy that really use min max formulation to their advantage.
00:54:33.884 - 00:55:29.192, Speaker B: And we can sort of apply the same ideas there as well. So, kind of the wrap up, the first part of the today, last hour and now, is that we saw that learn of Italy is very sensitive to the exact adversarial assumptions we are making, but that sensitivity is also partly brittle. And to really identify that brittleness, instead of using worst case analysis, we use beyond worst case analysis. Okay, that's it for today. As I promised you, I was very much wishfully thinking that I'm going to finish this part today, which I didn't. So, the plan for tomorrow is I will talk about the computational aspects, and I'll start here, but I'll talk about it very briefly, because otherwise we're not going to get to the other stuff. And then we'll talk about, more generally, non zero sum games.
00:55:29.192 - 00:55:41.948, Speaker B: I'm more generally thinking about general strategic behavior, and then later, collaboration. It's like we have three minutes or something. Right. Okay, I'm here for more questions, if you have. And yes, question.
00:55:42.076 - 00:55:42.684, Speaker C: Okay.
00:55:42.804 - 00:56:00.904, Speaker D: Yeah, it was very nice. It's a smooth analysis. Really enjoyed. Just out of curiosity, it seems very sensitive to the uniform fact. Is that the correct interpretation that you really need?
00:56:02.144 - 00:56:51.144, Speaker C: No. So the way we wrote it was for uniform because it just made. That's the way that historically, people define modern perspective on smooth analysis. You can essentially repeat this as long as you have a reference distribution. So if I fix a reference distribution minus some really weird corner cases, if I fix the reference distribution, then I tell you that you're allowed only distributions that are multiplicatively, at most as much larger everywhere, point wise, than this reference distribution. Then you can repeat everything I said and you can just change the coupling so that you're coupling it with the reference distribution rather than calling it with a uniform distribution. Does that make sense? Let's say that, like you thought gaussian distribution was the right one.
00:56:51.724 - 00:57:08.074, Speaker D: What, what I, what I'm trying to understand is that might say that, well, I'm going to use the uniform distribution, but there might be better distribution that you can use so that your regret becomes lower.
00:57:11.814 - 00:57:54.524, Speaker C: Right? So it's a little bit unfair, because I could have done the following. I could have said that. So uniform is reasonable. At the same time, the sense that, let me say that I made an assumption by saying that actually this distribution is only supported on five items. Now, all of a sudden I went from something that this little dimension was crazy high to something that's described on two, to the five many different functions. So it's a bit unfair to take advantage of smooth analysis in that way. And that's why it's usually referred to a distribution that itself has reasonable performance.
00:57:54.524 - 00:58:13.644, Speaker C: But it's usually, you don't make lower bound assumptions, you just make higher upper bound assumptions that are either uniform or buzzing or something that's kind of more reasonable, because otherwise you're, there are tricks that you can just like, make your problem just too easy, and you don't want to abuse that.
00:58:20.604 - 00:58:22.660, Speaker A: I mean, I'm curious what you would.
00:58:22.692 - 00:58:23.264, Speaker C: Say.
00:58:26.444 - 00:58:28.396, Speaker A: Are you going to cover it tomorrow or.
00:58:28.500 - 00:58:47.084, Speaker C: I'm going to cover it tomorrow. So I had planned the next section to actually be relatively short. I was not going to change the model. So for those of you coming back, I'm going to.
00:58:49.824 - 00:58:50.560, Speaker B: Tell you a little.
00:58:50.592 - 00:59:17.502, Speaker C: Bit more about computational algorithmic aspects of still online learning and Mnet, not the stochastic settings. And kind of like, what we're going to talk about is, first of all, what are the algorithms? And I'm going to give you a very brief statement of these, because I don't want to get into it. Other people have covered a bunch of these and sort of one example of.
00:59:17.518 - 00:59:42.896, Speaker B: A combinatorial structure that allows for efficient computation. And then I'll tell you a couple open problems. Let me tell you some of the open problems right now, so I don't have to tell them tomorrow. One open problem here is that I talked about smooth analysis. It's a version of online learning, but a little bit better than worst case. I told you that we can always borrow algorithms from the offline case. Sometimes the algorithms that we are borrowing from the offline case aren't efficient.
00:59:42.896 - 01:00:21.224, Speaker B: For example, the algorithm that I just gave you with that algorithm running on the net is really inefficient. The net is exponentially sized. So, how do you actually design an algorithm that uses the fact that your adversary is better than worst case, but also computationally, tries to use toolboxes that were developed for offline optimization, not toolboxes that were developed for online optimization? That's a general open problem for online learning. In particular, another type of open problem. Because if I tell you enough, maybe I don't have to do this tomorrow is sort of. Thanks, costas. You gave me the permission.
01:00:21.224 - 01:00:26.184, Speaker B: Yeah, I'll take five minutes.
01:00:26.524 - 01:00:27.972, Speaker D: I just wanted to ask a question.
01:00:28.028 - 01:00:29.292, Speaker A: About the previous slide.
01:00:29.388 - 01:00:31.904, Speaker C: Okay. Oh, this is just the.
01:00:32.554 - 01:00:33.962, Speaker A: I know, but I thought.
01:00:34.138 - 01:00:44.694, Speaker D: So you talk about adaptive adversary versus a stochastic adversary. Yeah, something in the middle. Olivia's adversary.
01:00:44.994 - 01:01:10.624, Speaker C: So essentially, you can ignore publius adversaries. So there's two ways to think about this. The results I gave you for publivity, for adoptive adversaries. So that adaptivity coupling is coupling an adaptive sequence with something that's non adaptive. So that's essentially saying that even in smooth analysis, you can ignore oblivious adversaries.
01:01:10.964 - 01:01:13.612, Speaker B: And we actually, before we came up.
01:01:13.628 - 01:01:42.612, Speaker C: With this coupling, we treated oblivious adversaries separately, because adaptive adversaries were just too difficult, and you can get actually similar results. So I think it really talks about this, the strength of the adaptive adversary results, rather than obvious adversaries being a special. But for this purpose, we can essentially ignore that. While I'm not going to abuse the time, I have to show you a little bit here, because we talked about.
01:01:42.668 - 01:02:56.194, Speaker B: Some characterizations of, statistical characterizations of when can I learn in the offline setting? When can I learn in the online setting? When does the min max theorem hold? And now you can also ask them computationally, when can I have a computationally efficient algorithm to achieve online learning? And maybe perhaps to also find something that's approximately close to a min max equilibrium. And we don't have a characterization here. There are some results for very specific restructured settings. So if you have, like, a linear function in small dimensions, basilis and clusters have a result about if sort of the. Maybe the rows are large, but there's few columns. In general, we need some assumptions, because there is an interesting result by Halton and Koren that says that even if I gave you an algorithm for offline optimization, just for minimization, for example, or just for maximization, you can't do them in Max efficiently. And not only they say it for online learning, but their lore bound is interesting in the sense that it also applies to actually min, max in a game.
01:02:56.194 - 01:03:58.644, Speaker B: And the thing that I'm going to, maybe, if I have time, just, just show, is that there is a different dimension. That's still another characterization, but it's a bigger way of thinking about it, where it's still about the game Matrix and how, how expressive the game matrix is. And that is going to be at least the best known result we know in terms of getting efficient online algorithms. And what that looks like is saying that I take a game matrix, it's not about the size of the game matrix, it's about the number of rows that I need to include in my some collection so that every two row looks different. So if this is very small, of course, if sort of y was small, this would have been small. You just include all of your columns. But even if your y is large, sometimes you can come up with a very few columns that essentially distinguish between any two rows.
01:03:58.644 - 01:04:23.424, Speaker B: And if you have that, you can get algorithms out of that kind of a structure. And I'm not going to go into any of this detail. In fact, I just wanted to show this because maybe tomorrow I'll come back and use this structure and still skip this section so I don't have to talk too much about computation. There is a lot of stuff beyond this tutorial as well, that.
01:04:26.884 - 01:04:27.784, Speaker A: Tomorrow.
01:04:31.204 - 01:04:42.744, Speaker C: Yeah, I just wanted to maybe show some. Sorry. Some references that I got. I don't want people to miss the reference. Exactly. Sorry, I don't know what. My computer is going crazy.
01:04:42.744 - 01:05:11.994, Speaker C: And these are references that, since I'm skipping this section, they're also interesting. Some of them are by actually people who are here in the semester as well, about sort of how to not just think about statistical stuff, but also the computational aspect of everything. And that's it. Thank you for the extra few minutes. It. Okay, I'm here if you have more questions. Otherwise, hopefully, I'll see you guys tomorrow afternoon.
