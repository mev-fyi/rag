00:00:00.280 - 00:00:48.206, Speaker A: Okay, thanks, Ryan. And thank you for the invitation. I'm not that great at all, this technology, so if you ask questions, I can see there's two different things for questions, and I don't know if I'll see them, but maybe the chair can tell me if there's one there or something like that. The other thing is, because of the time zone and other things, I'll be watching a lot of these talks out of sync. And so if you have any questions that you wanted to ask me, feel free to just send an email, and hopefully that'll all work out somewhat unconnected. Okay, so I'm going to be talking about cutting solutions to random CNF, and this is joint work with Andreas Galanis, Hang Guo, and Quan Yang. Okay, so we'll start with the KSAT model, which I'm sure you're familiar with.
00:00:48.206 - 00:01:23.324, Speaker A: We're going to let k be the number of literals in each clause. So here k is three. We can have n billion variables, so I guess they go up to six here and m clauses here, there's four. And the model we're going to work in is going to fix k. So the number of literals per clause, but we're going to work in the random case at model, which means that if you're given n the number of variables and m the number of clauses, you just choose the formula uniformly at random. So every literal, there's two n choices. You just take one.
00:01:23.324 - 00:02:23.846, Speaker A: Okay, so the way the model's going to work is actually that we're going to also fix the density of the formula, which is the number of clauses divided by the number of variables. And if you fix both k and the density, then the input is n, and then there's a random formula. And we're interested in how the probability of satisfiability changes as the density increases. Okay, so there's been lots of work on this question, so much that I had to make a pretty small font, loads of it, by people here who know more about this than I do. But a neat thing that's true is that there's a. There's a phase transition at some critical density alpha star depending on k, so that if your density is below this, then with high probability, probability one, the formula is satisfiable in the limit, as then goes to infinity, and above it, it goes to zero. So not very dense.
00:02:23.846 - 00:03:02.210, Speaker A: Likely to be satisfiable, drops to zero. And a few years back, ding. Sly and soon managed to actually pin down this alpha star. And so I'll put the value here for you, it's about two to the k times log two and some other stuff. Okay, so that's super good. And what we're going to do then is look at some algorithmic questions connected to this phase transition. Okay? Oh, and I noted here that their proof is by the second moment method, and I slipped in some notation that we're going to use.
00:03:02.210 - 00:03:28.892, Speaker A: So for us, Zedify is always going to be the number of satisfying assignments. And sometimes I'll call them solutions, means the same thing. Okay, so that's the model. So we've got a random formula, we fix the density. We know that if it's below alpha star, it's likely to be satisfiable. Above, it's likely not to be. All right, so the first question you might ask is let's suppose that the density is below alpha star.
00:03:28.892 - 00:04:38.704, Speaker A: So with probability one, the formula is satisfiable. Can you actually find a satisfying assignment? And the best polynomial time algorithm that we have for this is due to Amin Koya Oglan, and it succeeds with high probability over the choice of the formula when alpha. Maybe that's hard to see when alpha is a little bit less than the satisfiability threshold. So remember, the satisfiability threshold was about two to the k times the log of two. And Ambien's work here is saying that as long as alpha is most that divided by k, then you can actually find a satisfying assignment. And what we actually know is that this factor of k is kind of important because together with the mitras Amin showed, gives some evidence that the problem is likely to be hard if alpha is bigger than about two to the k over k, because that's where the solution space has a phase transition and the satisfying assignments are not connected. So, for example, if you had a local algorithm, you definitely wouldn't be able to get from between the solutions.
00:04:38.704 - 00:05:45.122, Speaker A: Okay? But we're interested also in the number of satisfying assignments. And so the second question you might ask is not just finding one, but can you approximate this number of satisfying assignments? And of course, as you all know, sometimes we don't really want to approximate z itself, but we might want to approximate its log to get it various physical quantities. Now, what we know about this, going back to Abb Montanari, is that actually this is concentrated around its expectation. So the log of z, that is, the log of the number of satisfying assignments, is concentrated around its expectation when alpha is below this two k over k, the place where the solution space is all connected. But unfortunately, we don't really have a formula for the expectation for this model, and that's because true and false are sort of asymmetric. So, although there are some models where we do have such formulas, this isn't one of them. However, we do have an approximation algorithm.
00:05:45.122 - 00:07:04.244, Speaker A: So, in 2007, Montanari and Shaw gave a polynomial time approximation scheme for the log when alpha is less than some quantity here, which I'll talk about in a second. Okay, so I want to pick out a few things about this polynomial time approximation scheme. Just means for any epsilon, you can get the log of the number of solutions quite close within a factor of one plus or minus epsilon. What is this found on alpha? You might be surprised at first, because it's little o of one as a function of, whereas I've been talking about two to the k, but it actually does have a meaning, and it's actually the Gibbs uniqueness threshold if you put the model on an infinite tree. Okay, and their method was, which we'll talk more about in a minute, is estimating the marginals. So, for every variable working out a close approximation to the probability that it's true in the uniform distribution on satisfying assignments, they used correlation decay or belief propagation to estimate these marginals. All right, so, for us, we have in this work two questions that we wanted to answer, and one is that we wanted a better approximation to z.
00:07:04.244 - 00:07:45.832, Speaker A: And that's because if you approximate the log within one plus or minus Cylon, of course, you're still exponentially off on zed. So we were hoping to get a good approximation to the number of satisfying assignments itself. That's what we do. But the second and primary focus is that we are interested in having a higher alpha, and in particular, in getting one that's exponential. So remember that the satisfiability threshold is about two to the k log two. We are interested in getting as close to that as we could, and in particular, in getting something exponential in k. Um, we achieved something exponential in k, but probably not the constants that you'll like.
00:07:45.832 - 00:08:32.044, Speaker A: So let me, uh, let me tell you about that. Uh, so what we get, we do get an f p test for z for densities, which are most two to the rk for some constant r. Actually, maybe it's easier to just do it formally. So, um, what the theorem says is that you can fix constants k naught and r, so that as long as the number of literals per clause k is big compared to k naught, and as long as alpha is at most two to the rk, so it's an exponential function, but it depends on this r. Then there's a polynomial algorithm with high probability of the choice of the formula, which, when it's given the formula, outputs a good approximation. I can see there's a question.
00:08:32.124 - 00:08:41.024, Speaker B: Yeah, Leslie, if you can't see it, I'll read it. It's from Prasad Raghavendra. He says, if the value of log z is concentrated on a random instance, wouldn't one output the expected value to be a PTas?
00:08:41.564 - 00:08:50.104, Speaker A: Okay, right. But, so, one situation is that it's concentrated, but we don't have a good formula for the expected value even.
00:08:50.804 - 00:08:57.784, Speaker B: Yeah, great. I also had a question. The algorithm is also randomized given phi, or it's deterministic given phi.
00:08:58.764 - 00:09:08.924, Speaker A: Ah, okay. I guess it doesn't matter in this case. It's going to be deterministic given phi, but I guess I'd be happy with one that's randomized given phi as well.
00:09:08.964 - 00:09:10.416, Speaker B: Yeah, it seems fine. I was just curious.
00:09:10.540 - 00:09:38.294, Speaker A: Decent question. Yeah. Okay, cool. And this value of r, if you try to figure out in our paper what it is, it's about one over 300. Now, although that's not optimized, I definitely doubt that we could improve it too much. So it's a pretty, we're shooting for exponential, but there's a lot of work to be done to get close to two, to the k, and I'll come back to that at the end. Okay.
00:09:38.294 - 00:10:38.058, Speaker A: Our approach, because the state space is disconnected in this regime, or the set of satisfying assignments, we can't really rely too easily on local algorithms. And our proof relies on an approach of Moitra. Really nice work of Moitra, which gives a fully polynomial time approximation scheme for bounded degree formulas. And what I mean by bounded degree is that he worked on the case where every variable can be in it most, a constant number of clauses. And I'll talk a lot about, well, a lot in 15 minutes about what the reason for this bounded degree. And really for us, the main difficulty is dealing with the fact that, of course, in a random formula, you do have high degree variables. Okay, so what I want to do now is I want to give you some notation and come back to estimating by the marginals, because that's what I, that's what Moitra did and what we're going to do.
00:10:38.058 - 00:11:32.564, Speaker A: All right, I'm just going to use Omega Phi to be the set of all satisfying assignments. If I don't, say, I'm always talking about the uniform distribution over that. And this will be familiar to a lot of you, a method for approximating z via the marginals is just to fix some satisfying assignment. And doing that, suppose that you take the variables in order, and suppose that for each variable j, you estimate, what's the probability that variable j takes the value AJ conditioned on the previous ones. If you multiply those together, you obviously get the probability of this assignment, which is in the uniform distribution, one over z. So lovely. If you can approximate these things, then you should take the estimate to be one over that.
00:11:32.564 - 00:12:20.744, Speaker A: Okay? And often this approach is used with correlation to k to estimate these marginals. We can't do that here because there's a whole lot of correlation. So following Moyche will be doing something else. So what moitre does in the bounded degree case is that he did what he called marking some of the variables in each clause. And the key fact about the marking is that every clause has a bunch of marked variables and also a bunch of unmarked variables. If you do that, he insisted on one more thing, which is that the formula can be satisfied using the marked variables. Okay, so then what you can do is suppose you estimate the marginals of the marked variables.
00:12:20.744 - 00:13:01.132, Speaker A: Then if you estimate those, once you have this assignment on, oh dear. Once you have this assignment on the marked variables, you've already satisfied all the clauses. So every other variable can be whatever it likes. So multiply by two to the number of unmarked variables, and you still get one over the number of assignments. Okay, so that's the goal. So the goal is to, for him, is to estimate the marginals of the marked variables. Now, the key fact that made the idea work is that the clauses never become short because you always have these unmarked variables hanging around.
00:13:01.132 - 00:13:47.454, Speaker A: And that's lovely, because all the marginals are near half, and that's really useful. How did he estimate the marginals? Well, he did that by satisfying a certain linear program, and I'm not going to be able to explain it in detail in a short talk, but it's better to get the ideas. The size of the linear program turns out to be polynomial bounded whenever a certain process couples quickly. So we'll talk about that. And if it couples quickly, then the linear programming is bounded. The linear program, you solve the linear program and you're able to then estimate these marginals, which gives you what you need. Okay, so I'll talk more about that, but that's the basic idea.
00:13:47.454 - 00:14:23.754, Speaker A: Okay, bad news for the random KSat formula is that unfortunately for us, of course, it's not bound in degree. So if you take any constant c, then there's going to be a linear number of variables that have degree bigger than c. And the maximum degree is clearly about log. Nice. And that's terrible, because these high degree variables, they don't have marginals near half. They have marginals that can be arbitrary, close to zero or one. And worse than that, there's kind of a domino effect, because even low degree variables can have bad marginals if they share clauses with a high degree variable.
00:14:23.754 - 00:15:26.610, Speaker A: So the basic technique is really nice, but we're not going to be able to, well, we have to figure out how to deal with these high degree variables. All right, but before I tell you more about that, what I want to do is tell you a little more about Moitra's method, because I think especially in a short talk, it's probably even best that you just take that away because it's quite cute. I told you that he estimates the marginals using a linear program. And the linear program is set up to model a certain coupling process. I'm going to tell you what the coupling process is, and then I'll give you a little idea what this linear program actually is. Okay, so let's say we have some variable v, which is just some variable whose marginal we want to estimate in the uniform distribution, which I called mu, what am I going to do? I want to estimate the marginal. And I could consider the probability that this variable could be true or it could be false.
00:15:26.610 - 00:16:34.320, Speaker A: And I could look at the set of solutions conditioned on those two things. And the coupling process that's relevant couples the uniform distribution conditioned on v being true, v being true, with the uniform distribution conditioned on v being false. So what the coupling looks like is you start out and you have two assignments, one of which makes v star true, the others that make v false. And at the outset they disagree on one variable, which is v. Okay, you'd like to kind of couple these two distributions. So to do that, what you do is you take a clause that's not satisfied in at least one of the copies, and you try to assign the values to the marked variables in that clause according to the marginal distributions, coupling them as well as you can. So there's a kind of a tree describing the coupling that we take two distributions and we consider making v true in both or true and then false or false, and then true or false and then false.
00:16:34.320 - 00:17:03.286, Speaker A: And the idea is that this is a thought process. We're doing this to create the linear program and we want to couple them so that they're very, you know, that most of the weight comes for agreement, okay? And if you, if you, if a variable causes a disagreement, it has to get added to the disagreement set, and then you have to keep going. So that's the coupling process. In very rough terms. What's the linear program?
00:17:03.470 - 00:17:06.494, Speaker B: Well, if you imagine, sorry, Leslie, can I ask a quick question?
00:17:06.574 - 00:17:07.070, Speaker A: Yeah.
00:17:07.182 - 00:17:12.674, Speaker B: Is the relationship between the, like, the marked variables and the unmarked variables and the disagreement set?
00:17:13.633 - 00:17:38.653, Speaker A: Yeah, sorry, it's kind of hasty. So, this is only learning the marginals of the marked variables. And in Moitre's world, the marked variables satisfy. And what he's going to do is just find the marginals of the marked variables, and then that's good enough, because he can multiply by two to the number of unmarked. For us, it's caused a bigger problem, as we'll see. But so these are all marked. Does that answer your question?
00:17:38.793 - 00:17:39.834, Speaker B: Okay, thanks.
00:17:40.134 - 00:18:37.424, Speaker A: Cool. Okay, and so what's going to happen is for this coupling tree, which you're just thinking about, what the linear program is going to do is it's going to have two linear programming variables for each node of the coupling tree, and we're not going to, in this talk, put down what exactly are the constraints of this linear program. But what you should think about is that what the constraints do is they essentially make the coupling behave properly. And there's constraints that say disagreement's pretty unlikely. Okay, so that's the point of this linear program. And what happens is two things. There are two key lemmas in Moitra's which are still going to be true for us, which, well, yeah, which are that, remember, we're trying to learn the number of satisfying assignments in which v is true, divided by the number of satisfying assignments in which it's false.
00:18:37.424 - 00:19:37.604, Speaker A: And if you can set the two constants in the linear program so that this ratio is between them, maybe our lower is super small and our upper is super big, then there will be a satisfying assignment of the LP that satisfies all the constraints. And the second fact is that if you get any solution to the LP between those two bounds, lower and upper, it actually gives you a kind of tight bound on omega one and omega two. So seeing this, you can now guess the basic algorithmic idea, which is that you somehow or other find in our lower and our upper that are really far apart and straddle the truth. And then you start just making them closer and closer by binary search. And as you do that, you're approximating the marginal that you want. Okay? Now, in much bounded degree case, the marginals are all really close to one, two, that's what the marking does. So coupling is super likely.
00:19:37.604 - 00:20:11.974, Speaker A: So that's really good. Oh, I should skipped all this stuff here. Sorry, I was watching the clock. What do we find? We find that we need the linear program to have polynomial sides, otherwise we can't run it. So we can't have too many nodes in this coupling tree. But if we look at the variables that are set in the two assignments, what is it? Well, it's the ones that disagree together with the variables that share clauses with them. And so what we're going to need is for that to be log n.
00:20:11.974 - 00:21:09.464, Speaker A: So that's going to be a really important thing. That's going to have to be true. In Moitre's case, marginals are all a half, so the coupling and goes quickly and that turns out to be true. Now, in the case of random K sat, we get bad variables, by which I mean variables that have bad marginals that are not near a half. And remember, there's a domino effect that they're the high degree ones and anything that shares clauses with high degree ones and so on. So what we do, our intuition, is that these high degree variables are not too clustered, and that's because the average degree of the formula is bounded. So what we do is we use an idea that was inspired by Amin and Panna Giadu, which is that we run a little kind of bootstrap percolation process to try to figure out which variables are causing the trouble.
00:21:09.464 - 00:21:51.974, Speaker A: So to start out with the high degree variables, they're bad. So we know that any clauses that have a lot of variables, they're bad. But the variables that are in bad clauses, they're bad. And we just keep doing this until there's no more badness in the world. And when we get done by construction, what we have is a set of good clauses that don't have many bad variables, and bad clauses only have bad variables. And the important point of all this is two things, the marginals of the good variables turn out to be near half. Well, that's kind of by construction, because good clauses don't have many bad variables.
00:21:51.974 - 00:22:58.520, Speaker A: And also there aren't that many bad variables. Well, there's a linear number, but it's a small linear number, so we're going to be able to deal with it. Okay, so what is our goal? So what our goal in our method is going to be, what we're going to do is we only mark the good variables and we can still use the low vish local lemma again to show that there is we can mark the good variables in such a way that we can that the good variables satisfy all the good clauses. We have to ensure that there exists an assignment of bad variables that satisfies the bad clauses because there's nothing else to satisfy those. We're going to learn the marginals of the good variables using a method sort of like Moitra's. But in order to do that endgame where you, where we used to multiply by two times the number of bad variables here, we have to actually be able to compute the number of satisfying assignments of bad clauses. And what we do is we do this by enumerating all assignments to bad variables.
00:22:58.520 - 00:23:37.992, Speaker A: So if that sounded a little exotic, the key fact is that when you look at the kind of factor graph formed by the random instance, the bad components have to have size log n. And that turns out to be a really key thing. And actually almost all the technical work is establishing this fact. Okay, so let me just, I can see I've only got a minute and a half, so I want to just say a little bit more about that. So what do I mean by this factor graph? Well, if you look at the formula and you look at the variables and the clauses and this edge here, oh, I can't color an edge. Okay. I don't, you probably can't see my pointer.
00:23:37.992 - 00:24:26.316, Speaker A: It doesn't matter. There's an edge from a variable to a clause. If the variable is in that clause, I've colored the bad ones blue. And what I just want is that if you look at a component in this factor graph and you look at its variables, what we need to prove is that with high probability these are only log n. And actually to make this method work, it's even worse, because during the coupling, if you ever encounter a bad variable, you have to add all variables from the bad component into the disagreement set. So we certainly need the bad components of size login, but you actually also need any variables that share clauses with them also to have size login. But that turns out to be true.
00:24:26.316 - 00:25:04.574, Speaker A: So that works. Okay, great. So I think what I want to do, since I've just got 21 seconds, is I want to, so the main technical part is proving that when you look at the random formula, that the connected components caused by these bad variables just have size at most log n. Supposing you do that, what you end up with then is an FP test, when the density is at most two to the arc. Ah, is that my timer? Yes. Whoops, sorry. Okay, so you end up with an FP test when the density is at most two to be rk.
00:25:04.574 - 00:25:31.394, Speaker A: And questions might be, could you improve the bound on alpha because we got exponential in k? But remember, the satisfiability threshold is actually two to the k log two. Could you get closer to that? And also a faster algorithm, because actually, this linear program, it's polynomial size, but it's not exactly small. Okay. And I think I'm going to stop there.
00:25:33.734 - 00:25:44.506, Speaker B: All right, thanks very much, Leslie. I don't know how we applaud. It's a constant dilemma in Zoom. Actually, I saw the suggestion. Yeah. That you should just applaud in American Sign language, which is very easy, I think. It's like this.
00:25:44.506 - 00:25:55.294, Speaker B: I don't know. We'll see if we can do it. Yeah. So, great. This is a wonderful time for questions, I think for questions, probably people can just unmute themselves and pipe up.
00:25:59.674 - 00:26:02.250, Speaker C: No, I don't think attendees can unmute themselves.
00:26:02.442 - 00:26:05.466, Speaker B: You're currently talking about. Is that because you're a panelist?
00:26:05.530 - 00:26:06.442, Speaker C: I'm a panelist, yeah.
00:26:06.498 - 00:26:07.338, Speaker B: Oh, I see.
00:26:07.466 - 00:26:08.146, Speaker A: Hmm.
00:26:08.290 - 00:26:12.264, Speaker B: Well, if you have a question, I suppose you should type it. It's a little strange.
00:26:14.164 - 00:26:16.584, Speaker C: Can I ask a question while typing?
00:26:16.884 - 00:26:17.604, Speaker B: Yeah.
00:26:17.764 - 00:26:25.644, Speaker D: By the way, we can also do the raise hand thing, and then the panelists can. I assume the hosts can unmute whoever has raised their hand.
00:26:25.764 - 00:26:28.224, Speaker C: But do we see attendees?
00:26:28.844 - 00:26:31.504, Speaker D: The raised hand thing is under the participants.
00:26:32.604 - 00:26:38.836, Speaker A: And by the way, I'm counting on you to tell me because I have my slideshow, so I can't see all that, but one of you can tell me.
00:26:38.940 - 00:26:40.268, Speaker B: Yeah, we'll tell you, Leslie. Yeah.
00:26:40.356 - 00:26:41.144, Speaker A: Okay.
00:26:45.684 - 00:26:46.708, Speaker B: Go ahead, David.
00:26:46.836 - 00:26:59.984, Speaker C: Yeah. Leslie, thank you for a very nice talk. I'm curious if the polynomial interpolation method by Barbina complex polynomial interpolation method can give anything in this regime. Have you thought about that?
00:27:01.564 - 00:27:17.814, Speaker A: Okay, we haven't thought about that on this problem, but actually on a rather similar. In a similar context. And actually, the. I don't know how to do it because. Again, of the high degree variables, which. Yeah, again, I don't. Right.
00:27:17.814 - 00:27:21.614, Speaker A: Okay. I should just have said, I have thought about it, but I don't know how to do it.
00:27:23.754 - 00:27:44.582, Speaker B: Yeah. Leslie, I have more questions for you that I can read out. I'm just trying to organize it here, but apparently I can unmute you if you raise your hand, but it's also fine to use this q and a feature. So I have a question from, uh, Nike sun. She said, I think you could miss. Uh, sorry, I think you said it, but I missed it. Could you briefly explain again why two to the RK is the bound.
00:27:44.758 - 00:28:14.942, Speaker A: Oh, okay. Uh, good question. I don't really think I did say it. Um. Uh, well, what do I say about that? Um, that's the point at which we could prove that, um, the bad closes of size at most log in. And furthermore, that in this coupling process, that if you kind of locally, what do you pick up that that's at most log n. It may be true, better than two to the RK, but that was where we stopped being.
00:28:14.942 - 00:28:21.234, Speaker A: I mean, maybe Nike knows, can prove something better, but that was, that was the bound at which we could prove those two things.
00:28:24.254 - 00:28:47.446, Speaker B: Great. I should mention also that, like, we built in this 15 minutes, so there could be, like, lots of questions. So if you just, like, want to leave and take a break, go ahead. But otherwise, we may just keep quizzing Leslie, if it's okay with her for, like, you know, the full 15 minutes. So I have another question. Well, two more questions, but I'll ask the first one from Prasad Teddali, which is also related to a question I was going to ask. Prasad asks what's known for knaesat.
00:28:47.446 - 00:28:58.920, Speaker B: And I was also going to ask, like, I mean, to what extent do the arguments depend on the fact that you're talking about KSAT as opposed to some general constraint satisfaction problem? But maybe you can start Prasad's question about nae satisfaction.
00:28:59.022 - 00:29:52.734, Speaker A: Okay, so. And there will probably be people on the participant list who know more about K nautical esat than I do, and they should come in and say more. But as far as I understand, because that's symmetric, it's easier to get at. Okay, I'm not saying this is a perfect answer, but it's easier to get at the expectation because of the fact that the true and false are actually symmetric. And so I imagine that. Let me just go back. If you were looking at methods like, where'd my slides go? So concentration around the expectation and then trying to compute the expectation, this should be easier for not all equals that, but those of you that are experts, I don't know what the bounds are and what's known.
00:29:52.734 - 00:29:58.780, Speaker A: Okay, maybe somebody does, though, on this call. I don't know.
00:29:58.892 - 00:30:16.144, Speaker B: Probably we have, like, over 80 people, which is awesome. Feel free to pipe up if you know the answer. In the meanwhile, I'll continue to ask questions of you, Leslie. So we have a question from Andres Corada. He asks, has someone considered using the disagreements between three, not two sets? Could that give more information?
00:30:17.924 - 00:31:02.194, Speaker A: Okay, that's an interesting. I don't know how to do it offhand, because where the two sets come from is in the coupling tree, what you do, and I think you start with some variable who's marginal, you want to learn, and you consider making it true and making it false. So these are the two assignments that you're considering, and as you make decisions, as you go down the tree, it's always there for those two things that you're comparing. So, I don't know. I don't really know how you would wheel in a third one, but that doesn't mean it doesn't make sense. It just means I don't know how to do it.
00:31:03.334 - 00:31:04.702, Speaker D: Could I ask a question?
00:31:04.878 - 00:31:05.634, Speaker A: Yeah.
00:31:06.534 - 00:31:50.754, Speaker D: So, I wasn't familiar before with Moitra's work, so I'm very happy to learn about it and about yours. Thank you. If every satisfying assignment had a witness consisting only of the marked variables, in other words, a partial assignment of those that explained why all the clauses can be satisfied, then, by enumerating all assignments of the marked variables, you would get an exact counting algorithm. So, is it obvious that there's no interesting density at which that happens? If, for instance, you choose the marked variables randomly, maybe with giving some priority to high degree variables.
00:31:52.134 - 00:32:31.594, Speaker A: Okay, so let me first take it in Moitra's context, where there are no high degree variables, and then come back to that. Yeah, so in his case, the number of marked variables is linear. Right? Okay, okay, but we don't learn them. Exactly. The marginals of the mart variables, we only learn them approximately. And the reason for this is that I kind of suppress the fact that in this coupling tree, what you have to do is you have to truncate it at some point. I mean, I said that you'll keep it polynomial size as long as the coupling dies out, but what's really going on is that the coupling dying out is allowing you to truncate.
00:32:31.594 - 00:33:10.454, Speaker A: But when you truncate, you do lose something. So you're not getting those marginals perfectly. So you get the marked ones marginals, you multiply them together, and then you know that the unmarked ones, in Moitre's case, they just two to the whatever. Now, in our case, you still account for the rest accurately, because what we get is that the components of the bad variables are actually small size, log n, and we do them by brute force. So you still get that aspect perfectly, but yeah, you were learning the marked variables roughly. Does that answer your question or no?
00:33:12.034 - 00:33:37.354, Speaker D: I think it does. I mean, if we had a set of variables where we knew that every satisfying assignment could be accounted for with a partial assignment of those variables and that all other variables were free, then it would just be a union of these subcubes and we could count everything. Exactly. But I guess we don't know that. And so.
00:33:38.934 - 00:34:03.704, Speaker A: Yeah, and also, they're big. So, in Moitra's method, in Moitra's case, we do know that the satisfying assignment can be satisfied by the marked variables. They're chosen to ensure this. But yet we don't know exactly what the marginals of the marked variables are, because there's a lot of them. There's a linear number of them, so we can't just brute force it or something.
00:34:04.084 - 00:34:12.384, Speaker C: I think it's very likely that counting exactly even the random instances is hard, just like counting random permanent is hard.
00:34:13.244 - 00:34:14.984, Speaker D: Even at very low densities.
00:34:16.004 - 00:34:23.854, Speaker C: I mean, super low. Below giant component, of course not. But above that, likely.
00:34:28.234 - 00:34:29.074, Speaker E: Oh, sorry.
00:34:29.194 - 00:34:38.602, Speaker C: No, no, no. This is not the direct refutation of the method Chris you're proposing, of course, but just generic thought. Go ahead, tell you.
00:34:38.738 - 00:35:01.309, Speaker E: Yeah, I just had another question. So, it seems like this method is also relying on the fact that many variables have marginals. Really close to a half, but at really high densities, close to the threshold. We don't expect that to be the case anymore. Correct. Is there really hope that this method.
00:35:01.341 - 00:35:34.674, Speaker A: Can go all the way? Probably not, actually. Definitely not. So, actually, what I wanted to say, except for that I made too much. So, yeah, what I wanted to say is that if you were going to try to get closer to the threshold and not all the way there, you might start with the monotone case. Okay. Because at least you've got a connected state space there and there. I wanted to mention some work of Herman, Sly and.
00:35:34.674 - 00:36:10.830, Speaker A: Sorry. It will become relevant. It sounds like it went off on a weird tangent, but there was a point here. There was a really nice work by Herman, Sly and zung that showed that how to do this in the bounded degree case where D is at most c to the two, to the k over two. And that's what I was trying to get to. And we do actually know that that's optimal up to the constant c, so we're not going to do better than two to the k over two. But maybe what I'm saying is, you know, why should it be two to the k over 300? So, I'm not.
00:36:10.830 - 00:36:16.640, Speaker A: I'm not suggesting you'll get all the way to the third threshold, but I don't think that ours is as good as you could do.
00:36:16.752 - 00:36:18.284, Speaker E: Okay, thanks.
00:36:18.904 - 00:36:20.280, Speaker C: I have a question, too.
00:36:20.472 - 00:36:43.734, Speaker B: Can I pause you just for 1 second, Siddhant. I'm afraid Nike got in before you, so hold that thought. I have a question from Nike Sun, a bit of a technical question, also related to the marginals. She asks if you considered a modified version of random KSAT, where all variables are constrained to have bounded degree. I would have thought that marginals could still be far from half due to the true false asymmetry. Is that issue already taken care of by Moitra's algorithm?
00:36:44.594 - 00:37:19.638, Speaker A: Yes. So the way that Moitra deals with this is to do with the marking. So if you go back. Where'd that go? So sorry about this. Yeah, so if you go back to the Moitre marking. So what Moitre starts with is there's two properties I want to talk about in the marking. One, each clause has to have a lot of marked variables, but more important has, well, more important for what I want to say, right? The second, it also has a lot of unmarked variables as well.
00:37:19.638 - 00:37:43.844, Speaker A: And what we're going to do is leave those entirely unconditioned. We're never going to look at them. And so there's kind of a lot of room there. So because of this marking that there's a lot of space around the marginals around there. A half. And, well, okay, so you can use the low calema to find such a marking. And then.
00:37:43.844 - 00:38:09.600, Speaker A: Yeah, and then the marginals, a near half. So he had that issue too. And that worked out in our case. The trouble is that the high degree variables and ones that share clause with them, they're just not near a half. And. Yeah, so we can't learn those and we, we can't satisfy all the clauses using the ones that we can learn. So we have to figure out what to do about that.
00:38:09.600 - 00:38:21.004, Speaker A: And the way to do that is learn the marginals of the so called good ones and prove that the bad ones are all broken into these small components that can kind of just be dealt with.
00:38:22.704 - 00:38:25.924, Speaker B: Thanks. Let's take one more question from siddhant. Siddhant, do you have your question?
00:38:26.544 - 00:38:40.134, Speaker C: Yeah. So actually, do these techniques also extend to counting, say, approximately satisfying solutions? Like, let's say you want to count number of solutions to satisfy more than 99% of classes, huh?
00:38:42.114 - 00:39:06.910, Speaker A: It's a good question. I haven't thought about it at all. So probably what I say will turn out to be nonsense, but. Yeah, that's interesting. Well, it sounds more complicated, but I don't, I don't see any reason why inherently why it wouldn't be a good thing to try. Yeah, it hasn't been done.
00:39:06.982 - 00:39:15.958, Speaker D: But a similar question would be the non zero temperature partition function. True.
00:39:16.006 - 00:39:22.694, Speaker B: Yeah. Maybe it's something that we can. We can all work out during the workshop and have some follow on work.
00:39:23.114 - 00:39:24.974, Speaker A: Sorry, can I ask the questions?
00:39:25.514 - 00:39:26.946, Speaker B: Who is asking? It was a Jane.
00:39:27.010 - 00:39:27.458, Speaker A: Jane.
00:39:27.546 - 00:39:31.362, Speaker B: Yeah, go ahead. Might be a little late for choming, but we'll continue. Go ahead.
00:39:31.418 - 00:40:08.244, Speaker A: All right, so this moisturise technique works only for maximum degree, at most two to the k over 50, because it will restrict how far your density can go if you want to apply this technique. Like, could you ever expect that the density can go over two to the k over 50? Oh, okay. So that's a good question, but I don't think that this was very optimized. I mean, you know, Moitra had to stop somewhere. I used to write the paper, but I don't think that there was anything magic. Yeah. Thank you.
00:40:09.224 - 00:40:23.390, Speaker B: All right, thanks very much, Leslie. This was great. I really enjoyed, like, that. We all got to ask our favorite questions of the speaker in, like, full time, so. So hopefully we'll have this for everyone else. Yeah, we can have some silent applause and. Yeah.
00:40:23.390 - 00:40:30.974, Speaker B: John Ming, do you want to set up your slides? Hopefully, you have the rights to share your screen. If you don't.
