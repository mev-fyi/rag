00:00:00.080 - 00:00:06.794, Speaker A: Point supervision, as far as I understand. And she talks about statistical rigor in genomics.
00:00:07.374 - 00:00:39.434, Speaker B: Thank you so much. Thank you so much, Doctor Peter Buna, for the introduction. And it is my great honor to be here to present. And actually this is my first in person presentation at Berkeley after I graduated in 2013. So to me this is very special and a great honor. So I named my group as the junction of statistics and biology, partly because my last name is so common. And another reason is that I had training in both biology and statistics.
00:00:39.434 - 00:01:38.004, Speaker B: So my first encounter of, I would say, genomics and bioinformatics was in 2007. At that time, I was just a fresh undergraduate from China, and I came here and I spent a gap year working with Peter and Haiyan because I was trying to switch my field and trying to be more quantitative. And Peter actually showed me this image, this nature paper, which is the pilot project from Encode, which Nancy just introduced. So I'm very grateful that I don't have to go through the background again. So that one was phenomenal at that time, and I was so impressed. And I have to say that although I was a biology major from China, and I know, as I would say, sort of undergraduate level, molecular biology and genetics, but to me, this whole genome wide genomics is very new and statistics is also very new to me. So I was just a fresh start, having to learn everything.
00:01:38.004 - 00:02:11.862, Speaker B: And this was a picture from Peter Anghayan's encode team. And I was here, and Ben Brown was there and Peter and are in the middle. The band will give a talk this afternoon that was, I think, around 2011, 2012. So I want to say that this is the fruit when I was part of the team. So we are part of the statistics team in encode. And Berkeley had this news in 2012 to introduce our contribution, and Peter's name is in the second paragraph. And fortunately, I was part of the team.
00:02:11.862 - 00:03:02.232, Speaker B: And as a result, at the time of my graduation, these four papers, I was part of them, and they are all related to the encode. I have to say I learned so much from the process and from the supervision of Peter and Hai Yan especially. I'm very grateful that they gave me this opportunity to learn from them, otherwise I might be doing something totally different at this time. Okay, so even back then, I had these questions, which has been, I would say I have been thinking about them constantly until now. And the first question is, are p values valid? So in all our textbook examples, we learn how to define a p value, how to calculate it. But as Nancy pointed out, the biological questions are more complicated. So then this question has been, has bothered me for a long time.
00:03:02.232 - 00:03:54.362, Speaker B: And I will show one example. And the second question I had was, why not starting with classical statistical methods? Because for a lot of the bioinformatics methods I read about during the encode phase, I was always like, huh, can we solve it with something simpler? And I will show one example for that. And finally, this is in line with what Nancy just presented. What is the proper null hypothesis? Because if we have different null as the default, we may get different answers from our statistical inference. So I will show one example for each question. Okay, so from now on, I will focus on the statistical rigor in multiple testing, because if you know, FDR has been one of the most well cited paper. The Benjamini Hochberg paper, because of multiple testing, is so popular in bioinformatics.
00:03:54.362 - 00:04:55.254, Speaker B: So in multiple testing, we work with p values, one p value per test per hypothesis, that if p values are well calibrated, they should follow uniform distribution between zero and one, or at least super uniform distribution if we are being conservative, and then if we want to aggregate the multiple tests and do one set of decisions. The aggregate criterion, which is mostly popular, is the false discovery FDR. It also needs calibration. Otherwise we are not really achieving what we claim, like 5% FDR. I want to list three common causes I have seen over the years about why p values might be yield post. The first common cause is what I call the formulation of a two sample test as a one sample test. This is what we call the chip sequencing data, which Nancy also briefly showed in her encode introduction.
00:04:55.254 - 00:05:48.914, Speaker B: Here we have two conditions. From each condition we obtain a chip sequencing chip seq sample. So here the x axis is our genome, and the y axis can be considered as the binding intensity of one protein to different regions in the genome. So the question here is, if we do comparison between the two conditions, can we call this region a peak? By a peak, we mean that the protein binds to this region more intensively in the experimental condition, like the condition of our interest, than in the background condition. So it's a comparison problem. And when I was a PhD student at Berkeley, there were two methods that were published, especially the max method. It was something we actually read about in our group meeting journal club.
00:05:48.914 - 00:06:35.842, Speaker B: So it was from Shirley Liu's group at Harvard. It has been the most popular method, and you can see the citation. So even though the method has some complicated data processing steps, but I try to simplify the statistical part using this table. In other words, we look at one region, so this box can be considered as one region, and we summarize the intensity values into a count value under each condition, each sample. So we get an x for the background count, y for the experimental count, and I use the capitalization to refer to them as random variables and these are not really observed with their distributions. But we do get one observation for each random variable. I call them small x small y.
00:06:35.842 - 00:07:35.228, Speaker B: So how the max method calculates the p value for this region is using this way. So it considers the count experimental count, which is our interest. Random variable follows a Poisson distribution under the null hypothesis, where the parameter as the x small x, which is the background observation. Then we want to know under this Poisson distribution, what is the right tail probability that this big y is less, greater or equal than the observed small y. So I saw this p value definition back then and I was very confused. I have to say I was wondering, is this a correct p value after thinking it through? I do not think so, because essentially what it does is that it seems y follows a Poisson distribution with parameter lambda. And the hypothesis is that under the null lambda equals small x, alternative lambda equals big x.
00:07:35.228 - 00:08:15.746, Speaker B: This can justify why the p value formula is like this. However, you can see that the null hypothesis involves x small x and considers to be a fixed value. But we know that the small x is a random observation. So that's why I said here the the randomness of small x is ignored. In other words, we are treating this two sample testing problem as a one sample testing problem. I had that doubt back then, but I didn't have a fit. I don't know how I can fix this p value issue because this scenario is essentially a two sample comparison problem, but each sample has only a size of one.
00:08:15.746 - 00:08:51.190, Speaker B: How can I do a test with one versus one comparison? I didn't learn this from textbook and I think we must rely on some very strong parametric assumption. And I think p value calculations are difficult, even though I doubt it. But I didn't have a fix until when I listened to Emmanuel Candace talk about knockups. I figured out, well, yeah, if we just want to control the FDR, p values are intermediates. They are not a final target. Can we get rid of the p values but directly control the FDR? Yeah. So, motivated by the knockoff FDR procedure.
00:08:51.190 - 00:09:29.164, Speaker B: Control procedure which I will elaborate. We came up with this paper published in genome Biology last year, trying to solve this problem for many such comparison problems without going through high resolution p values. I was very happy that we figured this out after so many years. That's the first cause. The second cause is what I see as the specification of a parametric model that does not fit data well. Another cause of eupos p values. This is a different application problem where we want to identify differentially expressed genes from rna sequencing data.
00:09:29.164 - 00:10:07.684, Speaker B: Here we can see that we're still doing a two condition comparison here. Each condition we have three experimental replicates. Every column is a gene, which means that we're going to compare each genes, two sets of measurements, and see if this gene is differentially expressed between the two conditions. Roughly, the left gene is differentially expressed. The right gene may not be. So for this simple problem, seemingly simple. There are two popular softwares that dominate the field, Desik two and HR.
00:10:07.684 - 00:10:43.156, Speaker B: And I have to say that they are both developed for small sample sizes, like this three versus three problem. We know that when sample sizes are so small, we do a statistic test. We don't have good estimate for the variance. The variance estimate will have very large variance. In this case, people usually do empirical bayes. They try to borrow information across genes to stabilize the variance estimate. And moreover, for each gene, each method, both methods, I should say, assume that the gene follows a negative binomial distribution under each condition.
00:10:43.156 - 00:11:32.720, Speaker B: So even though with only three values, it's very hard to say whether this negative binomial distribution works or not. That is the assumption. People use these methods. But I actually, in the collaborative project with computational biologist doctor Wei Li at UC Irvine, we found by accident that so, interestingly, from this immunotherapy dataset from cell 2017. So here the sample sizes are much bigger. So we have 51 and 58 in the two groups. We found that if we randomly shuffle the group label condition labels and we run these six hr, for many cases, the two methods can find more d genes from permitted data than from the original data.
00:11:32.720 - 00:12:26.884, Speaker B: So that raised a caution. We were wondering what's going on to look into this issue and figure out whether the discoveries these two methods made from the original data might be false discoveries. We check the model fitting, because here the sample sizes are reasonably large. We can check how well does the negative binomial model for every gene fits the data in each condition? Yeah, and we found that for the genes that are frequently identified from permeated data, the fitting was poor. For the genes that are rarely identified from permitted data, the fitting was better. So this at least confirms that the model fitting is the issue here. Negative binomial distribution does not hold well on this data set, because here, the samples under each condition are individuals, not experimental replicates.
00:12:26.884 - 00:13:20.536, Speaker B: So that can explain why the theoretical negative binomial distribution, which may make sense for replicates, does not fit well for human population data. Then for biologists, they always ask me, so what? What's the consequence? Then we show them that if we pretend that we didn't know, there might be issues with the de genes found by Desik two and HR. We just take those genes and run the, we call the functional enrichment analysis, trying to see which biological pathways are related to those genes. We can actually see that many pathways are immune related. Therefore, if we trust the result, we can report a very good finding. Yeah, there's immune response differences between the two groups of patients with immunotherapy and without or without immunotherapy. That can totally change the story.
00:13:20.536 - 00:14:12.062, Speaker B: So I think this is very interesting, and because I always had this question, why not classical statistical methods? So I just suggested to my collaborator, how about we try the Wilcoxon ranks on test? We can see that Wilcoxon doesn't have this issue. It doesn't find, it finds zero d genes from the real data. It also finds zero d genes from all permutated data. So in other words, we don't see anything unreasonable. We published this work in March this year, and we can see the number of axes means that the field is really interested in this problem. And although this is something we all know in this community, in the audience for large sample size data, non parametric methods can be an option. But I want to say that in the bioinformatics community, people are mostly driven by a massive popularity.
00:14:12.062 - 00:15:02.744, Speaker B: People go with the majority instead of being checking the assumptions like what we recommend them doing. So that's the large sample size, because here permutation can give us a lot of null possibilities for us to check false positives. What if the sample size are small? Back to the original setting, three replicates each condition. So I want to say that in this case, we want to propose our clear pair as a non parametric option for calibrating the FDR control. Even though we go with parametric methods like DeSeq two and Hr, which I will show later, that's the second cost. The third cost, about eo p value, is the treatment of inferred covariates as observed. This is a very common issue in genomic data analysis.
00:15:02.744 - 00:15:46.784, Speaker B: Here I'm moving to single cell rna sequencing data, which Nancy also introduced here. A typical analysis is called a pseudo time differential expression analysis. What does it mean? We define cell pseudo time which I think is very innovative. So it was proposed by a Berkeley CS PhD graduate coachnell in his postdoc work. Now he's at u Washington. So they proposed this concept called cell pseudo time, which is a latent unobserved temporal variable that is supposed to reflect a sales relative status among ourselves to infer the pseudo time. This test is called pseudo time inference is to estimate the pseudo time of our cells.
00:15:46.784 - 00:16:17.408, Speaker B: The approach is to order cells along a trajectory based on these cells high dimensional gene expression vectors. That's the task. Monocle three is the pioneer for doing this. Sandrine's slingshot is a popular software nowadays for doing this task. This is an illustration from the slingshot paper. Essentially we first do dimension reduction of cells. We can project cells from high dimensional space to a low dimensional principal component space.
00:16:17.408 - 00:17:05.196, Speaker B: Then the algorithm called minimum spanning tree is applied to connect the cell, which are pre clusters to connect the cell clusters and then the tree is smoothed out by the principal curves. And finally, each branch of the principal curve is considered as one trajectory. By trajectory we mean that it may refers to cell differentiation or it may refer to sales immune responses. Depending on the context, by projecting every cell to its closest trajectory, we can obtain the coordinates in the trajectory which will be normalized between zero and one, called pseudo time. This is the pseudotime. These dots are cells. We are showing one gene in each panel.
00:17:05.196 - 00:17:48.484, Speaker B: Basically the y axis is gene expression levels. We can see that if we look at the trend, the left gene should be considered as differentially expressed along pseudo time. The right gene may not. So why are we interested in doing this? The reason is that we believe the pseudo time refers some biological process like differentiation. Then we will be interested in which genes are driving this process or are correlated with this process. Then we need this differential expression identification. One thing I want to point out when I read this line of literature is that I found maybe we should consider the inference on certainty of cell pseudo time.
00:17:48.484 - 00:18:36.704, Speaker B: Let's look at this toy example. This is a very simple one branch pseudo time. If we consider the sampling uncertainty which we heard about yesterday here I reflect the sampling uncertainty by just doing subsampling of the cells in each subsample I redo the pseudotype inference. So for each cell I would obtain its pseudo times across the subsamples for which it's included. So we can see there is a dispersion, right? So it's not one value, but there's some uncertainty. That's the sampling uncertainty. However, I want to point out up to when we started working on this to our knowledge, existing method treats pseudo time as an observed covariate instead of inferred covariate.
00:18:36.704 - 00:19:22.116, Speaker B: So for this task, we proposed this method to cause pseudo time de to consider the uncertainty of pseudo time inference. So this is led by my student Dong Yuan. And so in short, we actually, our method was built upon, let me say, the Tracyq method, which was also from St. James group, and they used the generalized additive model to be the non linear regression approach to capture the relationship between every gene and pseudo time. So here is the procedure. So we started with the cells. We first do pseudo time inference to get a trajectory inference trajectory, and then we can fit a GaM model to every gene j.
00:19:22.116 - 00:20:09.958, Speaker B: Here, for the generalized part the distribution, we consider both negative binomial and zero inflated negative binomial because our gene expression are counts. After fitting the model, we can obtain a test statistic from the model. So if we don't assume the pseudo time to be random, we are done here. So we can use the test statistic, use the theoretical null distribution, and we get a p value. However, our argument is that here the theoretical distribution may not hold anymore because of the randomness in pseudo time. So therefore for the top part, we try to generate an empirical null distribution. So we use the subsampling approach instead of the bootstrap approach, because some pseudo time inference methods do not allow for repetitive cells.
00:20:09.958 - 00:20:34.718, Speaker B: So bootstrap is not an option. But we do subsampling with 80% of the cells. For each subsample we do pseudo time inference. So pseudo time inference is part of the whole pipeline. We want to do inference more then based on the inverse pseudo time. We want to disrupt the relationship between the gene and the pseudo time. To create the null cases, we permeate the cells, so any relationship, if existent, will be disrupted.
00:20:34.718 - 00:21:35.454, Speaker B: Then from these data we fit the gantt model, the same gam model as here, and then the test statistic values on each permediate subsample will be from the null. So that can give us the null distribution, empirical null distribution, and we can get a p value. So this is a very simple idea, but actually it works. So we can show that with this calibration we can make sure the p values are uniform between zero and one under the null, and compared to the state of the art method, Tracy and monocle three Du, we can show that this p value calibration can give us some actual use. We can show that the difference in the d genes between the method. So like the d genes our method can uniquely find will actually have some useful, I would say very reasonable gene ontology functional terms that can help us interpret the results much better. So I would say this shows the use of calibration p values in real applications.
00:21:35.454 - 00:22:13.684, Speaker B: However, I want to say that this method has its limitations. The obvious one is the computational time. We can see that if we want to get high resolution p value, say 0.001, we need at least 1000 rounds. So that can be time consuming. Here I want to say that our clipper again offers an FDR control approach by reducing the number of rounds we don't have to do so many null generation. Another thing we didn't consider yet is the other null, the more complete null, which means that in our case we just consider a null to be that a gene has no relationship with pseudo time.
00:22:13.684 - 00:23:07.206, Speaker B: But what if the cells don't follow a trajectory at all? The cells are just IID, no trajectory. But in that case r null doesn't capture. So to capture this null, we need a way to generate those complete null cells. So for this we propose a simulator to generate a null under this more complete null case. And besides pseudotime D, a related problem which is also a common practice in the field, is that we want to first divide cells into clusters, just like in the network problems we heard about, community detection is also a type of clustering problem with the clusters. Then we infer which genes are differentially expressed between two clusters. So for this problem, in the state of the art in the field, it's just by pretending clusters are predefined, they're classes instead of clusters.
00:23:07.206 - 00:23:54.436, Speaker B: So that we ignored that the clusters are learned from the same data. But there are methods that account for this, and they use the selective inference framework. So this is from David Tess group from Stanford, and also Daniela Wilton's group from U Washington, that they both assume that the genes are following gaussian distributions. So this is an assumption we hope to relax or remove. Here is our proposal. We want to use assay design three to generate a set of null cells, and then we want to use Clipper to achieve FDR control without relying on high resolution p values. I want to say that this was inspired by the gap statistics paper, which was for setting up null controls to find the number of clusters.
00:23:54.436 - 00:24:38.242, Speaker B: And I remember Hae Yang introduced this paper to me when I was a PhD student, and that's of course the knockoff procedure, which is the theoretical foundation for us. Ok, so this is a summary of what I said. So I talked about three common causes of u post p values, and for the remaining time I'll try to be quick and talk about our solutions. So first I'll quickly introduce Clipper, which I consider to be a p value free FDR control, a general procedure for genomics feature screening. And we use the FDR control theorem from Barbara and Candice from their knockoffs paper. So in short, Clipper only requires two components, contrast scores and a cutoff. So for the contrast scores, let's say we have d features.
00:24:38.242 - 00:25:27.554, Speaker B: Every feature will need to give it a contrast score, so that the distribution of contrast score should follow this shape, which is the theoretical foundation for the FDR control in the knockoff paper. So in other words, if a pitch feature is a null feature, its contrast score should be symmetric around zero. In other words, the absolute value of the contrast score should be independent of the sign. And for the features that are likely interesting, their contrast scores should be largely positive on the right tail. So with this, then this thresholding or cutoff result was from the knockoff paper. So we leverage this result, but we don't need to generate knockoffs as in their paper, because their case is more complicated. It's about a multivariate prediction problem, but in our case we're doing just marginal feature screening.
00:25:27.554 - 00:26:08.440, Speaker B: So we're looking at one feature at a time. So our hope is to get rid of the need for high resolution p values or parametric assumptions we can hardly verify, or large sample sizes required for nonparametric statistics. This is the key summary of this method idea. We need a target data and the null data to set up contrast. In the first example chip CP, culling the experimental condition and the background condition naturally sets up a contrast. I don't need to do any data generation. In the rna sequencing DG identification problem, we show that the permitted data can be used as null data.
00:26:08.440 - 00:26:52.888, Speaker B: The actual data is the target data. In the two single cell problems, the actual data is the target data, and the null data will be simulated from our simulator based on the specified null. The contrast can be established by applying some informative summary statistic to both target data and null data and do a contrast. I want to know that this t summary statistic in reality can be a complex pipeline, because in the field data processing is part is very important. And this can be hard to summarize like in a mathematical way. So I can say it can be a complex pipeline. So let's show the result applying Clipper to max and Homer and D 60 hr as an add on.
00:26:52.888 - 00:27:42.282, Speaker B: So using whatever the output and weight, set up the contrast uncontrolled FDR. We can see that we can greatly reduce their actual FDR to be equal or below the target FDR. That's what we aim to achieve, while we don't sacrifice too much power, so the power is still reasonable. So these two results actually show that this idea can be a very powerful and flexible approach for leveraging the power of existing methods while rectifying their FDR. Okay, but I didn't touch the last part yet. So for the single cell part, how can we generate a null data using our simulator? So that's the last piece, our simulator. So I want to say that the foundation of the simulator was our last year's work.
00:27:42.282 - 00:28:22.074, Speaker B: We call SC design two. It's a probabilistic single cell gene expression data simulator. So, for every gene, we allow the model selection from four count distributions, poisson, negative, binomial, and there are zero inflated versions. We fit each model marginally for each gene per cell types. But here, cell types are predefined, cells are clusters. But we want to relax this next slide. And the key is that we are using a probabilistic model, using a technique called gaussian copula, so we can capture gene gene correlations, which was not really the emphasis in previous simulators.
00:28:22.074 - 00:28:58.704, Speaker B: And also, I want to say that I first heard about copula when I was in Peter's group from our postal Chun Hwa Li's work, and she's currently at Penn State University. So I learned a lot from her. So we found that copula plays a crucial role. So this is the data used for fitting our model. This is the left out data untouched, and this is the synthetic data from our simulator with gaussian copula. Without gaussian copula, you can see the data look very different. And these are two state of the art simulators that do not explicitly capture correlations, but we still compare them to show.
00:28:58.704 - 00:29:44.136, Speaker B: Yeah, here, this explicit modeling of correlation is important. Okay, but this is not perfect, right? We only consider discrete cell types. And as my previous part said, the cells may follow a continuous trajectory. So we expanded that simulator to our third version. So now you can see that with this continuous, we use the Gan model, again, generalized additive model, as the baseline, as the backbone of the simulator. We can see that, yeah, we can generate cells along a continuous trajectory, so that can mimic real data better than our previous simulator, which generate data from discrete cell types. And furthermore, we are no longer restricted to just rna sequencing data.
00:29:44.136 - 00:30:10.638, Speaker B: We can work with attack sequencing. ANSI mentioned. It's about the open chromatin signatures or even protein data or even spatial coordinates data. And also here we make sure that we can output a likelihood, so we can do likelihood based model selection. For this, we use the vine copula technique so that we can output the likelihood for the multivariate model. So this is one example. This is a spatial data example.
00:30:10.638 - 00:30:43.784, Speaker B: We can see that it works very well. For this one, we used a two dimensional generalized additive model as the backbone. Also, thanks to my junior student Guangao, we actually added the functionality. So we cannot just output, not only just outputting counts, but also the reads, the raw level data. So the reads, this is the synthetic data. This is the real data. We can show that with the reads, we can even benchmark those computer science low level processing pipelines, not just the statistical parts with count data.
00:30:43.784 - 00:31:28.098, Speaker B: All right, so with the simulator, then we can generate null data. So, for example, this is the real data with apparently two clusters. But if we want to generate a null without any clusters, we can merge them using our simulator. It's just changing the parameters in our probabilistic model, which I think is very flexible. This is an example where the real data we just want to show the effect is actually one cluster, one cell type, no clusters. And if we don't assume we know this, we just do k equals two clustering. Using the dominant pipeline crad, or the k means clustering, we can always divide the cells into two clusters and do d gene analysis.
00:31:28.098 - 00:32:11.848, Speaker B: We can find d genes. But if we use our simulator to generate null data as the control, in this case, we can see the null data will look like real data because the real data doesn't have a cluster. So you see that if we force these existing pipelines to find d genes with k equals two, the FDR is equal to one because they will always find de genes, even though there are no two clusters. But using our contrast approach, we can control the FDR very well because we set up the proper null. All right, so that's the major part of my talk. I want to summarize here. I want to say that p values can be easily imposed in genomics data analysis.
00:32:11.848 - 00:33:10.000, Speaker B: But before we talk about p values, I think a more important question is, should a scientific question be formulated as multiple testing? For this question, actually, I collaborated with Doctor Xin Tong at USC, who was also Rachel's collaborator and who was professor Jian Qin Fan student. You can see we have a social network here among researchers working on different things. So we actually. So from my conversation with Shin, we both realized that our students and my biology collaborators are often confused about whether some multiple binary decision problems should be formulated as a multiple testing problem or as a binary classification problem. So, facing that issue, we wrote this article to conceptually distinguish their users and their application domains. So if we answer is yes, yeah, we should formulate this as a multiple testing problem. In other words, multiple inference problems.
00:33:10.000 - 00:33:54.006, Speaker B: Then I talk about the three causes using three examples, and I propose this FDR control framework, motivated by the knockoff and our simulator for generating null control. Beyond that, the simulator can provide a way for fair benchmarking of computational tools. We can see that from this website. The single cell field has more than 1000 tools already, so offering users some guidance is necessary. All right, so these are the publications I mentioned in my talk. And finally, this is something I want to share, because I know in this audience we have so many luminary statisticians. So I want to hear your thoughts about this problem, and I welcome any comments.
00:33:54.006 - 00:34:32.112, Speaker B: This is another work I didn't talk about today. It's about a problem in statistical genetics called QTL mapping. And there were several methods proposed for inferring hidden variables from a matrix, like individual by gene expression matrix. And several popular methods were proposed over the last decade. And I have to say, my student, Heather, I'm really thankful for her, because it was during a project where she was supposed to use a hidden variable inference method as part of the problem. But she's so detail oriented. She said, oh, I need to understand the method before I use it.
00:34:32.112 - 00:35:33.152, Speaker B: So she studied those methods, and she found by surprise that PCA actually is the best. So, at least by our comprehensive simulations. So, in other words, we wrote this article, and we showed our results, and our paper was, I will say, enthusiastically reviewed. And this was one anonymous reviews comment. These results may come as a surprise to some, given the nearly uncontestable status that method a has achieved within the community. But sadly, they reflect the fact that computational biology methods can rise to fame almost by accident, rather than by sound statistical arguments. So I think this is a problem that we should address, right? If we think we can provide sound statistical arguments, then how can we make our voice be heard? How can we change the practices? So statistics is more, I would say, paid more attention or receive a higher status in the scientific community? Finally, I want to thank my advisors, Peter, for inviting me here to give this talk.
00:35:33.152 - 00:36:13.454, Speaker B: And of course, my collaborators, Doctor Weidi and his post doc Yumei at UC Irvine. Because without their collaboration, I wouldn't find a scientific way to justify, yeah, Wilcoxon is the way to go, or Wilcox is the way for that application or how we can justify PCA is better because I think we need to use their language to convince the users. And also my Tulsa Xin Zhou and former student yiling for their work on Clipper and Xinjiang for the de gene part, and Tian Yee and Dong Yuan for their work on the simulators and for Dongyuan for the Sutan B as well. And also the funding agencies for support my research. Thank you very much.
00:36:16.674 - 00:37:08.458, Speaker A: Thank you very much. So finally, I understand what my public transportation Clipper card is doing. So any questions or comments? I mean, you have made a very interesting point, of course, also at the end, right. Sound statistical arguments are maybe more or should be taken more into account. I mean, you work hard in communicating that. I mean, what is your, this is more a high level comment. What is your experience when you show these things to your more bioinformatics collaborators? I should say the HR author, Mark Robinson is my colleague at Zurich.
00:37:08.458 - 00:37:24.354, Speaker A: So I definitely will tell him about. But can you maybe expand a bit or you think this needs more? I don't know. Publication is one way of communication that we speak up more, or people like you or.
00:37:24.694 - 00:37:57.462, Speaker B: Yeah. Thank you. I think Mark Robinson must be aware of that, our work, because we had a lot of Twitter conversations with other DMF authors on Twitter. And I think these are very helpful to me because I think, I feel like I wouldn't, I want to. I think the key message is that, is that it's not, not like we're saying d six and HR do not work, but they have their application domains. So we have to just let users be able to choose appropriate method for specific data set. But I understand that's very difficult for users because they're not statisticians.
00:37:57.462 - 00:39:04.762, Speaker B: So I feel one message want to convey from that paper is the importance of collaborating with a statistician for the method choice. And also, I think another thing I want to say about the communication with bioinformaticians, or I would say, yeah, bioinformaticians are more, better collaborators we can communicate with than experimental biologists because bioinformaticians use methods so they can understand what we are talking about. But for experimental biologists, the gap is even bigger. So we need to. Yeah, I think the gap is even bigger. But anyway, I want to say that what I learned from Doctor Weili, my collaborator, is that he always asked me, so what? He said, you said what? But so what? What's the consequence? What's the scientific consequence? So I think the reason why our paper about the d gene comparison can receive such considerable attention was because we use that immunotherapy data set, because that data set shows a striking difference using different methods. And the scientific conclusion is so important that can raise the scientists attention to pay attention to the problem.
00:39:04.762 - 00:39:16.774, Speaker B: So I feel like showing the scientific community the importance of using statistics in a more rigorous way is a way to advocate our methods. That's my point.
00:39:17.194 - 00:39:18.562, Speaker A: Thank you, Nancy.
00:39:18.618 - 00:39:19.254, Speaker B: Yeah.
00:39:25.314 - 00:40:03.498, Speaker C: So I think this is a great talk and wonderful work. But one thing that I ran into just talking to the biology community, one problem, as you described, is that p values that are not accurate get thrown around. But the other problem is just that I think as a field, they pay a little attention to p values in a sense that in these high throughput experiments, you have fDr, it narrows down to maybe the top few hundred genes, all of which passes because there's just so much signal, and then they pick the ten that has the most.
00:40:03.666 - 00:40:04.562, Speaker B: It doesn't have to be at the.
00:40:04.578 - 00:40:14.586, Speaker C: Top ten to do some high throughput experiments on those ten gets done one by each undergrad or postdoc, and then in the end the paper focuses on one.
00:40:14.650 - 00:40:15.974, Speaker B: Yeah, that's right. So.
00:40:18.554 - 00:40:22.538, Speaker C: What are your thoughts are on the role of p value in these studies?
00:40:22.626 - 00:40:52.246, Speaker B: Thank you. Yeah, that's a very good question. I also had a lot of discussion with people on this. Interestingly, as people, I think one biologist just explicitly told me, I don't trust peer values, but we need them. We need them to justify our procedure, to say, oh, that's how we get to that one discovery. So I don't like this. I feel like they are using statistics more like a declaration or like a rhetoric rather than scientific argument.
00:40:52.246 - 00:41:26.230, Speaker B: So, but I want to say that I didn't show the result here, but we do have results showing that it's not just the threshold changing using different FDR control procedures, it can also change the ranking. So if we change the ranking, the top ones may be different, like in that immunotherapy case. Right. If we use the Wilcoxon, we may think there's no genes to be verified. We need to maybe increase the sample size and then do the analysis again. But if we use these tool, we will be super enthusiastic and go ahead, do the experiments. I think that can change the practice.
00:41:26.230 - 00:41:47.004, Speaker B: Yeah, I agree. I feel like what we need to tell them is that it's not just publishing papers, it can change the scientific conclusion and hopefully they will be convinced. But my personal experience is that it's easier to convince computational biologists than experimental biologists. Because they understand our tools much better. Yeah. Thank you for the comment.
00:41:47.984 - 00:42:05.464, Speaker A: Okay, so let's thank both speakers again for their nice presentation. And it's lunchtime, and we will resume at 140 sharp. So please don't be too late for the next exciting session.
