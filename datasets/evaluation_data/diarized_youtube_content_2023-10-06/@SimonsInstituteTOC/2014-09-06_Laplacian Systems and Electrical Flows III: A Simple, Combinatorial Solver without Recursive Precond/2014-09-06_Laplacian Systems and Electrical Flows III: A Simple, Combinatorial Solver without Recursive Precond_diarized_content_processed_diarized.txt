00:00:00.320 - 00:00:48.566, Speaker A: Last two lectures, but with a very different approach that will hopefully avoid the machinery. And so the last one, I kind of accelerated as time went on, partially and largely it was because it was not one lecture worth of material that I was trying to get in there. And so I think the biggest selling point we're about to talk about today is that. Or this part is that it actually is that I'm going to, if I don't get too sidetracked, more or less, give you a full proof of an entire laplacian solver in the course of one lecture, modulo the construction of a low stretch spanning tree, which I think I'd only need another 15 minutes for, to get an almost linear algorithm. So, let me kind of tell you how to do this without preconditioning. And this talk is based on recent work with. Let's see if any of them are in the room with Aaron Sidfer and Lorenzo Arecchia and Zoan.
00:00:48.566 - 00:01:17.600, Speaker A: Zoom. There's Aaron. Aaron. Okay, so anyway, what's the idea? The idea is, actually I'm going to try to do a combinatorial algorithm instead of a numerical one. And, you know, the last one, I buried a lot of the combinatorics inside the construction of a low stretch fanning tree and recursive preconditioning here, we're going to make it very explicit. And what's I think, interesting about it is that it's intrinsically a different structure. It's not, I won't say the previous ones were in some sense multi grid ish.
00:01:17.600 - 00:01:49.802, Speaker A: They weren't quite multigrad multigrid, but they had the same moral feeling. They were preconditioning with some structural understanding of the kind of combination of long paths, and then using some sort of core structure and fine structure. This is nothing like that. And it's not going to use any of the machinery I developed. It won't be recursive, it won't be conjugate gradient at all. But you'll see that actually the intuition has certain strong overlaps, and the whole algorithm is really just going to fit on a blackboard. Actually, the whole algorithm is going to require a nice spanning tree where gneiss is a low stretch spanning tree.
00:01:49.802 - 00:02:29.900, Speaker A: And for people who missed the last one, I'll remind you what low stretch spanning trees are. So it's going to, all it's going to do is take a nice spanning tree, and then it's going to be an iterative procedure that is about five lines of pseudocode and data structure, a very simple, lightweight data structure, and that will give us a linear algorithm. The convergence proof is not going. So the last one, I actually really hid the convergence proof. Like, I waved my hands at a recursion and then waved my hands at an ability to analyze the recursion and then said, make all your numbers infinite precision. Um, and so here the convergence proof is there's an energy, it goes down every step by a multiplicative factor. So if you do that enough, it gets small.
00:02:29.900 - 00:02:58.880, Speaker A: And if it's on one blackboard, um, and no Chebyshev polynomials, no recursive preconditioning, no conjugate gradient. Okay, that's the setup. Uh, let me give a couple quick comments. The first one is that numerical stability of this is super easy to prove, which I think is nice. So the other one, actually, not only was it non trivial, it's sort of, I said not known. It's probably known by Richard Kang and eventually will exist. It's super hard to.
00:02:58.880 - 00:03:46.904, Speaker A: So, the algorithm I gave in the last analysis, to really make it rigorous in using things that are already out there in the literature, you need to use poly logarithmic precision. So it was n log squared n arithmetic operations, but each of them took a little longer. If you do them with, if you get charged for your arithmetic, that's probably unnecessary, and at least for the ones that I described before, Richard says he can make the analysis go through, but it's really not trivial. It takes some real work, and it's going to be a completely easy analysis here. When you charge for high precision arithmetic and pretend that you don't get to talk to Richard offline, this would then, I think, win. In the model where you get sort of charged for doing arithmetic with actual words, and it's a little slower if you don't. So it's going to be an extra log factor or two slower.
00:03:46.904 - 00:04:46.684, Speaker A: And I also think the approach is broadly applicable. So what we're going to do is we're going to open up the black box of the solver, and instead of just sort of recursively preconditioning things and then kind of looking at the high level numerics, what we're going to do is actually open everything up. And instead of taking a bunch of these very expensive iterations, sorry, a small number, you know, if you think, I want to solve something in n poly log n time, if I'm going to do that, then I kind of need to make the number of iterations be logarithmic, because otherwise, each iteration is going to at least take the time to write down the whole vector and multiply matrix vector product. So it seems like you're stuck with a small number of expensive iterations, linear time iterations here. We're actually going to not do that. What we're going to do is we're going to shoot for a linear ish number of logarithmic time iterations, and that's going to make our library much easier. And the basic setup is how I think we teach people not to solve linear equations, which is you want to solve ax equals b, you find a constraint, one of the rows dotted with x equals b I.
00:04:46.684 - 00:05:04.690, Speaker A: That's violated. You find it, you fix it, and you repeat. And that's actually essentially the whole algorithm. It's going to replace all this. And what's interesting also is you have to, I said before, you should look for where the lines go. You know that line graphs are your problem. The recursive precondition one did it by the greedy elimination step.
00:05:04.690 - 00:05:38.056, Speaker A: And the recursion right here, what we're going to do is do it with a very simple data structure. And what's going to happen is that the sort of recursive preconditioning and small number of large iterations is going to become a simple data structure, which actually you can think of as choosing the right basis to work in, and then a large number of very simple ones. Simple iterations. Okay, so now I want to just remind you of some background that I think was in a couple of the other talks, but it's important enough for them to do, and it's quick. So I just want to put it up. So I want to remind you, Laplacians and electrical circuits and how they relate. So, you know, the general setup is recall.
00:05:38.056 - 00:06:18.282, Speaker A: We can interpret the Laplacian in terms of electrical circuits. The idea is that you take your edges of your graph and you declare them resistors. You make the resistance of an edge with weight w be one over its weight, right? So a bigger edge can carry more stuff and has lower resistance. And then in this algorithm, I'm not going to use very much about electrical circuits. I'm only going to really use a couple very simple facts about them. One of them is that what do you get when you put electricity on a graph? So what we're going to do is we're going to tie a battery to the Snt or a current source to sn. And you have two numbers that dictate what's going on.
00:06:18.282 - 00:07:10.834, Speaker A: You have a potential vector, the voltages, which is a number assigned to vertices you have a current vector which assigns a flow to the edges. And then if we let chi sub v be the net current out of a vertex v, then those will call the demands. So you know, think current that comes into or out of the graph. And for simplicity during the talk, it's nicer when you draw electrical circuits to not have current coming into and out of every vertex. So I'm probably fairly consistently throughout the talk I'm going to make the demands that we're trying to solve be chi of t is sort of in one unit of current into s and out of t. So you can think of that as solving LX equals a vector that's got a one, a negative one and a whole bunch of zeros. Literally nothing I do today is going to have any dependence on that other than it means when I draw my pictures I can.
00:07:10.834 - 00:07:37.262, Speaker A: The electrical analogy is a little clearer to see everything is linear. So if that works for that, it's actually kind of boring to work for everything else. But I'll just make it a little bit cleaner to draw stuff. Okay. And so what do we need to know about electricity? So the answer is that it exists and it produces these numbers, right? It produces a set of voltages and a set of currents. And the main properties we're going to need are the following. One is that it's related to Laplacians.
00:07:37.262 - 00:08:20.258, Speaker A: That certainly would be useful for solving Laplacians. And what we're going to see is that. So one thing that's true, and I think Nikhil said, was that if you solve the linear system lv equals chi, that correspond, the answer v is the set of electrical potentials you get. If I took an electrical flow that routes demands chi. So if I solve lv equals chi, st, the one and the negative one thing, then the vector v is going to be the voltages you would get if you tied a battery. I should call this a current source for people who actually notice that, right? You said one unit of current from s to t and b is the voltages. Okay? The other thing is electricity.
00:08:20.258 - 00:09:01.194, Speaker A: The real property we're going to use about electricity is that it obeys Ohm's law. So it says that there are these voltages v, and the electrical flow on an edge is vi minus vj over the resistance, right? So the potential difference divided by the resistance. And we're also going to look at the energy dissipated by an edge. So this is just, you know, you take, this is the I squared r. So it's the flow squared on an edge times the resistance, which from the above equation is the potential difference squared divided by the resistance. That is the power dissipation by an edge. And you'll notice that this directly ties to v transpose Lv.
00:09:01.194 - 00:09:34.034, Speaker A: So, okay, that's all we're going to need about electricity. The main points are there's an energy function which happens to be v transpose Lv. The energy dissipated by a single edge is just the edge's contribution to that electrical current. So they flow conservation, and they are a flow that comes from the potentials. And you'll notice that, you know, this actually characterizes them as kind of a crucial point. Note that most flows are not given by potentials. You know, most flows do not have the property that the flow on an edge can be written consistently as a potential difference divided by a fixed resistance.
00:09:34.034 - 00:10:28.434, Speaker A: And the main fact this algorithm's going to look at about all of this is just that you could, by taking one derivative. Notice that I've written, you know, one way to describe the answer to the question lv equals high is to minimize this energy. And so in particular, you could either characterize the electrical flow as a solution to linear system or you could characterize it by the single, the flow that routes your demands and minimizes energy. Are there any questions on that? I did that a little quickly because I think everybody, most people here already know what I'm talking about on this, but tell me if I missed anything. Okay, so that's actually all we're going to need about electrical circuits. And so this was our setup. The main property we're going to use about this whole setup is just that voltages are very restrictive.
00:10:28.434 - 00:11:25.760, Speaker A: The unique flow that has voltages is the electrical flow. And why is that an actual restrictive property? Well, if I were to tell you an arbitrary flow, then if I look at the flow on a bunch of different paths between two vertices, they probably tell you different things about how far apart, what the potential difference should be between vertices. If something comes from voltages, then in particular if all the flows actually are voltages, the flow on every edge tells you the potential difference around the edge. And so in particular, if I take any cycle in the graph and what I do fere, is the potential difference as I go along the edge. If I go around the cycle, I certainly should get back where I started in potential. If I start at a given point, I say what's the new potential as I traverse the cycle? If they actually are consistent potentials, I'll get zero, right. That is actually essentially the whole algorithm, our algorithm, is going to just use that fact over and over.
00:11:25.760 - 00:12:00.364, Speaker A: So all the recursive preconditioning is going to get replaced with the following algorithm. Randomly sample a cycle c from a distribution I'll tell you about shortly. Check if when you go around that cycle, your flow. So we're going to try to, instead of finding the potentials, we're going to try to find the electrical flow. So we're not going to compute v, we're going to compute f, and then we're going to use that to find v by exactly these equations, by taking potential differences. And all you'll do is just randomly sample a cycle. Check if when you take your flow and you go around the cycle, it's consistent, it gets back to where it started.
00:12:00.364 - 00:12:25.010, Speaker A: If it isn't, fix it. How do you fix it? Well, you add the appropriate multiple of the cycle to the flow to fix it, and then you repeat. And that turns out to give a new linear time algorithm for laplacian systems. Good. Okay, so now let me actually fill in a little bit more in the details. So here's my graph. And again, I'm going to draw this with s and t.
00:12:25.010 - 00:12:52.210, Speaker A: And to make this a little more precise, this is a slightly coarse, slightly finer grained overview. And then I'll fill in one more round of details. What you're going to do, as your algorithm is you're going to initialize it with any reasonable st flow. So here, a path is the right thing, the shortest path, for example. And then the algorithm will do the following. It will call that a flow, right? So I want to route the demands of, I want to route one unit from s to t. Start with anything that routes the demands.
00:12:52.210 - 00:13:15.164, Speaker A: A short, say a good path. And then this is a flow, but it's not the answer, right? And the reason it's not the answer is if I go around cycles in the graph, it's clearly not conserving potential. By the way, I drew arrows on the graph. Note, this is still an undirected graph. This is just, when I talk about potential differences in flows, I know what sign to put on the value. So again, don't the arrows are there to indicate which way flow is going. It's still an undirected graph.
00:13:15.164 - 00:13:41.584, Speaker A: Okay, so the algorithm is repeatedly do the following. Randomly sample a cycle and then compute, as I said before, compute the sum around it. So here, you'll notice that. Let me do this in two steps. Notice that as you go around this cycle, you don't get back where you started, you get two instead of zero, right? So what we're going to do is add the right multiple of the cycle to fix that. So it's like two sevenths maybe, and you end up with this. So now we have fixed this cycle.
00:13:41.584 - 00:14:19.356, Speaker A: Now you go around this cycle and it still adds up to zero, but you've probably broken something else and you're just going to repeat. So to turn this into an actual legitimate algorithm, what are the main things I need to do? So, I need to tell you what cycles to use and tell you what probability distribution to use to sample them. I need to tell you how to implement the updates. So in particular, these cycles could be quite large. It's possible that the cycle could include most of the edges in the graph. And it would be nice to not pay n to update a cycle, but just to pay log n no matter what cycle you have. And that's actually a very easy data structure.
00:14:19.356 - 00:14:53.454, Speaker A: And then the last thing I need to do is convince you that you don't need to do this too many times. And so, to sort of put the whole talk into four bullet points, the cycles we use are going to be the cycles formed by adding an edge to a low stretch spanning tree. So, and I'll do these all in more depth, the sampling probabilities. What we use is that I'm going to sample each cycle with a probability proportional to the stretch plus one. The plus one means essentially you're counting the edge too. So it's this, the total length of the cycle. The updates are going to be a simple data structure link cut trees would suffice, but are aggressive overkill.
00:14:53.454 - 00:15:43.492, Speaker A: They're way more complicated than necessary, and the number of iterations is just going to be linear and it's going to roughly be proportional to the stretch of the tree. And that's it. So now let me go one more level of filling in the details for each of these. So I want to remind people what the spanning trees and cycles and stretch are, just because the last lecture that got a little bit compressed and I don't want to compress it too much here, because it's really the only combinatorial, the only piece we really need. So given the spanning tree, first of all, I claim this gives us a basis for this base of cycles. So for every edge that's not in the tree, you get a cycle and you get a path. So if I give you this edge e, and I fix the tree, then I'm going to write pe for the path in the tree that routes the edge.
00:15:43.492 - 00:16:27.764, Speaker A: So, right, so this is pe, and you can also get a cycle which is just putting the edge into the path. And so you have this, given every edge knot on the tree, you get a cycle and you get a path from the tree and the stretch, as I said last time, but I'll say it a little bit more slowly this time. The stretch is the ratio, think about the ratio of how far apart the vertices are if you sort of route in the tree or route along the edge itself. So what it is, is it's the ratio of the length of the path where you measure length and resistance, the total resistance of the path and the resistance of the edge itself. And it's that ratio. So in particular, if let's just for a second pretend the graph is unweighted for simplicity. In that case, the stretch of an edge is just the length of the path that wraps it in the tree.
00:16:27.764 - 00:16:56.954, Speaker A: And then, so here it was six. You could then define a cycle which just adds in one more edge. You could get the cycle resistance, which just adds in, it's just the total resistance of the whole cycle, including the edge. And note, that's just the resistance of the edge times one plus the stretch. And it's actually not a bad idea for much of this talk just to pretend your graph is unweighted, because it makes the picture clearer. But nothing we use will depend on that. And here the cycle has resistance seven.
00:16:56.954 - 00:17:36.214, Speaker A: And then we can define the total stretch of the tree to just be the sum of the stretch of all the edges, including the tree. And the number that's actually going to dictate our performance is going to be the stretch of the tree plus m minus two, n plus two. So as long as you don't have a super sparse graph, just think of this as stretch of t. So this is just here so I can say something formally correct, but think of it as just stretch of t plus n. And remember, the stretch of t has to be at least m, just because you're including all the edges. So just think stretch of t. But this is the formally correct one.
00:17:36.214 - 00:18:06.992, Speaker A: Okay, so low stretch spanning trees, as I showed last time. You know, there's different ways of putting a spanning tree on a graph, and they're not all equally good for what we're going to talk about. And the property we care about is the total stretch. And so for the grid, I drew this one, which you'll see has total stretch n to the 1.5, again, because all sort of, at least half the vertices are stretched by like root n over two or something. And then the good thing to do is the one that captures the locality and the graph structure correctly. And you'll notice that this is actually where all the combinatorics is buried.
00:18:06.992 - 00:18:55.834, Speaker A: So last time I promised that we were using graph structure a lot and then I didn't have to talk too much about graphs other than effective resistances and where did the graph decompositions and whatever that I promised must go. They come into the construction of low stretch banding trees and I not sure if I'll have time to do them. I have slides after my last slide with them and I'm happy to show them to people. And if I accelerate enough during the talk I might get to them, but most likely I won't. They're not that complicated, but they are strongly using the combinatorial structure of the graph. Okay, so the assertion we're going to use is that in very close to linear time we can find good low stretch bandage and it's just basically a collection of breadth first searches. It's not a, it's not a real, we're not really burying anything super complicated inside it, but from an algorithmic standpoint, like it's not slow.
00:18:55.834 - 00:19:31.054, Speaker A: Okay, so there aren't hideous constants here. Okay, so now it's down the number of iterations. So how do we bound the number of iterations? It's really just a potential function argument. So the idea is we're gonna define an energy, right? We're trying to do minimization, we're trying to minimize the energy over all st flows. So what we're going to do is we're going to forget that we were actually doing linear system solving and looking for vertices. We're just going to look for the flow that minimizes the energy over all flows that rudder demands. Where here we're making an st for simplicity and we're just going to track the energy.
00:19:31.054 - 00:19:59.834, Speaker A: So what we're going to track the energy, what we want to show is that it we're going to look at essentially the difference between our present energy and the optimal energy and we're just going to show it goes down multiplicatively every step. To do that, we need to have some way of understanding a bound on the optimal energy. Like we have to know how far we have to go. So we're going to write down the dual. So if that's our primal optimization problem, you can write out the dual. What is r? R is the resistance of the edge. So it's one over the weight.
00:19:59.834 - 00:20:13.826, Speaker A: I see. So that little r. Oh, sorry. The r is the vector of resistances, so. Right. So the r e assigned. I meant that r.
00:20:13.826 - 00:20:25.170, Speaker A: Yeah. What, why is. Oh, this is a definition. Oh, I wrote it sub r to indicate that it depends on everything. Yeah. Okay. Right.
00:20:25.170 - 00:20:58.278, Speaker A: So that's just the definition of energy with respect to resistances. And the dual is just the dual. It's a pretty simple dual, actually. And if you look at what it's doing, it's other than kind of annoying scaling. You should think of it as like, here's kind of the intuition for it. The intuition is that if you're minim, you can think of energy of the electrical flow. On the one hand, as you take the flow that minimizes energy r squared, you know, I squared r, the sum of the dissipated power overall flows, you could and fit when you fix the sending one unit of current.
00:20:58.278 - 00:21:44.294, Speaker A: Alternatively, you could imagine that what you'll do is you'll nail down the potentials of s and t, and then you'll look at the minimum of this thing. This should be equal to the energy contribution for each edge. And you could describe the electrical flow that sends a unit from s to t as either the thing that minimizes the energy overall flows, or the thing that says you fix the potentials appropriately and then minimize the energy overall potentials. Meaning let the spring sort of relax. Now, the reason it looks a little weird is that the potential difference that gives one unit of flow isn't one. It's a thing with the effect of resistance in it, right? And so you get this scaling out of that. But morally, the point is just that, on the one hand, you could minimize overflows, minimize energy over all flows, and get.
00:21:44.294 - 00:22:23.762, Speaker A: And you get sort of one side, right. A feasible flow gives you an upper bound on the energy and the effective resistance. Or if I give you a collection of feasible resistances, that's a lower bound, because it's an upper bound on one over the effect of resistance. And so that's the primal and the dual. Does it matter which is t? So the s and t here are here, because I said that for the picture, I'm going to be routing chi stuff. What I should, if I wanted to write this in full generality, all I would do is just say over all flows that route the demands chi and keep every equation the same. So I'll keep the right equation.
00:22:23.762 - 00:22:53.654, Speaker A: Actually, this would just be chi dot v. It's just a little messier to have the potential's intuition when you're sending stuff into and out of every vertex is why I did that. But nothing I write just erase every time you see a one and a negative one, just call it chi and we're done. It will change completely unchanged. It will go through coming changed. And we're just going to track the energy of the flow f versus the energy of f opt. And all we're really going to use is that the primal is bigger than the dual.
00:22:53.654 - 00:23:28.342, Speaker A: So for any flow, f and v. Note that we could, for voltages, we can compute an energy or compute an objective of a dual function, which more or less with energy, with the appropriate scaling. Right? So given voltages, you get an upper bound, given flows you get a lower bound. They're appropriately compared. This is a lower bound, that's an upper bound, and that's all we're going to use. So we'll define the gap to be the duality gap. It's going to be the difference between our present estimate of how far we could be.
00:23:28.342 - 00:24:08.234, Speaker A: We have a present flow. We're going to look at the difference between what we present, we know it to be and what we actually have, right? So this is between our mathematical feasible potentials and a feasible dual. This is the difference between the two. And just note in particular that my lower bound on the dual is at least the actual answer, right? So this is bigger than the actual distance from optimality. And what we're going to do is we're just going to show the following. This is the main inequality that gives us our software. It's that the gap is that what we'll do is we're going to show that the expected energy improvement in every step is going to be one over tau.
00:24:08.234 - 00:25:06.514, Speaker A: So I got buried, okay, so I had a slide that said a little bit more neurotically, what the potential, what the probability distribution was. So I'll give you a probability distribution in a second. And the expected energy improvement in one step is just going to be one over tau, where tau is the total stretch of the spanning tree, the number I wrote down before with the n minus n two, n plus one thing thrown in. And all we're going to show is that the gap is going to multiplicatively decrease by at least one over tau times the distance from optionality. And then that's it. Then you take tau iterations. Okay, so to tell you the energy decrease of a step, let me just, first, let's just see what happens with one individual edge, and then I'll tell you a bit about the probability distribution.
00:25:06.514 - 00:25:57.960, Speaker A: So if I pick one off tree edge e, then let's figure. What I want to check is suppose I fix the spanning tree, I pick one off tree edge e, and I'm going to ask, when I fix that cycle, what happens to the energy? So the algorithm is I'll pick an edge from some distribution, I'll look at the cycle, go around it and see what happens. And the question is, what does that do to our actual energy? So how much we're going to send around the cycle is going to be the thing that will make the cycle zero, which will actually be the thing that will optimally decrease the energy. And let's just define delta sub cell to be the thing that should be zero, but isn't. So it's going to say that if I take a fixed cycle delta of it will be the thing that says you go around the cycle and you add up what the potential change is along every edge. And it's what you get. It's what the difference around the cycle is when it should be zero.
00:25:57.960 - 00:26:14.358, Speaker A: Right. It's the sum of the potential drops around the cycle. Those primes are on the e's, not on the f's and r's are. Yeah. Yes, they are. Sorry, they're just very superscripted. Yes.
00:26:14.358 - 00:26:35.418, Speaker A: Good. So it just says I go around the cycle. It's what the potential difference is when I go around it. And each flow, each unit of flow you send around the cycle changes this by the total resistance of the cycle. Right. So if I push one unit of flow around the cycle, the total resistance is the change in the. In this quantity.
00:26:35.418 - 00:27:12.294, Speaker A: Are there any questions on this? Feel with me. Good. So to make it zero, you just, what do you add? You add that thing over r eight times the cycle to the flow, the thing that makes it actually equal zero. And you can check that that actually is the thing that minimizes that over all multiples of the cycle. To add this is the thing that makes the energy go down the most. And you can check what does to the energy if you just write out the algebra on it. The energy change is just the stuff we corrected the delta ce squared divided by capital rb divided by the total resistance of the cycle.
00:27:12.294 - 00:27:33.920, Speaker A: So ce is a cycle, right. It's in flow space. Right. So ce is I'm adding something to the flow. The idea is the cycle doesn't add up to zero, so I'm going to push flow around the whole cycle. So what does that do to the flow? I actually add a multiple of the cycle. This is an indicative function.
00:27:33.920 - 00:28:02.482, Speaker A: You should think of this as an indicator. Yes. And that's the change in the energy just by writing it out. So now let's start analyzing a bound on how far we are for optimality. The algorithm we have, this is how much progress we make when we fix something. Like when we fix one cycle, this is how much progress we do make. Now let's compare it to how.
00:28:02.482 - 00:28:41.050, Speaker A: So to know what has to happen, we have to know something that relates this value to how far we are from optimality. Like we want something that says when you look at how far you are from being optimal, you can somehow relate that number to the expected value of the. So for whatever distribution. Yeah. Update to f, right? And you measure the gap, some function of f and some function of v, which is the dual. Right is that you're not updating v as well. So the algorithm doesn't need to, the algorithm never has to during the iterative part of it, do that.
00:28:41.050 - 00:29:33.914, Speaker A: So the algorithm really is, I pick a single cycle, I'm going to pick the cycles. I might as well just say what the probability distribution is. Decrease energy, you mean decrease the gap, right? No, it's going to decrease the primal energy. So the idea is what we're going to say is I'm actually going to maintain only a flow. Given just a flow, I can compute a number, which is when I go around the cycle, what happens? What's the potential that I get back to? What's the total potential around the cycle? And that's supposed to be zero, but isn't that's going to be this delta? And all algorithmically you do is take that number, divide by re, and push that much more around your flow. All use of gaps and stuff is just in the analysis. And what you're going to show is that when the duality gap is big, you're going to expect to have a lot of, you're going to expect that this will make a lot of primal cycles, that the energy will go down a lot if a lot of cycles are badly violated.
00:29:33.914 - 00:29:53.706, Speaker A: Does that make sense? Isn't there a potential you get from the spanning tree given a flow? Sorry? Somewhere in there, isn't there a potential that you generate given a spanning tree and the flow. Not yet. Okay. Not yet. We'll leave that to downside. Not there yet. Right.
00:29:53.706 - 00:30:34.840, Speaker A: So, so far I haven't needed to. Right, so far all I needed to do is just, I'm just saying what happens when you fix one cycle and it doesn't actually depend on the spanning trigger? All this is just a function of a cycle and then we'll see how we'll talk about that when we actually relate this back to the cycles we're getting from the tree right now. This is true for every cycle, not just for tree cycles. Okay, so now let's get a bound on distance from optimality, and it's just going to be more or less straight from the duals. So now this is the answer to Gary's question. So somehow what we have to do is convince ourselves we have to somehow relate back to the distance from optimality. If we're going to say we make progress, that's one minus one over tau of the way to the answer.
00:30:34.840 - 00:31:30.204, Speaker A: And somewhere along the line we have to bound how far we are from the answer in terms of how violated things are. And what we're going to do is we're going to take our present flow. Remember, it's a primal only algorithm. So far in our analysis, what we're going to do is we're going to construct a collection of potentials. And remember, potentials will give us a dual vector and then we're just going to use that to, and then that's a feasible, that's going to give us a feasible dual, and we'll use that to bound the gap. And the straightforward calculation we're going to do is going to show the following, that if I give you a fixed flow and a fixed voltage, again, assuming I have them from somewhere, which we'll say in a second, then if I compare energy of the flow minus the voltage, the dual energy from the voltages, then, well, I claim that this is the gap, you gap between those two numbers, flow and voltages, and I just compute the difference between their energies. I claim it's got this very nice interpretation.
00:31:30.204 - 00:32:01.354, Speaker A: So what does this nice interpretation tell you? It tells you that you have two things that are supposed to match up. They're supposed to be equal. One of them is the flow on an edge. The other is the potential difference divided by the resistance. This sums over all the edges in the graph weighted by the resistance, the difference between those two quantities squared. So it's the difference between what the flow is and what the voltages think the flow should be. And now, in order to actually make this work, we have to figure out how we're going to construct potentials.
00:32:01.354 - 00:32:37.454, Speaker A: So the way we're going to construct potentials is exactly what Gary asked about. So again, this is all just for the analysis. Remember, the algorithm really is just independently sample cycles from the distribution. I said before, how are we going to construct the potentials? What we're going to do is we're going to use our low stretch banding train. So we want potentials that match our flow somehow, right? Like we want to get the best potentials to make these two things match up. But there aren't any potentials that match our flow because remember, the whole point here was that what we're going to do is kind of work towards consistency. We're going to maintain a flow and work towards it being specified by voltages.
00:32:37.454 - 00:33:07.702, Speaker A: So if it actually ever mapped consistently to any set of voltages, we'd get a zero and we'd be done. But we're going to. So what we're going to do is just pick something we know we can do consistently and just use it as a guess. So what we'll do is we're going to take our tree and note that as long as you don't have any cycles, you can always consistently define voltages because you just start at the root of the tree and you just make the potential differences be what they're supposed to be. And they never conflict because they're no cycles consistently or uniquely? Uniquely consistently. Yes. And so this is how we're going to pick the potential.
00:33:07.702 - 00:33:54.760, Speaker A: So what we're going to do is we have our flow. The potentials we'll use for a dual bound will just be the ones you get by only looking at the tree and using those to specify the potential differences on the tree edges and then everything else that fixes all the voltages. And so I wrote this out with an equation just for later use. We'll define the voltage over text a to be the sum over all edges in the graph, you should pick a root. Might as well make an s. For this picture, you'll sum over all, you'll take the path from s to a, you'll just sum the potential drops over all the edges. Does everybody make sense? People with me? Okay, so note that for an edge in the tree, this thing is zero, right? The fe is fe re.
00:33:54.760 - 00:34:46.640, Speaker A: So this thing multiply this by re. The potential drop in the what's supposed to be what it is are the same. And for non tree edges, the nice thing about it is that it's actually directly something we can read off from the delta ce that we talked about. So the difference between fe re, the actual value, and the vi minus v j I claim is just the potential difference as you go around the cycle, right? Because it's the two paths that you use to get from one endpoint to the other, the true path and the edge. So delta ce is just that value. And so in particular, the gap between these potentials and the flow is exactly the sum over all non tree edges of delta ce squared over re. Right.
00:34:46.640 - 00:35:42.512, Speaker A: So what this says in words is, it says that if you're far from, our goal is going to be to minimize energy, which we'll think of as working towards, we have a flow and we're going to work towards it being consistent with potentials, which are the optimality constraints. And this says if you're very far from being consistent with, sorry, if you're far from optimality in terms of energy, that's the same thing as being, as violating a lot of these cycle constraints fairly substantially. Right. You can write your distance from optimality in terms of how much the potentials don't agree along the tree. Okay, so what have we showed? We've shown now that the difference between the flow, present flow, our optimal flow, is bounded by the gap because the gap, and we've also computed the gap. So now let me just say out loud, very precisely, because I think my slide got varied what our distribution is going to be. So the algorithm is, all it's going to do is the following.
00:35:42.512 - 00:36:19.920, Speaker A: It's going to take only cycles from the fundamental cycles of low stretch spanning trait and it's going to sample each cycle proportional to the total, to its stretch plus to its total stretch. Right? So you're just going to sample them independently with that probability. And it's not going to change per iteration, it's just the fixed distribution at the beginning. You don't have to recompute anything. And then what we're going to do now is just look at the expected improvement in energy. So if we choose an edge e, that's the decrease in the energy we computed. And we are going, what we're going to do is we're going to choose edge with probability one over tau.
00:36:19.920 - 00:36:52.874, Speaker A: That was the total I defined before. This is to make the probabilities add up to one. And then the ratio of re to lowercase re, the total cycle resistance to the edge resistance. So the stretch of the edge plus one. And now let's just look at the expected energy improvement. The expected energy improvement is just what, it's the sum over all the edges on the tree of the probability times the improvement. And if you write it out, what do you get? We get exactly one over tau times the sum of delta ces squared over rn, which is just one over tau times the gap.
00:36:52.874 - 00:37:31.904, Speaker A: And that actually is a linear system solver almost. So what have we now done. We've now actually shown that our improvement is one over tau times our gap. So we've shown that the primal energy improvement is boundable in terms of the gap, which is bound on how far we are from optimal. So we'll just fill it in, right? So if I look at, let's say Di is the difference, not the gap, but our distance from the actual Oct, not our lower bound in the Oct. Then what we get is that the expected value of di is multiplied by one minus one over tau. So Di is at each iteration where our distance from optimal energy is.
00:37:31.904 - 00:38:00.834, Speaker A: And what we just showed was that for a given di, the expected value of di is less than or equal to one minus one over tau times the expected value of d minus. And we have to be a little careful with not screwing up our expected values. You can't really always just multiply stuff, but this one will work out. So what we'll do is we'll bound the initial energy by the total stretch of the tree times the optimal energy. And that's not too big a deal. You just have to pick a decent starting flow. So here, our shortest path works great.
00:38:00.834 - 00:39:14.886, Speaker A: And then the expected distance from optimality. The expected distance from the optimal energy is now just going to be by induction, one minus one over tau to the k times our starting energy. And so in, you know, so just the standard analysis of multiplication, you get that if k is like tau times log stretch over epsilon, then you get with an epsilon of optimality. And I haven't yet shown you how to implement the updates and stuff, but other than that, that actually is the whole algorithm. Any questions? Yeah, I was wondering whether starting point in some way, or the other small number of steps, something like, you know, for example, iteration precondition with the three. It wouldn't surprise me if the answer was yes. So the way that I should have said to do this, in the general case, the easiest way to do this for arbitrary demands is to use the low stretch spanning tree.
00:39:14.886 - 00:39:50.044, Speaker A: That's where our initial voltages will come from. So the way we'll get our initial guess is actually going to be our initial. We'll get our initial things from routing along low stretch banding tree. You probably can do better. I mean, I guess it's a trade off of how much you want to work to get the starting point and how there may be a sequel iteration that gets you. I would not be surprised if there was something a little better you can do. In our paper, we play with, we shave off some logs by actually playing with things that look roughly like your paper, your blown up tree paper.
00:39:50.044 - 00:40:25.784, Speaker A: But we use some of that a little bit to shave off some logs. Some of this is a little bit less necessary because of a paper that Aaron and the enthalpy wrote that is going to shave off a couple more logs anyway, which I'll get to at the end. But yeah, this is meant to be a little bit crude and just give us pop logs. There are slightly better things you can hope to do. Yeah. So if you have short cycles, yes. The question you have to worry about is the overlap of them.
00:40:25.784 - 00:41:04.004, Speaker A: So the thing that makes this a little tricky to parallelize if you don't work fairly hard at it. I mean, I would love to parallelize this, and I think that in quote, real life it's very plausible that it could be parallelized. The theoretical obstruction is that when you do the updates, you have to figure out you need a lot of sort of non intersection along the cycle, so you can simultaneously do the updates, or you need a data structure that will handle all that somehow that's the main obstruction to parallelization. But it's not implausible to me that this can be done. I don't know how to do it. The other thing is, note by the way, that I use the low stretch banding tree. Notice I gave you say, a low diameter graph, like a power law graph.
00:41:04.004 - 00:41:31.624, Speaker A: I could use a breadth first search tree. So I don't actually know another algorithm substantially simpler than the full kudis, Miller, peng or one of the other recursively preconditioned solvers that solves, say parallel graphs. This take a breath for a search tree and randomly sample cycles. That's it, right? So it's even non trivial in sort of simple cases with low diameter. Okay, so now let me just tell you the last thing you need to know. Oh, sorry. Let me write out a couple more steps.
00:41:31.624 - 00:42:07.070, Speaker A: So, for the low stretch tree, now is where the stretch comes in. So the interesting point was that you can run this algorithm for any spanning tree, but it only works well for the right spanning tree. It only works well for a low stretch spanning tree because we actually got the number of iterations as proportional to the total stretch. And that gives n log straight n. The log one over epsilon is nice to have. Now, note that you can actually, I made this a probabilistic algorithm that will probably improve the energy every step. But it's easy enough to check what your error is because you can compute a residual.
00:42:07.070 - 00:42:44.436, Speaker A: So you can turn this into an algorithm with a probabilistic runtime and a promise that you'll get the right answer, as opposed to a probably right answer. With a fixed runtime, you can tighten the log factors with a bit more work. And really all that's left is for me to convince you you can quickly update cycles and maybe do less. Regulars so why do we need to do this? It doesn't seem like we necessarily even needed to add a structure. But the reason we need a data structure is just that these cycles, they are on average fairly short. I promised you that the average cycle is log n length, but we're sampling them non uniformly. We're sampling them with probability proportional to their length.
00:42:44.436 - 00:43:24.696, Speaker A: So we're actually biased towards the bad cycles, towards the ones that are big. And so if what we were going to do is actually have to pay, if we did the naive way representing everything kept a vector of things on edges, then we could have to pay a lot for what long cycles. And there's no obvious reason why we could expect each iteration to be fast enough that we get a good running time. The expected length of a cycle might be bigger than we want. It might not be long n because we're biased towards the heavy cycles. So all we need to do is make a data structure that fixes that. The other thing is the stress is proportional to resistance, whereas the cost of updating a data structure would be the number of edges.
00:43:24.696 - 00:43:53.508, Speaker A: So in weighted graphs, the hope that low stretch will balance out of this is even less legitimate. Okay, so let me tell you what the data structure has to do. So what the data structure. So all we're going to do is support very simple queries. And again, the data structure link cut trees do this, but they're dynamic trees, and they're complicated because they're doing very non trivial things on dynamic trees. And we're only going to need static trees because we never change the tree or any of the members of this algorithm. So they're gross overkill.
00:43:53.508 - 00:44:35.984, Speaker A: And so there's a very simple way to do this without using them. So, okay, so what do we need to be able to do? We need to be able to keep track of the flow, and we need to be able to quickly compute the sum around the cycle of the potential drops. To do that, it's really enough to be able to query and update the tree potentials because the tree is where something interesting is happening. You can just, you can store each edge by itself and just add it into the values on the tree to get the things around the cycle. So the interesting behavior is what happens on the tree. And what we need to be able to do is compute the voltages that you deduce from the tree and then you can compute everything else. So what we need to be able to do is compute this va, which is the potential difference the tree proposes between the pair of vertices.
00:44:35.984 - 00:45:06.444, Speaker A: And we need to be able to do this in time, ideally log n, so a time that doesn't depend on how far the vertex is from a on the path length. And we want to be able to do this with updates that add alpha units of flow to a fixed edge. So the idea is that we want to be able to say here's a little bit more flow on an edge, and this edge could be in a whole bunch of cycles and a whole bunch of tree paths. So we don't want to have to pay for that. So this is one of the operations, length cut trees support. But it's like aggressive overkill. And it turns out you can do it with a very simple data structure in o of log n per operation.
00:45:06.444 - 00:45:59.144, Speaker A: And what's interesting actually is the data structure itself is just a spark basis for, it's actually a sparse linear algebraic basis for everything we write down. Okay, so let's see, how much have I left? Ten minutes. So I think the last ten minutes of the talk I'm going to do a chooser and adventure. I can do either the data structure or I can try to get a sketch of how to construct low stripe spanning trees slightly wish take a poll, or I can just pontificate. So raise your hand for pontiff, raise your hand for illustrative spanning trees data structure. I will do the worst case scenario, the combination of the two badly. So this is the data structure for a line.
00:45:59.144 - 00:46:36.634, Speaker A: The line is actually the hard case for it. Roughly what you want to be able to do. The question you're going to ask on a line is I want to be able to do operations that add numbers to these edges and I want to be able to compute the sum of those numbers between here and the endpoint. And basically you just build a binary tree on top of a line and then you keep track of the cumulative sums over the first totally vertices and you add a plot of them. That was the data structure badly. So again, all it is, is it's basically just that you take your tree, you divide it in half and you can hit and repeat. So this is for a line and the line is the worst case.
00:46:36.634 - 00:47:10.962, Speaker A: More generally you just build a binary decomposition of your tree and it's a little and you have to only add a block intervals, okay, onto low straight span entries, which I will not rush quite so badly. So this was my pontificate part of it, which was that it doesn't use tree preconditioners. No load strap. No, it's not actually using the tree as a preconditioner, but as a basis. And it has no recursive anything. I do want to mention that Aaron and Iencat have this beautiful paper that speeds it up further. So they get to take a square root, so they get of the number of iterations.
00:47:10.962 - 00:47:48.980, Speaker A: And what they do is actually use a coordinate descent analog of Nesterov's method for accelerated descent. And I highly recommend their paper. And maybe they'll even talk about it at the workshop I'm organizing later in semester. But. Okay, so, and that was why I brushed aside the can we speed at the starting points that their paper gets around it? Okay, so let me not do that. Let me jump ahead to constructing low stretch banding trees quickly, because I have what, ten minutes? Can I do this? Okay, so let me tell, this is where the combinatorics is actually buried. And remember I said we should worry about what happens to lines here.
00:47:48.980 - 00:48:24.634, Speaker A: It's very explicit. When you happens to lines, what happens to lines is our data structure does them. If I try to solve linear systems on the line, then our data structure is just doing them by hand. So we're just keeping, instead of doing some recursive thing that buries it inside the recursion. What we're doing is just maintaining a cycle basis. And so if you maintain a cycle basis, then it's very, then a cycle which previously was very complicated, it was structured to end and required sort of recursion and stuff. Cycle basis, while the cycle just gets a number and you're adding up numbers per cycle, and so they're getting collapsed by hand.
00:48:24.634 - 00:48:59.298, Speaker A: Let me see. So, to construct most spanning trees, what's the high level idea here? The high level idea is that somewhere we need to be able to sort of find the edges that are very, that are, that maybe relatives. Okay, so the construction I'm going to give, I will not give you the log n one that we actually use. That's a little bit of work. What I'm going to do is I'm going to give you an older one that will give a two to the root log n. Log n. So just to know what that is, that's less than any power of n, right? So it's like n, it's less than n to the epsilon.
00:48:59.298 - 00:49:29.406, Speaker A: Note this log is like square root of log n, whereas the log of any polynomial is like log n. So this is almost what I promised. It's n to the one plus log of one as opposed to polylog. And this has a much simpler construction, and I probably won't briefly sketch how to get stronger guarantees. The high level idea, this is exciting. The high level idea is that what we're going to do is we want to capture the sort of length structure of the graph. We want to understand its clustering.
00:49:29.406 - 00:50:15.332, Speaker A: And so what we're going to do is we're going to partition the graph into low diameter clusters without cutting too many edges. And then once I give you that, we're just going to recurse. So the idea here is I want to get a spanning tree that works well on that sort of captures the edge length. Well, if the graph had low diameter, a breadth first search tree would be great, because if a graph has low diameter, if I tell you the diameter of a cluster, then certainly a breadth first search tree won't stretch any edge by more than twice length. So what we'll do is we'll partition the mapping flow diameter clusters without cutting too many edges on each of those clusters, we'll build the DF's tree, and then we'll just collapse the clusters to points and recurse. So I should say about this is the alone Carpelle guest construction that predated these things. It was in a case over here.
00:50:15.332 - 00:50:42.994, Speaker A: It's really nice. Okay, so the partition is going to rely on two things. The main combinatorial core of this is a ball growing procedure. So it's going to say. So the real hard part is, well, both are sort of equally hard. The core combinatorial fact that we're going to use here is that you can actually do this low diameter cluster, and that actually is buried somewhere in every one of the linear system solving algorithms I know of. And what we're going to use is the following.
00:50:42.994 - 00:51:19.970, Speaker A: It's going to say, given a graph g, we can partition it into clusters with the following property. One is only one over t. Fraction of the edges go between different clusters. So if I just cut it into the clusters so I don't cut too many edges. And two is that every cluster has small radius o of t log n. This goes back to our buck, and it's kind of the core, the analysis in a lot of sparse cut papers, and it's a sort of classical ball grown construction. How do we prove this decomposition exists? The way we do it is as follows.
00:51:19.970 - 00:51:59.324, Speaker A: All we're going to do is find greedily. So what we're going to do is we're going to arbitrarily pick a starting vertex x and we're just going to, we want a low diameter cluster around it. We're just going to build it greedily. So we're going to start at a vertex x and we're going to look at a ball of radius 123-4567 and we're going to stop when either we get the whole graph or we get a cut that meets our condition. So you're going to keep growing the ball until either, until either you have the whole graph or you can find a cut that sort of gets the right fraction of edges. So let's define the ball to be the ball I just described. For now let's just pretend everything's unweighted.
00:51:59.324 - 00:52:40.392, Speaker A: It's just easier to describe. And let's look at the edges that go from the ball of radius r to the ball of radius r plus one. So what we're going to do is we're going to look at how many edges you cut if you just pull out that one cluster. So that's the ratio of the edges that go out of the ball to the volume of the ball. Now again, our goal is going to be to construct to cut a small fraction of the total edges. So here we'll look at just this single ball's contribution to edges that are cut and vertices that we remove. And we're just going to look at the cuts and we're going to stop growing for the first time that we sort of get the legitimate thing we're hoping for, that the ratio of edges cut to volume is less than the asserted, the allowed ratio that we're shooting.
00:52:40.392 - 00:53:10.228, Speaker A: So if we're shooting for the, for this lemma, we're not going to use it with these parameters. But it's convenient to think of it. Think of the assertion you can cut into logarithmic diameter clusters where you cut half the edges. And so we'll look here at the first time where the ratio of edges we cut to volume is bounded by that. So then all you're going to do is just remove that ball and repeat. And I claim this gives us our dime, our clustering. And the real point is just whenever we were, first of all, whenever we remove edges, we're removing t times as much volume.
00:53:10.228 - 00:54:06.568, Speaker A: So if this actually works, we may, I haven't yet promised you the low diameter thing, but at the very least I haven't cut too many edges, because every time I cut some edges, I remove enough volume to compensate for it. And now I claim that if you don't stop, it means that the volume of the ball is growing pretty quickly, right? So a bullet volume means total entry contains not vertex weight. So the idea is that, what does it mean that I kept going? What does it mean that I increased my radius by one? It meant that actually I got a lot more edges when I grew the ball. Because if the volume didn't increase a lot when I grew the ball one more step, then that would have meant there weren't too many edges leaving the ball and I would have had a sparse cut. So if I don't stop, that means EI over volume of b is one over t, at least one over t. And so in particular, it means that the total volume that I had these new edges goes up by one plus one over two, right? So the point is, one of two things happens in each iteration. Either you get a lot more stuff or you don't.
00:54:06.568 - 00:54:22.684, Speaker A: If you don't, then you call it a cut. If you do, then the ball grew a lot. And then in particular, note this can't happen too many times and you'll just get your whole graph right. So I keep going at radius r. The volume is one plus one over t to the r. And so that can't happen more than t log n times. Or I'll just have too many total edges.
00:54:22.684 - 00:54:44.268, Speaker A: And that's the proof. So this tells me there are low diameter clusterings. I'm going to skip dealing with multiplicities. You have to be a little careful because we're collapsing clusters. You have to deal with multigraphs correctly. It's not a big deal, but you have to do it. And now let me get to the, so you can sort of correctly handle multiplicities.
00:54:44.268 - 00:55:21.256, Speaker A: And I just want to push past that. And let me tell you the core of the recursive construction in my final three minutes. So the construction is now just going to take this low diameter clustering thing and just use it inside a recursion. So we'll partition our graph into these promise size pieces and then we're going to build the bfs tree and collapse into a point that gives us a new graph and we reverse. And my claim is, if we set our parameters right, this will be a low stretch band tree. So the reason for it is as follows. What we're going to do is we're going to look at how, how stretched an edge is as a function of sort of what level it gets collapsed on.
00:55:21.256 - 00:56:12.476, Speaker A: So the idea is if I tell you that I have a cluster of radius r, then any edge internal to the clusters think of the bottom level is stretched by most gain. If I take a breath for a search tree, then I don't stretch an edge too much by using shortest paths. It's down by the diameter. If I'm at a higher level, then if I take an edge after I've collapsed twelve times, then each one of the edges that I have corresponds to a path of length d one level below it. So what you get is that after I've done this recursively, I times an edge in the ith level can be stretched by a factor of the diameter of each of these clusters to the I, because each, you know, each edge at one level is a path at the level below it of it up to diameter. Dude. And so now we're just going to bounce stuff.
00:56:12.476 - 00:56:51.364, Speaker A: So the idea is that the work, the number of edges at level I is going to be, sorry, let me just put all this up now and then say what happens. So the idea here is the number of edges at level I is going to be, it's going to go down by t every level because of the fraction of edges we're cutting, right? So each level is supposed to cut few of the edges. So the number of edges at level I is like falling off as the I power. And then what we're going to do is just choose the right value of. We have one parameter here. The one parameter is t. We can choose sort of how aggressively to grow the balls versus cut the edges.
00:56:51.364 - 00:57:36.526, Speaker A: And let's see what has what we get as a function of t. What we'll get is that level I is just going to contribute to m times log to the I. So it's falling off fairly quickly at the rate that kind of matches up with it. So I claim if we choose t to be two to the o of root log n, log log n, I claim that we'll actually get exactly the right proof. So let me tell you kind of morally what's going on here, because I think it's hard to do recursions in three minutes. So the idea is that if I did log n levels of the standard parameters for this usually is that you remove half the edge every time we get log n levels. That turns out not to be too useful because you end up raising, you end up having this diameter to the ith power, which gives you something very big.
00:57:36.526 - 00:58:08.046, Speaker A: If I raise it to a log n instead, what we're going to do is we're going to be, we're going to say that somehow if I look at d to the I, it's a lot more important not to make I blow up, but not to make d blow up. So we're going to make d, the diameter that we're shooting for be a little bigger than log. Nice. We're going to make it two to the root log n ish. And then what we get for that is a little bigger is we're actually going to get the number of levels of recursion is only going to be square root of log n. That's just some log log factors. So what we're going to do is we're going to choose our parameters.
00:58:08.046 - 00:58:33.454, Speaker A: So the number of levels of this recursion is square root of log n instead of log n. And then, and the reason we want to do something like that is that you pay a lot for the edges at the top level. So you want the number of edges at each level to fall off really quickly. So we're going to make bigger diameter clusterings of diameter. That's like two to the root log n. It's just balancing parameters, but that's what happens. So that your edges, the number of edges at the levels fall off super quickly.
00:58:33.454 - 00:59:04.230, Speaker A: But each. So now each cluster has diameter two to the root log n and you only go up log n like root log n levels. And that gives us sort of our desired bound. So that's the high level point really, to get. So to get this kind of bound, to get an m to the one plus epsilon, all you have to do is grow breadth first, search trees, collapse into points and repeat. Where the better constructions come from is they look more like Bartali compositions they actually have. So they have to do a probabilistic cutting procedure.
00:59:04.230 - 00:59:37.148, Speaker A: They also have to do something like Bartol decompositions, but with multiple metrics. So they sort of have to understand there's sort of an asymmetry in this picture between the. Note that lengths of paths being long was really bad for this. So there's sort of an asymmetry between the cluster you're in and the other clusters they do, they grow stars instead of trees. And it takes some, you just do region growing, but the regions are not balls anymore. Yeah, but you need to do two different metrics. Like you need to grow a ball in one metric and then those 10.0
00:59:37.148 - 01:00:05.984, Speaker A: regions in another metric. It's still a region growing argument, but it needs a little bit more work anyway. So that actually now is a full linear system solver. Okay, so that's it. And I'm happy to answer any questions. Thanks, John.
