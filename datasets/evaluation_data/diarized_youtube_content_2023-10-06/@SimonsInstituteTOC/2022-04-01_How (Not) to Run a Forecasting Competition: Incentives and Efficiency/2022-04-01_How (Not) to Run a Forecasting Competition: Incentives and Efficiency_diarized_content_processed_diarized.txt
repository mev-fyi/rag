00:00:00.440 - 00:00:31.776, Speaker A: Welcome to the last talk of the day, which will be in present, in person. It's a real pleasure to introduce RAF. Ralph is a professor from the University of Colorado who's done a lot of, like, very interesting work in a diverse set of areas, from information elicitation, peer prediction, all the way to dynamical systems in case theory. And although his work is very diverse, it's also universally very interesting. So whatever you'll talk about, it will be a lot of fun. Okay, RAF, please.
00:00:31.960 - 00:00:32.416, Speaker B: Wow.
00:00:32.480 - 00:00:36.808, Speaker C: Okay, thanks. Yeah.
00:00:36.856 - 00:01:14.910, Speaker B: So, today's talk is about forecasting competitions. It's joint work with Bo Wagener and two of our students, Robert and Anish. And so, forecasting competitions, I mean this somewhat broadly. So in particular, all machine learning competitions. Well, not all, but many machine learning competitions on Kaggle, for example. But I think it's better and you'll see why it's better to think maybe more about human scale forecasting competitions, like good judgment and hybrid forecasting and so on. So here's kind of the setup.
00:01:14.910 - 00:02:01.244, Speaker B: And, you know, like Tim said, this is probably like the 10th talk on the forecasting and the workshop or something, but just to fix ideas. So I'll remind you of the general settings. So we have, let's say n forecasters and m events. And in each event, a forecaster will furnish some probability, say the probability of rain. And given all this information, so every forecaster submits their predictions for all the events. We eventually learn the outcomes, so we see what happens. And a competition is really just a mechanism that converts all of this information to a winner.
00:02:01.244 - 00:02:58.654, Speaker B: And so we're interested in designing that competition mechanism. So how should we do this? Well, I'll tell you what, basically all forecasting competition mechanisms do is something that we call in this paper the simple Max mechanism, where you take each of these predictions, so 25% chance of rain and rain, you take each of these pairs, prediction outcome, you score them in some way using a proper scoring rule, and you sum up the scores for each agent for each forecaster, what was their score at the end of the competition, and just pick the one with the highest score. So, fair enough. It's quite sensible, especially if you use a proper scoring rule. It feels like the right thing to do, but actually has strange incentive problems. And that's kind of the starting point for this work. So here's a very simple example.
00:02:58.654 - 00:03:37.324, Speaker B: We have three forecasters and one outcome. So we're trying to predict just the probability of rand tomorrow, and one forecaster is completely correct. So they know that there's a 50% chance of rain. But they also know that there are these two other bozos who, like one who thinks it's very unlikely and one who thinks it's very likely. And the problem is, yeah, they'll never win. Right? So they also know that they have no chance of winning, because if it doesn't rain, then this bozo looks like a genius. And if it does rain, and this one does so deterministically, no matter what happens, they know they're going to lose.
00:03:37.324 - 00:04:17.020, Speaker B: So what do they do? Well, they're going to deviate to either slightly less than 10% or slightly more than 90%. Now they at least have a one half chance of winning. And the issue here is that the proper scoring rule literature, it's designed around a slightly different incentive structure where you're trying to maximize your expected score. So we're thinking of these as payments, but that's not what this agent is trying to do. They're trying to maximize their chance of winning the competition. And those incentives don't align very well, especially when the number of events is short. So there aren't that many events.
00:04:17.020 - 00:05:13.644, Speaker B: And you might ask, okay, is this like, most of these competitions are not on one event. So is this a real problem? It turns out it is a real problem. There's this very interesting blog post by Andrew Langaroff, who won the 2017 Kaggle March mania competition, which is a March madness thing. And he details how he did it, and it's quite fascinating, like, so. So when I found this figure, to be honest, I was like, oh, thank God this is going to make this slide for my talk so easy, because look, in his description, he's modeling basketball games, but he's also modeling what other people are going to do and then computing a best response. So he's literally like, yeah, he's putting all this effort into finding a best response, and it's so clear that that's different than whatever this model number one might have predicted. Right.
00:05:14.264 - 00:05:20.524, Speaker D: How the competition is selecting winners. Are people getting probabilistic forecasts, for example?
00:05:20.824 - 00:05:22.544, Speaker C: Yeah, so we'll say that. Yeah.
00:05:22.624 - 00:05:24.648, Speaker B: So in this competition, and we're giving.
00:05:24.696 - 00:05:33.394, Speaker C: Probabilistic forecasts, I think you used the log loss, which is actually an interesting point that we might get to at the end. Is that what you were asking?
00:05:33.474 - 00:05:40.002, Speaker D: Well, I mean, there's an exponentially sized state space of who wins. You know, there's a large number of.
00:05:40.018 - 00:05:41.322, Speaker E: Brackets that could win.
00:05:41.498 - 00:05:44.414, Speaker D: And so what was the, I guess I want to know what the.
00:05:46.074 - 00:06:01.732, Speaker C: Oh, yeah, yeah. So I'll get to that in the next slide. Actually, I think it's like effectively you could think about about 2000. Binary outcomes is like equivalent to the outcome complexity. Is that what you mean?
00:06:01.868 - 00:06:03.156, Speaker D: Okay, I'll wait for more details.
00:06:03.220 - 00:06:05.144, Speaker C: Okay, yeah.
00:06:05.844 - 00:06:10.844, Speaker F: I've never forecast competition, so I don't know how it is.
00:06:10.884 - 00:06:12.344, Speaker C: Oh, I'm sorry, you have to.
00:06:14.244 - 00:06:38.654, Speaker F: What does it mean that the truth is that 50%? I assume that the truth is that if I go out, it rains or it doesn't rain. The competition waits to learn from un number for a day. Somehow it asks you, ten will rain with five, four.
00:06:39.434 - 00:06:46.906, Speaker C: Right. So is there a probability? Are these events actually drawn from some distribution? What is, what does it all mean?
00:06:46.970 - 00:06:59.894, Speaker F: I'm trying to understand. In the competition, when someone wants understanding that forecaster is good one, we take a deterministic action, like a ban. I learned that this forecasted.
00:07:02.034 - 00:07:03.522, Speaker C: Profits today.
00:07:03.618 - 00:07:26.864, Speaker F: We will have rain. It is 10 or like 50%. I think the question is, what does it mean that 50% is the right number? What does it mean for the probability to be the right number? Either it's going to rain or not. Is it likely to be many times, you know, he has to be correct.
00:07:26.904 - 00:07:31.160, Speaker C: Ok, the time or. Yeah. So I was anticipating a lot of.
00:07:31.312 - 00:07:33.284, Speaker B: Objections, but not this one.
00:07:34.784 - 00:07:46.124, Speaker C: No, it's a very good point. And it gets like deep into the philosophy of probability. At a technical level, I don't think it will matter too much, but it's been a while since I thought about it.
00:07:46.744 - 00:07:49.152, Speaker F: Again, I don't know how it is in the competition.
00:07:49.248 - 00:07:54.324, Speaker C: So let's just imagine for now there is some true probability. We don't have access to it.
00:07:54.774 - 00:07:59.638, Speaker F: And as a competitor, I should submit a probability. Not if iterate.
00:07:59.766 - 00:08:07.434, Speaker C: Yeah. Oh, but, right, so this is what you submit. Right. So you're submitting probabilities.
00:08:08.174 - 00:08:26.326, Speaker F: But the truth now I submit 50%. The organizer would say, okay, the truth was the drains. So you are 0.5 or the truth is that the correct probability is 10%.
00:08:26.430 - 00:08:51.514, Speaker C: Yeah. So the truth, all you learn is whether it range or not. Oik, oik. Yeah. So you never observe the true probability, right? Yeah. And so mapping this to typical kaggle composition, m would be the number of data points, right? Yeah. Would you say that this, like, when m is large, is this a still problem or lesser problem? It's a lesser problem when M is large.
00:08:51.514 - 00:08:57.444, Speaker C: And we'll be focusing a lot on how big m is. So we'll see.
00:08:57.604 - 00:08:59.660, Speaker B: We'll basically be trying to push this.
00:08:59.692 - 00:09:12.924, Speaker C: Into a regime where good things happen even for a smaller m. Sorry, m. Yeah, smaller m. Subcentral limit theorem, kind of. Right. Yeah, exactly.
00:09:13.004 - 00:09:16.828, Speaker B: If there's very, very good concentration in all the scores, then it's not going.
00:09:16.836 - 00:09:20.648, Speaker C: To be a big problem. Okay.
00:09:20.816 - 00:09:25.208, Speaker B: So, right. So sure enough, this is a problem.
00:09:25.256 - 00:10:11.434, Speaker C: That does show up in some sense. And what really is the problem with the misalignment of incentives? One, I say like the first order thing is it's not even clear whether we're picking the best forecaster. Like Andrew Glenngraf seems really good at these things. And, you know, maybe the model for number one was pretty decent, but maybe he won the competition because he was really good at realizing that he needed to best respond and carrying that out. So it's not even clear. And by best forecaster, I mean the forecaster's beliefs are closest to the true probabilities regardless of what they report. And whether or not there is a true probabilities is another question.
00:10:11.434 - 00:10:39.858, Speaker C: It's also a waste effort problem. If you think about, you know, say especially, this is kind of related to the third one. If you're trying to use these actual forecasts for aggregation down the line or something, you'd like them to be as good as possible and you're kind of wasting on the forecasters effort because they're strategizing instead of just, you know, instead of trying to improve model one, Andrew.
00:10:39.906 - 00:10:59.654, Speaker B: Has to spend his time on two and three also. So that's effort that could have gone into studying basketball more or something. I don't know how to do these things. Okay, so we'd like to solve this problem in some way. Any questions so far on the issue?
00:11:00.514 - 00:11:04.934, Speaker D: Well, it does seem also that the problem is that the winner take all mechanism.
00:11:05.864 - 00:11:06.644, Speaker C: Yeah.
00:11:09.224 - 00:11:10.584, Speaker E: Or it might be forbidden.
00:11:10.664 - 00:11:23.024, Speaker C: That is a very interesting comment, which we'll see later. But yeah, the fact that you're insisting on choosing a winner is creating this distortion, but we're still going to insist that you have to choose a win.
00:11:23.144 - 00:11:37.866, Speaker D: I just want to be clear. The problem comes up all the time when you have these winning all things. Like when you're winning a basketball game, three points become a lot more valuable game for the same issue. Right. You do kind of things would seem to maybe have lower expected value but.
00:11:37.890 - 00:11:40.298, Speaker E: Higher variance told the goalie.
00:11:40.466 - 00:12:03.678, Speaker C: Yeah, yeah, yeah. So the question is like, how can we somehow fix this problem so we're still picking a better team, picking the better forecaster. Okay, so Jens McKowski and friends were among some to identify this issue, and.
00:12:03.726 - 00:12:04.854, Speaker B: They came up with a very cool.
00:12:04.894 - 00:12:11.046, Speaker C: Mechanism that I'll detail later. The event lottery is forecaster or elf mechanism.
00:12:11.190 - 00:12:14.014, Speaker B: And it's truthful. So it's truthful in a sense that.
00:12:14.054 - 00:12:17.874, Speaker C: You optimize your chance of winning by reporting your beliefs.
00:12:18.254 - 00:12:20.878, Speaker B: And they also give this accuracy bound.
00:12:20.926 - 00:12:34.390, Speaker C: Where they pick the best forecaster when the number of events is n squared login. So n is the number of forecasters. And so it's a very cool paper, and we'll talk about elf a little bit.
00:12:34.502 - 00:12:36.726, Speaker B: But the kind of the starting point.
00:12:36.830 - 00:12:59.128, Speaker C: For us was thinking about whether this is a practical bound. So is n squared log n practical? Well, for machine learning competitions, probably. It's fine, I think. You're saying anything about this number of ms and greater than this. Yeah, I mean, I should have written, um, omega here, but I'll. I just didn't want to. Yeah, but, yeah, any.
00:12:59.176 - 00:12:59.912, Speaker B: Any number of.
00:12:59.968 - 00:13:15.784, Speaker C: Any number m greater than. Greater than. Yeah, n squared, like that. And if we go back to that March media competition, there were, as it turns out, 442 forecasters. But effectively, and I'm kind of converting.
00:13:15.824 - 00:13:21.392, Speaker B: Brackets into binary events, about 2000 events, whereas their bound would need, like, over a million.
00:13:21.528 - 00:13:23.184, Speaker E: So does this have something to do.
00:13:23.264 - 00:13:26.088, Speaker C: Basically with, like, looking at all pairs?
00:13:26.216 - 00:13:29.724, Speaker E: Like, n squared is enough to see all pairs. Right. And, like.
00:13:32.024 - 00:13:37.164, Speaker C: Maybe, I don't know. We'll talk about the analysis and we'll see where it comes from.
00:13:38.624 - 00:13:41.364, Speaker F: You already defined what a best forecast is.
00:13:42.024 - 00:14:05.824, Speaker C: Yeah. So I'll define it formally in a bit. But for now, just think, the forecaster whose beliefs best match the truth. Yeah. Every forecaster predict all the events or the subset of the events. Oh, everyone is predicting all on all the events. Right.
00:14:05.824 - 00:14:40.368, Speaker C: Yeah, probably you could extend this as you fancy. Some are sleeping sometimes. Okay, so the central question basically is, can we. Can we pick the best forecaster using fewer events than N Square? Logan, it's a good time to note that pack learning lower bounds. Well, it's not totally immediate, but you can modify them to give a log n lower bound, which probably makes sense. Yeah. About the best learner, which is the one that is closer to the truth.
00:14:40.368 - 00:15:06.496, Speaker C: It really depends on the notion of the. Yeah. So does it depend on the scope function? It does, yeah. We'll define accuracy in terms of the score and. Yeah, that'll be clear. Can I ask one more question? I'm sorry. So you're assuming all the forecasters go all at once? There's no strategizing over sort of analysis and stuff? Yeah.
00:15:06.496 - 00:15:21.356, Speaker C: So the strategizing is like, who goes first on the leaderboard and so on. Yeah. So for now, actually, the model is even more. There's a point about the model that I'll touch on at the very end where we're assuming immutable beliefs.
00:15:21.380 - 00:15:23.404, Speaker B: So you have your beliefs.
00:15:23.524 - 00:15:32.264, Speaker C: Even if you saw what everyone else forecast, you would not change your beliefs, which, of course, is not very realistic, but. Well, it turns out you can with that.
00:15:32.724 - 00:15:36.260, Speaker B: But, yeah, for the analysis, let's just assume you know what everyone else is.
00:15:36.292 - 00:15:38.504, Speaker C: Reporting, but you still believe what you believe.
00:15:41.904 - 00:15:44.184, Speaker B: Okay, so the best. So we have a gap here.
00:15:44.224 - 00:15:46.872, Speaker C: So n squared log n is what elf gets.
00:15:46.928 - 00:16:48.834, Speaker B: The best possible is log n. Interestingly, the simple max mechanism, there's enough concentration at login for a simple max to also pick the best with high probability, but only if you ignore the fact that it's not truthful and actually is an open question that I'm not going to state is trying to figure out what is actually going on with simple Max. I think that's an interesting question, but it's sort of like, it feels like a first price auction kind of equilibrium analysis. It seems quite tricky to pin down, but, yeah, this fact will come back. So, simple max, it kind of achieves the best, but only if you ignore the whole strategizing problem. Okay, so how close can we get to the log n lower bound? Well, it turns out if you look at elf. So originally we thought that maybe we would need to modify elf in some way to get this lower bound if it's possible.
00:16:48.834 - 00:17:23.754, Speaker B: But it turns out you can even improve the analysis of elf without modifying it to n log. Nice, but that's tight. So we need a new mechanism, and we'll give one. I'll leave some suspense at how close we get to login, and if there's time, I hope there's time even more relevant to sort of the nominal name of the workshop, I think is this nominal name. That was good application to online learning. So, yeah, we'll see. We'll see if we get there.
00:17:23.754 - 00:17:44.254, Speaker B: Okay, so let me tell you about this elf mechanism, but before I do that, we need some, we'll finally fix some notation, and I'll define accuracy in those things. Okay, so we have events one through t. Sorry, one through m for each event, t, and each forecaster.
00:17:44.334 - 00:17:44.574, Speaker C: I.
00:17:44.614 - 00:18:20.222, Speaker B: They're going to predict some pit. So the talk will focus on the binary event setting, and you can extend it. So, you know, they're going to predict, say, the probability of rain or. Sorry, that's their belief, and they're going to report something that might be different, rit. And we're going to assume that there's some ground truth probability of rain theta t, and then yt is the eventual observed outcome. Did it rain or not? Okay, and then there's a scoring rule that's floating around for the talk. It's safe to just think about quadratic score.
00:18:20.222 - 00:19:09.774, Speaker B: So given some report and an outcome, here's one minus squared error. Other things work, right. So once you have a score, now you can finally define accuracy. So we'll define accuracy as the average expected score of the beliefs where the outcome is drawn from the true, true distribution. Does that make sense? So this is like, this is the true score of your beliefs normalized across events. So that's your accuracy. And so formally, what we're trying to do, when I say pick the best forecaster, what I really mean is give me some epsilon and I want to pick some forecaster within epsilon are the best.
00:19:09.774 - 00:19:49.392, Speaker B: And for most of the talk, I'll suppress the dependence on epsilon, but it's always one over epsilon squared. Okay, so here's the elf mechanism. So the idea is to kind of. Right, so in each event t, you're going to assign a point to one of the forecasters. So every round you're going to assign one point and you're going to do so randomly in such a way that's biased by the scores for that round. And then you're going to pick the forecaster with the highest number of points. So this is quite similar to simple max in a way.
00:19:49.392 - 00:20:33.904, Speaker B: It's just that you're randomizing in every round first and then picking the forecaster with the highest number of points. And the point is that. Right. So you're sort of converting the score to a probability in a way where if you're just focusing on round t, well, forecaster I maximizes the probability that they win the point on that round by maximizing their expected score. So they want to choose the report that maximizes their expected score if they're just focused on that round t. The tricky part is to show that it's truthful when they are not just focused on round t, but it turns out it works. Okay, so that it is truthful.
00:20:33.904 - 00:20:46.512, Speaker B: The optimal strategy is to report your beliefs and. Right, so you want to maximize the points in each round. And it's a little subtle to show that doing so maximizes your chance of being the highest point getter.
00:20:46.568 - 00:20:48.872, Speaker C: Yeah, it's not completely clear why we.
00:20:48.888 - 00:20:52.416, Speaker F: Are looking at relationships between two different forecasters.
00:20:52.480 - 00:20:53.616, Speaker C: So you have like a bunch of.
00:20:53.640 - 00:21:06.450, Speaker F: Them, and one of them is the best. So I would think that you look at the loss for each of them and then see which of them does the best. So why does it help to compare forecasters?
00:21:06.562 - 00:21:25.296, Speaker C: You're saying, like, why am I subtracting the other forecasters here? Right. So the issue, basically, by the way, this is, this is a form of a wagering mechanism, if you've seen that before. Yeah. The issue is you need to normalize, whereas you need the probabilities to sum to one. So it turns out the right thing.
00:21:25.320 - 00:21:30.576, Speaker B: To do is subtract. So everyone gets a one, everyone gets.
00:21:30.600 - 00:21:35.400, Speaker C: A uniform probability, and then you bias it positively if your score is better.
00:21:35.432 - 00:21:38.484, Speaker B: Than the average of the rest negatively. Otherwise.
00:21:40.904 - 00:22:09.274, Speaker C: Yeah. What is the claim about the truthfulness of this mechanism? Is it truthful if n is bigger than n score? Like, truthful for all n? Because, like, I mean, like in your earlier example, like, m equal one, one guy, 50%, two guys, 1910. Why is the 50% guy gonna report truthfully? In that case? I'm trying to work this out. Right. So what's the probability that the middle person wins? Well, it's.
00:22:09.434 - 00:22:10.762, Speaker B: They get one over n for free.
00:22:10.818 - 00:22:13.386, Speaker C: And then one over n times this difference in scores.
00:22:13.530 - 00:22:16.346, Speaker E: And that's still an expectation over yt, right?
00:22:16.490 - 00:22:18.290, Speaker C: There's no expectation. Why?
00:22:18.322 - 00:22:18.922, Speaker F: Please don't.
00:22:18.978 - 00:22:34.418, Speaker C: When they, when they look at their expected probability of winning, then they want to maximize their expected score, because I'm paying you in probability now, so. Yeah. Yeah. Okay. You convert. I see. So that's.
00:22:34.418 - 00:22:46.430, Speaker C: Yeah, we're converting money to probability, so. Right. So it works out all right. Yeah.
00:22:46.462 - 00:22:47.398, Speaker B: It's a very cool paper.
00:22:47.446 - 00:22:49.038, Speaker C: I can encourage you to check it out.
00:22:49.166 - 00:23:24.064, Speaker B: The analysis is quite fun. And so. Right, so they show that it's truthful. And the accuracy bound just argues that via the hoeffding bound, if you have enough events, then the point totals, I should have said approximately proportional to the point totals, become proportional to the true accuracies. And that's good enough. And. Well, it turns out actually Hoeffding is not the right bound here, because this the.
00:23:24.064 - 00:23:33.236, Speaker B: So I didn't mention this, but the score. The score is always going to be bounded between zero and one. So the most, one over nick.
00:23:33.420 - 00:23:33.764, Speaker C: Right.
00:23:33.804 - 00:24:13.508, Speaker B: So the most that you could possibly win, the point, the probability, the highest chance you have of winning, the point is two overhead, even if you're really good. And that means this is actually, like, if you're counting up how many points I'm going to win, it's a low variance situation. So you'd be better off using the Bernstein bound. And that already, as it turns out, means you only need n log n events. But this is very close to uniform. And this is kind of, if you run this in practice and play around with this, like, in a classroom setting or something, it's very uniform, even in a modest number of outcomes. And so that gives a lower bound.
00:24:13.508 - 00:24:57.224, Speaker B: And I think the way to see this is suppose one forecaster was perfect. So every round they got the point with probability two over n, and everyone else is terrible. They get the point with probability a little less than one over n. Still, like by bols and bin's analysis, the chance that some terrible forecaster will rack up enough points to beat the best forecaster is still high when m is much lower than n log n. Right? Okay, so all of this says that elf. While a very cool mechanism, it needs n log n events. So naturally, we wonder if you can do better.
00:24:57.224 - 00:25:01.384, Speaker B: How am I doing that time?
00:25:03.824 - 00:25:12.924, Speaker C: Oh, there's a cop. Fascinating. Okay. Yes. All right, so, yeah, go ahead.
00:25:19.304 - 00:25:51.654, Speaker D: Imagine, like, a basketball game where it was like, whenever they hit a basket, we actually give them four points with probability half and zero points of probability a half. That could be like a way to, you know, add a little stochasticity to a basketball game. That's what you're proposing here. Rather than pay them the actual probability, you pay them a coin toss. With that probability, you put up a probability value, maybe go back to that slide, rather than pay them the money on the right, which is perfectly compatible, just paying them that, right. You just pay everyone that number. You're going to now tip off the coin according to that number.
00:25:51.654 - 00:25:58.644, Speaker D: You're saying that somehow this added stochasticity to the scoring mechanism generate some truthfulness.
00:25:58.944 - 00:26:08.120, Speaker C: Yeah, so, yeah, you're exactly right. Like, it's only because of our fixation in picking a winner that we're in this situation at all.
00:26:08.232 - 00:26:11.232, Speaker B: If we were happy watching basketball and.
00:26:11.288 - 00:26:27.814, Speaker C: Having, like, a monetary prize, I don't know why we're talking about basketball. I don't think it's a great analogy, maybe, but, yeah, if we're happy with, like, forecasting competitions, just doling out money at the end, instead of selecting a winner or a leaderboard, then there would be no issue.
00:26:28.354 - 00:26:30.266, Speaker D: Yeah, I mean, it seems like the.
00:26:30.290 - 00:26:32.834, Speaker E: Big issue with, the issue with the.
00:26:32.874 - 00:26:38.818, Speaker D: Competition you're proposing is that people are so focused on the tail and, like, what they can get because they have.
00:26:38.826 - 00:26:39.610, Speaker E: To become the art max.
00:26:39.642 - 00:26:40.386, Speaker D: They have to be the art max.
00:26:40.410 - 00:26:40.778, Speaker C: A lot.
00:26:40.866 - 00:26:49.214, Speaker D: Yeah, they're really interested in what the tail looks like and how, like how, how much dead, how much mass they can get in the tail. And they're not focusing on expected value.
00:26:49.894 - 00:26:52.278, Speaker C: Expected value, tail, like the upper tail.
00:26:52.366 - 00:26:54.062, Speaker D: The tail of the total number points that they get.
00:26:54.118 - 00:26:55.514, Speaker C: Yeah. Right.
00:26:55.854 - 00:26:57.006, Speaker D: If they were just focused on expected.
00:26:57.070 - 00:26:58.806, Speaker F: Value, which is what you kind of.
00:26:58.830 - 00:27:01.558, Speaker D: Want them to focus on. That's what the score mechanic score mechanism.
00:27:01.606 - 00:27:03.294, Speaker F: Assumes is that they're focused on aspect of value.
00:27:03.414 - 00:27:27.726, Speaker D: It's like, well, we do kind of pick a winner. They're not going to focus, they're going to focus on other things, other statistics. We don't really like that. So how do we kind of force them to stop caring about the tail? Because we're going to like generate a randomization for them already? Is that kind of the idea here? What's the intuition here? It's weird. You're just adding extra source of stochasticity to the payoff. The expected value of these payoffs are the same whether you toss the coin or just pay them that number.
00:27:27.870 - 00:27:49.850, Speaker C: No, you might not expect the payoff, the probability of winning. Yeah. It's like if you know the scoring rules literature when you're worried about risk preferences, if someone's very risk averse, they won't be maximizing their expected score. But you can fix that by just giving them a lottery. Okay. You'll win a million dollars with a little bit more probability. So you convert it to probability.
00:27:49.850 - 00:28:02.074, Speaker C: That's, that's how I would summarize their trick. Well, what we're going to do is something a little more similar to your intuition. Okay. Yeah.
00:28:02.114 - 00:28:45.544, Speaker B: And yeah, sorry, this is a great discussion. I just have to, you know, march along. So we tried, to be honest, we tried a lot of things before arriving at what I'm about to tell you. And we finally gave up and just came crawling back to simple Max. Like simple Max, you get this login bound, but you're not truthful. Can we work this out somehow? One reason that simple max doesn't work at an intuitive level is that it's very sensitive. Think back to that example where we had 50% in the middle and then 10% and 90%.
00:28:45.544 - 00:29:28.924, Speaker B: Like as soon as 50%, if they, that middle forecaster raises their report past 90%, all of a sudden they go from a 0% chance of winning to a 50% chance. So in machine learning we solve sensitivity problems like this to stabilize our selection or whatever. With regularization. Maybe we could try something like that. Here's the follow the regularized leader algorithm that you've seen many times already in the workshop. We'll be thinking about this in a batch setting. If there's time at the end, I'll talk about the online learning setting problem, where the fact that we use FTRl helps us out a lot.
00:29:28.924 - 00:29:57.204, Speaker B: But for now, let's just think about this. So, I'm going to choose a distribution PI over the forecasters. That's going to be the distribution from which I select the winner. And I'm going to select it like this. I'm going to choose PI like this. So, choose PI to maximize the weight that I put the expected score under PI of the forecasters. But I'm going to penalize with some regularization.
00:29:57.204 - 00:30:33.244, Speaker B: You know, if I take this learning rate, you know, this parameter eta to infinity, then I approach just the simple max. I want to put all my weight on the best forecaster, but I want to hedge a little bit. So if you plug in negative entropy, you get the familiar multiplicative weights. Okay, so the question is, maybe. Maybe could this work enough? Right, so for a large AdA, it works in one sense. We only need to log n events, but.
00:30:33.364 - 00:30:33.740, Speaker C: Because.
00:30:33.812 - 00:30:45.220, Speaker B: Because it looks like simple Max, but it's not truthful. So what happened? What about small Ada? Right, so here's the form for multiplicative weights.
00:30:45.252 - 00:30:45.460, Speaker C: Let's.
00:30:45.492 - 00:31:23.244, Speaker B: I'll just focus on multiplicative weights for the talk. Right. So you exponentiate ETA times the. The empirical scores of each of the forecasters and then normalize. But for really small ETA, this is roughly proportional to one plus ETA times the sum of the scores. So if you're trying to maximize your chance of winning, then you should maximize your expected scores again. So it's somehow, like Jake was saying, like, for small Ada, it seems that we're aligning the incentives again, where we're just getting you.
00:31:23.244 - 00:31:35.904, Speaker B: There's. You don't have to worry about the tail. We're adding enough randomness that all that doesn't matter. Just worry about your bias. Just worry about increasing your score.
00:31:38.564 - 00:31:39.180, Speaker C: Yeah.
00:31:39.292 - 00:31:49.068, Speaker E: This is also somehow, like, the extra softmax is, like, definitely much smoother for a bunch of reasons.
00:31:49.116 - 00:31:49.300, Speaker C: Right.
00:31:49.332 - 00:31:55.738, Speaker E: This is a way to get, like, truthfulness out of all sorts of stuff and, like, privacy out of all sorts of stuff.
00:31:55.866 - 00:32:09.974, Speaker C: Yeah. So this is. We originally started from differential privacy, actually. And we're looking at, like, Laplace moving. Yeah, exactly, but. Right, so Ada depends on M. Right.
00:32:09.974 - 00:32:24.874, Speaker C: So Ada will depend. Ada and M will be tied. Yeah. Okay, so let me try to formalize, though, what I mean, by this being close to truthful, and this is kind of where we had to do some thinking.
00:32:24.914 - 00:33:17.512, Speaker B: So let's recall dominating strategies. So, for two strategies, a and b for player I, we say a dominates b. If no matter what other players do, a is better for player I than B. And so truthfulness just means that reporting your beliefs is a dominant strategy. And we're going to find some approximate truthful notion which is slightly stronger than approximate truthfulness that you may have seen, where reports that are outside of a gamma ball around your belief are dominated. And it turns out that multiplicative weights is four eta approximately truthfully. So what that means is, by taking EtA to be very small, I can ensure that.
00:33:17.512 - 00:33:30.844, Speaker B: Well, I don't really know what the equilibrium is, if indeed there is one. But I can safely assume that everyone is reporting within gamma of their belief, because otherwise they'd be doing something silly.
00:33:38.014 - 00:33:48.838, Speaker F: I want to see more because e beta is small. In a regularization base, I would say that PI equals one. It is almost one.
00:33:48.886 - 00:33:51.474, Speaker C: So that. Yeah, almost uniform. Exactly.
00:33:52.814 - 00:33:55.798, Speaker F: But we don't like PI is part.
00:33:55.846 - 00:34:09.214, Speaker C: The probability that I select forecast or high. Right. So I think you're getting maybe exactly this question, which is, for large Ada, we're not truthful, but we're highly selective.
00:34:09.794 - 00:34:52.274, Speaker B: We make a definitive choice for small Ada. Sure, we're approximately truthful, but I mean, we're close to uniform. So if what we really care about is choosing the best forecaster with high probability, that seems not so great. But maybe there's a sweet spot. Maybe we can be truthful enough, but also selective enough. And it turns out that that is indeed the case. So if epsilon is this accuracy that I want choosing ETA to be order epsilon gives us an epsilon accurate forecaster with high probability, with log n over epsilon event, which matches the lower bound.
00:34:55.414 - 00:35:04.874, Speaker F: Yeah, but it is not fair to say that we match the lower bound because the lower bound was for the. The exact title.
00:35:07.854 - 00:35:23.014, Speaker C: It's the same. It's the same. Right. So the lower bound is the probability with which you can choose an epsilon accurate forecast certificate. What is the terminology? Epsilon. I mean, like, maybe all of them suck, right? Like, you mean the top guy. What do you mean? Epsilon.
00:35:23.014 - 00:35:27.714, Speaker C: Close to the top in epsilon of the best. Yeah.
00:35:32.714 - 00:35:33.514, Speaker B: Okay, so.
00:35:33.594 - 00:35:59.074, Speaker C: And also, this is an absolute truth you were saying earlier, and it's also a truthful. An epsilon, truthful. Yeah, it'll be order Epsilon, truthful. Right. Okay, so I have some time to show you a bit of the proof. First, I would like to to quote a great researcher in our fields, I really liked the way.
00:36:01.694 - 00:36:29.374, Speaker B: So I don't know, I don't know how much you want to see of this, but let's do it anyway. Okay, so, yeah, so let's set. Let's say. I think it's kind of. It's cool to see how everything can connect, maybe. Well, tell me if you agree after this talk. Okay, so again, so we're going to take ETA to be order Epsilon, and M is going to be order log n over Epsilon squared.
00:36:29.374 - 00:37:17.064, Speaker B: Okay? And the proof proceeds by just kind of carefully tracking these three quantities. So the empirical score of forecaster I, that's what the mechanism actually has access to. And I'm normalizing here by the number of rounds. And the expected score is sort of the next thing removed. You don't see the truth, but you can imagine what is the actual expected score of the reports. And then the thing that you really care about is the accuracy, where instead of the reports, I plug in the beliefs of that forecaster by saying we're choosing an epsilon accurate forecaster. What I mean is a forecaster within epsilon are the best.
00:37:17.064 - 00:37:33.604, Speaker B: So let I be the best forecaster and let's think about trying to upper bound the probability that we choose any epsilon bad forecaster. Right? So we have j is some epsilon bad forecaster. So the differences in the accuracies is epsilon. So I is up here, j is down there.
00:37:33.984 - 00:37:34.384, Speaker C: All right.
00:37:34.424 - 00:38:13.502, Speaker B: So approximate truthfulness tells us that the difference between these two quantities is small. So let me take you through that. We're four eta approximately truthful, which means that the beliefs and the reports are within four ETA, which is order epsilon. And so we're going to need our score to be elliptius. So there's some constant here. So I can go from the inputs to the outputs, which is the scores themselves. But squared error is two lipships, so we'll need that.
00:38:13.502 - 00:38:31.434, Speaker B: But aside from that, you tune the constants and you can say, well, I'm taking eta small enough so that the gap between the accuracies and the expected scores of the reports is epsilon over five, say.
00:38:34.124 - 00:38:34.460, Speaker C: Right?
00:38:34.492 - 00:39:22.784, Speaker B: And now, now we can apply Hoeffding to get some concentration to say, well, the empirical and expected scores are close enough. And so now we're still left with an order epsilon gap between the empirical scores. So good, so we have that if we have an epsilon bad forecaster, we know that there is an empirical gap with high probability of order epsilon between the scores that we see. So the only thing left then is to convert that into a multiplicative gap. So I'm picking forecaster high with much more probability than J. And this part is probably familiar if you've seen some analysis of multiple negative weights. So you rewrite the PI.
00:39:22.784 - 00:39:28.124, Speaker B: I's. I'm rewriting it in terms of these empirical scores, and I'm multiplying by m because I normalized.
00:39:29.324 - 00:39:29.660, Speaker C: Right.
00:39:29.692 - 00:40:01.704, Speaker B: So we can ask. All right, so under this rule, how much more likely are we to pick I than J? So the normalization cancels out. And now I get this, you know, this additive gap of order epsilon, I'm going to convert to a multiplicative gap. And now, all right, so recall that eta is order epsilon, and I'm going to take m to be order log n over delta over epsilon squared. And so all the constants, I'm just ignoring all the constants. So.
00:40:02.404 - 00:40:02.764, Speaker C: But.
00:40:02.804 - 00:40:36.312, Speaker B: Right, so I have an epsilon squared times m here, right. So I'm just left with n over delta. So I have like an order n over delta multiplicative gap. And now I'm good to go. So, if I look at the probability of choosing a bad forecaster, now that's bounded by delta over n, but there are most n bad forecasters. So the probability I choose any bad forecaster is delta. Questions?
00:40:36.368 - 00:40:38.164, Speaker C: Yeah. Delta should be epsilon.
00:40:38.904 - 00:40:47.364, Speaker B: Delta is different than epsilon, so Delta. Think of it as parameterizing the high probability bound, whereas epsilon is the accuracy.
00:40:50.384 - 00:40:58.186, Speaker C: Yeah. More about the theorems. If anyone wanted to set it, you got the point. Yeah, I was trying to interpret it.
00:40:58.210 - 00:40:59.818, Speaker F: I mean, so now you have this epsilon, right?
00:40:59.866 - 00:41:01.202, Speaker C: So if you set epsilon, so, like.
00:41:01.218 - 00:41:02.178, Speaker F: One of the square events.
00:41:02.226 - 00:41:03.474, Speaker C: Right. So then you.
00:41:03.634 - 00:41:04.530, Speaker F: Wait, hold on.
00:41:04.642 - 00:41:10.834, Speaker C: If it's at one of the other. One over n, right.
00:41:10.994 - 00:41:14.138, Speaker F: Well, if you measure. No, no, no, you got it to log in over.
00:41:14.226 - 00:41:18.850, Speaker C: Yeah. One over root n min. Yeah. Like, the best way to apply this.
00:41:18.882 - 00:41:20.386, Speaker F: Thing is, like, if you're so lucky.
00:41:20.530 - 00:41:22.454, Speaker C: That all of the scores were like.
00:41:24.074 - 00:41:31.906, Speaker F: And in that case, you can set. If you set up slan event wick, like, the top squared of the n forecasters are all your guarantees as the top squared.
00:41:31.930 - 00:41:33.574, Speaker C: And then four clusters are pretty likely.
00:41:37.994 - 00:41:39.974, Speaker G: If Epsilon was one over squared event.
00:41:40.354 - 00:41:43.214, Speaker F: Then you would recover, like, a similar sample complexity.
00:41:46.514 - 00:41:48.334, Speaker C: If you're lucky, and all the scores.
00:41:48.714 - 00:41:51.334, Speaker F: Spread out, they're roughly between zero.
00:41:52.514 - 00:42:00.394, Speaker C: So then, like, the top score of the forecasters, like, are pretty likely to like anyone.
00:42:00.554 - 00:42:04.362, Speaker F: So this is, like, about the utility of this guarantee. So we know how to think about.
00:42:04.378 - 00:42:19.622, Speaker C: The x and whether, like, what this gets us compared to when we select it. Well, for elf, there was an epsilon as well that I'm suppressing. Yeah.
00:42:19.678 - 00:42:21.902, Speaker B: And part of the reason is we.
00:42:21.918 - 00:42:30.434, Speaker C: Didn'T quite get the epsilon squared for the lower bound, I think. But the upper bound that we get is m log n over epsilon squared. So epsilon still.
00:42:35.814 - 00:42:38.834, Speaker F: Would you say, in the initial method?
00:42:40.014 - 00:42:44.694, Speaker C: Yeah. So all these accuracy bounds are stated as, what is the probability?
00:42:45.154 - 00:42:46.626, Speaker B: How many events do I need so.
00:42:46.650 - 00:43:01.722, Speaker C: That I take an epsilon accurate forecast over my problem? So, yeah, it's been there all along. Yeah. Any other questions?
00:43:01.858 - 00:43:03.426, Speaker B: Next up is online learning.
00:43:03.570 - 00:43:18.764, Speaker C: So, yeah, so this works for me. Error publisher. Yeah. So what do we need? We need. I think we need a strong concavity assumption.
00:43:18.924 - 00:43:20.084, Speaker E: And lipschitzness.
00:43:20.204 - 00:43:21.060, Speaker B: And boundedness.
00:43:21.092 - 00:43:46.956, Speaker C: And lipschitzness. Yeah. I think, like, boundedness is a very interesting question that gets into some discussion we had during Tim's talk about what happens with log score, and one round can kind of seal the fate. So I think it's quite tricky there. Yeah, it does seem like the other ones are necessary, too. Yeah. This matches where you get out of noisy Max, right? Yeah.
00:43:46.956 - 00:44:07.684, Speaker C: Like, noisy Max. Launch all the school boards, take the Max. Yeah. So, yeah, so other regularizers work, too, including the ones you get the truthfulness for free from privacy. Excellent. Truthfulness. And so that's a different truthfulness.
00:44:07.684 - 00:44:28.324, Speaker C: Yeah. So that truthfulness, that epsilon truthfulness, or that approximate truthfulness is like, I'm not too unhappy by other deviations. And this is saying. Right. This is saying that I actually would be silly to report something outside of. Yeah, so we.
00:44:33.594 - 00:44:37.894, Speaker E: So this is a question of, like, whether. Yeah.
00:44:39.714 - 00:44:58.750, Speaker C: It'S like, this is in the input to the utility, and the other truthfulness is sort of like, I'm sort of okay reporting my. Right. In fact, you're okay with whatever in that setting. Yeah. So, yeah, we needed something stronger because. Yeah.
00:44:58.782 - 00:45:03.874, Speaker B: We needed to be able to control the reports to make sure that we were picking a good forecaster.
00:45:04.214 - 00:45:36.094, Speaker C: Yeah. Would you be. Would it make it easier at all to pick the best accuracy if you didn't have the approximate truthfulness constraint, or would it be just as hard? I think the challenge is understanding the equilibrium. But like I said, it may be that simple. Max works, but everyone's, like, thrashing around trying to best respond that somehow the best player in that complicated game is still the best forecaster.
00:45:36.474 - 00:45:37.986, Speaker B: So that might be true, but you'd.
00:45:38.010 - 00:45:45.602, Speaker C: Have to figure out what people are doing. Yeah. So this might be a terrible question.
00:45:45.658 - 00:45:46.826, Speaker F: So I'm going to preface with that.
00:45:46.930 - 00:45:48.634, Speaker C: What if you choose the second best.
00:45:48.714 - 00:45:51.002, Speaker F: Like you remove the max and choose.
00:45:51.018 - 00:45:56.034, Speaker C: The second best, like in a second price auction type thing without work.
00:45:57.934 - 00:46:02.638, Speaker F: Something like, I will try to be one of the best but not the best. Yeah, yeah.
00:46:02.766 - 00:46:04.594, Speaker C: That's a very interesting question.
00:46:04.934 - 00:46:06.314, Speaker F: That's what we do in auctions.
00:46:06.774 - 00:46:09.638, Speaker C: Yeah, well, you don't select the second.
00:46:09.686 - 00:46:13.474, Speaker D: Best, you select the first best payment hands on.
00:46:17.094 - 00:46:20.526, Speaker C: Yeah, I don't know. That's very interesting.
00:46:20.710 - 00:46:32.604, Speaker F: Something that Jake said. I want to be sure we have a mechanism without money, but actually we have some money. The points are the money that we give to each other.
00:46:33.464 - 00:46:35.320, Speaker C: So here there are no points per round.
00:46:35.352 - 00:46:52.734, Speaker F: That was an alpha previous mechanism. I should see like that the money that the price of the mechanism, the price means the points that they give players. Is that correct as an analysis or it is a mechanism without money?
00:46:54.114 - 00:46:57.282, Speaker C: I think it's best to think of it as a mechanism without money where.
00:46:57.378 - 00:46:59.214, Speaker B: You know, you're selecting a winner.
00:47:01.154 - 00:47:13.454, Speaker C: Okay, so maybe we'll leave the other questions on that for the end because I feel morally obligated to tell you about the online learning stuff. Okay, so this is going to be pretty quick, actually.
00:47:15.514 - 00:47:43.788, Speaker B: Punchline. This works in online learning as well. Okay, so let me tell you a bit. I think it's might be good to take a deep breath and let go of the forecasting setting because otherwise this might be confusing. So now we're in a completely different setting. I didn't hear many deep breaths, but I'm assuming you're working on that, right. So now we're in the classic, let's think about the classic online learning settings.
00:47:43.788 - 00:48:12.104, Speaker B: We have a bunch of rounds. In each round you have some experts and they're giving you advice pit. These are the same pit. So the classic setting, we're assuming these are the beliefs, and the algorithm, let's say, is going to choose an expert at each round. And by choose an expert, that means predict whatever they said and they do so with some probability. The algorithm does so with some probability and the algorithm is trying to perform not much worse than the best expert measured in terms of regret.
00:48:12.804 - 00:48:13.196, Speaker C: Okay.
00:48:13.220 - 00:48:57.944, Speaker B: In the strategic setting, we imagine that now the experts can lie, so they have some beliefs, but maybe they report something else. And why would they do that? Well, they're trying to maximize their chance of being chosen. And you could think of this as like. So there's some work looking at the myopic case where in each round the experts are just trying to maximize their chance of being chosen the next round. But more generally so that's not a great model in some ways, we really like forward looking agents that maybe are trying to maximize the total number of times they're chosen. So anything like that will basically work here. So I'm not going to specify that.
00:48:57.944 - 00:49:34.854, Speaker B: So, in some sense, trying to maximize their chance of being chosen, and you can play around with what exactly they're trying to maximize. And now, the interesting thing is here we're stating this as the algorithm has the same goal, though, trying to perform well compared to the best expert. But here, importantly, we want to compare to the best experts beliefs, not whatever they said to game our learning algorithm. We want to have a guarantee that says we did well compared to the best forecaster, best expert, in the same sense that I've been describing.
00:49:37.444 - 00:49:37.908, Speaker C: Okay.
00:49:37.956 - 00:49:56.304, Speaker B: And so while this, there are some interesting results for the myopic case, it's been open to find such an algorithm for the general case. And it turns out the same analysis kind of gives you that. So we can achieve root t regret even in the presence of strategic experts.
00:49:57.044 - 00:49:58.664, Speaker C: And the proof is.
00:49:59.484 - 00:50:27.272, Speaker B: I think I can sketch the proof in three lines, basically. So you start with the standard regret guarantees for FTRL. FTrL says, well, if you set the learning rate like one over square root t, then we get rooty regret. But with respect to the reports. Right? So it's a worst case guarant guarantee. So no matter what the reports are, no matter what the outcomes are, I get rooty regret. And now approximate truthfulness kicks in.
00:50:27.272 - 00:50:50.164, Speaker B: So we can say, well, actually, the reports have to be within order ETA of the beliefs. So that's one over root t. So how much do we lose by the fact that the reports were not the same as their beliefs? Well, in each round, we pick up a one over t. Sorry, one over root t. So, overall, the extra regret.
00:50:50.284 - 00:50:58.264, Speaker C: From all that is root tip online learning, the adversary does what?
00:50:59.444 - 00:51:01.732, Speaker B: So the adversary is choosing the outcomes.
00:51:01.828 - 00:51:05.704, Speaker C: Okay. Like, literally. Right.
00:51:06.284 - 00:51:07.704, Speaker F: And also the beliefs.
00:51:08.324 - 00:51:15.020, Speaker C: Also the beliefs, yeah. I mean, you can think of. Yeah, this will hold for all beliefs. Right.
00:51:15.172 - 00:51:19.396, Speaker D: Well, if an adversary chooses the outcome, it doesn't feel like there's an underlying probability.
00:51:19.500 - 00:51:20.692, Speaker F: Yeah, yeah, sorry.
00:51:20.748 - 00:51:37.216, Speaker C: So. Yeah, so sorry. Yeah, I think I misspoke. So, by best expert, I mean using, like, the normal regret guarantee, but plugging in the pits. So it's the empirical, the empirical, best in hindsight. Yeah.
00:51:37.280 - 00:51:52.834, Speaker D: We just didn't take a deep enough breath because, like, the outcomes are chosen by some, that there should be something going on here. Like t is a theta t that we would like to know, asking people to think about it. But when you go to expert setting, you sort of really have to forget that there isn't an underlying probability.
00:51:54.134 - 00:52:29.768, Speaker C: Right. Yeah. It's still adversarial, is this theory? Most part, yeah. It does work for myopic. It works, yeah. So the formal statement is that any positive linear combination of the pits. So if I'm expert, I, and I care, my utility is some positive linear combination of the PI I t.
00:52:29.768 - 00:52:38.840, Speaker C: Sorry. So that the chances of being chosen on each round, then any of those will work. So my topic would be like this.
00:52:38.872 - 00:52:41.032, Speaker B: Weird thing where on round t I.
00:52:41.048 - 00:52:45.426, Speaker C: Only care about PI I t plus one. But that also is.
00:52:45.450 - 00:52:53.282, Speaker F: Okay, so myopic is something like markovian or anthrax, and it's only, oh, it.
00:52:53.298 - 00:53:06.814, Speaker C: Just means that on round t, experts only try to maximize their chance of being chosen on round t plus one. Then they, you know, they black out, forget everything that happened and do the same thing. Yeah.
00:53:07.114 - 00:53:10.334, Speaker E: So should I be thinking of multiplicative weights as the mechanism here?
00:53:10.864 - 00:53:11.576, Speaker C: Yeah.
00:53:11.720 - 00:53:25.164, Speaker E: So are there other applications besides these forecasting competitions where you might use a multiplicative weights mechanism and then get this kind of same result that you can be strategic?
00:53:26.304 - 00:53:27.564, Speaker C: I'm not quite sure.
00:53:28.304 - 00:53:36.524, Speaker E: I'm just trying to see that the somewhat generic. I don't know, I'm just trying to think there are other applications where you might be able to use this kind of same recipe.
00:53:37.584 - 00:53:41.616, Speaker F: It is only for multiplicative weight updates or it is for anything.
00:53:41.760 - 00:53:47.352, Speaker E: Well, maybe there are other mechanisms beside the forecasting environment where that might be. Okay, maybe not.
00:53:47.408 - 00:53:47.576, Speaker C: Yeah.
00:53:47.600 - 00:53:49.744, Speaker B: I mean, like the online learning setting.
00:53:49.784 - 00:54:04.084, Speaker C: Is in some sense an example where it feels like a very different setting, like it's adversarial. It's not, it's not like a forecasting competition, but yet the approximate truthfulness of the mechanism.
00:54:07.274 - 00:54:17.306, Speaker E: But it seems like there might be other settings where you want that same like you want to be approximately. You could potentially be strategic in an online learning setting, but it's really nice to have the approximate truthfulness, sort of.
00:54:17.450 - 00:54:18.266, Speaker C: Ah, yeah.
00:54:18.370 - 00:54:19.498, Speaker E: Trying to see if there was another.
00:54:19.586 - 00:54:42.754, Speaker C: Yeah, I mean, it seems like, it seems like a nice tool, maybe for other settings. Okay, let me, let me wrap up. I guess I just don't understand how it's an adversarial setting because how do I understand the agent's strategies if they don't, if the outcome is just chosen adversarially?
00:54:51.854 - 00:54:54.062, Speaker E: I think it's adversarial for the lurk.
00:54:54.238 - 00:55:11.568, Speaker C: Yeah. Yeah, exactly. So I think the agents still have beliefs, right? The experts have probabilistic beliefs, yes. Well, no, it doesn't even have to be.
00:55:11.616 - 00:55:14.008, Speaker B: It's just like, an assumption about their rationality.
00:55:14.056 - 00:55:24.006, Speaker C: So expert eye thinks that the outcome is drawn from pit, and so they're going to be best respond, you know, trying to maximize their chance of being chosen under that.
00:55:24.070 - 00:55:30.478, Speaker E: And your regret is going to be with respect to the best expert, as though they were not the horror, correct?
00:55:30.646 - 00:55:49.658, Speaker C: Yeah. Yeah. So I think that's a great example of something that, like, that's a positive linear combination of the PI. So, like, I want, my utility is.
00:55:49.706 - 00:55:50.294, Speaker B: Like.
00:55:52.674 - 00:56:10.234, Speaker C: Beta times my probability of being selected tomorrow, plus beta squared times the next one. That'll be fine. Okay, so this is the last slide, so what we showed.
00:56:10.274 - 00:56:52.638, Speaker B: So, in competitions and forecasting competitions, we get better event complexity. So elf already gets n log n events, so it can pick a best forecaster with n log n events. But curiously multiplicative weights and other FDRL algorithms achieve the optimal bound of login. And in online learning, we get rooty regret even in the presence of strategic experts. So I think there are kind of two interesting takeaways. The first is, maybe Kaggle should add some regularization. And I'm kind of, Jake knows well how Kaggle deals with helpful suggestions.
00:56:52.638 - 00:57:58.350, Speaker B: So I'm like, we've tried to get Kaggle to implement things in the past, so I'm trying to make my case very solidly. So there's some things that I think we need to work out, but I'm kind of hoping that maybe we can try this on some real competitions. And I think, you know, but for this audience, maybe a more interesting observation. Is that right? Often in algorithmic economics and algorithmic game theory, we ask, what is the cost of strategic behavior in various settings, like price of anarchy kinds of statements? And really, in these two settings, forecasting competitions, online learning, we're somehow able to match the best non strategic bounds up to constants, even when there is strategic behavior going on. And I would say, I don't, like, I don't understand this to an extent that satisfies me. Like, I think there's something going on here that would be interesting to dive into further. And I'll just end with a couple extensions and open questions.
00:57:58.350 - 00:58:41.196, Speaker B: So I mentioned that, you know, founded scoring rules with better lip shits and have some curvature are fine. We extended beyond binary outcomes. There are other regularizers that work, and here's an interesting one. So there's something that's not quite satisfying about this. Immutable belief model, and that's that every forecaster thinks that they're the best. If you think about, like, what's implied by everyone has their own beliefs, and no matter what other people say, they're sticking to their beliefs, then that means that they believe their expected score is going to be higher than everyone else's. And that's, like, a really weird feature of the model.
00:58:41.196 - 00:59:20.844, Speaker B: And you'd really like to be able to express things like, I believe that Jake is a better forecaster than I am. And to do that, you have to allow the reports of other agents to correlate with the truth. But it turns out that everything continues to hold if you have a more realistic model like that. And I think two interesting open questions. It's not clear whether there is an exactly truthful mechanism that achieves the log in bound. So, approximate truthfulness, we would argue, is pretty good. But you'd really like an exactly truthful mechanism, so it's not clear.
00:59:20.844 - 00:59:57.204, Speaker B: Yeah, that's a great open question. And finally, throughout all this work in this area, we've assumed that events are independent of each other. But intuitively, you should be able to tolerate some correlation between events, and you would like the bounds to degrade and, like, the number of bits of randomness in the total number of events, right. If they're all perfectly correlated, well, it's just like there's one event, so there's not much you can do. But if there's enough randomness, then hopefully this can extend.
00:59:57.664 - 01:00:00.284, Speaker C: Okay, thanks.
01:00:06.504 - 01:00:11.924, Speaker A: Thank you, Ralph. Questions? Yes, but more questions.
01:00:15.834 - 01:01:10.874, Speaker H: I really like your second point, and I was just curious to know. So, we have a lot of, you know, we know a lot of algorithms, or, like, I guess, indirect mechanisms. Like, I'm thinking something like an ascending auctions, where you have guarantees that you're going to converge to a certain thing. Maybe understanding action is not a good example. But, like, let's take something like fictitious play, like a dynamic that's decentralized and people maybe actually might be strategic at each round or something. They might lie so that they end up in a better spot, like assuming that their adversary also plays the same dynamic. Using this approach, do you think that you could guarantee in some way that you will converge to, I don't know, a close bowl of the actual spot that you were supposed to converge to? I don't know if that makes sense here you're saying that my experts can lie to me, and I'm still guaranteeing a certain optimality.
01:01:10.874 - 01:01:35.234, Speaker H: Do you think you can extend this approach to other stuff, like some auctions where we don't, like some auctions that are ascending, or we don't think of some incentives that are guaranteed to converge to a certain competitive equilibrium or something like that. Could you guarantee that we're going to converge the next competitive equilibrium by taking this approach?
01:01:35.394 - 01:02:39.214, Speaker B: Yeah, that's a great question. I'm not really an expert in auctions, but I would say that the general solution concept, such as you can call it that, of just trying to say that, okay, whatever people do, they're at least not doing this might be a useful tool in other settings like that. When you're trying to get some approximation guarantee about the behavior in some complicated auction, it does seem like we needed a lot of curvature assumptions that lined up, which fortunately worked out. So we needed the regularizers to have Lipschitz Hessians and things like that. So we had to carefully control the smoothness to show the approximate truthfulness. Those technical meat to the paper, actually, I'm not sure whether you could show such a thing in auctions. Like, maybe you would not have enough continuity or something.
01:02:41.034 - 01:02:58.024, Speaker H: Except thinking maybe, like, instead of like a strategic experts, if it was like, kind of more online comics optimization setting, but where, like, your parameters can be chosen adversarially by strategic, like, strategic, I don't know, adversaries. Could you obtain similar results?
01:02:59.404 - 01:03:00.132, Speaker B: I'm not sure.
01:03:00.188 - 01:03:00.904, Speaker H: Okay.
01:03:08.804 - 01:03:10.184, Speaker A: Any more questions?
01:03:22.904 - 01:03:23.552, Speaker C: Oh, hello.
01:03:23.608 - 01:03:45.760, Speaker G: Okay, yeah, it's kind of complicated. Well, a simple question, but you get to, uh. So, so there might be a difference. Uh, so if I understand correctly, like, if you go myopic versus, uh, um, like, all the way till the end, like, this will give you different strategies, right? Otherwise, like, so, like, yeah. And then, like, what, what happens if we're in, like, an anytime situation where.
01:03:45.792 - 01:03:49.958, Speaker B: We, like, so different strategies, like, the experts will do different things.
01:03:50.086 - 01:03:57.278, Speaker G: I guess the payoffs should be different. Otherwise, like, the myopic thing was also good for the, I don't know what you call it, but till the end.
01:03:57.366 - 01:03:58.606, Speaker B: Yeah, yeah. The more general.
01:03:58.710 - 01:04:14.774, Speaker G: So then, like, in an anytime setting, like, have you thought about how both the learning algorithm and the experts should think about things right, when they don't know, like, what the last round is? Oh, yeah. In all my learning.
01:04:16.514 - 01:04:50.544, Speaker B: Yeah, my general sense is, so this is not a well founded statement, but my hope slash guess is that everything could extend to the unknown horizon setting. Like, I don't think we're doing anything complicated enough that the standard tricks wouldn't work, but it might take some effort. But you mean, like, on the behavior of the. Yeah, I guess one thing that could be complicated is like. Yeah, it's a good question.
01:04:59.564 - 01:05:28.124, Speaker C: Yeah. So I guess where you could go wrong is like, say you set Theta to be decaying, like one over root tip. But the forecasters, the experts, knew more about the horizon than you did, and somehow that wasn't the right data to set at the beginning. This seems like the doubling trick or something should be okay. But, yeah, it's a good question.
01:05:31.984 - 01:06:12.416, Speaker A: Very interesting work, actually. I have a question about the model of rationality of the agents. So here, somehow you assume that the agents always bid within some range of their true valuation or their honest beliefs within that epsilon interval. But maybe if I was a Kegel participant and I knew somehow that Kegel was somewhat regularizing, maybe I have an incentive and it's not clear to me how I can solve this problem to compute the epsilon. And I could be tempted to learn myself. So a lot of time, I'm kind of, like, deviating from, I believe, just exploring.
01:06:12.480 - 01:06:13.084, Speaker C: Right.
01:06:13.584 - 01:06:18.644, Speaker A: Could I create. Is that. Is that a meaningful model, do you think? Or.
01:06:19.384 - 01:06:19.696, Speaker C: Yeah.
01:06:19.720 - 01:06:39.972, Speaker B: So I guess two responses. One is probably Kaggle should post their mechanism. Everyone would be upset if they didn't. So then you would know, like, okay. And they could post their guarantee. Like, famous scientists have shown that you should not deviate by more than the scammer or something like that.
01:06:40.028 - 01:06:41.904, Speaker A: Are you going to send some cool pictures?
01:06:45.124 - 01:06:56.618, Speaker B: But also, yeah, I would say that you wouldn't have a lot of feedback to learn anyway, unless you're participating in many competitions or something.
01:06:56.796 - 01:06:57.110, Speaker C: Right.
01:06:57.142 - 01:07:07.302, Speaker B: Because you're only. The winner is only revealed at the end of the competition. Or are you thinking maybe about the online learning setting where there's some feedback every round?
01:07:07.358 - 01:07:17.514, Speaker A: Yeah, I was thinking about the online learning setting, but I'm saying, like, what would be the cost for your mechanism, even if each agent lies exactly once, you know?
01:07:18.974 - 01:07:23.424, Speaker B: Oh, like, they lie by a lot, but only a few times.
01:07:24.524 - 01:07:25.020, Speaker C: Yeah. Right.
01:07:25.052 - 01:07:32.024, Speaker A: Because it doesn't really affect me. Like, it's a low risk for me, but maybe could it be like a big risk for the. For the mechanism?
01:07:32.844 - 01:07:33.584, Speaker D: So.
01:07:35.364 - 01:07:36.304, Speaker C: Thank you.
01:07:39.364 - 01:07:49.864, Speaker F: Just to be sure, your model is not very rational, but in playing the game, I would try just to compete in Kabul and just think a little bit.
01:07:51.044 - 01:07:53.144, Speaker C: Okay. So, like, system one.
01:07:55.204 - 01:07:56.304, Speaker F: System two.
01:07:56.764 - 01:08:00.268, Speaker C: Okay. I think it should be okay. There you go.
01:08:00.396 - 01:08:03.684, Speaker A: And on that note, let's thank rock again and all of the students.
