00:00:00.600 - 00:00:13.914, Speaker A: Welcome everybody. This is the last session of the day. We have two talks, and the first one, and even Vandenberge is going to tell us about corridor graphs and sparse semi definite optimization.
00:00:14.694 - 00:01:02.174, Speaker B: Thank you. So this is work I started more than ten years ago with Joachim Dahl, who was a postdoc at UCLA, who is now at Mozik, and then continued with Martin Anderson and Yvon sun, who were PhD students. So the results are not really new, but I thought it was a nice subject for this workshop because it's as an example of how discrete optimization and graph theory is or can be used in continuous optimization. So the talk is about sparse semi definite optimization. So we look at an SDP in standard form. N is the dimension of the matrix variable, always n by n. We have m equality constraints, x is positive semi definite.
00:01:02.174 - 00:01:57.134, Speaker B: We use a trace inner product for the objective and the equality constraints. And then we're interested in applications where the coefficients c and AI are extremely sparse. So that happens, for example, often in relaxations, SDP relaxations, of problems that have some underlying network structure. There are certain sparsity patterns that are very common. For example in robust d squares and robust optimization, or matrix normal optimization, you often have sparsity patterns that have an arrow structure with a diagonal and a few dense rows and columns, and then problems that just arise from modeling tools like CVx. They're often very sparse with some irregular sparsity pattern because of all the new variables and constraints that are introduced. So it's a problem with n or the n squared variables, because x is an n by n matrix.
00:01:57.134 - 00:03:04.760, Speaker B: And typically to make x feasible, x has to be dense to make it positive semi definite. But there is a lot of freedom in the choice of x because the coefficients in x, the entries of x that appear in the cons in the constraints, equality constraints, and the objective form a very sparse set of coefficients. And the other coefficients have to be, are not completely free, but it's sufficient that there exists to make x positive semi definite. So that's the type of structure we'd like to exploit in solving sparse semi definite optimization problems. And we'll look at the structure in x that results from the sparsity, or more accurately, the freedom in churn, choosing most entries of x that results from the sparsity using some tools from graph theory and sparse matrix theory. So to just give some notation. So we'll use the standard representation of a sparsity pattern of a symmetric matrix as a graph.
00:03:04.760 - 00:03:49.424, Speaker B: So there are n vertices. If it's an n by n matrix we represent it by an undirected graph with edges e, and the edges represent the off diagonal non zeros in the matrix, and it's in fact the absence of an edge that has the information. So, for example, in this matrix there is no edge between three and four, and that means that the three four entry must be zero. The other entries, the entries on the diagonal, and the off diagonal entries that are adjacent in the graph may or may not be zero. I think Pavel calls this syntactic zeros. It's a nice term for the zeros and the sparsity pattern. The other ones may or may not be zero.
00:03:49.424 - 00:04:48.970, Speaker B: So I'll use this notation for the set of matrices with a certain sparsity pattern. So n is the dimension symmetric matrices of order n and sparsity pattern e. Because of this definition, also a matrix can have or the sparsity pattern of a matrix is not unique. If a matrix has a certain sparsity pattern and I add an edge in the graph, then the matrix also has that extended sparsity pattern, and we'll call that an extension of a sparsity pattern. For some points in the talk, it's important to keep in mind that I always consider the diagonal of the matrix as non zeros and never force include the diagonal entries as the zeros in the sparsity pattern. And when I refer to Clique, I'll mean by clique a maximal complete subgraph in the graph. So it's more common to use the term clique for just a complete subgraph and maximal clique for a maximal complete subgraph.
00:04:48.970 - 00:06:01.474, Speaker B: But here I use clique for a maximal complete subgraph, and the cliques in the sparsity graph correspond to maximal dense principle submatrices in the matrix. For example, this 153 is a clique in the graph that corresponds to a three by three principal sub matrix of non zeros, again in the sense that all these entries may or may not be zero. So that's just notation, and then we can define for a given sparsity pattern, any given symmetric sparsity pattern, two interesting matrix cones the first one two matrix cones in the space of symmetric matrices with that sparsity pattern. The first one are the positive semi definite matrices with a given sparsity pattern. For example, for a band pattern, you can consider the positive semi definite band matrix with a band matrices with that pattern. So you can write it as an intersection of the positive semi definite matrix cone n by n matrices and the subspace of matrices with that sparsity pattern. So that's the cone of positive semi definite matrices with a given pattern.
00:06:01.474 - 00:06:51.814, Speaker B: And the second cone is the cone of sparse matrices that have a positive semi definite completion. And we can write it as a projection of the positive semi definite cone on the subspace of matrices with that sparsity pattern. So if you take any positive semi definitely n by n matrix projected on the sparsity pattern, just orthogonal projection, which means just set all the elements in x that must be zero in the sparsity pattern equal to zero, then you obtain a positive semi definite complete matrices. And so PI e will always denote the projection, orthogonal projection on the subspace of matrices with a given sparsity pattern. So these two cones are obviously convex. This is an intersection of convex cones. This is a projection of a convex cone.
00:06:51.814 - 00:07:35.776, Speaker B: They're closed, they're pointed, they have non empty interior, and those properties are always also fairly straightforward, except perhaps that the second one is closed. So if you project the positive semi definite matrix cone, you get a closed cone. But that follows from the assumption or the convention that the diagonal entries are all included in this part. Parsley pattern wouldn't be true otherwise. And there are dual cones. So these two cones form a pair of primal and dual cones for the standard trace inner product. So with that notation we can look at the sparse SDP and rewrite them in an equivalent notation.
00:07:35.776 - 00:08:58.180, Speaker B: So if this is a sparse, the standard form SDP that I started from, this is the dual of the standard form SDP. Then in the primal problem if the coefficients are very sparse, I can make the simplification that replace the conic inequality that x is positive semi definite by replacing or considering x as a sparse matrix with a given sparsity pattern with a constraint that must be positive semi definite, completable, and the sparsity pattern I use here is the union of the sparsity patterns of all the coefficients in the SDP. And this is actually not a new problem, it's more or less just notation, but it distinguishes between the entries of x in the sparsity pattern, and those are the entries that are multiplied with nonzero coefficients in C and AI. Those are the important entries and the other entries that in general non zero. But it's sufficient to know that they exist and make the matrix positive semi definite to make these two problems equivalent. So it's more or less just a change in notation, but it's an important one because x here is in general a dense matrix variable. The x matrix here is a sparse matrix variable with a given sparsity pattern.
00:08:58.180 - 00:10:24.144, Speaker B: The union of the sparsity patterns of all the coefficients, and the constraint is not that x is positive semi definite, but that x has a positive semi definite completion. If you look at the dual problems, then this is the standard dual of the standard formula p with a matrix s that is positive semi definite, a slack matrix in the inequality here, it's more obvious to make this change in variables, because if s is feasible in a constraint, then it's sufficient to consider. Or you can restrict s to a sparse matrix with a sparsity pattern that's given by this aggregate union pattern, because all feasible matrices s will have that sparsity pattern. So the cone here is a cone of positive semi definite matrices with that given sparsity pattern. These are obviously equivalent, and then also these are dual conic linear programming problems, but with different cones k and k star. So you can equivalently look at the standard form, these sdps within n by n matrix variables as conic linear programming problems in a much lower dimensional space of matrices with a given sparsity pattern, but they're no longer symmetric or self dual sdps. You use a different cone in the primal and in the dual parallel problem.
00:10:24.144 - 00:11:22.954, Speaker B: For example, if the pattern is a band matrix, this is just a banded matrix with a certain sparsity pattern. In general, this will be a dense positive definite semi definite matrix. So we'll try to exploit the property of these two cones using some results from graph theory. So I'll first give some definitions and properties of choral graphs, and then we look at three properties of choral sparsity patterns that are very useful when solving this pair of conic LP's. So choral graph defined like this. So it's an undirected graph with a property that every cycle of length four or more has a chord. So an edge joining non consecutive nodes in the cycle.
00:11:22.954 - 00:12:06.874, Speaker B: So this is a non chordal graph because we have a cycle of length four that doesn't have a chord. If I add an edge between a and c, or between b and d, it becomes chordal. Or equivalently, if you have a cycle of length greater than three, you can always shorten it to a cycle of length triangle by taking a shortcut between chords in there. Graph and choral graphs are known under different names. They're also known as triangulated in machine learning. They're known as decomposable rigid circuit graphs, and so on. So it's a topic that has been studied and discovered and rediscovered in many different fields, but first in combinatorial optimization in the early 1960s.
00:12:06.874 - 00:13:10.524, Speaker B: And it's an important class of the perfect graphs. And that's actually one of the first topics that was mentioned in this program in the lectures by Michel Gumans. And perfect graphs are graphs for which a list of problems that are in generally very difficult, like coloring or enumerating cliques, or finding the largest clique, are in fact easy. And in the case of chordal graphs, they're actually extremely easy, because they can be solved by simple greedy methods in linear algebra. And that is sort of the property or the application area that will be useful in this talk. There arise in the study of sparse Chileski factorization, and then later in the eighties and nineties, in the study of completion problems for symmetric matrices. In machine learning, they used, known as decomposable graphs and arising graphical models, and also in continuous optimization or nonlinear optimization, they arise in the study of partial separability.
00:13:10.524 - 00:14:17.354, Speaker B: So for this talk, the most important areas will be linear algebra, these the second one, and then partial separability, and in semi definite programming. They were first used in this paper from 97 and several other papers by Kojima's group since then. So for our purposes, it's the second application that's more useful than the definition in terms of chords of cycles. And this is a result from the very early result by Rose, who made a connection with cholesky factorization. So if you take a positive definite matrix with a given sparsity pattern e, and we use this notation for the Schlesky factorization. So a permutation matrix p, lower unit, lower triangular matrix l positive diagonal d, then rows showed that if e is choral, then there exists a permutation matrix for which you have zero fill. So the sparsity pattern of L is really the same as the sparsity pattern of a.
00:14:17.354 - 00:15:04.678, Speaker B: Of course, l is triangular, so it's a sparsity pattern of l plus l transpose that we compare with a, and you have to apply the inverse permutation if you compare it with a sparsity pattern of a. So subject to the reordering, it has the same sparsity pattern as a. So a has a zero fill Chileski factorization. And that's actually a very practical definition of a choral sparsity pattern. There are patterns for which you have zero filled chileski factorizations, and it's actually necessary and sufficient if you look at it as a property of the entire set of sparse matrices. In a sense that if a sparsity pattern is not coral, then you can always find for every permutation matrix, you can find matrices with that pattern. For which you have no zero fill.
00:15:04.678 - 00:16:05.084, Speaker B: So let's give factorization. So that's a very practical definition of choral sparsity patterns. So it gives us an idea of how common they are. So there are some simple patterns that we know are choral. The simplest non trivial one is a pattern with two overlapping diagonal blocks, and that obviously has a zero fill cholesky factorization for just the standard natural ordering. And that's also a very useful pattern when studying properties or algorithms for choral sparsity patterns, because it's often useful to first try to prove a result or derive an algorithm for this special simple case, because if it's true for this special case, it's usually true that you can extend it to general choral patterns using the three structures that I'll discuss. Band patterns are obviously choral, and then an arrow pattern or a block arrow pattern are also choral because you have a zero fill cholesky factorization.
00:16:05.084 - 00:17:13.998, Speaker B: And then more generally they arise in cholesky factorization of a general matrix. So if in this matrix the filled circles are the non zeros in a positive definite matrix, it's not coral. If you apply a cholesky factorization after reordering, you get a fill in these positions and by definition the fill pattern. So the added edges plus the original edges form a coral sparsity pattern. And it's actually a choral extension of the original sparsity pattern. And the fact that for many sparsity patterns that arise in practice, you have fairly efficient chordal extensions or reorderings that give you a small fill in a cholesky factorization, so indicates that that's actually quite useful. Otherwise partialesky factorization methods wouldn't be as useful as they are, and then skipping a lot of details.
00:17:13.998 - 00:18:01.836, Speaker B: An important property of choral sparsity patterns is that you can derive several click tree structures or in general elimination tree structures that represent them. So this shows a click tree, or in linear algebra, what's known as a supernodal elimination tree for this coral sparsity pattern. So every block in the tree represents a clique in the graph. Every clique is partitioned in two sets. So the top row is the intersection with the parent and the clique. The bottom row is the remainder after the intersection with the parent. And for a choral sparsity pattern, you can go construct trees like this with very special properties.
00:18:01.836 - 00:19:05.694, Speaker B: One property is that these bottom rows form a partition of all the vertices in the graph. So one through 17. Actually these first rows form a partition of the set one to 17, and that's already a simple proof that a coral graph cannot have more than n creeks, because they can have more. And a second property is that the cliques and the clique tree that contain a certain vertex, for example, vertex ten form a subtree in the clique tree. So that's the induced subtree property. So if you look for all the cliques that contain ten, and they form this ten tree, and that's the basis of many recursive algorithms that are formulated as recursions on these trees in topological or inverse topological order. The fact that the cliques that contain a certain vertex form a subtree in the clique tree.
00:19:05.694 - 00:20:26.814, Speaker B: So in linear algebra, these first rows will be called the super nodes or maximal supernodes. And there are many equivalent or relaxed definitions. If you re number these supernodes, you can rearrange them neatly in a matrix like this, where every supernode just defines a block column of columns that have the same structure, the same zero, non zero structure. And it's also useful to know that there exists very efficient methods for computing these click trees, or in fact, given a non coral graph and an ordering for constructing all the important statistics about this click tree without actually constructing it. So you can construct the supernodes in a click tree, the child parent structure in the click tree. Computer fill in that you have after the extension, all in almost linear time, in the size of the original graph, not the filled graph, and before actually computing the filled graph. So these are just what we will need to know about choral graphs.
00:20:26.814 - 00:21:09.054, Speaker B: And then if you look back, go back to the sparse SDP problem, you'll look at three applications of choral graph theory to sparse semi definite programming. The first one relates to very classical properties of these two sparse matrix cones that I defined in the beginning and they were derived in the eighties. So the first one is this. Suppose we consider the cone of positive semi definite matrices with a given choral sparsity pattern. Then you have this nice decomposition result. A diva matrix has a certain asset called sparsity pattern. Then it's positive semi definite if and only if you can write it as a sum of simple positive semidefinite matrices.
00:21:09.054 - 00:21:52.914, Speaker B: So this is a very simple chordal sparsity pattern with three cliques. The matrix with this structure is positive semi definite if and only if. You can write it as a sum of three positive semi definite matrices that have this very simple structure, one for each clique in the sparsity pattern. And the general formula gets a little complicated because the hi matrices here are these dense sub matrices. And then we have to copy them to the right position in the n by n matrix. So this is obviously decomposition like this, obviously sufficient to make the sum positive semi definite. But for choral matrices, it's also necessary.
00:21:52.914 - 00:23:08.524, Speaker B: And this actually appeared, I think, first in a paper by Gregon Antoine, who were studying sparse quasi Newton updates. So they were interested in the question, if this is, for example, the hessian of a quadratic function, a convex quadratic function, then this property means that you can write that quadratic function as a sum of simple quadratic functions that each involve only a few variables. So that's a partial separability structure in the matrix. And this is actually also easy to see if we it follows from this property of the zero filled cholesky factorization. So if you just look at the proof for a simple case of two overlapping blocks. So we want to show that every positive semi definite matrix with this structure can be written as a sum of these two. Well, we can take the zero fill cholesky factorization of this, which is a lower triangular matrix with the same structure, and then collect the outer products in this product, and take the first block column times its transpose and it gives us a matrix of this form, and then the second block column times its transpose, and you get a matrix of this form.
00:23:08.524 - 00:23:53.074, Speaker B: So it's actually just another way of saying that the matrix has a positive semi definite zero fill chileski factorization. And these matrices also, it turns out, are computed as a byproduct of a multifrontal cholesky factorization. They're known as the update matrices in a multi frontal Schlesky factorization. So computing a decomposition like this is also very cheap. In optimization. It's very useful because it means that you write this complicated cone of positive semi definite matrices with a given pattern as a sum of very simple cones. Therefore, the second matrix cone is the cone of positive semi definite completable matrices.
00:23:53.074 - 00:25:04.254, Speaker B: We have the dual result. And the dual result is that the matrix with this structure is positive semi definite completable. If every clique defines a positive semi definite submit, and that follows from the first result and duality. Because if this is the dual cone of a cone that you can write as a sum of simple cones, then the dual cone is the intersection of the dual cones. In this case, the intersection of the duals of the simple cones are just defined by these inequalities. So this historically actually came first, this result, but you can mentioned, also derive it from the first result and then duality. So these results are very useful in algorithms for solving a sparse SDP, because it means that in a sparse SDP we can always that was the sparse SDP in the non symmetric conic form, as I discussed, you can always assume that the sparsity pattern, the union of the sparsity patterns is coral by considering a chordal extension.
00:25:04.254 - 00:26:28.844, Speaker B: And then it means that we can take this complicated constraint x in this positive semi definite completable cone and look at this constraint as a union, as an intersection of very simple constraints. You can replace this constraint by just saying that every completely specified dense principle submatrix must be positive semi definitely. And that's the setting in which many different first order and splitting methods apply. So every time you have a complicated constraint set that can be written as an intersection of simple sets in a sense that you can easily project on the set, you can apply a whole range of different methods, for example ADMM dual block coordinate descent methods. And so the details are very different in different applications, but depending on the equality constraints, you can actually apply many types of decomposition and splitting methods. And this idea actually was first used not in first order methods, but in interior point methods, in the conversion methods that were presented in these papers. So that's the first application of choral graph theory on these sparse semi definite programming problems.
00:26:28.844 - 00:27:41.780, Speaker B: The second one is then interior point methods, using these properties to speed up interior point methods. So again, we can look at the problem as the original SDP with a pair of dual self dual symmetric constraints and primal and dual, or as a non symmetric pair of conic LP's with a different cone in primal and dual positive semi definite completable cone, or positive semi definite cone with a given pattern in the dual. The advantage is that we look at optimization problems in possibly much lower dimensional space than in this problem, especially x. Here is a sparse matrix, here it's a dense matrix in general. The downside is we cannot use symmetric primal dual interior point methods because the cones are not tools. But if we can efficiently formulate barrier functions for these two cones, we can formulate primal or dual path folding or bearing methods that actually solve these conical peak problems, primarily dual. So the question is, do we have efficient barriers for these two cones? So to answer that, we can first look at a simpler one.
00:27:41.780 - 00:28:30.092, Speaker B: It's the positive semi definite matrices with a given sparsity pattern, the second cone. So there it's obvious what a barrier would be. You just take the standard log, that barrier, but you restrict it to matrices with a given sparsity pattern. So that's the minus log, that s the gradient of the log, that barrier is just a negative of the inverse. But if you look at it as a function from the space of matrices with a given sparsity pattern, and the gradient becomes the projection on the sparsity pattern of the negative inverse. So here, and that's true in general for any sparsity pattern. But the question is, can we easily compute this projection? And to compute the projection, you have to find the entries in the inverse, which in general is a dense matrix.
00:28:30.092 - 00:29:16.204, Speaker B: But you're only interested in the entries that are in the sparsity pattern. For example, if it's a band pattern, it's the main band of the inverse of this matrix. So the expression is true in general. But the question is, can we compute this efficiently? And then for the hessian, it gets a little more complicated. But if you apply the Hessian to a sparse matrix in the same space, that means you linearize the gradients in that direction and you get this expression for the Hessian. So without a projection, again, it's the Hessian of the log dead barrier applied to a matrix y in this lower dimensional space. You have to project this expression to get the Hessian at s applied to a direction y.
00:29:16.204 - 00:30:56.304, Speaker B: And again, the question is, can we easily compute this expression without actually computing these inverses or computing this entire product, which would be dense? And so the answer is, that's true for, again, coral sparsity patterns. You have very efficient formulas for the barrier gradient and Hessian, and that again follows from the zero fill property of the Cholesky factorization. Because if you're asked to compute this barrier function in general, a very good way to do it is to take a sparse Cholesky factorization of s and then add the logs of the diagonal entries in the Cholesky factorization. So for example, if you use a multifrontal factorization to compute l and d, then this algorithm will, the multi frontal method will use a recursion on the elimination tree in topological order, starting at the leaves of the elimination tree and then moving towards the root of the tree. So that gives us a very efficient method for computing the log dead barrier for the gradient. You can actually give an l and D write an equation like this by just rearranging the definition of the Schlesky factorization, and you get an equation that the inverse of s satisfies. And then it turns out for the choral sparsity pattern, you can look at this equation and solve for exactly the entries in the inverse that you're interested in, exactly the ones in the pattern and you don't need to compute any entry in s inverse that you're not interested in.
00:30:56.304 - 00:32:15.974, Speaker B: So this, we looked at this, and there's another earlier report that we found by Tim Davis and a student who also formulated multifrontal like methods for computing the gradient. And in hindsight, it's maybe not surprising, because if you have an efficient method for computing, the better via the Chileski factorization, then by automatic differentiation should get an efficient method for computing the gradient. And it makes sense that if this is a recursion on the elimination tree in topological order, bottom up to the root, then this will be a recursion in topological order, inverse topological order, starting at the root to the leaves, and then for the Hessian, you get something similar. If you take the algorithm for computing the gradient that consists of first computing the Cholesky factorization and doing this recursion for the gradients, and you linearize all the steps in the algorithm, you get something very efficient for computing this Hessian applied to a symmetric sparse matrix. And that algorithm for computing the Hessian will involve two recursions for every y you'll have an, because you're linearizing these two steps. So that's where a choral sparsity helps. You have very efficient methods for computing the barrier, the gradient and the Hessian.
00:32:15.974 - 00:33:04.684, Speaker B: So to test this, we looked at some patterns from the University of Florida collection. These are some statistics on the sparsity pattern. So the dimension number of very small to about 100,000 number of nonzeros. And here we compare the cost of our own imperfect Cholesky factorization for a matrix with that pattern, with the cost of computing this projected inverse, this quantity. So the projected inverse of an dense matrix on the sparsity pattern. And the point is that certainly for the larger instances, it's almost exactly the same. So you can compute this projected inverse, or the gradient of the barrier in roughly the same cost as a Cholesky factorization of the same, the same sparsity pattern.
00:33:04.684 - 00:33:56.920, Speaker B: So this is for the positive semi definite cone. Yes, sorry, could you explain what it means to linearize the recursion if you write out the Cholesky factorization? Of course, I skipped all the details of a multifrontal Scholesky factorization, but there's a very simple recursion where you go through the clique tree, or the elimination tree, or the supernodal delimation tree starting at the leaves. And every step you do some simple calculations to compute the entire Scholesky factorization. So it's a nonlinear each calculation is a nonlinear simple formula. So if you linearize all these steps, you get an algorithm for the gradients just by applying the chain rule, and.
00:33:56.952 - 00:33:58.444, Speaker A: Then the line arise.
00:34:00.474 - 00:34:48.994, Speaker B: So if you put these two together, you first compute the cholesky factorization which maps s to l and d, then the gradient which maps l and d to this inverse. If you linearize all these steps to compute this expression, so the Hessian applied to y, which is the derivative of this in the direction y, just the chain rule, you get an algorithm for computing this. That's an interpretation of the algorithm. Once it's derived, it's proven in different ways. So the second dual of this cone is the positive semidefinite completable cone. There, it's more complicated. So we use the log dead barrier for the positive semi definite cone from convex optimization.
00:34:48.994 - 00:35:54.914, Speaker B: The suggested barrier for the positive semi definite completable cone is the dual or the conjugate barrier. So I would mean to evaluate it at a given matrix x with a positive definite completion. You solve this optimization problem in s. This is a sparse matrix, sparse positive semi definite matrix, and then the value of this optimization optimal value gives you the value of the barrier. So this optimization problem is actually also very interesting, because it turns out that the optimal s in this problem is the inverse of the maximum determined completion of the matrix x at which you evaluate the barrier. So this problem that arises in the definition of a conjugate barrier is equivalent to this optimization problem where you fix x. You're looking for a matrix z that completes x with maximum determinant, that the z, the maximum determinant completion is a dense matrix, its inverse will be this optimal s, and that's a sparse matrix.
00:35:54.914 - 00:36:54.394, Speaker B: And once from this maximum determined completion or its inverse, you get the gradient and the Hessian by just standard results for the genre transforms. So again, these are true. In general, for a general pattern, you would have to solve this minimization problem, or the maximum determinant completion problem. Numerically, for a choral pattern, you actually have very simple, again, multifrontal style algorithms for computing this matrix s, and therefore also the gradient in Hessian by recursion over the click tree. Again, it's a similar plot where we now compare the Cholesky factorization for a given pattern with the cost of computing the gradients of this dual barrier. And it's a little more expensive because there's some small linear equation that you have to solve. But roughly speaking, it's again of the same order.
00:36:54.394 - 00:37:59.674, Speaker B: So the conclusion, as you can formulate for a choral sparsity pattern, find the define the barrier functions for the primary and the dual cone, and compute all the barrier and the gradient, the hashing, very efficiently. So then you can formulate primal or dual barrier methods. The advantages that you save a lot in the cost of computing the Schur complement system, which is one of the main terms, and the complexity of a general positive semi definite interior point methods. For example, if you have a band matrix, a band pattern, and the number of constraints is fixed, and you vary n for a given bandwidth, you get a cost per iteration that's linear in n, because the cost of computing the gradient and the Hessian and so on is linear. For a primal dual symmetric method, it would be worse than linear would be. You expect something that's cubic given m and a given bandwidth, and the same is true for arrow patterns. This has been used in several papers.
00:37:59.674 - 00:38:46.904, Speaker B: So the closest to what I described is a paper by Vavazis. And then we had a paper in 2010. We have some results that we constructed some test problems where the cost of computing the Schur complement system in an interior point minute was really the dominant term, and the complexity. So we have very large matrices from size 2000 to 30,000 with a small number of constraints m. So it's really the cost of computing the sure complement system that dominates. And then you can solve this is a cost per iteration, and it's an old table from 2010, so all the numbers would be much lower today. I think the general trend would be similar, except maybe for SDPSC, because I think it has been an important upgrade since then.
00:38:46.904 - 00:38:56.804, Speaker B: So that's used in log in non symmetric interior point methods. Yes.
00:38:57.424 - 00:39:00.728, Speaker A: Do we know what the parameter of these barriers is?
00:39:00.816 - 00:39:53.034, Speaker B: It would still be n, still n. So last is a small comment on the minimum rank completion. So we discussed positive semi definite completion. In general, we had this important characterization of the matrix where the chordal sparsity pattern is completable with a positive semidefinite completion. If each of the cliques defines a positive semi definite matrix, we looked at the maximum determined completion, but you can also look at the minimum rank positive semi definite completion, which for a general pattern would be a very difficult problem. But for a choral pattern, it turns out that that's also quite easy. And there is a result from Dantzis from 92 that says that minimum rank, the rank of the minimum rank, positive semidef completion, is the largest of these ranks, the largest rank of all the completely specified submatrices.
00:39:53.034 - 00:41:09.156, Speaker B: So that's obviously a lower bound on the minimum rank of a rank of a minimum rank completion, but is actually equal in the case of a coral matrix, and that's easy to see also if you go back to the simple two block overlapping chordal pattern. So the question will be this, you have a matrix with this pattern you want to find replace the zero with something that keeps the entire matrix or that makes the entire matrix positive semi definite and of minimum rank. So the condition for this to be possible is that these two sub matrices are positive semidefinite and then the rank, the maximum of these two ranks gives us the rank of the minimum rank completion, and that can be seen as follows. So we take these specified these two blocks. You can factorize them with any factorization, but the number of columns we assume in these factorizations is the minimum of the two ranks. You can always add zeros to make it equal. So you know these two, or you can compute these two factorizations and then if you look at the block where they in the middle where they overlap, then you have these two matrices v and v tilde that give you this equality.
00:41:09.156 - 00:41:56.096, Speaker B: V v transpose is v tilde, v tilde tilde transpose. So that means that they're related by an orthogonal matrix. You can find an orthogonal matrix that maps x v equal to v tilde times q, and then from that matrix can construct an completion of the matrix that has the same dimension as the maximum of these two. Just compute these two blocks. You compute a new block, and if you work out this product, you see it agrees with the original matrix in the specified positions, but it obviously also has this same rank. So that's the construction for two blocks. And then you can apply that to a general chordal pattern by iterating this over a click tree.
00:41:56.096 - 00:42:47.654, Speaker B: You would start at the root of the clique tree. That's your second block in this simple example. And then you keep adding, constructing a matrix like this YY transpose that gives you a completion and you add one block in Y for each supernode at a time. You never have to revise work that you already done, so you complete a complete completion with rank equal to the maximum of these ranks. So that's also very useful or should be useful in algorithms, because it means in a sparse SDP that you can also in general this problem may be strictly feasible. So general X will be full rank. But you can also always replace x by low rank outer product with rank bounded by the largest clique in the aggregate pattern of C and AI.
00:42:47.654 - 00:43:34.624, Speaker B: This is useful for many purposes, can be used to round the result of an interior point method if you apply it in relaxation, and also to prove simple proofs of simple relaxations exactness. Okay, so I should stop here. So we looked at actually some applications of choral graphs and choral sparsity. Chord of sparse matrices in sparse semi definite optimization. So mainly in decomposition methods and the implementation of non symmetric barrier methods. Thank you. Any questions?
00:43:35.324 - 00:44:32.384, Speaker A: I just have a comment for audience that here's a place where you can have a bridge from discrete to continuous, because if you have an arbitrary sparse situation, then the problem of adding the minimum number of edges to a graph to make it chordal is empty hard. I don't, I forgot what the approximation status of this problem is, but this is definitely a very, very useful, you know, people use minimum degree or some very simple, simple heuristics, but if you can reduce the number of edges that has to be added to make the graph chordal, that's equivalent to making the least amount of fill in. So that's a very nice problem for approximation and so on. To thank the speaker again.
