00:00:13.040 - 00:00:14.834, Speaker A: And there is a sign up for the room.
00:00:17.454 - 00:00:42.614, Speaker B: Thanks so much, Omer. Hi everyone. My name is Emily Black. I'm a postdoc at Stanford University working at Reg Lab with Daniel Ho. Thank you so much for having me here today. I'm going to talk to you a little bit about fairness and unfairness beyond multi calibration. I'm going to talk about all of the choices that we have, that we, that we have the option to make when we're building a machine learning model and how we can leverage those choices to mitigate unfairness in machine learning.
00:00:42.614 - 00:01:25.492, Speaker B: So a lot of us have probably seen fairness papers that go something like this. We have a model and some prediction task. We have some notion of fairness that maybe we've defined, and we constrain a model to satisfy that fairness definition. And that's wonderful. That's excellent. It leads to sort of transferable solutions to fairness problems across a variety of models and application tasks. But what do we do when this doesn't work super well in practice? And is this the main tool that we have in our toolbox to mitigate unfair behavior on the ground? Well, there's kind of a lot going on underneath the hood when we make a machine learning model.
00:01:25.492 - 00:02:21.334, Speaker B: We have a lot of decisions to make, everything from what a model's prediction target should be. When we're trying to sort of approximate a predictable task from a policy or business one. We have all the little steps that we take, turning the data that we're given into stuff that a model can understand through feature engineering, even the hyper parameters that we have on the model. And this can all ostensibly affect fairness behavior. So while we most often intervene on, say, the optimization procedure or the prediction procedure, ostensibly we could choose kind of any choice that we make within the machine learning pipeline, the model building process to make fairer models. And there's been a lot of work recently showing that these choices do have a huge impact on model behavior and specifically on fairness behavior. So some of you may have heard about this really famous paper that came out recently from Ziad Obermeier et al.
00:02:21.334 - 00:03:40.226, Speaker B: About how switching the prediction task in a healthcare distribution model, so a model that was allocating healthcare in a population, you could reduce the racial bias and the predictions of that model by changing the prediction task from something that was essentially predicting how expensive that person's health care needs would be to something that was differently correlated with health, and that ended up reducing racial bias by quite a lot. A paper that I'm going to talk about in this presentation is how in IR's tax audit models. So choosing who is audited by the IR's, you can reduce the kind of overburdensome amounts of audits on lower income individuals by switching the prediction target in that situation as well. So I'll go more into that later. But even smaller changes in the model building process, like how you pre process your data, have also been shown to have huge fairness impact. So there's this interesting paper by Ada one from the University of Zurich, that shows that if you, you know, there's often this talk about differences in accuracy and translation across different languages and MLP models. What she shows is that if you, you can mitigate this at least partially and substantially by making sure that the size of the smallest unit of language that's going into these models is the same across all languages.
00:03:40.226 - 00:04:32.714, Speaker B: So you have some models that, you have some languages with longer words, some languages with shorter words. If you chop up the words inside languages with longer words, you actually can mitigate some of the translation accuracy differences across models. And even tiny changes like changes to model hyperparameters, can also impact model behavior pretty substantially. This doesn't just happen in predictions, as has been shown, for example, by Marx et al. But also in a model's internals. So it's how a prediction is made. Paper from ICLR last year, selective ensembles for consistent predictions in the beginning, shows how, even if you just change a deep model's random seed, the top features that it relies upon to make a prediction for a given point can actually change somewhat substantially.
00:04:32.714 - 00:05:36.730, Speaker B: So, you know, with all of this in mind, given that there's so many decisions to make and they seem to have such a huge impact on model behavior, a natural question to ask, maybe, is how can we leverage the choice that we make along the pipeline to change model fairness behavior? And what are the repercussions of the freedom that all these choices allow us? And I'm going to attempt to start to answer these questions by talking to you guys about two papers. In the first, I'm talking about an example of leveraging higher order changes to the modeling pipeline. So changing the prediction target and IR's tax income models and how this can impacts fairness with respect to income. And in the second paper, I'm going to talk about just the situation where, say you're like, okay, Emily, I can't change my whole gosh darn model. I already have my prediction target. I already have most of my data. Well, even when you have your prediction target set and your data set, and maybe you can't even change your model's accuracy that much for the sake of fairness.
00:05:36.730 - 00:06:50.800, Speaker B: Even in that situation, we still have a lot of flexibility to have models with different fairness behaviors. And there are, it's even easier, actually, in that situation to tie the fact that you do have flexibility and freedom to repercussions in the law. So I'm going to show how some, something called the disparate impact doctrine interacts with some of the freedom that we have along the machine learning pipeline. And while I'm going to show you two examples of papers that talk about sort of how we can leverage the pipeline and the repercussions of the choices that we have along that pipeline, I hope what you'll take away from this talk, you know, if you take away anything, is just how much freedom we really do have when making machine learning models and how we could. We should hopefully look into leveraging more of these choices to make fair models as we continue to make more and more models in more and more places. Okay, but first, as promised, I'll talk about the IR's. So in this project, through a novel collaboration with the Internal Revenue Service and Stanford, we are investigating the use of AI and ML systems in the tax audit selection process.
00:06:50.800 - 00:07:59.488, Speaker B: So who the Internal Revenue Service is selecting to investigate their tax returns because they think that there was something done wrong on their taxes. And we focus in particular on investigating fairness of AI selected audit allocations with respect to taxpayer income. And in order to investigate fairness with respect to taxpayer income, we sort of found that a lot of the common fairness definitions and desiderata didn't fit quite exactly with what we wanted. And also, if you're taking into account IR's buy in, there was maybe a little bit of discussions about what's the right thing to do and all this stuff. So instead of using some of the more sort of common fairness definitions, we relied upon this notion from the public finance literature called vertical equity, which is centered around treating different people appropriately differently to achieve fairness. I'll go into that briefly a little bit later. But what we find basically is that off the shelf fairness mitigation methods don't always function so well in public policy contexts, especially because of the fact that in this situation where we have a budgeted allocation problem.
00:07:59.488 - 00:09:16.534, Speaker B: So some of the mitigation techniques that are common in the literature sort of deal with the whole model and not the fact that you might only be using a small subset beyond sort of the conceptual differences with what we hope for behaviorally from the model. And what we find instead is that by changing some of the modeling decisions that we've made along the way when building an IR's tax audit model, namely changing the prediction target from just predicting binary, whether or not someone is likely to misreport to the expected value of how much they are likely to misreport on their taxes. That ends up being arguably a more effective method of mitigating unfair behavior with respect to income. But first, I'm going to talk about some background on IR's data, how tax audits work, and like the IR's in general. Then I'm going to give you some motivating facts based off of that data that we have access to about how audits work right now and how they may or may not be unfair. I'll briefly describe the audit problem and then I'll talk a little bit about vertical equity and how we sort of conceptualize that and then some results before I move on to my discussion of model multiplicity. So taxes fund the us government, but tax collection doesn't always work perfectly.
00:09:16.534 - 00:09:53.244, Speaker B: The estimated tax gap yearly is about $450 billion, which is about 20% of all income tax laws liability. And just in case, I'm sorry if I didn't cover this earlier, but the IR's is the Internal Revenue Service. That's the agency and the government that collects people's taxes that they pay. So that's good with everyone. Awesome. Okay, so there's a $450 billion, there's 20% of all income tax liability in this project. We're just working with individual income tax, so we're not thinking about businesses or anything like that.
00:09:53.244 - 00:10:41.928, Speaker B: So in order to recover lost revenue and to deter non compliance with the tax code, the IR's audits individuals. So they look into the tax forms that they think are not compliant with the tax code. But how these audits can proceed differ based on where you sit on the income spectrum. So if you're on the higher end of the income spectrum, if you have more money, you're likely to get a request for excess tax liability after an investigation has taken place. However, if you're on the lower end of the income spectrum, the IR's is most likely auditing you for supposedly falsely claiming a tax credit. And so they will send you a letter saying, we don't think that you deserve this tax credit and we're just not going to give you the money. And the onerous is on the taxpayer to prove that they deserve that tax credit.
00:10:41.928 - 00:11:32.904, Speaker B: So how the audit proceeds is a little bit different. Audits can pose a heavy burden on individuals and can even lead to financial hardship. A report from the national Taxpayer Advocate said that this is particularly true for low income individuals as they can rely on tax refunds for rent payments, car payments, etcetera. And so, given the high stakes nature of IR's tax audits, it's important that we find an allocation system that distributes audits equally with respect to taxpayer income, and that's what we're hoping to do here. So with all of this in mind, sort of the question that I'm approaching in this first product is how can we make a machine learning based audit selection system as fair as possible? Big asterisk, given institutional constraints with respect to an individual's income and what tools can we leverage? Yes. Why do you want to allocate them equally? Oh, we're not going to allocate them equally. Not at all.
00:11:32.904 - 00:11:50.484, Speaker B: The bigger gap to be coming from, like, higher income. Yes, definitely. I apologize if that was the impression that I gave. I mean, fairly, but definitely not equally. Yes. Yeah. Can you also just explain one more time how the two different audits work? Yeah.
00:11:50.484 - 00:12:49.184, Speaker B: So I know this is very helpful, but basically, when you're on the higher end of the income spectrum, the IR's will do an investigation first, usually one where they will either send out an auditor to you or like, have an auditor correspond with you in some way. And they won't kind of request the external funds until they've already kind of proven that you owe more money. Meanwhile, if you're on the lower end of the income spectrum and you're claiming a tax credit, so you're saying like, hey, you know, I make under the income cap for the earned income tax credit. That's an income subsidy for working families. I think I should get like a $6,000 tax refund from the IR's. The IR's will be like, it doesn't look like you qualify. Send you a letter saying that doesn't look like you qualify, and then won't send you the tax refund, and then it's on you to basically say, like, no, I really do deserve this credit.
00:12:49.184 - 00:12:51.884, Speaker B: Yes. Oh, sorry.
00:12:52.984 - 00:12:57.960, Speaker A: You set it to the modes of it. I mean, is it explicit? Because for higher income and lower income.
00:12:58.032 - 00:13:19.104, Speaker B: No, it's exclusive for the tax credits. So the credit refunds are different from the non credit refunds. It just so happens that most people claiming tax credits are on the lower end of the income spectrum, especially so. Especially the tax credits that they pay the most attention to. So the biggest tax credits, like the earned income tax credit, are on the lower end of the income spectrum. And so that's why this.
00:13:19.564 - 00:13:28.462, Speaker A: And is it related to whether you owe taxes or requesting a refund, or is it just related to whether the other disability, about the tax credit or income that you.
00:13:28.558 - 00:13:50.194, Speaker B: That's a good question. I know that they particularly pay attention to the earned income tax credit. In particular, I haven't heard about whether or not you're getting money back or paying money in general. I know that it's centered around, especially the earned income tax credit in the interest of time. Do you mind if I. Okay, great. All right, so this is a question that I'm sort of tackling.
00:13:50.194 - 00:14:25.684, Speaker B: What tools can we leverage to make a machine system as fair as possible, given institutional constraints? So, to answer these questions, I get to look at real IR's tax data, anonymized, of course. So I have two data sets, the national research program data, which is. Oh, my gosh, I can't believe a random sample, a statistical random sample that the IR's collects on a yearly basis. They use it to estimate the tax gap. We use it for research purposes so we don't have to worry about selection bias when we're doing these experiments. And then there's also operational audits. These are risk selected audits.
00:14:25.684 - 00:14:38.124, Speaker B: I have them from actually, 2010 to 2014 as well. And these are audits that are selected and have been done to actually recover the tax gap over the last several years. So, yes.
00:14:39.984 - 00:14:42.600, Speaker A: They randomly audited 70,000 people.
00:14:42.672 - 00:15:04.244, Speaker B: They randomly audited 70,000 people. Exactly. And so both of these datasets essentially look the same. You just know which one is which, because you need to know which one is the random sample and which isn't. And each row is just an individual person's tax return. And nonetheless. And then also the results of the audit so that they were audited and how much money they owed, if they owed money at all.
00:15:04.244 - 00:15:21.954, Speaker B: Yeah. The first kind of audits that you owe an auditor is. So they. They would. That's a good question. They would. So actually, it probably is both.
00:15:21.954 - 00:16:10.402, Speaker B: That's a good question. If they did correspondence audits in the random sample, I know that they have the same kind of examiner look over each person's tax returns and then decide, oh, my God, do they owe money or not? And then they would pursue the audit if they did, in fact, owe money. So they would probably take whichever method was the method that would use for that particular type of suspected tax fraud. But I'm not entirely sure that's a good question. So we use the NRP data to train all of our machine learning models, but we use operational data to investigate the status quo of IR's tax audit behavior, which I'm going to talk to you a little bit about really briefly now. So here we have a graph of the IR's audit rate by income in 2014. The y axis is audit rate and the x axis is income in thousands.
00:16:10.402 - 00:16:55.674, Speaker B: So each point on this line is a bucket of 10,000 people, sorry, a bucket of people that make between zero and $10,000.10 and 20, etcetera. So this is a million over here. And what we can see here, I'm going to talk about two sort of motivating facts here. Firstly, is that at least in most recent years, very low income individuals and high income individuals are audited at the same rate. So we can see here people making above around $50,000 or below $50,000 and between 700,000 and $1 million here, depending on where you hit this bumpy line, are audited at the same rate. And everyone in between is audited at a lower rate.
00:16:55.674 - 00:17:39.214, Speaker B: And I'll just say really quickly, this also transfers over to when we plot this graph in terms of income deciles. So that last graph was in terms of income buckets of $10,000. This is ten percentiles of income with the lower, with the number here being the lower bound of income on each of those percentiles. So this is a zero to 10th, the 10th to 20th, et cetera. And so what we can see is that that spike that looks small in that first graph actually makes up a large portion of the us population. And however, you know, that's maybe bad enough, but if we look about, we look at actual tax non compliance as a function of income. Again, this is income decile.
00:17:39.214 - 00:18:15.954, Speaker B: This is non compliance over $200. So, like whether or not someone misreports on their taxes above $200, we see that non compliance actually increases roughly monotonically in income. So these two facts together sort of bring us to. Oh, sorry. Yeah, the x axis here again was the income deciles with the income on the bottom end of the decile. And then the y axis was the misreport rate, the percentage of the population that's misreporting over $200. And these two facts together leads to sort of a motivating question of how we'll understand equity in this context.
00:18:15.954 - 00:18:22.434, Speaker B: If misreporting increases monotonically in income, why is there such a focus on the lower end of the income factor?
00:18:22.794 - 00:18:30.050, Speaker A: Is this when you say income. Is this sort of the verified post audit income or the reported income pre audit?
00:18:30.122 - 00:18:32.858, Speaker B: This is adjusted gross income. Post audit.
00:18:32.946 - 00:18:40.178, Speaker A: Post audit. So I would have guessed the spike was because you don't report any income. That's very suspicious.
00:18:40.306 - 00:19:18.920, Speaker B: Yeah. No, this is because I don't want to spoil everything, but I can actually, I'm not going to talk about the fact that it's about this too, too much in this presentation. So I'm happy to talk about this more offline, but basically, audits on the low end of the income spectrum are very cheap and the return on investment is almost unbeatable. So if you are in a resource constrained IR's, you don't have that much money. Your budget's been decreasing year to year. The best way, the most sure way to make sure you're getting millions of dollars back is actually by targeting low income individuals, especially because, you know, a letter instead of sending out a person. Right.
00:19:18.920 - 00:19:28.792, Speaker B: And then also they have this rule that if you don't respond to that letter, that's taken as an admission of non compliance and they won't send you the money. So you have all these low income people who may be. Yeah.
00:19:28.848 - 00:19:42.422, Speaker A: Make sure you get it. So if I remember, then is the $50,000 in this figure or the zero?
00:19:42.528 - 00:19:45.498, Speaker B: Perfect. Yeah.
00:19:45.586 - 00:19:50.890, Speaker A: So this is just, this is percent of income, of misreport, percent of returns that have any amount of misreported.
00:19:50.922 - 00:20:08.976, Speaker B: This is percent of returns that have over $200 misreported. Yes. No, no, no. No problem. I'm glad that you guys have questions. This is great. Okay, so I'll quickly define the audit problem before moving on to our understanding of equity in this context.
00:20:08.976 - 00:20:43.200, Speaker B: So the IR's has many, many, many objectives that it wants to accomplish when it's making an audit allocation, they want, among other things, an audit allocation that maximizes revenue. So they want to reclaim as much of the tax gap as possible. They also want to minimize the no change rate, so they don't like it when they audit individuals who do not actually owe excess tax. They also want to deter all taxpayers for misreporting. So even if you're not getting audited, you should be afraid that you will be because they don't want you to misreport on your taxes. Right. And it also has to respect two different forms of a budget.
00:20:43.200 - 00:21:33.570, Speaker B: One is a number of audits or a percentage of the population budget, because you have to have enough IR's auditors to do all the audits that you want to do. And if you do too many audits, that's not going to happen. And then this is a little bit slightly softer, but still exists. There's also, you have to stay within a dollar budget. Audits cost money to do, and you can't do more expensive audits or more audits than you have money within your organization to do. And then finally, you also have to respect some further institutional constraints about pressures that Congress have put on you to look into certain kinds of taxpayers or certain kinds of rules about what to look at when you're investigating tax returns. And on top of all of this, I'm still saying let's create an equitable system that is equitable as possible with respect to income.
00:21:33.570 - 00:22:40.614, Speaker B: So seeing all of this, one response might be, okay, let's make a crazy multi objective problem, solve all of this at once. But while there are some people at my lab actually working on coming up with an optimal auditing procedure, I think it's a really hard question to come up with a system that takes into account all of these constraints at once, especially because equity and deterrence are really hard to formalize in practice. And how we're going to think about this problem is a two part process. In one part, a machine learning model predicts risk for misreporting. And then in the second part of the process, there are some policy informed heuristics that take those predictions into an actual audit allocation. And I think it's also useful to think about these things beyond the fact that that's, you know, largely what actually happens in practice. One central question in this research project is, you know, how well do, um, some more typical machine learning fairness interventions work at preventing bias in machine learning models.
00:22:40.614 - 00:23:36.684, Speaker B: And for that we need our classic, like here is a classification model that does a prediction and we can slap some constraints. So useful for us. So with all that being said, the super, super simplified audit problem, uh, is that we are going to maximize the output of, um, we're going to maximize the output of predicted value from a machine learning model subject to a budget. So this is the predicted probability of misreport. If you're using a classification model that's trained on zero one outputs, or if you're using a regression model that's trained to predict the expected amount of how much people owe on their taxes, the tax liability, it would be the sum of those outputs such that the number of audits that you do is less than your audit budget or the percentage of audits that you do is less than your audit budget. So in this situation, we're using a budget as a percentage of the population, we use 0.644% because that is the average audit rate between 2010 and 2014.
00:23:36.684 - 00:23:50.444, Speaker B: So thus, after prediction, just to make things super clear and super simple, what this amounts to is that you have a machine learning model. You rank your predictions in terms of raw output, and you take the top. .0644% and that is your audit allocation.
00:23:50.944 - 00:23:55.924, Speaker A: All right, sorry. The constraint here is just on the number of audits, or it could be weighted.
00:23:57.604 - 00:24:38.194, Speaker B: Your audit budget is just the number of audits that you do. So, yeah, that's the way that we're starting. But the way that we're evaluating our allocations is by seeing a how much revenue they produce, how much is reclaimed by the IR's and the no change rate. So the rate of auditing individuals who do not actually owe more taxes, does this make sense to everyone? And also, of course, vertical equity, which I will get to in a moment, but deterrence and all those other things a little bit too complicated for now. So we're just basically saying we want high revenue, we want a low rate of auditing people who don't actually owe excess tax. And we're also going to think about fairness equity.
00:24:38.494 - 00:24:42.446, Speaker A: This is revenue, not sort of IR's profit.
00:24:42.630 - 00:24:44.014, Speaker B: This is net revenue.
00:24:44.134 - 00:24:47.010, Speaker A: It doesn't account for the cost of their audits.
00:24:47.142 - 00:25:54.414, Speaker B: So the revenue calculation does actually take into account the cost of their audits. So our goal here is to increase equity without decreasing revenue and increasing the rate at which individuals are audited do not actually owe excess tax. And of course, we want to create things, create the allocation to be as fair as possible. So I'm quickly going to talk about how we think about fairness in this context. So what are our fairness desiderata here? To understand this question, we borrow this idea from the public finance literature called vertical equity, which is centered around the idea that in order to achieve fairness, you should acknowledge the differences between individuals and treat them appropriately, differently in order to achieve fairness. And this is a little bit from, a little bit different from a lot of the common definitions in the fairness literature, which I would argue cluster sort of around this other related idea that the public finance literature calls horizontal equity, which is that we think of all people as equal, so we want to treat them the same all the time. So this leads to, you know, equalizing rates, demographic disparity, equalize true positive, false positive rates, et cetera.
00:25:54.414 - 00:26:29.644, Speaker B: So vertical equity isn't a fairness definition. I don't have a definition up on the slide. It's more of a framework from which we're thinking about how to think about equity in this context. And it requires information about the actual application to make sense. And so how do we make sense of it in this situation? Well, first, you know, we notice that audits pose varying burdens on people depending on their income. So people with less money might experience a higher burden from having an IR's tax audit. And so we prefer slightly, we prefer allocations that decrease audit focus on lower income individuals.
00:26:29.644 - 00:27:41.878, Speaker B: So we want things to be kind of monotonic. We prefer more monotonicity in our audit allocation and a little bit more. Another way that we're conceptualizing fairness here is that we also want audit selection to closely reflect the allocation of an oracle, where an oracle model in this case is a model that knows who misreported how much everyone misreported and selects individuals in terms in order of how much they misreported on their taxes. And how we're going to measure how much the a given model's allocation corresponds to that of the oracle is we just measure the overlap in their allocation. So how many people do they audit in common over the total number of audits, the calculation, or so we take the absolute number of people that they have in common and then we divide by the total number of people in the audit, which is going to be the same size. The oracle, the oracle he said is related to the amount that they disrepute. Is that the absolute amount of relative.
00:27:41.878 - 00:28:09.072, Speaker B: The absolute amount, yes. So now that we have this, now that we sort of know how we're thinking about fairness in this context, I can say finally, what do we want? We want to increase vertical equity without hurting revenue and no change rate. So that means we want more monotonic allocations that are close to this notion of the oracle with high revenue and low node change rate. Does this make sense?
00:28:09.208 - 00:28:12.048, Speaker A: Sorry, miss something, what does oracle mean? Is that like the graph?
00:28:12.216 - 00:28:25.422, Speaker B: That's the, so that's the model that basically selects people for audit in order of how much they're non compliant on their taxes. So it's like, it's kind of like a ground truth model, but it's like a model that has perfect knowledge.
00:28:25.558 - 00:28:28.150, Speaker A: And how much non compliant is that.
00:28:28.222 - 00:28:32.234, Speaker B: Like it's how much they extra money they owe to the IR's.
00:28:33.094 - 00:28:33.878, Speaker A: One violation.
00:28:33.926 - 00:29:02.594, Speaker B: It's the actual, it's the actual amount of money. Yeah, no problem. Okay, so now I will get to the results that we saw in this project. So firstly, you know, the IR's was looking to get into these more high power machine learning models to use for audit allocation. So we used random forests on the IR's tax data. This is not what they use on a day to day basis, at least not yet. So don't worry, this is not the models that they actually use.
00:29:02.594 - 00:29:36.228, Speaker B: But here we have the audit rate by income of the allocation of a random forest classification model predicting whether or not someone is likely to misreport on their taxes. So this model is trained on binary outcomes. And so on the y axis we have audit rate. On the x axis we have income in thousands per income decile, where the income is at lower end of the decile. The black line shows the audit rate by income of the random forests oracle, the random forests model allocation. And then the red line shows the allocation of that of the Oracle. And remember, we're talking about allocations here and not all of the models predictions.
00:29:36.228 - 00:29:55.992, Speaker B: So this is just the distribution over the top 0.64% of predictions, because this is the Oracle allocation. This is a model's allocation and not all of its predictions. So I'm not, I'm not saying that this is what, like every single person that the random forest model selected for audit, this is just the top, .644% which would be exactly who we would audit. It's about 1.25 million people.
00:29:55.992 - 00:30:00.176, Speaker B: Does that make sense? Yes.
00:30:00.240 - 00:30:01.964, Speaker A: How do you calculate the Oracle?
00:30:02.344 - 00:30:44.854, Speaker B: So we have the true amount of money that each person misreported in the NRP data? Yes, exactly. So, you know, the no change rate on this is pretty good. It doesn't audit virtually anyone who doesn't misreport on their taxes. It only has a no change rate of about 3.5%, which is an increase from some of the more simple model that the IR's have been using earlier. But it has a pretty big problem, which is that in this, using this model, it pretty much only audits people in the lower to middle income classes and doesn't target people on the higher end of the income spectrum at all. So to fix this problem, we first try to use some off the shelf bias mitigation.
00:30:44.854 - 00:31:25.446, Speaker B: We enforce equalized odds, which in this case is enforcing that across different income bins. So we bin incomes by the percentiles that we've been seeing. So each income bin is ten percentiles of the population, zero to ten is one bin, ten to 20 is another, et cetera. We enforce equalized odds over the income bins. So that means that we're enforcing that the probability that a compliant or non compliant tax payer is subject to audit does not depend on their income bin. And we implement this with Fairlearn from Microsoft. And as an example of our results, consider this graph of audit rate by income decile here.
00:31:25.446 - 00:32:11.840, Speaker B: That's the same x and y axis from before. And we just have the addition of this blue model, which is an equalized odds constrained random forest classifier. And we basically noticed three things. Firstly, is that because we have a budgeted allocation here, we're only using the top six four, 4% of the model's predictions. Equalized odds while it's satisfied on the whole population as a result of using this method or post processing methods, it's not satisfied on just this portion of the population. So equalized odds isn't exactly satisfied here. And, you know, but there is some marginal equity improvement, although it is marginal, there is some focus on higher income populations, but not a huge amount.
00:32:11.840 - 00:32:35.920, Speaker B: And definitely still like a huge gap with relation to the oracle. And as you can see, the oracle overlap is only about 4%. So we increased by about 4%, which isn't maybe amazing given what we're going for. And finally, what I'll note here is that there are some pretty high trade offs. So we decrease revenue from 3 billion originally to 0.5 billion. So around $2.5
00:32:35.920 - 00:33:23.540, Speaker B: billion decrease in revenue, not leaving you with very much. And we also increased the no change rate by about 10%. So not super amazing in terms of. Yeah. Do you have a sense of why it is like auditing high income individuals? So like at such a low rate, like why utilize audits make sense is the way to fix this or if there's. Yeah, so we, because we were saying earlier that like one of our notions of fairness was kind of like monotonicity. We showed that equalized odds under the, under certain conditions will enforce monotonicity in audit rate, but it's unclear how to enforce it over just a portion of the population.
00:33:23.540 - 00:33:36.504, Speaker B: So we did it over the whole population, then it didn't kind of like follow through. So that was the reason why equalize odds sort of made sense. We also did some other fairness metrics, just, you know, for, I don't know, completion sake. But this is what I'm focusing on in the presentation. Yeah.
00:33:37.564 - 00:33:42.744, Speaker A: Your predictor just outputs auditory things.
00:33:43.084 - 00:33:44.220, Speaker B: Yes, yes.
00:33:44.332 - 00:33:54.854, Speaker A: So you do the equalized dots to use this to target the amount owes or just whether.
00:33:58.074 - 00:34:05.538, Speaker B: Everything is trained here on zero one predictions of whether or not they were over $200. So it's like, were you non compliant or not?
00:34:05.666 - 00:34:08.474, Speaker A: Oh, but you're not going to audit those but what you're saying here is.
00:34:08.514 - 00:34:30.603, Speaker B: So this is the population of people that I am going to audit because this is the top .644% of those predictions because we're taking the maximum raw, you know, in this case, the maximum raw probability of misreport based on the random models, random force models prediction. So it's, since it's predicting zero one, it's kind of like, you know, how likely am I to have misreported on my taxes?
00:34:33.223 - 00:34:35.207, Speaker A: Random force is outputting just zero or one.
00:34:35.255 - 00:34:36.151, Speaker B: Yes, exactly.
00:34:36.287 - 00:34:37.727, Speaker A: One on just.
00:34:37.775 - 00:34:50.890, Speaker B: Sorry. No, it's so the, you can get the random forest to output between zero and one. Like how close to one it thinks it is. Right. So like it's trained on zero one, but there's a raw output that is between two. Exactly.
00:34:50.962 - 00:34:52.330, Speaker A: Oh, I see. Okay.
00:34:52.402 - 00:34:52.698, Speaker B: Yes.
00:34:52.746 - 00:34:56.226, Speaker A: But post processing for equalized odds then.
00:34:56.330 - 00:35:00.426, Speaker B: So this is in processing, we also have post processing and the results are.
00:35:00.450 - 00:35:06.122, Speaker A: Effectively the same even if you post process after sort of counting for the 6%.
00:35:06.258 - 00:35:23.080, Speaker B: Yeah. So if you. We didn't do post processing accounting just for the 6%. We did do. Okay, maybe it's somewhere in the paper, but I think the tricky thing. No, it's not. The tricky thing is there is like you have to keep on going deeper into the set to satisfy that.
00:35:23.080 - 00:35:54.104, Speaker B: So it's kind of unclear how you would do it on just the allocation, if that makes sense. Like to get equalized odds on just the top, .64% since you don't have the individuals to make it satisfy equalize odds, you would have to keep on pulling up from later and later in the ranking and. Yeah, yeah, yeah. So I guess with two questions. One is what would it look like instead of trying to predict the audit rate, you try to predict the actual amount. That's what we're getting to.
00:35:54.104 - 00:36:23.584, Speaker B: And then what about if you try to enforce like calibration conditions that are specific to just those highest bids instead of like snap. Maybe we can talk about later, but just in the interest of time. I'm about to get to what you're saying. Yes, because I also want to get to my second paper, hopefully. So, exactly what you said. So we didn't find this to be extremely effective. What can we do instead? We can change the model's prediction task as an intervention to increase equity in this setting.
00:36:23.584 - 00:37:04.704, Speaker B: So if we switch from training on zero one labels, whether or not someone has misreported to their taxes to the amount that they've misreported on the taxes that the model is now predicting, the expected amount of misreport we can see here already, this is a lot closer to the oracle and focuses a lot more on the upper income classes. So this is the audit rate by income of the audit allocation of a random forest model that is predicting the expected adjustment. And so I don't have to dwell too much about this. Basically, it works a lot better. The equity improvement is great. Has the largest oracle overlap up to 23%. It's just about as monotonic as the oracle, which is nice.
00:37:04.704 - 00:37:32.286, Speaker B: And it also has lower trade offs. So we actually have a $7 billion increase in revenue. So now we're increasing. We have a $10 billion revenue from this model, where we only had $3 billion when we did the predicting on zero one labels before. And while we do have a steep decrease in no change rate, at least we are only decreasing one metric instead of two. So the trade offs here, I would say, are better. Yeah.
00:37:32.470 - 00:37:38.714, Speaker A: The last method you showed is much closer to the actual deploy system, right?
00:37:40.134 - 00:37:51.678, Speaker B: Yes. So the IR's currently predicts on zero one levels, but as a result of some of the work that we've been doing, they're switching to regression, which is a fun success story. Yeah.
00:37:51.766 - 00:37:55.234, Speaker A: How does a no change rate track for income?
00:37:56.094 - 00:38:22.428, Speaker B: Yeah, so it's much higher in the higher income echelon. Like the rate of auditing people who don't actually owe excess tax is much higher. The higher income you go. And that's related to. I forget who was asking this question earlier. I think it was Charlotte. Why do we think the models are not targeting high income individuals? I think the feature set that the IR's has access to is abysmal for predicting non compliance on high income individuals.
00:38:22.428 - 00:38:45.374, Speaker B: And so you have just a whole lot more uncertainty on the higher end of the income spectrum, which is why even though audit rate increases monotonically in income, you just don't have the information to target that part of the population. And what I'm working on now as a follow up is building systems specifically designed for high wealth tax evasion. So that's like my follow up work.
00:38:47.194 - 00:38:50.814, Speaker A: What's like, what fraction of the variance can you explain? Generally.
00:38:52.554 - 00:39:02.570, Speaker B: That is a great question that I would love to know the answer to, and I should probably do some research to answer exactly that, but I don't know. I just couldn't hear.
00:39:02.642 - 00:39:06.894, Speaker A: Yeah. Oh, I was just asking like, what's like the r square for, like the current model?
00:39:07.634 - 00:39:08.414, Speaker B: Oh.
00:39:10.874 - 00:39:13.254, Speaker A: For this regression problem that you showed.
00:39:13.914 - 00:40:29.434, Speaker B: I am sure that is in the paper somewhere, but I don't know it on the top of my head. So the takeaways here are, you know, modeling changes beyond just slapping a constraint on top of a model during prediction time or during optimization time. Instead, intervening at the prediction target or other places along the pipeline can actually successfully increase desired fairness behavior and practice, which is awesome, because sometimes those traditional approaches don't often work super well out of the box, as we saw in this situation. And I will note really quickly, you could say, Emily, good for you. You solved audits. Like, does this just fix the whole situation? No, it's a little bit more complicated than that, and I'm happy to talk about that offline, but I definitely don't have time for it right now. So what if we can't take advantage of all of that flexibility? What if we can't rebuild our whole model from sort of the ground up? Well, you know, what if we have our data source and prediction target kind of set, and we have almost no wiggle room to make allowances for fairness with respect to accuracy? Well, what I'm going to talk about now is that we still have a lot of flexibility to increase fairness behavior, even when we're keeping the model exactly as accurate as it was before, or like within Epsilon.
00:40:29.434 - 00:41:21.864, Speaker B: And these constraints actually make it a lot easier to connect to the law and increase legal pressure on organizations to search for the fairest model out there. So now I'm focusing on the situation where we've already defined our prediction task, and perhaps we can't sacrifice any performance change for fairness. But as increasingly many works have shown, there are still so many models that have equivalent accuracy, yet differ substantially in other properties like fairness, but also robustness, interpretability, among others. This phenomenon has been called many different things by many people over the last several years. So starting all the way back in 2001, the Rashmon effect by Bremen or Breiman, I don't know how to pronounce his name. More recently, predictive multiplicity by Marx et al. Leaven out unfairness by a paper I had a little bit a while ago, um, and under specification by GMO et al.
00:41:21.864 - 00:42:50.814, Speaker B: And you know, what my co authors and I attempt to do in this second work that I'm going to talk to you about is to unify our understanding of this phenomenon, where there are multiple equally accurate models for the same task with noticeably different behaviors. Give a little bit more detail on how this arises and its technical, legal, and policy consequences. So we call this phenomenon of multiple models existing for the same prediction task model multiplicity, and under the umbrella of this term, we break things up into procedural and predictive multiplicity, which I'll define and talk about in a minute. I'll just note quickly that a lot of the work in the fact community sort of implicitly takes advantage of this, of this phenomenon, because the mere goal of saying I'm going to enforce a fairness constraint on a model and hope that it has minimal accuracy trade off or no accuracy trade off is implicitly taking advantage of the fact that that this is happening. But maybe we don't think about it as all these possible models existing for the same task as much. And even though that's a great thing that we can take advantage of, there are also risks posed by this phenomenon. Namely, if we have all these equally accurate models that exist for the same task that do differ in other behaviors, we can essentially be arbitrarily choosing between models that have suboptimal fairness behavior, suboptimal robustness behavior, or suboptimal predictions for individual people, which can lead to some fairness repercussions there.
00:42:50.814 - 00:43:41.666, Speaker B: But first, I'll start with some definitions. We have predictive multiplicity, which is when there are differences in predictions between equally accurate models. So you have the same person over different models with the same accuracy, leading to varying predictions. Maybe this is already seeming kind of like, whoa, there could be some fairness problems here based on which model you choose. So I'll get into that in a little bit. Um, more formally, if you have distribution d of data, um, and you have x or it pairs x, y from d, and you have some distribution of binary classification models M that have more or less equivalent accuracy. We define the predictive multiplicity over M to be, uh, the expectation over different models in M, that the, uh, that their prediction on two random points in D will be different.
00:43:41.666 - 00:44:16.004, Speaker B: So the disagreement is actually the expected disagreement over models in M. We also talk about procedural multiplicity a lot in the paper. This is where there are differences in model internals that don't get shown in predictions. So how we most often see this is when there's inconsistency in model explanations. So even if you have the same prediction, if you ask a model, why did I get this prediction? Models with the same accuracy might say like, oh, you didn't get it because you didn't get approved for credit, because your income wasn't high enough. And another one will say it's because your credit history wasn't long enough, or whatever it may be. And this can also lead to some inconsistency and confusion.
00:44:16.004 - 00:45:08.984, Speaker B: But I'm not going to formalize this, because most of the results that I have ever are centered around predictive multiplicity. So, if you're interested in this, please read the paper for more information, and I'll just briefly go through some theoretical results around accuracy, variance, and predictive multiplicity. So, you know, you might think, Emily, why are you worried about this? Obviously, the more accurate your models are, the less predictive multiplicity you're going to have. So we don't really have to think about this very much at all. And then, while some sense this is true, as no two models can disagree on a percentage of points in D, that's over two times the model's error. So if the error of all the models in m is l, you can't have more disagreement than two l star. And we also show that, though I won't go into the proof, is that in the limit, as the error of our models approach, the Bayes error, predictive multiplicity goes to zero.
00:45:08.984 - 00:45:40.998, Speaker B: While all of this, all of this is true. So it might seem like, okay, we don't really have to worry about predictive multiplicity, because the more accurate you get, you know, when you find your Bayes optimal model, you don't have to think about the fact that there are all these different models with all these different behaviors. But actually, things are kind of more complicated than this. Yeah. So, yeah, so, no, actually, it doesn't have to be zero. It just has to be a predictable problem. So the Bayes risk can't be such that the.
00:45:40.998 - 00:46:21.938, Speaker B: You always get, like, 50 50 errors. I can show you the proof in the paper if that works. Yeah, things are a little bit more complicated than this. And to see why, we have to turn to the standard bias variance decompositions of error. We show that multiplicity is tightly related to variance. And so any attempts to increase accuracy by increasing variance as well will actually lead to more model multiplicity and not less, obviously, until you get to like the optimal predictor. And so, to see this, we create this mode predictor over the models m, which just returns the most common prediction over all of the models m.
00:46:21.938 - 00:47:18.854, Speaker B: We define the variance to be the expected difference between the mode predictor and any given model. And what we show is that the model multiplicity is bounded on both sides by the variance, and thus, you know, increasing variance. So it will increase model multiplicity, and specifically reducing a model's loss by trading off bias for variance and increasing the model's variance will increase predictive multiplicity. So, you know, while models of optimal accuracy won't display predictive multiplicity. The road to improving accuracy will often be paved with more and more model multiplicity, depending on whether you're deploying more complex models to achieve this. And as we often don't get optimal models in practice, this actually happens quite a lot of the time, and this does actually happen in practice. This is a quick example from a paper of mine a few years ago.
00:47:18.854 - 00:48:02.238, Speaker B: Basically, we have deep models on the left and linear models on the right. The accuracy increase of going from linear to deep is over 10%. But if we see like, this is a little bit complicated, but right here, this is the amount of differences in predictions over leave one out changes to a model's training set in linear models. And then the blue is for deep. And as we can see, even though these models are much more accurate up here, there's a lot more differences in prediction. So a lot more predictive multiplicity in those deep models. So I'm going to start to close out my talk with, uh, some opportunities, concerns and recommendations based on the fact that model multiplicity exists.
00:48:02.238 - 00:48:57.484, Speaker B: As I kind of showed a little bit quickly in those theoretical results, we can't just expect that model multiplicity is going to go away with increased accuracy. We do actually have to grapple with the fact that there are multiple equally accurate models for any given prediction task. Um, so there are some nice things about this. We have a lot more flexibility than we maybe originally think about to take advantage of models to find and then use models that have the best fairness behavior among those that are equally accurate. And we can do this not just for predictions, but also for models internals. We can build models specifically with the behavior in mind of having there be, for example, more easily attainable recourse. So having, you know, a credit credit allocation model, you can prefer models that have an easier path for someone to be able to change their prediction to get a good outcome after getting a bad one.
00:48:57.484 - 00:49:56.756, Speaker B: And interestingly, this is where one of the places where a legal connection comes in. So the fact that there are so many equally accurate models with different fairness behavior can lead to legal pressure to search for and choose the most fair model. So this is based on the disparate impact doctrine. So the disparate impact doctrine is a piece of law that says it applies to credit, housing and employment contexts, among some others. And it says that if you are an organization and you're using a model or any other decision process that I'm talking about models here, that model can only treat individuals of different demographic groups differently. If the business can demonstrate a legitimate business need to have this disparate impact across groups. And in practice, this legitimate business need is often understood to mean that you need to have that disparate impact in order to get the accuracy that your model is presenting.
00:49:56.756 - 00:51:03.056, Speaker B: And so what model multiplicity shows is that even if, you know, even if the business doesn't want to give up anything in terms of accuracy, there is probably still a better model out there that has better fairness behavior if they weren't explicitly looking for that. And so there can be legal pressure to sort of search among the space of models that are equally effective to the one that you have and find the best one with respect to fairness. In this case, this would probably be demographic disparity, because that's often what's focused on in these legal matters. Of course, the flip side, and this is why the legal pressure is necessary, is that if you don't specifically look for fairer models, you're probably going to end up with one that isn't the best. I'm going to skip, skip this for now. Finally, I think, yeah, this is my last sort of idea. A nice thing about having many similar models with slightly different predictions is that it can be a natural bulwark against algorithmic monoculture, which is the fear that all companies in the same area.
00:51:03.056 - 00:52:17.554, Speaker B: So like all algorithmic hiring models, will use a very similar model to base their, to use in their company. And so when they use the same or similar model, they'll get all the same predictions and they'll systematically collude certain individuals. And we can see, since even like tiny changes to a model's building process can lead to a substantial number of different predictions, this may not be as huge of a concern as we might think, or at least in some cases it won't be. Of course, the flip side of that is that there is sort of a lack of justifiability in the current de facto, um, process for choosing which model to use. If we choose models on the basis of accuracy alone, uh, that is sort of akin to choosing between very disparate outcomes for individuals with no real justification, because both of these models performed equally well. So for the business's perspective, you could have used either of them, but for this individual, um, it had very different outcomes. And so model multiplicity brings attention to the fact that in order to fully justify model decisions and procedures in high stakes contexts, we have to justify the choice of model as well as the model's internal decision process.
00:52:17.554 - 00:52:44.586, Speaker B: And some even, yeah, this might not seem like, oh, it's a random choice, it's not that bad. And I want to distinguish between arbitrariness and random here. So I'm borrowing from this paper. May the odds ever be in your favor. Lotteries and law by Perry and Zarsky. And they say the, the decision to opt for chance must be reasoned. So you can have an arbitrary decision.
00:52:44.586 - 00:53:55.632, Speaker B: An arbitrary decision is one that's made without thought. You don't know that you're making a choice between these two disparate outcomes for a given person, but you can choose for that decision to be random for some, you know, legitimate reason, and that can be a fine thing. So what we're trying to get rid of is arbitrariness and not randomness necessary. There is a lot of writing in the law about how arbitrariness is undesirable from the Fair Credit Reporting act and the Equal Credit Opportunity act. They have a lot of sort of language about how there shouldn't be arbitrary decisions in high stakes applications like credit scoring and credit allocation. So we think that this might carry over to have some impact on how we should select models in these applications, given that there's this legal language that says that arbitrariness is bad, but there isn't anything in, like the exact letter of the law that says that you have to justify the model selection process at the moment, just the explanation within the model. Although a growing number of legal scholars are making the argument that in order to fully justify a model's decision, you shouldn't just justify why that particular model made a given decision, but also why that model was chosen in the first place.
00:53:55.632 - 00:55:04.980, Speaker B: And to sort of add to that justification, we basically recommend documenting why you've made every single choice along this pipeline and between that justification of how you got to the model that you arrive at, plus the model's decision justification, will justify why you have a given prediction for a given individual. And even if you do all of these things, you're probably still going to have a bunch of models that satisfy all of the things that you want from a given application. And if that's the case, you can aggregate if you still. Okay, so sometimes you might want randomness because you want to prevent algorithmic monoculture or something along those lines. But in other cases, you might actually just want one prediction, one non random prediction from for a given application, like, say, in criminal justice proceedings or other super high stakes situations. And in that case, you can just combine the outputs of a bunch of models that satisfy all of the things that you want them to satisfy, fairness and accuracy, or whatever they may be, maybe, perhaps. I'm taking the mode prediction and reaching a more stable output.
00:55:04.980 - 00:55:58.884, Speaker B: And I have a paper on how to reach stability by doing this called selective ensembling for consistent predictions. So feel free to look into that if you're interested in that idea. But that brings me to the end of this talk, and what I hope that you took away from this is that the AI pipeline can expand our toolbox of bias mitigation techniques. We saw this in the IR's case study. We can leverage a lot of the choices that we make along the model building pipeline to enforce fairness in different real world applications, and also that the choices along the AI pipeline have important policy and legal implications. So even the fact that we have so many equally accurate models, or really models in general for the same prediction task, means that we have to think carefully about why we're making each choice along the model building process. We should be able to justify that choice and hopefully document that choice.
00:55:58.884 - 00:57:07.078, Speaker B: And if we choose models that are not the fairest in certain situations, you know, if we're at a big company, we might be liable for not choosing that particular model. Yeah, that's my talk. Thank you so much, guys. Set of classifiers which act like for every random string. Right, right. That would have high product predictive multiplicity. Yeah, but so if you're, I think the point that we're trying to make here is that if you're cognizant that that's a choice that you're making, and you've decided that that's better for your application context, then that's fine.
00:57:07.078 - 00:57:51.384, Speaker B: You can choose to be random. That's great. What we're trying to prevent against here is that someone essentially is trying to make a credit allocation model, a loan allocation model. Like, all right, I'm just going to choose the most accurate one. Not knowing that there are a, like infinitely many models that satisfy that criteria. There's this whole set of models that are equally accurate, and that if you looked within them, you could find models that are much more fair or much more robust, or have other properties that they may desire, and they're just not taking advantage of that flexibility at all. So it's more the thoughtlessness that we're trying to go against and the fact that people, I think, often just think like, okay, optimize, optimize for my optimization criteria.
00:57:51.384 - 00:58:09.654, Speaker B: And I just assume that I'm going to get the optimal model. But really, we don't often get the optimal model in practice. And when we're not at the optimal model, we're in this situation where we have this kind of whole pool of models that we are equally effective and we should be reasoning among why we choose one over another within that set. That makes sense.
00:58:10.994 - 00:58:12.634, Speaker A: Let's take the rest of this.
