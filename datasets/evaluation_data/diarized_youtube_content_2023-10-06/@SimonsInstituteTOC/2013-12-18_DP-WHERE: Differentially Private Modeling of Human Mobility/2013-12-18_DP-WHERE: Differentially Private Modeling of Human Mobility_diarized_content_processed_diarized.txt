00:00:00.160 - 00:00:31.054, Speaker A: This is joint work with Dharakshan Mir, who is on her way. I'm told she had some transportation issues, as many of us did. Ramon Casres, Siberin Isaacman and Margaret Martinosi. And not working. Oh, do I have to do this? Right to do that? Let's see. Sorry. Okay, we'll give it 1 minute and then we'll give up.
00:00:31.054 - 00:01:22.314, Speaker A: Okay. And if this works. No, there. Okay, very good, very good. Sorry, I didn't preview the technology before going. So we're looking in the context of human mobility modeling. So, human mobility models, models of how people move about in their daily lives, have many applications in a broad range of fields, including things like mobile computing and provisioning for mobile computing, but also in urban planning, and where you put roads, where you build bridges and tolls, and things like this, in issues of epidemiology and understanding where people connect and how people connect, and also in ecology and understanding the implications of the human world on the natural world.
00:01:22.314 - 00:02:10.402, Speaker A: And so in particular, Ramon Kasiras, my colleague on this, is at, at and t where they have cellular telephone networks and they have the data. And this is a great source of the kinds of data that can be used to tell you where people are, because there are billions of cell phones in use. The cell phone providers need the locations of those phones so that they can provide service to those phones, so they already have those locations. And the phones are typically with the people who use the phones. And so this data can give you a very good sense of where people are. But of course, there are privacy issues with using this kind of data. So we don't want to reveal a lot of the information that's in that data, real individuals homes or works or locations at any given time.
00:02:10.402 - 00:03:32.172, Speaker A: And of course, stripping obvious identifiers is not good enough. And if you need more on that, go back and watch Katrina's tutorial in the archive. So our contributions in this work are taking a differentially private approach for modeling of big data in the context of realistically modeling how large populations move within different metropolitan areas, while ensuring that the privacy of individuals whose data, ensuring that the privacy of the individuals whose data is used, we get mobility accuracy for some particular metrics within a small number of miles from the nonprivate approach. And this is a practical application of demonstrating privacy in this sort of big data context. So what we build on is some work that I was not involved in, but several of the co authors on this work are, which is what they call the where modeling approach. And the idea in that work is to identify key spatial and temporal properties of human mobility, and to extract probability distributions that represent those properties from empirical data. In particular, the source that we'll be using in our version is these call detail records that talk about when and where phone calls were made.
00:03:32.172 - 00:04:43.706, Speaker A: With cell phones you can also use other sources of data in the more general version of the where modeling approach, and then they show how to intelligently sample from these distributions in order to create synthetic cdrs called detail records for synthetic people. And there's nothing special about the output needing to be called detail records in terms of understanding how people move. But it has the advantage that there's actually already a lot of software out there, both at cell phone providers and elsewhere, that are ready to deal with these call detail records. So that's useful. And also, there's something nice about having the output that you create be in the same format as the input that it was, so that in some sense at this point you can just substitute the data that's been through their modeling procedure and later through the differentially private version of it to go back and use it in whatever way you might have used the original unmodified cdrs. So the input to where already is a modified CDR. And in particular, you've got this call detail record for every voice, call and text message that a particular phone is making.
00:04:43.706 - 00:05:46.794, Speaker A: And the modified CDR is already, even though it's not enough to strip obvious identifiers, it's generally a procedure that is done. And so even within at and t and other cell phone providers, they typically are going to be very careful with the real data and have some kind of procedure for stripping out those obvious identifiers. So here already, even with the non private version of where the cell phone actual id were replaced with a remapped version, it's got the date and time. It actually has the start date and time and the end date and time of the calls. But we're just going to work for our purposes with the start date and time of the call. And it also has a starting and ending cell tower, which again, we're just going to think about the starting one. So for our purposes, it has a cell phone id that says this cell phone was at this place in time, at this place at this time, and the cell towers are by id, but we can replace them with locations using an auxiliary table.
00:05:46.794 - 00:07:12.864, Speaker A: So the work on actually then doing this mobility characterization from these CDRs takes these location samples that are these modified CDRs, they're sparse in time and coarse in space and don't have meaning associated with them. And so in a large of work over the last several years, Isaacman and others showed that you can use this information to put more context in it. So they showed that you can see things well, like how far people travel in a given day, and also that just from these locations where the phone is during the course of the day, or not even where the phone is, but actually where the phone has activity, it makes these calls or texts. You can infer accurately home locations, work locations, and other important locations. And they validated their result against ground truth, both from actual volunteers who were in the data that said, yes, you got mine right, or other sources, things like census data and some others can give you some estimates of daily range of travel. And one difference in this approach versus other approaches from getting these data is it's very light in terms of cost, in the sense that the data is already there for other purposes. And I mean, in some sense, this is why privacy is a problem when you have big data.
00:07:12.864 - 00:08:09.102, Speaker A: So this work is sort of a good example of showing the power of what you can get out of data and then sort of selectively adding in some privacy as well, while trying to retain that utility. So the where approach overall is to start with, the CDRs use their methods for inferring home and work locations for each id and adding those in. So what you end up with is a cell phone id, a date and time, a lat long location, and then appended with the inferred home location associated with that phone id and the inferred work location associated with that id. They, in their work, actually can go on with other locations. You know, the third most likely place that you would spend time. But in our work, we use just this version where you've got the two locations. And then they use these to apply a modeling procedure to create a model of the users and their call behavior based on these kinds of records.
00:08:09.102 - 00:09:04.986, Speaker A: And then they show how to use that model to create synthetic users and calls based on that model. And so once they've done the modeling, you can then sort of create as many synthetic data sets as you would want from that graphically. And I'm not going to go into a lot of detail of how this is done. And this is for the New York area here, which is one that they studied extensively, as well as LA and some others. First, you have, once you've had these inferred home locations, you have a distribution, represented by this heat map of home locations in the geographic area. Similarly, you have one for work distributions and then to select a syn, to create a synthetic user, you pick a home according to this. You know, by sampling for this distribution, they then look at the commute distributions, which are, so the whole area has a grid associated with it.
00:09:04.986 - 00:09:29.848, Speaker A: A coarser grid will sort of be more aggregate, a narrower grid will be more refined. But you have some grid. And so, associated with each cell of the grid, you have a distribution of commutes and miles, meaning distances between inferred homes and inferred works. And so once you've selected this home. Yeah, I was fascinating. Part of the records are the record of who calls who the call graph. I assume that you're not keeping any of that.
00:09:29.848 - 00:09:58.660, Speaker A: That's right. That's not in this data. I mean, obviously the providers have that data. Both we in Dpware and even in the original where that data was not made use of. And in some sense, a lot of the decisions that my co authors and the other people they worked with in creating where they were already thinking about privacy and trying not to do anything sort of, obviously privacy bad. And so that was one choice they made. There is not to do things like say, oh, you're in the same place and you have this relationship.
00:09:58.660 - 00:10:14.316, Speaker A: I mean, obviously there's all kinds of utility you can get from that, but that's not something worth, it's not even represented already in the inputs to the modeling procedure here. Yeah, sorry, if you've already said this. Should we just interpret home as like nighttime location and work as daytime location?
00:10:14.420 - 00:10:15.892, Speaker B: I mean, for some people it's a.
00:10:15.908 - 00:10:55.104, Speaker A: Little more subtle than that, but that's a good approximation. Yeah. And so I see my time is running quick, so I'll go through a little passage. Right. So they have this commute distribution that's associated with the grid. You choose from that, a specific one, sampling from that distribution, and then the way that their modeling procedure works, you then choose a work distribution on the induced distribution of work locations at that distance. D so in some sense it's keeping some of the correlation between the actual homes and actual works, or the inferred actual homes and actual works, but it's losing some of the correlation.
00:10:55.104 - 00:11:47.326, Speaker A: So it's another step that in some sense, in an intuitive way, is also trying to get at this privacy utility trade off. And then you choose a work location there, and that gives you your synthetic user. And then for making calls, again, they go back to the distributions induced by the data at some granularity and locate the person in their home or work locations and calls that they make according to activity times at each of those location cells. So some cells are very active for phone calls and some are less in the whole data set. And so they use that and you can repeat that then as needed for as many days, months, whatever, as you want, to produce as many synthetic users as you want, and as long a duration of their calls as you want. So in a graphical format, the way this looks is you have a bunch of steps that create distributions. These are the steps that actually rely on the input data.
00:11:47.326 - 00:12:41.622, Speaker A: And then you have a bunch of steps that just use those distributions in a way that they found works well to create synthetic users and their calls. And here's an example of a typical weekday in the New York area of the real cdrs over here and the where synthetic cdrs over here, just showing where people are making calls at given time. And you know, obviously this is just a visualization, not a, not a sort of a metric. But what I want you to see here is they look not too different. And so, you know, they did lots of work in their validation on sort of showing in some sense that they aren't too different. But moving on to the privacy part, which is here, you know, obviously there are privacy concerns when detailed call data is used. You know, even though we don't have the call graph of who's calling who, we have these inferred homes and works which we said were good because they're, they're very accurate.
00:12:41.622 - 00:13:20.806, Speaker A: We have usage patterns and locations. And as we know, it's not enough to simply de anonymize by stripping the obvious identifiers. And for all the reasons that Katrina addressed in her tutorial, differential privacy, we think is a very good privacy goal and one that we're going to use here. And I'm just going to skip through, since we've already heard that background on reasons you might care about differential privacy, on the actual definition and some consequences of it. But I will say for us, what makes neighbors so it's the same. We want two databases to be neighboring if they differ in one individual. And so in particular our rows, when I was drawing them, were the call detail records.
00:13:20.806 - 00:14:40.610, Speaker A: But our neighbors is going to be looking at the whole block of rows associated with any particular phone id, so that it really is about one individual and not just one individual call. And so then really what we've done in this work is use all the tools and existing results out there about good ways to achieve differential privacy in the specific context of this application. So, for example, you can easily take the distribution of homes and make the construction of that differentially private by using the Laplace mechanism, because the sensitivity is not very big. And so that's easily done by the Laplace distribution. And if you look at it for varying values of Epsilon in that New York City data set that they have, in fact, you have to get to really very tiny epsilons before you could even see the difference from the real data to the differentially private constructed data in this representation as a cumulative distribution function of that home distribution. So basically, what we did is go through all the distributions and look at what we found as a good way to add in differential privacy. That seems to work well with a small enough epsilon and good enough accuracy.
00:14:40.610 - 00:15:39.330, Speaker A: So, as I said, these are the only places where we actually have to go back to the data. And so what we do for Dpware is we insert, in our case, almost every step really is an adding noise type of step. But more accurately, this would be an algorithm for, a mechanism for putting in differential privacy to get to these distributions that were created in a differentially private manner. Everything else can be done exactly as it was before. And in particular, just like with wearable, we could stop here and release these distributions as a model, and anyone else could use them either in this exact way or any other way that they want to. So the overall approach then is we're going to produce individual distributions, these sort of empirical cumulative distribution functions, like the one that I showed you for home in differentially private manner. And if I show you how to do each one of them, then by composition theorems, the overall way of constructing the results, whether as you stop at the model or go on and continue to use it, you get DP.
00:15:39.330 - 00:16:25.458, Speaker A: Where is an epsilon differentially private algorithm for constructing these models and synthetic users, where the epsilon overall is a sum of the epsilons that I'm going to use for each of the distributions. So I'm not going to have time to tell you how each distribution is done. And I'm not sure how instructive it would be anyway, but the overall idea is what we were doing in this paper was really looking at ways of using these known methods in a combination that pursues redulsives here. And I should say so I described them all as distributions. In some cases, they're actually families distributions, because we have one for every grid cell or one for every hour. But the same idea applies. And then we had to make some sort of practical choices based on the intended application to metropolitan scale.
00:16:25.458 - 00:17:05.572, Speaker A: So things like, well, you'll see when an example comes up. So, for both home and work, there's low global sensitivity. And we were able to just use the Laplace method, plus some post processing techniques of hay and McLeod and others, to turn them back into a proper cumulative distribution function, because we wanted to produce something that, if you released those distributions, could still be used exactly as the where distributions could. So we wanted to make sure they were distributions. The others were a little bit more complicated. So, for commute distance, as I said, that you, we assume an underlying grid on the space, which can be as fine or coarse as you like. Each grid cell is going to have a distribution for the homework commute distances for that cell.
00:17:05.572 - 00:18:21.064, Speaker A: And so the way we compute them is similar to the way that wear already computes them, except that we need to add in some privacy, and we wanted to avoid having empty cells because that would create some problems for the privacy. And so we just added in, and this is where, you know, we said, you know, based on the scenarios we were imagining, these are both small commutes and shouldn't change the overall effect too much. But this is somewhere where certainly we're making a judgment that might or might not apply in all cases. And then after doing that, we used half of the privacy budget associated with the commute distance to determine histogram bins to use. And we did that using an efficient version of the exponential mechanism that was mentioned before, as was done in some other work, and efficient, in particular in terms of known methods for sampling were available here. And then we used the other half of the remaining budget to compute noisy counts on the bins themselves, using the PLOS mechanism to get the resulting distributions. So that's just an example of sort of, you know, mixing and matching from different existing techniques.
00:18:21.064 - 00:19:30.954, Speaker A: And similarly, in the others calls per day, we used some known techniques and got some results there, as well as in call time and hourly location. And then really what we were interested in looking and seeing does it seem to be working? So we came back to this case study of New York, which is why that's the one that I showed. So the raw data, the real data, the original data. A billion, over a billion phones, sorry, over a billion records from about 260,000 phones in a data collection period of 91 days in 2011. Synthetic data sets that we looked at, we looked at using both wear and Dpware for varying values of total epsilon, all added together for 10,000 phones and 6 million call detail records. And so we compare the resulting synthetic sets both to each other for the different values of epsilon and to the original cdrs and so here's now a video of the real cdrs on the left and the DP, where synthetic cdrs with Epsilon equals 0.23 on the right here.
00:19:30.954 - 00:20:03.988, Speaker A: And I think what I want you to see here on this one is it's still not too different, but it is more different than it was when it was the real cdrs. And where. And I think they're not quite aligned in time. We had some trouble with that. So if you, you can see this kind of diffuses a little earlier and then that one doesn't. I can't tell you whether that's just an artifact of the timing or whether it's actually in the real results right now. And if you look at the distributions, you can again see that the individual ones, very kind, aren't that different.
00:20:03.988 - 00:21:01.452, Speaker A: And there's more details in the paper. And then we also want to look really sort of, if your end goal is how you're going to use these things, we want to look at how is it really behaving overall. So we have some ways of doing that, looking at the popular density distribution, which is really what that movie was showing, and captures the aggregate behavior, and then also looking at some of the, the distances, how far do individuals move? And so very quickly, since I see my time is up, what we had a sort of an upper bound, if you use, where without your cdrs at all, from all public sources of data, using the best public sources of data that they could find to do that, you get. And so here, distance we want lower is closer to real and higher is bad. So if you use all public data with where you can get here. So if adding privacy puts you above that, there'd be no point to do that. You might as well just go back to the public data, where by itself, all with the cdrs is this blue one.
00:21:01.452 - 00:21:43.760, Speaker A: And then for varying values of overall epsilon, we get this. And this spike is really here, because in these smaller hours of the day, there's not very many calls. So the variance is higher. And then we also looked at this daily range, which the question you're asking is, how far do people travel? And so these are the mean and standard deviation for those or both for the real cdrs, for wear and for DP wear. Again with that epsilon, equal to 0.23 overall. So overall, so we have these synthetic cdrs produced by DPware, which mimic to some degree the real movements seen in the real cdrs, while preserving privacy in the sense of differential privacy.
00:21:43.760 - 00:22:27.032, Speaker A: And, you know, we think this is a good case study for differential privacy and for looking at how it really acts at. And t is actually currently using it internally for testing purposes. So I think at this point, I think all they really care about is that the data is in the right format and that they can handle a lot of it coming through. So we passed that, but we're hopeful that it'll be better than that. And actually, I can't know too much about that. I mean, T and T is very careful about its actual data and how it uses it and what it tells people about it and all these things. So I don't know if I'll be able to report more when it's actually being used, but I can tell you that it is too early for that now.
00:22:27.032 - 00:23:09.044, Speaker A: And I think some open questions that this brings up. Is this really good enough for real use? Clearly it will depend on the application, but we're hopeful that there are some applications that it would be good for. Also. We kind of did an ad hoc division of the overall epsilon both into those six parts for the six distribution, and then within them, some of them had part of the privacy budget used in one way and part in another. So in addition to the question that's not really well addressed, as bias as a community as a whole, yet of what is your epsilon to choose is even then, more subtly, how do you best divide up your epsilon even once you have one? This seems also to be very application dependent, and then we hope to be able to make those models available in the future.
00:23:16.644 - 00:23:21.504, Speaker B: A quick question while the next speaker sets up. So the data won't be available?
00:23:22.084 - 00:23:57.674, Speaker A: The raw data will not be available, I believe. I have no power over the raw data. I have no power over the synthetic data, in fact. But we hope the synthetic data might be available. How long does it take? I would have to consult with Sibrin and Dirakshan to give you an exact number. It was long enough that making changes to the algorithm at the last minute ran into trouble with paper deadlines, but not so long enough that we couldn't make it. So I want to say it was at least an overnight.
00:23:57.674 - 00:24:27.950, Speaker A: It would depend. Once you have the distributions, generating the synthetic data is quite easy, but creating the distributions from the raw data takes longer. Now, there's a sampling step that I didn't mention, but we actually sampled only, I think, something like 5% or 10%, the billion calls in the first place. And so the coarser sample that you do, the less data you're working with and the faster overall that would be. But Dirakshan will be here later and she can give you a better answer to that question. Yeah.
00:24:27.982 - 00:24:49.184, Speaker B: A few years back, sprint did some research on how they collected this kind of data. And what they figured out was you have to really blow out the zip codes to everybody was uniquely identified, the home and work zip code. Is that why your epsilon had to be that small to hide this?
00:24:50.684 - 00:25:17.244, Speaker A: No, no. I mean, so, yeah, maybe. I mean, so there's certainly, there's an interplay between because I told you about the grid for the commute distance, but there's even gridding for how finely grained, I mean, even the homes and works were not identified at the location of a building, but they were maybe zip codes. I mean, they were more, you know, they were lat long quadrants, but certainly there's some interplay there. And that's something else to look at for sure.
