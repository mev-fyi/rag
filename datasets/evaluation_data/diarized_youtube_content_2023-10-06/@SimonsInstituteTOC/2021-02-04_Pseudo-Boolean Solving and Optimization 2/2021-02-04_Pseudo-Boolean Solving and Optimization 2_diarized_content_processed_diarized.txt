00:00:05.040 - 00:01:29.484, Speaker A: Welcome to part two of the tutorial on sudo Boolean solving and optimization. This is part of the bootcamp for the satisfiability theory, practice and beyond program at the Simons Institute for the Theory of Computing. In part one, we covered some preliminaries. Now in part two, we're going to talk about pseudo Boolean solving. That is, how do we determine whether a pseudo Boolean formula, a collection of zero one linear inequalities, is satisfiable or not? And we're going to use as our starting point the conflict driven paradigm used in stat solving. And therefore we're going to start by reviewing how a CDCL solar works, so that next we can talk about how we can use this to do pseudoboolean solving, either by translating the pseudo Boolean problem to CNF so that we can run a CDCL solver on it, or in the other direction, lift the CDCL reasoning to pseudo Boolean constraints to get what I will call a native pseudo boolean solver or a cutting plane spaced pseudoboolean solver. So modern sat solving started in the 1960s with the DPL method, which works so that you assign values to variables.
00:01:29.484 - 00:02:53.574, Speaker A: You try to choose them in some smart way until you discover that accidentally you happen to falsify clause, and then you backtrack and try something else. What led to the modern revolution in SAT was then when to this framework was added the idea to learn from the conflicts leading to conflict driven clause learning, where every time you happen to falsify a clause, you don't just want to backtrack, you want to derive some explanation for the failure, and you add this as a new clause to the formula. And this will provably lead to a potentially an exponential improvement in the search. Because you can cut off large parts of the search space, you can avoid making the same mistake elsewhere in your search space. And an important part of conflict driven clause learning SAT solvers is that the statistics, the information gathered from these conflicts will also guide other aspects of how the solver is being used. We're not going to talk so much about that, but we're going to focus on the basics of conflict driven clause learning, how to assign values to variables and then analyze the conflicts when things go wrong. So here is an example formula just to make sure we're on the same page.
00:02:53.574 - 00:03:37.814, Speaker A: The first clause says that U or x or Y must be true or one. And then the second clause says that x must be true or y must be false, bar denotes negation, or z must be true and then we have a bunch of other clauses and they're all animated together. So we're looking for an assignment to an assignment, zero, one assignment to these variables that will satisfy all of these clauses. If there is such an assignment, we want the solver to find it. If there is no satisfying assignment, we want the solver to report that. So as you're listening to this talk, you can quickly try to think for yourself. Is this a satisfiable or an unsatisfiable formula? We'll try to run a SAT solver on this to figure out.
00:03:37.814 - 00:04:00.082, Speaker A: And the SAT solver, when it makes variable assignments, it has two different ways of making assignments. One of them are called decisions. This is when we just make a free choice to assign a value to a variable. Say the solver for some reason likes w. It picks w and it sets it to zero. And we'll write it in this way that w equals zero. And that's a decision.
00:04:00.082 - 00:04:41.946, Speaker A: That's why we have this d on top. Okay, good. Now the solver takes a look at the formula and it sees that actually given that w is false, there is a clause which has only one non falsified literal left. And this is the clause not u or w, where we can see that the value of u is now forced. We have to set w, we have to set u to false or else this clause will be falsified. So this is called a unit propagation. And so we will always unit propagate such choices because otherwise we'll immediately run into conflict.
00:04:41.946 - 00:05:14.670, Speaker A: And the notation here is that we set u to zero as explained by the clause not u or w. And we say that not u or w is the reason for this propagation. Okay, so we add this to the partial assignment that the solver is building here on the left of the slide. And then we look at the formula if there are any further propagations. Turns out that there are no further propagations. So the solver makes a decision again. So we always propagate until saturation.
00:05:14.670 - 00:05:56.264, Speaker A: But then we make a decision. And now the solver, for some reason, decides that it wants to set x to false. And now we see that since both u and x are false, the first clause propagates y to true. And now we get a chain of propagations, because now the second clause, since x is true and y is false, this propagates z to true. But now we have a contradiction with a fourth clause which wanted y or z to be false. Both of them have been set to true. So during the search phase, the forward phase, we always propagate if possible.
00:05:56.264 - 00:06:26.504, Speaker A: Otherwise we decide until, well, either we find a satisfying assignment. But here we run into a conflict. So we have a conflict clause, y or z is a conflict clause. So now we have to do conflict analysis. And one thing we could do what a DPL solver would have done, it would just have said, okay, clearly setting x to false was a bad decision. Let me reverse it and try to set x to true instead. But in conflict driven clause learning, we want to be smarter.
00:06:26.504 - 00:07:28.360, Speaker A: We want to learn a new clause from the conflict so that we can avoid this mistake in the future. And we do this by a case analysis. Over the last two clauses, we see that the conflict clause turned into a conflict clause because it wanted z to be false. But the reason clause for said being true wanted z to be true. So these two clauses have opposite opinions about Zed. So we can merge these two clauses and remove z because we see that any satisfying assignment, if there were a satisfying assignment satisfying these two clauses, then such an assignment would also have to satisfy x or not y, just by case analysis over Z. So, note that we have now derived a new clause that is not in our formula, but it's a clause that has to be true in any satisfying assignment of the formula.
00:07:28.360 - 00:08:35.786, Speaker A: So it's a valid consequence from the formula. Now we continue. We look at this new clause, x or not y, which is unhappy because Y is true. Y is true because of the clause U or x or Y. So we can resolve these two clauses over Y to get U or x, which is another clause that we didn't know but that is implied by the formula. So we repeat this going back on the so called trail of assignments, looking at the reason clauses and resolving them until we get a clause where there's only one variable left that was propagated after the last decision, or was decided as the last decision. And here we see that we actually have such a clause now because x was the last decision, but u was assigned earlier in the search.
00:08:35.786 - 00:09:28.354, Speaker A: So this is called a unique implication point clause or a UIP clause. And since this was the first such clause that we encountered during the conflict analysis, it is the first UIP clause. So we learn that clause and then we back jump. Now what do we do when we back jump? We roll back decisions so that this learned clause is no longer false, but so that it propagates. So we undo the decision on x, and we do undo all the propagations after that. But we keep the decision on w being false and the propagation of u being false and under this assignment, we see that this learned clause propagates. It propagates now that x must be true.
00:09:28.354 - 00:10:02.628, Speaker A: It flips the value of x compared to what we had before. So this now, x being true, immediately propagates that z must be true according to the third clause. But then the fifth clause gets unhappy because it wanted either x or z to be true. So we have another conflict here. The conflict analysis is very simple. We just resolve these two clauses over z. They have different opinions about z, but they both contain not x.
00:10:02.628 - 00:10:51.484, Speaker A: The only way of satisfying both of these clauses is to set x to false. So we're actually changing our opinion about x again here. And now, since this clause contains a single literal, we back jump all the way to the top without making any decisions. And now we start unit propagating again. Note that we have learned a clause. A so called unit clause contains only one literal, and that clause propagates that x must be false. And then the other clause that we learned, u or x, now propagates that u must be true, but that propagates according to the next to last clause, that w must be true.
00:10:51.484 - 00:11:17.356, Speaker A: But then the very last clause wanted w to be false. So now we have a conflict again. And note that we have run into a conflict without making any decisions. We have just added clauses that were implied by our formula. And then we have made obvious unit propagations. And these obvious unit propagations lead to a conflict. They lead to contradiction.
00:11:17.356 - 00:12:03.834, Speaker A: So this means that at this point the solver can conclude that the formula is unsatisfiable. However, just as a mental exercise, we can try to run this conflict analysis again, just to see what happens. And by resolving the last two clauses, we derive that u must be false. We derive the unit clause, not u. Then we derive the unit clause x, which is already looking fishy because we have a unit clause, not x before. And by case analysis over that, we get contradiction. So again, in practice, the solver would not do this final derivation from the last conflict, because it already knows that it will derive contradiction.
00:12:03.834 - 00:12:49.654, Speaker A: But we could do it mentally, and we'll see in a bit why we might want to do this. So, at a high level, here is how a CDCL solver works. So if the current assignment falsified the clause, then we do this conflict analysis. We use some learning scheme, and if we learn a clause without literals, that is contradiction, then we know the formula is unsatisfiable. Otherwise, we learn the clause and back jump. And again, note that learning the empty clause is again just a theoretically nice way of expressing it. Again, the solver would terminate before learning this empty clause because it knows it will learn this empty clause.
00:12:49.654 - 00:14:25.740, Speaker A: If, however, the current assignment does not falsify any clause, and if all variables are assigned, then apparently we have a satisfying assignment so we can report that and exit. Otherwise, if there is a clause that is propagating some variable to a value, a unit clause under the current assignment with only the literal x left and all other literals false, then we let c propagate x and add this propagated assignment to the solver trail. Okay? Otherwise, if there is no propagations, then the solver would think about whether it should restart the search from the top, remove all variable assignments. Otherwise, if the restart policy says that that's not what it should do, then it checks whether it's time for clause database reduction. If so, it throws away a lot of the learned clauses because the modern solver accumulates lots and lots of clauses, so it spends also a significant amount of time throwing away clauses that it has learned. Otherwise, if none of these cases apply, then we should use some heuristics, some decision scheme to choose some variable x and a boolean value b and decide x to b, and then check if we have any propagation again. And when we translate this to a pseudo boolean setting, we're going to ignore all these important details about restarts and database reduction and how to choose the decisions.
00:14:25.740 - 00:15:35.500, Speaker A: We're actually just going to focus on the conflict analysis. Before going there, let's make a detour and just talk about how can we analyze what a CDCL solver does. So, as I already hinted, there are lots of sophisticated heuristics, and they are very hard to describe formally and understand. What seems to be the best method of analyzing what CDCL solvers can do is to focus on the underlying method of reasoning and somehow assume that the heuristics can use this method of reasoning really efficiently. As we could see in the conflict analysis what conflict analysis essentially does. It does case analysis over clauses, and this is what is known as the resolution proof system, where we start with the clauses of a formula and we derive new clauses by the resolution rule, which is a case analysis over x. Essentially, if we have a clause c or x and another clause D or not x, then by case analysis is both of these clauses are true, then C or D must also be true.
00:15:35.500 - 00:16:53.284, Speaker A: So we can derive c or D as a new clause, and then we keep going until if f is satisfiable, we derive contradiction in the form of an empty clause so when a CDCl solver is run on an unsatisfiable formula, it in fact generates a resolution proof. So this means that if we have a formula for which we know that there can be no short resolution proofs, then that also provides a lower bound limit on the running time of a CDCl solver. Because the CDCl solver will in principle have to find a resolution proof, no matter whether it outputs this proof or not. And if we know that there are no short proofs, say that the best proofs there are exponentially long, then that means CDCl solvers have to run for exponential time. Now, importantly, this is ignoring the issue of preprocessing, which are some shortcuts that CDCl solvers make at the start and that are not necessarily captured by the resolution proof system. And this is an important area, but we don't have time to go into it. And still, even with this caveat, it seems fair to say that the resolution proof system is pretty good at characterizing what CDCL solvers can do in practice.
00:16:53.284 - 00:18:10.754, Speaker A: How does this then relate to the execution of a CDCL solver? Well, if we take this dry run that we had, and here are the conflict analyses, and here are the learned clauses, and we just add arrows showing how the learned clauses are used in later conflict analysis, and string all of this together, then it can in fact be shown that this yields a valid resolution proof. And this is not just in this example, this is a general fact. So from any CDCL execution, we get a resolution proof. So what brings us here for this program at the Simons Institute is the fact that state of the art CDCL sat solvers are amazingly good. So, as a computational complexity theorist, I was taught to think about sat as a hard problem, exponentially hard. But practitioners think about sat as often being easy in practice. So this raises the question of how can this be possible? And we don't have a very good theoretical understanding of this, why these heuristics work, and why the instances that we encounter in applications are easy.
00:18:10.754 - 00:18:51.234, Speaker A: Because the paradox is that resolution is a fairly weak proof systems. So there are many strong lower bounds for different formulas, as in these references. And by the way, I should mention that I'm dumping quite a few references in these slides. They are all at the end of the slides, full bibliographic references. So if you get hold of the slides from the Simons Institute webpage, you can check the references there. Or else at the end of the fourth video with the fourth part of this tutorial, I will also go through the pages with the references so that you can find them there. So resolution is quite a weak proof system.
00:18:51.234 - 00:19:57.854, Speaker A: And in fact, there are problems where it doesn't seem to be enough. There are problems where CDCL solvers are not fast, and where the explanation is that resolution is provably not just strong enough. So the obvious question is whether we can then harness stronger methods of reasoning. And in particular, the reason that we're here, that I'm giving this tutorial and that you're listening to it, is that pseudoboolean solving, which is just in principle another name for zero one interlinear programming is such a stronger method that corresponds to what the so called cutting planes proof system can do. And I'll talk about that in a minute. And even more interestingly, not only is cutting planes more powerful than resolution for CNF formulas, but cutting planes also allows us to go from just decision problems to the richer class of optimization problems. And this is a topic that we will return to in part three of this tutorial.
00:19:57.854 - 00:20:57.326, Speaker A: Okay, so now we have seen how CDCL solvers work, and now we want to apply this to a pseudo boolean formula. Recall that a pseudo boolean formula is a collection of pseudoboolean constraints, that is, zero one integer linear inequalities. So one simple way of doing pseudo boolean solving is to read it in your pseudo boolean formula, and then whenever it propagates, you provide a disjunctive clause that explains the propagation. And then essentially you just run a CDCL solver. The only difference is that the original constraints in your formula or PB constraints. But you don't see this because the reasons you put on the trail are clauses and for instance the sat four J solver supports this. So this is not so interesting, we won't talk about it.
00:20:57.326 - 00:21:57.894, Speaker A: But a more interesting this is kind of a lazy approach where you generate clauses as you need them. But a more eager approach is to take your pseudo boolean formula, rewrite it into conjunctive normal form to an equi satisfiable formula, and run a CD salesforce on it. And this was explored in minisat plus. And two good modern solver for solvers for this are OpenwbO and naps. And we'll talk a little bit about this, but then what I am really interested in and working on, and will try to get you excited about, is to instead use native reasoning with pseudo boolean constraints, which has been explored in Prs. Galena and Pueblo for instance. And the two modern solvers that I am aware of that are doing this are sat for J by the group in Lance and rounding Sat, which is from my research group.
00:21:57.894 - 00:23:14.600, Speaker A: So in particular, in what follows, I will talk about sat for J and rounding Sat, and what goes, what happens inside these solvers. But before doing so, we'll talk about this first approach, the eager re encoding to CNF in order to run CDCL solver. So we have to first talk about how to do this rewriting to CNF, because we saw in the tutorial that there can be an exponential explosion when you rewrite a pseudo Boolean constraint, just a cardinality constraint, saying that out of these n variables, at least half of them are true. This will be an exponentially large CNF formula, and this won't do. So what we do to circumvent this is to introduce new variables, extension variables, and they allow us to get a more compact encoding. And one way of thinking about this is that the new variables that we can think of writing down a circuit that evaluates whether the pseudo Boolean constraint is true, and the new variables and clauses in our CNF formula will just be constraints, forcing the circuit to be evaluated correctly. So since it is simpler, and since it is not the main focus of this tutorial, I will focus on cardinality constraints.
00:23:14.600 - 00:24:21.966, Speaker A: So these are pseudo Boolean constraints where all the coefficients are one and only a little bit. Talk about how to generalize this to arbitrary pseudo Boolean constraints. So we have this, we're summing up variables x from x one to xn, and we're comparing them to k, maybe a greater than constraint, maybe a less than equal constraint, maybe an equality. How can we do this? One simple idea is to introduce counter variables sij with the intended meaning that the sum of the I first variables x one up to xi is at least jack and the base case. So we have a sort of inductive encoding here where the base case is that, well, s eleven, the sum of the first variable is at least one. That's true exactly when x is true. So I write down that s eleven is true if, if x is true, and x eleven has to be false if x is false.
00:24:21.966 - 00:26:27.674, Speaker A: So this is here is an implication saying that if if x one is true, that forces x eleven to true, and if x one is false, that forces x eleven to false. And of course for j strictly larger than one. We're just adding unit clauses that the first variable cannot sum to j for j greater than one, which is hopefully clear. And now suppose that we've defined all variables for si minus one and j then we say that, well, if xi is true, then the sum of the I first variable is at least one. And if the sum of the I minus one first variable is already j, then certainly the sum of the I first variable variables is also j. So more interestingly, if here's an implication saying that if xi is true, and if the sum of the I minus first variables is j, then in fact the, the sum of the I first variables is j plus one, and then we add the reverse constraint saying that, well, the only way that the sum of the I first variables can be j plus one is either that that was already the case for the I minus one first variables, or if not, then xi has to be true and s minus one j also has to be true. So it should hopefully be clear that if we concatenate all of these clauses, then we will get snk will, given any truth value assignments to the x variables, x and k will correctly encode whether it is in fact the case that out of these n variables, k of them are true.
00:26:27.674 - 00:27:28.138, Speaker A: So we'll just take any, we can take any assignment to the x variables and they will just propagate through these clauses. So if we want to enforce a cardinality constraint sum of x one to xn greater or equal k, we do this by having this encoding and then just adding a unit clause, claiming that this has to be true. And similarly, if we want the less than equal constraint for k, then we just add the unit clause, saying that the sum does not reach k plus one. And for inequality we add both of these. So this is a simple encoding to explain, and if you think about it, it's not a very good encoding, where somehow, as circuits go, it's a very unbalanced circuit. We are adding new bits one at a time. If you think about it, we can do some more divide and conquer approach where we build a binary tree where we sum up the bits pairwise, and then we sum up them maybe four by four, and then eight by eight.
00:27:28.138 - 00:28:45.808, Speaker A: This is what's known as the totalizer encoding, where we can visualize it at a binary tree where the children have t bits, let's say AI and bi, and each parent outputs two tbit cj, which is the number of ones it sees in these AI's and bi's. So we should think of Cj as the analog of the s variables from the previous slide that Cj is true precisely if the sum of the input variables AI and bi are greater or equal j. So the base case in the leaves, we have two bits. And there the output is that, well, if either x one or x two is true, then clearly one bit is true. If both x one and x two are true, then in fact the sum is two. And then conversely, we say the only way that at least one bit is true is that if one bit is true, or conversely, if x one and x two are both false, then this in fact forces c one to be false. And the only way c two can be true is if both x one and x two are true.
00:28:45.808 - 00:29:45.184, Speaker A: And then the inductive step will just be. It's very simple. If from on the a side we have I bits true, and on the b side we have j bits true, then we set as the output bit that c I j has to be true. But the only way that c I j plus one can be true, if you think about it, is that either a I plus one is true, or b j plus one is true. So if both of these are false, then it cannot be the case that we sum up to I plus j plus one bits in the output. Then here we're assuming like default values, a zero, b zero equals one. So I'm going a bit quickly over these things.
00:29:45.184 - 00:30:57.526, Speaker A: There will be some other technical details later in the talk that are also a bit, and the reason for this is, I hope you can hit the pause button and think over this. So I won't explain this further, but instead move on. Okay, and again, the idea is that to enforce a carnality constraint, we just add for the root node the unit clause ck if we have a greater equal k constraint, and if we have a less equal k constraint when we add the unit clause, not c k one. And this can in fact be extended to arbitrary pseudo Boolean constraints, something that is known as the generalized totalizer encoding. Although the size of this encoding can be a bit bad, the blow up can be pretty bad in the worst case. So what do you do then, if the blowup is bad? Well, it turns out that there's also a completely general encoding that I will just sketch that basically builds an adder circuit to evaluate the general pseudo Boolean constraint. So here's a general pseudo boolean constraint sum AI li for literals li greater equal a.
00:30:57.526 - 00:32:16.770, Speaker A: So we focus on this linear form on the left hand side, and we think of the coefficients AI as being returned in binary, where throughout this slide we just imagine that b is large enough for everything to hold, that I'm going to say. So pick b large enough, and maybe the largest, most significant bits here will all be zero. But the point is that I want to be able to sum the AI's and still fit in b bits. Okay, so with this binary representation, it's kind of trivial to make the first observation that AI li summing over I is equal to summing over I and then summing over the bits j two to the j times the bit a, which is not a variable. This is like a constant that is known at compile time, because we know what these coefficients are times the literal li. Okay, so this is just two ways of writing the same constraint. And now what we do in other network encoding is we start with the least significant bit, so the bit in position zero, and we check where which bits we have where which literals appear with an odd coefficient.
00:32:16.770 - 00:33:23.290, Speaker A: And then we just start to add them up by encoding full adders. So we take any two bits or any three bits actually in and one bit out, and then a carry. So these are s out and cout are new variables. So note that this eliminates three bits of the least significant position, and we add one back. And then we do this with full adders or half adders until there's only one bit s left in the least significant position, and all the others have moved to the next to least significant position by carries. And then we start taking care of that position using that bucket, emptying that bucket in sorenesson terminology, which presented this encoding, and we do this. So basically we just write down in CNF a full adder for this whole expression.
00:33:23.290 - 00:34:43.552, Speaker A: And then in the end we will have a CNF encoding enforcing that this linear form sum of AI li, which could be written in this as this exponential sum will be summing from j goes from zero to b two to the j times s sub j, where sj are the output variables. And then we have lots and lots of other specifications forcing these sj to the correct values. Okay? And once we have these, we can also write down CNF formulas forcing this number sum two to the j times sj. For variables, sj should be greater or equal a if we're doing a sudo Boolean greater than constraint in normalized form. Recall that we're always assuming when we're dealing with pseudo Boolean constraints, unless stated otherwise, that they are in normalized form, meaning that all the AI's and a are non negative. Okay? And obviously this is, I'm skipping like most of the interesting details, and you can see the answers on paper from 2006. To read up on these details, what do we want from these CNF encodings? Well, one thing is what is referred to as generalized arc consistency, we want them to propagate.
00:34:43.552 - 00:35:53.906, Speaker A: So if I have a partial assignment rho, and if this original pseudo Boolean constraint c propagated a value under rho, then the Cnf encoding f sub c should propagate the same values. And in particular, if rho falsifies the pseudo Boolean constraint c, then it probably won't falsify f sub c, but f sub c should at least unit propagate to falsity to contradiction. Okay, so this is a desirable property. This sort of means that from a propagation point of view, when the solver is deciding and propagating, it doesn't really matter whether it's using the pseudo boolean constraint or the CNF encoding, because it's seeing the same propagations, kind of, sort of. Okay, so the sequential counter encoding and the totalizer encoding that we talked about have this generalized or consistency property. Other networks do not, so they're, they're worse in this regard. But generalized arc consistency is not everything.
00:35:53.906 - 00:36:34.644, Speaker A: Encoding size is another property that we care a lot about. We want polynomial size, and ideally as small a polynomial as possible. And here other networks really shine. That's an extremely compact encoding. Totalizer encodings are not as good, but they have at least fewer variables than sequential encoding, and this is because this binary tree divide and conquer. However, generalized totalizer encodings can get exponentially large, so in that respect they're incomparable to other networks. They have better propagation properties, but can have much worse size.
00:36:34.644 - 00:37:55.964, Speaker A: In fact, it is possible to achieve both a generalized or consistency and polynomial size encodings, but this appears to be mostly a theoretical result, and the generalized totalizer paper reports that the generalized totalizer encoding in practice seems to be at least as good. But I'm only scratching the surface here. It's in fact the case that this question of how to encode constraints into CNF, that's a research area in its own there's a rich literature on this, and so you can refer to the SAT handbook to find more references on this. So how does this perform? Well, the approach of taking a sudo boolean formula, re encoding the constraints into CNF, and running a CDCL solver can be really, really good. In fact, it sometimes beats the native sudo boolean solvers that I'm going to talk about later, hands down. And so what is the explanation for this? So we don't have a really clear understanding, but one simple observation is that we know that extension variables gives solver a lot of power. Now, we're introducing all these extra variables that we're talking about, partial sums.
00:37:55.964 - 00:39:37.244, Speaker A: So this gives the solver a way of branching over fairly complex statements, like the partial sum of the half of the variables is at least halfway to what I would like to get. There's no way a native Sudo boolean solver can say that, because it doesn't have such a variable to branch over. And when you start composing these variables into into clauses, if you think about it, since every variable is semantically a linear inequality, then a clause over such literals corresponds can correspond to some kind of generally shaped polytope. So this is very powerful. This is maybe an explanation why the CDCL based approach to sudo Boolean solving can be so good. However, we can also see that in order for this to work, you somehow want a friendly encoding, for instance, so that variables that are somehow related in the instance are close to each other in the Sudo Boolean encoding, so that they get picked up by close to each other in the CNF encoding and help propagation. For instance, if you take a pseudo boolean formula and you just randomly permute, say, the variable indices, or flip some science and then run this translation of the pseudo Boolean constraints semantically exactly the same constraints into CNF, but just because of this random permuting that causes a terrible hit in performance.
00:39:37.244 - 00:41:00.592, Speaker A: And also you can argue from a theoretical point of view that for some problems, there is no way these extension variables can make up for the loss in reasoning power of CDCL compared to pseudo Boolean reasoning. So some open questions about this CDCL based approach for for sudo Boolean solving. One somewhat intriguing question is that the way I presented these encodings are in fact not exactly the way they're written in the papers. So you have the clauses that do the propagation. There's a kind of forward propagation which says that if I have an assignment to the x's so that at least k exis are true, then the encoding should forward propagate the final counter variable to true. That's sometimes referred to as forward propagation. Backward propagation would be in the other direction, namely that if I have an assignment so that enough excise are false, like if n minus k plus one xis are false, then I should propagate that the cardinality constraint is in fact false, and this is referred to backward propagation.
00:41:00.592 - 00:43:11.942, Speaker A: So the forward propagation clauses you see are in green on this slide, and below that you have the backward propagation clauses in red. And in fact, in my understanding, although I haven't worked on this, but my understanding is that the solvers that do this rewriting do the forward propagation clauses, but not necessarily the backward propagation clauses. And it's a natural question to ask whether it would help to add both forward and backward propagation clauses. Or there presumably was some reason why, for instance, Openwbo chose not to do this. So does it actually hurt performance? If so, why can it be that too much propagation actually hurts you? You don't want this? Some more questions well, an obvious question from the discussion so far is how do we find a good encoding? Is there a best possible encoding? How do we do this trade off between propagation strength and encoding side? Can we say something? Can we bring tools from computational complexity theory to bear on this and say something intelligent? Can we understand this observation that CDCL based solvers are sometimes so much better than native sudo boolean solvers and sometimes not? Can we provide some theoretical computational complexity results explaining this from a more applied point of view? Could we somehow combine the strength of rewriting and cutting planes based reasoning? And a final question that is somewhat interesting if you think about it in practice, because this rewriting, there's going to be a lot of rewriting before you get to running the CDCL solver on your rewritten pseudo Boolean problem. How do we know that we're solving the right problem? How do we actually know that there's re encoding of the sudo Boolean constraints into CNF was correct? Can we somehow certify this? Can we prove this? This is also an interesting question, and the current state of the art seems to be that we just have to trust that this re encoding is done correctly. Okay, so this is what I wanted to say about CDCL based pseudo boolean solving.
00:43:11.942 - 00:43:57.898, Speaker A: Now we turn to what I refer to as native pseudo boolean conflict driven search, where we won't rewrite the constraints to CNF, we will operate on them natively. We'll massage the linear inequalities and such. But except for that, we want to do the same thing as conflict driven clause learning. So what does that mean? Well, we want to do variable assignments. We want to propagate when possible, and then we make an assignment and then propagate until saturation. Okay. And when we reach a conflict, we want to do conflict analysis to learn a new constraint, and we want to add this new learn constraint to the other constraint that we have in our formula instance.
00:43:57.898 - 00:44:45.194, Speaker A: And then we want to back jump just as a CDCL solver. But now the question is, how does this work? What does it mean that a sudo Boolean constraints propagates, or what would it mean to do sudo boolean conflict analysis? This is what we have to figure out. So I will use row to refer to the current assignment of the solver, which is also referred to as the trail. And not only is this an assignment, but it also contains the information. It's an ordered assignment because the assignments were made in some chronological order. And for every assignment we know whether it is a decision or whether it was propagated. And if it was propagated, we've written down which constraint propagated it.
00:44:45.194 - 00:45:31.680, Speaker A: An important notion for sudo Boolean constraint is slack, and a slack of a constraint under a partial truth value assignment. Rho measures how far Rho is from falsifying this constraint. So how far is rho from falsifying it? Well, you take all the literals that have not been falsified, literals that are true or could be made true. You take those literals and you sum up the AI's of these literals. That's like the maximal contribution that you could get from the left hand side. And then you subtract the degree of falsity. Again here for the slack definition, it's very important that we're using normalized form, so all of the integers here are non negative.
00:45:31.680 - 00:46:16.970, Speaker A: This is one example why it's so convenient to talk about normalized form, because the slack definition gets so much cleaner. Of course, we can always define slack for pseudo boolean constraints, but it gets nicer to describe what the definition is if we are allowed to assume normalized form. So assuming normalized form, everything is positive here. Then the given row, the contribution we can hope from the left hand side, is clearly summing. Overall summing. The AI's for all literals li that are not falsified by Rho, they're either satisfied or they are unassigned. And then we subtract the degree of falsity, and that's somehow the margin of error that we still have.
00:46:16.970 - 00:47:03.476, Speaker A: That's the slack that measures how nervous we should be about falsifying this constraint. So let's go back to this constraint we had before. X one plus two x two bar plus three x three plus four x four bar plus plus five x five greater equals seven, and let's see what the slack is. Well, under the empty assignment, we see that we can sum up the left hand side to 15, and the degree of falsity is seven. So 15 minus seven is eight. So there's a lot of slack in this constraint. Now suppose that the solver for some reason, decides to set x five to false.
00:47:03.476 - 00:48:03.708, Speaker A: So we extend the partial assignment. We represent the partial assignment also by just a collection of literals set to true by it. So x five is set to false means that the assignment row contains not x five. Now we see that x five gets falsified, no longer contributes to the slack, so the slack drops from eight to three. And now an interesting observation is that three is less than four, which is the coefficient of not x four. And what this means is that if we fail to set x four to the value that this constraint wants, then there's no way we're going to satisfy this constraint. If you think about it, and you can also see this, that if both x five and x four are set to the wrong values, then the remaining contribution we can get is one plus two plus three, which is six, which is less than seven.
00:48:03.708 - 00:48:35.290, Speaker A: And the way we see this is indeed that the current slack is strictly less than some coefficient. So when this happens, the solver should propagate the literal to true. So let's do that. Let's propagate x four to false. Let's propagate not x four to true. Then, since this is a contribution in the right direction, this doesn't change the slack. So the slack is still three.
00:48:35.290 - 00:49:13.556, Speaker A: Three. We just got the contribution we were asking for. Then suppose the solver goes off and does some other assignments. Maybe it sets x three to false, and then it says x two to true because it's looking at some other constraints. And now we can compute the slack of this constraint. Then x five doesn't contribute x four contributes four, x three doesn't contribute x two doesn't contribute x one is unassigned, so it still contributes. So the contribution on the left is five, but we need seven, so the slack is negative, it's minus two.
00:49:13.556 - 00:49:52.632, Speaker A: And what this means is that there's no way we can satisfy this constraint under row. So row has already violated this constraint. It has caused a conflict. So a conflict for a constraint is when the slack goes negative. And note that in contrast to CDCL, we can get a conflict even though all variables in it are not assigned. Here, we haven't assigned, the solver hasn't assigned x one yet, but we already know that it doesn't make any difference whatever you do with x one, it's already too late to repair the previous mistakes. Okay, so good.
00:49:52.632 - 00:50:27.380, Speaker A: So then we know what slack means. And slack sudo Boolean. Slack allows us to define what it means to propagate, and what it means to reach a conflict. So now we can do, now we have what we need to lift CDCl to lift decisions and propagations to a sudo boolean setting. Fantastic. Next up is conflict analysis. What should we do? Let's look at this example cnf formula we have again, and let's observe an interesting invariant that when we have the trail, the trail here is in chronological order from top to bottom.
00:50:27.380 - 00:51:36.216, Speaker A: So here the trail is w false, u false, x false, y true, z true. And the assignment always we start out with the assignment falsifying the conflict, obviously. So the trail falsifies the conflict. But then when we do the conflict analysis and derive the first clause, x or not y, and then even if we remove, so we eliminate z from these clauses and we can pop it from the trail, and we see that what remains of the trailer is falsifying this clause. Now the next step in the conflict analysis, we resolve over y and we can remove y from the trail. And now we only have w not u, not x on the trail, and that is still falsifying the clause that we have derived at this point. So the invariant we're maintaining is that at all points in time during the conflict analysis, every derived constraint explains why the assignment made so far was not good.
00:51:36.216 - 00:52:27.440, Speaker A: So clearly here, this full assignment, we see it wasn't good because not y or not z was violated. But here we can see that even this assignment, w zero u zero and x zero is not good because it will violate this clause, which is a consequence of our formula. So we can just, because of this invariant, we can keep going in our conflict analysis, and then we just terminate it and learn the constraint when we think that it looks nice. And what's niceness? Well, this is this concept of being a first UIP constraint or an asserting constraint. And what this means that when we do the back jump, some variable is going to flip. This is an asserting constraint, because when we undo the decision on x, this clause will propagate that x flips. Okay, so we just want to lift this to a pseudo boolean setting.
00:52:27.440 - 00:53:31.594, Speaker A: So first we have this resolution step, and we see that we can actually mimic this if we have x or not y or z, and then not y or not z, and we want to resolve them to get x or not y. Then if you just look at the pseudo boolean constraints corresponding to these clauses, and you add them up, and you recall that z plus z bar cancels to yield one, then you get x plus two y bar greater equal one, and I will refer to this as generalized resolution. When you have two constraints, you possibly multiply them by something to get a common coefficient for a clashing literal. Here we don't need to to multiply because Z has coefficients one on both sides, and then you just add them up. And then here. So here the clashing literal is x one. So we multiply it by the least common multiple c of a and b and multiply this constraint by c divided by a one.
00:53:31.594 - 00:54:26.914, Speaker A: Here we multiply by c divided by b one, and then we add, and then we see that x one will cancel and we get this constraint. So the math is a bit ugly, but you can look at this concrete example instead. Okay, so this is in some sense generalizing resolution, two pseudo Boolean constraints. Now, except that we didn't quite get what we wanted, because if we wanted the clause x plus y bar greater or equal one, but we got a two here. So that was not quite right. But if you think about it, if x plus two, y bar greater or equal one is true, then since we're only interested in integral solutions, zero one solutions, in fact, the constraint x plus two y bar greater equal one implies x plus y bar greater equal one. So I can, from this I can deduce this.
00:54:26.914 - 00:55:38.352, Speaker A: Note that, and this is what's known to the saturation rule, which more in general says that for any constraint in normalized form. So here we're using normalized form again to define saturation that if I have some AI li greater equal a, then you never need a contribution more than a. So if you have any coefficients that are larger than the degree of falsity, then you can decrease them to the degree of falsity. And note that this is sound over the integers, it's not sound over the reals, okay? Because over the reals, this constraint is much stronger than this constraint. Um, but since we're solving a boolean problem, we do need rules that are sound over the integers and not over just rational numbers or real numbers, because otherwise we would be doing a linear programming with our sudo boolean constraints. And linear programming is not strong enough to solve a zero one integer programming problem. And in fact, the generalized resolution rule as defined in hooker already includes this fix.
00:55:38.352 - 00:56:03.296, Speaker A: But for this discussion that we're having here today, it will turn out to be useful to separate the resolution step and then the boolean saturation step. So now we're almost done. I mean, we can do decisions and propagations with slack. Now we have generalized resolution, and then we can saturate. So now we can try to do conflict analysis. Let's do it. So here we have two constraints.
00:56:03.296 - 00:56:42.794, Speaker A: C one two x one plus two, x two plus two, x three plus x four, greater equality four. And then c two, which says two x 1 bar plus two, x two bars plus two, x three bar greater or equal to three. Now, if you think about that, this is just an obfuscated way. C one just says that a majority of x one, x two, x three has to be true, and c two just says that a majority of them have to be false. So this should be unsatisfiable. And the question is, how can we figure this out? Okay, so let's suppose that a solver starts with x one and sets it to false. Then we realize that.
00:56:42.794 - 00:57:14.436, Speaker A: So before we can, first, as a sanity check, we can see that the slacks here. Before assigning anything, both constraints have slack three, which is larger than any coefficient. So we indeed should make a decision. There's nothing propagating. And supposing that we decide x one to false. Now c one, the slack went down from three to one, which means that x two propagates to true, and also x three propagates to true. This is an interesting property.
00:57:14.436 - 00:57:42.654, Speaker A: In the pseudo Boolean setting, a clause can only propagate once in CDCL. But in general, the pseudo boolean constraints can propagate many times. So here c one fires twice and sets both of them to true. And then next we look at c two. And now, of course, c two has x one false, which is good. But both x two and x three are true, and that is not so good. So the slack is in fact now minus one.
00:57:42.654 - 00:58:05.704, Speaker A: That's a conflict. So now we want to do conflict analysis. So we know what we want to do. We resolve the reasons, the reason c one. We resolve it with c two over the propagated variable x three. And we use the generalized resolution rule. And if we do the math, we get x four, greater or equal one.
00:58:05.704 - 00:58:47.452, Speaker A: Okay, and now this is bad news, actually. Oh, we should also do saturation. But we already see that the coefficient is one, so saturation doesn't do anything. So why is this bad news? Well, we derived one interesting fact, to be sure. We're deriving now that any satisfying assignment to these two constraints must set x four to true. But we wanted to derive a constraint that explained what mistake the solver did with its assignments so far. And what we have left on the trail when we removed x three is that x one is false and x two is true.
00:58:47.452 - 00:59:26.570, Speaker A: Nothing is speaking about x four. So x four greater or equal one is definitely not explaining what went wrong. It's not a conflicting constraint. We broke our invariant, so this conflict analysis did not work, so we failed. So if you're bored at this point in the lecture, because this second part of the tutorial is probably going to be the longest part, and it seems that we're failing, so maybe you can stop watching this and watch something else instead. Pseudo boolean conflict analysis doesn't work too bad. Or if you want to keep on watching with me, then let's try to see what went wrong.
00:59:26.570 - 01:00:17.140, Speaker A: Let's write an accident report and try to do something about this. What went wrong? First, the observation is that actually generalized resolution is just taking positive linear combinations of constraints, and this happens to be sound over the reels. General resolution as such cannot see any truths that are not true. Also, over the rules, generalized resolution does not do boolean reasoning. It's the saturation rule that does Boolean reasoning. So in this particular case, we have the trail x one false, x two true. And if you look at it over the reels, then c one is getting a bit nervous, but it's saying okay, so x one is contributing two, sorry, x one is contributing zero, x two two is contributing one.
01:00:17.140 - 01:01:26.314, Speaker A: So we have a contribution of two. We need a contribution of four in total, so we need two more. But generalized resolution says, well, you know, x four could actually contribute one, and if I get a contribution of one there, then x three only needs to be a half. And this is actually why generalized resolution reported that, you know, if you can set x four to one, then I think we're good, and c two, if you look at it in the same way under the this was, we resolved c one and c two, and c two says that, well, I'm getting a two from x one and I'm not getting anything from x two, but I only need one unit more. So if I could just get a half from x three, then I get half from not x three, and then I'm good. So both of these constraints in a real setting, in a realistic setting, haha, they're pretty happy actually. And then we do the generalist resolution rule, and x three vanishes so that there's no trace of it.
01:01:26.314 - 01:02:09.574, Speaker A: And how could we get a conflict? Because generalized resolution doesn't see a conflict. Like all of these constraints seem to be happy. So that's, I don't know if I'm saying this in a good way, but this is kind of sort of my intuition for why this fails. So what do we need to do well, we need to make the reasoning Boolean. And what is it that went wrong? Well, one thing that went wrong is that c one only propagated x three to one two. And then the solver took over and realized that, well, a half is not a valid value, so it set it to one instead. But then when we did the generalized resolution rule, this half came back and bit us.
01:02:09.574 - 01:02:54.724, Speaker A: So it would be really nice if we could get a reason that actually propagates x three all the way to one, even over the reels. If you think about what this means in slack terminology, we would like actually the slack to be zero, because that says that we can't just have a partial contribute from this variable. We need it to contribute everything in its mind. Okay, but the constraint doesn't have slack zero. So what do we do? Well, a fix that turns out to work is to apply weakening. And weakening is you take a constraint and then you remove a literal LJ from it. Here it is.
01:02:54.724 - 01:03:41.638, Speaker A: And you say, well, how much could LJ have contributed? Well, AJ obviously. So I subtract AJ from the degree of falsity on the right hand side. And if this constraint is true, then this constraint also has to be true, but it's a weaker constraint because I've thrown away the information about LJ. But it also somehow, the weakening step. What it does is that it tells us that LJ will contribute the full boolean amount, and that somehow helps us to make the reasoning boolean. And this is the approach in Chai Kuhlmann that lies behind how pseudo boolean conflict driven solvers work. So what we want to do, let's try this out.
01:03:41.638 - 01:04:08.588, Speaker A: So we want to weaken the reason on some non falsified literal, but not the one that is propagated, because we want to resolve over that. So if we weaken it away, then we lose the possibility to resolve over it. And then we'll apply saturation on the weakened constraint, because we always want to do that. And then we try to resolve and hope for the best. So let's try. So here we go again. Here's the constraint c one.
01:04:08.588 - 01:04:41.774, Speaker A: We weaken it on x two, which is a non falsified literal, and then we get two x one plus two x three plus x four greater equal to. We saturate, nothing happens. We resolve again, and we get a constraint two x two bar plus x four greater equal one. Bummer. This is still not violated. So we're not maintaining the invariant of having a conflicting constraint. So let's try to weaken even more.
01:04:41.774 - 01:05:02.694, Speaker A: What else can we weaken. Well, x one is falsified, so we can't weaken that x three is propagating. That's what we want to resolve over. We could weaken away x four. So let's weaken both of them. Then we get two x one plus two, x three greater equal four minus one minus two. That's three.
01:05:02.694 - 01:05:32.526, Speaker A: Four minus three is one. And now note that now we can saturate this constraint. There's no way we need the coefficients two here we can bring them down to one. And now let's resolve over x three. And now we get two x two bar greater equal one. And this in fact explains what went wrong, because the trail assigned x two to true. But this constraint says that this was a mistake.
01:05:32.526 - 01:06:27.914, Speaker A: X two should have been assigned false. And now we haven't quite defined what asserting or Uip constraints are, but it should be clear that a unit constraint that just contains a single variable that is asserting. So we can back jump to top level. We can assert that x two has to be false, and once we know that x two is false, then c one will propagate x one and x three to true, and then we'll get a conflict again. And so we again have this situation where we get a contradiction at top level without any decisions. So now we can run conflict analysis again and derive a contradiction in the form of the constraint zero greater equal one. Or in practice, in a solar we would just terminate immediately, because we know that when we reach a conflict without having made any decisions, then the formula is unsatisfiable.
01:06:27.914 - 01:07:59.258, Speaker A: So in general, what we need to do is we need to do this, reduce the reason before resolving. So while we look at the resolvent, and if the slack is not negative, then we find some literal in the reason, except the one that we're resolving over some literal that is not falsified, we weaken the reason on this literal l prime, and then we saturate, and then we try again until resolving the reduced reason with the conflict constraint produces something that has negative slack. So if we exit out of this loop, then clearly it works, because we are only exiting when the slack of the resolvement is negative. But how do we know that we exit? How do we know that we don't crash? Well, very briefly, the reason that this works is that slack is sub additive. So if I have two constraints, C and D, and I multiply c by lowercase C and D by lowercase D, and then take the linear combination, then their slack under Rho is less than equal to c times the slack of C plus d times the slack of D. This is not an equality. So this is not additive, but it's sub additive, okay? And we know that on the conflict side, we have a constraint with negative slack.
01:07:59.258 - 01:09:08.170, Speaker A: And now whenever we weaken, if you think about it, the slack of the reason does not change, because weakening is assuming that literals give the right contribution to the slack, and we're never, except that would not be true if we weakened falsified literals. But we're not weakening falsified literals, so weakening leaves the slack unchanged. So suppose that we have weakened away everything and we're about to crash now because the while loop, this test, failed so far. But then what happens is that saturation will decrease the slack. And when we're in the very last loop, then there's only, the only thing that is left that we have not weakened away is the propagating literal l. Everything else is either falsified or has been weakened away. And now what is the propagating? What is the coefficient of the propagating literal? Well, it has to be at least the degree of falsity, because that's the only thing left contributing.
01:09:08.170 - 01:09:52.498, Speaker A: And if the coefficient of the propagating constraint was less than the degree of falsity, then we should already have a conflict. But weakening can never lead us into a conflict. Okay, so the coefficient of l is at least the degree of falsity, and then saturation brings it down so that they're actually equal. And that means that the slack, the very last time we're at the top of this while loop, the slack will be zero. So I'm going a bit fast maybe, but again, you can pause the video recording. So then we're home. That means that at the very last point in this while loop, we have slack zero on one side, slack negative on the other side, and then here's the resolvent.
01:09:52.498 - 01:11:05.874, Speaker A: And by the sub additivity of slack, we have something multiplied with zero, plus something else multiplied by something negative. So the resolvent will have negative slack. And this is why it works. So the conflict analysis now in a pseudo boolean solver will just be that while the conflicting constraint is not asserting, find the last literal assigned last on the trail in this conflicting constraint that occurs with the wrong sign in the conflict side constraint, find the reason for it, reduce the reason, resolve, and saturate, and keep going. Doing this, then we maintain the invariant that the c confl is what we derive on the conflict side, and it's always falsified by what remains on the trail. So we just do this until we like the conflicting constraint when it is asserting, then we learn it and back jump. This is essentially what lies at the heart of the conflict analysis used in sat four J.
01:11:05.874 - 01:12:02.342, Speaker A: And the only thing that changed from CDCL is this reason reduction line. So compared to CDCL, one challenge is that propagating is more complex. It's much harder for propagating on a clause. There's this famous two watch literal scheme which says that as long as you for a clause, no matter if it contains two clauses or 202 literals or 200 literals, as long as I know that a clause contains two literals that are unassigned, then it cannot propagate. And I don't care how large the clause is, if I know two literals in it that are unassigned, then it doesn't propagate. This is not true, obviously, for a pseudo boolean constraint, consider some of n variables being at least n minus one. Then as soon as I set one of these variables to false, all of the others will propagate to true.
01:12:02.342 - 01:12:57.396, Speaker A: So I'll have to watch everything to discover this. Another problem is if you think about how the pseudo Boolean conflict analysis works, this generalized resolution rule does a lot of least common multiple computations, which means that the coefficients grow and coefficient sizes can explode and make the integer arithmetic very, very expensive. And this is a concern in practice. An even more serious concern is if I'm evil and I take a CNF formula, and we know that we can encode disjunctive clauses as pseudo Boolean constraints, so I can take my CNF formula and just write it as a collection of pseudo Boolean constraints with coefficients one, then it's fairly easy to verify that actually this algorithm that I told you about will just be standard conflict driven clause learning. So it won't do it. It will only learn clauses. So you never leverage the power of the pseudo Boolean reasoning.
01:12:57.396 - 01:13:52.690, Speaker A: But what you do is you have super expensive data structures and you only get CDCL. So for CNF inputs, the pseudo Boolean solving algorithm will be terrible. So what can we do about this? Well, one observation is that the cutting planes proof system that I said that we're going to use in the literature, there's nothing such as a saturation rule. There's the division rule, which is sometimes referred to as huat al Gomore cut, although this cut terminology is a bit tricky because the term is used very differently in a mixed integer linear programming community, and I think here in the sat and proof complexity community. So it's safer to just refer to it as the division rule. Now, assuming normalized constraint again, the cutting planes proof system can be defined as having the following rules. All literals are non negative.
01:13:52.690 - 01:14:39.274, Speaker A: We can take a linear combinations of constraints with positive coefficients. Here we're assuming that things get appropriately normalized and then we can divide. So if I have a constraint sum AI li greater equal a, then I can divide everything by c and round up everywhere. And this constraint will be valid not over the reals but over the integers. Interestingly, cutting planes with division is an implicationally complete proof system. That means that if I have a collection of pseudo Boolean constraints that imply some other constraint, then this other constraint can be derived by cutting planes. So anything implied can can be derived.
01:14:39.274 - 01:15:11.746, Speaker A: However, for cutting planes with saturation, this is not true. So in that sense, cutting planes with saturation maybe seems weaker. And it's a natural question to ask whether maybe division could give us a better conflict analysis. So this is what we're going to talk about next. And this idea has in fact been proposed in general integer linear programming earlier in the solver cut set. So let's look at the same problem again. C one c two.
01:15:11.746 - 01:16:06.086, Speaker A: We have the same trail, but now we're going to use division to reduce the reason instead. So here's what we want to do. We we look at the coefficient of the propagating literal. So here the coefficient is two. We weaken away all non falsified literals with the coefficient not divisible by this literal coefficient, and then we resolve. Or we weaken on some such we weaken on some non falsified literal with coefficient not divisible by the coefficient of the propagating literal. So I'll explain more about this in a minute, but let's see what happens.
01:16:06.086 - 01:16:47.410, Speaker A: So here again, we had the reason and we're going to divide by two, which is the coefficient of the propagating literal two does not divide one. So we can try to weaken away x four. I mean, we already know that resolving c one and c two doesn't work, so we have to do some weakening. So let's weaken away x four. Then we get two x one plus two x two plus two x three greater or equal to three. Now we apply division by two. Then we get x one plus x two plus x three on the left we get 1212 on the right, which gets rounded up to two.
01:16:47.410 - 01:17:37.296, Speaker A: And now when we resolve this, everything cancels and we get zero greater equal one. So deep inside this is also another difference compared to CDCl, deep inside the conflict analysis suddenly, boom, we just derive contradiction and can terminate immediately. So it should be clear from this small toy example that obviously conflict analysis with division is better. That's not necessarily true. You can argue that maybe I chose this toy example specifically, and of course I did. But it does illustrate that division and saturation works differently, at least. So the division algorithm for reduction is we take the coefficient of the propagating literal and then we try to resolve again.
01:17:37.296 - 01:18:22.136, Speaker A: And if the slack is non negative, then we find some literal that is not falsified and is not divisible by the coefficient of the propagating literal. We weaken on that literal, and then we try to resolve again until we get something with negative slack, and then we're happy, and then we return. So, okay, great. I mean, it's, if we return, then we're happy, but why would we return? Well, again, we're looking for a reason with slack zero. If that happens, then we're happy. And as before, we're only weakening on non falsified literals. So weakening doesn't change the slack.
01:18:22.136 - 01:19:05.052, Speaker A: So from the beginning we know that the slack is greater, equal zero. Otherwise we will already have a conflict, but it's also strictly smaller than c, otherwise l would not be propagating. So we know that the slack is between zero and strictly smaller than C, and the weakening steps don't change this. And now let's do the same analysis. Let's suppose that we weakened away everything, all we have. We can weigh all literals with coefficients not divisible by C, and we're doing the final desperate resolution attempt. And if this fails, then we'll just crash, because there's nothing more to weaken.
01:19:05.052 - 01:20:09.922, Speaker A: Why will this work? Well, the observation now is that when we have done our weakenings, then anything that can cause rounding up in the left hand side of the reason constraint is eliminated. Actually, the only thing that remains are all divisible by c, so there's no rounding. So actually the slack of C reason divided by C is in fact the slack of this constraint divided by C. There's no rounding. Okay, so that means that we can just divide by C everywhere, and we get that zero is less equal than the slack is strictly smaller than one. Or in other words, the slack is in fact zero. So if not earlier, when we have weakened away all literals with coefficients not divisible by C, at that point, we simply have to get a conflicting resolve with negative slack.
01:20:09.922 - 01:21:23.364, Speaker A: Again, I'm going a bit fast, but you can try to do this offline later or hit the pause button in the video right now. And as you saw in the proof, we could in fact weaken away everything in one go if we wanted. And in fact, the way rounding site was presented in 2018 was by such a more aggressive reduction algorithm that just takes all the literals that are not falsified. And there are with coefficients not divisible by the coefficient of the propagating literal and just weaken away all of them in one go. And by the proof we just did, we know that this is a good constraint to resolve by. And in fact, the rounding sat conflict analysis does this round to one, both on the reason and on the conflict side, and also on the learned constraint. So, so the conflict analysis will be something like this.
01:21:23.364 - 01:22:39.714, Speaker A: So it seems. Now how does division compare to saturation? Well, it seems that four formulas where this sophisticated pseudo boolean reasoning doesn't help. It seems that division based conflict analysis is faster then the speed of conflicts is not. It's not as fast as in a conflict driven closed learning solver, but it's much appears to be much faster than for saturation based solving. When the pseudo boolean reasoning really kicks in, then conflict analysis slows down significantly, but it seems to pay off. And since you're doing division, we don't have this problem of coefficients growing and growing and growing because division will somehow tend to keep them small. And so in fact the default mode of rounding status even to do just fixed precision arithmetic because most of the time coefficients will stay small.
01:22:39.714 - 01:23:24.306, Speaker A: So this is nice. But division is not the end of the story. There are some benchmarks, for instance, for multiplier circuit verification. Some problems there where sat four j is much, much much better. Some problems that are not solved by the division versus saturation choice is, well, propagation is still hard to do efficiently. And also this division algorithm that I showed you also degenerates to resolution for CNF inputs. And something that we'll talk about later in this tutorial is that even when you have a pseudo Boolean formula that is infeasible, considered as a linear program, so you don't even need Boolean reasoning to figure out that there are no solutions, there are no real valued solution.
01:23:24.306 - 01:24:07.866, Speaker A: Even, even then, there are some benchmarks that are really, really hard for both sat four j and rounding set. So this ends the part where I discussed about concrete approaches for pseudo Boolean conflict analysis. And now we at least know the basics of how to build a pseudo Boolean solver. We have figured out how to do decisions and propagations. We have now two ways of doing conflict analysis either with division or with saturation. So now we have like a functioning pseudo Boolean solver to build on. Now let's talk about some other possible rules.
01:24:07.866 - 01:25:10.528, Speaker A: Let's talk about some open research programs. One interesting rule is to round constraints to cardinalities. For instance, suppose we have a constraint like three x one plus two x two plus x three plus x four. We can actually see here that one thing that follows from this if this constraint is true, then clearly at least two of these variables have to be true, right? So this constraint implies the cardinality constraint sum of x one up to x four is greater or equal to, and this is called reducing or rounding to a cardinality constraint. And the first solver, pseudo Boolean solver that used this conflict analysis with saturation, in fact only learns cardinality constraint. It rounds everything to cardinalities. They're easier to deal with, and they're arguing in this galena paper that most of the extra power from pseudo Boolean reasoning comes from cardinality constraints.
01:25:10.528 - 01:26:17.126, Speaker A: Anyway, not sure I necessarily agree with that, but that's their conclusion after having evaluated it so formally, the rule is that if I have this general pseudo Boolean constraint in normalized form, so again, AI, they're non negative, a is non negative. Then I can derive that at least t of these literals have to be true, where t is the minimal set I such that if I sum over the coefficients in I, I reach a. So basically I just sort the coefficients in order, and then I sum them up. And the question is, how many coefficients do I need to sum before I hit the degree of falsity here? If I sort them in order, then just one coefficient three is not enough because three is less than four. But three plus two is enough because that's five, that's greater or equal four. So that's how I can deduce that at least two of these variables have to be true. But it's not true that at least three of them have to be true, because if x one and x two are true, that's enough.
01:26:17.126 - 01:26:56.694, Speaker A: So I couldn't have a three here, but I can't have a two. So natural question is this now a new rule that we would like to add? Turns out that this is not a new rule. It's easy to simulate with weakening plus division. So in some sense, if you want to round to cardinalities, you already have it. Although the conflict analysis that I showed you will not do this, of course, but you could do this, for instance, as a post processing step, say, after conflict analysis. Another interesting rule is strengthening, which I will introduce by an example. Suppose I set x to zero and I have these three constraints, and let's propagate.
01:26:56.694 - 01:27:27.644, Speaker A: Clearly x propagates y to true. It propagates z to true. And then let's look at this constraint. This constraint is actually oversatisfied. I mean, I need to get one on the left hand side, but I have two. So from this I can actually conclude that X plus Y plus z is greater or equal to. Why can I conclude this? Well, either if x is true, then I already know that y plus z is greater or equal one.
01:27:27.644 - 01:27:53.426, Speaker A: So then definitely x plus y plus z is greater or equal to. But what if x is false? Well, if x is false, I checked that I got unit propagation and oversatisfaction. So then I also got two. So this is actually sound. And note that this is interesting. These are actually clausal constraints, if you think about it. And from these causal constraints I can derive a general cardinality constraint, and this is strengthening.
01:27:53.426 - 01:28:42.956, Speaker A: And rule apparently appears in operations research and more generally. Suppose if I set some literal l to zero and I have some previous constraint that I know is true a, but it gets oversatisfied by an amount k. Then from this fact I can deduce that another valid constraint is k times l plus some AI li greater equal a plus k. So what's nice with this in theory is that you can recover from bad encodings. For instance, if you had a strengthening rule in your solution, then you could get a CNF, and then you can run this strengthening rule and recover a better pseudo boolean encoding that would allow your solver to perform better in theory. In practice, I don't know of any solvers using the strengthening rule. It seems hard to get this to work.
01:28:42.956 - 01:29:57.374, Speaker A: Unfortunately, another rule that we discovered in our group, where my PhD student Stefan Goth discovered that it would be really helpful, is the following. And this so this is actually something that happens in our solver, that you derive a constraint something like two x plus three y plus two z plus w, say greater or equal three, and then you derive exactly the same constraint, except that x is false. Here, everything else is the same. By just looking at this, you realize that, well, if both of these constraints are true, x is only contributing to one of them, right? So in fact it has to be true that you can just remove x and say that, well, three y plus two z plus w is greater equal to three. Okay, the question is, can we, can we somehow derive this? Well, we can resolve over x, but that gives us this constraint six y plus four, z plus two w greater or equal four, and then you can try to saturate or you can divide, but you don't get this constraint. So I don't know of any way of deriving this with cunning planes. So here's like a more general statement of this.
01:29:57.374 - 01:31:05.400, Speaker A: Let's call it fusion resolution rule, and we are not aware of any obvious way of forecanning planes to derive this. We also don't know if it's impossible. It's just an observation that this actually shows up in some tricky benchmarks on which pseudo boolean solvers perform fairly poorly. If you had this rule, and if you could somehow have the solver efficiently detected, which is another challenge because detection would have to be really fast, but then this would be like a really helpful rule. So what are some other challenges for pseudo boolean solving? One challenge is, well, we know from a Sat solving context that preprocessing is really important, and in integer linear programming it's called pre solving, and it's also very important. But sudo Boolean solvers don't do it, and it seems that maybe on some instances it's really hurting. So why aren't we doing preprocessing for pseudoboolean solvers? So someone should do this.
01:31:05.400 - 01:31:57.410, Speaker A: One idea is of course to take CDCL ideas and try to use them. There's some preliminary work in a by now fairly old paper, and seems like it didn't work well enough to follow up on. Another idea would be to take an interlinear programming pre solver, Papillo, for instance, from the open source MIP solver scip, and try to somehow integrate it with a pseudo boolean solver. Another obvious problem that we already discussed. Well, what if your input is CNF? How could we get a pseudo boolean solver to perform better? One particular way is to recover cardinality constraints. The strengthening rule would do this, as we already saw, you could run it as a preprocessing step. This was done in a 2014 paper by Beera et al.
01:31:57.410 - 01:32:55.654, Speaker A: Although this seems more like it's expensive enough that it wouldn't work in practice, and it's also not strong enough. We had a paper suggesting to do this more like on the fly as an in processing step during search that works better, in fact yields some fairly interesting results, but still not enough to be competitive in practice. It is not available in the solver because it's not good enough. Another sort of anecdotal observation is that these days CDCL solvers are fairly stable, like if you take a formula and you permute it, you permute the variables, flip the polarities, maybe. It seems like not too much happens with the CDCL solver. Like the performance is roughly the same. I mean, obviously the formula is semantically the same, and somehow the CDC also is stable enough to not get confused by these permutations.
01:32:55.654 - 01:34:10.568, Speaker A: It seems that pseudo boolean solvers are much, much, much more sensitive to this, and I don't know why, and I don't know what to do about it in conflict analysis. Well, should we do division, or should we do saturation? Or maybe choose adaptively? Or maybe should we choose some other Boolean rule from interlinear programming where there are a lot of cut rules? There's something that I don't have to go into, but a very intriguing observation is that the appearance of irrelevant literals in pseudo Boolean solving. So I can just go back very quickly to this example we had when we did conflict analysis here. For instance, you realize that x four is totally irrelevant. Here, it will never influence whether c one is true or not. Okay, so whether x four is assigned to be true or false is completely immaterial. So and so you could actually remove this from the constraint without affecting anything.
01:34:10.568 - 01:35:15.824, Speaker A: That's a semantically sound thing to do. So you can say, but why worry about this? Well, it turns out that since the conflict analysis is so syntactic, these irrelevant literals can actually mess things up a lot in the conflict analysis. And there was a paper by the team in Lance showing that, in fact, irrelevant literals do show up in pseudo boolean conflict analysis. And the question is, how bad is that, and can something be done about it? General observation is that there are many, many more degrees of freedom in conflict analysis than is the case for Sat solving. For instance, it can be when you're doing your conflict analysis, that the slack is so negative that you could actually skip a resolution step without resolving away the variable. Clearly you couldn't do that in a clausal setting, because if you don't resolve away, if you leave a literal unassigned and a clause, then the clause is no longer falsified. But as we saw in a pseudo boolean constraint, it could contain quite a lot of unassigned literals and still be conflicting just because the slack is negative enough.
01:35:15.824 - 01:36:35.744, Speaker A: One question, is this somewhat mysterious weakening step during conflict analysis? How aggressively should we weaken? Should we weaken the minimal amount, the maximal amount, somewhere in between? Is it better to learn just cardinality constraints, as in Galena, or should we learn general sudobuden constraints. I am not aware of any modern, thorough evaluation of this. When you learn an asserting pseudo boolean constraint, it turns out that it can actually be asserting at several different levels. How can we detect this efficiently? Which level should you back jump to? Should you back jump as far as possible or as short a distance as possible? When you're doing the integer arithmetic, should you go for large coefficients, large position, or should you try to round down and keep things in small position? Lots and lots of these questions that just don't arise in CDCL. They do arise in the Sudo Boolean conflict driven solving. I don't think they have been investigated, except that in these papers from last that I'm citing here, they have done some work on this, but other than that I'm not aware of much, so it would be interesting to get a better understanding of this. Another thing in CDCL conflict analysis is that clause minimization is important.
01:36:35.744 - 01:37:30.104, Speaker A: The question is, could you do some kind of constraint minimization in a pseudo boolean setting? It's not obvious. This is would be very interesting to come up with something that works, and this is related to all of these questions are also related to when I when I have a pseudo Boolean constraints, how do I know whether it's good or not for clause? That's kind of clear. Roughly speaking, if a clause is like smaller than another clause, and it's a stronger constraint. But a sudo Boolean constraints can have many different dimensions. There's the size of the coefficients, the number of variables in it, so it's not so clear. If I give you two pseudo Boolean constraints, how can you tell which one is better or worse? And note that this would be interesting if you're, for instance, doing some kind of adaptive conflict analysis and trying to choose between division and saturation. Maybe you try to do both and then pick the constraint that looks better.
01:37:30.104 - 01:38:33.072, Speaker A: But in order to pick the constraint that looks better, you have to be able to answer the question what it means that it looks better. And also you can look at these different conflict analysis methods, and in a natural way they give rise to subsystems of cutting planes, and we don't know how these are related. A very interesting question is whether cutting planes when division is always strictly stronger than cutting planes with saturation. We do know that there are instances for which division is stronger than saturation, where the proof system with division is exponentially stronger than the proof system with saturation. We also know that there are other derivation steps where, like saturation can derive certain concrete constraints much more efficiently than division can. But we don't know if the proof system with saturation is stronger for such instances than the proof system with division. So we know that.
01:38:33.072 - 01:39:25.864, Speaker A: Okay, so division cannot simulate individual saturation steps efficiently, but maybe the proof system with division can just get around this and derive something completely different. Yes, we don't know in terms of solver heuristics. There are very well developed heuristics for CDCL solvers. Most of them, I would say, have just been copied to a pseudo boolean setting. But the question is, since sudo Boolean constraints are more general, maybe the heuristics should be adapted, like how you do variable selection, how you bump variables during conflict analysis. This is related to giving variables scores brownie points that they're contributing to the conflict analysis. And these brownie points will then make it more likely that the variables get selected for decisions.
01:39:25.864 - 01:40:50.682, Speaker A: One could consider different bumping schemes in a pseudo boolean setting depending on whether the literal appears falsified or unassigned, whether it cancels in the resolution step or not. Another area in CDCL where there have been developments in face saving where the latest idea seems to be you have like multiple faces. We don't have this in a sudo Boolean setting. We don't know if it matters. Also, since a number of years back in CDCL, the conventional wisdom seems to be that you should have different modes for like Sat focused search in CDCL, where the solvers tries to zoom in on a satisfying assignment, and then it changes its mind and says, well, no, maybe this was an unsatisfiable formula, and then it goes into an unsat mode where it tries to prove contradiction, and it like switches back and forth between sat mode and unsat mode. Should we do this for pseudo Boolean solving? Also, these are lots and lots and lots of open problems where you could do a combination of implementation work and experiments, and then try to analyze and explain. And essentially nothing has been done except there's a Roman Vallon's thesis addresses some of these questions.
01:40:50.682 - 01:42:02.714, Speaker A: Highly recommended reading. It's an in depth investigation, but I think there's lots of follow up work to do on this. Finally, as I already mentioned several times now, efficient unit propagation for pseudo Boolean constraints is hard detecting during conflict analysis, which whether I have already reached an assertive constraint is a non trivial problem. And in contrast to CDCL, how could we do proof logging? How can we certify that the results that the solver computes are correct? And here I should say that this is something that I'm very, very interested in and that we're working on. I would be happy to talk to you about it. During the Simons program, we have a proof checker that we call the very PB that we're very optimistic about that it will be able to do efficient proof logging for sudo boolean solvers, and not only pseudoboolean solvers, but also other types of solvers. But that's not the topic of this tutorial.
01:42:02.714 - 01:42:50.914, Speaker A: So this ends this long, long second part of the tutorial on sudo boolean solving. Thank you for sticking with me till the end. Now, what's next up is to say, okay, fine, suppose we have a pseudo boolean formula and we're not only interested in checking feasibility or whether it's satisfiable. There's also a linear objective function that we want to optimize. So we want to find the the solution which has the smallest value according to this linear objective. That's what we're going to talk about next. But this is the end of part two of the tutorial on sudo Boolean solving and optimization.
