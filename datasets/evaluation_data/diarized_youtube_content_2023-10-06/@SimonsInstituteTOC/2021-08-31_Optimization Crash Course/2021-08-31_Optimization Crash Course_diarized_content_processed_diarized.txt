00:00:02.720 - 00:00:13.514, Speaker A: All right, so welcome, everyone. I'm Filipe I Gore. I'm one of the organizers. So not all of us are here. Katie's here. And Aisha, you will see a lot of this morning. She's giving the first talk.
00:00:13.514 - 00:00:54.054, Speaker A: So, I don't know how many of you have been at the assignments, but the assignments is a great place to interact. In particular, those bootcamps have been actually great resources for me. I've learned a lot just watching the videos or being here this week. Since this semester, we have a fairly interdisciplinary program on geometric methods of optimization and sampling, involving people from a variety of communities, including, well, optimization, but beyond PDE's. And PD's, of course, have a lot of sub fields. I'm outside, so I can just describe it as PD's calculus of variations. Optimal transport is going to be featured prominently, algebraic methods.
00:00:54.054 - 00:01:47.054, Speaker A: So we've put together a bootcamp where hopefully, everybody can get up to speed, understand what everybody's working on, what the terminology of different fields are. And I'm really glad to announce that all the speakers have been really playing really nicely this game. And we have worldwide experts on different domains, and they've agreed to actually give fairly introductory talks so that we can really get up to speed with each other. So, the bootcamp is. So you've registered, so you're probably familiar with it, but it's separated in five days, on Monday and five themes. Monday is optimization, Tuesday is sampling, Wednesday is optimal transport, Thursday is PD's, and Friday is algebraic methods. So, we're going to talk today about optimization.
00:01:47.054 - 00:02:14.014, Speaker A: And so we have three talks, two morning talks, our crash course on optimization by Aisha Wilson. And this afternoon, we'll have two more talks on slightly more specialized topics that are optimization on curve spaces in particular. All right, so, without further ado, I would like to welcome Aisha Wilson from MIT, and she's going to give the first talk of this bootcamp. Thank you.
00:02:34.294 - 00:02:54.864, Speaker B: Okay, hopefully. All right, perfect. I can be heard, even though we're all wearing math here. So welcome, everyone. I am going to give two part tutorial on optimization. And this is going to be the first part. And it's going to primarily cover optimization on euclidean spaces.
00:02:54.864 - 00:04:06.204, Speaker B: And the topics I've chosen are situated so that hopefully they blend well with Nicolas's and Subvert's talk on optimization on Romania manifolds, as well as on more general spaces. So, had to shade some of the topics, obviously, because optimization is a huge, huge area, but hopefully that'll set the tone for this. So, in particular, some of the core concepts that I think will help get us on the same page are going to be, that are going to be covered in this first course are oracle models and various function classes that we often deal with in optimization. How do we generally establish convergence of iterative algorithms? How do we deal with constraints if we are optimizing over some complex set and some very, very famous or infamous lower bounds? Okay, with that, I'm going to start with a bit of notation. So hopefully this is not too heavy. I'm going to assume that we are working on a finite real vector space. So think of RD whenever you see X.
00:04:06.204 - 00:04:43.524, Speaker B: And then I, I can make this a little bit more general. So I'm going to look at its dual space. This is formed by the set of linear functionals on X, but we can think of it as dual space, as again, RD. So it's possible to introduce a self adjoint operator, which I have this notation b. It's going to be a mapping from this primal finite dimensional space to its dual space, and it's going to help me define a norm. For all intents and purposes, however, I think we'll think about b as just the identity matrix. So we're on euclidean space, but a lot of what I'm saying can be defined at this level of generality.
00:04:43.524 - 00:05:26.094, Speaker B: I'll be referring to some more general norms, particularly of these p linear forms. So think Hessians and higher order derivatives of functions. And so by, if I write the norm of them, this is what I mean. So be defined as this maximum over these unit directions. And finally, I'll be talking about, or using the notation of grad f for this, this symbol f as the set of partial derivatives. That's fairly standard, but we'll see that in more general spaces, if we're talking about the gradient, this is not what we mean. Okay, so let's get started.
00:05:26.094 - 00:06:06.138, Speaker B: One, if you take an optimization course, we often formulate the problem, or an optimization problem as follows. We would like to minimize some function over some set x or x prime. So here I'm noting x prime as our feasible or constraint set, and I'm assuming it's compact and convex. And so by convexity, I just simply mean that the chord between any two points in the set lies in the set. So hopefully people are familiar with convexity. It's a really, really nice notion, and that is what will be what we'll assume. Also, we'll assume that our constraint set is pretty nice and simple.
00:06:06.138 - 00:06:52.366, Speaker B: So I'll give you some examples, but we'll not be dealing as much with more complicated constrained optimization problems. Okay, so with that, one last thing to note is typically these feasible sets, or these constraint sets are some sort of combination of a simple basic set s, which is going to be convex and compact, and some functional constraints. That's the standard formulation of an optimization problem over a convex domain. Okay, so the motivating example will be, well, I'll have two. It's going to be, first, risk minimization problems in statistics. So these are pretty much everywhere. And the formulation is this.
00:06:52.366 - 00:07:40.764, Speaker B: We assume that we receive samples from some general sample space. This is a nice construct so that we presuppose drawn from a distribution, and our goal is to optimize some function over a decision vector. We can think of this vector typically as like a set of parameters, and so we can pose our risk minimization problem as follows. We would like to minimize over x the expectation with respect to my samples drawn of this more general or complicated function. This is normally decomposed into a loss and some sort of regularizer over my set. Of course, this is nice. I think the more practical problem, or the more prevailing problem nowadays, is empirical risk minimization problems, which are more or less a proxy for these risk minimization problems.
00:07:40.764 - 00:08:31.966, Speaker B: In this formulation, we have a finite set of data that we assume again comes from some distribution. Our goal is to generate a prediction function which will be parameterized by x. So this is the decision variable. And our goal will be then to minimize some sort of loss function over that prediction function and a label y coming from our data, plus some sort of regularizer to restrict how complicated our parameter can be. So we can think about the loss as our data fitting term in the regularizer as constraining it somehow. So those are the two motivating examples that we'll be drawing from. And hopefully, if you haven't seen it, the, the classic examples for some examples in empirical risk minimization are, first of all, regression.
00:08:31.966 - 00:09:21.014, Speaker B: So here we have some sort of prediction function y hat y hat, and our goal is to match some sort of quadratic loss between our prediction function and y. So these prediction functions can be very general. We can think of linear functions as well as just a neural network, so some sort of composition of linear maps and some nonlinearities in classification. We restrict y to take on values negative one and one, and our goal is to predict the sign, or to predict y by the sign of our prediction function. So typically we think of this true loss or the true object we're after as this zero one loss. So it's got this cutoff here. If we plot it and we typically, it's hard to optimize and we'll discuss why.
00:09:21.014 - 00:10:01.956, Speaker B: So we typically minimize proxies in its place. So some of the popular proxies are the hinge loss. So this loss function is a maximum over one minus this inner product between y or the product between y and the prediction function and zero. And this is typically used for support in the support vector machine formulation. There's also the logistic loss. This is a very popular loss that is used in the context of machine learning, deep neural nets and of course the infamous least squares regression loss function. So this is the template to have in the back of your mind when we talk about minimizing functions.
00:10:01.956 - 00:11:13.706, Speaker B: This is what we want to work with. Okay, so let's get into how do we actually construct iterative algorithms that will find solutions to our decision problems in risk minimization and in empirical risk minimization? Our goal again is to define or design an algorithm to find the minimizing value over our optimization problem. And we typically think of or formulate this in a complexity theoretic framework whereby we have some sort of iterative method and an oracle, and there's a game being played here. The game goes as the algorithm will send a query point to the oracle and the oracle will then update its information based on the query point. The oracle then sends back this information vector to the algorithm, which the algorithm can then use to update its state and ask for another query point. This is the standard black box formulation of optimization. And so here are some examples of oracle functional oracle models that are typically assumed.
00:11:13.706 - 00:12:16.904, Speaker B: So in a 0th order oracle formulation, the information being given to the algorithm is the value at x and the function value evaluated at x. In the first order oracle, we get in addition to that, the gradient second order, the Hessian. So we kind of have this nice cascading set of information oracles. And now here are some examples of algorithms which use this information. So here I'm going to give you an example of some first or some algorithms that are designed using first order oracles and some that are giving or providing updates given second order oracles. So each algorithm here takes on the form of updating its state based on its current state and some vector gradient descent. Perhaps the most widely known algorithm in optimization simply updates, chooses this vector field or Vk as according to this minimization problem involving the gradient.
00:12:16.904 - 00:13:25.604, Speaker B: And of course, we can think of this as if we take X to be all of RD as just choosing VK to be the negative gradient direction scaled by ETA. And the two other examples, we have the second order oracle, which is solving now this quadratic problem. And again, we can write this explicitly when evaluated over all of our d as eta times, the hessian of f at x k inverse times. And this cubic regularization of Newton's method was an algorithm proposed by Polyak and Nesterov in, I believe, 2006. And this optimizes or chooses its vector field, some optimization problem involving this quadratic expansion of our function and a cubic term. We'll revisit this algorithm a little bit later. Okay, so, having gone through the examples, I'm first going to discuss how we typically, or some organizing principles along which we analyze convergence.
00:13:25.604 - 00:14:23.102, Speaker B: So, upper bounds for functions or for an algorithm will depend on several things. It will depend on the geometry of the set. The function class we are optimizing, or function that we want to optimize, belongs to, and the evaluation metric that we're going to be using. So I'm going to hop into this first part of function classes first and discuss some common function classes that are within the machine learning context. The first is this big distinction between convexity, or convex functions and non convex functions. And so here we have as our definition of a convex function, that the chord between the two points on the function lies above the curve. Another way of formulating this is that the epigraph which I've defined here as the set xt such that f of x is, is lower bounded phi t.
00:14:23.102 - 00:15:08.204, Speaker B: This is going to be a convex set. There's another way. So, typically, when we're introducing notions of convexity, we leap to a first order notion involving its gradients. But we can also make reference to convexity without introducing a gradient by talking about the sub gradients. So we can say that a function is convex if the subgradient set of a function at a point x, which is defined here as essentially the supporting hyperplanes at x, is non empty at all points. So, classic examples. If you have some sort of non smoothness in your curve, there are a lot of supporting hyperplanes.
00:15:08.204 - 00:16:03.838, Speaker B: And I think the example that is given most often is the l one norm. So I'll write it down here, actually. Am I blocking this board? Can everyone see okay? Good. Perfect. So if f is just, let's say the absolute value function in 1D, then the sub gradient of f at x is given by negative one if x is less than zero, negative one and one if x is equal to zero, and one otherwise. So that's that's a very classical example. The sub gradients have a nice calculus to them that they're always, it's a compact set and there's a lot of things that you could say, but it's, it's, a lot of the things we'll be doing with gradients can be done with subgradients.
00:16:03.838 - 00:16:32.686, Speaker B: And so I wanted to introduce this notion here. But moving on, we have a first order condition that does involve gradients. We say that a function is convex if it's Bergman divergence, which I've given here by this notation df of xy. So this Bregman divergence of x at y. This is non negative. So canonically, the drawing is this. So we have this supporting hyperplane at y and the value at x.
00:16:32.686 - 00:17:03.818, Speaker B: And this, this gap here is what's the Bregman divergence of f x. And finally, if our function is twice differentiable, we can define convexity by the notion of its Hessian being positive, semi definite. And of course, if it's twice differentiable, all these definitions are equivalent. Okay, good. To have convexity in the back of our mind. It's a little pedantic, but I want us all on the same page. There's convexity.
00:17:03.818 - 00:18:02.584, Speaker B: Other lower bounds involve certain other notions of curves that lower boundary functions. Perhaps the most popular is strong convexity. This is the condition that our Bregman divergence is lower bounded not just by zero, but by quadratic with parameter mean. So we can think about the curve being lower bounded by a quadratic. There are also kind of this family of notions that get often called upon. So weak strong convexity is another notion that is in use, and it's a condition along which we can actually get linear rates of convergence for a lot of iterative methods. So an example of a weakly strong convex function, or weak strong convex function, is a function of the form g composed with a, where we have g as a strongly convex function and a is some sort of matrix.
00:18:02.584 - 00:18:33.358, Speaker B: So composition of a strongly convex function in a linear map. And of course, if you're weakly strong convex, you. Sorry, if you're strong convex, strongly convex, then you're weak strong convex. There's also the restricted secant property. This is another property that we can use to get linear convergence. And an example will be the same kind of formulation plus some linear term. There's this very infinite pl inequality.
00:18:33.358 - 00:19:11.484, Speaker B: I'm not going to attempt to say the name here on video, but it's just simply that the norm of the gradient is lower bounded by the optimality. Gap scaled by some constant mu. And so what's nice about the PL inequality is you need not be convex. In fact, here's an example of a function that satisfies the PLN quality that is non convex. And it looks like this if you zoom out a bit. So there's some sort of curvature there. But Pl fun functions that satisfy the PL inequality are nice in some sense, because all the minima are global minima.
00:19:11.484 - 00:19:59.930, Speaker B: And then last but not least, there's this quadratic growth condition. That'll come back later when we perhaps get into the sampling portion of our tutorial. This has nice connections to telegrams, inequality and more complicated spaces, but I think it's worth mentioning now. So there's this kind of cascading hierarchy here of functions. And I think the last thing that I'd like to mention is that while everything is, uh, with this coefficient or two scaling, we can actually define these more generally. So we can, we can kind of lower bound our function by this is what's called uniform convexity by this polynomial. So there are functions that satisfy that, for example, f of x is equal to x to the p, or absolute value of x to the p.
00:19:59.930 - 00:20:30.074, Speaker B: And then there's this also hierarchy involving more general notions. I think that's useful to keep in the back of your mind. Okay. And then of course, there are these classical upper bounds that we typically assume for the function and its derivatives. So we typically assume some sort of Lipschitz continuity for some of the gradients and the functions. And so they're classic examples. Most functions you'll ever deal with will have smooth gradients and smooth Hessians.
00:20:30.074 - 00:21:23.458, Speaker B: If you're over a compact space, then you'll also be lip sheds. And what's really nice about the smooth or the Lipschitz gradient condition, which we'll call smoothness, is that it is a quadratic upper bound for our functions. So we can also write this as the fact that the Bregman divergence is upper bounded by this quadratic scaled by l. Now I want to discuss convergence, having establish those notions. The other thing we needed was how do we measure optimality? And I'm going to discuss three ways that we tend to do that. So, suppose we have introduced an algorithm that just updates our state. And we would like to ensure that there exists a point that is epsilon optimal.
00:21:23.458 - 00:22:53.456, Speaker B: And this will mean roughly three things that is used in the literature. There are, of course, other notions of optimality we can introduce, but we care about criterion that certifies our function or optimality gap is less than or equal to epsilon, or that some sort of pseudo metrics that we can introduce either in these primal space or the dual space is also upper bounded by epsilon. So either we want the distance between the iterate and the optimal point to be small, or the distance between the gradient and zero, uh, to be small. Um, and so just if I've written here in some generality, because it can be be defined as such, but the thing to have in the back of your, not your mind, is just the l two norm, um, of x k minus x star, and the dual norm of the gradient. So these are typically the certificates of optimality we are interested in, and what the technique I'm going to describe for how we certify this optimality is that we just need some way of ensuring that these quantities are strictly dissipating along the trajectory of our algorithm. So this is where I think at least an organizing principle that I'll be introducing here and I think makes sense to me, is this idea of viewing an algorithm or an iterative algorithm in this context as a discrete dynamical system. So, not all algorithms can be viewed as such, but a lot of the algorithms we'll cover today can be.
00:22:53.456 - 00:23:50.208, Speaker B: So I think this will be a very useful notion. And so there is this idea in dynamical system theory that we all have some scalar valued function, and either we introduce it as something we would like to be conserved. This is typically when we're referring to some sort of physical system, like a lossless mechanical system or something like that. And there's also this notion. This is introduced by Lyapunov, where we would like this quantity to be dissipating along the trajectory of our dispute dynamical system. So the two criterion are this, so that's constant along the trajectory, or it's decreasing along the trajectory. And so I'm going to go through an example, but we can see that if you are strictly decreasing along the trajectory, and this quantity is one of our certificates of optimality, then of course we'll achieve our goal of establishing epsilon optimality.
00:23:50.208 - 00:24:25.104, Speaker B: So it's kind of just within the frame. Okay, so here's an example. This is very popular within a lot of optimization textbooks. So it's just that the optimality gap is strictly dissipating along the trajectory of gradient descent. And here I'm defining gradient descent with its step size restricted to be less than one over luck. And this is, this happens when f satisfies these two conditions. So it's convex and its gradients are smooth, or rather lipschitz.
00:24:25.104 - 00:25:07.954, Speaker B: The convexity condition is just for the strict property. We'll see a lot of these. This will go through for just Lipschitz gradients, or functions with Lipschitz gradients. And the proof is about three lines, which is why I'm going to actually draw it out here. So essentially, we write that the difference between this dissipating quantity and if we write it out explicitly, it's just the difference in our function gap. Next, we simply use this, this upper bound to upper bound that function gap. Then we use the algorithm and we plug that in and we get two terms that can be written in terms of the norm of the gradient squared.
00:25:07.954 - 00:25:55.898, Speaker B: And so this is where that step size condition comes in. We know that if our step size is small enough, then we can upper bound this entire energy function or quantity by negative term, and it will be strictly negative unless x k is equal to x star. And this is owing to the convexity property. So here we have the statement that this is going to provide a certificate of optimality for convex functions. Just this, just this three line proof, or four, I guess. But of course, there's been in the last, probably 30, 40, maybe even a little bit longer, a lot of refinements to these arguments. A lot of times people would introduce methods and then just give an asymptotic guarantee.
00:25:55.898 - 00:26:27.708, Speaker B: And that was that. That's fine. But there's been a lot of work in the optimization space of trying to give more precise upper bounds and guarantees of convergence. And so that isn't sufficient. And so typically, I'm going to discuss two broad strategies which we use to obtain global guarantees, non asymptotic global guarantees. The first is just trying to show some sort of time dependent optimality criterion is dissipating along the trajectory of our algorithms. This is underlying most proofs of iterative methods.
00:26:27.708 - 00:27:14.084, Speaker B: Not all, of course. Another technique that is often found in non convex optimization and stochastic convex. Stochastic convex and non convex optimization just is showing that one of the two identities hold. So we want the sum of that optimality criterion over the trajectory of our algorithm to be upper bounded by some constant times, some term that can be increasing. But is o of k, little o of k. So why does this give us a certificate of optimality for some point? Well, it's because we can lower bound both these terms by the length, which is actually k plus one. But we can also lower bound that by k times the minimal value that it takes.
00:27:14.084 - 00:28:04.224, Speaker B: And so if we just simply rearrange this argument, this gives us a rate of convergence of little o of one or alpha k over k for that minimum value if we keep track of it. So that's typically the formulation of proof in the stochastic and non convex setting. Okay, there are many, many times dependent dissipating quantities you can just write down. And in fact, these are non unique. And it's not often it's hard to just start off with one and just prove that it works. Nevertheless, I think it's instructive to reveal what these quantities are for various algorithms. So, in the standard setting of smooth convex optimization, there are quantities involving both the optimality gap and the distance.
00:28:04.224 - 00:28:47.064, Speaker B: And there are also these quantities that will give us this linear rate of convergence. And of course, there are also very similar quantities for Newton's method and cubic regularized Newton's method. I'm going to just go through one. It's going to be three lines, and very quick, just so you see. And this is actually a very non standard way of formulating this because I just wanted to highlight the energy function. The claim is that this quantity here is decreasing for the gradient descent algorithm with step size one over l. Or it could be less than one over l when f is l smooth and mu strongly conducts.
00:28:47.064 - 00:29:47.854, Speaker B: So the main thrust of trying to establish this works is simply to write down that difference in a really nice way. So it's going to be some sort of distance between our two iterates and some scale distance between our current or current iterate and the optimal value. So we already established that if you, you are, you have smooth gradients, that the function value here is going to be upper bounded by negative one over two l times the normal, the gradient squared. And next I'm just going to pull out the one over l. And then finally, this here is the, we can use the PL inequality that was introduced before. So this is actually, I can write this more generally, that this quantity is decreasing for l smooth functions and functions which satisfy the Pl inequality with parameter mu. So this is going to be decreasing.
00:29:47.854 - 00:30:30.074, Speaker B: This is a non standard way of presenting this proof. Typically we just write down the function value and do a lot of calculations, but at least this reveals what is actually going on. Okay, so I'm going to talk about constraints. There are tons of ways of dealing with constraints and this is a 1 hour, well, the first of a series. So I have to trim down what I'll be discussing. So I'm just going to be talking about projections and proximal point methods because I think those two will hopefully be in line with what will be going on today, I'm not going to discuss interior point methods and a kind of a lot of other ways of dealing with that are popular. So I apologize to fans of these techniques.
00:30:30.074 - 00:31:48.334, Speaker B: Maybe the next optimization tutorial will handle it. Okay, so clearly, if we have a set and we're running an iterative algorithm, there is some chance that our iterate will fall outside of our constraint set. And so we don't just want an optimal point, we want something we can certify as within our constraint set. And so a very standard way of doing this is if our set is somewhat easy, we can simply project the, the use this operator here to project the point onto our set. So this will be this euclidean projection. Of course, we can define the projection operator in different ways, but the projection satisfies really nice properties when our sets are compact and convex. So in particular, the projection satisfies this nice triangle kind of inequality, whereby we can say that if we project our point onto the set and consider another point in the set, then that original distance between them is going to be lower bounded by the point projected, the distance between the point projected on the set and the projected point, and the point in the set.
00:31:48.334 - 00:32:32.342, Speaker B: So we have this nice triangle inequality. And what this implies is that projections are non expansive. So when I project a point, things get closer together. This property alone will certify is all we need to kind of say that if we simply project the point after we run our iterative algorithm, it will still converge. So that's a nice way of dealing with it, if this is not too expensive. So if we can write down the projected gradient descent algorithm as follows. So again, I find this vector field that's just negative eta times the gradient direction, and then I simply project my future update onto the set.
00:32:32.342 - 00:33:34.374, Speaker B: And that gives me the update I would like. Okay, the other point, or the other thing I wanted to point out, is that projection is really just this very special case of a more general notion called proximal point mappings, or proximal mappings. So we can define the proximal mapping of a function as this minimization problem where we simply minimize the function over some ball anchored at some point v. So if we take our function to be the indicator of our compact set, then this proximal mapping is equivalent to the projection operator. But this proximal mapping. Of course, there are other definitions of f we can use. In particular, I think one of the most popular proximal mappings that's used is the l one.
00:33:34.374 - 00:34:20.073, Speaker B: For the l one ball. So here we can actually write down explicitly what this proximal mapping is. So we have, it's this shrinkage operator where I'm shrinking points to the origin. So I think maybe James Stein just shrinking points. Okay, so proximal operators are firmly non expansive and satisfy a lot of very similar properties that projection operators do. And it's because of this we can use, or we can certify that when the proximal operator is used, it typically has the same convergence properties as, as our original iterative method. I'll give you an example.
00:34:20.073 - 00:35:28.264, Speaker B: Suppose we can decompose our function, our risk function, into a loss plus a regularizer. Often what is done is we take a gradient step with respect to our loss, and then we project, or we use the proximal mapping on our regularizer. So think the l one ball. This is what is called ISta, the ISTA algorithm, and it's quite popular in dealing with very simple but easy to project onto constraints. And I think, well, just summarize rather than go through. Another proof is just that the same quantities that dissipate with respect to gradient descent performed on the full function also dissipate with respect to this proximal gradient descent algorithm, where I take a proc step over the regularizer, and I very rarely need any assumptions on the regularizer other than it is convex. Okay, the final topic that I wanted to discuss is lower bounds.
00:35:28.264 - 00:36:27.124, Speaker B: So, lower bounds, there's some classical ones that I think is healthy to be aware of and definitely motivates the introduction of certain other algorithms. So typically, when we're introducing a lower bound, what we're interested in is the minimum number of queries to an oracle before an algorithm produces a point that satisfies one of the following epsilon optimality conditions. So, of all these algorithms within a class, what is the minimal number of queries it will need to produce that point? And again, lower bounds will depend a lot on the geometry, the function class, which evaluation metric we'll use, and the information the oracle provides to the algorithm. So, here's an explicit example of a lower bound. This is very classical. It's found in, it was introduced by Nemorovsky and Judy in 1983. It's found in most complex optimization textbooks.
00:36:27.124 - 00:37:22.464, Speaker B: The oracle is the first order oracle. So it's going to return the function value in the gradient, the geometry, I'm kind of going to not assume any complex constraint set. It's all of RD. The function class is a set of convex functions that have smooth gradients, and the evaluation metrics are the optimality gap in this, this distance. The other additional constraint I'm going to place on the algorithm is that it has to produce a point in the span of the previous gradients. So with that construction, a very nice lower bound for this class of algorithms within this model is one over k squared for the gap between the iterate at x k and the optimal value. And actually, if you look at the distance, we can certify that it is not contracting.
00:37:22.464 - 00:38:04.494, Speaker B: It's contracting, but it's not contracting at a polynomial rate. Okay, so how do we establish lower bounds? Well, a very popular method is to exhibit a worst case function. So here the worst case function is this really is a quadratic function. It's a very structured quadratic function. So there are twos on the diagonals and negative ones on the off diagonals, this tri diagonal matrix. And if you look at what the gradient is of this quadratic function, we have this interesting form. So this l four times ax minus this e one direction.
00:38:04.494 - 00:38:53.304, Speaker B: If you actually compute the span of all the gradients, this gives you the span of all the unit directions, or the unit direction seen so far. If x zero is zero. What this is really saying is that every iteration of a first order method can only expand the search space by at most one direction. So if your number of iterates is fewer than the number of dimensions, then you have this resisting oracle here, whereby you cannot possibly find the optimal point. There are some algebra you can quickly check. You can actually write down the optimal solution, the optimal function value, and use that to actually obtain this lower bound. So this is actually algebraic.
00:38:53.304 - 00:39:35.842, Speaker B: It's a little bit harder to offer intuition as to why this holds, but it's a simple construction found, I think, of Nesterov's zero four book. But you can use a very nice argument to minimize or to find the minimum, or a nice lower bound for the point xd. So the Dth iterate and it's scaling by one over d, and therefore conclude that the lower bound is one over d squared. This very similar quadratic gives you a lower bound in the strongly convex setting. So here the difference is this function class I have. Now this quadratic lower bound. Instead of zero, I have this quadratic lower bound.
00:39:35.842 - 00:40:27.016, Speaker B: So it's strongly convex. And again, I require the algorithm to produce iterates in the span of the previous gradients. Now here the lower bound now looks like. So if I define what's called the condition number by l over mu, so this is going to be a quantity between one and infinity, the lower bound is on the order of one over square root of the condition number e to the one over negative one over the square root of the condition number. And the same lower bound pretty much holds for the iterates and the function value. So, what have we done so far? We're kind of nearing the end of this segment, but so far we've illustrated this. So we have a function that is upper bounded and lower bounded by a quadratic.
00:40:27.016 - 00:41:11.194, Speaker B: Then gradient descent through that very simple construction will provide an upper bound of one over one minus one over the condition number to the kitchen. The lower bound, however, is one over the square root of the condition number. And we can actually see that this square root condition number difference can make a big difference for poorly conditioned problems. Here we've written, or we're drawing the level sets of our function. And gradient descent simply just moves orthogonal to those level sets at every iteration. So if we have very oblong level sets, it's going to bounce back and forth. And so for poorly conditioned problems, this can make a dramatic difference.
00:41:11.194 - 00:42:07.124, Speaker B: Okay, so with that, I'm going to give us a little break. Part two is going to get more into more complex algorithms. So I'm going to talk about acceleration, the algorithm introduced to close this square root condition number gap, stochastic methods, randomized methods. This will be variance reduction, stochastic SGD. So very popular methods for empirical risk minimization problems and mirror descent. And I think mirror descent is an algorithm which will provide a really nice geometric well, has a very nice geometry appended to it, and will hopefully be a nice kind of leading into what Nicolas is going to talk about with respect to romanian manifolds. Okay, so let's grab some coffee and I'll be back soon.
