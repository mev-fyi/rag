00:00:00.360 - 00:01:02.494, Speaker A: From genomics. Thank you. The name is biomedical data science. It's too sexy to forget, so I have too many slides. So let me start with the important one, which is acknowledging my collaborators and anything that is really good and what I'm going to tell you, it's due to them and the mistakes are due to myself. And this will be sort of an advertising type talk, which I don't typically like, but what can you do in 30 minutes? So I'll invite you to look at the paper if you are more interested. My goals for today's are to try to convey that one example in which looking for a conditional, consistent hypothesis, it's a way to get closer to causal mechanism and then show you a little bit how the knockoff framework, it's a powerful way of testing conditional hypothesis and interest you in the problems of gene mapping.
00:01:02.494 - 00:02:27.776, Speaker A: So I work in a medical school, and one of the big questions that we have been grappling with in medicine and in genomics in the past few years is the interplay between race and medicine and race and genomics. And so there's papers like this one that emphasize that sometimes in medical education and in medical practice, we give too big of a role to race and we treat people differently because they are of different races, and there is really no need to do that. There are also papers like this, ones that instead point attention to the fact that we don't have enough data on diverse populations. And by having this lack of data, we don't treat, we don't provide to everybody the best possible care. So there is always this question of are we different or are we the same? And how is it that we account for the diversity that we can clearly see exist between different human population versus what are the very common elements that they share. So I'm going to look at this from one specific point of view, which is that one of genome wide association studies. The goal of this study is that we have observations on a trait y and on a set of genetic variants.
00:02:27.776 - 00:03:14.964, Speaker A: These are typically quite many. You can think about x as having 1 million different variables included. And our goal is to try to identify which genetic variants encapsulate all the information that is in the genome about the trait. We are particularly interested in coming up with a selection of positions in the genome that are relevant for this trait. Let me emphasize that we do not know what is the mechanism with which y depends on x. I've already mentioned the dimension. Another thing that I want to mention is that there is a fair amount of dependence between all of these different covariates, not all across, but locally they have strong dependence.
00:03:14.964 - 00:04:39.195, Speaker A: Since we are in this causal workshop, let me spell out clearly what do we mean with causality in this context. What people are interested in is identifying those genetic variants that directly impact the phenotype, so that if you have one particular allele at that position, you are going to have a higher risk of having a certain disease or you have a higher value or whatever quantitative trait you are talking about, irrespective of what you have everywhere in the genome. The main interest for this is because this is a way for us to understand what are the biological pathway behind these traits of interest. Typically, we cannot assume that these causal variants are really genotypes, so they might not be in our set of x's, and we can just guess their location by proxy. So if we have association between nearby variants and the phenotypes, we might be able to detect them. One thing to keep in mind is there a fair amount of variability in allele frequencies and linkage disequilibrium, which is the term we use to describe dependencies across the variants across different human populations. The standard way of analyzing this data, it's with univariates models.
00:04:39.195 - 00:05:57.914, Speaker A: So you test a bunch of hypotheses of independence between the trait and each of the single variants, and in a very simplistic form, you just use a linear model, you actually use mixed effect models. But for the sake of this talk, this is okay. And then you do all of these tests and you try to control a global error rate, like family wise error rate at the 5% level. Now, testing all of this univariate association is not a very fast way to getting at causal variants, because whenever you have a signal, whenever you have one variant, that it's important, all the neighboring variant will also be associated with the phenotypes. It's also a way that it's kind of antiquated in the sense you don't use all of the nice modern machine learning methods and you have results that are hard to interpret. How about different human populations? So, human populations have played a big role in GWAS, and the first type of interest was, oh, we see that there is variation across human populations. So we have to be careful about this because this could induce confounding, and we might find associations that have nothing to do with the trade, but have to do with the variability across populations.
00:05:57.914 - 00:07:32.504, Speaker A: This pushed people to look at data from homogeneous populations, so that if you concentrate only on northern Europeans, you are going to see variability that is associated to variation in phenotype and not to variation in population. However, that makes it difficult to export the results that you get from finnish discovery to the rest of the world. And so right now, really the main interest in the genomic field is to get data from other populations and see where is there a correspondence between the findings that we make in Finland and the findings we make in other populations. So what I want to talk to you about, it's an alternative approach to do these genome wide association studies that allows us to use machine learning methods and guarantees for us false discovery rate control of the discoveries. But by using these machine learning methods, output results that are more informative in terms of causal variant. And this goes through the main ingredients of this alternative approach are two real one is the idea that instead of testing independence between phenotypes and individual genetic loci, we try to test this conditional hypothesis. So we look, we say that we make a discovery when we find a SNP that has something to tell us about the phenotype on top of what the rest of the SNP's have to tell us.
00:07:32.504 - 00:08:36.906, Speaker A: We want to have some SNP that we want to reject the hypothesis of independence between the phenotype and the SNP given the rest of the genome. And then we want to control false discovery rate rather than family wise error rate, which is a rather conservative approach in this text. But in particular today, I want to talk to you about the idea that we have of trying to make sure that we make discoveries that are consistent across populations. Let me state precisely what I mean. Imagine that we have a number of EOF environments that in my case are different human populations, but we can do a little, be a bit more general. And so let's just talk generally about environments, and we can get observation from each of these environments, and we have outcome y and p explanatory variable x. This conditional independent hypothesis for each environment can be stated as the outcome y.
00:08:36.906 - 00:10:13.154, Speaker A: It's independent from the explanatory variable XJE given the rest of the explanatory variables in environment e, and what I mean with consistent hypothesis and hypothesis that it's null if there exists at least one of these environments where this conditional independence hypothesis is true. So if you make a consistent discovery, it means that you discover a variable XJ that it's not independent from y given all the other variables in every single environment. This I want to argue that, and I'm not, you know, certainly the first one to argue this, that this brings us closer to the identification of SNP's that truly have causal effect. If I find a SNP that it's associated to high cholesterol level in every single human population, I tend to believe that it's more likely to be causal than one that has association in some population and not in others. And of course, the work of Jonas Peters, Peter Bohlman Minhausen and everybody else has inspired quite a bit. Let me give you a graphical view of one particular way in which this can be true, that it's relevant for genetics. We have four covariates, z with a certain distribution, pze, that depends on the environment.
00:10:13.154 - 00:10:53.198, Speaker A: We're going to represent the relationship between these variables with this graphical fashion. The gray nodes are the observed variable and the white nodes, they're not observed. So you can see that the covariate, well, I don't know how well you can see it, but let me tell you the covariate c one, it's called c one because it's unobserved. So it could be a confounding factor. There is, these two dotted lines here represent the true mechanistic model that generates y. So y depends on c one and x one. Now let's see what we might observe in different environments.
00:10:53.198 - 00:11:46.412, Speaker A: In one environment it may be that variable x two, it's correlated with variable c one. When we try to reconstruct conditional independence, we might discover an association, a conditional dependence between x one and y and x two and y x two. It's there because x two carry information on c one, which is really important for determining y in another environment. Instead of having an association between c one and x two, I may have an association between c one and x three. In this environment. When I go and try to test conditional association independence, I'm going to make the discovery of a conditional dependence between x one and y and x three and y. And these discoveries are correct because in this environment x three carry the information for x two.
00:11:46.412 - 00:13:06.594, Speaker A: But now if I look across environments, I can see that this Green arrow, which is the one conditional independence, conditional dependence discovery that I have made that it's consistent across environments, is the one that corresponds to a true causal direct association, while the red one doesn't. And we think that this is not a bad representation of what might be going on in genetics, right? Remember what I told you? Often the causal variable, it's not typed and what you are going to do, it's. And sometimes you make discovery by proxy. And what's happened in genetics is true, it's that you actually have this change of distributions of the covariates across populations. And these two pictures, which I'm sure I'm going to be pretty cryptic if you're not in genomics, are pictures describing you how there is a very different distribution, marginal distribution of the alleles in different populations. This top picture tells you that looking at minor allele frequency, you can reconstruct from which part of the world do you come even only using 600 markers. And this bottom picture tells you that there is difference not only in marginal frequencies, but in joint frequencies.
00:13:06.594 - 00:14:45.844, Speaker A: Okay, now if we believe in a common causal model across environment, you can be quite precise about why looking at these consistent hypotheses, disease brings you closer to the discovery of true causal relationship. And with discovery of true causal relationship, I mean rejecting these causal nodes defined in this term. So if you think you have an outcome, a function that links your predictors and some noise to the outcome, this is the same function across environments, you can say that zj, it's null. If the outcome, the value of dysfunction f evaluated in all the covariates with zj, or any other value of zj is the same. But even if you do not believe in this common causal model, looking at this consistent hypothesis is interesting if you think of it from the genetic viewpoint, say that you want to put in place a screening procedure that says everybody who has this particular mutation, I'm going to follow up and check if they develop these disease, it's much better, even if that mutation is not really the true causal one. If you can do it for selecting the same mutation for every human population, you don't want to say when somebody comes to the clinic, what are your ancestors? And let me find different tests for you. Okay? This slide is supposed to tell you that testing these consistent hypotheses is not a trivial thing.
00:14:45.844 - 00:15:41.566, Speaker A: And since we have very little time, I'm not going to later on that, I'm sure you believe me. Let me just give you a flavor of how we go about doing this conditional independence testing. To do this, we use a framework that has been described as knockoff framework. Let me just give you a sense of the problem. To do this conditional hypothesis testing, we don't want to do marginal tests, but we want to analyze the relationship between a phenotype y and all of my explanatory variables at the same time. One way of doing that, if I give you the dimensionality of the problems when I have a million explanatory variable and say 200,000 observations, it's to look at the lasso. And here it's a much smaller set of simulations where I have generated some 500 explanatory variable and an outcome from that, and then I run the lasso.
00:15:41.566 - 00:16:46.104, Speaker A: And what I plot here is the estimated coefficient. The absolute value of the estimated coefficient with the lasso blue dots corresponds to variables that are not important for the phenotype, so they don't belong to the true model. And the red dots correspond to variables that do belong to the true model. And you can see that the lasso does a very good job at zeroing out the effect of many variables that are not important. But if you were to simply look at which variables have an estimated coefficient that it's bigger than zero, you have a number of non important variables that creep in. And the question is where are you going to threshold? What are you going to consider as important or not? It's not a trivial task. And the idea of knockoffs is to augment this problem with some dummy variables and keep track of what happens to this dummy variable and use them as a way of measuring when are you including irrelevant stuff.
00:16:46.104 - 00:17:26.369, Speaker A: And this is the idea of using gamma variables is not new, but the way to construct them, it's quite important. And just let me give you a few examples to give you a flavor of what's going on. You could say, let me augment the problem with dummy variables that have gaussian distribution and the same mean and the same variant covariance of my explanatory variable. And let's see what happens. And here. So I have a problem where I have now 1000 predictors. These are my previous fellows, and these darker blue ones are the coefficients in the lasso model of the gaussian dummy variable.
00:17:26.369 - 00:18:25.194, Speaker A: You can see that the lasso really understand that these are dummies because all of the coefficients are zero. But these dark dots are not a good proxy of the light blue dots here. If I look at what happens here, if I look at what happens to this gaussian dummy, I don't have any real guidance to know what to filter out here. The same way if I do permutation, which is a very natural way of trying to construct dummy variables. And instead, if I follow this magnificent recipe of the knockoffs, I get these dummy variables that have a distribution that it's indeed very close to this one. So there's lots of things to talk about the knockoffs, and we have spent quite a bit of time constructing knockoffs that are good for genetic data. What's new here is that we can modify this knockoff construction and the filtering process that comes after that to test this consistent hypothesis.
00:18:25.194 - 00:19:04.284, Speaker A: It's not a very complicated setup. Let's say that we have three environments. We have our original variables, outcomes variables. Then we can construct knockoffs for each environment. So the knockoffs have to be constructed so that they have the same distribution as the original variable and they are actually exchangeable. Now, this distribution doesn't have to be the same across environments, but in each environment you construct the appropriate knockoffs. And then from this original variable and the knockoffs, you can get some measure of importance of each variance.
00:19:04.284 - 00:19:55.672, Speaker A: The simplest way is to say you look at the, look, let's think of z, this measure of importance, to be exactly what we looked up here. So, the absolute value of the lasso coefficient. To measure the relative importance of a variable, you look at the difference between this measure on the original variable and the measure on the corresponding knockoff. So if you have a w that it's negative, it tells you that the knockoff of a variable is more important than the variable itself. So you probably will tend to believe that this is not something that you want to select. And instead, if w it's positive and big, you are going to want to select that variable and what selection to do or not. It's specified in the knockoff filter that we don't have time to go over with.
00:19:55.672 - 00:21:03.214, Speaker A: What I want to emphasize here is that you can extend this construction to this multiple environment setup. You don't need to assume constant distribution across environment, because that would be actually defined the purpose. And then once you have done this construction of knockoff and knockoff scores, you can actually, we introduce a new way of doing the knockoff filters that takes into account this information that's coming from each environment and returns for you p values that can be put in a selection procedures so that you guarantee FDR control. Now requiring that you have an association present in every single environment, so that you can reject the null in every single environment. It's rather strict, you might be interested to say, well, I want to see if I have, if I can reject the null in a certain proportion of environments. And that's what is known in the literature as partial conjunction hypothesis. And Benjamin and Heller described them in 2008.
00:21:03.214 - 00:21:48.722, Speaker A: And there has been quite some advance in terms of what are different tests that you can use and strategies. And so we can also take this partial conjunction viewport. Let me show you a couple of results of a genetic simulation and some real data analysis. Because we are strict on time. You'll just believe me that this genetic simulation has been done with some reasonable criteria. Okay, thank you. Well, anyway, so we have generated data using some standard way of doing things and constructed a phenotype.
00:21:48.722 - 00:22:49.564, Speaker A: So maybe what is interesting is that we have different populations. We take real data from five different groups and we generate a phenotype for each of these group. In this case, we are using a constant model. We're saying there is the same real model that determines the phenotype for each population. But what we do is that we select the SNP's that are important for these phenotypes so that they have different allele frequencies across the different groups. And to clarify, we select the same SNP to be important across population groups, but we don't impose them to have the same effect. So, in fact, what we are going to do is that the sign of the effect, it's a coin flip across populations and also the size of the effect, it's randomly sampled.
00:22:49.564 - 00:23:51.024, Speaker A: And why is that? Because this is one way of capturing what we think is going on in human population. We think we are all the same. That if I could, if for me, if in european or in Africa, we were able to reproduce the same genetic pattern, it would have the same effects. But in reality, what happens? We have some alleles that are present in Europeans and some alleles that are not, and some alleles that are present in Africans and some are not. So if there is a mutation that will really increase my cholesterol level, but this mutation is not present in the population I belong to, I will not be able to detect it in european. But if I were to introduce this mutation in european, it would have the same effect. Another complication is that even if there is the same biological pathway going on, often there is an interaction between genetics and environment, and different human populations are exposed to different environments.
00:23:51.024 - 00:24:59.754, Speaker A: So when you go ahead and try to detect and estimate the causal effect of one mutation in one population on the other one, you might see a difference. What we're trying to do here is to capture the idea of there is the same biological pathways involved in these phenotypes, but they are maybe perturbed in slightly different ways in different populations. All the causal variants are unmeasured. So we are in this situation where we just can detect by proxy. And so what do we describe as a true discovery? We're actually testing the genome blocks by block and saying, do we have a causal variant in this block? So we might not have typed it, but we can specify geographically, if you want, the localization of a position in the genome and say it's in this part. Does this portion of the genome contain a causal variant or not? Okay, so we go through our procedure and let me contrast for you these two different analysis in this panel. That's called separate analysis.
00:24:59.754 - 00:25:30.634, Speaker A: We analyze each population separately and you have lots of powers for two different. These two lines corresponds to different block size. We don't need to compare across. And then you have a variety. With irritability, we indicate different signal strength. So higher irritability means that you have a stronger genetic signal. Now, what's reported here, it's causal FDR.
00:25:30.634 - 00:26:42.844, Speaker A: And with causal FDR, we mean you make a false discovery. If you make a discovery that it's not in one of these blocks that contains the true causal variant, it might not be totally a false discovery in the sense that if you look at the patterns of dependency, you might have an association there. Okay? But it's an association that corresponds to my red arrows here. Okay? I'm trying to distinguish the red arrows that might be correct discoveries, but they are not causal discoveries from the green ones. The causal FDR refers to is your discovery false or not from the causal viewpoint. And so what you can see here is that even if our method would control the FDR in terms of simply looking at is conditionally independent or not, it does not control the causal FDR. When we do an single population analysis, when we combine them and we say, actually, let's look for this consistent hypothesis, we achieve this causal FDR control.
00:26:42.844 - 00:27:24.672, Speaker A: The lines that you're supposed to look at are the blue ones. The red ones are other sort of cheap ways of trying to achieve the same result, and clearly they do not. Okay? This is real data and the results are not that exciting if you want, but it's a UK biobank data. So this is a big biobank that it's mainly composed. It's composed by citizens of the United Kingdom. So in terms of ancestry, they are mainly british, as you can see. And then you have a big contingency of other Europeans, but you have some small groups of other populations.
00:27:24.672 - 00:28:17.896, Speaker A: So the difference in sample size means that really in these studies, we have power to detect deviation from independence substantially in british only, and maybe a little bit in european. In the other populations, we have very little power. So the moment we are going to try to combine and say, I want to have a consistent discovery, I wanted to be able to reject the null in all the populations, you are going to run into trouble. But that's how our analysis goes. This is just an example we are going to test if we have independence in at least one environment, sorry. Or at least two environments, or at least three, or at least four. These blocks correspond to the different resolutions at which we can analyze the genome, and they go from low resolution at the bottom to high resolution at the top.
00:28:17.896 - 00:29:01.074, Speaker A: So what's happening here is we are reporting the blocks that have been discovered as conditionally not independent from a given phenotype. And you can see that we make more discoveries at lower resolution. And then when we try to specify exactly where the causal variance is increasing the resolution, we get more precise outcomes. You can see that we make more discoveries. This is just one particular location in the genome put here for me to illustrate the type of discoveries we make. These are discoveries that are true in at least one population. When you restrict to at least two, the number of discoveries diminishes and it diminishes even more as you increase.
00:29:01.074 - 00:29:41.308, Speaker A: Here are the results for two phenotypes, height and platelet count. And these are the various type of resolutions. You can see that if we have very high resolution, we don't have very many discoveries. But let me guide you through this table. These are these multi environment, knockoff filter discoveries. So the one that I want to argue for you, these are the discoveries that you get if you pull everybody together and you analyze every individual as part of the same populations. And these are the discoveries that you make.
00:29:41.308 - 00:30:09.638, Speaker A: If you say, I analyze every population separately and then look for discoveries that are made in every single one. So you look at the intersection. Now, there's two numbers here. The first number is the number of discoveries we make. The second number is the number of discoveries we make. The first number is the number of discoveries that have been externally validated. So maybe you can just focus on the percentage.
00:30:09.638 - 00:31:05.786, Speaker A: And what you can see here is that when you do use this multi environment approach, you make discoveries that have higher chance of being externally validated than when you say you do the pooling approach and ignore these differences. And the polling approach is equivalent to really make a discovery that it's true in one environment at least. So enforcing that the discovery has to be true not just in one environment, but in at least two, increases your chances of being externally validated and gives you more discoveries than if you do the intersection. Okay, now I think we are definitely out of time. So I thank you for your attention. I hope I convinced you that genetic variation across human population. It's an interesting case study to think about.
00:31:05.786 - 00:32:22.098, Speaker A: What does it mean to recover a causal mechanism? And how do you leverage the fact that you have data from different environment and different populations to do that. I also want to argue that consistency across environments, it's meaningful even if you do not believe in the same causal model. So, and it's, it can be bringing you to the discovery of causal variance, even if the, the way in which they operate in different environment is different, but in simple. But it also brings you to the identification of variables that might be more useful for you for making policy or making prediction in a way that it's acceptable to your population. I didn't go into the details of how you do this testing, but I want to emphasize that these tests are valid without making any parametric assumption on how y depends on x, and without requiring any consistency of model across different environments. All right. And of course, there's many open challenges.
00:32:22.098 - 00:32:45.924, Speaker A: And thank you for this butter. And then the pancake. It.
