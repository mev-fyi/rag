00:00:00.080 - 00:01:07.718, Speaker A: Well, first, let's thank Sina for stepping up in heroic fashion to speak for us. Thank you. Sina's, I guess, my first and only postdoc so far together with David Danks. I guess maybe it's officially David's, but, you know, I felt like we're co parents and now a professor at Northeastern has a joint appointment in philosophy and computer science. So he's another weirdo of the right kind. And Sina has a really special ability, I think, to just kind of casually trespass across disciplinary boundaries and in a way that maybe isn't all that concerned with even where they are. And so it was really, I've learned a lot from working with Sina, and Sina's work has, I guess, prior to philosophy, was working in also cognitive science, did a PhD in philosophy and then came over.
00:01:07.718 - 00:01:41.458, Speaker A: So I guess, got interested in things going on around the moral and political questions people are asking and around fairness and machine learning. Come spent a couple years with us at CMU. He's collaborated, worked with folks in philosophy department, worked with folks, some of my students from machine learning, from natural language processing, in a way that, you know, is, I think, really inspired. So really glad to have him with us and see his talk. So thanks, Ina, for stepping up for us.
00:01:41.626 - 00:02:28.964, Speaker B: Thanks, Zach, and thanks everyone. So, given Zack's introduction, I should say in advance, I apologize for the last minute talk. So Zach said that I work on that, have been working on interpretability a little bit. I should say that it's been quite tangential. Most of my focus has been on issues of fairness and diversity in machine learning systems and diversity more generally. And I think. But some of these themes will come through the talk, and I should say that many of the things and themes that I will be talking about, I will be drawing on some of the previous discussions and joint work that I've had with my great collaborator, Maria D'Artega, who's at UT Austin.
00:02:28.964 - 00:03:57.388, Speaker B: So kind of the basic motivation for what I want to talk about today is that obviously, with the use of machine learning systems in all of these sensitive domains, people have started taking it seriously to evaluate their impact, evaluate their properties, and maybe intervene in the way that they're designed so as they kind of align them with our social desiderata, with what we care about. Right. But in many of these cases, I think it's fair to say that the focus of this evaluation and intervention practices has been on the predictive model itself, just one part of the pipeline. And in some previous work, I've criticized that and I know many people that are in this room have criticized this type of narrow focus on the predictive model. For instance, because we are just focusing on the model in a static setting, we might lose some of the dynamic aspects of the impacts of the model. And so what we think is the right intervention at the model level, in a static sense, might result in long term impacts that actually decouple from our intended aims. But what I wanted to talk about today is just this other basic thing that in many social applications, I think that typically motivate papers, typically make the news.
00:03:57.388 - 00:05:32.280, Speaker B: These models are actually not decision makers. They're used in decision support capacity. That is, they're used to provide predictive recommendations to some human user or a team of users that can then use these recommendations to make decisions or maybe actually ignore these decisions, but they act in that kind of decision support functionality. And the basic question for me is that when we actually take these functionality of decision support seriously, how should that change the way that we think about evaluating and designing the systems? And given our focus on interpretability, how should that change? In particular, some of the ways that kind of maybe typical discourse, especially among, I would say maybe some of the technical literature, but also in policy and ethical analysis, about the relation between interpretability and accuracy, and particularly the supposed trade off between the two. And to put my claim there, I come, as I said, from a background of thinking about multi agent systems and complex systems. So obviously, I'm going to say that the norms that we have for thinking about individual level performance and what might be the proper performance and what might be faulty performance are quite different from what we will have at the group level and sometimes at the individual, in the group level, we really want something that at the individual level might not look that good. So that should change the way that we design these systems and should change the way that we evaluate them.
00:05:32.280 - 00:06:20.082, Speaker B: But this is typically, unfortunately, not taken into account, and then we can think about why. So let me start by saying what I mean by the standard view that I said typically ignores these things. By standard, I don't need, I don't mean anything bad by it. It's just the typical way that things are done. Sometimes it's good, sometimes it might not be good. I think it's fair to say that in many cases, when we are designing this model, the models are kind of chosen on the basis of which model fits a particular objective better, which model minimizes a particular loss function, right. Which models kind of maximize the accuracy according to some particular measure, but the way that the headlines will put it.
00:06:20.082 - 00:07:45.300, Speaker B: And the way that many research papers put it, even in cases where the model is supposed to be serving or assisting human decision makers, this is done in a way that has this narrative of competition. That is, the model is outperforming the user in this predictive task. And the common assumption is that when we choose a model in this way, the model that minimizes unlock function, for instance, that's also the model that is going to be best from a decision support perspective. The challenge here is to make sure that this human decision maker actually is going to adhere with this decision so that their behavior is not, uh, does not exhibit algorithmic aversion, for instance. Right? So we don't have this good model there, and the expert is, uh, ignoring it. Now, of course, there might be other things that we care about other than accuracy, right? There might be models that are very accurate, but either humans don't use them, as I mentioned, or patients might not trust them. There might be a list of other things that we might care about, anything from kind of improving responsibility and safety of decisions to enhancing trust, both by the user decision maker or society, or decision subject, fostering autonomy, again, by the user and by the decision subject, enabling recourse for decision subject so that they can come and question the decisions, so that they can have ability for, let's say, informed self advocacy and so on and so forth.
00:07:45.300 - 00:08:42.714, Speaker B: And in many of these cases, it is thought that something like interpretability, explainability, maybe in some different sense, is going to serve those purposes. Now, there's lots of questions about how the same thing could serve all of these things, whether there are different things that serve all of these things. I will put all of these aside and kind of proceed with this nice formulation of the cost of imposing interpretability in some sense that in this paper that Shai has with some colleagues, that, okay, we have these other things. One typical way of thinking about them is that we can turn all of them into some type of bias or restriction on the way that we learn particular model. So this is, I think, quite a standard in many ways that people think about fairness. They think about fairness as some type of constraint on our learning. So instead of just learning as such, we learn or minimize loss, subject to some type of fairness constraint.
00:08:42.714 - 00:09:51.572, Speaker B: Again, I'm not a fan of that, but I think this is typical. And we can think about interpretability in the same type of, say, so restrict our attention to a subset of potential models that we could learn. Now, how do we decide what that subset is? Let's leave it to a side, according to some, let's say complexity or something else. But the worry then, of course, is that when we do this, this might have a cost, because obviously, instead of searching for the best models in the overall hypothesis space like this set, we are just focusing our attention on a particular subset and there might not be any overlap between them. Or in the best case scenario, the predictive accuracy of them might be similar to each other. And I think importantly, this type of view about the relationship between these two desiderata, accuracy and interpretability, is quite widespread in ethical and policy engagements with this domain. So when people are trying to decide what type of models we should use in particular domains like medicine, I think many cases people take this at face value.
00:09:51.572 - 00:10:35.164, Speaker B: They think that it actually, these two things correspond to some type of value trade off. So for instance, they say, oh, in some cases, when we are deciding how to allocate some limited medical resource, we care about fairness and interpretability. Right or wrong, it's taken to promote fairness. And there it's okay to sacrifice accuracy. But in certain other cases, when we care about diagnosis, maybe we just care about accuracy, and then we don't need to use an interpretable model. This is some of the way that people make proposals about what type of model we should use in a particular domain. But the common assumption obviously here is that model level properties are going to map onto some type of system level property.
00:10:35.164 - 00:12:22.070, Speaker B: That is, if I say that I want a model because it's because I care about the accuracy or quality of decisions, I should choose a model that is the most predictably accurate, I should not sacrifice for the sake of interpretability. So this is like what I mean by the kind of this standard lens, where the narrative is kind of the competition, choosing the best model according to some measure, that that best model will be the best model also for decision support, and that the properties of that model, if things don't go wrong, like for instance, if the humans don't exhibit certain types of algorithmic aversion, will map onto the properties of the system in the best case scenario, ideally, but obviously when we look at things from, not the, from the perspective of the properties of the model, but from the perspective of the properties of the team that we actually care about, then things might be quite different. Because at the group level, sometimes we have to actually make sacrifices for the properties of a part of a system, broader system, in order to make advances for the same property at the level of the system. Well, let's just see the way that this is typically talked about in the literature on group decision making. So, when we look at the group decision making, there are different types of mechanisms that are involved, the shape, the quality of decisions of a group. On the one hand, we care about the cognitive repertoire or cognitive attributes that individuals in their team are bringing to a common task. And on the other hand, we care about the processes by which we actually elicit and integrate this distributed information across the groups.
00:12:22.070 - 00:13:44.684, Speaker B: And in each of these two processes, two components of group decision making, there are different types of things that could go wrong, right? In terms of cognitive mechanisms, if everybody in the group thinks in the same way, have the same type of limitations, uses the same types of knowledge, that is, if you have cognitive homogeneity and informational homogeneity, then we are not going to be in a very good situation in terms of dividing labor in complex tasks. If I get stuck somewhere, my teammate will also get stuck there. If my information is corrupted with noise or has some type of error, it is very likely that also my teammate has the same type of information that has those problematic things. On the other hand, if you have diversity, cognitive information diversity, if I get stuck somewhere, a teammate might actually be able to do something that I can't do. And so as a team, we do better. If I need confirmation and it comes from a teammate who actually looks at the situation very differently, their confirmation might actually mean more, which is like typical way of when we discuss methodological triangulation and similarly with the way that we integrate the information. But I will just focus on this idea of cognitive diversity, the way that in teams, what we typically want is to have cognitive diversity of cognitive attributes among team members.
00:13:44.684 - 00:14:40.844, Speaker B: And I think that maybe, like a toy example, can really show the value of this type of cognitive diversity by, in the way that they enable complementary team performance in the human AI team. So, assume that we have a loss function that measures the predictive performance of different individuals. So, given this loss function we look at, and the lower, obviously it is, the better. So we see that the predictive model, a one, is the best on this predictive task, and assume that all of the individuals are doing the same task, which is not necessarily true. Sometimes humans are doing different things. So we can ask, is the model with the kind of lowest loss, the best decision support model? And the answer is, obviously, it depends. You might need to know more about the way that the strength and weaknesses of these models and their users is distributed across different cases.
00:14:40.844 - 00:15:39.074, Speaker B: So suppose instead of just knowing the average loss function, we also know the loss function across different regions of feature space. And suppose that they're kind of all equally equiprobable. When we look at this type of different regions, then we can actually see how the strength and weaknesses of each of these members might complement one another. So this model is very good here, but this human is very good here. And when we look at this variability within and between agents in performance, then we can actually put things together in a way that forms better teams, teams that are better than any single individual. So this is, for instance, if I knew all those loss functions, and for each region, I just decided to go with the prediction of the individual that is best at that region. Then in that case, this team, even though it does not include the best individual performer, is going to be the best team.
00:15:39.074 - 00:16:36.030, Speaker B: From a systems level perspective, the most accurate predictive model is not necessarily going to be the part of the best team and same type of thing. Of course, there's a smaller set of or subset of literature that does try to incorporate this into machine learning design. Right? So sometimes you can do this by changing your objective to make sure that the model, if it's making mistake, the mistakes are in places where humans are good. In other cases, you might decide to defer on cases where the model does not have enough confidence. You might differ to experts. Again, complementarity might arise if experts are actually good in those cases, otherwise, not so. The broader point, and there are many different instances of this, there are dynamic instances of this when we update the model and the performance of the overall team might change.
00:16:36.030 - 00:17:24.918, Speaker B: But the broader point really is that when we are thinking about the functionality of models in this other capacity, as opposed to autonomous decision makers thinking about them as decision support system, then we have to consider norms at the right level. Otherwise we are kind of committing a categorical mistake. We have to consider the norms at the system level, fault modes at the system levels, and the potential trade offs, such as diversity, stability at the system level. And it's been important because the model level properties do not necessarily map onto those system level things. And I should also note that same type of situation, I think also arise for issues of fairness. So I've been talking about interpretability, about the same type of thing. You can mention it in terms of discussions of fairness on their composition, but sometimes different parts of a bigger system.
00:17:24.918 - 00:18:46.624, Speaker B: It might be many different algorithms that are all doing a particular task, like misinformation detection, have a property that you like from a fairness perspective, but the overall pipeline might not have it, and I think this will also have implications for that case. And I think it's also important from the perspective of not just what interpretability is, which I will get to in a bit, but the aims of interpretability. So, so many times we say that they care about autonomy of users or the decision makers that use these systems. I think thinking about this from this system level perspective is a kind of a meaningful way of promoting that autonomy or promoting that trust. So I think when we consider situation from this way, through this lens of system level lens, then policy proposals that suggest simply that in certain cases where we care about accuracy, we should ignore interpretability. If we just take it at face value, that's going to be mistaken. At the very least, we should know that this interpretable model cannot give us something else, cannot provide us with a way to make the human decision makers better or to make the group better than any of the individuals.
00:18:46.624 - 00:20:10.564, Speaker B: I think this can also result in kind of this type of off the shelf justifications that are based on category mistakes that, oh, I like my predictions to be accurate and therefore my decisions to be accurate, and therefore I should go with the most predictive model and it might lose all these other possibilities. Okay, so I said in that case that interpretability might, even if at the level of the model, might be detrimental to the performance, or might be seen as detrimental to accuracy, at the system level, it might provide better predictive performance. But one could wonder whether interpretability itself is useful or required in this sense, or something else can. That is, whether it's the only means to achieving those, or whether there are other means. And if it is a means, interpretability, in what sense can we make it more specific and for what type of task? And I think literature here is a little bit, I wouldn't say divided. So maybe divided and mixed. So there are literature that shows that interpretability can benefit the performance of the human AI decision making teams, but they typically focus on cases where that interpretability just help the decision maker overcome algorithmic aversion.
00:20:10.564 - 00:21:14.684, Speaker B: So it doesn't actually discuss complementarity. The works that focus on complementarity, I think they suggest that more or less that explanations actually undermine our aims of complementarity. So remember that when we care about complementary performance, we really want each team member to actually kind of assume responsibility for the part of the task that they're good at and delegate responsibility otherwise. But in many cases, it seems like explanations actually make the users too comfortable, so they rely on the AI predictions too much, such that when their expertise is needed, they don't actually contribute. And in some suggestion, some other cases the suggestion is that if we simply provide confidence of the model, that might give us better reason. Now, some of you might think that, okay, that's transparency or interpretability enough for me, providing this confidence score. And some other people might think that that's not really interpretation or explanation.
00:21:14.684 - 00:22:03.896, Speaker B: I think we can kind of go beyond this by thinking a little bit more about some of the prerequisites of complementarity. So this was the case of complementarity that I had. This is the performance distributed performance of the human, and this is the model. So notice that here I am assuming from the perspective of someone who is integrating the system or choosing a particular model, that I know what this regimentation of space is. But suppose that I don't know it, and I have different types of, uh, averaging it. So in this case, I have this coarser grained regimentation of the space where I am looking at the average of this and average of this and same for this. When I look at things from that way, actually, there is no complementarity.
00:22:03.896 - 00:23:34.034, Speaker B: I look at these things like, okay, yeah, adding this model or adding this human together, we cannot have any complementarity here because the model is better than the human in both regions. Now, if I average things in the other way, suddenly I can see potential for complementarity, right? So maybe, and this is kind of similar, getting similar to the, some of the things that David was discussing is in terms of the interpretability of the not representation, but the regions where somebody might be good. So maybe some clustering methods, but maybe some aspects of the interpretable machine learning and explainable AI approaches can help us in figuring this out. But this is still based on kind of, I think, observed performance and observed error in particular cases, maybe we can go more fundamentally by thinking about kind of what actually at the deeper level, different individuals might bring to a task. And in some of the cases, based on just image classification, it seems that humans and machines make very different types of mistakes on different tasks. So obviously, I think you can guess which row is the road that is difficult for humans and which row is the row that is difficult for machines. Row B is the road that is difficult for machines, and row a is difficult for humans, but very easy for machines.
00:23:34.034 - 00:25:32.466, Speaker B: So I think one possibility here is, and this, it's particularly, I think, suitable for cases when we are thinking about medical diagnosis to maybe even go further and think about what type of perspective or kind of representation of a particular problem do individuals bring. Right? So if they're latching on different pattern, latching onto different patterns in the, let's say, image, or in the particular task, then that might give us some reason to form teams in that way. And maybe we can even combine it with these discussions of rational set that I know that many of you have worked on it, that have been discussed in relation to fairness and interpretability, but not, I don't think that much in relation to complementarity. Basically thinking about from the set of models that have exhibit similar type of performance, which one of them actually exhibits complementarity in terms of representation on a deeper level with human decision maker. So is interpretability required useful? I think we need to be kind of clear about overarching aim and the user. So do we want better performance than the status quo, than the human decision maker, or do we want actual complementary performance? If you want complementary performance, then we can think about different prerequisites for complementarity. And we should also think about, even with that focus, useful for whom? That is, with respect to what aim do we want it for a system designer that is choosing which model to use to human being, to the human decision, human expert, human decision maker, even if they don't want to kind of say anything more about the model to the decision maker, or do we want to interpretable in the sense of for the information integrator, which might be the human user itself.
00:25:32.466 - 00:26:49.656, Speaker B: And in each of these cases we have different measures of success. We might have a model that overcomes humans algorithmic aversion, and so it's successful in the case of improving performance compared to a status quo, but it's seen as a failure in terms of complementarity. When we specify that maybe we can have a better sense of interpretability. But yeah, so the overall point, I think, which is pretty obvious, is that what we want from the system level might be different from what we want from the individual level. It's easy at the individual level to focus on the properties of the model, but if you really want the use of these systems to make our institution as good as possible, then you need to consider those broader system level norms. And this is another paper, we don't really discuss this here, but this is another case where we try to go and consider how identity and demographic diversity can be beneficial throughout the pipeline of the thing. But this is again like the same type of the spirit of like what would happen if we, at each point we go beyond one label to consider diversity of labelers and so on.
00:26:49.656 - 00:26:59.364, Speaker B: But yeah, anyways, as Zach said, you probably noticed the last minute part of this, but thank you for your attention.
00:27:23.404 - 00:28:12.064, Speaker C: So I think this is an implication of your view, but I just want to make sure I'm understanding it correctly, which is if I care about accuracy, it's entirely possible that actually I should deliberately have a model that is not interpretable. So interpretability is, is just a means to whatever the system level characteristics are that I would want. Because, you know, if the user, the human decision maker is, somebody might game the system, I might want to deliberately obscure why I made the decision, why the algorithm yields the decision it does, or the recommendation it does, so that the human can't game the system. Is that, do you endorse that implication? That sometimes actually I should design my algorithm to mislead, trick, obscure things from the human decision maker?
00:28:12.564 - 00:28:52.524, Speaker B: So the should part is obviously the heavy ethical part that mislead. But if we, I think that's a fair implication that if we care about the system level performance, sometimes it might be the case that the model that is not interpretable serves some of the aims that we want. So if a model that is interpretable, again, in some sense, I don't know what sense, if it creates over reliance, then perhaps I don't want it. Actually, perhaps I want another model that increases vigilance. I don't know. So, yeah, but from that perspective, these two things can form a part.
00:28:58.984 - 00:29:01.576, Speaker D: Of that. I think that creates difficulties.
00:29:01.640 - 00:29:01.816, Speaker B: Right?
00:29:01.840 - 00:29:20.528, Speaker D: Because then who decides I can have ten different incomprehensible algorithms and then you human have to choose which one are you going to roll it out to see what happens? But then can have a lot of possible dangers that you don't know.
00:29:20.656 - 00:30:08.940, Speaker B: Right. So the idea is that that makes the kind of design task a lot more difficult and fragile in a way that instead of just like we choose going with a model that we think fit certain types of properties, we have to consider all the types of perturbations that might come from particular types of users, what they take to be incomprehensible, what they take not to be kind of incomprehensible. And I think so I'm not suggesting that we should go with particular types of users, but I think that there is a particular type of explanations that, or explanation related behavior that humans exhibit that they kind of defer to certain types of explanation, even if the explanation is irrelevant.
00:30:09.092 - 00:30:28.774, Speaker D: And I definitely get it that humans have biases and very strongly the question is how do we prove or show or have agreement that the machines will not create even more biases without understanding what the machine does? So maybe we should continue at the panel.
00:30:28.814 - 00:30:31.394, Speaker B: Yeah, yeah, very nice.
00:30:31.934 - 00:30:48.864, Speaker A: Thanks, Ben. Thanks, Nina. Okay, so we can reconfigure for the panel, which I think just means putting some. I promised everyone a bathroom break, so if anyone would like to go to the bathroom, now's your chance. It.
