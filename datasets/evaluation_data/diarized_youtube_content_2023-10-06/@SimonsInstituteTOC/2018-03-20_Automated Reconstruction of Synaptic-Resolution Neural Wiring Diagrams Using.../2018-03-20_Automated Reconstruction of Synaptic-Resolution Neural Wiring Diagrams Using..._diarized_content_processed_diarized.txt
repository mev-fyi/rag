00:00:04.360 - 00:00:13.702, Speaker A: I'm introducing my good friend Varenne, who is my good friend because I've known him for a long time, and he's a smart, fun guy.
00:00:13.798 - 00:00:14.434, Speaker B: But.
00:00:16.454 - 00:00:48.274, Speaker A: He and a few others have revolutionized iconic comics in very recent years by taking convolutional network approach. He and Sebastian Sung and others pioneered a decade ago and with the miracles of modern machine learning, have made them work better than I ever thought possible. His talk is back.
00:00:49.214 - 00:01:04.534, Speaker C: Thanks so much for the intro. Is my mic on? Okay. All right, well, thanks for coming to this talk. Please interrupt me at any time, as frequently as you like.
00:01:04.654 - 00:01:05.754, Speaker D: Are these slices?
00:01:06.654 - 00:01:37.574, Speaker C: That's right. So I'm going to get into all this, but, yeah, what you're seeing is a video of a bird that used to exist, but then was sacrificed and imaged using high resolution electron microscopy. And then this is the wiring that was automatically inferred from sliced. That's right. That's right. So the data I'm going to talk about today are from three different sources. We have a longstanding collaboration with the Max Planck Institute, Winfrey Dank's lab, the HMI genealogy campus, and then a recent collaboration with Jeff Lichman's lab at Harvard.
00:01:37.574 - 00:02:06.176, Speaker C: So I just want to acknowledge those collaborators who are providing all the data. We don't do any imaging at Google. We just work on the data. And so I'm going to talk about, broadly speaking, three different things. I'm going to introduce connectomics a little bit, although maybe I'll keep that on the shorter side, given the knowledgeable crowd here. And then I'll talk a little bit about the computer science that has been driving the progress that we've been involved with. So, what are the algorithms? A little bit about the infrastructure, not a lot.
00:02:06.176 - 00:02:48.144, Speaker C: And then, finally, I'll talk about some of the neuroscience which this progress has been enabling, in particular, in drosophila, in flies, in birds, zebra finch, and most recently in some human tissue. All right, so, connectomics, it's a word that was kind of foisted on us. There's different scales at which people blame Sebastian. You can blame whoever you want, as long as it's not me. It's not a great word, but we're stuck with it. So people talk about connectomics. So, for example, there's an NIH connectomics project, which refers to really human brain wide scale pathways imaged using things like DTI or functional MRI.
00:02:48.144 - 00:03:47.474, Speaker C: That's not what we're talking about today. Today we're talking about, or at least in my talk, we're talking about detailed wiring diagrams, which means synapse resolution. And if you're talking about synapse resolution, then you're more or less restricted to electron microscopy, or in some cases, various particular forms of sparse genetic fluorescent labeling. And just to put things in context here, that's a scale version of a human brain. And the frontiers of this synapse resolution mapping are really quite tiny compared to something like a human brain. So there's sub cubic millimeter, which is something like that little dot below the text where it says frontiers of synapse resolution connectomics. So, for the technologists in the field, the challenge is essentially to make the dot on the right as large as the dot on the left, sort of like a multi decade project, similar to what they went through in genomics, of making things cheaper, more efficient, better, et cetera.
00:03:47.474 - 00:04:22.460, Speaker C: And in the meantime, we hope we can do some neuroscience even with that dot. So the fundamental question that we're trying to answer from the technology point of view, from the measurement point of view, is which neurons are forming synaptic connections with each other. And to do that, you have to solve two problems. You have to associate each wire in the brain with a parent cell body, and second, you have to find specific connections. And here's sort of a cartoon of the basic problem. So imagine this is a brain where the large white dots are cell bodies and the lines are the wires. And then I ask you for the piece of wire that's inside that red circle.
00:04:22.460 - 00:04:59.804, Speaker C: Which cell does it belong to? Which cell body? There's nothing locally inside that red circle which will let you answer that question. You have to visually trace the data. And that's exactly what we have in electron microscopy of neural tissue. We have a grayscale set of cell bodies and wires, and we have to visually trace things. Now, that's not enough to form a connection in a graph. We also have to identify synapses, which are, you know, specific directed chemical communications between two different cells. And fortunately, we can do that with electron microscopy, by virtue of identifying postsynaptic densities, synaptic vesicles, characteristic structures of synapses themselves.
00:04:59.804 - 00:05:42.424, Speaker C: Of course, this is nothing new, right? This was done for C. Elegans, as we most of us know, in the seventies by Sidney Brenner and his colleagues at the MRC. And so they took a C. Elegans, they chopped it into sections, they imaged the sections using electron microscopy and printed them out, and then manually traced neurites from one section to the next. So you can see these markings here, which are literally how they're keeping track of where the neurons are going throughout all these sections. And this was compiled into a connectome, and it was finished and sort of formalized by Mietislowski and his colleagues not too long ago, about a decade ago. And of course, you can represent this then as an adjacency matrix.
00:05:42.424 - 00:06:01.056, Speaker C: And in fact, just to prove the point that nothing is really new. Even the idea of using computers to analyze this data is ancient. So we can go back to 1982, the year I was born. This is a textbook listing applications of computer vision technology.
00:06:01.200 - 00:06:03.240, Speaker D: Is that the same Ballard that's sitting here?
00:06:03.392 - 00:06:32.732, Speaker C: Is that a Ballard here? Yes. Oh, wow, that's amazing. I didn't even know that. Well, I don't know if you remember this page, but somehow I found it. It's quite a nice table, actually. You have all these different things you can work on if you're a computer vision researcher. And lo and behold, one of the things here is neuroanatomy, identifying the orientation, the shape of cells from neuroanatomical data.
00:06:32.732 - 00:07:10.734, Speaker C: So even in 1982, the idea of using computers to solve this problem was clear. But the problem was, of course, that computers were just not really up to the task. You couldn't even store one of these images, much less process it, annotate it, segment it and so on, at least as far as I understand. Maybe someone did that and I just haven't found the paper. Okay, so C. Elegans happened, but basically it was such a pain from every point of view that people forgot about this for a long time, until maybe 1015 years ago, when not analysis, but really on the imaging side. There were some advances in automated volume electron microscopy.
00:07:10.734 - 00:07:50.144, Speaker C: And Clay and his colleagues introduced a new TeM based technique. Winfred Dank and his colleagues introduced a block face technique. There was a couple of different sort of approaches that were proposed back then and have been sort of continued to develop along various lines since then. I'm not going to talk about the details of the imaging in this talk, though. So just for some rough timeline. 1970s, there were c. Elegans 2013, a bunch of studies were published in Drosophila by Mitya, in visual cortex by Clay and Winfried and others produce studies in the retina.
00:07:50.144 - 00:08:31.082, Speaker C: Currently, what's going on at the Allen Institute you heard about, there's a very large scale project to interrogate the mouse cortical column. Janelia is working on the whole Drosophila brain. Winford has many projects in Songbird. There's other work going on and maybe the next generation of machines and very large scale projects will try to do something like a whole mouse brain, although that really is quite a significant stretch for the technologies that we currently can imagine. But that's also what makes it interesting. So, here's an example of a data set. So, this is 10,000 by 10,000 pixels that you're seeing, and you're seeing basically one slice after another.
00:08:31.082 - 00:09:20.990, Speaker C: So, just a reminder that typically the way these data sets are collected is that you have a block of tissue. You image a single slice of the tissue either on the block or by removing it from the block with sectioning procedure, and then you image the next slice and so on. So it's kind of like a deli slicer approach to imaging a three dimensional structure. And we'll get more into the details of volumes like this in a little while. Okay, so, for a computer scientist, what's the situation here? So what do we have to do? So, there's a couple of different things to think about here. So, I mean, one is that the storage costs of these data sets are nontrivial, right? A cubic millimeter image at this resolution is roughly proportional to about a petabyte of data. So whole mouse brain would be hundreds of petabytes, human brains would be exabytes, and so on.
00:09:20.990 - 00:09:52.572, Speaker C: So, I mean, that's kind of an issue, but not really, because right now, most of the data sets are petabyte or sub petabyte, which we kind of know how to deal with. Compute. That's a more significant issue. So, if you want to run a convolutional network on every pixel of a trillion voxel data set, you got to think about how you're going to do that and whether there's ways to make it more efficient and so on. I'll talk a little bit more about that in the future, sorry. In a couple of slides. And, of course, algorithms.
00:09:52.572 - 00:10:21.416, Speaker C: Right. So, I mean, this is really the most fundamental challenge. I mean, the first two are essentially engineering, which are more cost issues than fundamental issues at this point. But, of course, algorithms. Humans can analyze this data, but computers historically have not been that great at it. So what are the algorithmic challenges? So, the first thing is something called image alignment. So, this is if you have many different photographs of the same scene, which is essentially the situation here.
00:10:21.416 - 00:10:56.974, Speaker C: They need to be registered into a common coordinate system. And in connectomic imaging, you may actually have tens to hundreds of thousands, even millions of two dimensional photographs, which need to be registered and stitched together into a coherent three dimensional coordinate system. And so this is actually, I think, actually an underappreciated problem in this field. A lot of people have talked about segmentation, which is what I will also mostly focus on. But you can't do segmentation or really anything else in this field until you solve this problem. And it's not an easy problem. I don't know if Clay wants to add any addendum to that.
00:10:57.594 - 00:11:06.694, Speaker A: Vinfried Declay, 2005. I can prove to you that your approach will never work because you can't align it.
00:11:08.434 - 00:11:52.614, Speaker C: Okay, proof by Winfried. Right? So, alignment it remains, actually, in some sense, not quite a solved problem. We kind of know how to do it, but it's much more painful than it really should be. The next problem, once you have an aligned stack, is segmentation. So, this is an example of a human segmented neuron within one of these image volumes. So the raw data is on the left as a two dimensional series of slices, and on the right is a human segmented neuron within this data set at a cost of 40 to 50 hours of human time per neuron. So it's an interesting situation in which humans can annotate the data to create what we refer to as ground truth, but it's just too costly to do it for all the data in the volume.
00:11:52.614 - 00:12:30.604, Speaker C: Okay, so what's the situation here in terms of computer vision? And so the basic thing, and maybe it's obvious now, is that the techniques which we've had traditionally in computer vision don't work. They make too many mistakes. So what does it mean to make a mistake? So, imagine you have an adjacency matrix. You can imagine two different things. So every white dot here represents a particular directed connection which has been inferred from the underlying tissue. So one type of mistake you can make is a false positive, right? Maybe you posit a connection that doesn't actually exist. Or of course, you can have a false negative, right? So you miss a connection that should have been there.
00:12:30.604 - 00:13:21.444, Speaker C: So if the state of algorithms was such that our error rate in this matrix was a few percent, that might actually be okay, right? I mean, even genetic sequencing circa 2018 generates error rates of a few percent, depending on what you're doing and which species. So, from the biological point of view, that might have been okay. But really, when you apply conventional or even state of the art machine learning approaches off the shelf, you get basically a matrix that is completely unreliable. And this has to do with, basically, percolation of errors. A single error, a localized error in the segmentation process, can cause non localized errors in the connectivity matrix. So imagine you split off a whole branch of a neuron that causes thousands of errors in the connectivity matrix. And in fact, it gets even worse when you deal with under segmentation errors.
00:13:21.444 - 00:13:36.604, Speaker C: Okay, so how to increase accuracy. All right. Any questions about what I talked about so far? Comments? All right. Okay. Methods. Okay, so this has been our progress over the past few years.
00:13:37.464 - 00:14:00.860, Speaker D: There is an issue which you did mention, tomography. So one of the. There is a form of Em where you tilt the specimen, and that allows you to get a 3d reconstruction with equal accuracy on the Z direction. So why is it people aren't doing that? That seems to me like to get around all the computational issues, or at least some of them. I'm not old.
00:14:00.972 - 00:14:11.868, Speaker C: Well, you can do tomography of thin sections in order to get isotropic resolution in the underlying data, but I don't think that sidesteps the fundamental issues of large scale alignment and segmentation.
00:14:11.916 - 00:14:18.764, Speaker D: You do, like a few microns, right? That's a pretty big slice tomography.
00:14:18.924 - 00:14:20.164, Speaker C: Mitra, you've worked on this.
00:14:20.244 - 00:14:32.996, Speaker A: It's very, very slow to do tick tomography. And the thing about EM atomics volumes, you gotta scream at millions of pixels. You just gotta do it.
00:14:33.020 - 00:14:33.668, Speaker C: Okay. Yeah.
00:14:33.716 - 00:14:44.326, Speaker B: It doesn't scale well. That's basically because it's too slow. And also, when you have wide sections and you start tilting them, they go out of focus, so it complicates the mechanics.
00:14:44.390 - 00:14:44.830, Speaker C: Right.
00:14:44.942 - 00:15:00.046, Speaker E: Okay, so what, I mean, since you asked for questions, what is the over and under segmentation rate? So, not the error rate on the matrix, but for any given neuron that you can reconstruct, what's the probability that you'll override the segment?
00:15:00.110 - 00:15:07.434, Speaker C: Right. So this is exactly what this slide is about, in some sense, the metric that I'm going to talk about. But just before I do that, you had one question also. Yes.
00:15:07.474 - 00:15:11.770, Speaker D: I wonder how consistent. Two human beings finding the nerve neurons.
00:15:11.922 - 00:15:38.700, Speaker C: Yeah, basically, humans are very consistent. As long as you subtract noise, which is related to occasionally humans just make a mistake randomly, or they're lazy, or maybe they're just totally untrained. So people have different ways of tired. Maybe they're tired. Yeah. So what I mean is that you can structure a human effort in order to get reliable tracings, but it's a little bit non trivial. Right.
00:15:38.700 - 00:15:55.956, Speaker C: So you have to do things redundantly over multiple humans and then combine them. But I would say, I mean, occasionally you get to a fundamental ambiguity in the data set where even experts cannot resolve what's going on, but that is very rare at this point, at least in the data sets I work with. Okay, so metrics.
00:15:55.980 - 00:16:00.704, Speaker B: So what was the intrinsic resolution of the images?
00:16:01.024 - 00:16:18.844, Speaker C: So that varies depending on the method, but it can be anywhere from, let's say, 4 nm in x y to, let's say 8 in z. So some of the data sets are more isotropic than others, depending on the imaging methods.
00:16:20.624 - 00:16:21.312, Speaker D: Yeah.
00:16:21.448 - 00:16:25.760, Speaker A: Are you trying to identify only chemical synapses or also gap junctions?
00:16:25.952 - 00:17:02.584, Speaker C: So we would like to have gap junctions in most EM datasets that I've seen. It's difficult. Um, you really need very high resolution, very high signal to noise, uh, very high quality standing in order to reliably infer gap junctions from at least the kind of Em that people produce these days. I mean, in principle, it's. It's possible with Em. It's just the current acquisition regimes are not optimized for gap junctions. Okay, so, all right, so to talk about methods, I'm first gonna talk about what is our metric, right? So if you work at Google, you need a metric.
00:17:02.584 - 00:17:35.413, Speaker C: This is what we came up with a couple of years ago, and we've sort of taken this very seriously in our group, and it's what we call expected run length. And here's the idea. If you pick a random point in a random neuron, then, on average, how far can you trace out from that neuron before you make any kind of mistake? So it's kind of like a self driving car metric. How far can the computer go before a human has to take over or something like that. Right? And so we like this metric for a couple of reasons. First, it's actually a pretty stringent metric because we terminate it at any kind of error. Right.
00:17:35.413 - 00:18:00.588, Speaker C: So, you know, missing a single spine would actually terminate this metric, right. So it's not easy to make progress on it. The second thing is that it's easy to relate to biology, right? So we know that as we get to larger and larger organisms, the total path length of the average neuron also increases. So what that says is that in order to reconstruct larger organisms, we actually need to make progress on this metric.
00:18:00.716 - 00:18:06.304, Speaker B: But it varies with the tissue. Yeah, there are some places which are very easy.
00:18:06.724 - 00:18:23.420, Speaker C: Right. So, I mean, ultimately this. It's an average over structures in the tissue that we've imaged. And if you're comparing numbers from two different volumes, then you may need to do some adjustments to make them. Comparable variance is also important. Yep. And so this has been our.
00:18:23.420 - 00:19:00.326, Speaker C: So this is log scale just to be clear here, so this is around 5000 x from where we were a couple of years ago, and it's now, you know, around four or five millimeter range. And basically there's been two errors of technology here, which I'll get into the red era and the green error. Okay. But at a high level, you know, for computer scientists, it's important to keep a couple things in mind. So, you know, these are very large data sets, and it's really a single data set, right? In the sense that any two voxels could fundamentally be related to one another. So you have trillions of elements. You're absolutely restricted to linear time methods.
00:19:00.326 - 00:19:46.474, Speaker C: You cannot do anything even quadratic on this data set. And really, you need to decompose the problem in some way, because even if you have a trillion elements and a linear time method, you don't really want to shove a trillion elements into a linear time method. So the first era of technology in this field really had a pipeline aspect to it. So there was convolutional networks, which were used to predict local structures like boundaries or other representations of local geometry. You would form candidate regions locally using things like watershed or connected components applied to that boundaries that were inferred by the convolutional network. And then finally, you would agglomerate things together. So you would have basically pieces of neurites that would then be assembled into larger holes, ideally neurons, by some other machine learning procedure.
00:19:46.474 - 00:20:29.696, Speaker C: And so it's interesting, I used to have these slides, or I guess I still have these slides, explaining what convolutional networks are and why you should use them. And the reason I have these slides in there is because we started using these things in 2005 and 2006, when really, I mean, nobody else except for Jan LeCun and maybe Jeff Hinton were interested in these methods. And I have to say, on a personal level, it was kind of strange, because in 2007, I was using convolutional networks to do neuroanatomy, and neither of those things were fashionable at MIT. So I thought I was really getting myself in some trouble here. But anyway, over a couple of years, we basically, do you know how to.
00:20:29.720 - 00:20:34.724, Speaker D: Tell the pioneers from the followers? The pioneers are the ones with the arrows in their backs.
00:20:39.724 - 00:21:27.750, Speaker C: All right. Well, over a couple of years, we basically introduced various innovations to convolutional networks to try and optimize them for this problem, making them 3d, introducing loss functions that were specialized for segmentation. This was when I was in Sebastian's lab, and I worked closely with another grad student Sriniv Turaga, who's at Janilia these days. This was actually, I thought, quite an interesting line of work and to some extent successful. So, for example, this was used in the reconstruction of the retina that was published from Mortz Helmstadter and colleagues in 2013. But if we go back to this graph, that only got us to, let's say, the first increment there, ten to the first, or, sorry, ten to the second. And since then, progress has been through a different method.
00:21:27.750 - 00:22:30.686, Speaker C: So what is the basic issue with this approach? So there's a number of things, but at a high level, you can say one thing is that it's not trained in an end to end way. You have these multiple steps which are doing different things. They don't really know about each other, and the whole thing has not been trained or optimized jointly for the final result. So how could we address that? So we decided a couple years ago with my group at Google to sort of revisit the kind of approach to this problem. And what we decided to do was pursue something we call flood filling networks. And conceptually, this is very different, actually, than the previous approach I described. The idea here is that you seed a neural network from a single pixel in the volume, and then it recurrently fills out the shape of just that one object, right? So one major difference here is that in the previous convolutional network approach, we would first do a pass over the entire volume predicting local geometry, but without knowing anything about the global structure for each object, sort of simultaneously.
00:22:30.686 - 00:23:16.880, Speaker C: Right? And then we would sort of accumulate those decisions into a global interpretation. Here we have a single neural network, which is just recurrently filling out the shape of a single object. And one way to think about this, although I wouldn't take it too seriously, is that it's a bit more cognitive, right? So if you have a human trace this data, they don't grab 18 pencils and start tracing all objects same time, right? They focus on one object at a time, and then maybe they have to switch due to some ambiguity and come back and so forth. But it's a little bit more of that style. And so what you're seeing here is the network architecture on the left, or at least a reduced form of it, and on the right is the 2d version of this process, where the yellow dot represents the current field of view, or the center of the current field of view of the network. And then the blue are the pixels that it's adding basically to its inference about the shape of one object. And of course, this is just shown in 2d.
00:23:16.880 - 00:23:33.352, Speaker C: But fundamentally, our datasets are three dimensional. So this is what this process looks like in three D. And so this is, I mean, it's a rendering on the one hand, but on the other hand, it's a literal sort of description of what this algorithm did in that it started from a single pixel and then grew out the shape of the object within the data set.
00:23:33.408 - 00:23:35.848, Speaker D: So do you take more than one slice at a time when you're.
00:23:35.936 - 00:23:37.832, Speaker C: Yes, this is all in 3d happening.
00:23:37.888 - 00:23:42.974, Speaker B: Yeah, in principle, you do it in parallel on different objects.
00:23:43.144 - 00:24:01.774, Speaker C: Right? So that's right. So we divide a large image volume into subregions within a sub region. We do one object at a time and then we stitch things together. So there does ultimately need to be both parallelization and of course you have to densely fill the object space.
00:24:02.434 - 00:24:18.236, Speaker E: So I'm not sure I understand the distinction. So you're saying instead of just contour tracing, you're like filling, is that the difference? In one case, you're drawing a contour and you're doing that slice by slice, and then you're matching contours that match for one object.
00:24:18.340 - 00:24:59.464, Speaker C: So in the previous approach, the fundamental concept was something called a boundary map. And so that was basically you infer a grayscale value for every pixel that labels its probability of being either a boundary or an interior. But boundary interior is a binary kind of designation, which says nothing about the object structure itself, right? So then you take that boundary map and you feed it to something like connected components or watershed or graph cuts or whatever, and from that you get distinct regions, right? So those are two separate steps here. The neural network itself is producing the final object, right? One at a time, but it's producing the final object. So I would say that's the most fundamental distinction.
00:25:01.924 - 00:25:04.944, Speaker B: In the flood algorithm. How do you avoid leaks?
00:25:05.304 - 00:25:34.928, Speaker C: Well, that's up to the training process, right, to optimize the neural network to avoid that, because that's, you know, so one of the nice things here is that the loss function now becomes very naturally related to the nature of the errors, because if you start leaking into a separate neuron, you're going to accumulate huge amounts of error in your loss function. And that was not the case with the boundary map, because if you just miss a single pixel and say, okay, it's not a boundary, whereas it should have been, that doesn't naturally relate or easily or obviously relate to its global effect on topology.
00:25:35.096 - 00:25:36.488, Speaker D: So what is the loss function?
00:25:36.576 - 00:25:52.924, Speaker C: So the loss function is literally just a pixel level accumulation of errors here. So if you miss a large part of a neuron because it's based on a single object at a time, that's going to incur a huge penalty. Does that make sense? Or.
00:25:55.944 - 00:25:57.456, Speaker B: So maybe you should say how this.
00:25:57.480 - 00:25:59.464, Speaker D: Is used in the training stage.
00:26:00.884 - 00:26:30.144, Speaker C: Right, so in the training stage, you know, it's kind of like this animation here because it's in 3d where, you know, for each object in our training volume, we have a ground truth reconstruction, right. And so the training phase is we seed the algorithm from inside some particular object, we let it fill it out, and then each time it completes one of these recurrent steps, we can compute the loss function against the ground truth object.
00:26:30.844 - 00:26:35.224, Speaker B: So what is the straining size and how many ground truth images do we need?
00:26:35.524 - 00:26:49.476, Speaker C: Yeah, so that's a good question. So you basically need tens of millions of voxels that are labeled, which sounds like a lot, but actually if you have the right tool, it's not that hard to get that much training data. Yeah.
00:26:49.620 - 00:26:51.264, Speaker B: How many people were doing it?
00:26:51.404 - 00:26:52.204, Speaker D: How many?
00:26:53.904 - 00:27:05.204, Speaker C: I mean, I guess so for the songbird datasets, the total amount of human time that went into volumetric tracing was I think on the order of 400 hours.
00:27:09.024 - 00:27:14.184, Speaker D: So how big is the convnet that you eventually use to do the 3d pulling?
00:27:14.264 - 00:27:31.554, Speaker C: So the convnet itself is not, I mean, it's actually a very simple convolutional network. It's just convolutions. It has no pooling or subsampling and that's like ten or twelve layers. But that whole thing is recurrent. Right. So basically there's a big arrow going from the output of that back to its input. And so that is the notion of recurrence here.
00:27:31.554 - 00:27:35.926, Speaker C: Yeah. How do you know if you made.
00:27:35.950 - 00:27:37.014, Speaker B: An error or not?
00:27:37.174 - 00:28:01.414, Speaker C: Yeah, so I mean, it's all based on human ground truth. Right. So for a particular region of the data we've had humans go through and create pixel accurate tracings of, of the neurons. So there's actually a number of things that go into doing this at very large scale that's detailed in a bioarchive paper. That's up. And this is in review. And also there's now open source code for this if you're interested.
00:28:01.414 - 00:28:35.514, Speaker C: This is another way to represent the progress where a few years ago for, I mean, this is the same neuron on the left and right, but just from different reconstruction pipelines where a few years ago the reconstruction looked like what you have on the left. So things were heavily split there were lots of things missing and so on. And on the right now, you have three, four pieces, which are basically correct, much closer to what you'd expect or want. I'll briefly mention there's some other approaches here for dealing with issues in the data. So these are real data sets. There's all kinds of issues. For example, you might have.
00:28:35.514 - 00:29:14.320, Speaker C: So here in the middle of this slice here, you see this shard, which is some piece of dirt that was imaged on that slice. And these are 10,000 by 10,000 pixels. Here, I'm just showing a very zoomed out view. So the question is, how do you deal with this in your pipeline? Do you detect something that's gone wrong? Do you just hope the algorithm deals with it? And so on. So one thing that we've been experimenting with are an approach called generative adversarial networks. This is a very new, reasonably new idea in machine learning. And the idea is you sort of pit two neural networks against each other.
00:29:14.320 - 00:29:51.302, Speaker C: One is trying to generate data, the other is trying to discriminate whether it's true data or fake data. It's actually a very, very interesting idea. And if you haven't read about this, I really recommend taking a look at it, because it turns out to be a surprisingly flexible and powerful way to train neural networks. And here's what you can do with it. So, I mean, let's say we had a slice of data on the left with some huge crack going through it, right? This is just the kind of stuff that happens when you're dealing with trillions of voxels of imaging data. Using this algorithm, we can kind of repair it so that it looks like what's on the right. So that's the output of the algorithm.
00:29:51.302 - 00:30:03.836, Speaker C: And the benefit of this is then we don't have to do a lot of extra engineering in our pipeline to deal with special cases like this. Right? We can kind of detect those cases, heal them using this type of approach, and then go on.
00:30:03.900 - 00:30:12.044, Speaker D: So it looks like it actually filled in a big, much bigger than the black part, because if you look at the mitochondria, they have cut off a fold in the middle.
00:30:12.124 - 00:30:15.708, Speaker B: It's a fold that sticks above the plane of the section.
00:30:15.756 - 00:30:16.716, Speaker D: A fold. Okay.
00:30:16.820 - 00:30:18.624, Speaker C: So it gets to see the newspaper.
00:30:19.284 - 00:30:29.808, Speaker B: And you push the two sides of the newspaper towards each other, and then there is a ridge, and that's why it's dark, because the electrons have to cast a shadow from the whole region.
00:30:29.936 - 00:30:32.336, Speaker D: You said that you can look at neighboring sections to help.
00:30:32.440 - 00:31:05.644, Speaker C: Right. So it would only really be able to do this if it could look at the neighboring sections. Right. Because otherwise it really would not have much basis on which to. I mean, you could still train it to hallucinate stuff, but you might trust the results less. Okay, so this idea, flood filling networks as turned into a pipeline, which is described in that paper, is what's really been driving the progress in recent years up to a number of millimeters now of what we call this expected run length. And that's, it's really changed what we can do just with pure automation.
00:31:05.644 - 00:31:19.768, Speaker C: So coming back to this set of things that we have to worry about as computer scientists in this field, storage, I'm not really going to talk about in detail. I don't know whether the engineering details are really so interesting here.
00:31:19.856 - 00:31:22.970, Speaker D: So you haven't seen about crowdsourcing, right? Is that still something that's done?
00:31:23.082 - 00:31:46.534, Speaker C: Why I. Crowdsourcing? Well, I guess there are people who are still interested in this. I think Sebastian is still looking at this and so on. I don't know. I guess there's a number of things to say about crowdsourcing. One is that it's actually a lot of work to do it. You have to create this whole online platform and then find ways to advertise it and get people interested.
00:31:46.534 - 00:32:08.368, Speaker C: So I think that's just a personal decision as to whether you want to work on that versus just improving the automated methods themselves. And in the long term, I think computers are going to solve this even beyond the extent they already have. And then computers are just no need for the crowd. Yeah, I mean, maybe not.
00:32:08.416 - 00:32:13.204, Speaker A: We're working with Sebastian and things are advancing so quickly with segmentation.
00:32:13.624 - 00:32:14.160, Speaker D: Exactly.
00:32:14.192 - 00:32:18.252, Speaker A: What you tell the crowd to do keeps changing. So the crowd is on hold.
00:32:18.428 - 00:32:44.470, Speaker C: I think experts are still interesting. So, for example, Genelia. I mean, they don't have crowdsourcing, but what they have is they have employees who essentially become experts in drosophila. And you're an enemy. And that is actually, I find very useful. Right. Because you can talk to someone at a very high bitrate about the reconstruction results and they can resolve very subtle cases, whereas with crowdsourcing, you know, it's hard to do that.
00:32:44.470 - 00:32:51.074, Speaker C: Right. I mean, the hard cases remain almost unsolvable using that approach. And the easy cases are going to go to automation anyway.
00:32:53.174 - 00:32:55.374, Speaker B: This is in general provision, I suppose.
00:32:55.494 - 00:33:29.394, Speaker C: Yeah. So storage, computation. I don't want to say a lot about this, except that there's just a lot of very sophisticated engineering that's been done, Internet companies and elsewhere, to handle these problems, and that's more or less directly usable for large scale data analysis problems in neuroscience and elsewhere. So I think it makes sense to take advantage of that effort. I will say something about computation costs. The approach I described is pretty computationally costly. It takes thousands of gpu hours to process a dataset.
00:33:29.394 - 00:34:21.748, Speaker C: But I think what we found in our project and other projects at Google is that once you've found the solution, there's lots of ways to optimize it. So even on just the software level, we've been able to reduce the amount of computation required by an order magnitude, and now there's specialized hardware that people are developing for neural networks that are also an order magnitude more efficient. So I'm not too worried that this is going to be a problem in the long term. Okay, so that's about it, I'll say, for methods, and now I'll talk about some of the specific projects these are being applied to, unless there's other questions about methods that I can address. Ok. Ok, so the first project is in the songbird, and so this is with Winfred and Jorgen at the Max Planck Institute, and Michael Fee is collaborating on the analysis and modeling front, so. And by the way, I'm not.
00:34:21.748 - 00:35:24.916, Speaker C: I'm going to talk about, like, I guess, two different systems, neither of which I'm actually an expert in, in terms of the modeling or theory. So I might have to call up Hila and friends to help me out here. But songbird is interesting because there's both sequential motor performance component as well as a learning component, and there's different pathways which have been anatomically identified within that system that are responsible for those different components. The area of the brain that we've been studying so far is what's called area x, and this is basically a basal ganglia analog within songbird. So HVC is the site of motor production or timing, at least in motor production in this system. LmAN is a source of motor variability, and then area x receives signals from all these different structures and is involved on the learning side. So a couple of years ago, they imaged part of this dataset, or, sorry, part of that volume.
00:35:24.916 - 00:35:54.412, Speaker C: So this is area x now, and this is actually a relatively small volume by contemporary standards. So this is around 600 billion voxels, 0.1 mm on each side. But, you know, we can do this now mostly automatically. Right. So after we apply the algorithm, there's under 1000 hours of human effort to finish the analysis of this data set, and 1000 hours, maybe that sounds like a lot, but it used to be 200,000. Right.
00:35:54.412 - 00:36:21.760, Speaker C: So it's really much more tractable. So what you're seeing here now is the reconstruction of processes. And then the gold studs are synaptic sites that have also been automatically identified. So here's synapse between two different neurites, and then we'll zoom out and see all the structures. This was actually the movie that was playing before. Okay.
00:36:21.832 - 00:36:24.512, Speaker D: Why did you pick area x rather than HVC, which.
00:36:24.608 - 00:36:43.004, Speaker C: So I'll get to that in a moment. Okay, well, actually, I mean, I will say that. So the reason area X was chosen a number of years ago was that HVC is relatively large. And to comprehensively answer the questions that you'd like to in HVC, you need a large volume, whereas there are specific questions which I'll get to in area X, which could be done with a volume of this size.
00:36:44.054 - 00:36:52.670, Speaker B: So I've heard that in some song birds there's neurogenesis. New neurons generated new songs each year.
00:36:52.822 - 00:37:02.854, Speaker C: Yeah, I don't know a lot about that. I mean, all I can say is that we won't be able to say anything about neurogenesis from this data, which is completely static in that area.
00:37:02.974 - 00:37:05.086, Speaker E: HBC has a lot of neurogenesis.
00:37:05.270 - 00:37:31.170, Speaker C: Yes. It's a different part of the songbird. Yeah. I mean, fundamentally, this is a static picture of the brain at one point in time. So even the dynamics of synapses are not captured in this data and never will be from this type of data, although people have found interesting ways to actually talk about those issues, even with static wiring diagrams. And maybe I'll get to that a little later on. So this was a volume that was taken a few years ago.
00:37:31.170 - 00:37:49.060, Speaker C: This is a more recent volume. This is actually 25 times bigger now. And there's something I want to show you here. So this is a video of this volume reconstructed. So this is 11 trillion voxels now. But I think one interesting thing is that videos are no longer compelling. Right.
00:37:49.060 - 00:38:00.424, Speaker C: I mean, I think Hollywood has sort of done too good of a job of creating videos of stuff. So I'm going to try to see if I can just load this volume up in real time.
00:38:02.914 - 00:38:05.494, Speaker B: I think you're competing with Hollywood pretty well.
00:38:06.994 - 00:38:45.874, Speaker C: The thing is, they've already done it, right? So it just doesn't. Sorry, 1 second. All right. Okay, so this is an 11 trillion voxel piece of brain, which is in some Google data center. And I think the really sort of interesting thing now is that. Why is this not. Oh, I see.
00:38:45.874 - 00:39:14.684, Speaker C: It's actually not the right okay, so, you know, I can just start to basically click on neurons here and see their structure. So I'll just show you some random stuff. So I'm clicking on cell bodies in case it's not clear what I'm doing, because those are the things which at this zoomed out view, are most obviously readable.
00:39:15.784 - 00:39:16.644, Speaker D: Viewable.
00:39:17.744 - 00:40:08.292, Speaker C: Then you can basically just zoom in and start looking at the structure of these neurons. And I don't know actually what you want to do with looking at them. But I will say though, that the few neuroscientists that we worked with, when they look at their system from this point of view, it sort of changes the way you think about things. And in some sense, I think finally the technology for neuroanatomy is getting up to the scale of the task, although I'm sure prior neuroanatomist said exactly the same thing. So anyway, that's just to give you a little demo of what really the technology is capable of now, taking very large volumes and interacting with them in real time at the level of morphology. But of course. Oh, and this tool that I'm using to look at the data, this is something called neuroblancer.
00:40:08.292 - 00:40:41.540, Speaker C: It's also open sourced by our group and is used by quite a lot of people in connectomics now and can be used in many other image contexts. It's worth checking out if you look at images. Of course, the real point though is that we can take reconstructions like this and convert them into an adjacency matrix which represents the connectivity. So this is what tensorflow might use to represent an artificial network, except this was some actual part of the bird brain here. Okay, so you have it completely densely reconstructed.
00:40:41.692 - 00:40:44.156, Speaker D: You're just showing us a few of the things you've reconstructed, right?
00:40:44.220 - 00:40:55.796, Speaker C: No, these two volumes have been densely reconstructed. So this is a putative connectivity matrix, pending debugging concerns. But, yeah, it's dense, but the dendrites.
00:40:55.820 - 00:40:58.124, Speaker B: Don'T seem to have a lot of spines on them.
00:40:58.244 - 00:41:33.644, Speaker C: So that's an interesting thing about this area of the songbird, is that it's not as spiny as what you would find in like a mouse cortex or something like that. And we don't exactly understand why that is, but it may just be a biological difference a little more. Are you saying like, I mean, are you saying there's like a very skinny axis which is the somas of the neurons in the volume? Well, you would normally expect an adjacency matrix to be square. The reason it's not here is because there's sparse connectivity. Basically that's biased between particular elements. So we just remove that part of the matrix. Okay.
00:41:33.644 - 00:41:36.544, Speaker C: Does that mean there's like projections, axons.
00:41:36.584 - 00:41:37.384, Speaker B: Coming in that don't?
00:41:37.424 - 00:41:44.764, Speaker C: Yeah, that's right. So there's a lot of axons that are coming in and only sending. Right. So we just kind of collapse the matrix into a. Yeah.
00:41:47.024 - 00:41:55.212, Speaker A: So in principle, you have access to size of the postsynaptic density, number of vesicles and so on. So you could go from a black white to a gray.
00:41:55.408 - 00:42:18.264, Speaker C: That's right. Yeah, that's definitely something which we're looking at. In some of the EM datasets, it's easier to characterize the size and number of SQLs than others. So in Clay's data, for example, that stuff is very easily visible because it's so high resolution. And some of these other data sets, we have lower resolution, so it's harder to measure some of those quantities.
00:42:19.444 - 00:42:21.972, Speaker B: Do you do the non neural system as well?
00:42:22.148 - 00:42:42.704, Speaker C: So the Glia are definitely here. The interesting thing about Glia is that they're hard to reconstruct. So they have very, you know, they're almost like adversaries within the segmentation process because they wrap around things and they have these sheet like morphologies occasionally. So we haven't focused on getting them right, but in principle they are there.
00:42:43.604 - 00:42:47.220, Speaker A: Do you have some automatic way to dissolve cell type?
00:42:47.412 - 00:43:30.624, Speaker C: Yeah. So this is something Jorgen Kornfeld and his colleagues at MPI have worked on, where as long as you have some ground truth, let's say, from light level reconstructions or something like that, you can start to associate shape with certain kinds of cell types and then do the same thing at the EM level. So let me just quickly go through this here. There's a very nice but somewhat complicated model of reinforcement learning in songbird. And basically it has to do with three different signals that come into this area x. There's behavioral context, which comes from HVC X projecting axons from HVC. Then there's reward signal, which is just dopaminergic release within area X.
00:43:30.624 - 00:44:52.192, Speaker C: And then there's what's called an eference copy. Okay, so the idea here is that there's a structure called Lman, which is generating variability in the production of song, and HVC is keeping time of where it is in the song. And then if basically the bird produces a variation which is better from its point of view, then three sort of simultaneous things can happen, or three simultaneous things have input into area X and it learns the association basically to strengthen the variation that was just produced. And this makes a very specific prediction in that you want to change the strength of the connections that come from HVC, but not the ones that come from Lman, because that's just an e frame copy, and it would make no sense to bias preferentially for just an e fring copy. So I'll skip the details because it really gets a little bit involved here. But basically the prediction is borne out where we find that actually the axons that come in from HVC almost exclusively terminate on spines, as opposed to ones from l man, which have a broader distribution, but are mostly on the shafts. And so at least this is consistent with the idea of preferential connectivity supporting this form of plasticity, which is related to the model for reinforcement learning in this circuit.
00:44:52.192 - 00:45:14.036, Speaker C: And so this is just an example, right? This is sort of a very specific prediction from an existing model, which now you can dissect in some reasonable detail using this kind of reconstruction. But really, there's a lot of other questions that one might be interested in. How much time do I have? 20 minutes. Okay. All right. We're fine then. Okay.
00:45:14.036 - 00:46:20.794, Speaker C: Second system here, Drosophila. So, for those who don't know, Drosophila fruit fly, very important model organism in biology, particularly in molecular biology and genetics, but increasingly in neuroscience also, there's now two different large scale images of the fruit fly. So one comes from Claire Reid's former student, Davie Bach, who's been at Genelia for a number of years and has now imaged the first complete Drosophila brain using serial section transmission electron micrograph technology. And it's really an incredible achievement to be able to go through the entire flybrain as a single 3d image and see all the different parts of it and all the different structures there. So that is one data set which we have segmented now. It's 220 trillion voxels, actually pretty large, and there's ongoing work to assist, use our segmentation to assist human proofreaders and pull out long range connectivity, all that kind of stuff, really. This is tremendously exciting.
00:46:20.794 - 00:46:49.780, Speaker C: There's just a lot of very basic questions you can ask about the organization of an entire brain that are now just there for answering. There's another dataset. So this is a different team at Genelia called fly EM. They've been pursuing a different approach. They're using focused ion beam scanning electron microscopy. So here's an example of some of their data. So this is, in most respects, the higher resolution techniques.
00:46:49.780 - 00:47:26.632, Speaker C: You get eight by eight by 8 nm isotropic data. So here are three orthogonal views of the underlying data. You can see they all look pretty similar, which is one of the benefits of this technique. There's no anisitropy from the imaging itself. They have an automatic synapse detector. So this is the classic t bar structure, which denotes a directed chemical connection in the fly brain. If you view it as sort of like a pseudo fluorescent label, it actually looks like the molecular stains for synapses that have been produced in these drosophila brains.
00:47:26.632 - 00:47:51.184, Speaker C: So this is our segmentation of that data set. So again, this was the flood filling algorithm. Flood filling recurrent networks applied to this data set. There's an example of one neuron being segmented within the data set. And of course you can then apply that to the whole thing. And again, look at sort of brain wide segmentation, although I should be clear, this is half the brain, they call it the hemi brain because it's half.
00:47:52.684 - 00:47:58.024, Speaker E: The alignment issues don't exist as much anymore. Right, because there's no slicing.
00:48:00.404 - 00:48:44.174, Speaker C: So that's not exactly true. So actually, let me just pause this while. Okay, well, this will play again, my answer. So actually for the focused ion beam SEM, what they have to do is they do something called hot knife sectioning, which is about every 20 microns, they cut the tissue into a different slab. And there's technical reasons they have to do this, but that creates a different problem in that you really have to make sure you don't lose any atoms when you do that cutting process. And you then have to stitch together those two faces of those two tabs, and they have like 13 of them for this hemi brain. So there's a different kind of alignment problem.
00:48:47.074 - 00:48:48.498, Speaker B: For how many flies?
00:48:48.666 - 00:48:51.534, Speaker C: So this is just one fly. This is 0.5 flies.
00:48:52.594 - 00:48:55.066, Speaker B: I just wonder how similar two different flies are.
00:48:55.130 - 00:49:02.140, Speaker C: Yeah, that's a very interesting question and we'll find out soon. Let's see. So these are the parts we know, right?
00:49:02.252 - 00:49:15.744, Speaker B: So in the visual processing part, they're identical. The eyes, the visual what? I mean, visual processing part, the optical, essentially. There's very little less.
00:49:16.404 - 00:49:16.692, Speaker C: Yeah.
00:49:16.708 - 00:49:18.412, Speaker E: And then there's recent work from Mala.
00:49:18.428 - 00:49:20.544, Speaker B: Muhammad, the age one set or something like this.
00:49:20.964 - 00:49:21.704, Speaker C: Yeah.
00:49:22.044 - 00:49:29.754, Speaker E: But then like one synapse upstairs, you know, downstream from the sensory periphery, already things become non stereotyped.
00:49:30.854 - 00:49:41.354, Speaker B: That's good, but it's not enough. So one question is, from the segmentation, do you automatically get the synapsis strength or not?
00:49:42.054 - 00:50:23.364, Speaker C: No. So the segmentation, it doesn't know about synapses. So they do have a synapse detector, so they find synapses. So in Drosophila, the situation is a little bit different. So it turns out that if two neurons are connected at all, meaning if they have at least one connection, then often they have more than one connection, and that number can be treated then, as a strength segmentation gives you. It tells you which things are touching, right? And so that's sort of a necessary condition for connection, but it's not sufficient. So you also then have to do the work of finding these very particular localized structures which denote a synapse itself.
00:50:23.364 - 00:51:28.174, Speaker C: Okay, so one of the biological questions which they're looking at here, and this is directly related to what Hila talked about yesterday, is something called ring neurons. So there are these neurons in Drosophila which encode basically a form of head direction. And so they're shown here in this green fluorescent image that was taken with light microscopy. And so this is a result published by Vivek Jayaraman's lab from Genelia. And what you see on the bottom right there is a two photon calcium image, or video of head direction. Well, you're seeing the video of the neurons, but the point is that the movement of the head is correlated with movement along what appears to be a circular axis of tiled neurons in this part of the fly's brain. So I think I'm doing a poor job of explaining this, but the idea is that there's literally a ring, an anatomical ring, within the fly's brain.
00:51:28.174 - 00:51:58.646, Speaker C: And the activity along that ring seems to correlate with the fly's internal sense of direction. So it picks an orientation based on a visual landmark that's local to its current environment. And then it has a coordinate system around where it's picked that landmark. And as it moves its head, activity along this ring actually moves also. So Hila discovered sort of topological rings in her data. Flies are simpler, they're more literal. So there's an anatomical ring.
00:51:58.646 - 00:52:06.412, Speaker C: Right. And so just to clarify it, within.
00:52:06.468 - 00:52:08.060, Speaker D: The ring, there is just a neuro pill.
00:52:08.092 - 00:52:08.664, Speaker C: Right?
00:52:09.044 - 00:52:12.196, Speaker D: I mean, there's no cell bodies in the neuropill. We're not looking at.
00:52:12.260 - 00:52:14.812, Speaker C: No, no. Yeah. This is neurons here. Yeah, this is just neuropill.
00:52:14.948 - 00:52:17.180, Speaker D: So it's a bump of activity in the neuropill.
00:52:17.252 - 00:53:10.666, Speaker C: Yeah, I mean, Drosophila, the cell bodies are kind of second class citizens, it seems to be. So let's see where is. So this is actually just pops out of the automated segmentation. This is the ring neurons, which are, you can see the ring right. And this is exciting for a couple of different reasons. I mean, obviously, one is the reconstruction result, but beyond this, there's basic questions about that physiology that you just saw. Right? So how is that bump of activity? How is it created, how is it maintained, and how does it move along with the fly's movement? And there's various network models that ring attractors which have been proposed, and we hope that we can basically resolve some of the details of what exact kind of model is creating the activity by looking at the connectivity within this region.
00:53:10.730 - 00:53:14.834, Speaker D: Now, you don't actually know if one of those neurons is inhibitory or not, do you?
00:53:14.994 - 00:53:39.404, Speaker C: So that's a bit difficult to tease out, although there's a lot of. So this is where the stereotype stereotypy of Drosophila pays off, in that there's a lot of work going on at Genelia to sort of independently figure that out through other assays. And then the hope is that we can project that data onto here, just because these circuits are often quite stereotyped.
00:53:41.664 - 00:53:43.204, Speaker E: Is this all the neurons?
00:53:43.504 - 00:53:55.444, Speaker C: This is a subset? Yeah, I mean, this is just a subset that looks sort of sparse enough to still see what's going on. In fact, not that many of them at all, actually.
00:53:55.564 - 00:53:58.464, Speaker A: But it's just these two nuclei. Well, the cell bodies.
00:54:01.324 - 00:54:05.820, Speaker C: Yeah, these are the cell bodies over here. So there's, like, four of them, and.
00:54:05.852 - 00:54:11.836, Speaker A: Then the cell bodies are arranged in these two nuclei. Or are there more?
00:54:12.020 - 00:54:26.440, Speaker C: Oh, yeah, the cell bodies up there. Well, those up there. I don't know if any of those are cell bodies. They may just be other neuro pillows.
00:54:26.552 - 00:54:27.924, Speaker E: Yeah, it looks like fluffy.
00:54:28.744 - 00:54:49.636, Speaker C: Yeah, it might be just neuropill. Yeah. I guess I should learn more about. Okay, so that's that. And of course, again, that's the initial question in this data set. But there's really quite a lot of things you might be interested in.
00:54:49.660 - 00:54:52.804, Speaker D: These caliber is. I mean, you've put how many years in?
00:54:52.924 - 00:54:53.420, Speaker C: Ten years?
00:54:53.452 - 00:54:56.824, Speaker D: More. Pretty much full time.
00:54:57.564 - 00:55:01.940, Speaker C: I work full time, sure. I don't know what the question is.
00:55:01.972 - 00:55:19.028, Speaker D: Exactly, but there's probably about ten others like you, right? And then add in all of the computing and infrastructure. I mean, are we talking about, you know, a $10 million project, $100 million project? What's the. What are we doing?
00:55:19.028 - 00:55:21.764, Speaker C: What do you mean? At Google or both?
00:55:22.344 - 00:55:28.764, Speaker D: Pardon? How much does Google put in and how much? What fraction of the total workforce for the whole world?
00:55:33.184 - 00:55:36.796, Speaker A: Here's a nondisclosure. I bet the rule to spend $20 million.
00:55:36.820 - 00:55:47.124, Speaker C: I mean, you know, it's a small team at Google that uses a significant amount of computation. So, I mean, it's. It's. Yeah, I mean, it's not a trivial investment, but I did not spend 100.
00:55:47.164 - 00:55:49.052, Speaker A: Million on this, so.
00:55:49.108 - 00:55:56.464, Speaker D: So Clay was just telling me he's estimating the world has spent 20 million. Is that a number you would probably less be willing to.
00:55:56.804 - 00:55:58.244, Speaker C: You're saying Google has spent 20 million?
00:55:58.284 - 00:56:03.924, Speaker A: No, the world, $20 million over the past decade on segmentation.
00:56:04.464 - 00:56:08.444, Speaker C: Oh, I really calculated that number.
00:56:09.864 - 00:56:11.684, Speaker A: It's not five, it's not 50.
00:56:12.304 - 00:56:35.842, Speaker C: Well, it's very hard to estimate because to some extent, we are also now riding on advances that are coming from computer science more broadly, which is almost uncountable at this point in terms of deep learning research. So I think it's hard to estimate what the dog is.
00:56:35.858 - 00:56:40.738, Speaker D: Okay, so let me put it that way. Suppose that I wanted to do another fly brain or half fly brain, right?
00:56:40.786 - 00:56:41.226, Speaker C: Correct.
00:56:41.330 - 00:56:45.162, Speaker D: And I say, well, you guys know how to do it. How much are you going to charge me for it?
00:56:45.298 - 00:56:48.374, Speaker C: I mean, you know, Terry, we're not going to charge you anything. You know.
00:56:59.304 - 00:57:01.124, Speaker B: How about help with our income tax?
00:57:01.784 - 00:57:28.334, Speaker C: Well, I don't know. Really? Yeah. Okay, third dataset, final data set. So this is recently, this is work with Jeff Lichtman at Harvard. So, something interesting is that my colleagues at Google, they're not sure about mice and birds, or, sorry, even flies. Mice or birds. Maybe they're interesting, maybe they're not, but at least they're still convinced that humans are interesting.
00:57:30.154 - 00:57:31.202, Speaker E: Until next year.
00:57:31.258 - 00:57:32.014, Speaker B: For now.
00:57:32.794 - 00:58:13.716, Speaker C: For now. So here's the data. So the way this was acquired, a 37 year old woman with severe epilepsy went in for surgery to address the epilepsy. And the shocking thing is that just to get to the site where they want to do the real surgery, they have to remove other parts of her brain just to get to the surgical site. And normally those are just thrown in the trash. But Jeff has a collaboration and has now gotten access to that tissue. And I think something that a number of groups are finding now is that human tissue, for whatever reason, stains extremely well for electron microscopy.
00:58:13.716 - 00:58:44.714, Speaker C: So a kind of black art electron microscopy imaging in general is how do you stain tissue so that it images well? And, you know, for example, Janelia had to put in a lot of effort to optimize it for Drosophila and so on. For humans, it appears you just grab some brain out of the brain and then, you know, stuff it in a beaker with some appropriate solution. And actually it turns out quite nicely. So what we're looking at here. So this is a single synapse in this woman's brain. Also interesting is she's still walking around somewhere, right?
00:58:45.734 - 00:58:48.382, Speaker D: I hope she knows where that piece of brain is.
00:58:48.518 - 00:58:51.878, Speaker C: It's fully anonymized. I don't think she has. I mean, I guess she signed away.
00:58:51.966 - 00:58:53.662, Speaker D: She may not know what happened to it.
00:58:53.678 - 00:59:14.064, Speaker C: Right. I don't know exactly what they signed, but, I mean, we definitely don't know who it is and are not supposed to. And, yeah, in any case, so this is a single synapse. You know, there's the presynaptic axon with lots of vesicles, and then there's the postsynaptic density. It all looks very nice. Probably some myelin around that axon on the bottom right. And now we'll just zoom out.
00:59:14.064 - 00:59:49.494, Speaker C: So there's a scale bar on the bottom left, and that's basically going to go from nanometers to hundreds of microns. And so one interesting thing you can see here is that as you zoom out, there's really no discernible structure. So, I mean, it's just interesting, really, right? I mean, all the relevant detail in the brain is at the nanometer scale. And now this is a few millimeters by about 1 mm. So this is a pretty large data set. So this is 500 trillion voxels now, which, on the one hand is a lot of data, but this is still only 110 millionth of a human brain. Right.
00:59:49.494 - 01:00:28.144, Speaker C: So we're quite far from doing anything like a whole human brain. And so, as I mentioned, the data looks very nice. It segments actually pretty cleanly. So I think there's, you know, I'm optimistic that actually we'll be able to say something about detailed circuitry in humans before too long. And just a technological note here is that they're using this so called multi beam scanning electron microscope that Zeiss developed. So normal electron microscopes, or at least SEMs, they have a single beam in detector. This one has basically an array of them to parallelize imaging, either 60 or 90 or so on to speed things up.
01:00:28.144 - 01:01:01.006, Speaker C: Okay, so just a final couple of slides here. So there's been a lot of progress, I would say, on reconstruction, and I think that's good. I mean, I think we don't really want to work on reconstruction forever. That's just a means to an end. So the question now is, what do we do with these things? And one obvious limitation of the current generation of studies is that they're all n equals one. We talked about this earlier. It's one fly, one visual cortex, and I think they will be valuable.
01:01:01.006 - 01:02:14.174, Speaker C: But if you look at genomics, a lot of the interesting power came out of comparative approaches, finding what's conserved across specimens evolutionarily and so on. So I think a lot of the interesting statistical and computational questions here will come from a similar setting. So, imagine we have different genetic perturbations of an organism and we know their connectome, or if we have different behavioral perturbations and we know their connectome, then how can we characterize the structural variability and understand how variation in structure leads to variation in function? And so I think there's just a lot of open questions in computer science which need to be addressed here. And I think it's interesting, again, the analogy with genomics, right, where something as simple as blast, which is basically, you know, decades of research and string matching that was kind of modified for genomics, had a big impact. And similarly, there's lots of ideas in network science and graph theory which can be applied here, but I think it's going to take a lot more than that to really figure out how to analyze these data sets. Okay, that's it. Just want to acknowledge collaborators and my colleagues at Google who contributed to this work.
01:02:14.174 - 01:02:14.914, Speaker C: Thanks.
01:02:21.714 - 01:02:23.074, Speaker D: So, this was fantastic.
01:02:23.154 - 01:02:38.130, Speaker C: I want to ask you how far away from getting, actually an adjacency matrix, perhaps weighted matrix for Drosophila? Sorry, weighted the adjacency matrix, the connectivity matrix for Drosophila. For the brain Drosophila.
01:02:38.162 - 01:02:39.854, Speaker A: How far are we from getting there?
01:02:41.094 - 01:03:16.180, Speaker C: Yeah, I think so. For example, with that hemi brain datasets, I think later this year we'll have a draft of the connectivity matrix for that data set, which is half a flybrain. And then we're going to repeat that process for the whole flybrain, which they're currently imaging. So over the next year or two, we really should have something, either half of it or close to the whole thing. For Drosophila, at least the strong connections. I think if you want to get to 99.9% accuracy, then that's going to take more human proofreading time.
01:03:16.180 - 01:03:20.624, Speaker C: But I think we should have something that's pretty reasonable within the next year or two.
01:03:23.524 - 01:03:42.834, Speaker E: That'S beautiful and very exciting. And so what? So just following up on that question, and maybe on Terry's question, too, so once you've got like one fly hemigra hemi brain adjacency matrix, or, you know, one, two heavy brains, how much longer to get the next one? Now that you've developed all the technology and will it be public?
01:03:43.614 - 01:04:24.150, Speaker C: Right. So I think what we're finding now is that the methods actually generalize to new data sets surprisingly well. There's some specific work we've been doing along those lines because, you know, I mean, the whole point of computer science is to automate things in some sense. And of course, you know, we don't really want to have to create new ground truth and optimize a method each time. But we're finding that actually things work pretty well across different data sets. So, especially if it's the same species and the same imaging method, the amount of additional research that's required should be very close to zero. Obviously, you need the computer time, but that's also getting better.
01:04:24.150 - 01:04:58.884, Speaker C: So I think the main cost at some point will really be the imaging, and the reconstruction will be a minor factor compared to that. In terms of making it public, I think that's up to Janelia. They put in whatever, ten years of continuous investment to try and figure out how to do this. So I think it's the standard plan where they're going to try and publish a paper that, along with the connectome, has some preliminary analysis and so forth. But I think definitely the idea here is that this should be resource which other people can start to try and understand.
01:05:02.904 - 01:05:39.584, Speaker A: So right now, people focus on these dense reconstructions of small volumes. And my question is, in order to get more at these communication questions between brain regions, are you thinking, or is there already some people, and I'm very naive, there, are there some people who are already doing. Doing not a dense reconstruction, but rather sparse reconstruction that can get, for example, at the ratio of incoming nerve fibers versus local neurons, that you get a connectome that can tell you something about.
01:05:39.744 - 01:06:23.964, Speaker C: Yeah, yeah, I mean, I think there's a couple of things to say about that, and I think probably clay has interesting things to say about that, too. So on the imaging side, if you want synaptic connectivity at all, there's no difference really, right. I mean, you just have to put in the work to image the tissue at that resolution, and you don't get to ignore certain structures. So the sparse versus dense doesn't really come into the imaging side unless you're willing to go to non synaptic to lower resolution. So if you just want to image axons, for example, and the extreme version of that would be doing something totally non invasive like DTI or something. At the reconstruction point of view, it's. I mean, for automated reconstruction, it makes surprisingly little difference.
01:06:23.964 - 01:06:40.056, Speaker C: The methods sort of end up segmenting all the data. Now, at that point, you can decide, okay, I'm only going to proofread some fraction of the structures that I'm interested in, but so far the mentality has been, let's just do everything. Clay, do you want to chime in?
01:06:40.080 - 01:06:51.264, Speaker A: Yeah. I mean, for long distance, you said it. For long distance, you have to do the imaging, and that's currently hard, although we and others have ideas.
01:06:52.764 - 01:07:04.268, Speaker D: So someone asked about strengths. So there's a strong correlation between the strength of the synapse and the size of the spine head volume. And is that something you can get at automatically?
01:07:04.396 - 01:07:16.542, Speaker C: Yeah, I mean, definitely, you know, if we go back to. If we go back to this image here, we can literally just sit here and count vesicles in that axon, and.
01:07:16.558 - 01:07:17.942, Speaker D: That'S also correlated with the strength.
01:07:18.038 - 01:07:32.234, Speaker C: Right. So I think Kristin Harris has done a lot of this beautiful work where she has simple parameterized models where you input the interface area and volume size and so on. And so with data of this quality, you can definitely get that.
01:07:33.074 - 01:07:38.962, Speaker D: So instead of just an adjacency matrix, you can actually have something much richer having to do with.
01:07:39.058 - 01:07:58.586, Speaker C: You can have a weighted directed graph with signs along that, not just the size of it, but also the location that matters. If you have an excitatory synapse that's on the very distal dendrites versus a synapse that's right on the Soha, that could have a different weight or effect. So, yeah, I mean, even just a square adjacency matrix.
01:07:58.730 - 01:07:59.494, Speaker D: Oh, yeah.
01:08:00.034 - 01:08:22.454, Speaker C: Yes. I think that's an interesting question of how to capture the geometry. Right. Because on the one hand, dealing with the full segmentation is quite cumbersome. On the other hand, just a connectivity matrix seems to ignore a lot of the interesting detail or potentially interesting detail. So we do have the geometry. I think the question is, I mean, how do you want to work with it? Right.
01:08:22.454 - 01:08:25.490, Speaker C: There's a question over here, or.
01:08:25.562 - 01:08:41.578, Speaker B: Yeah, this adds out of ignorance, but I wonder, wouldn't it be nice to have something that would leave a trace of activation so that you could have sort of a mark? Mark the circuitry that were lit up.
01:08:41.746 - 01:08:42.482, Speaker C: Right.
01:08:42.658 - 01:08:45.734, Speaker B: And trace that, selectively trace that?
01:08:46.234 - 01:08:46.562, Speaker D: Yeah.
01:08:46.578 - 01:09:47.744, Speaker C: I think, you know, Clay has sort of done, I think, most of the work along these lines where prior to doing this type of imaging, you first functionally characterize the tissue. You have the tissue intact and alive, and then you're showing its stimuli and you're measuring the calcium traces for specific cells, and then you go in and kill the animal and reconstruct just the cells that you're interested in. So I think for now, that's the most realistic version of that. Maybe in the future, people invent methods to. To leave more information at the EM level about the function of the neuron. Also, is there any genetic marker you could make that really looks really dark in the end that could label a certain population? Yeah, there have been EM GFP analogs that have been proposed, but I haven't seen them seriously used for connectomics yet. Every new thing you introduce into that pipeline just makes.
01:09:47.744 - 01:10:04.528, Speaker C: It just couples everything. It just becomes quadratically more complicated, the whole experiment, because it's hard enough to optimize these parameters as it is, and then you introduce something new anyway. So I think eventually it'll happen as sort of a version two of connectomics. But I don't know, Clay. Do you know of any?
01:10:04.616 - 01:10:27.686, Speaker A: Yeah, we and others that there are all sorts of genetically encoded things that can be made electron dense. But you said it, you look at the tissue the wrong way and these beautiful segmentations go south. So for Rev one, don't do anything fancy. Is em a faceful representation of the truth?
01:10:27.750 - 01:10:32.630, Speaker C: I mean, you have a beautiful synapse, but sometimes there's a shrinkage effect. For example.
01:10:32.702 - 01:10:32.862, Speaker D: Yeah.
01:10:32.878 - 01:10:57.420, Speaker C: Extracellular space has been one of the things which EM has not been traditionally very great at preserving. So. So I think classical estimates are something like 15%, 18% of the tissue is actually extracellular space, whereas often in EM that we observe, it's more like a few percent or 5%. So people have been working on methods to preserve extracellular space more faithfully. I mean, if you do an adjacency.
01:10:57.452 - 01:10:59.636, Speaker A: Matrix, it makes a difference, right?
01:10:59.780 - 01:11:15.514, Speaker C: Well, it shouldn't affect the adjacency matrix too much, because, remember, it's conditional on not just two things. Tough, touching. But also finding a synapse. Right. So you also need to find, you know, this PSD in the vesicles and that, you know, that shouldn't be affected by the amount of extracellular space.
01:11:16.734 - 01:11:21.794, Speaker D: So how much more work would it be to tag each synapse with an XYZ position?
01:11:23.774 - 01:11:26.006, Speaker C: I mean, that's. Yeah, I mean, that's.
01:11:26.030 - 01:11:28.754, Speaker D: No, in the data set you're going to make public.
01:11:29.054 - 01:11:47.524, Speaker C: Oh, yeah. I mean, so that's actually. So that's something we're thinking about now is sort of. What are all the different kinds of metadata? I shouldn't use that word. I hate that word. Annotations, which would be useful to people in analyzing this data. So there's obviously the geometry of every neuron.
01:11:47.524 - 01:12:12.872, Speaker C: We could simplify the geometry into skeletons. So that's kind of like the neuromorpho style representation, the locations of the synapses. Any additional information about the synapses, like we talked about, the spine head area or spine head length and so on. Yeah, I mean, it's an interesting question how to best summarize the data for theorists and modelers.
01:12:13.048 - 01:12:23.356, Speaker D: And you might not know ahead of time what's the most interesting metric that will show up. So as long as you can keep going back to the same data set, then I think you're okay.
01:12:23.420 - 01:12:43.948, Speaker C: I mean, I asked Marcus Meister this question, and he suggested just converting the reconstruction essentially to the neuron format, which people use in simulation, because in some sense, that is already a structured description of a neural network. I haven't really investigated enough to think about it or to know more about it. But I mean, in principle, I can.
01:12:43.956 - 01:12:47.988, Speaker D: See what medial access transform to get the skeleton. You've done that already, right?
01:12:48.036 - 01:12:48.836, Speaker C: Then you just need to know the.
01:12:48.860 - 01:12:50.444, Speaker D: Diameter and how you're home free.
01:12:50.524 - 01:12:53.344, Speaker C: Yeah, that kind of thing. We definitely already have.
01:12:54.604 - 01:12:59.116, Speaker D: Could Google support some interactive queries from the community?
01:12:59.260 - 01:13:00.932, Speaker C: Say the community, say we want to.
01:13:00.948 - 01:13:02.996, Speaker D: Extract this type of summary.
01:13:03.180 - 01:13:04.636, Speaker B: Do you think Google will respond to.
01:13:04.660 - 01:13:06.644, Speaker D: That and help the community to do that?
01:13:06.724 - 01:13:47.430, Speaker C: Well, I think our strength is really trying to solve sort of the deep technical problems, problems. I think solving the infrastructure, the persistent infrastructure and community issues. I don't know if that's the most sensible thing for us. I think the Allen Institute, for example, they have probably the best track record of figuring out how to do that. So I think we'd like to make sure we support that the field moves in that direction, because I definitely think these should be the kind of things that just like in genomics, anybody can download the data, propose a new method, and then publish it. So I think it's just a question of how we get there. Just tali.
01:13:47.430 - 01:13:47.926, Speaker C: Yeah.
01:13:48.030 - 01:13:55.154, Speaker B: So I guess this is all model free in some sense. I mean, you're not assuming any prior knowledge about the shape of neurons.
01:13:55.614 - 01:13:57.486, Speaker C: That's right, yeah, in principle.
01:13:57.670 - 01:14:16.724, Speaker B: So what do you think about can it improve things? Or if you actually use, for example, and Yan Sege and others are working on very detailed models of neurons, and if you know them, you can actually. Probably, if their models are right, it can improve your method. If not, you can improve the models.
01:14:17.224 - 01:14:52.682, Speaker C: Yeah, I think some kind of high level priors make sense. Like, for example, each neuron has only a single cell body, right. I think there has been concern about getting too specific because exceptions are not that hard to find. So, for example, people have talked about, okay, well, a neuron should never have a loop, right? It should never sort of connect back to itself in terms of two different neurites. But actually that happens not that infrequently. So I think, I mean, I think the strength of this approach is that we don't try and assume too much.
01:14:52.778 - 01:14:54.894, Speaker B: And there are neurons without cell bodies.
01:14:55.234 - 01:15:01.404, Speaker C: Okay, that's. Well, I said just two cell bodies are bad, right? Zero cell body is fine.
01:15:03.064 - 01:15:06.384, Speaker B: But there must be some bias, some prior due to the network itself.
01:15:06.424 - 01:15:34.948, Speaker C: Well, they come in through the training data. And so that is definitely, you know, the network will definitely try and respect the training data. We just try. I mean, but it's so local. Like, the form of training which is given to these networks is so local that it's hard to imagine that it would, you know, every time it reconstructs a neuron, it's going to look like a medium spiny neuron or something like that. That seems hard to imagine. But I think there's just other methods to verify what the result is.
01:15:34.948 - 01:15:51.174, Speaker C: For example, in drosophila genelia has a huge database of what single neurons look like from light microscopy. And so we can actually check how consistent the two datasets are. One more question I have.
01:15:51.834 - 01:16:30.354, Speaker D: This is kind of a comment, but also a question, which is that, you know, when you go to the public, and it's really important that we communicate to the public what we're doing. And it's true for Google, too, right? Google has to have a presence in the public. And I think that we're becoming more and more sensitive of privacy. And there's issues coming up. You know, you've got about Facebook recently, they're going to get creamed. It seems to me that Google should put more effort into letting the public know about Google Brain and what you're doing, in a sense that it would be something. It's something that's benefiting everybody.
01:16:30.354 - 01:16:56.694, Speaker D: We're being able to look at the actual structure of how the cortex is put together. And it seems to me it'd be the interest of Google to do that. And science in general, if that would be, if not publicized, at least have some sort of pamphlet or something that you put on your website that people can look for it with their students and so forth.
01:16:57.074 - 01:17:34.724, Speaker C: Yeah, I mean, that's something we've been talking about recently. So, I mean, there actually is more and more applications of computer science to other areas of science that are going on at Google. Like, for example, a few weeks ago, there was publicity, although the paper was prior to that, about using deep learning to identify novel exoplanets from Kepler data. There's a whole lot of work like that going on in terms of applying computer science to the frontiers. I think, in general, the bias there, though, is that they wait until there's actual results before they talk about it. So I think maybe we're getting to that point where it's worth saying something.
01:17:37.294 - 01:17:38.710, Speaker D: Just a quick thing over here.
01:17:38.742 - 01:17:52.126, Speaker B: You were a little surprised about the axon feeding back to itself, but that, in fact, is how we construct clocks in computers. We have a cell and feeds back to itself and bam, bam, bam, bam.
01:17:52.310 - 01:18:09.994, Speaker C: Right? I mean, recurrence is definitely, you know, in neural circuits in general. Seems, you know, is ubiquitous. But I guess the conventional wisdom was that at the single neuron level, there wasn't that much recurrence, but that was just the conventional. Okay.
01:18:16.494 - 01:18:17.774, Speaker A: Does neuron want take over?
