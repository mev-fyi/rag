00:00:00.320 - 00:00:08.954, Speaker A: Who's at the University of Michigan. Hand the floor to you. You've got your 15 minutes. Then there's the discussion.
00:00:11.014 - 00:00:52.354, Speaker B: All right, get this queued up. Yeah. So I'm. I'm Ben. I'm a postdoc at the University of Michigan in the School of Public Policy. I'm really excited to be able to share some of this work and continue this really great set of conversations we've had. So I'll be talking about a paper that was published last year or earlier this year, in computer law and security review, and sort of picks up yesterday, Helen kicked us off with this question of what are the notions about humanity that are baked into AI? And this paper, and this talk is really thinking about a related question of what notions about humanity are baked into our policies and regulations pertaining to AI.
00:00:52.354 - 00:02:25.380, Speaker B: And the central argument is a critical one, showing that our current approaches to regulating AI with respect to the role of the human is flawed and represents a pretty bad approach for ensuring values of equity and democracy. So the broad background of this work is thinking about specifically the use of algorithms in public policy. On the one hand, there's sort of these hoped for promises of accuracy, fairness, and consistency through the use of algorithms. But on the other hand, there have been lots of controversies around errors and bias and inflexibility, and sort of the doubts about whether algorithms are actually appropriate to be using and incorporating into high stakes decision making in government, which leads to this important policy question of how can governments improve public policy using algorithms, while at the same time preventing the harms that are often associated with these tools. So one of the mechanisms for answering this question that many regulators across the world have turned to is this idea of human oversight, of the decisions rendered by humans. So when you talk about human oversight, this isn't sort of any form of people being involved in overseeing AI, but specifically human oversight at the level of specific decisions about individual people. So to give a flavor for what these policies mean, one recent version of this comes in the European Commission's proposed AI act from last spring.
00:02:25.380 - 00:03:46.374, Speaker B: They write, for high risk AI systems, human oversight is strictly necessary to mitigate the risks to fundamental rights and safety posed by AI. Similarly, a recent canadian directive describes that decisions likely to have high or very high social impacts cannot be made without having specific human intervention points during the decision making process, and the final decision must be made by a human. So there's this idea that we can have algorithms that are providing recommendations and predictions as long as the final decision. There's a human sort of intermediating between the output of the algorithm and the actual decision, should this person be released before their trial, should this person receive benefits? And so on. And I was really drawn to the emergence of these policies as someone who had been doing several years of work in experimental work, looking at human algorithm collaborations. My PhD was in computer science, and the core of that work was a set of experimental studies looking at how people interact with algorithms in practice. And sort of the broad theme of these papers is that people respond to algorithms in idiosyncratic ways, and often ways that introduce new forms of errors and biases that wouldn't be detectable just from looking at the algorithm in isolation.
00:03:46.374 - 00:04:53.402, Speaker B: So I was particularly struck by the sort of differences between what I was finding in my research and what all of these policies were saying about human oversight being this great corrective for the flaws of AI. So I'll start by talking about what these policies actually say. And this project, I looked at 41 different policies from across the world. And these really break down into three different categories of human oversight that I'll sort of describe in order from least to, to most stringent in terms of how they describe the role of the human. So the first approach that you're probably all most familiar with is this idea of restricting solely automated decisions. The canonical example here is Article 22 of the GDPR, which writes that data subjects have the right to not be subject to a decision based solely on automated processing. And this was really the first strategy that was introduced, and I think the easiest to critique the sort of issues around the idea that, well, this introduces a very simple role for the human right.
00:04:53.402 - 00:05:46.704, Speaker B: The human can just be sort of a checkbox, very simple process that's not actually providing input. So I'll spend most of the time talking about these other two approaches that, in part have been responses to the relatively straightforward critiques to this first approach. The second approach is to require human discretion. In some ways, this is the same idea as the first one, but is putting the emphasis on the importance of human discretion. This comes up, in particular with a lot of the guides for high stakes risk assessments. For instance, in the guide to the Compass pretrial risk assessment tool, the guidance says that staff should be encouraged to use their professional judgment and override the computed risk as appropriate. The important thing is that the human is not just there at all, but that they're using their discretion to decide whether or not to follow the algorithm's advice.
00:05:46.704 - 00:06:43.154, Speaker B: The third approach is really the most recent one to have taken off, particularly in response to concerns about some of the issues of putting the human in a very culpable role, and that human not actually being able to use their discretion fully. So there's this idea of meaningful human oversight. And one example of this comes from the article 29 data protection working party providing guidance on the GDPR, specifically in reference to article 22, writing that to qualify as human involvement, the controller must ensure that any oversight of the decision is meaningful, rather than just a token gesture. Typically, this idea of meaningful oversight involves three mechanisms that are suggested. The first is that people should have the ability to override the algorithm. So that's quite similar to the second approach. The second is that people should be able to understand the algorithm.
00:06:43.154 - 00:07:39.184, Speaker B: So we want to have transparency and explanations to ensure that the human overseer actually understands what's going on. And finally, that people should avoid relying, avoid relying too heavily on algorithms. We don't want them just agreeing all the time with what the algorithm says. They should be incorporating other pieces of information and other considerations than just what the algorithm suggests. So the next question is to look at do human oversight policies actually work? There's a lot of many advocates of human rights and equity with respect to AI are really pushing for this idea of human oversight. But I was really drawn to this question of, does this actually work? If it doesn't work, then we end up in the worst possible situation where we've passed a bunch of regulations. So we think we've addressed many of the problems, but in fact, the human oversight as a mechanism is failing to address the concerns that we want to actually deal with.
00:07:39.184 - 00:08:58.614, Speaker B: So in the paper, I talk about two central flaws with these policies. The first being that human oversight policies are actually not supported by empirical evidence. So I was drawing a lot here on my own prior work, as well as much of the other experimental work from other HCI researchers, and some of the empirical evidence about how judges and other government street level bureaucrats are responding to algorithms in practice. And the broad takeaway from this is that our current empirical evidence pretty much calls into question all of the assumptions underlying human oversight policies. So the first major point is that simply giving people discretion to ignore or override algorithms does not necessarily improve outcomes across a wide range of domains. There's longstanding research showing that automated decision support systems can alter human decision making in unexpected and undesirable ways. The most notable example of this is automation bias, right? That even if people are not required to follow an automated systems advice, they tend to overly trust that system simply because they believe that it's right, and they reduce the amount of attention and scrutiny that they're otherwise putting into the decisions.
00:08:58.614 - 00:10:15.606, Speaker B: But then the broader issue that comes up with these predictive algorithms is that people are actually quite bad at judging the quality of algorithmic outputs and determining when to override those outputs. In experimental research, I found that people are often incorrect in terms of determining when they should override the system, which direction they should override in favor of, and often introduce new racial biases into the outcomes that that result from these systems. And even when you look at in practice, there's a number of studies that show that judges are often overriding pretrial risk assessments in ways that are leading to much higher detention rates than would be expected, and are also increasing racial disparities in pretrial release. And unfortunately, even when we turn to these ideas of more meaningful human oversight, the problems don't disappear, although these ideas seem to suggest, okay, well, maybe we can fix those issues with human discretion by making that discretion more meaningful. But that's actually really hard to do in practice if we think about the three different tenants of meaningful human oversight. The first is the idea of being able to override the algorithm, as I described in the previous slide. That often doesn't go very well.
00:10:15.606 - 00:11:31.644, Speaker B: The second is that, okay, well, what if we introduce explanations or transparency to help people better understand the algorithm? Most research shows that this doesn't actually improve human oversight. What it tends to do is increase human trust in the algorithm, even if the explanation is incorrect or even if the algorithm is incorrect. Explanations are often giving people increased trust that the system is correct even when it's not. And then our goal of having people not rely too heavily on the algorithm is really hard to achieve. The issue is not just automation bias, but also that people tend to overweight the factors that are emphasized by the algorithm. So if you give people making pretrial release decisions a risk assessment that emphasizes the likelihood of recidivism, then people are likely to make decisions that are based more heavily on that factor of risk relative to other considerations that they would be balancing out that recidivism concern with if they weren't given the algorithm. And so these first set of flaws lead to a second issue, which is that human oversight policies ultimately tend to legitimize flawed and unaccountable algorithms in government.
00:11:31.644 - 00:12:22.614, Speaker B: There's really two dimensions here that I think are quite salient. The first is how human oversight provides a false sense of security and adopting algorithms. If we look at some, some of the key cases, pretrial risk assessments, and others, you have legislatures and judges describing why it's okay to use these systems because of human oversight, essentially saying it would not be allowed if we didn't have human oversight, but it is allowed with the human oversight. So we have this false sense of security that we're protected from errors and biases because of this human. And then the second issue is around diminishing accountability for institutional decision makers, that because we're assuming that the human overseer is able to correct for errors and biases, the blame when something goes wrong tends to fall on those individuals. So we can't. We say the algorithm wasn't the problem, was not the algorithm.
00:12:22.614 - 00:13:20.124, Speaker B: The problem was that the human overseer didn't do the correct job of identifying and correcting for issues. So it essentially means that the algorithm and the institutional leaders and developers are kind of released from scrutiny and blame because we have this other. A quality control mechanism that we can point the finger to. So I'm happy to talk through some examples of how that plays out. But then the real question is, what's the alternative? Right? We don't want to simply say, well, human oversight doesn't work, so let's get rid of the human and just let the algorithm operate autonomously. The whole reason we want the human oversight is because we don't trust these algorithms in practice. So what I argue is that we need to move away from a framework that sort of thinks about this spectrum between human decision making and algorithmic decision making, and just tries to find the right balance and instead come up with a broader strategy for thinking about, when do we incorporate algorithms into decision making and how do we make those decisions.
00:13:20.124 - 00:15:18.238, Speaker B: So what I propose in the paper is a shift from this idea of human oversight to a broader model of institutional oversight, thinking about not just is there a human in the loop affecting these decisions, but how can we increase the burden on government agencies to better justify their decisions, to use algorithms at all and make those decisions democratic and publicly accountable, helping to shift some of that political emphasis away from this human overseer towards the agencies and the political leaders that are actually choosing to implement these systems in the first place, and put much more of the discussion and oversight there, rather than on this idea of just putting the algorithm in place, as long as there's oversight. So I think about this in two stages as a process. The first would be having a stage of really requiring a government agency to better justify why they want to use an algorithm, what sort of benefits they think it could achieve, and whether or not the human algorithm interaction could actually work as desired. And then subjecting this type of justification to democratic review and approval before an agency is actually able to use these systems in practice. So really shifting much of the discussion of oversight upstream to the larger institutional processes of whether algorithms are used at all. So I'll wrap up by just highlighting two lessons that come from this work, and two open questions that I think are particularly relevant for our discussions here. The first lesson is about our need for AI regulation, but it's really important to think about how do we develop robust regulation that is not sort of translating our values into rigorous socio technical regulation that's really grounded in how these systems work, how they're creating harm in practice and for both design and regulation.
00:15:18.238 - 00:16:39.374, Speaker B: What I argue this means is that we can't just be looking at algorithms as isolated technical instruments, but instead need to think about how they're being used in practice, how do people actually interact with these systems in practice, and really designing for those interactions, and regulating with an eye towards those interactions. So I think about this in terms of shifting from this concept of the human in the loop, which really centers the algorithm, to an idea of algorithm in the loop systems, where we're really taking the human decision making process as the primary mode of emphasis, and recognizing that the algorithm is just an additional input into that process. We're not focusing on the algorithm as the main thing that we care about. So two broader questions that come from this work. One, what is the political salience and effectiveness of the human as an antidote to fears about harmful AI? When we sort of want to talk about bringing in the human, is that an effective strategy, or do we risk essentializing and romanticizing the human as a tool of political rhetoric? And then more broadly, what are the principles for determining the appropriate balance or combination of human and algorithmic judgment in decision making? How do we think about the different values and benefits that each of these different modes bring to bear as their particular strengths? So thanks for your attention. Looking forward to the discussion and Connell's talk.
