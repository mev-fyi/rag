00:00:01.480 - 00:00:28.372, Speaker A: Thank you very much, Dick, and thanks for having me for this talk. Oops. Okay. Oops. Okay. So, title, I'm not sure about the title, what it has to do with the talk, but what I want to do in this talk is kind of give you a glimpse into algorithms research. And I'm going to do that by focusing on one particular problem and many different results related to this problem.
00:00:28.372 - 00:01:20.058, Speaker A: So the problem I want to talk about is the assignment problem, or maximum weighted matching. The input here is a bipartite graph we have here with weights on the edges, and the goal is to output a matching. So a set of edges that share no common endpoints, that maximizes. For the purposes of this talk, I'm always talking about maximization, the sum of the weights on the matched edges. So, for example, in this graph, these orange edges have a total weight of eight, and they provide, give you the maximum weight in matching. A special case that's very interesting and important, and I will probably talk about quite a bit. In the talk is the case where all weights are equal to one or zero, and so we represent the ones with edges, and the goal is simply to find the largest possible matching.
00:01:20.058 - 00:02:08.414, Speaker A: So in this case, it's the orange matching, and we'll use the term perfect matching when every single node in the graph is matched. Okay, so why did I decide to talk about this problem? I couldn't figure out what to talk about, so I decided to talk about this problem. And the reason is that it really is a treasure trove, and there's been immense amounts of work on this problem. So, first of all, it has endless applications, and I'll discuss a lot of those applications later in the talk. But for right now, you can think about matching men and women or jobs and employees and so on. It actually has a very glorious mathematical history, which I'll be telling you a little bit about. It's one of the cornerstone problems in the development of combinatorial optimization.
00:02:08.414 - 00:03:14.314, Speaker A: It's a showcase for beautiful, beautiful algorithmic results, and I can use it to illustrate some of the complexities that we're dealing with now in our special semester on algorithms and uncertainty. So my hope is to give you a little bit of a taste of most of the points that are on this slide. So here's what I'm going to do in the talk. I'm going to give a very partial history to touch on the glorious mathematical history of this problem. And then there's then going to be relatively short a technical piece where I'm going to present a gem and my criteria for a gem was that it had to be something beautiful and clever and elegant, but something I could more or less explain in 15 minutes. So I'm going to actually try to give more or less a complete proof for one result that I'll tell you about later. Then I'll talk a little bit about online algorithms in the context of matching and the assignment problem and some of the new questions that we're trying to address now that relate to a lot of the online market platforms.
00:03:14.314 - 00:04:13.904, Speaker A: Okay, so let me start with a very partial history, many, many missing references. Okay, so the history of this problem is over 100 years old. So some of the foundations for bipartite matching, and especially unweighted bipartite matching, were laid by the hungarian danish konig in terms of graphs and the algebraist for being in terms of matrices. Koenig wrote a pair of papers in 1916 that had many of the important results about bipartite matching. When I just say bipartite matching, I mean unweighted, just in general. And both of them proved a version of the so called marriage theorem that was rediscovered later by Philip hall and has come to be known as Hall's marriage theorem. So what does this theorem say? Suppose we have n men and n women, and an edge between a woman and a man means that they're compatible.
00:04:13.904 - 00:05:10.174, Speaker A: And the question is to characterize when it's possible to match, to come up with n marriages, to match every woman with one man or one to one matching. So the marriage theorem, as I'm sure many of you know, is that it's possible to do this if and only if for every set of k men, they're collectively compatible with at least k women. So, for example, here, these two men are compatible with only this one woman. And that's a proof that there's no perfect matching in this graph, no way to match everybody. So it's kind of obvious that this sufficient is necessary. But the marriage theorem says it's also sufficient. Okay, a very major development in the theory of matching was the hungarian evergari's proof in 1931 of a min max relationship.
00:05:10.174 - 00:06:04.764, Speaker A: So what he showed is that there's a set of numbers you can associate with the vertices in the graph. And suppose that these set of numbers cover each of the edges in the sense that if I sum the weights, the two numbers across any edge, their sum is at least the value on that edge. Okay, so among all such sets of numbers, let's find the set that minimizes the total sum. That's called the minimum cover. And he proved that the weight of that minimum cover is equal to the weight of the maximum matching in the graph. And so this is actually, some of you probably recognize that this is an instantiation of linear programming duality for this problem. And this was a tremendously important development that led to later algorithms, although linear programming duality itself had not yet been developed at this point.
00:06:04.764 - 00:06:44.454, Speaker A: But people were starting to get very interested in algorithms for very practical reasons. There were a lot of real assignment problems that people wanted to solve, and people were starting to understand that having a finite algorithm was not sufficient to be able to solve it. So, for example, there was a psychologist named Thorndike who was working on assignment problems for the purpose of classifying military personnel. And he went to mathematicians and asked them for help. He really wanted to solve these problems. He asked them for help, and they say, oh, well, this is a solved problem. Just try all the permutations and take the best one.
00:06:44.454 - 00:08:02.204, Speaker A: And he was kind of frustrated by this, and to quote him, rather cold comfort to the psychologist, when one considers that only ten men in ten jobs mean over three and a half million permutations. So maybe a mathematical solution, but it's not a practical solution. So another great person who worked on this problem was Julia Robinson, the great mathematician, who was later a professor at Berkeley, by the way. She was actually at the Rand Corporation in the late forties, early fifties, with a bunch of other great mathematicians, and she was working on the traveling salesman problem, and she came up with a cycle canceling method to solve the assignment problem that was actually a good, important precursor to some of the ideas that turned out to be very important. And she, although didn't prove anything about running time here, she thought it would be a reasonable approach in practice at the same time. Okay. Von Neumann and Dantzic were in the process of developing linear programming, and linear programming duality motivated largely by world War two applications.
00:08:02.204 - 00:08:46.840, Speaker A: Von Neumann, in the context of zero sum games, was doing this. And so at a lecture he gave in Princeton in 1951, he formulated the assignment problem as a zero sum game. I'll just quickly describe this formulation for the unweighted case, just bipartite matching. So we have some streets in Manhattan, let's say, and we have, at certain intersections, there are safe houses, and there's a cop and a robber. Okay? So the robber needs to choose a safe house to hide in, and the cop is going to choose one of the streets or avenues to drive down. And the cop wins the game if he drives down the street where the robber is hiding. A street where the robber is hiding.
00:08:46.840 - 00:09:39.584, Speaker A: So in this case, if he drove down second street or Fourth Avenue, since the robber's hiding right here, he would win the game. Okay? So von Neumann proved that the optimal strategy in this zero sum game for the robber is to actually find a maximum matching in the corresponding bipartite graph. So the bipartite graph is between the streets and the avenues with an edge. If there's a safe house on that corner, and find a maximum matching in that, and then hide, equally likely, at any of the locations in that maximum matching. And the optimal strategy for the cop is to find a minimum cover. So a minimum set of streets and avenues that will cover all of the safe houses and choose at random one of those. Basically, he reproved Evregari's theorem in the context of zero sum games.
00:09:39.584 - 00:11:06.234, Speaker A: Now, at the same time, Dantzic had recently developed a simplex algorithm and basically showed that the assignment problem is a linear programming problem with an integer optimal solution, and showed that using one of the fastest computers of the day, that you could solve a ten by ten assignment problem with a simplex method in 20 minutes. Okay, so the big breakthrough came in the mid fifties, when Harold Kuhn developed the first polynomial time algorithm. And this was a really cornerstone paper, and this is actually a citation for the paper that came some 50 years later. But this paper and developments at this time in integer and linear programming were very important for the sort of development of the whole field of combinatorial optimization and the development of primal dual type algorithms. So he called his algorithm the hungarian method, even though he was american. And the reason is that, as he says himself, the ideas for his algorithm were latent in the works of Koenig and Evergari. And he describes sort of realizing and coming up with some of these ideas, reading Koenig's graph theory book and seeing a footnote in that book, referring to Evergari's paper, which was in Hungarian.
00:11:06.234 - 00:11:51.528, Speaker A: And then he went out and basically knew no Hungarian, but he got a dictionary, and he spent two weeks learning Hungarian, translating the paper, and realized that he could combine the ideas in this paper with Koenig's ideas to get an efficient algorithm. And he describes how he solved several twelve by twelve assignment problems in under 2 hours each, and says, this must have been one of the last times when pencil and paper could beat the largest and fastest electronic computer in the world. So, okay, so. And then Munkresh, actually, I'm not sure how to pronounce his name. Munkhurst or Munkresh. I'm not sure. Okay.
00:11:51.528 - 00:13:03.884, Speaker A: Anyway, an American, another American, who observed that the algorithm runs in actually time, which is strongly polynomial time. So in time, that doesn't depend on the numbers on the edges, but rather just on the number of vertices in the graph. And there were numerous subsequent sharpenings of more and more efficient algorithms for assignment, including very important work by DeKarp and others. And there are so many papers on this topic that I can't even list them on the slide. An interesting postscript is that in 2006, it was discovered that the great 19th century mathematician Jacobi had actually written a set of manuscripts around 1836, published posthumously, written in Latin, that derived an algorithm similar to Kuhn's hungarian algorithm, hungarian method. And he was solving some differential system of differential equations, and it showed how to reduce the problem he was solving to the problem of solving an assignment problem, and then came up with his own algorithm, although even the terminology was not available at that time. The word matrix hadn't even been invented at the time that he wrote his paper.
00:13:03.884 - 00:13:57.766, Speaker A: But I should say that Kuhn took this very well, and he went around and gave a talk in 2007 called the hungarian method for the assignment problem, and how Jacobi beat me by 100 years. So he was a very good natured guy, it appears. I never met him, but. So that's my brief history. I will just also mention that there's huge strands of really interesting literature related to the assignment problem having to do with algebraic approaches and random assignment problems, and counting matchings, and economic literature. And I've put pictures of Berkeley professors, some of the Berkeley professors, who made really fundamental contributions on these topics. This is David Gale, who is a great mathematician and economist, who worked a lot on matching and competitive equilibria type problems.
00:13:57.766 - 00:14:25.974, Speaker A: So there was a whole parallel literature in economics that was very interesting. And this is, I mean, I assume you know the computer scientist, but this is David Aldous from the statistics department, who did a lot of great work on random assignments, assignment problems. And actually, yeah. Anyway, so I figure you gotta like the talk if you see Berkeley professors on the slide. So. Okay, so we've come to the technical, little technical part. I want to show you one really, really cool result.
00:14:25.974 - 00:15:21.874, Speaker A: And it's about matching in bipartite graphs. So there's an extensive history of important developments just on the basic bipartite matching problem. I'm not going to discuss any of these, except to observe that for, you know, this general bipartite matching problem, if n is the number of vertices and m is the number of edges, then we don't know of any linear time algorithm, linear in the number of vertices and edges for this problem. However, when we restrict attention to the special case where every vertex has the same degree. Okay, in this case, it's known that the graph has a perfect matching, in fact d perfect matchings. Then a series of developments led to the development of a linear time algorithm for this problem. So you could solve the problem of finding a matching in a regular bipartite graph in linear time.
00:15:21.874 - 00:15:57.254, Speaker A: So what I want to tell you about is a relatively recent paper by Goel, Kaprolov and Khanna. And in their paper they proved that you can't do any better than this if you're going to use a deterministic algorithm. Okay. But they were able to come up with a sub linear randomized algorithm. Of course, this is sublinear if the degree is large, larger than log n. Okay. And I'm going to try to explain this result to you because I think it's super cool and pretty can be more or less explained relatively quickly, I hope.
00:15:57.254 - 00:16:51.184, Speaker A: Okay, so the key idea is to use the key idea in finding matchings in bipartite graphs. Or a key idea is the idea of an augmenting path. So suppose I have a partial matching, then an augmenting path is just an alternating path of unmatched edges with matched edges that starts and ends with an unmatched edge. So if I have an augmenting path for a matching, let's say of size two, then now by finding that augmenting path, I have a matching of size three, namely switch from the purple edges to the blue edges. Okay, so an obvious way to find a matching is just repeatedly. Oh, I should also say that when there is no more augmenting paths, then the matching is provably of maximum size. So a natural way to find a matching is just to just grow the matching by repeatedly finding an augmentation augmenting path.
00:16:51.184 - 00:17:32.744, Speaker A: And the obvious way to do that is using some kind of search algorithm like breadth first search. If you do that, then finding an augmenting path will take linear time and that would yield an algorithm that runs in time, basically Nm. So you're augmenting every time getting one new edge. So the new idea that kaprolav et al. Had is to use random walks to do much better. So I'm going to explain to you how they go about constructing their random walk in order to get efficiently find an augmenting path. So it's going to be a series of steps till I get to the actual construction but here.
00:17:32.744 - 00:18:10.834, Speaker A: Let's go. So suppose you have a partial matching. First thing you're going to do is orient every matched edge up and all other edges down. So it's kind of immediate that if there's an augmenting path on the top graph then there's a corresponding directed path on the bottom graph. Okay, and the other thing I want you to observe right now is, you know, if we think about the, we have the vertices that are unmatched and the vertices that are matched. So in this directed graph the top unmatched vertices each have out degree d. The bottom unmatched vertices each have in degree d.
00:18:10.834 - 00:19:14.894, Speaker A: In my example, d is two because degree of every vertex is two and the top matched vertices have out degree d minus one and the bottom matched vertices have in degree d minus one. Okay, so now what I'm going to do is contract matched edges. So I'm just going to identify the two endpoints so there will be no more matched edge and leave all the other edges intact. So the result of this step is that now for the matched vertices, these super vertices, they each have both in degree d minus one and out degree d minus one. Okay, and now what I'm going to do is extend this graph to make it so that the entire graph, in the entire graph, every vertex has the same same in degree and out degree. So what I'm going to do is add two vertices, let's say s and t. S will have d edges to each of these vertices that had out degree d.
00:19:14.894 - 00:20:10.954, Speaker A: And there'll be d edges from each of the vertices that had in degree to t. And then I'm going to also have the same number of edges coming into t, which is equal to the number of edges going out of s from t to s. So the result of this transformation is that the in degree in this final graph at every node is equal to its out degree. Moreover, if there is an augmenting path in the original graph, for example, this one that we were looking at before, then there's a corresponding path in the new graph from s back to itself. So boom, boom, boom, boom, boom, boom. Okay, so what does this tell us? If I want to find an augmenting path in the original graph, what I can do is simply find a path in this graph from s back to itself. Ok, and I'm going to do that by running a random walk.
00:20:10.954 - 00:21:04.994, Speaker A: So how will this random walk work? From every vertex I'm going to pick a uniformly random edge to traverse. And since I want to find a path back to where I started, I just need to bound the expected time for this random walk to starting at s to get back to s. Okay? And I'm gonna call that expected time h sub ss, the hitting time starting at s to get to s. Now we know that for nice Markov chains, otherwise called ergodic, the fundamental theorem of Markov chains tells us that that hitting time is exactly one over the state stationary probability of the node s. What's the stationary probability of a node in a Markov chain? It's the long run frequency of being in that node. Okay. It's also a solution to this system of equations.
00:21:04.994 - 00:22:02.914, Speaker A: It's stationary in the sense that once you have this distribution over where you are, you have that same distribution at the next step. Okay? And this should be kind of intuitive, because if the expected time for me to starting at s to get back to s is h sub ss, then I spend roughly my frequency of being in state s is one over that hitting time. Ok, so now we know we can find an augmenting path in time that's roughly this hitting time. And all we need to do is figure out what this stationary distribution is to compute it. But this is where we take advantage of the fact that the in degree of every node is equal to the out degree. Because for such graphs, for such random walks, we can prove easily that the stationary probability of each node is its out degree divided by the total number of edges. Okay? And this is a proof of that fact.
00:22:02.914 - 00:23:00.664, Speaker A: But we're not going to go through that. So we now have the stationary distribution, which means we now know the expected time for this random walk to come back to where it starts. And with, indeed, with suitable data structures, you can actually find this augmenting path with that time. So now we just need to put this all together to figure out what the running time of the algorithm is. So we just said that each augmenting path can be found in time proportional to this return time from s to itself in an iteration where we have I edges in the matching. So here I is two. The out degree of node s is d times n minus I, and the total number of edges in the graph is just dn for the original graph at most dn plus d times n minus I out of s and into t and from t to s.
00:23:00.664 - 00:23:46.954, Speaker A: Okay? So this enables us to do a quick calculation and argue that the return time from s to itself is basically n over n minus I plus three at most. That and therefore our total expected running time. We're augmenting n times to get bigger and bigger matching. Our total expected running time is the time for each of those random walks, which is easily shown to be o of n log. Nice. Whether or not you got that or not, this is a really cool way of solving this problem very efficiently. It gives sublinear running time when the degree is large.
00:23:46.954 - 00:24:33.644, Speaker A: And so, in my opinion, this classifies as a true gem. Okay. But in case you didn't follow that part, you don't need to know anything about that for the rest of the talk. Okay, I'm now going to turn to something totally different. Okay, so, so far we've been discussing this traditional algorithms perspective, where you're given the whole input and you want to compute the whole output efficiently. In this case, the input's the bipartite graph and you want to find maximum matching efficiently. But this doesn't capture many of the real world settings where we want to solve problems, where in particular we need to start computing outputs before we ever see the entire input.
00:24:33.644 - 00:25:11.340, Speaker A: And applications of this sort of thing include memory management and scheduling, load balancing, dynamic data structures, routing, and also matching. Okay. Is an important setting where you often want to solve the problem, want and need to solve the problem online. So let's define a version of this problem for online bipartite matching. Let's just, for concreteness, think of a graph between women and men where an edge means that the men and women are compatible. So here's, I'm going to define a particular online problem. The men are all there to begin with, and the women show up one at a time.
00:25:11.340 - 00:26:15.298, Speaker A: And when a woman shows up, you learn who she's compatible with, and you need to make an irrevocable decision as to who to match her to. And the goal is to get a matching nearly as large as what you would have gotten had you known the entire graph ahead of time. So you're making decisions as you go without knowing the whole graph. And the way we're going to evaluate the quality of an algorithm is using something we call a competitive ratio. So for any particular online algorithm a, we're going to look at the worst case ratio over the size of the matching it produces and the size of the optimal matching. So what we have here is kind of a game between the designer, the algorithm designer, and an adversary who's trying to, whatever your algorithm is, the adversary is going to try to give you inputs that make this thing very, very large. Okay? So I want to show you immediately that if you're going to use a deterministic algorithm, you can't do better than 50%.
00:26:15.298 - 00:26:46.846, Speaker A: In other words, there's no online deterministic algorithm that can guarantee on every input that the size of the matching it finds is at least 50% of the optimal matching. So here's why. I mean, here's just a very simple example. So suppose I have woman one shows up and she's compatible with both men. Your algorithm is going to choose one of these edges to match her. Your algorithm will either match her with man one or with man two. But whatever you do, the adversary knows your algorithm.
00:26:46.846 - 00:27:25.880, Speaker A: So the adversary is just going to make sure that you screwed up. So if you matched her with man one, then woman two will show up, who can only be matched with man one. If you match her with man two, woman two will show up, can only be matched with man two. So you will get, your matching will be of size one, whereas in both cases, the optimal matching has size two. And I can repeat this, of course, any number of times to get a graph as large as I want. But the point is that your algorithm is not going to get better than 50% in this kind of situation. Now, one thing you could consider doing is randomizing to try to foil the adversary.
00:27:25.880 - 00:28:09.744, Speaker A: And this is one of the most beautiful and important techniques we have in theoretical computer science. We already saw it in the previous algorithm I described. But the idea is, of course, if, if your algorithm chooses at random which man to match woman one to, then the adversary, not knowing the random choice but knowing that you're randomized, is going to screw up at least half the time, right? So basically we'll screw up half the time. Exactly. And you will basically, with probability one half, you'll be able to get a matching of size two. With probability one two, you'll be able to get a matching only of size one. So now you'll be able to get 75% of the optimal algorithm.
00:28:09.744 - 00:28:52.104, Speaker A: So we can ask the question now, is it the case that randomizing produces, guarantees us this kind of very good competitive ratio, something like 75% that we're getting in this example. And the answer is no, it does not work well at all. In fact, it doesn't really work better than one half. To see, simply choose a random available neighbor to match the newly arriving woman to. So let me try to just quickly show you that. So here I have a bipartite graph, and the women are divided up into two groups of size n, l and l. The men are divided up into two groups of size n, r and r.
00:28:52.104 - 00:29:18.494, Speaker A: Prime. Every woman in l has an edge to every man in r prime. But the perfect matching in this graph is the cross matching, the horizontal matching. And suppose the women arrive in this order. Okay, so the first woman that arrives has n plus one neighbors. She chooses one at random. So her chance of matching across is one over n plus one.
00:29:18.494 - 00:30:04.264, Speaker A: The next woman to arrive has at least n neighbors up here, so her chance of matching across is at most one over. Nice. The third woman to arrive has at least n minus one neighbors up here. So her chance of I might have gotten off by one. But anyway, the point is that after these women in l arrive, because they have so many edges up to the top, they're very likely to mostly match to the top. And what you can show with a simple calculation is that just after the women in l arrive, only about log n out of the n, men in r are actually matched. What that means is that almost none of the men in r prime are available.
00:30:04.264 - 00:30:58.570, Speaker A: So that when these women arrive who can only match across, they're sunk. There's nobody there for them to match to. So an analysis of this example shows that really, if you just match at random, you're not really going to beat this one half asymptotically, this 50%. So in an ingenious result by Dikhar Bhumish Vazirani and Vijay Vazirani, they actually came up with a different randomized algorithm that gets a significant improvement over the 50%. And what is their algorithm? What they do is initially they randomly permute the menu. And then when a new woman arrives, they assign her to the highest ranked available man that she's compatible with. So this sounds almost the same.
00:30:58.570 - 00:31:28.186, Speaker A: But what's happening here is that these random decisions are getting correlated between the different women. And what they can show magically is that this gets you up to 63% competitive ratio. They can guarantee you always get at least 63% of the optimal. And also, this bound is tight. There's no randomized algorithm that can get better than 63%. Okay, so this is a really amazing result. And it was actually very prescient for a lot of work that's going on now.
00:31:28.186 - 00:32:05.090, Speaker A: And in the last ten years, for example, which involves generalizations of this problem, one of the examples is ad auctions. I mean, we know we're all being bombarded by ads. You know, when you do a search on Google, you get some ads along with your regular search results. When you go to Facebook, you see ads and so on. How do those ads get there? It's some kind of complicated matching process that's going on online. Okay. And so people have been considering, like Mehta Sabari Vasrani and Vazor Vazirani, generalizations of the online bipartite matching.
00:32:05.090 - 00:33:03.042, Speaker A: They're quite a bit more general. I'm not gonna really define this, but that kind of extend this online bipartite matching result and also are able to get in quite a more complicated setting. Also, at least 63% of the optimum revenue in this case. So I'm not going to explain this. I just want to say that that result of Dixon, Umesh and Vijay was a precursor to a huge body of recent literature that has been very, very important in online matching. And it's part of a beautiful and growing literature. This literature, for example, has led to the development of some very nice design methodologies for online algorithms based on the primal dual framework, breakthrough results on a variety of problems, and also kind of orthogonally.
00:33:03.042 - 00:33:46.866, Speaker A: I just want to mention a significant literature on online problems in stochastics settings. Okay, so the last thing I want to do for the last few minutes is talk about the new frontier. Okay? So I chose to talk about assignment problems because so many of the modern applications that we care about and are interested in involve assignments. So we're doing matching all the time. For example, eBay, Amazon and Etsy are matching buyers with sellers and products. Uber and Lyft are matching drivers with passengers. Airbnb is matching travelers with lodging.
00:33:46.866 - 00:34:29.686, Speaker A: Elant and odesk are matching employees with employers. We discussed matching ad slots with advertisers. We're matching organ donors with recipients. We're matching people looking for love with other people looking for love. So there are a lot of matching problems that are getting solved all the time out in the real world. And a question is, what can we, as algorithm designers, contribute to the understanding of all these and many other problems? So, I just kind of want to give, explain a little bit about what some of the new issues that arise in these modern settings. So, let's go back to the example of selling advertising slots.
00:34:29.686 - 00:35:41.684, Speaker A: So suppose you have a search engine with advertising slots for sale, and you have a set of advertisers, and the number on the edge is how much it's worth to the ad, or how much the advertiser is willing to pay to have their ad shown in this particular slot. So from the perspective of the search engine who's trying to make money, they would like to find a maximum weight matching here and basically just charge those advertisers those numbers. The problem is that these numbers are the private information of these advertisers. And these advertisers are not just going to tell the search engine exactly how much they're willing to pay, especially if they can get what they want at a lower price. So, for example, this advertiser here, if he knew that this was the exact scenario and that the algorithm was going to be maximum weighted matching, might want to say, well, I'm only willing to pay three instead of five, because then he'd get the same slot, but he would have saved $2 or whatever this represents. So what we have here is actually a game, a game between the advertisers. The advertisers are competing with each other for these slots.
00:35:41.684 - 00:36:26.998, Speaker A: They have to decide on their strategy of what to report to the search engine. They have to do that by making some assumptions about what the other advertisers are doing. And so this is a game in the sense of game theory. And, you know, for example, in the context of Google's search auctions, Google gets to design the game, the advertisers play the game. And the question is, how should this game be designed so that the result is as effective as possible, taking this strategic behavior into consideration. So this is just one of the important features of all these modern applications. The nodes in the bipartite graphs that we deal with in these modern applications are strategic.
00:36:26.998 - 00:37:14.822, Speaker A: You know, if you're a passenger using Uber, you want to get the cheapest ride possible in the quickest amount of time. If you're a driver, you want to get paid as much as possible. So there's a lot of room for manipulation and strategy on the part of all the participants in all of the examples that I gave earlier. Another really interesting thing is that the platforms, the designers of the systems by which this matching is taking place have the opportunity to incentivize behavior. One way they can incentivize behavior is by their choice of how information is disseminated to the participants. So, for example, in Uber, you don't get to choose which driver you want. They do the assignment in a centralized fashion.
00:37:14.822 - 00:38:16.714, Speaker A: You don't actually know anything about what's going on in that graph when you order your Uber. On the other hand, when you buy stuff on Amazon or eBay, then you get to see a lot of search results and you make the choice of who, what to match with. However, even there, there's a lot of control on the part of the platform and a lot of ways that they can affect your behavior, in particular in terms of how they rank search results. That has an important impact on how you choose your match. So there's a lot of interesting questions there about how to, how, and what information to disseminate. Of course, the second key way that a platform has an opportunity to incentivize behavior is via pricing. So for example, surge pricing in Uber guarantees that, or is an attempt to make sure that when there's heavy demand for drivers, the drivers will show up because they're going to get paid more.
00:38:16.714 - 00:39:02.954, Speaker A: Another key feature of these applications is very complex dynamics. So not only do we have nodes arriving and departing, we have incomplete information. We don't actually necessarily know the values on all the edges. Maybe we have some stochastic information. And another aspect of the complicated dynamics is that because individuals are trying to match themselves, they might try matching with a number of different possibilities, and might continue trying until they succeed. So the result is a kind of a queuing type phenomenon that happens as multiple people are trying to match with the same person. So this introduces some interesting questions into the mix.
00:39:02.954 - 00:40:03.590, Speaker A: And finally, we're collecting data at an astronomical rate, you know, both about the supply and demand patterns, about the values people have, but also about how happy people were with their matches. And that's something we get by ratings and reputation values on these various sites. So there's the opportunity to do very fine grained mining and actually have a lot of impact on these marketplaces. And the bottom line is, it creates a lot of new and challenging research problems at the intersection of algorithms, economics, machine learning, and stochastic optimization. And I guess in a nutshell, this is kind of the topic of our special semester, if I were to put it that way. So let me just end by mentioning one result that I was involved with. This is with all, all these wonderful people here we were looking at commodities of a temporal nature, where demand and supply fluctuate stochastically over time.
00:40:03.590 - 00:40:57.356, Speaker A: So things like cloud services or electricity markets. And we were interested in understanding how effective we could be with posted pricing. That kind of takes into account the variations in supply and demand. So the natural approach here, we were looking at a stochastic setting. The natural approach here is to set prices so that roughly, at any point in time, expected demand and supply are balanced. However, you have to deal with the fact that sometimes, because we're looking at expectations here, sometimes bad things will happen and there could be cascading bad events that occur in this kind of system. And basically what our result shows is that good, under some model, which I'm not explaining to you roughly, balancing the supply and demand is sufficient to achieve high system efficiency.
00:40:57.356 - 00:41:27.264, Speaker A: And basically this result is sort of a combination of pricing and queuing and online matching and a few other things. And I'm going to be talking about it at the next workshop at Simons, if you're interested. So I'm going to end here. I've sort of tried to give you a sense of some of the questions that we're trying to address. We're dealing with uncertain and dynamic inputs, opportunities for machine learning. We're making decisions adaptively. We have to deal with strategic behavior.
00:41:27.264 - 00:41:39.804, Speaker A: We have data sets that are too large to store, and we have the opportunity to shape online marketplaces. And I think this creates many, many interesting research opportunities. And I thank you for your attention.
00:41:49.884 - 00:41:51.204, Speaker B: Take some questions.
00:41:51.324 - 00:41:51.868, Speaker A: Sure.
00:41:51.996 - 00:41:52.944, Speaker B: Questions.
00:41:55.484 - 00:42:17.334, Speaker A: You mentioned beginning in the fifties. That's my understanding. I don't know. I mean, they were. Oh, go ahead.
00:42:27.914 - 00:42:36.254, Speaker B: And he pointed out the difference between exhaustive and, so he called it algebraic.
00:42:44.974 - 00:42:58.514, Speaker A: My impression is that the assignment problem was very important in the development of some of the ideas about the difference between polynomial time and exponential time, although the real formalisms and the real understanding came later, I think.
00:43:04.434 - 00:43:16.654, Speaker B: And they were satisfied to prove that their algorithms turned. So it wasn't common usage.
00:43:21.554 - 00:43:30.454, Speaker A: I actually got most of my information from Shriver, the big yellow books. So I'm not an expert on this history by any means.
00:43:31.484 - 00:43:39.940, Speaker B: Okay, other questions. Yes. You discussed a situation where a randomized.
00:43:40.012 - 00:43:43.624, Speaker A: Algorithm had a worst case of.
00:43:45.724 - 00:44:04.164, Speaker B: Runtime than the deterministic algorithm. Now, that's true. When you have, in the worst case, you're considering someone trying to beat you in the real world, you would have not data random or had some pattern to it. In that case, would it ever be better?
00:44:06.864 - 00:44:37.060, Speaker A: It might be. It's a difficult question for me to answer. But, you know, I think that one of the great strengths of worst case analysis is that you have this super strong guarantee so that in that rare event, maybe it's rare, I don't know, that things look adversarial. Inputs look adversarial. Your algorithm still works well. And in fact, I should say that these are worst case bounds on the running time of the algorithm. The algorithm actually might run much faster in practice.
00:44:37.060 - 00:45:04.124, Speaker A: So it also, you know, again, it could be that even this algorithm on the stochastic inputs is close to optimal for the stochastic inputs. But at the same time, I think we're trying very hard now to move beyond worst case and try to understand better, you know, how to think about questions so that we're both addressing the worst case and addressing the real stochastic nature of inputs.
00:45:10.304 - 00:45:11.744, Speaker B: Thanks for a fantastic talk.
