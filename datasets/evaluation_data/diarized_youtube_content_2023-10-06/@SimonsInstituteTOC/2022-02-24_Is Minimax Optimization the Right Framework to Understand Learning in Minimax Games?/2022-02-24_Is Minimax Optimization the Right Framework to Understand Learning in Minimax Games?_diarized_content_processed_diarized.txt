00:00:02.240 - 00:00:04.874, Speaker A: From Midland University of Montreal. Take it away.
00:00:06.174 - 00:01:03.548, Speaker B: Hello everyone. Thank you to, thank you Custis, for the nice introduction. And thank you to the organizer of this workshop and of this semester assignments to give me the opportunity to present today. Yeah, today I'm going to talk about going beyond min max optimization, and in particular taking into account the learning dynamics in adversarial learning. And I'm going to focus on two specific aspects. So in the context of Gans, I'm going to focus about the implicit bias of atom for gan training, which is basically the goal is going to be to go beyond min max optimization by considering the fine grained dynamics of the optimizers. And then there is going to be a second part about going beyond min max optimization by replacing the inner optimization methods by a sampling, the inner optimization loop by sampling.
00:01:03.548 - 00:01:57.964, Speaker B: And it's going to be in the context of adversarial training. So the first part is inspired by a joint work with Sami Artur and Wang Zi, and it's about the implicit bias of Adam for Gantry. So I think if you're here, you know what the Gan is. So I won't get into the details of this. I trust you. And basically what we care about is that this gan is a game between two players, the generator and the discriminator, where the discriminator basically wants to maximize this, like basically the minus cross entropy, while the generator aims at minimizing this payoff. And interestingly, yesterday, I think we talked a lot about hidden convex concave payoff.
00:01:57.964 - 00:03:01.526, Speaker B: And the interesting point to notice is that the payoff of Gan is actually hiddenly convex concave. So if you consider this payoff as a payoff of the function of the discriminator and the distribution of the generator, it's linear for the generator because it's just under expectation here. And it's concave because the log is concave, unfortunately because it's too high dimensional. I mean, we cannot solve this min max even though it's convex concave in the infinite dimensional space. What we do, as always, the solution is to parameterize our network. So we parameterize the classifier, our discriminator file and the generator, and we end up with a min max problem that is now unfortunately finite dimensional, but unfortunately non convex and concrete. And so how do we solve this? How people have solved this in practice, basically at the beginning at least, they solved it the deep learning way.
00:03:01.526 - 00:03:58.874, Speaker B: So they just computed the stochastic gradient for theta, update theta with their favorite optimizer. Compute the stochastic gradient for omega update omega and so on and repeat. And the two things I want to, the main thing I want to focus on today is this idea of like, you have to pick an optimizer. And there is a huge discrepancy between when you use, for instance, standard stochastic gradient descent and when you use other optimizers as ATM. Before that, maybe just a small parentheses. People use stochastic gradient descent, but we all know, and we have seen a lot today, that it doesn't work. If we take the, I mean, in general, if we take the simple bilinear game, stochastic and even not stochastic gradient descent, ascent diverge geometrically away from the solution.
00:03:58.874 - 00:05:09.962, Speaker B: So that's why there has been a huge line of work of people trying to propose new principled methods for gans in the context of Gans. So all these works got inspired from the way older literature than this. So, for instance, Kostis has a paper on optimistic methods for gans. And optimism dates from 19, I think 16, something like this, 80, 80, and extra gradient dates from 1979. So, yeah, but this paper was kind of like revamping the old methods in the context of games. But unfortunately, there is an inconvenient truth, which is that, at least personally, when I try to run these experiments on Gans, what I saw is this significant gap between stochastic gradient descent, ascent and atom. So this is basically the FID inception distance.
00:05:09.962 - 00:05:59.160, Speaker B: Lower is better. This is what matters here. And on four different data sets, I had tuned the hyperparameters. So the basically learning rates, most of it for stochastic gradient descent ascent and Adam for the ascent and descent direction. And I couldn't get rid of this huge gap between the performance of Adam and SGD. And that's a problem, because when I tried then to use the principal methods that I proposed in the past. So, for instance, stochastic extra gradient, when I try to apply stochastic extra gradient to gain training, what really happened is I could improve SGDA, but I couldn't close the gap between the performance of SVGA and the performance of alum in this context of image data set.
00:05:59.160 - 00:06:28.444, Speaker B: What kind of tuning did you try? I was tuning the learning rate of both the gradient ascent and gradient descent clipping. Standard gradient clipping and learning reschedule. So I did try the learning schedule. I'm not sure I tried the. Because in part also it's a common claim that you have that's recently disproved. Okay, I want to disprove it too. That's my goal today.
00:06:28.444 - 00:07:15.734, Speaker B: But. So I agree that, like, my goal today is to try to address this practical gap and to figure out why Adam is working that way and beyond the anecdotes, because it's my personal history. I want to show you some facts that are more about the community. If you look at the state of the art in image generation, from basically 2017 to almost 2020, it was all gans. After this, it was diffusion model. So it doesn't work anymore. But if you look at all the SOTA gans that has been released, all of them, I checked, all of them were using Adam as an optimizer.
00:07:15.734 - 00:07:43.430, Speaker B: Also, if you look at all this. Yes, sorry. So y axis is the FID. So the state of the art is always decreasing as a function of time, and x axis is the time. And so there is the GP paper, spectral normalization, begin constants, and some other improvements on the CIFAR ten data set.
00:07:43.622 - 00:07:47.230, Speaker A: So they, they use Adam, but they.
00:07:47.262 - 00:08:38.806, Speaker B: Changed the formulation of the gan, or improved the architecture. But in terms of optimizer, it always has been Adam that was. And if you look at the papers that, who proposed, let's say principal methods such as optimism or anchoring, like all these paper when they were trying to do some experiments on cifar, also plugged Adam on top of their techniques. And yeah, I'm part of this. And so, the question I want to answer today is really like, why does Adam. What does. Sorry, what, not why, what does Adam do that SGD does not? So a very, like a quick basically reminder of what Adam is.
00:08:38.806 - 00:09:21.238, Speaker B: So basically, we have this update where we have the step size. So we update theta t by using a specific vector with like a step size. Mt is basically a momentum, and we have a coordinate wise from normalization that depends on the squared value of the passive gradient. So yeah, it's what's called an adaptive methods. And there is many justification to explain why it works well in practice. But this is not what I want to focus on, because all these explanations are in the context of minimization. And what I want to deal with is the context of min max.
00:09:21.238 - 00:10:25.096, Speaker B: Moreover, in the context of minimization, as mentioned a bit earlier, there is some work that are trying to argue that Adam can be replaced. For instance, in the context of image classification, there is this very nice oral presentation from 2017 that basically detail how a well tuned SGD can outperform Adam. And there is the best paper that iclear 2018, that basically showed that there was a mistake in the convergence proof of Adam. So if we want to use principal methods, we may not want to use Adam on top of it because then we break or the convergence. So yeah, so what we're going to like to do is to try to deconstruct Adam in order to figure out what it has, what it does, what is the specific thing it does during the training. So basically it's just an update vector that has a direction and a norm that differs from its SD. Nothing crazy here, but.
00:10:25.096 - 00:11:07.300, Speaker B: So the crazy thing we tried is to try to disentangle the effect of the direction and the effect of the norm in the training. So we proposed two methods that we call Ada layer and Adadia, which basically corresponds to one of the optimizer. Use the norm of the Adam optimizer. You compute in parallel the SGD update and the Adam update. And for one, you use the norm of Adam and the direction of SGD. So you take your SGD update and you divide by its norm and multiply by the norm of Adam and you do the same for the other player. And conversely add idea, you take the direction of Adam and you multiply by the norm of SGD.
00:11:07.300 - 00:11:57.554, Speaker B: So now what we have is like two methods. One follow the norm of Adam and the other one, sorry, follow the direction of Adam and the other one follow the direction of SGD. And experimentally that's pretty interesting. What we get is that the one that doesn't follow the direction of Adam, so it follows the direction of SGD but has the same norm as Adam, is matching almost perfectly. And like, I mean within error bar, outperforming Adam. So basically what it says is like what we only care about, the first takeaway from this experiment is that it seems that it's only the norm of the Adam vector of the atom update that helps in performance. The second thing we noticed experimentally is that actually the norm of this vector was constant across time.
00:11:57.554 - 00:12:41.670, Speaker B: So if we summarize this, the method that was working almost as well as Adam was this one with the direction of SGD that was normalized and the norm of Adam. But if the norm of atom is constantly, we end up with this very simple optimizer that is basically normalizes it. Hypothesis is that this optimizer should work as well as Adam. And this is just normalizes it. This is the thing we tried. Basically we took the previous plot that was a bit unfortunate, and we added now some just layer wise normalized sDGA, normalize the CDL, where we normalize across all the hyper, all the parameters. Sorry.
00:12:41.670 - 00:13:34.168, Speaker B: And what we get is we are able, within error bar, to match the performance of Adam by just using this like mini batch normalization steps. So the conclusion experimentally is that normal SDGD is a proxy of, for Adam, or actually I prefer the Adam is a proxy for normalized SGD. And this can be a explanation of why it works well for games. Yes. So your experiment is for GDA, right? Yes. We can do the same thing for regular, right, well, sorry, minimization, just minimize this. Do we have the same behavior that like LR, what was the name of the tool? N normalized, is it still better in the regular minimization? So that's a good question.
00:13:34.168 - 00:14:12.522, Speaker B: We haven't tried in the minimization, but in minimization you can already outperform Adam with SGD. So in some sense between these two algorithms, sorry, but between the two algorithms that you have. Yes. Also something to notice that I don't talk about in the talk is in the context of Gans, people use atom with some specific hyperparameters that are different from the one in minimization. In particular, they put no momentum and change a bit the beta two. That is probably changing the behavior of Adam. So I don't think, and I think that's the point of this talk.
00:14:12.522 - 00:14:19.134, Speaker B: I think Adam in the context of Gans does something very different than what, what it does in the context of like standard minimization.
00:14:19.914 - 00:14:27.934, Speaker C: Just to be sure. In mean max randomization, they add or they are not adding more moments.
00:14:31.904 - 00:14:45.936, Speaker A: So is there any of like the hyperparameters that you know, arise out of your hyperparameter tuning? Do you see any negative momentum, positive momentum, like the step size required different, yes.
00:14:46.000 - 00:15:19.540, Speaker B: So for Adam it's like zero momentum. That is most of the time like the best at the rice. And it's always like a two times scale step size. So different step size for the generator and discriminator. I don't remember exactly which one is larger than which. And I think for novice you need, I'm going to say mistake, I think maybe, no, I don't remember if it's larger, but it's different from your step size. Has to be different.
00:15:19.540 - 00:16:09.680, Speaker B: Basically it makes sense from this slide because this guy is just like plugged into the step size. So the step size of Adam is, when you tune, is very different from the step size of what you normalize, probably larger. You should get larger step size for normalized LGD, but I'm not 100% sure. So yeah, as an experimental conclusion we have that Adam is a proxy for normalized SGD. And. Okay, why is it so great? First of all, I'm very happy because it's way much more principled. And why is it, why do we care? Because now we can try to analyze and understand what normalized SGD does in the context of gans, what is its implicit bias.
00:16:09.680 - 00:16:49.738, Speaker B: And it's also way easier to tune. And there is the room for improvement. Now, if we have like extra gradient, it's way easier to try to analyze normalized extra gradient than to try to analyze extra atom or optimistic atom. So yeah, I feel like it kind of. Okay, it kind of opens the room for a lot of like analysis and understanding. And so what we did is a preliminary analysis of the comparison of normalized SDGA versus SGD in the context of a gan. That is already quite challenging.
00:16:49.738 - 00:17:38.154, Speaker B: So we had a linear generator where basically what you want to learn is the weight vi, and so zi are sample or binary latent variables that are basically sparse, non negative and non positively correlated. So this is basically like what it summarized here. And you want to learn basically the weights here and the distribution that is the target is just a mixture of two mods, u one and u two, that are positively correlated. And basically your distribution is either, you can get your random variable is either u one, the realization. Sorry, of your random variable, or either u one or u two. So basically what you would like to learn with vi is these two vector u one and u two. And you would like to learn the right weights.
00:17:38.154 - 00:18:31.036, Speaker B: So basically that depends on these two random variable s one and s two. Interestingly, there is this work by Alan Su and Li that argues that these zi, that are sparse, non negative and non positively correlated can be seen as the distribution of the hidden layers of deeper knowledge. So this is what they kind of show in their analysis, that this is how like basically deeper hidden layers behave. I think. I think they prove like some like in this vein. The result, this is like a sparse coding model, basically. Yeah, so the Zi, or like Bernoulli sparse code.
00:18:31.036 - 00:19:12.514, Speaker B: Yeah, but yeah, I mean, it's basically like there are sparse most of the time you have only one zi, but you have ten. That's very unlikely to z. And so what we have in order to make it interesting is a two layer on our next with basically a cubic nonlinear. So it's cubic around zero, and then it's going to be linear at some point just to deal with the lip synapse. We don't want the lip sync of the discriminator to go to plus infinity. But yeah, it's roughly a cubic discriminator. And we want to basically study the min max, this nimmax objective.
00:19:12.514 - 00:20:22.482, Speaker B: And the two results we were able to show are the following. So basically the first result is that for sdGA, for any step size choice that is constant sdgsuffer from mod collapse, precisely that, we can only learn the direction u one plus u two. So you're never able to learn with your generator, with the weights vi, either u one or u two. You're only able to learn the sum which is like, I mean the average of the mean of the distribution, in some sense something proportional to the mean of the distribution. So this is kind of the statement that is like a bit more detailed and this is with like high probability. But yeah, the spirit of the statement is that we are not able to learn the right node. And basically if we look at what like some illustration, what happens is like the discriminator in any case is because of the cubic nonlinearities, is learning this direction, the direction of u one plus u two.
00:20:22.482 - 00:20:54.830, Speaker B: And once like the weights of the generator start to catch up, it's too late. And they are only, they only basically get the direction u one plus u two. So here is like u one. Here is u two in green. This is the decision boundary of the discriminator. And something you notice in Gans is that the generator only gets signal from the discriminator. So the only signal that this data points will get is, comes from the decision boundary of the discriminator.
00:20:54.830 - 00:22:07.176, Speaker B: And so if the decision boundary of the discriminator has converged to something that is linear and directed by u one plus u two, you cannot learn anything else. And that's kind of the spirit, like the spirit of the proof on the other side, if you have picked the correct choice for normalized SGD, you will learn the direction of both modes. So with the right, with enough iteration and enough steps, I mean the right step size tuning, what we were able to prove is that the generator is able to learn the direction of both mode, meaning that there is a non vanishing probability that basically we are close. We are, we are close. There is a non vanishing mass of the generated distribution that is close to un and u two. So the remarks here is like, this is proven in a very specific setting that doesn't, is not as general as you know, non convex, non concave gans and any architecture. It's for a specific choice of generator and discriminator, and it's only about the direction.
00:22:07.176 - 00:23:05.826, Speaker B: So one very weakness of our analysis is that we are not able to learn the correct UI, but we are able to only learn the correct direction. So we need to renormalize to get the result. Also, we have no guarantees that normal SGD learns the correct weighting, meaning that we have no guarantees that this probability is actually equals to the correct weight. So like, because the target distribution is a mixture of u one and u two, and there is no guarantee we recover the correct mixture coefficients. So the illustration is the following. So, because of the normalization of the gradient, the generator is able to basically learn very quickly something. And what end up happening in this like experiment is that as we can see, some of the mass goes on one mode and some of the mass goes on the other one.
00:23:05.826 - 00:24:27.544, Speaker B: Of course our theory cannot, I mean, we don't prove that all the mass is like on both modes, but at least we get a non zero on each of them. So, the conclusion of the first part is that Gans has some, in some sense very bad approximate stationary point and local equilibrium. And it seems that like normalized gradient could be one of the key for training. Gans and Adam in some sense was used as a proxy for normalized SGD. And so in some sense, if we want to study minimax optimization, and in particular convergence in the context of learning, sometimes convergence guarantees are not enough, because we have this very bad stationary point. And so we need to consider the dynamics of our optimizer in particular, like what is called, what I call here, and like what people like to call implicit biases of each learning algorithm is important. But in that specific setting, it's really about understanding the fine grained, having a fine grade understanding of the dynamics, really like figuring out in the context of Gans what is going on and why normalized gradient does something that's SGDA doesn't.
00:24:27.544 - 00:25:13.164, Speaker B: So in some sense. And if we want to do this, of course, like general non convex, non concave minimax optimization is too general. So, like the questions that remains open. I mean, of course we all, our analysis is in the case of batch of size one. But more interestingly, can we extend this result to like more complex generator and discriminator, and understand this, like in the general setting of Gans, maybe we could get have a stronger result in that case where we don't only learn the directions. And can we extend this idea to other like multi agent settings. Sorry about that.
00:25:13.164 - 00:26:15.692, Speaker B: Such as multi, multi agent, for instance, rental smart learning. Okay, so that concludes the first part of my talk. And now I have, I think, 15 minutes for the second talk, which is exactly what I need to talk about. Like basically a distributional, robustness perspective on adversarial training, where I basically want to go beyond this, like min max optimization by replacing the max inner optimization by some sampling. So in that work, we are in the context of adversal examples. So very quickly, what is another example? There is this very nice quote by Alexander Madrid that says that machine learning is amazing. It's a wonderful technology because it can make pig flies.
00:26:15.692 - 00:27:21.264, Speaker B: Pigs fly. And how do you do this? You take a picture of pig, you take your favorite classifier that classified this picture as a pig. You add this imperceptible transformation that depends on the classifier that is called adversal examples, and you get these new pictures classified with a 99% accuracy as a airline. Now, our goal in this work and in like, basically the robustness literature is to have like robust classifier, meaning that we don't want to have flying peaks. So we want to train basically a model that will be robust against this perturbation. And one of the main idea, one of the ideas to do this is to do what is called adversarial training, meaning that you train your model against these adversary examples. And so the point I want to make today is, okay, but when we look at this inner maximization, what we do is like, we look at the x tilde that doesn't come from the data distribution anymore.
00:27:21.264 - 00:28:17.484, Speaker B: So in some sense, what the classifier sees is a new distribution that is a perturbed version of the data distribution. And so, the way we may want to reformulate this maximization is by a maximization with respect to the distribution. And the question here is like, what is the p that we should set in order to have this equality between these two, to basically have the adversarial trending formulation. So, for a general p, this is called like distributional robustness. And I think we're going to have a very nice talk about this rate of terminal. It's a very old field that starts before 2000, and that there is some people that recently draw connections with adversary examples. I'm not the first one to notice that we can have.
00:28:17.484 - 00:28:22.976, Speaker B: Basically, there is a connection between these two formulation. Yes.
00:28:23.160 - 00:28:39.090, Speaker C: Just to be sure that they understand. It's the first time that I see that rig. You claim that you, I have an integrand. This is what I see. I have an integral, they have a maximum operator over a function. They can put out the maximum operator.
00:28:39.202 - 00:28:41.114, Speaker B: If you put the red constraint here.
00:28:41.234 - 00:28:45.094, Speaker C: I see. And the p would be a corvette.
00:28:46.234 - 00:28:48.690, Speaker B: And in that case it will.
00:28:48.882 - 00:28:49.842, Speaker C: Okay.
00:28:50.018 - 00:29:27.548, Speaker B: Yes, that's the whole point. Actually, the p I can do you the spoiler. The spoiler p is the vassal Steinbell. It's like the infinite versace table of size epsilon around the data distribution center that the data description. But it's a very good question because the next slide is giving you the details about this. So the novelty here is really trying to figure out what is the right p. So really making this tight connection about when like what is the p to get this equality.
00:29:27.548 - 00:30:14.714, Speaker B: And basically what we figured out is that there is a connection between optimal transport and infinite, infinite Wasserstein bold constraint. So in some sense you can see x as the result of a transportation of x by following this constraint, epsilon. And so in some sense, what you the p add, the intuition is that it shouldn't be transported. Yeah. More than epsilon with respect to a certain metric. So yeah, so the way we will prove this is like if you, if we don't, we want to transport x tilde not too far from x. So we want a transportation map that transport the data distribution to the adversarial distribution with the constraint that basically.
00:30:14.714 - 00:31:31.462, Speaker B: So we have this like transportation map that says who is transported to who and what we want as a constraint in order to satisfy the blue. The blue constraint is that the support is only constituted of the y's here that are the same, and x and x tilde that are smaller than epsilon with respect to this l infinity norm. So once we have this, we can realize that actually this exactly corresponds to what is called the infinite infinite verse ten distance. So it's basically the infinite Wasserstein distance with respect to the l infinity norm. And what we got at the end is this very nice equality between and basically distributional robustness with respect to this ball. Okay, but why do we want to do this? Because now we have this perspective that the adversary distribution is just the result of a transportation of the original data distribution. And optimal transport has some very nice tool and tricks with regard to adversarial transport.
00:31:31.462 - 00:32:41.454, Speaker B: And in particular, something very convenient often is to use the entropy regularizer in order to, when you do transport between two distribution. And so if we do this, if we add an entropic regularizer to our original problem, now we get a class form solution for the optimal adversarial distribution. So basically when we solve this maximization with an entropic regularizer, we get a class form that is basically this one. What it says roughly, is that the density at a example is proportional to exponential lambda, the value of the loss. The higher the value of the loss is, the more likely the higher the density of the adversary distribution is at. So it makes sense. The problem is, of course, it's harder to compute the normalization of this density, but we can basically simulate this distribution using longer Monte Carlo sampling.
00:32:41.454 - 00:33:53.806, Speaker B: And so what we do now, because we have this class form for the inner maximization problem, is to replace optimization that was trying to solve the inner maximization problem by a sampling step, where we will try to sample this adversary distribution. And why is it important and relevant? Because it leads to a very natural and practical implementation of adversarial training that consists on some slight modification of the previous methods. So basically now, because we want to sample from a distribution, we need to maintain this distribution. So we kind of have this perspective tells us that we should memorize the adversary examples that corresponds to each data point in between the step and keep them in memory as the current adversarial distribution. So this is what you have here. You initialize basically your adversary examples, and at each time step, you load your memory of the adversary examples. You compute your gradient, or the sign of the gradient, in this case, to update, to basically update your adversary examples.
00:33:53.806 - 00:34:50.098, Speaker B: And you use Langevin, so you follow the gradient and add some gaussian noise in order to kind of sample from this distribution. And then once you have this, you do your gradient descent step that are the standard updates of the model against what is the adversary data set, and you save what is very important to attacks. So the novelty here is that instead of each time step, you recompute the adversary example against your model. You have this adversary distribution that you slightly update as you learn your model. And the key insight is that because we had used this entropy regularization, we have that the function, the adversary distribution is a continuous function of the model parameter. So as our model trains, our adversal distribution slightly changes. And so we can do this alternative steps in order to update each other.
00:34:50.098 - 00:35:24.430, Speaker B: And we save a lot of time because now we only need one or two inner steps of Langevin instead of like 20 or 30 in practice to update our adversary. So, I mean, all this wouldn't be interesting if it doesn't work in practice. So, fingers are crossed. We have the first experiments that are mnist. So y axis is the, I think the adversary error rate. Yes, yes, it's. So, error rate is adversarial test error versus PGD 14.
00:35:24.430 - 00:36:19.984, Speaker B: So, on the test set, we compute adverse examples and we look at the robustness of the classifier. So lower is better. And so what we see is that, of course, because we do less step of the inner problem, it takes a bit of time to take off for the learning of the model. But at the end of the day, this is faster than PGD 40. But what I didn't take into account here, because here it's number of epochs, is the fact that each step is significantly shorter, because I only do one or two gradient steps for the launch line instead of 20 of us. So if I look actually at the time comparison, what I get is basically x faster improvement, because each inner step is like basically one gradient step instead of 50. And so at the end of the day, we get a significant improvement in terms of performance.
00:36:19.984 - 00:36:46.840, Speaker B: And this scales up also to CIFAR, where we get like ten times faster improvement to get the performance of the standard training against PGD 14. And yes, sorry, the y axis is the adversary test error against PGD 14, projected gradient descent, gradient ascent. Sorry, 14.
00:36:46.912 - 00:36:47.584, Speaker D: So that's change.
00:36:47.624 - 00:36:58.364, Speaker B: If you use for example, PGD 200. 200. I never use this one for evaluation. For evaluation, yeah, of course not for training. That's a good point.
00:37:00.204 - 00:37:05.972, Speaker D: The behavior. So for evaluation, it is better to use at least hundred steps.
00:37:06.148 - 00:38:20.064, Speaker B: Okay, this is not it. I think we can definitely do it. So what I can tell you is we were doing fast sine gradient method before and it was even more in our favor, the results. So I will try your PGZ one, but. Okay, so the conclusion for the part two is that we have provided this distributional robustness on adversarial training by like tightly, by defining what is the right distributional robustness constraint in order to formulate it as a, to formulate adverse alternative as distributional robustness. With the tropic regularization, we got a class form for the, basically this min max problem, for the inner maximization of the min max problem that allowed us to replace the inner maximum optimization by sampling. And basically the two tricks is basically, now we have to maintain an adversary data set, that kind of warm start the sampling, because this adversary data set is a continuous function of the classifier and we got used speedup.
00:38:20.064 - 00:39:04.772, Speaker B: Of course, there is two main caveats to this method. First, we need to keep an adversary dataset in memory, so we need to maintain it so it's more expensive. But at least in the case of Cifar and mnist, it's not a problem. And even in the case of Imagenet, we are working on some tricks to make it work. And it should, at least we should have something that fits in memory. I don't know if it's like will train well, but we should be able to have some proxy for the adversary dataset. And the second caveat is an interesting one, because it opens some questions, which is like, it's hard to deal with data augmentation, because now you cannot maintain an adversary example.
00:39:04.772 - 00:39:50.484, Speaker B: For each augmented example, you don't know, I mean, you don't know in the future how your example will be augmented, basically. So you have to figure out some tricks. But I want to point out that if you look at, like, the standard adversary training methods on CIFAR, it was trained with no data augmentation. It seems that using the preliminary experiment we do seems to indicate that data augmentation improved the clean test error, but doesn't improve that much the test error against adversarial examples. So, that's it for me. Thank you, everyone, and I will now welcome your questions.
00:39:58.024 - 00:39:59.604, Speaker A: Questions for good here.
00:40:08.364 - 00:40:52.074, Speaker D: I have a question on the second part. I think the distribution, like taking out the distribution to Washington, that is pretty straightforward. But why do you gain anything using the entropy regularization? Because when you add entropy regularization before you have a coupling between your x and x primes, it's a deterministic coupling. These are your adversarial examples and clean examples. But when you add entropy regularization, basically, no, you don't have a one to one mapping. So because you are increasing the entropy to get the closed form. So basically, one sample, basically, one clean sample can potentially be coupled to it is adversarial example, maybe some other example.
00:40:52.074 - 00:40:56.514, Speaker D: So why do you. What's the intuition that it should help?
00:40:57.454 - 00:41:13.284, Speaker B: That's a very good question. So, at least I have a theoretical intuition. We are currently investigating, you know, doing an ablation study, whether or not it works. Like it really helps in practice. But in theory, I think the good intuition is really the fact. It's what I put. Yeah.
00:41:13.284 - 00:42:19.962, Speaker B: Here is the fact that now the distribution is continuous, is a continuous function of the classifier f. And so while we are training f, the distribution is like slightly shifting to. So, for instance, you know, if you have an adversary example before the regularization, what you have is like an adversary example could be in a given corner. And while your function will change, like in a discontinuous way, the best adversary example should be in another part of the space, but with the entropic regularization at the discontinuity point, basically, we will have now a uniform. They would be, like, as much weighted in the distribution. And so we're going to mostly go from one to the, to the other. So it kind of allows, like, not to like to basically it kind of justified the fact that if we maintain and warm start the, the data set with the previously stored examples, they should, the next one should be close to the previous one.
00:42:19.962 - 00:42:33.834, Speaker B: So we don't need to do like a lot of steps. Actually we can prove it. It's continuous with respect to, I think with the kl. We can prove it.
00:42:40.854 - 00:43:01.770, Speaker A: So I have one question on this slide. So in the, so from, from lines it's hard, like too many symbols but like in line six to wherever, 1112. So where you do the longevant. So are you touching every picture?
00:43:01.882 - 00:43:04.394, Speaker B: No. Yeah, you are just touching a mini batch.
00:43:04.514 - 00:43:17.782, Speaker A: Excuse me, a mini batch. But like doesn't that mean that kind of like, you know, some, some pictures are like in some older, like they have not evolved enough and so like is that a problem?
00:43:17.878 - 00:43:18.438, Speaker B: I agree.
00:43:18.526 - 00:43:18.870, Speaker A: Yeah.
00:43:18.942 - 00:43:55.640, Speaker B: So I. Theoretically or practically? Practically it's not a problem. We know, you know, we run the algorithm. Yeah. The point is if your mini batch size is large enough, you, you have gone through, you don't do, you have gone through the whole data set in not that many like iteration. If you, if you look at the experiments for adversary training, I think this is, I think you have a very relevant point. If we were doing only one or ten passes about the data.
00:43:55.640 - 00:45:02.694, Speaker B: But in adversarial training, like at least in the context of mnIST and Cifa, like mnIST we do 100 epochs and actually we do. I stopped the computations. But like you can go way, way further, you can go up to like hundreds of epochs and so it becomes less irrelevant because we really have like we have passed through all this store data points like large number of times. But that's a very fair point that for instance in like imagenet, maybe it won't work that well because we may not be able to do like hundreds of epochs. Though there is maybe some. Yeah, some tricks to consider. Like for instance, we could use examples for each, like for some images that are closed and like use this one for all the images so they kind of cluster like the representative adversary examples in order to make a pass over it several times during the training.
00:45:04.404 - 00:45:21.564, Speaker A: I mean presumably the same issue. So kind of like the question you asked about, you know, incorporating transformations is related to that, right? Because kind of like there, you never.
00:45:21.604 - 00:45:23.740, Speaker B: See the exact same examples. For instance.
00:45:23.852 - 00:45:25.980, Speaker A: Yeah, like you may not see the same. Exactly.
00:45:26.012 - 00:46:04.074, Speaker B: So, but so what you can do in that case is like having, so there is a lot of data augmentation for which you can backpropagate through it. So you could have an adversary example on the image before doing the transformation and computing the adversary examples and adding the data, basically augmentation as a sort of new layer. So there is a trick for this one, but I agree that for very large datasets, it's a bit. Yeah, maybe we can try to cluster the images, but, yeah, that's working. Progress to scale up to, like, imagine it and even larger data sets.
00:46:05.054 - 00:46:24.344, Speaker A: Thank you. Any further questions? All right, so see you again. Wait, when are we starting? In ten minutes. Oh, I see. Again. See you in ten minutes. Thanks again.
