00:00:00.840 - 00:00:26.310, Speaker A: Thank you very much. It's a huge honor to be here. Today. I'm going to talk about information measure estimation and how to have some magical procedure to boost your sample size from end to end times login without conducting any more experiments. It may look weird, but later, hopefully you'll be convinced that it's exactly a right description of our results. It drunk work with my lab mate Karthi Wenkard and Yan Jun, who is still an undergrad in Tsinghua w e but interning our lab. He's so good.
00:00:26.310 - 00:00:56.652, Speaker A: Actually, we are not working for him and my advisor Saki Weissman Stanford, who was supposed to be giving this talk. But later we decided I'm going to come first to vet your appetite. What's the question? We start with the simplest one. Suppose I observe n samples id from a discrete distribution p with support size s, but I don't know its size. But I don't care about that size. I care about these two functional or two class of functionals, the Shannon entropy, of course, very important information theory. And also I care about this function f alpha, defined to be summation ph alpha.
00:00:56.652 - 00:01:39.040, Speaker A: Any alpha larger than zero has got many interpretations. And also, if you take the log and normalize a little bit, it's going to be the rhynian entropy which he men should talk about later. So we're interested in this not only because they are interesting in practice and also because they exhibit different levels of smoothness, because these functional each have a sort of a non smooth point at zero. We want to see how the estimation problem's difficulty varies according to level smoothness. So this question is very fundamental and it falls into general realm of questions where you observe some samples from certain distribution parameterized by certain theta p. But you don't care about theta, you care about the functional. So we need to first go to statistical literature and see what is known about that.
00:01:39.040 - 00:02:06.484, Speaker A: We start with Shannon entropy. Thanks to Cardiff for suggesting this picture. Then the most ambitious question we can ask is that I want s channel entropy. I give you n samples, what's optimal estimator? However, go to statistical you will find that in finite sample theory there's no consensus on what is optimal. There are many of them, but unfortunately for this problem, it's even worse for none of them. We can compute the optimal estimator. So basically we are really screwed up in the finite sample.
00:02:06.484 - 00:02:44.196, Speaker A: But actually it's well known in finite sample theory, if you ask the statistician, I have to tell you, in general, it's hopeless. So it has been at least observed 50 years ago that we need to go beyond this. We need to go to a zympotic. So we ask a question. Suppose what I want is the optimal estimator for entropy. When the number of samples go to infinity, when by the problem size s is fixed, what's the optimal estimator? This problem is also non trivial, but now is already classical is in the textbook. We define the quantity called empirical entropy HPN, where PN is an empirical distribution and p hat is simply the empirical frequency of symbol I.
00:02:44.196 - 00:03:12.692, Speaker A: In our other words, it's basically evaluates entropy functional at the empirical distribution. It's basic plugging approach can be shown to be maximum like the estimator seems to be quite good. Actually, we can show to be quite good. So using the modern symptomatic theory, we can show that its estimator is really good in asymptotic sense, even in the constant. So we can never improve it even in the constant in symptomatic sense. So it seems we can just go home because we can show it's already extremely good. Given this extensive efforts.
00:03:12.692 - 00:03:50.304, Speaker A: However, there's something missing. If you recall that we have actually completely give up in their finite sample theory and we have got something works really well in asymptotics, then what about I use MLU and it's finite? If it is also very good in some sense, now we can also go home, because this is what we actually use every day. There's nothing to improve. However, the sad news is that in general, we're going to show for a large class of problems, including all of them we discussed. It's going to be a really bad idea. It's bad to the extent that if you use this in practice, probably your application will completely diverge. But actually, if you use the best one, it's going to converge and the gap is really big.
00:03:50.304 - 00:04:23.252, Speaker A: So hopefully it's going to be interesting. And before talking about our optimal schemes, we need to discuss our evaluation criteria. We use the standard statistical decision theory framework. We denote by Ms all distribution with forces. We define a quantity called the risk function of suppose I use f hat. The risk function of f hat is defined like this. We define a quantity called the maximum risk, which basically is a maximized risk function over all possible distributions supported on s elements that basically quantifies what's the worst case performance of my estimator.
00:04:23.252 - 00:04:54.644, Speaker A: And also, given a quantity called the minimax risk, which is a maximum risk, you infomize over all possible estimators. In other words, it's it quantifies what's the minimum possible worst case performance I can achieve. And what I hope is that the maximum risk of our estimator is going to be at least close to the minimax risk. And then we should be really good. In this criterion, we use the following notation to quantify these two constant sequence. And now we are going to start analyzing this. So before we start to construct the optimal estimator, probably we need to analyze MLE.
00:04:54.644 - 00:05:25.868, Speaker A: But otherwise, how can we know how to improve that? So in order to analyze out risk, as Yihong suggested, we need to analyze both bias and variance. Now you cause the disk 101. The bias is defined to be the functional minus expectation of your estimator, and the variance is simply the definition of variance. What about the MLE? Now we compute that. We compute numbersymptotically the maximum risk of MLE estimated entropy. And there are two terms, as we said, squared by and variance. And actually it is very interesting to us because let's think.
00:05:25.868 - 00:06:10.152, Speaker A: If you fix s, let n go to infinity, you'll only be able to see the second term, because the first term vanishes at the rate in n faster than one over n. However, if n and s may grow together, actually the first term will dominate because the first term will not vanish if n is smaller than s, but second term will easily vanish if n is larger than log s squared. So then we make a conclusion. In general, for this problem, first of n s is the threshold for consistency. Second thing, that the bias actually is dominating and we need to reduce bias, or we give a theorem showing it's impossible to reduce bias is what we are going to do. So. But this problem is also extremely fundamental in statistics, because whenever statistics estimation, we want to either consider the balance of bias variance.
00:06:10.152 - 00:06:30.712, Speaker A: So let's first review what we know about reducing the bias. The general approaches. One idea you may come up to mind, which is Taylor series. Right? I have Hpn. What about I do Taylor expansion of HPn around the true point p, compute certain expectations and cancel certain terms. Correct certain terms. It was tried in 1950s and shown by Panisky.
00:06:30.712 - 00:06:54.212, Speaker A: That does not work. It requires steel s samples. Bad idea for this. And there's also an idea called jackknife. But this idea, you correct that, but you plug in empirical distribution into that p. That's in general idea of Taylor series expansion. You can expand, but you correct using your unknown estimate here article jackknife seems to reduce the bias in a zympic sense.
00:06:54.212 - 00:07:12.508, Speaker A: However, Shombezin not to work in this high dimensional setting. Bad news. There also idea called bootstrap. This is called bootstrap bias reduction. However, it's also resembling technique really interesting statistical methodology. And we recently showed also does not work. Really disappointing, but we don't give up.
00:07:12.508 - 00:07:41.130, Speaker A: We also think the idea called B's estimator. So what about we first design a prior and we compute the Bayes response with this conditional expectation. Under that prior, probably they will have a good frequency properties. We computed that and also it does not work. I still don't give up. I think of another idea, which is what about I first use the direct line prior to smooth distribution, so called Laplace smoothing. Then I plug in the entropy functional and we should also does not work.
00:07:41.130 - 00:08:09.578, Speaker A: It's really bad. Actually there are many more in the literature, I'm not exhausting all of them, so it's really frustrating. We look at this question, but later there is a recent breakthrough by valiant. Valiant showing that the exact phase transition for entropy has made is s over log s. It means that it sufficed to take n much larger than s over log s to achieve consistent estimation. Now all the methods we analyze, but some of them are hard to analyze, cannot do it. But we look at slightly detail at their approach.
00:08:09.578 - 00:08:38.770, Speaker A: They essentially have two ideas. One idea is based on linear programming, really cool idea, but has not yet been shown to achieve the minimax rates. In other words, cs term is called dependence on epsilon. It means that although I know n much larger than s over log s, x converts to zero. But how fast is not clear. Second thing achieves this, but only works for n, not too big, but we want to make sure that whenever my n ss I want it to be optimal. And also it's not clear how their approach can achieve minimax theories.
00:08:38.770 - 00:09:08.930, Speaker A: For other functional such as f alpha we consider because we also think as a general mathematical theory we should be able to deal with very large class of functions. So we ask a very ambitious question. We would like to find the systematic methodology to improve MLE achieving minimax series for very wide class of functions. And we think it might be easier actually than dealing with only a single function. We have to say this problem is still widely open, but later we are going to discuss what we have done. So we are thinking about the reason why Taylor series fails. The reason why bootstrap jackknife fails.
00:09:08.930 - 00:09:37.858, Speaker A: We think the reason why it fails is that for example, Taylor series fails because Taylor series is a local expansion idea. We know that Taylor series only quantifies a functional in a local sense, but our question is a minimax estimation. We take the worst case over all possible cases. So its very uniform criterion. But you use a local technique. So in some sense its not really compatible with the criteria. The reason why bootstrap and jackline may feel is that theyre so generic ideas.
00:09:37.858 - 00:10:04.982, Speaker A: We proposed a really hard minimax estimation problem, highly dependent on the functional. However, if that works, it means that some generic idea, without considering the functional form f, suddenly miraculously succeed. We think that too good to be true. So probably we think you start with first principle. Consider simplest setting, since we want to reduce bytes. What about I write down the expression of bytes and see what looks like. So I take the simplest case, x binomial random variable, which means s is two.
00:10:04.982 - 00:10:25.516, Speaker A: Suppose I use whatever estimator gx to estimate fp. I write on a bias. It's nothing but this. And we found that suddenly we can draw two conclusions. The one conclusion is that only polynomial with order no more than n can be estimated with a bias. Because bias doesn't mean this bias is zero for all p. But that immediately implies f.
00:10:25.516 - 00:10:58.968, Speaker A: P equals that guy. But that guy is polynomial with order no more than n. Second thing, that the bias actually corresponds to a polynomial approximation error, which means that probably if one item to reduce bias, it suffice to design the polynomial g, such that it approximates that FP really well. This idea actually not proposed by us in 2003. And Paninstri proposed to try to solve this problem. Suppose try to minimize this supreme of a bias g, and try to find a polynomial g. Try to find the coefficient g such you can minimize that, because that's essentially minimal bias you can ever have.
00:10:58.968 - 00:11:33.044, Speaker A: And unfortunately, it does not work. It's very disappointing. We feel this idea is so good, how can this not work? But later we thought that there is a simple reason explaining why it does not work. Because as you remember, at the beginning, we said, hey, Lacan told us as angle to infinity, ML is really good. It means that when angle is infinity, whatever estimator you give me, it has to be similar to MLE, which similar to plugin. However, will that scheme as angle to infinity give me something similar to MLE? It is not very clear. Actually, we can show it far from it.
00:11:33.044 - 00:12:16.818, Speaker A: So basically, it simply has violated already existing tag conclusions in the destiny to fail. But then we think the idea is still very good, because it clearly captures what is a bias. And best polynomial approximation has been introduced by Yihong. And it's basically the polynomial that minimize the maximum deviation of function. And then that polynomial here's our approach for all the functionals we consider with the decomposed region into two for each amputation frequency p hat. If p hat is smaller than log n over n, we declare you operating in an unsmooth region. Because the function we consider p log p or ph to alpha as zero probably is non differentiable or is not even times differentiable.
00:12:16.818 - 00:12:30.446, Speaker A: We think it's bad and we replace that functional. We simply throw that function away. I think this function too bad. I replace that by the polynomial. So that function will just disappear. It's a we replace with bad polynomial with order log. Nice.
00:12:30.446 - 00:13:05.434, Speaker A: Then we apply unbiased estimate of that polynomial, because as we discussed before, any polynomial with order no more than n can be estimated with our bias. We estimate that for that part, smooth is easy. We seem to plug in actually, but turns out you still need to do certain correction, which is something we are still trying to understand. Now, a simple methodology we apply to any function we consider and turns out to achieve maximum risk. The best results then. But you may ask me the following question. I understand that you need to cut the threshold, because you think your functional.
00:13:05.434 - 00:14:06.174, Speaker A: The problem of the bytes mainly contribute by the point near zero, but why is a cut off login over n? I understand you need to do approximation by approximation log n this stuff, but you need to understand this to do it. It is paper by Sergey Bernstein in 1938 best approximation certain function. We need to use these hardcore mathematics to finally solve this. Here it's time to acknowledge that these ideas really inspired by pioneers in the field for the final pioneers paper by Lepzki Nimorowski in spunk in 1999, who used a trigonometric polynomial approximation to deal with nonparametric settings. Then they had a very interesting paper by Tony Sai and Maclean eleven that deal with our norm estimation and realize that it's going to be really nice important to replace trigonometric polynomial by polynomial. And later, after we submit our work, we realize Lee Hong and student Peng Quen independent realized the idea best polymer approximation to the Android problem and devised a really nice technique in the lower bound, which he has just explained. But there's something more to be said.
00:14:06.174 - 00:14:33.452, Speaker A: What we showed is also something that this methodology essentially has very deep root in the statistics methodology. And actually it's a duality with shrinkage. And here many experts on nonparametric estimation. In short, it means that the shrinkage is designed to reduce the variance and our methodology is designed to reduce the bias. So whenever you have something in practice, either on to reduce bias or you want to reduce variance. Now you have both methodologies. Now here are some mathematical results.
00:14:33.452 - 00:14:54.142, Speaker A: We don't have time to discuss too many details. So f alpha, remember is this. And here is l two risk moe for all class of functional. We considered, hopefully still remember the first one. This entropy we showed before, there are two terms. Whenever there are two terms, the first term corresponds to a squared bias, second term corresponds to the variance. And we can see that they are.
00:14:54.142 - 00:15:28.700, Speaker A: We actually obtained both upper lower bounds of this. So it's an exact characterization of maximum rates of MLE. And you can easily see that when alpha is larger than one, actually the rate does not depend on s. So basically means really easy to estimate. But now we ask, what's the minimax rate? How does minimax improve or MLe by how much we think this? So you can just see that for the maximum rate MLE, usually only bytes is dominating. You pick out the dominating term and you replace that n with n times log n. You replace that n with n times log n.
00:15:28.700 - 00:15:46.176, Speaker A: You replace that n with n times log n. You replace that n with n times log n. Here you cannot do, because one overnight is already the best. Even s is one, so you can never do anything. This is already the best. But actually, after you take the log, if you estimate really entropy, you still need to replace nv slogan. So here's our methodology.
00:15:46.176 - 00:16:23.576, Speaker A: We think we've discovered very interesting phenomena we call effective sample size enlargement. It means that for a very large class of problems, the performance of the minimax read optimum estimator with n samples. Essentially that with mle with n times log n sample is precisely a logarithmic enlargement. We also talk about these things in the sample complexity perspective, which is more common in Cs community. For entropy, MLE achieves s and the optimal s or log s. First shown by valiant variant f alpha is s one over alpha, which actually, you think alpha is less than one. So s one over alpha is actually super polynomial, super linear.
00:16:23.576 - 00:16:56.788, Speaker A: It's really hard to do. And here, what you actually do is to slightly improve that. You have logarithmic factor alpha larger than one. Everything does not depend on s, so it's actually a lossy compression of our general results. Okay, then we would like to start more bubble entropy estimator, because probably something that people want to use in practice. First of all, we analyze, we emphasize just one realization of general methodology, whose generality we'll still discuss later, because we found this problem very fancy, very difficult, and we also achieved the minimax rates. And we also need to emphasize agnostic to the knowledge of s.
00:16:56.788 - 00:17:30.683, Speaker A: So it's interesting. 1 may think that you need to first known the knowledge of s to estimate entropy, but actually we show it's not necessary. So basically by estimating knowledge of s, the support says as E. Hongshou is a very difficult question. But actually for the entropy estimation, it's easy because you don't need to do that difficult part and also easily implementable. Recall that the only non trivial computation we're gonna do is best polynomial approximation based order log n, which grows really slowly. We use the following nice software developed by Nick Traverson's group in Oxford University.
00:17:30.683 - 00:18:01.472, Speaker A: Then they compute the 500 order with half a second, and in empirical performance we all perform with every available entropy. Estimator based are really a lot in the literature and our code to be released this week. But now you can send an email and we can send you the code. You can play with that. But however, after we show this to some statisticians, we usually get many constructive criticism, which we really appreciate. But they say that we may not use this meter unless you prove it's adaptive and we own. What does that mean? What do you mean by proving adaptive? Yeah, let me give you this example.
00:18:01.472 - 00:18:46.252, Speaker A: So what do you have shown? What you have shown to me is that the maximum risk of your estimator is nearly the same of the minimax risk over all estimators. But I do really think you take the worst case over all p. Doesn't make any sense. Why? Usually you say, oh, there are applications where s is large. However, usually one case as is large, is in language modeling. Let's consider chinese language, chinese language, already known as the available characters, at most at least 80k, really large number. However, if you consider an author's work, do their distribution on their character really like worst case over all possible distributions supporting 80k? No, the most frequently used symbol, at most five k.
00:18:46.252 - 00:19:25.100, Speaker A: So basically you actually know something, the distribution, his style, but actually you don't know exactly. So there's that. What about I do the following? Suppose I already know a priori, my entropy is upper bounded by h, which is a small number, which is much, much smaller than log s. I already know it. And what I want is that you construct me an estimator such that your maximum risk search over this set, which is a set of distributions supporting s elements, and also whose entropy is very small. I want the worst case over that set to be similar to the minimax risk over z set, which means that suppose I already know the p's in that set. I designed the best estimator.
00:19:25.100 - 00:20:03.072, Speaker A: So basically I want estimate of doing that. But here band of these two other questions, but I don't know this h, right? In general, I may have something, but I don't know exactly. Probably I take the worst case that h is still log s. But they ask a question, can estimator satisfy all the requirements, all of these for all s and all age, without knowing any s, without knowing h? Is that really possible? We found this a really big challenge because it means that it's really not clear whether we can really do it. But we lack challenges. So we showed it is adaptive. So recently we characterized the completely is a minimax risk of estimation of entropy.
00:20:03.072 - 00:20:24.754, Speaker A: If you take a suprema over z set for any age, and we show that they are essentially in any set, in any region and just up to a universal constant. And then the first part is very similar to what we had before. The second part is roughly satisfied. But it's nice we see the corollary. They're slightly confusing. It hardly interpret. I need to agree.
00:20:24.754 - 00:21:04.858, Speaker A: What's the corollary? We imply that if epsilon is at least h over log s, which might be very small, remember that we are considering case that h is very, very small compared to log s. Then it requires n as one minus epsilon h over h samples to achieve the root mean square arrow epsilon. But let's consider suppose because h over log s is very small, I take h will be a constant forever. For example, epsilon also a constant. What is one minus epsilon over h is one minus delta. So basically here, the number of samples I need is no longer a logarithmic improvement over s. It's s one minus delta, which is very, very small.
00:21:04.858 - 00:21:36.698, Speaker A: So basically it means that if I give you a prior knowledge of the entropy h upper bound by small h, it really helps a lot. But then you may wonder, what does mle mean? Probably MLE still requires s. Actually, it's not. The MLE still requires slightly more than that, still a slightly logarithmic factor. So you see. Oh really? Then in this case, if I give you the upper bound, that two entropy is no more than h, even the MLE can be easily sublinear, because this time it's sublinear. It's just a logarithmic factor.
00:21:36.698 - 00:22:07.310, Speaker A: So they immediately say that here still the log n to n times log n for performance per phenomenon still true. We find this very intriguing because this is already no longer a question we investigate before. But this general phenomenon still holds true, which is really hard to understand. But we found that we think this kind of phenomena can be more popular and more general than what we investigated. However, then we discussed all of these results with some pure mathematicians. They also read some constructive criticism. They feel that, oh, kind of cool.
00:22:07.310 - 00:22:28.550, Speaker A: However, that's. Is that really general? I want general. Yes, exactly what does general mean? I mean that all the functional you can have considered, you have one discontinuous or non differential point. Only one point for one point. I understand your methodology. Your methodology seems to say that you cover the one point by interval, but 1d interval for sure. I know that.
00:22:28.550 - 00:22:50.268, Speaker A: Can you give me something? It's not differential, not at the one point. Then we consider this example. L1 distance. This l one distance. The non differential point is a whole line. Because you think of this as a bivariate function sy, the point that is not differentiable is the whole line x y. Then here I also need to mention recent breakthrough by radianvarin.
00:22:50.268 - 00:23:24.990, Speaker A: This shows that the optimal scaling is s over log s, which means that if you sample n times from p, sample n times from q, the number of samples needed to estimate this guy is so log s. However, we are still slump small catch, which is only shown to be optimal when n is not really bigger than s. Then what we want something more. What about n is slightly larger. I want still this to be optimal. For this, we applied our general methodology and we solved it. We showed that minimax l two reads is s n log n, where mle is s or n exactly n still be enlarged to n times log n.
00:23:24.990 - 00:24:10.136, Speaker A: And we show that the construction in vv cannot achieve this when it's larger than s. So basically it's not because they didn't analyze, it's because it's by definition in construction cannot really do it. But this is very complicated because currently the non smooth region is no longer one point. It's not very clear that you use one interval to cover it. But how to cover a line, you may think probably I use a small square, I cover at the really boundary cover square cover this, or I use a band to cover this whole line, or I use some other shape. We don't have time to discuss our result, but we need to share with you some bad news. The bad news that you may think, oh, we still use idea of best polynomial approximation in 2d is not unique until now.
00:24:10.136 - 00:24:38.082, Speaker A: There's no efficient algorithm to compute it. And also even you still think, oh, I don't need a really bad polynomial, I just need a polynomial whose approximation property is quite good. It's also bad news. I can give you some polynomial or achieve best polynomial approximation really, but it cannot be used. And also all the non smooth regime construction idea in this all fail. It's very disappointing. So the solution, you might get some hint from our paper or wait for our soon to be finished paper and later.
00:24:38.082 - 00:24:58.836, Speaker A: I don't have time, but I want to show you one example. So some people also ask us that we don't really care. Logarithmic improvement. It seems still cool in theory, but it doesn't really make any sense in practice. We want to show you. Actually it's not the case because we didn't talk about the constants. We take the sequence n is two s over log s, because we know for entropy estimation, s over log s is the threshold, is the right scaling.
00:24:58.836 - 00:25:27.396, Speaker A: We take this s equally sampled from ten to two to ten to seven for each and s sample 20 times from uniform distribution. They really give a sense little bit s over n. So actually it means really big, means you really in the under sample region s very big compared to n. Here the performance as last I'm going to show you can see that s n is the two over a. It's really big. The MLE is really, really bad here at the entropy here at this point is roughly 14 mle achieves a mean square error 4.5 hours, essentially zero.
00:25:27.396 - 00:26:06.580, Speaker A: So there's another application in machine learning that I cannot talk about, which also significant Eightfold improvement in learning three graphical models. And also we also didn't have talk about many other more stuff. For example, we didn't say how you analyze the bias MLE, which also was not known before. And we need to connect this with another field of approximation theory started over 100 years. These are why bootstrap jack can fail and multivariate settings, nonparametric settings, non ID settings and all who apply this some hindsight. So perhaps we are going to skip this. There are some related work, really nice inspiring pioneers that we suggest everyone to read.
00:26:06.580 - 00:26:20.476, Speaker A: And there's also later he mentioned we're going to talk about this work. That is some of our work. First one is work. Second one is analyze MLe. This is an application this on divergence. It's adapt estimation. This show directly doesn't work.
00:26:20.476 - 00:26:31.764, Speaker A: This gives the general methodology of analyzing Taylor series, bootstrap and jackknife. And this answers one question that we'll take once we discussed on how to analyze the gap of transiting equality. Thank you very much.
