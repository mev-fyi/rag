00:00:00.320 - 00:00:03.554, Speaker A: Tutorial? Never. You can take it from here.
00:00:05.614 - 00:00:38.066, Speaker B: Thank you. And thanks everyone. I hope you guys had some good, enjoyable break. Again, I'm sorry I'm missing the opportunity and I hope we'll be able to be there later in the semester. So I want to go back to. Again, the same rule says, before we'll apply, please chime in if you have questions. I'll do my best to monitor the chat window, but admittedly that's a little hard.
00:00:38.066 - 00:01:56.158, Speaker B: So if you can help, that would be appreciated. Okay, so we were defining this sort of alternate variant on smoothness, which is the one I want to use as my example in a second. So I rewrote here the classical version is the cost that the cost of the sum of overall costs of what people would get if they changed some special strategy is less than something times the optimal. Oh, sorry. The alternate version for utility games, defining it more formally would be, I guess the inequality turns around and maybe the social welfare of the current solution comes in, should be a subtraction. Social welfare, remember, is a mix of the player's value plus the revenue. And so what seemed to be this sort of better version, or somehow cleaner version that Wasili and I suggested is forget this social welfare put in something that explicitly distinguishes the revenue.
00:01:56.158 - 00:02:40.176, Speaker B: So the sum of the utilities that people get somehow compared to what the optimum is, minus the auctioneer's revenue. And this, just as before, it very simply implies a price of energy, band, or maybe even more simply, at a Nash equilibrium. Oh, that inequality is backwards. Also, the utilities of players should be bigger than this alternate version. And then all I have to do is rearrange the inequality. And notice that the sum of these utilities plus the revenue, that's what social welfare is, and compares to lambda times the optimum here. I guess this lambda is a below one multiplier.
00:02:40.176 - 00:03:26.356, Speaker B: Uh, because it's a greater than equality, the welfare is not bigger than twice the optimum. It's hopefully bigger than something like half the optimum. So the price of energy tends to be a number above one, which has the one over lambda here. But it's the as simple as it gets as a band. What I want to show you is that this sort of bands tend to naturally, or seem to naturally apply in auctions. In many auctions, all I'm going to do is first price, because this is not a tutorial on auctions. But so again, you're thinking of maybe this is again a single item auction with a player that has his value.
00:03:26.356 - 00:04:44.014, Speaker B: He knows what it is. And I guess what? I want to have him think about what would happen if he bid half his value in a first price auction. Bidding your real value is definitely dangerous or useless, because if you have to pay it, then you get no value even if you win the item. Well, maybe half your value is reasonable, or some other multiplier like that. And if I think of half the value, then the following inequality is really easy to argue, that the utility, no matter what everyone else bids that you get if you bid half value, is greater than half your value minus the price. And I guess that's simply because either he wins, in which case his utility is half the value, because he only pays the price, which was half the value of his bid, which is half the value, and the other half is his value, and that we don't even need the subtraction or he lost, in which case the price is above his bid, and then this side is negative and he's getting zero. In fact, I guess I can add that this is greater not just than this inequality, but also than zero, because utilities in first price action is you always have non negative utility, no matter unless you bid above your value, it's guaranteed.
00:04:44.014 - 00:05:42.298, Speaker B: And then I can take this inequality and say that if you satisfy the no regret property, then whatever happens should be better than having bid half true value. And I can add this by the using this inequality for the person with the top valuation and the zero part for everyone else. And I get that this is, this auction is half smooth, half is the parameter here, the lambda I used here. That is, it's true that the sum of the values is greater than the maximum value, half of the maximum value minus the price. So I guess that proved that this gives a price of energy band of two. And I admit that factor of half that I used here is, I guess is simple. But actually you can do a little bit better by improving this to e minus one over e instead.
00:05:42.298 - 00:06:31.074, Speaker B: So this is not actually a sharp band, but something like this is giving you a sharp band. This kind of proofs are super powerful, as we sort of did in some version, and maybe even more powerful in other ways. It works on bayesian auctions. That is what happened here is that the proposed bid, this a star, was only a function of the person's value. All I have to do is take expectation over it. Same band applies. All I have to do is take expectation of bands also applies to normal learning outcomes for the very simple classical, the same analysis we have seen before, adding up the inequalities.
00:06:31.074 - 00:07:09.790, Speaker B: And I guess the last important point to make here is that I'm not actually asking these bidders to bid half they value or whatever times they value. They can do whatever they want. They just should not regret this. If they're doing even worse than had been consistently bidding half their value, then they must be doing something wrong. And I have a proposal for them that improves their behavior. And I guess the theorem here is that this first price auction is one half smooth. The bid only depends on the player's value.
00:07:09.790 - 00:07:55.864, Speaker B: And when that happens, then this analysis extends to vision outcomes, that is, randomized outcomes, randomized meshes, or all kinds values of no regret learning outcomes. So that's a nice general extension theorem. In fact, life is even better in extension theorem land if I think of a version of this with multi unit demands. So that is, these are players who would like to buy one of these houses. And they have values, I guess the optimal social welfare here. So everyone just wants one house. Optimal social welfare would be a maximum matching problem.
00:07:55.864 - 00:09:00.494, Speaker B: I should assign everyone to some house so that the sum of values get maximized. The special bid I want to propose here is that they should bid on the has that the optimum assigns to them have their value. And if I do the same analysis, which I won't go through, it's somehow the same inequality applies. And I'll just tell you the one sentence that it applies, because the analysis I actually did show you in a single item somehow will apply item by item. This person who, you know, a person assigned to, assigned to a house chain, the optimum will have utility, at least half his value. For this has minus the price that someone else paid on that house. And summing out over all players, I get exactly the same band, the same analysis, and in fact the same improvement in the e minus one over e instead of the factor of factor of two.
00:09:00.494 - 00:09:54.560, Speaker B: The difficulty I want to point out here is that in the previous version, the players are capable of beating half their value. Here, in order to beat half the value on your optimum, assign has kind of yet to need to know which one that might be. And in a bayesian version, or in a learning version with various opponents, it might not be always the same has. But turns out the extension CRM is more powerful. And this is originally from a pair of papers by Tim Rafgarten and Wassily. And then the current version is our paper a year later. That extension theorem even applies if values are drawn from independent distributions despite the difficulty, and extends to learning outcomes.
00:09:54.560 - 00:10:47.054, Speaker B: Also, it's a nice message. It says that we want to think about learning outcomes even in a bayesian setting, that even everything is uncertain and changing. All you have to do is this sort of proof for the full information version and it automatically extends, which I think is very nice. I will not go through the proof actually more dominantly, because again, this is not a price of energy tutorial, but learning in games tutorial. But the message is still nice. And I guess I want to point out the difficulty without maybe pointing out the solution. What I want to say is that they should not be regretting bidding what the optimum would make them bid, but there is no way for them to that's not consistent and there's no way for them to know what that might be.
00:10:47.054 - 00:11:53.184, Speaker B: Instead, I want to consider some slightly unusual or crazy thing of sampling the opponent and bidding as if you guessed what the opponents are. That is, you're bidding according to the right distribution, what the optimum would give you, but not necessarily correlated with what people are doing. And turns out this works out. What I want to point out about it is actually one of the things I started in the beginning is the difficulty in life. That is, this is an application where there are too many bits to consider. So optimal bidding is difficult and learning it is also difficult or maybe not doable. So I guess this is coming from a paper of Vasily and Costas and I guess they pointing out that in this application bidding is truly problematic.
00:11:53.184 - 00:12:11.676, Speaker B: Like let's just think about where we stand. There are very many possible bids to consider. In particular, you can bid all on the item. So just one item. Your choice. For every item you bid on, you can bid multiple values. It turns out bidding on one item is not that good an idea.
00:12:11.676 - 00:12:35.734, Speaker B: You might lose it. Maybe rationally, you want to bid high on one item and low on others. I'm not so sure what you should do. You can make some elegant simplifications. Don't beat above your value. That seems like a dumb idea. In the first price auction you can discretize things and say you multiply epsilon only and you don't lose so much.
00:12:35.734 - 00:13:24.996, Speaker B: But even with that, finding the optimal bid, or even approximating the optimal bid is hard. And it's a natural, pretty simple reduction from set cover where the example is just like what I suggested, you just want to win one house. In their example, you don't even care about which house. All of them have the same value for you. But turns out the other bids on the houses are very high. So you actually might want to bid on multiple houses to get to make sure you're winning at least one, and that's where the set cover is. You should be in enough houses, but not too many, so that you win a house, but hopefully not too many because every single one you have to pay for.
00:13:24.996 - 00:15:06.948, Speaker B: So this meant as a sort of hint of why set cover might be relevant here. That's, I guess, deciding what is your best bid, even if you know the distribution, even if or you have the past distribution, is a computationally hard problem, so it might not be doable strategy. So I want to give you a little bit of outline of what are the things I'm hoping to at least touch on in whatever half an hour I have, or maybe 40 minutes. And I think all of these or most of these are nice open directions, or enough open directions that would be nice to make some progress during the semester. So what I started with is pointing out in the previous slide that bidding might be finding the optimal bid, even if you know the distribution of what the opponents are doing is computationally hard, so you can think about, okay, what might be possible when that is true. I guess it's nice to think about what happens if, for some reason, the game players or the opponents aren't stable and people playing for only partial times. And one issue that I have been recently more thinking about is what happens in games where it's not a clean new copy of the game every iteration, but there is a sort of clear carryover effect.
00:15:06.948 - 00:16:39.730, Speaker B: I guess one dominant example of this thing is advertisement auction, where the carryover effect is named your budget. Namely, if you win the action, you have less money left than if you lose the action. So that's a pretty clear effect between iterations at the moment, I guess I won't be able to tell you anything very good about those actions, but I can tell you a little bit about what I do know, and I guess one sort of more dominantly open area is, and I'll come back to this in some number of slides, if then cooperation is constructive, then no regret. Looks like an exceptionally dumb thing to do, and what might be the better thing to do is a good question, or better ways to analyze things. So let's just actually start with the example that I studied with with that when it's hard to bid what Costas and Vasily are suggesting, that they can change the benchmark. And rather than thinking about your exact bid, maybe you should think about what subset of items you're asking for, and maybe an easier thing to achieve. No regret for is this condition that's written here.
00:16:39.730 - 00:18:05.640, Speaker B: I'm guessing English what it's trying to do is instead of averaging against what's the optimal bid in which you win the item when it's cheap and not win the item when it's expensive, just look at the average price of the item and try to compare compete with that one. That is, assuming that you have to win, you have to pay not the actual current price, but the average price of the item over time. That's your benchmark. What set of items would you want to buy if you had to pay the average price? And turns out this notion is easier and one can compete for this one. And in fact, what they do is show that both that that variant of follow the per term leader does achieve, does do okay with this. What you need as a subroutine is asking every iteration, given the history of prices on the average price so far, what would you want to buy, which is a much more doable task in many applications. And turns out this form of no regret is enough to guarantee social welfare with a sort of similar proof than what we have seen before.
00:18:05.640 - 00:19:40.134, Speaker B: So I'm not going to elaborate on too much. The other place where there are enough positive results is learning in games with changing population, or when there's churn in the population model that Todoris and Vasili and I have been using is players come and go in the players and maybe gets replaced. Every player in each iteration gets replaced with some probability ivanukim have the time that people stay in the game long enough to allow them to learn. And I guess the first thing I should ask you is what should I expect them to learn? Like the history I claim that we're going to need a little bit stronger notion of learning here. And I guess maybe that's almost only, the only thing I'm going to show you about this. If this red player here is around for a period of time, it's easily possible, especially if this is a long enough period that the churn works out so that, you know, when he starts a certain strategy is really good for him. But after a while this guy so vanish and some other guys come who make his routing paths very crowded and he would be better off switching to a different one.
00:19:40.134 - 00:20:29.558, Speaker B: So a natural form of learning, that would be nice. Or actually, let's start with an ideal form of learning. What I really want him to be able to do is this. Choose his strategies so that he compares favorably to the instantaneous optimum every instant of the time. That is, as the population changes the optimum, which I guess I'm thinking of this alternate strategy AI t that is the optimum at time t. This is what I wanted to compare him to. But maybe you won't find it too surprising that with changing population, the optimum is changing and this is going to be way too sensitive.
00:20:29.558 - 00:21:40.272, Speaker B: And I guess just as a simple example, as a tester, if you have a matching problem and there are all these players and the optimal matching is something, and then only one player vanishes and the optimal matching changes for everyone in the game, because matching is a sensitive object. This is true for most optimization problems, that the true optimum is easily very sensitive. And so the benchmark that I hoped for of comparing to every time t from the instantaneous optimum doesn't appear to be learnable, like that's just too hard to learn. But there is something that is close enough and it is actually learnable. And that would be this one, which is adaptive learning. That there is a version where if I compare what utility this person gets or what cost this person gets compared to a sequence of changes, not necessarily the optimum, but something like that. But I know that the sequence only changes rarely.
00:21:40.272 - 00:22:29.714, Speaker B: In particular, the model we are using here is changing it like k times. That will push up the error parameter with a factor of k. But as long as k is not very big, that is, the change in the solution that you try to learn is not very frequent. You can do this. And one nice message, at least for me personally, is that the way to achieve this is to make, is to make your learning recency biased. That is, rather than comparing always to the history, you should more heavily favor the more recent part of your history, which is nice because humans actually learn that way. So that's one of the outcomes of the human subject.
00:22:29.714 - 00:23:56.324, Speaker B: Experiments on whether people do satisfy the no regret property all on their own suggest that, well, almost, except people tend to be recency biased. So this suggests that at least in a changing environment, which honestly, our life is a changing environment, recency bias is actually reasonable. And that's what reaches high social welfare and at the super high level, without proofs, what we're proving is that for games that satisfy the smoothness property, if we can find a sequence of approximately optimal solutions that don't change very often, then adaptive learners will get close to the price of energy. And again, not too surprisingly, what we're doing is assuming that they learning to not regret this alternate strategy, not very changing alternate strategy, and then compare to the welfare that this close to optimal alternate strategy achieves. To be very concrete, I guess our error bands have three parameters, the classical price of energy band. There is a bit of a loss due to regret error, as we always had. And there is this last item, which I guess we call beta, is the loss, because our stable solution is only approximately optimal and not exactly optimal.
00:23:56.324 - 00:24:59.828, Speaker B: So what I did so far, and this is going to be a stopping point and I'm going to maybe switch gears to something that I view as a is more open, is tell you about results that are very smoothness framework heavy. That is, I showed you some variant of the smoothness framework, both the original one and this action variant. And then I view two sets of results, or two results that are extending it to other kind of games. So I guess I view the Costas and Vasily results on multidimensional actions also like that. It's using a very similar argument to smoothness. And what they do is not exactly no regret learning, but still it's a variant of follow the perturbed leader. So it's similar.
00:24:59.828 - 00:26:12.094, Speaker B: And that what we're doing here with Costas and Todoris is definitely, that is an extension of, of using heavily, using the smoothness framework. One important assumption that I like to spend my remaining half an hour on is sometimes this whole no regret or the smoothness framework is not applicable. And one case this is when there is some carryover effect, or some other sign of direct effect in between the games. Beyond the fact that of course everyone is learning from the past data, there is some other effect happening here. One painful place where this happens is cooperative games. And I will only spend maybe two slides or one on cooperative games because I don't have anything very intelligent to offer. But it's a painful place where no regret seems very much the wrong definition.
00:26:12.094 - 00:27:16.678, Speaker B: And just to drive the example, they point home, think of two person game, repeated prisoner dilemma. I guess these are utilities. So if a person cope, if, if they boss the fact players, the fact they both get, and that is half the police, then they both get ten year prison terms. If one of them defects and have the police and the other one doesn't, then the person who defected gets a small reward, and other one gets an even bigger prison sentence. And of course, if both of them continue to cooperate, then the police can't penalize them. Obviously, the only good thing to do is for both of them to cooperate, at least ideally in the repeated game. But the only Nash equilibrium is to defect.
00:27:16.678 - 00:28:02.774, Speaker B: Defect. And in fact, that's the only no regret strategy, is to defect all the time. If you do anything else than the fact that's not that you have regret. And that's because what this regret notion suggested is you're not taking into account that if you change your strategy, the opponent might change the strategy. Your strategy or your actions have an effect beyond your payoff. It's inducing the other guy to do something, and that's not being taken into effect. That as you will change your strategy, then your opponent will also change your strategy.
00:28:02.774 - 00:28:39.348, Speaker B: That is hoping that if you always defect, you can only expect the opponent to defect. Also, if you cooperate, maybe he will cooperate. Also, there are suggested learning algorithms of what might be better here. The more recent aurora decal Tevari paper calls them policy regret, which is a nice name. That is the you regret for trying a particular kind of algorithm, and they have some results on things that maybe you can prove. Certainly it's a regret that's achievable. It's a doable thing.
00:28:39.348 - 00:29:47.104, Speaker B: It is a very reasonable, algorithmically simple with me, maybe making a decent behavioral assumption would be nice to be able to prove that it actually pays off in many classes of games. What I can tell you about is work I've been doing with my current graduate, one of my current graduate students in package ratting or rating style game, which also has a similar carryover effect. Certainly much simpler than cooperative games. And maybe I spent some minutes to tell you a little bit about that one. So I guess this is morning rush hour traffic. I do not expect anyone to cooperate with me or traffic of some sort, but I want to point out that there is a carryover effect here too. Maybe not in the morning rush hour, because no matter how bad Monday morning traffic is, by Tuesday morning, before Tuesday morning, everyone went home or wherever they went.
00:29:47.104 - 00:30:41.984, Speaker B: But if I think of the smaller timescale either cars or packets in the second by second, then any car or packet that caused congestion and minute number, second number, whatever, a millisecond later they're still there causing the very same congestion. So there is a carryover effect. Or in case of packet routing, when the congestion is big, this packets get dropped and then they don't get resend. It's still a carryover effect. The packets didn't make it to the destination and they need resending. So I guess the question that we were trying to ask is what can we say about price of energy balance in games where there is dependence between the rands in the particular game? I can ask the question of the style of the packet of the traffic routing. Early traffic routing paper we had with Tim.
00:30:41.984 - 00:31:39.702, Speaker B: How much extra capacity guarantees good performance, but the question I want to emphasize for this talk is the second one, what does learning mean here? And is no regret learning the sort of right form of learning? And here the carryover effect is simpler than whether or not your opponent cooperates. So I'm sort of hoping one can maybe answer this question. It's an easier thing to answer and hope more hopeful to immediately make progress on it. So let me tell you a little bit of the model that we're using. Again, I will not do a lot of sort of detailed maths, but I do want to show you both the results and the issue with learning. So I have to show you what the model is. So our model is a very simple queuing model.
00:31:39.702 - 00:32:30.174, Speaker B: There are queues with packets. Every queue has some packets arriving at some late, like maybe for Qi, they Bernoulli process with vape lambda servers are capable of serving packets. Maybe they have array too. But if you don't like that, then they could be perfect. Like they can be perfect at serving packets won't affect the results. And the assumption is that each time each queue can send one packet to one of the servers, the servers can process one packet at a time. And I'm going to assume for simplicity, that they return all the unserved packets that is dropped them and they have to get resent.
00:32:30.174 - 00:33:39.004, Speaker B: And I guess the tiebreaking as to if there are multiple packets, which one do they serve? We're going to assume that they try to. Their packets are timestamps and they making an attempt to try to serve the oldest of the packets. This is really helpful because it helps the old packets clear, which will turn out to be really necessary if I want to have any hope of any positive results. So again, the question we're asking, how much extra capacity do I need to serve every packet? I spent maybe 2 seconds to sort of, or two slides to help you a little bit, like get what the model does. So if I go down to one queue, one server, then that's a very classical 101 queuing theory model. Lambda arrival rate, mu service rate. If lambda is less than mu, then you're okay, and the expected queue size will remain banded.
00:33:39.004 - 00:34:27.024, Speaker B: You can think of it as a biased book on the half line biased towards zero. If turns out lambda equals mu, then the Q will grow, maybe not at a linear rate, but still grow. And if lambda is greater than mu, that's very clear. There's more packets arriving than you can serve, and there will be an infinite line of packet, unserved packets. And that's, you know, this is sharp and simple. The model was originally suggested by a nip 16 paper by Krishna, Sami, San, Johari and Sakotai who actually had von Q and multiple servers. And it's a version of multi arm bandit problem where they wanted a finer analysis.
00:34:27.024 - 00:35:22.268, Speaker B: Not only that this queue will learn which one is the good server to send the packets to, but rather how long the queue behind him before he figures out where to send it. So obviously the necessary condition is that one of the servers is good enough to take his arriving packets. Remember, he can only send to one place at a time, but they did a finer analysis understanding how long will it take to to get this served. Our version there are multiple queues and multiple servers. To answer the question of how much extra capacity needs, I guess we had to start with sort of a baseline measure. Suppose I were to coordinate how much do I need? And that's not very hard to establish. I guess because this is a baseline optimization measure.
00:35:22.268 - 00:36:17.004, Speaker B: I just assume that everything is ordered, that queues and servers come in ordered by the arrival rate and capacity. And the condition is that the highest arrival rate queue must have a server that can take its packet. So this is the condition here with k equals one. That is, the highest lambda is less than the highest u, that also the two top arriving queues have told arrival rate less than the two top servers. That is, the two top lambdas are less than the two top news. In sum, again, two queues can only send to two servers, so we need that condition. This condition is obviously necessary for every k, and it turns out it's a sort of matching style argument, not very hard to prove that it's actually sufficient.
00:36:17.004 - 00:37:28.600, Speaker B: But our mains theorem says that if all queues use no regret learning to figure out what to do, then twice as much is enough. And this turns out to be sharp. So again, the necessary condition for coordinated situation is that top kqs have arrival rate less than the sum of the service rates of top case servers. Turns out if they only doing it via learning algorithm, then the necessary condition is that top k queues, the arrival rate is less than half the service rate of the top k servers. At this late point in life I certainly don't want to go through a whole detailed proof, but maybe birth to at least flip through quickly slides and I will post the slides that have the outline. The proof is a potential function argument. The potential we are using, or at least the intuition for the potential we are using, came from sum of many potentials.
00:37:28.600 - 00:38:40.494, Speaker B: Phi tau is the number of packets in the system that are tau age or older. And I sum this over all the packets. So old packets contributes a lot to this potential function, the younger packets contribute less. Obviously this potential function is good in the sense that if I keep this potential low, then there can't be old packets there because a very old packet contributes itself a ton to this potential function. The main tool we use is a nice probability theorem by P. Mantel and Rosenthal that if I have any random process with a function phi, which when phi is high is regular, which we can sweep under the rug, but when phi is high, it has negative drift, that is, it tends to decrease or decreases in expectation, then it remains bounded in expectation overall times. And I guess the main thing we have to prove that the no regret assumption and the factor of two slack really implied the negative drift when dysfunction is high.
00:38:40.494 - 00:39:41.084, Speaker B: So maybe a sentence of why no regret helps. I guess I really prefer to think of this particular time tau and the the packets that are tao or older. That's a nice group, because overall as a group they have priority over everything else. So this is a group of things that has priority. I fix a long window and the best servers and I can think of a option a they send to those servers and remember the condition is that they have planning capacity, actually twice as much, which I don't even need for this part of the argument, and they send it to that even half the time, then we're okay. We have enough cleared that the expectation should, the potential function should decrease. Of course we don't know they're standing there, but that's where no regret will kick in.
00:39:41.084 - 00:40:30.130, Speaker B: If they're not standing there, then they must be not regretting that they didn't stand there. And if they're not sending there, then these queues, the best servers, the best serving servers, are spending empty half the time and they will have priority. They could have gotten a lot of service out of them by no regret. They must be getting at least as much service someplace else. And again, I take it all these timescales and add them up, and that's why we get the band. To be honest, I admit that there are some slight details here. The technical extra detail is that in first of all, this is an infinite process, so one has to be a little careful with expectation.
00:40:30.130 - 00:41:37.134, Speaker B: Even low probability events are always going to happen. And second, the technical condition on the pmantal Rosenthal result requires that, that the process has bounded moments, which the way I described it does not. So we have to switch the probabilistic description. But this is not what I wanted to emphasize the part that I wanted to actually get across and spend some time on is this last thought that I was hoping to have you guys all appreciate is an interesting example for this process. So remember, going back a page, the process was that these queues have packets arriving. I guess in this case, these two queues, both have packets arriving a bit higher late than half. So like, you know, half of the time they get a packet, roughly speaking, at random.
00:41:37.134 - 00:42:01.698, Speaker B: And we have two servers here. One is a perfect server. If anyone sends a packet, he can send one, he can serve one. The other one is less than half rate. It's not a very good server. So what do we know about this? First of all, I want to start by saying that there is plenty capacity here. Centralized scheduling rules.
00:42:01.698 - 00:42:26.952, Speaker B: It does satisfy easily. Like remember, the rule was that the arrival rate for the highest arrival package should be less than the service rate of the best server. Yeah, 0.51 is less than one. All set. And second, that the top two arrival rates, which combine to 1.2 here, should be less than the top two servers.
00:42:26.952 - 00:43:09.944, Speaker B: Again, we are home. Roughly speaking, one against 1.5. Yes, surely it does not satisfy the factor of two band actually on either count. But most dominantly because the two top numbers sum to above one, and this is only one and a half, so it's missing it by quite a margin. And indeed, knowing that learners are not going to succeed, I claim that what knowingly learners do is both of them send to the top servers. Now why do they do that? Well, at this point, they should get a service rate of a half. I mean, they're about symmetric.
00:43:09.944 - 00:43:35.090, Speaker B: Sometimes one is older, sometimes the other one. But roughly speaking, they're splitting the rate. So getting across at 50 50, and I made the arrival rate just a little bit above. That's not good enough, but that's what they're going to get, 50%. But more noticeably, the other server is worse. So they're getting 50% on top. That's better than 0.47.
00:43:35.090 - 00:44:34.752, Speaker B: So clearly we're home, 50% is optimal. But yet, of course they're getting winner gross because 50% is not good enough. They slowly, slowly increasing and growth. Okay, so far nothing interesting happened. It's an example where our theorem doesn't apply because there is not a factor of two, and indeed learning is not good enough. But here is the painful pact. If one of them would change to this, by which I mean a randomized strategy in which he sends 10% of the time to the bottom server and 90% of the time to the top server or some other percent like that, what do you think would happen? So I claim that in a more smarter learner, this is a selfish move that I claim that it selfishly helps.
00:44:34.752 - 00:45:13.264, Speaker B: Now, a priori, you might say that. No, no, no. It's a, you know, it's a nice move towards the top guy because 90% of the time he competes, but 10% of the time he doesn't. So he's really helping the top guy who splits the traffic 90% of the time, but 10% it's all his. So he has an advantage, which is very true. It definitely helps the top guy, but it turns out actually helps the bottom guy also. They're both clear and it's not very hard to see why.
00:45:13.264 - 00:46:35.702, Speaker B: Chu, he's helping his neighbor more than he's helping himself, but he's helping enough to both of them because of what he's doing down here. When he goes up, he's quite often the older packet and therefore he actually going to win way more than half the time when he competes, which is happening because some of the time he's using the lesser server and this way actually both of them clear. But notice he actually has regret. He has even more regret than what we had before I made him switch to this user because now his success rate on top is better than a half, and yet this is actually good for him. So somehow regret appears to be the wrong notion, and in fact, it does actually help even in the actual CRMs. So what I gave you a little hint of how we proved that if everyone is a non regret learner, we need a factor of two buffer in the capacities. If I instead assume that they somehow have this patient or better evaluation of what's going on and choose optimal strategies that way, then I don't need a factor of two.
00:46:35.702 - 00:47:29.892, Speaker B: I need a bit less than that e over e minus one, which is a sharper band. But instead of telling you why the second theorem is true, I just want to emphasize what exactly happened here. So what I'm claiming is that these cues nor grad learning was too myopic. And this is something that, you know, such a learning heavy community, I'm sure you guys all appreciate. But maybe it's worth still writing out the improved notation in this game because of the carryover effect. What happens in iteration t doesn't only depend on what you did in iteration t. It depends on the whole damn history like of who had a packet and who could send and how old was your packet, all dependent on what happened in the past.
00:47:29.892 - 00:48:36.076, Speaker B: So a better notation would be, is your cost or maybe utility at your player I at iteration t, given the whole history up to t, and I want to sum this over time, t. And what no regret does is compares it to this animal here on the other side, which is the cost if everything stayed the same. But on iteration t, you tried something else. This is what Norigrad proofs about. This is what nor regret considers, and it is not nor regret proofs to prove to you that that's what it's considering, it's best to actually think of what are norget algorithms doing? They're playing explore exploit strategy. That is, every now and then they're randomly trying to do something and then evaluating how well it works and then repeating the thing that worked well in the past. This is explore doing this, switching to something for a second and then backing off.
00:48:36.076 - 00:49:21.658, Speaker B: That's explore, and that's what they're doing, and that's the benchmark that they're competing against. And maybe what we really meant is this. That is what would have happened had I consistently used this x strategy. That's what we really meant. Or maybe I don't mean all the way from the beginning, but at least for a while, to see the long term effect of this strategy and not just the instantaneous effect. Again, paging back to the example that I find very insightful, the instantaneous effect of using this is very painful because this is a low rate. And remember, what he's getting there is even better than half.
00:49:21.658 - 00:50:04.644, Speaker B: But the long term effect is good. And I guess a more patient learner would somehow take into account the long term effect. So, you know, the band we prove is actually not a learning band. So maybe less relevant. We proved something about the stationary equilibrium of this game, as I guess an equilibrium analysis for that particular thing. This e minus e over e minus one bond is actually sharp. But maybe that's not the main message for this talk.
00:50:04.644 - 00:50:41.130, Speaker B: Instead, I should sort of try to tell you a little bit of what are the things that I would like to think about using this special semester. I guess before that, of course. I love learning. It's a good way to enter a game. No need for common priority. But here's a sentence I said. A lot takes advantage of the opponent playing badly that I never could make a sense out of in a sort of more sort of a theory framework that would explain.
00:50:41.130 - 00:51:32.514, Speaker B: It makes sense in English, but it would be nice to actually prove something like this of one way. Is it actually a way? Like if you play in a zero sum game. You play the Nash equilibrium, then you guaranteed to get the Nash outcome no matter what the hell the opponent does. If the opponent is bad, you should be able to do better. And learning, I think does that, but I guess it's not in a way that I know how to argue about it. I guess I try to show you that learning sometimes does well and even okay to have some carryover effect though there are many, many carryover effect games where we don't know how to do similar analysis. And I guess the sort of killer application or dominant application is auctions with budgets.
00:51:32.514 - 00:53:01.062, Speaker B: There are some nice papers that try to think about what does learning mean in a budgeted scenario, but I think there are lots of things I wish I knew better and I don't. And then I guess the last question I got to is can there other forms of learning that can do better? I'm hoping policy regret of Aurora decal tavare might be an interesting target here, or just actually a more patient regret. That is, instead of trying everything once you check on the long term effect, which is roughly speaking, what the no policy regret algorithm of Aurora Decca and Tavares doing also is trying strategies for one step, for two step or four step, and sort of see how it works. Unfortunately, this is a conjecture. I ended the talks about this problem in earlier runs, but very recently this new paper pointed out that no policy regret as a behavioral assumption is not strong enough to get our improvement from the factor of two in the particular queuing application. So it does seem like the example is very artificial. So maybe there is a slightly better assumption that one can go around, but that conjecture is not as good as it used to be.
00:53:01.062 - 00:53:16.340, Speaker B: So thanks so much for paying attention and I guess tolerating my online version. And I love to answer some questions if people have questions very much.
00:53:16.372 - 00:53:18.984, Speaker A: Eva, do you have any questions?
00:53:26.164 - 00:54:10.938, Speaker C: So thank you so much for the great tutorial. So I have one question about the first price auction. So you mentioned that that game is smooth. It's half smooth, but then in that game you only have the bidders and then there is no auctioneer who is also optimizing its prices. So I'm just wondering if the notion of smoothness can be extended if you also include the platform or the auctioneer into the system. So I'm having like a bunch of bidders and also that they're optimizing their bidding strategy and there's a platform reduction here which optimizes is bidding a strategy to do? I still have a smooth game, I.
00:54:10.946 - 00:55:15.312, Speaker B: Guess we have to discuss what the framework actually is, but I guess I'm certainly not aware of results that claim this. The simplest version would be to say that what the actioner can do is set a reserve price. So he's committed to running first price auction, but he can announce that, or maybe not announce, but his strategy is to set a price himself and say, I won't sell it unless it's at least such and such. We know from the classical Meyers on saying that regular one shot actions that setting reserve is a good idea. It pushes the bidders to bid higher, even though, of course, if you don't sell the item, then there is no social value. But it does increase the revenue of the auctioneer, so it's a selfish move for them. And it's a good point.
00:55:15.312 - 00:56:10.464, Speaker B: If the price of annual results would work with reserve price setting auctioneer, the sort of broader question, which I think is interesting, but less well or less clearly, mathematically phrasal, or at least, I don't know how to phrase it well, if the auctioneer is deciding the format of the auction, so exactly what should be the rules of the auction? Just because that's more open and it's not clear what the strategies are in that game. I mean, for the action. But there might be million people in the audience who might know the answer better to this than better than I do. Feel free to speak up and offer a better reference or a better pointer.
00:56:17.704 - 00:56:45.954, Speaker A: Any more questions from the audience? One question I had about the last stateful framework is that it was still in the fully adversarial scenario. Is there anything in between where maybe there's some stochastic variant of how the system reacts, or maybe have offline data of how the system reacts in the long term, and that could be useful blend between adversarial and plastic that would help.
00:56:48.334 - 00:56:50.662, Speaker B: This is for the queuing question.
00:56:50.838 - 00:56:57.554, Speaker A: Yeah, for the queuing question. Or more generally, when you have. When your actions have long term lasting impact.
00:57:00.694 - 00:57:36.854, Speaker B: I mean, there is something stochastic in the baby setting up the queueing question, because the arrival rates are stochastic. So there is some stochastic uncertainty that we actually already using to our advantage. I guess it's a nice, like generally, you know, stoicity should help, I guess. A nice question, and I'm not. I don't have a clear answer right now. Sorry.
00:57:44.874 - 00:58:27.164, Speaker D: Hi. A very interesting talk, first of all, and I also really enjoyed the last part about the carryover effect. I was curious, what is the. So you presented a specific application where clearly there was impact to the state of people's choices. What is the. What would be a broader framework for us to think about carryover effects? Like, do you think of it as a broader question? I mean, of course you think of it as a broader question, but do you have a framework for us to think about where, you know, stays affected by actions and, you know, in some way, and how should we think about it more generally?
00:58:27.884 - 00:58:55.956, Speaker B: That's a cool question. Actually, I was. I don't know, it's a nice suggestion. I haven't. I did think about, like, the other sort of killer application with such an effect is budget. And that's a, you know, budget has this, like, I guess a different effect on the game is this cooperation, which I know many people saw the bat. Certainly the, you know, I talked to many people about it.
00:58:55.956 - 01:00:03.284, Speaker B: I guess Aurora is definitely someone I spent time talking about this. It's hard because somehow you have to know, will the opponent react? And it just somehow seems, I don't know, at some point I was more hopeful that one can get results of games like cost sharing games, where one would hope that moment, like, if I. If I start doing something, maybe the opponent will try to share with me and that will benefit both of us. I saw this kind of games, both the budgeted version and this package rat thing has the nice property that the carryover effect is not really dependent on other people reacting to you. Yes, they can react to you. That's always a lot. But in this example, when you start using the lesser server, it's indirectly helping you or it's helping you.
01:00:03.284 - 01:00:20.280, Speaker B: Even if the other guy doesn't react, it does nothing. It still helps you. And the budget is that way too. The carryover effect is not about other people's reaction. It's directly on your space. You have more money, you have less money. So I was more thinking of other examples where.
01:00:20.280 - 01:00:41.804, Speaker B: Where this might apply. And I guess if I think of my personal agenda here, is trying to prove similar results on other examples and then maybe frame them as a framework. But maybe you're right that I should take the framework question more immediately, upfront. Interesting.
01:00:46.564 - 01:00:58.116, Speaker A: Thank you very much, Eva, and thanks, everyone for attending. And now there's going to be a reception for the next hour. And thanks, Eva, again for the wonderful talk.
01:00:58.220 - 01:01:11.044, Speaker B: Thank you all for listening and enjoy your reception. I hope in a month or so I will actually come for one of the workshops and get to talk to all of you. Bye, everyone.
