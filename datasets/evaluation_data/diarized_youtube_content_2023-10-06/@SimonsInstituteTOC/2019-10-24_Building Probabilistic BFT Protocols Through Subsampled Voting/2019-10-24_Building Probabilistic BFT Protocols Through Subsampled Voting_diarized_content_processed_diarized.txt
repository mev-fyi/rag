00:00:02.120 - 00:00:04.318, Speaker A: Okay. All right. Thank you, guys.
00:00:04.366 - 00:01:02.948, Speaker B: So I'll be talking about an interesting modification of classical condenser protocols. So what I really want to show you guys at the end of this talk is a protocol that is, in the normal case operation, effectively optimal. And it's basically o of one message complexity, logarithmic number of steps in latencies, but in implementation, and I'll show you a real life implementation of it, it's still performing in sub second finality across a network of 2000 plus nodes. And it can go 10,000, 10,0000 without really much of a hitch. So this work, by the way, while it's from our labs and Cornell, it has some roots in work of Jared Saya and Valerie King. They started introducing some of these ideas earlier on, but they never really caught on, even though they are really, really powerful. So, consensus until now, we have two major families.
00:01:02.948 - 00:01:22.702, Speaker B: It's Nakamoto style and voting based protocols. I don't need to tell you guys exactly how this work, but classical has a really nice property. It's final. As soon as you reach quorum, you're basically done. You don't need to wait for any other craziness. You can make it asynchronous. So as soon as you get a quorum, you finalize, you get the certificate and you're done.
00:01:22.702 - 00:02:08.248, Speaker B: They have super high performance because of that. So the second that you reach your required decorum, you can basically give a certificate to the client and that can run into the sub second latencies. Unfortunately, because you do need to query everybody and you need to grab everybody's votes, there happens to be clearly a bottleneck here. Whether you move that to the leader or not. If you do move that to a leader, that leader will be bottleneck entirely by the ability to accumulate votes and then show votes to everybody else in the network, Nakamoto introduced something fundamentally revolutionary, which is that it said, screw voting. Let's just throw identities out the window. We're just going to now basically reach consensus on whatever this longest chain, heaviest chain thing is.
00:02:08.248 - 00:03:07.480, Speaker B: As long as you show me something that looks heavier than anything else in the world, then I'm going to assume that is a canonical chain and I'm going to go with it. That's great, because it now is effectively independent of the number of nodes because you don't even need identities anymore. You just need to broadcast blocks and hear what the longest chain is. Unfortunately, because you have a bounded synchronicity assumption and there is a bounded size to every block, there is, in fact a bounded amount of messages and therefore usually very low amount of transactions that can be fit into this, especially if you want to sustain white clients. And you can't just simply increase the block size or make the confirmation times shorter simply because then you're going to preclude a whole lot of machines from the network, from ever participating. You're going to make the network centralized towards really powerful machines. There's a lot of debates and obviously the space is very divided on how block sizes should be.
00:03:07.480 - 00:03:50.792, Speaker B: And there's bitcoin, cash in bitcoin, and there's been a lot of forks in regards to this. So there's been a lot of work in the past few years trying to improve on these pitfalls we've seen in Nakamoto. In all suggestions, make blocks bigger, introduce some form of sharding. Layer two, like lightning networks, has been very popular and it's catching on to some degree. Although it has a set of problems, there's notions of preconcensus to reduce latencies. So you use effectively a voting based mechanism to finalize blocks, or rather get a snapshot of what the current blocks are and then finalize them through proof of work. And then layer zero, get really, really good at distributing blocks around the network.
00:03:50.792 - 00:04:41.570, Speaker B: And there's been a bunch of companies that have spun off doing exactly that on the classical end. I don't need to tell you guys, there's been decades and decades of really fantastic work trying to optimize protocols in all sorts of ways. Oftentimes they end up optimizing for worst case scenarios and worst case scenarios. Even nowadays in blockchains, if catastrophic events and really bad scenarios happen, they tend to not really matter because we end up doing social consensus anyway. And it's a little bit disappointing because they're supposed to be unstoppable, decentralized systems. But ultimately there is really a whole lot of parties that can have a lot of say here outside of the protocol. But in classical, we've seen all kinds of optimizations to optimize that worst case round number optimizations.
00:04:41.570 - 00:05:44.984, Speaker B: We've used cryptographic tricks, we've used aggregation signatures and so on to reduce the message complexity. Sharding is of course another one and improving on the leader election. The question is, can we sort of make something really simple that doesn't require any kind of craziness, but still can get to this almost magical o of one message complexity effectively a network that performs the same regardless of whether you have 10,000 nodes or 1 million nodes. So we kind of really need to rethink it. And I'm going to propose the following. So in classical voting protocols, you sample everybody else, you grab what their state is, you create a certificate, you can create an aggregation on that certificate, reduce it, move it on to the next round, so you can reduce the message complexity for that round. But ultimately you're aggregating what everybody's saying and showing it to everybody else, that there has been some certificate and has been some consensus reached amongst everybody.
00:05:44.984 - 00:06:41.996, Speaker B: Instead, what if we don't do that? What if we effectively do the following? Let's suppose that we're all in this room and instead of, and we're trying to decide whether we're trying to go to Chipotle or Mohs. Is Moe's on this coast? No, it's a Chipotle like alternative, and they both serve burritos. And we're trying to decide which one of these to go. And some people like Chipotle, some people like Moe's, but we all just want to go to the same place. So everybody in this room, instead of me saying, hey guys, we're going to go to Chipotle, or I want to go to mosquito, instead, everybody samples their local set of people and asks, hey, where do you want to be? And gets that majority, and you repeat this locally over a few rounds. Turns out this is sufficient and I'll get to the kinds of message complexity this achieves, but it's sufficient to achieve really high security distributed consensus effectively independent of the network size. This has been known for a lot of time in voter based models.
00:06:41.996 - 00:06:59.184, Speaker B: In stochastic processing, voter protocols can scale independently of the network size by just sampling local, by really just doing local observations, because local observations tend to estimate global state pretty accurately, especially if you keep repeating it.
00:07:00.564 - 00:07:06.900, Speaker A: Yes. What's the statement exactly? I think you need login moves tunnel. Right.
00:07:07.052 - 00:07:35.996, Speaker B: The latency to confirmation will be login and the message complexity, and I'll show you why it'll be o of one because we amortize it in the, in a real implementation, binary consensus. In fact, it will be message complexity and latency will be login in a variable o of one per node in the amortized setting. Okay, but I haven't gotten there yet. So basic mechanism is super simple. We have a bunch of nodes. They are red or blue. They're trying to decide zero one.
00:07:35.996 - 00:07:50.934, Speaker B: They're trying to design a binary consensus. Every node samples a random subset of other nodes, gets their value. In this case, the red node samples a majority blue. This node here samples a majority blue, and then it becomes blue itself, and.
00:07:51.014 - 00:08:10.694, Speaker A: Yes. Sounds like you and Rafael's talk. Essentially, it's saying, let's take any probabilistic process, stochastic process that has an avalanche going one direction or the other and adapt it to full consensus. Yes.
00:08:11.354 - 00:08:23.610, Speaker B: Turns out it's a pretty powerful technique. So we'll build it aggressively. And every node here has no notion of rounds. It does not care. It just samples. Keeps going. It doesn't really care.
00:08:23.610 - 00:09:13.956, Speaker B: No synchronization, nothing. It just keeps going. And every node does sampling independent of other nodes. Turns out, and I'm not. I adapted this talk from a talk that I gave at Potsy, collocated with Potsi, and I wanted to keep it just high level, not going to the mathematics of it. But the math shows that, effectively, if you just do this random sampling, and then you go to whatever the sum threshold majority of nodes are saying in your local neighborhood, then as soon as you get to a point where the majority of the network, in this case, like, out of 1000 nodes, the majority, like 80 plus percent, are a particular color, it becomes really unlikely for the other, you know, color, the minority color, to ever win out again. So if most of us have decided that we're going to Chipotle, but some people are still at Mos, it's very unlikely that most people will ever win out.
00:09:13.956 - 00:09:20.644, Speaker B: In fact, it's very, very likely, you know, one minus epsilon, that we will all go to Chipotle just to ask a question.
00:09:20.724 - 00:09:40.986, Speaker A: Sure. Yes. Not be controlled by the adversary. Okay, so what is it that underlines the security assumption here that makes this probabilistic and enlightenment not just an action that the adversary can force.
00:09:41.130 - 00:10:05.280, Speaker B: So let's make sure that I state this from the very beginning so there's no confusion. There is no global randomness. Everybody just samples locally. Okay, and the second thing you're asking what the adversary can do. In our model, yes. The adversary can do anything it wants. It can check well, it can only not change the state of correct nodes.
00:10:05.280 - 00:10:14.352, Speaker B: So it is bounded by some percentage. It can view the state of every other node infinitely quickly, and it can change its own values infinitely quickly.
00:10:14.408 - 00:10:16.644, Speaker A: But can you fully control your network?
00:10:17.224 - 00:10:46.982, Speaker B: The message delays, yes, in our model, yes. And it will shut down liveness in that case. But if you assume that it doesn't control the network, message delays. This system will make progress, but it will still be safe even if it controls the message delays. So liveness, synchronicity, safety, doesn't matter. It's good across it. Okay, so you may have heard of me trolling people online about avalanche.
00:10:46.982 - 00:11:18.018, Speaker B: And this is what we're on Twitter, this is what we're building. So avalanche is effectively one of the implementations of this family of consensus protocols, this probabilistic business protocols. It builds a dag, and I will show you a linear version of this very soon. But it builds a dag because payments, if you want to build a cryptocurrency, are commutative. You don't really need to order them in a linear order. And I'm not going to go through the, because this happens to be like a secondary talk almost on how to build DAG based payments. And it gets really complicated.
00:11:18.018 - 00:12:03.460, Speaker B: But effectively, if this means Alice pays Charlie as a transaction, and then I introduce another transaction that says Charlie pays Fiona, if I sample this transaction and people say this transaction looks good, then it automatically means this one is good and this one is good as well. So every single transaction is queried only once, never again. You move on to the next transaction that comes after it. I'll skip this. So in a real implementation, this is 2000 nodes, up to 2000 nodes. This is actual real c implementation, geo distributed on AWS with latency simulated no performance changes across the number of nodes. It doesn't really matter if it's 125 all the way to 2000, it performs the same, and we haven't gone to 10,000 and so on.
00:12:03.460 - 00:12:37.084, Speaker B: But I think this sort of shows roughly how this performs subsecond latencies, which is pretty nice. So about 200 milliseconds on average, and maximum about like one to 2 seconds on the tail. Okay, so avalanche achieves a partial ordering, which is good for commutative operations like payments. But if you want to achieve a total ordering, let's do it. So, pretty simple actually. It's effectively avalanche. But the dag builds every node has one child and one parent.
00:12:37.084 - 00:13:18.158, Speaker B: Every node samples. If this is a fork in the system, and each one of these blocks, let's be one transaction per block. Every node samples it only once. Initially, there is a low threshold for choosing the fork. Let's say I just simply choose a simple majority, whichever is a simple majority, so I can tie break at some point. If I achieve some sort of a depth here, I'm going to call this depth DL, then I move to a higher threshold majority. And if I get a high threshold majority for some number of beta consecutive rounds, I am statistically guaranteed that we have all moved to this chain right here.
00:13:18.158 - 00:13:46.374, Speaker B: As long as we don't violate the number of byzantine nodes being above some threshold and I can choose this value to whatever. If I choose it to be roughly two thirds, then I can tolerate up to one third for liveness in byzantine nodes. And if I make it really high for safety guarantees, then I obviously can tolerate less in liveness. So a byzantine adversary can just choose never to reply and we don't make any progress. But I think safety over liveness here is quite important.
00:13:47.354 - 00:13:47.754, Speaker A: Yeah.
00:13:47.794 - 00:14:33.144, Speaker B: So it achieves that and it becomes, at some point this becomes the finalized state. So it's interesting, we get about Oauth one message complexity, and I think it's an interesting set of family of consensus protocols. They are safe under asynchrony. The adversary can choose whatever message delays it wants live, only under synchrony, of course. And effectively they just give you, the claim is here in the normal case operation, they give you optimal message complexity. I want to show you as well. I'm here to plug and I want to show you a real, this is actually the entire Ethereum virtual machine running on Snowman.
00:14:33.144 - 00:14:51.144, Speaker B: It is live. We're only blasting it little by little, but it is live. It's working and it's running on a 25 node implementation, which is low. But that's because we don't want to overpay on AWS. So we can blast it to 2000 and so on and it'll be good. So I think that's it. Thank you.
00:14:51.144 - 00:14:57.232, Speaker B: We have some time for questions.
00:14:57.408 - 00:15:16.192, Speaker A: Yes, couple of questions. So you said the sampling is local, right? Yes. So it is possible two node sampling, same set of nodes can also come. Yep. And a lot of them can actually. I mean, because benzene nodes say one third is there, but those can actually be part of those. A lot of them actually.
00:15:16.192 - 00:15:22.352, Speaker A: And they equivocate for one set of node. They can say red, another set, they say blue. How do you detect that?
00:15:22.448 - 00:15:23.784, Speaker B: You don't. You don't care.
00:15:23.944 - 00:15:27.992, Speaker A: So, I mean, when do you come to know that equivocation, I mean, it's.
00:15:28.008 - 00:15:28.808, Speaker C: Not going to affect.
00:15:28.896 - 00:15:30.704, Speaker A: How do you realize that you don't.
00:15:30.744 - 00:16:06.084, Speaker B: Care about detecting equivocation? That's the nice thing about this. We can build more sophisticated versions of this that detect it, but you don't care. You basically the following statement guarantees that you are safe if you achieve high threshold majority for some beta consecutive rounds. So I see like 80% blue for 20 times in a row. I know for a fact there is no chance as long as the number of Byzantines hasn't been, you know, it's not above the assumption that we have set up, then there is no way for the other color to ever win out. So you don't care about detecting equivocations here.
00:16:06.124 - 00:16:09.044, Speaker A: So how many rounds can you require, and how much is the sample size.
00:16:09.084 - 00:16:36.434, Speaker B: You'Re assuming, in this case, for the experiment that we ran, we ran a sample size, so the security guarantee that we put up for that experiment was a failure. Once every 20,000 years running, with a network running, assuming at 10,000 transactions per second, that was the failure rate that we chose. And the sample size, I think, was about 20 nodes per round, and the threshold majority was about 80%.
00:16:36.744 - 00:16:38.528, Speaker A: How many rounds did you require to reach?
00:16:38.616 - 00:16:49.964, Speaker B: Oh, it's log. It was, like, less than 17, for sure. Yeah, but if you amortize it in the blockchain, then it's over.
00:16:52.864 - 00:16:55.272, Speaker D: So the adversary can control the network.
00:16:55.368 - 00:16:55.888, Speaker A: Right.
00:16:56.016 - 00:16:59.952, Speaker D: So he can make sure that only the red guys can talk to each other, and only the blue guys can.
00:16:59.968 - 00:17:00.768, Speaker A: Talk to each other.
00:17:00.896 - 00:17:10.659, Speaker D: And he can do this for as long as he wants. Red and blue will not each decide because they need to get some aggregate level of signatures.
00:17:10.771 - 00:17:14.023, Speaker B: No, they just need to, because the repeated samplings will fail.
00:17:14.883 - 00:17:15.235, Speaker A: Oh.
00:17:15.259 - 00:17:19.179, Speaker B: Because they will sample. The adversary shuts it down. It doesn't get the consecutive.
00:17:19.251 - 00:17:22.859, Speaker D: They're successfully sampling within their little groups, their divided groups.
00:17:22.891 - 00:17:37.990, Speaker B: No, they have view of the network. They're sampling randomly from the entire view of the network. Sure, sure, it can. But even if it's completely splits in half and I keep sampling, I see that like, I'm not getting threshold majority from. Because this entire half of the stadium is shut down.
00:17:38.142 - 00:17:39.794, Speaker D: What is threshold majority?
00:17:40.254 - 00:17:44.158, Speaker B: If you set it, let's say, 80%. I say that if I need to.
00:17:44.166 - 00:17:46.374, Speaker D: See an aggregate signature that represents 80%.
00:17:46.414 - 00:17:59.614, Speaker B: Of the network, no aggregate means something specific. I sample, let's say, ten people in this room, and if I see eight of them say the same thing, then I consider that to be successful. Like a successful round.
00:17:59.734 - 00:18:04.942, Speaker A: If I understand correctly, you're assuming that you have authenticated point to point connection to members of the six.
00:18:04.998 - 00:18:11.374, Speaker B: Yes, yes, yes. You know the number without me noticing it.
00:18:11.414 - 00:18:15.934, Speaker A: If I have authenticated point to point connection. Correct. Correct.
00:18:16.094 - 00:18:22.294, Speaker D: Ah, you choose the quorum ahead of time. The adversary does not get to choose the quorums.
00:18:22.374 - 00:18:23.034, Speaker A: Yes.
00:18:23.434 - 00:18:24.218, Speaker D: Okay.
00:18:24.346 - 00:18:27.738, Speaker A: Right. Which brings me to my next question, which is, how far are you from.
00:18:27.786 - 00:18:45.414, Speaker B: Developing a quorum change, from developing what we have? So, I mean, the quorum change, which quorum changed. But let's be clear here. The quorum change for the. Yeah, I'm not sure. There's no quorum here. It's only local. There's no canonical quorum.
00:18:46.234 - 00:18:50.014, Speaker A: How do you fair sampling without knowing what the participants are guessing?
00:18:50.504 - 00:18:54.244, Speaker B: So, no, you uniformly sample from the set of participants.
00:18:55.104 - 00:18:57.604, Speaker A: The question is, how do you change that set?
00:18:58.144 - 00:19:13.084, Speaker B: You mean, how do new participants come into the system? Ah, for a staking based protocol. They come in, they say, I would like to become a validator. So they submit a staking transaction. They lock up some number of coins, and then they can participate in the system.
00:19:13.384 - 00:19:17.056, Speaker A: So you're very compelled on chain. Do you have it implemented yet?
00:19:17.160 - 00:19:23.484, Speaker B: Yes, yes, this is fully implemented currently in testing mode. I see what you were asking for.
00:19:23.784 - 00:19:24.564, Speaker A: Yes.
00:19:24.944 - 00:19:59.552, Speaker B: State your models and results again more formally, like synchronous, partially synchronization, adaptive static safe. Yes, safe. Under a asynchronous network, the number in normal case scenario, which means that the adversary is not the adversary, is less than the assumptions that you assume and so on. Then you get logarithmic number of steps to confirmation and o of one amortized message complexity. Pki. You do need PKI. You do need PKi, correct?
00:19:59.608 - 00:20:02.328, Speaker A: Yes. How many faults still tolerate? One half or 30?
00:20:02.456 - 00:20:19.304, Speaker B: You can parameterize it. So currently we parameterize for high safety and lower, we set the threshold to about 80%, which means that it's about 20% for liveness. So if more than 20% of nodes are byzantine, you shut down liveness.
00:20:21.124 - 00:20:34.944, Speaker A: Can I repeat my question from the beginning and see if you take any of this probabilistic voting processes that I know. The mean time it takes to converge is log m. Yeah.
00:20:35.084 - 00:20:45.168, Speaker B: It's the mixing time of the Markov chain. Yeah. So why did you say or one with, ah, amortize? Because for every. So if you had a binary, I.
00:20:45.176 - 00:20:53.120, Speaker A: Mean, what I'm saying is classical results, there's no adversary. The communication graph is complete. Sure. There's no problem at all. Yes.
00:20:53.232 - 00:21:10.904, Speaker B: For every round in a classical protocol, you can amortize, like, for example, hot stuff does. You get o of n, because at every round, you do sampling of the entire network. For us, we do a constant size sampling. That sample that we choose, it's actually independent of the network size. So it's o per round.
00:21:13.404 - 00:21:16.064, Speaker A: But that doesn't converge to a majority.
00:21:17.164 - 00:21:25.424, Speaker B: It converges to a majority in log n, in login, number of rounds. But per round, I only have message complexity, constant size.
00:21:28.604 - 00:21:35.164, Speaker A: So to achieve consensus, you need to login steps. Correct. So n log n steps in the whole network?
00:21:35.284 - 00:21:43.436, Speaker B: In the whole network, yes. To achieve it's amortized. Absolutely correct. It's in the time. Exactly right. Exactly right.
00:21:43.460 - 00:21:54.628, Speaker A: Yes, yes. In the network partition case, I still don't see how it's not possible for the two side of network to have arrive at different states.
00:21:54.676 - 00:22:12.354, Speaker B: Oh, it's possible. It's just very unlikely. If you split the network in half and I keep sampling and I'm unable to reach half the network, every one of my samples will fail. I can time out just to make progress, but I will not reach this beta consecutive super majority threshold. It'll be very unlikely. The supermajority means like 80%.
00:22:13.374 - 00:22:19.286, Speaker A: So he was saying that there's a quorum, you have to connect to a set of colon. Or does it?
00:22:19.430 - 00:22:26.854, Speaker B: No, the quorum is local. You choose it locally for every round. It's just a random set of quorum nodes.
00:22:28.314 - 00:22:29.094, Speaker A: Yes.
00:22:30.234 - 00:22:32.426, Speaker C: Synchronous or asynchronous in terms of the.
00:22:32.450 - 00:22:43.254, Speaker B: Communication for if you want liveness, it's synchronous. Any protocol has to be synchronous. Yeah, correct. Correct.
00:22:46.074 - 00:22:55.404, Speaker C: If I speak in login rounds world or an asynchronous world. So how do I know that the result that I'm getting is correct?
00:22:55.444 - 00:22:56.024, Speaker A: Because.
00:22:56.524 - 00:23:24.364, Speaker B: Oh, you don't set. I see. You don't set. So what the protocol says is that if you get beta number of rounds, consecutive super majority, then you know that that decision has been finalized. It doesn't say that it's necessarily the case. For example, if the byzantine adversary controls the network and splits it in half, it will happen in logarithmic number of rounds. If the network, if the adversary splits the network in half, it'll take exponentially many rounds for it to finalize.
00:23:27.184 - 00:23:27.964, Speaker A: Yes.
00:23:28.504 - 00:23:32.312, Speaker B: If I want the same tolerance for liveness and safety, yes.
00:23:32.368 - 00:23:33.896, Speaker A: Is it the one third or one half?
00:23:34.000 - 00:23:43.324, Speaker B: It's one third, yes. That's where they meet. That's all voting based protocols that are safe under asynchrony, that's what they meet.
00:23:47.344 - 00:23:53.400, Speaker A: How much degradation do you get if you start introducing faults? Have you introduced some faults in your example? Yes.
00:23:53.472 - 00:24:49.146, Speaker B: So if you have a leaderless version, so there's no node that creates any univalency, like the network can be split in half every single time. Then there is a well known upper bound of square root n, number of byzantine nodes to stall logarithmic number of rounds, progress, liveness. So it would take more than logarithmic number of rounds if the byzantine adversary is more than square root n. If you introduce a leader and the leader just really means a soft ticketing service, like you send it a transaction and then it sends it to a few other nodes and those will do. Then the propagation, then that leader, it becomes effectively the same as other classical based protocols. So you can get it to one third, roughly one third minus some errors, because there's some probability there of failure. It's like effectively classical voting based protocols, except that I don't use the entire quorum of the network.
00:24:49.146 - 00:25:25.084, Speaker B: I still get those guarantees of super majority. But every single sample that I get, unlike in classical, where I know for sure that it's probability one, that my view, my local view is the same as the global view, because by definition my local view is the global view. In our version, the local view has some error from the global view. So I repeat the local view a few times to get a very high estimate of the global view. That's really all it's doing in the background. And then it turns out that each one of these rounds can be amortized. So you can get really nice performance here.
00:25:31.484 - 00:25:43.104, Speaker A: Yes, yes. Show some red and some blue. But they don't care. They don't have an opinion. Right. So they need to get this initial color. Yes, yes.
00:25:43.104 - 00:25:55.276, Speaker A: How is that defined? They ask a bunch of people, yeah. And most of them have no view again, right, because they're gray. Yes, yes. So how does this initial propagation happen?
00:25:55.460 - 00:26:14.322, Speaker B: That is happening through sample. So whenever I sample somebody and I say, first time I hear about red, I ask other people, hey, what do you prefer, red or blue? They don't care. Exactly. So they give you empty answers. They say, we don't have this. So that was a failure on your end. So, okay, I repeat it.
00:26:14.322 - 00:26:30.804, Speaker B: Now. Each one of those guys, when they first hear about red, they say, oh, okay, let me ask what other people think about red. Now I know about red. It was my first value that I saw. It's the one that I prefer, I guess, because of the first one. So now I'm going to sample other people. If somebody else sees blue for the first time, they go to blue and they start sampling other people.
00:26:31.984 - 00:26:44.456, Speaker A: So when I initially do this process, most people say, I have no idea. And one person says, actually, I think it's blue. Yes. Will you accept blue?
00:26:44.600 - 00:26:55.186, Speaker B: If the majority of. I will accept anything any individual says, but I will only move to the majority color. I will only change my mind if I'm correct.
00:26:55.330 - 00:26:58.130, Speaker A: But if you have no opinion, you will just take whatever.
00:26:58.242 - 00:27:00.774, Speaker B: No, you will take the majority per protocol, correct?
00:27:02.354 - 00:27:02.810, Speaker A: Yes.
00:27:02.882 - 00:27:10.934, Speaker B: If you're byzantine, you can choose to say red, blue constant. You can do whatever you want. You can say anything you want.
00:27:11.554 - 00:27:11.866, Speaker A: Yes.
00:27:11.890 - 00:27:15.810, Speaker C: Karthik, how do you move from binary to multivalued scenario?
00:27:15.962 - 00:27:36.726, Speaker B: Ah, so, great question. So in the leaderless case. Yeah, so the leaderless case, each fork is defined via a patricia tree. So you have. I mean, each block is defined as a hash. You are making progress on the first bit of the hash. That's the binary decision.
00:27:36.726 - 00:28:00.634, Speaker B: So each fork will have either a zero or one as the next decision. And if the fork, if you want to really make it such that you have 256 decisions, then you would have to create a fork that is exponentially large, basically would mean that at every bit, you would have to create two forks as an attack.
00:28:01.094 - 00:28:05.314, Speaker C: But I can corrupt the value by just corrupting one bit of your hash.
00:28:05.734 - 00:28:07.594, Speaker B: Corrupt the value would.
00:28:08.134 - 00:28:12.452, Speaker C: So the way I understood it is you are doing this bit by bit for the hash values.
00:28:12.508 - 00:28:13.784, Speaker A: Yes, yes.
00:28:15.124 - 00:28:41.652, Speaker C: And you are going for either red or blue. And the guarantees that you have at the end of it is you will have one of the two values. You don't know which. So if I am an adversary, if I have, let's say, a third stake, I can, and let's say all the honest want to commit to one, I can come up with a zero value and make them commit to a zero value just for one bit. And then the entire hash is corrupted because everything is ok except for one bit.
00:28:41.788 - 00:29:04.280, Speaker B: And you have, I mean, you check the validity of the block, the hash first. And now we start. So if I see that I have two blocks, if I have these two, and these both look like valid forks, this one starts off with a bit of zero. This one starts off with a bit of one. I'll decide on that side first. Ok. I decided the first bit is zero, which is this one.
00:29:04.280 - 00:29:06.324, Speaker B: So I discard this one.
00:29:08.144 - 00:29:13.696, Speaker A: All right. We need you to not be byzantine and mind the next. Absolutely, absolutely. Yeah.
00:29:13.720 - 00:29:15.192, Speaker B: Yeah. I'm doing a liveness attack here.
00:29:15.208 - 00:29:15.352, Speaker A: Yeah.
00:29:15.368 - 00:29:19.344, Speaker B: So I think I'm done with talk. So please talk in one.
