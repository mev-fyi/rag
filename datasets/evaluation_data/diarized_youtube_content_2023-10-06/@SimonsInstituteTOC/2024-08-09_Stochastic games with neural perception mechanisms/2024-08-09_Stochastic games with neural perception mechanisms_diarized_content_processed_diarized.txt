00:00:00.160 - 00:00:44.100, Speaker A: You want to hear Marta telling us about mechanism? But I just have a quick announcement. One of the participants of the workshop yesterday positing for Covid. Okay, so somebody said it's Covid. We had the same experience two weeks ago when somebody has Covid, nobody else got. But just be aware, kind of self self aware. Apparently the office, if you go to rooms, I think 118. If you suspect you have anything, they have access to Covid tests.
00:00:44.100 - 00:01:14.590, Speaker A: So if you take COVID test and just be responsible and mindful of other people. And we have some people who are remote working remotely. And there is a Google Doc and Jordan will monitor it and he may have remote questions. Yes. Just keep it off until.
00:01:21.860 - 00:01:49.542, Speaker B: Good morning. Sorry, I'm still kind of living on UK time, according to my laptop here, so it's great to be here, I think. I've been to the Simons Institute several. You will adjust. Okay. Is that better? Yeah, yeah. Okay.
00:01:49.542 - 00:03:05.740, Speaker B: And I'm particularly pleased that there are so many game theory experts, because what I'm going to talk about is actually trying to use game theory, but for the purposes of modeling and verification and strategy synthesis. But I'm also trying to take game theoretic models to the setting that we are all seeing around us, which is driven by AI and machine learning. So the topic is stochastic games with neural perception mechanisms, and they are neurosymbolic stochastic games. And I will get to this and try to explain the main ideas. So, this work started more than I 20 years ago with my collaborators, Dave Parker and Gessen Norman, when we followed people like Ken McMillan and Ed Clark, who were working on model checking for real systems. But we started working on model checking for real probabilistic systems. Now, perhaps that car is a little bit ambitious.
00:03:05.740 - 00:03:37.630, Speaker B: You will see examples of various components of cardinal cars, but I don't know if you're aware. I've just actually seen it in the museum, computer history museum. This is the original Google car, which has been retired now. It doesn't have a driving wheel. That's the original autonomous car. But if you go to San Francisco, you can actually get a ride in an autonomous car. But it's not the Google one, it's a different make.
00:03:37.630 - 00:04:23.000, Speaker B: So the idea of what we call probabilistic model checking verification for probabilistic systems is to model a system or a system component which has probabilities, give it a specification as a temporal logic, and feed it into a probabilistic model checker. Would you go out onto the road? I think you probably see a little bit of examples later. I think maybe we should. Maybe. No, I think it's four nines. Four nines reliability. So it's 0.001.
00:04:23.000 - 00:05:00.822, Speaker B: Yeah. Okay, well, and then, you know, the idea is that once you've given it this specification in temporal logic, you just get an answer. Yes, this is satisfied. You know, perhaps you get plots of probabilities and you can also extract strategies. So in terms of the timeline, we started working on this around 2000. You know, this is when prism originates from. But in fact, the first algorithms go back to the 1980s and include people like Moshevadi and Korkubetti, Sianna Kakis, etcetera.
00:05:00.822 - 00:06:09.620, Speaker B: But now the area is considered of mature and industrial relevance, and there are several other model checkers. And because probability is pervasive, probability is not just used for randomization and quantification of uncertainty, but it is also used in simulations, it's used in systems biology, et cetera. It has spread into other domains. And I can mention that there are about 700 papers that use or extend prism in these domains. But what this talk is really about is questioning whether the original models that we started with, which were Markov chains and Markov decision processes, are actually well suited to the kind of abstractions that we need for the complex scenarios. And just think about this autonomous car. We have human and artificial agents, and in reality, they have distinct goals.
00:06:09.620 - 00:07:10.616, Speaker B: And the difference from Markov chains and previous situations is that they have autonomy. I mean, these cars actually drive themselves, so they make decisions for the drivers. And the scenarios are both competitive, collaborative, there is context, et cetera. So sort of going back to about 2016, I started saying, well, we need to adopt a game theoretic view also for these kinds of situations, because we have to account for the uncontrollable behavior in order to synthesize strategies that can actually be safe. And gains, of course, have been used a lot in other areas, in economics, in particular, security, energy management. You might think that they are not using autonomous driving. I can prove you wrong.
00:07:10.616 - 00:08:13.650, Speaker B: This is a paper from Berkeley from a few years ago, where they have actually used dynamic games. What for? So here they've used dynamic games, but situations such as merging into traffic are not really well suited to rational behavior. What you have to do is you have to do negotiation, and they do it as a kind of interplay of the short and long horizon. Okay, so I will first start by giving a brief overview of what we can now do in prism. Actually, Prism games, which is an extension of prism to model concurrent stochastic games. I claim that concurrent stochastic games are actually a realistic abstraction. We started off with turn based games, but because they are very convenient in turn based games, there is at most one player that controls the state.
00:08:13.650 - 00:09:11.758, Speaker B: So in fact you can reduce the analysis to MDP, mark of decision process analysis. But for concurrent games, you cannot make that assumption. You have several players acting concurrently. And what these players do is first, independently, they make a choice of the action. And then once that choice is made, then there is a joint, in this case the stochastic game. So it's a joint probability distribution that takes the game to the next local, to the next global state, and the symbol bottom is introduced so that you can have no action, and that will model an asynchronous behavior of the agents, but within the synchronous concurrent game behavior. Okay, so stochasticity arises from various situations.
00:09:11.758 - 00:09:42.864, Speaker B: Sometimes we use randomization, such as in various protocols. There are even game theoretic protocols for wireless. But in other cases, it's just simply uncertainty. Just one way to quantify uncertainty. Now, what we can do for concurrent stochastic games is we can do model checking. Well, first, to do model checking, we need specifications. And for specifications there is a variant of a logic, ATM alternating temporal logic.
00:09:42.864 - 00:10:31.596, Speaker B: So think about it. So, logic with coalition operators, and it is extended with probabilities following PCTL and also rewards, so that we can do cumulative reward based analysis. But in contrast to mdps and turn based games, what we have to do is we have to solve zero sum two player games. But we do that within prism and also prism games through value iteration. So the complication is. So even though the formulation is like for turn based games, for turn based games, because there is only one player controlling the state, you can simply maximize or minimize. But here what we do is.
00:10:31.596 - 00:11:07.710, Speaker B: So I'm showing you an example of reachability of the stick state. Well, if it's reached, then it's one. Otherwise you have to solve a matrix game. Okay, you have to compute the value. Now you have to compute the value. In this case, I'm showing you an example of reachability. But later, I focus mostly on computing optimal policies for infinite horizon, because I will get to the kind of games that I'm working with.
00:11:07.710 - 00:12:30.574, Speaker B: But in this case, notice that, I mean, we can do it through LP, but this is required for every iteration of value iteration. But you know, it works. But we can also extend it. And that's already implemented with equilibria based properties, because there is no reason to assume that games are zero sum, we have distinct objectives. So instead of specifying zero sum properties, we just focus on one coalition, okay? And one coalition will maximize the probability of reaching the goal, irrespective of what all the other members do, okay? Which is all the other players of the game are grouped into another coalition. In this case, what we have is we can specify, say, a pair of agents or coalitions, and each will have their own objective, and we can ask them to say maximize or minimize. And in fact, what we work with is we work with Epsilon Nash equilibria.
00:12:30.574 - 00:13:33.490, Speaker B: These are the standard Nash equilibria, but we avoid undecidability problems. And we work with Epsilon Nash equilibria. But from we select these equilibria according to different constraints. The classical constraint is social welfare, and social welfare just maximizes the sum of the rewards, okay? There is also something nonstandard that we use, which is minimizing the cost, and this is simply maximizing the negative of the rewards. But we also bring a concept called social fairness from economics. And what social, this is non standard in the games that we see around here. But what it does is that it minimizes the difference between the players, okay? That's why it is called fairness.
00:13:33.490 - 00:14:48.378, Speaker B: So what we can do is we can also specify these equilibria based properties, and we can extend the verification of concurrent stochastic games to not only verify, but also synthesize these equilibria where you can select between Nash, or actually a correlated equilibrium, and you can select because between social welfare, social cost and. And social fairness. Now, in this case, it turns out that you have to solve a bi matrix game. But in cases where one of the players has already reached the state, this is an example, simple example of reachability. Then we can reduce the rest to an MDP reachability problem. Okay? So that's, you know, implemented, and this is based on a standard algorithm known as the labeled polytopes, but it is, you know, included in there. So just to give you an example, this also has been implemented for multiplayer games.
00:14:48.378 - 00:15:39.380, Speaker B: So this is a typical car parking, automated car parking example, where we have these green slots are the free slots. And what we want to do is we want to synthesize an equilibrium strategy, which in this case, for example, we, we can maximize social welfare, but we can also minimize the cost. So this is the rewards operator, that's the probability operator, and we minimize the cost in terms of the distance traveled. Okay? So that's something that you can play around with. But I showed you the example of a car parking. Now, I want you to think about the reality. So if you go around San Francisco, you will see these autonomous cars and their vision is actually implemented using neural networks.
00:15:39.380 - 00:16:52.930, Speaker B: So the question is, well, to what extent can we extend the stochastic games framework by allowing the same complex scenarios, but complicate them even more, but model the realistic situation. The agents actually have perception, and that perception is implemented as neural networks. Now, because of that, that what we have to consider is continuous state spaces. Why? Because no one trains neural networks on discrete values. Even if you have discrete values, all these values will be mapped into real vectors and real valued vector spaces. Okay, so this is the one thing. But the other thing is we also have to consider partial observability, because once you go to continuous state spaces that working with real value, teachers cannot observe individual states and cannot work to value iteration with uncountably many states.
00:16:52.930 - 00:18:06.050, Speaker B: Because I'm focusing on implementation, I'll be now looking at various assumptions that I can put on this setting to actually give me something that is tractable. Okay, that's the distractible, but for realistic neural networks. Okay, so this lecture is giving an overview of several papers that are sort of various stages of acceptance, where we started off with a fully observable case. We also have a variant for a. Yeah, and then we have the POMDP, which is a restricted one, and most recently stochastic gains. And the focus, as I said, is more on what are the challenges in implementing these techniques so that we can use them in a way that, say, model checking is used in industrially relevant problems. Okay, so what I do is I actually try to work with systems that are, in some sense, I should admit, in some very weak sense, neurosymbolic.
00:18:06.050 - 00:19:18.290, Speaker B: Now, neurosymbolic is an umbrella term, and I will explain in a minute what that in a week neurosymbolic combination is that I'm focusing on. But the history of AI itself started with symbolic and logic based systems. And at the time, the neural aspects were very small, because even though the concepts existed and there were people like Jeffrey Hinton who actually tried to implement them, there were no GPU's. So there was not enough compute power to implement them. But of course, the situation has now changed. And as you can see, this is the deep learning, and it's trying to dominate symbolic AI. And I think for us, kind of working on the symbolic side, I think what I'm trying to say, we really have to take that into account, because otherwise we will be kind of pushed into a niche that is not relevant for the real applications.
00:19:18.290 - 00:20:21.680, Speaker B: The real challenge of neurosymbolic, if you talk to people in AI, is how to combine them so that you get high performing. That is, you know, the best possible recognition, fastest, etcetera, neurosymbolic AI systems. But I work on this kind of combination, and this. In a lecture at Aaaidhe, Henry Kautz proposed six different ways for combining symbolic and neural, and I'm showing you only three. So, here you have symbolic, which calls neural as a sub procedure. Here, neural calls symbolic as a sub procedure. I kind of take this view that, well, what we want is we want symbolic and neural to work side by side, exchange information.
00:20:21.680 - 00:21:40.426, Speaker B: And this view permeates the rest of the lecture, because now, if you think, what does it mean for a neural. For a neural network to provide information that the symbolic system can use, I will want to convert it to symbolic information. Focus of this is not on performance, but on safety and rigorous engineering of such systems. So what are neural perception mechanisms? Okay, so what I'm trying to do is I'm trying to take concurrent stochastic games as before, okay, games on graphs or whatever, symbolic, to start with, discrete. But I now I want to add neural networks to them, and I restrict them to adding neural perception mechanisms to the behavior of the agents. Okay? Now, why? Because neural networks have really been very successful. They are very good, very fast, provide fast inference.
00:21:40.426 - 00:22:26.082, Speaker B: They convert signals. So the information that we get from the environment signals, sometimes they are called impressions of the environment, into concepts. Okay? So really, is this a car? Is this a red traffic light? Okay. Behavior prediction. So, I'm assuming that in my game, each agent uses a neural network classifier. That's already a restriction, because I want to use the learned information. Each of these pieces of information is a large concept, but I restrict them to finitely many, because I want to use them in the controller of a conventional symbolic controller.
00:22:26.082 - 00:23:38.634, Speaker B: So, these percepts are learned concepts. They are stored locally. So they are a bit different from the way that POMDPS work, and they are used in decision making. I focused on trained neural network classifiers, and in fact, the theory depends on Relu, but it is not restricted to Relu, because there are ways convex relaxation of other neural networks that produce actually linear boundaries. I also assume white box access to neural networks actually want to consider exactly the regions, the regions of the environment that are classified as these concepts. You may be aware that there is some work, for example, from Hilavigent, from Sanjeev Sashi, on perception contracts, but they assume black box access to neural networks. So they are useful for testing, and also from carina, where they built probabilistic abstractions based on confusion matrices.
00:23:38.634 - 00:23:58.830, Speaker B: So what we do is something much more involved. Okay. But in some sense, you can say that we, at least to start with as a baseline, we have to consider how far we can push it. And that's been the challenge of this work. Okay. Yeah.
00:24:04.140 - 00:24:17.628, Speaker C: I'm a little confused about what white box buys you in this scenario, because the black box, I sort of kind of consider the black box neural network as an abstraction for yet another sensor in the system.
00:24:17.684 - 00:24:18.116, Speaker B: Right.
00:24:18.228 - 00:24:25.780, Speaker C: It kind of senses things from the environment, passes data. But with the white box, what extra things are you hoping to achieve?
00:24:25.820 - 00:24:29.554, Speaker B: So I'm kind of hopefully show you on the next slide.
00:24:29.722 - 00:24:33.790, Speaker C: Okay. I was hoping for like a problem statement before you show us the solution.
00:24:39.090 - 00:25:23.620, Speaker B: Okay. So what I do with these neurosymbolic games is, you know, I have an agent, the agent has a neural network. In fact, I have two agents or two coalitions, and they have to observe the environment. And they can observe the environment only through the perception function. Okay? So they have no direct access to global states. They can extract this learned concept, store it locally and use it in decision making. So you can do knowledge acquisition from the learning concept.
00:25:23.620 - 00:26:01.534, Speaker B: And this is reflected in the state space. Okay. And also in the, in the observation function and the decision making. I have, you know, not included. So what is the white box access giving me? If I take something like four by four car parking example, I have two cars and two possible spots for parking. I need localization. I can use a neural network to provide localization.
00:26:01.534 - 00:26:55.200, Speaker B: And localization will take the position of the car and give you the perceived grid cell. Okay? And this can be obtained as a neural network which is trained. If you look there, you see that the boundaries are not perfectly aligned. And even though this is a single color, this is not a single polytope. In fact, this is, you know, a number of polytopes. So I work with these, what I call land percepts, which are finite connected covers, which you can reduce to polytopes, polyhedra, these kinds of polyhedral representations.
00:26:58.380 - 00:27:05.900, Speaker C: So a filter of some sort would not be able to achieve the same thing as in, you just ask for all the data and filter it down.
00:27:06.060 - 00:27:24.140, Speaker B: Yes, you can do exactly the same once you've reduced it to the features. Yes, you do exactly the same. So then the feature is the learned concept. I'm giving you an example where you just use localization, but for anything land. That's the idea.
00:27:25.360 - 00:27:29.608, Speaker C: You're not changing the nature of the neural network. Or are you changing the nature as.
00:27:29.624 - 00:27:34.660, Speaker B: In, no, no, I'm not changing the nature of the neural network.
00:27:35.240 - 00:27:43.640, Speaker C: So it's just that you sort of say, I'm not interested in the entire output, just give me this particular part or this abstraction of the output.
00:27:43.800 - 00:28:00.120, Speaker B: I will build for verification. I will build an abstraction which will work with these polyhedra. Exactly. Right. So what.
00:28:04.060 - 00:28:10.604, Speaker D: I also have a question. So in your state space, the last component is the environment, right?
00:28:10.692 - 00:28:11.292, Speaker B: Yeah.
00:28:11.436 - 00:28:26.260, Speaker D: And this perception one, perception two. These components, they are obtained from the environment. So isn't it, is this redundant, this information? Or can there be many?
00:28:26.760 - 00:28:52.800, Speaker B: No. So I mean, that's, no, this is not redundant information. It just, in fact, we restrict this set of states a little bit more to ensure compatibility. So we only allow states for which these perceptions are here. But no, this information is not redundant. This information is used in decision making.
00:28:54.500 - 00:29:03.092, Speaker D: By redundant, I mean given location one, location two, plus an element of se.
00:29:03.276 - 00:29:09.800, Speaker B: Right. Because I'm assuming two agents, but I'm showing you a picture of just one agent.
00:29:11.010 - 00:29:19.866, Speaker D: By redundant I meant something else. Then you have this observation function which gives you a single value given these.
00:29:19.898 - 00:29:23.590, Speaker B: Two locations plus an element, because that's observation for I.
00:29:25.570 - 00:29:35.430, Speaker D: Now, in your state space, can there be states where this perception value is different from the one obtained by the observation function.
00:29:37.290 - 00:29:46.270, Speaker B: In the states? No, what I'm saying is this what you mean? I'll ask for compatibility between this and that.
00:29:46.570 - 00:29:53.202, Speaker D: And compatibility means that it is exactly the perception obtained by the observation function.
00:29:53.266 - 00:29:59.914, Speaker B: Yes. Okay. Yeah, I think so. This is one of the restrictions.
00:30:00.042 - 00:30:20.200, Speaker E: Yes, just, sorry, one more clarification is the goal of this abstraction to extract a meaningful abstraction where agents actually operate and can observe the environment or to abstract the whole state space. So to clarify, do you also include something that the neural network doesn't percept.
00:30:22.220 - 00:30:24.476, Speaker B: That the neural network doesn't?
00:30:24.668 - 00:30:35.856, Speaker E: So if you extract this abstraction with the neural network, so this high level concepts, there must be something also left out that the neural network didn't. Do you also include that into the abstraction?
00:30:36.008 - 00:31:05.080, Speaker B: Well, so it's, so in this, in this case, in the way this is defined at the moment, I don't, I assume I have a full cover of the environment, but you can specify it so you can modify it, for example, so that you, you have areas maybe where you don't know, but that would be the next step. Step something that you don't know. Okay.
00:31:05.120 - 00:31:06.192, Speaker E: Yeah, fair enough. Thanks.
00:31:06.256 - 00:32:22.200, Speaker B: Yeah. But at the moment, we assume that it's all fully covered. Right? Okay, so what can we do? So we focus on zero sum, fully observable to start with, and a discounted setting over the infinite horizon and under full observability, and under some restrictions, which include this compatibility restriction, but also includes Borrell measurability. Because we are working with continuous, in fact, hybrid state spaces, we can show that value exists and it is a fixed point of a minimax operator. But optimal value in the limit may not be finitely representable. Since I have an uncountable space, I just cannot do value iteration over these uncountable vectors. In fact, in order to do value iteration, I exploit structure.
00:32:22.200 - 00:33:45.916, Speaker B: And that involves first taking a neural network and computing the preimage of the neural network, which is then piecewise continuous as a function from the environment to the percepts. Okay? And that gives me the initial decomposition. So this is what working these percepts is. But on top of that, what I also have to consider is rewards. So I have to factor in the rewards, which means I may need to refine these regions further, because what I want to have is I want to have an abstraction which is based on polyhedral decomposition, where all polyhedra have the same percepts and the same rewards. Okay? But on top of that, what I also have to do is I have to put additional assumptions, because as I do value iteration, I will start with these initial abstraction and then I move forward with the transition relation. And there is no reason to assume that this abstraction will be preserved.
00:33:45.916 - 00:34:21.980, Speaker B: That is, it will be a finite decomposition. What I get where all the regions will have the same perception, the same rewards. So I need additional assumptions for that. But if I do that, I can then represent a value function in terms of piecewise continuous value functions. And this is aligned with work that's been done in PomDP's. And in fact, that's, you know, what is. So this is the value iteration.
00:34:21.980 - 00:35:21.644, Speaker B: But even that value iteration approach is just not practical. It's just not practical, you know, even for small neural networks. So what I'm showing, even small neural networks, no, I'm saying this is considered as a baseline and really understanding the challenges I think that we have. So what I'm showing you here is an example of an eight by eight vehicle parking, but it's under the regression, simpler regression. Okay? So just basically the ceiling of the value to give you the grid cell. So it's simply, this one is not using the neural network, but the shading shows you the value function. So that's the value function that is computed and that's, you know, for one of the cars, this is a strategy that you can obtain over the grid cells.
00:35:21.644 - 00:36:51.524, Speaker B: This is the parking action, it's the position of the car, and the arrows show the probability, and the intensity also shows the probability values. Okay? Right. We also do implement pre image for neural networks, which then allows me to build the abstraction in terms of those polytopes. But because we solve a game here, we have to solve a zero sum game at each step. So that's for strategy synthesis under the fully observable case. What I can also do still for the fully observable case and still for the setting where I have an infinite uncountable state space, I can start from a fixed point and I can unfold the game into a discrete game, but this is dependent on a particular starting point. And for this, what we work, what we can define is we can define sub game perfect equilibria, but we look in this case at the undiscounted case, but only up to the finite horizon, but look for equilibria that are optimal over the horizon.
00:36:51.524 - 00:37:45.560, Speaker B: And the example that we have is actually a lower. So this is a real neural network. The VCAs is part of the a casin collision avoidance standard. Vcas is only a part of it. The original system is 46 neural networks that have been trained on data. The reason we can work with this is because this is low dimensional, because you only have the relative attitude, the two climb rates and also the horizontal separation. So you have just the four inputs to work with as opposed to having the whole range.
00:37:45.560 - 00:39:09.792, Speaker B: But it is a real neural network. And the idea of that neural network is after this trained from data, it will produce to the own ship, to the aircraft, it will produce an advisory, and that advisory will be a set of advisories. And this will tell it what acceleration the agent can choose from. Okay, and just to show that we can work with this low dimensional neural network setting, here you have some equilibria, and these states actually show there is a trust value, which I don't have time to explain. And the two advisories, and as you can see, you know, it has up to horizon three, horizon four, and the typical advisory would be something do not climb or clear of conflict. So the neural network provides these advisories and then the controller has to select one, which is why we've added the trust value so that, you know, you can choose it. Okay, but, okay, so so far I have told you that what I want to work with is these continuous, in fact, hybrid state spaces, if you consider the local state space of the agent.
00:39:09.792 - 00:39:21.300, Speaker B: But because these are real valued, you know, vector spaces, I have to. I'm also forced to consider partial observability. So partial.
00:39:24.800 - 00:39:29.872, Speaker A: As far as I recall, once you have probability and partial observability, everything is undecidable. No?
00:39:30.056 - 00:39:30.872, Speaker B: Yes.
00:39:31.056 - 00:39:33.060, Speaker A: Okay, now we're going to solve it. Good.
00:39:35.160 - 00:40:22.576, Speaker B: Well, but if you. Even for POMDPS, it's already undecidable. But of course, if you talk to engineers, robotics engineers, they just run POMDP algorithms, you know, finite horizontal variance. I mean, you know, for me, you know, I'm really saying to the community, you know, we need to make sure that we don't get pushed into some niche and ignore, you know, problems which actually, you know, are driving around San Francisco. Okay, that's my point. And you can just give up when it's undecidable. But I think that the line I'm taking is them looking for restrictions and then learn from the inefficient solution to try to improve it.
00:40:22.576 - 00:40:58.270, Speaker B: That's the line take. Okay, so this is in fact the same syntactically, exactly the same as before, because I've already introduced the observation functions. But to start with, I considered a fully observable case. Okay, the fully observed. But, you know, here I have the same. I have the same situation. I'm starting with the POMDP, which is a single agent, and I have, you know, these observations.
00:40:58.270 - 00:41:46.426, Speaker B: I only have the single agent, and here I'm showing you the local transitions. They take the local state percept and action into account and give me a distribution on the local states. Okay, so that's that. There is a slight difference with the way that p observations work. Now, because this is partially observable, I have to work with a belief space, but it's a belief space over a hybrid, hybrid state space, okay. Hybrid state space, which involves the uncountable states as well as the finite states. And, I mean, this is just the example that I will show you again.
00:41:46.426 - 00:43:03.458, Speaker B: I'm using this here. It's a neural network, and this is used for localization. And what I want to do is I want to adapt the same methodology that I had before, but now to work in the partially observable case, as we said, everything is undecidable. So, in fact, what we do, influenced by work from Charles University, Karel Horak and others, where they formulated a finite state variant of these asymmetric games. So the restriction is that I have a continuous environment. They only allow finite environments, discrete environments, and I have one agent which observes and observes the environment only through this perception function. But the other agent is fully observable.
00:43:03.458 - 00:43:58.674, Speaker B: That means it can see directly the states and also directly can access the states of the agent. The other agent. Why? This is to avoid, get to decidability, you know, implementability, but also to avoid the problem with nested belief. Okay, so the nested beliefs, if you have two partially observable agents, you need to model what the agent knows about what the other agent knows. And this is recursive. So this is a trick that allows you to avoid those nested beliefs. But it actually also matches the application because we consider this only for modeling purposes, just to study the worst case.
00:43:58.674 - 00:44:41.948, Speaker B: This is not reflecting the real situation, that's reflecting the real situation. But this is only used to study the worst case. So the idea is that agent one uses a learned perception. Agent two is fully informed now. And again, I consider the discounted reward because undiscounted is undecidable. And, okay, so what I do want to compute is to say, I want to compute the optimal policy in this setting. And I think we need to be very careful about what safety would mean in this setting as well.
00:44:41.948 - 00:45:43.310, Speaker B: I'm highlighting that this is about optimal policy synthesis as opposed to safety directly. So to motivate that particular model, you know, with a real neural network, what I have is a situation where I have an autonomous vehicle, and the autonomous vehicle is partially informed. Okay, the autonomous vehicle will be using neural networks for vision. But I'm not focusing on vision. I'm focusing on one example where it uses a neural network to predict the pedestrians intention. So imagine that there is a pedestrian on the side of the road. And in fact, there are neural networks that are trained from video data that would produce these three percepts which are unlikely to cross, likely to cross and very likely to cross.
00:45:43.310 - 00:46:30.994, Speaker B: And what I want to do is I want to synthesize a strategy to minimize the likelihood of the crash. So in that sense is optimal. But I'm not saying to guarantee that the crash cannot occur, because I just cannot do that with this. So just to show you what the real neural network looks like, it is a recurrent neural network. It's actually an LSTM. And that means that it's trained from videos. RNN's process sequences of frames, so there would be a longer sequence of frames.
00:46:30.994 - 00:47:14.238, Speaker B: We have simplified it to just two frames. So we have the position of the previous box and that's the position of the current box. Okay. But, you know, I just can't handle RNN's at the moment. But I can work with this simplification. Okay, so this is an example of what this NN based perception does to the, you know, the input. So we have the positions x, y, which is the top left corner position of the previous box, and then we have the next box.
00:47:14.238 - 00:48:06.260, Speaker B: So what I'm showing you here is for a particular previous box position. These distances are relative to the vehicle. And as you can see, a crash occurs for small values. So it's unsurprising that what you get is for small values, you actually get very likely to cross. And then you'd expect the color controller to take that into account with slow death. So that's what the network looks like. And in terms of the modeling, well, I have a vehicle and the local state is speed.
00:48:06.260 - 00:48:55.588, Speaker B: I have a very simple dynamics with acceleration deceleration, and the percept is the pedestrian intention. So one of those three, but of course those three. But in fact there will be several, you know, polytopes in here that have to be represented, which we haven't tried to merge. This looks better than it is actually looking inside the tool. And then I have acceleration deceleration, pedestrian simplified. So either crosses or goes back to the sidewalk. Okay, and what do I do? Well, I mean, again, I consider the zero sum discounted expected reward over the infinite horizon.
00:48:55.588 - 00:50:07.508, Speaker B: But now for this restricted asymmetric setting. As I mentioned before, value iteration is not workable in this context. So what I'm working with is an extension of HSVI, which is a value iteration method known as heuristic search. Value iteration, instead of approximating the value, it computes lower bounds and upper bounds and terminates when they are sufficiently close and uses heuristics to guide the surge towards better lower and upper bounds. So Horrock and colleagues implemented this for finite state asymmetric games. We have extended it to work on the abstraction of the uncountable states. And this is what the abstraction would look like.
00:50:07.508 - 00:51:05.004, Speaker B: If you are familiar with point based methods, they are based on alpha functions. So for finite state pom, DP's pom based methods work with alpha vectors, which just work with simplexes of the belief space. But they are vectors because you can represent them finitely. But here I have uncomfortably many beliefs, so instead I have to work with alpha functions. And these alpha functions are simply the polyhedra, which are classified to the same percept. Okay? And, you know, this is, this has been implemented and can be computed. And what can we show? So I'm showing you HsVI, but for an NspoMDP version.
00:51:05.004 - 00:52:04.530, Speaker B: So just the one agent that I showed you before, and you can see that the lower and upper bound converges. What we can also see. So this is for the neural network perception. And it's the question is, can you reach this parking pot? But these black boxes are obstacles. So you can also combine it with avoiding obstacles. And it's showing you the value function and, you know, again, the intensity. It's also showing you three paths which are drawn from the initial region, but I select the particles in initial regions, and then I can generate the paths, and these paths that you can see actually reach and avoid the obstacles.
00:52:04.530 - 00:52:51.270, Speaker B: To show you a little bit under the hood, what the computation involves. So for these value computations, this is the initial setting. Okay, so that's the initial decomposition that I get via preimage and combining with rewards. And then if you look at the. After the first five, after the first 25 and all polyhedral decompositions. Okay, we need good polyhedral libraries which support operations such as merging, which we have not worked with here. It's just working with polytopes and going back to the pedestrian example.
00:52:51.270 - 00:53:36.320, Speaker B: Right. So what you see is this is the crash region, and that's one of the strategies that has been extracted. What you see at the dots, the green and red dots, are actually the intention. So you see here, this intention was detected quite late. The intention to cross was detected quite late, but still the system managed to synthesize a strategy. So it's showing a speed underneath and acceleration deceleration. But in this case, the intention was detected late, but the crash could not be avoided.
00:53:36.320 - 00:54:48.120, Speaker B: I don't know, Moshe, whether this answers your question. I think in this case, at least, we can explain why. But most paths are actually safe, so I think I'll skip it. We also have an online, a variant of an online strategy synthesis method, which uses the bounds to precompute. And just to show you, this is a pursuit evasion scenario that is the same one as the one that's considered in Horak's work, except in our case, it's a game with this neural perception. So I have a pursuer, and the pursuer uses a neural network, and this is a very small neural network and only a three times three grids. And the idea is that the pursuer uses the neural network and tries to detect the evader, but the evader is fully observable because we are learning about the worst case.
00:54:48.120 - 00:55:44.456, Speaker B: So what I'm showing you here is snapshots for a precise neural network and a coarser neural network. So, just demonstrates how this can be used. For example, for testing, the red dot is the position of the pursuer, and the arrows mean the strategies. As you can see, the red dot is aligned at the top with the blue box, and the blue box is the percept. So the blue box is the percept that is learned from this neural network. Now, for the cose neural network, this is where the pursuer is, but this is where the pursuer thinks it is. Because the neural network has not been trained properly.
00:55:44.456 - 00:56:23.930, Speaker B: I think it's missing some critical data points. I think, in the training. Now, the green shows the pursuers belief of where the evader is. So that's how far, you know, we can get. And I think it's. I don't know. I hope that this kind of brings this home that we are really trying to work with white box access and trying to work with the regions, you know, of the inputs into the neural network that are then mapped into percepts.
00:56:23.930 - 00:57:35.088, Speaker B: Okay, so, well, I hope that I've given an overview of some advances in what we've done in formulating eurosymbolic games. These are still very restricted, but I personally think that this is a sensible position to take to allow the symbolic and neural to work side by side, but it is not the position to take if you want to optimize and improve performance. I think we probably need tighter integration and then, you know, specific methodologies. And I was able to work with exactly learned percepts. I do not think that that's a sensible way to take into the toolbox. I think we need to start working on some appropriate approximations. Now, the methods have very high computational complexity, so it's just apart from, even if you restrict them to the selectable problems, they have very high computational complexity.
00:57:35.088 - 00:58:34.674, Speaker B: But someone told me we don't worry about training of neural networks. Okay, so if you have a neural network that's trained and then you want to verify it, and verification also takes as long as training. I think we probably need to change the way that neural networks are trained and then maybe adapt verification to this. And I've shown you some realistic case studies. Now, there are many challenges, and there are many open problems because we've only looked kind of a little bit under the hood. Now, for bodily, human, and artificial agents, what is known is that rationality is not reproduced in experiments. And because humans can be altruistic, they are not motivated just by materialistic concepts.
00:58:34.674 - 00:59:48.208, Speaker B: So what we need to do is we need to extend modelling verification to psychological games, which actually already existed in economics, but they have not been covered. It would be good to get a characterization of this polyhedral abstraction, for example, in terms of bisimulation, etcetera. And so far I have worked with exact pre image computation for neural networks, but there are methods to approximate the pre image, as for example disjoint unions of polytopes, and these could be used instead. But then we have to start looking at error bounds. We have a baseline, so at least we will have a better understanding of error bounds. We also want smarter algorithms combined with something. And ideally we want to have probabilistic perception, which can be added in my other strand of work as a bayesian neural network and of course further applications and case studies.
00:59:48.208 - 01:00:51.970, Speaker B: And, you know, just to finish off. Okay, I, what I have been focusing on is kind of reviewing what verification has been successful in so far, but trying to tackle, you know, the real challenges that are out there and they are moving in, you know, in a big way. And I have focused on methods for rigorous verification, but also looking for something where we can improve safety, we can improve performance, not just performance, reliability. Now, to, to make progress, we do need to look more closely at these combinations of neural and symbolic. And I think there is already some work. I think we are talking about guarantees for neural network, adversarial robustness guarantees. We need SEM guarantee interfaces, interface theories for neural networks.
01:00:51.970 - 01:01:18.890, Speaker B: And finally, we also need to kind of understand the strengths of neural networks, which is the statistical aspects, and try to use them in combination with symbolic methods to the advantage of both. So that's all I wanted to say. Just wanted to show acknowledgement. Yeah.
01:01:28.830 - 01:01:40.062, Speaker A: I take the first question. Have you thought of using this white box analysis for explainable AI? The white box analysis, I think could be also relevant to explainable AI.
01:01:40.246 - 01:02:43.890, Speaker B: Yes, I have thought about it, but. Well, I think the problem with AI machine learning is it just keeps running ahead, focused on better and better performance, so the architectures get more and more complicated. And you know, I've said this before in other venues, I think that with, say, neural networks, we are at the stage of C programming, spaghetti code C programming. And I think as soon as these systems will need to be maintained, they will be deployed and they need to be maintained, then I think there will probably be people who will come up with better ways to structure this, and that will give you explainability. So you can have, for example, you can have, you know, inside you could have a neural component hidden under an assumed guarantee contract. That might be good enough for now.
01:02:46.750 - 01:02:47.970, Speaker A: Other questions?
01:02:53.360 - 01:03:16.000, Speaker F: I have a question on how coupled are the network and the controller synthesis in your approach? Meaning, do you assume that the concept that you get from the network are an exact representational reality or are you able also to manage the fact that what you get from the network cannot be accurate? So your controller can manage some, yeah.
01:03:16.040 - 01:03:43.864, Speaker B: So we don't consider ground truth. I just take a neural network and I have a method that kind of assumes something about the network and under those assumptions will give you a good or a bad tool. I think you probably do need to consider ground truth, or at least implicit ground truth, but for this work we haven't done it.
01:03:44.012 - 01:03:55.824, Speaker F: Okay, I have a second question, if I may. And what happens if you take your controller and you change the net, the neural network with a new one, you.
01:03:55.872 - 01:03:58.040, Speaker B: Take the control and you change the.
01:03:58.120 - 01:04:02.744, Speaker F: Network to the perception? Can you say something or.
01:04:02.832 - 01:04:17.078, Speaker B: Well, I mean, no, I mean, then the value function will be misaligned because the value function is really worked, I think, with the particular free image decomposition. Yeah.
01:04:17.254 - 01:04:18.510, Speaker F: Okay, thank you.
01:04:18.550 - 01:04:44.460, Speaker B: Yeah, so I think for that what we would need is we'd need some kind of theory of abstractions and property preserving. I think this is what, what model checking really depends on. A lot of problems are, you know, undecidable or highly complex. But if we have good abstractions, property preserving abstractions, and we can use them, people do use them in industrial settings.
01:04:49.440 - 01:05:24.100, Speaker G: I have a, maybe I got a little bit confused about something. So the, so you had a, he had a schema about the environment and then the agent that takes the visual perception information. Right, right. So the, so the decision that is, okay, so you have this also the very likely or unlikely to cross. Right. So this is kind of the environment. This is, this should be like information from the environment or this actually a decision made by the agent here.
01:05:24.480 - 01:05:34.270, Speaker B: No, that's the interpretation, the neural network interpretation of the environment.
01:05:35.210 - 01:05:39.562, Speaker G: And the agent should make some decision here based on.
01:05:39.746 - 01:05:58.230, Speaker B: Yes, and then the agent, so the agent receives this and then depending on that, on that percept, which is stored, depending on that percept and, you know, the local states and then decides whether to accelerate or decelerate.
01:05:59.520 - 01:06:12.460, Speaker G: But the, the agent is, so here do we, but you assume that the agent is white box. Right. It's also, you have the states as well or you just think of it as a.
01:06:14.600 - 01:06:32.870, Speaker B: For optimal strategy synthesis. I assume I have white box access because I want to have access you know, compute the pre image, and then I have to have access to these polyhedral representations. Right. Okay. Got it.
01:06:33.610 - 01:06:39.050, Speaker A: Okay, we postpone further questions for the afternoon panel with a coffee. Thank you.
