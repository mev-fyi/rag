00:00:00.640 - 00:00:29.482, Speaker A: Oh, right, the microphone. Hi Auth, thank you very much for having me. It's very nice to be here. So today I'm going to be talking about a paper with this rather long title, efficient structure in matrix recovery and nearly linear time algorithms for solving inverse symmetric and matrices. So this talk, everything I'm going to say is joint work with two students of mine at Stanford, Arun Jambalampati and Kiran Shirker. Um, we tried really hard this week. It is so close.
00:00:29.482 - 00:01:20.270, Speaker A: The paper is not quite on archive yet. It'll be online like Monday, Tuesday if you'd like a copy of the work or a draft before then, just email me after the talk and I'd be happy to send it to you in the next day. All right, so the talk about is about this joint work with Arun and Quran and the problem that motivates this talk and everything I'm going to say in this talk is a rather fundamental, influential problem over the last decade of solving a laplacian system. So the motivating problem for this talk is we have some laplacian matrix l, we have some vector b, and we want to solve this linear system lx equals b. So just to remind you what these are, in case the definitions aren't super fresh. So l is a laplacian matrix. If it's a square matrix that's symmetric, it also has to be a z matrix, which just is a fancy way of saying every off diagonal entry needs to be non positive.
00:01:20.270 - 00:02:09.056, Speaker A: It needs to obey this tight diagonal dominance property, meaning that every diagonal entry is exactly equal to the sum of the absolute values of the entries in the corresponding row or column. Another way laplacian matrices are commonly introduced. So you can also define laplacian matrices in terms of undirected graphs. So if you have some undirected graph g with vertices v and edges e and has some non negative edge weights w, you can define the Laplacian of that graph by simply taking the diagonal matrix where every diagonal entry is equal to the total weighted degree of that vertex. And you get the Laplacian by taking that diagonal degree matrix and subtracting its weighted adjacency matrix. So you can see, this gives a natural bijection between these two objects. What this says is Laplacian matrices.
00:02:09.056 - 00:03:02.892, Speaker A: We can think of as if we have a arbitrary Laplacian matrix. We can make a bijection between it and an arbitrary undirected graph simply by assign looking at the entry ij and the Laplacian for I not equal to j and viewing that as minus the weight of an edge from j and I to the graph. Now, solving Laplacian systems is an incredibly well studied problem. Over the last decade, or possibly probably longer, it's had numerous applications, and I think part of that has to do with a number of different perspectives you might take on solving Laplacians and their applications. Right. So the way I just motivated solving Laplacian systems was, is a rather natural linear algebraic problem. You can also show the problem is equivalent to the problem of setting up a network of resistors, fixing how much electric current enters or leaves every junction of the resistor network, and trying to compute where the electric flow goes.
00:03:02.892 - 00:05:10.250, Speaker A: You can also view the problem as more or less equivalent to the problem of saying, suppose we have some undirected graph, and suppose we consider the natural random walk on this undirected graph, where we add some vertex, pick a random outgoing edge, random edge proportional to its weight, follow that edge and repeat. Then, if we want to solve something like we fix some vertex a and b, and for every other vertex, we want to know that the probability that the random walk gets the vertex a before b, that problem of computing those escape probabilities for all vertices, is also essentially the problem of solving Laplacian systems. So I think one of the reasons for the wide influence of Laplacian system solving over the last decade has to do with these different views of Laplacian systems, and the fact that they sort of naturally owe themselves to both sort of continuous optimization perspectives, where we can view them as these linear algebraic problems and try to solve them by sort of natural continuous iterative methods like gradient descent and coordinate descent, and the fact that we can also view them as these more combinatorial problems involving graphs, resistor networks and random walks, which allows these problems to be viewed by trying to solve them through different sorts of combinatorial decompositions of the graphs. So owing in part to these two perspectives, there's been a lot of work over the last decade on how to use and improve methods from each class and couple them together to get faster and faster algorithms for solving laplacian systems. The first result showing you can do this in full, was this beautiful result of Spielman and Tang in 2004 that showed you could solve all these problems in nearly linear time. Since then, there's been extensive work getting faster and faster iterative methods and new combinatorial decompositions, and work in this broad area of research you may have heard of known as the Laplacian paradigm, where laplacian system solvers have been used incrementally black box to then get faster algorithms for a whole host of combinatorial and numerical problems. So for a very large list of rapidly growing list of problems like max flow, multicommodity, flow matrix scaling, at some point in the last decade, in some parameter regime, the fastest algorithms we've had in some way shaped or form used our ability to solve laplacian systems in nearly linear time.
00:05:10.250 - 00:06:23.150, Speaker A: And moreover, beyond just these sort of direct applications of using laplacian system solver's black box, there's been a host of applications by taking these iterative methods that have gone into solving laplacian systems and using those techniques for solving more and more problems. So that's the background for this talk. The question that really motivates what I'm going to present in the rest of it is the question of, okay, so we can solve laplacian systems in nearly linear time. What other sort of structured linear systems can we solve in nearly linear time? How far can we push the Laplacian paradigm from just solving Laplacian system to solving broader and broader classes of linear systems, and thereby hopefully getting new iterative methods and faster algorithms for more and more applications? All right, so even from the original work in Spielman and Tangent earlier, it's been known that there's broader classes of linear systems that you might want to solve that really easily reduce to the question of solving laplacian systems. For example, there's a rather standard reduction that says if you want to solve a symmetric, diagonally dominant system or an STD system. So these are systems in a symmetric matrix where every diagonal entry is larger than the sum of the absolute values in the corresponding row or column. There's a pretty straightforward reduction.
00:06:23.150 - 00:07:21.610, Speaker A: You sort of double the number of vertices, copy things the right way, and you can reduce that problem to solving Laplacians. Um, there's also some really cool recent work showing you can generalize this to solving block diagonally dominant systems, where instead of having every diagonal entry large and the sum of the absolute values in the corresponding row or column, you get a natural block analog of this. You can show you can still solve these in near linear time whenever the block sizes are small. There's also this really cool result, I think, of Deitsch and Spielman in 2008 that showed you could do the following. Suppose I have some matrix c, that's some n by n matrix, where every row of C has at most two non zero entries. Okay, now suppose I look at the matrix C transpose c, right? So this matrix is the sum of the outer products of a bunch of two sparse vectors. So this matrix is the result of just adding a bunch of two by two PSD matrices where the rest of the entries are set to be zero.
00:07:21.610 - 00:07:58.944, Speaker A: Yeah, sorry, should I think of C as like the edge vertex incidence matrix? So that's the proof that this is more general than solving Laplacians. So, yes, and this says you can extend beyond that, but. Exactly right. So, yeah, in this special case where C is something known as the incidence matrix, which I'll define later, which is essentially where every row has exactly one positive entry and one negative entry, this problem is exactly the same as the problem of solving Laplacians. However, this is much more general. And there was this cool result of Deitch and Spielman in 2008 that showed whenever you're given this matrix c, you can solve linear systems in C transpose C. These are known as factor width two matrices.
00:07:58.944 - 00:08:49.444, Speaker A: So they showed that you could solve factor factor with two matrices in nearly linear time. Um, recently we showed you can extend this a bit further. You can also, we showed you can also solve these systems in nearly linear time, even if you're not given the factorization beforehand. Takes a little bit more work. All right, so can you go beyond this? What other linear systems can we solve in near linear time? One sort of linear system, you might notice is conspicuously missing from the list is all of these were symmetric matrices. So nowhere have I said anything about getting faster algorithms for solving asymmetric matrices. However, there's been a line of work over the last few years showing that you can not extend directly using laplacian system solvers to solve these, but you can extend the machinery that's been used to solve laplacian systems to get solvers for, um, asymmetric variants of these.
00:08:49.444 - 00:09:27.354, Speaker A: So you can solve linear. So in a line of work over the last few years, we showed you can solve basically directed analogues of laplacian systems. These are out degree matrices minus adjacency matrices. They correspond to things like computing stationary distributions and evaluating policies. So you can use the machinery to solve these in nearly linear time. And just recently showed you could extend this a bit further to solving a linear system class of linear systems that are a bit more general, known as m matrices. So these are matrices where you have some multiple of the identity minus some non negative matrix, where that non negative matrix has spectral radius at most s.
00:09:27.354 - 00:10:29.564, Speaker A: These show up in a number of applications, and solving them corresponds to things like trying to compute perone vectors and solving certain graph kernels. All right, so until recently, this was sort of the state of the art. There's been a very successful paradigm or suite of techniques for solving linear systems faster. And it's also beyond solving these systems, it's introduced a number of different algorithmic frameworks for solving these linear systems. However, if you go through the sort of solvers that have been developed over the last decade for provably solving nearly linear systems, well, there's been a number of advances coming from them, and they're very interesting and very powerful techniques. You can show that there are still some ways in which laplacian system solving is somewhat brittle. There's still a number of classes of linear systems we would like to solve in near linear time that are very closely related to solving laplacian systems, and yet we still don't know how to solve them any better than sort of the trivial runtime of ignoring any of their structure and solving them like an arbitrary system.
00:10:29.564 - 00:11:13.376, Speaker A: So let me define a few of those. All right, so the first is something I'm going to call a perturbed Laplacian. Suppose we want to solve the following problem. Suppose we're given some symmetric matrix a, and we're promised that a is spectrally close to some Laplacian. So when I say specularly close, I mean that in terms of the standard PSD matrix ordering of looking at the quadratic form, the quadratic form for a is strictly less than the quadratic form for l, but it's not too much less. Right? So it's also larger than the quadratic form of L, as long as we scale it by gamma, which you should think of as some small constant or something to be polylogarithmic. So this first problem is closely related.
00:11:13.376 - 00:12:05.400, Speaker A: You can ask, can we solve linear systems in these matrices in nearly linear time? And you could also ask, can we recover the Laplacian? So you imagine it would be nice, we're given this matrix, and not only would be nice to solve it, but sort of write down the graph that roughly corresponds to what this linear system is. So why would you want to solve this problem? One of the applications of solving laplacian systems is when you solve a number of, when you do a number of physical simulations, you try to approximate it by some linear system. And in turn, laplacian systems are then used as preconditioners for solving these. Right? So a standard paradigm for solving some of these things is you write down your approximation to get some linear system you want to solve. You show that it's closely related to some Laplacian and then you solve it. It'd be nice if we could sort of generically prove. You can always do that.
00:12:05.400 - 00:13:06.870, Speaker A: Like whenever you wrote down the matrix, if it was close to some laplacian, be nice to prove you could provably get that matrix and solve it. It also just seems like a fairly broad class of systems that seem rather natural. Um, all right, so why, first question you should hopefully be asking is, okay, why can't we extend our solvers to actually solving these systems? And the shortest explanation I can give for why solving these matrices are somewhat challenging is suppose I have an arbitrary symmetric matrix. It has the all ones vector in the kernel, and every eigenvalue lies in some bounded range, right? So think I just have, like, some random entries, and then I just, like, project orthogonal to the all ones vector. All right, that matrix is spectrally well approximated by the Laplacian of the complete graph. However, seeing that you have to do something to compute all the eigenvalues. And one of the nice things about Laplacians, I defined Laplacians in terms of a natural relation between the diagonal and the off diagonal entries of the matrix.
00:13:06.870 - 00:14:01.366, Speaker A: So you can just look at the matrix and tell that it's a Laplacian by looking at that relationship. However, if I have a random matrix, right, that's orthogonal with the all ones vector, right, and you want to check that it's a Laplacian, you can't just do this sort of direct checking of the entries. You could have very, very large values, and the eigenvalues could be small because of signs of the off diagonals canceling. Moreover, if I take a number of, if I take that same construction and do it on a subset of the coordinates, and do that for a number of different subsets of the coordinates and add them all up, that'll also be a perturbed Laplacian. And now, finding that structure gets much more complicated, and it's unclear how to use than any of the machinery to solve this, particularly given that most of the machinery uses this sort of entry wise analysis of the matrix. Another class of linear systems you might want to solve. You can imagine.
00:14:01.366 - 00:14:40.288, Speaker A: Suppose I'm given a matrix a. I want to solve a linear system in it. And suppose this matrix a, I'm promised, is the pseudo inverse of some Laplacian. So it's the matrix that's the inverse. The matrix you get by inverting all the eigenvalues except for the zero eigenvalue. All right, so suppose I'm given this pseudo inverse of a Laplacian, and I want to solve a linear system in it, or, and then ideally also recover the Laplacian. Well, why would you want to do this? You can show that being given explicitly the pseudo inverse of the Laplacian is the same thing as being given all pairs effective resistances in a graph.
00:14:40.288 - 00:15:27.638, Speaker A: This is a natural pair wise relationship between edges of a graph you would care about. You can also show it's basically the same as saying, suppose for an undirected graph, I'm given a set for every pair of vertices, the weighted commute time between those pair of your vertices. So the expected amount of time it would take for a random walk to go from one vertex to the other and back again. Right, so these are natural things you might expect to measure about a graph in different settings. And trying to recover l from the pseudo inverse is essentially the same then as this problem. Okay, all right, so again, hopefully you're asking, all right, that sounds interesting, but why can't we use our previous machinery to solve this problem? Um, there's two reasons. One is one that I guess, uh, you know, I'd hope to correct with time.
00:15:27.638 - 00:16:05.464, Speaker A: We don't have a lot of tools for just directly manipulating the pseudo inverse. It's not immediately clear what perturbations you can do to the pseudo inverse that preserve the property of it being the pseudo inverse of a Laplacian. So it's not quite clear how to manipulate them. There's one other argument I can give, which is not a direct hardness result. It just sort of shows sort of the difficulty in going between a Laplacian and its inverse. You might imagine, since I've told you we can solve laplacian systems in nearly linear time, you might imagine from that Laplacians have such nice structure. I can actually do better than matrix multiplication and like writing down its inverse.
00:16:05.464 - 00:16:53.654, Speaker A: So maybe I could hope that I'll be able to write down the inverse of a pseudo inverse of a Laplacian and maybe use all the nice structure that they have to not just solve, but go between the matrix and inverse and back again. So in some recent work, you can show that's not the case. You can show even if your Laplacian is fairly nice and well conditioned, the problem of exactly writing down its pseudo inverse is essentially is a subcubic boolean matrix multiplication, hard. So if you wanted to do that, you likely have to use a lot of the algebraic machinery used for fast matrix multiplication. So it says somehow something else would be needed here. Okay, one more question. Uh, you might ask.
00:16:53.654 - 00:18:01.372, Speaker A: So you try to generalize this even further. You could say, suppose, um, I want to solve a linear system in symmetric matrix a, and I'm promised that a is the inverse of some symmetric m matrix, right? So suppose you want to solve linear systems in this. What are these systems? So remember m matrices I defined in the beginning as these matrices that are some multiple of the identity minus some non negative matrix where the spectral radius is less than that multiple of that identity. You can show that such symmetric M matrices are, what they are is they're just positive diagonal scalings of symmetric diagonally dominant matrices with non positive off diagonals. And it turns out these matrices actually do arise in a number of contexts. You can show that symmetric M matrices, sorry, inverse symmetric M matrices are the covariance matrices of a commonly occurring class of distributions known as MTP two distributions that are used both, they both arise in number of contexts, and they're used to a number of model of probabilistic processes. So solving the problem.
00:18:01.372 - 00:18:44.840, Speaker A: So solving inverse symmetric m matrices is the same as solving certain inference problems on, uh, when you have these mtp two distributions. Right. And why is this problem hard? Well, um, it's not too hard to show. This problem is essentially more general than the problem of solving laplacian pseudo inverses. So if you believe me at all about laplacian pseudo inverses being hard to solve, this is only harder. All right, so these are the main questions that motivate what I want to say in the rest of the talk. Can we solve perturbed Laplacians, laplacian pseudo inverses, or inverse m matrices faster? And I should point out that the benchmark for these, even though they seem to have a lot of nice graph structure, be closely related to things we can solve in nearly linear time.
00:18:44.840 - 00:19:35.052, Speaker A: Before the work, I'm going to tell you about essentially the best running time for solving all of these systems. In the worst case, ignored all the structure and just solved them using fast matrix multiplication. All right, so any idea how to solve these systems, or any questions on the problem? All right, so hopefully most of you are thinking hard about how to solve these systems. Some of you might also be asking a rather natural, obvious question about the elephant in the room. You might be asking, wait a second. Hopefully I've convinced you. These are interesting problems and they seem worthwhile to solve.
00:19:35.052 - 00:21:28.584, Speaker A: But isn't this a workshop on sublinear algorithms and nearest neighbor search? We're like halfway through the talk, nothing that seems remotely relevant to any of these topics has come, come up. Might be asking him, am I in the wrong room? Are you in the wrong room? What's going on here? Why are we talking about this? And so you might be naturally asking, how is any of this relevant to anything about the workshop? Now, the reason I'm telling you about these problems and the reason I'm presenting it here has to do not so much with these problems themselves, but interestingly, very interestingly, I think the way in which we actually show you can solve these problems faster. So the main results I'm going to tell you about, or roughly sketch out in the rest of this talk is the following interesting theorem that it turns out you can solve all of these problems in quadratic time, so you can solve them in n squared, so you can solve them as if Omega was two, even though it's not. But interestingly, the way you can, we show you can solve, well, we don't know if it's, I guess not two, I guess, I guess it might be two. Interestingly, the way we show you can solve all of these systems is we show you can solve it in actually a very, very restricted oracle model. We show you can solve these problems in n squared, where instead of having direct access to the input, instead of directly being able to look at these matrices and do algebraic manipulations of the entries, you can still solve these problems in n squared time if all you can do is do matrix vector multiplies with the matrices, and you do only a poly logarithmic number of matrix vector multiplies. So you can show for each of these problems solving a Laplacian, solving a Laplacian pseudo inverse, solving a nearby Laplacian, you can solve it where all you do in your algorithm is apply the matrix to just poly logarithmic number of vectors.
00:21:28.584 - 00:22:22.764, Speaker A: So, note, this is trivially much less time the number of queries than is needed to actually fully and exactly recover the matrices we're trying to solve. And we show, so they run in sort of sublinear number of queries that would actually need to recover the input. And interestingly, we show that in these results, we don't just get these n squared solving times, but in the same time we can actually write down a spectral approximation to the matrix. So we can actually get a spectral, um, approximation to the Laplacian or the graph in each of these cases. So in other words, this is a, uh, we'll prove this theorem by giving a certain type of matrix compressed sensing or matrix recovery result, showing that it's in a bumber of different settings, it's possible to really efficiently approve an approximation to a graph using just a very small number of queries. Yeah. So the number of queries is related to the rank of the spectral approximation, or.
00:22:22.764 - 00:22:43.548, Speaker A: No, so. Right, so Laplacians, if it's on a corrected graph or full rank, they're n. Yeah. We're only going to make a polylogarimic number of queries, and we're still going to get the full spectral approximation. We will be writing down rank n matrices. I see, yeah. So note that there's no possibility immediately every matrix vector multiply is giving us n bits.
00:22:43.548 - 00:23:20.528, Speaker A: It's just giving us n bits in a highly correlated way. Moreover, I guess it's important that we know for each of these matrices there are sparse approximations to them. But actually we'll show that's needed actually more for runtime than for existence of this result. Also, just a few, few notes about this. Every time I write o tilde, I'm hiding poly logarithmic factors, including the log factors for solving the linear systems. And when I talk about solving an n squared time for the problem of perturbal Laplacians, there's a polynomial dependence on gamma. I know that's kind of necessary.
00:23:20.528 - 00:24:01.188, Speaker A: If there wasn't a polynomial dependence on gamma, I'd basically be showing we can solve arbitrary PSD systems in n squared time, which. Well, then I'd be giving a different talk, and it probably wouldn't, then it might not be an appropriate talk. All right, any questions on this result before I discuss how we prove it? The claim clear. Okay. All right, so goal for the rest of the talk is I'm just going to really high level sketch these results of how, I guess, bad pun, not intended, sketch these results. Um, we're going to show you can solve all of them by. What I'm going to do is ultimately reduce these to a sort of general matrix recovery problem.
00:24:01.188 - 00:24:44.724, Speaker A: And I'm going to show you can solve this very general matrix recovery problem by careful combination and slight extension of a variety of very, very powerful but sort of known tools. Um, this, combined with a little bit of extra structure, we'll need to prove about Laplacians and the pseudo inverse and perturbs. Laplacians will then be enough to get the results. And I want to emphasize that I think when we first phrased these problems, it was very unclear how to solve it, how to actually get this sort of running time. The machinery I'm going to use is very powerful, but sort of the approach I'm going to take ends up being fairly systematic. And I'm presenting it because I think this general matrix recovery theorem is a very powerful one that's worth using in a number of other contexts. And I'm hoping this opens the door to more work.
00:24:44.724 - 00:25:08.528, Speaker A: But I think the magic of the result is more in that this problems are at all solvable and that this general matrix theorem is true. Once I tell you how to solve it, there's a lot of technical analysis you have to do, but you hopefully would believe that the ingredients work out. Questions? Yeah, sorry, I'm missing something. So you gave these hardness results for problem two. That was exactly writing down the pseudo inverse. So good. So if you have.
00:25:08.528 - 00:25:38.402, Speaker A: And also, it wasn't. We technically haven't shown hardness the other way. So the hardest, the hardest is just if you have a Laplacian, if you have an algorithm which arbitrarily, given, given an arbitrary Laplacian, can write down its pseudo inverse. If you can do that in subcubic time, you can do boolean matrix multiplication, subcubic time. So there's nothing about approximation. And interestingly, and I would like to show this, it doesn't say anything about going from the pseudo inverse to l, which is the one we actually care about for this problem. So it's only sort of.
00:25:38.402 - 00:26:03.674, Speaker A: It's not a direct hardness argument, it's just showing why it's non trivial, suggesting that it's non trivial. Good question, though. You had a question earlier. Question just how small gamma can be to make this still non trivial. Yeah, I guess any constant. It's still even for any constant. Before this, we didn't know how to do it.
00:26:03.674 - 00:26:29.714, Speaker A: And this does hold for any gamma. So even if gamma grows with n, we can still show this some interesting problem. Yeah. So even if gamma is like one over n to the 100, this is. We can do it. It's just, you know, you get some plus probably one over 100, you can't get down to like one over something. So we didn't make any attempt to actually optimize the gamma in the work.
00:26:29.714 - 00:27:05.406, Speaker A: It's probably something less than ten. It's probably not less than one, but I don't know what it is. Any other questions? All right, so goal is to prove this. There. All right, so how are we going to solve these problems? This is the. These are the problems we want to solve. I first just want to point out right off the bat, for solving all of these, we're going to use a very general linear system solving tool, probably the first tool you should look at whenever you want to provably solve, when your system to high pre precision.
00:27:05.406 - 00:27:53.054, Speaker A: And that's preconditioning. It's a very powerful reduction for solving linear systems that we're gonna make heavy use of. And everything that I'm gonna say and preconditioning just says the following sort of simple fact. Suppose I wanna solve a linear system in some matrix a, and I don't know how to solve linear systems in a, but I do know how to solve linear systems in b, right? And if I run this very simple iterative method, which in every iteration applies a to a vector and applies b inverse to a vector, this method very, very quickly converges to solving to a solution of the linear system. Um, and it converges at a rate that just depends on how well a multiplicatively approximates b. So if they multiplicably approximate each other up to, say some factor of gamma, then I'll take some gamma number of iterations, right? Or every gamma iterations, we'll get one more bit of precision. So I'll usually write such running times as o tilde of gamma.
00:27:53.054 - 00:28:39.620, Speaker A: Okay, so why is that relevant for this? This says, actually, for all of these problems, the hard part is not, the hard part is not actually in building the solver. What preconditioning says is the hard part is actually just doing the recovery, right? Because what this says is say you want to solve this problem and I gave you an approximation of l. Well, I said we could solve linear systems in Laplacians in nearly linear time. So you can use its pseudo inverse as a preconditionaire here. And you would solve linear systems in the original matrix, where the number of iterations would just depend on gamma, right? So if you can recover l, you can solve. Right. Similarly here, this says if I can just recover an approximation of l that is like the b inverse here, I can use that as a preconditioner and I could solve.
00:28:39.620 - 00:29:24.396, Speaker A: And similarly here, this says I just need to recover approximation to the m matrix itself, and that would be a good preconditioner. So the hard part of all these problems is actually doing the recovery, not doing the solving. Okay. All right, just one quick picture for what this is actually saying. If you haven't thought a lot about what it means to get matrix vector access to a Laplacian, so what this means, one special case of the theorem I'm going to sketch out says is suppose we have this large undirected graph, and we play the following game in every round of this game, we assign every vertex in the graph some real value. And what we then get back is for every vertex, we get the sum of the difference of the value of that node and the value of each of its neighbors. So we give everything a value.
00:29:24.396 - 00:30:16.234, Speaker A: Each node sums over all the edges that difference and spits back the result. So now what the result is saying is, if we play, so that operation is just in English, what it means to apply l to the vector x and get the output. So what this theorem is saying is, if you play this game for a polylogarithmic number of rounds, that's enough to spectrally approximate the graph. Okay, now when I say it that way, a natural thing that some of you might be thinking about is, you know, variants of this problem have been discover, been studied under the context of doing matrix sketching or trying to do get certain sketches of graphs to recover properties of them. Can we use that machinery? What you described was just applying l to the spectra, right? Yes, but you have l inverse. So this is like a special case. It's not all of them.
00:30:16.234 - 00:30:39.818, Speaker A: Yeah. So this is mostly most analogous to perturb Laplacians. So the general theorem you can apply in this is like perturbed Laplacians when gamma is one, which is still non trivial. So this is a special case of everything, because if gamma is one, I'm saying I'm given a laplacian, but I only have matrix vector access to it. Oh, I see. Right. So even in that case, we know if I had the laplacian, I could solve it.
00:30:39.818 - 00:31:18.462, Speaker A: But if I only have matrix vector, how many do I need to recover the graph and solve? This is saying poly logarithmic, and that involves playing this game. Okay, all right. But the way I said it, a natural tool you might look to use is different types of matrix sketching, and there have been different sorts of linear sketches from graphs you could hope to apply. Right. So I think that's the first idea you would take to solve this problem, um, to explain how that works, um, and why, unfortunately, we can't use it directly, I need to introduce a little bit more laplacian jargon that'll be very useful for analysis. So some, first, some quick definitions for any edge from I to j. I'm going to use delta e to denote this delta I comma j.
00:31:18.462 - 00:31:57.254, Speaker A: And this is just the difference of an indicator vector for I and an indicator for vector for j. So this is just the vector with a one in coordinate I and a minus one in coordinate j. Now, for any such edge e. I'm going to let l sub e denote the Laplacian of that edge. So the Laplacian of a graph where that edge is the only thing in the graph, and it's not too hard to see that, you know. So we set a Laplacian of a graph, you know, had off diagonals equal to the weight, and diagonal equals sum of the off diagonals. So Laplacian of a single edge is just going to be this matrix that has one in the diagonal entry for I, one in the diagonal entry for j, and minus one on the corresponding off diagonals there, and zero everywhere else.
00:31:57.254 - 00:32:29.266, Speaker A: If you think about it for a sec, that's the same as just the outer product of delta e and delta e transpose. So what this means is we can write, using this, we can write Laplacians a bit more compactly. So if I let b denote the incidence matrix of the graph, this is simply the matrix where every row e of this matrix is. There should be a transpose here. The transpose of this, I'm going to call it like indicator of an edge. Then we just argue. You can see that the definition of a Laplacian does linearize correctly.
00:32:29.266 - 00:32:49.888, Speaker A: So the placing of the graph is just the sum over all edges, the weight of the edge Laplacian of the edge, which is the sum over all edges, the weight of the edge delta, delta e transpose. And we've defined b. So this is the same as b transpose wb. Okay, sorry. W is the diagonal. Good. The diagonal matrix where the diagonal entry is the way.
00:32:49.888 - 00:33:08.092, Speaker A: Wait, so the ethyth entry is the weight of e, right. So that gives us the right weighting of the outer products. So you're basically just scaling those entries. Yeah, for when we have a weighted graph. Yep. Right. So if this was a graph of an edge of weight five, this would just be five minus five minus five.
00:33:08.092 - 00:33:49.102, Speaker A: Good question. All right, so there's another nice way we can write this. One thing that's not the nicest about this representation is this definition of b does depend on the graph, but it's not too hard to see. If I let b sub n denote the incidence matrix of the complete graph, I can also write l as bn, wn, bn, bn bn transpose wnbn, where all I do is let wn be the matrix where for every edge that was in the graph, I set the diagonal entry to the weight. And all these extra edges I add. For the complete graph, I set to be zero. Now that we have all those definitions, um, there is a natural thing we could try to do to solve this problem.
00:33:49.102 - 00:34:47.794, Speaker A: It has been shown that given this representation, there is a way of taking a linear sketch of, uh, these matrices to very efficiently. Sorry, uh, uh, take a sketch that a small number of measurements does recover the graph, and this is a motivate. This is the closest analogous theorem to the one I'm going to show I know of. Um, so this was joint work with a number of us a few years ago said that if your weights are zero, one, so you have a unweighted graph, essentially show that there exists a distribution over matrices with, um, uh, just a, yeah, there exists a distribution over matrices such that if we apply that matrix to wnbn, we get a matrix that just is a log n by n matrix. So PI has just a login number of rows. Um, and it's the case that from this sketch you can recover a special approximation to the graph. Right.
00:34:47.794 - 00:35:38.004, Speaker A: So the way to think about this result is it says, we showed you can, if you have the ability to apply just a polygon rhythmic number of matrix vector multiplies to WnBN, the transpose of this, that does suffice to recover a spectral approximation. All right, so this is cool. This shows given certain representations of the graph, it is actually possible to do this. Unfortunately, I don't know how to apply this result to our setting. And sort of the key issue is one of the things this algorithm does, or like the analysis of this sketch does, is it uses that in this PI matrix. When you're working on this representation, you can randomly sample the rows of this matrix, which corresponds to randomly sampling the entries of the matrix. And you can do this in a way where you're sampling fairly independently between the rows of the matrix.
00:35:38.004 - 00:37:03.444, Speaker A: However, if only all I have is matrix vector access to l, I don't know how to get the same independence or do the same sort of independent sampling, so don't quite know how to apply this machinery. But going through this analysis and thinking of what we're trying to do, or this fact that we can always just write every one of our Laplacians as the sum of weights of edges of a graph, it does suggest another natural approach we could take at the end of the day for things like perturbed Laplacians, right. We are trying to find this matrix l, that's close to a, and we could just write down the optimization problem that corresponds to, and that's just some semi definite program. So if I just scale things a little unshifted, this problem is the same is saying, you know, I want to find some weights on every edge of the complete graph, right? Such that when I look at that weighted complete graph, what I get is a special approximation to a. Right. The recovery problem just is this, after scaling to put gamma on the other side. So if we ignore for the second or actual oracle model and how we access the input, right, solving this problem is just the same as solving the semi definite program, right? This assumption that a is specially close to l means that we know that there exists some weights such that this weighted complete graph is actually close to the matrix a we want to recover.
00:37:03.444 - 00:37:38.470, Speaker A: Okay? So what we could do is just try to solve this semidefinite program, and somehow we can solve this semidefinite program in a way that runs in very few iterations and makes very few queries to the matrix a. We could use this to prove the result. And for this, fortunately, most of the work had already been done for us. Right. So let me generalize the problem. I just said you could write down this problem not just for Laplacians, but in even more general form. You could say essentially what we want to solve is the following.
00:37:38.470 - 00:38:36.888, Speaker A: We're given some collection of PSD matrices, so m different PSD matrices. And suppose we're promised that there exists some weighting, non negative weighting of these PSD matrices. That's specially close to some other given PSD matrix b, right? So in the case of Laplacians, this m is the set of edges of the graph. So this matrix recovery problem asks, can we find this vector alpha? Can we find some rewriting of the Mi that's pretty close to b? So maybe we only lose constants over the best thing we could hope for. All right, now, it turns out when I write writing, this semi definite program was mostly solved basically implicitly in some very cool work of yin tat, Li and Hisan. They gave an algorithm that actually solves this problem in a poly logarithmic number of iterations. In the case when gamma is very small, like a sufficiently small constant, it's a very powerful result that you can solve this problem fast at all.
00:38:36.888 - 00:39:29.530, Speaker A: It works by using packing covering semi definite program solvers in another iterative framework. That's kind of cool. This problem was also written out explicitly in the Laplacian case and solved in some other recent work. Our results shows that what we're going to do is show that the first step in trying to solve our problem is to show that we can. If you look carefully at these algorithms, you can extend them a bit to solve in the general gamma case. And if you look very carefully at what the actual oracle axis the inputs required, it's something that we can hope to generalize from. So taking this perspective and generalizing this result a little bit, you can show the following that there's an algorithm that solves this problem in a poly one over gamma number of iterations, where in every iteration the cost is the cost of doing the following things.
00:39:29.530 - 00:40:12.612, Speaker A: A polylogaritic number of times. You need to be able to apply b and its inverse to a vector. You also need to have some factorization of b. So you need to be able to compute Cx for some matrix C such that C C transposes B. And also for any given weightings of the Mi, you need to be able to compute the result of sum of I alpha mi product applied to a vector, meaning this matrix, you have to be able to apply to a vector, and you need to be able to compute the quadratic form of every mi for sum x. So if you do that in every iteration, you can solve the problem in a polyloritic number of steps. So where does this come from? Really high level.
00:40:12.612 - 00:40:46.222, Speaker A: These first two operations are, the way this is solved is you do a change of basis to make the b basically like the identity this is, or just what I wrote is a bit of overkill for doing that operation. And then these steps are essentially what you need to be, once you've done this change of basis, to be able to compute gradients and do Jl. Right. And then feeding that through this very powerful iterative method lets you show this result. Okay, so why is this useful for us? This first parse seems a bit daunting, I say. I'm saying we want to solve this problem where we can only do matrix vector access to b. And here I'm requiring a lot more than that.
00:40:46.222 - 00:41:15.704, Speaker A: I'm also requiring its inverse and a factorization of it. Right. However, the second part in the case of Laplacians is not too bad, right? These mi are just going to be the indicators of edges. They're all too sparse. There's only n squared of them. All of this we can do in n squared. So to prove our results, all we need to show is that somehow we don't really need access to all of this, right? That there's a way to do something a bit more efficient.
00:41:15.704 - 00:42:22.552, Speaker A: So the main, uh, uh, framework results we use to get the applications is we show that you can still solve this problem at the same time with the same oracles where you can drop the need for being able to apply this factorization and being able to solve, right. And what you need in order to be able to drop these assumptions is all you essentially need is the same assumptions, but in a matrix you explicitly have. So if in every iteration instead you can solve linear systems and weighted combinations of the Mi, where you know the weighted combinations rather than being the b that you don't know, and you can get a factorization of any weighted combinations of the Mi which you have explicitly, then you can get the same result. So you can get around the need of being able to apply these operations to this input matrix you don't have explicitly. You can do it just with things you've already computed. What's nice about this is, at least in the Laplacian case, each of these operations is n squared, right? So this result, for example, immediately gives the perturbed Laplacian result. So how do we solve this? This seems strong.
00:42:22.552 - 00:42:58.882, Speaker A: What I'm saying is we want to be able to do things like compute b inverse and apply its factorization to a vector, and all we have is access to b. And we have the same oracles for explicit Delphi. Um, however, there is an idea, there's a rather simple idea you can use to get around this problem. It's a natural set of techniques that's shown up again and again in solving different linear systems and designing different iterative methods. I first know of it from some cool result of Lee, Miller and Peng in 2013. We've used it over and over again for solving linear systems. It's just an easy idea known as basically regularization and preconditioning, which.
00:42:58.882 - 00:43:35.016, Speaker A: Yeah, what do you need from this vectorization? Cc transpose. You just need to apply c to a vector. For Laplacians, it's just trivial. So if you want to boost it, we end up requiring the square run so you can get a Chileski factorization. There's possibly ways to get around it, but this works, and getting around it wouldn't save you anything for the applications, as far as I know. Good, good question. Right, so I'm running out of time, so I'll just say the idea very simply, very quickly, the idea is to get around this problem of not having these oracles for b.
00:43:35.016 - 00:44:22.406, Speaker A: All you do is you say we'll take our Matrix B and we'll add a huge multiple. Yeah, a huge multiple of the identity, and we'll suppose we can solve, or we'll take our matrix b, we'll add some multiple the identity or add some multiple of the mi. These are matrices we know explain and we'll try to solve the problem on these matrices. We can still do matrix vector multiplies with this matrix because we have this part explicitly, right. And the idea is just to show if we, the idea is just for very, very large values of lambda, this problem becomes easy because it's basically the identity. So you can solve linear systems in it, you can compute square roots, you can do whatever you want. But the idea then is to say, once I have solved this problem for one value of lambda by preconditioning, I can use it to implement the oracles for a slightly smaller value of lambda.
00:44:22.406 - 00:45:23.314, Speaker A: So it says, if I have matrix vector access. So it says, if I've solved this problem for one value of lambda, I can use the factorizations I've computed for it and the ability to solve it, to solve linear systems and compute factorizations for a slightly smaller value of lambda through an iterative method. So the idea is to just run that framework I gave on the previous, I explained the previous slide, repeatedly decrease lambda, and that gives the result, okay, that's a high level. And actually doing this directly lets us solve perturbable placians. So that's all well and good, but what about pseudo inverses? Right? What do we do there? So the idea is we'll do the same rough idea. We'll take our matrix a, that's the pseudo inverse of a Laplacian, and add some big pseudo inverse to it, and try to argue that for very large values of that we can still solve and do everything. We can use it to precondition the next one.
00:45:23.314 - 00:46:19.644, Speaker A: Basically, if you can solve, if you can solve factor and solve factor and apply this for one, then you can solve factor and apply it for the next and recurse down. Unfortunately, when I say this, you should be a little bit skeptical because of the following fact. It's not necessarily the case that if I have some non negative combinations of some alpha mi and I look at it pseudo inverse, and I take some like non negative combinations of beta I mi and look at pseudo inverse, and then add them and take their pseudo inverse, that's not necessarily still a non negative combination of the original mi. You know, imagine these are just like two different matrices, right? The pseudo inverse of the sum of the pseudo inverse could be a very, very different looking matrix. So it's not clear how to argue that these pseudo inverses even have a nice representation to work with. So it's not quite clear how to apply this to the pseudonym. However, there's a really nice trick.
00:46:19.644 - 00:46:48.064, Speaker A: Anyone know this? It's like one sentence. Turns out we can prove this. This fact just is true for Laplacians. So it turns out if you take the pseudo inverse of Laplacian, add a multiple of the pseudo inverse of the complete graph, and take its pseudo inverse, that's still a Laplacian. So you can just work with that fun fact. I haven't seen it somewhere. It's nice.
00:46:48.064 - 00:47:36.044, Speaker A: How do you do inverse M matrices? This seems hard, because there's this diagonal scaling thing I haven't talked about. Turns out there's a rather simple trick you can use here. Turns out if you have an inverse M matrix and you just apply it to a vector, and that will give you some non negative matrix, such that if you take its diagonal, make it diagonal, and you scale your matrix by it, then it's symmetric diagonally dominant. And if you're symmetrically diagonally dominant, you're non negative combinations of two sparse vectors, you know explicitly, and you can run the framework. You might be worried about the same pseudo inverse invert issue, but you can show it holds for STD matrices too, that if a is an inverse of a symmetric STD matrix with non positive off diagonals, so is a plus lambda. I kind of cool. This lets us run the framework.
00:47:36.044 - 00:48:14.768, Speaker A: All right, so that's all I want to say. Few quick open problems. I think the really big open problem here is, can we actually make this run even faster? So the actual iteration cost of this, the time to run every iteration, is n squared. Or in this general case, where you have arbitrary PSD matrices, you basically have to apply every one of those matrices to a vector, which can be very expensive. Is there a way to do this faster? It's very closely related to the problem of trying to do streaming spectral sparsification faster. One of the things I really like about this question is if you could do it, it would imply the following really nice fact. For Laplacians, there's all sorts of, like, implicitly represented Laplacians.
00:48:14.768 - 00:48:52.948, Speaker A: You might want to get like tensors of graphs or random walk Laplacians. And if you could prove this, it would say the time that it takes you to apply a Laplacian to a vector is essentially the time it takes you to solve systems in it, too. Regardless of how complicated simplicity representation is, um, the difficulty in this is running the framework, you sort of have to do solves of really complicated implicit laplacians inside it. That's roughly related to a bunch of nearest neighbor problems that we don't know how to solve right now. Other natural open problem is there are actual, this shows that in theory these problems have enough of structure, we can solve them faster. You know, we're only solving them in quadratic time. They're not very practical algorithms.
00:48:52.948 - 00:49:35.008, Speaker A: You know, can we actually better use the structure of these problems to get something better? And I think one ni, other really interesting question is, note that this algorithm is very adaptive, right? It is making a number of adaptive queries to Laplacian to do the recovery. Is that necessary? So it's not necessary when you have the incidence matrix once you're in this framework, do you need it? We don't know. That's it. Thanks. Yeah, so it's time for a break very soon, but maybe one quick question anyone or we. Yeah, Michael, what are the nearest neighbor problems that you referred to? I'll talk to you about that offline. Good question.
00:49:35.008 - 00:50:25.554, Speaker A: I would like to hear that. It's what you can, what you can do is you could, a natural approach to make this run faster is you would open up the SDP solver and you look at what you're doing in every iteration and say what would, uh, you know, how, what kind of access would I need or what would I need to be able to do to not actually explicitly have to keep track of a weight for every single edge. And if you do that, your weights are the results of like a sequence of previous linear system solves that go inside this packing semi definite programming algorithm. And if you do that naively, you get things where, you know, you have like difference of two metrics and want to get the larger of the two. And it's, it's not clear some of them, if all of them are, some of them seem properly hard. It's not clear if there's one that's just right. Well, thank you.
00:50:25.554 - 00:50:30.134, Speaker A: So I think that's a break now.
