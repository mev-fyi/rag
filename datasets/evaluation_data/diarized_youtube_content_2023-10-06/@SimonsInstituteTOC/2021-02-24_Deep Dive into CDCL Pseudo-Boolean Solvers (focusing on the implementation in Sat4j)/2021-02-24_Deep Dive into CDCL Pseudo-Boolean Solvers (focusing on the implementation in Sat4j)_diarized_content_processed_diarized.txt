00:00:00.120 - 00:01:10.082, Speaker A: And beyond for today is open. So we're meeting to talk about Sudo Boolean solving, which is what sat solving people say when they talk about zero one integer linear programming. And we are. Our speakers are Daniel Lebert and Romain Vallon, and they're going to tell us about Sat for J, which is arguably the most well known, and I think definitely the most widely used Sudo Boolean solver, both in academia and industry. And so they've promised us an in depth investigation of what's going on inside. And I think our setup for this week too, is that the speakers will make this clear that I think that there's a intention to have it divided up into two parts where you can leave halfway, but like 1 hour regular seminar and then a break. And then after that we go really deep, an even deeper deep dive into technicalities.
00:01:10.082 - 00:01:14.974, Speaker A: So without further ado, Daniel and Roman, please, the floor is yours.
00:01:16.994 - 00:02:27.344, Speaker B: Thanks for the introduction and for inviting us. Today we are going to see what are the challenges when we want to implement the CDC algorithm in pseudo boolean solvers. First, let me introduce the context of pseudo boolean solving. In the early 2000, there has been resolution in the architecture of sad solvers with both the adoption of the CDCL approach and the use of efficient heuristics and data structures, which allows, thanks to a kind of black box approach, to make sat solvers work on a wide range of application problems without having to fine tune too much the solver to efficiently solve the problems. And with this revolution, we got two order of magnitudes speed up on some benchmarks compared to previous generation of solvers. And now more than solvers can now deal with problems containing millions of variables and clauses. So here is a brief overview of the CDCL architecture implemented in solvers.
00:02:27.344 - 00:03:38.526, Speaker B: And here we are mostly going to focus on the proof system used by the solver, and more precisely in the conflict analysis implemented in set solvers. And in these solvers we have. So the CDCL algorithm needs some invariants to work, and this invariance in set solvers are the fact that constraint propagates only once. So basically we have clauses, and once every little but one are falsified, we propagate one literal, and those constraints have a single assertion level, which is the one where all literals but one are falsified. And when clause has always propagates one literal, we say that it is a reason. And sometimes we have conflicts when all literals are falsified. And during the conflict analysis we combine reason and conflicts, and we know inside solvers that combining a reason and a conflict will always produce a conflict, and finally to detect the accession.
00:03:38.526 - 00:04:51.924, Speaker B: So basically, when we learn a new constraint, we have a syntactical criterion to detect at which level we should back jump, and we will see that all these invariants will be broken when we consider them in pseudo boolean solvers. And here the pseudo boolean solver we will consider is set for j. So an open source library for sad solving written in Java and developed since 2004, which supports pseudoboolean solving and Maxat solving. It has native pseudo boolean support and implements various proof system to reason with pseudo boolean constraints and the solver is available@satfordj.org dot so now let us define what are pseudo boolean constraints. So first I would like to motivate the use of pseudo boolean constraints by the fact that modern sat solvers are very efficient in practice, as I said before. But some instances are completely out of reach for sat solvers, and especially because of the weakness of the resolution proof system that is used during conflict analysis.
00:04:51.924 - 00:06:33.404, Speaker B: And this is particularly true for instances that require the ability to count, such as pigeonhole principle formulae. We state that you cannot put n pigeons into n minus one holes, and here modern site solvers perform only if we have more than 20 pigeons. But pseudobullian soldier based on cutic planes may solve such instances in linear time. So let us see how we can represent such problem with pseudo Boolean constraints. And a pseudo Boolean constraint in general is a constraint of this form, which is a weighted sum of literal, where all coefficients are integers, literals are li are literals, which means that we have Boolean variable or its negation, with the fact that the negation of a variable is equal to one minus this variable in the model general way, we have any relational operator here among these five operators here, and the degree of the constraint which is the right hand side of the constraint here is also an integer. So for example, we have here pseudoboolean constraints, here ABC and doctor littles, and we have various kinds of coefficients. And without loss of generality, we are going to consider conjunctions of normalized Pb constants which are of this form.
00:06:33.404 - 00:08:00.364, Speaker B: So here all coefficients are non negative integers, and the degree is also a non negative integer, and the relational operator is always equal is always at least here. So this is without loss of generality, the sense that any pseudo boolean constraint, as I defined before, can be represented in this form. So, for example, if we get the constraint represented before, we can just rewrite it under this form, just by applying basically multiplication by minus one to have the right relational operator, and by using the equality not v is equal to one minus v to make sure that all coefficients are positive integers. So there are some fun facts with pseudoboolean constraint as this one. For example here, without any assignment, we can see that this constraint propagates c to true. So indeed, if c is not satisfied, we can see that the maximum sum we can get on the left hand side would be eight. And since we need at least ten here, c has indeed to be satisfied to satisfy the constant.
00:08:00.364 - 00:09:01.504, Speaker B: So here we can see that a Pb constraint can propagate truth value without any assignment. And basically a Pb constraint can propagate multiple truth value at different decision levels, since for instance here if we satisfy c, it's not enough to satisfy the constraint. So later on we will need to satisfy another lateral that may be propagated. And actually this constraint here is equivalent to c and the constraints in which c has been satisfied, which may be more easy, easily written as c and aob. And what is interesting to see here is that for instance in this representation not d, sorry, it's a or not, it should be a or not b actually here. And what we can see is that d does not appear in this representation. And I will come back to this observation later on in this talk.
00:09:01.504 - 00:10:20.924, Speaker B: So let us now consider our pigeonhole principle formula I told you before. So to represent this problem, we will use boolean variables pij to denote that the pigeon I is put in an all j and to state that pigeon eye should be in that hole, we just say that for each pigeon it must be in at least one of the hole j. Using this constraint here, which is actually equivalent to a clause, and to say that each hole cannot host more than one pigeon, we will use the utmost one constraint here for each hole to represent these facts, and then we will be able to normalize it as I presented before. And now let us see how we can prove unsatisfiability on this formula. So first let's make it by hand. So on this example. So first, as I said, we will normalize atmosphere constraints in order to make sure that all constraints are normalized.
00:10:20.924 - 00:11:17.236, Speaker B: And then we will just add, if we add all these constraints. So basically by adding constraints, I mean we add the left hand side together and the right hand sides together, and when lit table and its negation are present, they just cancel out by producing one. And with this addition we get this constraint here. If we now add the constraint five plus the constraint six. Then we get here three greater or equals to four, which is obviously false. And this is here how we prove the unsatisfiability of the pigeonhole print on the p general formula written using pseudoboolean constraints. So here I want to stress out that the constraints are indeed also the boolean constraints.
00:11:17.236 - 00:12:41.054, Speaker B: And in theory constraints are often represented in CNF, I mean with clauses, because we want to compare the proof system under the same input. So, and here from a serial point of view we didn't we do not want our codings, we do not want, well, we want to have the same constraints and we need somehow to recover the cardinality constraints if we start from the original clauses, and Daniel will later on talk on how we can recover such constraints. But in practice the way the constraints are expressed matters in the sense that for instance, it is easier to read pseudo boolean constraints. For instance, the atmosphere constraints is easier to read than the equivalent conjunction of clauses. And also the number of constraints is different because we need several clauses to represent that most one constraint we have. And if we have other constraints that are not closes, we can use different inference rules. So for instance, using the cutting pens proof system to make the reasoning.
00:12:41.054 - 00:14:32.394, Speaker B: And in practice it is very important to have Pb constraints in Pb solvers, because if a Pb solver is given a CNF formula as input, it will simply behave as a slow sat solver because it has many data structures to deal with pseudo boolean concern that will not be used since we only have clauses here. So let us now see how the solvers actually reason on pseudo Boolean constants, and in many cases, in particular in the case of set for j, the proof system that use is one which is known as generalized resolution, which is composed of these two rules here. So the first rule is a cancellation rule which states that if we have a literal in a constraint and the negation of this literal in another constraint, we can combine these two constraints in order to eliminate the literal l, and this will produce a constant, since l plus not l is equal to one. So here what we do is we multiply this constraint by beta, we multiply this constraint by alpha, and the constant we get is this one with the constant produced by the addition of l and not l. And the other rule simply state that if coefficients are greater than the degree, then we can just replace the coefficient by the degree, and this will preserve equivalence. And with these two rules here we can implement conflict driven constraint learning algorithm, which will combine reason and conflicts with these tools to derive new constraints. So let us see how it works in this example.
00:14:32.394 - 00:15:19.764, Speaker B: So suppose first that we have this assignment. So here, red literals means that the literals are falsified, while green literals mean that the literals are satisfied. So if now C is falsified is this constraint here we can see that not b must be satisfied to satisfy the constraint, since otherwise it only remains three on this side. So not b is propagated here. And we say that this constraint is the reason for not being. But now if we look at this other constraints, we can see that the left hand side is only equal to five, while we need six, and it is not possible to have this six here. So this constraint is conflictual.
00:15:19.764 - 00:16:21.626, Speaker B: So what we can do here is to perform a cancellation so as to eliminate b. So here is what we here is what we do. So basically here, since the coefficient of not b is six and the coefficient of b is four, here we can just multiply this constraint by two to get twelve for not b, and this constraint by three, so that we can we get twelve for b, and this will indeed eliminate the coefficient b and we get the constraint we have below. But now if we look at the left hand side, we can see that we have 15 here and it is still possible to have six. So we have in total 21 here. So this constraint is no longer conflicting. So we have broken a CDCl invariant, which states that when we combine reason and conflicts, we preserve the conflict.
00:16:21.626 - 00:17:17.534, Speaker B: So we need to find a way to restore this conflict. And this is achieved by the weakening rule. So this rule has to remove a little wall from a constraint by decreasing the degree of this constraint. So for instance here, if we want to eliminate l, we just remove it and decrease the degree by the coefficient of l. We can also partially weaken the literal by decreasing sorry, I have an example for this. So here, if we want to weaken CoA, we just decrease the degree by three and we get this constraint here. And for partial weakening we can just decrease the coefficient and the degree by the same amount.
00:17:17.534 - 00:18:28.284, Speaker B: So here, for instance, we decrease the coefficient of l by k and the degree, and here we can decrease the coefficient of b by two, and we can also decrease the degree by two to get this constraint, which is entailed by the original constant. So this rule can be applied and we will see later how to preserve conflicts. But there are actually many different ways to apply it to preserve conflicts. So the original approach is to just weaken away little from the reason, and we repeat the operation until we have the conflict. We get a conflicting constraint thanks to the application of this saturation rule. Indeed, if we just weaken the welded walls in itself, it will not modify the conflict status of the constraint. It will be actually the application of the saturation rule that will guarantee to derive a conflicting constraint.
00:18:28.284 - 00:19:48.252, Speaker B: But here, with this operation, we need to repeat multiple times to just weaken, saturate, check whether the conflict is preserved, and redo until we have preserved the conflict. So the cost of this operation is not negligible in practice. So we need another solution which takes advantage of this property given by Dickson, which says that if the coefficient of the litable to be cancelled out is equal to one in at least the conflict of the other reason, then we know that the derived constraint will be conflicting. So the idea is just to make sure that this coefficient will be equal to one thanks to the application of the weakening rule, our first approach is to weaken away ineffective little. So these little are those that do not play a role in the conflict, which is also quite which is also different compared to set solvers. Because in clauses, all litables in the clause play a role in either the propagation or the conflict for this clause. So we can, in the case of PB consent, we can, we can ineffectively tell the way, and let's see this on an example.
00:19:48.252 - 00:21:00.514, Speaker B: So let us suppose that we have this constraint under this current partial assignment. So here we can see that not b has to be propagated, because otherwise we will only have d and e that remain, and they will not be able, with the not a already satisfied, to produce the six we want. But if we need to propagate not b, it's really because of falsified literals. Actually because satisfied literals do not impact the fact that the constraint is conflicting or not, it's really falsified literals. So here the idea is to weaken away all ineffective literals and to apply the saturation rule. And this will guarantee that the coefficient of the literal, the propagated literal here, which is not b, will be one, since actually using this approach will always derive a clause. And there is also something that is quite interesting, is that actually some falsified literals may also be ineffective.
00:21:00.514 - 00:21:54.904, Speaker B: So for instance, if we have this conflicting constraint in which all literals are falsified, then we can choose one of the literals that has a coefficient one. So for instance, let's say c, and this literal is ineffective. So we can also have chosen b or f. So it depends on what. Maybe we can use heuristics to choose which literals to weaken away. And here, indeed, if we weaken C away, we get this clause, which is still conflicting. And this approach is actually equivalent to the approach implemented in solvers such as satire or satvage resolution, which lazily infer clauses to use a resolution based reasoning during conflict analysis instead of a cutting plane reasoning.
00:21:54.904 - 00:22:01.824, Speaker B: But it is actually quite slower because the way it is computed is not the same.
00:22:03.384 - 00:22:04.644, Speaker A: Roman, a question.
00:22:05.864 - 00:22:06.644, Speaker B: Yes.
00:22:07.024 - 00:22:27.924, Speaker A: So if you go back to this example, so the fact that you can remove C and still have a conflict, is there, like did you have some algorithmic way of deciding this, or you're just illustrating that we could have done this and you leave open, like, what would be the procedure for identifying such.
00:22:28.844 - 00:22:33.064, Speaker B: So basically the idea is that.
00:22:35.324 - 00:22:35.660, Speaker C: We.
00:22:35.692 - 00:23:19.264, Speaker B: Can weaken in this case, if its coefficient is strictly less than the degree. Okay, and it will. So basically we just, algorithmically speaking, we, we can weigh all literals that are not falsified, and then we can apply the weakening progressively to, if we weaken away a little with coefficient lesser than the degree, we know that we will not derive a tautology. And since all literal are falsified, we know that it will remain conflicting.
00:23:24.144 - 00:23:43.912, Speaker A: I see, because you're saying. Yeah, because since the coefficient is smaller than the degree, I mean, it's, it's not only C is failing to contribute enough, someone else must also be failing. So even if you remove C, then you can, there's still blame to go around for the other guys, and that's why you still have a conflict.
00:23:44.088 - 00:23:44.424, Speaker B: Yes.
00:23:44.464 - 00:23:45.844, Speaker A: Okay, got it, thanks.
00:23:50.124 - 00:25:06.044, Speaker B: So another way to get a coefficient of one for the coefficient on which the cancellation rule will be applied is the approach proposed in rounding set, which is to apply the division rule so as to make sure that the coefficient of the propagated or conflicting literal will be equal to one by dividing by its coefficients. And to make sure to preserve conflict, the idea is to weaken away some and falsify laterals. And I will show you this on an example. So first I want to show that the division rule simply states that we can divide all coefficients and the degree by the same amount, and then we round up coefficient ten degrees. So the approach proposing rounding set here, if we want, for instance, to perform the cancellation on the literal b here. So the coefficient of b is seven. So here we have some coefficients that are not divisible by seven.
00:25:06.044 - 00:26:14.664, Speaker B: And the approach of running set is to weaken away all unfalsified literals for which the coefficient is not divisible by seven. So here the idea is to weaken away a, since eight is not divisible by seven and f, and we will get this constraint here. Then this constraint is divided by the coefficient of b, which is seven, and we get this clause here. And another approach may be to apply partial weakening. So, for instance, here, instead of completely weakening a away, we can just weaken it partially so that its coefficient become seven instead of eight. So we just, instead of decreasing the coefficient, the degree, sorry, by eight, we will decrease it by one only. So actually, there are many different variants of this strategy, because, as I said, the idea of all these strategies is to have the coefficient of the literal on which the constellation rule is applied equal to one in at least one of the constraints.
00:26:14.664 - 00:27:38.234, Speaker B: So we can either apply the weakening on the conflicting constraint, on the reason, or on both. So what we can see here is that for the different weakening strategy I presented here, the more the one having the best performance is cert four j partial running set, which is an implementation of running set in cert four j, which uses partial weakening, and which applies the weakening on both constraints. Then we have the implementation of running set inside four j and the weakening of ineffective iteration both constraints. So here we, we can see that, in a sense, applying the weakening on both constraints is in general better than applying it in only one constraint. And one thing which is quite surprising here is that the after the, after having applied on both the best strategies, are applying only on the conflicting constraint. And this is surprising because since the early development of PB solvers, the weakening has only been applied on the region. And here, this experiment suggests that it is actually better to do this on the conflict.
00:27:38.234 - 00:27:55.922, Speaker B: So, and now that we have, thanks to the weakening world, we know that we can preserve conflicts and restore this invariant, but we need now to know when to stop the analysis.
00:27:56.018 - 00:28:35.524, Speaker A: Sorry, sorry for being slow. Another question, if you go back. So if we look at these plots, the point is that for all of these solvers, you're taking one rule, one strategy, and you're using it consistently. Yes, yes. And then you have the virtual best solver that is beating everything by quite a bit. So just to point out, if we, for the, I'm thinking for the Simons program as a like, your research question is like how if some additivity could help close the gap between the vbs and all these fixed alternatives, right?
00:28:36.104 - 00:30:19.264, Speaker B: Yes. And maybe one thing we thought about is maybe to try to use algorithm selection techniques to try to combine these strategies by trying to identify which seems to be the best, and to maybe combine the best of all strategies to reach the vbs. So now to, when we have preserved the conflict thanks to the application of the weakening wall. We need to know when to stop the analysis, because inside solvers we know that we can stop when the DeWalt close contains only one literal that is assigned at the current decision level. And we know that the back jump is performed at the decision level, which is the deepest, except of course the current one. And we know that we can bind up to this decision level. But in case of pseudo boolean solvers, we have no such syntactical detection actually, because to identify the highest back jump level to which we can back jump, we need to consider the weights of the literal, since a pseudo Boolean constraint, as I said before, may propagate literals at different decision levels.
00:30:19.264 - 00:32:06.312, Speaker B: So we need to identify the decision level, the first decision level at which this constraints propagates at least one interval, and we need to do some more computations compared to set solving. So this is yet another invariant of CDCL that we have broken here. So, but we have also Nakuli's hill in the cuticle planes proof system, which is the fact that some literals may be irrelevant in the constraint produced by the proof system. So here, for instance, if we want to cancel d out in these two constraints, we will just add them, since the coefficient is already the same, and we get this constraint here and we can see that the literal c is irrelevant in the sense that its truth value does not have an impact on the truth value of the of the constraint. And actually, if you recall the one of my first examples in which we have written a constraint, then one little disappear when we add the representation c or a and not b. And this is typically the case when we have irrelevant intervals in pseudo Boolean constants. So this is what I said about irrelevant that does not have an impact on the truth value of the constraints.
00:32:06.312 - 00:33:12.094, Speaker B: But what is interesting here is that irrelevant literal may be removed from the constraint while preserving equivalence. So here c can simply be removed and we will have equivalents. So we made some experiments and found out that many relevant literals are actually produced by set four j in many different families of benchmarks. So here we have box plots which states for each family of instances how many irrelevant variables were produced. So for each box we have the minimum and maximum which are presented here, and the horizontal bars here are the. So the curtil of the distribution and the horizontal bar in the middle is a median. So we can see that we can have many irrelevant literals produced during conflict analysis, and sometimes more than 10,000.
00:33:12.094 - 00:34:22.944, Speaker B: For instance, in the case of Euclid Pb benchmarks. So irrelevant literals are not only theoretic problems, they are indeed produced in PB solvers, but the problems is really when these little become artificially relevant. So let's see what it means on this example. Here we have the constraint I presented before with the literal c, which is irrelevant, and we have here these other constraints in which c is also irrelevant for similar reasons as for this constraint. But if we combine these two constraints to eliminate the literal a, we get this constraint here, which is equivalent to this clause. And of course, since this is a clause c is relevant, since in all non topological clauses. Sorry, since for non tautological clauses all literals are always relevant.
00:34:22.944 - 00:35:24.114, Speaker B: But here observe that if the irrelevant literal c had been removed in the original constraint, it would not appear in the derived constraint here. So we would get the clause b or d instead of the clause b or c or d, which is stronger than the clause we derived. Which means that because of irrelevant intervals, we can produce weaker constraints during conflict analysis. The problem is that detecting whether a literal is irrelevant is NPR. So we can not just say, okay, maybe some literals will be irrelevant, we will remove them and we will have a stronger constraint. No, this is more complicated. So we propose an incomplete algorithm to remove irrelevant literals from the constraints, and to this end we will use this reduction to the subsetsum problem.
00:35:24.114 - 00:37:00.614, Speaker B: So here we have that literal l is irrelevant in the constraint here, if this equality has no solution for k in one to alpha. So let's let me explain this a little bit. So intuitively, if we have if here we have indeed a solution to this equality, this means that this solution will not in itself be a solution to this constraint here, because the we do not have delta, we have less than delta, since we have delta minus k. But if we satisfy l at this point, then we will go from a model, from a counter model of the constraint to a model of the constraint, since alpha plus delta minus k, because of this interval, will be greater than delta, so the literal will be relevant. So this is the intuition behind this reduction here. So here, for instance, c is irrelevant in this constraint because it is not possible to have one for this equality and two for this equality. So we can, with this reduction to subset, some apply the classical dynamic programming algorithm to detect whether a literal is irrelevant or not.
00:37:00.614 - 00:38:29.684, Speaker B: And more interestingly, since this algorithm compute all possible sums, we will know in this, in a single run of the algorithm whether there exists a solution for one of the equalities to consider, and this will be in pseudo polynomial time, thanks to the dynamic programming algorithm. The problem with this algorithm is that it depends on the value of the degree of the constraint, and in practice the degree may be very big in the derived PB constraint. So it would be not efficient enough to simply solve subset sum on the constraints. So we need an incomplete approach to solve this subsetsum instances. And to this end we will use a modulo approach because what we want to know is to make sure that literal is irrelevant before removing it. So basically what we want is to make sure that the literal we remove are irrelevant. We those allow to consider as relevant literals that are irrelevant because what this allows to simply remove literals without considering its coefficients, as I represent in the next slide.
00:38:29.684 - 00:40:01.264, Speaker B: So we can allow to miss irrelevant literals and this can be applied thanks to an algorithm modulo fixed number. So basically here, instead of computing the subset sum, the possible sums, the real possible sums, we will compute them modulo the chosen number. And the intuition is that if the sum exists, then it will also exist modulo. And similarly, if no sum exists, it will not exist modulo. So if we cannot find the solution to the subsetsum problem modulo the chosen number, then we are sure that the literal will be irrelevant. And we can even compute the subset sum solutions on several numbers to make sure that, well, to improve the accuracy of the algorithm, since the probability of making an error will decrease if we have different numbers, since basically if as soon as it is not possible to find some modulo one of the numbers, we know that it is not possible. It will not be possible without considering the modulo here.
00:40:01.264 - 00:40:57.892, Speaker B: So now when we know that a literal is irrelevant, we can just remove it while preserving equivalence. Is a literal irrelevant, and there are two ways to do so. So here, if we consider this example, we can just locally assign the literal c to zero, since we know that it is irrelevant. And if we do, and if we do so, we just actually remove it from the constraints and we get this constraint here, we can also assign it to one which is actually equivalent to the weakening approach to get this constraint here, which is which will actually, because of the saturation, will be equivalent to a or b. And we can see that this constraint is also equivalent to a or b. So here there is no difference. But in practice there may be some difference and some constraint.
00:40:57.892 - 00:42:13.092, Speaker B: It may be preferable to use one of the approach or another one. And we use a heuristic to decide which strategy to apply, because as I said, sometimes it may be better to use one strategy and some other times it may be better to use another one. So we implemented the removal in set four j, and we can see here the difference, the impact on the size of the proof bid by the solver. So here we consider the size of the proof because the algorithm remains costly, and we wanted to really evaluate the impact of the relevant litables on the strength of the reasoning made by the solver. So we can see that there is no clear difference because there are points over and below and quite equally divided into below and above the red line here. But what is interesting here is this particular family which has vertical scover instances, and we will consider them. Yeah, more specifically.
00:42:13.092 - 00:43:35.244, Speaker B: So what we did here is that we generated more instances of this family to see whether the trend is confirmed. And indeed we can see that the number of cancellation steps applied during conflict analysis is exponentially smaller when we remove irrelevant database. And actually if we look at the behavior of Satford j on this family, we can see that the first constant learned by the solver has this form here where n is a scale parameter of the instance. And what we can see is that all literals x one to x n one are irrelevant since they are not enough to make this side greater than n and x. If x is satisfied, it obviously satisfies the constraint. And actually this constraint is equivalent to the unit clause x. And what is also interesting on this family is that no other irrelevant literals are detected in the other constraint derived by Sat for j, which means that even few irrelevant literals produced at the very beginning of the exploration of the search space may lead to the prediction of an exponentially larger proof.
00:43:35.244 - 00:45:14.958, Speaker B: And this tends to suggest, this suggests that it is really important to find a way to avoid the production of irrelevant literals, since detecting them is very costly. And this is what is shown on this eto plot here, where we compare the runtime of the solver with and without the detection of irrelevant literals, really in terms of times here. And we can see that the solver is much slower in general when this removal of irrelevant literal is activated because of the cost of the algorithm. So here we will, instead of removing them, it would be very interesting to avoid producing them in the first place. And a possibility is to actually use one of the rules I presented before about the weakening of ineffective litables. So recall that, for instance, we have ineffective litables here because here they are not falsified, and here this one has a coefficient that is lower than the degree. So in this case we can consider them as locally irrelevant in the sense that if we simplify the constraint here, for instance, by saying that okay, not a is satisfied, so we can say that this is equal to three.
00:45:14.958 - 00:46:37.972, Speaker B: So we move it to the right hand side and we get three here this literal is simply removed. It is not falsified. It is falsified, sorry. And the constraint that remain is actually three, not b plus d plus c greater than three, and d and not e are irrelevant in this case, and we can say that they are irrelevant under the current partial assignments, whereas the irrelevant errors I presented before were globally relevant independently of the current assignment. And here it is easy to detect the ineffective data valves compared to the globally relevant literals. So we can make sure that irrelevant literals are removed by the weakening of ineffective literals. However, here we are, we must apply the weakening rule, whereas for irrelevant interval we could just simply remove literals from the constraints, because here we we are not sure that the literals are irrelevant.
00:46:37.972 - 00:48:03.608, Speaker B: And indeed, for instance, not a, d and e are relevant in the constraint, they are only ineffective. So this is why we need the weakening rule instead of the simple removal. But this leads to the inference of weaker constraints, since as I said, we only derive clauses here and the main idea of removing irrelevant literals were in the first place to have stronger constraints. So here we we need to find just middle between removing only relevant literals and removing more than the relevant literals, which is too, it's too much in terms of maintaining the strength if we want to maintain the strength of the constraints. Okay, so now we've seen that we need to adapt some things if we want to the cutting page proof system to fit in the CDCL architecture. But we can also further adapt PB solvers to the CDCL architecture, because here. So for the moment we mostly consider the conflict analysis procedure.
00:48:03.608 - 00:49:57.744, Speaker B: But there are also other important features in CDCL, such as the decision heuristics, the deletion of land clauses, and the restart policy. And it is well known that these features are very important for set solvers to be efficient. So we need to adapt them in PB solvers, because currently most of these heuristics are reused, as is from set solvers, without considering the particular properties of PB constraints. And what we found out is that if we consider the size of the coefficients or the current partial assignment when computing the heuristics, or choosing the which constraint to delete and when to restart, then we can significantly improve the solver and in particular set four j in this case. So if we consider, for instance, the implementation of the generalized resolution in set for J. We can see that from the default implementation which uses strategies inherited from set solving, we can incrementally improve it until the best combination we have here and I will present later on which are the different strategies here. And we have similar observations for the implementation of running set in set four j and the implementation of funding set in Satvaj with the use of the partial weakening wall used to maintain conflicts during conflict analysis.
00:49:57.744 - 00:51:29.694, Speaker B: Here is a comparison of SAt four j with a running set and in particular the different strategies implemented inside four j. And we can see that from the default implementations of SAT four j generalized resolution and the two implementation of running set inside four j. We can almost reach the performance of running set only by applying the best, the combinations of the best strategies. And thanks if we, in addition to this, if we compute, if you compute now, if we use both version of Sat four j which combine the in parallel, the cutting page based inference and resolution based inference, we have quite competitive results inside four j here, both because of the resolution proof system which is efficient, which remains efficient on some benchmarks, and thanks to the different adaptations of the strategies I just presented briefly in the previous slides. So that's it for the first part of this presentation. So if you have any questions, please let me know.
00:51:36.714 - 00:52:32.144, Speaker A: Thanks a lot, Roman. So there was in fact one question that was discussed in the chat already, but I'm bringing it up just because I don't think the chat makes it into the recording that is posted afterwards. So mate sauce asked, has it been tried to use statistics and machine learning to supervise learn the best strategy to get closer to VBS? So this he was asking long ago in the context of these, I think in the context of these different weakening strategies. But of course you could also consider it in the context of this bumping also, and Daniel Leber asked that this has not been done. And I guess, yeah, so I don't know if you want to add anything to this while we're thinking about other questions to ask you.
00:52:33.324 - 00:53:01.614, Speaker B: Well, actually we have some, we have perspectives, the idea of using machine learning to try to find the best strategies, maybe using, as I said earlier, automatic algorithm configuration or other, other strategies that we have in mind. But this is on our to do list, say.
00:53:06.274 - 00:53:54.984, Speaker A: I mean, one can also discuss, I guess, whether you want to throw machine learning at this or whether we want to understand what's happening. I mean, you could also imagine that you have some principled way of, of choosing in between different rules, like for instance, if you have several different alternatives at every step in the conflict analysis. Maybe you try all of them. Now you get a bunch of candidate constraints and you compare them and check which one looks best and then continue with this. I think this might be as I'm not very good at mixed integer linear programming, but as far as I understand, when they do cut generation, for instance, they generate lots and lots and lots of different cuts. And then they look at them and have some heuristics for saying, oh, I like these ones. They will keep these and we'll throw away the rest.
00:53:54.984 - 00:54:46.152, Speaker A: I think we haven't been doing this in sat and pseudo boolean song. I have tons of questions, but I shouldn't hog all the time I'm looking in the chat. If someone wants to unmute and ask anything, you know, if you don't do it, then I will ask another question soon. Okay, so maybe while people are thinking another question, if you go back to the plot that you just had, do you. Yeah. Do you know what this. I mean, it's a very flattering picture, I guess, for rounding set, so I shouldn't complain.
00:54:46.152 - 00:55:08.764, Speaker A: But I'm wondering, this might also be because you're measuring running time. So I wonder if would you have the same plot but using some proxy for the quality of the proof search, such as if you plotted number of conflicts instead, would it look the same or would the solvers be much, much, much closer?
00:55:10.814 - 00:55:47.274, Speaker B: Well, I not sure. I made it for all experiments. I made some at some point, but I'm not sure. I think I made it to compare different variants in cell phone, but not with running sets during, while I was making these experiments. But it should not be too hard to, to draw other plots. I still have all those. So it should be easily easy to do the other plots and take this.
00:55:48.974 - 00:55:49.782, Speaker A: Because I'm.
00:55:49.878 - 00:55:50.406, Speaker C: Yeah, yeah.
00:55:50.430 - 00:55:54.598, Speaker A: I mean, it's like. It would be because it's.
00:55:54.646 - 00:55:54.950, Speaker C: It.
00:55:55.022 - 00:56:25.724, Speaker A: I mean it could still be that maybe for some reason rounding set is really good, or it could be that. No, no. If you're, if, if they would be like, if you would have a level playing field, then actually these latest optimizations of yours are actually way better. Yeah. So this is, someone is asking how much. Paul Beam is asking how much of the difference is Java versus C, Java versus C, and Danielleber is asking probably a factor three, four.
00:56:26.664 - 00:57:14.496, Speaker C: Yeah, maybe I can comment on this. In 2011, there was a version of Minisat written by Kasten Sims called J Minisat and that was reproducing exactly the behavior of minisat. And there was a factor of 3.25 or something like this between J minisat and minisat. So, but it's complicated to say that because there are things you cannot really do in Java that you can do in C and C. So there are features that Armin presented miss talk the first week that are hardly done. I don't know how to do them on with Java, so I mean, you have to pay a price.
00:57:14.496 - 00:57:42.754, Speaker C: But there is also the just in time compiler that also on some cases also does a pretty good job. So. Well, ten. Well, okay, yeah, there are cases. Clearly site four J is not for there for speed. Right. But, well, I mean, so it's not, I think it's quite okay.
00:57:42.754 - 00:58:16.984, Speaker C: And maybe something we should mention is. So there is a difference between running set one and rounding set two based on the use of arbitrary precision arithmetic. And I think this is something we have in Java. We know that it's slow, but this is what we use. And so sad. Four J could be more efficient using just primitive types, but then there would be issues with overflows. So yeah, it's a trade off.
00:58:20.244 - 00:59:15.594, Speaker A: Yeah, but you would see, so I mean, you eliminate, you'd get some feeling for what's going on by just comparing conflict counts because that's, that's somehow a proxy or you know, that like the total sum of the length of all conflict analysis or something that would tell you something. So there are a number of comments in the chat that I'm again reading out because I don't think they make it onto the recording otherwise. Regarding the speed up from Java to Ciaran, McHreesh is saying that for subgraph solving, which is more bit parallel, the factory is over ten. And then Kieran had another comment. Actually, I don't know if you want to unmute and elaborate, Kieran, regarding this question of this heuristic, figuring out whether a suitable boolean constraint is good or bad, there are some heuristics used in some constraint programming solvers for when to use true propagation for linear equalities. Do you want to elaborate?
00:59:15.974 - 00:59:16.470, Speaker B: Yes.
00:59:16.542 - 01:00:13.614, Speaker A: So I'll talk about this more tomorrow in the encoding session. But for linear inequalities, doing the perfect propagation is NP hard. It's again, it's a subset, some kind of thing. But some solvers will look and see, okay, if the coefficients are small of a number of variables are small according to some carefully calculated set of heuristics, it will turn the linear equalities into what's called a table constraint, which is effectively a DNF representation of a constraint in some forms. So this is something that constraint programming solvers do automatically. They will rewrite the constraints that you put in. It might be worth having a look at the heuristics that CP solvers use to decide when to do this as well.
01:00:13.614 - 01:01:23.926, Speaker A: Right. This is also a good time to advertise that, because this encoding question also came up, as Roman mentioned, if you have a problem and you want to use a pseudo boolean solver, please make sure not to encode it in conjunctive normal form, because that's a very bad idea. Try to write it as more natural pseudo Boolean constraints. Then this is connected also to the question of encodings. And there will be a workshop seminar tomorrow at 530 central european time, I guess, about precisely different encoding problems. Any other questions for Romam before we take a ten minute break? So I think not. Then why don't we? So it's now three minutes, 33 minutes past the hour in whatever time zone you're in, except if you're in India, I guess, then it's three minutes past the full hour.
01:01:23.926 - 01:11:01.634, Speaker A: So, you know, even you're trying to be generic and still you can't be generic enough. So let's reconvene in ten minutes from now for those of us who wants to go even deeper into the pseudo Boolean solving deep type. But for now, we're taking a break. Thank you. So we are preparing to restart in just a few seconds. We're having fascinating discussions about time zones in the chat while we're waiting. Nepal is apparently 45 minutes off the standard time zones, together with Chatham island and australian central western time.
01:11:01.634 - 01:11:46.384, Speaker A: There you go. Any other questions that have arisen while we were having the break? Before we let Daniel and Roman loose again, feel free to unmute, especially now, I think, during the second half, people who, and if you want to ask a question and you can't because you're an attendee rather than a panelist, then there's a way of promoting you so that you can unmute and speak. Just send a message in the chat and we'll take care of it in no time. But I see no questions. So I guess, Daniel, that means it's over to you again.
01:11:50.884 - 01:12:38.114, Speaker B: Okay, so let's now make a deeper dive into sat four j. And first I want. I would like to come back to the fine tuning of sat four j with a different strategy by giving some more details on how we implemented the variance of the CDCL strategies. So let's consider again a conflict analysis with these two constraints here. So first we say that, okay, b is satisfied c is falsified, d is falsified, which propagates not a and not f. And then we have here a conflict. So we apply the cancellation rule to derive new constraints.
01:12:38.114 - 01:13:52.042, Speaker B: And here this constraint is learned, because if we come back to decision level here, two, we can see that we propagate a here. Yes, we propagate a at this point. So now we can see that the PB constraints that are involved in the conflict analysis and the constraint that is learned at the end of conflict analysis are quite different compared to the clauses that are handled by SAT solvers. So first, let's see how v seeds and EV seeds in the case of minisat based solvers can be changed to take into account the properties of PB constraints. And first I will briefly recall how it works. Basically, all variables that are encountered during conflict analysis get bumped to improve to increase their score, their evcit score. So if we do this for pseudo boolean constraints, we will bump all variables of the reason here, for instance.
01:13:52.042 - 01:15:07.626, Speaker B: So not a, not f, d and e will be bumped at some point. So we will increase and increment the score. But so as I said, this does not, this is not exactly the same thing as for clauses, since the literals do not have the same role in the constraint. So an approach that has been, that has been proposed in for adapting these seeds is to take into account only the original cardinality constraints. And the problem is that if we only consider original cardinality constraints, we do not consider non constraints and non cardinality constraints and any general PB constraints. So this does not solve our problem of adapting these seeds to PB constraints. So here we can take into account the coefficients into three new strategies, which are bumping the degree of the constraint.
01:15:07.626 - 01:17:12.854, Speaker B: So basically here we just estimate the row estimate of the number of clauses, because this is how it is, it works for cardinality constraints, since the degree is equivalent is equal to the number of clauses needed to represent the cardinality constant. And this is what is done in the VC's adaptation of Dickson. And we just wanted to generalize this strategy. Here we can also bump the coefficient to try to estimate the role of the coefficients. So here ANF, for instance, will be bumped by three, in this case Dn, and we can also make a ratio of the degree by the coefficient if we want to generalize both the approach of descent to bend by the degree and to take into account the coefficient of the literal and do the role of the litable in the constraint. So we implemented all these strategies in Satvaj and also another strategy, which is the ratio coefficient by degree, which is actually an approach which was proposed in Pueblo, which is instead of dividing the degree by the coefficient, we just divide the coefficient by the degree, which estimates the impact the role of the literal in the relative impact of the literal in the constraint compared to all the other litter bolts. And we can see that this is the only strategy that is better than the default approach when we consider at least coefficients.
01:17:12.854 - 01:18:15.768, Speaker B: But this is coefficients are not the only difference between Pb constraint and clauses, and in particular, assignments do not play the same role with PB constraints and clauses. So here, for instance, there are unassigned literal in the reason they are satisfied literal in the reason that were not propagated at this point point. And this is quite different compared to clauses when, because when we have a reason, we know that all literal are falsified except one, which is the one that is propagated. So we need to know which literals are involved in the propagation. So we have, we know that falsified literals are most likely to be involved in the constraint. But there is also the notion of effective literals I presented before. And we know that some falsified literals may do not have an impact on the propagation.
01:18:15.768 - 01:19:04.644, Speaker B: So we consider other strategies, which are the bump assigned. So we only bump literals that are assigned by the current assignments. Bump falsified which bump only litterals that are falsified and bump effective, which only bump literals that are effective. So here. So I forgot to say that we also bump the literal that is propagated. So this literal will always be satisfied since it is propagated but it is involved in the conflict, since propagating the ctrl leads to a conflict. So this is why we also bump not f here.
01:19:05.504 - 01:19:08.684, Speaker A: Would you remind us what an effective variable is?
01:19:09.464 - 01:20:20.642, Speaker B: Yes. So it's. So an effective variable is always first falsified. And if we remove all by weakening all literals that are not falsified, if coefficients of literals are smaller than the degree, then we consider these literals are ineffective. So stated otherwise, ineffective literals are literal, for which, if we weaken this literal, the way the conflict is preserved, all the propagation is preserved here. So because the intuition behind this is that we want to bump really the literals that are involved in the conflict, and effective little are really those that are involved in the conflict. And when we implemented all these strategies in Satvaj, we can see that actually all these new strategies are better than the default implementation.
01:20:20.642 - 01:20:57.960, Speaker B: Which bump all Lito's that are encountered during conflict analysis, whatever the current assignments. And we can see that indeed, the idea of this strategy is to really bump literals that are involved in the conflicts allows to improve the solver, because each time we get more precise on what we mean by involved in the conflicts, we indeed improve the performance of the solver here. And as we can see, the bump effective strategy is indeed the best of the strategies we have here.
01:20:58.152 - 01:21:23.464, Speaker A: And just to check that we're on the same page, and to understand that, the point is that if we look at these three strategies that you had on your last slide in the conflict, in the, in the context of conflict driven clause learning, there is no distinction because they're all the same. It's impossible to tell which one of these three CDCL does, because they're all the same. But when you lift from clauses to pseudoboolean constraints, then you see the difference.
01:21:23.764 - 01:21:25.064, Speaker B: Yes, indeed.
01:21:26.764 - 01:21:27.864, Speaker A: Okay, thanks.
01:21:32.584 - 01:23:04.824, Speaker B: So another way to adapt classical CDCL strategies in pseudo boolean solvers is to consider the measure for the quality of learn constraints. So basically in SAT solvers, at some point we need to delete clauses, because if we have too many learned clauses, then it would slow down unit propagation and use a lot of memory, and we need to remove some of them. And to choose which to remove, we need a quality measure. And this quality measure may also be used to trigger restarts, as proposed in glucose, for instance. But once again, if we use the quality measure used in set solvers, they do not consider the properties of PB constraints, and we can adapt them to better measure the quality of learned PB constraints. So first we have the size of the clauses, which is a naive quality measure, because the longer this clause, the lower is strength, especially from a unit propagation viewpoint, because we need to have many little specified before the constraint propagate something. But this is not the case actually for PB constraints, because as I said, we may have many literals, but one that is propagated immediately without making any assignments.
01:23:04.824 - 01:24:07.434, Speaker B: So the size does not reflect its propagation power. And this is because actually the size of PB constraints also takes into account the coefficients that appear in these constraints. So here, for instance, we have that there are different coefficients, so we need to take them into account, and they may also become very big also. So this is another point to be considered, because if coefficients become very big, we need to use arbitrary precision encodings instead of fixed precision encodings, and we need to perform more complicated arithmetic operations because they are not executed directly by the processor. But we need algorithms to perform these operations. So we also do not want to have a coefficient with big coefficients. So a possibility is to consider constraints with big coefficients of bad quality.
01:24:07.434 - 01:25:23.324, Speaker B: And to this end we may consider, for instance, the degree of the constraint. So here, for instance, it's seven, because since we have the saturation rule, we can know that the degree of the constraint is an upper bound of the coefficients in the constraint. So with only the degree we can evaluate the size of the coefficients in the constraints, and we can either consider the view as a quality measure, or the size in terms of number of bits required to represent it. To measure the quality of the constraint, we can also generalize the LBD measure of clauses to PB constraints. So here the idea is to consider the number of decision levels that appear in a clause. The problem is that if we look at this constraint, for instance, there is a literal that is unassigned. So what is its decision level? So we need to take this into account to define an LBD measure for PB constraints.
01:25:23.324 - 01:26:20.038, Speaker B: And also there are satisfied details. Whereas when we consider LBD, we either measure it on a conflicting constraint or constraints that propagate depending on whether the constraint is learned or is used as a reason. Again, and in this case there is only one literal that is at most one literal that is satisfied, which is the one that is propagating. And if it is propagated, it means also that there is a falsified literal at the same decision level in this construction. So we need to take this into account in some way. There are many different, it should be five here because I added one more after this. So the different LBD measures, maybe the LBD computed over assigned literals.
01:26:20.038 - 01:28:12.234, Speaker B: So we do not take into account the literals that are unassigned because there is no decision level for such literals. The LBDs measure works similarly. But instead of ignoring unassigned literals, we say that, okay, these literals are assigned that some decision levels that does not exist, but we consider this as same decision level for all literals. The LBDD for different yes, it's we consider that all unassigned literals are assigned at a different decision level. So we count all unassigned literals and we add this to the LVDA, and then we compute either the LBD over five literals or over effective literals to take into account the fact that these literals are more involved in the conflict or the propagation. So with this new strategy, so the new definitions of LBD for PB constraints and the quality measures based on the size of coefficients, we can define new deletion strategies for learn the constraints, because as for set solvers, PB solvers must delete constraints, but this is not as needed, let's say as for set solvers, at least inside four j, because cutting points based insurance, it's much, is much slower than resolution inference. So the solver in a sense learns less constraints, but since one constraint is equivalent to many clauses in general, this kind of compensates the fact that we learn few constraints.
01:28:12.234 - 01:29:47.764, Speaker B: But since we learn a few constraints, there is less impact of many constraints on the efficiency of the solver. But still we implement the long constraint deletion based on the different quality measures I presented before. So, based on the different LBD measures and the degree of the constraint, and we also use the quality measures as a restart policy based on the additive restarts proposed in glucose, which evaluates the quality of the recently done constraints. So here, if the constraints that are learned more recently are of poor quality compared to all the other constraints that have been learned, then we perform a restart. And once again we use the different quality measures to trigger restarts here. So we can see that for deletion there is not a big impact. Still, it's all better than the default, but to a little extent, and quite interestingly, doing no deletion at all is better than deleting based on the activity of the constraint, as in miniset, which is a default approach inside four j.
01:29:47.764 - 01:29:51.520, Speaker B: So we can see. Yes, yeah.
01:29:51.552 - 01:30:00.684, Speaker A: Do you, how aggressively are you erasing constraints? Are you using like glucose settings for this?
01:30:01.144 - 01:30:05.832, Speaker B: Yeah, yes, I think, yes, this is.
01:30:05.848 - 01:30:15.294, Speaker C: The same algorithm as in miniset. So you just remove half of the non blocked constraints.
01:30:17.394 - 01:30:20.614, Speaker A: So minus, I think glucose are different in this regard. So.
01:30:21.034 - 01:30:25.094, Speaker C: Yeah, but. So we.
01:30:26.794 - 01:31:15.184, Speaker A: I can cut to just why I'm asking is like, if so, so the way glucose does it roughly give or take, like after n conflicts, glucose will have something like square root n constraints left in the clause database. That's like the rough scaling, glossing over some details. Now, if actually not deleting clauses at all might be competitive, then this sort of raises the question, well, you could interpolate between aggressive deletion and less aggressive deletion. So maybe there's like a curve hiding there that will go much further to the right. But this, I guess this would be a topic for further future exploration.
01:31:23.084 - 01:32:41.552, Speaker B: So here is for the deletion strategies. So as we can see, few impact, at least in CED four j. Well, little impact in CET four j for these experiments. For restart, we have also something similar and quite interestingly the Picosat restarts perform quite well, which are the static Picosat restart policy with inner and outer restarts, which is quite competitive compared to all the other, even the LBD based implemented inside four J. And still the size here seems to matter for having good performance with restarts. And if we combine restart and deletion, since they use the same quality measure we have that the degrees, the degrees remain performance here and the activity based. So here are the static restart policies which are which are combined with default activity quality measures.
01:32:41.552 - 01:33:48.974, Speaker B: And these are not so competitive compared to the other business. This may be due to the fact that the activity based restart policy is not really performance on the benchmarks here, but still we can see that the VBS is quite far from all of these strategies. And this was also the case for the restart policies. So once again, there might be room for improvement here, maybe based on automatic configuration algorithms for instance. And so with all this, these strategies, we have combined all of them, since they can be combined and they are independent in some way, we try to get the best. And if we combine all of them in third four j, we can see that we improve the solver. And especially the fact the variant of the bump strategies is the one with the biggest impact on the solver.
01:33:48.974 - 01:34:53.664, Speaker B: And combining this strategy with the other allow us to improve the solver, but not enough to reach the vbs. So there is once again some room for improvements here. We need to try to identify what are also the interactions between these heuristics, since they are still tightly linked in the solver. So maybe there may be also interactions that we have not taken into account that may also explain why we are not. We do not read the VBs here. So here is once again the plot that compares with other server and this time there is also all Sudo Boolean solvers of the latest competition in 2016. So here we can see that sat four J is quite competitive now with all these strategies compared to all state of the art solvers, but still quite far from the first version of running set here.
01:34:53.664 - 01:35:19.804, Speaker B: And it may be interesting to see whether we can improve the performance of funding set with different strategies presenting here in sat forte. So before letting Daniel speak about this point as an answer to Jakob's talk on the bootcamp week, maybe I will take some questions if there are some.
01:35:29.164 - 01:35:57.564, Speaker A: So there is some chat activity going on which I will read out. So it looks like. So there are two differences that you could have from with regard to Minnesota and glucose when it comes to constraint deletion, but it seems like the number of constraints that are kept is roughly as in glucose. It should be squaring like square root n. But Daniel points out that the selection has been done on activity and not on lbd based method. I hope we got this right.
01:35:57.944 - 01:36:34.824, Speaker C: Yeah. So there are many different strategies in lambda strategies in cytoplaj and so for us it's a glucose if it's based on LBD and then we have inherited the other one from activity, from Minisat. So I don't think. I don't remember that. I mean this has been more than twelve years ago, but I think this is really the classical activity relation of ministers that we have.
01:36:35.684 - 01:37:15.740, Speaker A: So I think that very. It's very dangerous to answer, Daniel, when it comes to like, you know, sat solving. I'll make a fool out of myself immediately, but I'll try anyway. So I think like some very early versions of miniset, you could. The question is how do you decide when is the next time you're going to do a database cleaning? And if instead of doing an additive increase, you would increase the time limit by say multiplying by 1.25 or something, so that your database pruning becomes more and more infrequent. That would actually lead to a linear scaling.
01:37:15.740 - 01:37:49.044, Speaker A: So that now after n conflicts, you have like theta of n constraints in your database and then you can play with the multiplicative factor to decide, you know, what, what the linear factor is. But this might be worth exploring, especially if it seems like, I mean, if not erasing at all is competitive, then this might be worth exploring. Like you would clear out the bad constraints but you'd keep much more around and maybe that would be helpful.
01:37:50.064 - 01:38:16.774, Speaker C: Yeah, well we have many different ones because we even have so typically what we call a timer that will just look at how much memory you have or other things. So. Well we can, we can check and we can discuss this. No problem. Should I talk about any other questions or.
01:38:17.234 - 01:38:26.094, Speaker A: Yeah, that's what I wanted. So I don't see anything in the chat. Let me just check.
01:38:28.874 - 01:38:33.960, Speaker C: Okay. Maybe we should talk about recovering cardinality constraints.
01:38:34.082 - 01:38:36.584, Speaker A: That sounds exciting. Why don't we do that?
01:38:38.044 - 01:39:58.974, Speaker C: Exciting? I don't know. So, next slide please. Okay, so one of the issue actually we have with, well, we set for j and with many other systems is that if you just try to solve, let's say, the pig and all on the CNF, it will just not work. And so this is really painful because even people trying to play with story example will probably find the closer representation. And so there is that issue that, okay, we need to, it would be nice because a human can do it, that a solver can just figure out from the CNF how to prove efficiently the thing. So if we go back to the first example that Roma showed you about what we call the human proof for pigeonholes, actually, you see that now you do not have really the kinetic constraints. What you have is a clue representation of the kinetic constraint.
01:39:58.974 - 01:41:05.784, Speaker C: So for, for the kinetic constraints, p eleven plus p two, one plus p 31 less or equal to one. Actually you have three clauses for a, for b and for c. And so what is really, so what is really the input you should be able to deal with should be juice clauses. And actually the first step you have to do with those clauses is to sum up the three clauses to get a pseudobulin constraint. And that plaudibulum constraint will get two literals for p eleven, for p two one, or for p 31. And from this pseudobulent constraints, you can just divide them by two, and then you will recover your cardinative constraints. So here we should, so they are written greater or equal to two, but this is exactly the same thing as the negating all the literals and strict less or at most one.
01:41:05.784 - 01:42:11.774, Speaker C: And then you can continue with the proof and then you get your contradiction. So really the issue is, how is it possible? Well, is it possible in the SAT solver to find out how to sum up some of the clauses and then how to apply the division? Because here it works, because on the left hand side, all the literals do have the same coefficient. And you are going to, from the, you are going to round up the degree divided by two, which will get you two. And this is how you get your cat. And actually, so we've been discussing about this with Jakob when I was visiting him. And so I came back to Lance and then we started working on this. So, and maybe next slide.
01:42:11.774 - 01:43:06.374, Speaker C: And so, just to give you an example, so this is with Sat four J. If you take a pigeonhole problem with Android one, pigeons and Android holes. So it will be not possible to solve it. If you take any version of Cyt four j either general light resolution of bayesian resolution. But if you input, if your input is made of kinetic constraints, then the system based on resolution will work. And so, well, I mean, if you think about constraint programming, they will tell you, well, constraints matter. So your input should be the input with pre, preserve as much structure as possible, as much information as possible.
01:43:06.374 - 01:44:03.950, Speaker C: On the other hand, we have our colleagues from theoretical contest science saying, well, but if you want to play with proof system, we should have the same input. So the idea was, okay, can we find a way, well, to recover, to retrieve those kaninity constraints in the solver, but definitely not in the way a human will do it. And the idea was to work on this and to do it as a pre processing step next time. So the thing is. Yeah, well, I mean, so we have several. One of the issue we have is, so I show you one, what we call the pairwise encoding in the example. But there are actually many different ways to encode kinetic constraints.
01:44:03.950 - 01:45:20.520, Speaker C: It depends of the degree, depends if it's an equality or if it's an utmost, or at least. And so many different people have been working on many different codings, which have different strengths, depending on what you want in terms of number of variables, number of clauses, in maintaining or not a consistency, and so on. And so the issue here is, if you want to do, to recover your kinetic constraints from the CNF, in some ways, you would have to be able to recognize all those encodings and new ones. And this is not, well, this will hardly not working. So the approach was, okay, there is something we do in sat that we do often, it's unit propagation. And so the point is to try to use unit propagation to try to recover the kinetic constraints. And the nice thing is that in principle it should work for any encoding preserving at constant c.
01:45:20.520 - 01:46:18.626, Speaker C: And this is the nice property. Well, this is in principle, because at some point there is a combinatorial problem. So we have limitations. And so this was a work we've done with Amin, with Norbert, and with Emmanuel, and more recently Jakob and Jan worked on the, well, let's say dynamic in processing detection. So in our case, what we want to do is recover all the kinetic constraints and then apply CDCM, well, apply general resolution. What Jan and Jakob have done is that they integrated the detection on the fly, which means that they may not have to recover everything to be able to solve the problem. Those are two different approaches, so let's see how it works.
01:46:18.626 - 01:47:30.774, Speaker C: So there is a nice property, actually, if you think about what is a close, so this is what I mentioned earlier, if you have a closed x one or x two, or x n. Actually what it means, it means that this is an atmos n minus one constraints, where all the literals are negated. And if you think about this, what we want to achieve is from that kind of utmost k constraints, we want to be able to recover literals m one to mp, so that you get a more informative stronger, utmost constraints, and to detect m one to mp, we are going to use unit propagation. So we can see THAt in one example. So what we want to do, we have a small formula on the left. Those are only binary clauses. And what we want to do is to recover X one plus x two plus x three less or equal to one.
01:47:30.774 - 01:48:08.926, Speaker C: So let's see how we are going to do this. So let's first consider the first clause. This is MinuS one or not minus. So not X one or not X two. So this is actually equivalent to X one plus X two less or equal to one. And so from this, what we want to achieve is to perform unit propagation and to be able to see what happens if we propagate x one. And what happens if we propagate x two.
01:48:08.926 - 01:48:48.534, Speaker C: So there is a small animation. So here we have x plus one, x one plus x two less or equal to one. Now, next one, we try to propagate x one. So X one, Propagate, not x two, propagate not x four, not x four, Propagate not x three, ANd we are done. So the unit propagation of x one propagates X one and not x two, not x three, not x four. So we do the same thing for X two. And so with x two, we are going to propagate not x one.
01:48:48.534 - 01:49:54.658, Speaker C: X two will propagate not x five, not x five will propagate not x three. So what we will get, if we take the intersection of x one and x two, we will have not x three. So it means that we can add in our constraints x one plus x two, we can add x three to our at most one constraint. And that will allow you to allow us to retrieve a tiny constraint of size three instead of two. And actually we can try to propagate what happens? And this process will terminate, because if I try to propagate x one, x two and x three, there will be no more intersection and the process stops. And so this is. So on the next slide, we have the algorithm, and we will see that the idea is simple.
01:49:54.658 - 01:51:00.484, Speaker C: So you pick any clause that you, so a close of type k plus one will give you kinetic constraints, atmos K. And from that you try to recover literals that will allow you to increase the size of your atmos K constraints. So the main issue here, so it's not a big deal if k is one, okay? But when k is increasing, you will have to compute all subsets of your literals of size k. And that is actually the issue with the approach, because you have to generate them, do the unit propagation and compute the intersection. And then you can do this until the intersection is empty. So we can see some examples of the experiment. In that case, we.
01:51:00.484 - 01:52:05.304, Speaker C: So Amin had a syntactic detection for atmos one in lingering. And then Norbert has the implementation of both syntactic recognition and the proposed, what we call it semantic one meson unit propagation. And in that case we feed site four j with generalized resolution on them, and we compare that with sat four j without pre processing. And SBSAT has also some detection of kinetic constraint in that case, and this is with 900 seconds timeout. So if you look at the table of results, you will see that. So we have 14 instances of the pivot node problems with six different encodings. And so lingering finds and is faster to find the power line, because this is the syntactic syntactic detection.
01:52:05.304 - 01:53:14.254, Speaker C: So risks with the syntactical detection could also provide to site four j in australian tickle trying to solve some of the sequential or unproductive encodings, and almost the case for pairwise. And then the approach with the semantical encoding allows to solve many of the encodings, but not all. And therefore SBSAt had the best result on the, on the lateral coding, for example. And you see that if you look at Satraj without any pre processing, I mean it's over one or two benchmarks. So this is really ridiculous. And so all the, so the time for citroj to solve the benchmarks should be nothing, because it's really the time of the preprocessing if you have all the kinetic. So this is our on the pigeon principles.
01:53:14.254 - 01:54:38.856, Speaker C: We have also other examples of benchmarks with many kinetic constraints. So those are called balance block design, and those were really in the set competition on the crafted category, the very odd benchmarks, but actually they are just about counting. So if you retrieve the kinetic constraints, they are just very easy to solve. And actually you see that on all those cases, satrage was able to solve them either using the syntactical or the semantical recognition, because those are only at most two atmosphere constraints. And the last case is the so called challenging problem on the next slide where we. So the challenge benchmarks from Alan van Gelder on Eva Spence was a case in which you have to, it takes, you could run a clasp at that time, which was the best solver on crafted benchmarks. You can run it during 44 hours, it would not solve it.
01:54:38.856 - 01:56:01.428, Speaker C: But if you retrieve all the constraints with sat four J, if you recover it using the semantic approach, you will get at most three constraints and you can solve it within a second. And I just did it on my computer, even with the pure, pure Java approach, because you notice that the implementation of the preprocessor was done in C by Norbert. And just to check how many of the constraints we could recover. So we took some schedule grids. So you have typically many different constraints because you have the rows, the columns, the cubes, and typically the approach takes recover almost all the constraints if you use the syntactic ones, and the semantic watt detects all of them. So this is sort of a very well, it works well because here we have a small k. So the issue being that is generic, but we have the issue that in practice it only works if you have, if you need to recover constraints with small k.
01:56:01.428 - 01:56:15.384, Speaker C: And so if it, if he doesn't recognize the encodings, you will just get a bunch of different constraints with a few, with few literals. I think I'm done, Omar.
01:56:21.064 - 01:56:30.044, Speaker B: Okay, so maybe, unless there are some questions for Daniel, I will continue with some open questions about pseudo boolean conflict analysis.
01:56:45.764 - 01:56:48.424, Speaker A: Yeah, sounds good, I think. Let's continue.
01:56:48.964 - 01:57:53.464, Speaker B: Okay, so I would like to present two interesting questions about sudo boolean conflict analysis. So first, the first question is about improving the back jump level at which the back jump is performed after the conflict analysis in pseudo boolean solver. So the idea is to follow the classical set approach and say okay, when I found, once I found constraints that propagate something, then I learned this constraint that I propagate. So as I said earlier, at the first level at which the constraint propagates something. But actually this approach, which is equivalent to, which is the first UAP approach used inside solvers, is not always optimal in Pb solvers. So let's consider this example. So it's quite a hard example, because to see what happens, we need to have a quite big constraints.
01:57:53.464 - 01:58:37.976, Speaker B: So basically here we have two constraints and we want to do so. This first constraint here is conflicting, and this constraint here has propagated at decision level for this three literals here. So we perform a conflict analysis and we cancel the two literal f and j out by adding these two constraints. And we get this constraint here. And actually this constraint is assertive. Sorry, this constraint is assertive. And we can just stop here.
01:58:37.976 - 01:59:54.884, Speaker B: And I think it's, it propagates b at decision level two if I don't say a mistake. But the important fact here is that if we continue the conflict analysis here and we say okay, I can continue and I will eliminate the some literals, so one of c, d or j, depending on the order of the propagations. And actually these three literals will be cancelled out by the cancellation rule. And we get this constraint here with this time propagates b at decision level one. So we have improved the back jump level by continuing to perform conflict analysis. So this shows that the first UIP is not optimal here. And intuitively the idea is that in the case of clauses, the first UIP is optimal because when if we continue to perform resolutions, we will only add decision levels, and this will not change the back jump level.
01:59:54.884 - 02:01:09.762, Speaker B: Or if it's sorry, if we change it, it can only become deeper in the search tree because the literal which has the decision level at which to bike jump will remain in the closet. But here this is not the case because of the coefficients that may. So the idea is that if we add new literals, it does not mean it will decrease, it will make the back jump level deeper because of the coefficient. Maybe this will not have enough impact to make the bank jump level less interesting. But the problem is that of course in some cases the back jump level will become not will. Well, in general, by jump level we do not know in advance whether performing the cancellation here will improve the back jump level or worsen it. So it would be interesting to know when to stop, actually.
02:01:09.762 - 02:02:12.116, Speaker B: So, because the algorithm I presented earlier is not always optimal, so we need to find a way to improve it. And currently there is no real solution, and because we need to know when to stop. And really if it will in two sense, the first one is whether we will improve the back jump level and whether we will not break on use of invariance. So for instance, if we back jump too high and we back jump, but if we continue the conflict analysis too high, maybe we can lose the assertion, the concept will not be assertive anymore and it will never be possible. So to make it assertive again, and this is something we tried, at some point we tried. Okay, so if we continue, but in some cases the constraint is not assertive anymore and it will not become assertive again. So there is no possible reason.
02:02:12.300 - 02:02:34.714, Speaker A: So I mean, you could always run the decision learning scheme, no, like you should always be able to just run the full decision scheme all the way to the top. And worst case, like you get a clause that is asserted. So I don't see that you would ever lose assertiveness. Maybe we should take it offline.
02:02:35.214 - 02:02:52.702, Speaker B: Yes, maybe this was, this was really a first 1st tried, so we just tried to continue. And at some point, the constraint was not assertive. So this is really an open question at this point.
02:02:52.758 - 02:03:35.534, Speaker A: Yeah, I think you can get it back, but it's like, again, it's worth driving home the difference from CDCL, because like, one of the nice things with first UIP learn constraints is that, you know, provably, that among all the constraints that you will ever see, regardless of, you know, whether you do recursive clause minimization later or something, the first UIP is the one that will cause the longest back jump guaranteed 100% provably. Like for, for pseudo Boolean solvers, that's actually false. You can improve your back jump, or you can even, like, you can even maybe derive contradiction in one shot if you keep doing your conflict analysis beyond the first UIP point.
02:03:35.834 - 02:03:41.050, Speaker B: Yes indeed. And actually this is the case for pigeonhole principle formulae.
02:03:41.242 - 02:03:42.174, Speaker C: Exactly.
02:03:42.634 - 02:03:48.694, Speaker B: So this was actually the first example we had on this.
02:03:51.064 - 02:03:51.520, Speaker C: Okay.
02:03:51.552 - 02:03:58.272, Speaker B: And the second question is about improving conflict detection. So here, for instance.
02:03:58.328 - 02:04:06.444, Speaker A: So there's a good question, actually. So how is back jump level computed? How do you compute the back jump level?
02:04:07.744 - 02:05:14.118, Speaker B: So first we need to know whether the constraint is assertive. So we just try to see whether the constraint propagates something at the lowest decision level, meaning without considering the current one, just the one before. And to compute the back jump level, we look at the different decision levels at which they are falsified later on in the constraints, and we check whether it propagates something. So basically this is done with the slack. So, which is the sum of the coefficients of non falsified iterals minus the degree. And if at a given decision level, the slack is lower than a coefficient of a literal in the constraint, then the constraint propagates this literal. So, and because the constraints may propagate it errors at different decision level.
02:05:14.118 - 02:05:44.194, Speaker B: We need to check whether we have this property of the slack at each decision level, at least at the beginning with the idea is to check it at the decision developing in the constraint. But since we know that propagations are only triggered by falsified litter walls, we can only check this for decision level at which there is a falsified iterating the constraint. Does your question.
02:05:48.014 - 02:06:01.554, Speaker A: A follow up question? Your constraint might propagate something at some higher level, but the propagated literal is not relevant for the conflict. Would you take this into account when back jumping?
02:06:02.594 - 02:06:06.374, Speaker B: Oh, I'm not sure to see the.
02:06:09.314 - 02:06:18.614, Speaker A: Well, I think one thing, that weird thing that can happen is that you can actually propagate something to the same value. Right. But you could propagate it earlier than before.
02:06:18.914 - 02:06:25.574, Speaker C: If you propagate something, then the literal is relevant. So.
02:06:32.894 - 02:06:39.394, Speaker A: Another good question in the chat, but let's do this one first. So, what did you want to say, Daniel?
02:06:39.974 - 02:06:56.194, Speaker C: No, I mean, well, I mean, it depends. So if the literal is propagated, then it is relevant. Okay. Because it means we. If at some point we have to propagate it, it is relevant for the constraint.
02:06:56.774 - 02:06:57.662, Speaker A: But I think.
02:06:57.798 - 02:07:09.382, Speaker B: Yeah, it's not. It's not relevant in terms of general relevance. It is in the sense of. For the conflict. It is useful for the conflict. I think it is the sense of the question.
02:07:09.558 - 02:07:10.434, Speaker C: Okay.
02:07:11.014 - 02:07:39.776, Speaker A: But this also raises a question that I think it's worth driving home. This point that. Another difference from CDCL, which we already said, but it. If so, let's say it again. So you can actually get the situation where a constraint is propagating at a number of different levels. So you get a choice whether you want to back jump, you know, a few levels or some medium amount of levels, or you want to back jump very far, because the constraint is asserting at several levels. How would you choose?
02:07:39.920 - 02:07:44.284, Speaker C: No, you need to do it at the first level where it is asserted.
02:07:44.924 - 02:07:53.604, Speaker A: You will. You will pick that, like the nearest to the conflict where it is assertive. No, the farthest away.
02:07:53.764 - 02:08:04.220, Speaker C: Yeah. The fastest away the first time it becomes assertive, else it breaks your solver invariant, at least the classical CDCL invariant, because you.
02:08:04.252 - 02:08:12.364, Speaker A: It breaks the CDCL invariant. But actually, rounding set would not do this. It will like. It will just. It will do the shortest possible back jump, I think.
02:08:15.144 - 02:08:17.368, Speaker C: Closest the route. Yes, yes, yes.
02:08:17.496 - 02:08:20.244, Speaker A: Oh, no, I like the closest to the conflict.
02:08:20.784 - 02:09:07.684, Speaker C: Yeah. Do your assignment levels different? No, I mean, we have the same. So we are still in the old fashioned CDCle. It means that. So the idea is, because you learn a new clause, one of the invariants from CDCL is that as soon as you have something that can propagate it, you propagate it. So to be in a state where you have to go back to a state as if your children constraint would propagate if it was there for the beginning. So this is the reason why you have to propagate it from the highest to the root.
02:09:08.744 - 02:09:53.014, Speaker A: So again, I don't think we do this, actually. So I think what Armin is looking for is in some sense correct, that this is somewhat similar to the situation with chronological backtracking. I think in general, at least the way rounding set is coded up. Unless one of the members of my group changed it since last time I understood how this works. Rounding set will it will be in general possible that you have something on the trail that has a correct reason, but if you started from the top, then it would propagate at an earlier level. Now, but you don't know this because you didn't back jump that far. That is a situation that you can have, at least in rounding site.
02:09:53.874 - 02:10:22.560, Speaker C: Yeah, no, but I know Amin is correct that if you do currently called backtracking, you have different data structures and it will work differently at site four days. Still old fashioned. And for this you need to go back to the first decision level, the smaller, the closest to the route. The first time the constraint propagates, you have to go at that level and propagate at that level.
02:10:22.752 - 02:10:53.504, Speaker A: Okay, and another, yeah, so another question was from, I'm looking for it now from Paul Beam. So the combinations, these combinations during conflict analysis presumably have been chosen as syntactic combinations of two constraints. Is there a geometric way to understand the highest level inferred from a pair of conflicting constraints? I guess the highest level at which something inferred from a pair of constraints would be propagating. Is that the way to read the question, Paul?
02:10:56.184 - 02:12:02.024, Speaker C: Yes, well, I mean, for us it's really driven by the invariance of CDCL. It's just that you need to maintain the fact that you are going to propagate as soon as propagation is detected. And for this you need to go back to the, the highest level in which it propagates. Yeah, maybe the premise that if you do it another way, unless you use some techniques from conduct logical CDCL, I mean, it will just break the solver because. So the solver expects that everything that has been propagated at some point has been done. So if you do not maintain that, it will just not work.
02:12:03.044 - 02:12:10.308, Speaker A: I think everything would still work, actually. So maybe we should take that offline. Stefan. Yes, one comment about this.
02:12:10.476 - 02:12:27.716, Speaker B: I think the thing is, it's true that you lose propagation guarantees on your learned constraints. So it might be that your learned constraint would have propagated something. However, your existing constraints database will never be violated with respect to propagation, which.
02:12:27.740 - 02:12:29.740, Speaker A: Is in contrast to doing non chronological.
02:12:29.772 - 02:12:40.104, Speaker B: Backtracking, where I think also original constraints can be violated. And this is why it will kind of, it will still produce the right result because all original constraints are always proportional.
02:12:40.704 - 02:12:43.524, Speaker A: Also some learned constraints for mispropagations.
02:12:44.984 - 02:13:01.684, Speaker C: Yeah, but while in some ways you do not really want to do to make any difference between learn constraints and original constraints, even if you can remove some learned constraints, but. Well.
02:13:03.904 - 02:14:18.014, Speaker A: So I think one reason why this works is getting a bit technical is like, so what goes wrong when you do chronological backtracking in CDCL? One thing weird thing that can happen is that you don't have, you can like crash through a decision level because suddenly you thought you had like you were looking for a first UIP, but suddenly all literals at the last decision levels are somehow gone. And this is because your trail was out of water. And so if you want to do chronological backtracking, you have to extend CDCL to deal with this. But in pseudo Boolean solving, we have to deal with this anyway, because even if your trail is perfectly kosher and in order, it can still happen that you get cancellations, so that suddenly you have nothing at the last decision level, and then you're looking at it on the conflict side and you lost your first UIP constraint. What should I do? Well, the answer is just go continue to the next level. So in some sense, like the extra code, you need to deal with chronological backtracking, since CDCL, you already have it there in pseudo Boolean solving, because you need to deal with that case anyway. That's sort of how I view it regarding whether you could somehow get to the highest back jump level by some geometric area.
02:14:18.014 - 02:15:13.854, Speaker A: I mean, I guess it's like the, in some sense, I guess it's the least number, it's the smallest partial assignment for which the LP, the linear program is still infeasible in some sense. So I guess you could like maybe talk to an LP solver and get something. The question is like, the LP solver will take a long time to get back to you with the answer, but definitely there's like you could think about. This is a general question, like given a trail and given a conflict, could you do something more intelligent than this, like trivial cutting planes derivation that we have? I think that's a great question that is somehow hidden inside Paul Beam's question as I read it. Okay, sorry for interrupting.
02:15:15.794 - 02:15:58.184, Speaker B: Okay, so the next question I would like to present is about conflict detection in general. So here we have a set of constraints that are the current partial assignment, and we have actually two conflicts that occur simultaneously. So key two is conflicting and key four is also conflicting. So at some point the solver has to choose one conflict on which to perform the analysis. And actually we can. So if we use key four, we get this constraint here. But something which is quite weird is that if we instead.
02:15:58.184 - 02:17:01.214, Speaker B: Well, I need to check. Okay, no, here there is something which is quite weird, because actually here. So at this step, we want to perform a cancellation between this constraint, which is derived with key four and key three. And so we want to cancel out the literal, not d. And actually the reason for nod is t two, and which is quite strange is that key two is also conflicting. So this is also a broken invariant from CDCL, which is that we have conflictual reasons. And this is because actually we know that a constraint may propagate a little wall and then still become conflicting.
02:17:01.214 - 02:18:25.906, Speaker B: And this, depending on the order of the propagations, we can just have conflictual reasons. So there is a way to not have conflictual reason here, which is actually to choose k two as the conflict, but we need somehow to know that we need to choose k two. So here there are quite two questions. In one, which is how to choose the conflict in this case, and if we have a conflictual reason, what do we do with this conflictual reason? There are two first ideas that may come, which are we just continue the cancellation as if the reason was not conflictual. Or we can just say, okay, I have a conflictual reason, so maybe I just stop analyzing the conflict, I add, and I use this conflict instead. And there are probably other possibilities that we don't think about for the moment, but still we need to know that this may happen. And actually in this case, well, the two constraints we had, so this one here and this one are not comparable.
02:18:25.906 - 02:19:34.334, Speaker B: So there is no clear best way to choose a conflict. But intuitively, it's as if we have a conflictual reason. It's as if we did not choose, we did not select the earliest conflict in some sense. So this is really an open question once again, on this point that I wanted to present here. And to the last part of this presentation, I would like to discuss about the normalized forms that I presented at the very beginning of this talk, which has this form. So here we chose to have normalized form with at least operator. And so the first question is, why do we need a normalized form? So basically, we need to have normalized form, because when we apply the cancellation rule, we need to have littles on the same side of the relational operator.
02:19:34.334 - 02:19:36.570, Speaker B: So this is intuitively what?
02:19:36.642 - 02:19:36.922, Speaker C: Why?
02:19:36.978 - 02:21:16.754, Speaker B: We need to have the same relational operator on all constraints, so as to be able to linearly combines the constraints. But this does not prevent us from using an atmost operator, so why do we not use it? So typically, this is because if we want to represent clauses, it's quite easy, we just need to have an at least one constraint so if we have clauses that are represented natively, we can use many of the data structures that are designed for CNF formula in SAT solvers, such as for instance the watch literal scheme. And if instead we use the ATMos operator, then we can also have two words, scheme. But this time instead of updating watch literals, when literals get falsified, we need to update them when they get satisfied. So this kind of change the how the algorithm work at this, at this level in the solver. But the problem with the atlas operator is that it is not always practical to have this, this representation. For instance, if we consider optimization problem, so an optimization problem, you have an objective function and we want to minimize weighted sum of litables under the constraint of the problem.
02:21:16.754 - 02:22:37.514, Speaker B: So let the optimization algorithm works as follows in Pb solvers. So first we run the solver and it will find a model which assigns a certain value to the literals of the objective function. And so there is a value for this objective function that is computed based on this model. And to improve this model we just say okay, now I want to have an objective function that has a smaller value. So we just add this constraint to the solver and to say we want to minimize and if we normalize it, so here it should be at least here. So we need to compute the sum of all coefficients here. But if we do so, and if the coefficients are quite big, then the degree of the constant will be also bigger even if the value of the objective function is small, since the sum may be very big.
02:22:37.514 - 02:24:06.238, Speaker B: So here the problem is that as we use the saturation rule to have an upper bound of the coefficient, then if the degree is very big in some sense, the saturation rule will be less applicable so that we cannot decrease much the size of the coefficients when constraints are combined. So this may lead the coefficients to grow during the cancellation step used in apply during conflict analysis. So maybe it would be interesting to keep the natural atmosphere presentation in Atlanta, at least in this case, to represent the constraints. But the problem is that as I said, we have the saturation rule, but it is not applicable to utmost constraints. So we need to find another rule to replace it in solvers. And actually there is rule that is used as pre processing in mixed integer programming solvers, which is this one, which can be used instead of the saturation rule when we have utmost constraints. And here we need.
02:24:06.238 - 02:25:43.864, Speaker B: So in this rule we need to update both the coefficient of the literal and the degree, which is a biovalue which is computed based on the coefficients of the other laterals. So it may be harder to understand and maybe to compute than saturation. But with this rule we can replace the saturation rule and deal with at most constraints. So if I recap briefly what I said on normalized constraints, so we need normalized constraint to apply the cancellation rule. So we need to have to choose either to use at most or at least as a relational operator. The atlas operator is useful to represent clauses, but sometimes we would prefer to use an atmost operator, for instance an optimization problem. And we need to adapt again the proof system if instead of using at least constraint we use at most constraints, for instance using the rule I presented in the slide before and ideally it would be interesting to have a way to represent both normalized forms with at most and at least, and to apply the operation on which on the representation that seems better as this is done, for instance for SaT encodings of pseudo boolean constraints.
02:25:43.864 - 02:27:06.424, Speaker B: So now I will conclude so first brief recap of what I presented in this talk. So, current implementation of cutting planes proof system in PBSolver are not fully satisfactory as we cannot exploit in PBS in current PB solver its full strength. So for instance, if we want to derive cardinality constraints, we need to add additional steps that are not taken into account during the conflict analysis or at some other points, and to derive PB constraints in general from clauses. And also we presented that irrelevant literal may be introduced during conflict analysis when combining constraints and this little irrelevant literal way then become artificially relevant mainly to the inference of weaker constraints. And two, as a countermeasure, we can apply the weakening rule and finish active litter walls. But this is a really aggressive countermeasure that only derives causes. So we need it would be interesting to have something less aggressive and the best weakening strategies that we have in SAT for J currently is application of the partial weakening rule followed by a division step.
02:27:06.424 - 02:29:10.744, Speaker B: And also I presented a complementary heuristic implemented in CDCLPB solvers, which can be better adapted from the SAT definitions to better take into account the properties of PB constraints and to improve the performance in satvage, at least in this talk as perspectives. It would be interesting to find other strategies for applying cutting planes rule to exploit more power. So for instance the addition rule that may allow to derive general PB constraints from clauses and also we may design new strategies to prevent the production of irrelevant litter walls instead of removing them and using and that may be less aggressive that than the weakening of ineffective litter walls. And we can combine the weakening strategies and also the different CDCL strategies I presented to exploit their complementarity and to better study their interactions, for instance using automatic configuration algorithm. And so here I presented mostly the, the impact of the new strategies inside project and on decision problems. So it would be interesting to implement the new strategies in other solvers, such as funding set, for instance, and to study the impact on optimization problems or other problem in general that decision problems. And as also it would be interesting to find answer to the open questions I presented, such as how to improve the detection of the optimal back jump level during conflict analysis and how to deal with conflictual reason and that may be encountered during conflict analysis.
02:29:10.744 - 02:29:16.224, Speaker B: So thanks for your attention and I will take any questions that you may have.
02:29:20.724 - 02:29:45.164, Speaker A: Thanks a lot, Roma, and thanks Daniel. So on behalf of everybody, I'm giving you a round of applause, a true deep dive into the inner workings of sudo Boolean solving. We had a lot of questions throughout the talk, and it's already fairly late here in Europe, so we should let you off the hook fairly soon, I guess. But are there some final questions from the audience?
02:29:48.864 - 02:29:56.844, Speaker C: So I'll ask one. So you mentioned it in one line. So when you were discussing the general.
02:29:57.384 - 02:29:58.632, Speaker D: This is a great talk by the.
02:29:58.648 - 02:30:25.234, Speaker C: Way, but you were mentioning one line, you just other ways of applying cutting planes. So so far there's the round to one, and then there's the forms of generalized resolution. Are there any other versions of cutting planes that are more general that have actually been applied or. Yeah, so that's successful.
02:30:26.694 - 02:31:14.704, Speaker B: So, implemented in current pv solvers only as a version based on generalized resolution or the implementation of running set with the division and the round two one. And the really, the main lack in this PB solvers is the fact that they do not implement the addition rule that allows to combine any kind of constraint without having to having clashing literals to cancel out. And this is really because this is how the CDC architecture works. We perform cancellations along the trail. And the main perspective, I think is to try to find a way to implement a general addition rule in solvers.
02:31:15.884 - 02:31:16.664, Speaker C: Thanks.
02:31:25.864 - 02:31:39.724, Speaker D: So just to follow up on this question, just like also when you bought other cutting plane systems. So what about what do they do in integer programming, for example? Are those techniques applicable here?
02:31:42.104 - 02:31:48.604, Speaker B: I'm not really familiar with integer programming solvers.
02:31:49.404 - 02:32:01.236, Speaker D: Yeah, so this is what I was curious about. I just, I know that they apply lots of cutting planes during their algorithm, but I don't know exactly the details. I was just hoping if there's some discussion on that, but.
02:32:01.260 - 02:32:01.944, Speaker C: Okay.
02:32:04.364 - 02:32:53.640, Speaker A: So I could. So the, the point is that you don't have, they do add lots of cutting planes, but they do it during the search phase. They don't have this kind. So what they totally don't have is this conflict analysis based on linear constraints that sudo boolean solvers have. This seems to be entirely lacking. They do have a conflict analysis, but to the best of my understanding, after talking to people who actually know how MIP solvers work and implement them, the way MIP solvers do conflict analysis is that they extract clausal reasons for everything. And then they basically run CDCL, which is much, much weaker than what Roman described.
02:32:53.640 - 02:33:13.884, Speaker A: But then on the other hand, what they will do during the search phase, they will, you know, they will solve LP relaxations and then repeatedly add cutting planes and resolve the LP relaxations. And this is something that pseudo Boolean solvers have traditionally not been doing at all.
02:33:15.264 - 02:33:26.964, Speaker D: Yes, I think they do the LP and then they try to make a cutting plane that improves their lp. But this is not applicable, like in the CDCL setting. I suppose.
02:33:32.224 - 02:33:59.244, Speaker A: So. I mean, we have a paper that does this self advertising, a paper that is constrained with, meant for, like in the conflict analysis. Specifically in the conflict analysis. Well, conflict analysis is doing cutting planes, right? I mean, what we're discussing, saturation and division, are both cutting planes. So in that sense it is, you know, it is some kind of, it is already applying cutting planes rules, but it's doing it differently.
02:34:00.104 - 02:34:04.260, Speaker D: Yeah. Okay, thanks.
02:34:04.452 - 02:34:53.544, Speaker C: Yeah. And I think that the main issue we have currently is that we produce those irrelevant literals. And this is really. So we need to find a way to prevent producing those irrelevant literals. And so maybe the rules are good, but just the way we are applying them. There may be some cases where we should not or we should weaken before. We probably need to find the new settings or new constraints before to adapt the rules, maybe to apply them in a CDCL context.
02:34:55.594 - 02:35:27.954, Speaker A: I mean, somewhat related also to this is that you could certainly envision this was not quite what I think you were asking about, Dimir, but you could take the rich library of different cut rules that exist in mixed integer linear programming and you could say, well, what if I don't want to use saturation or division or I want to use some other cut rule? This you could totally do, to the best of my knowledge. Well, unless there are exciting news from Lance forthcoming, I don't think this has been done.
02:35:30.054 - 02:35:30.914, Speaker D: Okay.
02:35:31.934 - 02:35:34.794, Speaker A: And just, it's certainly conceivable to do it.
02:35:35.294 - 02:35:35.662, Speaker B: Okay.
02:35:35.678 - 02:36:04.534, Speaker D: So it would be okay to do it. And just a follow up question. So this is maybe beyond the scope of the talk itself. So just. If somebody could comment. So if you want to do this kind of CDCL for integer. So here we're doing sudo Boolean, but if you have, like an integer constraint or linear constraint, is there a way that one can do the CDCL style with integer constraints? With linear integer constraints?
02:36:06.994 - 02:36:31.254, Speaker E: Well, there's this bound propagation, which is. There's a couple of things on this, but I don't think it's neither competitive for the Mips thing, but also not competitive for the pseudo boolean. But, like, there are. People in the Kate community have been working on this, so. Nuneven Hoist and Constantin Korvin in Wolkhunkov.
02:36:32.194 - 02:36:33.050, Speaker C: Yeah.
02:36:33.242 - 02:36:46.818, Speaker A: There's a paper by Demura and Jubanovich cutting to the chase. So this is. But it somehow doesn't seem to fly. It doesn't seem to work in practice.
02:36:46.866 - 02:36:49.414, Speaker E: Three groups which have exactly tried that.
02:36:49.714 - 02:37:00.174, Speaker D: Yeah, I mean, there's also this inset approach which tries to just kind of resolve linear inequalities and hopes that it gets a conflict, but it's not guaranteed to actually get it.
02:37:01.514 - 02:37:01.994, Speaker C: And.
02:37:02.074 - 02:37:08.504, Speaker D: But it seems like as. That means that these things never seem to be as competitive as other techniques for that.
02:37:10.244 - 02:37:10.588, Speaker C: Okay.
02:37:10.596 - 02:37:12.584, Speaker D: I was just wondering if there are any other comments.
02:37:20.844 - 02:37:56.576, Speaker E: Oh, I have one comment, but I couldn't find the paper. I thought it was by Lakhtar, but by Danielle's colleague, where he had, like, a way of reducing the back jump level, even in CDCL, by kind of doing a cyclic resolution. And you allowed it in one particular case. And the surprising fact there was that this never worked. Also, lactose figured it out. So it's completely clear, sort of like, at least in that context, like going for the best back jump level was not good. So they have a paper on this, but they only have negative results.
02:37:56.576 - 02:38:08.812, Speaker E: And I also had this, and it was completely unclear to me why reducing the conflict level is counterproductive anyhow. So I couldn't find the paper. So I look, maybe somebody has. Do you recall it, Daniel?
02:38:08.868 - 02:38:09.036, Speaker B: Or.
02:38:09.060 - 02:38:09.628, Speaker E: I don't know.
02:38:09.716 - 02:38:14.188, Speaker C: Yeah, but I think this is a paper in 2008. No. Or something.
02:38:14.236 - 02:38:17.236, Speaker E: Yeah, but I couldn't find it. So it's a pretty old one.
02:38:17.260 - 02:38:32.174, Speaker C: Yes, I think it's a paper at Iktai. No, I think it's ichthyne zero eight or something like this. But yes, it's Lagdard Gilles and said.
02:38:45.434 - 02:39:25.098, Speaker A: Okay, any other comments or questions before? I think. I mean, I think we deserve the right to call this a day, but just to give a final opportunity to people. So then I think let's just thank you again for an amazing seminar. Thank you so much. And for. I think it's my take on this. I'm obviously biased, but this shows that there are so many good questions that could be discussed either immediately now after the talk on Gathertown, or, you know, in some other venue at the.
02:39:25.098 - 02:39:34.454, Speaker A: At the Simons. So thank you. Thank you again so much for the seminar. Bye.
