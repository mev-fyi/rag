00:00:01.440 - 00:00:44.212, Speaker A: Hello everyone. Welcome. I am Ben Lev Volk and I'll be speaking about joint work with Mrinal Kumar. And this paper is about something called rigid matrices and linear circuits, and some of these words may be unfamiliar, so let me just start with giving some background. As you may know, it's an important open question in complexity theory to prove superlinear circuit lower bounds on basically any powerful enough model of computation. One special case of this problem is the question of proving circuit lower bounds for computing linear transformations. So here we have a linear transformation from f to the n to f to the n.
00:00:44.212 - 00:01:30.294, Speaker A: F is some field, and we want to prove circuit lower bounds for such an object. And the model of computation is a very standard model of computation. It's called a linear circuit. A linear circuit is basically just like an arithmetic circuit that only uses linear operations which are addition and multiplication by scalar. A nice way to represent it is by a labeled graph. So here we have this graph, and every edge is labeled by a constant from the field, and every gate computes a linear combination of its children, where the coefficients of the linear combinations are the labels of the edges. So for example, we have this gate here with the labels alpha and beta that computes alpha times x one plus beta times x two.
00:01:30.294 - 00:02:42.574, Speaker A: This is a very natural model. It's also a pretty powerful model, and we don't know to prove superlinear circuit low bounds. One approach for analyzing these models was suggested by valiant many years ago. And valiant proved that if m is a matrix such that the linear transformation which is represented by m has a linear circuit of size order n and depth order log n, then you can decompose m as a sum of a low rank matrix and a very sparse matrix. Specifically, low rank means rank epsilon n and sparse means sparsity n to the one plus epsilon for every constant epsilon greater than zero. So just as a definition, we call a matrix rs rigid if you can't write it as a sum of a rank r matrix in an S parse matrix. So unsurprisingly, it's also an open problem to construct an explicit rigid matrix with parameters that will imply these lower bounds.
00:02:42.574 - 00:03:34.984, Speaker A: The best explicit constructions that we know are roughly r n squared over rigid matrices. So you can see that when you plug in r epsilon n, you get sparsity parameters which are not good enough for lower bounds. In this paper, we studied this problem of constructing explicit rigid matrices, or more generally explicit matrices, which require large circuits. And we look at those sets of rigid or non rigid matrices and we prove they have some surprising properties. Our approach is something that comes from algebraic geometry. So let me try to explain some basic definitions. Let's look at the set of non rigid matrices and denote it by u.
00:03:34.984 - 00:04:19.928, Speaker A: It's a subset of all n by n complex matrices. For this talk, let's assume the field is always the complex numbers. It's not necessary, but it will be the most convenient thing to do. And the set of non rigid matrices, well, we don't understand it very well, right? It has this weird shape and we don't really know how it looks like. We would like to understand how it looks like. One thing we can do is move from the geometry to the algebra by looking at the ideal of polynomials that are zero nu. This is a set that I denote by I of u, and it belongs to the set of n squared variate polynomials.
00:04:19.928 - 00:05:05.054, Speaker A: So this is a different world, the algebraic world. Now this is also a weird looking set that we don't really understand. And we can also move back to the geometry world by looking at the set of points where all the polynomials in I of U are zero. This is a set that we denote by u bar. So in words, these are the common zeros of the polynomials in I of U. And this set is called the zaritsky closure of, uh, but actually in our case, it's also the euclidean closure of U. This is a non trivial theorem, but it's true.
00:05:05.054 - 00:06:04.642, Speaker A: Now this set U bar is potentially larger than u, but it's a little bit nicer because it's closed and it has some nice properties. And we would like to understand the structure of this set through the interplay between algebra and geometry. Now let me try to explain how this is related to lower bounds. We call a non zero polynomial p in I of U, an equation for u. And a good way to think about such p is that it can serve as a proof that some point or some matrix, in our case is not in the set u, because if p of m is non zero, then in particular m is not in u by definition, and that would mean that m is rigid. So p is a proof for rigidity. Actually, a slightly stronger things hold.
00:06:04.642 - 00:07:00.680, Speaker A: Actually, p of m being nonzero implies that m is not in the closure of u. So we want to understand those proofs. For a proof to be useful, it's helpful if it's simple, if it's something that we can get our hands on and potentially compute p. So this setup is actually a very common setup in algebraic complexity theory. Many lower bounds for algebraic circuits, for limited model of algebraic circuits, even though they were not phrased like that originally, they are actually exactly in this setting and in the context of matrix rigidity. This line of study started in the works of Kumar, Lokam, Patankar and Sarma, and then Gesmundo, Hauenstein, Eichenmayr and Landsberg. And they studied this set of non rigid matrices and the corresponding ideal enclosure.
00:07:00.680 - 00:08:03.674, Speaker A: And in particular, one thing they proved is that there exists an equation of exponential degree. This is of course some bound, some explicit boundaries, and it actually holds for all range of parameters r and s. But one thing that they conjectured is that for an interesting range of parameters like epsilon, n and n to the one plus epsilon rigid matrices, which are the parameters that will imply circuit lower bounds, you can maybe do something much better and get equations of polynomial degree. And what we prove in this paper is that the conjecture is true. And here is our main theorem. This is actually something slightly stronger than the conjecture in the previous slide. It says that there is an equation for matrices which are not epsilon n, epsilon n squared rigid of degree at most n cubed.
00:08:03.674 - 00:09:13.084, Speaker A: Again, an equation just means that there is a non zero polynomial and it has degree at most n cubed, and it is zero on all the matrices which are not epsilon n epsilon n squared rigid. In fact, we can even prove something stronger and get a polynomial in n degree bound for a larger class of matrices. You can look at all matrices that have a circuit of size at most epsilon n squared, and there's an equation of polynomial degree. Also for this class of matrices, epsilon in these theorems is some small constant. You can take it to be something like 1100. We didn't try to optimize the constant, and we can also prove similar theorems for other similar models, like three dimensional tensors of low ranking or low slice rank, and similar things like that. One downside of our proof is that it's an existential argument and we don't produce an explicit equation.
00:09:13.084 - 00:10:03.514, Speaker A: Explicit here can mean several things. For example, you can think of explicit as having a polynomial size arithmetic circuit. The good news, however, is that it's sort of a win win situation. Of course, if there's an explicit equation, it's very good news for circuit lower bounds, because we could use that equation to prove lower bounds, as I sketched in the previous slide. However, if there's no explicit equation, it's also good news for circuit lower bounds, because it means we have identified a family of hard polynomials. So we can actually make this win win situation into a formal theorem, which I will sketch later in the talk. The proof of our main theorem is actually very simple, and I will be able to show you most of the details.
00:10:03.514 - 00:11:06.912, Speaker A: Now let's first remind ourselves, what is the theorem that there's an equation of degree at most n cubed for the set of non rigid matrices. The main lemma we need in the proof is the following. I claim that there's a polynomial map q, which has four epsilon n squared variables, and it's a map to n by n matrices. And this map has first of all low degree degree at most n squared, and it also contains in its image all non rigid matrices with parameters epsilon n and epsilon n squared. So again, the lemma claims that there is a low degree map in a few variables that captures in its image all the non rigid matrices. Lets first see how this lemma implies the theorem. Lets look at these two vector spaces.
00:11:06.912 - 00:12:04.540, Speaker A: One is the set of all n squared varied polynomials of the great most n cubed. This is the vector space in which we will find the equation, and the other is the set of four epsilon n squared varied polynomials of degree at most n to the fifth. Okay, and let's consider a linear map between these two vector spaces that takes a polynomial p in the left one to the polynomial p composed with q. So you can do this calculation of the degrees and figure out that it's indeed map between these two vector spaces. And for a fixed view q from the lemma, it's also a linear map because it respects addition and multiplication by scalar. And now let's calculate the dimension of these spaces. So the space of m variate polynomial of degree k has dimension m k.
00:12:04.540 - 00:13:00.164, Speaker A: Choose m, and we just plug in those values. And the point is that for n large enough, the dimension on the left is strictly bigger than the dimension on the right. And what it means is that this map has a non trivial kernel. There's some non zero polynomial p zero in the left vector space that is being sent to zero under this map. So when you compose it with q, it's the zero polynomial. But now it means that if I take a non rigid matrix m and I compute p zero of m, now m is q of v for some vector v because, because m is in the image of q by the assumption on q, but p zero of q of anything is zero because it's the zero polynomial. So this equals zero, which completes the proof.
00:13:00.164 - 00:14:02.954, Speaker A: So now all that remains to prove is this main lemma. So now let's prove the lemma. Let's first remind ourselves, what is the lemma that there is this low degree map on a few variables that contains in its image all the non rigid matrices. And there are two parts to the lemma. The first part is to show that there's a map that contains all low rank matrices and this has two epsilon n squared variables and degree two. And the second part of dilemma is to prove that there's a map that contains all sparse matrices and this one has degree n squared and again two epsilon n squared variables. Once we have those two parts, we just take q to be q one, q two on disjoint sets of variables, and by definition of what it means to be non rigid, this will satisfy the property.
00:14:02.954 - 00:14:44.784, Speaker A: Let's start with the construction of q one. This is very easy. We want a map that contains all low rank matrices. Well, we know that if m is a rank r matrix, then you can factor it as u times v where u is n r and v is r by n. So this is exactly what we do. Just symbolically, we think of u as an n by epsilon n symbolic matrix of variables, and v is an epsilon n by n symbolic matrix of variables. And we just compute the multiplication u times v and we think of it as a polynomial map in those variables of the matrix.
00:14:44.784 - 00:15:16.494, Speaker A: And of course by this basic property that every low rank matrix can be factored. This map would work. If you want an explicit expression. This is what the map looks like in the ijth coordinate. You just take the sum from k equals one to epsilon n of uik vkj. This was part one. Now let's do part two, which is a map that contains all sparse matrices.
00:15:16.494 - 00:15:54.406, Speaker A: I rewrote it in a more convenient way. We want this map with two s variables and degree at most n squared that would contain all s parse matrices. And it turns out this was actually done before us in a work of Spilke and Volkovich in a completely different context. They were interested in polynomial identity testing for read one's formulas. But the construction nevertheless is the same construction. So let me describe it to you. The first thing you do is you pick distinct n squared complex numbers so they can be the integers one through n squared.
00:15:54.406 - 00:16:52.834, Speaker A: For convenience, I want to denote them as alpha ij. This will make the notation easier later. And let's denote by lij the corresponding Lagrange interpolation polynomials. These are polynomials of degree n squared minus one that have the property that when you compute lij of Alpha ij, you get one. And when you compute lij of alpha I j prime for any different number in this set you get zero, and on nonlinear inputs we can't control these polynomials, but it won't matter if you want a completely explicit expression. This is how it looks like. Now to describe the map q two, what we do in the ijth coordinate, we put this polynomial sum of k goes from one to s of lij of xk times yk.
00:16:52.834 - 00:17:50.662, Speaker A: Now it's very simple, but there's a lot of notation, so let's go over it slowly. Our variables are x one through xs and y one through y, and we think of x one through xs as controlling the locations that we keep non zero in the matrix. And then we use y one through y to control the values that we actually put in the matrix. So we want to show that all sparse matrices are in the image of this map. And let's think of a sparse matrix that has the elements beta one through beta s in the locations I one j one through isjs. Let's for simplicity say that I one j one is the one one entry of the matrix. And let's think what happens when we put x one being alpha eleven in this map that we see here.
00:17:50.662 - 00:18:54.784, Speaker A: It means that the coefficient of y one will be one in the one one coordinate, because it would be l eleven of alpha eleven. And on every other coordinate it will be l I prime j prime of alpha eleven, where I prime, j prime are not one one. So it will be zero. So again, the coefficient of y one will be one on the coordinate and zero everywhere else. And similarly, whenever we fix xk to be alpha ij, it means that the coefficient of yk will be one in the ij coordinate and zero everywhere else. So by fixing the x variables, we can send these y variables to the s locations that we want them to be, and then we can just set the y's to be the elements that we want in those entries of the matrix. And this is how this map contains all sparse matrices in the image.
00:18:54.784 - 00:19:33.584, Speaker A: And that's it, right. Once we have this map, this completes the second part of the lemma. So this shows that we have this map that contains all non rigid matrices. And this completes the proof of the main theorem in the paper. We also construct such maps for other models of computation. For example, we want to construct a map that contains in the image all matrices that have a small linear circuit, and it has a low degree and a small number of variables. And this is a bit more complicated than this, though also not very complicated.
00:19:33.584 - 00:20:54.564, Speaker A: But I don't want to show this, I want to show you the application to circuit lower bounds that I promised in the title. And as I said, this relies on some win win arguments, depending on whether we have explicit equations for non rigid matrices or we don't have. So this is a theorem that we prove that suppose the polynomial identity testing problem is in p. Then at least one of the following two lower bound results hold either there's a p space algorithm that outputs the list of coefficients of a polynomial without a polynomial sized arithmetic circuit, or there's an efficient construction of epsilon n, epsilon n squared rigid matrices with an NP oracle. It's a bit of an odd statement. You can compare it to a well known theorem of carbonezon impalezzo that also shows that if there's any derandomization of the pit problem, then at least one of two lower bound results hold, and we don't know to prove their lower bounds nor our lower bounds. You can also compare it with some recent results that show explicit constructions of rigid matrices with an NP oracle.
00:20:54.564 - 00:21:46.274, Speaker A: Unlike ours, these results are unconditional, but they dont provide rigid matrices with parameters that imply circuit lower bounds. These are the papers of Alman and Chen and Bangalay, Hauscha, paradise and Tal. And the question of which lower bound holds depend on whether there are explicit equations. So if there aren't explicit equations, then it's option one, and if there is, and if pit is in p, then it's option two. It's actually even true if pit is in NP. So pit has an efficient randomized one sided error algorithm, but it's the wrong sided pit is in core p, but we don't know that it's in NPC. And our results, and also the carbonates in palliator lower bounds hold even if pit is in NP.
00:21:46.274 - 00:22:29.014, Speaker A: So I will give a brief sketch of how we prove this theorem. The first observation is that the equations we find are actually something that a p space algorithm can output. It's a solution to some linear system of equations of an exponential size. And using some standard small space algorithms for linear algebra, you can output the list of the coefficients of the equations with a p space algorithm. Now if this is a family of hard polynomials of polynomials with no polynomial size arithmetic circuit, then we are done. There's nothing to prove in this case. The first option of the theorem holds.
00:22:29.014 - 00:23:26.142, Speaker A: The other option we need to consider is that this is actually a family of easy polynomials. In this case, let's show how this implies a construction of rigid matrices with an NP oracle. We also need to assume that you can do polynomial identity testing, because basically what you do is you guess the circuits for these equations and then you verify that they are the correct equations using the pit algorithm. Once we verify these are the correct equations, it's actually easy using the PRT algorithm to also find a nonzero input to these equations. And as we argued, whenever we have an m such that p is non zero, it means that m is a rigid matrix. So if we have an NP orocal, all these steps can be done in deterministic polynomial time. This is a very high level description of the proof.
00:23:26.142 - 00:24:14.328, Speaker A: There are some technical difficulties we have to solve. So if you actually go and read the paper, you might have to suffer through many technical details. But don't be alarmed, this is essentially what's going on there. Let me finish with some open problems. The first open problem is, are there explicit equations? As I said, explicit can take many forms, and you don't have to stick to our definition of explicitness exactly. But it is a very interesting question, and it's clear that under any definition, explicit equations will be useful for proving circuit lower bounds. Ruling out explicit equations unconditionally might be hard just because we cannot prove lower bounds.
00:24:14.328 - 00:25:34.184, Speaker A: But even ruling it out under some reasonable complexity assumptions will be very interesting. And even if not, or even if we figure out that we don't expect to have explicit equations, then it's an interesting question of whether this polynomial degree bound can help in the construction of rigid matrices, because one way to interpret it is as saying that this set of non rigid matrices is not very complex from a geometric point of view, as one would maybe expect. And if it's not that complex, maybe we can understand it better. One first step that could be interesting here is looking at very small instances of rigidity, like four x four matrices or five x five matrices, and figuring out explicit equations for them. And then maybe trying to stare at the equations, you get enough time and generalize it to general n, or conjecture about generalization for every n. Our methods work very well in the asymptotic regime. But when you actually try to do the calculation for four x four or five x five matrices, you get that you need a very high degree.
00:25:34.184 - 00:26:35.800, Speaker A: And of course, computers can solve this linear system and produce equations, but the degree is so high that it's hard to imagine one could just generalize it by staring at it. But maybe there's a different argument that works better for small values of n and can produce useful equations. And the last open problem is a bit more vague, and basically asks about this notion of the closure of rigid matrices or border rigidity. We know that looking at the closure was very useful in other contexts. In algebraic complexity theory, for example, border tensorank was very useful in designing matrix multiplication algorithms, and the border of the class of polynomials with small algebraic circuits is also a very useful concept. So it's very interesting to understand the border version of the rigidity problem and to figure out whether this point of view can provide further insights on rigidity. Alright, that concludes what I had to say.
00:26:35.800 - 00:26:41.184, Speaker A: If you want more details, you can either email us or read a paper. Thank you for listening.
