00:00:00.200 - 00:00:46.242, Speaker A: Thanks, Kristin. Okay, so we all know the story of the last decade, and that is we have made significant advancement in visual representation thanks to nothing but deep networks. However, if you think the learning of these deep networks or the learning paradigm that we are using still is quite narrow in scope, so how does that learning currently happen, especially in computer vision? So the first step we do is we collect large scale data set, we label the data, and then we perform this supervised one shot learning. And by one shot, I mean you learn your network, you use it for a task, and then you forget it. Then you start training another network for another task, and so on. Now, compare this learning pattern, the one we use, to how the learning happens in human. In the case of humans, first, learning is unsupervised in nature.
00:00:46.242 - 00:01:17.320, Speaker A: So we are not using tens of different categories of mushrooms or cats to learn about our world. In fact, most of the learning happens without any labels in the case of humans. Second, we are not doing one shot learning in case of humans, we are doing more continuous and lifelong learning. And finally, most of the learning in case of humans is integrative. I mean, you use diverse sources for diverse tasks. You are not just doing one model for vision, one model for audio, a third model for NLP, and so on. But forget all these reasons.
00:01:17.320 - 00:01:57.830, Speaker A: There is one another reason why current learning is just not going to be good enough. Because manual labeling, that's what our current approach is based on, is just not scalable. For example, the biggest data set envision, which is the imagenet data set, has collected 1 million bounding boxes over the five year time period. If you talk about things like common sense knowledge, there are like CYC has 2 million common sense rules collected over the last 30 years. Now, compare this to the scale at which we want learning to perform. For example, Facebook, and that is a conservative estimate, has more than 400 million images uploaded daily. And when it comes to common sense learning, we still don't know how many such facts exist.
00:01:57.830 - 00:02:37.954, Speaker A: Some people estimate there are billion or trillion such facts that we need to collect. So my research focuses on scaling up the learning by building self supervised systems. Now, in this talk, I'm going to talk about three things. First, I'm going to talk about how we can learn visual representations by supervision from data itself. So what we are going to do is we are going to come up with the auxiliary task task where ground truth is free, but it's still going to be learning a very useful representation. However, all this learning still happens from passive images of videos, as Kristen talked about in her talk. Most of the learning in the case of humans, is embodied in nature.
00:02:37.954 - 00:03:31.530, Speaker A: We, as babies, we push objects, we poke objects, we put objects in our mouth, and this is all this data helps us to learn about our visual world. So in the second part of the talk, I'm going to talk about a version of this learning in our case. So we have a robot that basically physically tries to grasp objects, tries to push objects, poke objects, and in the process learns visual representation that is good for detection or categorization. But all this promise of self supervised learning is based on the promise of large scale data. Sanjeev asked the question to Kristin, how much data are you using, for example, in YouTube? And most of the answers, like people ask me, when you are learning, most of the answer time. The answer I give is, I am in CMU. I don't have that many GPU's, so I can only run it up to like 1 million images or so on.
00:03:31.530 - 00:04:04.256, Speaker A: But do we actually know if data is going to be helpful when you're trying to do deep learning? Maybe the performance of deep learning networks is going to plate you after we use 1 billion images. So everyone is using 1 billion. In fact, if you have to look at what's happening in a field, the researchers completely believe so. So we know that deep learning is a product of three things, the GPU's, the model sizes and data. GPU has increased exponentially in the last five years. Model sizes have increased exponentially in the last five years. But look at the data, it's just constant.
00:04:04.256 - 00:04:48.888, Speaker A: Everyone still uses Imagenet 1 million images to train their data set. So this must be the case that researchers are believing that data is no more useful. It's all about models or it's all about GPU powers. So in the last part of the talk, I'm going to give a teaser result. So this is a paper that's going to come up on archive, hopefully in a week, where we are going to scale up learning from 1 million to 300 million images, and we are going to show some surprising results that you find of what is the power of data actually instilling these deep networks? And I'm going to just give like a five minute teaser of the results. Okay, so let me start from the first part where we are trying to learn from images and videos in a self supervised manner. And we all know how the current supervised learning currently works.
00:04:48.888 - 00:05:53.534, Speaker A: So the way you do is you take an image, you feed it through a deep network, and you try to predict a label for the image. And for example, in this case, the label you're trying to predict is beagle. And your hope is that by doing this task, you are learning a representation that can somehow factorize materials, geometry, parts and so on. And because your representation is currently factorizing all these things, the same representation can be used for, objection, semantics in segmentation, estimating geometry and so on. Of course, this raises an interesting question. Is there something peculiar about this semantic task? Like, given an image, try to predict the label that helps you to learn this representation? Or can we actually learn this representation without using any semantic or manual labels at all? Something that can be still transferable to other tasks? Because if we can get rid of these manual labels, then we can scale up learning to billions of images, which is what we want to do at some point of time. Now, interestingly, in the case of NLP, they have a solution, which is what a lot of you know as word two vec.
00:05:53.534 - 00:06:47.216, Speaker A: So the idea in word two egg, just to let people know who don't know about it, is that if you're trying to learn a representation for a word like appliance, you try to predict the word in the context of this word. So, for example, for appliance, you want to learn a representation that can predict things like store bought toasters and so on. Now, because you can actually go into the paragraphs and see what word occurs next to appliances, you already have the ground truth here. So you can use this as the words that are occurring in the paragraph as ground truth to train this representation. So, inspired by this, we thought that we can also learn a deep network which is with the same idea. So what we are going to do, as a first try, was that given an image patch, we try to predict what is on the left of that image patch or right of that image patch. So, for example, for a toilet bowl, you want to predict that there must be toilet paper on the left of it or on the right of it.
00:06:47.216 - 00:07:21.124, Speaker A: And we tried this, and this didn't work, because what we are trying to do is trying to predict these pixels. And predicting pixels is a high dimensional regression problem. And these networks that we are trying to learn from scratch are not really good at predicting the pixels initially. So as a second step, we tried something simpler. Let us suppose you have two patches, a and b, and you try to predict what is the relative spatial layout of these patches. Let me make it simpler. So I'm going to put the a patch in the middle, and I'm going to say that B patch is in one of the eight locations.
00:07:21.124 - 00:08:01.676, Speaker A: Hopefully, all of you should be able to solve this problem and see where this patch is coming from. And hopefully all of you thought that the patch is coming from here. And the way you're solving this task is by associating these patches with what you have seen in the past. For example, you recognize that this is the top of the bus, this is the bottom of the bus, and generally this is the relative spatial arrangement in the case of buses. So you can see that we are trying to predict a spatial layout task. Like, how are these two patches configured with respect to each other? There's nothing semantic about this task, but to solve this problem, you have to use semantics. You have to understand that this is a bus and it corresponds to these objects.
00:08:01.676 - 00:08:29.078, Speaker A: In the past you have seen. So even though the task is non semantic, you actually end up to solve this task, you have to go via semantic representation. So our idea was very, very simple. Given an image, we sample two patches. First we sample random, sample a patch, then we sample a second patch from one of these eight locations. We take both these patches, feed it into a siamese network. So basically both the patch will compute the representation through CNN.
00:08:29.078 - 00:09:20.656, Speaker A: And after you have the final representation of these patches, you basically learn a classifier on top that is going to predict the relative spatial layout. So this is an eight class problem and that you are trying to solve. Now, in this case, ground truth is free because you are just plucking out the patches from an image, so you know how they are configured with respect to each other. And so you can basically keep plucking pair of patches from millions of images, and you can pluck out billions of such pair of patches and you just train a network to do these two learn for this task. And hopefully, once you have learned the network and you take your representation that you have learned at the end of CNN, that should be somewhat semantic in nature. So if you do some kind of nearest neighbor, if this is a query image, you hope to retrieve something like this as a nearest neighbor. And so this is a result that we get from our nearest neighbor.
00:09:20.656 - 00:09:42.700, Speaker A: And the interesting thing is that whatever learning happened was within an image. We never told the, told the. We never gave any supervision that this is a cat, this is a cat, or these two objects are similar and so on. Yet the CNN somehow figured to generalize across different instances of the same example. Yes. Does it have the property that, like.
00:09:42.772 - 00:09:45.564, Speaker B: Animals are closer and those kind of things?
00:09:45.644 - 00:10:16.150, Speaker A: Yeah, so I think that's. Yeah, that's exactly how it works. So basically, animals are more closer and however you have the idea is that as you start seeing farther and farther nearest neighbor, they start to get more noisy. So the very close ones are really good. But as you start getting farther and farther, they will start to get noisy. However, it will still put like dogs, closer than buses or something like that. Of course, the trivial implementation of just plucking two patches does not really work because you have to avoid some trivial shortcuts.
00:10:16.150 - 00:11:24.356, Speaker A: For example, if I show you these two patches, and I ask you, how are they configured with respect to each other, you can easily predict that this is the configuration, because you are using perceptual grouping and saying, thinking that there should be straight lines that should be created. So to overcome this simple laws of perceptual grouping, we basically include a gap between the patches we are sampling, and we are doing some kind of jittering, so that it's hard to predict what is the right layout by just looking at the relative, by looking at the perceptual elements. The another case was that we basically fed these two patches into the CNN, and surprisingly, the CNN was able to guess the right relative spatial layout for these two patches. If you think there is no queue inside these patches, that should help you to figure out what is the right relative spatial layout for these two patches. So then we started looking into the deeper insight, what the network is learning, and we found that this is the nearest neighbor for this patch. There's nothing semantic or there's nothing really clear. Why are these the nearest neighbor? Interestingly, we found out that all these patches that are coming as nearest neighbor are coming from the lower half of the image.
00:11:24.356 - 00:11:59.814, Speaker A: So somehow your network, or cornet, has learned to encode the location inside the image itself. So, to confirm this, we took a network and we learned a network to predict the absolute spatial location inside the image. So, given a patch, you want to predict where in the image it came from. And surprisingly, for 30% of the images, this network works really good, and it can predict, for example, this patch is coming from bottom half, this patch is coming from top half. And that seems to be really, really surprising. There's no queue. I mean, even humans cannot do it.
00:11:59.814 - 00:12:29.394, Speaker A: Now, it turns out what our network was using was chromatic aberration. So generally what you see is like all the wavelengths of light converge to same point, the focus, the focal point. However, the reality is something like this, that the blue and the red lights are converging away from the center. And so the network was able to use these chromatic aberrations, these small pixel differences that are not visible to human eye to figure out the right spatial location of these patches. Yes.
00:12:30.094 - 00:12:36.630, Speaker B: Aberration is spherically symmetric. Why should it differentiate between top right and top left bottom?
00:12:36.742 - 00:12:57.004, Speaker A: So if you look, this is how the aberration looks like. So in the case of bottom right, the magenta is going away. Yeah. Yes, sorry. How did you find out? By trying multiple things. I think actually what happened was I told the student that he cannot, he.
00:12:57.044 - 00:12:59.100, Speaker B: Cannot go on until he figures this out.
00:12:59.252 - 00:13:36.224, Speaker A: And so it was a month of looking at nearest neighbors, so it took us a lot of time to figure this one out. This is something which was really, really hard, and I think that's why it's a very good story to talk about. Okay, so to overcome this, we tried something simple. You can either learn it on black and white images or you can start dropping color channels randomly. Both the cases, it seems to work reasonably well. Okay, so now that we have, I have described the approach, what we did was we are first going to see how does the nearest neighbor for our approach looks like. So on the left is input images, on the right are the nearest neighbor for those patches.
00:13:36.224 - 00:14:19.486, Speaker A: And you can see that for the wheels, it retrieves more wheels, for horse legs, it retrieves horse legs, for the plane, it retrieves other planes and so on. You can do it for a random initialization. And of course in random initialization it does not look as good. And finally you can look at the Alexnet network and you can see the case of Alex network again, the nearest numbers look really good and comparable to our stuff. So our stuff is performing quite comparable in the case of nearest neighbor. For quantitatively evaluating, we are going to do exactly what Kristen did in her talk. And so now that she has explained the pre training, hopefully I don't need to explain a lot, but we are going to take this network that we have learned to predict relative spatial layout and we are going to fine tune this network for the task of object detection.
00:14:19.486 - 00:14:45.292, Speaker A: And so we have a small object detection dataset called VoC 2007, which has around 5000 images. So the dataset is much, much smaller compared to pre training, which is done on million of imagenet images. And we are going to compare the performance. So we are going to compare two networks. First is Alexnet, which is a seven layer network. So this is the performance. If you pre train with imagenet 1 million labels, you get 54% mean average precision.
00:14:45.292 - 00:15:02.948, Speaker A: If you do not, do not do any pre training, start from random initialization and then just fine tune it. For the VOC 2007 data you get 40.7 and our approach gets in the middle, 46.3. So it is able to learn something meaningful, but not as meaningful as imagenet at this point of time. Yes.
00:15:03.036 - 00:15:03.820, Speaker B: What's the tuning?
00:15:03.852 - 00:15:43.152, Speaker A: So you allow back propagation to change it? Yeah, you allow back propagation to change it, but with a lower learning rate. So we do not have that fast of a learning rate with these 5000 extra images that we are taking. And these 5000 images have semantic labels because we want. Generally, in the fine tuning, you keep the learning rate lower because most of the representation you feel has been learned during the pre training task. However, our task is much harder. So even at this case, it's like doing 30% performance for relative spatial layout. So we can increase the capacity of the network and use VGG, for example.
00:15:43.152 - 00:16:39.850, Speaker A: Now, in case of VGG, if you're not using any pre training, you're getting 42% mean average precision. If you're using imagenet, you get 68% mean average precision, but using our network, you get 61.7. So you can see we are starting to close the gap between the imagenet and ours, while the pre training is, the random initialization is just not working, even if you grow the capacity of the network. So the take home message for this part was that we, the surprising part was that we never actually connected two instances of cats and said that these are same objects or these are cats, and yet the network somehow learned to connect across the instances. Now, the second take home is that it's learning a representation, but it's nowhere, I mean, still far away from imagenet. And so we have still some way to go. So the next thing was we try to see, okay, why is it not working? I mean, as good as imagenet, what are we missing in this supervision? So if you think what our approach is learning is intra class invariance.
00:16:39.850 - 00:17:28.327, Speaker A: So for example, it learns to say that this and this are the same examples, because the relative location of the face and the leg is similar. But if the viewpoint of the cat changes now the relative location between the leg and the face would change. And so the representation that it learns for this face and this face is going to be completely different because now the legs are completely in a different location. So what we are missing is a viewpoint invariance in this task. So what we want to find out is that is there some other data where we can get viewpoint invariance? Now, interestingly, there is one source of data where viewpoint invariance comes for free, and that is videos. So what we do is we download thousands of videos from YouTube, we track patches. And then in these videos, the objects are moving.
00:17:28.327 - 00:18:01.668, Speaker A: So they will either do the deformation, like the deformations will change, or the viewpoints of the object will change. So, for example, initially the person is standing, and by the time the video ends, the person is in a weird slanted position. Similarly, the dog, the viewpoint completely changes by the time the video starts and video finish. So we can use all this data to learn viewpoint invariance between instances. So, the approach we try is something very, very simple. So, what we do is we track these patches in thousands of videos. So, here is a track.
00:18:01.668 - 00:18:45.942, Speaker A: For example, we took the first patch of the track, the last patch of the track. We know that we are tracking the same instance of an object. And what we are going to do is we are going to learn a representation which puts this patch and dispatch representation very close to each other. So what we are going to do is we are going to do a siamese triplet network this time. So what we are going to have is the two patches, the first patch of the track, the last patch of the track, try to learn a representation such that the FC seven of these two patches is close to each other, like they are close by in the final learned representation. And a third random patch is as far away as possible from these two patches. And so this is the contrastive loss that we are focusing, learning and embedding that puts the patches from same instances as close as possible.
00:18:45.942 - 00:19:21.494, Speaker A: And so it looks something like this, the loss that these two patches, the distance is less than the distance between this patch and a random third patch. So, for learning this network, we tracked 8 million patches from 100,000 videos. We are going to use tracking algorithms with no learning, so that we are not biasing the sampling of the patches. So we are rejecting frames with no motion. We are rejecting frames where there's a lot of camera motion. What we are looking for is for something like this, where there is a lot of object motion and very less camera motion. We find out the patches by simple sliding window to know that this patch is moving a lot.
00:19:21.494 - 00:19:57.060, Speaker A: The features inside this patch is moving a lot. And we then track this patch for 30 frames, and we get basically the initial frame and the final frame for the patch. And so here are some of the tracks you can see in this case, there is bikes like changing the viewpoints. Again, there's a deformation of the bike and so on. And so here are some of the examples. And again, you have to see that these examples, that the viewpoint invariance and deformation is significant. So, for example, here the viewpoint is changing between the cars, there is a deformation of the face of the pug, and then there is this horse.
00:19:57.060 - 00:20:29.516, Speaker A: Again, the viewpoint is changing, and so on. Um, someone asked Kristin about this thing. Um, like, if do you use random? So in our case, we start with sampling, doing random patches. However, if you actually just use random patches, the network learns very, very slowly. So what we do is some kind of hard negative mining. So for the first ten epochs, which is 150,000 iterations, we just use a third random patch. But after that we do some kind of hard negative mining, where from a batch of patches, we only select the ones which have very high loss to backprop.
00:20:29.516 - 00:21:08.202, Speaker A: And we do this hard negative mining for 200,000 iterations for faster convergence. And so, just to see what does a network end up learning. So here are some of the pool five neurons, which fire a lot for some of the neurons. So if one neuron, for example, corresponds to animal face, and you can see that in this case, like there are some dog faces, but there are sometimes sheep faces as well, which come close by, there's a potted plant neuron, there's a horse legs or legs neuron, animal legs neuron. There's a neuron corresponding to cars and so on. If you do nearest neighbor, here are some of the nearest neighbors. So on the left is imagenet, nearest neighbor.
00:21:08.202 - 00:21:37.598, Speaker A: On the right are our nearest neighbors. And you can see sometimes in our nearest neighbors, the viewpoints are very different from the query images. And it still seems to extract this because that's what we are learning in our case, the viewpoint invariance. So on the performance chart, we are going to go back to the same task. We are going to fine tune for VOC 2007. And this time, when we are just learning viewpoint invariances, we do 60.5, which is less than the one which I showed 61.7.
00:21:37.598 - 00:22:11.320, Speaker A: However, we have to remember that the invariance learned here is completely different than the invariance being learned here. So of course, the next step should be to somehow combine these two approaches together. And if you think of in terms of invariances, this is what the first approach is, learning invariance to instances. This is what second approach is, learning invariance to viewpoint. So we need to somehow combine to get the gains out of both, because for recognition, we want both these invariants. It's not just one. So the first intuition is to just do some kind of multitask learning.
00:22:11.320 - 00:22:53.612, Speaker A: However, multitask learning for these kind of stuff does not work, because in the end, a network is not able to differentiate at what layer, what invariances have to be learned. So what we ended up doing was something much more simpler. And we actually combined the approaches in the data space. So what we did was we took lots of videos. And given the videos, we first used the layout prediction to find out what are the closest patches inside the videos. So, for example, this car patch turned out to be closest to this car patch using the initial, the first approach, which I talked about, the layout prediction. Now what you can do is once you know these two patches are similar, you can start tracking these patches inside the videos and you will get a completely different viewpoints of the same car.
00:22:53.612 - 00:23:23.318, Speaker A: So this is the invariance that is being learned by the second approach. And now we just do simple transitivity in the graph, or loop closures in the graph, and we find new invariances that we should be learning. So, for example, you should be learning that this and this should also be close by in the feature space, or this and this should be close by in the feature space. And this is a third example. So you can just sample more data, more pairs of patches by using such an approach. And here are some of the examples that we are like sampling. So you can see that these bikes are labeled similar.
00:23:23.318 - 00:24:02.654, Speaker A: Then you can just use simple, create a big graph of data and try to do simple loop closures and find more examples, invariant examples. And you can then use the same approach, exactly similar to what we did for the previous learner siamese triplet network with contrastive loss. And now you can see that we can actually get the gain of both the performances. So now we are at 63.2, which is very close to imagenet. And even though we have not used any labeled data at this point of time to learn the representation, just to highlight that we are using, for the first task, 1 million imagenet images without label. For the second task, we are using 100,000 videos, again without labels.
00:24:02.654 - 00:24:04.870, Speaker A: Completely. Yes.
00:24:05.022 - 00:24:12.078, Speaker B: How do you, for the videos, how do you select the patch and then track it? Can that just be done automatically using other vision techniques?
00:24:12.126 - 00:24:44.602, Speaker A: Yeah, that is what we did. I mean, so the slide which I was talking about, we are just tracking lots of sift features. And from that we are selecting what is the right patch to track, which is that the track, which is the patch that is moving a lot, is basically corresponding to the object that is moving inside the video. And that's what we are sampling inside these videos. Essentially, how does this compare with the auto encoder approach? Because the auto encoders approach have never been able to show such good performances. In the case of. So they are more generative models, auto encoders.
00:24:44.602 - 00:25:19.182, Speaker A: But when it comes to showing performances on this vision task, they have never come even close. I mean, they are very, very far away to these kind of approach, self supervised approaches. Okay, so, um, so this is the first part where I'm trying to learn, uh, representations from images and videos. However, in both the cases, I'm using passive data, like Imagenet was a data set that was collected by someone and given to me. Similarly, videos are YouTube videos. It's like more like couch potato, which Kirsten was pointing out. But in the case of humans, learning happens more like this.
00:25:19.182 - 00:26:27.744, Speaker A: As babies, we are building block towers, we are stacking, we are throwing things in the air, we are putting things in the mouth, and we use all these actions and the supervision from effect of these actions to train our representations. So, as a next step, we wanted to build a similar idea, and we wanted to have a robot that will interact with the world in a similar manner, try to poke things, grab things, and so on, and see if we can learn a representation out of it. Specifically, we are going to have a robot that is going to grasp objects, push objects, randomly poke objects, so it has a skin sensor on the finger, and we are going to learn a representation that is going to predict these tasks, like actions, that is going to perform these actions. And hopefully, the visual representation that we are learning for these actions is still going to be good enough for tasks, for example, image classification, image retrieval, like the standard vision task. And that's what we want to check that can we actually learn these representations from actions themselves? So what are the robot tasks? We do three different robot tasks. The first one is the grasping. So what we have is a robot that goes and randomly tries to grasp object on the tabletop setting.
00:26:27.744 - 00:26:56.836, Speaker A: So sometimes the robot will be successful, sometimes it will fail. The success is measured by the force sensor in the wrist. So in the wrist, basically, you have a force sensor, and as soon as you lift something, it measures the force, and the face will turn happy. So it's like automatically labeling whether the grasp was successful or the grasp failed. And so basically, we have lots of data. So we did this 50,000 times. So we have 50,000 grasp data points, and we know whether the grasp was successful or it failed by running this 16 hours a day.
00:26:56.836 - 00:27:44.862, Speaker A: So, in this case, we ran the robot 16 hours a day for 700 robot hours to collect all this data. And the task formulation is very simple. Given an image patch, we are going to compute its representation and then at the end predict, if you try to grasp at this angle, will it be successful in grasping or not? If you try to grasp at this angle, will it be successful or not? So these are ten different, 18 different binary yes and no questions that we are answering. The second task we had is planar pushing. So we basically start pushing thousands of objects. So we did, I think, 5000 pushes for 5000 different objects. And what we are formulating the task as is, we'll store the initial image, the final image after push, and the network given the initial image and the final image is going to predict what push force did.
00:27:44.862 - 00:28:20.234, Speaker A: You apply that you can get this change in the state of the world. So you have to predict the push, push forces. And we are parameterizing the push by six coordinate, five coordinates, the starting x, starting y, ending x and ending y of the push. The third task is poking. So you have a skin sensor in the finger and you try to push objects into the table and you try to see how the force is measured on the skin sensor. So if you have a soft object, the force will be slow, if you have hard object, the force will be very rapid, the slope of the force. So the task is simple.
00:28:20.234 - 00:29:02.718, Speaker A: Given an image of an object, you try to predict the slope and the intercept of the force, essentially. And the fourth task we have a simple Viewpoint invariance. So you take an image of an object in this Viewpoint, then the robot turns around, takes the image of the same object in a different viewpoint and so on. And so you do it for 100,000 times and you collect this Viewpoint invariance data. So in this case we are using the same contrastive loss that if the viewpoint is from the same object, we want them to be closing the space. If we are taking images of different object, they should be far away in the final learn space. So we collected 80,000 viewpoint tasks, 1000 pokes, 5000 pushes and 40,000 grasping data points.
00:29:02.718 - 00:29:29.790, Speaker A: And we learned a multitask network. So this is a standard Alexnet that we are trying to learn and we are going to put these different losses at different layers, because the task are different. So grasping is a hard task. So it will be after fourth convolutional layer that we will try to predict the grasp. For the poking, we will try to predict after 6th convolution, after the 6th fully connected layer, we'll try to pick these outputs for pushing. It's a much simpler task to predict. So after three convolutional layers, we can just do prediction of push.
00:29:29.790 - 00:29:39.414, Speaker A: And when it comes to viewpoint invariance, this is the hardest of the task. So it will be coming at the 7th layer. The loss will be coming at the 7th layer. And so basically we learned, yes.
00:29:39.494 - 00:29:41.718, Speaker B: Did you have to tune that empirically or did?
00:29:41.886 - 00:30:16.512, Speaker A: So this is something we tried and it worked. We didn't try a lot here. So in fact, in a next paper we tried a much simpler architecture and it works reasonably well as well. So basically, I don't think that architecture details that matter that much, but this is what we used in this paper currently. So in terms of representation, we end up learning, again, these are the nearest neighbor. So the first image is the query, and then there are nearest neighbor and you can see that they completely make sense for bottles, we get more bottles and so on in terms of classification. So again, the representation that we are learning is quite reasonable.
00:30:16.512 - 00:30:43.088, Speaker A: So if you take random initialization, you get 46% classification rate. If you take Alexnet pre trained with imagenet, you get 82. And we are in the middle with 69% mean average precision. We also show that each task is important. So for example, without grasping data we see a maximum fall, but similarly without poking data, we see a big fall. So all these data points are really, really useful. It's not as if one data point overpowers the other data points.
00:30:43.088 - 00:31:07.150, Speaker A: Essentially we do retrieval. In the case of retrieval, we even beat imagenet network. Basically, because this is for this task, our data is much more suitable. Now, biggest problem in this setting is that our data is very limited. We are doing all these tasks in our lab and we are doing it in a tabletop setting. For vision tasks, you need to detect cats and dogs. Hardly there are any cats on dogs on my robot table, essentially.
00:31:07.150 - 00:31:49.790, Speaker A: So as a next step to generalize this, we are actually moving to drones. So we have a drone with the skin sensor all around it, and it's trying to hit as many objects as possible to basically gather the haptic data for all the objects possible in the world. So you can see it hits whiteboard, it hits the bookshelves slowly, sometimes it's going to hit the couches. And basically it's trying to gather large scale data of haptic feedback of all the possible objects inside CMU offices, all the offices, grad students. It has hit multiple grad students as well. So we do it when the humans are actually present in the scene, because this robot is pretty safe to fly. And so this is how the data looks like.
00:31:49.790 - 00:32:42.486, Speaker A: You have all these data, the camera on the drone, which is seeing the objects. And here is the haptic data that we are essentially reading from here, how much time I have. Okay, so I'm going to jump the next two or three slides, because I really want to talk about in the last five minutes of the last work. Okay, so the last work, which is like more of a teaser of something that's going to come up on archive soon, but I still want to talk about it, is that all the self supervised learning, or in fact a lot of the success of deep learning, is based on models, GPU's and data. And as I was talking about, both the GPU's and models have grown substantially, but the data doesn't seem to be growing at all, especially in the case of vision. So what is the case? Why is that? And so the question is, why is there no real effort in making imagenet ten x 100 x and so on? So there are two hypothesis that I could come up with. There might be more.
00:32:42.486 - 00:33:22.810, Speaker A: By the way, the first hypothesis is that all of us believe that it's not about data anymore, it's all about models. And so what we are going to see is splat tuning effect. If I create a 2 million data set size, I'll see maybe 1% gain in performance and so on. The second hypothesis, which is, I think what vision people actually believe, is that it's not about general representation anymore, it's more about task representation. So we should be collecting data for individual tasks. So right now imagenet is being used for general pre training, but if you want to do object detection, we should collect bounding box data and so on. So for example, in the last few years we have collected cocoa data set, which has 100,000 images for object detection or image segmentation.
00:33:22.810 - 00:33:54.524, Speaker A: So there are two possible hypothesis, and we have to see which one is true or is there any other truth which we actually do not know about. And so what we want to make sure is we are not repeating the mistakes of the past there. We actually underestimate data all the time. So in a recent work, what we are doing is we are trying to revisit this effectiveness of data, especially in the deep learning era. So what we are going to do is we are going to do an empirical study to see what will happen if you scale up the data 300 times. So we have a 300 million image data set. By the way, this data set is inside Google.
00:33:54.524 - 00:34:19.060, Speaker A: So it has 19,000 categories, object categories. There are 375 million labels inside these images. So it's 1.26 labels per image. The labels are automatically obtained. There's no human labeling these 300 million images, they're automatically obtained by a web, signals, user feedback, and this is the data that is being used in image search, for example. And we measured by random sampling how much noise is in these labels.
00:34:19.060 - 00:34:37.704, Speaker A: So they are 20% noise. So if you send a label to a user 20% of the time they'll reject the label. And they say this is the wrong label for the image. So this is kind of noisy, but not as noisy as probably yahoo. Hundred million flicker in this case. So I want to put a disclaimer before I show the results. First, we have no knowledge of training networks with 300 million images.
00:34:37.704 - 00:35:05.914, Speaker A: So what we are doing is we are taking whatever we know about training from imagenet, like the learning rate schedules. Learning these networks is complicated. It's an art, and we do not know the art of 300 million images. So we are using the art of 1 million images and applying it on 300 million images. So whatever we are getting is underestimating the impact of our data, essentially. And second, this takes, currently this network has been training for 2.5 months on a 50 gpu in parallel, and this has only finished three epochs.
00:35:05.914 - 00:35:39.234, Speaker A: We wanted to still submit the paper to the conference, so we still submitted it with whatever results we have. But it's bound to get better from here on, hopefully. So the first finding is probably reasonable. I don't think it's that surprising is that representational learning is still very, very important. You can still get huge, some reasonable gains by pre training on 300 million images. So for example, on VOC 2007, you're getting 5% improvement by pre training on this 300 million data, and then fine tuning out on VoC. On coco, you are getting more than around 3.5%
00:35:39.234 - 00:36:08.454, Speaker A: semantic segmentation, you're getting more than 4% improvement on the performance. So this one, I would say is reasonable, people expected less, but maybe this is on the right number. But the surprising one is the second one. The relationship between performance and data is still linear. So on the right I'm plotting the log scale of data, like the orders of data magnitude. On the y axis, I'm plotting the mean average performance. And you can see the performance keeps on increasing, even at 300 million.
00:36:08.454 - 00:36:29.364, Speaker A: There's no plateauing effect, even at 300 million. This kind of, at least for me, and I would love to hear in coffee breaks and so on what people think about it. And this is for another data set, this is Voc. So in Voc we are reaching 80, 81%. So you can see it's still pretty linear, but it's still starting to little bit converge. And there are two curves. The first curve is fine tuning.
00:36:29.364 - 00:36:59.812, Speaker A: The second curve is not using any fine tuning, just using the raw representation that you're using now. It should be pointed out that imagenet is somewhere in the middle, even with 1 million images, because it's 1 million clean data. 1 million data with soft maxes. We do not have a soft maxes that we can apply because we know that there's a chair in this image, but there could also be a table. There's no way we know anything. So overall, I think the conclusion that we are getting is that data is still the biggest impact making thing in this thing. There's net.
00:36:59.812 - 00:37:30.568, Speaker A: If you look at the performance to the number of layers, the curve is saturating. So it's for people who are doing machine learning. The machine learning models might be saturating, but data is not saturating the performance. There's still a lot in this data. If you can somehow get bigger, bigger data set sizes, you can do much, much better in performance. Okay, with that, I think I'll end since I'm already out of time. So the future is bright for unsurprised because it seems that data is going to be really, really helpful.
00:37:30.568 - 00:38:12.754, Speaker A: All right, thanks. Yes, everybody use 21,000 extra classes, 20,000 of which are cracked, but 1000 which are usable. I know that you can use them. I mean, so I think there's one paper which tried to use 1500 classes. I mean, there's no real study out there which shows how much will happen with, with 1500, they showed only like very small percentage improvement. I think Alyosha has a paper where they are showing that the Satan performance is saturating with. If you actually remove the imagenet data, it doesn't seem to help.
00:38:12.754 - 00:38:37.732, Speaker A: Now, interestingly, there is a data point that we have, but we are not putting in the paper because we are not sure. We also have some labeled images, and labeled images don't seem to be helping that much. It's the unlabeled data that seems to be helping the networks much more significantly than the labeled data. Now, the reason we didn't put it's a negative result. It's hard to say whether we are not training the network properly or is it something which is true or not. Yes.
00:38:37.868 - 00:38:49.304, Speaker B: Can you some comments about using a generative model versus discriminant model and recognition? For example, how does the layout estimation compare with the context enclosure?
00:38:51.444 - 00:39:18.804, Speaker A: So most of the, if you look at the benchmark results. Most of the cases, these discriminative models are seen doing significantly better than the generative models. And I'm talking about gans and everything. So in our lab, we are trying these generative models as well. And it turns out that the discrete models are doing significantly better than the generative models. But that doesn't mean that that's going to remain true in the future. It might be that the generative models are still far away because they're trying to predict pixels.
00:39:18.804 - 00:39:46.148, Speaker A: They might be modeling nuisances which are not that important to model or something. But right now, in performance basis, discriminative models are reasonably ahead of. There is a discriminative version of generating pixels as well, where you try to generate the images within the whole. Like you hide half of the image and you predict the rest of the image, but the discriminative version of it intends to work better than the generative version, even in that case. Yes.
00:39:46.276 - 00:39:57.246, Speaker B: Quick follow up on that. I mean, when you say that the discriminative versions are working better, that's still the three, the siamese triples, not a siamese tuple, where you're just sort of.
00:39:57.270 - 00:40:01.646, Speaker A: Saying, I mean, both these networks are doing substantially, I mean, I don't know, what do you mean by siamese topple?
00:40:01.670 - 00:40:13.878, Speaker B: And so, okay, basically you have supervision there. You know, these are two images that are the same. And so obviously you're giving, you're driving it much bigger.
00:40:13.926 - 00:40:36.554, Speaker A: That is why it is discriminative, because we have supervision. The only difference is that that's why I call it, if you see throughout the slides, I never use the term unsurprised because machine learning people might not like this to be called as unsupervised. We are calling it self supervised. There are, the data is supervising the whole thing. So I made sure that I discriminate between myself and the generative models, especially when I'm presenting this. Yeah.
