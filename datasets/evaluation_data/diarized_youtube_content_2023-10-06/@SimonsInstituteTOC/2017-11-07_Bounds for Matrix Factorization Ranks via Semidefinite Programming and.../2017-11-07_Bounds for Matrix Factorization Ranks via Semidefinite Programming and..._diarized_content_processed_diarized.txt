00:00:07.080 - 00:00:13.342, Speaker A: Okay, so next up we have Monique Laurent. I'll talk about lower bounds for matrix factorization ranks. We have non computer polynomial optimization.
00:00:13.478 - 00:00:59.724, Speaker B: Okay, thanks a lot. Thanks a lot to the organizers for the opportunity to speak here. It's great to be here. So my talk is based on joint work with Sandor Gripling and David Delat and Sabosa standing here. Okay, so what I want to do in this talk, I want to discuss some matrix factorization ranks, and I want to present a common framework to get lower bounds for these matrix factorization ranks. So let me start maybe just to fix the notation to recall what are the factorization ranks we will be talking about. So first, if you have a non negative matrix which could be rectangular, two ranks which we have already heard a lot about today, the non negative rank and the PSD rank.
00:00:59.724 - 00:01:45.080, Speaker B: But maybe let me introduce them again in maybe a slightly different definition, as you might be used to. So the non negative rank, you can define it as the smallest dimension D, where you can find non negative vectors. So vectors Xi for the rows and vectors yj for the columns, and having the properties at the inner product of x I and y j gives you the entry aij of the matrix and the PSD rank. It's the same definition. But now instead of having vectors which label the rows and the column, you now have PSD matrices Xi, y. And you want that the trace in a product gives you the entries of the matrix. So here in this talk, I will take the matrices, I will take them to be hermitian PSD.
00:01:45.080 - 00:02:30.824, Speaker B: Of course, you could also restrict yourself to having real symmetric would be the real version for the PSD rank. But the bounds I will discuss will actually work for the complex version of the PSD rank. So this is an asymmetric setting where you have different factors for the rows and for the columns. What you can also do is look at the symmetric setting when the matrix is symmetric, and then you require that you use the same factors for the rows and for the columns. So if you use the factors which are non negative vectors, you get the notion of completely positive rank. And of course this notion is well defined when such a factorization exists. Now it's not clear whether such factorization always exists, and when it exists, the matrix is said to be completely positive.
00:02:30.824 - 00:03:08.354, Speaker B: And so this is the symmetric analog of the non negative rank. And the symmetric analog of the positive semi definite rank would be what we call the completely positive semi definite rank. So same thing. Now we want to find the PSD hermitian matrices as factors for the rows so same factors for the rows and for the column. And such a factorization doesn't always exist. And when it exists, you say that the matrix is completely PSD. So by definition, so the CP cone is contained in the CPSD cone, which itself is contained in the PST cone.
00:03:08.354 - 00:04:14.514, Speaker B: So these are the four metrics factorization ranks we are interested in. And the goal of the talk is to present one approach which works for all the four matrix factorization ranks. Okay, so motivation for the non negative rank and the PSD rank. One of the main motivation, which I don't need to explain here today, is to bounce linear and semi definite extension complexity, maybe. Let me say a few words of motivation about the CPCon, the CPSDCon. So the CPCoN, the big motivation for it is the fact that you can use it to formulate like combinatorial problems like the stable set problem or coloring problem, or large class of quadratic programming problems. What about the CPS decon? It has been introduced recently in order to model quantum analog of graph parameters like the stable set number or the coloring number, which you can define them in terms of existence of.
00:04:14.514 - 00:04:59.876, Speaker B: You can formulate them as you have a system of equations and you are trying to find zero one solutions. And now there are quantum analogs of these parameters which can be defined. You take that same system of equations, but instead of looking for binary solution, you now look for solutions which are positive semi definite matrices of any size. So this is one way of, a very quick way to describe what these quantum graph parameters are. But the fact is that you are looking for variables, positive semi definite matrices of arbitrary size. And this is why you end up with a CPSD cone. Another motivation for the CPSD cone is that it actually permits to model more generally bipartite quantum correlations.
00:04:59.876 - 00:05:57.374, Speaker B: So we don't need to go about very much detail about the definition, but a quantum correlation is any vector, any vector which, something funny when I move here. So any vector which is of that shape. So you want to find a unit vector of a certain dimension, and you want to find hermitian PSD matrices satisfying certain condition. And this object should give you the bipartite correlation. And so there is this dimension which is free here, and the smallest dimension where you can find this object, it's entanglement dimension of the quantum correlation. So the point is that the set of quantum correlations, so this is important object in quantum information. And the point is that you can realize it as in a fine slice of the CPSD core.
00:05:57.374 - 00:06:56.744, Speaker B: And another result is that if you restrict yourself to synchronous quantum correlations, which means that it's a face, it's a special face of the set of quantum correlation. For these synchronous quantum correlations, then the entanglement dimension, you can actually get it as the CPSD rank of an associated matrix. So we find. So this is a motivation also for studying this notion of CPSD rank of a matrix. So the set of quantum correlations is important in quantum information, and it was open for a long time whether this set, very simple topological property, is a set closed, yes or no. And it's only very recently that it was shown by Slofstra that actually the set of quantum correlation is not closed. And this is for parameters large enough, and as a consequence that it's not closed.
00:06:56.744 - 00:07:37.484, Speaker B: It's a slice of the CPSD cone. So as a consequence of CPSD cone is not closed, and the parameters of Slofstra implies that it's true for size at least 1900 about very recently, there is another proof of the result of Slofstra, which is actually much shorter and much simpler and which works for smaller parameters. And as a consequence of this new result, the CPSD cone is not close for size at least ten, so it is of course size up to four. The CPSD cone coincides with the CP cone, so it's closed. So the interesting parameters remain between five and nine.
00:07:38.144 - 00:07:41.120, Speaker C: So does it say something also about the conjecture?
00:07:41.232 - 00:08:14.240, Speaker B: But this does not so CPSD. There are some links with Cone's conjecture, but this doesn't imply anything for both Kohn's conjecture. That would be great. Okay, so let's get started about our matrix factorization rank. So here are just a few very basic inequalities, just to show that for the PSD rank, at the non negative rank you have obvious upper bound. Same thing for the CP rank, you have the caratheodori upper bound. For the CPSD rank.
00:08:14.240 - 00:09:11.450, Speaker B: Well, no upper bound exists in terms of symmetric size, because if such an upper bound would exist, then the cone would be closed, and we know it's not closed, so no upper bound exists. Lower bounds again, some very trivial lower bounds rank for the non negative and the CP rank, and square root of the rank for the PSD rank and the CPSD rank. And I should have added the two nice bounds we have learned from Hamzah just now. Ok, so there are more low bounds, which are known for the non negative rank and the CP rank. And I wanted to discuss these bounds, which are from a paper by Hamsar and Pablo, because they are going to be related to the hierarchies of bounds I'm going to define after that. So these bounds are just based on the definition of the non negative rank or the CP rank in terms of having an atomic definition. So let's just look at the non negative rank.
00:09:11.450 - 00:10:01.878, Speaker B: Another way to say it. What is it? It is the smallest number d, so that you can write your matrix as the sum of d rank one non negative matrices. And now to get a bound, you just look at what are the properties that this rank one non negative matrices entering the decomposition satisfy. So they are rank one, they are non negative, and they are upper bounded by a. And now if you look at a is equal to this, you divide by d on both sides and you get a convex combination of one over da in terms of frank one matrices in this set. So now in order to get the bound, you can say bound tau plus, you can say, so you look at the smallest scalar alpha, for which one over alpha a lies in this convex set. You can do the same game for the CP rank.
00:10:01.878 - 00:10:38.936, Speaker B: So it's the same thing, except that now the rank one factors here are symmetric matrices or PSD, so you can add, and on top of that there are a minus r is PsT. So this is tau Cp. So these are nice bounds, but you cannot compute them. This is not clear how you would compute them. So Pablo and Hamza introduced efficient SDP lower bounds, which are relaxation of these two bounds, tau plus and tau sauce. We'll see later. I didn't give the exact details.
00:10:38.936 - 00:11:35.184, Speaker B: We'll see later how it relates. So for the non negative rank, there is of course another lower bound, which is well known, which is a rectangle covering boundaries. And let me mention it, because Hamza and Pablo also make a link between zero bounds and this rectangle cover bound. So the rectangle cover bound can be seen as you make the rectangle graph of the matrix and you look at the coloring number, so you can express it as a coloring number of a certain graph associated to the matrix. And in fact, if you look at the fractional coloring number, so the tau plus tau plus bound is at least as good as a fractional coloring number, and the tau so's is at least as good as the theta number. So this makes a link between all these bounds. So I will come back to these bounds later because we are going to see how they fit within the hierarchy of bounds which I'm going to define.
00:11:35.184 - 00:12:30.550, Speaker B: Okay, so the definition of these bounds I just showed you. What was crucial was the fact that you had an atomic definition, try to hide the matrix as a sum of rank one matrices. So this works for non negative rank and for CP rank, but it doesn't work for PSD rank and CPSD rank, because there is no definition, there is no atomic definition. And the way to get an approach which works for all the bounds is to go back to that initial definition I gave at the very first slides to decompose, to look, try to decompose your matrix as a gram of certain factors which could be either non negative or matrices. And that's the approach we take. So we are going to use tools from polynomial optimization. So I thought, I just recall just in a few lines, what are the tools we are going to use for our purpose.
00:12:30.550 - 00:13:39.634, Speaker B: So we are going to use polymer optimization ISO in commutative setting with commutative variables. And that's when we are going to deal with non negative rank and with CP rank or non commutative optimization when we deal with factorization by PST matrices. So here in a nutshell, what would be the polynomial optimization problem? In the commutative setting, you want to minimize a polynomial f subject to a number of polynomial inequalities. So the variable is a vector and you have a number of polynomial inequalities. So f and the g's are polynomials in commutative variable in the non commutative setting. So now instead of having a variable x which is a vector, so which is a tuple of real numbers, you now have a matrix, a tuple of matrices where the matrices x one up to xn are hermitian of a certain size, and the constraints look like this. So you have a polynomial now which is a polynomial in non computative variable, which you evaluate at the matrices.
00:13:39.634 - 00:14:26.348, Speaker B: So if you see a variable x one, you replace it by matrix x one. So g of x is going to be a matrix, and the constraint is that this matrix should be PST and so these are the constraints and the objective function. So you take the polymer f, you evaluate it as the matrix tuple x. So you get a matrix, then you take the trace and you normalize by the dimension, because now the dimension is free in this program. So you have to normalize in order so that it makes sense. So you take the minimum of the normalized trace of f evaluated at the tuple of matrices x. So here in this program, I have a matrices of arbitrary sizes.
00:14:26.348 - 00:15:14.252, Speaker B: So it makes sense to look at the analogous program where instead of having matrices of arbitrary size, you would have operators in possibly infinite dimensional Hilbert space. So that's the next program. So instead of having matrices, now we take sister algebra equipped with a trace. So the trace, just think, is just the abstract analog of the matrix trace. But to normalize one tau of one is one and the variables x one xn are just elements of the algebra, which when you evaluate the polynomial at them, you get a positive element in the algebra. And now the objective you want to do is to minimize tau of f of x. Okay, so this is a commutative infimum.
00:15:14.252 - 00:15:56.804, Speaker B: This is a non commutative infimum with matrices. So like finite dimensional, and this is the infinite dimensional infibop. So this is a clear list of inequalities. So what is known, it is known how to get SDP lower bounds for these problems. So in the commutative setting, you would minimize. So this is a moment formulation of the problem which would what you do is you try to find a linear function acting on the space of polynomials up to a certain degree, and you put some constraints on l. And the first constraint you would put is that l of one is one and you want to minimize l.
00:15:56.804 - 00:17:09.214, Speaker B: And on the certain archimedean condition, it's known that these bounds which you get, let's call them ft, they converge to the infimum here. If you want to attack this problem here, what you do, you do the same, but now you take a linear function which is acting on the space of non commutative polynomials with a degree bound. And what do we know about the convergence? So the bounds, the SDP bounds are now going to converge to the infinity. So you end up here, this is what you want to find, but this is where you end up. And how do you in order to get to that one finite version? So there is this result which is if you have the SDP bound of order t, which admits an optimal solution, which is flat, which means it satisfies certain stability condition of the rank. Then you can claim that actually the SDP bound gives you the true optimum in the commutative case and the true optimum in the non commutative case. So these are in a very quick nutshell, so results which we are going to use in our setting.
00:17:09.214 - 00:18:00.580, Speaker B: So to lower bounds, what we want to do is to get lower bounds for the matrix factorization ranks, we are going to adapt the cis framework. We are going again to have a linear functional which acts on polynomials is on commutative or non commutative. But now the difference is that before it was normalized, but now we don't normalize so the crucial point is that l, if you look at the L, you are going to plug in an identity. So this is the trace of identity, which is exactly the dimension. So this is why we are going to minimize L subject to a bunch of constraints. Okay, so let's do it for the CPSD rank. So the goal is we have a matrix and we would like to find lower bounds on the CPSD rank.
00:18:00.580 - 00:18:42.264, Speaker B: How to do that? So by definition a has a gram factorization by PSD matrices xi and say they are d by d and d is a CPSD rank. So what we do, we want to find one of, we want to define a linear functional acting on the ring of non computative polynomials. And the way to do that is we take the trace evaluation at x, which simply means that at the polynomial p, you just evaluate p at x and then you take the trace. So that's the trace evaluation. And so our x are hermitian PSD. So this could be a complex number. So then you just take the real part of it.
00:18:42.264 - 00:19:30.238, Speaker B: And now let's see what are the properties which are satisfied by this linear functional. So first of all, l, if you look at L, it is a trace of the identity. So that's the size, that's the parameter. We are looking after this D second property. If you evaluate L at xix, you get the trace of x I xj, which is by definition entry aij. So this recaps your matrix a and now let's write up a bunch of properties that L is satisfying, it's symmetric, it is threshold. So this comes from the fact that the trace has this property, trace of Ab is equal to trace of Ba and it's positive, which means that if you evaluate it at a hermitian square, you get a non negative number.
00:19:30.238 - 00:20:12.140, Speaker B: And this is because the trace of a PSD matrix is a non negative number and you can add more positivity constraints. You want to have your feasibility region as small as possible. And here is a positive constraint you can put so what does it say? It says that if I here would put my matrix xi. So this matrix here is going to be PST, because AII is a trace of xi square. So and the trace is bigger than the largest eigenvalue. So this is why this matrix evaluated at Xi. So you get a positive semi definite matrix.
00:20:12.140 - 00:20:57.972, Speaker B: So this is PSD, so the trace is non negative. So this is a non negativity condition which you are allowed to impose. So in short phrase, these two conditions are saying that l is non negative on the quadratic module generated by this localizing polynomial. And you want to have such polynomials because they are going to buy you the archimedean condition which you need in order to have to be able to show convergence results. And we get lower bounds now by minimizing l over the set of linear functionals which satisfy these conditions. Now you forget zero, but you can put this condition. So let's give a name to this bounds.
00:20:57.972 - 00:21:37.598, Speaker B: Let's call it ct. So for any integer t you get the bound, or you could also take t is infinity. Then it means that you have your l which is acting on the full polynomial ring. And we would remember we want actually to find matrices at the end of the day. So let's call c star, which is a parameter which you get when you add up some finiteness condition. So from l you can make the moment matrix and you could ask that the rank of this moment matrix as finite rank. We are going to see more equivalent characterization for these parameters.
00:21:37.598 - 00:22:49.834, Speaker B: So we get a hierarchy of bounds. As t grows, you get a better beta bound c infinity, the finite reversion c star, and all of these are at most CPSD rank by construction. Ok, what can we say about the convergence for first of all, the bounds converge to the infinity parameter and this infinity parameter? Actually we can reformulate it in this way in terms of sister algebras. So here it's writing the minimum alpha for which one over alpha a admits a gram factorization in some sister algebra. This is what this is saying. And if you would here take your sister algebra which is finite dimensional, then you get the finite version of the parameter, equivalently this finite version of the parameter, that's also the minimum of l of one, where your l actually can be written as a conic combination of trace evaluations at x satisfying the localizing conditions. So here we see that the CPSD rank is having a gram factorization.
00:22:49.834 - 00:23:21.184, Speaker B: So it corresponds to being just a trace evaluation of an x. And here this is a conic combination of trace evaluation. So this is a convex versions of this. So all these are convex programs, and so this is convex. And this is why it's difficult to go to this non convex parameter here. Okay. And if you have a flat optimal solution, then the bounds, you get to the finite reversion here, but you stay below this.
00:23:21.184 - 00:24:13.950, Speaker B: So the bounds here I have given you some basic conditions which you can put into bounds, but you can add more constraints. Maybe we don't in trust of time, I don't give the details. So I have explained how to define the bound for the CPSD rank, and the same definition just works for the other metrics, factorization ranks. So for the rank plus and the PSD rank, it's an asymmetric setting. You just use two sets of variables for the rows and for the columns, or it's a partial setting if you prefer, and for non negative rank and CP rank. So factors are non negative vectors, but you can view them as diagonal PSD matrices. And diagonal PSD matrix is pairwise commute.
00:24:13.950 - 00:25:03.916, Speaker B: So this is a commutative setting. So let's show how the bounds for the CP rank would relate to the bounds of Hamza and Pablo. So in the case of the CP rank. So it's exactly the same definition as what we have seen before. But because we are now dealing with matrices which are diagonal and pairwise commute, we can put more localizing constraints, because Xix, it's again a diagonal matrix, so it's PSD, and we can put more localizing constraints. We can get richer bounds. We can also add additional constraints which are some positivity constraints.
00:25:03.916 - 00:26:04.308, Speaker B: So if you take a monomial ultra times a localizing polynomial, and you evaluate at l, so you get the trace of a diagonal matrix which is PSD, so you get a non negative number. So we can exploit the fact that you are now evaluating a diagonal matrices in order to put additional constraints. And there is an additional constraint which is inspired by the work of Pablo and Hamsa, which is a tensor constraint which you can put okay, so how does the bounds relate? So we get a hierarchy of bounds which are all upper bounded by this parameter tau CP. And how to get tau CP actually to get tau CP, you get it. If you add these, strengthening these two additional constraints and the finite reversion of the bound gives you exactly the Tau CP. And the level two bound is at least as good as the Tau so's. So this is one way of getting the Tau CP.
00:26:04.308 - 00:27:02.724, Speaker B: Another way of getting it would be instead of taking these two constraints, you would add additional localizing constraints and possibly an infinite number of localizing constraints, some additional constraints, so you could get it as the asymptotic limit, actually of a sequence of scps. So for the non negative rank and for the PSD rank. So this is the asymmetric setting. So now we don't have any more diagonal entries. So it's not so clear to how we are going to put the localizing constraints in order to get the Archimedean condition. But so what we can do now is to rescale the factor and in this way we are still able to put some localizing constraints with ensure the archimedean condition. And in this case you get immediately the tau plus, you get it as the infinite parameter.
00:27:02.724 - 00:27:42.552, Speaker B: Okay, so let me conclude on showing how the bounds compare on two small examples. So the first example is here. It's a geometric example. You take a blue rectangle contained in the red box. And so the blue rectangle is parameterized by two parameters a and b. And you want to decide when is it possible to put a triangle nested between the rectangle, the inner rectangle and the outside box. And this can be decided by looking at the slack matrix of this pair of nested rectangle.
00:27:42.552 - 00:28:17.870, Speaker B: And there is a triangle which is nested between the blue rectangle in the outside box precisely when the rank of this slack matrix is equal to three and the rank is equal to three. So this is known, there is a closed form formula. This happens precisely when this quadratic equation holds. So on the picture, this is the region where this happens. So in the white region, the non negative rank is equal to three. In the colored region the non negative rank is equal, equal to four. And then it's a nice playground to see how the bounds are going to.
00:28:17.870 - 00:28:52.994, Speaker B: Are the bounds strong enough to give you a certificate that the non negative rank is equal to four, strictly more than three? So the color code is as follows. Here this is the bound of order one. So here in this black region, the bound of order one is already strictly bigger than three. These two regions that the tau, it's written here in these two regions, this is a tau so's plus, which is strictly bigger than three. The next one is yellow. So this is the bound of order two. And here this is the bound of order three.
00:28:52.994 - 00:29:26.812, Speaker B: So in this full region, the bounds up to order three permits to certify that. So it shows how they become stronger and stronger as you increase the order. And for the PSD rank, I'm finished almost for the PSD rank. So here's a very small example, but just as a matter of testing how the bounds work. So this is a three by three circulant matrix. The PSD rank is at most two precisely when this in this quadratic equation holds. So this is inside this region.
00:29:26.812 - 00:29:54.636, Speaker B: So in the white region. And if you compute the bound of order two, it satisfies already that the PSD rank is strictly more than two in the yellow region. So there's only the blue region where it's not certified. I think it's telling me to stop speaker to stop. So I will stop. Yeah, I think I should stop. Yeah.
00:29:54.636 - 00:29:55.904, Speaker B: Thank you very much.
00:30:09.614 - 00:30:11.270, Speaker C: Is there an economical way to add.
00:30:11.302 - 00:30:12.674, Speaker B: These constraints between.
00:30:15.174 - 00:30:26.974, Speaker C: Localizing the square root of aii Xi minus, for example, the CPSD rank? What's, I mean, this is one concern that you can. What's the canonical way? Somehow.
00:30:30.154 - 00:30:54.002, Speaker B: They were there. I couldn't explain them, but there are these constraints. What you can do is you take a linear combination of your matrices, the square, and so let's take a vector v, so v av minus this linear sum of vix I squared. So this is a big class of. Yeah.
00:30:54.098 - 00:30:58.170, Speaker C: Is there a way to choose? So you chose a particular choice of.
00:30:58.202 - 00:31:06.814, Speaker B: V. So this is. No, it's difficult to know ahead. What are the constraints which are going to work? No, this is more a trial. And.
00:31:10.354 - 00:31:19.550, Speaker A: So, in the picture, in the color coded pictures, it looked like some of the sets were not convex. Oh, are, some of them are shuttered.
00:31:19.622 - 00:31:20.754, Speaker C: Do they have to do.
00:31:23.174 - 00:31:41.514, Speaker B: It's different. This is different. These are the a's and B's for which the non negative rank for is a parameter. So this doesn't have to be convex. This is not the feasible region of the SDPR. It's something else. It.
