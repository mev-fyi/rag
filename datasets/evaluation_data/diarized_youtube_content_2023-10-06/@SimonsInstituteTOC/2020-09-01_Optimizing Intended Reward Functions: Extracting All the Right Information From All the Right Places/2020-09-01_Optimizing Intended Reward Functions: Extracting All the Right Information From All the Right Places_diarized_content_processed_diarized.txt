00:00:00.120 - 00:00:20.634, Speaker A: Okay, so let me make an announcement on this card. Oh, someone's asking for the zoom link. Yeah, I don't know. I couldn't pin it, but I see that they do have the zoom link. Probably they just want to pin it. Yes, I wanted to pin it, but I. You want to give introductions?
00:00:21.374 - 00:01:08.442, Speaker B: Sure, I'd be delighted to. So we're really excited to have Ankar Dragon come give our first sort of invited outside talk for our RL Simons sort of semester kickoff. Anka's done a lot of really amazing work in human robot interaction, thinking about different forms of trying to get the reward model correct. And she's won a slew of rewards, including the NSF career, a p case Sloan, a huge number of things, have acknowledged her really amazing research accomplishments, and her graduates have already went on to join a number of wonderful places, including Dorset Sudig, who's a fellow faculty at Stanford. So I'm super excited to hear about her work today on optimizing intended reward functions. Thank you, Anka.
00:01:08.578 - 00:02:20.048, Speaker C: Thank you so much, Emma. Yeah, the joy of being at Berkeley is working with these amazing students. And actually today I'll talk about kind of a piece through a lot of people's work. But I'd say that one of the major influencers of this work is Dylan, had been Menel, who is also moving on to a faculty position at MIT. Super proud of Dylan, so I wanted to brag a little bit about him as well. So I figured given that this is DRL bootcamp, it makes a lot of sense to talk about a topic that's not only one of my favorite topics that I've gotten really obsessed by in the past four years, but also a topic that's basically kind of challenging the very definition of reinforcement learning as a problem that this camp is all about and replacing it maybe with a much more difficult definition that is maybe also still work in progress in terms of the definition itself, but definitely work in progress in terms of how we might able to solve that more complex problem. So that's a little bit of what the talk is about.
00:02:20.048 - 00:03:12.868, Speaker C: Now, my background is in robotics and I work on different kinds of robots. I work on robot arms. I work on autonomous cars and quadrotors and indoor mobile robots. And traditionally, my work is not so much about reward functions, reward modeling. Traditionally, my work was a lot, and still is a lot on how to get these robots to achieve their objectives when they're not alone in the world, when they need to be coordinating and collaborating with people as part of what they're doing so, that's what I do. But then it turns out that a major bottleneck in terms of being able to do that well is defining what their objective is in the first place. So let me give you one of my favorite examples in this.
00:03:12.868 - 00:04:00.208, Speaker C: So, when I got to Berkeley, I started working with Dorsa. Dorsa got me into autonomous cars. And the first thing that I noticed was that cars tend to be very conservative. So this orange car here needs to. Is an autonomous car, and it needs to get into the left leg and kind of that in order to take a left turn or something like that. And so we can write down a reward function that says maximize efficiency, avoid collision, stay on the road, etcetera. And if we do that, the first thing you'll notice is that the car won't actually do very well, because essentially what it will do is it will predict that the human driver in the leftmost lane will plan on keeping on going through in their lane.
00:04:00.208 - 00:04:40.798, Speaker C: And so the autonomous car will have to slow down and merge behind them in order to avoid collisions with them. That will be the optimal thing to do. And the reason that's not great is because if you have a lot of traffic coming in into that left lane now, you're screwed. Because your options are you either get stuck and wait for a big enough gap in traffic. Maybe that happens in 30 seconds, maybe in five minutes, who knows? Or you keep going and then you miss your lane. And depending on how you tune the trade offs and the reward function, one of these two things will happen. Those are basically the only two staying options you have at your disposal, um, because you won't go for something that leads to collision.
00:04:40.798 - 00:05:34.378, Speaker C: And, um, and if a human is presented with these two options, what they do is they say, screw it, and invent a third option. And that third option is to kind of nudge yourself in there. Despite your prediction of what this other driver, uh, in your left lane is going to try to do, you nudge yourself in there because you know that the other person will probably slow down. And it makes. And so we worked hard on giving the autonomous cars, and maybe robots more generally, this understanding that they get to influence what other humans do. And we thought about this as an underactuated dynamical system, where essentially, as a robot, you have actions that don't just influence your state, they also influence the actions that a human ends up taking. You don't have direct access to those.
00:05:34.378 - 00:06:32.974, Speaker C: You can't control those degrees of freedom, but you influence them through your actions. And so this kind of modeling of the human robot interaction problem essentially fixed a lot of these challenges of conservativeness that we were seeing. But there's a flip side, and the flip side is that now, if the car can gain a tiny little bit of efficiency, it has no problems cutting the other person off and making them decelerate. 5 meters/second squared, 7 meters/second squared doesn't matter as long as you can rely on them to decelerate. So as long as it's safe, no problem essentially being kind of a jerk to other drivers. And so this exposed to us something about the reward function, which is that, oh, wait, we can't just incentivize the car to be safe and efficient. It needs to fundamentally also care about the other people around.
00:06:32.974 - 00:07:04.714, Speaker C: In particular, it needs to care about their efficiency as well, so that you don't end up with autonomous cars that are jerks to other people. And so we revisited the reward function. We added a courtesy term, so far so good, improve things in these kind of highway scenarios. But then at an intersection, so this is a four way stop. Imagine the autonomous car pulls over at the same time as the human driven vehicle. And then what does the car do that optimizes that new reward function? Well, it doesn't just wait for you. It doesn't go.
00:07:04.714 - 00:07:54.700, Speaker C: It doesn't wait for you to go, what it decides to do. So the optimal behavior here seems to be to back off away from the intersection, because that incentivizes the person in the white vehicle to go through faster through the intersection, because now the autonomous car is not a potential obstacle that they need to avoid. And that's great for the courtesy term. It remarkably actually works. In user studies, people go faster to the intersection. So that's not really the problem. The problem with this being the optimal behavior is that if there's someone else in the back or if you have a passenger in your car, now, that passenger is really anxious, thinking, why the heck is my car backing up? Right? And so, even with the addition of courtesy, you don't, you're not there.
00:07:54.700 - 00:08:56.366, Speaker C: Your reward still is missing important aspects like passenger comfort or legibility to other drivers and so on and so forth. And it feels like this endless list. And so we're getting good at taking task specifications in the context of RL reward functions, and then use planning, use reinforcement learning, depending on what you know, what you don't know about the problem to turn that into optimal behavior. But what we often sleep kind of under the rug is that it's really hard to write down the correct reward functions. It's really hard to specify what you want these robots to do, because how do you capture comfort in an autonomous car? How do you write that down? And how do you trade off between safety and not crossing the double yellow line and efficiency? Right. What's kind of the relative importance of these things? How do they combine? So all of these are really hard questions, and these are examples from autonomous driving. I'll also share one example that I really like.
00:08:56.366 - 00:09:41.040, Speaker C: This comes from OpenAI. They did some work on this game. It's called coast runners or coastal runners. And basically there's this boat in white that is trying to race in a game, and it's not doing a great job. So here's the actual course. And you see our boat doing this weird loop while everyone else is actually racing and what's going on. So this is trained to deep reinforcement learning, and it's not a bug with the deep reinforcement learning that this policy is actually doing a great job, because what it's doing is it's found this loop and it timed it in such a way that it just gets these turbo boosters whenever they show up.
00:09:41.040 - 00:10:25.792, Speaker C: And each turbo booster gives it a bunch of points. And the reward function that was specified here was points in the game. And so the boat is like, well, you know, why bother trying to race and trying to win? That's all really difficult. Let me just do this loopity thing and get a lot of points that way. And, you know, that's not what we intended when we said points in the game, but that's what this level, in this particular level of coastal runners, that's what that optimizing that leads to. And so specifying rewards is hard. You end up with the behavior kind of taking you by surprise, sometimes negatively, in new situations.
00:10:25.792 - 00:10:53.484, Speaker C: And I think overall, the problem stems from the fact that the problem definition is wrong. This is what I was trying to hint at in the beginning. So we pretend like the AI problem definition is you have an agent. There's a state space. The agent can take actions that affect the state. There's some reward function that falls from the sky. And then the problem definition is find a policy that optimizes that reward function cumulatively in expectation.
00:10:53.484 - 00:11:50.312, Speaker C: And we define it this way because, I mean, it makes sense. You have to make progress on something, and this is a concrete problem that you can actually make progress on, but that's not where the real problem is, right? So let's be clear about that. The real problem is that there's an agent, there's state their actions, but there's a human, and the human wants something. And what the agent is supposed to do is whatever the human wants the agent to do. And so I think that's the problem that we should be going for, that it's not about optimizing a specified reward, it should be about optimizing intended reward, um, where intended means whatever a person actually wants you to optimize at some implicit level. And so that's what we should try to build these robots to do. But that's hard because, you know, what the heck is intended reward? So what does it mean to optimize intended reward? Let's make this a little bit more concrete.
00:11:50.312 - 00:12:33.670, Speaker C: So what I did here is I decided on a parameterization of the reward function. So we're going to choose some space. It could be a linear combination of features. So these thetas would then be weights on those important features telling us how to trade off. It could be a deep neural network, right? That take the weights in a deep neural network that takes state and action as input and then outputs a scalar as the output, whatever it is, right? We choose this space, we better hope it's expressive enough to capture what we actually want. And then the idea here is that these parameters theta, they're hidden state, so they're sort of implicitly internal to the human. The robot doesn't get to just access them.
00:12:33.670 - 00:13:58.950, Speaker C: And so what the robot needs to do in an ideal world, and of course then practical consideration around compute come into play. But an idea of what the robot needs to do is to maintain an estimate of these parameters, essentially have a belief, a probability distribution over what these reward parameters might actually be, and try to estimate somehow, right? Try to have updates on this belief with evidence. And so if you think about that problem definition, then it sort of makes what we currently do in reinforcement learning look a little bit subpar, because what we currently do in reinforcement learning is the person, the engineer, the designer attempts to specify a reward function like I was doing with the cars, like OpenAI engineers were doing with the boat. And let's call this theta tilde. And this is what's important to note is that this is someone's best attempt, best current attempt, maybe at a reward function, their best guess at what should actually incentivize good behavior. But note that this is not necessarily one and the same as the true parameters, data that would actually describe the reward function that leads to the robot doing the correct thing across time in every new situation. So we're going to call this data tilde.
00:13:58.950 - 00:14:30.994, Speaker C: And so what, what we currently do in RL is someone specifies a theta tilde and the robot basically updates its belief automatically to, well, that must be the definition of the reward function. So in other words, my belief automatically becomes one if theta equals theta tilde, zero otherwise. So that's just, that's, you know, that's ground truth. That's it. It basically treats the person's specification as if they're some sort of perfect oracle, right. A perfect sensor that just elucidates the reward. Exactly.
00:14:30.994 - 00:15:10.800, Speaker C: And we've seen a number of examples now where that's not actually true, where Theta Tilde is not what we actually want and you kind of keep on having to modify it as you encounter new and new situations. And on top of that, after we deployed the robot, that's it. That's what, you know, that the belief has been updated to this, you know, one on theta tilde, zero otherwise. And that's it. That's what the robot optimizes for. So here's a robot that's programmed to trade off efficiency with maximizing these distances from obstacles like the table, the floor and so on, and for collision avoidance purposes. And just, it's in my olap.
00:15:10.800 - 00:16:12.286, Speaker C: But imagine that it's in my home and it's helping me. I can't load the dishwasher and I see it moving and it's carrying this mug that it has in its, in its end effector. And I get worried because it's keeping that pretty high upright and I worry that if it drops it, it will break. And so what I might do as the end user, as the user of this device is I might see that behavior and intervene to fix it. So I might actually push the robot closer to the table, closer to the ground to correct its behavior. And now basically that the fact that I did that should tell the robot that something's up. The fact that I did that should tell the robot, look, if what I was optimizing for was the correct thing to optimize for, the person wouldn't have needed to intervene.
00:16:12.286 - 00:17:33.403, Speaker C: The fact that the person intervened means that something's up, right? But the moment I set the reward to be set in stone and basically all the probability mass on one particular reward function, we're depriving the robot of that ability, right? The robot is just going to take this as some sort of disturbance that's in the way and it's just going to say, oh my God, I'm so glad the disturbance is over and I get to go back to doing the task in the way that I think is optimal. So that's a problem. So I think that optimizing specified word makes a lot of sense. But now, from this lens of needing to optimize intended reward instead of the specified reward, essentially what we're finding is that robots tend to learn too little from everything that happens to them, like people saying stuff to them, people correcting them, and so on, and learn too much overlearn, I'd say, from the reward that was specified. So we're focusing on one source of information, but I think there's a lot of other information that we're leaving on the table, like this robot that I was pushing on was doing. And so it turns out that we leave more information on the table than we might think. So people might physically interact with the robot.
00:17:33.403 - 00:18:20.450, Speaker C: They might say stuff in natural language. They might. They might sort of intervene and stop the robot from whatever it's doing. So, like, if the robot is about to go over a bottle of water and you switch it off in a panic to protect it, right? Well, this is something that you do because of what's about to happen right now. But I'd argue that it's also very useful information about the reward, because whatever the robot was about to do, it was so bad that even just being stopped is probably better. And so from that, you should have a better idea of what you should do in general, because you have a better idea of what the reward is. So maybe my favorite.
00:18:20.450 - 00:19:33.658, Speaker C: So there's all these examples of, what am I called? Information that robots are not really capitalizing on right now. My favorite one is this one. So imagine that someone asks you to clean up a room, and you enter the room, and you see this beautiful house of cards assembled in the room. Now, I bet that even though the person didn't spell out, oh, but don't clean up my house of cards, each and every one of us would basically kind of read between the lines and leave the house of cards there, right? No one would destroy the house of cards and clean up. And so somehow, the very existence of the house of cards is evidence that the human cares about it and doesn't want to destroy. So I'd say that to what we do, to what we say, and even how we set up the environment, we leak information about what we want, and therefore, we leak information about what the robot should optimize for. And I think this information can fill in the blanks of what we forgot to specify ahead of time or sort of things that we miss specify ahead of time.
00:19:33.658 - 00:21:39.428, Speaker C: And so the question is how do we enable robots to extract this information, to capitalize it towards updating its belief and being able to do better in the future on new tasks? So in other words, how do we go from observing the feedback that the oldest information that the person is giving us and turning that into a belief update? And what's hard about this is that we need to arm the robot with a human model, an observation model, this thing that tells us, given the reward function that they actually want, implicitly what feedback, what behavior from the human would we expect, what environments should look like, and so on and so forth. We need a model that works for all this diversity of human feedback types. So in other words, kind of on the same page about the problem set up here of wanting to create such an observation model that can take in all of these different sources of information and leverage them to update, to update the robot's understanding about the belief. What I want to do for the rest of our time together is first talk about how we might do this for the very special case of a specified reward. So if the person actually specifies a reward, how we might build an observation model for that. And then I want to generalize to all these other different types. Okay, so when you specify a reward for your robot, what does it mean for the reward you specify? To not define the actual reward you want, but to be merely kind of evidence observation about the reward you actually want the robot to optimize, right? How do we mathematically capture this process that makes you as a designer, as an engineer, not actually end up with the right theta parameters? Why would you not end up with that? How come there's this disconnect between theta Tilde, what you specify and what you want? And it's not that, you know, you take your true data and add some gaussian noise around it.
00:21:39.428 - 00:22:47.030, Speaker C: Like we can't just build an observation model, right? That by slapping some gaussian noise around theta Tilde around Theta and say, okay, oh, that's the distribution data tilde is drawn from. There's, there must be something going wrong, right, for us to actually end up with the wrong. So we thought about this for a while, and what we realized was that at the core of what's going wrong is that you as a human designer, you can't possibly look at every possible environment and make sure that the reward you specify ends up working well everywhere in every single possible environment that the robot encounters. So what drives the sub optimality? What drives this difference between what you specify and what you actually want is that you only look at some developers. I'm going to call it a development set in kind of industry terms, or the training set of environments. And you decide data Tilde based on that. And so usually what goes on when you get surprised by this behavior is some fundamental difference between kind of what we might call the development test set and the deployment set, right, where you actually deploy the robot.
00:22:47.030 - 00:23:34.814, Speaker C: So we're not gonna worry about bad behavior during development because during the varmint you're actually tuning up the reward function and you could still get a chance to catch that and fix it. We're going to worry about when we see bad behaviors at deployment time. So when everything works well during development and you produce the data tilde, and then at deployment time the robot chooses the wrong behavior. Like in the boat case, for some reason, the boat encounters this level where it can win with a lot of points, but it decides instead to kind of spin in circles and get even more points that way. Right. And why does it do that? Because the designer specified score number of points as the reward function. And so the idea here is that we don't just write down a cost function and deploy the robot, we tune.
00:23:34.814 - 00:24:29.534, Speaker C: These costs are these rewards on a development set. So score as a reward function must have worked on the development set, right? So the development set must have been these levels in the boat game where you could win with many points or you could get fewer points, but then you would lose. So in other words, score and winning were correlated at development time, but that correlation was broken at deployment time, and that's kind of what went wrong. So the key idea here is what we're going to say is that the specified reward, okay, it's not the same as the true reward. So we should not expect that the behavior induces everywhere to be the right behavior. But we do know for sure where it does induce good behavior. It induces good behavior on the environments that the designer used during development time to tune it and test the reward function.
00:24:29.534 - 00:25:16.450, Speaker C: And whatever the designer actually landed on as the specified reward, the behavior, it incentivizes on those development environments or development scenarios. That's good behavior, meaning it has high true reward. So that's what we're going to assume. That's, we're going to say the disconnect between specified and true reward really comes from this difference between development and deployment time. And what we're going to, based on observation model on, is that the specified reward only works well on, we only know it to work well on the development set and not everyone. Maybe another way to say this is that what we specify is, comes in a context and it's important for the robots to not interpret it literally, but to interpret it as part of that context. And the context here is this development set that we're working with.
00:25:16.450 - 00:26:17.966, Speaker C: That's our context that we're drawing on to define the reward function. So mathematically the probability of me writing down theta tilde does not just depend on what Theta star. The thing that I want is it also depends on the development environments. And so it depends basically if you look at what behavior theta tilde incentivizes in the development environments, then the lower the cost or the higher the reward of that behavior is with respect to theta star, the more likely I am to write down that theta tilde as the reward function. So kind of to get more intuition of this, basically here's what this observation model does. What it does is it says, here's a plot where on the x axis we have all trajectories and the y axis we have the reward, the negative reward, lower is better, the negative reward on the development set. And this is going to be respect to different data.
00:26:17.966 - 00:26:52.464, Speaker C: So there's Theta star. That's what I actually say one, but of course I don't know that. It's only implicit in my head. Here's another set of theta parameters. I'm not going to specify that because if I optimize that, that leads to a trajectory that does pretty poorly with respect to Theta star, I'm going to catch that. That's not going to be what I end up specifying to the robot. But there's all these other reward functions that end up producing trajectories on Mdevel that Theta star likes, that it did have good reward respect to R Theta star.
00:26:52.464 - 00:27:55.614, Speaker C: And then the problem comes when that breaks at deployment time. The problem comes when in the deployment environments all those different reward functions no longer agree on what's good and what's bad. In particular, they no longer agree with Theta Star. And so what this model tells us is that it kind of cautions the robot that if Mdeval was the environment that these were based on or the set of environments, it cautions the robot that any of those green, green, possible cost functions, reward functions are possible, are things that the designer might specify if they actually want data star because there's not a good way for them to disambiguate between them. So kind of back to the boat real quick here. Behaviors in Mdevel and kind of just for the purpose of understanding this, let's say that the robot kind of magically has access to these many different features that might matter, like winning, like score, whatever it is. And so we told the robot to maximize score, we chose theta two.
00:27:55.614 - 00:28:22.198, Speaker C: And now instead of just optimizing theta two everywhere, what the robot does is it says, okay, let me look at what behavior disincentivizes, what policy policies come out of this in the development set. Well, in the development set, I end up getting a lot of points if I do this. I also, by the way, end up winning. Now, let me look at which of these reward functions in my hypothesis space. Like that behavior. Well, of course, maximizing score likes that behavior. Kind of duh.
00:28:22.198 - 00:28:51.490, Speaker C: That's what I optimized to obtain it. Minimizing winning, minimizing score. Do not like that behavior. And so that's how the robot knows that the person says maximize score, and it trusts that the person didn't mean don't win or don't optimize score, minimize score. Right. But what the robot doesn't trust is that the person could have really disambiguated between winning and score because they both like the same behavior in there. And so now the robot has this uncertainty.
00:28:51.490 - 00:29:25.528, Speaker C: It's not that the robot. I want to pause here and emphasize something. It's not that if you take Theta Tilde as evidence about Theta star, all of a sudden the robot knows that the person wants the robot to win and not do an optimized score. That's not what's happening at all. But what's happening that I think is really important is that the robot gets this uncertainty where it trusts some things that the person said, but it. But it knows that it doesn't know whether the person actually wants score, like they literally said, or maybe they actually want winning. And this uncertainty is really valuable.
00:29:25.528 - 00:29:56.500, Speaker C: So we do this belief update, we get this uncertainty. And now what we can do is we can plan an expectation, which is kind of hedging our bets a little bit between the different options. We can also do risk averse planning. So kind of protect against the kind of the worst case, some notion of worst case reward function that's still high likelihood. We try this, by the way, in motion planning for arms. We do a lot of motion planning kind of around people. And what's hard with even this problem is that there are these different aspects of the task that are important.
00:29:56.500 - 00:30:46.628, Speaker C: So these aspects are like avoid collisions, stay far enough away from fragile objects like the vase, but also have a comfortable distance away from the person. And, you know, by the way the person's torso different from the person's head and so on. Right. How do you prioritize all these different things? How do you mesh them together? And as designers, we kind of maybe implicitly know what we would want the behavior to look like, but we have a hard time sort of spelling out the weights and a cost function that correspond to that. And so what we do is we tune it on certain environments, and, you know, we look at the resulting trajectories, and everything's great. And then here's a new environment where we tried a reward function that worked well on a lot of test development environments, and here it just decides that the optimal thing to do is, like, to plow through the base. Why? I have no idea why.
00:30:46.628 - 00:31:22.314, Speaker C: I mean, could we fix this with hard constraints? Yes, but that's not the point. The point is that there's always things that come at attention, and you always tune them for, and they work across several things, and then they break. And I don't know that this is different from the training environments or anything like that. I couldn't tell you. There's no, like, features that are no longer correlated. There's something going on, but I don't know what's going on qualitatively. And so what the robot does here is takes that reward that I specified on those training environments as evidence does.
00:31:22.314 - 00:32:08.328, Speaker C: This whole update that we were talking about obtains a posterior, now has this uncertainty, and then plans an expectation over that, and it ends up not actually colliding with stuff and doing a kind of a better job keeping maintaining these trade offs that we'd like. So this technique leads to better behavior with less time being put in, into designing the cost function. It's what we found. We also did some. These are results from some user studies with people actually trying to define these costs functions. Now, what's kind of not so great about this? So, of course, a big limitation of everything that I've talked so far is that we get convenient access to the right features. Like with the boat in the toy example that I gave winning, we magically knew that that's a thing.
00:32:08.328 - 00:32:58.930, Speaker C: Or with the motion planning, we know that these distances are important, right. We just don't know how to kind of weight them, prioritize them, but we know that they're important. And so we had some experimentation, but that's still ongoing, and it turns out it's pretty hard to do, but we had some experimentation with what if you don't get access to these features? And we looked at this very simple. This was kind of, our initial experimentation was in this little toy environment called lava land, where basically you have these different types of cells, and you don't get to know what the type of cell is. You get some sort of noisy measurements that relate to the type of cell. So you get these, we call these raw observations to stimulate, like, what would happen if you had cameras looking at the world. But it's a much more simplified example.
00:32:58.930 - 00:33:41.730, Speaker C: And then the development set. So the terrain is like grass, dirt, target, and then there's lava, but the training, the development environments have no lava. And so you end up, as a programmer, you end up designing a reward function by kind of training these classifiers to detect grass, to detect dirt. And then you put in trade offs between these things, and that's how you specify the reward. And so you don't build a classifier for lava because there's no lava. And you don't think that I'm going to ship the robot to Hawaii so that it needs to encounter lava. And now when you actually do inference using that specified reward, turn that into a posterior, and we do MCMC sampling for that because the space is pretty high dimensional.
00:33:41.730 - 00:34:29.703, Speaker C: When you turn that into a posterior, and then you do risk of responding with respect to that. Kind of, amazingly enough, that leads to the robot avoiding lava at test time. And so kind of implicitly, even though the robot doesn't have a notion of lava implicitly in the space of reward functions that it's maintaining a belief over, there are going to be reward functions that like lava and don't like lava at some implicit level, and the robot realizes that it has uncertainty over that, and then that turns into the risk avoiding behavior, being to avoid the lava. So I thought that that was really promising. What really bugged me about this, though, is that if lava were a pot of gold, then the robot would also avoid the pot of gold. Right. Because that's also new, and that's also not something that I took, talked about.
00:34:29.703 - 00:36:02.534, Speaker C: And so where we've been moving with this line of work has been to essentially, well, I should say that, okay, so here's kind of a real example where the pot of gold thing happens. So autonomous driving, right? We take in these development environments, we specify reward function, the robot does the posterior update, and guess what? Everything sucks now because there's so much uncertainty about what might be good, what might be bad, that you just can't make progress, you end up being really, really conservative. And so that's a problem. And so what we've been trying to do now is leverage that posterior to go back to the person and seek an environment, kind of design an environment for them, which means kind of initial configuration of all these different things that could be in a scene that's kind of as much as possible an edge case in which sort of the, what they kind of specified so far breaks. And our version of that was find an environment that maximizes expected info gain. And what that turns into is proposing to the person environments where essentially the current reward function has high regret with respect to the true reward function. So basically, because they're environments in which the rewards sort of tend to contradict with each other, what happens is a lot of these environments end up being things where the behavior that you would normally see would not be the right one.
00:36:02.534 - 00:36:36.714, Speaker C: So that's pretty cool. And then that leads to actually being able to, I'll skip this, but actually perform better over time with fewer environments. So that I think the active part is really important. It's this notion of exposing the person to edge cases so that you can actually make progress on what you don't know. So step one was know what you don't know. Step two was actually make progress on resolving what you don't know. And I think though, the thing that's kind of bottlenecking all of this is how you decide to parameterize the reward function.
00:36:36.714 - 00:37:18.004, Speaker C: So at the very beginning of the talk, when I said our real problem is optimize intended reward, the next thing I said was, okay, here's how we're going to actually make progress on this. We're going to choose a way to parameterize the reward function, and we're going to say the parameters of this are unknown. And that choice I'm not sure how to really resolve because I have two options. I mean, I have a spectrum of options with two extremes. I could design the features and then I don't have a lot of expressiveness there. So I could do things that resolve, like the motion planning example that we were talking about. But it's really hard to do things that will then sort of come up with, oh, lava might be a thing like there's just no way to express that in that space.
00:37:18.004 - 00:38:03.492, Speaker C: I could use deep neural networks and then they can express all of this. But then I have to have uncertainty over neural networks and I have to work with the person and keep querying them until I resolve that ambiguity. And because everything in their mother could be a reward function, now that's very limiting, right? In terms of the number of queries that I'm going to make. This is a pretty fundamental trade off, and I have no idea how to solve this. Okay, so this was all under how much amount of time? Okay, this was all under what if the person specifies something? But I promise that we'll also try to generalize this. Right, so this is one bit of evidence that you get. It's not the definition of the reward, it's evidence.
00:38:03.492 - 00:39:01.580, Speaker C: And this is how we can actually leverage it to understand more about what the person actually wants. So what about all these other types? And let me share a little bit about two other types that we know a lot how to do. One of them is comparisons. So if the person, if you show the person two trajectories and ask them which one is better, and if they say side a, the trajectory on the left is better than side b, do that, we know how to turn that into evidence about the underlying reward function. And in particular the way we do that is through this notion that the person, we think of the person as making a rational choice with respect to the reward. So we think of the person as having these two choices, a and b, and they base the choice on the cumulative reward of one trajectory versus the other. And generally we tend to say, allow for a little bit of noise.
00:39:01.580 - 00:39:23.210, Speaker C: The person isn't perfect at making this choice. The person picks may be in proportion to the exponent of the reward. So that's one thing. Now this is this observation model we've been seeking. We know this, it's been very well studied. A second example where we know how to interpret human feedback really well is demonstrations. And so that's inverse reinforcement learning.
00:39:23.210 - 00:39:51.034, Speaker C: In particular, this would correspond to bayesian inverse reinforcement learning, where we'd say, you know what? The person has choices. Now there are just all of the possible trajectories that they could have demonstrated. In this scene. They picked one of them. So now they made an implicit choice as opposed to an explicit choice. But it's still a choice, and it's still a choice with respect to the reward function, the cumulative reward of the trajectory. So that's inverse reinforcement learning.
00:39:51.034 - 00:40:27.682, Speaker C: And again, you can make that noisy. And I want you to notice that it's the same thing that comes up where you have the probability of a trajectory being proportional to the exponent of the reward. The thing that's different now is that the person didn't make a choice between two things. So that normalizer is not over two trajectories, over all possible trajectories that the person could have demonstrated but didn't. So those are two well studied aspects. When we have our specified rewards, that's also a choice. So the choices here are all the possible reward functions that the person could have specified.
00:40:27.682 - 00:41:13.304, Speaker C: Implicitly, they made a choice, they chose one of them, they chose theta tilde, and they, the problem is, what would it mean for them to choose based on the reward? What is the reward of a, not a trajectory, but a specified reward function? Sort of. It's that you get a syntax mismatch here. And so our answer was that they choose the specified reward based on the reward of the behaviors that that specified reward induces in the development sense. Right. So it's again a choice that they're making. It's again respect to the reward function. But we just grounded the specified reward, this proxy, into behavior so that we can evaluate the reward function.
00:41:13.304 - 00:42:11.038, Speaker C: So they compared that with any other possible reward function that they could have specified and implicitly made a choice based on that. And that was that model. Good. So when, so in general, what we can think of is that the person has some choices. They're not necessarily behaviors, but they're choosing implicitly or explicitly among those choices based on a kind of a grounding of those choices into behaviors, and evaluating the reward of that grounded choice. So they're choosing, if choices are c, if they're choosing C star, they're comparing the behaviors corresponding to C star, the reward of those versus the reward of behaviors corresponding to their other choices, and then they're picking in proportion to that. And so far this is fit with comparisons, it's fit with demonstrations, and it's fit with the specified reward.
00:42:11.038 - 00:43:08.448, Speaker C: And it turns out that when you ask how you should extract this leak information from other things, the crazy thought for us is, well, maybe we can just interpret anything as a reward rational choice that the person is making, sometimes implicit. And it turns out that you can, you just have to figure out what were the choices, right? Because sometimes they're implicit, like what, what the, what they could have said but didn't, what they could have shown but didn't, and then how these all translate to behavior. So with corrections, for instance, what I was showing in the beginning that I push on the robot, well, that's an external torque that I'm applying, and you can think of it as a choice that I'm making. It's a choice between different external toys that I might be able to apply. And it's also based on the reward of the grounding of that external torque into a trajectory, which is something that we call deformed trajectories. And that leads to I push on the robot. It interprets that as a choice.
00:43:08.448 - 00:43:42.264, Speaker C: It takes it in as evidence about the reward. And lo and behold, the robot actually stays closer to the ground. Who would have thought? Good robot, less frustrating when the robot. When I turned the robot off, that's also a choice because I could have left, I could have done nothing. I could have let it run, and I didn't. And so these two choices, you know, turn the robot off, not turn the robot off. Map also to behavior, right? They mopped the robot being stopped there for time horizon versus robot continuing along what it was planning to do.
00:43:42.264 - 00:44:35.544, Speaker C: And so the person is telling us that one thing is better than the other. They're making a choice. And then finally, my favorite one is, what's up with this house of cards? How does that relate to truth? How do we get information from that? And so here's our version of this. So far, we've talked about these sources of information that look at human behavior of some shape or form. So if I'm in a room and the person didn't tell me to not break this vase that's in this room, I would have had to see the person actually actively avoid the vase for me to figure out that the vase is important. So that's kind of what inferring from behavior looks like. But if I just walk into this room and I see the vase there, and I know that the universe didn't just start this way, right? People have been in this environment.
00:44:35.544 - 00:45:28.144, Speaker C: I should be able to actually figure out that the vase is important from just the existence of that base in the environment. If I deploy the agent in an environment where the human has been acting, because the human has been acting according to their preferences, that state of the environment itself leaks information about what people want. So that's kind of the house of cards part. And so the way we've gone about interpreting this is, lo and behold, we think of it as a choice. Now, what are the possible choices? Well, they're states of the world, right? Because you get one state and not any other states. That could have been the current state of the world, but wasn't. How do you choose based on reward? How does the person choose a state? Well, the trick here was that a state of the world does correspond to trajectories, and does grounded trajectories that you can evaluate reward on.
00:45:28.144 - 00:46:22.264, Speaker C: It corresponds to trajectories that have happened, and they end at that particular state. And what the person is doing when they're making the choice of a state of the world is essentially, you can think of it as comparing between the trajectories that ended at that state that you actually see versus trajectories that did not end at that state versus all other states that then map to trajectories that end there. And so the way this works is the robot sort of stimulates the past in its head and says, well, if the person wanted to break the vase, they would have just gone through and broken the vase. If they didn't care about the vase, they would have also gone through and broken the vase because that's efficient. So, so if they didn't want to break the vase, they wanted not to break the vase, then they would have avoided it. And that's the only one that actually corresponds to the current situation that I'm seeing. And so that's how we get that kind of information.
00:46:22.924 - 00:46:26.772, Speaker A: Anka, so there are some questions in the Q and A.
00:46:26.788 - 00:46:28.144, Speaker C: Okay, yeah.
00:46:31.364 - 00:46:33.468, Speaker A: Do you want to take a look at them or.
00:46:33.516 - 00:46:36.468, Speaker B: Anka, if you're about to be done, then we could pop whichever you prefer.
00:46:36.516 - 00:46:50.750, Speaker C: We can. Yeah, let me do. I think I'm maybe 1 minute away. Okay, so let me do that and then we'll talk. Cool. Okay, so just a couple of things on this information that you can get from the state of the world. So a couple cute examples.
00:46:50.750 - 00:47:25.540, Speaker C: Indulge me. So when you are in this room with a vase and the person says, go to the door, and you're like, oh, okay, well, the vase is still there, so probably the person wants me to avoid it, and the robot goes and avoids it. Really cute example actually is with not just avoiding unintended side effects, but actually creating desirable side effects. So room with a train. The train is still running. The train would run out of batteries and die, but the train is still running. So instead of just going to the door, the robot cutely goes, takes the battery, repairs the train, and then goes, because it figures out that's something that the person probably cares about.
00:47:25.540 - 00:48:03.644, Speaker C: So anyway, I thought that was really cute in general. Yeah. I think that we shouldn't take these rewards for granted. I think we should really think about the real problem. Now that we have all this progress in RL, it's getting a good time to start thinking about the real problem of trying to optimize what people actually want. And that means that you have to make sense of all of these different types of information that they're giving you, sometimes without even realizing that they're giving you this information. And I think that thinking of them as making choices that make sense with respect to the reward in one shape or another has been a way to get towards that goal of making sense of this information.
00:48:03.644 - 00:49:02.180, Speaker C: Of course, the thing that I worry a lot about these days is that they're not that rational with respect to the reward function. And I have a lot to say on that. Maybe we'll come up in Q and A, but, yeah, I think task specifications, it be them rewards or constraints or whatever they are, they're so easy to get wrong, and we shouldn't really rely on them. We should really involve the robot into this problem of, look, let's work together to figure out what it is that you want me to do so that I can do that well. And the generalization of this is something that we call assistance games, where we really think of the human and the robot working in the same space together at the same time, when the human is trying to teach the robot what they want and the robot is trying to take actions that gather information about that. So, with that, thank you very much. And these are the great students that did this work, as well as faculty collaborators here on this work, Peter Avila and Stuart Russell.
00:49:02.180 - 00:49:07.584, Speaker C: So, thank you. And we can go to Q and A. I'll just stop sharing. Thanks, Emma.
00:49:08.724 - 00:49:09.620, Speaker B: That was super interesting.
00:49:09.652 - 00:49:10.332, Speaker C: I don't know. Can you.
00:49:10.348 - 00:49:36.514, Speaker B: I don't know if you see them, Anka. So in case it's helpful, I'd be happy to curate. So, one of the first questions that we got was sort of asking, and this is related to a later follow up question, too, was sort of, there's also the possibility of dynamics changing over time or reward non stationarity. And so my impression of your talk is that you're mostly thinking about a case where the reward function is latent but static as opposed to dynamically changing.
00:49:37.294 - 00:49:57.654, Speaker C: That's right. So the work that I've presented so far, we think of these parameters as being fixed or static. We just don't know them. And so our estimate of them changes. But there's this one piece of work that we've done that maybe. That maybe get well. So, first of all, I think that actually changing rewards are almost like a red herring.
00:49:57.654 - 00:50:31.214, Speaker C: And let me explain how. So when you look at people, if you have the wrong feature space and the wrong model of them, it might definitely seem like their reward is changing, like their preferences are actually changing over time. But. But there's this one bit of work. We called it a. The assistive bandits, assistive multi armed bandits. And the thought there was that maybe people are actually themselves still discovering what their preferences are.
00:50:31.214 - 00:51:15.540, Speaker C: So if you go on Netflix and you pick a movie, it's not that that's the optimal thing according to your preferences, that you're figuring out what genres you like, what genres you don't like, and so on. You're gathering this information and updating. And so in this little bit of work, we were modeling people as actually learning about their underlying reward. So the person's estimate of their own reward was changing. So that means that what they were acting according to was changing, but the actual underlying reward function, their actual underlying preference, wasn't. So it's very interesting to think about when is the reward actually changing versus when we just have either the wrong model or the wrong representation such that according to what we have access to with seems like is changing. So, but it's not.
00:51:15.540 - 00:51:23.304, Speaker C: In any case, if people are actually learning about what they want, turns out really important to account for that. Otherwise, your reward inference get. Gets really screwed.
00:51:25.404 - 00:51:36.076, Speaker B: Great. And another question was, and I'm not so familiar with this one, but you might be, is, how does this approach compare to semantic navigation approaches? That's not an area that I'm personally familiar with, but maybe that's something you're.
00:51:36.100 - 00:51:43.972, Speaker C: Familiar with, semantic navigation, maybe Ignacio, who asked this, can people unmute themselves and clarify? Is that so?
00:51:44.068 - 00:51:47.412, Speaker B: Chubba, you can chime in if I'm wrong about that, or if you want.
00:51:47.428 - 00:52:47.588, Speaker C: To update, wow, I don't know, the brave museum, my best guess at. But, you know, this, the basically, in any form of navigation, I think the differentiation is even if you know about the semantics of the world, you still have to have something that defines, this is good, this is bad. And so even if you have ways to sort of get at these high level features, right, like, you actually can't say, oh, that's lava. You still have to attach something that says, well, that's good or that's bad. And what we worried about is these situations where there's so many things think about ahead of time that you won't have. You won't attach values to these different sort of types of objects or walls that you could encounter. And so we're kind of worried that, okay, you'll forget something.
00:52:47.588 - 00:53:23.730, Speaker C: Or even worse, maybe if you have access to lava and the person forgot to say that it's bad, it's very trivial to know, okay, I don't know. I have uncertainty over there that. But if you have access to these things and the person said one thing, but because, you know, features were actually correlated, and so you know, they put the weight on this, but that's not, that's mean. More like the cause, the effect, not the cause. And the underlying thing that you really want to penalize is something else. That's the type of reward design mistake people make all the time. It's so easy to look at the sort of the behavioral aspect and say, well, that's bad, that's good, and not look at the underlying causes.
00:53:23.730 - 00:53:51.348, Speaker C: But when you don't do that, you're basically looking at things that correlate with the reward that you want on the environments that you're looking at. And then that correlation breaks later, and then you get in trouble. So I think, I don't know if that really answered the question, but that's how I would think about that. Like, semantics are useful in giving you a lot of information and maybe structuring the reward space that you're doing inference in. But I still worry about people not being able to specify the reward properly within that space. Thanks.
00:53:51.396 - 00:53:57.424, Speaker B: Could you clarify a little bit, or somebody wanted you to, Ahmed wanted you to re explain what you meant by implicit choice by the human.
00:53:58.204 - 00:54:42.434, Speaker C: Yeah, so, good question. So when the person asks the person to compare two trajectories, a or b, they're making an explicit choice, right? They're saying a is better than b. When they're, excuse me, when they're demonstrating a behavior or when they're choosing a reward function or when they're using natural language to say that was bad or do this or and whatnot, that choice is implicit. It's implicit in the sense that the set of choices wasn't presented to them explicitly so that them and the robot know both. That's what we're looking at. And then they picked one out of it. It was implicit in that they demonstrated this behavior, but implicitly, they're saying that that behavior is better than all of the other things that they could have demonstrated, but they didn't.
00:54:42.434 - 00:55:22.144, Speaker C: And it's important to realize the implicitness, because essentially what this can lead to is this form of misspecification, where you're interpreting the choice set that they had the options that they have as being one thing, but they weren't thinking about that. They were thinking about something else. So, oh, I didn't realize I could turn the robot off. And so now you're like, well, you didn't turn the robot off. That has meaning, but it doesn't, because that's not what they were thinking about as an option. And so realizing the implicitness of it means that you can start looking at potential problems that arise from the specification, this mismatch between what the person consider as alternatives versus what the alternatives actually were.
00:55:23.804 - 00:55:49.512, Speaker B: And I guess to build on that, one of the things that it makes me think about is there's certainly nice connections also to the imitation learning and inverse reinforcement learning community. And there, I think there's often been an implicit assumption that the demonstrations are from an optimal agent. And so do you think part of the many contributions you guys are making is say we're thinking about more rich models of demonstrations and given an online reward function instead of just an optimal agent that is making these demonstrations?
00:55:49.568 - 00:56:33.168, Speaker C: Yeah, so this is the bit of work that I had no time to go into, but that's a really good question, Emma. So maybe I'll do 1 minute on this because it's so important. So, yeah, whenever I kind of talk about this, something that people understandably ask and get concerned about is that people don't make rational implicit or explicit choices. They're just not rational. And the way the field has kind of papered over this, in the case of comparisons and demonstrations, is with this kind of adding noise, right? Saying, well, the probability of the choice is proportional to the exponent of the reward of that choice. And that goes back all the way to some economics models. Lucas axiom of choice is like the root of that.
00:56:33.168 - 00:57:46.140, Speaker C: And then sort of it evolved from then on. The problem is that that just plasters over kind of papers over with noise, some systematic biases that people have. And the trouble is that we have decades of behavioral economics research that is really pointing to how these biases, there are all sorts of them, right? And so there's a lot of work that's describing these biases, someone qualitatively or saying, okay, here's what describing what happened in particular situations, there's a few, like prospect theory, for instance, that have made it into sort of ways to model that mathematically. Now, one thing that we've, we've been making some headway on is this notion that maybe people appear to be really biased and suboptimal, but maybe they're actually, they are rational, but under a different kind of model of the world, under different assumptions about how the world works. And that might be what explains, not that they're, they are suboptimal, let's get it clear, but they're suboptimal in a way that makes sense given the compute constraints that they have, that notion of bounded rationality, but also given sort of the, the assumptions that they're making. About the world. And so, for instance, people operating a robot have no idea about the dynamics of that system.
00:57:46.140 - 00:58:39.894, Speaker C: So they have a hard time predicting the consequences of the action on the robot. But their actions, which are really suboptimal and noisy, make a lot of sense under this very simplified physics, intuitive physics model that they kind of have, that they're operating under. If you realize that, then all of a sudden human actions might make a lot more sense. Sense, right. So there's kind of these transformations on the belief that they have about the world, maybe their observation model, the way that they're processing information, their dynamics model that they're using to make decisions, their prospect theory is basically the perception of the reward, that their reward sort of gets kind of capped in both ways. And if you, we've been trying to estimate those alongside with the reward, it's challenging because there's a lot of ambiguity, right. You can flip the dynamics model around and get a totally different thing.
00:58:39.894 - 00:58:49.374, Speaker C: But at least when you couple that with the uncertainty methods, you end up sort of understanding different options, and then you can start probing at the space.
00:58:49.954 - 00:59:34.352, Speaker B: I think that's super interesting. And I've seen Tom Griffiths, who I think you guys have collaborated too, on some of this work and seen how, I guess, if we think about sort of the boundary computation that people might have, a lot of these things might be rational. I want to pull a question from, um, uh, the chat message, which I'm not sure if you see it or not, but it's from Akshay, um, and he was wondering about if, if the RL algorithm is using the uncertainty and actually just correct me if I've got this wrong, um, its behavior might change, but then the human might also be thinking about how all the RL, if I know that the RL algorithm is going to do Bayesian updating. Um, could this change how the human is trying to provide information? Like, if I try to correct the robot midway, I might overcorrect it. So it'd be like, no, really, this is what my cost function is. And so I'm using my model of how the robot works.
00:59:34.488 - 01:00:26.434, Speaker C: Yeah, well, that is a fantastic question. In fact, in a sense, my PhD thesis was dedicated to this question because it was basically on how you can actually act differently to convey information about your goal, about your intent. But, yeah. So at the very end of the talk, I referenced something that we call assistance games, and that's kind of our overall formalism for the problem of, I'm working together. The robot and I are in a team and we get rewarded by the same overall reward, which is the reward that I care about. But the robot doesn't know the parameters of that reward. And so that solving, computing equilibria of it's, you know, we call it a game, but it's really a deck bomb DP because the reward is shared.
01:00:26.434 - 01:01:02.676, Speaker C: So there's no, there's no, it's not general sum, it's not zero sum, there's common payoff. But, but, but the strategies that come out of that look like the person actually trying to be informative. And the robot crucially, kind of understanding that the person is going to try to be informative. And so you end up with this. But we can only do it for toy tasks is the problem. But you end up with these really beautiful behaviors where the robot to the person, basically, there would be, for one option, there would be for one intent, there would be a very clear way to signal that. And then for another intent, there's not a clear way to signal that.
01:01:02.676 - 01:01:31.308, Speaker C: Basically anything you could do, you would be able to do for the, you should do for the other intent as well. And so the person, but, but the person either waits or even does one of these other things, and then the robot knows that the person would have been informative if they could have been. So that's not it. And so, you know, you kind of get this like turtles all the way. It's super interesting. But there's, there's a cautionary tale too, which is some work that Smith did where we were looking. This is in NuI is looking at the specification of this model.
01:01:31.308 - 01:02:16.174, Speaker C: So in light of this work of, let's assume that people are actually trying to be informative and leverage that kind of came Smitha's paper, which showed that if you assume people are informative and you get that wrong, that's really bad. Whereas if you assume that people are not trying to be informative, they're just optimal kind of in isolation, just doing their thing, their rational choice thing. And then that's what you're using. The fact that the person is best responding to that is helpful to you anyway. So you can leverage that. So the mismatch is the robustness to misspecification is much higher when you're just assuming a naive person than if you're using. You're trying to assume that a person is informative.
01:02:16.174 - 01:02:46.654, Speaker C: And even when in experiments we were trying to be informative, even when we fit the model to that, the model is still not perfect, and I kid you not, the inference was worse, assuming that fit model, which was more accurate to what they were actually doing than assuming the naive person. So naive person is really robust. So anyway, really beautiful question, actually. I thank you so much for that. It's really nuanced and complicated. But yeah, we've looked into some of that, I guess.
01:02:46.954 - 01:03:26.522, Speaker B: And actually I said, thanks so much for the answer. And chaba, I think we have to leave in a minute, so I'll let you be the final chapter. But I had a really quick question, which is, I was curious how you think about the trade off between the cost of elicitation, essentially because you only had time to really briefly talk about some really interesting work on using these priors to, to then either design environments or ask people, which I think is a super interesting direction. But the question of how do you, how frequently can you sort of elicit information from people about their preferences and their reward function and when to do it? You know, do you do it at the beginning? Do you do it at the end? Do you do it in the middle? Is certainly a question we're thinking about for some of our own work. And so I was curious how you guys approached that.
01:03:26.698 - 01:03:47.834, Speaker C: Yeah, we didn't really. So we do expected info gain, which are just like you do it until you don't really reason about the value of that information. And, I mean, as you know, the ideal thing to do would be to really cast it as a palm DP, and then you would just figure out when to ask, when to not ask, when to hold, when to go with the thing.
01:03:48.494 - 01:03:51.474, Speaker B: Is it possible to actually compute that, at least so far.
01:03:53.254 - 01:04:26.204, Speaker C: And so in that sense, the thing that we find ourselves doing over and over again is using looking at the reduction in entropy. And, you know, maybe at best having some notion of how much you prioritize that versus just acting greedily according to what you know already and setting that trade off in some semi smart ways. But yeah, I don't think I have any great answers compared to what you already know on that topic. Yeah, I don't know how to compute value of information.
01:04:26.864 - 01:04:50.134, Speaker B: I think the super interesting question as we think more about humans and robots collaborating. So maybe, chapa, I think we're a little bit over time, and I want to be really respectful of Anka taking the time to join us, but at least I will audit Lee, thank you again. And I know it's weird to be talking to 100 people and not seeing them, but thank you so much for your time. I think that was a terrific talk. And I know we've. I think, recorded the whole thing, and so we'll be sharing it with people more broadly.
01:04:50.674 - 01:04:53.674, Speaker C: Sounds great. Okay. Thank you very much for organizing.
01:04:53.834 - 01:04:56.374, Speaker B: Thanks so much. Stay well, everybody.
01:04:56.834 - 01:04:58.354, Speaker A: Yep. See you guys tomorrow.
