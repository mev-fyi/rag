00:00:00.080 - 00:00:33.188, Speaker A: Celeste Kid, who is a professor at Berkeley, and she works on language acquisition, knowledge acquisition in young children using a combination of computational behavioral methods. And she says she draws inspiration both from classical psychology. So she's in the department of psychology and also with new computational methods with very current techniques. And she's a wonderful speaker. And I've met her on a panel before in Berkeley. She's been at Rutgers. She got her PhD at Rutgers, I think postdoc at MIT.
00:00:33.188 - 00:00:48.916, Speaker A: And I think there's a lot to learn from human learning when we try to apply it to even non humans, because some people think of babies as not quite human yet. So, Celeste, please take it away, and thank you very much for willing to give us this talk.
00:00:49.060 - 00:01:23.450, Speaker B: Thank you so much, Shafi. I really appreciate it. And thank you very much to all organizers for inviting me here to talk talk. So today I'm going to be talking about how humans gain the foundational knowledge that allows them to communicate. I called my talk the foundations of early human learning, but obviously, I was like, it's also the foundations of later human learning. So even though a lot of the work is with babies, it's not really about babies. It's how people learn to communicate, and more broadly than that, how they form beliefs and learn things.
00:01:23.450 - 00:02:39.234, Speaker B: In general, language acquisition is a huge topic. So I'm necessarily going to have to pick and choose just a subset of different aspects to focus on. Specifically, I'm going to be focusing on a few core general principles that human infants employ in the process of learning to communicate with others. Language learning. I thought Jonathan's talk did a great job of setting up, involves building a lot of higher order representations from lower level ones, starting with things like the place of articulation, aspects of the sounds, then moving up to syllables, then from syllables moving up to words, and from words to what the words refer to in the world, the reference to concepts and categories, and so on and so on. Language is one of the few human conceptual domains in which we have a handle on the psychological components of the conceptual representations in a way that allows us to know something about the way that they're related to each other, that kind of hierarchy. So that feature of it makes it a potentially useful domain for studying the nature of concepts and learning in general.
00:02:39.234 - 00:03:46.274, Speaker B: What I want to do in this talk is walk through some key principles that human use, starting in infancy, but continuing through adulthood, to not just learn language, but to learn everything else. I'm going to limit myself to just four ways in which we as humans learn to communicate and learn about things in the world. So the first thing that baby humans need to do in order to get a handle on communication is to track statistics. Specifically, they're tracking statistics from their environment. Babies enter the world able to attract co occurrences and use them to make sense of the world. Actually, not just at birth are they able to do that as they track statistics about things like the rhythmic properties of their language in utero, and are born with some rudimentary knowledge of some aspects of their linguistic environments. One of the major pieces of evidence that we have for infants ability to do statistical learning is work by Jenny Safran with Dick Aslan and Lyssa Newport.
00:03:46.274 - 00:04:39.954, Speaker B: This work examined how infants hear a continuous speech stream and somehow figure out where the individual words are. Unlike words on the written page, spoken words don't have any breaks between them. If you ever heard a language that you don't speak, you know it's difficult to be able to pull out just one word from it. So that's the position that infants are in when they're, when they're learning their target language. If you consider a phrase that a baby might hear, something like pretty baby, from the infant's point of view, that could all be one word, or it could be four pre t, b and b, or two or three. With just that string, by itself, there's no way to know, except for the two word edges. You know that those are boundaries, but where the other boundaries are how many words this is is unclear.
00:04:39.954 - 00:05:29.778, Speaker B: So the idea that Jenny Safran and her colleagues had was that maybe infants could track statistics across syllables, across utterances as they're listening to speech incoming, and use those statistics in order to work out where the words were. So if you look at these phrases, you start to notice that certain chunks track together. So a baby could hear a phrase like pretty mama and pretty kitty, and notice that pretty tends to move together as a unit. So maybe pretty is one word they could hear happy baby and silly baby. And notice again that baby is moving together as a unit. And in that way, by tracking those transitional statistics, figure out where the word boundaries are. There's a catch.
00:05:29.778 - 00:06:22.552, Speaker B: For them to be able to do that, you'd have to demonstrate that infants are capable of tracking the transitional statistics between a relatively large number of different types of units over many different, many different syllables. So they designed an experiment to test to see if infants could do that. And in this experiment, they played two minutes of this speech stream for infants composed of nonsense syllables like this. Let me play a little segment so you can kind of hear it and you get the gist. So that's just a little clip and whoops. This nonsense language that they played to infants was constructed in a way that it was composed of novel word like units that move together. So the transitional statistics between the syllables are different.
00:06:22.552 - 00:07:37.644, Speaker B: If you were tracking the transitional statistics, you could theoretically identify two different types of items that could be pulled from this stream. Novel words composed of syllables that are tracking together, and what they called part words, which occurred in the speech stream and contained syllables that all occurred equally often in the speech stream, but that did not occur as often as a unit together. After just two minutes of exposure to this nonsense language, infants showed that they were able to differentiate between these two different types of items from the speech stream, the ones that traveled together as units and the ones that did not. Specifically, they showed a greater interest in the part words, a pattern commonly interpreted as indicating that infants found those part words surprising. This was some of the first big evidence that infants can track statistics and use them to form new concepts. And it started a wave of other experiments showing that it wasn't just in the domain of speech, that just tracking environmental statistics as they come in could be useful. So this was not just limited to the auditory domain.
00:07:37.644 - 00:08:40.684, Speaker B: This is some visual stimuli from an experiment by Joseph Pfizer and Dick Aslan showing that infants can also track visual co occurrence statistics. This is some visual scenes that are composed of a kind of grammar. They're these little units that are like words that infants are presented with. And just as they did with the auditory stimuli, they're able to notice when two shapes are in the same configuration across multiple different displays. So they're able to infer the lexicon from a surprisingly small amount of exposure. Statistical learning is a powerful domain, general mechanism that could potentially explain a lot of great learning feats, including learning language. It's simple, it's powerful, and the concepts it allows you to learn can build over time in the sense of building up a hierarchy to acquiring full blown language.
00:08:40.684 - 00:09:51.674, Speaker B: And we know that that ability is shared by many other species, including non human primates, rats, songbirds, and many others. There's a great 2018 review by Kiara Santolin and Jenny Sefron that I'd recommend checking out. It came out in trans and cognitive sciences in 2018. We know that human infants use this general learning mechanism to build up their knowledge over time, building bigger representations out of the last set of building ballocs that they acquired. So initially, human infants track statistics over syllables in order to learn that there's a spoken word apple. Then they track the co occurrence statistics between that word and objects in the world, in order to learn what apple is actually referring to. To eventually using statistics in order to form a category of apples that includes red apples and green apples, but excludes oranges and peaches, and so on and so on, increasingly building higher levels of representation.
00:09:51.674 - 00:10:43.412, Speaker B: So the goal of the infant is to eventually acquire an adult like understanding about how things work in the world, including different concepts and how those concepts can be combined. This mechanism I mentioned is not just powerful for learning about language, it's also what humans are using in order to learn about what causes what in the world. It allows you to learn how things work. So this is how you build up your adult model of the mental model of the world. The better and the more accurate your predictions. With your mental model, the better decisions that you can make and the more you can optimize utility. But this is where it gets really interesting.
00:10:43.412 - 00:11:31.774, Speaker B: This mental model of the world that's generating the expectations, it's also helping you generate expectations about where in the world you should search for the next set of information. So the early things that you learn are feeding into your subsequent decisions. It's directly shaping where you attend to, where you look, and ultimately what you know of the world. The difficulty in finding truth for us as humans is that there's far more knowledge in the world than any one person could possibly ever obtain. Which means that an individual is forced to pick and choose each moment. When a person has selected one thing to learn from, they're necessarily missing out on everything else. So each choice matters because it's coming with a huge opportunity cost.
00:11:31.774 - 00:12:18.804, Speaker B: As a very simple example, you only have one set of eyes. So if you are sitting and you are looking in one direction, you're necessarily missing out on anything that's happening. All the directions that you're not fixating, like behind you, for example, that simple fact has really profound implications. It means that to be a person in the world, you have to pick and choose. You're going to be having to form beliefs based on just a small subset of all of the information that's theoretically available to you in the world. And you're not conscious of it. But you're making at least half a million decisions, very quick decisions every day, about where to look, what to explore, what to click, what to read, what to listen to, who to talk to, et cetera, et cetera.
00:12:18.804 - 00:13:08.426, Speaker B: How you make those decisions is informed by what you already know, what you believe to be true, and where you expect to have uncertainty. You have to make those decisions very quickly, so there's no time to lament them. And you're limited to sampling and forming beliefs, of course, based on what's available to you as an individual. So the second thing human infants do to learn is that they seek out what isn't understood. Our systems have a very effective built in mechanism for helping us avoid wasting time and navigating the challenges of information overload in the world. And that mechanism's common name is boredom. For example, our systems engage boredom when we're confident that we know everything that there is to know.
00:13:08.426 - 00:13:48.252, Speaker B: This is very rational. It's like, why should you keep watching if you already know everything? There's no learning value. These systems are good in that they prevent us from hyper fixating, but they do so at the cost of sometimes shutting down the information tap before we've gotten the right idea. So they are not perfect systems. I'm not going to be focusing on that in today's talk, but last week, I gave a tech talk at x that does focus on that. So if you're interested, you can search for that or message me. In my lab, we've used eye trackers to study the principles that guide our sampling and learning.
00:13:48.252 - 00:14:35.874, Speaker B: I'm going to tell you about a set of studies we did with human infants in this section. These studies taught us a lot about human infants. Solve that sampling problem that I made reference to how we, as humans, weigh information gain against opportunity cost. In this series of experiments, we showed infants displays that were designed to elicit expectations while allowing us to construct a reasonable, probabilistic model of these expectations moment to moment as the displays unfolded over time. In these displays, objects popped out of boxes in some order. I'll show you what that looks like. These experiments with infants always start with an attention getter, and we use a baby who is laughing because babies really like looking at other babies.
00:14:35.874 - 00:14:57.314, Speaker B: Do you guys hear it? Okay. Okay, so there's objects popping out of boxes. There's exactly one object in each box. Across the trials, there's different boxes with different objects. Yes. Hopefully you get the intuition that some are more surprising. There's the attention getter again.
00:14:57.314 - 00:15:27.226, Speaker B: And I'll show you just a couple, a couple moves of that one. That one keeps, keeps going. This is cuter. This is a baby sitting and doing the experiment. So as long as she kept looking, the design is contingent. As long as she keeps looking, the displays keep unfolding, objects keep popping out of boxes. The mom is wearing headphones so that the parent can't influence what the kid is interested in.
00:15:27.226 - 00:15:29.094, Speaker B: She almost looks away, but doesn't.
00:15:36.314 - 00:15:37.054, Speaker A: And.
00:15:37.714 - 00:16:17.354, Speaker B: She looks away for criterion time of 1 second. Laughing baby comes back, she resets, and we can just do that over and over again so we can get a large amount of data from. From each baby. Okay. To quantify these expectations, we applied a textbook example from bayesian statistics, a Dirichlet, which provides an estimate of probability that each box will activate next, given the previous sequence of events. At each point in time, the baby either looks away or they don't. And if they don't, then we move on to the next trial.
00:16:17.354 - 00:17:21.364, Speaker B: The next. Sorry, the next box pops up. We can take the posterior of each action at each moment in time and then use it to compute a surprisal value for each event in the sequence. With this in hand, we can then plot the probability of infants actual look away behavior as a function of the surprisal value of the event. This allows us to uncover the relationship between idealized probabilistic expectations and children's behavior in a simple and independent way. And when we do that, we get what we've described as a Goldilocks effect, by which I mean that infants tend to look away at events that are highly predictable, but also they tend to look away at events that are highly surprising, like this. It also applies, controlling for other factors, like the posterior entropy, and even holds when we model transitional probabilities, which are important in language learning rather than doing each event independently.
00:17:21.364 - 00:18:14.754, Speaker B: It's true not just of visual attention, attention to visual sequences, but also auditory ones as well. That's really important to know, since auditory statistics are likely to be most important for the first stages of language learning. Oh, and I should also mention, I was like, we have new data. There's a paper under review. It's also true for older children, children over three years old. We just did some new work that was led by now Upenn postdoctoral scholar Laura Sosky, who's working at the center for Autism Research. Here's an example of an infant doing an auditory version of the task, one in which the model is estimating the surprisal based on the occurrence of items in the sound sequence, not things in the visual sequence.
00:18:14.754 - 00:19:30.710, Speaker B: We're still using the eye tracker and using that to allow the infant to control what they see. We're using a visual display in order to keep their attention, but we're holding it constant and changing the items in the auditory sequence. So, same guy over and over again, and the sounds are what's varying looks away, she gets another trial. Look away. We get that same Goldilocks effective disengagement at points of high and low surprisal here as well. So this guiding principle doesn't just apply at the group average level. It also applies at the level of individual inference.
00:19:30.710 - 00:20:32.974, Speaker B: This kind of U shaped function, we think, represents a broadly applicable strategy for organizing your search for information in the world. Learning scientists had previously suggested that this kind of a pattern might be helpful for allowing users to seek out things that are valuable for learning. For example, if you imagine picking a book to read, you can imagine that this ABC book is not very attractive as an option, because as an adult who knows my abcs, I already know this. There's not a lot of new information that I can gain there. But on the opposite end of the spectrum, if you're to go for the most new information possible, say you pick a book on a topic you don't know anything about, I don't know anything about billiards. In a language that you do not yet speak, you could potentially learn two things at once, but you really can't, because you can't get any traction. I was like, you're missing the base levels of representation that you need in order to make sense of it.
00:20:32.974 - 00:22:03.438, Speaker B: Instead, the most attractive option falls in the Goldilocks zone. So something like a book in a language I speak on a topic on which I have some background. This is a new one that I'm reading right now, which is very good. These kinds of decisions, we think, originate with the same type of cognitive system, which favors material that is a little bit surprising, but not overly surprising, given what you currently understand. Importantly, in order to exhibit such regularities, linking attention and an idealized model of the stimuli, infants have to be tracking those statistics and building expectations about the world and using those in order to guide their decisions about whether to continue watching or to disengage. This idea is consistent with a lot of other evidence that we have from developmental psych that suggests that very young humans seek out experiences that violate their expectations, but just a little. So, for example, in displays like this, if a ball is animated to bounce around in a box with a hole in one side, just one hole in one side, and three holes in the other, infants attend longer when the ball exits out of the less likely side, which you might expect is an indicator that means that they find that surprising, and they attend for less long when the ball does the more likely thing.
00:22:03.438 - 00:23:12.150, Speaker B: So if the ball flies out of the side that has more holes in it, possibly the more expected side, then they're not as interested. If it doesn't violate your expectations, it carries little information, so you're less interested. There's cool work from people like Laura Schultz and Liz Bonowitz with older kids suggesting that young humans don't just follow this pattern in terms of deciding where to attend to, they also follow it when they're deciding what to play with. So they have this study where there's these toys that look like this, and kids will play longer with the toy when they haven't yet observed evidence that makes clear what the causal structure of the toy is. So these boxes have two levers and two puppets that pop out of the top. The manipulation is the demonstration that they get before they're offered the opportunity to play with it. Either both levers are pulled at once, so it is unclear which lever leads to which puppet popping out, or they're demonstrated one at a time.
00:23:12.150 - 00:24:08.388, Speaker B: And if you demonstrate the confounded evidence, that toy is a lot more attractive, indicating that kids are seeking opportunities to learn about the causal structure of the world. Okay, three out of four things. This is the third thing. Once you're tracking the statistics and using them to guide your search, you can also leverage statistically reliable cutes to gather additional information about communication and everything else worth learning. I'm going to pick just two examples from our lab. For example, we've looked at toddler's ability to leverage a somewhat unexpected cue in order to learn language more effectively. That unexpected cue is speech disfluencies, including, uh, and, um, while you probably don't intentionally produce these, they are a feature of natural speech.
00:24:08.388 - 00:24:44.814, Speaker B: And if you think that you don't produce them, I promise you for not memorizing things you do. And if you'd like, I can point them out, but you'll be self conscious for days. So I warn you. So the thing about speech is fluencies is that they occur in predictable places, specifically before words that are infrequent, and also before object labels that are new to the conversation. So pretend that these two objects are present on a counter. And I said I was going to make some orange juice. Now imagine if I asked if you could please hand me the.
00:24:44.814 - 00:25:39.304, Speaker B: You probably feel your eyes drifting over to the juicer and not to the orange. And if this were a real life situation, it's very likely you'd start to reach over to the juicer without me even having gotten to the word juicer. So you can anticipate I'm going to say juicer, because oranges are previously mentioned if I was talking about orange juice. And oranges are also a more frequent word than juicer. So because they were just mentioned and they're more frequent, they're less likely to induce processing difficulties that result in the speech disfluency. Um, small humans are also capable of drawing that inference about a speaker's referential intentions. In an experiment that we ran a while back, we presented toddlers who spoke just, just a few words themselves.
00:25:39.304 - 00:26:43.078, Speaker B: So these are not super verbal kids with similar situations, each one featuring a frequent mentioned object and then one unmentioned novel one. We used eye trackers, like before, to measure where on the screen that they looked. And specifically, we were interested in the period right before the object was named. That spot was either occupied by a bit of fluent speech or by lengthening, like the in a field pause disfluency. Like, uh, we wanted to know if the disfluency prompted the infants to look towards the weirder unmentioned object. And what we found that this is a skill that develops over time and starts to emerge around two years of age and continues to develop through the early portion of a kid's life. This is something that adults are able to do, as if adults are able to use this cue in order to anticipate what somebody is going to talk about next.
00:26:43.078 - 00:27:36.644, Speaker B: And even young kids, although they're not saying very many words themselves, they've picked up on this statistical regularity. For two year olds, this is a very useful trick, because kids are still in the process of learning what words in the world mean. They're in a period of heavy vocabulary development. So allowing a kid to anticipate that something that is infrequently labeled is about to be mentioned so that they can be ready. And looking at it by the time the object label is actually mentioned is hugely helpful. This is just one in a whole line of evidence demonstrating how young kids are able to leverage partial knowledge in order to gain more. I want to show you just one more example from a study led by Emily Sumner and Erica DeAngelis that we did a few years ago.
00:27:36.644 - 00:28:05.314, Speaker B: This study, how we did it, is we asked kids to choose among two options in several different games that we made up. I'm going to show you a video. What I want you to do is see if you can spot the regularities in how this two year old is responding. Does the playground have pine or oak trees? Oak trees. Oak trees. Look, it's a tree. An oak tree.
00:28:05.314 - 00:28:27.634, Speaker B: Where are you gonna put it? Next to the tv? Is the school fence brick or wood, wood, wood. Here you go. Where's it gonna go? Oh, very nice. Is Rory's shirt yellow or red? Red.
00:28:30.814 - 00:28:31.510, Speaker A: Right.
00:28:31.662 - 00:29:38.074, Speaker B: So she's choosing the second thing. And this was a very common pattern for kids at this age to exhibit. Overwhelmingly kids in the early stages of language production, when they're first starting to say, say words and respond to questions like these, they are very likely to choose the second of two choices that are offered upwards of 75% of the time. In the task I just presented, this effect, you can see, was large, and it didn't just occur in our weird lab tasks. It also occurred in naturalistic responses to parental questions that we pulled from a giant corpus of kids speech recordings called childless. This bias is very strong at two, and it decreases in a naturalistic environment as children learn more words and as they mature. But you can bring it back in older children if you present them with choices that are longer and involve more syllables.
00:29:38.074 - 00:30:36.324, Speaker B: So if you're trying to coerce your child into making a particular choice while still wanting them to feel like they are making a choice themselves, this is a great strategy to use as you increase the number of syllables. Presumably it increases the demands on their working memory. And we don't know exactly why it might be the case, but a good possible explanation is the second thing is just more accessible. This is a good demonstration of what you see in young kids is sometimes not what you think it is. In language development research, speech productions, word productions are often taken as a gold standard as evidence that the kid knows something about the concept to which that word refers. So, like, this is a great demonstration of kids saying things that they probably don't know. They may not know what the reference actually is.
00:30:36.324 - 00:31:27.666, Speaker B: Okay, fourth and final thing you need to do to learn to communicate, you continuously update everything that you have learned. Human beliefs are inferences, not records. They're probabilistic expectations. A lot of the time, the way that we talk about kids languages, as though once they've learned something, they filed it away and they're done. It's baked into how we talk about these things. So you say things like, my toddler learn the word spoon, and you might think that if a child is always using a word in the correct context and always recognizing it when it's spoken to them, that that concept is at least mostly, mostly built. But that's not at all how it works.
00:31:27.666 - 00:32:02.520, Speaker B: And we don't have a whole lot of empirical work with kids probing exactly what their concepts look like, but it is very likely that they are far away away from adult concepts. Okay. So with every new piece of data that a kid experiences, they're updating their concept a little bit. And we also have evidence that that's not just true of kids who are in the early stages of building their vocabulary. That's.
00:32:02.552 - 00:32:02.896, Speaker C: That's true.
00:32:02.920 - 00:33:12.206, Speaker B: Adults, too. So with something like a spoon, with every spoon that you encounter, you update your concept just a teeny bit. And if you move to a place that has spoons that are more atypical, or in the future, if culinary design trends shift towards shorter, fatter spoons or something like that, your concept shifts right along with those changes, and you don't even notice that that's happening. Your beliefs about spoons change right along with the shifts in the data that you're observing. Since every child and every person takes a different path through life and encounters their own unique set of sequence of spoons, in this case, it makes sense. It follows that every person is going to have their own unique set of concepts and more generally, their own unique set of beliefs about what's true in the world that maybe often roughly match, but which are unlikely to be identical. So we were interested in measuring the degree to which people in the population share common concepts.
00:33:12.206 - 00:34:16.614, Speaker B: We were interested in how much diversity there is, and given that we expected to see some diversity, are people aware of it when they're communicating? To do this, we asked people to rate similarities between common concepts, following classic methods in cognitive psychology from people like Roger Shepard and others. Some of these concepts were political, and some were just common nouns. We were interested in discovering how many latent versions of each concept lived in the population. This is a t sneaplot, colored according to the clustering results from a chinese restaurant process where response vectors for each concept were clustered across individuals in the population. For instance, based on the responses people gave to similarity judgments to Richard Nixon, we arrived at a distribution over different Nixon concepts that people might have in the population. In this experiment, people were actually asked twice, so this is their relative reliability in answering.
00:34:17.234 - 00:34:20.466, Speaker A: Sorry, what did you ask here? I missed it.
00:34:20.570 - 00:34:24.214, Speaker B: Oh, that's okay. I'll go back to. So. So they're asked a series of questions.
00:34:24.634 - 00:34:25.642, Speaker A: Okay. Okay.
00:34:25.778 - 00:34:55.414, Speaker B: Yeah, so it's a. It's a binary choice. And as, like, you get asked to compare everyone to every other one. The reason why we use this approach, instead of just saying, like, I don't know, something more free form, we want to be controlling for the context, so we're asking you to rate it in this particular context. And everybody has the same context. Text. If you thought that we all had basically the same concepts for these words, what you'd expect to see is one single cluster.
00:34:55.414 - 00:35:37.520, Speaker B: But what we find instead is that there are about five to ten different distinct concept clusters for each of these labels in the population. That said, there's not no difference between all of these things. We use the species. Oh, sorry, I forgot a detail. We use a species count estimator from ecology to extend the number of clusters in our sample to that of the entire population on the planet. Using methods like good turing estimators, we find that the number is approximately what we should expect to see for the entire population in our sample. When we started this work, we expected to see less variation for concrete things that you could observe and more variation for the abstract ones.
00:35:37.520 - 00:36:24.258, Speaker B: And that turns out to mostly be true. But what we were surprised to see is how much variation there is even among the concrete, the concrete concepts, although people do vary more on some concepts than others. So here is salmon, just pulling out that has about three main types of concepts. There's more agreement here and fewer, fewer, fewer, fewer types of concepts for salmon, but there's less agreement on this one. Joe Biden. The increased number of clusters here indicates that Joe Biden means fundamentally different things to different people. For this, in our experiments, in this other experiments, in this area, we get our similarity data in a way that's controlling for the context, as I just mentioned.
00:36:24.258 - 00:37:19.084, Speaker B: So even in this same context, people's concepts vary quite a bit. We weren't just interested, though, in the absolute number of concepts we were interested in. Given that there's this diversity, do people know when they use a word that the person who is hearing it may not be activating the same concept that they are? So, to look at this, we collected a measure of conceptual alignment across people. And the way that we did that was we asked people questions like this. How many people out of 100 would agree with you that x is an example of concept y? We asked people to report how many people they expected would share their concept. And what we found was this. This plot represents people's expectations for others to possess the same concepts as them.
00:37:19.084 - 00:38:07.852, Speaker B: If people were perfectly in tune with the variance in others concepts, you'd expect to see all of the data clustered along that diagonal line. But instead, what you see is that people tend to overestimate how much other people will agree with their particular version of a concept. In other words, they expect that when they use a particular word, other people will share their concept more often than is actually true. So two people using the same word in the same context, they do not necessarily mean the same thing. What's worse, they generally are not aware of this fact. It's easy to see how this could be a potential hindrance to effective communication. That was the last one.
00:38:07.852 - 00:38:34.096, Speaker B: So there you have it. That's four ways in which we learn. These represent mechanisms we employ to learn how to communicate as humans. But like I mentioned, they're a lot more general than that. They're also employed to form all kinds of mental representations, and in turn, they form the foundations of what we believe to be true. Number one, humans continuously track statistics. Two, they seek out what isn't understood.
00:38:34.096 - 00:39:20.242, Speaker B: Three, they leverage statistically reliable cues. And four, they continuously update, even for things that you might not think of as being continuously updated, as things like spoons. That's all I want to say, but I want to thank my lab, especially all of the people in pink there who directly worked on the projects that I spoke about today. Thank you again so much to the organizers, and thank all of you for coming and listening to me. I'm happy to answer some questions if we have some time. Also if we don't have some time, there's my email and my Twitter and my everything. So I'll be right here till the pandemic ends in Berkeley.
00:39:20.438 - 00:39:34.414, Speaker A: In Berkeley, thank you very much. I think there is a question, or at least one, in the chat from Marie Manili.
00:39:40.674 - 00:39:54.484, Speaker B: Reward production kids, I agree with you. Productivity certainly does not entail comprehension knowledge. This reminds me of a seminole, at least. Speech language pathology. Speech and language. Oh, this paper. I was like, I don't know that paper.
00:39:54.484 - 00:40:24.106, Speaker B: I will check that out. Yeah, useful. Useful to know. It's one of those things that people don't like. Everybody kind of knows, but we, as developmental psychologists, like, how do you know what word a kid knows? There's lots of cases of interesting anecdotes if you actually go in and probe what the kid knows when they're using a particular word. I can't remember whose example this is, but their kid used the word late in all of the right contexts. They recognized it.
00:40:24.106 - 00:40:53.274, Speaker B: They reacted correctly when the parent used the word late. But when you explicitly asked the kid, what does late mean? They said, late is when you go to school and everyone already has their boots off. So that's part of the definition. But that's a very limited piece of what late means, and you'd never know that. Most kids are not able to so eloquently articulate, like, their definitions for work. So a lot of this just goes undiscovered because we don't have great ways of getting at that.
00:40:55.374 - 00:40:59.194, Speaker A: And I think there was an earlier question by Marie as well, in 1249.
00:41:01.414 - 00:41:57.794, Speaker B: Oh, right. Regarding statistical learning, am I right to assume that the existing body of literature only accounted for statistical learning exhibited in typically developing infants? No, that's not the case. So I mentioned there's more than one person who has looked at statistical learning in children with autism. Laura Sosky has been interested in that topic and didn't have a chance to collect a full set of that population before she finished her PhD. But we've piloted that study. You don't need very much data before you can get a sense of what the linking function looks like. Based on the pilot data, kids with autism didn't look any different from typically developing.
00:41:57.794 - 00:42:42.280, Speaker B: With the caveat that, like we, before we published it, we want a larger sample. Older kids also, yes, appear to have that u shaped function that I talked about, other people who have worked on statistical learning in special populations. Jenny Safran actually has some work that was in progress and I don't know if it's published yet, but she was looking specifically at statistical learning in kids with autism. Am I missing. Sorry, too many windows. Oh, yeah. Are you aware of similar studies? This is from Paul Best.
00:42:42.280 - 00:43:25.164, Speaker B: Are you aware of similar studies experimenting the tendency to choose a second option in adults? We have looked at response biases in adults and it's more common for adults to actually exhibit the other bias. So if you are tired, adults usually have a primacy bias in those same kinds of questions. If you are tired, then that is likely amplified. If you don't care, then that's amplified. Why it flips may have to do with adults being better able to anticipate that a choice is coming up. So I don't care. I just want to get it over with where kids probably are not able to anticipate it until the end of the sentence.
00:43:25.164 - 00:43:31.424, Speaker B: They're just kind of lagged. So the choices have been named before they realize that there's a choice that they need to make.
00:43:36.964 - 00:44:09.464, Speaker A: Any more questions? Then I would like to. We're going to have a few minutes of the closing, but I want to first express my thanks to everybody and all the talks in the workshop been really incredibly stimulating, fantastic. And also thank you guys for organizing. Thanks for the last session to Jonathan and to Les. So I think David wants to start a few to say some summary words. And Michael.
00:44:10.764 - 00:44:12.664, Speaker D: Yeah, sure. Michael, do you want to start?
00:44:13.884 - 00:44:14.220, Speaker B: Yeah.
00:44:14.252 - 00:44:56.086, Speaker E: So thanks everyone for joining. I think it was a very interesting workshop. At least I can speak for myself. I learned a lot. I think it was extremely diverse talks from biologists, from computer scientists, covering really an improbable mixture of topics that I think otherwise, you wouldn't find, basically, people that I think would almost never meet together at any other conference. So I think, really thanks to the Simons Institute for hosting such an interesting event. And I think we heard a lot of different things, but also a lot of common things.
00:44:56.086 - 00:46:01.664, Speaker E: Well, to me, I think the use of machine learning methods, well, I'm obviously biased, but seems like a very promising topic that was mentioned continuously in the research of different species, and not only mentioned, but also used what we've seen today, for example, about research we've seen. I should be pronouncing it properly. I think that it doesn't sound like bad research and also, well, obviously, what we are doing with the whales. I think the elephant sounds also very promising for using machine learning research. Then some analogies that could be drawn from human language processing and developmental studies in children could be applied to non human species as well. So David actually had maybe a few ideas about the follow up, and maybe he wants to kick off maybe a short discussion about what should be the next steps.
00:46:02.444 - 00:46:42.230, Speaker D: Yeah. Well, thank you, Michael, and thank you, Shafi, and thank all the panelists and all the participants who stayed with us for, for all these hours on screen, I guess. Yeah, this was really, really interesting. And my head is exploding. And I love the interdisciplinary flair. I guess I've been in the last few weeks thinking about the pandemic, too. I've been reading some Norbert Wiener and cybernetics and really realizing how novel it is to really do core disciplinary interdisciplinary work is pretty not incredibly common in academia, at least.
00:46:42.230 - 00:47:35.494, Speaker D: So, thinking about next steps? Yeah, we would just love, this is our first foray doing this kind of event, and we would love some feedback from everyone on what they liked and what they didn't like and what could be some next steps. And are there other speakers or other research areas that we missed? And, yeah, and, you know, and of course, you know, our projectseti.org, comma, we want everyone to sign up and so you can get updates, and we could. We could keep in touch with you and try to lure you into this tangled web we're creating. But, yeah, just to open up the forum to everyone here on, you know, on their thoughts and feedback of the, of the two days and how I can get this plant in better shape is some other.
00:47:39.114 - 00:47:41.134, Speaker A: Talking to him her.
00:47:41.714 - 00:47:43.814, Speaker D: I'm not doing well here, so.
00:47:47.674 - 00:49:00.154, Speaker A: So I guess my thought is probably the thought of everyone, and that's. I would love to see some translation between the work on vision and the work on natural language, or on interpreting signals. Obviously, all of this stuff is signals, but the question is, how do you interpret? Seems like, for example, first candidate would be like the internal learning that Michal was talking in the context of NLP. In general, speaking as someone is a cryptographer, which is completely the opposite. Rather than trying to understand, you're trying to hide. But there are works in differential crypto analysis, in breaking, which are quite sophisticated, but they actually reminded me of some things that Michal was talking about in terms of what would be the analogy to a patch and getting examples from ciphertext and other things that sort of emerged here, this issue of indistinguishability, trying to characterize something that's natural, as distinguishable from random, trying to generate things by this again, but in other settings.
00:49:02.494 - 00:49:37.662, Speaker C: I'm actually wondering whether this approach of the unsupervised approach for language to language translation, whether it could be applied, for example, for translating fMRI data to images, thinking of these as two different languages, and applying a similar type of mechanism. Right now I'm using other ways of my own, encoders and decoders, but I didn't think of this shared embedding space and so on, as it could be very well tools I could borrow from NLP. So this is interesting.
00:49:37.838 - 00:50:03.654, Speaker E: It is possible that the reason why it works for human languages is that language, basically, this embedding captures not only the structure of the language per se, but the structure of the way that we express. We see our world. And probably it is very similar, no matter if you speak in French or in Chinese. So it's probably something deeper than just the language itself. But there are.
00:50:05.114 - 00:50:38.076, Speaker C: But that's why I was asking about how similar those languages are. Because fMRI, brain activity and images are completely different, and they're organized in different locations. Doesn't like in language, you still have the same temporal order. You said one sentence and then another sentence, and then a third. Maybe the words in the sentence are somewhat different in one language or another, but you won't mix the sentences, whereas in brain regions, things would be organized totally differently inside the brain than they would be inside the image. So I'm not sure, you know, so.
00:50:38.100 - 00:50:51.744, Speaker E: Some languages actually are completely different. So if you. If you look at Turkish, it has reverse order compared to English, let's say, so exactly reversed. So this is a nightmare for interpreters.
00:50:51.784 - 00:50:57.484, Speaker C: For example, within the sentence, not reversed within the page.
00:50:57.784 - 00:51:02.320, Speaker E: Yeah, of course, within the sentence. But sentence is probably already what I'm saying.
00:51:02.392 - 00:51:41.504, Speaker C: Like our idea, if, let's say you have an object inside the image, it has both low level features as well as semantic features. Those things would be in separate regions in the brain, but they're in the same region in the image. In that sense, I meant whether they're organized, you know, spatially or in the image. Spatially in the brain versus temporally in different languages, the temporal order is maintained, maybe not locally, but definitely above a certain scale. It's maintained across languages, whereas here, I'm not sure. I'm just wondering. That's why I was asking about the alignment of the embedding spaces and so.
00:51:41.544 - 00:52:09.064, Speaker E: On, but in from rise signals, do you see? So the simplest model for language, or at least that preceded the deep learning, is co occurrence. So if, for example, two words appear one after another frequently, do you see something similar in fMRI? So, if you see two visual objects in an image and you see them separately, will you get some kind of mixed response of each of the individual responses or something completely different?
00:52:09.364 - 00:52:41.620, Speaker C: I don't know, and I don't think these things are known well enough in the brain. I mean, people are trying to. And I'm not a neuroscientist, so take everything I say with a grain of salt, but not everything we see in the image. Do we know where they are expressed inside the brain? Okay. It's not like I could tell you. Here is the mapping from this to that, whereas in languages, you can tell me. So this is more like the Rosetta stone, but, you know, with another language that doesn't appear there and you want to translate it now, and you have no idea, you don't have any knowledge.
00:52:41.620 - 00:52:52.504, Speaker C: Well, neuroscience have more knowledge about the language that you. But it's definitely not the same spatial organization as it is in the image and not as unknown. Yeah.
00:52:54.404 - 00:53:47.934, Speaker A: I guess. Another thing I found interesting in the bat research, I don't think he's online anymore. But this idea of compressibility as a measure, there was some point in the talk, is whether the model is learning well, right, or versus memorizing. So, I know that this is a big question in machine learning in general, but it's also been used as a definition sort of in cryptographic settings, sort of. You know, there's equivalence, as shown, between definitions based on compressibility and definitions based on prediction. It's interesting to try to show that they're equivalent to each other, and whether you can prove that not just an empirical sense, but in some sort of a more analytical, theoretical sense.
00:53:50.474 - 00:54:33.154, Speaker E: So it's also interesting. I also liked very much what Jonathan was trying towards the end, and about trying to understand the language of, well, aliens in this. In this case, basically a language that is invented, that is learned by agents. I think NLP mostly focused on existing languages for very practical consideration. I remember once doing a test in some unnamed organization in Israel, where the task was to. To translate from an invented language with a vocabulary and a short explanation of the rules. And then you're given a couple of pages and you need to translate.
00:54:33.154 - 00:54:52.294, Speaker E: So that was an interesting exercise. And I wonder if, even if you remove the rules, if you just are given, let's say, if you applied modern technology to Rosetta Stone, just one piece of it in one language, would it work? Would it produce something that is understandable?
00:54:59.434 - 00:55:20.976, Speaker C: How about cryptography? If you have a. You had, let's say, the Enigma code, and you knew it came from a german language, but you don't know what the. So this would be like, translating from one language to the other, not having the one to one correspondences, but do knowing which language this corresponds to.
00:55:21.080 - 00:55:26.084, Speaker A: That's an interesting question. That's an interesting question. To apply sort of modern methods.
00:55:29.464 - 00:55:30.804, Speaker C: Cryptographic.
00:55:32.544 - 00:56:07.524, Speaker A: For breaking things that we know that are breakable, I would find it extremely unlikely that it would break modern things, because then you would have factoring algorithms that are based on some total. You know, it would be. I don't believe it. But to break things like the enigma, where. It's an interesting question. I don't know that people have actually looked at it. I guess everyone here, this idea of compression and how compressible a thing is, is a measure of complexity and distinguishing between different species.
00:56:07.524 - 00:56:34.444, Speaker A: So, you know, we have these elephants, or even within whales, we have the sperm whales. And I guess you have some sort of hierarchy that you suspect that some are more sophisticated than others, and then you have different recordings and different dialects. Is there some notion of complexity that one suspects that could be related to, could be measured something that seems totally unrelated to, like, compression?
00:56:34.944 - 00:57:38.480, Speaker E: So I was asking if there are some notions on what kind of language other species have. So, in human language, we have some ideas about what are the formal properties or what kind of languages say in the language hierarchy. In the Chomsky language hierarchy, we know that human languages are mostly context free. In some cases, there's some context sensitive phenomena, and some phenomena are actually regular or even subregular in realms of morphology. Do we have any such idea about human communication? Any way to characterize it along these formal lines? I don't know. I think probably human, non human communication systems that, at least from what I read, are even not considered a language in the pure sense. They lack some of the features that human languages have.
00:57:38.480 - 00:57:44.364, Speaker E: So it's probably even difficult where to start. What are the basic units?
00:57:46.224 - 00:59:10.014, Speaker F: I think it's been a real challenge to, other than the very simplest sort of tests that a smear ecology can apply, to have a meaningful conversation about that. So, I mean, when the meeting that I first met Michael and David, you know, not so long ago, there was a linguist, Kevin Ryan, who may or may not actually be listening in still. And I think the first thing I said to David after that is that I've never had someone who calls himself a linguist even sort of interact with us on the level of having those kinds of conversations. And that gets into David's point about why this is exciting from an interdisciplinary perspective. Yeah. I mean, agreeing on a universal definition of complexity across, you know, even primatologists and whale biologists, as being a contentious, debated thing for a long time, let alone the shifting bar of what might constitute a sort of stepwise progression towards what a language is and what a communication system is, rather than just having these two buckets, one that is uniquely human and one that is. Yeah, I think that's one of the reasons why this is exciting, is to get at those concepts.
00:59:10.174 - 00:59:46.884, Speaker A: But I think there is compression based work to distinguish between different writers, their work sort of in between different languages, so certainly couldn't. So it's agnostic to biology or species or anything. It just talks about predictability, compression and prediction, and it can distinguish this way which species we're talking about. It doesn't indicate necessarily complexity, but it would be, even for distinguishing purposes, would be interesting. So it's a tool that has nothing, doesn't know anything about the domain. Right.
00:59:58.984 - 01:00:16.704, Speaker E: So Kevin is here now we have Kevin too, as well, the linguist of Project SETI. Well, I'm not sure that the team, he's available, but maybe he can comment as probably the best expert on anything related to language.
01:00:22.564 - 01:00:31.984, Speaker F: Yeah, I think he may not be able to, because he's a participant rather than a panelist. So I think he can only use the chat for us.
01:00:33.124 - 01:01:33.934, Speaker D: Something that Kevin said in our workshop was really the importance of context, of that being important. And I was thinking of Yossi in his talk today, talked about how he hand annotated that from the six bets, and then he said he actually solved that, but he didn't show how he solved it. I was really curious to think about that, on how we're thinking about these projects. It's like, where do you focus on? And I think the work on babies was also interesting because we sort of have an idea on humans. If we're trying to understand human language, at least we know ourselves, hopefully better than an elephant or a plant or a whale. But to really understand which parts to look, I mean, I feel like that's one of the. With the limited budget and limited time, which aspects of the context are most important to find? Are we for the whales, for instance, we're focusing really on audio.
01:01:33.934 - 01:01:54.774, Speaker D: Are we missing out by not including video? Is there parts of the context that we should be focusing on more? These are, I think, why hearing from so many different areas, that helps us kind of figure out where to search as we put our resources into figuring out context.
01:01:58.034 - 01:02:02.274, Speaker A: I think Kevin is on now, but you get to unmute yourself if you want to speak.
01:02:06.494 - 01:03:11.264, Speaker G: Oh, hey, I have to go in a couple minutes. But, yeah, I'm really interested in these questions, and I'm not so interested in whether x is language or not, because that's a fairly arbitrary distinction and very human centric like. Human language has these properties and this complexity, and we can ask how similar other species are to humans, but I'm more interested in the question of just the one that you raised, Shafi. How complex are these systems, and where does that complexity reside? There's all these different modules of human language that were discussed today, things like physical morphology, syntax, semantics, and so forth. And we'll be investigating all of them and seeing where they fall in the Chomsky hierarchy and just more generally, how complex they are, how much information they can convey.
01:03:15.404 - 01:03:16.504, Speaker D: Thanks, Kevin.
01:03:19.464 - 01:04:30.612, Speaker H: Just this discussion has reminded me of something I think I talked yesterday about. Coup rumbles, these rumbles that mothers give to their infants when they're comforting their calves. And an elephant, once I learned from an elephant that she was about to give birth, or that her mother was about to give birth from listening to her reach to her mother's vulva and give this same call. And I couldn't figure out what was going on and then noticed that there was this mucus coming from the mother, and she gave birth later that day. So, I mean, there are these complex things that are happening, that elephants are communicating to one another, and that I just feel like we're just scratching the surface there. You know, we see them doing all these very complex things, and then there's all this sound. So somewhere in the future, we're gonna.
01:04:30.612 - 01:04:34.396, Speaker H: We're gonna figure it out. But we have a long way to.
01:04:34.420 - 01:04:58.564, Speaker A: Go with that promise. I think maybe we should say goodbye, and we'll plan for another event. And again, thanks, everyone. It was really great. One of my best two days this summer.
01:05:01.504 - 01:05:02.844, Speaker H: Thank you so much.
01:05:03.224 - 01:05:04.176, Speaker D: Bye, everyone.
01:05:04.360 - 01:05:05.324, Speaker E: Thanks, everyone.
01:05:05.864 - 01:05:06.704, Speaker B: Thank you.
01:05:06.824 - 01:05:07.564, Speaker A: Bye.
01:05:08.544 - 01:05:09.024, Speaker H: Thank you.
