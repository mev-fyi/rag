00:00:00.640 - 00:00:54.766, Speaker A: My name is Jason Gaytonde, and I'm very excited to be telling you about our work on budget pacing and repeated auctions. This is joint work with Ying Kai, Lee, bar Light, Brendan Lussier and Alex Livkins. So, to motivate the results in this paper, I want to first talk about online advertising. When a user searches for a keyword on a search engine like Google or Bing, what will happen is that advertisers will take in data about that keyword and perhaps some other user features, and then they'll determine how much they want to bid for the right to show you their advertisement. And the goal of this is they want you to click on their link and potentially buy whatever they're selling. So, for instance, if a user searches for watches, you might see advertisements from advertisers like Apple or Chanel or Kohl's. Because of the frequency of these keyword searches, online advertising is now increasingly dominating the marketing landscape.
00:00:54.766 - 00:01:37.026, Speaker A: So here's just a couple of plots showing how the growth of revenue has exploded in the last two decades for companies like Google and Facebook. And nowadays, they're in the hundreds of billions of dollars. The way we can think about these ads is you can think about them as being allocated by repeated auctions for each keyword that is searched. You can think of this as a new repeated auction. And so what advertisers do is, as I said, they're going to submit bids to this Internet platform to determine whether they will actually be displayed as part of this view. And also these bids will determine what prices they have to pay to show their advertisement. And note that, as I said, these bids can vary by their keyword, they'll vary by what users are showing them, in particular, what sort of features they have.
00:01:37.026 - 00:02:50.212, Speaker A: So, different searches will have different values to the advertisements, but for an advertiser, they can't think of each auction completely in isolation. Typically, they will have a binding global budget constraint that will constrain how much they can spend over the course of an entire campaign. And this campaign management is actually a very complicated task, because, as I said, of the frequency of these searches, and there's a variety of options, like in payments and different ad formats and different kind of contexts that can appear to them. So the key practical and theoretical challenge in understanding these settings is how do advertisers optimally allocate budgets across time? In the learning theory community, this would be considered a special case of what's called bandits with knapsacks, because they need to choose how to bid at each time and they're subject to a global knapsack constraint, namely their budget, because this is a very difficult task for those dimensions that I mentioned. Major online platforms are nowadays providing automated budget management services that'll help advertisers adjust campaign parameters on their behalf. And this is kind of what it looks like. What advertisers will do is they'll declare preferences and constraints.
00:02:50.212 - 00:03:55.124, Speaker A: So, for instance, quantities like their budget and how much they're willing to bid in total for any given advertisement. And then they'll give this information to an automated bidder or a bidding algorithm. And that automated bidding algorithm is going to take in that information, and they're going to take in the values they have for each impression, and they'll map it into bids. Then an auction will be run to see which advertisements are allocated, and then the automated bidder will see the outcomes of these auctions, and then they'll use that input to update their campaign details, so they'll change their bidding strategy. And at the end of the campaign, the advertiser will see the results of this automated bidder in terms of their total spend and the aggregate results. And so because of this is a very flexible scheme, and it particularly lowers the barrier of entry for advertisers, these bidding agents are now near universally adopted across all mature advertising platforms. But because of the success of these bidding agents, we now have some very pressing theoretical and practical questions.
00:03:55.124 - 00:05:08.846, Speaker A: The first one is what can we say about the aggregate market outcomes when all the advertisers are using these automated bidding agents? In particular, what kind of outcomes arise and how good are they when all advertisers are doing this? Moreover, can we identify a class of algorithms that will both achieve good individual guarantees for any given advertiser, as well as good aggregate and overall market performance. So in our work, what we show is that the answer to these two questions is yes. Namely, what we show is that a class of what we call gradient based pacing algorithms will satisfy both optimal aggregate guarantees in terms of overall market efficiency, and they will simultaneously provide strong individual guarantees for each individual advertiser. And I will explain these terms a little bit more in detail in a couple slides. But when I say these aggregate guarantees, I'm talking about some notion of overall welfare. And in particular, we'll be talking about something called liquid welfare, which is kind of a standard notion of welfare in these budgeted settings. And one thing I want to stress now, and I'll emphasize the importance later on, is that our results are going to hold without any convergence of these learning algorithms.
00:05:08.846 - 00:06:11.126, Speaker A: And this stands in stark contrast to kind of previous equilibrium analyses of these kinds of settings. And I'll also mention this later on, but all of our results are going to hold for a wide range of auction formats, including the standard ones that you may have heard of, like first price, second price, and generalized second price. So before I can get into the formal model and kind of tell you our results in a bit more detail, I want to quickly talk about some related work in the setting. So, repeated auctions have been studied in many, many papers, and there's also a lot of work for repeated auctions with budget constraints. I want to just talk, though, about the literature that's most related to our work, which is about pacing and second price auctions. So when these advertisers have what's called quasi linear utility, and in second price auctions with budgets, it turns out that there's a pure Nash equilibrium in what's called pacing strategies. I'll explain this a little bit more later, but roughly speaking, what it's saying is that an agent can restrict their bidding strategies to a very simple form.
00:06:11.126 - 00:07:01.644, Speaker A: What they will do is they'll take their value for the given item at any given time, and then they're going to shade it by some shading factor or a pacing multiplier. Here it's going to be this. They're going to divide by this one plus mu k if they're bitter k, and they're going to do this for every single item that's up for auction. So they're going to have this single multiplier, and they're going to use it to shade all of their bids using their values. Furthermore, it's known that when you have a pacing equilibrium in these settings, it turns out that this liquid and welfare that I mentioned before, of all the agents, is actually at least half of the optimal liquid welfare for any allocation of the goods, and in particular for an optimal allocation that tries explicitly to optimize liquid welfare. And moreover, this factor of one half is optimal. This is not improvable.
00:07:01.644 - 00:08:00.798, Speaker A: However, these results are all about equilibrium and equilibrium strategies. And even though there's this nice kind of compact form for equilibrium strategies in second price auctions with this kind of utility function, in practice, however, agents are actually going to have to learn how to bid because they're going to receive their values online and they can't just look at them in hindsight and compute something. So agents are going to have to learn to bid using some sort of dynamic learning algorithm. And this is going to be the subject of our work, and this has actually also been studied before. So, Balsero and Gore recently showed that this kind of gradient based pacing will actually converge to these optimal equilibrium multipliers that I mentioned. But they rely on certain convexity assumptions of some associated lagrangian function. And this is actually a very, very strong assumption that it's not clear whether it's reasonable to assume it or not, and in particular for general valuations of the goods by the different advertisers.
00:08:00.798 - 00:09:03.354, Speaker A: It turns out that computing these equilibrium pacing multipliers is what's called PPad hard, which is just some complexity class. But the upshot is that you should not expect any efficient algorithm to converge to these equilibrium multipliers. And therefore, what this result is saying is that we shouldn't just take this convergence of learning to equilibrium multipliers for granted. What our work though, shows is that this gradient based pacing that we'll introduce in a second, it still obtains this optimal liquid welfare guarantee even without converging to equilibrium multipliers. So remember that because of this equilibrium guarantee, if it were the case that learning converges to equilibrium, then we would get this one half of the optimal liquid welfare. However, because of this hardness result, we know that potentially this cannot happen. But our work shows us that nonetheless, these learning algorithms will still get the same liquid welfare guarantees, even without requiring convergence of the dynamics.
00:09:03.354 - 00:09:53.500, Speaker A: And so, with that kind of in mind, I can now tell you about our model. So, the model of repeated auctions is as follows. We're going to have capital t rounds, and a platform is going to sell a unit of some good at each round. So think of it as, for instance, in our example, the right to show your advertisement to a new user. We're going to have n advertisers, each with an associated auto bidder, and each advertiser is going to have some budget b, sub I, for the ith advertiser. And this budget cannot be exceeded over the course of these t rounds of auctions. The way that the values are going to be generated for advertiser at each time is we're going to say that the values v one t through vnt at time t is drawn from some distribution capital f independently across time.
00:09:53.500 - 00:10:44.436, Speaker A: However, we're allowing these values to be correlated across components. And so what each agent is going to do is they're going to see their realized valuation. So agent k will see v sub KT, and they're also going to see the past results of these previous auctions, and they're going to use that information somehow to submit a bid, which we'll call b, sub kt. And then what the auction is going to do is it's going to take in those bids, we'll collect them into a vector b one t through BNt. And what the auction is going to do is it's going to allocate x sub kt units of the good to agent k, and it's going to charge p sub kt price as payments for their allocation. And these payments are going to be non negative. Now, these functions, x, sub Kt and p, sub kt, they will depend on the specific auction format.
00:10:44.436 - 00:11:36.044, Speaker A: So, for instance, if you've heard of first price auctions, the x sub kt, it'll be one. If you're the agent that submitted the highest bid, and the price you will pay is exactly the bid that you submitted. And the second price auction, the allocation rule, will be the same, but the price you will pay, if you're the winning bidder, is going to be the second highest price. So with that model of these repeated auctions, let me now tell you our notion of aggregate welfare. As I mentioned this before, it's known as liquid welfare. And an agent's liquid welfare for an allocation is just what their maximum willingness to pay for the allocation is, including their budget constraints. So, a bit more mathematically, given the sequence of valuations v and a sequence of allocations x, the liquid value obtained by an agent k is the following.
00:11:36.044 - 00:12:30.674, Speaker A: In this sum over here, the sum of this x, sub kt and v, sub kt, this x, sub kt and v, sub kt, their product is measuring how much of the good and how much value per unit good that the agent is getting at time t. So each summon here is telling you how much value the agent is getting from this allocation and given their valuations at time t. And then the sum is just taking it over all time. This is how much they would be willing to pay for their allocation if they had no budget constraints whatsoever. However, because they're only able to pay for their allocation up to their budget, we take the minimum of their budget and this sum over here. So that's the liquid value obtained for a single agent. And then we'll say that the liquid welfare of some allocation sequence x for all bidders is just going to be the sum across bidders of their liquid values for the allocation.
00:12:30.674 - 00:13:00.262, Speaker A: And this is the sort of welfare notion we're going to be studying in the rest of this talk. And now let me tell you what pacing means. As I've mentioned a couple times. So all pacing is, is a very convenient and simple class of linear bidding strategies. And I'll describe it a bit more in just a second. But these strategies have very strong theoretical properties, some of which I mentioned from this previous literature. And the idea is extremely simple.
00:13:00.262 - 00:13:51.554, Speaker A: Each bitter k is just going to maintain some pacing multiplier, which we'll call mu, sub kt at each time t. And this is going to be some number that's at least zero. And they're going to use this number to shade their values to give their bit at a given time. In particular, what they're going to do is they're going to take their value at time t and they're going to divide by one plus mu, sub kt. And remember, because this pacing multiplier is non negative, they're really just lowering their value to give their bid. So this is a very convenient class of linear bidding strategies because it's parameterized by a single number, and you just take your value and multiply by this number to obtain your bid at any given time. What do pacing algorithms do? Well, let's let p sub t as before, denote the round t expenditure of some agent, and let's write row to be their budget divided by the total time.
00:13:51.554 - 00:14:36.418, Speaker A: This is their target spend rate. If they could completely smooth out their spending, they would want to spend this much per round. And so what they'll do is if their expenditure in the given round is at least this target spend rate, what they'll do is they'll increase this multiplier, mu, sub t plus one, for the next round. And the effect of this is in their bidding strategy, it'll increase the denominator here and it'll make them shade more. And in particular, they'll bid less aggressively in the next round. If, on the other hand, their expenditure is less than their target spend rate, they'll decrease this multiplier. The effect of this is it'll actually lower the denominator, which will make this bid go up in the next round, so they will bid more aggressively.
00:14:36.418 - 00:15:32.236, Speaker A: That's all we mean by pacing algorithms. Now, gradient based pacing is just a particular instantiation of this that's been studied in the literature, like I said, from this balsera and gore work. And the idea is, just given some learning rate epsilon, we're going to update muti in this previous scheme using some very concrete updates. In particular, we'll take the previous multiplier and we'll subtract off Epsilon times this difference between the target spend rate and the actual expenditure in the previous round. And all we'll do is we'll just project it back onto the non negative numbers to make sure that this pacing multiplier is indeed greater or equal than zero. And this is called gradient based pacing because you can interpret this difference between the target spend rate and the current round expenditure as a sort of gradient for some lagrangian function. And so this is exactly how we're going to do the updating of the multipliers with all of this notation.
00:15:32.236 - 00:16:19.164, Speaker A: Now set, I can tell you about our main result on aggregate welfare of gradient based pacing. So the theorem is as follows. Let's fix any, what I'll call a core auction, which I'll say a bit more about in a second, but it includes all these important classes like first price, second price, and generalized second price. And let's fix any distribution capital f over the agent value profiles that show up at each round. And now let's suppose that each agent is going to use this gradient based pacing algorithm I just described to bid. And let's write x to be the corresponding valuation or the corresponding allocation rule that would be obtained if all agents use this gradient based pacing. So in particular, this x is going to be a random variable, because it depends on the realization of values drawn from this capital f.
00:16:19.164 - 00:17:41.904, Speaker A: What we show though, is that if you take any other allocation rule y, we have the following guarantee. We have the expected liquid welfare obtained through this gradient based pacing by all the advertisers is going to be at least a half of the expected liquid welfare obtained under this completely arbitrary allocation sequence y up to some sublinear term over here. So in particular, what we're saying is that this gradient based pacing is attaining at least a half of the best possible liquid welfare that could be obtained for the realized sequences of values in expectation. And so, like I said, this core auction, there's some technical definition, but I just want to say it's a very large class that includes first price, second price and generalized second price auctions. This result is allowing for arbitrary correlation across values in a given time period for the agents. And just to reiterate, this result and this factor of one half over here is the optimal liquid welfare guarantees that you could obtain even for offline equilibrium strategies. But as we said before, because we're now studying these learning algorithms, we do not require any convergence at all to equilibrium, and we're still getting this optimal liquid welfare guarantee up to the sub linear terminal.
00:17:41.904 - 00:18:44.334, Speaker A: So we've shown some really nice aggregate guarantees of these gradient based pacing algorithms. And from a platform's perspective, that's really important, because it's saying kind of, the market is relatively efficient. But then why should any given advertiser use it? To justify that, we need to provide some stronger individual guarantees that we can say for a given advertiser, it's doing something well to maximize your own welfare. So we also supplement this aggregate guarantees with new individual guarantees for this gradient based pacing with respect to the objective of value maximization, which is a standard objective in online markets. And so the setting is as follows. For these results, suppose that for a given agent k, their value v, sub kt, and the competing bids b minus kt are drawn from some distribution g at each time t. So this is a stochastic environment, because we're assuming that the competing bids are drawn from some distribution, possibly correlated with their own, with the agent's values.
00:18:44.334 - 00:19:29.214, Speaker A: We're going to also assume that we're in what's called a monotone bang per buck auction. So, informally, all we're kind of saying is that the value of the goods you obtain per your unit spend is decreasing in your bid. And this is a property that's kind of very natural, and it holds in all these settings we've mentioned before. First price, second price, generalized second price. And so our main theorem for individual guarantees is the following. Let's let capital y of mu denote the expected value of the bidding strategy that just bids, using mu as the pacing multiplier. So at each given time, pretend that you were to bid these, your bid is just your value divided by one plus mu until you run out of your budget.
00:19:29.214 - 00:20:39.898, Speaker A: What we show is that for any MBB auction, the value of the goods that is obtained by this gradient pacing strategy with the step size of order one over root t is going to be at least the best of this y of mu over any multiplier mu up to some no regret term, which is t to the three quarters. So this in particular, this is vanishing in average if you average over time. And so what I want to say is this is giving some new individual guarantees for value maximization that extends prior no regret guarantees for second price auctions with quasi linear utilities. And in particular, we extend it to kind of any mvv auction. And this is for the goal of value maximization, which is also a very important objective. And so with the remaining time, what I want to do now is just give a brief proof sketch of the first result I showed you, which is this aggregate welfare result. So, let's recall the statement of this result, it was for any core auction and any distribution capital f over the agent values.
00:20:39.898 - 00:21:30.928, Speaker A: Suppose that all agents use this gradient based pacing. Then for any other allocation rule y, we know that the expected liquid welfare obtained when all agents use gradient based pacing is at least a half as what would be obtained if we allocated using this arbitrary rule y up to some sublinear terms. And remember, this was an optimal factor. So, just to give a very, very brief glimpse of what the proof kind of looks like, the key idea is that what we're going to do is we're going to track the evolution of these pacing multipliers over time. So in this figure, remember, the pacing multipliers go from zero to infinity. So pretend we have plotted them and, you know, we plot them over time, and we plot the evolution of the muse. What we're going to say is let's call an epoch to be an interval t one through t two.
00:21:30.928 - 00:22:28.624, Speaker A: We're going to call it an epoch if the pacing multiplier is equal to zero at the start of the epoch, and it's strictly positive for every other number in this interval. So we've drawn in this, highlighting the epochs in this evolution of news. What one can show using the form of this gradient based pacing is that on any epoch, your spend on average is approximately this target spend rate row, which is budget over time times the length of this epoch. And this comes from the definition of the gradient based pacing. With a lot of work. What we can do is we can show that you can get this win win analysis and it kind of reasons as follows. If your pacing multiplier is strictly greater than zero, often this plot would be kind of shaded a lot in terms of epochs, then the agent must be spending a lot by the lemma.
00:22:28.624 - 00:23:37.344, Speaker A: And in that case we can actually lower bound the liquid welfare obtained by the agent. On the other hand, if this pacing multiplier is equal to zero, often that agent might not be spending much, but they are bidding their true value at each time. Just given the, if you remember the definition of mu, and in particular that means whether or not this agent is winning the item, some bidder must be paying this value in a core auction. And what we can show is using these core auction properties, that we can also show that there's high liquid welfare. And we kind of show that no matter what this evolution looks like, we can kind of get this win win analysis and get this optimal factor one half. And so with that, I just want to conclude with a few future research directions. So one is kind of can you generalize this result to broader classes of learning algorithms beyond this gradient based pacing? And also kind of, what's the interplay between these individual guarantees and liquid welfare guarantees? Can you kind of improve one while keeping the other? In non truthful auctions like first price auctions, you can ask for better regret bounds than this t to the three quarters.
00:23:37.344 - 00:24:34.944, Speaker A: You can ask for regret with respect to stronger benchmarks than what we mentioned, which was kind of this best pacing multiplier in hindsight. And also, you can ask about whether you can make these algorithms that the autobidders are using robust to the case where advertisers are also being strategic so they know the strategies that the autobidders are using. They might be tinkering with the auction format. Can these autobidder algorithms be made robust to these kind of strategic considerations? So, just to remind of the key takeaways, what we do is we show that when all agents are simultaneously applying this gradient based pacing, we show that the liquid welfare obtained over the course of these dynamics is at least half of the optimal expected liquid welfare. And remember, this half is optimal. And as I stressed before, our result does not require any convergence of these dynamics whatsoever. And it'll hold for any core auction, which is kind of a very broad class of options.
00:24:34.944 - 00:24:55.184, Speaker A: And also, like I said, these algorithms are not just giving these good aggregate guarantees, they're also giving these very strong individual guarantees that justifies their use for any given advertiser in terms of these no regret performance bounds with respect to the best fixed pacing multiplier in hindsight. And with that, thanks so much.
