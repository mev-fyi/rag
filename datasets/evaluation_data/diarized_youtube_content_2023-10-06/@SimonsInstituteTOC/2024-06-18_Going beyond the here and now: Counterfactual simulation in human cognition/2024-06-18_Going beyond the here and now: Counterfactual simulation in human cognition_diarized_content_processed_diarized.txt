00:00:00.760 - 00:00:21.038, Speaker A: The first talk will be from Toby. And take it away. Thank you so much. So the title is still the same as it was, like, you know, in the book, that the content has changed quite a bit. I tried to adapt a little bit to what people were talking about and also to what I'm kind of most excited about. So, yeah, my name is Toby Gestenbeck. I lead the causality and cognition lab at Stanford.
00:00:21.038 - 00:01:16.596, Speaker A: And one of the things that we're interested in is building a computational framework for understanding responsibility. So responsibility itself, I think, is an interesting concept to study. And I also like it because there's many tricky questions that we need to solve along the way. For example, we need to be able to answer questions such as what causal role somebody's action played in bringing about the outcome, and also what the action that they took reveals about the kind of person that they are. For that first question, we need some kind of intuitive theory of how the world works so we can relate the actions that somebody took to the outcomes that resulted from those actions. And for that second question, we need some intuitive theory of how people work so we can reason backwards from the action that we have observed them taking to the kind of mental states that gave rise to those actions, like the beliefs, desires, and intentions and so on. And a critical part in both of those, for both of those questions is, I think, that people have this ability to build mental models of the physical and the social world, and that they use these mental models in a variety of ways to get there.
00:01:16.596 - 00:01:55.546, Speaker A: And I will illustrate this with sort of three different examples, one of which is about judging causation. The other one is how we can use evidence from different sense modalities to figure out what happened. And the third one is the role that causal abstraction plays in how we are learning and interacting with the world. Another way to think about it is that the first part is about how we answer why question, why something happened. The second part is about what what happened, how we figure out what happened in the past. And the third one is how we resolve how something happened, and also how that the way in which we resolve that affects how we're generalizing. Okay, I should also warn you, I'm a psychologist, as you know, so there's going to be quite a bit of audience participation this time.
00:01:55.546 - 00:02:20.430, Speaker A: So get your hands ready. So. And we're going to start straight away. So here's an experiment in which I asked participants just to say whether they thought that ball a caused ball b to go through the gate, and you're going to be my participants now. So I'll show you the clip. Let's do a show of hands. Who thinks that, yes, ball a caused ball b to go through the gate in this case.
00:02:20.430 - 00:02:56.466, Speaker A: Okay, so you've maybe been sufficiently primed, right. In my experiments, of course, I don't tell them anything about counterfactuals, but you might not be surprised, given the title here on the slide. That's what I think is going on, right when people are judging why ball B went through the gate in this case. So they look at what actually happened, that the balls collided and that b ended up going through. But that in itself is not sufficient that you have to go beyond that by simulating what would have happened in a relevant counterfactual situation. So you have the ability to basically rerun this in your mind, imagining that ballet hadn't been there on the scene, and then using your intuitive understanding of the physical domain. That's simple here, to simulate where we would have ended up.
00:02:56.466 - 00:04:00.804, Speaker A: And the basic idea is that the more certain you are then that the counterfactual outcome would have been different from the one that actually happened, the more inclined you'd be to say that, yes, this happened because of that, or a caused b to go through the gateway. So we've already learned a little bit about Perl from Alison's talk and also from Dan Yammuns talk today. So Pearl has developed this framework for how we could capture the causal structure of the world in terms of things like causal baseness or structural equations, and how we could have a do operate on top of that to think about the consequences of interventions. And that's been very much inspiration for the kind of work that I've been doing. But I've also then started looking in domains like people's intuitive understanding of physics that isn't as easily represented in terms of structural equations, where then the idea is that people's models of the world is maybe a little bit more similar to a physics engine, and that we need to think about what the kind of counterfactual interventions are that people can perform on these kinds of probabilistic computer programs. In this case, you might have the ability to imagine removing an object from the scene and rerunning it. So these models are probabilistic, and because they are, they allow us to make quantitative predictions about the kind of judgments that we think people might make.
00:04:00.804 - 00:04:29.712, Speaker A: And the probabilistic part here comes in. In that you don't know for sure what would have happened in this kind of factual situation, right? You have to use your intuitive understanding of physics, so you don't have access to the ground truth here. But instead, you use your physics simulator that we've also seen in Kelsey's talk and others to maybe generate some samples of what would have happened. So you might say, oh, maybe the ball would have ended up there. Maybe it would have ended up there. Maybe it would have ended up there. But in this case and all the simulations that I ran, I was pretty sure that ball B would have missed if ball a hadn't been there.
00:04:29.712 - 00:05:20.528, Speaker A: So I should be certain that, yes, a caused b to go through the gate. But if it's a different situation, such as this one here, I might have more uncertainty about what would have happened in the relevant counterfactual situation here as to whether Ball B might have missed or might have gone through anyhow, even if ball a hadn't been present in the scene. And we can then use that model, or also just ask a group of participants to say, what would have happened if ball a hadn't been there to predict the kind of causal judgment that they should reach. And what we found that when we do this is that we find a very close correspondence. So what you see here on the x axis is just an experiment that was run on a separate group of participants, where we asked them this kind of factual question, what would have happened if Bolle hadn't been there? And on the y axis, we asked them, do you think that ball a caused Bobby to go through the gate? And what you see that there's a very close correspondence between these two. We can do more than that. So I was just.
00:05:20.528 - 00:06:06.544, Speaker A: These days, maybe some of you have seen this science. I've been walking over in the mornings to this menu, and there's this kind of ad here for icar berkeley on seeing possibilities. Because the model that we're building, in some sense, makes predictions about the process by which people are reaching their answers, namely, by going through this process of counterfactually simulating. So sort of exploring the possibility of what would have happened if Bolly hadn't been present in the scene. So we did that by putting people in front of an eye tracker, showing them these kind of simple clips that I showed you just before, and then asking them different questions again between participants who write the kind of question that they were asked to answer. In this case here, we asked a participant after the clip, whether they thought that a prevented Bobby from going through the gate. And I'm going to show you their eye movements, playing the clip at half speed here.
00:06:06.544 - 00:06:53.466, Speaker A: And what you see is the participants not just looking at Ball B. They're trying to simulate where Barbie would have ended up if ball a hadn't been present in the scene. And these kinds of simulations, we don't see sort of all the time. We see them specifically when they're asked a causal question, and we just ask them some question, for example, about at the end of the clip, and they know what question they're going to be asked to answer at the end, whether the ball completely missed the gate. We don't see these kind of fast saccades to where Ball B would have ended up if ball a hadn't been present in the scene. So they specifically do it for answering causal questions. There was a quick interaction for those of you who were earlier in Dan's talk about, okay, what about counterfactuals? Is this really sort of counterfactual reasoning? Because the eye movements are sort of happening before the collision is happening, and I don't have time to go into it, but just a quick promotion for myself.
00:06:53.466 - 00:07:58.082, Speaker A: So there's a paper where I explored that there is an important difference between hypothetical and counterfactual reasoning. And then if your goal is to explain how people judge why something happened, and because of what it was that it happened, you really need these kind of factuals, and the hypotheticals aren't enough. We've been using this kind of model in many different domains. One is still kind of billiard ball domain, but just a little bit more complex. Philosophers love this because it's a case of double prevention, where b prevented a from preventing e from going through the gate. We can also look at cases of omission, right? Where you might ask, oh, is it the case that you think that ball b went into the gate because ball a didn't hit it? And you could imagine what would have happened if Ball A had hit it, and to what extent you think the outcome would have been different in that case, or even in situations where nothing is happening at all, like when you're judging here, for example, to what extent this black block is responsible for these other blocks staying on the table, where you could imagine the kind of thing that you're doing is kind of like playing mental jenga in your mind, but imagining, okay, what would have happened to that scene if the block hadn't been there, and judging responsibility in this way. So, for a long time, I was stuck in physics world.
00:07:58.082 - 00:08:30.354, Speaker A: I was post stuck in Josh Tettenenbaum's lab, and they were really interested in intuitive physics at the time. But I'm a psychologist by background, and I was more interested in social psychology. People are much more interested in billiard balls, I guess. But I just didn't have the kind of skills that I needed to actually try to think about how people simulate what other agents could have done. But now that I'm a professor and I have students who have the relevant skills, I started looking into, again, these sort of phenomena that in some sense more interesting. And this is in the case of helping and hindering. And the students I highlight here is my PhD student, Sarah Wu, and then also our research assistant, Amshruti Sritar.
00:08:30.354 - 00:09:10.978, Speaker A: And you're going to see Tomer next. This is one of Tomer's papers from 2009, where they looked at helping and hindering and where the goal was to see whether people could infer from these movements, like how these different agents are interacting with each other in this grid world, what the intentions of the agent were, whether they had positive intentions, basically placing a positive utility on the other person's utility or a negative sign on the other person's utility. So trying to minimize their utility. And this kind of worked really well in this case. But there's also, like, an important distinction between intending to help or hinder and actually helping or hindering. I don't have children yet, but this is still the example that I'll use here. And maybe for those of you who have children, can kind of appreciate the difference.
00:09:10.978 - 00:09:39.344, Speaker A: So when you go grocery shopping with your child at some age of the child, they might be intending to help you, but they're not actually helping. They're making it worse. It takes longer in the relevant counterfactual sense, it would have been easier to do it without them, even though they had the intention to be helpful. So we need counterfactuals here. So we explored this also in a simple kind of grid world setting where we had this kind of red agent here. And their goal is to get to the star. There's the blue agent here who wants to either help or hinder red.
00:09:39.344 - 00:10:13.420, Speaker A: You don't know that at the beginning, and importantly, red doesn't know that at the beginning. There's some static walls, and there are also these objects here that only blue can move around. And so let's see what happens in this clip. So Hollywood ending. And then we show people clips like that. And in one of the groups, we asked him, how responsible was blue for red success, like in this case? And the idea is that people, at least in part, are doing something similar to what they're doing in this physical domain. Right? Again, they have now some intuitive domain theories, like a mental model of what's happening here.
00:10:13.420 - 00:11:13.586, Speaker A: That's, again, maybe a probabilistic program, but in this case, it's not just some kind of physics engines, but it's an engine that's actually now where a planning agent, where agents are making decisions about, you know, how to interact with the world and also might be making inferences about each other as they're going along by trying to infer what the other agent's goals are, for example. But we can then use that same model, though, to simulate what the causal role was that somebody, some action played, and also to infer, just like Tomer did in his work, what the intentions were that some agent had. And we postulated that both of these things, just like I was showing earlier, are important when we're assigning responsibility to others. Role matters, but also it matters. What they did tells me about the kind of person that they are. And just focusing here on this causal attribution part to make the analogy really clear, right, in the physics domain, we observe the physical world, and we have a mental model that allows us to simulate what the relevant counterfactuals would have looked like in that case. And then, in a similar way, we can do that in the intuitive psychology domain, too, right here.
00:11:13.586 - 00:11:50.964, Speaker A: Now, simulating how maybe the red agent would have moved if the blue agent hadn't been present in the scene. We show participants a whole bunch of different kinds of cases. Here are just a few of those. And when the first thing that we did, we said, oh, let's do the same thing that we're doing in the physical domain. So we're just looking at the counterfactual. So how, to what extent do you think that, you know, the red agent wouldn't have succeeded if the blue agent hadn't been there? And try to use that to predict people's responsibility judgment, you know, that works okay, ish. But doesn't look like kind of as perfect as it did earlier in the physical domain, where there was really this one to one mapping basically between kind of counterfactual simulation and causal judgment.
00:11:50.964 - 00:12:50.844, Speaker A: But what we found is that if we include also this other part of the model, where we assume that participants are making inferences about what the intentions of the agents are, then we're getting a very good fit to participants judgments of responsibility across the different cases, suggesting that when people are judging responsibility, they care about the causal role that the agent played and also about what the actions revealed about the kind of agent that they are. I have a paper out. This is, I promised you the last paper plug, but that just came out in trans and cognitive sciences, that lays out that framework, this kind of actual simulation model, and as it applies to kind of physical reasoning as well as social reasoning. Okay, so that's part one, how people use counterfactual thinking or simulations to judge causation and attribute responsibility. So in part two, now I'm going to focus on multimodal inference. So this is a bit more like Sherlock at a crime scene, getting different information and trying to figure out what happened. So in this case, maybe who was, you know, who was guilty of the crime.
00:12:50.844 - 00:13:12.832, Speaker A: And in some, there's a lot that we get from vision alone, right? So. And our intuitive theories with the vision. But if you see those examples here, you might infer what happened to this coke can. Or if you're close enough, you might realize that this was a dog that picked the sausage, you know, out of this frying pan. And you can still see the paw of the. Of the dog there. And sometimes, you know, we only have visual information, but sometimes we have auditory information, too.
00:13:12.832 - 00:13:49.934, Speaker A: So this is in the case of the assassination of JFK, where some people said they'd heard one shot and other people had said that they had heard multiple shots from different directions, leading to different theories about what happened in that case. So I took a much simpler approach and developed this kind of Plinko domain here. Also, being surrounded by Josh Tannenbaum and Josh McDermott certainly said, like, oh, maybe sound is kind of important for figuring out causality. And get ready for some participant, for some audience participation again, but I'll show you how the world works first. So this is the world Plinko box. This ball drops, you hear some sounds as collides. It plays a beep sound, you know, when it's being dropped.
00:13:49.934 - 00:14:28.174, Speaker A: So just like for you, we did this to our participants. We just showed them a few clips like that. And then in one of our experiments, we just asked them to predict where the ball is going to land. And I'm going to use this creepy hand, move it slowly from left to right, and you just clap when it's there where you think the ball is going to end up. My ways of getting, like, lukewarm applause, you know, before the. Before the end of the talk. In our experiment, we asked participants to click ten times, right? So this would allow them also to give us bimodal distributions if they wanted to.
00:14:28.174 - 00:15:07.778, Speaker A: So this would be an individual participants response, for example, and then we take all the clicks and kind of put some smooth Gaussians on top of that, and that's not the distribution that we want to explain. Like, in this case, the ground truth in this case was actually here. So people tend to underestimate, in the setup at least, how far the ball ends up going. So we can predict people's judgments in this case. Again, using a similar approach, like I did with the counterfactuals earlier, by assuming that people have something like a physics engine in their mind, but with uncertainty. But we here assume that they might have uncertainty exactly about how the ball is being dropped, and then also when it collides with obstacles, how exactly that collision is going to be playing out. And now, by generating many simulations from this noisy simulator, we also get a distribution like that.
00:15:07.778 - 00:15:38.154, Speaker A: That's not just the ground truth, which in this case is this path here. And this model that only has basically these two different sources of noise actually captures people's judgments about where the ball is going to end up, in this case, very well. So we have a relatively good model now of where people are thinking the ball is going to end up basically a good forward model. But we wanted to do a Sherlock style, which means we want to go in the other direction. We want to see the ball at the goal and then figure out where it was dropped from. So your turn again. The hand's going to move, and you clap when it's at the hole where you think the ball was dropped from.
00:15:38.154 - 00:16:07.940, Speaker A: Clap multiple times. Okay, three is the winner here. In the experiment, we just given these text boxes, people put in numbers that added up to 100%. Right, in this case, indicating their beliefs in the different holes. So how do we do that task? Well, basically, by assuming that people are forward, simulating where they think the ball would end up if it was dropped in that hole. Right. And so we can then see, okay, there's some simulations of where the ball would end up if it was dropped here or here or here.
00:16:07.940 - 00:16:31.966, Speaker A: Right. And then we can then just compute the likelihood of the ball ending up here under these different hypotheses. And then, assuming a uniform prior over the different holes, can compute the posterior. And in this case here, that's what the posterior from the model looks like, the little circles. And this is what people's judgments are. So sort of like you, I think most clap for hole three in this case, because, yeah, most of the time, the ball ends up here, very close to where it actually was. But this was vision only.
00:16:31.966 - 00:17:21.174, Speaker A: Right. Finally, we're entering the kind of multimodal part where I'm covering the box up first, and I'm just playing the sounds as the ball is being dropped. So, listen up, and now I uncover the box, and I'll ask you again, where do you think it was dropped? Okay, so, in the experiment, most people now are pretty sure that it was this one in the middle, because there's a beep sound at the beginning. There's a boof, the ball colliding with the obstacle and the ball landing in the sand. And the idea is, now you just have these two sources of information, but you still have your visual information that you can use to simulate where you think the ball would end up. But you also have this auditory information here that they are encoded very simply by basically just looking at the time points at which collision happened. And we can now compare the ground truth time points at which we had the collision sounds.
00:17:21.174 - 00:17:49.414, Speaker A: Two, where we would have. Where we would have expected to hear the sounds if the ball was dropped there. In this case. Now it's only this one here that gives us these sort of two collision sounds at the right time, whereas these possibilities don't. So the model now shifts most of its beliefs to hole two, and people did that, too, in this case. So, in this example. And again, we ran a whole ton of these, but here you're getting a very different answer, depending on whether you only had visual evidence or whether you had visual and auditory evidence as well.
00:17:49.414 - 00:18:57.958, Speaker A: So we really like this paradigm, and we've been exploring it also by trying to get more detail on the kind of way in which people are doing the tasks a little bit closer, maybe also to some of the detailed ways in which Kelsey has been exploring her tools domain. So we put people in front of an eye tracker, and then this time, only had them press a button, like whether it was hole one, two, or three, in different conditions of the experiment. So now we're getting eye movements, we're getting reaction times, how long it takes them to decide, and also responses of where they think it's going to end up. And we built a model that now basically simulates these different possibilities and now tries to actually capture what whole people think it is, how LONg it's going to take them, and where they're going to be attending to. Where the idea is, what they're going to be attending to is, in part, driven by the possibilities that they're simulating. We've also been exploring it from a developmental perspective, where the key innovation was to put elmo behind the box. So now you have to figure out where Elmo dropped the ball and that developmentally, and we're able to see an interesting trajectory in young children's ability to integrate both the auditory and visual information to figure out what happened.
00:18:57.958 - 00:19:46.596, Speaker A: So in this case, it was only around seven or eight years of age that children seemed to be answering in a way, and that they were taking into account the auditory information, in this case that they heard. And finally, getting a little bit closer to Sherlock, we've also been exploring this account. Now, in a setup where you have multiple agents and you want to kind of figure out who did it. So simple whodunit kind of scenario, where there are these two people here and one of them took a night snack from the fridge, and you want to figure out who it was. And you might have some visual evidence, like some cookie crumbles here or something on the floor, but also sometimes you might have auditory evidence, too. So you hear the way in which the agents interacted with the world when they opened the door, opened the fridge, and so on, and you want to use that information to figure out who it probably was that did it. This kind of thing we can also do with GPD four.
00:19:46.596 - 00:20:41.344, Speaker A: And why not? We tried it. And when you look at the aggregate results, this is basically how well people are doing and correctly inferring the person who did it, if they only have visual evidence, if they have auditory evidence, or if they're getting both. And you see overall that there's a sort of, oh, yeah, seems like GPT four, at least the one where we're giving it the kind of visual information in terms of more like a scene graph rather than the image. If it just got the image, it wasn't really able to deal with that. But if you then look at the trial by trial kind of results, then you see that at least the responses that GBD four gives are not very well correlated with human responses. So even though the overall pattern looks kind of good, the by trial correlation, you know, looks very bad. And a model that we've been building that, again, relies on simulation, basically trying to simulate what paths agents could have taken and the kind of auditory signal that you should have gotten if the agent had that path captured participants judgments much better.
00:20:41.344 - 00:21:30.560, Speaker A: Okay, so that was part two on the multimodal inference stuff. So where it seems like people are capable of integrating this evidence for multiple sense modalities here, in this case, vision and sound by relying on the mental model of the world and simulating and being able to integrate the two pieces in that kind of way. Yeah. Yeah. So actually, in all the experiments we do, and then in the eye tracking one, its sort of self, the trial ends, like, as soon as they press a button. And in the ones that we did when we ran it online, where we asked them to give, like, different kind of probabilities for the different holes, it was also they could use as much time as they wanted to. Were you interested in what would happen if it was timed? Yeah, yeah, no, it's interesting.
00:21:30.560 - 00:22:09.218, Speaker A: I mean, that that model that we. That I only kind of briefly alluded to, that we build of the eye tracking that has, you know, it's also a little bit similar, actually, to Kelsey's work and that we're building some priors into it. We're saying, like, oh, in general, if you have very little time, you're probably just going to go with something like, well, the hole that it was closest to. So that's a good prior to start off with. Maybe that's the first hypothesis you want to explore. If the ball is all the way on the left, maybe try with the left hole first, and then kind of go through the different options in that way. So my sense is that, yeah, they might still be able to do it if you only show it to them very briefly, partly maybe by being able to hold it in working memory if you don't mask it.
00:22:09.218 - 00:22:28.474, Speaker A: But it's also possible that in those settings, they're then more relying on kind of, you know, relational information rather than kind of simulating. Yeah. In a very detailed way. Yeah, sounds good. Yeah. Yeah. So that should be enough for this part here, the causal abstraction part.
00:22:28.474 - 00:22:47.676, Speaker A: And so that I want to highlight. My student, actually not my student. He was just visiting over the summer, Steven Shin. I wish he was my student, but he's going into med school. But he was working, you know, very hard and well on this, on this project. And this is motivated also, you know, one more plug for my next. For the next presenter.
00:22:47.676 - 00:23:32.224, Speaker A: Tomer has this paper and ticks where they're really exploring this idea. To what extent people's mental model of the physical world might be similar to the kind of physics engines that we have in our computers these days. Right. And the importance also of abstraction, like, in this way that sometimes you might not be representing, you know, objects at a high level of detail, but only good enough for what we need for the task at hand. So in this project, we were interested in whether people are building abstractions that are abstractions basically of their world, models that are suited to the task at hand, and whether the kind of models that they're building, how they're explaining the data, then shows up in the way that they're generalizing beyond the data that they've seen. Okay, so very simple task. Again, here's a block on a ramp and no audience participation this time.
00:23:32.224 - 00:24:00.828, Speaker A: So don't worry. But what participants had to do is predict whether this group is going to cross the line, yes or no. And so they would say something or maybe no. At the beginning, you don't have, you know, you don't know, but then you see something. Okay, this one didn't make it, and so on. So it was just basically two different ramps, right? A yellow ramp and a blue ramp, and then two different kinds of blocks, a red block and a black block. And in this case, here, it was the color of the block that determined whether or not it was going to make it, you know, past the finish line.
00:24:00.828 - 00:24:24.984, Speaker A: But as you also see that the ramps would affect it, too. So in this case, for a blue ramp, they're sliding a little bit further than they are, um, for yellow ramp in this case. So this is a very easy task where people can learn to do that very quickly. Right. This is just across 16 trials, averaging the, the accuracy on this task. But then to participants surprise, afterwards, we asked them a different question that they hadn't been asked before. So kind of surprise task.
00:24:24.984 - 00:25:11.520, Speaker A: And in this case, we just asked them, okay, which one shows where this cube is actually going to end up in this case? And so something that they didn't have to answer before and where we thought, are they going to be making systematic mistakes now? Because they've learned to ignore, essentially part of the information that is there that wasn't relevant for the task that they had to do and acquired the right abstraction, which is, oh, basically the thing that I need to care about is the color of the block. Different experiment. It was the color of the ramp that mattered and used that to make their predictions. And so what we're seeing is sort of exactly that. So if you're asking participants for this one here, with the correct responses, it's going to end up in position one. They're not sure whether it would end up in position one or position two, but they know it's not going to be in position three or four, which is on the other side of the line, and similar kind of patterns of errors in these other cases. So systematic errors that seem to encode.
00:25:11.520 - 00:25:35.464, Speaker A: Okay, they didn't encode it at the level of detail that we presented it, but just for what they needed to do the task. So when we did this, somebody said, okay, well, you have all this stuff about world models and intuitive physics, but maybe you don't need that. It's all just perceptual. You're encoding it perceptually with respect to that line. The dynamics don't matter. And also you're talking about causality and stuff like that. But maybe it's just associations that people are learning between the kind of beginning state and the end state.
00:25:35.464 - 00:26:01.426, Speaker A: So those were our challenges that we then dealt with. And for much of the work that we do, we kind of pre registered our predictions. And in this case, I just drew something up. I mean, we also wrote the predictions in text, but this was something that I drew on the board and then I was very excited. So this is kind of a. Imagine a drum roll, what the results actually looked like. And this is one of the best predictions I guess I have had of what would come out of the study.
00:26:01.426 - 00:26:26.014, Speaker A: You don't know yet what these bars mean. I'm going to tell you in a second, but I was very excited to see that exactly what we thought would happen. That's not always the case. So we did a slightly different version of this task, same setup, but this time we didn't show participants the kind of dynamic animation in between. We just showed them an initial state. They say yes or no, and then they would see the final state of where the block ended up. So that doesn't really matter.
00:26:26.014 - 00:26:52.166, Speaker A: In terms of this kind of surprise test performance, it looks kind of exactly the same as it does as it did with the video task earlier. So maybe the person was right. The physical dynamics doesn't matter. Maybe it's just about associations. But one thing that we can now do with this different setup is we can also have a generalization task where we show them something that they haven't really seen before. It's the same blocks, but on a different ramp now that they haven't seen before. And we just asked them where you think the cubes are going to end up.
00:26:52.166 - 00:27:22.794, Speaker A: And so now we predict, in a situation, right, where they learned that the red one, you know, goes further than the black one, they're going to say, oh, yeah, it's going to be this one here. The red one's going to be further than the black one. And why did you even put the ones here on the left hand side? That's weird. Whereas if we now put two different blocks that they haven't, two of the same, two blocks of the same kind on these two different ramps that they've seen, before, they didn't really need to pay attention to the ramp right. In their prediction task. So we don't, we think, oh, they're going to think it's going to be on the right side, but they don't know which one's going to go further. So they don't know between these two here.
00:27:22.794 - 00:27:41.150, Speaker A: And that's exactly what we found. Right. So when it's the different blocks, they know the red one's going to go further than the black one. When it's the different ramps, they don't know which one's going to go further. And now when we flip it, now we show them another generalization task, same participants, and we put the ramp the other way around. Right. And now still, duh.
00:27:41.150 - 00:28:04.960, Speaker A: Okay, well, the red one's going to go further, so you're going to think this one's going to happen. And in the other situation, when you don't know which ramp has more friction, let's say you don't know which of these two is going to happen, but it's going to go this way. Now in the generalization. And again, that's what we find. So now comes the important twist and I think hopefully have still one or two minutes. That was what we call the ramp forward condition. And now we flip it around.
00:28:04.960 - 00:28:39.722, Speaker A: We just have the ramp be backwards at the beginning. So you see trials like this at the beginning when you're asked to make your predictions and for some associative count or something or some perceptual account, probably shouldn't matter which way the ramp faces at the beginning. So you have a task like this and the block ends up here in this case. But if you're doing more than that, if you're doing more than just learning associations or kind of encoding the information perceptually, if you're trying to tell some causal story of what happened, then here you have a sort of simple causal story at hand. It's like the Newton story. Oh, it's just blocks, sliding down ramps. But you don't have that story available to you in the side here at the bottom.
00:28:39.722 - 00:29:25.124, Speaker A: It's not just blocks sliding down ramps because they would go on the left hand side. So maybe something else, maybe the block is pushed up by some invisible force towards the finish line or it's pushed up the ramp. Might be some other hypothesis that you could generate about the data. In this case, you need some Newton plus. And first, what we show here now when you do the surprise task that we asked participant at the end in this ramp, backward condition, you still see the same kind of errors that you saw here. So there's not all that much of a difference there. But now you'll see that it's going to look very different than the generalization task because now if we show you, if you've learned that the blocks are flying to the right hand side and we show you this one here, you're going to think, okay, well, just like in the other stuff that I've seen earlier, the red one's going to end up further towards the goal than the black one.
00:29:25.124 - 00:30:17.382, Speaker A: And when it's the two things where you didn't learn about the friction of those ramps, you're going to be uncertain between these two, but you're still going to think it's going to go on the right hand side, just like you saw earlier in training. And again, that's what we're finding in this case. So clearly predicting they're going to go here in this case and here, uncertain between these two options here. But now, if you flip it right now, if you basically put it in the intuitive direction from earlier, now people kind of don't know because it depends on the story that you generated of the data that you've seen. If your story was like, oh, yeah, some invisible force is flicking this towards the goal, then you're going to think, this one's going to happen here. But if you think like, oh, no, these ramps are just shooting up blocks, then you think that thing here is going to happen on the left hand side. And when it sees two different ramps, you might be totally lost because you don't know, is it going to go left, is it going to go right? And also which one's going to go further? So we should see them answering sort of across all of these different options.
00:30:17.382 - 00:30:59.772, Speaker A: And again, that's exactly what we found. So bimodal distribution here, they either think the red one's going to be further on that side or on the left hand side, and then it's the two different ramps, they're all over the place. They don't know whether it's going to go, you know, which one's going to go further. So this was this deconstructed plot showing, predicting whether it's going to go left or right, depending on what they had learned and the kind of story that they constructed of the data. So overall, I think this last project shows that people are building these causal models that are suited to the task at hand. So they're encoding the information that they need and then also the causal model that they are building basically the causal story that they're telling determines the way in which they're going to be generalizing to new situations. So this was why, what and how, and I think I'm at time.
00:30:59.772 - 00:31:13.944, Speaker A: So these are the conclusions. So thanks for your attention. Yeah.
00:31:14.524 - 00:31:31.394, Speaker B: So if you think of Kahneman's observation that people sometimes pass a quick judgment and sometimes reason about it, I would have expected that you find bimodal distributions, people who are willing to spend more time thinking. Did you see any of that?
00:31:32.174 - 00:31:39.314, Speaker A: Are the bimodal distributions. Now, which ones were you interested in? Because there were, I guess, two different projects that I talked about.
00:31:39.614 - 00:32:10.664, Speaker B: I'm wondering if everyone is equally willing. So, okay, what Canada would do? It would have some subjects. So the subject would come hungry to the experiment, and to some he would give water with sugar, to some water with sweetener. And the ones who had sugar were willing to invest more thinking and would get different types of answers from other people. And here there is some level of commitment to solving the problem. So I would have expected that some of your subjects would have tended to take shortcuts.
00:32:10.764 - 00:32:37.708, Speaker A: Yeah, yeah, that's certainly true. Like, for example, like in that. In that Plinko task. Right. There's relatively big inter individual differences, like in terms of, you know, how long it is that people take to make a guess, like where the ball was dropped. Even in the. When we did the eye tracking with the billiard balls, we also saw that some participants really engaged a lot in these sort of what we take to be counterfactual simulations, whereas some people not as much.
00:32:37.708 - 00:32:51.636, Speaker A: Right. So I think there is, yeah. A different degree of, you know, engagement and. And you could probably modulate it by basically, you know, rewarding people like, to different amounts, like in. In the task. Right. And so, so, yeah.
00:32:51.636 - 00:33:13.428, Speaker A: And also, I guess, related to Josh's question earlier. Right. There's certainly a mode in which we can solve these kind of tasks and that doesn't rely on this more detailed simulation. And that might be something, again, depending on how engaged you are, how. How. How much you want to solve the problem, you're going to do more of that. So, yeah, one more.
00:33:13.556 - 00:33:20.228, Speaker C: Okay, so here you're relying on a huge amount of prior knowledge.
00:33:20.276 - 00:33:21.140, Speaker A: That's right. Yeah.
00:33:21.212 - 00:33:34.634, Speaker C: Right. So, you know, gravity, understanding of physics, frequent friction, understanding what kind of problems psychologists usually give to people. In this sense, there's huge amounts.
00:33:34.714 - 00:33:35.394, Speaker A: That's right.
00:33:35.514 - 00:33:53.842, Speaker C: Another setting would be to just completely blank slate. You create your own world with your own physics where everything is completely from scratch. And then you basically try to teach the people to do simulation in that completely new world. What would you expect to find in that case?
00:33:53.938 - 00:34:30.974, Speaker A: Yeah. So maybe I can relate it closest to the first project that's about causal judgments. So the thing that I would expect there at first is like, when your model of the world is not particularly good, yet when you don't have the ability to simulate what the relevant counterfactual looks like, that should be reflected in your causal judgment. But if somebody asked you, was this because of that? You should say, I don't know. I'm aware of the fact that people sometimes make strong claims even though they may not have that capability. But that's at least what the model is suggesting, that there's a tight link between basically how good a simulator you have of the world and the causal judgments that you're going to be reaching because of that. So that's at least one.
00:34:30.974 - 00:34:37.094, Speaker A: One prediction in that instance. Okay, so we should move on to the last talk. So let's thank Toby.
