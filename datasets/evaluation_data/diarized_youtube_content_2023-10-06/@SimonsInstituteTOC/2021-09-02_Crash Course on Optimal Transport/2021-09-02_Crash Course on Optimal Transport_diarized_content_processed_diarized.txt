00:00:00.240 - 00:00:06.634, Speaker A: Before. So I think at least now you can see the screen. Sure.
00:00:07.614 - 00:00:26.634, Speaker B: Okay, so, hello, everyone. Welcome to the third day of the bootcamp. Today we'll be on. Okay, so today we have a day about optimal transport. And Nikkei Kobeli accepted very nicely to give us a talk on a crash course on human transport. So thank you, Nikayla.
00:00:27.464 - 00:00:55.104, Speaker A: So, thank you very much for the invitation to this very nice program. I'm honored to give this short lecture on optimal transport. So, it's a crash course. So I will cover quite a lot of basic material, but without entering too much into details and proof, I will write on the iPad, as I were in, let's say, as with a whiteboard. I hope you will see it. Please, if you don't see something, tell me to zoom. I will do it.
00:00:55.104 - 00:01:47.874, Speaker A: So, the outline of this presentation is the following. I will first start with a brief introduction to the problem of optimal transport. Then I will introduce the Kantorovich duality that is a very important tool in optimization also. And then I will arrive to the core of the optimal transport theory. That is, theorem is one of the fundamental theorems in the theory. Then I will pass to the definition of vast stein distances, busses and space. And the goal is to give an informal introduction to gradient flows in reverse spaces, the Benamubregnier formula, to finally arrive to a formal introduction of Otto's calculus to show that the heat equation is the gradient flow in waste test time of the entropy.
00:01:47.874 - 00:01:49.684, Speaker A: Can you hear me well?
00:01:50.064 - 00:01:50.804, Speaker B: Yes.
00:01:51.464 - 00:02:13.344, Speaker A: Okay, perfect. So, let me just give you some references. Some are quite classical. There are two books by Cedric Vilani. One is topics in optimal transportation is the first one he wrote in 2003. Then I read some parts of these books by Ambrogio Giglia Savare. That is about gradient flows.
00:02:13.344 - 00:02:56.668, Speaker A: In particular, gradient flows in metric spaces and in the space of probability measures. So it's a very, very complete, let's say, reference on running flows much more than. Much, much more than what I would say today. Then, more recently, there have been this contribution by Filippo Santambroggio that is called optimal transport for applied mathematicians. So that is centered in the topic. And then Cuturi Pere wrote in 2018, a book on computational optimum transport. And finally, Figal Diclaudo wrote a book that is called an invitation to optimal transport, buses and distances and gradient flows in 2021.
00:02:56.668 - 00:03:32.082, Speaker A: And this is, let's say, introductory book to the topic. So if you then will need more precise references, just let me know and I can provide some more detailed references. But essentially, in all these books, you will find what we talk about today in a way or another. So I'm trying to move. Okay, so let me start with a very brief overview. Okay. Can you see one? Yes.
00:03:32.082 - 00:04:26.224, Speaker A: So, the problem of optional transportation has been first introduced by Gaspar Monge, french mathematicians, actually a french engineer in 1781. In a way. Just to answer to a very practical question. The question is, assume one extracts soil from the ground to build fortification. What is the cheapest possible way to transport the soil? So imagine that, like here, we have this amount of. Of soil that we want to extract, and we want to bring it to build the fortification. So we want to transport, in a sense, here we don't have probability measure, we don't see probability measures, but we can think that this disposition of soil is distributed like a probability measure.
00:04:26.224 - 00:05:26.764, Speaker A: Mu and this construction, this building is a probability density. Nu. And we want to find the cheapest possible way to bring this mass from one place to the other. And cheapest, in which sense? So, to formulate this question rigorously, one needs to specify how much one pays to move a unit of mass from the point x that I pictured here to the point y. And in Mont's case, the ambient space was r three, and the cost was simply the euclidean distance, so the travel distance. Now, this was the most classical formulation of optimal transport. Then many, many years passed by until Leonid Kantrovich, in the forties, essentially in the 40th of last century, proposed a new, let's say, more modern version of the optimal transport problem.
00:05:26.764 - 00:06:16.374, Speaker A: Let me remark that in monge formulation, the optimal transport problem is deterministic. So, I'm saying that there is just one point where the mass that is in x can only go in a point that is, y equals t of x. Okay? In 1940s, Kantorovich instead proposed a different, a different notion of optimal transport that could take into account some non deterministic situation that are very common in real life. Let me present it with an example. So, suppose that we have several bakeries. B one. Bn.
00:06:16.374 - 00:07:15.246, Speaker A: Here, I pictured three bakeries, because b one, b two and b three are bakeries, and several coffee shops, c one, c two, c three and c four. So in, let's say, cj goes from one to one. These bakeries are located at points xi, and they sell a quantity of bread that is alpha. Let me write it. So this is bread produced by b one. And this is the position, I mean, the label that I do to this, to this bakery, it's a label. So these coffee shops instead, are in positions and they buy a quantity of bread that is beta j.
00:07:15.246 - 00:08:27.804, Speaker A: So let me now change. So I hope you can still see well. So, okay. Alpha I, larger than zero is the amount of bread produced by the bakery bi beta j, larger than zero is the amount of bread that is bought needed by the coffee shop cj. So we assume the first assumption is that the offer is equal to the demand. So the demand is sum from j that is equal from one to m of beta j, and offer is sum from I that goes from one to n of alpha I. And we normalize everything to one because we want to deal with probability measure in the end.
00:08:27.804 - 00:09:35.034, Speaker A: So now this problem is incompatible with munch formulation, because each bakery may supply bread to multiple coffee shops, and one coffee shop may buy bread from multiple bakeries. So, for this reason, Kantrovich introduced a new cost formulation. So, given c of xi y, this is the cost to move one unit of mass from xi to yj. It looks for matrices gamma ij I that goes from one to n, and j that goes from one to m, such that we ask for the, for the following assumption. A gamma ij, is it large enough? I'm writing.
00:09:35.374 - 00:09:36.438, Speaker B: Yes, yes.
00:09:36.606 - 00:10:19.014, Speaker A: Okay. Gamma j is larger than zero. That means that the amount of bread going from xi to yj is non negative. B for every I sum over j that goes from one to m of gamma ij is equal to alpha I. That means that the total amount of bread sent to the different coffee shop is equal to the production c for all j, we have that sum from I that goes from one to n. Gamma j is equal to beta j. So the total amount of bread, both from the different bakeries is equal to the demand.
00:10:19.014 - 00:11:31.242, Speaker A: And finally, d, that gamma j minimizes the total cost, that is sum over ij from one to n, and then gamma ij c of xi y. So, okay, so the total transport cost is minimized. So it's interesting to observe that the constraint a is a convex constraint, while b, c and d are linear with respect to gamma j. So linear. So in a sense, we are just trying to minimize a linear problem with convex or linear constraint. So, this already shows a bit why this formulation can be very handy. Let me just come back 1 second to the picture.
00:11:31.242 - 00:12:35.646, Speaker A: Just to show here, I pictured a quick example, and I show you how to compute the gamma ij. So. So, for example, gamma three three is alpha three because the bakery b three only sells bread to c three, and gamma two three is beta three minus alpha three, because it has to compensate the fact that the coffee shop number three receives bread. Also from the bakery Petri, is it clear? This simple example, can I go on? I assume yes. So if there are no question, I go on. So, optimal transport has attracted a lot of interest over the years in application due to its connection to several areas of mathematics. And in particular, all the applications are heavily linked to the geometry and also to the choice of the cost field function c of x and y.
00:12:35.646 - 00:14:54.158, Speaker A: So, for example, let me just mention few applications. So, there have been application to the study of the Euler equation, isobarimetric inequality, sub bullet inequality, etcetera, but also in the study of evolution, pd's like for whose medium equations or dt of, um, this is a bullet equal to lambda of u to the m or dtu equal divergence of grad w, convolution uu, and so on. And for example, in these situations, the, the most common, commonly used cost is the quadratic cost. So c of xy equal to x minus y squared, or for example, so energy, this energy, or also c of xy equal x minus y to the p with p strictly larger than one. But also, there have been several applications to probability and to kinetic theory. So, kinetic theory is my main, let's say, area of research. And so, for example, the optimal transport, in particular with the cost either c of xy equal x minus y, or with the quadratic cost x minus y squared, have been used a lot for the study of transfer equilibrium, for kinetic equations or for stability, for example, for the Blasto Poisson equation, and to study the derivation of this equation for plasma physics from the n particle models.
00:14:54.158 - 00:16:04.258, Speaker A: So it's a very useful tool, and also for the study of Ricci, Corbacher or romanian manifold. And here the cost is still quadratic, but is in most cases the riemannian distance d of x y square. Okay, so of course there are much more applications, but I'm sure you will see plenty of application in the next lectures. Let me now go on with the notation. So, the notation I'm using is really classical. You have already seen it in previous lectures, but just for simplicity. So for simplicity, I will focus on the case of the euclidean quadratic cost for most of the time.
00:16:04.258 - 00:17:23.234, Speaker A: So, cost equal x minus y squared on RD. So this will be our model case. And in particular, although I will consider the case of Rd in general, I indicate by x the space where the first measure lives, and the source measure is indicated most of the time by mu, and y is the space where is the target measure that is mostly indicated by mu. So you think about the case x equal to y equal to rt. And another important notation is p of x. That is the set of probability measures over x. So let me start now with several basic definitions.
00:17:23.234 - 00:18:27.278, Speaker A: So, the most important definition will be the one of transport map. But to define a transport map, let me first define a push forward. So definition push forward measure. So let t from x to y. Then nu is in p of X mu. No, mu is in p of X. We define the image measure or push forward the measure t push forward mu in p of y such that t push forward mu of a is mu of the pre image of a for a border set in y.
00:18:27.278 - 00:19:22.918, Speaker A: Let me explain with a picture. In the case of transport map. So if now mu is in p of x and nu is in p of y, a map d from x to y is called transport map. If t push forward new is equal to new. So let me show with a picture this. So this is x, this is the space y, this is the measure mu, and this is the measure nu. So we take a borehole set in nu.
00:19:22.918 - 00:20:43.608, Speaker A: This is a so t cell sends nu on to nu. So t is a transport map. If we have that t is the push forward of nu on nu. And the condition is that mu gives to the, to the bottle set t minus one of a the same weight, let's say the same mass that mu gives to the border set a. Okay? So the natural, the natural first question that we have to ask ourselves is whether these transport maps always exist or not. And unfortunately, one of the main problem we will see, one of the main problem in Mont formulation compared to the Kantrovich formulation, is that optimal transport map and transport map in general don't always exist. So remark so given mu and nu, the set of maps such that t push forward mu is equal to nu.
00:20:43.608 - 00:22:13.854, Speaker A: So the set of transport map from mu to new may be empty. Think for example to the case of mu equals delta in x zero. So if mu is a Dirac delta centered in x zero, then the t push forward mu is necessarily the Dirac delta in t of x zero. So unless new is Dirac delta, then the set of transport map from new to new is empty. Okay, so let me now go on to another fundamental definition. That is, that is the one that will be used to define the counterovich problem in optimal transport, and is the definition of coupling or transport plan. So definition or transport plan.
00:22:13.854 - 00:23:35.380, Speaker A: So the term coupling is mostly used in probability, but we can use both term. We can use both term. So we call gamma a measure in the, in the product space x times y, a coupling of mu and nu. If, if mu is the first marginal of gamma and nu is the second marginal of gamma, that means that the push forward of gamma, the push forward of gamma with the first projection is mu and PI y push forward gamma is nu. So PI x is the function that associates to the point x y x. So it's the projection of the first variable and PI y is the projection of the second variable. So here, let me do another piece.
00:23:35.380 - 00:25:11.518, Speaker A: Picture. So suppose this is gamma, this is x and this is y, and if we take the projection by x of gamma, we will find mu. And if we take the projection, the projection by y of gamma, we will find nu. And this is, this is fundamental for what we will say in the following so I recall that gamma is a measure on the product space. So we denote by gamma of mu and new the set of all couplings of mu and u. So now an important remark is that given mu and nu, the set gamma of nu nu is always non empty. For example, the measure mu tensored nu is an admissible coupling.
00:25:11.518 - 00:26:55.004, Speaker A: So it's a coupling of mu and nu. So belongs to gamma of nu. And this is another big difference with the notion of transport math, that a coupling between two measures always exist, while a transport map from two measure is not always necessary that it exists. So again, on this subject of transport map versus transport couplings, so it's important to note that if t from x to y is a transport map from mu to nu, so satisfies that t push forward mu is equal to nu. Then there is a natural way to induce a transport coupling between mu and nu using the mapped. So we consider the map identity times t that goes from x to x times y and associate to x the graph of t. Then gamma T defined as the push forward of mutual the push forward of identity times t of the of the measure mu is a probability measure on x term y and belongs to gamma of Nu Nu.
00:26:55.004 - 00:27:53.756, Speaker A: So it's a coupling. So any transport map in use a coupling gamma T. This is another fundamental notion, and this was covering essentially the basic definitions that we needed to now be able to explain Monge and Pantarovich formulations for optimal transport. So monge so assume, now again new in p of x, new input Y. Assume C to be the cost that goes from X times Y. Assume the cost to be bounded from below and continuous. So now a little disclaimer.
00:27:53.756 - 00:28:49.082, Speaker A: I will, I will only mention theorems and formulation in the, in the most, let's say in the easiest possible setting. But several things can be relaxed and put in greater generality. But for simplicity, I will just talk about continuous cost here. So, the Monge and Kantrovich problems can be stated as follows. So, the total mon transportation cost between new and new is the infimum on all transport maps of the integral on x of c of X t of x D mu of x. So, the infimum is taken on all the transport maps from Nu to Nu. And I will call this m.
00:28:49.082 - 00:29:55.448, Speaker A: So this is the Monge problem, while the counter of each problem. So the, the total Kantorovich transport cost between mu and Nu is the intima on all the transport plans of this quantity that is integral on x times y c of Xy, d gamma of Xy, such that gamma is a coupling between nu and Nu. And you see also that the Kantorovich problem is much more symmetrical than the mon one. So there are a lot of indication that solving the counter rabbits problem is easier than solving monge problem. In fact, for example, already monge problem consists in minimizing the transportation cost among all transport maps. And this set of transport map we have seen that can be empty, while Kantorovich consists in minimizing on a set that is never empty, for example. So we call optimal transport map.
00:29:55.448 - 00:32:16.260, Speaker A: So, optimal transport map or coupling the map or coupling that realizes Cm or Ck, so the map cabinet. Okay, so now we have a definition of optimal transport map and optimal transport coupling. Another remark is that we know that if t push forward mu is equal to nu, then the associated coupling gamma t can be taken considering the graph of t push forward mu. And in addition, t and gamma t have the same cost. Therefore, the total montage cost between mu and nu is always larger of equal to the total contribution cost. Okay, now some natural questions at this point consist in asking whether minimizers exist. So, do minimizers of m or k exist? And if yes, can we characterize them? So, can we say something about their structure? And let me start from the existence of an optimal coupling.
00:32:16.260 - 00:33:36.102, Speaker A: From what I have already mentioned, in general, solving the counterovic problem is easier than solving the monge problem. So also for existence, a natural strategy is to first prove existence of an optimal coupling and then try to prove to prove existence of an optimal transport map. So, theorem. So let see a cost that is bounded from below and continues mu in p of x nu in py. Then there exists an optimal coupling gamma bar in gamma of mu mu. And then once we have existence, the second natural question is about uniqueness. So, is gamma bar unique? And if in general we have found this optimal coupling, we know that every transport map induces a coupling.
00:33:36.102 - 00:35:11.454, Speaker A: But here we can ask ourselves, is gamma bar induced by transport map? So in general, just to have some intuition of this about the possible answer to this question, let me give you a couple of examples. So just to, okay, so, examples, okay, assume that mu is a delta in x zero. So we have x zero and we have a delta, so we have mass one here, and nu is one half delta y zero plus one half delta y one. So there are other two points, y zero and y one. And here we have mass one half and mass one half. So there is only, let's say the only thing that we can do is to send from x zero one half to y zero and one half to y one. So it exists unique, a minimizer that is gamma bar equal to one half delta x zero y zero plus one half delta of x zero y one.
00:35:11.454 - 00:36:35.044, Speaker A: But of course, as we said, this is not induced by transport number. So we already answered to the second question. That is not, let's say it's not necessarily induced by a transport map. Second example, we assume x equal to y equal to two reals. The cost is quadratic, and let's put four points so x one is in zero zero x two is in one one. Then we have, okay, this should be y one is in ten and y two is in zero one. So and suppose that mu is now one half delta x one plus one half delta x two.
00:36:35.044 - 00:38:13.364, Speaker A: Nu is one half delta in y one plus one half delta in y two. So how can we send new onto new, so we can start from x one and we can say, okay, let's now send a quantity alpha in zero one half in web one, say include, and then we send the rest. So one half minus alpha to y two, and let's do the same from x one. So here we send alpha still in zero one half, and then we send the remainder one half minus alpha or on y one. So we see that the, the couplings given by alpha delta x one y one plus one half minus alpha delta x one y two plus one half minus alpha delta x two. Y one plus alpha delta of x two, y two with alpha in zero one alpha, these are all optimal. So these are all optimal couplings.
00:38:13.364 - 00:39:21.156, Speaker A: And this rules out the uniqueness without further assumption. And this was in a sense predictable, because the Kantrovich problem is a linear optimization problem over a set with convex constraint or linear constraint. So a priori, since the problem is linear in gamma, there is no reason to expect uniqueness without any further, let's say hypothesis. And this is what I wanted to say about optional transport maps and optimal transport couplings, and about the existence of an optimal transport coupling. And let me now present very briefly the counter which duality. That is a very powerful tool from both the theoretical and numerical point of view. So I will not directly use the contravict duality today, because, simply because I'm not entering in the proof.
00:39:21.156 - 00:40:59.574, Speaker A: But you will probably see dual problems in. In the following lectures, I will give first the general theorem of contravict duality, but also the version that was originally proposed by Kantrovich for the distance cost function, because that version is really simple and useful. So let me now go to Kantorovich duality. So, as many of you know, if you have a linear minimization problem with convex constraint in general, it admits a dual problem that is formulated as a maximization problem. And this is just because of now theorems in convex analysis. So, in this specific case, we have seen the cost continuous bounded from below, and now assume that the inf on gamma coupling between mu and nu. So this is essentially the Kantorovich problem.
00:40:59.574 - 00:41:51.822, Speaker A: I'm sorry, this is not the x, this is the gamma is finite. Then we can reformulate this problem in the following way. So we can, this is our primal problem. I'm sorry, let me write it below, actually. So we have the minimum on gamma in capital gamma mu nu of the integral of c d gamma. This is our primal problem. This is equal to the max on phi of x plus c of y plus c of x y larger than zero.
00:41:51.822 - 00:43:03.904, Speaker A: We'll put a plus here just to show that it's the same equation of integral of minus phi d mu plus integral of minus c d nu. And this is our dual problem. So we can transform a minimization problem or in a maximization problem. And you can see that now, for example, you have that new and new in the primal problem appear because they are in the minimization. So we are minimizing on the couplings between mu and mu, while here they appear in what I'm minimizing. So in the integrals, and in the same way, the cost in the primal problem is in the integral, while here the cost is now in the constraint. And this is very common in convex analysis.
00:43:03.904 - 00:44:45.944, Speaker A: But let me now show the special case that was special case that was introduced by Kantrovich himself. So again, we have x equal to rd, and now the cost is x minus y. So the kilian distance, then the minimum on gamma were coupling between mu and u of integral on x times y of the euclidean distance between x minus x and y in the gamma. This is equal to the maximum over phi of all phi one lip sheet function of what of integral over x of phi integral over y of phi d nu minus integral over x of phi d nu by one leap sheets, I mean simply that phi of x minus phi of y is smaller than x minus y for all xy in x. So it just lip sheets with constant one. And this Kantrovich duality in the case of the cost x minus y is very simple. So it was really widely used.
00:44:45.944 - 00:46:06.014, Speaker A: In particular, let me mention that this contrabanduality played a big role in improves in kinetic theory, because one of the first proofs that used optimal transport in kinetic theory has been the proofs of mean field limit in the case of Blasov equations of and in general the workforce of lasso equation by dubrushing that in the seventies used the bounded leap sheet distance or the one Wasserstein distance. And in general this contravicture duality to show that if you have two solutions of this equation of this class of equation, then you have stability. And this was very useful since then. But by the way, Dubrushin was also the person who assigned the name of Wasserstein distances to the distances of optimal transport. And this was a bit unfair, because actually Kantorovich introduced those distances. But Dubrushin at the time was not aware of his work. So Dubrushni is the culprit of what why we really use Wasserstein distances instead of Kantrovich distances.
00:46:06.014 - 00:47:25.054, Speaker A: And let me now go on. So we are now ready for the brainier theorem that is really a cornerstone of the optimal transport theory. I will state the Benier theorem only for quadratic cost, even if it applies also more generally to several strictly convex costs. But let me state it in the simplest possible way. So let x be equal to y be rd, the cost be quadratic. Suppose that the integral over rd of x squared d mu plus integral over rd of y squared d nu is not infinite. Then assume that mu is absolutely continuous with respect to the Lebeb measure.
00:47:25.054 - 00:48:59.290, Speaker A: Then we can say that there exists a unique optimal transport coupling between mu and new gamma bar. And moreover, what we can say is that gamma bar is actually induced by a map, so is induced by map t t. Now is an optimal transport map. But on top of that we have a, we have a structure that is the t is the gradient of phi for phi convex. So not only we have an optimum transport coupling, but we have that the coupling is induced by the map and that the map has actually a lot of structure because it's the gradient of a convex function. And you know that convexity in obtaining optimization problem is really a key, let's say feature, and in particular the fact that the optimum transport maps are gradient of something. So the role of the, of the gradients of functions is really central in the optimal transport theory.
00:48:59.290 - 00:49:53.094, Speaker A: And we will see it also when I will formally introduce autocalculus. So there is, as I said, there is an analog for the cost x minus y to the p with p strictly larger than one. In this case, we have to require that we have bounded p moments. So instead of the power two, we shall have then here the power p and here. And in that case we have existence and uniqueness of an optimal coupling gamma bar, and is still induced this gamma bar by some transport t. But in this case t is not the gradient of a convex function, but is more complicated. It comes with a more complicated formula that is less useful in general than the one that we have in the quadratic course.
00:49:53.094 - 00:51:03.504, Speaker A: So let me give you now another definition. So we want to arrive at the definition of bussest and distance. So first we need to define the space of probability measures with finite p moment. So, oops, probability measures with finite p moment. So again, let x equal rt. Let p be in one plus infinity. Then we call capital p of p of x the set of all probability measures on x sigma, such that the integral over x of x to the power p in the sigma of x is finite.
00:51:03.504 - 00:52:37.800, Speaker A: So this is also referred to as moment condition. So this is okay. And now that we have the space that will be the protagonist of the following, I can finally define the buses and distances. So, definition the buses time distance probably will change page and so on. So given, given now two measures mu and nu in the space of probability measures on x with finite p moment, then the p bussest and distance between mu and nu is defined. We take the infinum on all the couplings between mu and nu of what of the integral on x time y. In this case, it's x of x minus y to the power p, the gamma of x and y to the power one over p.
00:52:37.800 - 00:54:02.496, Speaker A: So we see that this is the power one over p of the optimal transport cost by Kantorovich. So we have that the relation of these distances with the optimal transport problem is very crystal clear, because it's really defined in this way, and it's not very easy to prove. In particular, the triangular inequality. But we can prove that the p bussets and distance so the w so busses time p is a distance on the space of probability measures on x with finite p moment. So we will refer to pp of X with the distance busses time p as Bussestyn space. And these fastest and distances are really exceptionally useful and in many sense easy to work with. In particular in the case of the two buses and distance, and of the one busses and distance that are used for different situations.
00:54:02.496 - 00:55:10.010, Speaker A: In particular, a very nice feature of the busses and distances is that they are very weak norm. But we have this theorem that tell us that suppose that we are in a compact set. For simplicity, we set everything in compact sets, then so p larger than one. If we have mu nu, mu mu n a sequence of probability measures on x and mu a probability measure on x, then we have that the Wasserstein distance, the p Wassestein distance, is equivalent to the weak star convergence. So saying that mun weekly star converts to mu in this case is equivalent to say that the p bussestent distance between mu and mu tends to zero. So on compact sets, the busses and distance convergence is equivalent to the weakstaff convergence. And.
00:55:10.010 - 00:56:16.674, Speaker A: And this is also a reason why these distances are very practical with respect to other distances between probability measures. Think also about the simple fact that, for example, if you have two Dirac deltas centered at two points, x zero and x one, if you consider for example, the total variation distance between these two deltas, the total variation distance is always equal to two, no matter how close are the supports of these deltas, while the Wasserstein distance instead between two delta of Dirac is essentially the euclidean distance between the support of the deltas. You can do this as an exercise. So, it's a distance that is very sensitive also to atomic measures. And this is great for, for many applications. For example, this is great in kinetic theory when one want to derive equations from particle models. So, but come back to optimal transport.
00:56:16.674 - 00:57:33.814, Speaker A: We now are left for this first hour with the construction of geodesics. So x now is again equal to rt. Assume that mu zero and mu one are probability measure on x. So we have mu zero, we have mu one, and we want to construct, so we have a transport map t. And we want to construct a geodesic. So we want to in a way interpolate mu zero and mu one using this optimal transport map t. Okay, so if t is the optimal transport map between mu and nu, I'm sorry, mu zero, mu one in this case it's easier mu one.
00:57:33.814 - 00:58:51.424, Speaker A: Then the geodesic mu t takes the form. So a map t t push forward mu zero, where this map tt is the linear interpolation between the identity map and t. So t t of x is defined as one over t minus t x minus t t of x. So in and so by geodesic we can always have in mind if we want to think about the geodesic between two points, is the shortest path between these two points. Informally, and with this definition of geodesic, we can say that in this case that t is an optimal transport map. The geodesic is also said to be a constant speed geodesic. So the geodesic mu t is constant speed.
00:58:51.424 - 01:00:13.344, Speaker A: That means that if you take the p Wasserstein distance between mu s and mu tilde, this is t minus s, the Wasserstein distance between mu zero and mu one for all s and t between zero and one. And. Okay, I've not mentioned yet, but mu t are all measures in the space p p of x. So the space of probability measure with finite p moment. And let me briefly mention that in the case that we don't have at our disposal, so it doesn't exist, an optimum transport map, there is also a formula that is equivalent, but in the case of having an optimal coupling. So a formula that is expressed in terms of the coupling and I don't really know with the delay at the beginning how much time I have, but I can stop here for this first hour and then we can start again with the introduction to guardian flows.
01:00:13.884 - 01:00:44.524, Speaker B: Okay, thanks Nikera. Are there any questions? Is there any question from Jr? Maybe I have one. Just on the Vernier theorem, like the, you have a cost which, which is strictly convex function of x minus y. So you said that we have the existence of the optimal transport map, but that is not necessarily the gradient of the convex function, right?
01:00:44.864 - 01:00:45.552, Speaker A: Yes.
01:00:45.688 - 01:00:48.524, Speaker B: So when is this straight only for the two.
01:00:49.304 - 01:01:38.284, Speaker A: So in this case. So if the cost between x and y is for example, x minus y to the power p with p strictly larger than one, then we don't have the t is the gradient of a convex function. But we have something like t is equal to grad phi modulus of grad p minus two with fee conducts, something like that. All right, I'm not really, really sure of the formula, but there is a formula that relates the optimum transport map with gradient of functions again. So you're welcome.
01:01:40.264 - 01:01:48.504, Speaker B: So thank you. Will be back at 1115 because we're a bit late on the schedule. Sorry for that. Thanks, Miguel.
01:01:48.544 - 01:01:52.384, Speaker A: Again, no thanks to you. See you soon. Bye.
