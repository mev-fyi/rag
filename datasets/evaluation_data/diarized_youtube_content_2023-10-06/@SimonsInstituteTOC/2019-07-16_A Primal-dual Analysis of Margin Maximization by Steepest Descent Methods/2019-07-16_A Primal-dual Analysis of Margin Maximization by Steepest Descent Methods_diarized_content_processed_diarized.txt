00:00:00.360 - 00:00:22.110, Speaker A: Okay. Yeah. The next talk is by Matthew Snowguschi on margins and winning descent, I guess. Okay, thanks for having me. And thanks also for Jason for talking a lot about margins. In fact, we've heard a lot about margins, both in this workshop and the last workshop. So all of everything I'm presenting today is joint work with Zoo AG.
00:00:22.110 - 00:00:54.040, Speaker A: And always, whenever I mention him, I don't really know how to introduce him. Some kind of co conspirator or collaborator or friend or co researcher. And today I'll say that he's my co enthusiast. We're very enthusiastic about margins. We just have, like, tons of little lemmas lying around. And when I was thinking about what to talk about today, apologies for the non informative title, which allowed me to talk about anything I wanted. So in the boot camp, we had Nati talk for 3 hours about these sorts of questions.
00:00:54.040 - 00:01:41.244, Speaker A: Jason just talked for an hour. Surya is going to talk. So when Zu and I, when we like, once a month, when we contact each other and say, oh, you know, so what's the progress on? So how's deep rel. How's Max margin proofs for deep value going? Sometimes we just get frustrated and we say, wait, why does margin maximization even make sense in the linear case? So, in contrast to the other three talks that talked about these phenomena, my main goal today is actually very, very modest. I hope I can pull it off, which is, I want everybody to leave the room with a very clear intuition on why margin maximization happens even in the linear case. And let me explain what I mean by clear intuition. And of course, it's down to me to pull this off.
00:01:41.244 - 00:02:19.122, Speaker A: If you stop me in the street and you just say, with me not knowing anything about you, you just say, why can I minimize a convex function with gradient descent? I'll just say, put the marble on the hill and watch it roll down. So, hopefully for today, we can get the same thing for margin maximization, a very kind of intuitive explanation for what's going on. And just to be clear, thank you also, Misha, for accidentally setting up my second slide. So, let me just be clear about the phenomenon. I start with some initialization, in this case, orthogonal to the max margin direction. So we can just look at this. I've got plus points, minus points.
00:02:19.122 - 00:02:47.250, Speaker A: This is the origin. Max margins that way. So you run gradient descent or whatever you want on the logistic loss, exponential loss. I'm going to use some kind of exponential tail on everything today. It goes that way. So I hope again, from today, everyone will feel very comfortable with this phenomenon, that it'll be very intuitive why this happens, to be clear, erm, could be anything in a cone of solutions. All of those would get zero training error.
00:02:47.250 - 00:02:50.544, Speaker A: Yeah, but your data here is two dimensional.
00:02:50.624 - 00:02:51.244, Speaker B: Right.
00:02:51.624 - 00:03:16.484, Speaker A: Your data is in two dimensions. Right. What is that picture? Oh, let's revisit this. And it draws three dimensional data or four dimensional data. So it's here. And anyway, let's. Let's come back to this, because my question is, what happens when you do it in three? Okay, so the pop quiz for me for how will I present this today is if in 40 minutes, you're convinced about me having to answer your question.
00:03:16.484 - 00:03:27.634, Speaker A: Okay, let's see. In 40 minutes. Okay, so that's the question we want to say why? So here's the. Here's the. Yeah. 38. So here's the.
00:03:27.634 - 00:03:50.434, Speaker A: Yeah, I have 60 minutes still. So here's the plan for today. The first part, I just want to give some overview of some motivation and discussion of margins. Of course we heard about it, so much. So for the intuition, I'm going to give you two intuitions. One is in the primal and one is in the dual. And then finally, I'll give some brief nonlinear comments.
00:03:50.434 - 00:04:30.106, Speaker A: Kind of by complete miracle this morning I was debating pinging JSON and asking to see a slides. But somehow, magically, not only do we complement each other, but actually somehow exactly the set of things I say about the nonlinear case is exactly complementary to what Jason said. So, thank you, Jason. That went very nicely. Okay, so first, let me tell you about margins and also complimenting Jason's slides. So Jason very kindly gave the usual introduction to deep learning as solving cancer and famine and all these things, and not costing any energy in the process. So let me give another motivation for why to study deep learning, which is this alien spaceship appeared.
00:04:30.106 - 00:04:48.786, Speaker A: This alien spaceship with this bizarre technology. I don't know why any of this stuff works. And we break into this thing and we start taking things apart. This is exactly how I studied deep learning. It's like, oh, is the following true? Every time I think the following is true, I run the experiment. I'm wrong every single time. And then I try to prove a theorem.
00:04:48.786 - 00:05:20.964, Speaker A: Sometimes I most say I fail, sometimes I succeed. But for me, my approach to deep learning is really like investigating some bizarre alien technology that just happened to crash land into my gpu. And so in this philosophy, I view math as a lens. And what's kind of exciting is that, and has been exciting about this program this summer, is we don't know which math we should use. Many people say things like, oh, uniform convergence is busted. But we can also use physics. We can use all sorts of weird stuff to analyze what's going on in this alien spaceship.
00:05:20.964 - 00:05:56.024, Speaker A: And the lens I'm going to use today is margins. So, while I'm still. I still have those 30 minutes for Misha. So let me just test everybody's beliefs. So, here I have the same data, okay? I have one data set, and I've run three different svms with three different kernels and obtained three solutions. These are contours, okay? These are contours of the real value outputs. And I want everyone in the audience to guess what the contours for the relu look like.
00:05:56.024 - 00:06:14.188, Speaker A: I just want everyone to guess. What do you believe the contours look like? If you want me to say some things about width. Apologies. I don't actually remember. I think I did depth two and width roughly order n. So feel that it's, like, somewhat in the parameterized. It's going to get.
00:06:14.188 - 00:06:32.400, Speaker A: It's somewhat in the open settings. So what does everyone believe? The best. Just guess. Make it commit to a guess for what you think the contour looks like. I should say that this is one of the cases that I was very confident what I thought would happen, and I was wrong. This is the contour. I have to say, this completely blew my mind.
00:06:32.400 - 00:06:57.544, Speaker A: Why? So just look at. So, first of all, look how smooth and nice this boundary is. No regularization, by the way. Look how smooth and nice the boundary. And especially, look that there's no bump near these data points. You know, if neural nets were, like, really aggressively overfitting, I would expect this contour to kind of weave in and out here, kind of like it does for this narrow RBF kernel SVM. I found this plot utterly shocking.
00:06:57.544 - 00:07:20.084, Speaker A: You know, if you. After you stop me in the street and ask me about marbles and hills, if you then. If you then asked me, which classifier should you use? If you're in a pinch and you know nothing about your data based on this, I'd pick the Relu network. For me, this was shocking. This is fully connected? Yeah, yeah. Two layer, like order n parameters. So let me show.
00:07:20.084 - 00:07:57.944, Speaker A: Oh, and let me say that as far as I know, the origin of margins is Novikov, 1962 for the perceptron convergence theorem. But of course, it's probably earlier. Similarly, you know, there's so many ideas and so even around then that are misattributed and things like that. Probably know this was another karatsuba and Kolmogorov type thing. Okay, so let me give you another plot to kind of motivate margins a little bit better. So the margin here is the usual multiclass definition of margin. I take the output on the correct label and look at the best output for an incorrect label, and I normalize it in some way.
00:07:57.944 - 00:08:16.276, Speaker A: For now, I ignore the normalization. And what you can do here is you can make a CDF. Okay. I do this for the training set, and I get a CDF of these margins. And so this is something that people investigated very heavily in the nineties for boosting. And if you plot this CDF. So again, we want this plot, it's hard to see.
00:08:16.276 - 00:08:29.544, Speaker A: There's a zero here. We want all these numbers to be very positive. That means the margins are good. If you run coordinate descent, or adaboost or whatever you want to call it, then it pushes things to the right. So it's hard to see once again. But at this .0 is scrolled off.
00:08:29.544 - 00:08:48.776, Speaker A: Okay, so this happens with coordinate descent and boosting. And if you rerun it with. Rerun it with deep learning. And here I've actually told you what's going on, you get the same kinds of plots. Okay, zero scrolls off. So what I'm saying is that. So margins are one lens that we're going to use.
00:08:48.776 - 00:09:35.364, Speaker A: And it appears that if we run gradient descent, and as of course Jason just told us, it appears to be maximizing margins, it's hard to see because where zero is. But the point is that the CDF, which gives the margin distribution, is fully shifting to the right. So this means that gradient descent is. I did not tell you what CF is shifting to the right is. So, Nati's point here is that, as was mentioned by Jason, I'm using homogeneous neural nets here, which mean that constants can just come in and out, possibly with an exponent. And so the scale of this matters a lot, because if I just only cared about the numerator, I could just multiply everything by 1000 and get 1000 times better. So now let me do some normalization, which, thank you very much, Nati is crucial to this question.
00:09:35.364 - 00:10:28.264, Speaker A: So, if I look at Cifar with true and random labels and I do this plot, then they're on top of each other. Margins on normalized, say nothing about random and true labels. But if you normalize it in some way, then you get this separation. And I don't want to dwell on this today, but I'll just say that the normalization I use in all these plots is given by a generalization bound. So you can take questions on that if you want, but, but the norm that I use for the normalization is given by a generalization bound. And this now separates these things. Ok? And so now if we plot what Nati was asking about, now if we plot this margin distribution over gradient descent epochs, then we see that it starts out with many incorrect margins and then slowly tries to shift them all.
00:10:28.264 - 00:10:47.434, Speaker A: So I stopped this once, everything was past zero. This is zero here. Okay. So gradient descent applied to this was actually for Alexnet. So gradient descent applied to Alex net is maximizing margins and it is getting a large margin predictor even after normalization.
00:10:48.054 - 00:10:53.254, Speaker B: So you're talking about margin as if it's a one dimensional property. Of course you're plotting an infinite dimensional property.
00:10:53.334 - 00:11:01.686, Speaker A: Oh, no, no, no. This is a one dimensional property. The margin is just the output. Because it's the range. It's the range. I take the. It's all written out here.
00:11:01.686 - 00:11:18.806, Speaker A: This is the true label, and then this is the maximum over the incorrect labels. So even if it's a k output thing, I get a one output. This is a fully univariate thing. CDF. Yeah. Oh, I see, you were talking. Yeah.
00:11:18.806 - 00:11:22.730, Speaker A: So you're saying. Yes.
00:11:22.802 - 00:11:41.834, Speaker B: So, you know, I mean, these are two different, quite different things. What do you mean by maximizing? I guess. And also, I mean, I could have a generalization bound that depends on, for example, sweeping to the right, you know, performing a balance between how much am I going to pay in my margin loss, and then how much do I get to decrease my broad marker.
00:11:41.954 - 00:11:42.210, Speaker A: Right.
00:11:42.242 - 00:11:46.114, Speaker B: But I could also potentially have a much more sophisticated bound. Depends on the whole curve. Right?
00:11:46.154 - 00:12:25.830, Speaker A: So which of these is important? Yes, let me rephrase at least the last part into a question. So it is true that many of the bounds, they have two terms. One, which is the margin error or the number of margin violations, and ordinary complexia scales inversely with the margin. And yes, if essentially all these bounds I've ever seen either are explicitly stated or can be provisina femimum over those margins in terms of depending on the entire curve of the distribution. I'll return this later. Okay? Okay, so let me clean up what I'm saying a little bit, because I'm already somehow magically running behind. So.
00:12:25.830 - 00:13:02.578, Speaker A: And it's absolutely not your fault. I mean, just say it after you ask the question. When I realize that I'm, you know, it sounded wrong, like, Dan, why are you. So let me just briefly say the concrete bound, which has the form that Dan was mentioning. You know, this bound has been mentioned many times in, you know, the workshop. And so I'll just, I'll just have some retrospective comments. But to be clear, the way the bound looks is you give me your, your row, Lipschitz activation needs to go through zero at zero, then with probably one minus delta over the data, you give me a margin.
00:13:02.578 - 00:13:23.674, Speaker A: Give me the weights, then. And as he was saying, I can make this not just the maximum margin. I can. I can fail on a bunch of points. Okay, so the bound looks like this data times Lipschitz over the margin times a penalty for being a nonlinear function. Just this part is tight with, with a lower bound. It's this part where we pay for the deep network.
00:13:23.674 - 00:13:52.904, Speaker A: It goes like this. Let me just say briefly in about 20 seconds, because I think most people that mention this bound to mention that it's proved. It's proved to be an induction on layers. Every layer, you give yourself some cover budget. At the end, you solve Lagrangian. And that's where these funny exponents come from. And I should just say that when you do this induction, you're paying kind of a worst case penalty at every layer, and you don't really have a good coordination across layers.
00:13:52.904 - 00:14:14.458, Speaker A: People criticize this bound in various ways. I'll give you another one. So I can just add dummy nodes all throughout the network and add zero connections, cancellations everywhere, blow everything up. Function is the same. Bound goes to infinity. And part of this is because it's not very coordinated in what's going on inside the network. It does this worst case sort of induction across layers.
00:14:14.458 - 00:14:45.952, Speaker A: So, here's some retrospective comments. Now that it's been a couple in deep learning time, multiple ice ages have come and gone, multiple civilizations have collapsed, the aliens have repopulated us and all these kinds of things. So, uh, so I should say that a margin assumption is a very complicated assumption on the data. Just think about it. For, for linear predictors, I can just. I can already, you know, I can just visualize what an l two margin means. It's just got this little gap around.
00:14:45.952 - 00:15:03.034, Speaker A: But geometrically saying that I've got a large margin, deep value network, like, oh, my God. Okay, right. That's. That's nuts. Okay, that's. That's one comment. So, I'm just saying that often when I talk with Adam here, he tells me, how am I circumventing certain hardness results, and how, in my plans for optimization.
00:15:03.034 - 00:15:22.892, Speaker A: Well, the margin assumption is very complicated. Okay? So that's one comment. Another comment is that, as I said, margins are just a lens. Margins are just one way to study generalization. There are many other. You can throw that part out, or there are many other parts of the story. Perhaps, as Nati mentioned, the normalization, this was the normalization is more important and a harder part of the story to derive.
00:15:22.892 - 00:15:41.924, Speaker A: You know, there are many parts of the story. Another thing I should say is that subsequent work has adjusted this setting a lot. That's how they get better bounds. And I should also say that this is a purely technical question. I mentioned this to one person in the audience. It's a purely technical question. The only known proof of this is via this technique called matrix covering.
00:15:41.924 - 00:16:04.302, Speaker A: No one on earth knows how to prove this directly via rademacher complexity. For instance, you apply peeling. No one knows how to do it. Just a funny, funny technical comment for anyone who wants to play with it. And there are many choke points that cause this. One of them is that the bound allows arbitrary multivariate gates. That's how we handled max pooling.
00:16:04.302 - 00:16:36.886, Speaker A: So there's some random retrospective comments on this bound. There are many ways to improve the bound and many ways to refine the class and things like that. So I should say that this. So, the paper provided two things. One was the generalization bound, which is part two of the story. So it says, if your function class, or if your predictors have certain reasonable properties, or if you call this not reasonable, whatever you want to call it, if it has these properties, it generalizes. And it gave empirical evidence that this happens.
00:16:36.886 - 00:17:13.198, Speaker A: It gave empirical evidence in the form of plots like this, that gradient descent on deep networks actually does output these nice large margin classifiers. So again, it had a two part story, an empirical evidence, and a theoretical component. And what I want to talk about today, similarly to Jason, or most of what Jason talked about, is the optimization question. So, I want to prove that gradient descent gets good max margin predictors. And finally, I can get to what I talked about 20 minutes ago. Apologies for that. Now, I want to give you guys the clean intuition on.
00:17:13.198 - 00:17:45.774, Speaker A: Or a clean intuition, I hope, on why margin maximization is actually happening. So, first, let me just say that there's a large history in this. So the perceptron proof, of course, is a guarantee on gradient descent with the relu loss for linear functions, assuming margins. And then this. Shapir, Freund, Bartlett Lee paper gives a positive margin. Tongzhang and bin yu. This one gives Max margins in zero five.
00:17:45.774 - 00:18:03.274, Speaker A: Then, as Jason mentioned, I got Max margin and I'll mention multiple times this proof by Daniel Sodri. Surya, over here. Nati. And Aladhofer. And. Sorry. What's sorry? I forgot his first.
00:18:03.274 - 00:18:30.374, Speaker A: So, I'll mention these. But there are three essential techniques, and I'll cover two of them today, and Jason very kindly did the third. So, one is controlling things in the primal, and vaguely, I said that these three techniques work in the primal. Then there's an approach using duality, and I'll make some comments about this proof. This proof, for me, is actually in the primal and dual. I'll talk about that a little bit. Apologies to the authors for saying random things about their proof.
00:18:30.374 - 00:18:59.154, Speaker A: And then there's, of course, the regularization path approach, which is what Jason and Tongyu followed about 2030 minutes ago. Okay, so, good. So, that was the margin setup. Margins are a thing. They're one way that we can analyze this alien spacecraft. Okay, so let me give. Give you what I hope is one intuition on why this is happening.
00:18:59.154 - 00:19:38.044, Speaker A: Okay, so, here's the example again. So, it just slowly goes to the, I guess, the left for all of you. So, let me just establish some notation that I'll use for both parts. Maybe I'll write some of it up here just so that we don't have to get confused. So, I'm taking all my data, and I'm putting it in this matrix, and then I'm defining an unregularized or, sorry, an unnormalized loss, just because I don't want to write n a couple times. So, here's my risk, where I haven't written one over n. I'm using the exponential loss for simplicity.
00:19:38.044 - 00:19:58.632, Speaker A: And those of you that, you know, you can just think that, oh, wait, this is x of. So a times w. These are the individual margins. And so x of aw. Log is monotone. So if I apply log, this is like, log some x. So it's already looking like I have max margin.
00:19:58.632 - 00:20:15.260, Speaker A: Okay. It's looking like I have maximum over the margins. But what's not in here is that normalization. So, here's a quick calculation. So, if I write down the minimum margin and snot you pointed out, I have to normalize it. So, this is my definition of the minimum margin. I look over all the training points.
00:20:15.260 - 00:20:34.636, Speaker A: Oh, yeah. Let's just say these are norm one at most. So, here's the definition of the margin. It's just this l, two normalized minimum over the output margins. And I just insert our log in an exp. And then I just use definition of the risk. And so this is just log one over the risk.
00:20:34.636 - 00:21:13.234, Speaker A: Okay? So this is lower bounded. So if I can. So here's the intuitive proof. Okay, here's the proof idea. So if I can just say, firstly, that every time this goes up by one, this goes up by gamma, the maximum margin, then, and if I can show the denominator goes to infinity, then I'm done. Okay? So I'm saying norms go to infinity, and for every unit of norm, you increase the numerator by gamma. Okay? Then asymptotically you just take an integral, or just think about them accumulating the numerator is going to be essentially gamma times the norm, and the bottom is going to be norm.
00:21:13.234 - 00:21:44.184, Speaker A: Okay, you're just going to get gamma. Everybody follow. So that is the intuition I want everyone at least to leave. This is the main intuition I want everybody to leave with one perspective is that gradient descent in the linear case, and yes, you can correct this for the homogeneous case. For instance, in a natural way, what gradient descent is doing is for every unit of norm, it's getting one unit of margin. And I think this actually answers your question. Whatever you want to do, it's just squeezing out everything in the other directions.
00:21:44.184 - 00:22:09.256, Speaker A: There's no room, there's nowhere to go. Okay, so that's the intuition. Numerator goes by gamma, denominator goes by one. Okay, this intuition specific to exponential loss. Yes, basically in exponential loss, you only care about the worst. Yes, that's what essentially you're saying. Well, we still need to prove this, but.
00:22:09.256 - 00:22:44.624, Speaker A: Yeah, no, I'm talking about the intuition. Yeah, anything for more general losses of this kind of any intuition. Yes, I can say a number of things about other, about other losses, but cryptically for now I'll just say that you better have an exponential tail. There are actually some negative results for that. The key, the solution has to go to infinity. Yeah. If it doesn't go to infinity, then maybe.
00:22:44.624 - 00:23:23.660, Speaker A: But then we'd have to hedge really carefully. The simple answer is yes. Well, it depends on, I mean, if we take a regularization path. And how do I set it up? I mean, if the loss is really close, I mean, there are weird examples I can instruct, but just stick with exponential, at least in percolate and utilization vectors. If you don't include explosive realization and the loss, the endometrium doesn't go to infinity and you initialize in direction orthogonal to data, then yes, you would always have to find it. All I'm saying is that, yeah, if what you're asking is whether asymptotically we get the maximizer direction then. Exactly.
00:23:23.660 - 00:23:54.670, Speaker A: Then. Yeah, but I was baking in some epsilons anyway, so let's go through this really quickly. So here's, so now I'm proving this gamma for each unit of norm statement. Oh, oops, sorry, I was going to write it on before I put it up. Let me just step through, now that it's up, let me just step through this with all of you. So let's look through the numerator terms. So I'm just starting from what I wrote over here, the lower bound and the margin.
00:23:54.670 - 00:24:10.838, Speaker A: Okay. And I'm going to just look at this in terms of the derivative. Okay. And then at the end you integrate over this. So the, okay, it's log of the risk. So take the derivative, chain rule. I spit out one over the risk times a chain rule, one more time.
00:24:10.838 - 00:24:38.714, Speaker A: Derivative with respect, with respect to the parameters, derivative with the parameters respected time. And by the definition of gradient flow, these are just the negation of each other. So I just get the norm squared. Meanwhile, in the denominator, there's a couple ways to see this, but this is upper bounded by this. One way is to write it as square root of the norm squared and yada yada. So now these cancel and I just end up with the norm of the gradient divided by the risk. Okay.
00:24:38.714 - 00:25:18.194, Speaker A: And, you know, I'll put the slides up. This is just, so if you stop me in the street and you require more, more details, this is the proof I go through in my head. And now here's how we get that this is gamma. So, oh, let me just say that, let me introduce some more notation again. So by chain rule, the gradient, the gradient of this thing is a transpose x. Okay, so this is my gradient, and I'm just introducing this notation where I pushed the r into this term to normalize it. Okay, so now I get this normalized thing.
00:25:18.194 - 00:25:52.922, Speaker A: Okay? So I've got that this thing is lower bounded by this expression. And I just have to show this is gamma. And here's one way to do it. I go back to 1962, or, you know, after all the civilization collapses, the next version of 1962. And this is literally copy paste from the Novikov perceptron proof. So I take over the gradient, and this is a norm. So the variational form soup over unit vectors, I just write in the a max margin direction.
00:25:52.922 - 00:26:10.894, Speaker A: It turns out it's unique. Don't need that here, though. So I just take this inner product, expand things out, and now I've just got only one vector. That's the data I write out this inner product. And by definition, the max margin. Each one of these is at least gamma. Okay, so we're done with the first part.
00:26:10.894 - 00:26:41.854, Speaker A: So again, what we've said is that for every unit of norm, we increase the margin by at least gamma. Okay? So for every new norm, this log one over risk increased by gamma. If the norm goes to infinity, then we get the max margin direction. Okay, but, you know, that's what's going on in my head here. I look at this and I say, every time I increase by one, I'm increasing my margin by gamma. And that's just going on. Okay, because I'm going a little slow.
00:26:41.854 - 00:27:16.050, Speaker A: Let me just say that there are a couple ways to prove that the weights go to infinity. Um, maybe you can check, check this later if you wish. And let me just say that you, you can show, though, that the weights in general go like log t. And this will curse us a lot later. So, as Jason was saying, there are a lot of things that happen in a sort of intermediate regime. This log t is so slow that it's, it's like asymptotic. So again, I'm only talking about the linear case right now, but there are many of these things that carry over for various deep settings.
00:27:16.050 - 00:27:45.454, Speaker A: And the phenomena there will be, will take exponential time to reach. Okay, do you have a question or. I guess. Well, it might be good because it helps us regularize, right? You mean it suggests that we should regularize, is that what you're saying? Or. No, it might help us regularize. Oh, you mean if there's something really weird that happens way out? Sure. So, yeah, summarizing this into a theorem, as Jason mentioned, I spent some time on this a long time ago.
00:27:45.454 - 00:28:32.130, Speaker A: I gave you the gradient flow intuition. Converting the gradient flow statement into a gradient descent statement. You have to just prove a smoothness inequality and you get almost everything the same. Just prove a slightly different smoothness inequality for l one and l two, for coordinate descent and gradient descent. And the setting, the regime Nati talked about where we're not talking about just getting maximum, we're talking about the convergence rate of the normalized predictor to the actual unique direction. We can convert this to a statement of the above form with the same rate just by Cauchy Schwarz and adding and subtracting and stuff like that. Okay, so here it is again.
00:28:32.130 - 00:29:32.602, Speaker A: One unit of margin, gamma units, and margin for one unit of norm. Let me just make a quick comment about the non separable case. So one motivation to study the linearly separable data is you can say, well, once we beef up this proof to the deep relu case, deep relus get zero training on everything, so everything is separable. On the other hand, it might take forever to get a good margin in that regime. And while me and Zoe did analyze the non severable case, I just want to say that if you go back to the bounds, like Dan mentioned, all the bounds they look like in fema over gamma of the probability that your predictor has a gamma margin violation plus one over whatever your complexity measure is over n. So we have this, the infima in this is not in general going to be attained for making this term zero. So just an open problem.
00:29:32.602 - 00:29:50.574, Speaker A: All of these results I've talked about, as far as I know, nobody on earth knows how to prove a rate to get to some good margin for most of the data. Okay, I don't know how to prove a good margin for 90% of the data. If you try to adapt these proofs to that, you're in a world of hurt.
00:29:52.674 - 00:29:55.174, Speaker B: Do you think that's relevant for the deep railroad?
00:29:57.014 - 00:30:21.394, Speaker A: Well, I think so. Let me, let me say it another way. You were asking about maybe we can make a generalization bound that's sensitive to the entire distribution of margins. So I tried to solve that problem, I got nowhere. And I said, well, a simpler one is just to control, like different slices to distribution, and I got nowhere on that too. So I could. So if you believe you're a distribution version of the problem, then I would say this is a very miniature open problem towards that.
00:30:21.394 - 00:30:29.914, Speaker A: Okay, let me give you the dual version.
00:30:31.574 - 00:30:40.534, Speaker B: I think there are versions of this where you study the rate at which the margin distribution decays near the boundary. Like Cibikov type conditions.
00:30:40.654 - 00:30:42.114, Speaker A: Yeah, there are assumptions.
00:30:42.534 - 00:30:46.234, Speaker B: It's not the whole distribution, it's just the tail of it and how it vanishes.
00:30:46.714 - 00:31:20.062, Speaker A: And I also don't know how to use those in this kind of analysis, this optimization analysis. Ok. But yeah, the story I wanted you guys all to get was that for every unit of norm, I get gamma unit of margin. And that's why this works. Ok, let me tell you about a dual view. Let me actually be pretty brief in this section, except for one. One kind of old observation actually.
00:31:20.062 - 00:31:54.676, Speaker A: So let me just summarize these slides as I glide through them. One is if we just write down the convex duality, notice that this is exactly the normal of the gradient. So this gives us an even more direct way to prove this gamma lower bound. I can say that whatever this is, it's lower bounded by the, by the dual value. Okay, so that's like a. Don't even have to talk to Novikov through the ice ages. Let me also say that if you write down a regularized risk, that these terms also come out.
00:31:54.676 - 00:32:30.434, Speaker A: And I should say that. So as Jason mentioned, I worked on this in grad school. It was actually this that caused me to learn duality, because I was studying the exponential risk, the nephemum didn't exist, all the convex op books assumed a minimizer exists. So I said, okay, what I'm going to do is I'm going to work in the dual, where they will, the global maximizer always exists. So, ok, now let me draw you a little cute picture, same as before, but I'm doing one funny thing. I'm scaling points by the dual weight, ok, so I define these dual variables that are just the elements of the loss. So I didn't make any sense.
00:32:30.434 - 00:33:11.122, Speaker A: It's everything that you get without the chain rule. It's these things lost on the individual points. Okay, so as I run gradient descent and highlight points according to that quantity, you see it concentrates the decision boundary. So this kind of gives a support vector view of what's going on here. The gradient is dominated by those points closest to the boundary. So if you look at this thing, you know it's going to be a convex combination of, or after I normalize a combination of data points and it's focusing on the supporting points. Ok, so here's the dual view of the algorithm I want to give you.
00:33:11.122 - 00:34:01.420, Speaker A: So here's the definition of gradient descent where I've just expanded the chain rule for you. That's why it's plus instead of minus. And what I'm going to do is I'm going to try to rewrite this as an update in terms of these dual variables. Just to be clear, the dual variables are, okay, so I want to rewrite this in terms of that. I'm just going to do basic algebra to get there. So first, so I want to, one way to look at it is I want to convert this into this. I multiply by a, I take exponential of negation of both sides, and now I just use the definition, okay, so I've rewritten this algorithm as something totally in the dual variables.
00:34:01.420 - 00:34:44.718, Speaker A: And I don't know about all of you, but I look at this and I see power method, okay? And I'm very happy about this because we're saying that the algorithm is finding directions both in the primal and the dual. It's finding directions and finding directions. Sounds like power method to me. So this, this to me looks like power method, except, you know, what is this thing? So it's a bizarro power method in some weird geometry. And this geometry has a name. This is just mirrored ascent with the KL divergence. But what's cool now is that before we were saying, oh, kind of implicitly gradient descent is finding Max margin directions when the dual is explicitly finding max margin directions.
00:34:44.718 - 00:35:17.670, Speaker A: Okay, this is mirror descent on the dual objective for the max margin problem. Okay? It's explicit Max margin. And I should say that this connection is known. It's known for 20 years. So Manfred was convenient. And then this was actually used to give the first convergence proof of adaboost. So as we were discussing in solutions of Adabust, infinity can't open a convex op textbook to reason about what it's doing.
00:35:17.670 - 00:35:35.278, Speaker A: The original convergence proof used something like this. It gave no rates. This connection is a little bit. This was used to prove a convergence analysis on the risk. Right, but I mean on the risk, not on the margins. On the risk. This is explicitly maximizing margin.
00:35:35.278 - 00:36:04.814, Speaker A: But adaboos doesn't maximize margin. What is the. Oh, sure. So this objective, in order to get the max margin, in order to get the max margin to happen, I'm going to have to make an assumption on the step size, which is not the adaboo step size. Let me just give a, let me just give a little more factoid. On top of what Jason said. If you look back in the adaboo's papers, they're actually constructions, data sets for which it does not find the max margin.
00:36:04.814 - 00:36:44.246, Speaker A: So that's not just like a upper bound artifact, it's real. Okay, so here's actually the thing with constant step size. So if you translate this into the boosting setting, it's not what they use. So we can get something interesting here, which is that these norm of gradients, these actually form a monotone sequence. Okay? So this thing is like, your observed margin is always shrinking. Okay? And just, uh, as promised, I was gonna make a, I was gonna make a comment on this proof by Sodri et al. Or.
00:36:44.246 - 00:37:07.672, Speaker A: So it's like rude, right? So three of you are here. So this just gives a convergence rate in the dual. It doesn't say anything about converging to the max margin direction, even if this is with the right step size. With some work, you can get a convergence rate from here in the primal. So you can do this as two step proof. Step one, convert. Step two, convert it into a result about the primal.
00:37:07.672 - 00:38:01.554, Speaker A: And while it's not the same proof, when I look at your guys proof, I see all the same steps, but just in a different order and packaged differently, just like some strange, strange math, space permutation. Okay. In the remaining miniscule amount of time, let me just tell. So, yeah, the key thing I wanted you to get from this talk were two perspectives that give you margin maximization. One was this, for every unit of distance you go, one unit of margin perspective where the units are scaled by gamma, and the other is this dual view where we're doing explicit margin maximization in the dual. Okay, so let me just give some nonlinear comments. One is so in the deep linear case, which has come up three times now, here's the theorem you can prove.
00:38:01.554 - 00:38:19.702, Speaker A: And I'll just, I want to tell you. So Nadav is here. He talked about this yesterday. I know that he's very excited about the conditions on the theorem. So let me just read them off for you. This assumes separability and an additional assumption about the support vectors that can be dropped. So please just treat that as technical.
00:38:19.702 - 00:38:47.286, Speaker A: I'm just being careful to include that my risk is no worse than the zero vector, and I'm not at a stationary point. I don't start at a saddle or something. So gradient to do something. So if you do either gradient flow or some gradient descent with a small step size on this, then as Jason mentioned, this happens. Okay, this is all consistent with what Jason said. All I'm doing here is I'm writing it in a different way. And that's why I do not get the two over lift.
00:38:47.286 - 00:39:25.118, Speaker A: It's what he had on the right hand side. I want to make a different comment about this. So, if you dissect what this means, because the solution is just a rank one thing, all the matrices align, they have all this nice cancellation. So this wl w one, these all become rank one matrices. Rank one matrices where all the inner terms are the same. So this next one, ul minus one, these are the same. And this last one is the max margin predictor.
00:39:25.118 - 00:39:54.552, Speaker A: So it's like very nice structure. And all I'll say is that if you, if you convert this into a. Oh, sorry. I was gonna also say that for deep relia, I was gonna make some comments, but Jason made better ones. So just go back in time 50 minutes. One cool thing you can do from this perspective is one cool way to plot this alignment property is this alignment that I mentioned here implies that you're actually solving a max margin problem at every layer. Okay.
00:39:54.552 - 00:40:23.404, Speaker A: So I was excited about this because it gave me some understanding of what happens in intermediate layers. So if you map the data forward. So that's, this is the input data. I said at the first layer, it's solving a max margin problem. If you look at the next layer, map the data through the first layer, and you see what's happening here, you again solve a max margin problem. So kind of all the suffixes of the network are solving max margin problems on the prefixes of the network. Cool thing, that.
00:40:23.404 - 00:40:57.916, Speaker A: So I'm saying don't underestimate what this means. This actually has kind of amazing structure inside of it. Okay, so just to summarize the main thing, I, of course, got kind of ahead of myself as always. But the main thing I wanted everyone to extract from the talk today, other than the usual margin propaganda, is that the max margin phenomenon is kind of very reliable. And there's two simple intuitions for it. One simple intuition is that. Or, sorry, that word is banned.
00:40:57.916 - 00:41:36.414, Speaker A: I think Jason said, can't use the word. So there are two intuitions for it. One intuition is that for every unit of norm, I get one gamma unit of margin. So that's the primal intuition and the dual intuition is that it's an explicit max margin solver in the dual. I'll stop there. Okay, I guess let's break for 30 minutes.
