00:00:24.440 - 00:00:41.124, Speaker A: Specific applications of the topics of this program, especially the area of economics and energy. And we are very pleased to have Anna talking about concept in algorithmic mechanism.
00:00:41.704 - 00:01:23.060, Speaker B: Okay, well, thanks for showing up. I know that if it was me, I wouldn't have come on the last day. After four intense, very intense days, wonderful days of great talks, this is going to be light. So let me just introduce the area for those of you that are not that familiar with an example. So when we teach algorithms, we learn about maximum weighted matching as an example of a problem we want to solve. So here the problem is your input is a weighted bipartite graph, and your goal is to find a matching that maximizes the sum of the edge weights. In this particular graph, the orange edges have a weight of eight, and that's the maximum matching.
00:01:23.060 - 00:02:18.684, Speaker B: So now let's consider an application of this problem. For example, you have a search engine that has ad slots for sale. So the nodes on the right hand side represent the ad slots, the nodes on the left hand side represent the advertisers, and the numbers on the edges represent how much that advertiser is willing to pay to have their ad shown in that slot. And you could imagine that the search engine wants to make as much money as possible. So they would like to compute a maximum weight matching. But there's a fundamental problem here, because where are these numbers coming from? These numbers are the private information of those advertisers, and there's no reason those advertisers are just going to announce those numbers and, you know, let the search engine take as much money as possible from them. In particular, what they might try to do is lie in order to get a better deal.
00:02:18.684 - 00:03:20.494, Speaker B: So, for example, in this case, this guy here might actually, rather than say he's willing to pay $5 for this slot, he might say he's willing to pay only $3. He would still, if the, if there's a maximum matching algorithm being run here, he would still only get, only, he would still get the slot, but he would have made dollar two, so to speak. Of course, you know, it's tricky for these guys to figure out what they should tell the search engine as for what their values are, because in order to do that, they have to speculate about what all these other advertisers are reporting. And that's kind of a tricky problem. So I just want to point out that this is a game here. In this particular case, if Google is a search engine, Google is designing the game. They get to choose the procedure by which these reports of how much the advertisers are willing to pay are mapped to allocations and payments, and the advertisers are the people playing the game.
00:03:20.494 - 00:04:20.036, Speaker B: And so the question becomes, how should Google design this game so that when these advertisers are playing are behaving in their own best interest, Google's goals are achieved. So there are a lot of situations where the input is the private data of agents who are going to selfishly promote their own best interests. And the field of mechanism design asks, how do we merge algorithm design with incentives? And so there's many, many real world mechanism design settings. I already have discussed sponsor, I mean, or alluded to, sponsored search auctions and display advertising. This is one of the killer apps. Of course, we all know that Google makes a lot of money and basically this is how they make their money. And I like this quote from their chief economist, Hal Varian, which is, he said, what most people don't know is that all that money comes in pennies at a time.
00:04:20.036 - 00:05:15.974, Speaker B: And that's because they're running millions and millions and millions of ad auctions all the time and collecting pennies from these advertisers. But those pennies are adding up. There are many, many other applications of mechanism design in the real world. Kevin spoke about spectrum auctions. People have studied kidney exchange and the incentives there, healthcare systems, recommendation systems, routing on the Internet, resource allocation in the cloud platform designed for a sharing economy. So here I'm thinking about things like Uber and Airbnb, which are solving these massive optimization problems that involve incentives. Bitcoin is a system designed that depends on miners doing something to help keep track of payments that have occurred and they have to be incentivized appropriately, appropriately, and a variety of other topics.
00:05:15.974 - 00:06:17.034, Speaker B: And I just one thing, I don't know if this afternoon we'll even hear about pricing in the context of energy, but these are like real problems. And I'm not sure to what extent our research is having that much impact on these real problems. And I think that that's a really important thing for us to be thinking about. So what characterizes these problems? Well, there are many participants with diverse incentives, the private information of each agent. Okay? And they are like, in my previous example, the advertisers were the agents, is unknown to the designer of the mechanism and presumably the other agents. In fact, they may not even know their own information. So we always, in this area, we always assume that the agent knows their own, or we usually assume that the agent knows their own value for a particular outcome.
00:06:17.034 - 00:06:56.496, Speaker B: But in fact, that really may not be the case. I mean, I know that I, for one, don't know how to value a house that I'm considering buying. And I'm sure that a lot of us have this problem. They have varying attitudes towards risk. They behave with varying degrees of myopia. They may be exhibiting learning behavior of the type we learned about earlier in the week. If you think about the problems that I had on this, the applications I had in the previous slide, these are all very complex optimization problems, and they involve dynamic and repeated interaction.
00:06:56.496 - 00:07:37.414, Speaker B: And I sort of have this in bold because I think it's also one of the aspects where we sort of understand the least. And it's actually very complicated. There are results recently, including by people in this room like Christos and Alex and others. But these are really, really complicated problems that we don't understand very well. So what am I going to do in this talk? Well, not much. Okay, good news, maybe. And I'm going to try to meander through a few topics, very briefly touching on them, no proofs, no nothing.
00:07:37.414 - 00:08:14.904, Speaker B: And just try to mostly connect a little bit with some of the things that were discussed earlier in the week. Just loosely connect and hopefully just give you a gestalt for some of the things that have been going on in the last n years in this field. Anyway, that's the plan. And I apologize for incomplete references and anybody I offend. Okay, so I want to start with applications of profit inequalities. Matt gave this gorgeous lecture on profit inequalities. So let's see a little bit about how they're used in this setting.
00:08:14.904 - 00:08:42.988, Speaker B: So, just to remind you what he was talking about, you have a sequence of prizes. Each of these prizes is drawn from some known prior distribution. So you want. Okay, so, sorry, go on. You know all the priors. And what you do is you see these actual values one at a time. And at the moment that you see a value, make an irrevocable decision as to whether or not to keep that prize or not.
00:08:42.988 - 00:09:46.742, Speaker B: And the moment you keep a prize, the game is over. Okay. What you're trying to do is do well compared with a profit, who gets to see all the values in advance, and would therefore have an expected reward, which is the maximum of the values. Okay, so Matt described a very simple algorithm that will guarantee you half of that, will guarantee you a competitive ratio of two, which is simply compute the expected maximum and take the first prize that is above half of that. Okay, so that's really cool. There's another version that is often used in mechanism design for kind of explain one reason we might use this one, which is to choose a threshold t such that the probability that any of these values are above t is one half and take the first prize that's above t. That also gives you a factor of two competitive ratio of two.
00:09:46.742 - 00:10:31.546, Speaker B: And that's very similar proof to the one Matt gave. And so both of these guarantee that the expected value of the prize selected is half of the expected maximum. So I want to show you just an obvious application of this in the context of single item auctions. So first of all, let's review a few little things about a single item auction. So in a single item auction, we have a bunch of agents. Each of them has a private value for the item, meaning they're indifferent between, if their value is $100, they're indifferent between owning that item and having $100 in their hands. They're going to, the auctioneer is going to specify some rules for how, given whatever it is they bid.
00:10:31.546 - 00:11:14.404, Speaker B: Those are mapped to a winner in a price. And the, the agents here are going to submit bids that hopefully will maximize their own utility. And the definition of utility here is the value they get for the item if they win it, minus whatever they pay. Okay, so one of the simplest goals you can have if you're designing an auction is to maximize social surplus, or this is sometimes called, I'll say surplus for short, or sometimes called social welfare or efficiency. And the goal would be to allocate to the bidder that has the highest value. So somehow implement the max function. Okay? That's the goal here.
00:11:14.404 - 00:12:06.234, Speaker B: And there's the very famous result, Nobel Prize winning result, which says that the Vickery second price auction, which is you allocate to the highest bidder at the second highest bid, incentivizes truth telling. So it's in each agent's best interest to report their true value no matter what anybody else is doing. So for example, if we have two agents and their values are 180, then with the bidder at 100 will win at a price of 80. And so his utility will be the difference, which is 20. So this is a great, great result. And you know, there are many extensions that many of you know about, but there are some problems with this auction. So for example, it's susceptible to collusion and this is outside the model, really.
00:12:06.234 - 00:12:46.256, Speaker B: So if, for example, agent one was to promise agent two to pay him $50 if he bids ten, ok, then what would agent one's utility be? Well, he would win the item at $10 in the auction. He would pay the other, a bidder, $50, and so his utility would end up being 40. The second agent's utility would be 50. They would both be better off. Okay, so this is not the only issue with this auction. It can be slow and inconvenient. Actually, Vickrey auction is what sort of, is one of the auctions? One of the things that's done on eBay, essentially.
00:12:46.256 - 00:13:30.236, Speaker B: And, you know, often those auctions there take days or weeks to run, and that's kind of not that great. It may require more, more communication than we would like. The agents have to report their values, which they may not want to do for privacy reasons, or because those values may be used against them in the future. All the bidders need to be present at the time the auction is run. So if we're willing. So these are problems. But if we're willing to set for approximate optimality, and we have a prior, so that is, we know something in advance about the value, the distributions, these agents values are drawn from, then we can simply use the profit inequality and run a posted price auction.
00:13:30.236 - 00:14:07.332, Speaker B: So what would we do? We choose a threshold t, for example, such that the probability that there's any bidder whose values above t is one half. And we would simply post a price of t, and whoever grabs the item first gets it. And we know by the profit inequality result that this guarantees we get within a factor of two of the expected match. Okay, so what are some features of this auction? It's very simple to implement. The whole thing is described by one number. You just post it. It's very simple for the agents to play.
00:14:07.332 - 00:14:50.312, Speaker B: They don't have to think much. In fact, they don't even really even necessarily need to exactly know their value. They just need to know whether their value is above this threshold. It's robust in the sense that it doesn't matter if the bidders aren't all there at the same time, or come in some arbitrary or even worst case order. It's resilient to collusion. There's nothing you can do now to collude. And actually, one nice thing about this, which is the only reason I switched from the version matt presented to this version, is that you can see that with this version, even if you change the distributions of the values above or below the threshold, it doesn't change the result.
00:14:50.312 - 00:15:35.132, Speaker B: So it's actually robust in that sense, too, to not even being, to allowing variety of distributions. Okay, so that's really cool. Actually, it's a very simple example, but I think it's cool. And it's kind of the beginning of a motivation for why we love posted prices and why profit inequalities are so awesome. I'm just going to briefly mention a beautiful generalization of this due to Feldman, Graven and Luttier. So they were considering the following problem. You have an auctioneer has m items and there's n bidders, and each bidder has a value for each subset of items.
00:15:35.132 - 00:16:16.162, Speaker B: So for example, she might have some value for this pair of items, but a different value for this item by itself, different value for this item, and so on. Okay, so each of the bidders has a value for each subset of the items. And let's assume for now that these values are, that these value functions are submodular, so they're sort of decreasing marginal returns to an agent for larger subsets. Actually, the result of I'm mentioning goes beyond this sub modular setting. But I just, this is just easy to understand. Easier to understand. Um, and suppose that the goal of the auctioneer is to allocate items to implement the max function.
00:16:16.162 - 00:17:12.290, Speaker B: So the auctioneer wants to choose which subset of these items to allocate to player one, which subset to pL2, which subset to player three, so as to maximize the values that they get. So, um. Oh, okay, so just a little bit about this problem. So this problem, just even as an optimization problem, forget incentives, forget the fact that the values are private, is actually a hard problem. There are one minus one over e approximations that are known. This again, I'm talking here, if you knew all the values, you didn't have to solicit them from the bidders. Now, if you want to come up with a truthful mechanism that will approximate this objective, then the best known result we have right now is basically logarithmic in the number of items.
00:17:12.290 - 00:18:55.078, Speaker B: So this is a big gap here, and I'll discuss that a little bit more in a moment. But something that's really neat and sort of surprising, and it's the result of a long line of really beautiful work in price of anarchy, is that if you were to do something really simple like take each of the items and run a separate first price auction on each of them, that is, have each bidder submit one bid for each item, and then whoever submits the highest bid on the first item gets that at their bid, and the second item, whoever submits the Heisberg, gets that, and so on. Then magically in equilibrium, you get a one minus one over e approximation to the optimal social welfare. Okay, so this is really neat and actually there's more general, more general ways to state these results. But what Feldman, Grav and Lucci did was they took some of the ideas from those and they combined them with some of the profit inequality ideas, and they were able to show that there's a simple posted price mechanism that gets a constant approximation. So just to kind of say a few words, what they do is they figure out, sort of given the priors, what's the expected value that the optimal solution gets from each item, and they post half that expected value. Ok? So they post a set of prices, one per each item, allow the bidders to come in, and when a bidder comes in, they choose their favorite bundle of, given the prices, their favorite bundle.
00:18:55.078 - 00:20:00.512, Speaker B: So perhaps her utility for this is her value for those two, minus those two prices. And then they keep coming in, and whoever wants to picks whatever they want, their favorite bundle, and these prices guarantee a constant fraction of the optimal expected surplus. So very nice. This really nice result gives a very simple, simple mechanism that doesn't care when these bidders come in, what order they come in. It's just awesome. As I said already, this is a case where we don't know how to design a truthful mechanism that gets close to the approximation ratio that we get in the algorithmic setting. This result that I just mentioned is part of a line of work that tries to bridge this gap between algorithmic computation, straight algorithmic computation, and computation with incentives, and tries to bridge it by introducing priors.
00:20:00.512 - 00:21:04.238, Speaker B: Okay? So here they're able to essentially get close to the optimal using priors. And I just want to say that the question I mentioned that's kind of unresolved here. The worst case version of this problem is one of the big driving questions in the field, which is, to what extent is computability with incentive compatibility harder than without? So this is, you know, if you're going to do computations and you have to be incentive compatible, you have to incentivize the agents to report their values truthfully. That imposes quite severe constraints on what the mechanism could do. And so, question is, how does that affect what we can compute, what problems we can solve? Okay, so a little bit about profit maximization here. Profit inequalities are also very useful. So in order for me to tell you that, I've got to tell you a little bit about maximizing profit or revenue in auctions.
00:21:04.238 - 00:21:42.884, Speaker B: So this, assuming known priors is completely solved in Nobel Prize winning work by Meyerson. So again, sorry, I should just say. So the problem is, again, just design an auction now, knowing that the values are coming from these priors so that the auctioneer revenue is maximized. Okay. And so, just to the simplest case here, suppose there was only one bidder. I'm the seller, you're the buyer, and I know your value. I'm sorry, I know the distribution from which your value comes.
00:21:42.884 - 00:22:34.484, Speaker B: So you could imagine that there's some complicated thing I should do. But in fact, you can show, or Marcin showed and others showed earlier, that the optimal auction is just opposed to price. And what is that price? It's going to be the price at which the price I get times the probability you're going to buy at that price. You'll only buy at that price if your value is above the price is maximized, and it's called the monopoly price for the distribution. Okay, so what about in more general setting? For example, if the bidders are iid, then it turns out that the optimal auction is just the Vickrey auction with a monopoly reserve price. So the reserve price is like an opening bid on eBay. And what that means, essentially, is that nobody can win the auction unless they're willing to pay at least that much.
00:22:34.484 - 00:23:16.054, Speaker B: And what the Vickrey auction does is it gives the item to the highest bidder above the reserve price, if there is any, and it charges that person the maximum of the reserve price and the second highest bid. And this is the optimal thing you can do in this setting. In the non iid setting, things are quite a bit more complicated. Here's the optimal auction. You ask bidders to report their values for each value. You compute this weird function of their value, which is called the virtual value. And notice that here it has all this dependence on the distribution.
00:23:16.054 - 00:24:08.616, Speaker B: And what Myerson showed is that the expected revenue of an auction is exactly equal to the expected virtual value of the winner. So what that suggests is that what you want to do is allocate to the bidder with the highest virtual value, as long as that's positive, and that turns out to be the optimal auction. And then you set the prices so that you guarantee truthfulness. Meaning in this case, the winning bidder will pay the smallest bid he could have made and still wonder. So I guess the important thing to notice about this is that it's pretty complicated. You need to know that these virtual value functions everywhere in order to implement the auction. It's also kind of weird.
00:24:08.616 - 00:24:49.764, Speaker B: So, for example, if your first bidder had a value drawn from exponential distribution with parameter one, and your second bidder had a value drawn from a uniform distribution on zero one, and their values were 1.5 and 0.8. Then when you do this transformation, it turns out that in this optimal auction, the second bidder, the one with the value of 0.8, will win and pay 0.75. So kind of weird things can happen. Not the bidder who's not the highest can win. And the result is that the, I mean, the point of this little example is that it's complicated, unintuitive, and it depends on detailed distributional information.
00:24:49.764 - 00:26:03.858, Speaker B: Okay? So you can ask the question, is there a simpler, more practical, more robust way if we're willing to sacrifice optimality? So again, profit inequalities to the rescue. So, as we said, Myerson tells us that our revenue will be the expected virtual value of the winning bidder. So how would we apply a profit inequality? Instead of thinking of the values as the prizes, we think of the virtual values as the prizes. Actually, I'm taking a max of the virtual value with zero, okay, because we never want to allocate if the virtual value is negative. So we think of these prizes in virtual value space and we try to get the largest one. So how would we do that? We could choose a threshold so that the probability that the maximum of the virtual values is above t is equal to one, two, okay, and if we do that, then while, ok, this is a fixed price in virtual value space, it actually does not correspond to a fixed price in value space, in value space. What this would do is it induces some price.
00:26:03.858 - 00:26:49.774, Speaker B: What is for each bidder? What is the price at which their virtual value is equal to that threshold. Okay, so basically what you do is you do this calculation, which you can do, you could do just knowing the distributions. And that determines some threshold price for each bidder. And you just sell to the first bidder that's willing to pay his personal reserve price. And this gives a two approximation. Admittedly, this is more complicated because now we have bidder specific posted prices, but it's still way simpler than the optimal auction. Okay? And it has all these nice properties that I was discussing earlier.
00:26:49.774 - 00:27:34.100, Speaker B: So this is called oblivious posted pricing, because we don't care what order the agents show up in. If the designer is allowed to consider the agents in the order of his choosing, then you can get better approximations. So I guess I'll briefly tell you how this is done. So the way this is done is as follows. First, rather than solve the optimal oxygen problem, what these guys did is they formulated the optimal ex ante relaxation. What that means is essentially the fractional version of the problem. Suppose I just want to allocate the item in expectation at most once.
00:27:34.100 - 00:28:17.102, Speaker B: What's the optimal revenue I could get then? So what would I do? I would solve this program. I just define QI to be the probability that agent I wins the auction. I want to allocate in the item and expectation at most once. And now things are much simpler in that I can optimize, point wise for each agent once I know what this Qi is. So I would want to maximize the revenue I get from agent I, assuming that I sell to him with probability Qi. And this is actually a convex program so it can be solved. And if the distributions are nice, I don't even want to say what that means.
00:28:17.102 - 00:29:14.974, Speaker B: Then it turns out that the optimal solution to this program is a posted pricing. So it would specify for each agent a price. And what would the revenue be? The price is such that the probability that the agent accepts that price has a value above that price is exactly equal to Qi. So this is the most revenue you could get if you're willing to sell in expectation at most once, and you can then say, well, why can't I just run this posted price auction and get this revenue? The problem is that I'd be sampling for each of these distributions. I'd be tossing a coin for each agent separately and independently, whether or not they're willing to accept that price. And when I do that, I might end up selling more than one item. It may be that more than one of them is above the price, and that's not feasible for me in the real setting.
00:29:14.974 - 00:30:08.208, Speaker B: So, since I need to be feasible, but I can choose the order in which I approach each the agent, perhaps the right thing to do. I mean, the natural thing to do is order my attempts to serve by decreasing price. So, first, try the agent for which my price is highest, then try the agent for which my price is second highest, and so on. And if I do that, then my revenue will be just the sum over the agents. The probability that agent's willing to accept this price times the probability that no earlier agent was accepted their price. Okay, so, for example, if the prices are all one, and these probabilities are one over n, then the ex ante optimum is one. But the revenue of this auction is one minus one over e.
00:30:08.208 - 00:31:17.698, Speaker B: And this is actually the worst case. And there's this very nice result due to Aggarwal et al. That says from stochastic optimization, that says that if you have a submodular function. So this function is mapping subsets of this set n to real numbers. If it's submodular and you have some distribution over subsets of those items, and the distributions have marginals qi, then if you look at the ratio between the expected value of that function, when you sample independently with probabilities qi compared to sampling from this correlated distribution that has marginals qi, then that correlation gap is at least one minus one over e. This independent sampling always gets at least one minus one over e of the correlated distribution for this function. So in this case, what is the submodular function? It's just the max over the prices in the subset.
00:31:17.698 - 00:32:13.692, Speaker B: So it follows directly from this theorem that this mechanism with which you take the solution to the x anti problem, you post those prices, approach them in order of decreasing prices, gives you a one minus one over e approximation. So actually, this is all much more interesting in settings beyond single item. For example, if you conserve many agents, for example, the matriarch feasibility constraint. But I'm not going to get into any of that. I just to summarize this little section on posted prices, there are many results on posted price auctions, often based on profit inequalities. They're beyond single item. I've mentioned all these many reasons we love posted prices in mechanism design, and I guess one takeaway is that auctions can be approximately optimal without being complex.
00:32:13.692 - 00:33:09.584, Speaker B: And this is part of a kind of major kind of drive within the field that was sort of articulated by heartland and roughgarden to understand the tradeoffs between simple mechanisms and optimal mechanisms. And there's been a huge line of research to try to understand this. This work falls within that category. But of course, the results all depend on really understanding the optimal auctions, which Myerson gave us. And the other takeaway here is that the optimal, or approximately optimal auction, even if it's very complicated, if you can understand it, then you get a benchmark for evaluating more practical designs. So there's a huge ongoing quest, including by many people in this room, to understand optimal and approximately optimal options in ever more complex settings. And I'm actually not going to touch on that at all.
00:33:09.584 - 00:34:07.112, Speaker B: But it's a very exciting line of research. So all of this, everything I've discussed so far, depended on the fact that we had an accurate prior in our hands. Actually, we're also assuming independence between agents and for some of these, for the results I presented and for most results we have. So you can ask the question, where does this prior come from? Okay. And presumably it comes from previous experience on the market. So Tim was discussing how, you know, in sponsored search auctions, I can use bids that people submitted yesterday in order to infer a prior, and then, you know, start doing something today. It may come from on the fly market analysis, but, you know, there are lots of questions once you do this kind of average case analysis.
00:34:07.112 - 00:35:23.440, Speaker B: What if the prior is not accurate? So the results are potentially sensitive to small errors in the prior? What if the prior is changing over time? We're seeing different populations or whatever. And even if a little birdie was to whisper in your ear exactly what the prior was at any moment, if you're running some ongoing dynamic system, you may not want to redesign the mechanism every time the prior changes. So this kind of raises this notion, or suggests a kind of hybrid type of analysis. Rather than use the optimal auction or something based on it, for every single setting, is there some way we can come up with an auction that's simultaneously near optimal for lots of priors? And Tim mentioned this yesterday. This is the notion of prior independence. So we assume the values of the agents come from some prior. And what we want to do is design a single auction that has no knowledge of the prior, so that no matter what the prior is, the expected value of the auction is within some constant factor of the expected value of the optimal tailored to that prior.
00:35:23.440 - 00:36:30.934, Speaker B: So we'd like this to work no matter what the prior is in, let's say, among a large class of priors. So of course it's not possible without any assumptions whatsoever on the prior. But it turns out that under relatively benign assumptions, you can actually get results like that, like this, in this setting. The starting point, or an inspiration, I guess, for this work, is an older result by two economists, Bulow and Klemperer, who showed that in a single item IID setting, regular distributions, whatever that is, the expected revenue of the optimal auction tailored to the distribution, is less than or equal to the expected revenue of just the simple VickRey auction. Sell to the highest bidder to as long as you add one more bidder. Ok, so this is a resource augmentation type result. And kind of what it's saying is that a little more competition is more important than precise knowledge of the prior.
00:36:30.934 - 00:38:01.894, Speaker B: Ok, another thing you could see it as saying is that there's high value from one extra sample from the distribution, or alternatively, that a random price from the distribution is almost as good as the optimal reserve price. So you can see that if you think about the case of n equals one, in the case of n equals one, this is just post the monopoly price, it's the revenue from the monopoly price posting. And what is Vickre with two agents? Well, Vickrey with two agents just has each agent is like setting a reserve price for the other agent. Okay? So it's like setting a random price from the distribution for each agent, and each one of those will contribute half the revenue, which means that the expected revenue from just picking a random price from the distribution and posting that for one agent is at least half of the optimal revenue. Okay? And so what these guys were able to do was take this and really fly with it. And what their idea was, they introduced this mechanism they called the single sample mechanism, which is you just pick one random bidder, or one random bidder of each type, if they're partitioned into different types of distributions, and use his bid as a reserve for the others. So essentially do market research on the fly.
00:38:01.894 - 00:38:40.014, Speaker B: And this mechanism gets a two approximation to the optimal mechanism that's tailored to the distributions, basically, no matter what the distribution, with a little asterisk. Ok, so this is like really cool. They came up with a beautiful new geometric argument for making this, for getting this result. And there are many, many extensions beyond single item. Again, I'm just using single item to keep things simple. But the interesting cases are well beyond that. And it was the beginning of a flurry of activity on prior independent auctions that's still ongoing.
00:38:40.014 - 00:40:08.128, Speaker B: Okay, so once you see that one sample is enough to get a two approximation, then it naturally raises the question of how many samples are necessary and sufficient to get a one minus epsilon approximation to the optimal expected revenue. So, basically, a recent line of research, very exciting, is looking at the sample complexity question here, a la what Elie told us about earlier in the week. I get to see some number of samples from the distribution, and if they're not IID, then each one of these represents one sample from each bidder. And I run some kind of learning algorithm that outputs an auction, and I want this auction to be good when I run it on a new sample. And so basically, you can take standard ideas from sample complexity and adapt them to this setting and do some work, and you can come up with bounds on how many samples are necessary and efficient to get good approximations. So some of the results are, well, without any assumption that the distributions, you can't do anything in the IID setting. You just need a small number of samples, constant number of samples, to get a one minus epsilon approximation.
00:40:08.128 - 00:41:17.604, Speaker B: This result is actually from the original Dang Watnutai paper et al. And then people have been getting improvements on the sample complexity for the non IAD setting, but suffice it to say that it's significantly more complicated. You need to know a lot more about the distribution in order to be able to get a good approximation. And most recently, Jamie Morgenstern and Tim have been importing ideas from learning theory. Specifically the idea of pseudo dimension, which is a generalization of vc dimension from binary valued functions to real valued functions, and basically getting bounds on sample complexity in terms of pseudo dimension. So the kind of cool thing is that this pseudo dimension seems to match our intuitive notion of what the simplicity or complexity of a family of auctions. And, you know, I won't go into that, but basically it implies good sample complexity bound so for distributions whose support is zero to h, essentially linear in the pseudo dimension, sample suffices.
00:41:17.604 - 00:41:22.604, Speaker B: So this is part of a really interesting line of research. Yeah.
00:41:25.134 - 00:41:26.406, Speaker A: Does it mean that you choose one.
00:41:26.430 - 00:41:28.710, Speaker B: Over epsilon bidders, take their maximum money.
00:41:28.742 - 00:41:29.798, Speaker A: And use that as a result?
00:41:29.926 - 00:42:21.134, Speaker B: Actually, you can actually, what I'm talking about here is the setting where you sort of imagine you first see s some number of samples from the distribution, and then you run the auction on a new set of n bidders. Okay? So obviously, you know, if this is something large compared to the number of bidders, then you would sort of have to do that. You'd have to use prior samples. If it's very small, as in the IID setting, then you can actually sample from the people that show up themselves. And that was what was done in those prior independent auctions. So this intersection between learning theory and mechanism design is gaining a lot of traction. It's very exciting and I think there's still a lot to do.
00:42:21.134 - 00:43:43.306, Speaker B: One thing that I find really interesting and complicated about this, just like as an open question, is there's an implicit assumption here that the people providing the samples, which are other agents, are not strategic, okay? They're just providing their samples so that later when they come back, the auctioneer can use those samples against them and make a lot of money. So the question one very, it's very important for us to understand how learning interacts with strategy, and I think that there's far from any detailed understanding on that. So I was going to say something about prior free, but I'm going to skip that. Happy to tell anyone about it later. I'm just going to wrap up with a few more comments about AGT and learning and kind of repeat myself a little bit. So sorry. So, you know, if you take these e commerce settings, you know, Amazon or eBay or these sponsored search settings or Uber or so on.
00:43:43.306 - 00:44:37.284, Speaker B: The interactions in the marketplace are very dynamic, and there's many repeated interactions between agents, okay? And these agents are uncertain about one another. They may be exhibiting learning behavior, they may be doing all kinds of stuff, but they're definitely strategic, or they may be strategic, or maybe they're myopic, I don't know. But they know that information they reveal today may be used against them tomorrow. And there's lots and lots of data that they can take advantage of. But how do you learn from that data when that data is provided strategically? So I just want to give you a small example just to illustrate about how repeated interactions are very tricky. So this is the fishmonger problem, named the fishmonger problem by Amos. So you have a fishmonger, every day he shows up with a fresh fish.
00:44:37.284 - 00:45:32.194, Speaker B: And every day a buyer wants a new fresh fish. The same buyer comes every day. I mean, we all love our sushi, right? So the buyer has a value, which is, let's, for example, drawn from a uniform distribution on zero one. But the buyer's value doesn't change from day to day. Every day they have the same value. Okay, so every day the seller and the buyer meet, and there's another transaction, or no transaction, depending on the price, the seller says. And the thing about this is that you might ask, well, there's the potential for the seller to learn this agent's value, and can he eventually start getting the full value from the agent every day? You might think that was the case.
00:45:32.194 - 00:46:13.694, Speaker B: So before I say something about that, let's just observe that in this setting, if they were only going to meet once, single buyer, single seller, we know that the optimal thing to do is to post a price. And the price that you would want to post would be one two. If you post a price of one two, then the buyer will accept. We'll take that price with probability one half, if its value is above one half. So the expected revenue is going to be one quarter for a single interaction. Spec deduction, year revenue. Okay, now, what if they're meeting for two days? So here's what, again, just want to clarify.
00:46:13.694 - 00:46:33.774, Speaker B: The buyer's value is not changing. Okay. You know, the distribution, it's drawn from up front, but it's not changing. So you might think, okay, well, I might post a price of 0.5 on the first day. I'm the seller. If they accept that price, then I can assume that their marginal is uniform on 0.51.
00:46:33.774 - 00:47:12.686, Speaker B: If it's uniform on 0.51, it turns out that the optimal price to set is 0.5. But if they reject, then presumably I can assume their value is uniform on zero to 0.5 and then the optimal price to set is 0.25. So if I could do this and this is what happened, then I would actually make more than one quarter on average per day because I'd be getting a little extra here in the setting where they wouldn't have bought on the first day. But this isn't what's going to happen if you take a buyer with value 0.6. If he accepts on both days, then his utility is 0.1,
00:47:12.686 - 00:47:39.154, Speaker B: plus 0.1 is 0.2. But if he rejects on the first day and accepts on the second day, then his utility is 0.6, -0.25 which is 0.35, his utility would be higher. So this buyer, anticipating the possibility that the price is going to go down if he rejects, he's waiting for the sale, is going to not do what you want him to do.
00:47:39.154 - 00:48:27.576, Speaker B: Now, of course, if the seller. So here, in fact, if the seller could commit to selling at the price of 0.5 every day, then he could certainly get in expectation one quarter every day. But if he can't commit, it turns out that for two days, the total expected revenue he can get is at most 45. So not only can he not do any better, he will actually do worse if he can't commit to future prices. Ok. And just more generally, if they meet n days in a row, then without the seller's ability to commit to future prices, the most revenue he can get from a strategic buyer.
00:48:27.576 - 00:49:25.998, Speaker B: Ok, again, it's a strategic buyer is little o of n. So that's kind of interesting. And it just points to some of the complexities that arise once you have repeated interaction. If you take strategy into account. Now, there's lots of ways of changing models and understanding things in different ways, but I just think it's kind of neat. So learning, there's many, many questions. You know, if you think about data that's being provided to an actual learning algorithm, then how do you incentivize providers of data to put the effort in to give you high quality data? So this is like the crowdsourcing setting where you want to incentivize high quality work, or how do you learn from data when the data is both noisy and strategically presented? So, for example, suppose the providers of the data actually want to influence the outcome of the learning algorithm.
00:49:25.998 - 00:50:07.516, Speaker B: So there's a huge body of work that's emerging from various communities, not just ours. On questions like this, I just point to one reference from our community. But I think there's lots and lots of questions to answer here. And I thought I would just end by describing one stylized question that's been considered. I think there will be a talk about this in the next workshop. The first paper, I'm just going to briefly tell you what the second paper does. So first, to motivate this, if you think about an app like Waze, which recommends traffic routes for you to take, uses basically crowdsourcing to figure out what the traffic situation is.
00:50:07.516 - 00:50:46.054, Speaker B: All over the world, all over the place. Sorry. Then they want to make good recommendations, but in order for them to make good recommendations, they need the drivers to be exploring the space, right? To some extent. Okay, drivers may not want to explore the space. They might want to do the thing that's best for them. Or similarly, a retailer like Amazon wants users to explore products and review them so that many products have reviews. And ultimately they can converge to best pricings and recommendations, but they have to rely on their users to explore the space.
00:50:46.054 - 00:51:56.090, Speaker B: So in these situations, Waze and Amazon are like some principal that would like to run a proper learning algorithm to figure out what the best recommendations they can make are or what the best products are. But in order to do that, exploring is necessary. On the other hand, a user's goal in any individual, in any particular situation, is to exploit. Okay? So the question is, how can the principle incentivize the users to do a sufficient amount of exploration that the long term learning is as good as it can be? And how good can it be? So the model that these guys considered was a bayesian multi arm bandit setting exactly of the type Kamesh spoke about earlier this week. You have k arms and they're bayesian arms, so you have some sort of prior over the reward distribution. Or maybe they're Markov chains like what he described. And every time an arm is pulled, you can update your prior over the rewards that it provides.
00:51:56.090 - 00:52:53.976, Speaker B: At every step a user shows up having seen all the past rewards before. They make the selection and then they choose, basically they choose some arm and they get some reward. And the assumption here is that the user just wants to maximize their reward right now, whereas the principal wants to maximize the long term reward. So here I've written it as discounted rewards, where gamma is kind of a discount factor. So they want the system to operate as best as possible in the long run. So there's this kind of trade off between the user who wants to use a myopic policy? And the principal that wants to use an optimal policy in this case is the gittens index policy that Kamesh mentioned. So the way they approach this question in this paper is they introduce the notion of incentive payments.
00:52:53.976 - 00:53:49.564, Speaker B: So at each time a bonus is announced for each arm, and now the bonus is, the assumption is that the user will choose the arm that maximizes their bonus plus the expected reward they get from that arm. And what they do in the paper, and I'm not going to say anything about it, is they really precisely characterize the trade off between the incentive payments and the opportunity cost. So if you're willing to pay lots of money, then you can get them to implement the optimal policy, the gittins index. But on the other hand, if you're, you know, that costs you a lot. If you're not willing to spend the money on incentive payments, then you're going to lose a lot in the rewards relative to the optimal policy. And the question is, what is that trade off? And that's what they answer. So I'm going to wrap up with just saying lots of things I didn't discuss.
00:53:49.564 - 00:54:34.312, Speaker B: I didn't discuss modeling of agents, for example, value distributions that aren't independent. I didn't discuss mechanism design in more complex settings, which is really all the rage. And people like Kai and Matt and others have been doing lots of great work. Yang and Matt and others do lots of great work in that setting. And I didn't discuss complexity of equilibria. So there are all these great results by people like Christos and others showing that things like Nash equilibrium are hard. Is there any opportunity, or what is the opportunity here to go beyond worst case? And they're along the lines of what Tim was discussing yesterday, and there are some results of that type, but I'm sure there's much more to be done.
00:54:34.312 - 00:55:10.384, Speaker B: And then, as I said, dynamic mechanism design is a very exciting topic. And I just refer back to that list of applications that I had on one of the earliest slides and say that we're not doing enough to actually impact those applications. Perhaps for those that care about that, but maybe we don't care. I don't know. So if you want to know more, I recommend Jason Hartline's book and Tim Ruffgarden's lecture notes and all the videos from the semester last year on economics and computation. And thank you.
00:55:15.904 - 00:55:36.244, Speaker A: Questions? Yes. Before you show that if I think it was a given distribution, that if you can choose the order of the players, then you have a gap of one minus one over eight. What if you cannot choose the order?
00:55:36.584 - 00:55:56.260, Speaker B: You can get a factor two. That was the basic posted pricing. So that's what that order buys you there? Yeah.
00:55:56.332 - 00:55:57.624, Speaker A: Not well. Right.
00:55:59.124 - 00:56:18.200, Speaker B: Yeah. Well, there we can just do it. Yeah. VCG, whatever. Okay. Oh, the results of like Feldman et al. That I mentioned there, they can also do it by sampling and get.
00:56:18.200 - 00:56:27.004, Speaker B: Yeah. And then they, I don't know that they have results related to the sample complexity there, but it's very good.
00:56:27.304 - 00:56:27.744, Speaker A: Thank you.
