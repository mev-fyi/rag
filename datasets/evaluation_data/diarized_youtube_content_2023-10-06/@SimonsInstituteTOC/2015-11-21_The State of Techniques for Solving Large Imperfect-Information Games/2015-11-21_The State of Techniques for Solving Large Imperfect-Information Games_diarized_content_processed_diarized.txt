00:00:01.280 - 00:00:15.754, Speaker A: All right. Hi. Thanks for coming back from lunch so quickly. Is the sound okay? Yeah. Okay, great. So after lunch, you know, with dark bracket camera slides, I had to turn off the lights, kind of. So if you fall asleep, I will walk up to you.
00:00:15.754 - 00:00:46.394, Speaker A: All right, so this is talking about the state of techniques for solving large, imperfect information games. And there's been tremendous progress in the field over the last twelve years. So if you haven't followed this field, this should be a treat. This is joint work with a lot of people. I'm going to show the different segments with the names of the people at the beginning of every segment. And so this is an overview talk, as they asked me to do. All right, so, incomplete information games that we're talking about, they're like this.
00:00:46.394 - 00:01:08.980, Speaker A: So there's a game tree, like you'd have in chess, say, but you also have these information sets where the player whose turn it is to move doesn't know which node in the information set he's in. You also have nature move. Nature can make moves. That's a white note here. And nature doesn't play strategically, but stochastically. And you could have any number of players. Some of the results are for end player games.
00:01:08.980 - 00:01:32.590, Speaker A: Some are specific for two player zero sum. So here, this would be two player zero sum. There's the red player and the blue player, and then there's nature. All right, tackling such games. Well, the techniques I'm going to talk about are domain independent, so they apply to that class of games in general. And techniques for complete information games like chess don't apply here at all. Because of the information sets, challenges are unknown.
00:01:32.590 - 00:02:21.974, Speaker A: State uncertainty about what others and nature will do, and most importantly, interpreting signals and avoiding signaling too much. So, I need to understand, what does somebody's action signal to me about their private information and what my actions signal to them about my private information. And Nash equilibrium is a great definition of how you should take those signals into account. But it's just a definition. You have to marry it up with algorithms to compute these solutions to make this operational. So, most real world games are, of course, multi step and incomplete information, like negotiation, multistage auctions, sequential auctions, political campaign allocation, military applications, next generation cybersecurity, and security games. So, Milin is going to talk about security games that he's done a lot of work in.
00:02:21.974 - 00:02:45.954, Speaker A: Typically, there you look at the Stackelberg model. One guy moves, then the other guy moves, the world ends. Maybe in the future we can have more sequentiality. And that allows you to do more, and Milido and I are talking about that also. Cyber security. So we've done some work on wireless jamming, some ideas for operating system security and so forth. And here's a wild and crazy idea that I have medical treatment.
00:02:45.954 - 00:03:25.094, Speaker A: We're doing an application where we steering evolution and biological adaptation using computational game theory and opponent exploitation techniques there. I'll be happy to talk about that more offline. Poker has become really the benchmark. I wouldn't call it an application, but the benchmark for testing these different types of algorithms and different groups work on the same problems, build on each other's results, publish, and get better and better over time. I'll show you these graphs over time. And it's been truly amazing how much improvement there's been in poker. There's hidden information, of course, uncertainty about future events.
00:03:25.094 - 00:03:59.800, Speaker A: You need deceptive strategies to play well. And the game trees are very large. Here's an example from tv. This is heads up, no limit Texas Holden. You see a little bit of this on tv, but most of it happens actually on the Internet, ranging from tiny stakes to huge stakes, where individual pots can be over a million dollars. All right, so here's the leading approach for solving these types of games. First, you have the original game, then, and by the way, this isn't given as a game tree.
00:03:59.800 - 00:04:07.764, Speaker A: It would be way too big to give us a game tree. So you could think about that given as a piece of paper with the rules written out, but in computer understandable form.
00:04:08.744 - 00:04:21.570, Speaker B: So if you had to make a poker bot for like two, two player Texas hold them. I understand that you go for the min max equilibrium. If it's a three or higher, then I have no clue why I should compute.
00:04:21.722 - 00:04:34.934, Speaker A: Right? Yeah. The abstraction part here is about n player general sum. When we get to the actual equilibrium stuff, I'm going to talk mostly about two player zero sum, and then I'm going to pop back.
00:04:35.874 - 00:04:44.244, Speaker B: So, do you use. But my question, I guess, was, do you use these algorithms for three player poker? And how are they? Good.
00:04:44.324 - 00:05:09.804, Speaker A: Okay, let me take the three player thing at the end when I'm going to talk about multiplayer. But so far, if you like, think about two player and think about just Nash. All right, so you run an automated abstraction algorithm to get a smaller, strategically similar game. Then you run a custom equilibrium finding algorithm to solve that, to get a nash or approximate Nash. And then you model. Use a reverse model to map the answer back into the original game. Yeah.
00:05:10.544 - 00:05:15.004, Speaker C: Where do we get Nash as opposed to Bayes Nash, did you say? That I asked.
00:05:15.504 - 00:05:46.946, Speaker A: Yeah. Nash and base Nash are the same thing when it comes to these types of games. So Nash is just base Nash when you have beliefs like this. All right, lossless abstraction. So this sounds like an oxymoron. The idea was that we can make games smaller by filtering the information a player receives. So, for example, in poker, if you're really receiving an ace of a particular suit, we'll just tell you that you received an ace, and we can make the game smaller like this.
00:05:46.946 - 00:06:25.894, Speaker A: Of course, we can lose something as strategically important that way. But we came up with an algorithm that smells out automatically all of these things that are unimportant and gets rid of all of that spurious stuff from the game tree. So we used it to solve Rhode Island Holden, which was an AI challenge problem with 3 billion nodes in the game tree. Without abstraction, the sequence form LP had 90 million rows and columns, so it was too big to solve. The game shrink abstraction algorithm ran in 1 second, and then the remaining LP had 1.2 million rows and columns. And at that time it was just barely solvable with a little supercomputer in my lab.
00:06:25.894 - 00:06:58.770, Speaker A: So we found the exact Nash for this, and that was the largest incomplete information game sold by then by over four orders of magnitude. The biggest one before that had 140,000. All right. Of course, while the lossless abstraction algorithm gets rid of 99% of the game, typically that's not enough. The remaining game is typically still too large. If you talk about large games like Texas, Holden, lossless, you mean absolutely lossless. Absolutely lossless.
00:06:58.770 - 00:07:01.162, Speaker A: So you get the exact Nash, any.
00:07:01.258 - 00:07:05.914, Speaker C: 1% of probability, this can possibly matter, right?
00:07:06.034 - 00:07:32.814, Speaker A: Totally lossless, exact Nash. That's right. Sounds weird that you can even do it, but it's basically you can think about the game having isomorphisms that don't matter and you can get rid of those. And typically in poker games that's about 99% of the game. So the remaining game is 1% of the original. I was just going to say the same.
00:07:33.394 - 00:07:34.354, Speaker B: Not the numbers.
00:07:34.434 - 00:07:35.666, Speaker C: How are you measuring that?
00:07:35.810 - 00:07:46.778, Speaker A: Oh, I'm measuring the size of the game. The typical measures are either nodes or information sets, and they both work.
00:07:46.866 - 00:07:48.014, Speaker C: They're both languages.
00:07:49.184 - 00:07:54.044, Speaker A: Roughly. I can make a game where you don't get anything, so there's nothing to abstract out.
00:07:54.864 - 00:08:00.000, Speaker C: The question I asked about how you got away from, I mean, the information sets are hiding a lot.
00:08:00.152 - 00:08:18.108, Speaker A: They are hiding a lot. Yeah, it's amazing, but it's true. So, simple example, really trivial example. So if you start the game with two aces. It doesn't matter which aces you have. It may later matter, but it doesn't matter right now. Okay, question.
00:08:18.236 - 00:08:24.380, Speaker D: Is there like inherent, like tie breaking rules in poker where like, it matters whether this is a heart or a spade or.
00:08:24.492 - 00:08:28.464, Speaker A: Yeah. No. What's the question?
00:08:29.324 - 00:08:33.744, Speaker D: So how is, how can the information be filtered if everything is actually different?
00:08:34.164 - 00:09:15.536, Speaker A: Well, that was. I just gave you an example that in the beginning in Texas Holdem, if you have two aces right then at that moment, it doesn't matter what suits they are, it can matter later. And that's okay. The algorithm will figure out that at that later point it will matter. Yeah, I'm gonna stop the questions and is anybody sleeping? I'll walk up to you. All right, so for bigger games like Texas Holden, you'll have to use lossy abstraction and abstract more to make games that are small enough. So for example, in two limit Texas Holden, two player limit Texas Holden, where whenever you bet, you have to bet a given amount.
00:09:15.536 - 00:09:44.864, Speaker A: You have ten to the 14 information sets. That's something that you can just write down. In two player, no limit Texas Holden, which is a more popular and larger game, you have ten to the power of 161 nodes, information sets in the game. So there's no way you can write that down. That's more than the number of atoms in the universe. And even if you had for each atom in the universe, a whole universe within it, and you counted those subatoms, it's still more than that. All right, so now we're going to have to abstract more.
00:09:44.864 - 00:10:56.042, Speaker A: I'm going to just say what the main ideas are from this whole sequence of algorithms that happened 2007 to 2013 of practical abstraction algorithms. The main ideas were integer programming, meaning that you don't just greedily abstract, you're actually planning your abstraction carefully. How many children does every node in the abstract game get to have potential aware abstraction? That's the idea that hands don't really just abstract based on their value, but based on how they transition probabilistically to the next level, already abstracted notes and then imperfect recall. So sometimes it's a good idea to forget something that you already knew that that matters so as to be able to computationally afford more refined abstraction of the present. All right, so the leading practical abstraction algorithm right now combines the last two of those ideas and obviates the first. Here's, in short, what it does. There's a bottom up path of the tree clustering using histograms over next round clusters, histograms of the probability transitions to the next round clusters, which you already made, because you're going bottom up.
00:10:56.042 - 00:11:05.534, Speaker A: And distance is earth mover. Distance turns out to be better than l one or l two, which were the early ones used in this field. All right.
00:11:07.274 - 00:11:10.738, Speaker B: Because as you said, there are not enough particles in the universe.
00:11:10.866 - 00:11:55.186, Speaker A: Yes. Okay, you're too smart. So there's actually a technique that you actually do this in a special data structure that doesn't. So this whole game tree, all right, for distribution, this was by then the world's largest shared memory supercomputer. We ran the algorithms there to generate tartanian seven, which is our program that won the heads up, no limit Texas Holden world championships among computers called the annual computer poker competition. It's the current reigning champion. The idea here is that on the supercomputer, and actually especially on distributed architecture, you can access local memory quickly, but remote memory very slowly.
00:11:55.186 - 00:12:18.034, Speaker A: So what we do, we want to make sure that the game is disjointly distributed onto the different servers so no information sets cut across. And then the game solving algorithm moves as if the architecture forms a tree as well. So. Well, let me leave it at that, using against the potential aware idea.
00:12:19.334 - 00:12:25.230, Speaker D: So here, I guess you're solving the zero sum game, getting an optimal strategy, and then it worked very well in a multiplayer game.
00:12:25.342 - 00:12:49.898, Speaker A: No, this is two player. Heads up means two player. Yes. All right. So this whole literature went kind of, what works well? What works well? What works well? Lots of good ideas, but it was all kind of heuristic in that there was very little theory. And now we have some beginnings of a theory here, and I'll say a few words about that. So, lossy game abstraction with bounds.
00:12:49.898 - 00:13:38.104, Speaker A: What I'd like to do is to say that if you run this abstraction algorithm, then solve for Nash in the abstract game, that strategy has bounded regret in the original game. Yeah, this is tricky due to the abstraction pathology. So unlike single agent settings like mdps in games, making a finer grained abstraction can actually cause your strategy to have more regret in the original game. The prior lossy abstraction algorithms have no bounds. The first exception was for stochastic games. And now we can do this for general, extensive form games for both action and state abstraction. So in poker, you could think about action abstraction is bit sizing abstraction, and state abstraction being card abstraction.
00:13:38.104 - 00:14:19.832, Speaker A: All right, the main theorem goes like this. And now I'm really just summarizing a lot of stuff into one slide. For any Nash equilibrium in the abstract game, any reverse mapped strategy into the original game is an epsilon Nash equilibrium. In the original game, you have to do this reverse mapping in a particular way, which we call undivided lifted strategy. But that's not an assumption, because we are doing the reverse mapping. We are free to pick it where Epsilon is Max over player's epsilon, where this is the player's epsilon. And what is that? Well, it's two times the reward error, which is defined recursively on the game.
00:14:19.832 - 00:15:32.328, Speaker A: So nodes reward errors. How it differs from the abstraction if it's a leaf, and you recursively define it up the tree, so that at player nodes, it's a maximum error over the child nodes, and at nature nodes, it's a weighted sum. Then you add the sum of heights in the game tree for that player of the maximum utility in the abstract game times the nature distribution error at height j. And here's a little subtlety, that we can actually do this with the nature distribution error here, which is important. Instead of the player distribution error, which is dependent on the strategies, which would give us a chicken and egg problem, and then sum over the heights of natures turns in the game tree, two times the nature distribution error at that height times again the maximum utility in the abstract game. So if you didn't follow that, the bottom line here is that we have on the left hand side quantities that we can easily measure from the abstract gain and the real game. So we can just look at them myopically and say, what are those quantities? And then we can tie it to Nash equilibrium here.
00:15:32.328 - 00:15:40.004, Speaker A: And this is kind of weird in that. How is this possible? I just told you that there's abstraction pathology. So what gives?
00:15:49.484 - 00:15:50.908, Speaker B: Are we supposed to tell you?
00:15:50.996 - 00:16:10.104, Speaker A: Yes, the question is that I told you that we make a finer and finer abstraction. The solution can get worse, because there's abstraction pathology in games which is not there in mdps, not in any single agent setting. But here I'm giving you a bound. How is that possible?
00:16:13.784 - 00:16:15.352, Speaker D: Is it because it's a bound?
00:16:15.528 - 00:16:44.374, Speaker A: Yes, you heard my talk before, but thank you. So it is a bound. So it leaves some room for the abstraction pathology. And you actually see that in the experiments. All right, hardness results. Determining whether two subtrees are what's called extensive form game tree isomorphic, which is a subroutine used in the abstraction algorithms, is graph isomorphism complete. And computing the minimum size abstraction given a bound that you allow, is NP complete.
00:16:44.374 - 00:17:37.094, Speaker A: So you might say, wait a second, coming to cost this issue, especially with two player zero sum games which are in principle polynomially solvable. Now we're solving an exponential problem as a preprocessor, how can that make sense? Well, it's not that hard in the usual case. And we're also working in a special structure, so we don't have to write the whole thing down. All right, we also extended this to imperfect recall games, so you can actually get some bounds there. And there's actually an interesting role in modeling. So if you think about modeling in game theory or any setting, really, modeling is abstraction of reality. So until now, we had no idea when we made our game modeling choices how the answers from that model then would apply to reality.
00:17:37.094 - 00:18:03.354, Speaker A: But now these are the first ties between reality and modeling as well. If you can measure the left hand side of quantities between reality and the model, at least bound them. All right, so that was abstraction. Let's talk about game solving. And now we're going to move into the two player, zero sum setting. All right, so here's a historical scalability. Now, Amy, we're talking about nodes in this slide on the y axis on a log scale.
00:18:03.354 - 00:18:50.114, Speaker A: It started by kind of the sequence form and simplex algorithm, then moved to the barrier algorithm. And here's where the annual computer poker competition was executed and everything went off. You see the super exponential scalability improvement here with these custom algorithms. And nowadays a more relevant measure is the number of information sets. Now that we have that on the x axis. And you can see that this exponential growth of what's almost solvable, solvable to small epsilon, really truly small, not some big approximation ratio, has continued to grow exponentially. So this one, the last one, which is a real data point, is heads up limit.
00:18:50.114 - 00:19:51.954, Speaker A: Texas Holdem is almost solved so small that you couldn't actually tell even if you played a lifetime against it at human speed, whether you're beating it or not. Even if you got to look into its brain, then what's here? This is a new technique which we call regret based pruning. If you know the counterfactual regret minimization algorithm, it's a new technique for doing pruning there. So basically, if any sequence is reached with zero probability because its regret is negative, we are actually projecting saying, ok, how many iterations would it take of CFR, at least before that regret can get back to positive and we can skip that whole subtree for that many iterations. And then we have a technique where we actually go back, and if the regret is now positive after that time in one iteration, we can fix that subtree in a way that still honors the CFR convergence rates.
00:19:52.334 - 00:19:58.750, Speaker D: David, are these algorithms, all in general, building from CFR from that first paper there.
00:19:58.822 - 00:20:32.090, Speaker A: No, no, no. So this was in the gradient algorithm space, excessive gap technique, fast EGT, which we'll talk about this was that these were all CFR. So these are University of Alberta, a guy from California. This is CFR from our group with some goodies on top. This is CFR from University of Alberta. And then this is our new improvement over CFR, which also works for CFR. This is dotted because we don't really know.
00:20:32.090 - 00:21:15.454, Speaker A: We've only tried it on small games and we get one to three orders of magnitude. We haven't done the supercomputing run of this yet, but wait to see that in about half a year. So I might argue that this chart itself is evidence that this is the wrong measure of game complexity, because the numbers on the y axis here are sort of well beyond any sort of cognitive limits of human players. And the thing that you're exploiting in all these. Okay, let me take that offline in interest of time. I mean, I want to talk to you about that, but there's a lot coming, and you too. All right, so the leading equilibrium finding algorithms for two player zero sum.
00:21:15.454 - 00:21:55.894, Speaker A: Now we're really two player zero sum. They're really two families that are totally different from each other, but they have selective superiority. So it's kind of amazing that they're at the same level, although they're totally different and have totally different strengths and weaknesses. So, counterfactual regret minimization is based on no regret learning. This is based on Nesterov's excessive gap technique. Most powerful innovations on top of that here, each information set can have its own no regret learner, which makes it possible. Instead of doing no regret learning in the whole space of strategies and sampling, you can sample the tree instead of actually doing full traversals.
00:21:55.894 - 00:22:23.044, Speaker A: Here are most powerful innovations. Smoothing function for sequential games. This is called a prox function, and without the prox function, this technique doesn't work at all. It wasn't clear that you can actually even make one for sequential games, but you can. Then there's some changes to the algorithm itself, more aggressive decrease of smoothing and balanced smoothing. And then there's a memory improvement, which applies to all settings where the available actions don't depend on chance. That happens to be a feature of poker at least.
00:22:23.044 - 00:22:52.524, Speaker A: And there you can use a decomposition technique for representation to get to have the memory a square root of original. How many iterations does this need? Well, pretty bad. One over epsilon squared, where epsilon is epsilon in the epsilon equilibrium. But each iteration is fast. Here we're going much faster in a number of iterations, one over epsilon, but each iteration is slow. Both of them parallelize. They have selective superiority.
00:22:52.524 - 00:23:24.160, Speaker A: And one of the advantages of this is that this can be run on imperfect recall abstractions, although there are no realistic bounds for that. But at least you can run it. You can press the return button and it goes here. You can't do that because it inherently assumes the sequence form. And then here there's a new order log, one over epsilon algorithm, which seems a lot better than this. And it is. But when you have close to equilibrium already at the end, this is much better.
00:23:24.160 - 00:23:51.174, Speaker A: But there's a condition number which depends on the game in front of that. All right, better first order methods. So, first order methods are gradient based methods, and EGT is one example of that. So far, EGT is the most scalable of those for game solving. But here's some new theory for this. So there's a new prox function for first order methods like EGT. And mirrorproxy gives the first explicit convergence rate bounds for general zero.
00:23:51.174 - 00:24:53.410, Speaker A: Some extensive home games, it also is better, even some gas classes for which there were some. And most importantly, it introduces gradient sampling into these gradient based techniques. So Andrew Gilpin and I, a few years ago, we already did matrix sampling, and we got a little bit of benefit, but nothing like the CFR sampling schemes. Now, you can actually do similar kind of sampling as it's in CFR, in the gradient based techniques. So this gives us the hope that we can actually have the iterations be as fast as CFR, while still having the smarts of the optimization framework of gradient based instead of regret based. And that also allows us to run on imperfect recall abstractions, again with no bounds, but at least we can press the return button. All right, one other idea I'll just mention, if you have good intuitions about what the answer should be, you can actually use that to make the equilibrium finding faster.
00:24:53.410 - 00:25:15.962, Speaker A: So here's some intuition from Sam Gansfree for Texas Holden endgames. He believed that this is a qualitative structure of endgames. Here's player one strategy, players two strategy, weaker hands, stronger hands. And it should be qualitatively like that. Or he said, maybe not. Maybe it's like that, or maybe it's like this. And that's okay.
00:25:15.962 - 00:26:11.614, Speaker A: We can actually put these into an integer program and say, are some of those guesses, the qualitative guesses? Right. And the idea behind the integer program is that we have to be indifferent at all of those boundaries. And the integer program is just finding where those boundaries are. And if there's no equilibrium that corresponds to your guess, it's going to tell you that your guesses suck. Here's a new idea, simultaneous abstraction and equilibrium finding. So I'm not going to go into the details, I'll just say what is the problem that's being solved here and how it does it at the high level? So we can't solve a game without abstracting it first. But at the same time, in principle you can't abstract without solving, because what should be abstracted together depends on the probability vectors at those information sets.
00:26:11.614 - 00:26:40.224, Speaker A: So if two information sets have the same probability vector, those should be bucketed together. But we don't know the probability vectors, they're the output of the equilibrium finding. So this is a chicken and egg problem. Simultaneous abstraction and equilibrium finding abstracts and solves simultaneously. So it cuts through the chicken and egg problem. And you might wonder, okay, well, once you change the abstraction, you have to restart the equilibrium finding, so you lose a lot of work. But that's not the case.
00:26:40.224 - 00:27:23.704, Speaker A: We have a theory of how you can carefully restart CFR by discounting the iterations you have so far. So you get a good warm start. And it's important that you do the discounting just right according to the theory, because if you're even a little bit off, turns out that in practice as well, this goes astray. Abstraction size traditionally must be tuned to the available runtime. So when we look at the computer poker competition, every team does this every year. They said, okay, how much super computing do I have available? I'm going to tune my abstraction size to that. But here the abstraction size automatically increases over time, so you don't have to set the abstraction size up front.
00:27:23.704 - 00:27:53.242, Speaker A: And we talked about the pathology that larger abstractions may not lead to better strategies, but worse strategies. Here the technique guarantees convergence to a full game equilibrium, not the equilibrium of the abstract game. All right, so now that's all I was going to say about. Thank you, we're good so far. We talked about equilibrium. Let's talk about opponent exploitation. Now.
00:27:53.242 - 00:28:27.600, Speaker A: So traditionally there are two approaches. There's the game theory approach, kind of what we talked about, and the opponent modeling approach. So in the game theory approach, you abstract and then solve for approximate equilibrium. And Nash is safe in two player zero sum games, in that if the opponent plays something else, it can't hurt me, but it doesn't maximally exploit weaknesses in the opponent. The opponent modeling is kind of opposite. You can think about the machine learning approach where you learn what the opponent does and somehow best respond to that. Turns out that in practice in Texas holdem at least, it needs prohibitively many repetitions to learn.
00:28:27.600 - 00:29:44.574, Speaker A: So it gets crushed by the game theoretic approach in the competitions. Also you have this get taught an exploited problem where I can actually teach costis that hey, I have this mistake in my strategy, I have this mistake, and then when costess moves to exploit it, I'm going to exploit him huge. And this actually happens in poker a lot, plus in a lot of other games. All right, so the idea here is that let's hybridize the two approaches, so we start playing a precomputed near equilibrium with the techniques we talked about. And then as we learn that the opponent or opponents deviate from the equilibrium, we start to adjust the strategy to exploit them. But starting from the equilibrium, instead of jumping to the best response, and this requires no prior knowledge about the opponent, it significantly outperforms game theory based base strategy in two player limit Texas Holdem against trivial opponents and weak opponents at some groups submitted to the competition against the strong opponents, you can just not turn it on. So one simple idea is that you just said, okay, if I'm losing, I'm not going to turn it on, or playing equal, I'm not going to turn it on, but if I'm winning, then I'm going to turn it on to win even more.
00:29:44.574 - 00:30:40.906, Speaker A: Other modern approaches to opponent exploitation. So you could run an epsilon safe best response, which is a best response where you guarantee that you don't lose more than epsilon per iteration in expectation, or you can pre compute a small number of strong strategies and use no regret learning to choose among them at runtime. Another thought is that can we actually get around to get taught an exploited problem? Can we get a safe exploitation strategy? And at first blush it seems that you can't at all. And there were actually two incorrect propositions in the literature that showed this. One of them us, by the way, that you cannot get this, but it turns out that you can. So what is safe? Safe strategy achieves at least the value of the repeated game or sequential game in expectation. Clearly game theoretic strategies are safe, but they don't exploit maximally.
00:30:40.906 - 00:31:05.270, Speaker A: Can you exploit more and still be safe? Well, one idea is that risk what you want so far, if I want $100, I'm going to run an epsilon safe based response where epsilon is dollar 100. Safe or not, safe or not, it's safe, right?
00:31:05.302 - 00:31:06.126, Speaker D: Because you're risking.
00:31:06.150 - 00:31:14.046, Speaker A: Okay, wrong, safe, wrong. Okay. It's not safe because I could have just gotten lucky and now I, it's.
00:31:14.070 - 00:31:16.782, Speaker D: Not a first step criteria, it's an average criterion.
00:31:16.838 - 00:31:33.690, Speaker A: Right? Right. So I could have just gotten lucky. And if I compromise my hundred dollars, I'm okay with my upside. Looking from an ex ante perspective in expectation, but my full downside is still there. So in expectation, I'm not safe. Here's another idea. So that's not safe.
00:31:33.690 - 00:32:07.078, Speaker A: Other idea. Risk what you want so far in expectation over nature's and own randomizations. In other words, risk the gifts that the opponent has given us, assuming the opponent plays a best response in states where we don't know, safe or not. I know this is scary as anything, Kevin, that's not safe. It is safe. Okay, so that's safe. That is safe.
00:32:07.078 - 00:32:43.064, Speaker A: And there are other things that are safe also beyond game theoretic strategies. But number two is distinguished. A strategy for a two player, zero sum game is safe if and only if. It never risks more than the gifts received according to two. So any other safe strategy has to be more conservative than number two. And this can be used to make, number two can be used to make any opponent model or exploitation algorithm safe by saying, okay, your model is recommended in this action. Is it safe according to our definition? If so, we're going to play it, if not, we're going to go back to equilibrium.
00:32:43.064 - 00:33:13.518, Speaker A: We looked at the whole literature of opponent exploitation algorithms. None of them were safe. And number two, in addition to being theoretically the most opportunistic, it also experimentally performs the best. And also you don't have to know the gifts exactly. As long as you can lower bound the gifts that the opponent has given you, you can use that in the algorithm. Okay, so that's all I was going to say. Talk about technical things, except for the conclusion that's coming up.
00:33:13.518 - 00:33:53.270, Speaker A: And now let's talk about how do programs do against humans in poker? Okay, so we talked about Royal Island Holden, 3 billion nodes in the game tree bots play optimally, heads up limit Texas Holden. This is two player Texas Holden, where whenever you bet you have to bet a given amount, ten to the 14 information sets. So you can actually write this down after you've done lossless abstraction. And that's how this is actually solved. You do lossless abstraction and then you run a new algorithm called CFR from University of Alberta. So bots surpassed prose in 2008. There's a little manna machine competition in zero seven where bots lost.
00:33:53.270 - 00:34:32.352, Speaker A: And in 2008, there was a little one where they won. These are like maybe a couple of thousand hands each. And there was a science paper this year where this game was almost solved. So solved to one millibit blind per hand, which is such a small difference from optimal that even if you play for 70 years at human speed, you will not be able to tell the difference. So what about the bigger and harder and more popular game of no limit Texas Holdem? Again, heads up two player. This has ten to the 161 information sets. Well, the best part was the reigning champion is tartanian seven.
00:34:32.352 - 00:35:11.263, Speaker A: There's another competition coming in early 2016. We'll see what happens. This actually beat all of the competitors with statistical significance. And then we took this and improved it to what we call Claudico, a new, better program. And then we organized this brains versus AI event where we had four of the top ten pros at heads up no limit Texas Holden, come to Pittsburgh, play in a casino in control environments with duplicate matches to reduce variance. 80,000 hands, 20,000 hands per player. Over two weeks, morning to evening, they were playing against the program.
00:35:11.263 - 00:35:37.604, Speaker A: All right, here's the world champion, by the way he's playing, and this is literally what it looks like. Here's the AI. Nobody in the seat. Here's him. Features are totally empty because they play so fast that the live audience can't understand it. But on the other hand, on Twitch, we had 50,000 live viewers, like podcast aficionados from all over the world who play at these speeds. They were super excited.
00:35:37.604 - 00:35:57.340, Speaker A: Okay, so here they are. Number one guy in the world, number two guy in the world of three. And these other guys are actually in the top ten as well. This guy is the one that actually did best against Claudicle. All right, and here's our team. Noam Brown, Sam Cansfree, myself. This is what it looked like on Twitch.
00:35:57.340 - 00:36:29.088, Speaker A: So you can actually see all of these hands. You can go to YouTube now they're all on YouTube, so you can retroactively look at the hands and see all the mistakes and good plays if you're a poker fan. The humans took it very seriously. So they had $100,000 that they were splitting as participation fees based on how they did relative to each other. So the Pennsylvania gaming board didn't allow us to gamble, so we could never win anything, even if we had beaten them by a lot. So all of the money went to the humans by definition. But how much each one got, depending on.
00:36:29.088 - 00:36:46.830, Speaker A: Depending on performance. So they were serious. So every morning they're stretching. They're taking care of their sleep hygiene. I think they had one glass of red wine during those two weeks. Very serious. They flew in an analysis guy from Florida that was doing computer analysis of the hands day and night.
00:36:46.830 - 00:37:11.436, Speaker A: So this was actually not really a human team, but a computer supported human team that we were facing here. All right, how did we do? Well, the pros won overall by 90 1 mg blinds per hand. This is so close that it's not statistically significant at the 95% confidence level, even after those 80,000 hands. And 95% is what we use in the annual computer poker competition.
00:37:11.540 - 00:37:13.972, Speaker D: What's MBB? What is MBB?
00:37:14.108 - 00:37:39.824, Speaker A: Milli. Big blinds per hand. 1000th of a big blind. And big blind is twice the amount of the smallest increment you can bet. There have been actually two relatively recent challenges where the top humans played against each other, and in both of them, the winner won by more. Then the Gmail beat us. So this is another statement of how close it was.
00:37:39.824 - 00:38:06.004, Speaker A: Three of the pros beat cloudy, got one, lost to it, and pros won nine days. We won four days of the 13 days of play. So it's pretty close, but we're still behind. I think in a year or two, this will be fixed strengths and weaknesses. Strengths beyond what pros usually do. So these are cloud eco strengths. It does tiny bets and huge all in bets onto little plots that humans never do.
00:38:06.004 - 00:38:26.970, Speaker A: Perfect balance. So whenever it bets, it bets with a perfect balance of hands. So you can't really read anything into it. And this was very interesting that the pros picked up on that right away. Randomization. So we are using mixed strategies, mixed behavioral strategies that are not range based. So even the best pros kind of think range based.
00:38:26.970 - 00:38:47.246, Speaker A: And they said, with what range of hands would you do something? And that means that they're playing a pure strategy. Doesn't mean that they're always betting with the strong hands. They mix in. I shouldn't say mix. They put in some of the weak hands into the range as well. But at the end of the day, it's a pure strategy. Well, we're not some things that are considered really bad in poker books.
00:38:47.246 - 00:39:16.596, Speaker A: What's called limping and donk petting, just both of those things. So it kind of puts in question the poker weaknesses. Well, course handling of what's called card removal in the end game, the algorithm for endgame solving was fine. It's just that we had to use an abstraction there as well that was coarse enough so that we could do it in 20 seconds. And that's why the humans observed this problem. Action mapping. Well, the whole approach is that if the opponent plays an action, we have to map that action back into our abstraction.
00:39:16.596 - 00:39:52.108, Speaker A: There's some exploitability there that's not just a function of cloud eco, it's actually an issue with the whole framework that's been used today. And no opponent exploitation, because we knew that the humans were going to be great, and our opponent exploitation is great against weak players. We didn't turn it on at all. So that allowed them to try all sorts of stuff against us at will without any penalty to that. All right, let's multiplayer poker. Well, this is costasist question bots aren't very strong at all. Exceptions are certain jumpfold endgames that we've solved near.
00:39:52.108 - 00:40:32.098, Speaker A: Exactly. So let's conclude we talked about domain independent techniques abstraction first lossless abstraction, then the best practical lossy abstraction, which includes potential where imperfect recall and earth mover distance. Lossy abstraction with bounds both for action and state abstraction and all also for modeling. And it would be nice to marry these two things together in the future. Right now they're still some separate simultaneous abstraction and equilibrium finding. There's some theory and experiments on the reverse mapping. How should you map the opponent's actions into our abstraction if they move something that's out of the abstraction and then end game solving.
00:40:32.098 - 00:41:22.778, Speaker A: We didn't have time to talk about those. Equilibrium finding can now solve games with ten to the 14 information sets to very small epsilon. We talked about the gradient based approaches called taking CFR to one over epsilon with the gradient based, and then to log one over epsilon with the condition number in front, and a new framework for gradient based algorithms that allows us to do tree sampling and all sorts of other good things that are already part of CFR. So we can actually now maybe get the best of both worlds. We talked about regret based pooling for CFR, which is a new thing, and qualitative knowledge and guesswork to help equilibrium finding. And then we talked about opponent exploitation. Basically, one idea was marrying the equilibrium based approach with opponent exploitation, and the other one was to save opponent exploitation.
00:41:22.778 - 00:41:27.962, Speaker A: And there was of course, more. So I could stop here, or we can talk about future research directions.
00:41:28.138 - 00:41:29.738, Speaker D: Probably stop there, I think.
00:41:29.866 - 00:41:31.214, Speaker A: Okay, thank you.
00:41:36.294 - 00:41:49.950, Speaker D: A couple of questions. While Milan sets up, just in regards to the chicken and egg you left as a kind of puzzle, you said there's a chicken and egg, but then how have people been doing it beforehand if there's a chicken and egg?
00:41:50.142 - 00:41:52.022, Speaker A: Oh, abstraction and equilibrium finding.
00:41:52.118 - 00:41:53.382, Speaker D: Abstracting if you have to solve to.
00:41:53.398 - 00:42:08.784, Speaker A: Abstract, you don't have to do abstraction very well. And that's basically the standard approach which I started with. So you run some abstraction algorithm that's not informed by the equilibrium, and then you solve for equilibrium.
00:42:09.604 - 00:42:11.464, Speaker D: What is it informed by?
00:42:12.484 - 00:42:20.300, Speaker A: It's informed by these things like potential how states transition to next states based on probability vectors and things like that.
00:42:20.452 - 00:42:23.064, Speaker D: And if you get that wrong, then it becomes lossy.
00:42:24.124 - 00:42:29.044, Speaker A: Even if you get that right, that can be lossy. Yeah.
00:42:31.184 - 00:42:38.680, Speaker D: And do you have a comment on how EGT compares to CFR at this point?
00:42:38.792 - 00:43:05.370, Speaker A: Ah, yes. So you do see in small games, you see that the EGT is converging faster, like it's bound theory. Bound would suggest. But as the game gets bigger, the crossover gets thrown further and further out. You do see the same shape, but it gets thrown further and further out. And that's for EGT. That works in the sequence form, and that's why it's so important not to work in the sequence form anymore.
00:43:05.370 - 00:43:32.914, Speaker A: And the new stuff opens the door that we can actually do gradient based algorithms like EGT in a tree based, sampling based setting like CFR. So is that going to work? We'll know in about a year, I think. I mean, it's working, but is that going to outperform CFR, the best variance of CFR, like CFR plus with RBP? I don't know yet.
00:43:35.614 - 00:43:42.246, Speaker D: Are there games that you guys for tournament settings or for cash games? In other words, are there budgets?
00:43:42.390 - 00:43:59.214, Speaker A: Yes. So this is all one hand at a time. So, cash game. I mentioned a little bit about the end game, solving for three player jump, fold endgames. So for that very special kind of endgame, we've almost exactly solved the end games. But by and large, no.
00:43:59.674 - 00:44:01.634, Speaker D: Why don't we thank Thomas one more time?
