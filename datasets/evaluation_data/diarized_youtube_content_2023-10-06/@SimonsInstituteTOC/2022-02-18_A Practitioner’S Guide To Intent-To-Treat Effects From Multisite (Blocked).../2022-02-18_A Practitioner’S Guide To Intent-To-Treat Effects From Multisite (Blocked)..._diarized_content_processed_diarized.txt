00:00:01.080 - 00:00:44.036, Speaker A: Great. Thank you all for having me and I appreciate being here. And I'm sorry I'm not in person. I had tried to make that work, but I couldn't figure it out. I also appreciate given the camera angle, it feels like I'm giving a talk while crouched in the upper back corner of a lecture hall, which is probably usually where I'd want to be when giving a talk. So I'm going to take some comfort in that. So this talk is a very applied talk, but hopefully it'll be of interest and comes out of trying to think through how people go about analyzing multi site or blocked randomized trials.
00:00:44.036 - 00:01:51.528, Speaker A: These show up a lot in education. You can think of them as you have a bunch of schools and you're randomizing students within schools to treatment and control, and then you're looking. So within each school you can estimate a treatment impact. And the question is how do you combine all of those individual school impact estimates across all of the schools to get an overall estimate of impact? And so there's a little story that Mike put together about sort of setting the stage, which I like, so I'll go through, which is Mike's collaborator Alex comes to Mike and says, I'm planning an evaluation that'll randomize 6000 people within 20 sites. Alex works in early college, like community colleges is something to think about there. Can you write the part of the analysis plan that describes the estimation model for the overall average ITT intention to treat effect? And Mike, who's my collaborator, probably thinks something like, why don't you do your own job? But he goes, okay, he likes to help. And then what he did is he reached out to me so we can think through this simple question and try to come up with whether there is any sort of definitive answer.
00:01:51.528 - 00:02:33.974, Speaker A: Short story is there's not one of the many answers that one might give would be an estimator. That would be something like a fixed effect estimator where you have within this you would fit a fixed effect model. You have a fixed effect for each site. You have an overall coefficient beta for your treatment effect. T is your treatment, you have some residual, you would use some sort of robust standard errors and you'd call it day. So that is one standard recipe. But the question is, what is that recipe doing and is it the best recipe? This talk is going to look at a variety of recipes and we're going to compare those recipes across a variety of data sets to see if these decisions matter in practice.
00:02:33.974 - 00:03:02.384, Speaker A: First, let's talk about the s demand. What are we actually estimating when we talk about an overall average treatment effect in a multi site trial. Now, what I'm about to show you is very basic, but I think it just sets some ground, sets the groundwork or whatever. Word goes there, my words are failing me. Um, and, uh, it also will just make. Everything follows will have a common language. So, first we talk about the program effect for a person.
00:03:02.384 - 00:03:24.464, Speaker A: This is not something we can ever estimate, but we can certainly hypothesize about it. You've probably done the show all week. So you have a person and you have the potential outcomes. You have the program or no program. You treat the people in the program if you're in the program. Otherwise not. You have these y one and the y zero, and then the difference, the beta I would be the individual treatment effect for individual I.
00:03:24.464 - 00:04:04.534, Speaker A: Now, the fundamental problem of causal inferences, we can't actually see both those potential outcomes. So we have to look across groups. But from a conceptual point of view, we can still think about all those individual effects. So for a group of people at a site, here's a site, we have a bunch of people, and we can think of those pairs of potential outcomes, and we can average them across the site. So we average the individual effect for person I in site j, average across all those people, you get beta j, which would be the average impact for that site. All right, so in this case, the average effect might be 0.25, because we're inducing one person to be happy with our treatment.
00:04:04.534 - 00:04:32.156, Speaker A: So where this gets interesting is in a multi site trial. Here is a multi site trial with two sites. If the sites are different sizes, we immediately have two potentially different estimands. So in site one, beta one is 0.25. In site two, beta two is 0.5. The overall person average effect is 0.416. But the overall site effect is the simple average of the quarter and the half, which gives us 0.375.
00:04:32.156 - 00:05:27.724, Speaker A: So we actually have two answers. And when you look at what people say in literature, when they're busy analyzing multi site trials, which they spend a lot of time doing, I would say that it is extremely unclear, as in, no one really ever says anything about it one way or another, whether they're estimating a person average effect or a site average effect. And as we're going to see, it turns out that people are often estimating neither of these effects at all. So we can write for, as an estimate, we can say beta person is just the person weighted average of the site average effects. And then beta site is the simple average of the site average effects. The next issue is, are we estimating for the people we have in our experiment or the superpopulation? This is very common to talk about in the causal inference literature and is not so common to talk about outside of the causal inference literature. So this point is worth making.
00:05:27.724 - 00:06:15.940, Speaker A: So here's our individual evaluation sample. Those are the people that we actually rounded up for our experiment. Here is some very large population, and in fact, we probably have some relevant superpopulation. That is, not all the people that we can imagine, but some target subset. This quickly gets into issues of generalizability, of which there is a long literature. But in this particular instance, the reason we think it's relevant to make this distinction is when you look at various ways of estimating and calculating standard errors, it's often very unclear and sometimes hard to figure out whether you are estimating an average impact for the finite sample or for the superpopulation when you use the sort of the standard tools that people use for estimating impacts for a multi site trial. So this gives us four estimates.
00:06:15.940 - 00:07:16.914, Speaker A: You can be estimating a person average or a site average estimate, and you can be looking at a finite or superpopulation framework. I'll also add that other work that I have with some other collaborators actually points out that the number of superpopulation frameworks that we have is more than one deciding what is being sampled from what. And, for example, do we have an infinite sample of. Do we have infinite population of students within each of the sites? So it's an infinite, infinite model, or are we sampling sites where the students themselves are finite, but we have an infinite population of sites we can also think of, like large finite population, which opens the door for finite population correction. There are many variations of this, which are much more salient when you're thinking about this as a blocking kind of style rather than a multi site experiment. But at the end of the day, a multi site experiment is just a blocked experiment. So a lot of the literature goes back and forth, even though the language doesn't necessarily flow.
00:07:16.914 - 00:08:12.294, Speaker A: All right, so four estimans. There's actually more, but those are the four biggies, estimators. What we did is we looked around and we tried to think of all of the various ways people might estimate a site average or estimate an overall average impact in a multi site trial. We came up with 15 or so, and I'm just going to go through them relatively quickly. Most of them, or all of them, really boil down to a weighted average of the site average estimate. So beta hat our estimate of the overall average treatment impact boils down to wj, which is some weight given to site j, which is a function of how many people are in that site and other features, and then beta hat j, which is the average treatment estimate for that site. So you can think of beta hat j as the average of the treated people in the site, minus the average of the control people in the site.
00:08:12.294 - 00:08:47.954, Speaker A: And we take all of those individual estimates, we get a weighted average to get our overall single estimate, cross site estimate. All right, so here are, uh, how we estimate those beta hat j. Just to be very explicit, we randomize in the site. In this case, we're randomizing. Um, sorry, we're. We're randomizing one person to, uh, control, and three people to treatment, which is blue. And then our beta hat j is just going to be the average of those three people, minus the average of the one person left over, and we get 0.6
00:08:47.954 - 00:09:10.934, Speaker A: for our estimate. Um, and great. I just want to make sure that everyone's on the same page. All right, so, classes of estimators. Of all the estimators we identified, we sort of have three families or general styles of estimator. One is design based. This comes out of basically the name and style causal inference world.
00:09:10.934 - 00:09:54.524, Speaker A: There's all sorts of linear regression, the econometrics models, classic linear regression, some weird versions of that. And then there's multi level modeling, which is pretty particularly prevalent in education contexts, where you have a random effects model and you fit your data to that. So, for design based estimators in the world of education, there's a fairly nice sort of overview of them written by Peter Chauchet. Peter Shauche and others wrote this software package called RCT. Yes. Which basically analyzes all sorts of randomized experiments using very low assumption, design based methods, and gives you point estimates and standard errors. And they wrote this as a documentation for that package.
00:09:54.524 - 00:10:10.320, Speaker A: All right. Some nice features. They're simple. They clearly connect to the estimate. You look at a design based estimator built into. That is a very clear statement of what it is you are estimating, which is not the case with other estimators. They're generally unbiased.
00:10:10.320 - 00:10:45.690, Speaker A: Not always, but usually. And there's software, so they're easy to use. The design based person average is just what you'd expect. It is the person weighted average of the individual site estimates. And the design based site estimator is the simple average of the individual site estimate. So here we see that those weights are just very directly part of the estimator you just follow the recipe of what are you trying to estimate? And you end up with your estimator. The standard errors that go with this, there are straightforward standard errors.
00:10:45.690 - 00:11:36.364, Speaker A: They boil down into two kinds of standard error. The finite sample standard errors are Neyman style. They look at the variation of the units within each of the groups, and then you average those variances across your groups with the weights that we're looking at in these formula. The superpopulation standard error estimators. What they actually do is they look at the variation in the beta hat j, and the variation in the beta hat j is going to be compounding of the estimation uncertainty of each of the beta hat j themselves. And the uncertainty of some sites have different average treatment effects than other sites. And by looking at all of that together, by looking at the variation, the beta hat j, you can end up with how much extra variation there is by generalizing to the hypothetical superpopulation.
00:11:36.364 - 00:12:25.214, Speaker A: So it's a sampling framework rather than a assignment mechanism framework for linear regression. There's a lot of textbooks, so I'm not going to give you a citation, but we identified eight different variants, which are primarily how you calculate the standard errors and whether you include interaction terms or not. So, fixed effects models is where you have that fixed effects, and then you can calculate standard errors using heteroscedastic or cluster robust standard errors. Or you can just use the old style standard error. It turns out that these are precision weighted. They're neither person nor site average estimates in terms of their estimate. But given that the weights are proportional to NJ, the number of people in a site times the proportion of people treated in the site times the proportion of people not treated in the site.
00:12:25.214 - 00:13:33.208, Speaker A: If you have the same proportion of people treated in all your sites, they collapse back into a person weighted estimate. So we basically said or believe that the implied estimate of a fixed effects estimator is a person average estimate, person average average treatment effect. But it's looking for precision. So it's going to focus more on the sites that have more balance in the treatment assignment because you have more precision in estimating impacts for those sites. Weighted regression is you just add weights to try to debias your estimate to make your weights no longer precision weighted, and they coincide with the design based estimators we just discussed. And then finally, you can fit a fully interactive model where you interact a site dummy with your treatment assignment. And then as a second step, you average those individual estimates to get an overall person or site average using those weights that we saw for the design based, these fully interactive models, they're useful because you can do covariate adjustment very naturally, whereas the design based estimators.
00:13:33.208 - 00:14:10.236, Speaker A: It's unclear how to do covariate adjustment. And in fact, if you read the design based literature, what you'll see is the design based covariate adjusted estimators tend to be interactive linear models where you calculate the standard errors slightly differently. And then finally we have multilevel models. There are three that we identify that are sort of widely used. One is called the fixed intercept random treatment coefficient. This is like the in vogue current cool model. And what it does is have a fixed effect for each site, and then you put a random effect for the treatment impact for that site.
00:14:10.236 - 00:15:00.184, Speaker A: So this is allowing different sites to have different treatment impacts. And the fixed effects is gives you a debiasing effect. If you use the second model, the random intercept random treatment, if you have a different proportion of units treated at different sites, you can actually bias your treatment effect estimate because of those different proportions. And the correlation of the random effects can cause some trouble. Finally, there's the random intercept constant coefficient, where you just have a single treatment effect. This collapses to the fixed effects model. It's slightly different, but it is negligible how different these two are, which means that even though it looks like you have a random intercept, which feels like a superpopulation kind of quantity, in fact, that one is estimating a finite person person weighted estimate.
00:15:00.184 - 00:15:43.878, Speaker A: The other thing about multilevel models is it seems like they're estimating a superpopulation site kind of quantity, because you have this random effect for the average treatment effect of the site. And then the beta there, where you have beta j equals beta plus a random effect in the top center of the screen. I don't know if my mouse is doing anything but this beta. You would think that it's site averaged. But in fact, the FErc model is adaptive. If it doesn't see that there's a cross site variation, then it collapses to precision weighting. And you can see that in the weights expression there where that tau hat is the amount of cross site impact variation.
00:15:43.878 - 00:16:19.258, Speaker A: And if you don't estimate any, then that's going to be zero and you're going to end up with your precision weights again. So it's adaptive. It tends towards site average when you have evidence that the sites are different from each other. Okay, so we have four estimands. We have a bunch of estimators. We've classified those estimators as to what estiman they're targeting then the question is like, are we just wasting time? Does any of this matter? And so what we did is we looked at twelve studies. These are all actual large scale randomized evaluations with multiple sites, and we just fit all the estimators to all the studies.
00:16:19.258 - 00:17:14.600, Speaker A: We also ran a simulation inspired by these studies, has the same sort of statistical properties in terms of how much the intra class correlation coefficient and how much variation there is, and how much treatment effect. Um, all of that. Good stuff. The simulation sort of allowed us to dig into a few extra questions, but the fundamental piece of work is what happens in practice? Do these decisions change your answer in any way that we care about? So the first research question is, does the choice of estimator or estimate matter for the estimate? And here's like one particular, um, uh, trial where here are all of our estimators, and you can see that some of them gave a slightly higher estimate than the other. And we have a difference of about 0.202 standard deviations, or 0.02 effect size units, which is not the biggest deal in the world because we're usually looking for impacts at around 0.1
00:17:14.600 - 00:17:56.424, Speaker A: or 0.2. And also we see that in this particular study, it was pretty bleak overall. Like, there's not evidence that there was much of an impact. If we look across all of those studies and all of the outcomes of those studies that we had, we can categorize how. What is the difference between the smallest estimate we get across the 15 and the largest estimate? In some sense, this is a researcher degrees of freedom plot. Like, how much wiggle room is there just by choosing a different way of analyzing your data? And this is a measure of how much these decisions matter in practice, most of the time, they don't really matter. And this is across whether you're targeting a site average or a person average effect.
00:17:56.424 - 00:18:26.470, Speaker A: Occasionally, however, we are seeing differences in the impact estimates of 0.075 or even 0.1, which is, I would argue, a big deal if we group by whether they are person or site targeting. You see that for all of the person weighted estimators on the left, it's really hard to change your estimate. You can move them a little bit towards 0.01 in terms of a shift, which is not really that big of a deal. For the site average, however, we can start.
00:18:26.470 - 00:19:08.878, Speaker A: That's where we see all of the larger shifts. We can move things a much larger amount on the site side. Does the choice of estimator matter for the estimate of the standard error? Here again, we're looking at Tennessee Star, and we see different widths of our error bar. Some of the intervals seem wider than others. The question is how much so similar to the prior plot on beta, what this is is the ratio of the largest standard error to the smallest standard error across all the estimators. Regularly we're seeing that some standard errors are more than 200% larger than other standard errors. Sometimes we get four times the size.
00:19:08.878 - 00:20:02.694, Speaker A: So depending on your model you pick, you are going to get standard errors that are very, very different. If we divide all of our estimators by type of estimator, finite versus superpopulation person versus site, we see in the top left that across all of the finite person estimators, all the standard errors are basically the same. For the super population site, we can get a great deal of variation in the standard errors. We checked whether standard errors were calibrated. This is from the simulation study. Generally, the standard errors are calibrated. And what's really driving the difference in standard errors is whether you think you're doing finite sample inference or superpopulation inference, that the cost of generalizing to a superpopulation is large and the standard errors for a superpopulation inference are therefore going to be much larger.
00:20:02.694 - 00:20:49.694, Speaker A: But that's not the entire story. The other piece of this is that standard errors are sometimes hard to estimate. For finite sample standard errors, the standard errors, you can estimate them very well. There's not a lot of instability. But for the superpopulation standard errors, the standard errors in the blue box plots there, we're looking at what is the ratio of the standard deviation of the standard error, that is the standard error of the standard error to the actual standard error. This is again from a simulation studies across a wide variety of scenarios, and we're essentially seeing that there's, for a superpopulation standard errors, there's a lot of instability in estimating those standard errors. It's hard to get a handle on the precision.
00:20:49.694 - 00:21:51.954, Speaker A: In particular, sometimes the superpopulation standard errors are so unstable that this is our empirical data. In the top left pink rectangle, we're seeing that the standard errors sometimes in the superpopulation, are smaller than the standard error for the finite, which is nonsensical. This is due to instability in estimating the superpopulation standard error. That's what's driving these discrepancies. And this again just digs into from simulation, the chance of a superpopulation standard error being lower than an estimated finite is actually quite high, especially when the cross site impact variation is relatively low. So what's happening is when you have no actual variation in your impacts across sites, the superpopulation can erroneously estimate that there is just due to bad luck that's going to inflate your standard errors. Sorry.
00:21:51.954 - 00:22:50.794, Speaker A: When there's not very. I got that backwards. When there's not very much across site variation, then what happens is the superpopulation, you can end up with your point estimates, look under dispersed, and you end up with a standard error that is not taking into account the within cluster sampling variability because you've essentially erased it by your cross site sampling variability just due to bad luck. So I mentioned that some of these estimators are precision weighted. They're attempting to, they're giving you a biased estimate in exchange for increased precision. Question is, how does that play out in practice? So, first, if we looked at fixed effects models versus unbiased finite sample person models, we see that across all of our empirical estimates, you get essentially the same answer. The difference between these two estimators is very, very small, and we saw that earlier.
00:22:50.794 - 00:23:45.944, Speaker A: We also see that the standard errors are essentially identical as well, which means that these precision gains from using a fixed effects model is neither giving you much bias, and it's also not giving you much increase in your precision. For FERC, which is also doing a trade off, where the more cross site variation there is, the more it tries to estimate the site average. The less variation there is, the more it tries to estimate a person average. When we compare that to the unbiased design based, we see that the point estimates move a lot more, and we also see that the standard errors are all over the place. Sometimes FeRC is doing better than the design based and sometimes it's doing worse. So it's really hard to sort of make a clear call on what's going on. When you use FErC, which is a biased, but hopefully more precise estimator than the design based.
00:23:45.944 - 00:24:48.014, Speaker A: And that's because to explore that, we did a series of simulations to look at that and came to the conclusion that this is due to instability in the design based estimator. If you actually look at the design based estimators performance in terms of estimating standard errors, it's really, really shaky. It's a hard thing to estimate cross site impact variation. And so if you're relying on that to get your standard error estimate, you can end up in all sorts of weird places, which is what that shows. All right, just very briefly at the end, what about covariate adjustment? We replicated everything with covariate adjustment, and we see a small change in estimates. So if these are looking at how much the estimates change with and without covariates across all of our various outcomes, the gray line is a shift of 0.01. And we see that most of the time point estimates, the the point estimate shift due to covariate adjustment is less than that.
00:24:48.014 - 00:25:30.954, Speaker A: We also looked at whether our standard errors were improved. So that red line is the standard errors are the same. And then if we go to the left, that means that covariate adjustment has reduced the standard errors by whatever percent. And we see most of the time that the standard error reduction is less than 10%. So covariates, basically in the context we looked at, aren't really included improving precision much at all. So just a quick pitch for our paper. Our paper, which is published somewhere, it's published in the Journal of Research on educational effectiveness.
00:25:30.954 - 00:26:17.476, Speaker A: It has two additional papers tacked to it. One is a technical appendix, which basically walks through all the estimators I discussed, gives some details and notes on their use. It's a nice reference that I would encourage people to check out if interested. And then you can also look at the multifactor simulation, which looks at a lot of these questions under hypothetical multilevel model data generating process, and sort of digs into some of the things that I alluded to. Overall, the stand that we take is the estimate choice matters a lot, and the beta estimator choice matters for the site super estimand, but doesn't really matter otherwise. And the standard error estimator matters for site but less for person. And then the superpopulation site estimators differ the most.
00:26:17.476 - 00:26:41.030, Speaker A: That's the hardest thing to estimate, is a superpopulation site average effect. A lot of instability there. Perhaps that means you should turn to more bayesian approaches when doing that kind of work. All right, that's it. I hope that was of interest. I'm happy to talk offline. Just email me whatever you like.
00:26:41.030 - 00:27:07.514, Speaker A: Happy to share the paper, although that's out in the world. And, oh, there's also an r package that just sort of implements all of these estimators for a given data set. So it's really easy to compare these estimators to each other. And I'm happy to share that as well. Thank you. Okay. Yeah.
